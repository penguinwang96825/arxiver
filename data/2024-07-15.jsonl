{"title": "Nash Equilibrium between Brokers and Traders", "abstract": "We study the perfect information Nash equilibrium between a broker and her\nclients -- an informed trader and an uniformed trader. In our model, the broker\ntrades in the lit exchange where trades have instantaneous and transient price\nimpact with exponential resilience, while both clients trade with the broker.\nThe informed trader and the broker maximise expected wealth subject to\ninventory penalties, while the uninformed trader is not strategic and sends the\nbroker random buy and sell orders. We characterise the Nash equilibrium of the\ntrading strategies with the solution to a coupled system of forward-backward\nstochastic differential equations (FBSDEs). We solve this system explicitly and\nstudy the effect of information, profitability, and inventory control in the\ntrading strategies of the broker and the informed trader.", "published": "2024-07-15 09:23:05", "link": "http://arxiv.org/abs/2407.10561v2", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "When AI Meets Finance (StockAgent): Large Language Model-based Stock Trading in Simulated Real-world Environments", "abstract": "Can AI Agents simulate real-world trading environments to investigate the\nimpact of external factors on stock trading activities (e.g., macroeconomics,\npolicy changes, company fundamentals, and global events)? These factors, which\nfrequently influence trading behaviors, are critical elements in the quest for\nmaximizing investors' profits. Our work attempts to solve this problem through\nlarge language model based agents. We have developed a multi-agent AI system\ncalled StockAgent, driven by LLMs, designed to simulate investors' trading\nbehaviors in response to the real stock market. The StockAgent allows users to\nevaluate the impact of different external factors on investor trading and to\nanalyze trading behavior and profitability effects. Additionally, StockAgent\navoids the test set leakage issue present in existing trading simulation\nsystems based on AI Agents. Specifically, it prevents the model from leveraging\nprior knowledge it may have acquired related to the test data. We evaluate\ndifferent LLMs under the framework of StockAgent in a stock trading environment\nthat closely resembles real-world conditions. The experimental results\ndemonstrate the impact of key external factors on stock market trading,\nincluding trading behavior and stock price fluctuation rules. This research\nexplores the study of agents' free trading gaps in the context of no prior\nknowledge related to market data. The patterns identified through StockAgent\nsimulations provide valuable insights for LLM-based investment advice and stock\nrecommendation. The code is available at\nhttps://github.com/MingyuJ666/Stockagent.", "published": "2024-07-15 06:49:30", "link": "http://arxiv.org/abs/2407.18957v4", "categories": ["q-fin.TR", "cs.AI", "cs.MA"], "primary_category": "q-fin.TR"}
{"title": "Enhancing Medication Recommendation with LLM Text Representation", "abstract": "Most of the existing medication recommendation models are predicted with only\nstructured data such as medical codes, with the remaining other large amount of\nunstructured or semi-structured data underutilization. To increase the\nutilization effectively, we proposed a method of enhancing medication\nrecommendation with Large Language Model (LLM) text representation. LLM\nharnesses powerful language understanding and generation capabilities, enabling\nthe extraction of information from complex and lengthy unstructured data such\nas clinical notes which contain complex terminology. This method can be applied\nto several existing base models we selected and improve medication\nrecommendation performance with the combination representation of text and\nmedical codes experiments on two different datasets. LLM text representation\nalone can even demonstrate a comparable ability to the medical code\nrepresentation alone. Overall, this is a general method that can be applied to\nother models for improved recommendations.", "published": "2024-07-15 05:51:11", "link": "http://arxiv.org/abs/2407.10453v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Throw Away Data: Better Sequence Knowledge Distillation", "abstract": "A critical component in knowledge distillation is the means of coupling the\nteacher and student. The predominant sequence knowledge distillation method\ninvolves supervised learning of the student against teacher-decoded outputs,\nand is exemplified by the current state of the art, which incorporates minimum\nBayes risk (MBR) decoding. In this paper we seek to integrate MBR more tightly\nin distillation training, specifically by using several high scoring MBR\ntranslations, rather than a single selected sequence, thus capturing a rich\ndiversity of teacher outputs. Our experiments on English to German and English\nto Japanese translation show consistent improvements over strong baseline\nmethods for both tasks and with varying model sizes. Additionally, we conduct a\ndetailed analysis focusing on data efficiency and capacity curse aspects to\nelucidate MBR-n and explore its further potential.", "published": "2024-07-15 06:11:18", "link": "http://arxiv.org/abs/2407.10456v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CIBench: Evaluating Your LLMs with a Code Interpreter Plugin", "abstract": "While LLM-Based agents, which use external tools to solve complex problems,\nhave made significant progress, benchmarking their ability is challenging,\nthereby hindering a clear understanding of their limitations. In this paper, we\npropose an interactive evaluation framework, named CIBench, to comprehensively\nassess LLMs' ability to utilize code interpreters for data science tasks. Our\nevaluation framework includes an evaluation dataset and two evaluation modes.\nThe evaluation dataset is constructed using an LLM-human cooperative approach\nand simulates an authentic workflow by leveraging consecutive and interactive\nIPython sessions. The two evaluation modes assess LLMs' ability with and\nwithout human assistance. We conduct extensive experiments to analyze the\nability of 24 LLMs on CIBench and provide valuable insights for future LLMs in\ncode interpreter utilization.", "published": "2024-07-15 07:43:55", "link": "http://arxiv.org/abs/2407.10499v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Generative Artificial Intelligence: Roadmap for Natural Language\n  Generation", "abstract": "Generative Artificial Intelligence has grown exponentially as a result of\nLarge Language Models (LLMs). This has been possible because of the impressive\nperformance of deep learning methods created within the field of Natural\nLanguage Processing (NLP) and its subfield Natural Language Generation (NLG),\nwhich is the focus of this paper. Within the growing LLM family are the popular\nGPT-4, Bard and more specifically, tools such as ChatGPT have become a\nbenchmark for other LLMs when solving most of the tasks involved in NLG\nresearch. This scenario poses new questions about the next steps for NLG and\nhow the field can adapt and evolve to deal with new challenges in the era of\nLLMs. To address this, the present paper conducts a review of a representative\nsample of surveys recently published in NLG. By doing so, we aim to provide the\nscientific community with a research roadmap to identify which NLG aspects are\nstill not suitably addressed by LLMs, as well as suggest future lines of\nresearch that should be addressed going forward.", "published": "2024-07-15 09:07:07", "link": "http://arxiv.org/abs/2407.10554v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NoviCode: Generating Programs from Natural Language Utterances by\n  Novices", "abstract": "Current Text-to-Code models demonstrate impressive capabilities in generating\nexecutable code from natural language snippets. However, current studies focus\non technical instructions and programmer-oriented language, and it is an open\nquestion whether these models can effectively translate natural language\ndescriptions given by non-technical users and express complex goals, to an\nexecutable program that contains an intricate flow - composed of API access and\ncontrol structures as loops, conditions, and sequences. To unlock the challenge\nof generating a complete program from a plain non-technical description we\npresent NoviCode, a novel NL Programming task, which takes as input an API and\na natural language description by a novice non-programmer and provides an\nexecutable program as output. To assess the efficacy of models on this task, we\nprovide a novel benchmark accompanied by test suites wherein the generated\nprogram code is assessed not according to their form, but according to their\nfunctional execution. Our experiments show that, first, NoviCode is indeed a\nchallenging task in the code synthesis domain, and that generating complex code\nfrom non-technical instructions goes beyond the current Text-to-Code paradigm.\nSecond, we show that a novel approach wherein we align the NL utterances with\nthe compositional hierarchical structure of the code, greatly enhances the\nperformance of LLMs on this task, compared with the end-to-end Text-to-Code\ncounterparts.", "published": "2024-07-15 11:26:03", "link": "http://arxiv.org/abs/2407.10626v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DOCBENCH: A Benchmark for Evaluating LLM-based Document Reading Systems", "abstract": "Recently, there has been a growing interest among large language model (LLM)\ndevelopers in LLM-based document reading systems, which enable users to upload\ntheir own documents and pose questions related to the document contents, going\nbeyond simple reading comprehension tasks. Consequently, these systems have\nbeen carefully designed to tackle challenges such as file parsing, metadata\nextraction, multi-modal information understanding and long-context reading.\nHowever, no current benchmark exists to evaluate their performance in such\nscenarios, where a raw file and questions are provided as input, and a\ncorresponding response is expected as output. In this paper, we introduce\nDocBench, a new benchmark designed to evaluate LLM-based document reading\nsystems. Our benchmark involves a meticulously crafted process, including the\nrecruitment of human annotators and the generation of synthetic questions. It\nincludes 229 real documents and 1,102 questions, spanning across five different\ndomains and four major types of questions. We evaluate both proprietary\nLLM-based systems accessible via web interfaces or APIs, and a parse-then-read\npipeline employing open-source LLMs. Our evaluations reveal noticeable gaps\nbetween existing LLM-based document reading systems and human performance,\nunderscoring the challenges of developing proficient systems. To summarize,\nDocBench aims to establish a standardized benchmark for evaluating LLM-based\ndocument reading systems under diverse real-world scenarios, thereby guiding\nfuture advancements in this research area.", "published": "2024-07-15 13:17:42", "link": "http://arxiv.org/abs/2407.10701v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What distinguishes conspiracy from critical narratives? A computational\n  analysis of oppositional discourse", "abstract": "The current prevalence of conspiracy theories on the internet is a\nsignificant issue, tackled by many computational approaches. However, these\napproaches fail to recognize the relevance of distinguishing between texts\nwhich contain a conspiracy theory and texts which are simply critical and\noppose mainstream narratives. Furthermore, little attention is usually paid to\nthe role of inter-group conflict in oppositional narratives. We contribute by\nproposing a novel topic-agnostic annotation scheme that differentiates between\nconspiracies and critical texts, and that defines span-level categories of\ninter-group conflict. We also contribute with the multilingual\nXAI-DisInfodemics corpus (English and Spanish), which contains a high-quality\nannotation of Telegram messages related to COVID-19 (5,000 messages per\nlanguage). We also demonstrate the feasibility of an NLP-based automatization\nby performing a range of experiments that yield strong baseline solutions.\nFinally, we perform an analysis which demonstrates that the promotion of\nintergroup conflict and the presence of violence and anger are key aspects to\ndistinguish between the two types of oppositional narratives, i.e., conspiracy\nvs. critical.", "published": "2024-07-15 14:18:47", "link": "http://arxiv.org/abs/2407.10745v1", "categories": ["cs.CL", "I.2.7; J.4"], "primary_category": "cs.CL"}
{"title": "Codebook LLMs: Evaluating LLMs as Measurement Tools for Political\n  Science Concepts", "abstract": "Codebooks -- documents that operationalize concepts and outline annotation\nprocedures -- are used almost universally by social scientists when coding\npolitical texts. To code these texts automatically, researchers are increasing\nturning to generative large language models (LLMs). However, there is limited\nempirical evidence on whether \"off-the-shelf\" LLMs faithfully follow real-world\ncodebook operationalizations and measure complex political constructs with\nsufficient accuracy. To address this, we gather and curate three real-world\npolitical science codebooks -- covering protest events, political violence and\nmanifestos -- along with their unstructured texts and human labels. We also\npropose a five-stage framework for codebook-LLM measurement: preparing a\ncodebook for both humans and LLMs, testing LLMs' basic capabilities on a\ncodebook, evaluating zero-shot measurement accuracy (i.e. off-the-shelf\nperformance), analyzing errors, and further (parameter-efficient) supervised\ntraining of LLMs. We provide an empirical demonstration of this framework using\nour three codebook datasets and several pretrained 7-12 billion open-weight\nLLMs. We find current open-weight LLMs have limitations in following codebooks\nzero-shot, but that supervised instruction tuning can substantially improve\nperformance. Rather than suggesting the \"best\" LLM, our contribution lies in\nour codebook datasets, evaluation framework, and guidance for applied\nresearchers who wish to implement their own codebook-LLM measurement projects.", "published": "2024-07-15 14:20:09", "link": "http://arxiv.org/abs/2407.10747v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Contrastive Decoding via Language-Agnostic Layers Skipping", "abstract": "Decoding by contrasting layers (DoLa), is designed to improve the generation\nquality of large language models (LLMs) by contrasting the prediction\nprobabilities between an early exit output (amateur logits) and the final\noutput (expert logits). However, we find that this approach does not work well\non non-English tasks. Inspired by previous interpretability work on language\ntransition during the model's forward pass, we discover that this issue arises\nfrom a language mismatch between early exit output and final output. In this\nwork, we propose an improved contrastive decoding algorithm that is effective\nfor diverse languages beyond English. To obtain more helpful amateur logits, we\ndevise two strategies to skip a set of bottom, language-agnostic layers based\non our preliminary analysis. Experimental results on multilingual reasoning\nbenchmarks demonstrate that our proposed method outperforms previous\ncontrastive decoding baselines and substantially improves LLM's\nchain-of-thought reasoning accuracy across 11 languages. The project will be\navailable at: https://github.com/NJUNLP/SkipLayerCD.", "published": "2024-07-15 15:14:01", "link": "http://arxiv.org/abs/2407.10795v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning\n  and Format Alignment", "abstract": "Adapting general large language models (LLMs) to specialized domains presents\ngreat challenges due to varied data distributions. This adaptation typically\nrequires continual pre-training on massive domain-specific corpora to\nfacilitate knowledge memorization, followed by training to apply this knowledge\nfollowing human instructions and preferences. However, this method may result\nin inefficient knowledge memorization due to a lack of awareness of knowledge\nutilization and imposes substantial demands on LLMs to simultaneously learn\nknowledge utilization and format alignment with limited training samples. To\nfacilitate the domain adaptation of LLM, we revise this process and propose a\nnew domain adaptation framework including domain knowledge learning and general\nformat alignment, called Mix-CPT. Specifically, we first conduct a knowledge\nmixture continual pre-training that concurrently focuses on knowledge\nmemorization and utilization, allowing for mutual reinforcement. To avoid\ncatastrophic forgetting during the continual pre-training process, we further\nincorporate a logit swap self-distillation constraint. Subsequently, leveraging\nthe knowledge and capabilities acquired during continual pre-training, we\nefficiently perform instruction tuning and alignment with a few general\ntraining samples to achieve format alignment. Extensive experiments demonstrate\nthat our proposed Mix-CPT framework can simultaneously improve the task-solving\ncapabilities of LLMs on the target and general domains compared to the\ntraditional adaptation methods.", "published": "2024-07-15 15:20:13", "link": "http://arxiv.org/abs/2407.10804v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Naturally Occurring Feedback is Common, Extractable and Useful", "abstract": "Human feedback data is a critical component in developing language models.\nHowever, collecting this feedback is costly and ultimately not scalable.\nInspired by the way human interlocutors provide spontaneous unsolicited\nfeedback to each other, we propose to extract feedback that users naturally\ninclude when interacting with chat models. We manually annotated conversations\nto confirm the presence of naturally occurring feedback in a standard corpus,\nfinding that as much as 30% of the chats include explicit feedback. Comparing\nto older datasets, we find that naturally occurring feedback is more prevalent\nin recent conversation datasets, suggesting that more than ever, naturally\noccurring feedback can serve as a valuable resource for feedback data. We\npropose a method for automatically extracting this feedback, and apply it to\nover 1M conversations to obtain hundreds of thousands of feedback samples. The\nextracted feedback shows promise: training with it improves over baseline\nmodels and enhances model alignment to human preferences.", "published": "2024-07-15 17:41:34", "link": "http://arxiv.org/abs/2407.10944v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MMM: Multilingual Mutual Reinforcement Effect Mix Datasets & Test with\n  Open-domain Information Extraction Large Language Models", "abstract": "The Mutual Reinforcement Effect (MRE) represents a promising avenue in\ninformation extraction and multitasking research. Nevertheless, its\napplicability has been constrained due to the exclusive availability of MRE mix\ndatasets in Japanese, thereby limiting comprehensive exploration by the global\nresearch community. To address this limitation, we introduce a Multilingual MRE\nmix dataset (MMM) that encompasses 21 sub-datasets in English, Japanese, and\nChinese. In this paper, we also propose a method for dataset translation\nassisted by Large Language Models (LLMs), which significantly reduces the\nmanual annotation time required for dataset construction by leveraging LLMs to\ntranslate the original Japanese datasets. Additionally, we have enriched the\ndataset by incorporating open-domain Named Entity Recognition (NER) and\nsentence classification tasks. Utilizing this expanded dataset, we developed a\nunified input-output framework to train an Open-domain Information Extraction\nLarge Language Model (OIELLM). The OIELLM model demonstrates the capability to\neffectively process novel MMM datasets, exhibiting significant improvements in\nperformance. The OIELLM model and datasets is open-source in HuggingFace:\nhttps://ganchengguang.github.io/MRE/", "published": "2024-07-15 17:50:43", "link": "http://arxiv.org/abs/2407.10953v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "YouTube-SL-25: A Large-Scale, Open-Domain Multilingual Sign Language\n  Parallel Corpus", "abstract": "Even for better-studied sign languages like American Sign Language (ASL),\ndata is the bottleneck for machine learning research. The situation is worse\nyet for the many other sign languages used by Deaf/Hard of Hearing communities\naround the world. In this paper, we present YouTube-SL-25, a large-scale,\nopen-domain multilingual corpus of sign language videos with seemingly\nwell-aligned captions drawn from YouTube. With >3000 hours of videos across >25\nsign languages, YouTube-SL-25 is a) >3x the size of YouTube-ASL, b) the largest\nparallel sign language dataset to date, and c) the first or largest parallel\ndataset for many of its component languages. We provide baselines for\nsign-to-text tasks using a unified multilingual multitask model based on T5 and\nreport scores on benchmarks across 4 sign languages. The results demonstrate\nthat multilingual transfer benefits both higher- and lower-resource sign\nlanguages within YouTube-SL-25.", "published": "2024-07-15 18:08:34", "link": "http://arxiv.org/abs/2407.11144v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Actuation without production bias", "abstract": "Phonetic production bias is the external force most commonly invoked in\ncomputational models of sound change, despite the fact that it is not\nresponsible for all, or even most, sound changes. Furthermore, the existence of\nproduction bias alone cannot account for how changes do or do not propagate\nthroughout a speech community. While many other factors have been invoked by\n(socio)phoneticians, including but not limited to contact (between\nsubpopulations) and differences in social evaluation (of variants, groups, or\nindividuals), these are not typically modeled in computational simulations of\nsound change. In this paper, we consider whether production biases have a\nunique dynamics in terms of how they impact the population-level spread of\nchange in a setting where agents learn from multiple teachers. We show that,\nwhile the dynamics conditioned by production bias are not unique, it is not the\ncase that all perturbing forces have the same dynamics: in particular, if\nsocial weight is a function of individual teachers and the correlation between\na teacher's social weight and the extent to which they realize a production\nbias is weak, change is unlikely to propagate. Nevertheless, it remains the\ncase that changes initiated from different sources may display a similar\ndynamics. A more nuanced understanding of how population structure interacts\nwith individual biases can thus provide a (partial) solution to the\n`non-phonologization problem'.", "published": "2024-07-15 19:38:03", "link": "http://arxiv.org/abs/2407.11202v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncertainty is Fragile: Manipulating Uncertainty in Large Language\n  Models", "abstract": "Large Language Models (LLMs) are employed across various high-stakes domains,\nwhere the reliability of their outputs is crucial. One commonly used method to\nassess the reliability of LLMs' responses is uncertainty estimation, which\ngauges the likelihood of their answers being correct. While many studies focus\non improving the accuracy of uncertainty estimations for LLMs, our research\ninvestigates the fragility of uncertainty estimation and explores potential\nattacks. We demonstrate that an attacker can embed a backdoor in LLMs, which,\nwhen activated by a specific trigger in the input, manipulates the model's\nuncertainty without affecting the final output. Specifically, the proposed\nbackdoor attack method can alter an LLM's output probability distribution,\ncausing the probability distribution to converge towards an attacker-predefined\ndistribution while ensuring that the top-1 prediction remains unchanged. Our\nexperimental results demonstrate that this attack effectively undermines the\nmodel's self-evaluation reliability in multiple-choice questions. For instance,\nwe achieved a 100 attack success rate (ASR) across three different triggering\nstrategies in four models. Further, we investigate whether this manipulation\ngeneralizes across different prompts and domains. This work highlights a\nsignificant threat to the reliability of LLMs and underscores the need for\nfuture defenses against such attacks. The code is available at\nhttps://github.com/qcznlp/uncertainty_attack.", "published": "2024-07-15 23:41:11", "link": "http://arxiv.org/abs/2407.11282v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Model-based FMRI Encoding of Language Functions for\n  Subjects with Neurocognitive Disorder", "abstract": "Functional magnetic resonance imaging (fMRI) is essential for developing\nencoding models that identify functional changes in language-related brain\nareas of individuals with Neurocognitive Disorders (NCD). While large language\nmodel (LLM)-based fMRI encoding has shown promise, existing studies\npredominantly focus on healthy, young adults, overlooking older NCD populations\nand cognitive level correlations. This paper explores language-related\nfunctional changes in older NCD adults using LLM-based fMRI encoding and brain\nscores, addressing current limitations. We analyze the correlation between\nbrain scores and cognitive scores at both whole-brain and language-related ROI\nlevels. Our findings reveal that higher cognitive abilities correspond to\nbetter brain scores, with correlations peaking in the middle temporal gyrus.\nThis study highlights the potential of fMRI encoding models and brain scores\nfor detecting early functional changes in NCD patients.", "published": "2024-07-15 01:09:08", "link": "http://arxiv.org/abs/2407.10376v1", "categories": ["q-bio.NC", "cs.CL"], "primary_category": "q-bio.NC"}
{"title": "Expanding the Scope: Inductive Knowledge Graph Reasoning with\n  Multi-Starting Progressive Propagation", "abstract": "Knowledge graphs (KGs) are widely acknowledged as incomplete, and new\nentities are constantly emerging in the real world. Inductive KG reasoning aims\nto predict missing facts for these new entities. Among existing models, graph\nneural networks (GNNs) based ones have shown promising performance for this\ntask. However, they are still challenged by inefficient message propagation due\nto the distance and scalability issues. In this paper, we propose a new\ninductive KG reasoning model, MStar, by leveraging conditional message passing\nneural networks (C-MPNNs). Our key insight is to select multiple query-specific\nstarting entities to expand the scope of progressive propagation. To propagate\nquery-related messages to a farther area within limited steps, we subsequently\ndesign a highway layer to propagate information toward these selected starting\nentities. Moreover, we introduce a training strategy called LinkVerify to\nmitigate the impact of noisy training samples. Experimental results validate\nthat MStar achieves superior performance compared with state-of-the-art models,\nespecially for distant entities.", "published": "2024-07-15 04:16:20", "link": "http://arxiv.org/abs/2407.10430v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore\n  Non-Determinism", "abstract": "Current evaluations of large language models (LLMs) often overlook\nnon-determinism, typically focusing on a single output per example. This limits\nour understanding of LLM performance variability in real-world applications.\nOur study addresses this issue by exploring key questions about the performance\ndifferences between greedy decoding and sampling, identifying benchmarks'\nconsistency regarding non-determinism, and examining unique model behaviors.\nThrough extensive experiments, we observe that greedy decoding generally\noutperforms sampling methods for most evaluated tasks. We also observe\nconsistent performance across different LLM sizes and alignment methods, noting\nthat alignment can reduce sampling variance. Moreover, our best-of-N sampling\napproach demonstrates that smaller LLMs can match or surpass larger models such\nas GPT-4-Turbo, highlighting the untapped potential of smaller LLMs. This\nresearch shows the importance of considering non-determinism in LLM evaluations\nand provides insights for future LLM development and evaluation.", "published": "2024-07-15 06:12:17", "link": "http://arxiv.org/abs/2407.10457v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IDEAL: Leveraging Infinite and Dynamic Characterizations of Large\n  Language Models for Query-focused Summarization", "abstract": "Query-focused summarization (QFS) aims to produce summaries that answer\nparticular questions of interest, enabling greater user control and\npersonalization. With the advent of large language models (LLMs), shows their\nimpressive capability of textual understanding through large-scale pretraining,\nwhich implies the great potential of extractive snippet generation. In this\npaper, we systematically investigated two indispensable characteristics that\nthe LLMs-based QFS models should be harnessed, Lengthy Document Summarization\nand Efficiently Fine-grained Query-LLM Alignment, respectively.\nCorrespondingly, we propose two modules called Query-aware HyperExpert and\nQuery-focused Infini-attention to access the aforementioned characteristics.\nThese innovations pave the way for broader application and accessibility in the\nfield of QFS technology. Extensive experiments conducted on existing QFS\nbenchmarks indicate the effectiveness and generalizability of the proposed\napproach. Our code is publicly available at\nhttps://github.com/DCDmllm/IDEAL_Summary.", "published": "2024-07-15 07:14:56", "link": "http://arxiv.org/abs/2407.10486v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "How and where does CLIP process negation?", "abstract": "Various benchmarks have been proposed to test linguistic understanding in\npre-trained vision \\& language (VL) models. Here we build on the existence task\nfrom the VALSE benchmark (Parcalabescu et al, 2022) which we use to test\nmodels' understanding of negation, a particularly interesting issue for\nmultimodal models. However, while such VL benchmarks are useful for measuring\nmodel performance, they do not reveal anything about the internal processes\nthrough which these models arrive at their outputs in such visio-linguistic\ntasks. We take inspiration from the growing literature on model\ninterpretability to explain the behaviour of VL models on the understanding of\nnegation. Specifically, we approach these questions through an in-depth\nanalysis of the text encoder in CLIP (Radford et al, 2021), a highly\ninfluential VL model. We localise parts of the encoder that process negation\nand analyse the role of attention heads in this task. Our contributions are\nthreefold. We demonstrate how methods from the language model interpretability\nliterature (such as causal tracing) can be translated to multimodal models and\ntasks; we provide concrete insights into how CLIP processes negation on the\nVALSE existence task; and we highlight inherent limitations in the VALSE\ndataset as a benchmark for linguistic understanding.", "published": "2024-07-15 07:20:06", "link": "http://arxiv.org/abs/2407.10488v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Boosting Zero-Shot Crosslingual Performance using LLM-Based\n  Augmentations with Effective Data Selection", "abstract": "Large language models (LLMs) are very proficient text generators. We leverage\nthis capability of LLMs to generate task-specific data via zero-shot prompting\nand promote cross-lingual transfer for low-resource target languages. Given\ntask-specific data in a source language and a teacher model trained on this\ndata, we propose using this teacher to label LLM generations and employ a set\nof simple data selection strategies that use the teacher's label probabilities.\nOur data selection strategies help us identify a representative subset of\ndiverse generations that help boost zero-shot accuracies while being efficient,\nin comparison to using all the LLM generations (without any subset selection).\nWe also highlight other important design choices that affect cross-lingual\nperformance such as the use of translations of source data and what labels are\nbest to use for the LLM generations. We observe significant performance gains\nacross sentiment analysis and natural language inference tasks (of up to a\nmaximum of 7.13 absolute points and 1.5 absolute points on average) across a\nnumber of target languages (Hindi, Marathi, Urdu, Swahili) and domains.", "published": "2024-07-15 10:00:22", "link": "http://arxiv.org/abs/2407.10582v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of Validating Synthetic Data for Formula Generation", "abstract": "Large language models (LLMs) can be leveraged to help with writing formulas\nin spreadsheets, but resources on these formulas are scarce, impacting both the\nbase performance of pre-trained models and limiting the ability to fine-tune\nthem. Given a corpus of formulas, we can use a(nother) model to generate\nsynthetic natural language utterances for fine-tuning. However, it is important\nto validate whether the NL generated by the LLM is indeed accurate to be\nbeneficial for fine-tuning. In this paper, we provide empirical results on the\nimpact of validating these synthetic training examples with surrogate\nobjectives that evaluate the accuracy of the synthetic annotations. We\ndemonstrate that validation improves performance over raw data across four\nmodels (2 open and 2 closed weight). Interestingly, we show that although\nvalidation tends to prune more challenging examples, it increases the\ncomplexity of problems that models can solve after being fine-tuned on\nvalidated data.", "published": "2024-07-15 12:16:33", "link": "http://arxiv.org/abs/2407.10657v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for\n  Improved Quality and Efficiency in RAG Systems", "abstract": "Retrieval-augmented generation (RAG) techniques leverage the in-context\nlearning capabilities of large language models (LLMs) to produce more accurate\nand relevant responses. Originating from the simple 'retrieve-then-read'\napproach, the RAG framework has evolved into a highly flexible and modular\nparadigm. A critical component, the Query Rewriter module, enhances knowledge\nretrieval by generating a search-friendly query. This method aligns input\nquestions more closely with the knowledge base. Our research identifies\nopportunities to enhance the Query Rewriter module to Query Rewriter+ by\ngenerating multiple queries to overcome the Information Plateaus associated\nwith a single query and by rewriting questions to eliminate Ambiguity, thereby\nclarifying the underlying intent. We also find that current RAG systems exhibit\nissues with Irrelevant Knowledge; to overcome this, we propose the Knowledge\nFilter. These two modules are both based on the instruction-tuned Gemma-2B\nmodel, which together enhance response quality. The final identified issue is\nRedundant Retrieval; we introduce the Memory Knowledge Reservoir and the\nRetriever Trigger to solve this. The former supports the dynamic expansion of\nthe RAG system's knowledge base in a parameter-free manner, while the latter\noptimizes the cost for accessing external knowledge, thereby improving resource\nutilization and response efficiency. These four RAG modules synergistically\nimprove the response quality and efficiency of the RAG system. The\neffectiveness of these modules has been validated through experiments and\nablation studies across six common QA datasets. The source code can be accessed\nat https://github.com/Ancientshi/ERM4.", "published": "2024-07-15 12:35:00", "link": "http://arxiv.org/abs/2407.10670v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Qwen2 Technical Report", "abstract": "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.", "published": "2024-07-15 12:35:42", "link": "http://arxiv.org/abs/2407.10671v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "$\\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific\n  Domain through Complementary Granularity", "abstract": "Recent studies show the growing significance of document retrieval in the\ngeneration of LLMs, i.e., RAG, within the scientific domain by bridging their\nknowledge gap. However, dense retrievers often struggle with domain-specific\nretrieval and complex query-document relationships, particularly when query\nsegments correspond to various parts of a document. To alleviate such prevalent\nchallenges, this paper introduces $\\texttt{MixGR}$, which improves dense\nretrievers' awareness of query-document matching across various levels of\ngranularity in queries and documents using a zero-shot approach.\n$\\texttt{MixGR}$ fuses various metrics based on these granularities to a united\nscore that reflects a comprehensive query-document similarity. Our experiments\ndemonstrate that $\\texttt{MixGR}$ outperforms previous document retrieval by\n24.7%, 9.8%, and 6.9% on nDCG@5 with unsupervised, supervised, and LLM-based\nretrievers, respectively, averaged on queries containing multiple subqueries\nfrom five scientific retrieval datasets. Moreover, the efficacy of two\ndownstream scientific question-answering tasks highlights the advantage of\n$\\texttt{MixGR}$ to boost the application of LLMs in the scientific domain. The\ncode and experimental datasets are available.", "published": "2024-07-15 13:04:09", "link": "http://arxiv.org/abs/2407.10691v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Sibyl: Simple yet Effective Agent Framework for Complex Real-world\n  Reasoning", "abstract": "Existing agents based on large language models (LLMs) demonstrate robust\nproblem-solving capabilities by integrating LLMs' inherent knowledge, strong\nin-context learning and zero-shot capabilities, and the use of tools combined\nwith intricately designed LLM invocation workflows by humans. However, these\nagents still exhibit shortcomings in long-term reasoning and under-use the\npotential of existing tools, leading to noticeable deficiencies in complex\nreal-world reasoning scenarios. To address these limitations, we introduce\nSibyl, a simple yet powerful LLM-based agent framework designed to tackle\ncomplex reasoning tasks by efficiently leveraging a minimal set of tools.\nDrawing inspiration from Global Workspace Theory, Sibyl incorporates a global\nworkspace to enhance the management and sharing of knowledge and conversation\nhistory throughout the system. Furthermore, guided by Society of Mind Theory,\nSibyl implements a multi-agent debate-based jury to self-refine the final\nanswers, ensuring a comprehensive and balanced approach. This approach aims to\nreduce system complexity while expanding the scope of problems solvable-from\nmatters typically resolved by humans in minutes to those requiring hours or\neven days, thus facilitating a shift from System-1 to System-2 thinking. Sibyl\nhas been designed with a focus on scalability and ease of debugging by\nincorporating the concept of reentrancy from functional programming from its\ninception, with the aim of seamless and low effort integration in other LLM\napplications to improve capabilities. Our experimental results on the GAIA\nbenchmark test set reveal that the Sibyl agent instantiated with GPT-4 achieves\nstate-of-the-art performance with an average score of 34.55%, compared to other\nagents based on GPT-4. We hope that Sibyl can inspire more reliable and\nreusable LLM-based agent solutions to address complex real-world reasoning\ntasks.", "published": "2024-07-15 13:45:40", "link": "http://arxiv.org/abs/2407.10718v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "CLAVE: An Adaptive Framework for Evaluating Values of LLM Generated\n  Responses", "abstract": "The rapid progress in Large Language Models (LLMs) poses potential risks such\nas generating unethical content. Assessing LLMs' values can help expose their\nmisalignment, but relies on reference-free evaluators, e.g., fine-tuned LLMs or\nclose-source ones like GPT-4, to identify values reflected in generated\nresponses. Nevertheless, these evaluators face two challenges in open-ended\nvalue evaluation: they should align with changing human value definitions with\nminimal annotation, against their own bias (adaptability), and detect varying\nvalue expressions and scenarios robustly (generalizability). To handle these\nchallenges, we introduce CLAVE, a novel framework which integrates two\ncomplementary LLMs, a large one to extract high-level value concepts from a few\nhuman labels, leveraging its extensive knowledge and generalizability, and a\nsmaller one fine-tuned on such concepts to better align with human value\nunderstanding. This dual-model approach enables calibration with any value\nsystems using <100 human-labeled samples per value type. Then we present\nValEval, a comprehensive dataset comprising 13k+ (text,value,label) tuples\nacross diverse domains, covering three major value systems. We benchmark the\ncapabilities of 12+ popular LLM evaluators and analyze their strengths and\nweaknesses. Our findings reveal that combining fine-tuned small models and\nprompt-based large ones serves as a superior balance in value evaluation.", "published": "2024-07-15 13:51:37", "link": "http://arxiv.org/abs/2407.10725v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Graphusion: Leveraging Large Language Models for Scientific Knowledge\n  Graph Fusion and Construction in NLP Education", "abstract": "Knowledge graphs (KGs) are crucial in the field of artificial intelligence\nand are widely applied in downstream tasks, such as enhancing Question\nAnswering (QA) systems. The construction of KGs typically requires significant\neffort from domain experts. Recently, Large Language Models (LLMs) have been\nused for knowledge graph construction (KGC), however, most existing approaches\nfocus on a local perspective, extracting knowledge triplets from individual\nsentences or documents. In this work, we introduce Graphusion, a zero-shot KGC\nframework from free text. The core fusion module provides a global view of\ntriplets, incorporating entity merging, conflict resolution, and novel triplet\ndiscovery. We showcase how Graphusion could be applied to the natural language\nprocessing (NLP) domain and validate it in the educational scenario.\nSpecifically, we introduce TutorQA, a new expert-verified benchmark for graph\nreasoning and QA, comprising six tasks and a total of 1,200 QA pairs. Our\nevaluation demonstrates that Graphusion surpasses supervised baselines by up to\n10% in accuracy on link prediction. Additionally, it achieves average scores of\n2.92 and 2.37 out of 3 in human evaluations for concept entity extraction and\nrelation recognition, respectively.", "published": "2024-07-15 15:13:49", "link": "http://arxiv.org/abs/2407.10794v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning\n  with Knowledge-guided Retrieval Augmented Generation", "abstract": "Retrieval-augmented generation (RAG) has improved large language models\n(LLMs) by using knowledge retrieval to overcome knowledge deficiencies.\nHowever, current RAG methods often fall short of ensuring the depth and\ncompleteness of retrieved information, which is necessary for complex reasoning\ntasks. In this work, we introduce Think-on-Graph 2.0 (ToG-2), a hybrid RAG\nframework that iteratively retrieves information from both unstructured and\nstructured knowledge sources in a tight-coupling manner. Specifically, ToG-2\nleverages knowledge graphs (KGs) to link documents via entities, facilitating\ndeep and knowledge-guided context retrieval. Simultaneously, it utilizes\ndocuments as entity contexts to achieve precise and efficient graph retrieval.\nToG-2 alternates between graph retrieval and context retrieval to search for\nin-depth clues relevant to the question, enabling LLMs to generate answers. We\nconduct a series of well-designed experiments to highlight the following\nadvantages of ToG-2: 1) ToG-2 tightly couples the processes of context\nretrieval and graph retrieval, deepening context retrieval via the KG while\nenabling reliable graph retrieval based on contexts; 2) it achieves deep and\nfaithful reasoning in LLMs through an iterative knowledge retrieval process of\ncollaboration between contexts and the KG; and 3) ToG-2 is training-free and\nplug-and-play compatible with various LLMs. Extensive experiments demonstrate\nthat ToG-2 achieves overall state-of-the-art (SOTA) performance on 6 out of 7\nknowledge-intensive datasets with GPT-3.5, and can elevate the performance of\nsmaller models (e.g., LLAMA-2-13B) to the level of GPT-3.5's direct reasoning.\nThe source code is available on https://github.com/IDEA-FinAI/ToG-2.", "published": "2024-07-15 15:20:40", "link": "http://arxiv.org/abs/2407.10805v7", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Employing Sentence Space Embedding for Classification of Data Stream\n  from Fake News Domain", "abstract": "Tabular data is considered the last unconquered castle of deep learning, yet\nthe task of data stream classification is stated to be an equally important and\ndemanding research area. Due to the temporal constraints, it is assumed that\ndeep learning methods are not the optimal solution for application in this\nfield. However, excluding the entire -- and prevalent -- group of methods seems\nrather rash given the progress that has been made in recent years in its\ndevelopment. For this reason, the following paper is the first to present an\napproach to natural language data stream classification using the sentence\nspace method, which allows for encoding text into the form of a discrete\ndigital signal. This allows the use of convolutional deep networks dedicated to\nimage classification to solve the task of recognizing fake news based on text\ndata. Based on the real-life Fakeddit dataset, the proposed approach was\ncompared with state-of-the-art algorithms for data stream classification based\non generalization ability and time complexity.", "published": "2024-07-15 15:23:21", "link": "http://arxiv.org/abs/2407.10807v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM Circuit Analyses Are Consistent Across Training and Scale", "abstract": "Most currently deployed large language models (LLMs) undergo continuous\ntraining or additional finetuning. By contrast, most research into LLMs'\ninternal mechanisms focuses on models at one snapshot in time (the end of\npre-training), raising the question of whether their results generalize to\nreal-world settings. Existing studies of mechanisms over time focus on\nencoder-only or toy models, which differ significantly from most deployed\nmodels. In this study, we track how model mechanisms, operationalized as\ncircuits, emerge and evolve across 300 billion tokens of training in\ndecoder-only LLMs, in models ranging from 70 million to 2.8 billion parameters.\nWe find that task abilities and the functional components that support them\nemerge consistently at similar token counts across scale. Moreover, although\nsuch components may be implemented by different attention heads over time, the\noverarching algorithm that they implement remains. Surprisingly, both these\nalgorithms and the types of components involved therein can replicate across\nmodel scale. These results suggest that circuit analyses conducted on small\nmodels at the end of pre-training can provide insights that still apply after\nadditional pre-training and over model scale.", "published": "2024-07-15 15:38:51", "link": "http://arxiv.org/abs/2407.10827v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "An Actionable Framework for Assessing Bias and Fairness in Large\n  Language Model Use Cases", "abstract": "Large language models (LLMs) can exhibit bias in a variety of ways. Such\nbiases can create or exacerbate unfair outcomes for certain groups within a\nprotected attribute, including, but not limited to sex, race, sexual\norientation, or age. In this paper, we propose a decision framework that allows\npractitioners to determine which bias and fairness metrics to use for a\nspecific LLM use case. To establish the framework, we define bias and fairness\nrisks for LLMs, map those risks to a taxonomy of LLM use cases, and then define\nvarious metrics to assess each type of risk. Instead of focusing solely on the\nmodel itself, we account for both prompt-specific- and model-specific-risk by\ndefining evaluations at the level of an LLM use case, characterized by a model\nand a population of prompts. Furthermore, because all of the evaluation metrics\nare calculated solely using the LLM output, our proposed framework is highly\npractical and easily actionable for practitioners. For streamlined\nimplementation, all evaluation metrics included in the framework are offered in\nthis paper's companion Python toolkit, LangFair. Finally, our experiments\ndemonstrate substantial variation in bias and fairness across use cases,\nunderscoring the importance of use-case-level assessments.", "published": "2024-07-15 16:04:44", "link": "http://arxiv.org/abs/2407.10853v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Weighted Grouped Query Attention in Transformers", "abstract": "The attention mechanism forms the foundational blocks for transformer\nlanguage models. Recent approaches show that scaling the model achieves\nhuman-level performance. However, with increasing demands for scaling and\nconstraints on hardware memory, the inference costs of these models remain\nhigh. To reduce the inference time, Multi-Query Attention (MQA) and\nGrouped-Query Attention (GQA) were proposed in (Shazeer, 2019) and (Ainslieet\nal., 2023) respectively. In this paper, we propose a variation of Grouped-Query\nAttention, termed Weighted Grouped-Query Attention (WGQA). We introduced new\nlearnable parameters for each key and value head in the T5 decoder attention\nblocks, enabling the model to take a weighted average during finetuning. Our\nmodel achieves an average of 0.53% improvement over GQA, and the performance\nconverges to traditional Multi-head attention (MHA) with no additional overhead\nduring inference. We evaluated the introduction of these parameters and\nsubsequent finetuning informs the model about the grouping mechanism during\ntraining, thereby enhancing performance. Additionally, we demonstrate the\nscaling laws in our analysis by comparing the results between T5-small and\nT5-base architecture.", "published": "2024-07-15 16:07:13", "link": "http://arxiv.org/abs/2407.10855v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Spider2-V: How Far Are Multimodal Agents From Automating Data Science\n  and Engineering Workflows?", "abstract": "Data science and engineering workflows often span multiple stages, from\nwarehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. As\nvision language models (VLMs) advance in multimodal understanding and code\ngeneration, VLM-based agents could potentially automate these workflows by\ngenerating SQL queries, Python code, and GUI operations. This automation can\nimprove the productivity of experts while democratizing access to large-scale\ndata analysis. In this paper, we introduce Spider2-V, the first multimodal\nagent benchmark focusing on professional data science and engineering\nworkflows, featuring 494 real-world tasks in authentic computer environments\nand incorporating 20 enterprise-level professional applications. These tasks,\nderived from real-world use cases, evaluate the ability of a multimodal agent\nto perform data-related tasks by writing code and managing the GUI in\nenterprise data software systems. To balance realistic simulation with\nevaluation simplicity, we devote significant effort to developing automatic\nconfigurations for task setup and carefully crafting evaluation metrics for\neach task. Furthermore, we supplement multimodal agents with comprehensive\ndocuments of these enterprise data software systems. Our empirical evaluation\nreveals that existing state-of-the-art LLM/VLM-based agents do not reliably\nautomate full data workflows (14.0% success). Even with step-by-step guidance,\nthese agents still underperform in tasks that require fine-grained,\nknowledge-intensive GUI actions (16.2%) and involve remote cloud-hosted\nworkspaces (10.6%). We hope that Spider2-V paves the way for autonomous\nmultimodal agents to transform the automation of data science and engineering\nworkflow. Our code and data are available at https://spider2-v.github.io.", "published": "2024-07-15 17:54:37", "link": "http://arxiv.org/abs/2407.10956v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Q-Sparse: All Large Language Models can be Fully Sparsely-Activated", "abstract": "We introduce, Q-Sparse, a simple yet effective approach to training\nsparsely-activated large language models (LLMs). Q-Sparse enables full sparsity\nof activations in LLMs which can bring significant efficiency gains in\ninference. This is achieved by applying top-K sparsification to the activations\nand the straight-through-estimator to the training. We also introduce Block\nQ-Sparse for batch training and inference. The key results from this work are,\n(1) Q-Sparse can achieve results comparable to those of baseline LLMs while\nbeing much more efficient at inference time; (2) We present an\ninference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is\neffective in different settings, including training-from-scratch,\ncontinue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for\nboth full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the\nsynergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the\ncornerstone and a clear path to revolutionize the efficiency, including cost\nand energy consumption, of future LLMs.", "published": "2024-07-15 17:59:29", "link": "http://arxiv.org/abs/2407.10969v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Empowering Persian LLMs for Instruction Following: A Novel Dataset and\n  Training Approach", "abstract": "Instruction-tuned large language models have demonstrated remarkable\ncapabilities in following human instructions across various domains. However,\ntheir proficiency remains notably deficient in many low-resource languages. To\naddress this challenge, we begin by introducing FarsInstruct a comprehensive\ninstruction dataset designed to enhance the instruction following ability of\nlarge language models specifically for the Persian language a significant yet\nunderrepresented language globally. FarsInstruct encompasses a wide range of\ntask types and datasets, each containing a mix of straightforward to complex\nmanual written instructions, as well as translations from the Public Pool of\nPrompts, ensuring a rich linguistic and cultural representation. Furthermore,\nwe introduce Co-CoLA, a framework designed to enhance the multi-task\nadaptability of LoRA-tuned models. Through extensive experimental analyses, our\nstudy showcases the effectiveness of the FarsInstruct dataset coupled with\ntraining by the Co-CoLA framework, in improving the performance of large\nlanguage models within the Persian context. As of the current writing,\nFarsInstruct comprises 197 templates across 21 distinct datasets, and we intend\nto update it consistently, thus augmenting its applicability.", "published": "2024-07-15 19:17:31", "link": "http://arxiv.org/abs/2407.11186v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automated essay scoring in Arabic: a dataset and analysis of a\n  BERT-based system", "abstract": "Automated Essay Scoring (AES) holds significant promise in the field of\neducation, helping educators to mark larger volumes of essays and provide\ntimely feedback. However, Arabic AES research has been limited by the lack of\npublicly available essay data. This study introduces AR-AES, an Arabic AES\nbenchmark dataset comprising 2046 undergraduate essays, including gender\ninformation, scores, and transparent rubric-based evaluation guidelines,\nproviding comprehensive insights into the scoring process. These essays come\nfrom four diverse courses, covering both traditional and online exams.\nAdditionally, we pioneer the use of AraBERT for AES, exploring its performance\non different question types. We find encouraging results, particularly for\nEnvironmental Chemistry and source-dependent essay questions. For the first\ntime, we examine the scale of errors made by a BERT-based AES system, observing\nthat 96.15 percent of the errors are within one point of the first human\nmarker's prediction, on a scale of one to five, with 79.49 percent of\npredictions matching exactly. In contrast, additional human markers did not\nexceed 30 percent exact matches with the first marker, with 62.9 percent within\none mark. These findings highlight the subjectivity inherent in essay grading,\nand underscore the potential for current AES technology to assist human markers\nto grade consistently across large classes.", "published": "2024-07-15 19:55:37", "link": "http://arxiv.org/abs/2407.11212v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Making New Connections: LLMs as Puzzle Generators for The New York\n  Times' Connections Word Game", "abstract": "The Connections puzzle is a word association game published daily by The New\nYork Times (NYT). In this game, players are asked to find groups of four words\nthat are connected by a common theme. While solving a given Connections puzzle\nrequires both semantic knowledge and abstract reasoning, generating novel\npuzzles additionally requires a form of metacognition: generators must be able\nto accurately model the downstream reasoning of potential solvers. In this\npaper, we investigate the ability of the GPT family of Large Language Models\n(LLMs) to generate challenging and creative word games for human players. We\nstart with an analysis of the word game Connections and the unique challenges\nit poses as a Procedural Content Generation (PCG) domain. We then propose a\nmethod for generating Connections puzzles using LLMs by adapting a Tree of\nThoughts (ToT) prompting approach. We evaluate this method by conducting a user\nstudy, asking human players to compare AI-generated puzzles against published\nConnections puzzles. Our findings show that LLMs are capable puzzle creators,\nand can generate diverse sets of enjoyable, challenging, and creative\nConnections puzzles as judged by human users.", "published": "2024-07-15 21:05:25", "link": "http://arxiv.org/abs/2407.11240v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Bridging Sequence-Structure Alignment in RNA Foundation Models", "abstract": "The alignment between RNA sequences and structures in foundation models (FMs)\nhas yet to be thoroughly investigated. Existing FMs have struggled to establish\nsequence-structure alignment, hindering the free flow of genomic information\nbetween RNA sequences and structures. In this study, we introduce OmniGenome,\nan RNA FM trained to align RNA sequences with respect to secondary structures\nbased on structure-contextualised modelling. The alignment enables free and\nbidirectional mappings between sequences and structures by utilising the\nflexible RNA modelling paradigm that supports versatile input and output\nmodalities, i.e., sequence and/or structure as input/output. We implement RNA\ndesign and zero-shot secondary structure prediction as case studies to evaluate\nthe Seq2Str and Str2Seq mapping capacity of OmniGenome. Results on the EternaV2\nbenchmark show that OmniGenome solved 74% of puzzles, whereas existing FMs only\nsolved up to 3% of the puzzles due to the oversight of sequence-structure\nalignment. We leverage four comprehensive in-silico genome modelling benchmarks\nto evaluate performance across a diverse set of genome downstream tasks, where\nthe results show that OmniGenome achieves state-of-the-art performance on RNA\nand DNA benchmarks, even without any training on DNA genomes.", "published": "2024-07-15 21:10:40", "link": "http://arxiv.org/abs/2407.11242v3", "categories": ["q-bio.GN", "cs.CL"], "primary_category": "q-bio.GN"}
{"title": "Target conversation extraction: Source separation using turn-taking\n  dynamics", "abstract": "Extracting the speech of participants in a conversation amidst interfering\nspeakers and noise presents a challenging problem. In this paper, we introduce\nthe novel task of target conversation extraction, where the goal is to extract\nthe audio of a target conversation based on the speaker embedding of one of its\nparticipants. To accomplish this, we propose leveraging temporal patterns\ninherent in human conversations, particularly turn-taking dynamics, which\nuniquely characterize speakers engaged in conversation and distinguish them\nfrom interfering speakers and noise. Using neural networks, we show the\nfeasibility of our approach on English and Mandarin conversation datasets. In\nthe presence of interfering speakers, our results show an 8.19 dB improvement\nin signal-to-noise ratio for 2-speaker conversations and a 7.92 dB improvement\nfor 2-4-speaker conversations. Code, dataset available at\nhttps://github.com/chentuochao/Target-Conversation-Extraction.", "published": "2024-07-15 22:55:27", "link": "http://arxiv.org/abs/2407.11277v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Evaluating Large Language Models with fmeval", "abstract": "fmeval is an open source library to evaluate large language models (LLMs) in\na range of tasks. It helps practitioners evaluate their model for task\nperformance and along multiple responsible AI dimensions. This paper presents\nthe library and exposes its underlying design principles: simplicity, coverage,\nextensibility and performance. We then present how these were implemented in\nthe scientific and engineering choices taken when developing fmeval. A case\nstudy demonstrates a typical use case for the library: picking a suitable model\nfor a question answering task. We close by discussing limitations and further\nwork in the development of the library. fmeval can be found at\nhttps://github.com/aws/fmeval.", "published": "2024-07-15 12:15:08", "link": "http://arxiv.org/abs/2407.12872v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models", "abstract": "Cognitive textual and visual reasoning tasks, including puzzles, series, and\nanalogies, demand the ability to quickly reason, decipher, and evaluate\npatterns both textually and spatially. Due to extensive training on vast\namounts of human-curated data, LLMs and VLMs excel in common-sense reasoning\ntasks, however still struggle with more complex reasoning that demands deeper\ncognitive understanding. We introduce NTSEBench, a new dataset designed to\nevaluate cognitive multi-modal reasoning and problem-solving skills of large\nmodels. The dataset contains 2728 multiple-choice questions, accompanied by a\ntotal of 4,642 images, categorized into 26 different types. These questions are\ndrawn from the nationwide NTSE examination in India and feature a mix of visual\nand textual general aptitude challenges, designed to assess intelligence and\ncritical thinking skills beyond mere rote learning. We establish baselines on\nthe dataset using state-of-the-art LLMs and VLMs. To facilitate a comparison\nbetween open source and propriety models, we propose four distinct modeling\nstrategies to handle different modalities -- text and images -- in the dataset\ninstances.", "published": "2024-07-15 01:21:56", "link": "http://arxiv.org/abs/2407.10380v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "By My Eyes: Grounding Multimodal Large Language Models with Sensor Data\n  via Visual Prompting", "abstract": "Large language models (LLMs) have demonstrated exceptional abilities across\nvarious domains. However, utilizing LLMs for ubiquitous sensing applications\nremains challenging as existing text-prompt methods show significant\nperformance degradation when handling long sensor data sequences. We propose a\nvisual prompting approach for sensor data using multimodal LLMs (MLLMs). We\ndesign a visual prompt that directs MLLMs to utilize visualized sensor data\nalongside the target sensory task descriptions. Additionally, we introduce a\nvisualization generator that automates the creation of optimal visualizations\ntailored to a given sensory task, eliminating the need for prior task-specific\nknowledge. We evaluated our approach on nine sensory tasks involving four\nsensing modalities, achieving an average of 10% higher accuracy than text-based\nprompts and reducing token costs by 15.8 times. Our findings highlight the\neffectiveness and cost-efficiency of visual prompts with MLLMs for various\nsensory tasks. The source code is available at\nhttps://github.com/diamond264/ByMyEyes.", "published": "2024-07-15 01:33:54", "link": "http://arxiv.org/abs/2407.10385v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SuperPADL: Scaling Language-Directed Physics-Based Control with\n  Progressive Supervised Distillation", "abstract": "Physically-simulated models for human motion can generate high-quality\nresponsive character animations, often in real-time. Natural language serves as\na flexible interface for controlling these models, allowing expert and\nnon-expert users to quickly create and edit their animations. Many recent\nphysics-based animation methods, including those that use text interfaces,\ntrain control policies using reinforcement learning (RL). However, scaling\nthese methods beyond several hundred motions has remained challenging.\nMeanwhile, kinematic animation models are able to successfully learn from\nthousands of diverse motions by leveraging supervised learning methods.\nInspired by these successes, in this work we introduce SuperPADL, a scalable\nframework for physics-based text-to-motion that leverages both RL and\nsupervised learning to train controllers on thousands of diverse motion clips.\nSuperPADL is trained in stages using progressive distillation, starting with a\nlarge number of specialized experts using RL. These experts are then\niteratively distilled into larger, more robust policies using a combination of\nreinforcement learning and supervised learning. Our final SuperPADL controller\nis trained on a dataset containing over 5000 skills and runs in real time on a\nconsumer GPU. Moreover, our policy can naturally transition between skills,\nallowing for users to interactively craft multi-stage animations. We\nexperimentally demonstrate that SuperPADL significantly outperforms RL-based\nbaselines at this large data scale.", "published": "2024-07-15 07:07:11", "link": "http://arxiv.org/abs/2407.10481v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.GR"], "primary_category": "cs.LG"}
{"title": "Learning Dynamics of LLM Finetuning", "abstract": "Learning dynamics, which describes how the learning of specific training\nexamples influences the model's predictions on other examples, gives us a\npowerful tool for understanding the behavior of deep learning systems. We study\nthe learning dynamics of large language models during different types of\nfinetuning, by analyzing the step-wise decomposition of how influence\naccumulates among different potential responses. Our framework allows a uniform\ninterpretation of many interesting observations about the training of popular\nalgorithms for both instruction tuning and preference tuning. In particular, we\npropose a hypothetical explanation of why specific types of hallucination are\nstrengthened after finetuning, e.g., the model might use phrases or facts in\nthe response for question B to answer question A, or the model might keep\nrepeating similar simple phrases when generating responses. We also extend our\nframework and highlight a unique \"squeezing effect\" to explain a previously\nobserved phenomenon in off-policy direct preference optimization (DPO), where\nrunning DPO for too long makes even the desired outputs less likely. This\nframework also provides insights into where the benefits of on-policy DPO and\nother variants come from. The analysis not only provides a novel perspective of\nunderstanding LLM's finetuning but also inspires a simple, effective method to\nimprove alignment performance.", "published": "2024-07-15 07:30:28", "link": "http://arxiv.org/abs/2407.10490v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "TCM-FTP: Fine-Tuning Large Language Models for Herbal Prescription\n  Prediction", "abstract": "Traditional Chinese medicine (TCM) has relied on specific combinations of\nherbs in prescriptions to treat various symptoms and signs for thousands of\nyears. Predicting TCM prescriptions poses a fascinating technical challenge\nwith significant practical implications. However, this task faces limitations\ndue to the scarcity of high-quality clinical datasets and the complex\nrelationship between symptoms and herbs. To address these issues, we introduce\n\\textit{DigestDS}, a novel dataset comprising practical medical records from\nexperienced experts in digestive system diseases. We also propose a method,\nTCM-FTP (TCM Fine-Tuning Pre-trained), to leverage pre-trained large language\nmodels (LLMs) via supervised fine-tuning on \\textit{DigestDS}. Additionally, we\nenhance computational efficiency using a low-rank adaptation technique.\nMoreover, TCM-FTP incorporates data augmentation by permuting herbs within\nprescriptions, exploiting their order-agnostic nature. Impressively, TCM-FTP\nachieves an F1-score of 0.8031, significantly outperforming previous methods.\nFurthermore, it demonstrates remarkable accuracy in dosage prediction,\nachieving a normalized mean square error of 0.0604. In contrast, LLMs without\nfine-tuning exhibit poor performance. Although LLMs have demonstrated\nwide-ranging capabilities, our work underscores the necessity of fine-tuning\nfor TCM prescription prediction and presents an effective way to accomplish\nthis.", "published": "2024-07-15 08:06:37", "link": "http://arxiv.org/abs/2407.10510v2", "categories": ["cs.CL", "cs.AI", "cs.CE"], "primary_category": "cs.CL"}
{"title": "Leave No Knowledge Behind During Knowledge Distillation: Towards\n  Practical and Effective Knowledge Distillation for Code-Switching ASR Using\n  Realistic Data", "abstract": "Recent advances in automatic speech recognition (ASR) often rely on large\nspeech foundation models for generating high-quality transcriptions. However,\nthese models can be impractical due to limited computing resources. The\nsituation is even more severe in terms of more realistic or difficult\nscenarios, such as code-switching ASR (CS-ASR). To address this, we present a\nframework for developing more efficient models for CS-ASR through knowledge\ndistillation using realistic speech-only data. Our proposed method, Leave No\nKnowledge Behind During Knowledge Distillation (K$^2$D), leverages both the\nteacher model's knowledge and additional insights from a small auxiliary model.\nWe evaluate our approach on two in-domain and two out-domain datasets,\ndemonstrating that K$^2$D is effective. By conducting K$^2$D on the unlabeled\nrealistic data, we have successfully obtained a 2-time smaller model with\n5-time faster generation speed while outperforming the baseline methods and the\nteacher model on all the testing sets. We have made our model publicly\navailable on Hugging Face\n(https://huggingface.co/andybi7676/k2d-whisper.zh-en).", "published": "2024-07-15 10:25:14", "link": "http://arxiv.org/abs/2407.10603v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Arena Learning: Build Data Flywheel for LLMs Post-training via Simulated\n  Chatbot Arena", "abstract": "Assessing the effectiveness of large language models (LLMs) presents\nsubstantial challenges. The method of conducting human-annotated battles in an\nonline Chatbot Arena is a highly effective evaluative technique. However, this\napproach is limited by the costs and time required for human annotation. In\nthis paper, we introduce Arena Learning, an innovative offline strategy\ndesigned to simulate these arena battles using AI-driven annotations to\nevaluate battle outcomes, thus facilitating the continuous improvement of the\ntarget model through both supervised fine-tuning and reinforcement learning.\nArena Learning comprises two key elements. First, it ensures precise\nevaluations and maintains consistency between offline simulations and online\ncompetitions via WizardArena, a pipeline developed to accurately predict the\nElo rankings of various models using a meticulously designed offline test set.\nOur results demonstrate that WizardArena's predictions closely align with those\nfrom the online Arena. Second, it involves the continuous improvement of\ntraining data based on the battle results and the refined model. We establish a\ndata flywheel to iteratively update the training data by highlighting the\nweaknesses of the target model based on its battle results, enabling it to\nlearn from the strengths of multiple different models. We apply Arena Learning\nto train our target model, WizardLM-$\\beta$, and demonstrate significant\nperformance enhancements across various metrics. This fully automated training\nand evaluation pipeline sets the stage for continuous advancements in various\nLLMs via post-training. Notably, Arena Learning plays a pivotal role in the\nsuccess of WizardLM-2, and this paper serves both as an exploration of its\nefficacy and a foundational study for future discussions related to WizardLM-2\nand its derivatives.", "published": "2024-07-15 11:26:07", "link": "http://arxiv.org/abs/2407.10627v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Balancing the Scales: Reinforcement Learning for Fair Classification", "abstract": "Fairness in classification tasks has traditionally focused on bias removal\nfrom neural representations, but recent trends favor algorithmic methods that\nembed fairness into the training process. These methods steer models towards\nfair performance, preventing potential elimination of valuable information that\narises from representation manipulation. Reinforcement Learning (RL), with its\ncapacity for learning through interaction and adjusting reward functions to\nencourage desired behaviors, emerges as a promising tool in this domain. In\nthis paper, we explore the usage of RL to address bias in imbalanced\nclassification by scaling the reward function to mitigate bias. We employ the\ncontextual multi-armed bandit framework and adapt three popular RL algorithms\nto suit our objectives, demonstrating a novel approach to mitigating bias.", "published": "2024-07-15 11:28:16", "link": "http://arxiv.org/abs/2407.10629v1", "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Prompt Selection Matters: Enhancing Text Annotations for Social Sciences\n  with Large Language Models", "abstract": "Large Language Models have recently been applied to text annotation tasks\nfrom social sciences, equalling or surpassing the performance of human workers\nat a fraction of the cost. However, no inquiry has yet been made on the impact\nof prompt selection on labelling accuracy. In this study, we show that\nperformance greatly varies between prompts, and we apply the method of\nautomatic prompt optimization to systematically craft high quality prompts. We\nalso provide the community with a simple, browser-based implementation of the\nmethod at https://prompt-ultra.github.io/ .", "published": "2024-07-15 12:04:32", "link": "http://arxiv.org/abs/2407.10645v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Transforming Agency. On the mode of existence of Large Language Models", "abstract": "This paper investigates the ontological characterization of Large Language\nModels (LLMs) like ChatGPT. Between inflationary and deflationary accounts, we\npay special attention to their status as agents. This requires explaining in\ndetail the architecture, processing, and training procedures that enable LLMs\nto display their capacities, and the extensions used to turn LLMs into\nagent-like systems. After a systematic analysis we conclude that a LLM fails to\nmeet necessary and sufficient conditions for autonomous agency in the light of\nembodied theories of mind: the individuality condition (it is not the product\nof its own activity, it is not even directly affected by it), the normativity\ncondition (it does not generate its own norms or goals), and, partially the\ninteractional asymmetry condition (it is not the origin and sustained source of\nits interaction with the environment). If not agents, then ... what are LLMs?\nWe argue that ChatGPT should be characterized as an interlocutor or linguistic\nautomaton, a library-that-talks, devoid of (autonomous) agency, but capable to\nengage performatively on non-purposeful yet purpose-structured and\npurpose-bounded tasks. When interacting with humans, a \"ghostly\" component of\nthe human-machine interaction makes it possible to enact genuine conversational\nexperiences with LLMs. Despite their lack of sensorimotor and biological\nembodiment, LLMs textual embodiment (the training corpus) and resource-hungry\ncomputational embodiment, significantly transform existing forms of human\nagency. Beyond assisted and extended agency, the LLM-human coupling can produce\nmidtended forms of agency, closer to the production of intentional agency than\nto the extended instrumentality of any previous technologies.", "published": "2024-07-15 14:01:35", "link": "http://arxiv.org/abs/2407.10735v2", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Qwen2-Audio Technical Report", "abstract": "We introduce the latest progress of Qwen-Audio, a large-scale audio-language\nmodel called Qwen2-Audio, which is capable of accepting various audio signal\ninputs and performing audio analysis or direct textual responses with regard to\nspeech instructions. In contrast to complex hierarchical tags, we have\nsimplified the pre-training process by utilizing natural language prompts for\ndifferent data and tasks, and have further expanded the data volume. We have\nboosted the instruction-following capability of Qwen2-Audio and implemented two\ndistinct audio interaction modes for voice chat and audio analysis. In the\nvoice chat mode, users can freely engage in voice interactions with Qwen2-Audio\nwithout text input. In the audio analysis mode, users could provide audio and\ntext instructions for analysis during the interaction. Note that we do not use\nany system prompts to switch between voice chat and audio analysis modes.\nQwen2-Audio is capable of intelligently comprehending the content within audio\nand following voice commands to respond appropriately. For instance, in an\naudio segment that simultaneously contains sounds, multi-speaker conversations,\nand a voice command, Qwen2-Audio can directly understand the command and\nprovide an interpretation and response to the audio. Additionally, DPO has\noptimized the model's performance in terms of factuality and adherence to\ndesired behavior. According to the evaluation results from AIR-Bench,\nQwen2-Audio outperformed previous SOTAs, such as Gemini-1.5-pro, in tests\nfocused on audio-centric instruction-following capabilities. Qwen2-Audio is\nopen-sourced with the aim of fostering the advancement of the multi-modal\nlanguage community.", "published": "2024-07-15 14:38:09", "link": "http://arxiv.org/abs/2407.10759v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation\n  Framework", "abstract": "Methods to evaluate Large Language Model (LLM) responses and detect\ninconsistencies, also known as hallucinations, with respect to the provided\nknowledge, are becoming increasingly important for LLM applications. Current\nmetrics fall short in their ability to provide explainable decisions,\nsystematically check all pieces of information in the response, and are often\ntoo computationally expensive to be used in practice. We present GraphEval: a\nhallucination evaluation framework based on representing information in\nKnowledge Graph (KG) structures. Our method identifies the specific triples in\nthe KG that are prone to hallucinations and hence provides more insight into\nwhere in the response a hallucination has occurred, if at all, than previous\nmethods. Furthermore, using our approach in conjunction with state-of-the-art\nnatural language inference (NLI) models leads to an improvement in balanced\naccuracy on various hallucination benchmarks, compared to using the raw NLI\nmodels. Lastly, we explore the use of GraphEval for hallucination correction by\nleveraging the structure of the KG, a method we name GraphCorrect, and\ndemonstrate that the majority of hallucinations can indeed be rectified.", "published": "2024-07-15 15:11:16", "link": "http://arxiv.org/abs/2407.10793v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Foundational Autoraters: Taming Large Language Models for Better\n  Automatic Evaluation", "abstract": "As large language models (LLMs) advance, it becomes more challenging to\nreliably evaluate their output due to the high costs of human evaluation. To\nmake progress towards better LLM autoraters, we introduce FLAMe, a family of\nFoundational Large Autorater Models. FLAMe is trained on our large and diverse\ncollection of 100+ quality assessment tasks comprising 5M+ human judgments,\ncurated and standardized using publicly released human evaluations from\nprevious research. FLAMe significantly improves generalization to a wide\nvariety of held-out tasks, outperforming LLMs trained on proprietary data like\nGPT-4 and Claude-3 on many tasks. We show that FLAMe can also serve as a\npowerful starting point for further downstream fine-tuning, using reward\nmodeling evaluation as a case study (FLAMe-RM). Notably, on RewardBench, our\nFLAMe-RM-24B model (with an accuracy of 87.8%) is the top-performing generative\nmodel trained exclusively on permissively licensed data, outperforming both\nGPT-4-0125 (85.9%) and GPT-4o (84.7%). Additionally, we explore a more\ncomputationally efficient approach using a novel tail-patch fine-tuning\nstrategy to optimize our FLAMe multitask mixture for reward modeling evaluation\n(FLAMe-Opt-RM), offering competitive RewardBench performance while requiring\napproximately 25x less training datapoints. Overall, our FLAMe variants\noutperform all popular proprietary LLM-as-a-Judge models we consider across 8\nout of 12 autorater evaluation benchmarks, encompassing 53 quality assessment\ntasks, including RewardBench and LLM-AggreFact. Finally, our analysis reveals\nthat FLAMe is significantly less biased than these LLM-as-a-Judge models on the\nCoBBLEr autorater bias benchmark, while effectively identifying high-quality\nresponses for code generation.", "published": "2024-07-15 15:33:45", "link": "http://arxiv.org/abs/2407.10817v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BiasScanner: Automatic Detection and Classification of News Bias to\n  Strengthen Democracy", "abstract": "The increasing consumption of news online in the 21st century coincided with\nincreased publication of disinformation, biased reporting, hate speech and\nother unwanted Web content. We describe BiasScanner, an application that aims\nto strengthen democracy by supporting news consumers with scrutinizing news\narticles they are reading online. BiasScanner contains a server-side\npre-trained large language model to identify biased sentences of news articles\nand a front-end Web browser plug-in. At the time of writing, BiasScanner can\nidentify and classify more than two dozen types of media bias at the sentence\nlevel, making it the most fine-grained model and only deployed application\n(automatic system in use) of its kind. It was implemented in a light-weight and\nprivacy-respecting manner, and in addition to highlighting likely biased\nsentence it also provides explanations for each classification decision as well\nas a summary analysis for each news article. While prior research has addressed\nnews bias detection, we are not aware of any work that resulted in a deployed\nbrowser plug-in (c.f. also biasscanner.org for a Web demo).", "published": "2024-07-15 15:42:22", "link": "http://arxiv.org/abs/2407.10829v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7; H.3.3"], "primary_category": "cs.CL"}
{"title": "Leveraging LLM-Respondents for Item Evaluation: a Psychometric Analysis", "abstract": "Effective educational measurement relies heavily on the curation of\nwell-designed item pools (i.e., possessing the right psychometric properties).\nHowever, item calibration is time-consuming and costly, requiring a sufficient\nnumber of respondents for the response process. We explore using six different\nLLMs (GPT-3.5, GPT-4, Llama 2, Llama 3, Gemini-Pro, and Cohere Command R Plus)\nand various combinations of them using sampling methods to produce responses\nwith psychometric properties similar to human answers. Results show that some\nLLMs have comparable or higher proficiency in College Algebra than college\nstudents. No single LLM mimics human respondents due to narrow proficiency\ndistributions, but an ensemble of LLMs can better resemble college students'\nability distribution. The item parameters calibrated by LLM-Respondents have\nhigh correlations (e.g. > 0.8 for GPT-3.5) compared to their human calibrated\ncounterparts, and closely resemble the parameters of the human subset (e.g.\n0.02 Spearman correlation difference). Several augmentation strategies are\nevaluated for their relative performance, with resampling methods proving most\neffective, enhancing the Spearman correlation from 0.89 (human only) to 0.93\n(augmented human).", "published": "2024-07-15 16:49:26", "link": "http://arxiv.org/abs/2407.10899v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Benchmarking Vision Language Models for Cultural Understanding", "abstract": "Foundation models and vision-language pre-training have notably advanced\nVision Language Models (VLMs), enabling multimodal processing of visual and\nlinguistic data. However, their performance has been typically assessed on\ngeneral scene understanding - recognizing objects, attributes, and actions -\nrather than cultural comprehension. This study introduces CulturalVQA, a visual\nquestion-answering benchmark aimed at assessing VLM's geo-diverse cultural\nunderstanding. We curate a collection of 2,378 image-question pairs with 1-5\nanswers per question representing cultures from 11 countries across 5\ncontinents. The questions probe understanding of various facets of culture such\nas clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on\nCulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of\ncultural understanding across regions, with strong cultural understanding\ncapabilities for North America while significantly lower performance for\nAfrica. We observe disparity in their performance across cultural facets too,\nwith clothing, rituals, and traditions seeing higher performances than food and\ndrink. These disparities help us identify areas where VLMs lack cultural\nunderstanding and demonstrate the potential of CulturalVQA as a comprehensive\nevaluation set for gauging VLM progress in understanding diverse cultures.", "published": "2024-07-15 17:21:41", "link": "http://arxiv.org/abs/2407.10920v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better\n  Together", "abstract": "Natural Language Processing (NLP) systems are increasingly taking the form of\nsophisticated modular pipelines, e.g., Retrieval Augmented Generation (RAG),\nwhere each module may involve a distinct Language Model (LM) and an associated\nprompt template. These compound systems often lack intermediate labels or\ngradient flow to optimize each module, making their end-to-end optimization\nchallenging. Here we seek strategies to optimize both the module-level LM\nweights and the associated prompt templates of such systems to maximize a\ndownstream task metric. We propose for the first time combining the weight and\nprompt optimization strategies to optimize a modular LM pipeline by alternating\nbetween the two to get the same LM to teach itself. In experiments with\nmulti-hop QA, mathematical reasoning, and feature-based classification using\nmistral-7b, llama-2-7b, and llama-3-8b, these BetterTogether strategies\noptimizing the weights and prompts of a pipeline together outperform directly\noptimizing weights alone and prompts alone by up to 60% and 6%, respectively,\non average across LMs and tasks. BetterTogether optimizer is released in DSPy\nat http://dspy.ai", "published": "2024-07-15 17:30:31", "link": "http://arxiv.org/abs/2407.10930v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Representing Rule-based Chatbots with Transformers", "abstract": "What kind of internal mechanisms might Transformers use to conduct fluid,\nnatural-sounding conversations? Prior work has illustrated by construction how\nTransformers can solve various synthetic tasks, such as sorting a list or\nrecognizing formal languages, but it remains unclear how to extend this\napproach to a conversational setting. In this work, we propose using ELIZA, a\nclassic rule-based chatbot, as a setting for formal, mechanistic analysis of\nTransformer-based chatbots. ELIZA allows us to formally model key aspects of\nconversation, including local pattern matching and long-term dialogue state\ntracking. We first present a theoretical construction of a Transformer that\nimplements the ELIZA chatbot. Building on prior constructions, particularly\nthose for simulating finite-state automata, we show how simpler mechanisms can\nbe composed and extended to produce more sophisticated behavior. Next, we\nconduct a set of empirical analyses of Transformers trained on synthetically\ngenerated ELIZA conversations. Our analysis illustrates the kinds of mechanisms\nthese models tend to prefer--for example, models favor an induction head\nmechanism over a more precise, position-based copying mechanism; and using\nintermediate generations to simulate recurrent data structures, akin to an\nimplicit scratchpad or Chain-of-Thought. Overall, by drawing an explicit\nconnection between neural chatbots and interpretable, symbolic mechanisms, our\nresults provide a new framework for the mechanistic analysis of conversational\nagents.", "published": "2024-07-15 17:45:53", "link": "http://arxiv.org/abs/2407.10949v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fast Matrix Multiplications for Lookup Table-Quantized LLMs", "abstract": "The deployment of large language models (LLMs) is often constrained by memory\nbandwidth, where the primary bottleneck is the cost of transferring model\nparameters from the GPU's global memory to its registers. When coupled with\ncustom kernels that fuse the dequantization and matmul operations, weight-only\nquantization can thus enable faster inference by reducing the amount of memory\nmovement. However, developing high-performance kernels for weight-quantized\nLLMs presents substantial challenges, especially when the weights are\ncompressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,\nlookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup\ntable engine for LUT-quantized LLMs, which uses offline restructuring of the\nquantized weight matrix to minimize bit manipulations associated with\nunpacking, and vectorization and duplication of the lookup table to mitigate\nshared memory bandwidth constraints. At batch sizes < 32 and quantization group\nsize of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster\nthan existing GEMM kernels. As an application of FLUTE, we explore a simple\nextension to lookup table-based NormalFloat quantization and apply it to\nquantize LLaMA3 to various configurations, obtaining competitive quantization\nperformance against strong baselines while obtaining an end-to-end throughput\nincrease of 1.5 to 2 times.", "published": "2024-07-15 17:55:42", "link": "http://arxiv.org/abs/2407.10960v4", "categories": ["cs.LG", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen\n  Representations", "abstract": "This paper introduces FUNGI, Features from UNsupervised GradIents, a method\nto enhance the features of transformer encoders by leveraging self-supervised\ngradients. Our method is simple: given any pretrained model, we first compute\ngradients from various self-supervised objectives for each input. These\ngradients are projected to a lower dimension and then concatenated with the\nmodel's output embedding. The resulting features are evaluated on k-nearest\nneighbor classification over 11 datasets from vision, 5 from natural language\nprocessing, and 2 from audio. Across backbones spanning various sizes and\npretraining strategies, FUNGI features provide consistent performance\nimprovements over the embeddings. We also show that using FUNGI features can\nbenefit linear classification, clustering and image retrieval, and that they\nsignificantly improve the retrieval-based in-context scene understanding\nabilities of pretrained models, for example improving upon DINO by +17% for\nsemantic segmentation - without any training.", "published": "2024-07-15 17:58:42", "link": "http://arxiv.org/abs/2407.10964v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Building Intelligence Identification System via Large Language Model\n  Watermarking: A Survey and Beyond", "abstract": "Large Language Models (LLMs) are increasingly integrated into diverse\nindustries, posing substantial security risks due to unauthorized replication\nand misuse. To mitigate these concerns, robust identification mechanisms are\nwidely acknowledged as an effective strategy. Identification systems for LLMs\nnow rely heavily on watermarking technology to manage and protect intellectual\nproperty and ensure data security. However, previous studies have primarily\nconcentrated on the basic principles of algorithms and lacked a comprehensive\nanalysis of watermarking theory and practice from the perspective of\nintelligent identification. To bridge this gap, firstly, we explore how a\nrobust identity recognition system can be effectively implemented and managed\nwithin LLMs by various participants using watermarking technology. Secondly, we\npropose a mathematical framework based on mutual information theory, which\nsystematizes the identification process to achieve more precise and customized\nwatermarking. Additionally, we present a comprehensive evaluation of\nperformance metrics for LLM watermarking, reflecting participant preferences\nand advancing discussions on its identification applications. Lastly, we\noutline the existing challenges in current watermarking technologies and\ntheoretical frameworks, and provide directional guidance to address these\nchallenges. Our systematic classification and detailed exposition aim to\nenhance the comparison and evaluation of various methods, fostering further\nresearch and development toward a transparent, secure, and equitable LLM\necosystem.", "published": "2024-07-15 07:20:02", "link": "http://arxiv.org/abs/2407.11100v3", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Unconstrained Open Vocabulary Image Classification: Zero-Shot Transfer\n  from Text to Image via CLIP Inversion", "abstract": "We introduce NOVIC, an innovative real-time uNconstrained Open Vocabulary\nImage Classifier that uses an autoregressive transformer to generatively output\nclassification labels as language. Leveraging the extensive knowledge of CLIP\nmodels, NOVIC harnesses the embedding space to enable zero-shot transfer from\npure text to images. Traditional CLIP models, despite their ability for open\nvocabulary classification, require an exhaustive prompt of potential class\nlabels, restricting their application to images of known content or context. To\naddress this, we propose an \"object decoder\" model that is trained on a\nlarge-scale 92M-target dataset of templated object noun sets and LLM-generated\ncaptions to always output the object noun in question. This effectively inverts\nthe CLIP text encoder and allows textual object labels from essentially the\nentire English language to be generated directly from image-derived embedding\nvectors, without requiring any a priori knowledge of the potential content of\nan image, and without any label biases. The trained decoders are tested on a\nmix of manually and web-curated datasets, as well as standard image\nclassification benchmarks, and achieve fine-grained prompt-free prediction\nscores of up to 87.5%, a strong result considering the model must work for any\nconceivable image and without any contextual clues.", "published": "2024-07-15 19:53:02", "link": "http://arxiv.org/abs/2407.11211v4", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MetaTool: Facilitating Large Language Models to Master Tools with\n  Meta-task Augmentation", "abstract": "Utilizing tools with Large Language Models (LLMs) is essential for grounding\nAI agents in real-world applications. The prevailing approach involves few-shot\nprompting with demonstrations or fine-tuning with expert annotations. However,\nmere in-context demonstrations may fail to cover sufficient knowledge for\ncomplex tools and tasks. Training on solution paths is also hindered by the\nhigh cost of expert annotations and generalizing to new tools. A core challenge\nof generalizable tool use lies in understanding the \"meta\", or fundamental\nnatures of tools that are transferable across tasks, such as causality and\nconstraints. In this paper, we present MetaTool, a novel tool learning\nmethodology designed to generalize across any reusable toolset. Our approach\nincorporates a self-supervised augmentation technique derived from a series of\nmeta-tasks. This involves predicting masked elements in the tool execution\nprocess. The self-supervised procedure enables scalable generation of\nhigh-quality QA data, which is handy for supervising tool understanding. By\nincorporating meta-task data into task-oriented training, our method\nsignificantly enhances the performance of open-source LLMs, achieving results\ncomparable to ChatGPT in both tool-based planning and chatting scenarios.\nThrough large-scale instruction tuning, the MetaTool model demonstrates\nimpressive zero-shot generalizability on new tasks.", "published": "2024-07-15 10:15:41", "link": "http://arxiv.org/abs/2407.12871v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluation of RAG Metrics for Question Answering in the Telecom Domain", "abstract": "Retrieval Augmented Generation (RAG) is widely used to enable Large Language\nModels (LLMs) perform Question Answering (QA) tasks in various domains.\nHowever, RAG based on open-source LLM for specialized domains has challenges of\nevaluating generated responses. A popular framework in the literature is the\nRAG Assessment (RAGAS), a publicly available library which uses LLMs for\nevaluation. One disadvantage of RAGAS is the lack of details of derivation of\nnumerical value of the evaluation metrics. One of the outcomes of this work is\na modified version of this package for few metrics (faithfulness, context\nrelevance, answer relevance, answer correctness, answer similarity and factual\ncorrectness) through which we provide the intermediate outputs of the prompts\nby using any LLMs. Next, we analyse the expert evaluations of the output of the\nmodified RAGAS package and observe the challenges of using it in the telecom\ndomain. We also study the effect of the metrics under correct vs. wrong\nretrieval and observe that few of the metrics have higher values for correct\nretrieval. We also study for differences in metrics between base embeddings and\nthose domain adapted via pre-training and fine-tuning. Finally, we comment on\nthe suitability and challenges of using these metrics for in-the-wild telecom\nQA task.", "published": "2024-07-15 17:40:15", "link": "http://arxiv.org/abs/2407.12873v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Quantification and Validation for Degree of Understanding in M2M\n  Semantic Communications", "abstract": "With the development of Artificial Intelligence (AI) and Internet of Things\n(IoT) technologies, network communications based on the Shannon-Nyquist theorem\ngradually reveal their limitations due to the neglect of semantic information\nin the transmitted content. Semantic communication (SemCom) provides a solution\nfor extracting information meanings from the transmitted content. The semantic\ninformation can be successfully interpreted by a receiver with the help of a\nshared knowledge base (KB). This paper proposes a two-stage hierarchical\nqualification and validation model for natural language-based\nmachine-to-machine (M2M) SemCom. The approach can be applied in various\napplications, such as autonomous driving and edge computing. In the proposed\nmodel, we quantitatively measure the degree of understanding (DoU) between two\ncommunication parties at the word and sentence levels. The DoU is validated and\nensured at each level before moving to the next step. The model's effectiveness\nis verified through a series of experiments, and the results show that the\nquantification and validation method proposed in this paper can significantly\nimprove the DoU of inter-machine SemCom.", "published": "2024-07-15 03:37:42", "link": "http://arxiv.org/abs/2408.00767v1", "categories": ["cs.IT", "cs.CL", "math.IT"], "primary_category": "cs.IT"}
{"title": "Decoding AI and Human Authorship: Nuances Revealed Through NLP and\n  Statistical Analysis", "abstract": "This research explores the nuanced differences in texts produced by AI and\nthose written by humans, aiming to elucidate how language is expressed\ndifferently by AI and humans. Through comprehensive statistical data analysis,\nthe study investigates various linguistic traits, patterns of creativity, and\npotential biases inherent in human-written and AI- generated texts. The\nsignificance of this research lies in its contribution to understanding AI's\ncreative capabilities and its impact on literature, communication, and societal\nframeworks. By examining a meticulously curated dataset comprising 500K essays\nspanning diverse topics and genres, generated by LLMs, or written by humans,\nthe study uncovers the deeper layers of linguistic expression and provides\ninsights into the cognitive processes underlying both AI and human-driven\ntextual compositions. The analysis revealed that human-authored essays tend to\nhave a higher total word count on average than AI-generated essays but have a\nshorter average word length compared to AI- generated essays, and while both\ngroups exhibit high levels of fluency, the vocabulary diversity of Human\nauthored content is higher than AI generated content. However, AI- generated\nessays show a slightly higher level of novelty, suggesting the potential for\ngenerating more original content through AI systems. The paper addresses\nchallenges in assessing the language generation capabilities of AI models and\nemphasizes the importance of datasets that reflect the complexities of human-AI\ncollaborative writing. Through systematic preprocessing and rigorous\nstatistical analysis, this study offers valuable insights into the evolving\nlandscape of AI-generated content and informs future developments in natural\nlanguage processing (NLP).", "published": "2024-07-15 18:09:03", "link": "http://arxiv.org/abs/2408.00769v1", "categories": ["cs.DL", "cs.AI", "cs.CL"], "primary_category": "cs.DL"}
{"title": "PutnamBench: Evaluating Neural Theorem-Provers on the Putnam\n  Mathematical Competition", "abstract": "We present PutnamBench, a new multi-language benchmark for evaluating the\nability of neural theorem-provers to solve competition mathematics problems.\nPutnamBench consists of 1692 hand-constructed formalizations of 640 theorems\nsourced from the William Lowell Putnam Mathematical Competition, the premier\nundergraduate-level mathematics competition in North America. All the problems\nhave formalizations in Lean 4 and Isabelle; a substantial subset also has Coq\nformalizations. PutnamBench requires significant problem-solving ability and\nproficiency in a broad range of topics taught in undergraduate mathematics\ncourses. We use PutnamBench to evaluate several established neural and symbolic\ntheorem-provers. These approaches can only solve a handful of the PutnamBench\nproblems, establishing the benchmark as a difficult open challenge for research\non neural theorem-proving. PutnamBench is available at\nhttps://github.com/trishullab/PutnamBench.", "published": "2024-07-15 19:57:15", "link": "http://arxiv.org/abs/2407.11214v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO", "cs.PL"], "primary_category": "cs.AI"}
{"title": "Unraveling the Truth: Do VLMs really Understand Charts? A Deep Dive into\n  Consistency and Robustness", "abstract": "Chart question answering (CQA) is a crucial area of Visual Language\nUnderstanding. However, the robustness and consistency of current Visual\nLanguage Models (VLMs) in this field remain under-explored. This paper\nevaluates state-of-the-art VLMs on comprehensive datasets, developed\nspecifically for this study, encompassing diverse question categories and chart\nformats. We investigate two key aspects: 1) the models' ability to handle\nvarying levels of chart and question complexity, and 2) their robustness across\ndifferent visual representations of the same underlying data. Our analysis\nreveals significant performance variations based on question and chart types,\nhighlighting both strengths and weaknesses of current models. Additionally, we\nidentify areas for improvement and propose future research directions to build\nmore robust and reliable CQA systems. This study sheds light on the limitations\nof current models and paves the way for future advancements in the field.", "published": "2024-07-15 20:29:24", "link": "http://arxiv.org/abs/2407.11229v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AstroMLab 1: Who Wins Astronomy Jeopardy!?", "abstract": "We present a comprehensive evaluation of proprietary and open-weights large\nlanguage models using the first astronomy-specific benchmarking dataset. This\ndataset comprises 4,425 multiple-choice questions curated from the Annual\nReview of Astronomy and Astrophysics, covering a broad range of astrophysical\ntopics. Our analysis examines model performance across various astronomical\nsubfields and assesses response calibration, crucial for potential deployment\nin research environments. Claude-3.5-Sonnet outperforms competitors by up to\n4.6 percentage points, achieving 85.0% accuracy. For proprietary models, we\nobserved a universal reduction in cost every 3-to-12 months to achieve similar\nscore in this particular astronomy benchmark. open-weights models have rapidly\nimproved, with LLaMA-3-70b (80.6%) and Qwen-2-72b (77.7%) now competing with\nsome of the best proprietary models. We identify performance variations across\ntopics, with non-English-focused models generally struggling more in\nexoplanet-related fields, stellar astrophysics, and instrumentation related\nquestions. These challenges likely stem from less abundant training data,\nlimited historical context, and rapid recent developments in these areas. This\npattern is observed across both open-weights and proprietary models, with\nregional dependencies evident, highlighting the impact of training data\ndiversity on model performance in specialized scientific domains.\nTop-performing models demonstrate well-calibrated confidence, with correlations\nabove 0.9 between confidence and correctness, though they tend to be slightly\nunderconfident. The development for fast, low-cost inference of open-weights\nmodels presents new opportunities for affordable deployment in astronomy. The\nrapid progress observed suggests that LLM-driven research in astronomy may\nbecome feasible in the near future.", "published": "2024-07-15 19:28:14", "link": "http://arxiv.org/abs/2407.11194v2", "categories": ["astro-ph.IM", "astro-ph.EP", "astro-ph.GA", "astro-ph.SR", "cs.AI", "cs.CL"], "primary_category": "astro-ph.IM"}
{"title": "Mechanistic interpretability of large language models with applications\n  to the financial services industry", "abstract": "Large Language Models such as GPTs (Generative Pre-trained Transformers)\nexhibit remarkable capabilities across a broad spectrum of applications.\nNevertheless, due to their intrinsic complexity, these models present\nsubstantial challenges in interpreting their internal decision-making\nprocesses. This lack of transparency poses critical challenges when it comes to\ntheir adaptation by financial institutions, where concerns and accountability\nregarding bias, fairness, and reliability are of paramount importance.\nMechanistic interpretability aims at reverse engineering complex AI models such\nas transformers. In this paper, we are pioneering the use of mechanistic\ninterpretability to shed some light on the inner workings of large language\nmodels for use in financial services applications. We offer several examples of\nhow algorithmic tasks can be designed for compliance monitoring purposes. In\nparticular, we investigate GPT-2 Small's attention pattern when prompted to\nidentify potential violation of Fair Lending laws. Using direct logit\nattribution, we study the contributions of each layer and its corresponding\nattention heads to the logit difference in the residual stream. Finally, we\ndesign clean and corrupted prompts and use activation patching as a causal\nintervention method to localize our task completion components further. We\nobserve that the (positive) heads $10.2$ (head $2$, layer $10$), $10.7$, and\n$11.3$, as well as the (negative) heads $9.6$ and $10.6$ play a significant\nrole in the task completion.", "published": "2024-07-15 19:59:53", "link": "http://arxiv.org/abs/2407.11215v2", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CL", "cs.NA", "math.NA", "68T01", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Towards zero-shot amplifier modeling: One-to-many amplifier modeling via\n  tone embedding control", "abstract": "Replicating analog device circuits through neural audio effect modeling has\ngarnered increasing interest in recent years. Existing work has predominantly\nfocused on a one-to-one emulation strategy, modeling specific devices\nindividually. In this paper, we tackle the less-explored scenario of\none-to-many emulation, utilizing conditioning mechanisms to emulate multiple\nguitar amplifiers through a single neural model. For condition representation,\nwe use contrastive learning to build a tone embedding encoder that extracts\nstyle-related features of various amplifiers, leveraging a dataset of\ncomprehensive amplifier settings. Targeting zero-shot application scenarios, we\nalso examine various strategies for tone embedding representation, evaluating\nreferenced tone embedding against two retrieval-based embedding methods for\namplifiers unseen in the training time. Our findings showcase the efficacy and\npotential of the proposed methods in achieving versatile one-to-many amplifier\nmodeling, contributing a foundational step towards zero-shot audio modeling\napplications.", "published": "2024-07-15 12:04:56", "link": "http://arxiv.org/abs/2407.10646v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Mutual Learning for Acoustic Matching and Dereverberation via Visual\n  Scene-driven Diffusion", "abstract": "Visual acoustic matching (VAM) is pivotal for enhancing the immersive\nexperience, and the task of dereverberation is effective in improving audio\nintelligibility. Existing methods treat each task independently, overlooking\nthe inherent reciprocity between them. Moreover, these methods depend on paired\ntraining data, which is challenging to acquire, impeding the utilization of\nextensive unpaired data. In this paper, we introduce MVSD, a mutual learning\nframework based on diffusion models. MVSD considers the two tasks\nsymmetrically, exploiting the reciprocal relationship to facilitate learning\nfrom inverse tasks and overcome data scarcity. Furthermore, we employ the\ndiffusion model as foundational conditional converters to circumvent the\ntraining instability and over-smoothing drawbacks of conventional GAN\narchitectures. Specifically, MVSD employs two converters: one for VAM called\nreverberator and one for dereverberation called dereverberator. The\ndereverberator judges whether the reverberation audio generated by reverberator\nsounds like being in the conditional visual scenario, and vice versa. By\nforming a closed loop, these two converters can generate informative feedback\nsignals to optimize the inverse tasks, even with easily acquired one-way\nunpaired data. Extensive experiments on two standard benchmarks, i.e.,\nSoundSpaces-Speech and Acoustic AVSpeech, exhibit that our framework can\nimprove the performance of the reverberator and dereverberator and better match\nspecified visual scenarios.", "published": "2024-07-15 00:47:56", "link": "http://arxiv.org/abs/2407.10373v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Masked Generative Video-to-Audio Transformers with Enhanced\n  Synchronicity", "abstract": "Video-to-audio (V2A) generation leverages visual-only video features to\nrender plausible sounds that match the scene. Importantly, the generated sound\nonsets should match the visual actions that are aligned with them, otherwise\nunnatural synchronization artifacts arise. Recent works have explored the\nprogression of conditioning sound generators on still images and then video\nfeatures, focusing on quality and semantic matching while ignoring\nsynchronization, or by sacrificing some amount of quality to focus on improving\nsynchronization only. In this work, we propose a V2A generative model, named\nMaskVAT, that interconnects a full-band high-quality general audio codec with a\nsequence-to-sequence masked generative model. This combination allows modeling\nboth high audio quality, semantic matching, and temporal synchronicity at the\nsame time. Our results show that, by combining a high-quality codec with the\nproper pre-trained audio-visual features and a sequence-to-sequence parallel\nstructure, we are able to yield highly synchronized results on one hand, whilst\nbeing competitive with the state of the art of non-codec generative audio\nmodels. Sample videos and generated audios are available at\nhttps://maskvat.github.io .", "published": "2024-07-15 01:49:59", "link": "http://arxiv.org/abs/2407.10387v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DDFAD: Dataset Distillation Framework for Audio Data", "abstract": "Deep neural networks (DNNs) have achieved significant success in numerous\napplications. The remarkable performance of DNNs is largely attributed to the\navailability of massive, high-quality training datasets. However, processing\nsuch massive training data requires huge computational and storage resources.\nDataset distillation is a promising solution to this problem, offering the\ncapability to compress a large dataset into a smaller distilled dataset. The\nmodel trained on the distilled dataset can achieve comparable performance to\nthe model trained on the whole dataset.\n  While dataset distillation has been demonstrated in image data, none have\nexplored dataset distillation for audio data. In this work, for the first time,\nwe propose a Dataset Distillation Framework for Audio Data (DDFAD).\nSpecifically, we first propose the Fused Differential MFCC (FD-MFCC) as\nextracted features for audio data. After that, the FD-MFCC is distilled through\nthe matching training trajectory distillation method. Finally, we propose an\naudio signal reconstruction algorithm based on the Griffin-Lim Algorithm to\nreconstruct the audio signal from the distilled FD-MFCC. Extensive experiments\ndemonstrate the effectiveness of DDFAD on various audio datasets. In addition,\nwe show that DDFAD has promising application prospects in many applications,\nsuch as continual learning and neural architecture search.", "published": "2024-07-15 05:23:35", "link": "http://arxiv.org/abs/2407.10446v1", "categories": ["cs.SD", "cs.AI", "cs.DB", "eess.AS"], "primary_category": "cs.SD"}
{"title": "BandControlNet: Parallel Transformers-based Steerable Popular Music\n  Generation with Fine-Grained Spatiotemporal Features", "abstract": "Controllable music generation promotes the interaction between humans and\ncomposition systems by projecting the users' intent on their desired music. The\nchallenge of introducing controllability is an increasingly important issue in\nthe symbolic music generation field. When building controllable generative\npopular multi-instrument music systems, two main challenges typically present\nthemselves, namely weak controllability and poor music quality. To address\nthese issues, we first propose spatiotemporal features as powerful and\nfine-grained controls to enhance the controllability of the generative model.\nIn addition, an efficient music representation called REMI_Track is designed to\nconvert multitrack music into multiple parallel music sequences and shorten the\nsequence length of each track with Byte Pair Encoding (BPE) techniques.\nSubsequently, we release BandControlNet, a conditional model based on parallel\nTransformers, to tackle the multiple music sequences and generate high-quality\nmusic samples that are conditioned to the given spatiotemporal control\nfeatures. More concretely, the two specially designed modules of\nBandControlNet, namely structure-enhanced self-attention (SE-SA) and\nCross-Track Transformer (CTT), are utilized to strengthen the resulting musical\nstructure and inter-track harmony modeling respectively. Experimental results\ntested on two popular music datasets of different lengths demonstrate that the\nproposed BandControlNet outperforms other conditional music generation models\non most objective metrics in terms of fidelity and inference speed and shows\ngreat robustness in generating long music samples. The subjective evaluations\nshow BandControlNet trained on short datasets can generate music with\ncomparable quality to state-of-the-art models, while outperforming them\nsignificantly using longer datasets.", "published": "2024-07-15 06:33:25", "link": "http://arxiv.org/abs/2407.10462v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LiteFocus: Accelerated Diffusion Inference for Long Audio Synthesis", "abstract": "Latent diffusion models have shown promising results in audio generation,\nmaking notable advancements over traditional methods. However, their\nperformance, while impressive with short audio clips, faces challenges when\nextended to longer audio sequences. These challenges are due to model's\nself-attention mechanism and training predominantly on 10-second clips, which\ncomplicates the extension to longer audio without adaptation. In response to\nthese issues, we introduce a novel approach, LiteFocus that enhances the\ninference of existing audio latent diffusion models in long audio synthesis.\nObserved the attention pattern in self-attention, we employ a dual sparse form\nfor attention calculation, designated as same-frequency focus and\ncross-frequency compensation, which curtails the attention computation under\nsame-frequency constraints, while enhancing audio quality through\ncross-frequency refillment. LiteFocus demonstrates substantial reduction on\ninference time with diffusion-based TTA model by 1.99x in synthesizing\n80-second audio clips while also obtaining improved audio quality.", "published": "2024-07-15 06:49:05", "link": "http://arxiv.org/abs/2407.10468v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GROOT: Generating Robust Watermark for Diffusion-Model-Based Audio\n  Synthesis", "abstract": "Amid the burgeoning development of generative models like diffusion models,\nthe task of differentiating synthesized audio from its natural counterpart\ngrows more daunting. Deepfake detection offers a viable solution to combat this\nchallenge. Yet, this defensive measure unintentionally fuels the continued\nrefinement of generative models. Watermarking emerges as a proactive and\nsustainable tactic, preemptively regulating the creation and dissemination of\nsynthesized content. Thus, this paper, as a pioneer, proposes the generative\nrobust audio watermarking method (Groot), presenting a paradigm for proactively\nsupervising the synthesized audio and its source diffusion models. In this\nparadigm, the processes of watermark generation and audio synthesis occur\nsimultaneously, facilitated by parameter-fixed diffusion models equipped with a\ndedicated encoder. The watermark embedded within the audio can subsequently be\nretrieved by a lightweight decoder. The experimental results highlight Groot's\noutstanding performance, particularly in terms of robustness, surpassing that\nof the leading state-of-the-art methods. Beyond its impressive resilience\nagainst individual post-processing attacks, Groot exhibits exceptional\nrobustness when facing compound attacks, maintaining an average watermark\nextraction accuracy of around 95%.", "published": "2024-07-15 06:57:19", "link": "http://arxiv.org/abs/2407.10471v2", "categories": ["cs.CR", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Classification of Heart Sounds Using Multi-Branch Deep Convolutional\n  Network and LSTM-CNN", "abstract": "This paper presents a fast and cost-effective method for diagnosing cardiac\nabnormalities with high accuracy and reliability using low-cost systems in\nclinics. The primary limitation of automatic diagnosing of cardiac diseases is\nthe rarity of correct and acceptable labeled samples, which can be expensive to\nprepare. To address this issue, two methods are proposed in this work. The\nfirst method is a unique Multi-Branch Deep Convolutional Neural Network (MBDCN)\narchitecture inspired by human auditory processing, specifically designed to\noptimize feature extraction by employing various sizes of convolutional filters\nand audio signal power spectrum as input. In the second method, called as Long\nshort-term memory-Convolutional Neural (LSCN) model, Additionally, the network\narchitecture includes Long Short-Term Memory (LSTM) network blocks to improve\nfeature extraction in the time domain. The innovative approach of combining\nmultiple parallel branches consisting of the one-dimensional convolutional\nlayers along with LSTM blocks helps in achieving superior results in audio\nsignal processing tasks. The experimental results demonstrate superiority of\nthe proposed methods over the state-of-the-art techniques. The overall\nclassification accuracy of heart sounds with the LSCN network is more than 96%.\nThe efficiency of this network is significant compared to common feature\nextraction methods such as Mel Frequency Cepstral Coefficients (MFCC) and\nwavelet transform. Therefore, the proposed method shows promising results in\nthe automatic analysis of heart sounds and has potential applications in the\ndiagnosis and early detection of cardiovascular diseases.", "published": "2024-07-15 13:02:54", "link": "http://arxiv.org/abs/2407.10689v5", "categories": ["eess.SP", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Towards Enhanced Classification of Abnormal Lung sound in Multi-breath:\n  A Light Weight Multi-label and Multi-head Attention Classification Method", "abstract": "This study aims to develop an auxiliary diagnostic system for classifying\nabnormal lung respiratory sounds, enhancing the accuracy of automatic abnormal\nbreath sound classification through an innovative multi-label learning approach\nand multi-head attention mechanism. Addressing the issue of class imbalance and\nlack of diversity in existing respiratory sound datasets, our study employs a\nlightweight and highly accurate model, using a two-dimensional label set to\nrepresent multiple respiratory sound characteristics. Our method achieved a\n59.2% ICBHI score in the four-category task on the ICBHI2017 dataset,\ndemonstrating its advantages in terms of lightweight and high accuracy. This\nstudy not only improves the accuracy of automatic diagnosis of lung respiratory\nsound abnormalities but also opens new possibilities for clinical applications.", "published": "2024-07-15 15:40:02", "link": "http://arxiv.org/abs/2407.10828v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cluster and Separate: a GNN Approach to Voice and Staff Prediction for\n  Score Engraving", "abstract": "This paper approaches the problem of separating the notes from a quantized\nsymbolic music piece (e.g., a MIDI file) into multiple voices and staves. This\nis a fundamental part of the larger task of music score engraving (or score\ntypesetting), which aims to produce readable musical scores for human\nperformers. We focus on piano music and support homophonic voices, i.e., voices\nthat can contain chords, and cross-staff voices, which are notably difficult\ntasks that have often been overlooked in previous research. We propose an\nend-to-end system based on graph neural networks that clusters notes that\nbelong to the same chord and connects them with edges if they are part of a\nvoice. Our results show clear and consistent improvements over a previous\napproach on two datasets of different styles. To aid the qualitative analysis\nof our results, we support the export in symbolic music formats and provide a\ndirect visualization of our outputs graph over the musical score. All code and\npre-trained models are available at https://github.com/CPJKU/piano_svsep", "published": "2024-07-15 14:36:13", "link": "http://arxiv.org/abs/2407.21030v1", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
