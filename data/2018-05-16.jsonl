{"title": "Learning to Write with Cooperative Discriminators", "abstract": "Recurrent Neural Networks (RNNs) are powerful autoregressive sequence models,\nbut when used to generate natural language their output tends to be overly\ngeneric, repetitive, and self-contradictory. We postulate that the objective\nfunction optimized by RNN language models, which amounts to the overall\nperplexity of a text, is not expressive enough to capture the notion of\ncommunicative goals described by linguistic principles such as Grice's Maxims.\nWe propose learning a mixture of multiple discriminative models that can be\nused to complement the RNN generator and guide the decoding process. Human\nevaluation demonstrates that text generated by our system is preferred over\nthat of baselines by a large margin and significantly enhances the overall\ncoherence, style, and information content of the generated text.", "published": "2018-05-16 01:27:58", "link": "http://arxiv.org/abs/1805.06087v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What's in a Domain? Learning Domain-Robust Text Representations using\n  Adversarial Training", "abstract": "Most real world language problems require learning from heterogenous corpora,\nraising the problem of learning robust models which generalise well to both\nsimilar (in domain) and dissimilar (out of domain) instances to those seen in\ntraining. This requires learning an underlying task, while not learning\nirrelevant signals and biases specific to individual domains. We propose a\nnovel method to optimise both in- and out-of-domain accuracy based on joint\nlearning of a structured neural model with domain-specific and domain-general\ncomponents, coupled with adversarial training for domain. Evaluating on\nmulti-domain language identification and multi-domain sentiment analysis, we\nshow substantial improvements over standard domain adaptation techniques, and\ndomain-adversarial training.", "published": "2018-05-16 01:29:14", "link": "http://arxiv.org/abs/1805.06088v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Robust and Privacy-preserving Text Representations", "abstract": "Written text often provides sufficient clues to identify the author, their\ngender, age, and other important attributes. Consequently, the authorship of\ntraining and evaluation corpora can have unforeseen impacts, including\ndiffering model performance for different user groups, as well as privacy\nimplications. In this paper, we propose an approach to explicitly obscure\nimportant author characteristics at training time, such that representations\nlearned are invariant to these attributes. Evaluating on two tasks, we show\nthat this leads to increased privacy in the learned representations, as well as\nmore robust models to varying evaluation conditions, including out-of-domain\ncorpora.", "published": "2018-05-16 01:57:47", "link": "http://arxiv.org/abs/1805.06093v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Narrative Modeling with Memory Chains and Semantic Supervision", "abstract": "Story comprehension requires a deep semantic understanding of the narrative,\nmaking it a challenging task. Inspired by previous studies on ROC Story Cloze\nTest, we propose a novel method, tracking various semantic aspects with\nexternal neural memory chains while encouraging each to focus on a particular\nsemantic aspect. Evaluated on the task of story ending prediction, our model\ndemonstrates superior performance to a collection of competitive baselines,\nsetting a new state of the art.", "published": "2018-05-16 04:17:18", "link": "http://arxiv.org/abs/1805.06122v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Robust Neural Machine Translation", "abstract": "Small perturbations in the input can severely distort intermediate\nrepresentations and thus impact translation quality of neural machine\ntranslation (NMT) models. In this paper, we propose to improve the robustness\nof NMT models with adversarial stability training. The basic idea is to make\nboth the encoder and decoder in NMT models robust against input perturbations\nby enabling them to behave similarly for the original input and its perturbed\ncounterpart. Experimental results on Chinese-English, English-German and\nEnglish-French translation tasks show that our approaches can not only achieve\nsignificant improvements over strong NMT systems but also improve the\nrobustness of NMT models.", "published": "2018-05-16 04:51:29", "link": "http://arxiv.org/abs/1805.06130v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Training of Candidate Extraction and Answer Selection for Reading\n  Comprehension", "abstract": "While sophisticated neural-based techniques have been developed in reading\ncomprehension, most approaches model the answer in an independent manner,\nignoring its relations with other answer candidates. This problem can be even\nworse in open-domain scenarios, where candidates from multiple passages should\nbe combined to answer a single question. In this paper, we formulate reading\ncomprehension as an extract-then-select two-stage procedure. We first extract\nanswer candidates from passages, then select the final answer by combining\ninformation from all the candidates. Furthermore, we regard candidate\nextraction as a latent variable and train the two-stage process jointly with\nreinforcement learning. As a result, our approach has improved the\nstate-of-the-art performance significantly on two challenging open-domain\nreading comprehension datasets. Further analysis demonstrates the effectiveness\nof our model components, especially the information fusion of all the\ncandidates and the joint training of the extract-then-select procedure.", "published": "2018-05-16 06:14:31", "link": "http://arxiv.org/abs/1805.06145v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Color naming reflects both perceptual structure and communicative need", "abstract": "Gibson et al. (2017) argued that color naming is shaped by patterns of\ncommunicative need. In support of this claim, they showed that color naming\nsystems across languages support more precise communication about warm colors\nthan cool colors, and that the objects we talk about tend to be warm-colored\nrather than cool-colored. Here, we present new analyses that alter this\npicture. We show that greater communicative precision for warm than for cool\ncolors, and greater communicative need, may both be explained by perceptual\nstructure. However, using an information-theoretic analysis, we also show that\ncolor naming across languages bears signs of communicative need beyond what\nwould be predicted by perceptual structure alone. We conclude that color naming\nis shaped both by perceptual structure, as has traditionally been argued, and\nby patterns of communicative need, as argued by Gibson et al. - although for\nreasons other than those they advanced.", "published": "2018-05-16 07:34:00", "link": "http://arxiv.org/abs/1805.06165v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Unified Model for Extractive and Abstractive Summarization using\n  Inconsistency Loss", "abstract": "We propose a unified model combining the strength of extractive and\nabstractive summarization. On the one hand, a simple extractive model can\nobtain sentence-level attention with high ROUGE scores but less readable. On\nthe other hand, a more complicated abstractive model can obtain word-level\ndynamic attention to generate a more readable paragraph. In our model,\nsentence-level attention is used to modulate the word-level attention such that\nwords in less attended sentences are less likely to be generated. Moreover, a\nnovel inconsistency loss function is introduced to penalize the inconsistency\nbetween two levels of attentions. By end-to-end training our model with the\ninconsistency loss and original losses of extractive and abstractive models, we\nachieve state-of-the-art ROUGE scores while being the most informative and\nreadable summarization on the CNN/Daily Mail dataset in a solid human\nevaluation.", "published": "2018-05-16 12:17:09", "link": "http://arxiv.org/abs/1805.06266v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Annotation of Locative and Directional Expressions in Arabic", "abstract": "In this paper, we introduce a rule-based approach to annotate Locative and\nDirectional Expressions in Arabic natural language text. The annotation is\nbased on a constructed semantic map of the spatiality domain. Challenges are\ntwofold: first, we need to study how locative and directional expressions are\nexpressed linguistically in these texts; and second, we need to automatically\nannotate the relevant textual segments accordingly. The research method we will\nuse in this article is analytic-descriptive. We will validate this approach on\nspecific novel rich with these expressions and show that it has very promising\nresults. We will be using NOOJ as a software tool to implement finite-state\ntransducers to annotate linguistic elements according to Locative and\nDirectional Expressions. In conclusion, NOOJ allowed us to write linguistic\nrules for the automatic annotation in Arabic text of Locative and Directional\nExpressions.", "published": "2018-05-16 14:26:32", "link": "http://arxiv.org/abs/1805.06344v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "#phramacovigilance - Exploring Deep Learning Techniques for Identifying\n  Mentions of Medication Intake from Twitter", "abstract": "Mining social media messages for health and drug related information has\nreceived significant interest in pharmacovigilance research. Social media sites\n(e.g., Twitter), have been used for monitoring drug abuse, adverse reactions of\ndrug usage and analyzing expression of sentiments related to drugs. Most of\nthese studies are based on aggregated results from a large population rather\nthan specific sets of individuals. In order to conduct studies at an individual\nlevel or specific cohorts, identifying posts mentioning intake of medicine by\nthe user is necessary. Towards this objective, we train different deep neural\nnetwork classification models on a publicly available annotated dataset and\nstudy their performances on identifying mentions of personal intake of medicine\nin tweets. We also design and train a new architecture of a stacked ensemble of\nshallow convolutional neural network (CNN) ensembles. We use random search for\ntuning the hyperparameters of the models and share the details of the values\ntaken by the hyperparameters for the best learnt model in different deep neural\nnetwork architectures. Our system produces state-of-the-art results, with a\nmicro- averaged F-score of 0.693.", "published": "2018-05-16 15:43:21", "link": "http://arxiv.org/abs/1805.06375v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CASCADE: Contextual Sarcasm Detection in Online Discussion Forums", "abstract": "The literature in automated sarcasm detection has mainly focused on lexical,\nsyntactic and semantic-level analysis of text. However, a sarcastic sentence\ncan be expressed with contextual presumptions, background and commonsense\nknowledge. In this paper, we propose CASCADE (a ContextuAl SarCasm DEtector)\nthat adopts a hybrid approach of both content and context-driven modeling for\nsarcasm detection in online social media discussions. For the latter, CASCADE\naims at extracting contextual information from the discourse of a discussion\nthread. Also, since the sarcastic nature and form of expression can vary from\nperson to person, CASCADE utilizes user embeddings that encode stylometric and\npersonality features of the users. When used along with content-based feature\nextractors such as Convolutional Neural Networks (CNNs), we see a significant\nboost in the classification performance on a large Reddit corpus.", "published": "2018-05-16 16:38:38", "link": "http://arxiv.org/abs/1805.06413v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Composite Semantic Relation Classification", "abstract": "Different semantic interpretation tasks such as text entailment and question\nanswering require the classification of semantic relations between terms or\nentities within text. However, in most cases it is not possible to assign a\ndirect semantic relation between entities/terms. This paper proposes an\napproach for composite semantic relation classification, extending the\ntraditional semantic relation classification task. Different from existing\napproaches, which use machine learning models built over lexical and\ndistributional word vector features, the proposed model uses the combination of\na large commonsense knowledge base of binary relations, a distributional\nnavigational algorithm and sequence classification to provide a solution for\nthe composite semantic relation classification problem.", "published": "2018-05-16 20:41:33", "link": "http://arxiv.org/abs/1805.06521v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Relatedness for All (Languages): A Comparative Analysis of\n  Multilingual Semantic Relatedness Using Machine Translation", "abstract": "This paper provides a comparative analysis of the performance of four\nstate-of-the-art distributional semantic models (DSMs) over 11 languages,\ncontrasting the native language-specific models with the use of machine\ntranslation over English-based DSMs. The experimental results show that there\nis a significant improvement (average of 16.7% for the Spearman correlation) by\nusing state-of-the-art machine translation approaches. The results also show\nthat the benefit of using the most informative corpus outweighs the possible\nerrors introduced by the machine translation. For all languages, the\ncombination of machine translation over the Word2Vec English distributional\nmodel provided the best results consistently (average Spearman correlation of\n0.68).", "published": "2018-05-16 20:43:45", "link": "http://arxiv.org/abs/1805.06522v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Naive Psychology of Characters in Simple Commonsense Stories", "abstract": "Understanding a narrative requires reading between the lines and reasoning\nabout the unspoken but obvious implications about events and people's mental\nstates - a capability that is trivial for humans but remarkably hard for\nmachines. To facilitate research addressing this challenge, we introduce a new\nannotation framework to explain naive psychology of story characters as\nfully-specified chains of mental states with respect to motivations and\nemotional reactions. Our work presents a new large-scale dataset with rich\nlow-level annotations and establishes baseline performance on several new\ntasks, suggesting avenues for future research.", "published": "2018-05-16 21:39:02", "link": "http://arxiv.org/abs/1805.06533v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are BLEU and Meaning Representation in Opposition?", "abstract": "One of possible ways of obtaining continuous-space sentence representations\nis by training neural machine translation (NMT) systems. The recent attention\nmechanism however removes the single point in the neural network from which the\nsource sentence representation can be extracted. We propose several variations\nof the attentive NMT architecture bringing this meeting point back. Empirical\nevaluation suggests that the better the translation quality, the worse the\nlearned sentence representations serve in a wide range of classification and\nsimilarity tasks.", "published": "2018-05-16 21:42:21", "link": "http://arxiv.org/abs/1805.06536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Deep Ensemble Model with Slot Alignment for Sequence-to-Sequence\n  Natural Language Generation", "abstract": "Natural language generation lies at the core of generative dialogue systems\nand conversational agents. We describe an ensemble neural language generator,\nand present several novel methods for data representation and augmentation that\nyield improved results in our model. We test the model on three datasets in the\nrestaurant, TV and laptop domains, and report both objective and subjective\nevaluations of our best model. Using a range of automatic metrics, as well as\nhuman evaluators, we show that our approach achieves better results than\nstate-of-the-art models on the same datasets.", "published": "2018-05-16 23:30:01", "link": "http://arxiv.org/abs/1805.06553v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extending a Parser to Distant Domains Using a Few Dozen Partially\n  Annotated Examples", "abstract": "We revisit domain adaptation for parsers in the neural era. First we show\nthat recent advances in word representations greatly diminish the need for\ndomain adaptation when the target domain is syntactically similar to the source\ndomain. As evidence, we train a parser on the Wall Street Jour- nal alone that\nachieves over 90% F1 on the Brown corpus. For more syntactically dis- tant\ndomains, we provide a simple way to adapt a parser using only dozens of partial\nannotations. For instance, we increase the percentage of error-free\ngeometry-domain parses in a held-out set from 45% to 73% using approximately\nfive dozen training examples. In the process, we demon- strate a new\nstate-of-the-art single model result on the Wall Street Journal test set of\n94.3%. This is an absolute increase of 1.7% over the previous state-of-the-art\nof 92.6%.", "published": "2018-05-16 23:42:04", "link": "http://arxiv.org/abs/1805.06556v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual Augmentation: Data Augmentation by Words with Paradigmatic\n  Relations", "abstract": "We propose a novel data augmentation for labeled sentences called contextual\naugmentation. We assume an invariance that sentences are natural even if the\nwords in the sentences are replaced with other words with paradigmatic\nrelations. We stochastically replace words with other words that are predicted\nby a bi-directional language model at the word positions. Words predicted\naccording to a context are numerous but appropriate for the augmentation of the\noriginal words. Furthermore, we retrofit a language model with a\nlabel-conditional architecture, which allows the model to augment sentences\nwithout breaking the label-compatibility. Through the experiments for six\nvarious different text classification tasks, we demonstrate that the proposed\nmethod improves classifiers based on the convolutional or recurrent neural\nnetworks.", "published": "2018-05-16 09:10:21", "link": "http://arxiv.org/abs/1805.06201v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Composing Finite State Transducers on GPUs", "abstract": "Weighted finite-state transducers (FSTs) are frequently used in language\nprocessing to handle tasks such as part-of-speech tagging and speech\nrecognition. There has been previous work using multiple CPU cores to\naccelerate finite state algorithms, but limited attention has been given to\nparallel graphics processing unit (GPU) implementations. In this paper, we\nintroduce the first (to our knowledge) GPU implementation of the FST\ncomposition operation, and we also discuss the optimizations used to achieve\nthe best performance on this architecture. We show that our approach obtains\nspeedups of up to 6x over our serial implementation and 4.5x over OpenFST.", "published": "2018-05-16 15:56:17", "link": "http://arxiv.org/abs/1805.06383v1", "categories": ["cs.CL", "cs.DC"], "primary_category": "cs.CL"}
{"title": "DINFRA: A One Stop Shop for Computing Multilingual Semantic Relatedness", "abstract": "This demonstration presents an infrastructure for computing multilingual\nsemantic relatedness and correlation for twelve natural languages by using\nthree distributional semantic models (DSMs). Our demonsrator - DInfra\n(Distributional Infrastructure) provides researchers and developers with a\nhighly useful platform for processing large-scale corpora and conducting\nexperiments with distributional semantics. We integrate several multilingual\nDSMs in our webservice so the end user can obtain a result without worrying\nabout the complexities involved in building DSMs. Our webservice allows the\nusers to have easy access to a wide range of comparisons of DSMs with different\nparameters. In addition, users can configure and access DSM parameters using an\neasy to use API.", "published": "2018-05-16 21:06:59", "link": "http://arxiv.org/abs/1805.09644v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "FollowNet: Robot Navigation by Following Natural Language Directions\n  with Deep Reinforcement Learning", "abstract": "Understanding and following directions provided by humans can enable robots\nto navigate effectively in unknown situations. We present FollowNet, an\nend-to-end differentiable neural architecture for learning multi-modal\nnavigation policies. FollowNet maps natural language instructions as well as\nvisual and depth inputs to locomotion primitives. FollowNet processes\ninstructions using an attention mechanism conditioned on its visual and depth\ninput to focus on the relevant parts of the command while performing the\nnavigation task. Deep reinforcement learning (RL) a sparse reward learns\nsimultaneously the state representation, the attention function, and control\npolicies. We evaluate our agent on a dataset of complex natural language\ndirections that guide the agent through a rich and realistic dataset of\nsimulated homes. We show that the FollowNet agent learns to execute previously\nunseen instructions described with a similar vocabulary, and successfully\nnavigates along paths not encountered during training. The agent shows 30%\nimprovement over a baseline model without the attention mechanism, with 52%\nsuccess rate at novel instructions.", "published": "2018-05-16 06:29:18", "link": "http://arxiv.org/abs/1805.06150v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.RO"}
{"title": "A Comparison of Modeling Units in Sequence-to-Sequence Speech\n  Recognition with the Transformer on Mandarin Chinese", "abstract": "The choice of modeling units is critical to automatic speech recognition\n(ASR) tasks. Conventional ASR systems typically choose context-dependent states\n(CD-states) or context-dependent phonemes (CD-phonemes) as their modeling\nunits. However, it has been challenged by sequence-to-sequence attention-based\nmodels, which integrate an acoustic, pronunciation and language model into a\nsingle neural network. On English ASR tasks, previous attempts have already\nshown that the modeling unit of graphemes can outperform that of phonemes by\nsequence-to-sequence attention-based model.\n  In this paper, we are concerned with modeling units on Mandarin Chinese ASR\ntasks using sequence-to-sequence attention-based models with the Transformer.\nFive modeling units are explored including context-independent phonemes\n(CI-phonemes), syllables, words, sub-words and characters. Experiments on HKUST\ndatasets demonstrate that the lexicon free modeling units can outperform\nlexicon related modeling units in terms of character error rate (CER). Among\nfive modeling units, character based model performs best and establishes a new\nstate-of-the-art CER of $26.64\\%$ on HKUST datasets without a hand-designed\nlexicon and an extra language model integration, which corresponds to a $4.8\\%$\nrelative improvement over the existing best CER of $28.0\\%$ by the joint\nCTC-attention based encoder-decoder network.", "published": "2018-05-16 10:43:47", "link": "http://arxiv.org/abs/1805.06239v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Conversational Analysis using Utterance-level Attention-based\n  Bidirectional Recurrent Neural Networks", "abstract": "Recent approaches for dialogue act recognition have shown that context from\npreceding utterances is important to classify the subsequent one. It was shown\nthat the performance improves rapidly when the context is taken into account.\nWe propose an utterance-level attention-based bidirectional recurrent neural\nnetwork (Utt-Att-BiRNN) model to analyze the importance of preceding utterances\nto classify the current one. In our setup, the BiRNN is given the input set of\ncurrent and preceding utterances. Our model outperforms previous models that\nuse only preceding utterances as context on the used corpus. Another\ncontribution of the article is to discover the amount of information in each\nutterance to classify the subsequent one and to show that context-based\nlearning not only improves the performance but also achieves higher confidence\nin the classification. We use character- and word-level features to represent\nthe utterances. The results are presented for character and word feature\nrepresentations and as an ensemble model of both representations. We found that\nwhen classifying short utterances, the closest preceding utterances contributes\nto a higher degree.", "published": "2018-05-16 10:51:56", "link": "http://arxiv.org/abs/1805.06242v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.NE"], "primary_category": "cs.CL"}
{"title": "A Context-based Approach for Dialogue Act Recognition using Simple\n  Recurrent Neural Networks", "abstract": "Dialogue act recognition is an important part of natural language\nunderstanding. We investigate the way dialogue act corpora are annotated and\nthe learning approaches used so far. We find that the dialogue act is\ncontext-sensitive within the conversation for most of the classes.\nNevertheless, previous models of dialogue act classification work on the\nutterance-level and only very few consider context. We propose a novel\ncontext-based learning method to classify dialogue acts using a character-level\nlanguage model utterance representation, and we notice significant improvement.\nWe evaluate this method on the Switchboard Dialogue Act corpus, and our results\nshow that the consideration of the preceding utterances as a context of the\ncurrent utterance improves dialogue act detection.", "published": "2018-05-16 12:58:18", "link": "http://arxiv.org/abs/1805.06280v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.NE"], "primary_category": "cs.CL"}
{"title": "A robust self-learning method for fully unsupervised cross-lingual\n  mappings of word embeddings", "abstract": "Recent work has managed to learn cross-lingual word embeddings without\nparallel data by mapping monolingual embeddings to a shared space through\nadversarial training. However, their evaluation has focused on favorable\nconditions, using comparable corpora or closely-related languages, and we show\nthat they often fail in more realistic scenarios. This work proposes an\nalternative approach based on a fully unsupervised initialization that\nexplicitly exploits the structural similarity of the embeddings, and a robust\nself-learning algorithm that iteratively improves this solution. Our method\nsucceeds in all tested scenarios and obtains the best published results in\nstandard datasets, even surpassing previous supervised systems. Our\nimplementation is released as an open source project at\nhttps://github.com/artetxem/vecmap", "published": "2018-05-16 13:23:48", "link": "http://arxiv.org/abs/1805.06297v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Defoiling Foiled Image Captions", "abstract": "We address the task of detecting foiled image captions, i.e. identifying\nwhether a caption contains a word that has been deliberately replaced by a\nsemantically similar word, thus rendering it inaccurate with respect to the\nimage being described. Solving this problem should in principle require a\nfine-grained understanding of images to detect linguistically valid\nperturbations in captions. In such contexts, encoding sufficiently descriptive\nimage information becomes a key challenge. In this paper, we demonstrate that\nit is possible to solve this task using simple, interpretable yet powerful\nrepresentations based on explicit object information. Our models achieve\nstate-of-the-art performance on a standard dataset, with scores exceeding those\nachieved by humans on the task. We also measure the upper-bound performance of\nour models using gold standard annotations. Our analysis reveals that the\nsimpler model performs well even without image information, suggesting that the\ndataset contains strong linguistic bias.", "published": "2018-05-16 22:50:17", "link": "http://arxiv.org/abs/1805.06549v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
