{"title": "Fine-Grained Argument Unit Recognition and Classification", "abstract": "Prior work has commonly defined argument retrieval from heterogeneous\ndocument collections as a sentence-level classification task. Consequently,\nargument retrieval suffers both from low recall and from sentence segmentation\nerrors making it difficult for humans and machines to consume the arguments. In\nthis work, we argue that the task should be performed on a more fine-grained\nlevel of sequence labeling. For this, we define the task as Argument Unit\nRecognition and Classification (AURC). We present a dataset of arguments from\nheterogeneous sources annotated as spans of tokens within a sentence, as well\nas with a corresponding stance. We show that and how such difficult argument\nannotations can be effectively collected through crowdsourcing with high\ninterannotator agreement. The new benchmark, AURC-8, contains up to 15% more\narguments per topic as compared to annotations on the sentence level. We\nidentify a number of methods targeted at AURC sequence labeling, achieving\nclose to human performance on known domains. Further analysis also reveals\nthat, contrary to previous approaches, our methods are more robust against\nsentence segmentation errors. We publicly release our code and the AURC-8\ndataset.", "published": "2019-04-22 00:55:37", "link": "http://arxiv.org/abs/1904.09688v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Roles and Entities: Datasets and Models for Natural\n  Language Inference", "abstract": "We present two new datasets and a novel attention mechanism for Natural\nLanguage Inference (NLI). Existing neural NLI models, even though when trained\non existing large datasets, do not capture the notion of entity and role well\nand often end up making mistakes such as \"Peter signed a deal\" can be inferred\nfrom \"John signed a deal\". The two datasets have been developed to mitigate\nsuch issues and make the systems better at understanding the notion of\n\"entities\" and \"roles\". After training the existing architectures on the new\ndataset we observe that the existing architectures does not perform well on one\nof the new benchmark. We then propose a modification to the \"word-to-word\"\nattention function which has been uniformly reused across several popular NLI\narchitectures. The resulting architectures perform as well as their unmodified\ncounterparts on the existing benchmarks and perform significantly well on the\nnew benchmark for \"roles\" and \"entities\".", "published": "2019-04-22 05:06:48", "link": "http://arxiv.org/abs/1904.09720v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SocialIQA: Commonsense Reasoning about Social Interactions", "abstract": "We introduce Social IQa, the first largescale benchmark for commonsense\nreasoning about social situations. Social IQa contains 38,000 multiple choice\nquestions for probing emotional and social intelligence in a variety of\neveryday situations (e.g., Q: \"Jordan wanted to tell Tracy a secret, so Jordan\nleaned towards Tracy. Why did Jordan do this?\" A: \"Make sure no one else could\nhear\"). Through crowdsourcing, we collect commonsense questions along with\ncorrect and incorrect answers about social interactions, using a new framework\nthat mitigates stylistic artifacts in incorrect answers by asking workers to\nprovide the right answer to a different but related question. Empirical results\nshow that our benchmark is challenging for existing question-answering models\nbased on pretrained language models, compared to human performance (>20% gap).\nNotably, we further establish Social IQa as a resource for transfer learning of\ncommonsense knowledge, achieving state-of-the-art performance on multiple\ncommonsense reasoning tasks (Winograd Schemas, COPA).", "published": "2019-04-22 05:36:37", "link": "http://arxiv.org/abs/1904.09728v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tetra-Tagging: Word-Synchronous Parsing with Linear-Time Inference", "abstract": "We present a constituency parsing algorithm that, like a supertagger, works\nby assigning labels to each word in a sentence. In order to maximally leverage\ncurrent neural architectures, the model scores each word's tags in parallel,\nwith minimal task-specific structure. After scoring, a left-to-right\nreconciliation phase extracts a tree in (empirically) linear time. Our parser\nachieves 95.4 F1 on the WSJ test set while also achieving substantial speedups\ncompared to current state-of-the-art parsers with comparable accuracies.", "published": "2019-04-22 06:57:43", "link": "http://arxiv.org/abs/1904.09745v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Curious Case of Neural Text Degeneration", "abstract": "Despite considerable advancements with deep neural language models, the\nenigma of neural text degeneration persists when these models are tested as\ntext generators. The counter-intuitive empirical observation is that even\nthough the use of likelihood as training objective leads to high quality models\nfor a broad range of language understanding tasks, using likelihood as a\ndecoding objective leads to text that is bland and strangely repetitive.\n  In this paper, we reveal surprising distributional differences between human\ntext and machine text. In addition, we find that decoding strategies alone can\ndramatically effect the quality of machine text, even when generated from\nexactly the same neural language model. Our findings motivate Nucleus Sampling,\na simple but effective method to draw the best out of neural generation. By\nsampling text from the dynamic nucleus of the probability distribution, which\nallows for diversity while effectively truncating the less reliable tail of the\ndistribution, the resulting text better demonstrates the quality of human text,\nyielding enhanced diversity without sacrificing fluency and coherence.", "published": "2019-04-22 07:17:18", "link": "http://arxiv.org/abs/1904.09751v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Judging Chemical Reaction Practicality From Positive Sample only\n  Learning", "abstract": "Chemical reaction practicality is the core task among all symbol intelligence\nbased chemical information processing, for example, it provides indispensable\nclue for further automatic synthesis route inference. Considering that chemical\nreactions have been represented in a language form, we propose a new solution\nto generally judge the practicality of organic reaction without considering\ncomplex quantum physical modeling or chemistry knowledge. While tackling the\npracticality judgment as a machine learning task from positive and negative\n(chemical reaction) samples, all existing studies have to carefully handle the\nserious insufficiency issue on the negative samples. We propose an\nauto-construction method to well solve the extensively existed long-term\ndifficulty. Experimental results show our model can effectively predict the\npracticality of chemical reactions, which achieves a high accuracy of 99.76\\%\non real large-scale chemical lab reaction practicality judgment.", "published": "2019-04-22 12:35:38", "link": "http://arxiv.org/abs/1904.09824v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Poly-encoders: Transformer Architectures and Pre-training Strategies for\n  Fast and Accurate Multi-sentence Scoring", "abstract": "The use of deep pre-trained bidirectional transformers has led to remarkable\nprogress in a number of applications (Devlin et al., 2018). For tasks that make\npairwise comparisons between sequences, matching a given input with a\ncorresponding label, two approaches are common: Cross-encoders performing full\nself-attention over the pair and Bi-encoders encoding the pair separately. The\nformer often performs better, but is too slow for practical use. In this work,\nwe develop a new transformer architecture, the Poly-encoder, that learns global\nrather than token level self-attention features. We perform a detailed\ncomparison of all three approaches, including what pre-training and fine-tuning\nstrategies work best. We show our models achieve state-of-the-art results on\nthree existing tasks; that Poly-encoders are faster than Cross-encoders and\nmore accurate than Bi-encoders; and that the best results are obtained by\npre-training on large datasets similar to the downstream tasks.", "published": "2019-04-22 02:18:00", "link": "http://arxiv.org/abs/1905.01969v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Unsupervised Pretraining and Sentence Structure Modelling for\n  Winograd Schema Challenge", "abstract": "Winograd Schema Challenge (WSC) was proposed as an AI-hard problem in testing\ncomputers' intelligence on common sense representation and reasoning. This\npaper presents the new state-of-theart on WSC, achieving an accuracy of 71.1%.\nWe demonstrate that the leading performance benefits from jointly modelling\nsentence structures, utilizing knowledge learned from cutting-edge pretraining\nmodels, and performing fine-tuning. We conduct detailed analyses, showing that\nfine-tuning is critical for achieving the performance, but it helps more on the\nsimpler associative problems. Modelling sentence dependency structures,\nhowever, consistently helps on the harder non-associative subset of WSC.\nAnalysis also shows that larger fine-tuning datasets yield better performances,\nsuggesting the potential benefit of future work on annotating more Winograd\nschema sentences.", "published": "2019-04-22 03:00:40", "link": "http://arxiv.org/abs/1904.09705v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Compositional generalization in a deep seq2seq model by separating\n  syntax and semantics", "abstract": "Standard methods in deep learning for natural language processing fail to\ncapture the compositional structure of human language that allows for\nsystematic generalization outside of the training distribution. However, human\nlearners readily generalize in this way, e.g. by applying known grammatical\nrules to novel words. Inspired by work in neuroscience suggesting separate\nbrain systems for syntactic and semantic processing, we implement a\nmodification to standard approaches in neural machine translation, imposing an\nanalogous separation. The novel model, which we call Syntactic Attention,\nsubstantially outperforms standard methods in deep learning on the SCAN\ndataset, a compositional generalization task, without any hand-engineered\nfeatures or additional supervision. Our work suggests that separating syntactic\nfrom semantic learning may be a useful heuristic for capturing compositional\nstructure.", "published": "2019-04-22 03:12:09", "link": "http://arxiv.org/abs/1904.09708v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "hf0: A hybrid pitch extraction method for multimodal voice", "abstract": "Pitch or fundamental frequency (f0) extraction is a fundamental problem\nstudied extensively for its potential applications in speech and clinical\napplications. In literature, explicit mode specific (modal speech or singing\nvoice or emotional/ expressive speech or noisy speech) signal processing and\ndeep learning f0 extraction methods that exploit the quasi periodic nature of\nthe signal in time, harmonic property in spectral or combined form to extract\nthe pitch is developed. Hence, there is no single unified method which can\nreliably extract the pitch from various modes of the acoustic signal. In this\nwork, we propose a hybrid f0 extraction method which seamlessly extracts the\npitch across modes of speech production with very high accuracy required for\nmany applications. The proposed hybrid model exploits the advantages of deep\nlearning and signal processing methods to minimize the pitch detection error\nand adopts to various modes of acoustic signal. Specifically, we propose an\nordinal regression convolutional neural networks to map the periodicity rich\ninput representation to obtain the nominal pitch classes which drastically\nreduces the number of classes required for pitch detection unlike other deep\nlearning approaches. Further, the accurate f0 is estimated from the nominal\npitch class labels by filtering and autocorrelation. We show that the proposed\nmethod generalizes to the unseen modes of voice production and various noises\nfor large scale datasets. Also, the proposed hybrid model significantly reduces\nthe learning parameters required to train the deep model compared to other\nmethods. Furthermore,the evaluation measures showed that the proposed method is\nsignificantly better than the state-of-the-art signal processing and deep\nlearning approaches.", "published": "2019-04-22 08:08:12", "link": "http://arxiv.org/abs/1904.09765v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
