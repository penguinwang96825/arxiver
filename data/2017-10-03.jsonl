{"title": "Event Identification as a Decision Process with Non-linear\n  Representation of Text", "abstract": "We propose scale-free Identifier Network(sfIN), a novel model for event\nidentification in documents. In general, sfIN first encodes a document into\nmulti-scale memory stacks, then extracts special events via conducting\nmulti-scale actions, which can be considered as a special type of sequence\nlabelling. The design of large scale actions makes it more efficient processing\na long document. The whole model is trained with both supervised learning and\nreinforcement learning.", "published": "2017-10-03 03:24:28", "link": "http://arxiv.org/abs/1710.00969v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Annotation and Detection of Emotion in Text-based Dialogue Systems with\n  CNN", "abstract": "Knowledge of users' emotion states helps improve human-computer interaction.\nIn this work, we presented EmoNet, an emotion detector of Chinese daily\ndialogues based on deep convolutional neural networks. In order to maintain the\noriginal linguistic features, such as the order, commonly used methods like\nsegmentation and keywords extraction were not adopted, instead we increased the\ndepth of CNN and tried to let CNN learn inner linguistic relationships. Our\nmain contribution is that we presented a new model and a new pipeline which can\nbe used in multi-language environment to solve sentimental problems.\nExperimental results shows EmoNet has a great capacity in learning the emotion\nof dialogues and achieves a better result than other state of art detectors do.", "published": "2017-10-03 05:19:12", "link": "http://arxiv.org/abs/1710.00987v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is Structure Necessary for Modeling Argument Expectations in\n  Distributional Semantics?", "abstract": "Despite the number of NLP studies dedicated to thematic fit estimation,\nlittle attention has been paid to the related task of composing and updating\nverb argument expectations. The few exceptions have mostly modeled this\nphenomenon with structured distributional models, implicitly assuming a\nsimilarly structured representation of events. Recent experimental evidence,\nhowever, suggests that human processing system could also exploit an\nunstructured \"bag-of-arguments\" type of event representation to predict\nupcoming input. In this paper, we re-implement a traditional structured model\nand adapt it to compare the different hypotheses concerning the degree of\nstructure in our event knowledge, evaluating their relative performance in the\ntask of the argument expectations update.", "published": "2017-10-03 06:14:31", "link": "http://arxiv.org/abs/1710.00998v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MMCR4NLP: Multilingual Multiway Corpora Repository for Natural Language\n  Processing", "abstract": "Multilinguality is gradually becoming ubiquitous in the sense that more and\nmore researchers have successfully shown that using additional languages help\nimprove the results in many Natural Language Processing tasks. Multilingual\nMultiway Corpora (MMC) contain the same sentence in multiple languages. Such\ncorpora have been primarily used for Multi-Source and Pivot Language Machine\nTranslation but are also useful for developing multilingual sequence taggers by\ntransfer learning. While these corpora are available, they are not organized\nfor multilingual experiments and researchers need to write boilerplate code\nevery time they want to use said corpora. Moreover, because there is no\nofficial MMC collection it becomes difficult to compare against existing\napproaches. As such we present our work on creating a unified and\nsystematically organized repository of MMC spanning a large number of\nlanguages. We also provide training, development and test splits for corpora\nwhere official splits are unavailable. We hope that this will help speed up the\npace of multilingual NLP research and ensure that NLP researchers obtain\nresults that are more trustable since they can be compared easily. We indicate\ncorpora sources, extraction procedures if any and relevant statistics. We also\nmake our collection public for research purposes.", "published": "2017-10-03 08:19:24", "link": "http://arxiv.org/abs/1710.01025v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards an Inferential Lexicon of Event Selecting Predicates for French", "abstract": "We present a manually constructed seed lexicon encoding the inferential\nprofiles of French event selecting predicates across different uses. The\ninferential profile (Karttunen, 1971a) of a verb is designed to capture the\ninferences triggered by the use of this verb in context. It reflects the\ninfluence of the clause-embedding verb on the factuality of the event described\nby the embedded clause. The resource developed provides evidence for the\nfollowing three hypotheses: (i) French implicative verbs have an aspect\ndependent profile (their inferential profile varies with outer aspect), while\nfactive verbs have an aspect independent profile (they keep the same\ninferential profile with both imperfective and perfective aspect); (ii)\nimplicativity decreases with imperfective aspect: the inferences triggered by\nFrench implicative verbs combined with perfective aspect are often weakened\nwhen the same verbs are combined with imperfective aspect; (iii) implicativity\ndecreases with an animate (deep) subject: the inferences triggered by a verb\nwhich is implicative with an inanimate subject are weakened when the same verb\nis used with an animate subject. The resource additionally shows that verbs\nwith different inferential profiles display clearly distinct sub-categorisation\npatterns. In particular, verbs that have both factive and implicative readings\nare shown to prefer infinitival clauses in their implicative reading, and\ntensed clauses in their factive reading.", "published": "2017-10-03 11:48:37", "link": "http://arxiv.org/abs/1710.01095v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Lexical Choice in Neural Machine Translation", "abstract": "We explore two solutions to the problem of mistranslating rare words in\nneural machine translation. First, we argue that the standard output layer,\nwhich computes the inner product of a vector representing the context with all\npossible output word embeddings, rewards frequent words disproportionately, and\nwe propose to fix the norms of both vectors to a constant value. Second, we\nintegrate a simple lexical module which is jointly trained with the rest of the\nmodel. We evaluate our approaches on eight language pairs with data sizes\nranging from 100k to 8M words, and achieve improvements of up to +4.3 BLEU,\nsurpassing phrase-based translation in nearly all settings.", "published": "2017-10-03 18:15:10", "link": "http://arxiv.org/abs/1710.01329v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transferring Semantic Roles Using Translation and Syntactic Information", "abstract": "Our paper addresses the problem of annotation projection for semantic role\nlabeling for resource-poor languages using supervised annotations from a\nresource-rich language through parallel data. We propose a transfer method that\nemploys information from source and target syntactic dependencies as well as\nword alignment density to improve the quality of an iterative bootstrapping\nmethod. Our experiments yield a $3.5$ absolute labeled F-score improvement over\na standard annotation projection method.", "published": "2017-10-03 22:40:05", "link": "http://arxiv.org/abs/1710.01411v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Which phoneme-to-viseme maps best improve visual-only computer\n  lip-reading?", "abstract": "A critical assumption of all current visual speech recognition systems is\nthat there are visual speech units called visemes which can be mapped to units\nof acoustic speech, the phonemes. Despite there being a number of published\nmaps it is infrequent to see the effectiveness of these tested, particularly on\nvisual-only lip-reading (many works use audio-visual speech). Here we examine\n120 mappings and consider if any are stable across talkers. We show a method\nfor devising maps based on phoneme confusions from an automated lip-reading\nsystem, and we present new mappings that show improvements for individual\ntalkers.", "published": "2017-10-03 11:44:40", "link": "http://arxiv.org/abs/1710.01093v1", "categories": ["cs.CV", "cs.CL", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Finding phonemes: improving machine lip-reading", "abstract": "In machine lip-reading there is continued debate and research around the\ncorrect classes to be used for recognition. In this paper we use a structured\napproach for devising speaker-dependent viseme classes, which enables the\ncreation of a set of phoneme-to-viseme maps where each has a different quantity\nof visemes ranging from two to 45. Viseme classes are based upon the mapping of\narticulated phonemes, which have been confused during phoneme recognition, into\nviseme groups. Using these maps, with the LiLIR dataset, we show the effect of\nchanging the viseme map size in speaker-dependent machine lip-reading, measured\nby word recognition correctness and so demonstrate that word recognition with\nphoneme classifiers is not just possible, but often better than word\nrecognition with viseme classifiers. Furthermore, there are intermediate units\nbetween visemes and phonemes which are better still.", "published": "2017-10-03 13:32:40", "link": "http://arxiv.org/abs/1710.01142v1", "categories": ["cs.CV", "cs.CL", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Speaker-independent machine lip-reading with speaker-dependent viseme\n  classifiers", "abstract": "In machine lip-reading, which is identification of speech from visual-only\ninformation, there is evidence to show that visual speech is highly dependent\nupon the speaker [1]. Here, we use a phoneme-clustering method to form new\nphoneme-to-viseme maps for both individual and multiple speakers. We use these\nmaps to examine how similarly speakers talk visually. We conclude that broadly\nspeaking, speakers have the same repertoire of mouth gestures, where they\ndiffer is in the use of the gestures.", "published": "2017-10-03 13:02:41", "link": "http://arxiv.org/abs/1710.01122v1", "categories": ["cs.CV", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Decoding visemes: improving machine lipreading", "abstract": "To undertake machine lip-reading, we try to recognise speech from a visual\nsignal. Current work often uses viseme classification supported by language\nmodels with varying degrees of success. A few recent works suggest phoneme\nclassification, in the right circumstances, can outperform viseme\nclassification. In this work we present a novel two-pass method of training\nphoneme classifiers which uses previously trained visemes in the first pass.\nWith our new training algorithm, we show classification performance which\nsignificantly improves on previous lip-reading results.", "published": "2017-10-03 14:01:32", "link": "http://arxiv.org/abs/1710.01169v1", "categories": ["cs.CV", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Decoding visemes: improving machine lipreading", "abstract": "Machine lipreading (MLR) is speech recognition from visual cues and a niche\nresearch problem in speech processing & computer vision. Current challenges\nfall into two groups: the content of the video, such as rate of speech or; the\nparameters of the video recording e.g, video resolution. We show that HD video\nis not needed to successfully lipread with a computer. The term \"viseme\" is\nused in machine lipreading to represent a visual cue or gesture which\ncorresponds to a subgroup of phonemes where the phonemes are visually\nindistinguishable. A phoneme is the smallest sound one can utter, because there\nare more phonemes per viseme, maps between units show a many-to-one\nrelationship. Many maps have been presented, we compare these and our results\nshow Lee's is best. We propose a new method of speaker-dependent\nphoneme-to-viseme maps and compare these to Lee's. Our results show the\nsensitivity of phoneme clustering and we use our new knowledge to augment a\nconventional MLR system. It has been observed in MLR, that classifiers need\ntraining on test subjects to achieve accuracy. Thus machine lipreading is\nhighly speaker-dependent. Conversely speaker independence is robust\nclassification of non-training speakers. We investigate the dependence of\nphoneme-to-viseme maps between speakers and show there is not a high\nvariability of visemes, but there is high variability in trajectory between\nvisemes of individual speakers with the same ground truth. This implies a\ndependency upon the number of visemes within each set for each individual. We\nshow that prior phoneme-to-viseme maps rarely have enough visemes and the\noptimal size, which varies by speaker, ranges from 11-35. Finally we decode\nfrom visemes back to phonemes and into words. Our novel approach uses the\noptimum range visemes within hierarchical training of phoneme classifiers and\ndemonstrates a significant increase in classification accuracy.", "published": "2017-10-03 17:29:24", "link": "http://arxiv.org/abs/1710.01288v1", "categories": ["cs.CV", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Visual speech recognition: aligning terminologies for better\n  understanding", "abstract": "We are at an exciting time for machine lipreading. Traditional research\nstemmed from the adaptation of audio recognition systems. But now, the computer\nvision community is also participating. This joining of two previously\ndisparate areas with different perspectives on computer lipreading is creating\nopportunities for collaborations, but in doing so the literature is\nexperiencing challenges in knowledge sharing due to multiple uses of terms and\nphrases and the range of methods for scoring results.\n  In particular we highlight three areas with the intention to improve\ncommunication between those researching lipreading; the effects of\ninterchanging between speech reading and lipreading; speaker dependence across\ntrain, validation, and test splits; and the use of accuracy, correctness,\nerrors, and varying units (phonemes, visemes, words, and sentences) to measure\nsystem performance. We make recommendations as to how we can be more\nconsistent.", "published": "2017-10-03 17:45:32", "link": "http://arxiv.org/abs/1710.01292v1", "categories": ["cs.CV", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Visual gesture variability between talkers in continuous visual speech", "abstract": "Recent adoption of deep learning methods to the field of machine lipreading\nresearch gives us two options to pursue to improve system performance. Either,\nwe develop end-to-end systems holistically or, we experiment to further our\nunderstanding of the visual speech signal. The latter option is more difficult\nbut this knowledge would enable researchers to both improve systems and apply\nthe new knowledge to other domains such as speech therapy. One challenge in\nlipreading systems is the correct labeling of the classifiers. These labels map\nan estimated function between visemes on the lips and the phonemes uttered.\nHere we ask if such maps are speaker-dependent? Prior work investigated\nisolated word recognition from speaker-dependent (SD) visemes, we extend this\nto continuous speech. Benchmarked against SD results, and the isolated words\nperformance, we test with RMAV dataset speakers and observe that with\ncontinuous speech, the trajectory between visemes has a greater negative effect\non the speaker differentiation.", "published": "2017-10-03 17:59:43", "link": "http://arxiv.org/abs/1710.01297v1", "categories": ["cs.CV", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Understanding the visual speech signal", "abstract": "For machines to lipread, or understand speech from lip movement, they decode\nlip-motions (known as visemes) into the spoken sounds. We investigate the\nvisual speech channel to further our understanding of visemes. This has\napplications beyond machine lipreading; speech therapists, animators, and\npsychologists can benefit from this work. We explain the influence of speaker\nindividuality, and demonstrate how one can use visemes to boost lipreading.", "published": "2017-10-03 19:10:46", "link": "http://arxiv.org/abs/1710.01351v1", "categories": ["cs.CV", "eess.AS"], "primary_category": "cs.CV"}
