{"title": "SPACER: A Parallel Dataset of Speech Production And Comprehension of Error Repairs", "abstract": "Speech errors are a natural part of communication, yet they rarely lead to\ncomplete communicative failure because both speakers and comprehenders can\ndetect and correct errors. Although prior research has examined error\nmonitoring and correction in production and comprehension separately,\nintegrated investigation of both systems has been impeded by the scarcity of\nparallel data. In this study, we present SPACER, a parallel dataset that\ncaptures how naturalistic speech errors are corrected by both speakers and\ncomprehenders. We focus on single-word substitution errors extracted from the\nSwitchboard corpus, accompanied by speaker's self-repairs and comprehenders'\nresponses from an offline text-editing experiment. Our exploratory analysis\nsuggests asymmetries in error correction strategies: speakers are more likely\nto repair errors that introduce greater semantic and phonemic deviations,\nwhereas comprehenders tend to correct errors that are phonemically similar to\nmore plausible alternatives or do not fit into prior contexts. Our dataset\nenables future research on integrated approaches toward studying language\nproduction and comprehension.", "published": "2025-03-20 23:12:00", "link": "http://arxiv.org/abs/2503.16745v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CodeScientist: End-to-End Semi-Automated Scientific Discovery with Code-based Experimentation", "abstract": "Despite the surge of interest in autonomous scientific discovery (ASD) of\nsoftware artifacts (e.g., improved ML algorithms), current ASD systems face two\nkey limitations: (1) they largely explore variants of existing codebases or\nsimilarly constrained design spaces, and (2) they produce large volumes of\nresearch artifacts (such as automatically generated papers and code) that are\ntypically evaluated using conference-style paper review with limited evaluation\nof code. In this work we introduce CodeScientist, a novel ASD system that\nframes ideation and experiment construction as a form of genetic search jointly\nover combinations of research articles and codeblocks defining common actions\nin a domain (like prompting a language model). We use this paradigm to conduct\nhundreds of automated experiments on machine-generated ideas broadly in the\ndomain of agents and virtual environments, with the system returning 19\ndiscoveries, 6 of which were judged as being both at least minimally sound and\nincrementally novel after a multi-faceted evaluation beyond that typically\nconducted in prior work, including external (conference-style) review, code\nreview, and replication attempts. Moreover, the discoveries span new tasks,\nagents, metrics, and data, suggesting a qualitative shift from benchmark\noptimization to broader discoveries.", "published": "2025-03-20 22:37:17", "link": "http://arxiv.org/abs/2503.22708v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Design and Implementation of an FPGA-Based Tiled Matrix Multiplication Accelerator for Transformer Self-Attention on the Xilinx KV260 SoM", "abstract": "Transformer-based large language models (LLMs) rely heavily on intensive\nmatrix multiplications for attention and feed-forward layers, with the Q, K,\nand V linear projections in the Multi-Head Self-Attention (MHA) module\nconstituting a decisive performance bottleneck. In this work, we introduce a\nhighly optimized tiled matrix multiplication accelerator on a\nresource-constrained Xilinx KV260 FPGA that not only addresses this challenge\nbut sets a new standard for efficiency and performance. Our design exploits\npersistent on-chip storage, a robust two-level tiling strategy for maximal data\nreuse, and a systolic-like unrolled compute engine that together deliver\nunparalleled speed and energy efficiency. Integrated with DistilBERT for Q, K,\nand V projections, our accelerator achieves an unequivocal 7x speedup over ARM\nCPU implementations (PyTorch) and an extraordinary 200x improvement over naive\nNumPy, reaching a throughput of up to 3.1~GFLOPs for matrix multiplications on\n(64,768) x (768,3072) matrices while operating at a conservative 100 MHz. These\nresults decisively demonstrate the transformative potential of FPGA-based\nacceleration for critical Transformer operations, paving the way for scalable\nand energy-efficient deep learning inference on edge devices.", "published": "2025-03-20 22:15:42", "link": "http://arxiv.org/abs/2503.16731v2", "categories": ["cs.AR", "cs.CL", "cs.LG", "B.7.1; C.1.4"], "primary_category": "cs.AR"}
{"title": "Natural Language Generation", "abstract": "This article provides a brief overview of the field of Natural Language\nGeneration. The term Natural Language Generation (NLG), in its broadest\ndefinition, refers to the study of systems that verbalize some form of\ninformation through natural language. That information could be stored in a\nlarge database or knowledge graph (in data-to-text applications), but NLG\nresearchers may also study summarisation (text-to-text) or image captioning\n(image-to-text), for example. As a subfield of Natural Language Processing, NLG\nis closely related to other sub-disciplines such as Machine Translation (MT)\nand Dialog Systems. Some NLG researchers exclude MT from their definition of\nthe field, since there is no content selection involved where the system has to\ndetermine what to say. Conversely, dialog systems do not typically fall under\nthe header of Natural Language Generation since NLG is just one component of\ndialog systems (the others being Natural Language Understanding and Dialog\nManagement). However, with the rise of Large Language Models (LLMs), different\nsubfields of Natural Language Processing have converged on similar\nmethodologies for the production of natural language and the evaluation of\nautomatically generated text.", "published": "2025-03-20 22:12:08", "link": "http://arxiv.org/abs/2503.16728v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CAARMA: Class Augmentation with Adversarial Mixup Regularization", "abstract": "Speaker verification is a typical zero-shot learning task, where inference of\nunseen classes is performed by comparing embeddings of test instances to known\nexamples. The models performing inference must hence naturally generate\nembeddings that cluster same-class instances compactly, while maintaining\nseparation across classes. In order to learn to do so, they are typically\ntrained on a large number of classes (speakers), often using specialized\nlosses. However real-world speaker datasets often lack the class diversity\nneeded to effectively learn this in a generalizable manner. We introduce\nCAARMA, a class augmentation framework that addresses this problem by\ngenerating synthetic classes through data mixing in the embedding space,\nexpanding the number of training classes. To ensure the authenticity of the\nsynthetic classes we adopt a novel adversarial refinement mechanism that\nminimizes categorical distinctions between synthetic and real classes. We\nevaluate CAARMA on multiple speaker verification tasks, as well as other\nrepresentative zero-shot comparison-based speech analysis tasks and obtain\nconsistent improvements: our framework demonstrates a significant improvement\nof 8\\% over all baseline models. Code for CAARMA will be released.", "published": "2025-03-20 21:41:16", "link": "http://arxiv.org/abs/2503.16718v1", "categories": ["cs.SD", "cs.CL", "cs.LG"], "primary_category": "cs.SD"}
{"title": "WaveFM: A High-Fidelity and Efficient Vocoder Based on Flow Matching", "abstract": "Flow matching offers a robust and stable approach to training diffusion\nmodels. However, directly applying flow matching to neural vocoders can result\nin subpar audio quality. In this work, we present WaveFM, a reparameterized\nflow matching model for mel-spectrogram conditioned speech synthesis, designed\nto enhance both sample quality and generation speed for diffusion vocoders.\nSince mel-spectrograms represent the energy distribution of waveforms, WaveFM\nadopts a mel-conditioned prior distribution instead of a standard Gaussian\nprior to minimize unnecessary transportation costs during synthesis. Moreover,\nwhile most diffusion vocoders rely on a single loss function, we argue that\nincorporating auxiliary losses, including a refined multi-resolution STFT loss,\ncan further improve audio quality. To speed up inference without degrading\nsample quality significantly, we introduce a tailored consistency distillation\nmethod for WaveFM. Experiment results demonstrate that our model achieves\nsuperior performance in both quality and efficiency compared to previous\ndiffusion vocoders, while enabling waveform generation in a single inference\nstep.", "published": "2025-03-20 20:17:17", "link": "http://arxiv.org/abs/2503.16689v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Through the LLM Looking Glass: A Socratic Self-Assessment of Donkeys, Elephants, and Markets", "abstract": "While detecting and avoiding bias in LLM-generated text is becoming\nincreasingly important, media bias often remains subtle and subjective, making\nit particularly difficult to identify and mitigate. In this study, we assess\nmedia bias in LLM-generated content and LLMs' ability to detect subtle\nideological bias. We conduct this evaluation using two datasets, PoliGen and\nEconoLex, covering political and economic discourse, respectively. We evaluate\neight widely used LLMs by prompting them to generate articles and analyze their\nideological preferences via self-assessment. By using self-assessment, the\nstudy aims to directly measure the models' biases rather than relying on\nexternal interpretations, thereby minimizing subjective judgments about media\nbias. Our results reveal a consistent preference of Democratic over Republican\npositions across all models. Conversely, in economic topics, biases vary among\nWestern LLMs, while those developed in China lean more strongly toward\nsocialism.", "published": "2025-03-20 19:40:40", "link": "http://arxiv.org/abs/2503.16674v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Accelerating Antibiotic Discovery with Large Language Models and Knowledge Graphs", "abstract": "The discovery of novel antibiotics is critical to address the growing\nantimicrobial resistance (AMR). However, pharmaceutical industries face high\ncosts (over $1 billion), long timelines, and a high failure rate, worsened by\nthe rediscovery of known compounds. We propose an LLM-based pipeline that acts\nas an alarm system, detecting prior evidence of antibiotic activity to prevent\ncostly rediscoveries. The system integrates organism and chemical literature\ninto a Knowledge Graph (KG), ensuring taxonomic resolution, synonym handling,\nand multi-level evidence classification. We tested the pipeline on a private\nlist of 73 potential antibiotic-producing organisms, disclosing 12 negative\nhits for evaluation. The results highlight the effectiveness of the pipeline\nfor evidence reviewing, reducing false negatives, and accelerating\ndecision-making. The KG for negative hits and the user interface for\ninteractive exploration will be made publicly available.", "published": "2025-03-20 19:12:32", "link": "http://arxiv.org/abs/2503.16655v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models for Explainable Activity Recognition in Smart Homes: A Critical Evaluation", "abstract": "Explainable Artificial Intelligence (XAI) aims to uncover the inner reasoning\nof machine learning models. In IoT systems, XAI improves the transparency of\nmodels processing sensor data from multiple heterogeneous devices, ensuring\nend-users understand and trust their outputs. Among the many applications, XAI\nhas also been applied to sensor-based Activities of Daily Living (ADLs)\nrecognition in smart homes. Existing approaches highlight which sensor events\nare most important for each predicted activity, using simple rules to convert\nthese events into natural language explanations for non-expert users. However,\nthese methods produce rigid explanations lacking natural language flexibility\nand are not scalable. With the recent rise of Large Language Models (LLMs), it\nis worth exploring whether they can enhance explanation generation, considering\ntheir proven knowledge of human activities. This paper investigates potential\napproaches to combine XAI and LLMs for sensor-based ADL recognition. We\nevaluate if LLMs can be used: a) as explainable zero-shot ADL recognition\nmodels, avoiding costly labeled data collection, and b) to automate the\ngeneration of explanations for existing data-driven XAI approaches when\ntraining data is available and the goal is higher recognition rates. Our\ncritical evaluation provides insights into the benefits and challenges of using\nLLMs for explainable ADL recognition.", "published": "2025-03-20 18:23:03", "link": "http://arxiv.org/abs/2503.16622v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Classification of User Reports for Detection of Faulty Computer Components using NLP Models: A Case Study", "abstract": "Computer manufacturers typically offer platforms for users to report faults.\nHowever, there remains a significant gap in these platforms' ability to\neffectively utilize textual reports, which impedes users from describing their\nissues in their own words. In this context, Natural Language Processing (NLP)\noffers a promising solution, by enabling the analysis of user-generated text.\nThis paper presents an innovative approach that employs NLP models to classify\nuser reports for detecting faulty computer components, such as CPU, memory,\nmotherboard, video card, and more. In this work, we build a dataset of 341 user\nreports obtained from many sources. Additionally, through extensive\nexperimental evaluation, our approach achieved an accuracy of 79% with our\ndataset.", "published": "2025-03-20 18:11:26", "link": "http://arxiv.org/abs/2503.16614v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "XAttention: Block Sparse Attention with Antidiagonal Scoring", "abstract": "Long-Context Transformer Models (LCTMs) are vital for real-world applications\nbut suffer high computational costs due to attention's quadratic complexity.\nBlock-sparse attention mitigates this by focusing computation on critical\nregions, yet existing methods struggle with balancing accuracy and efficiency\ndue to costly block importance measurements. In this paper, we introduce\nXAttention, a plug-and-play framework that dramatically accelerates\nlong-context inference in Transformers models using sparse attention.\nXAttention's key innovation is the insight that the sum of antidiagonal values\n(i.e., from the lower-left to upper-right) in the attention matrix provides a\npowerful proxy for block importance. This allows for precise identification and\npruning of non-essential blocks, resulting in high sparsity and dramatically\naccelerated inference. Across comprehensive evaluations on demanding\nlong-context benchmarks-including RULER and LongBench for language, VideoMME\nfor video understanding, and VBench for video generation. XAttention achieves\naccuracy comparable to full attention while delivering substantial\ncomputational gains. We demonstrate up to 13.5x acceleration in attention\ncomputation. These results underscore XAttention's ability to unlock the\npractical potential of block sparse attention, paving the way for scalable and\nefficient deployment of LCTMs in real-world applications. Code is available at\nhttps://github.com/mit-han-lab/x-attention.", "published": "2025-03-20 17:59:58", "link": "http://arxiv.org/abs/2503.16428v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking.", "published": "2025-03-20 17:59:38", "link": "http://arxiv.org/abs/2503.16419v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Survey on Evaluation of LLM-based Agents", "abstract": "The emergence of LLM-based agents represents a paradigm shift in AI, enabling\nautonomous systems to plan, reason, use tools, and maintain memory while\ninteracting with dynamic environments. This paper provides the first\ncomprehensive survey of evaluation methodologies for these increasingly capable\nagents. We systematically analyze evaluation benchmarks and frameworks across\nfour critical dimensions: (1) fundamental agent capabilities, including\nplanning, tool use, self-reflection, and memory; (2) application-specific\nbenchmarks for web, software engineering, scientific, and conversational\nagents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating\nagents. Our analysis reveals emerging trends, including a shift toward more\nrealistic, challenging evaluations with continuously updated benchmarks. We\nalso identify critical gaps that future research must address-particularly in\nassessing cost-efficiency, safety, and robustness, and in developing\nfine-grained, and scalable evaluation methods. This survey maps the rapidly\nevolving landscape of agent evaluation, reveals the emerging trends in the\nfield, identifies current limitations, and proposes directions for future\nresearch.", "published": "2025-03-20 17:59:23", "link": "http://arxiv.org/abs/2503.16416v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "The Emperor's New Clothes in Benchmarking? A Rigorous Examination of Mitigation Strategies for LLM Benchmark Data Contamination", "abstract": "Benchmark Data Contamination (BDC)-the inclusion of benchmark testing samples\nin the training set-has raised increasing concerns in Large Language Model\n(LLM) evaluation, leading to falsely inflated performance estimates and\nundermining evaluation reliability. To address this, researchers have proposed\nvarious mitigation strategies to update existing benchmarks, including\nmodifying original questions or generating new ones based on them. However, a\nrigorous examination of the effectiveness of these mitigation strategies\nremains lacking. In this paper, we design a systematic and controlled pipeline\nalong with two novel metrics-fidelity and contamination resistance-to provide a\nfine-grained and comprehensive assessment of existing BDC mitigation\nstrategies. Previous assessment methods, such as accuracy drop and accuracy\nmatching, focus solely on aggregate accuracy, often leading to incomplete or\nmisleading conclusions. Our metrics address this limitation by emphasizing\nquestion-level evaluation result matching. Extensive experiments with 10 LLMs,\n5 benchmarks, 20 BDC mitigation strategies, and 2 contamination scenarios\nreveal that no existing strategy significantly improves resistance over the\nvanilla case (i.e., no benchmark update) across all benchmarks, and none\neffectively balances fidelity and contamination resistance. These findings\nunderscore the urgent need for designing more effective BDC mitigation\nstrategies. Our code repository is available at\nhttps://github.com/ASTRAL-Group/BDC_mitigation_assessment.", "published": "2025-03-20 17:55:04", "link": "http://arxiv.org/abs/2503.16402v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Do Visual Imaginations Improve Vision-and-Language Navigation Agents?", "abstract": "Vision-and-Language Navigation (VLN) agents are tasked with navigating an\nunseen environment using natural language instructions. In this work, we study\nif visual representations of sub-goals implied by the instructions can serve as\nnavigational cues and lead to increased navigation performance. To synthesize\nthese visual representations or imaginations, we leverage a text-to-image\ndiffusion model on landmark references contained in segmented instructions.\nThese imaginations are provided to VLN agents as an added modality to act as\nlandmark cues and an auxiliary loss is added to explicitly encourage relating\nthese with their corresponding referring expressions. Our findings reveal an\nincrease in success rate (SR) of around 1 point and up to 0.5 points in success\nscaled by inverse path length (SPL) across agents. These results suggest that\nthe proposed approach reinforces visual understanding compared to relying on\nlanguage instructions alone. Code and data for our work can be found at\nhttps://www.akhilperincherry.com/VLN-Imagine-website/.", "published": "2025-03-20 17:53:12", "link": "http://arxiv.org/abs/2503.16394v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners", "abstract": "Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they struggle to generalize these updates to multi-hop\nreasoning tasks that depend on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we observe that current layer-localized KE approaches, such as MEMIT\nand WISE, which edit only single or a few model layers, struggle to effectively\nincorporate updated information into these reasoning pathways. To address this\nlimitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method\nthat enables more effective integration of updated knowledge in LLMs. CaKE\nleverages strategically curated data, guided by our circuits-based analysis,\nthat enforces the model to utilize the modified knowledge, stimulating the\nmodel to develop appropriate reasoning circuits for newly integrated knowledge.\nExperimental results show that CaKE enables more accurate and consistent use of\nupdated knowledge across related reasoning tasks, leading to an average of 20%\nimprovement in multi-hop reasoning accuracy on MQuAKE dataset compared to\nexisting KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE.", "published": "2025-03-20 17:14:34", "link": "http://arxiv.org/abs/2503.16356v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Survey on Long Context Language Modeling", "abstract": "Efficient processing of long contexts has been a persistent pursuit in\nNatural Language Processing. With the growing number of long documents,\ndialogues, and other textual data, it is important to develop Long Context\nLanguage Models (LCLMs) that can process and analyze extensive inputs in an\neffective and efficient way. In this paper, we present a comprehensive survey\non recent advances in long-context modeling for large language models. Our\nsurvey is structured around three key aspects: how to obtain effective and\nefficient LCLMs, how to train and deploy LCLMs efficiently, and how to evaluate\nand analyze LCLMs comprehensively. For the first aspect, we discuss data\nstrategies, architectural designs, and workflow approaches oriented with long\ncontext processing. For the second aspect, we provide a detailed examination of\nthe infrastructure required for LCLM training and inference. For the third\naspect, we present evaluation paradigms for long-context comprehension and\nlong-form generation, as well as behavioral analysis and mechanism\ninterpretability of LCLMs. Beyond these three key aspects, we thoroughly\nexplore the diverse application scenarios where existing LCLMs have been\ndeployed and outline promising future development directions. This survey\nprovides an up-to-date review of the literature on long-context LLMs, which we\nwish to serve as a valuable resource for both researchers and engineers. An\nassociated GitHub repository collecting the latest papers and repos is\navailable at:\n\\href{https://github.com/LCLM-Horizon/A-Comprehensive-Survey-For-Long-Context-Language-Modeling}{\\color[RGB]{175,36,67}{LCLM-Horizon}}.", "published": "2025-03-20 17:06:28", "link": "http://arxiv.org/abs/2503.17407v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates", "abstract": "Recent findings reveal that much of the knowledge in a Transformer-based\nLarge Language Model (LLM) is encoded in its feed-forward (FFN) layers, where\neach FNN layer can be interpreted as the summation of sub-updates, each\ncorresponding to a weighted column vector from the FFN's value parameter matrix\nthat often encodes human-interpretable concepts. In light of this, we\nhypothesize that model performance and behaviors can be further enhanced and\ncontrolled by modulating the contributions of these sub-updates based on their\nrelevance to the input or target output style, and propose LLMBRACES, a novel\nand efficient method that computes relevance scores associated with value\nvectors in FFN layers and leverages these scores to dynamically adjust the\ncontribution of sub-updates. By optimizing sub-update contributions, LLMBRACES\nrefines the prediction process, leading to more accurate and reliable outputs,\nmuch like a 'brace' providing support and stability. Moreover, LLMBRACES can be\nextended to support conditional control over generation characteristics, such\nas sentiment, thereby offering fine-grained steering of LLM outputs. Extensive\nexperiments on various LLMs-including Qwen2.5-1.5B, Llama2-7B, and\nLlama3-8B-demonstrate that LLMBRACES outperforms baseline approaches in both\nfine-tuning and zero-shot settings while requiring significantly fewer tunable\nparameters, up to 75% fewer compared to LoRA. Furthermore, LLMBRACES excels in\nsentiment-controlled generation and toxicity reduction, highlighting its\npotential for flexible, controlled text generation across applications.", "published": "2025-03-20 16:55:26", "link": "http://arxiv.org/abs/2503.16334v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Big Help or Big Brother? Auditing Tracking, Profiling, and Personalization in Generative AI Assistants", "abstract": "Generative AI (GenAI) browser assistants integrate powerful capabilities of\nGenAI in web browsers to provide rich experiences such as question answering,\ncontent summarization, and agentic navigation. These assistants, available\ntoday as browser extensions, can not only track detailed browsing activity such\nas search and click data, but can also autonomously perform tasks such as\nfilling forms, raising significant privacy concerns. It is crucial to\nunderstand the design and operation of GenAI browser extensions, including how\nthey collect, store, process, and share user data. To this end, we study their\nability to profile users and personalize their responses based on explicit or\ninferred demographic attributes and interests of users. We perform network\ntraffic analysis and use a novel prompting framework to audit tracking,\nprofiling, and personalization by the ten most popular GenAI browser assistant\nextensions. We find that instead of relying on local in-browser models, these\nassistants largely depend on server-side APIs, which can be auto-invoked\nwithout explicit user interaction. When invoked, they collect and share webpage\ncontent, often the full HTML DOM and sometimes even the user's form inputs,\nwith their first-party servers. Some assistants also share identifiers and user\nprompts with third-party trackers such as Google Analytics. The collection and\nsharing continues even if a webpage contains sensitive information such as\nhealth or personal information such as name or SSN entered in a web form. We\nfind that several GenAI browser assistants infer demographic attributes such as\nage, gender, income, and interests and use this profile--which carries across\nbrowsing contexts--to personalize responses. In summary, our work shows that\nGenAI browser assistants can and do collect personal and sensitive information\nfor profiling and personalization with little to no safeguards.", "published": "2025-03-20 16:21:47", "link": "http://arxiv.org/abs/2503.16586v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CR", "cs.CY", "I.2; I.2.1; I.2.7; H.3.4; K.4; K.4.1; H.1; H.1.2; H.5.2; H.4.3"], "primary_category": "cs.HC"}
{"title": "Fin-R1: A Large Language Model for Financial Reasoning through Reinforcement Learning", "abstract": "Reasoning large language models are rapidly evolving across various domains.\nHowever, their capabilities in handling complex financial tasks still require\nin-depth exploration. In this paper, we introduce Fin-R1, a reasoning large\nlanguage model specifically designed for the financial sector. Fin-R1 is built\nusing a two-stage architecture, leveraging a financial reasoning dataset\ndistilled and processed based on DeepSeek-R1. Through supervised fine-tuning\n(SFT) and reinforcement learning (RL) training, it demonstrates performance\nclose to DeepSeek-R1 with a parameter size of 7 billion across a range of\nfinancial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA\nand ConvFinQA tasks between those LLMs in our evaluation, surpassing larger\nmodels in other tasks as well. Fin-R1 showcases strong reasoning and\ndecision-making capabilities, providing solutions to various problems\nencountered in the financial domain. Our code is available at\nhttps://github.com/SUFE-AIFLM-Lab/Fin-R1.", "published": "2025-03-20 15:46:18", "link": "http://arxiv.org/abs/2503.16252v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distributed LLMs and Multimodal Large Language Models: A Survey on Advances, Challenges, and Future Directions", "abstract": "Language models (LMs) are machine learning models designed to predict\nlinguistic patterns by estimating the probability of word sequences based on\nlarge-scale datasets, such as text. LMs have a wide range of applications in\nnatural language processing (NLP) tasks, including autocomplete and machine\ntranslation. Although larger datasets typically enhance LM performance,\nscalability remains a challenge due to constraints in computational power and\nresources. Distributed computing strategies offer essential solutions for\nimproving scalability and managing the growing computational demand. Further,\nthe use of sensitive datasets in training and deployment raises significant\nprivacy concerns. Recent research has focused on developing decentralized\ntechniques to enable distributed training and inference while utilizing diverse\ncomputational resources and enabling edge AI. This paper presents a survey on\ndistributed solutions for various LMs, including large language models (LLMs),\nvision language models (VLMs), multimodal LLMs (MLLMs), and small language\nmodels (SLMs). While LLMs focus on processing and generating text, MLLMs are\ndesigned to handle multiple modalities of data (e.g., text, images, and audio)\nand to integrate them for broader applications. To this end, this paper reviews\nkey advancements across the MLLM pipeline, including distributed training,\ninference, fine-tuning, and deployment, while also identifying the\ncontributions, limitations, and future areas of improvement. Further, it\ncategorizes the literature based on six primary focus areas of\ndecentralization. Our analysis describes gaps in current methodologies for\nenabling distributed solutions for LMs and outline future research directions,\nemphasizing the need for novel solutions to enhance the robustness and\napplicability of distributed LMs.", "published": "2025-03-20 15:18:25", "link": "http://arxiv.org/abs/2503.16585v1", "categories": ["cs.CL", "cs.CV", "cs.DC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn't", "abstract": "Enhancing the reasoning capabilities of large language models (LLMs)\ntypically relies on massive computational resources and extensive datasets,\nlimiting accessibility for resource-constrained settings. Our study\ninvestigates the potential of reinforcement learning (RL) to improve reasoning\nin small LLMs, focusing on a 1.5-billion-parameter model,\nDeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA\nA40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy\nOptimization (GRPO) algorithm and curating a compact, high-quality mathematical\nreasoning dataset, we conducted three experiments to explore model behavior and\nperformance. Our results demonstrate rapid reasoning gains - e.g., AMC23\naccuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing\no1-preview - using only 7,000 samples and a $42 training cost, compared to\nthousands of dollars for baseline models. However, challenges such as\noptimization instability and length constraints emerged with prolonged\ntraining. These findings highlight the efficacy of RL-based fine-tuning for\nsmall LLMs, offering a cost-effective alternative to large-scale approaches. We\nrelease our code and datasets as open-source resources, providing insights into\ntrade-offs and laying a foundation for scalable, reasoning-capable LLMs in\nresource-limited environments. All are available at\nhttps://github.com/knoveleng/open-rs.", "published": "2025-03-20 15:13:23", "link": "http://arxiv.org/abs/2503.16219v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MathFusion: Enhancing Mathematic Problem-solving of LLM through Instruction Fusion", "abstract": "Large Language Models (LLMs) have shown impressive progress in mathematical\nreasoning. While data augmentation is promising to enhance mathematical\nproblem-solving ability, current approaches are predominantly limited to\ninstance-level modifications-such as rephrasing or generating syntactic\nvariations-which fail to capture and leverage the intrinsic relational\nstructures inherent in mathematical knowledge. Inspired by human learning\nprocesses, where mathematical proficiency develops through systematic exposure\nto interconnected concepts, we introduce MathFusion, a novel framework that\nenhances mathematical reasoning through cross-problem instruction synthesis.\nMathFusion implements this through three fusion strategies: (1) sequential\nfusion, which chains related problems to model solution dependencies; (2)\nparallel fusion, which combines analogous problems to reinforce conceptual\nunderstanding; and (3) conditional fusion, which creates context-aware\nselective problems to enhance reasoning flexibility. By applying these\nstrategies, we generate a new dataset, \\textbf{MathFusionQA}, followed by\nfine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental\nresults demonstrate that MathFusion achieves substantial improvements in\nmathematical reasoning while maintaining high data efficiency, boosting\nperformance by 18.0 points in accuracy across diverse benchmarks while\nrequiring only 45K additional synthetic instructions, representing a\nsubstantial improvement over traditional single-instruction approaches. Our\ndatasets, models, and code are publicly available at\nhttps://github.com/QizhiPei/mathfusion.", "published": "2025-03-20 15:00:41", "link": "http://arxiv.org/abs/2503.16212v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Accurate Scene Text Recognition with Efficient Model Scaling and Cloze Self-Distillation", "abstract": "Scaling architectures have been proven effective for improving Scene Text\nRecognition (STR), but the individual contribution of vision encoder and text\ndecoder scaling remain under-explored. In this work, we present an in-depth\nempirical analysis and demonstrate that, contrary to previous observations,\nscaling the decoder yields significant performance gains, always exceeding\nthose achieved by encoder scaling alone. We also identify label noise as a key\nchallenge in STR, particularly in real-world data, which can limit the\neffectiveness of STR models. To address this, we propose Cloze\nSelf-Distillation (CSD), a method that mitigates label noise by distilling a\nstudent model from context-aware soft predictions and pseudolabels generated by\na teacher model. Additionally, we enhance the decoder architecture by\nintroducing differential cross-attention for STR. Our methodology achieves\nstate-of-the-art performance on 10 out of 11 benchmarks using only real data,\nwhile significantly reducing the parameter size and computational costs.", "published": "2025-03-20 14:35:46", "link": "http://arxiv.org/abs/2503.16184v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CodeReviewQA: The Code Review Comprehension Assessment for Large Language Models", "abstract": "State-of-the-art large language models (LLMs) have demonstrated impressive\ncode generation capabilities but struggle with real-world software engineering\ntasks, such as revising source code to address code reviews, hindering their\npractical use. Code review comments are often implicit, ambiguous, and\ncolloquial, requiring models to grasp both code and human intent. This\nchallenge calls for evaluating large language models' ability to bridge both\ntechnical and conversational contexts. While existing work has employed the\nautomated code refinement (ACR) task to resolve these comments, current\nevaluation methods fall short, relying on text matching metrics that provide\nlimited insight into model failures and remain susceptible to training data\ncontamination. To address these limitations, we introduce a novel evaluation\nbenchmark, $\\textbf{CodeReviewQA}$ that enables us to conduct fine-grained\nassessment of model capabilities and mitigate data contamination risks. In\nCodeReviewQA, we decompose the generation task of code refinement into\n$\\textbf{three essential reasoning steps}$: $\\textit{change type recognition}$\n(CTR), $\\textit{change localisation}$ (CL), and $\\textit{solution\nidentification}$ (SI). Each step is reformulated as multiple-choice questions\nwith varied difficulty levels, enabling precise assessment of model\ncapabilities, while mitigating data contamination risks. Our comprehensive\nevaluation spans 72 recently released large language models on $\\textbf{900\nmanually curated, high-quality examples}$ across nine programming languages.\nOur results show that CodeReviewQA is able to expose specific model weaknesses\nin code review comprehension, disentangled from their generative automated code\nrefinement results.", "published": "2025-03-20 14:07:31", "link": "http://arxiv.org/abs/2503.16167v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs", "abstract": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio.", "published": "2025-03-20 14:01:56", "link": "http://arxiv.org/abs/2503.16163v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Lighter and Robust Evaluation for Retrieval Augmented Generation", "abstract": "Large Language Models are prompting us to view more NLP tasks from a\ngenerative perspective. At the same time, they offer a new way of accessing\ninformation, mainly through the RAG framework. While there have been notable\nimprovements for the autoregressive models, overcoming hallucination in the\ngenerated answers remains a continuous problem. A standard solution is to use\ncommercial LLMs, such as GPT4, to evaluate these algorithms. However, such\nframeworks are expensive and not very transparent. Therefore, we propose a\nstudy which demonstrates the interest of open-weight models for evaluating RAG\nhallucination. We develop a lightweight approach using smaller, quantized LLMs\nto provide an accessible and interpretable metric that gives continuous scores\nfor the generated answer with respect to their correctness and faithfulness.\nThis score allows us to question decisions' reliability and explore thresholds\nto develop a new AUC metric as an alternative to correlation with human\njudgment.", "published": "2025-03-20 13:58:32", "link": "http://arxiv.org/abs/2503.16161v1", "categories": ["cs.CL", "cs.AI", "62-08", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Automatically Generating Chinese Homophone Words to Probe Machine Translation Estimation Systems", "abstract": "Evaluating machine translation (MT) of user-generated content (UGC) involves\nunique challenges such as checking whether the nuance of emotions from the\nsource are preserved in the target text. Recent studies have proposed\nemotion-related datasets, frameworks and models to automatically evaluate MT\nquality of Chinese UGC, without relying on reference translations. However,\nwhether these models are robust to the challenge of preserving emotional\nnuances has been left largely unexplored. To address this gap, we introduce a\nnovel method inspired by information theory which generates challenging Chinese\nhomophone words related to emotions, by leveraging the concept of\nself-information. Our approach generates homophones that were observed to cause\ntranslation errors in emotion preservation, and exposes vulnerabilities in MT\nsystems and their evaluation methods when tackling emotional UGC. We evaluate\nthe efficacy of our method using human evaluation for the quality of these\ngenerated homophones, and compare it with an existing one, showing that our\nmethod achieves higher correlation with human judgments. The generated Chinese\nhomophones, along with their manual translations, are utilized to generate\nperturbations and to probe the robustness of existing quality evaluation\nmodels, including models trained using multi-task learning, fine-tuned variants\nof multilingual language models, as well as large language models (LLMs). Our\nresults indicate that LLMs with larger size exhibit higher stability and\nrobustness to such perturbations. We release our data and code for\nreproducibility and further research.", "published": "2025-03-20 13:56:15", "link": "http://arxiv.org/abs/2503.16158v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Only a Little to the Left: A Theory-grounded Measure of Political Bias in Large Language Models", "abstract": "Prompt-based language models like GPT4 and LLaMa have been used for a wide\nvariety of use cases such as simulating agents, searching for information, or\nfor content analysis. For all of these applications and others, political\nbiases in these models can affect their performance. Several researchers have\nattempted to study political bias in language models using evaluation suites\nbased on surveys, such as the Political Compass Test (PCT), often finding a\nparticular leaning favored by these models. However, there is some variation in\nthe exact prompting techniques, leading to diverging findings and most research\nrelies on constrained-answer settings to extract model responses. Moreover, the\nPolitical Compass Test is not a scientifically valid survey instrument. In this\nwork, we contribute a political bias measured informed by political science\ntheory, building on survey design principles to test a wide variety of input\nprompts, while taking into account prompt sensitivity. We then prompt 11\ndifferent open and commercial models, differentiating between instruction-tuned\nand non-instruction-tuned models, and automatically classify their political\nstances from 88,110 responses. Leveraging this dataset, we compute political\nbias profiles across different prompt variations and find that while PCT\nexaggerates bias in certain models like GPT3.5, measures of political bias are\noften unstable, but generally more left-leaning for instruction-tuned models.", "published": "2025-03-20 13:51:06", "link": "http://arxiv.org/abs/2503.16148v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Investigating Retrieval-Augmented Generation in Quranic Studies: A Study of 13 Open-Source Large Language Models", "abstract": "Accurate and contextually faithful responses are critical when applying large\nlanguage models (LLMs) to sensitive and domain-specific tasks, such as\nanswering queries related to quranic studies. General-purpose LLMs often\nstruggle with hallucinations, where generated responses deviate from\nauthoritative sources, raising concerns about their reliability in religious\ncontexts. This challenge highlights the need for systems that can integrate\ndomain-specific knowledge while maintaining response accuracy, relevance, and\nfaithfulness. In this study, we investigate 13 open-source LLMs categorized\ninto large (e.g., Llama3:70b, Gemma2:27b, QwQ:32b), medium (e.g., Gemma2:9b,\nLlama3:8b), and small (e.g., Llama3.2:3b, Phi3:3.8b). A Retrieval-Augmented\nGeneration (RAG) is used to make up for the problems that come with using\nseparate models. This research utilizes a descriptive dataset of Quranic surahs\nincluding the meanings, historical context, and qualities of the 114 surahs,\nallowing the model to gather relevant knowledge before responding. The models\nare evaluated using three key metrics set by human evaluators: context\nrelevance, answer faithfulness, and answer relevance. The findings reveal that\nlarge models consistently outperform smaller models in capturing query\nsemantics and producing accurate, contextually grounded responses. The\nLlama3.2:3b model, even though it is considered small, does very well on\nfaithfulness (4.619) and relevance (4.857), showing the promise of smaller\narchitectures that have been well optimized. This article examines the\ntrade-offs between model size, computational efficiency, and response quality\nwhile using LLMs in domain-specific applications.", "published": "2025-03-20 13:26:30", "link": "http://arxiv.org/abs/2503.16581v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for Multilingual Medical Question Answering", "abstract": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 35.03% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds.", "published": "2025-03-20 13:25:03", "link": "http://arxiv.org/abs/2503.16131v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cultural Alignment in Large Language Models Using Soft Prompt Tuning", "abstract": "Large Language Model (LLM) alignment conventionally relies on supervised\nfine-tuning or reinforcement learning based alignment frameworks. These methods\ntypically require labeled or preference datasets and involve updating model\nweights to align the LLM with the training objective or reward model.\nMeanwhile, in social sciences such as cross-cultural studies, factor analysis\nis widely used to uncover underlying dimensions or latent variables that\nexplain observed patterns in survey data. The non-differentiable nature of\nthese measurements deriving from survey data renders the former alignment\nmethods infeasible for alignment with cultural dimensions. To overcome this, we\npropose a parameter efficient strategy that combines soft prompt tuning, which\nfreezes the model parameters while modifying the input prompt embeddings, with\nDifferential Evolution (DE), a black-box optimization method for cases where a\ndifferentiable objective is unattainable. This strategy ensures alignment\nconsistency without the need for preference data or model parameter updates,\nsignificantly enhancing efficiency and mitigating overfitting. Our method\ndemonstrates significant improvements in LLama-3-8B-Instruct's cultural\ndimensions across multiple regions, outperforming both the Naive LLM and the\nIn-context Learning (ICL) baseline, and effectively bridges computational\nmodels with human cultural nuances.", "published": "2025-03-20 12:34:01", "link": "http://arxiv.org/abs/2503.16094v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Redefining Toxicity: An Objective and Context-Aware Approach for Stress-Level-Based Detection", "abstract": "The fundamental problem of toxicity detection lies in the fact that the term\n\"toxicity\" is ill-defined. Such uncertainty causes researchers to rely on\nsubjective and vague data during model training, which leads to non-robust and\ninaccurate results, following the 'garbage in - garbage out' paradigm. This\nstudy introduces a novel, objective, and context-aware framework for toxicity\ndetection, leveraging stress levels as a key determinant of toxicity. We\npropose new definition, metric and training approach as a parts of our\nframework and demonstrate it's effectiveness using a dataset we collected.", "published": "2025-03-20 12:09:01", "link": "http://arxiv.org/abs/2503.16072v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Tuning LLMs by RAG Principles: Towards LLM-native Memory", "abstract": "Memory, additional information beyond the training of large language models\n(LLMs), is crucial to various real-world applications, such as personal\nassistant. The two mainstream solutions to incorporate memory into the\ngeneration process are long-context LLMs and retrieval-augmented generation\n(RAG). In this paper, we first systematically compare these two types of\nsolutions on three renovated/new datasets and show that (1) long-context\nsolutions, although more expensive, shall be easier to capture the big picture\nand better answer queries which require considering the memory as a whole; and\n(2) when the queries concern specific information, RAG solutions shall be more\ncompetitive especially when the keywords can be explicitly matched. Therefore,\nwe propose a novel method RAG-Tuned-LLM which fine-tunes a relative small\n(e.g., 7B) LLM using the data generated following the RAG principles, so it can\ncombine the advantages of both solutions. Extensive experiments on three\ndatasets demonstrate that RAG-Tuned-LLM can beat long-context LLMs and RAG\nmethods across a wide range of query types.", "published": "2025-03-20 12:04:40", "link": "http://arxiv.org/abs/2503.16071v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Two-stage Incomplete Utterance Rewriting on Editing Operation", "abstract": "Previous work on Incomplete Utterance Rewriting (IUR) has primarily focused\non generating rewritten utterances based solely on dialogue context, ignoring\nthe widespread phenomenon of coreference and ellipsis in dialogues. To address\nthis issue, we propose a novel framework called TEO (\\emph{Two-stage approach\non Editing Operation}) for IUR, in which the first stage generates editing\noperations and the second stage rewrites incomplete utterances utilizing the\ngenerated editing operations and the dialogue context. Furthermore, an\nadversarial perturbation strategy is proposed to mitigate cascading errors and\nexposure bias caused by the inconsistency between training and inference in the\nsecond stage. Experimental results on three IUR datasets show that our TEO\noutperforms the SOTA models significantly.", "published": "2025-03-20 11:56:14", "link": "http://arxiv.org/abs/2503.16063v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Meta-Learning Neural Mechanisms rather than Bayesian Priors", "abstract": "Children acquire language despite being exposed to several orders of\nmagnitude less data than large language models require. Meta-learning has been\nproposed as a way to integrate human-like learning biases into neural-network\narchitectures, combining both the structured generalizations of symbolic models\nwith the scalability of neural-network models. But what does meta-learning\nexactly imbue the model with? We investigate the meta-learning of formal\nlanguages and find that, contrary to previous claims, meta-trained models are\nnot learning simplicity-based priors when meta-trained on datasets organised\naround simplicity. Rather, we find evidence that meta-training imprints neural\nmechanisms (such as counters) into the model, which function like cognitive\nprimitives for the network on downstream tasks. Most surprisingly, we find that\nmeta-training on a single formal language can provide as much improvement to a\nmodel as meta-training on 5000 different formal languages, provided that the\nformal language incentivizes the learning of useful neural mechanisms. Taken\ntogether, our findings provide practical implications for efficient\nmeta-learning paradigms and new theoretical insights into linking symbolic\ntheories and neural mechanisms.", "published": "2025-03-20 11:33:59", "link": "http://arxiv.org/abs/2503.16048v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SeniorTalk: A Chinese Conversation Dataset with Rich Annotations for Super-Aged Seniors", "abstract": "While voice technologies increasingly serve aging populations, current\nsystems exhibit significant performance gaps due to inadequate training data\ncapturing elderly-specific vocal characteristics like presbyphonia and\ndialectal variations. The limited data available on super-aged individuals in\nexisting elderly speech datasets, coupled with overly simple recording styles\nand annotation dimensions, exacerbates this issue. To address the critical\nscarcity of speech data from individuals aged 75 and above, we introduce\nSeniorTalk, a carefully annotated Chinese spoken dialogue dataset. This dataset\ncontains 55.53 hours of speech from 101 natural conversations involving 202\nparticipants, ensuring a strategic balance across gender, region, and age.\nThrough detailed annotation across multiple dimensions, it can support a wide\nrange of speech tasks. We perform extensive experiments on speaker\nverification, speaker diarization, speech recognition, and speech editing\ntasks, offering crucial insights for the development of speech technologies\ntargeting this age group.", "published": "2025-03-20 11:31:47", "link": "http://arxiv.org/abs/2503.16578v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Incomplete Utterance Rewriting with Editing Operation Guidance and Utterance Augmentation", "abstract": "Although existing fashionable generation methods on Incomplete Utterance\nRewriting (IUR) can generate coherent utterances, they often result in the\ninclusion of irrelevant and redundant tokens in rewritten utterances due to\ntheir inability to focus on critical tokens in dialogue context. Furthermore,\nthe limited size of the training datasets also contributes to the insufficient\ntraining of the IUR model. To address the first issue, we propose a multi-task\nlearning framework EO-IUR (Editing Operation-guided Incomplete Utterance\nRewriting) that introduces the editing operation labels generated by sequence\nlabeling module to guide generation model to focus on critical tokens.\nFurthermore, we introduce a token-level heterogeneous graph to represent\ndialogues. To address the second issue, we propose a two-dimensional utterance\naugmentation strategy, namely editing operation-based incomplete utterance\naugmentation and LLM-based historical utterance augmentation. The experimental\nresults on three datasets demonstrate that our EO-IUR outperforms previous\nstate-of-the-art (SOTA) baselines in both open-domain and task-oriented\ndialogue. The code will be available at https://github.com/Dewset/EO-IUR.", "published": "2025-03-20 11:26:46", "link": "http://arxiv.org/abs/2503.16043v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating Test-Time Scaling LLMs for Legal Reasoning: OpenAI o1, DeepSeek-R1, and Beyond", "abstract": "Recently, Test-Time Scaling Large Language Models (LLMs), such as DeepSeek-R1\nand OpenAI o1, have demonstrated exceptional capabilities across various\ndomains and tasks, particularly in reasoning. While these models have shown\nimpressive performance on general language tasks, their effectiveness in\nspecialized fields like legal remains unclear. To address this, we present a\npreliminary evaluation of LLMs in various legal scenarios, covering both\nChinese and English legal tasks. Our analysis includes 9 LLMs and 17 legal\ntasks, with a focus on newly published and more complex challenges such as\nmulti-defendant legal judgments and legal argument reasoning. Our findings\nindicate that, despite DeepSeek-R1 and OpenAI o1 being among the most powerful\nmodels, their legal reasoning capabilities are still lacking. Specifically,\nthese models score below 80\\% on seven Chinese legal reasoning tasks and below\n80\\% on two English legal reasoning tasks. This suggests that, even among the\nmost advanced reasoning models, legal reasoning abilities remain\nunderdeveloped.", "published": "2025-03-20 11:14:39", "link": "http://arxiv.org/abs/2503.16040v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hybrid-Level Instruction Injection for Video Token Compression in Multi-modal Large Language Models", "abstract": "Recent Multi-modal Large Language Models (MLLMs) have been challenged by the\ncomputational overhead resulting from massive video frames, often alleviated\nthrough compression strategies. However, the visual content is not equally\ncontributed to user instructions, existing strategies (\\eg, average pool)\ninevitably lead to the loss of potentially useful information. To tackle this,\nwe propose the Hybrid-level Instruction Injection Strategy for Conditional\nToken Compression in MLLMs (HICom), utilizing the instruction as a condition to\nguide the compression from both local and global levels. This encourages the\ncompression to retain the maximum amount of user-focused information while\nreducing visual tokens to minimize computational burden. Specifically, the\ninstruction condition is injected into the grouped visual tokens at the local\nlevel and the learnable tokens at the global level, and we conduct the\nattention mechanism to complete the conditional compression. From the\nhybrid-level compression, the instruction-relevant visual parts are highlighted\nwhile the temporal-spatial structure is also preserved for easier understanding\nof LLMs. To further unleash the potential of HICom, we introduce a new\nconditional pre-training stage with our proposed dataset HICom-248K.\nExperiments show that our HICom can obtain distinguished video understanding\nability with fewer tokens, increasing the performance by 2.43\\% average on\nthree multiple-choice QA benchmarks and saving 78.8\\% tokens compared with the\nSOTA method. The code is available at https://github.com/lntzm/HICom.", "published": "2025-03-20 11:09:18", "link": "http://arxiv.org/abs/2503.16036v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging Fabricated Claims with Humorous Content", "abstract": "This paper presents the Deceptive Humor Dataset (DHD), a novel resource for\nstudying humor derived from fabricated claims and misinformation. In an era of\nrampant misinformation, understanding how humor intertwines with deception is\nessential. DHD consists of humor-infused comments generated from false\nnarratives, incorporating fabricated claims and manipulated information using\nthe ChatGPT-4o model. Each instance is labeled with a Satire Level, ranging\nfrom 1 for subtle satire to 3 for high-level satire and classified into five\ndistinct Humor Categories: Dark Humor, Irony, Social Commentary, Wordplay, and\nAbsurdity. The dataset spans multiple languages including English, Telugu,\nHindi, Kannada, Tamil, and their code-mixed variants (Te-En, Hi-En, Ka-En,\nTa-En), making it a valuable multilingual benchmark. By introducing DHD, we\nestablish a structured foundation for analyzing humor in deceptive contexts,\npaving the way for a new research direction that explores how humor not only\ninteracts with misinformation but also influences its perception and spread. We\nestablish strong baselines for the proposed dataset, providing a foundation for\nfuture research to benchmark and advance deceptive humor detection models.", "published": "2025-03-20 10:58:02", "link": "http://arxiv.org/abs/2503.16031v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided Improvement", "abstract": "Large language models (LLMs) have recently transformed from text-based\nassistants to autonomous agents capable of planning, reasoning, and iteratively\nimproving their actions. While numerical reward signals and verifiers can\neffectively rank candidate actions, they often provide limited contextual\nguidance. In contrast, natural language feedback better aligns with the\ngenerative capabilities of LLMs, providing richer and more actionable\nsuggestions. However, parsing and implementing this feedback effectively can be\nchallenging for LLM-based agents. In this work, we introduce Critique-Guided\nImprovement (CGI), a novel two-player framework, comprising an actor model that\nexplores an environment and a critic model that generates detailed nature\nlanguage feedback. By training the critic to produce fine-grained assessments\nand actionable revisions, and the actor to utilize these critiques, our\napproach promotes more robust exploration of alternative strategies while\navoiding local optima. Experiments in three interactive environments show that\nCGI outperforms existing baselines by a substantial margin. Notably, even a\nsmall critic model surpasses GPT-4 in feedback quality. The resulting actor\nachieves state-of-the-art performance, demonstrating the power of explicit\niterative guidance to enhance decision-making in LLM-based agents.", "published": "2025-03-20 10:42:33", "link": "http://arxiv.org/abs/2503.16024v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Corrective In-Context Learning: Evaluating Self-Correction in Large Language Models", "abstract": "In-context learning (ICL) has transformed the use of large language models\n(LLMs) for NLP tasks, enabling few-shot learning by conditioning on labeled\nexamples without finetuning. Despite its effectiveness, ICL is prone to errors,\nespecially for challenging examples. With the goal of improving the performance\nof ICL, we propose corrective in-context learning (CICL), an approach that\nincorporates a model's incorrect predictions alongside ground truth corrections\ninto the prompt, aiming to enhance classification accuracy through\nself-correction. However, contrary to our hypothesis, extensive experiments on\ntext classification tasks demonstrate that CICL consistently underperforms\nstandard ICL, with performance degrading as the proportion of corrections in\nthe prompt increases. Our findings indicate that CICL introduces confusion by\ndisrupting the model's task understanding, rather than refining its\npredictions. Additionally, we observe that presenting harder examples in\nstandard ICL does not improve performance, suggesting that example difficulty\nalone may not be a reliable criterion for effective selection. By presenting\nthese negative results, we provide important insights into the limitations of\nself-corrective mechanisms in LLMs and offer directions for future research.", "published": "2025-03-20 10:39:39", "link": "http://arxiv.org/abs/2503.16022v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Autonomous AI imitators increase diversity in homogeneous information ecosystems", "abstract": "Recent breakthroughs in large language models (LLMs) have facilitated\nautonomous AI agents capable of imitating human-generated content. This\ntechnological advancement raises fundamental questions about AI's impact on the\ndiversity and democratic value of information ecosystems. We introduce a\nlarge-scale simulation framework to examine AI-based imitation within news, a\ncontext crucial for public discourse. By systematically testing two distinct\nimitation strategies across a range of information environments varying in\ninitial diversity, we demonstrate that AI-generated articles do not uniformly\nhomogenize content. Instead, AI's influence is strongly context-dependent:\nAI-generated content can introduce valuable diversity in originally homogeneous\nnews environments but diminish diversity in initially heterogeneous contexts.\nThese results illustrate that the initial diversity of an information\nenvironment critically shapes AI's impact, challenging assumptions that\nAI-driven imitation threatens diversity. Instead, when information is initially\nhomogeneous, AI-driven imitation can expand perspectives, styles, and topics.\nThis is especially important in news contexts, where information diversity\nfosters richer public debate by exposing citizens to alternative viewpoints,\nchallenging biases, and preventing narrative monopolies, which is essential for\na resilient democracy.", "published": "2025-03-20 10:37:29", "link": "http://arxiv.org/abs/2503.16021v3", "categories": ["cs.CY", "cs.AI", "cs.CL", "J.4"], "primary_category": "cs.CY"}
{"title": "ECKGBench: Benchmarking Large Language Models in E-commerce Leveraging Knowledge Graph", "abstract": "Large language models (LLMs) have demonstrated their capabilities across\nvarious NLP tasks. Their potential in e-commerce is also substantial, evidenced\nby practical implementations such as platform search, personalized\nrecommendations, and customer service. One primary concern associated with LLMs\nis their factuality (e.g., hallucination), which is urgent in e-commerce due to\nits significant impact on user experience and revenue. Despite some methods\nproposed to evaluate LLMs' factuality, issues such as lack of reliability, high\nconsumption, and lack of domain expertise leave a gap between effective\nassessment in e-commerce. To bridge the evaluation gap, we propose ECKGBench, a\ndataset specifically designed to evaluate the capacities of LLMs in e-commerce\nknowledge. Specifically, we adopt a standardized workflow to automatically\ngenerate questions based on a large-scale knowledge graph, guaranteeing\nsufficient reliability. We employ the simple question-answering paradigm,\nsubstantially improving the evaluation efficiency by the least input and output\ntokens. Furthermore, we inject abundant e-commerce expertise in each evaluation\nstage, including human annotation, prompt design, negative sampling, and\nverification. Besides, we explore the LLMs' knowledge boundaries in e-commerce\nfrom a novel perspective. Through comprehensive evaluations of several advanced\nLLMs on ECKGBench, we provide meticulous analysis and insights into leveraging\nLLMs for e-commerce.", "published": "2025-03-20 09:49:15", "link": "http://arxiv.org/abs/2503.15990v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extract, Match, and Score: An Evaluation Paradigm for Long Question-context-answer Triplets in Financial Analysis", "abstract": "The rapid advancement of large language models (LLMs) has sparked widespread\nadoption across diverse applications, making robust evaluation frameworks\ncrucial for assessing their performance. While conventional evaluation metrics\nremain applicable for shorter texts, their efficacy diminishes when evaluating\nthe quality of long-form answers. This limitation is particularly critical in\nreal-world scenarios involving extended questions, extensive context, and\nlong-form answers, such as financial analysis or regulatory compliance. In this\npaper, we use a practical financial use case to illustrate applications that\nhandle \"long question-context-answer triplets\". We construct a real-world\nfinancial dataset comprising long triplets and demonstrate the inadequacies of\ntraditional metrics. To address this, we propose an effective Extract, Match,\nand Score (EMS) evaluation approach tailored to the complexities of long-form\nLLMs' outputs, providing practitioners with a reliable methodology for\nassessing LLMs' performance in complex real-world scenarios.", "published": "2025-03-20 09:38:44", "link": "http://arxiv.org/abs/2503.16575v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "InhibiDistilbert: Knowledge Distillation for a ReLU and Addition-based Transformer", "abstract": "This work explores optimizing transformer-based language models by\nintegrating model compression techniques with inhibitor attention, a novel\nalternative attention mechanism. Inhibitor attention employs Manhattan\ndistances and ReLU activations instead of the matrix multiplications and\nsoftmax activation of the conventional scaled dot-product attention. This shift\noffers potential computational and energy savings while maintaining model\neffectiveness. We propose further adjustments to improve the inhibitor\nmechanism's training efficiency and evaluate its performance on the DistilBERT\narchitecture. Our knowledge distillation experiments indicate that the modified\ninhibitor transformer model can achieve competitive performance on standard NLP\nbenchmarks, including General Language Understanding Evaluation (GLUE) and\nsentiment analysis tasks.", "published": "2025-03-20 09:30:35", "link": "http://arxiv.org/abs/2503.15983v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 (Primary) 68T07, 68Q32 (Secondary)", "I.2.6; I.2.7; I.5.1"], "primary_category": "cs.CL"}
{"title": "Exploratory Study into Relations between Cognitive Distortions and Emotional Appraisals", "abstract": "In recent years, there has been growing interest in studying cognitive\ndistortions and emotional appraisals from both computational and psychological\nperspectives. Despite considerable similarities between emotional reappraisal\nand cognitive reframing as emotion regulation techniques, these concepts have\nlargely been examined in isolation. This research explores the relationship\nbetween cognitive distortions and emotional appraisal dimensions, examining\ntheir potential connections and relevance for future interdisciplinary studies.\nUnder this pretext, we conduct an exploratory computational study, aimed at\ninvestigating the relationship between cognitive distortion and emotional\nappraisals. We show that the patterns of statistically significant\nrelationships between cognitive distortions and appraisal dimensions vary\nacross different distortion categories, giving rise to distinct appraisal\nprofiles for individual distortion classes. Additionally, we analyze the impact\nof cognitive restructuring on appraisal dimensions, exemplifying the emotion\nregulation aspect of cognitive restructuring.", "published": "2025-03-20 09:23:35", "link": "http://arxiv.org/abs/2503.15979v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Group Policy Optimization: Towards Stable Training and Token-Efficient Reasoning", "abstract": "Since DeepSeek-R1 popularized, Group Relative Policy Optimization (GRPO) has\nbecome the core part of Reasoning LLMs training. However, we find some\ndeficiency that influences RL stability and inference efficiency. Thus, we\npropose Adaptive Group Policy Optimization (AGPO) which contains two simple but\neffective modifications: a revised advantage estimation method to mitigate\nzero-variance situations; a length-based reward, incentivizing the model to\navoid overthinking. The experiments demonstrate our methods achieve more stable\ntraining and comparable or superior performance with significantly fewer tokens\nin reasoning steps.", "published": "2025-03-20 08:48:57", "link": "http://arxiv.org/abs/2503.15952v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Fight Hallucinations, Use Them: Estimating Image Realism using NLI over Atomic Facts", "abstract": "Quantifying the realism of images remains a challenging problem in the field\nof artificial intelligence. For example, an image of Albert Einstein holding a\nsmartphone violates common-sense because modern smartphone were invented after\nEinstein's death. We introduce a novel method for assessing image realism using\nLarge Vision-Language Models (LVLMs) and Natural Language Inference (NLI). Our\napproach is based on the premise that LVLMs may generate hallucinations when\nconfronted with images that defy common sense. Using LVLM to extract atomic\nfacts from these images, we obtain a mix of accurate facts and erroneous\nhallucinations. We proceed by calculating pairwise entailment scores among\nthese facts, subsequently aggregating these values to yield a singular reality\nscore. This process serves to identify contradictions between genuine facts and\nhallucinatory elements, signaling the presence of images that violate common\nsense. Our approach has achieved a new state-of-the-art performance in\nzero-shot mode on the WHOOPS! dataset.", "published": "2025-03-20 08:44:10", "link": "http://arxiv.org/abs/2503.15948v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "From Chaos to Order: The Atomic Reasoner Framework for Fine-grained Reasoning in Large Language Models", "abstract": "Recent advances in large language models (LLMs) have shown remarkable\nprogress, yet their capacity for logical ``slow-thinking'' reasoning persists\nas a critical research frontier. Current inference scaling paradigms suffer\nfrom two fundamental constraints: fragmented thought flows compromising logical\ncoherence, and intensively computational complexity that escalates with search\nspace dimensions. To overcome these limitations, we present \\textbf{Atomic\nReasoner} (\\textbf{AR}), a cognitive inference strategy that enables\nfine-grained reasoning through systematic atomic-level operations. AR\ndecomposes the reasoning process into atomic cognitive units, employing a\ncognitive routing mechanism to dynamically construct reasoning representations\nand orchestrate inference pathways. This systematic methodology implements\nstepwise, structured cognition, which ensures logical coherence while\nsignificantly reducing cognitive load, effectively simulating the cognitive\npatterns observed in human deep thinking processes. Extensive experimental\nresults demonstrate AR's superior reasoning capabilities without the\ncomputational burden of exhaustive solution searches, particularly excelling in\nlinguistic logic puzzles. These findings substantiate AR's effectiveness in\nenhancing LLMs' capacity for robust, long-sequence logical reasoning and\ndeliberation.", "published": "2025-03-20 08:34:53", "link": "http://arxiv.org/abs/2503.15944v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Automatic Continual Learning: A Self-Adaptive Framework for Continual Instruction Tuning", "abstract": "Continual instruction tuning enables large language models (LLMs) to learn\nincrementally while retaining past knowledge, whereas existing methods\nprimarily focus on how to retain old knowledge rather than on selecting which\nnew knowledge to learn. In domain-specific contexts, maintaining data quality\nand managing system constraints remain key challenges. To address these issues,\nwe propose an automated continual instruction tuning framework that dynamically\nfilters incoming data, which identify and reduce redundant data across\nsuccessive updates. Our approach utilizes a small proxy model for efficient\nperplexity-based filtering, and updates the proxy to ensure that the filtering\ncriteria remain aligned with the evolving state of the deployed model. Compared\nto existing static data selection methods, our framework can effectively handle\nincrementally acquired data and shifting distributions. Additionally, it\naddresses practical deployment challenges by enabling seamless model updates,\nsupporting version rollback and incorporating automatic checkpoint evaluation.\nWe evaluated the system in real-world medical scenarios. It reduced\ncomputational costs by 66.7% and improved model performance, and achieved\nautonomous updates, thus demonstrating its effectiveness for automatic\ncontinual instruction tuning.", "published": "2025-03-20 08:00:41", "link": "http://arxiv.org/abs/2503.15924v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Structured Prompts to Open Narratives: Measuring Gender Bias in LLMs Through Open-Ended Storytelling", "abstract": "Large Language Models (LLMs) have revolutionized natural language processing,\nyet concerns persist regarding their tendency to reflect or amplify social\nbiases present in their training data. This study introduces a novel evaluation\nframework to uncover gender biases in LLMs, focusing on their occupational\nnarratives. Unlike previous methods relying on structured scenarios or\ncarefully crafted prompts, our approach leverages free-form storytelling to\nreveal biases embedded in the models. Systematic analyses show an\noverrepresentation of female characters across occupations in six widely used\nLLMs. Additionally, our findings reveal that LLM-generated occupational gender\nrankings align more closely with human stereotypes than actual labor\nstatistics. These insights underscore the need for balanced mitigation\nstrategies to ensure fairness while avoiding the reinforcement of new\nstereotypes.", "published": "2025-03-20 07:15:45", "link": "http://arxiv.org/abs/2503.15904v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Gene42: Long-Range Genomic Foundation Model With Dense Attention", "abstract": "We introduce Gene42, a novel family of Genomic Foundation Models (GFMs)\ndesigned to manage context lengths of up to 192,000 base pairs (bp) at a\nsingle-nucleotide resolution. Gene42 models utilize a decoder-only\n(LLaMA-style) architecture with a dense self-attention mechanism. Initially\ntrained on fixed-length sequences of 4,096 bp, our models underwent continuous\npretraining to extend the context length to 192,000 bp. This iterative\nextension allowed for the comprehensive processing of large-scale genomic data\nand the capture of intricate patterns and dependencies within the human genome.\nGene42 is the first dense attention model capable of handling such extensive\nlong context lengths in genomics, challenging state-space models that often\nrely on convolutional operators among other mechanisms. Our pretrained models\nexhibit notably low perplexity values and high reconstruction accuracy,\nhighlighting their strong ability to model genomic data. Extensive experiments\non various genomic benchmarks have demonstrated state-of-the-art performance\nacross multiple tasks, including biotype classification, regulatory region\nidentification, chromatin profiling prediction, variant pathogenicity\nprediction, and species classification. The models are publicly available at\nhuggingface.co/inceptionai.", "published": "2025-03-20 07:10:04", "link": "http://arxiv.org/abs/2503.16565v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "q-bio.GN"], "primary_category": "cs.LG"}
{"title": "Chem42: a Family of chemical Language Models for Target-aware Ligand Generation", "abstract": "Revolutionizing drug discovery demands more than just understanding molecular\ninteractions - it requires generative models that can design novel ligands\ntailored to specific biological targets. While chemical Language Models (cLMs)\nhave made strides in learning molecular properties, most fail to incorporate\ntarget-specific insights, restricting their ability to drive de-novo ligand\ngeneration. Chem42, a cutting-edge family of generative chemical Language\nModels, is designed to bridge this gap. By integrating atomic-level\ninteractions with multimodal inputs from Prot42, a complementary protein\nLanguage Model, Chem42 achieves a sophisticated cross-modal representation of\nmolecular structures, interactions, and binding patterns. This innovative\nframework enables the creation of structurally valid, synthetically accessible\nligands with enhanced target specificity. Evaluations across diverse protein\ntargets confirm that Chem42 surpasses existing approaches in chemical validity,\ntarget-aware design, and predicted binding affinity. By reducing the search\nspace of viable drug candidates, Chem42 could accelerate the drug discovery\npipeline, offering a powerful generative AI tool for precision medicine. Our\nChem42 models set a new benchmark in molecule property prediction, conditional\nmolecule generation, and target-aware ligand design. The models are publicly\navailable at huggingface.co/inceptionai.", "published": "2025-03-20 07:07:30", "link": "http://arxiv.org/abs/2503.16563v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "q-bio.BM"], "primary_category": "cs.LG"}
{"title": "Parameters vs. Context: Fine-Grained Control of Knowledge Reliance in Language Models", "abstract": "Retrieval-Augmented Generation (RAG) mitigates hallucinations in Large\nLanguage Models (LLMs) by integrating external knowledge. However, conflicts\nbetween parametric knowledge and retrieved context pose challenges,\nparticularly when retrieved information is unreliable or the model's internal\nknowledge is outdated. In such cases, LLMs struggle to determine whether to\nrely more on their own parameters or the conflicted context. To address this,\nwe propose **CK-PLUG**, a plug-and-play method for controlling LLMs' reliance\non parametric and contextual knowledge. We introduce a novel knowledge\nconsistency metric, Confidence Gain, which detects knowledge conflicts by\nmeasuring entropy shifts in token probability distributions after context\ninsertion. CK-PLUG then enables fine-grained control over knowledge preference\nby adjusting the probability distribution of tokens with negative confidence\ngain through a single tuning parameter. Experiments demonstrate CK-PLUG's\nability to significantly regulate knowledge reliance in counterfactual RAG\nscenarios while maintaining generation fluency and knowledge accuracy. For\ninstance, on Llama3-8B, memory recall (MR) of RAG response can be adjusted\nwithin a broad range (9.9%-71.9%), compared to the baseline of 42.1%. Moreover,\nCK-PLUG supports adaptive control based on the model's confidence in both\ninternal and external knowledge, achieving consistent performance improvements\nacross various general RAG tasks. Our code is available at:\n$\\href{https://github.com/byronBBL/CK-PLUG}{\\text{this https URL}}$.", "published": "2025-03-20 06:26:28", "link": "http://arxiv.org/abs/2503.15888v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FutureGen: LLM-RAG Approach to Generate the Future Work of Scientific Article", "abstract": "The future work section of a scientific article outlines potential research\ndirections by identifying gaps and limitations of a current study. This section\nserves as a valuable resource for early-career researchers seeking unexplored\nareas and experienced researchers looking for new projects or collaborations.\nIn this study, we generate future work suggestions from key sections of a\nscientific article alongside related papers and analyze how the trends have\nevolved. We experimented with various Large Language Models (LLMs) and\nintegrated Retrieval-Augmented Generation (RAG) to enhance the generation\nprocess. We incorporate a LLM feedback mechanism to improve the quality of the\ngenerated content and propose an LLM-as-a-judge approach for evaluation. Our\nresults demonstrated that the RAG-based approach with LLM feedback outperforms\nother methods evaluated through qualitative and quantitative metrics. Moreover,\nwe conduct a human evaluation to assess the LLM as an extractor and judge. The\ncode and dataset for this project are here, code: HuggingFace", "published": "2025-03-20 06:14:02", "link": "http://arxiv.org/abs/2503.16561v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "InCo-DPO: Balancing Distribution Shift and Data Quality for Enhanced Preference Optimization", "abstract": "Direct Preference Optimization (DPO) optimizes language models to align with\nhuman preferences. Utilizing on-policy samples, generated directly by the\npolicy model, typically results in better performance due to its distribution\nconsistency with the model compared to off-policy samples. This paper\nidentifies the quality of candidate preference samples as another critical\nfactor. While the quality of on-policy data is inherently constrained by the\ncapabilities of the policy model, off-policy data, which can be derived from\ndiverse sources, offers greater potential for quality despite experiencing\ndistribution shifts. However, current research mostly relies on on-policy data\nand neglects the value of off-policy data in terms of data quality, due to the\nchallenge posed by distribution shift. In this paper, we propose InCo-DPO, an\nefficient method for synthesizing preference data by integrating on-policy and\noff-policy data, allowing dynamic adjustments to balance distribution shifts\nand data quality, thus finding an optimal trade-off. Consequently, InCo-DPO\novercomes the limitations of distribution shifts in off-policy data and the\nquality constraints of on-policy data. We evaluated InCo-DPO with the\nAlpaca-Eval 2.0 and Arena-Hard benchmarks. Experimental results demonstrate\nthat our approach not only outperforms both on-policy and off-policy data but\nalso achieves a state-of-the-art win rate of 60.8 on Arena-Hard with the\nvanilla DPO using Gemma-2 model.", "published": "2025-03-20 06:05:36", "link": "http://arxiv.org/abs/2503.15880v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid Question Answering", "abstract": "Non-factoid question-answering (NFQA) poses a significant challenge due to\nits open-ended nature, diverse intents, and the need for multi-aspect\nreasoning, which renders conventional factoid QA approaches, including\nretrieval-augmented generation (RAG), inadequate. Unlike factoid questions,\nnon-factoid questions (NFQs) lack definitive answers and require synthesizing\ninformation from multiple sources across various reasoning dimensions. To\naddress these limitations, we introduce Typed-RAG, a type-aware multi-aspect\ndecomposition framework within the RAG paradigm for NFQA. Typed-RAG classifies\nNFQs into distinct types -- such as debate, experience, and comparison -- and\napplies aspect-based decomposition to refine retrieval and generation\nstrategies. By decomposing multi-aspect NFQs into single-aspect sub-queries and\naggregating the results, Typed-RAG generates more informative and contextually\nrelevant responses. To evaluate Typed-RAG, we introduce Wiki-NFQA, a benchmark\ndataset covering diverse NFQ types. Experimental results demonstrate that\nTyped-RAG outperforms baselines, thereby highlighting the importance of\ntype-aware decomposition for effective retrieval and generation in NFQA. Our\ncode and dataset are available at https://github.com/TeamNLP/Typed-RAG.", "published": "2025-03-20 06:04:12", "link": "http://arxiv.org/abs/2503.15879v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey", "abstract": "Large Language Models (LLMs) excel in text generation, reasoning, and\ndecision-making, enabling their adoption in high-stakes domains such as\nhealthcare, law, and transportation. However, their reliability is a major\nconcern, as they often produce plausible but incorrect responses. Uncertainty\nquantification (UQ) enhances trustworthiness by estimating confidence in\noutputs, enabling risk mitigation and selective prediction. However,\ntraditional UQ methods struggle with LLMs due to computational constraints and\ndecoding inconsistencies. Moreover, LLMs introduce unique uncertainty sources,\nsuch as input ambiguity, reasoning path divergence, and decoding stochasticity,\nthat extend beyond classical aleatoric and epistemic uncertainty. To address\nthis, we introduce a new taxonomy that categorizes UQ methods based on\ncomputational efficiency and uncertainty dimensions (input, reasoning,\nparameter, and prediction uncertainty). We evaluate existing techniques, assess\ntheir real-world applicability, and identify open challenges, emphasizing the\nneed for scalable, interpretable, and robust UQ approaches to enhance LLM\nreliability.", "published": "2025-03-20 05:04:29", "link": "http://arxiv.org/abs/2503.15850v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entropy-based Exploration Conduction for Multi-step Reasoning", "abstract": "In large language model (LLM) reasoning, multi-step processes have proven\neffective for solving complex tasks. However, the depth of exploration can\nsignificantly affect the reasoning performance. Existing methods to\nautomatically decide the depth often bring high costs and lack flexibility, and\nthus undermine the model's reasoning accuracy. To address these issues, we\npropose Entropy-based Exploration Depth Conduction (Entro-duction), a novel\nmethod that dynamically adjusts the exploration depth during multi-step\nreasoning by monitoring LLM's output entropy and variance entropy. We employ\nthese two metrics to capture the model's current uncertainty and the\nfluctuation of uncertainty across consecutive reasoning steps. Based on the\nobserved changes, the LLM selects whether to deepen, expand or stop exploration\naccording to the probability. In this way, we balance the reasoning accuracy\nand exploration effectiveness. Experimental results across four benchmark\ndatasets demonstrate the efficacy of Entro-duction. We further conduct\nexperiments and analysis on the components of Entro-duction to discuss their\ncontributions to reasoning performance.", "published": "2025-03-20 05:03:26", "link": "http://arxiv.org/abs/2503.15848v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "F\u00f9x\u00ec: A Benchmark for Evaluating Language Models on Ancient Chinese Text Understanding and Generation", "abstract": "Ancient Chinese text processing presents unique challenges for large language\nmodels (LLMs) due to its distinct linguistic features, complex structural\nconstraints, and rich cultural context. While existing benchmarks have\nprimarily focused on evaluating comprehension through multiple-choice\nquestions, there remains a critical gap in assessing models' generative\ncapabilities in classical Chinese. We introduce F\\`ux\\`i, a comprehensive\nbenchmark that evaluates both understanding and generation capabilities across\n21 diverse tasks. Our benchmark distinguishes itself through three key\ncontributions: (1) balanced coverage of both comprehension and generation\ntasks, including novel tasks like poetry composition and couplet completion,\n(2) specialized evaluation metrics designed specifically for classical Chinese\ntext generation, combining rule-based verification with fine-tuned LLM\nevaluators, and (3) a systematic assessment framework that considers both\nlinguistic accuracy and cultural authenticity. Through extensive evaluation of\nstate-of-the-art LLMs, we reveal significant performance gaps between\nunderstanding and generation tasks, with models achieving promising results in\ncomprehension but struggling considerably in generation tasks, particularly\nthose requiring deep cultural knowledge and adherence to classical formats. Our\nfindings highlight the current limitations in ancient Chinese text processing\nand provide insights for future model development. The benchmark, evaluation\ntoolkit, and baseline results are publicly available to facilitate research in\nthis domain.", "published": "2025-03-20 04:26:40", "link": "http://arxiv.org/abs/2503.15837v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChatGPT and U(X): A Rapid Review on Measuring the User Experience", "abstract": "ChatGPT, powered by a large language model (LLM), has revolutionized everyday\nhuman-computer interaction (HCI) since its 2022 release. While now used by\nmillions around the world, a coherent pathway for evaluating the user\nexperience (UX) ChatGPT offers remains missing. In this rapid review (N = 58),\nI explored how ChatGPT UX has been approached quantitatively so far. I focused\non the independent variables (IVs) manipulated, the dependent variables (DVs)\nmeasured, and the methods used for measurement. Findings reveal trends, gaps,\nand emerging consensus in UX assessments. This work offers a first step towards\nsynthesizing existing approaches to measuring ChatGPT UX, urgent trajectories\nto advance standardization and breadth, and two preliminary frameworks aimed at\nguiding future research and tool development. I seek to elevate the field of\nChatGPT UX by empowering researchers and practitioners in optimizing user\ninteractions with ChatGPT and similar LLM-based systems.", "published": "2025-03-20 02:51:11", "link": "http://arxiv.org/abs/2503.15808v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "Mixture of Lookup Experts", "abstract": "Mixture-of-Experts (MoE) activates only a subset of experts during inference,\nallowing the model to maintain low inference FLOPs and latency even as the\nparameter count scales up. However, since MoE dynamically selects the experts,\nall the experts need to be loaded into VRAM. Their large parameter size still\nlimits deployment, and offloading, which load experts into VRAM only when\nneeded, significantly increase inference latency. To address this, we propose\nMixture of Lookup Experts (MoLE), a new MoE architecture that is efficient in\nboth communication and VRAM usage. In MoLE, the experts are Feed-Forward\nNetworks (FFNs) during training, taking the output of the embedding layer as\ninput. Before inference, these experts can be re-parameterized as lookup tables\n(LUTs) that retrieves expert outputs based on input ids, and offloaded to\nstorage devices. Therefore, we do not need to perform expert computations\nduring inference. Instead, we directly retrieve the expert's computation\nresults based on input ids and load them into VRAM, and thus the resulting\ncommunication overhead is negligible. Experiments show that, with the same\nFLOPs and VRAM usage, MoLE achieves inference speeds comparable to dense models\nand significantly faster than MoE with experts offloading, while maintaining\nperformance on par with MoE.", "published": "2025-03-20 02:31:57", "link": "http://arxiv.org/abs/2503.15798v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Grammar and Gameplay-aligned RL for Game Description Generation with LLMs", "abstract": "Game Description Generation (GDG) is the task of generating a game\ndescription written in a Game Description Language (GDL) from natural language\ntext. Previous studies have explored generation methods leveraging the\ncontextual understanding capabilities of Large Language Models (LLMs); however,\naccurately reproducing the game features of the game descriptions remains a\nchallenge. In this paper, we propose reinforcement learning-based fine-tuning\nof LLMs for GDG (RLGDG). Our training method simultaneously improves\ngrammatical correctness and fidelity to game concepts by introducing both\ngrammar rewards and concept rewards. Furthermore, we adopt a two-stage training\nstrategy where Reinforcement Learning (RL) is applied following Supervised\nFine-Tuning (SFT). Experimental results demonstrate that our proposed method\nsignificantly outperforms baseline methods using SFT alone.", "published": "2025-03-20 01:47:33", "link": "http://arxiv.org/abs/2503.15783v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can one size fit all?: Measuring Failure in Multi-Document Summarization Domain Transfer", "abstract": "Abstractive multi-document summarization (MDS) is the task of automatically\nsummarizing information in multiple documents, from news articles to\nconversations with multiple speakers. The training approaches for current MDS\nmodels can be grouped into four approaches: end-to-end with special\npre-training (\"direct\"), chunk-then-summarize, extract-then-summarize, and\ninference with GPT-style models. In this work, we evaluate MDS models across\ntraining approaches, domains, and dimensions (reference similarity, quality,\nand factuality), to analyze how and why models trained on one domain can fail\nto summarize documents from another (News, Science, and Conversation) in the\nzero-shot domain transfer setting. We define domain-transfer \"failure\" as a\ndecrease in factuality, higher deviation from the target, and a general\ndecrease in summary quality. In addition to exploring domain transfer for MDS\nmodels, we examine potential issues with applying popular summarization metrics\nout-of-the-box.", "published": "2025-03-20 00:57:38", "link": "http://arxiv.org/abs/2503.15768v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HiAER-Spike: Hardware-Software Co-Design for Large-Scale Reconfigurable Event-Driven Neuromorphic Computing", "abstract": "In this work, we present HiAER-Spike, a modular, reconfigurable, event-driven\nneuromorphic computing platform designed to execute large spiking neural\nnetworks with up to 160 million neurons and 40 billion synapses - roughly twice\nthe neurons of a mouse brain at faster-than real-time. This system, which is\ncurrently under construction at the UC San Diego Supercomputing Center,\ncomprises a co-designed hard- and software stack that is optimized for run-time\nmassively parallel processing and hierarchical address-event routing (HiAER) of\nspikes while promoting memory-efficient network storage and execution. Our\narchitecture efficiently handles both sparse connectivity and sparse activity\nfor robust and low-latency event-driven inference for both edge and cloud\ncomputing. A Python programming interface to HiAER-Spike, agnostic to\nhardware-level detail, shields the user from complexity in the configuration\nand execution of general spiking neural networks with virtually no constraints\nin topology. The system is made easily available over a web portal for use by\nthe wider community. In the following we provide an overview of the hard- and\nsoftware stack, explain the underlying design principles, demonstrate some of\nthe system's capabilities and solicit feedback from the broader neuromorphic\ncommunity.", "published": "2025-03-20 23:54:33", "link": "http://arxiv.org/abs/2504.03671v1", "categories": ["cs.NE", "cs.AI", "cs.DC"], "primary_category": "cs.NE"}
{"title": "SuperARC: A Test for General and Super Intelligence Based on First Principles of Recursion Theory and Algorithmic Probability", "abstract": "We introduce an open-ended test grounded in algorithmic probability that can\navoid benchmark contamination in the quantitative evaluation of frontier models\nin the context of their Artificial General Intelligence (AGI) and\nSuperintelligence (ASI) claims. Unlike other tests, this test does not rely on\nstatistical compression methods (such as GZIP or LZW), which are more closely\nrelated to Shannon entropy than to Kolmogorov complexity. The test challenges\naspects related to features of intelligence of fundamental nature such as\nsynthesis and model creation in the context of inverse problems (generating new\nknowledge from observation). We argue that metrics based on model abstraction\nand optimal Bayesian inference for planning can provide a robust framework for\ntesting intelligence, including natural intelligence (human and animal), narrow\nAI, AGI, and ASI. Our results show no clear evidence of LLM convergence towards\na defined level of intelligence, particularly AGI or ASI. We found that LLM\nmodel versions tend to be fragile and incremental, as new versions may perform\nworse than older ones, with progress largely driven by the size of training\ndata. The results were compared with a hybrid neurosymbolic approach that\ntheoretically guarantees model convergence from optimal inference based on the\nprinciples of algorithmic probability and Kolmogorov complexity. The method\noutperforms LLMs in a proof-of-concept on short binary sequences. Our findings\nconfirm suspicions regarding the fundamental limitations of LLMs, exposing them\nas systems optimised for the perception of mastery over human language.\nProgress among different LLM versions from the same developers was found to be\ninconsistent and limited, particularly in the absence of a solid symbolic\ncounterpart.", "published": "2025-03-20 23:11:30", "link": "http://arxiv.org/abs/2503.16743v1", "categories": ["cs.AI", "cs.IT", "math.IT"], "primary_category": "cs.AI"}
{"title": "Towards Agentic Recommender Systems in the Era of Multimodal Large Language Models", "abstract": "Recent breakthroughs in Large Language Models (LLMs) have led to the\nemergence of agentic AI systems that extend beyond the capabilities of\nstandalone models. By empowering LLMs to perceive external environments,\nintegrate multimodal information, and interact with various tools, these\nagentic systems exhibit greater autonomy and adaptability across complex tasks.\nThis evolution brings new opportunities to recommender systems (RS): LLM-based\nAgentic RS (LLM-ARS) can offer more interactive, context-aware, and proactive\nrecommendations, potentially reshaping the user experience and broadening the\napplication scope of RS. Despite promising early results, fundamental\nchallenges remain, including how to effectively incorporate external knowledge,\nbalance autonomy with controllability, and evaluate performance in dynamic,\nmultimodal settings. In this perspective paper, we first present a systematic\nanalysis of LLM-ARS: (1) clarifying core concepts and architectures; (2)\nhighlighting how agentic capabilities -- such as planning, memory, and\nmultimodal reasoning -- can enhance recommendation quality; and (3) outlining\nkey research questions in areas such as safety, efficiency, and lifelong\npersonalization. We also discuss open problems and future directions, arguing\nthat LLM-ARS will drive the next wave of RS innovation. Ultimately, we foresee\na paradigm shift toward intelligent, autonomous, and collaborative\nrecommendation experiences that more closely align with users' evolving needs\nand complex decision-making processes.", "published": "2025-03-20 22:37:15", "link": "http://arxiv.org/abs/2503.16734v1", "categories": ["cs.AI", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Towards Automated Semantic Interpretability in Reinforcement Learning via Vision-Language Models", "abstract": "Semantic Interpretability in Reinforcement Learning (RL) enables\ntransparency, accountability, and safer deployment by making the agent's\ndecisions understandable and verifiable. Achieving this, however, requires a\nfeature space composed of human-understandable concepts, which traditionally\nrely on human specification and fail to generalize to unseen environments. In\nthis work, we introduce Semantically Interpretable Reinforcement Learning with\nVision-Language Models Empowered Automation (SILVA), an automated framework\nthat leverages pre-trained vision-language models (VLM) for semantic feature\nextraction and interpretable tree-based models for policy optimization. SILVA\nfirst queries a VLM to identify relevant semantic features for an unseen\nenvironment, then extracts these features from the environment. Finally, it\ntrains an Interpretable Control Tree via RL, mapping the extracted features to\nactions in a transparent and interpretable manner. To address the computational\ninefficiency of extracting features directly with VLMs, we develop a feature\nextraction pipeline that generates a dataset for training a lightweight\nconvolutional network, which is subsequently used during RL. By leveraging VLMs\nto automate tree-based RL, SILVA removes the reliance on human annotation\npreviously required by interpretable models while also overcoming the inability\nof VLMs alone to generate valid robot policies, enabling semantically\ninterpretable reinforcement learning without human-in-the-loop.", "published": "2025-03-20 21:53:19", "link": "http://arxiv.org/abs/2503.16724v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Comparative Analysis of Deep Learning Models for Real-World ISP Network Traffic Forecasting", "abstract": "Accurate network traffic forecasting is essential for Internet Service\nProviders (ISP) to optimize resources, enhance user experience, and mitigate\nanomalies. This study evaluates state-of-the-art deep learning models on\nCESNET-TimeSeries24, a recently published, comprehensive real-world network\ntraffic dataset from the ISP network CESNET3 spanning multivariate time series\nover 40 weeks. Our findings highlight the balance between prediction accuracy\nand computational efficiency across different levels of network granularity.\nAdditionally, this work establishes a reproducible methodology that facilitates\ndirect comparison of existing approaches, explores their strengths and\nweaknesses, and provides a benchmark for future studies using this dataset.", "published": "2025-03-20 21:04:20", "link": "http://arxiv.org/abs/2503.17410v1", "categories": ["cs.LG", "cs.AI", "cs.NI"], "primary_category": "cs.LG"}
{"title": "QuartDepth: Post-Training Quantization for Real-Time Depth Estimation on the Edge", "abstract": "Monocular Depth Estimation (MDE) has emerged as a pivotal task in computer\nvision, supporting numerous real-world applications. However, deploying\naccurate depth estimation models on resource-limited edge devices, especially\nApplication-Specific Integrated Circuits (ASICs), is challenging due to the\nhigh computational and memory demands. Recent advancements in foundational\ndepth estimation deliver impressive results but further amplify the difficulty\nof deployment on ASICs. To address this, we propose QuartDepth which adopts\npost-training quantization to quantize MDE models with hardware accelerations\nfor ASICs. Our approach involves quantizing both weights and activations to\n4-bit precision, reducing the model size and computation cost. To mitigate the\nperformance degradation, we introduce activation polishing and compensation\nalgorithm applied before and after activation quantization, as well as a weight\nreconstruction method for minimizing errors in weight quantization.\nFurthermore, we design a flexible and programmable hardware accelerator by\nsupporting kernel fusion and customized instruction programmability, enhancing\nthroughput and efficiency. Experimental results demonstrate that our framework\nachieves competitive accuracy while enabling fast inference and higher energy\nefficiency on ASICs, bridging the gap between high-performance depth estimation\nand practical edge-device applicability. Code:\nhttps://github.com/shawnricecake/quart-depth", "published": "2025-03-20 21:03:10", "link": "http://arxiv.org/abs/2503.16709v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Limits of trust in medical AI", "abstract": "Artificial intelligence (AI) is expected to revolutionize the practice of\nmedicine. Recent advancements in the field of deep learning have demonstrated\nsuccess in a variety of clinical tasks: detecting diabetic retinopathy from\nimages, predicting hospital readmissions, aiding in the discovery of new drugs,\netc. AI's progress in medicine, however, has led to concerns regarding the\npotential effects of this technology upon relationships of trust in clinical\npractice. In this paper, I will argue that there is merit to these concerns,\nsince AI systems can be relied upon, and are capable of reliability, but cannot\nbe trusted, and are not capable of trustworthiness. Insofar as patients are\nrequired to rely upon AI systems for their medical decision-making, there is\npotential for this to produce a deficit of trust in relationships in clinical\npractice.", "published": "2025-03-20 20:22:38", "link": "http://arxiv.org/abs/2503.16692v2", "categories": ["cs.LG", "cs.AI", "cs.CY"], "primary_category": "cs.LG"}
{"title": "GAIR: Improving Multimodal Geo-Foundation Model with Geo-Aligned Implicit Representations", "abstract": "Advancements in vision and language foundation models have inspired the\ndevelopment of geo-foundation models (GeoFMs), enhancing performance across\ndiverse geospatial tasks. However, many existing GeoFMs primarily focus on\noverhead remote sensing (RS) data while neglecting other data modalities such\nas ground-level imagery. A key challenge in multimodal GeoFM development is to\nexplicitly model geospatial relationships across modalities, which enables\ngeneralizability across tasks, spatial scales, and temporal contexts. To\naddress these limitations, we propose GAIR, a novel multimodal GeoFM\narchitecture integrating overhead RS data, street view (SV) imagery, and their\ngeolocation metadata. We utilize three factorized neural encoders to project an\nSV image, its geolocation, and an RS image into the embedding space. The SV\nimage needs to be located within the RS image's spatial footprint but does not\nneed to be at its geographic center. In order to geographically align the SV\nimage and RS image, we propose a novel implicit neural representations (INR)\nmodule that learns a continuous RS image representation and looks up the RS\nembedding at the SV image's geolocation. Next, these geographically aligned SV\nembedding, RS embedding, and location embedding are trained with contrastive\nlearning objectives from unlabeled data. We evaluate GAIR across 10 geospatial\ntasks spanning RS image-based, SV image-based, and location embedding-based\nbenchmarks. Experimental results demonstrate that GAIR outperforms\nstate-of-the-art GeoFMs and other strong baselines, highlighting its\neffectiveness in learning generalizable and transferable geospatial\nrepresentations.", "published": "2025-03-20 19:59:39", "link": "http://arxiv.org/abs/2503.16683v1", "categories": ["cs.CV", "cs.AI", "I.4.10"], "primary_category": "cs.CV"}
{"title": "GauRast: Enhancing GPU Triangle Rasterizers to Accelerate 3D Gaussian Splatting", "abstract": "3D intelligence leverages rich 3D features and stands as a promising frontier\nin AI, with 3D rendering fundamental to many downstream applications. 3D\nGaussian Splatting (3DGS), an emerging high-quality 3D rendering method,\nrequires significant computation, making real-time execution on existing\nGPU-equipped edge devices infeasible. Previous efforts to accelerate 3DGS rely\non dedicated accelerators that require substantial integration overhead and\nhardware costs. This work proposes an acceleration strategy that leverages the\nsimilarities between the 3DGS pipeline and the highly optimized conventional\ngraphics pipeline in modern GPUs. Instead of developing a dedicated\naccelerator, we enhance existing GPU rasterizer hardware to efficiently support\n3DGS operations. Our results demonstrate a 23$\\times$ increase in processing\nspeed and a 24$\\times$ reduction in energy consumption, with improvements\nyielding 6$\\times$ faster end-to-end runtime for the original 3DGS algorithm\nand 4$\\times$ for the latest efficiency-improved pipeline, achieving 24 FPS and\n46 FPS respectively. These enhancements incur only a minimal area overhead of\n0.2\\% relative to the entire SoC chip area, underscoring the practicality and\nefficiency of our approach for enabling 3DGS rendering on resource-constrained\nplatforms.", "published": "2025-03-20 19:54:05", "link": "http://arxiv.org/abs/2503.16681v1", "categories": ["cs.GR", "cs.AI", "cs.AR"], "primary_category": "cs.GR"}
{"title": "Echoes of Power: Investigating Geopolitical Bias in US and China Large Language Models", "abstract": "Large Language Models (LLMs) have emerged as powerful tools for generating\nhuman-like text, transforming human-machine interactions. However, their\nwidespread adoption has raised concerns about their potential to influence\npublic opinion and shape political narratives. In this work, we investigate the\ngeopolitical biases in US and Chinese LLMs, focusing on how these models\nrespond to questions related to geopolitics and international relations. We\ncollected responses from ChatGPT and DeepSeek to a set of geopolitical\nquestions and evaluated their outputs through both qualitative and quantitative\nanalyses. Our findings show notable biases in both models, reflecting distinct\nideological perspectives and cultural influences. However, despite these\nbiases, for a set of questions, the models' responses are more aligned than\nexpected, indicating that they can address sensitive topics without necessarily\npresenting directly opposing viewpoints. This study highlights the potential of\nLLMs to shape public discourse and underscores the importance of critically\nassessing AI-generated content, particularly in politically sensitive contexts.", "published": "2025-03-20 19:53:10", "link": "http://arxiv.org/abs/2503.16679v1", "categories": ["cs.CY", "cs.AI", "cs.HC"], "primary_category": "cs.CY"}
{"title": "Accelerating Transformer Inference and Training with 2:4 Activation Sparsity", "abstract": "In this paper, we demonstrate how to leverage 2:4 sparsity, a popular\nhardware-accelerated GPU sparsity pattern, to activations to accelerate large\nlanguage model training and inference. Crucially we exploit the intrinsic\nsparsity found in Squared-ReLU activations to provide this acceleration with no\naccuracy loss. Our approach achieves up to 1.3x faster Feed Forward Network\n(FFNs) in both the forwards and backwards pass. This work highlights the\npotential for sparsity to play a key role in accelerating large language model\ntraining and inference.", "published": "2025-03-20 19:37:12", "link": "http://arxiv.org/abs/2503.16672v1", "categories": ["cs.LG", "cs.AI", "I.2"], "primary_category": "cs.LG"}
{"title": "Leveraging OpenFlamingo for Multimodal Embedding Analysis of C2C Car Parts Data", "abstract": "In this paper, we aim to investigate the capabilities of multimodal machine\nlearning models, particularly the OpenFlamingo model, in processing a\nlarge-scale dataset of consumer-to-consumer (C2C) online posts related to car\nparts. We have collected data from two platforms, OfferUp and Craigslist,\nresulting in a dataset of over 1.2 million posts with their corresponding\nimages. The OpenFlamingo model was used to extract embeddings for the text and\nimage of each post. We used $k$-means clustering on the joint embeddings to\nidentify underlying patterns and commonalities among the posts. We have found\nthat most clusters contain a pattern, but some clusters showed no internal\npatterns. The results provide insight into the fact that OpenFlamingo can be\nused for finding patterns in large datasets but needs some modification in the\narchitecture according to the dataset.", "published": "2025-03-20 19:35:15", "link": "http://arxiv.org/abs/2503.17408v1", "categories": ["cs.LG", "cs.AI", "I.2.10; I.2.7; I.5; H.3.3; H.3.1"], "primary_category": "cs.LG"}
{"title": "Aligning Text-to-Music Evaluation with Human Preferences", "abstract": "Despite significant recent advances in generative acoustic text-to-music\n(TTM) modeling, robust evaluation of these models lags behind, relying in\nparticular on the popular Fr\\'echet Audio Distance (FAD). In this work, we\nrigorously study the design space of reference-based divergence metrics for\nevaluating TTM models through (1) designing four synthetic meta-evaluations to\nmeasure sensitivity to particular musical desiderata, and (2) collecting and\nevaluating on MusicPrefs, the first open-source dataset of human preferences\nfor TTM systems. We find that not only is the standard FAD setup inconsistent\non both synthetic and human preference data, but that nearly all existing\nmetrics fail to effectively capture desiderata, and are only weakly correlated\nwith human perception. We propose a new metric, the MAUVE Audio Divergence\n(MAD), computed on representations from a self-supervised audio embedding\nmodel. We find that this metric effectively captures diverse musical desiderata\n(average rank correlation 0.84 for MAD vs. 0.49 for FAD and also correlates\nmore strongly with MusicPrefs (0.62 vs. 0.14).", "published": "2025-03-20 19:31:04", "link": "http://arxiv.org/abs/2503.16669v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Code Evolution Graphs: Understanding Large Language Model Driven Design of Algorithms", "abstract": "Large Language Models (LLMs) have demonstrated great promise in generating\ncode, especially when used inside an evolutionary computation framework to\niteratively optimize the generated algorithms. However, in some cases they fail\nto generate competitive algorithms or the code optimization stalls, and we are\nleft with no recourse because of a lack of understanding of the generation\nprocess and generated codes. We present a novel approach to mitigate this\nproblem by enabling users to analyze the generated codes inside the\nevolutionary process and how they evolve over repeated prompting of the LLM. We\nshow results for three benchmark problem classes and demonstrate novel\ninsights. In particular, LLMs tend to generate more complex code with repeated\nprompting, but additional complexity can hurt algorithmic performance in some\ncases. Different LLMs have different coding ``styles'' and generated code tends\nto be dissimilar to other LLMs. These two findings suggest that using different\nLLMs inside the code evolution frameworks might produce higher performing code\nthan using only one LLM.", "published": "2025-03-20 19:30:22", "link": "http://arxiv.org/abs/2503.16668v1", "categories": ["cs.NE", "cs.AI"], "primary_category": "cs.NE"}
{"title": "Input-Triggered Hardware Trojan Attack on Spiking Neural Networks", "abstract": "Neuromorphic computing based on spiking neural networks (SNNs) is emerging as\na promising alternative to traditional artificial neural networks (ANNs),\noffering unique advantages in terms of low power consumption. However, the\nsecurity aspect of SNNs is under-explored compared to their ANN counterparts.\nAs the increasing reliance on AI systems comes with unique security risks and\nchallenges, understanding the vulnerabilities and threat landscape is essential\nas neuromorphic computing matures. In this effort, we propose a novel\ninput-triggered Hardware Trojan (HT) attack for SNNs. The HT mechanism is\ncondensed in the area of one neuron. The trigger mechanism is an input message\ncrafted in the spiking domain such that a selected neuron produces a malicious\nspike train that is not met in normal settings. This spike train triggers a\nmalicious modification in the neuron that forces it to saturate, firing\npermanently and failing to recover to its resting state even when the input\nactivity stops. The excessive spikes pollute the network and produce misleading\ndecisions. We propose a methodology to select an appropriate neuron and to\ngenerate the input pattern that triggers the HT payload. The attack is\nillustrated by simulation on three popular benchmarks in the neuromorphic\ncommunity. We also propose a hardware implementation for an analog spiking\nneuron and a digital SNN accelerator, demonstrating that the HT has a\nnegligible area and power footprint and, thereby, can easily evade detection.", "published": "2025-03-20 19:24:30", "link": "http://arxiv.org/abs/2503.21793v1", "categories": ["cs.NE", "cs.AI"], "primary_category": "cs.NE"}
{"title": "MobilePlantViT: A Mobile-friendly Hybrid ViT for Generalized Plant Disease Image Classification", "abstract": "Plant diseases significantly threaten global food security by reducing crop\nyields and undermining agricultural sustainability. AI-driven automated\nclassification has emerged as a promising solution, with deep learning models\ndemonstrating impressive performance in plant disease identification. However,\ndeploying these models on mobile and edge devices remains challenging due to\nhigh computational demands and resource constraints, highlighting the need for\nlightweight, accurate solutions for accessible smart agriculture systems. To\naddress this, we propose MobilePlantViT, a novel hybrid Vision Transformer\n(ViT) architecture designed for generalized plant disease classification, which\noptimizes resource efficiency while maintaining high performance. Extensive\nexperiments across diverse plant disease datasets of varying scales show our\nmodel's effectiveness and strong generalizability, achieving test accuracies\nranging from 80% to over 99%. Notably, with only 0.69 million parameters, our\narchitecture outperforms the smallest versions of MobileViTv1 and MobileViTv2,\ndespite their higher parameter counts. These results underscore the potential\nof our approach for real-world, AI-powered automated plant disease\nclassification in sustainable and resource-efficient smart agriculture systems.\nAll codes will be available in the GitHub repository:\nhttps://github.com/moshiurtonmoy/MobilePlantViT", "published": "2025-03-20 18:34:02", "link": "http://arxiv.org/abs/2503.16628v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Recipe for Generating 3D Worlds From a Single Image", "abstract": "We introduce a recipe for generating immersive 3D worlds from a single image\nby framing the task as an in-context learning problem for 2D inpainting models.\nThis approach requires minimal training and uses existing generative models.\nOur process involves two steps: generating coherent panoramas using a\npre-trained diffusion model and lifting these into 3D with a metric depth\nestimator. We then fill unobserved regions by conditioning the inpainting model\non rendered point clouds, requiring minimal fine-tuning. Tested on both\nsynthetic and real images, our method produces high-quality 3D environments\nsuitable for VR display. By explicitly modeling the 3D structure of the\ngenerated environment from the start, our approach consistently outperforms\nstate-of-the-art, video synthesis-based methods along multiple quantitative\nimage quality metrics. Project Page: https://katjaschwarz.github.io/worlds/", "published": "2025-03-20 18:06:12", "link": "http://arxiv.org/abs/2503.16611v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance", "abstract": "Recent advances in video generation have led to remarkable improvements in\nvisual quality and temporal coherence. Upon this, trajectory-controllable video\ngeneration has emerged to enable precise object motion control through\nexplicitly defined spatial paths. However, existing methods struggle with\ncomplex object movements and multi-object motion control, resulting in\nimprecise trajectory adherence, poor object consistency, and compromised visual\nquality. Furthermore, these methods only support trajectory control in a single\nformat, limiting their applicability in diverse scenarios. Additionally, there\nis no publicly available dataset or benchmark specifically tailored for\ntrajectory-controllable video generation, hindering robust training and\nsystematic evaluation. To address these challenges, we introduce MagicMotion, a\nnovel image-to-video generation framework that enables trajectory control\nthrough three levels of conditions from dense to sparse: masks, bounding boxes,\nand sparse boxes. Given an input image and trajectories, MagicMotion seamlessly\nanimates objects along defined trajectories while maintaining object\nconsistency and visual quality. Furthermore, we present MagicData, a\nlarge-scale trajectory-controlled video dataset, along with an automated\npipeline for annotation and filtering. We also introduce MagicBench, a\ncomprehensive benchmark that assesses both video quality and trajectory control\naccuracy across different numbers of objects. Extensive experiments demonstrate\nthat MagicMotion outperforms previous methods across various metrics. Our\nproject page are publicly available at\nhttps://quanhaol.github.io/magicmotion-site.", "published": "2025-03-20 17:59:42", "link": "http://arxiv.org/abs/2503.16421v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "DreamTexture: Shape from Virtual Texture with Analysis by Augmentation", "abstract": "DreamFusion established a new paradigm for unsupervised 3D reconstruction\nfrom virtual views by combining advances in generative models and\ndifferentiable rendering. However, the underlying multi-view rendering, along\nwith supervision from large-scale generative models, is computationally\nexpensive and under-constrained. We propose DreamTexture, a novel\nShape-from-Virtual-Texture approach that leverages monocular depth cues to\nreconstruct 3D objects. Our method textures an input image by aligning a\nvirtual texture with the real depth cues in the input, exploiting the inherent\nunderstanding of monocular geometry encoded in modern diffusion models. We then\nreconstruct depth from the virtual texture deformation with a new conformal map\noptimization, which alleviates memory-intensive volumetric representations. Our\nexperiments reveal that generative models possess an understanding of monocular\nshape cues, which can be extracted by augmenting and aligning texture cues -- a\nnovel monocular reconstruction paradigm that we call Analysis by Augmentation.", "published": "2025-03-20 17:59:12", "link": "http://arxiv.org/abs/2503.16412v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "RoboFactory: Exploring Embodied Agent Collaboration with Compositional Constraints", "abstract": "Designing effective embodied multi-agent systems is critical for solving\ncomplex real-world tasks across domains. Due to the complexity of multi-agent\nembodied systems, existing methods fail to automatically generate safe and\nefficient training data for such systems. To this end, we propose the concept\nof compositional constraints for embodied multi-agent systems, addressing the\nchallenges arising from collaboration among embodied agents. We design various\ninterfaces tailored to different types of constraints, enabling seamless\ninteraction with the physical world. Leveraging compositional constraints and\nspecifically designed interfaces, we develop an automated data collection\nframework for embodied multi-agent systems and introduce the first benchmark\nfor embodied multi-agent manipulation, RoboFactory. Based on RoboFactory\nbenchmark, we adapt and evaluate the method of imitation learning and analyzed\nits performance in different difficulty agent tasks. Furthermore, we explore\nthe architectures and training strategies for multi-agent imitation learning,\naiming to build safe and efficient embodied multi-agent systems.", "published": "2025-03-20 17:58:38", "link": "http://arxiv.org/abs/2503.16408v1", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "SA-Occ: Satellite-Assisted 3D Occupancy Prediction in Real World", "abstract": "Existing vision-based 3D occupancy prediction methods are inherently limited\nin accuracy due to their exclusive reliance on street-view imagery, neglecting\nthe potential benefits of incorporating satellite views. We propose SA-Occ, the\nfirst Satellite-Assisted 3D occupancy prediction model, which leverages GPS &\nIMU to integrate historical yet readily available satellite imagery into\nreal-time applications, effectively mitigating limitations of ego-vehicle\nperceptions, involving occlusions and degraded performance in distant regions.\nTo address the core challenges of cross-view perception, we propose: 1)\nDynamic-Decoupling Fusion, which resolves inconsistencies in dynamic regions\ncaused by the temporal asynchrony between satellite and street views; 2)\n3D-Proj Guidance, a module that enhances 3D feature extraction from inherently\n2D satellite imagery; and 3) Uniform Sampling Alignment, which aligns the\nsampling density between street and satellite views. Evaluated on\nOcc3D-nuScenes, SA-Occ achieves state-of-the-art performance, especially among\nsingle-frame methods, with a 39.05% mIoU (a 6.97% improvement), while incurring\nonly 6.93 ms of additional latency per frame. Our code and newly curated\ndataset are available at https://github.com/chenchen235/SA-Occ.", "published": "2025-03-20 17:54:29", "link": "http://arxiv.org/abs/2503.16399v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Graph of Effort: Quantifying Risk of AI Usage for Vulnerability Assessment", "abstract": "With AI-based software becoming widely available, the risk of exploiting its\ncapabilities, such as high automation and complex pattern recognition, could\nsignificantly increase. An AI used offensively to attack non-AI assets is\nreferred to as offensive AI.\n  Current research explores how offensive AI can be utilized and how its usage\ncan be classified. Additionally, methods for threat modeling are being\ndeveloped for AI-based assets within organizations. However, there are gaps\nthat need to be addressed. Firstly, there is a need to quantify the factors\ncontributing to the AI threat. Secondly, there is a requirement to create\nthreat models that analyze the risk of being attacked by AI for vulnerability\nassessment across all assets of an organization. This is particularly crucial\nand challenging in cloud environments, where sophisticated infrastructure and\naccess control landscapes are prevalent. The ability to quantify and further\nanalyze the threat posed by offensive AI enables analysts to rank\nvulnerabilities and prioritize the implementation of proactive countermeasures.\n  To address these gaps, this paper introduces the Graph of Effort, an\nintuitive, flexible, and effective threat modeling method for analyzing the\neffort required to use offensive AI for vulnerability exploitation by an\nadversary. While the threat model is functional and provides valuable support,\nits design choices need further empirical validation in future work.", "published": "2025-03-20 17:52:42", "link": "http://arxiv.org/abs/2503.16392v2", "categories": ["cs.CR", "cs.AI", "cs.DC"], "primary_category": "cs.CR"}
{"title": "Attentional Triple-Encoder Network in Spatiospectral Domains for Medical Image Segmentation", "abstract": "Retinal Optical Coherence Tomography (OCT) segmentation is essential for\ndiagnosing pathology. Traditional methods focus on either spatial or spectral\ndomains, overlooking their combined dependencies. We propose a triple-encoder\nnetwork that integrates CNNs for spatial features, Fast Fourier Convolution\n(FFC) for spectral features, and attention mechanisms to capture global\nrelationships across both domains. Attention fusion modules integrate\nconvolution and cross-attention to further enhance features. Our method\nachieves an average Dice score improvement from 0.855 to 0.864, outperforming\nprior work.", "published": "2025-03-20 17:49:01", "link": "http://arxiv.org/abs/2503.16389v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Deconstructing Long Chain-of-Thought: A Structured Reasoning Optimization Framework for Long CoT Distillation", "abstract": "Recent advancements in large language models (LLMs) have demonstrated\nremarkable reasoning capabilities through long chain-of-thought (CoT)\nreasoning. The R1 distillation scheme has emerged as a promising approach for\ntraining cost-effective models with enhanced reasoning abilities. However, the\nunderlying mechanisms driving its effectiveness remain unclear. This study\nexamines the universality of distillation data and identifies key components\nthat enable the efficient transfer of long-chain reasoning capabilities in LLM\ndistillation. Our findings reveal that the effectiveness of long CoT reasoning\ndistillation from teacher models like Qwen-QwQ degrades significantly on\nnonhomologous models, challenging the assumed universality of current\ndistillation methods. To gain deeper insights into the structure and patterns\nof long CoT reasoning, we propose DLCoT (Deconstructing Long Chain-of-Thought),\na distillation data enhancement framework. DLCoT consists of three key steps:\n(1) data segmentation to decompose complex long CoT structures, (2)\nsimplification by eliminating unsolvable and redundant solutions, and (3)\noptimization of intermediate error states. Our approach significantly improves\nmodel performance and token efficiency, facilitating the development of\nhigh-performance LLMs.", "published": "2025-03-20 17:46:38", "link": "http://arxiv.org/abs/2503.16385v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Reinforcement Learning-based Heuristics to Guide Domain-Independent Dynamic Programming", "abstract": "Domain-Independent Dynamic Programming (DIDP) is a state-space search\nparadigm based on dynamic programming for combinatorial optimization. In its\ncurrent implementation, DIDP guides the search using user-defined dual bounds.\nReinforcement learning (RL) is increasingly being applied to combinatorial\noptimization problems and shares several key structures with DP, being\nrepresented by the Bellman equation and state-based transition systems. We\npropose using reinforcement learning to obtain a heuristic function to guide\nthe search in DIDP. We develop two RL-based guidance approaches: value-based\nguidance using Deep Q-Networks and policy-based guidance using Proximal Policy\nOptimization. Our experiments indicate that RL-based guidance significantly\noutperforms standard DIDP and problem-specific greedy heuristics with the same\nnumber of node expansions. Further, despite longer node evaluation times, RL\nguidance achieves better run-time performance than standard DIDP on three of\nfour benchmark domains.", "published": "2025-03-20 17:33:08", "link": "http://arxiv.org/abs/2503.16371v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI"}
{"title": "JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse", "abstract": "Recently, action-based decision-making in open-world environments has gained\nsignificant attention. Visual Language Action (VLA) models, pretrained on\nlarge-scale web datasets, have shown promise in decision-making tasks. However,\nprevious work has primarily focused on action post-training, often neglecting\nenhancements to the foundational model itself. In response, we introduce a\nnovel approach, Act from Visual Language Post-Training, which refines Visual\nLanguage Models (VLMs) through visual and linguistic guidance in a\nself-supervised manner. This enhancement improves the models' capabilities in\nworld knowledge, visual recognition, and spatial grounding in open-world\nenvironments. Following the above post-training paradigms, we obtain the first\nVLA models in Minecraft that can follow human instructions on over 1k different\natomic tasks, including crafting, smelting, cooking, mining, and killing. Our\nexperiments demonstrate that post-training on non-trajectory tasks leads to a\nsignificant 40% improvement over the best agent baseline on a diverse set of\natomic tasks. Furthermore, we demonstrate that our approach surpasses\ntraditional imitation learning-based policies in Minecraft, achieving\nstate-of-the-art performance. We have open-sourced the code, models, and\ndatasets to foster further research. The project page can be found in\nhttps://craftjarvis.github.io/JarvisVLA.", "published": "2025-03-20 17:21:58", "link": "http://arxiv.org/abs/2503.16365v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Neural Networks: According to the Principles of Grassmann Algebra", "abstract": "In this paper, we explore the algebra of quantum idempotents and the\nquantization of fermions which gives rise to a Hilbert space equal to the\nGrassmann algebra associated with the Lie algebra. Since idempotents carry\nrepresentations of the algebra under consideration, they form algebraic\nvarieties and smooth manifolds in the natural topology. In addition to the\nmotivation of linking up mathematical physics with machine learning, it is also\nshown that by using idempotents and invariant subspace of the corresponding\nalgebras, these representations encode and perhaps provide a probabilistic\ninterpretation of reasoning and relational paths in geometrical terms.", "published": "2025-03-20 17:21:23", "link": "http://arxiv.org/abs/2503.16364v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Palatable Conceptions of Disembodied Being: Terra Incognita in the Space of Possible Minds", "abstract": "Is it possible to articulate a conception of consciousness that is compatible\nwith the exotic characteristics of contemporary, disembodied AI systems, and\nthat can stand up to philosophical scrutiny? How would subjective time and\nselfhood show up for an entity that conformed to such a conception? Trying to\nanswer these questions, even metaphorically, stretches the language of\nconsciousness to breaking point. Ultimately, the attempt yields something like\nemptiness, in the Buddhist sense, and helps to undermine our dualistic\ninclinations towards subjectivity and selfhood.", "published": "2025-03-20 17:05:16", "link": "http://arxiv.org/abs/2503.16348v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "HiQ-Lip: The First Quantum-Classical Hierarchical Method for Global Lipschitz Constant Estimation of ReLU Networks", "abstract": "Estimating the global Lipschitz constant of neural networks is crucial for\nunderstanding and improving their robustness and generalization capabilities.\nHowever, precise calculations are NP-hard, and current semidefinite programming\n(SDP) methods face challenges such as high memory usage and slow processing\nspeeds. In this paper, we propose \\textbf{HiQ-Lip}, a hybrid quantum-classical\nhierarchical method that leverages Coherent Ising Machines (CIMs) to estimate\nthe global Lipschitz constant. We tackle the estimation by converting it into a\nQuadratic Unconstrained Binary Optimization (QUBO) problem and implement a\nmultilevel graph coarsening and refinement strategy to adapt to the constraints\nof contemporary quantum hardware. Our experimental evaluations on fully\nconnected neural networks demonstrate that HiQ-Lip not only provides estimates\ncomparable to state-of-the-art methods but also significantly accelerates the\ncomputation process. In specific tests involving two-layer neural networks with\n256 hidden neurons, HiQ-Lip doubles the solving speed and offers more accurate\nupper bounds than the existing best method, LiPopt. These findings highlight\nthe promising utility of small-scale quantum devices in advancing the\nestimation of neural network robustness.", "published": "2025-03-20 16:58:40", "link": "http://arxiv.org/abs/2503.16342v1", "categories": ["cs.LG", "cs.AI", "quant-ph"], "primary_category": "cs.LG"}
{"title": "Enhancing Software Quality Assurance with an Adaptive Differential Evolution based Quantum Variational Autoencoder-Transformer Model", "abstract": "An AI-powered quality engineering platform uses artificial intelligence to\nboost software quality assessments through automated defect prediction and\noptimized performance alongside improved feature extraction. Existing models\nresult in difficulties addressing noisy data types together with imbalances,\npattern recognition complexities, ineffective feature extraction, and\ngeneralization weaknesses. To overcome those existing challenges in this\nresearch, we develop a new model Adaptive Differential Evolution based Quantum\nVariational Autoencoder-Transformer Model (ADE-QVAET), that combines a Quantum\nVariational Autoencoder-Transformer (QVAET) to obtain high-dimensional latent\nfeatures and maintain sequential dependencies together with contextual\nrelationships, resulting in superior defect prediction accuracy. Adaptive\nDifferential Evolution (ADE) Optimization utilizes an adaptive parameter tuning\nmethod that enhances model convergence and predictive performance. ADE-QVAET\nintegrates advanced AI techniques to create a robust solution for scalable and\naccurate software defect prediction that represents a top-level AI-driven\ntechnology for quality engineering applications. The proposed ADE-QVAET model\nattains high accuracy, precision, recall, and f1-score during the training\npercentage (TP) 90 of 98.08%, 92.45%, 94.67%, and 98.12%.", "published": "2025-03-20 16:55:38", "link": "http://arxiv.org/abs/2503.16335v1", "categories": ["cs.AI", "cs.ET"], "primary_category": "cs.AI"}
{"title": "Knowledge-guided machine learning model with soil moisture for corn yield prediction under drought conditions", "abstract": "Remote sensing (RS) techniques, by enabling non-contact acquisition of\nextensive ground observations, have become a valuable tool for corn yield\nprediction. Traditional process-based (PB) models are limited by fixed input\nfeatures and struggle to incorporate large volumes of RS data. In contrast,\nmachine learning (ML) models are often criticized for being ``black boxes''\nwith limited interpretability. To address these limitations, we used\nKnowledge-Guided Machine Learning (KGML), which combined the strengths of both\napproaches and fully used RS data. However, previous KGML methods overlooked\nthe crucial role of soil moisture in plant growth. To bridge this gap, we\nproposed the Knowledge-Guided Machine Learning with Soil Moisture (KGML-SM)\nframework, using soil moisture as an intermediate variable to emphasize its key\nrole in plant development. Additionally, based on the prior knowledge that the\nmodel may overestimate under drought conditions, we designed a drought-aware\nloss function that penalizes predicted yield in drought-affected areas. Our\nexperiments showed that the KGML-SM model outperformed other ML models.\nFinally, we explored the relationships between drought, soil moisture, and corn\nyield prediction, assessing the importance of various features and analyzing\nhow soil moisture impacts corn yield predictions across different regions and\ntime periods.", "published": "2025-03-20 16:52:25", "link": "http://arxiv.org/abs/2503.16328v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "OmniGeo: Towards a Multimodal Large Language Models for Geospatial Artificial Intelligence", "abstract": "The rapid advancement of multimodal large language models (LLMs) has opened\nnew frontiers in artificial intelligence, enabling the integration of diverse\nlarge-scale data types such as text, images, and spatial information. In this\npaper, we explore the potential of multimodal LLMs (MLLM) for geospatial\nartificial intelligence (GeoAI), a field that leverages spatial data to address\nchallenges in domains including Geospatial Semantics, Health Geography, Urban\nGeography, Urban Perception, and Remote Sensing. We propose a MLLM (OmniGeo)\ntailored to geospatial applications, capable of processing and analyzing\nheterogeneous data sources, including satellite imagery, geospatial metadata,\nand textual descriptions. By combining the strengths of natural language\nunderstanding and spatial reasoning, our model enhances the ability of\ninstruction following and the accuracy of GeoAI systems. Results demonstrate\nthat our model outperforms task-specific models and existing LLMs on diverse\ngeospatial tasks, effectively addressing the multimodality nature while\nachieving competitive results on the zero-shot geospatial tasks. Our code will\nbe released after publication.", "published": "2025-03-20 16:45:48", "link": "http://arxiv.org/abs/2503.16326v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Structured-Noise Masked Modeling for Video, Audio and Beyond", "abstract": "Masked modeling has emerged as a powerful self-supervised learning framework,\nbut existing methods largely rely on random masking, disregarding the\nstructural properties of different modalities. In this work, we introduce\nstructured noise-based masking, a simple yet effective approach that naturally\naligns with the spatial, temporal, and spectral characteristics of video and\naudio data. By filtering white noise into distinct color noise distributions,\nwe generate structured masks that preserve modality-specific patterns without\nrequiring handcrafted heuristics or access to the data. Our approach improves\nthe performance of masked video and audio modeling frameworks without any\ncomputational overhead. Extensive experiments demonstrate that structured noise\nmasking achieves consistent improvement over random masking for standard and\nadvanced masked modeling methods, highlighting the importance of modality-aware\nmasking strategies for representation learning.", "published": "2025-03-20 16:34:14", "link": "http://arxiv.org/abs/2503.16311v1", "categories": ["cs.LG", "cs.AI", "cs.SD"], "primary_category": "cs.LG"}
{"title": "Speeding up design and making to reduce time-to-project and time-to-market: an AI-Enhanced approach in engineering education", "abstract": "This paper explores the integration of AI tools, such as ChatGPT and GitHub\nCopilot, in the Software Architecture for Embedded Systems course. AI-supported\nworkflows enabled students to rapidly prototype complex projects, emphasizing\nreal-world applications like SLAM robotics. Results demon-started enhanced\nproblem-solving, faster development, and more sophisticated outcomes, with AI\naugmenting but not replacing human decision-making.", "published": "2025-03-20 16:32:13", "link": "http://arxiv.org/abs/2503.16307v1", "categories": ["cs.AI", "I.2; K.3"], "primary_category": "cs.AI"}
{"title": "Bridging Technology and Humanities: Evaluating the Impact of Large Language Models on Social Sciences Research with DeepSeek-R1", "abstract": "In recent years, the development of Large Language Models (LLMs) has made\nsignificant breakthroughs in the field of natural language processing and has\ngradually been applied to the field of humanities and social sciences research.\nLLMs have a wide range of application value in the field of humanities and\nsocial sciences because of its strong text understanding, generation and\nreasoning capabilities. In humanities and social sciences research, LLMs can\nanalyze large-scale text data and make inferences.\n  This article analyzes the large language model DeepSeek-R1 from seven\naspects: low-resource language translation, educational question-answering,\nstudent writing improvement in higher education, logical reasoning, educational\nmeasurement and psychometrics, public health policy analysis, and art\neducation.Then we compare the answers given by DeepSeek-R1 in the seven aspects\nwith the answers given by o1-preview. DeepSeek-R1 performs well in the\nhumanities and social sciences, answering most questions correctly and\nlogically, and can give reasonable analysis processes and explanations.\nCompared with o1-preview, it can automatically generate reasoning processes and\nprovide more detailed explanations, which is suitable for beginners or people\nwho need to have a detailed understanding of this knowledge, while o1-preview\nis more suitable for quick reading.\n  Through analysis, it is found that LLM has broad application potential in the\nfield of humanities and social sciences, and shows great advantages in\nimproving text analysis efficiency, language communication and other fields.\nLLM's powerful language understanding and generation capabilities enable it to\ndeeply explore complex problems in the field of humanities and social sciences,\nand provide innovative tools for academic research and practical applications.", "published": "2025-03-20 16:25:24", "link": "http://arxiv.org/abs/2503.16304v2", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY"}
{"title": "Unleashing Vecset Diffusion Model for Fast Shape Generation", "abstract": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.", "published": "2025-03-20 16:23:44", "link": "http://arxiv.org/abs/2503.16302v2", "categories": ["cs.CV", "cs.AI", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Diffusion-augmented Graph Contrastive Learning for Collaborative Filter", "abstract": "Graph-based collaborative filtering has been established as a prominent\napproach in recommendation systems, leveraging the inherent graph topology of\nuser-item interactions to model high-order connectivity patterns and enhance\nrecommendation performance. Recent advances in Graph Contrastive Learning (GCL)\nhave demonstrated promising potential to alleviate data sparsity issues by\nimproving representation learning through contrastive view generation and\nmutual information maximization. However, existing approaches lack effective\ndata augmentation strategies. Structural augmentation risks distorting\nfundamental graph topology, while feature-level perturbation techniques\npredominantly employ uniform noise scales that fail to account for\nnode-specific characteristics. To solve these challenges, we propose\nDiffusion-augmented Contrastive Learning (DGCL), an innovative framework that\nintegrates diffusion models with contrastive learning for enhanced\ncollaborative filtering. Our approach employs a diffusion process that learns\nnode-specific Gaussian distributions of representations, thereby generating\nsemantically consistent yet diversified contrastive views through reverse\ndiffusion sampling. DGCL facilitates adaptive data augmentation based on\nreconstructed representations, considering both semantic coherence and\nnode-specific features. In addition, it explores unrepresented regions of the\nlatent sparse feature space, thereby enriching the diversity of contrastive\nviews. Extensive experimental results demonstrate the effectiveness of DGCL on\nthree public datasets.", "published": "2025-03-20 16:15:20", "link": "http://arxiv.org/abs/2503.16290v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "AI Agents in Cryptoland: Practical Attacks and No Silver Bullet", "abstract": "The integration of AI agents with Web3 ecosystems harnesses their\ncomplementary potential for autonomy and openness, yet also introduces\nunderexplored security risks, as these agents dynamically interact with\nfinancial protocols and immutable smart contracts. This paper investigates the\nvulnerabilities of AI agents within blockchain-based financial ecosystems when\nexposed to adversarial threats in real-world scenarios. We introduce the\nconcept of context manipulation -- a comprehensive attack vector that exploits\nunprotected context surfaces, including input channels, memory modules, and\nexternal data feeds. Through empirical analysis of ElizaOS, a decentralized AI\nagent framework for automated Web3 operations, we demonstrate how adversaries\ncan manipulate context by injecting malicious instructions into prompts or\nhistorical interaction records, leading to unintended asset transfers and\nprotocol violations which could be financially devastating. Our findings\nindicate that prompt-based defenses are insufficient, as malicious inputs can\ncorrupt an agent's stored context, creating cascading vulnerabilities across\ninteractions and platforms. This research highlights the urgent need to develop\nAI agents that are both secure and fiduciarily responsible.", "published": "2025-03-20 15:44:31", "link": "http://arxiv.org/abs/2503.16248v1", "categories": ["cs.CR", "cs.AI", "I.2.7"], "primary_category": "cs.CR"}
{"title": "Flight Testing an Optionally Piloted Aircraft: a Case Study on Trust Dynamics in Human-Autonomy Teaming", "abstract": "This paper examines how trust is formed, maintained, or diminished over time\nin the context of human-autonomy teaming with an optionally piloted aircraft.\nWhereas traditional factor-based trust models offer a static representation of\nhuman confidence in technology, here we discuss how variations in the\nunderlying factors lead to variations in trust, trust thresholds, and human\nbehaviours. Over 200 hours of flight test data collected over a multi-year test\ncampaign from 2021 to 2023 were reviewed. The\ndispositional-situational-learned, process-performance-purpose, and IMPACTS\nhomeostasis trust models are applied to illuminate trust trends during nominal\nautonomous flight operations. The results offer promising directions for future\nstudies on trust dynamics and design-for-trust in human-autonomy teaming.", "published": "2025-03-20 15:22:39", "link": "http://arxiv.org/abs/2503.16227v1", "categories": ["cs.HC", "cs.AI", "cs.ET", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "cs.HC"}
{"title": "Logic Explanation of AI Classifiers by Categorical Explaining Functors", "abstract": "The most common methods in explainable artificial intelligence are post-hoc\ntechniques which identify the most relevant features used by pretrained opaque\nmodels. Some of the most advanced post hoc methods can generate explanations\nthat account for the mutual interactions of input features in the form of logic\nrules. However, these methods frequently fail to guarantee the consistency of\nthe extracted explanations with the model's underlying reasoning. To bridge\nthis gap, we propose a theoretically grounded approach to ensure coherence and\nfidelity of the extracted explanations, moving beyond the limitations of\ncurrent heuristic-based approaches. To this end, drawing from category theory,\nwe introduce an explaining functor which structurally preserves logical\nentailment between the explanation and the opaque model's reasoning. As a proof\nof concept, we validate the proposed theoretical constructions on a synthetic\nbenchmark verifying how the proposed approach significantly mitigates the\ngeneration of contradictory or unfaithful explanations.", "published": "2025-03-20 14:50:06", "link": "http://arxiv.org/abs/2503.16203v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Large Language Models for Water Distribution Systems Modeling and Decision-Making", "abstract": "The design, operations, and management of water distribution systems (WDS)\ninvolve complex mathematical models. These models are continually improving due\nto computational advancements, leading to better decision-making and more\nefficient WDS management. However, the significant time and effort required for\nmodeling, programming, and analyzing results remain substantial challenges.\nAnother issue is the professional burden, which confines the interaction with\nmodels, databases, and other sophisticated tools to a small group of experts,\nthereby causing non-technical stakeholders to depend on these experts or make\ndecisions without modeling support. Furthermore, explaining model results is\nchallenging even for experts, as it is often unclear which conditions cause the\nmodel to reach a certain state or recommend a specific policy. The recent\nadvancements in Large Language Models (LLMs) open doors for a new stage in\nhuman-model interaction. This study proposes a framework of plain language\ninteractions with hydraulic and water quality models based on LLM-EPANET\narchitecture. This framework is tested with increasing levels of complexity of\nqueries to study the ability of LLMs to interact with WDS models, run complex\nsimulations, and report simulation results. The performance of the proposed\nframework is evaluated across several categories of queries and hyper-parameter\nconfigurations, demonstrating its potential to enhance decision-making\nprocesses in WDS management.", "published": "2025-03-20 14:39:11", "link": "http://arxiv.org/abs/2503.16191v1", "categories": ["cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Explainable AI-Guided Efficient Approximate DNN Generation for Multi-Pod Systolic Arrays", "abstract": "Approximate deep neural networks (AxDNNs) are promising for enhancing energy\nefficiency in real-world devices. One of the key contributors behind this\nenhanced energy efficiency in AxDNNs is the use of approximate multipliers.\nUnfortunately, the simulation of approximate multipliers does not usually scale\nwell on CPUs and GPUs. As a consequence, this slows down the overall simulation\nof AxDNNs aimed at identifying the appropriate approximate multipliers to\nachieve high energy efficiency with a minimum accuracy loss. To address this\nproblem, we present a novel XAI-Gen methodology, which leverages the analytical\nmodel of the emerging hardware accelerator (e.g., Google TPU v4) and\nexplainable artificial intelligence (XAI) to precisely identify the\nnon-critical layers for approximation and quickly discover the appropriate\napproximate multipliers for AxDNN layers. Our results show that XAI-Gen\nachieves up to 7x lower energy consumption with only 1-2% accuracy loss. We\nalso showcase the effectiveness of the XAI-Gen approach through a neural\narchitecture search (XAI-NAS) case study. Interestingly, XAI-NAS achieves 40\\%\nhigher energy efficiency with up to 5x less execution time when compared to the\nstate-of-the-art NAS methods for generating AxDNNs.", "published": "2025-03-20 14:26:47", "link": "http://arxiv.org/abs/2503.16583v1", "categories": ["cs.LG", "cs.AI", "cs.AR"], "primary_category": "cs.LG"}
{"title": "Neural Combinatorial Optimization for Real-World Routing", "abstract": "Vehicle Routing Problems (VRPs) are a class of NP-hard problems ubiquitous in\nseveral real-world logistics scenarios that pose significant challenges for\noptimization. Neural Combinatorial Optimization (NCO) has emerged as a\npromising alternative to classical approaches, as it can learn fast heuristics\nto solve VRPs. However, most research works in NCO for VRPs focus on simplified\nsettings, which do not account for asymmetric distances and travel durations\nthat cannot be derived by simple Euclidean distances and unrealistic data\ndistributions, hindering real-world deployment. This work introduces RRNCO\n(Real Routing NCO) to bridge the gap of NCO between synthetic and real-world\nVRPs in the critical aspects of both data and modeling. First, we introduce a\nnew, openly available dataset with real-world data containing a diverse dataset\nof locations, distances, and duration matrices from 100 cities, considering\nrealistic settings with actual routing distances and durations obtained from\nOpen Source Routing Machine (OSRM). Second, we propose a novel approach that\nefficiently processes both node and edge features through contextual gating,\nenabling the construction of more informed node embedding, and we finally\nincorporate an Adaptation Attention Free Module (AAFM) with neural adaptive\nbias mechanisms that effectively integrates not only distance matrices but also\nangular relationships between nodes, allowing our model to capture rich\nstructural information. RRNCO achieves state-of-the-art results in real-world\nVRPs among NCO methods. We make our dataset and code publicly available at\nhttps://github.com/ai4co/real-routing-nco.", "published": "2025-03-20 13:57:33", "link": "http://arxiv.org/abs/2503.16159v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Unify and Triumph: Polyglot, Diverse, and Self-Consistent Generation of Unit Tests with LLMs", "abstract": "Large language model (LLM)-based test generation has gained attention in\nsoftware engineering, yet most studies evaluate LLMs' ability to generate unit\ntests in a single attempt for a given language, missing the opportunity to\nleverage LLM diversity for more robust testing. This paper introduces PolyTest,\na novel approach that enhances test generation by exploiting polyglot and\ntemperature-controlled diversity. PolyTest systematically leverages these\nproperties in two complementary ways: (1) Cross-lingual test generation, where\ntests are generated in multiple languages at zero temperature and then unified;\n(2) Diverse test sampling, where multiple test sets are generated within the\nsame language at a higher temperature before unification. A key insight is that\nLLMs can generate diverse yet contradicting tests -- same input, different\nexpected outputs -- across languages and generations. PolyTest mitigates\ninconsistencies by unifying test sets, fostering self-consistency and improving\noverall test quality. Unlike single-language or single-attempt approaches,\nPolyTest enhances testing without requiring on-the-fly execution, making it\nparticularly beneficial for weaker-performing languages. We evaluate PolyTest\non Llama3-70B, GPT-4o, and GPT-3.5 using EvalPlus, generating tests in five\nlanguages (Java, C, Python, JavaScript, and a CSV-based format) at temperature\n0 and sampling multiple sets at temperature 1. We observe that LLMs frequently\ngenerate contradicting tests across settings, and that PolyTest significantly\nimproves test quality across all considered metrics -- number of tests, passing\nrate, statement/branch coverage (up to +9.01%), and mutation score (up to\n+11.23%). Finally, PolyTest outperforms Pynguin in test generation, passing\nrate, and mutation score.", "published": "2025-03-20 13:47:06", "link": "http://arxiv.org/abs/2503.16144v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE"}
{"title": "Self-Learning-Based Optimization for Free-form Pipe Routing in Aeroengine with Dynamic Design Environment", "abstract": "Pipe routing is a highly complex, time-consuming, and no-deterministic\npolynomial-time hard (NP-hard) problem in aeroengine design. Despite extensive\nresearch efforts in optimizing constant-curvature pipe routing, the growing\ndemand for free-form pipes poses new challenges. Dynamic design environments\nand fuzzy layout rules further impact the optimization performance and\nefficiency. To tackle these challenges, this study proposes a\nself-learning-based method (SLPR) for optimizing free-form pipe routing in\naeroengines. The SLPR is based on the proximal policy optimization (PPO)\nalgorithm and integrates a unified rule modeling framework for efficient\nobstacle detection and fuzzy rule modeling in continuous space. Additionally, a\npotential energy table is constructed to enable rapid queries of layout\ntendencies and interference. The agent within SLPR iteratively refines pipe\nrouting and accumulates the design knowledge through interaction with the\nenvironment. Once the design environment shifts, the agent can swiftly adapt by\nfine-tuning network parameters. Comparative tests reveal that SLPR ensures\nsmooth pipe routing through cubic non-uniform B-spline (NURBS) curves, avoiding\nredundant pipe segments found in constant-curvature pipe routing. Results in\nboth static and dynamic design environments demonstrate that SLPR outperforms\nthree representative baselines in terms of the pipe length reduction, the\nadherence to layout rules, the path complexity, and the computational\nefficiency. Furthermore, tests in dynamic environments indicate that SLPR\neliminates labor-intensive searches from scratch and even yields superior\nsolutions compared to the retrained model. These results highlight the\npractical value of SLPR for real-world pipe routing, meeting lightweight,\nprecision, and sustainability requirements of the modern aeroengine design.", "published": "2025-03-20 13:45:13", "link": "http://arxiv.org/abs/2504.03669v1", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "J.0; J.6"], "primary_category": "cs.LG"}
{"title": "Machine Learning-Based Genomic Linguistic Analysis (Gene Sequence Feature Learning): A Case Study on Predicting Heavy Metal Response Genes in Rice", "abstract": "This study explores the application of machine learning-based genetic\nlinguistics for identifying heavy metal response genes in rice (Oryza sativa).\nBy integrating convolutional neural networks and random forest algorithms, we\ndeveloped a hybrid model capable of extracting and learning meaningful features\nfrom gene sequences, such as k-mer frequencies and physicochemical properties.\nThe model was trained and tested on datasets of genes, achieving high\npredictive performance (precision: 0.89, F1-score: 0.82). RNA-seq and qRT-PCR\nexperiments conducted on rice leaves which exposed to Hg0, revealed\ndifferential expression of genes associated with heavy metal responses, which\nvalidated the model's predictions. Co-expression network analysis identified\n103 related genes, and a literature review indicated that these genes are\nhighly likely to be involved in heavy metal-related biological processes. By\nintegrating and comparing the analysis results with those of differentially\nexpressed genes (DEGs), the validity of the new machine learning method was\nfurther demonstrated. This study highlights the efficacy of combining machine\nlearning with genetic linguistics for large-scale gene prediction. It\ndemonstrates a cost-effective and efficient approach for uncovering molecular\nmechanisms underlying heavy metal responses, with potential applications in\ndeveloping stress-tolerant crop varieties.", "published": "2025-03-20 13:41:31", "link": "http://arxiv.org/abs/2503.16582v1", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "primary_category": "cs.LG"}
{"title": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video Streaming", "abstract": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN).", "published": "2025-03-20 13:00:36", "link": "http://arxiv.org/abs/2503.16112v1", "categories": ["cs.NI", "cs.AI", "cs.MM"], "primary_category": "cs.NI"}
{"title": "AIMI: Leveraging Future Knowledge and Personalization in Sparse Event Forecasting for Treatment Adherence", "abstract": "Adherence to prescribed treatments is crucial for individuals with chronic\nconditions to avoid costly or adverse health outcomes. For certain patient\ngroups, intensive lifestyle interventions are vital for enhancing medication\nadherence. Accurate forecasting of treatment adherence can open pathways to\ndeveloping an on-demand intervention tool, enabling timely and personalized\nsupport. With the increasing popularity of smartphones and wearables, it is now\neasier than ever to develop and deploy smart activity monitoring systems.\nHowever, effective forecasting systems for treatment adherence based on\nwearable sensors are still not widely available. We close this gap by proposing\nAdherence Forecasting and Intervention with Machine Intelligence (AIMI). AIMI\nis a knowledge-guided adherence forecasting system that leverages smartphone\nsensors and previous medication history to estimate the likelihood of\nforgetting to take a prescribed medication. A user study was conducted with 27\nparticipants who took daily medications to manage their cardiovascular\ndiseases. We designed and developed CNN and LSTM-based forecasting models with\nvarious combinations of input features and found that LSTM models can forecast\nmedication adherence with an accuracy of 0.932 and an F-1 score of 0.936.\nMoreover, through a series of ablation studies involving convolutional and\nrecurrent neural network architectures, we demonstrate that leveraging known\nknowledge about future and personalized training enhances the accuracy of\nmedication adherence forecasting. Code available:\nhttps://github.com/ab9mamun/AIMI.", "published": "2025-03-20 12:32:35", "link": "http://arxiv.org/abs/2503.16091v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Allostatic Control of Persistent States in Spiking Neural Networks for perception and computation", "abstract": "We introduce a novel model for updating perceptual beliefs about the\nenvironment by extending the concept of Allostasis to the control of internal\nrepresentations. Allostasis is a fundamental regulatory mechanism observed in\nanimal physiology that orchestrates responses to maintain a dynamic equilibrium\nin bodily needs and internal states. In this paper, we focus on an application\nin numerical cognition, where a bump of activity in an attractor network is\nused as a spatial numerical representation. While existing neural networks can\nmaintain persistent states, to date, there is no unified framework for\ndynamically controlling spatial changes in neuronal activity in response to\nenvironmental changes. To address this, we couple a well known allostatic\nmicrocircuit, the Hammel model, with a ring attractor, resulting in a Spiking\nNeural Network architecture that can modulate the location of the bump as a\nfunction of some reference input. This localized activity in turn is used as a\nperceptual belief in a simulated subitization task a quick enumeration process\nwithout counting. We provide a general procedure to fine-tune the model and\ndemonstrate the successful control of the bump location. We also study the\nresponse time in the model with respect to changes in parameters and compare it\nwith biological data. Finally, we analyze the dynamics of the network to\nunderstand the selectivity and specificity of different neurons to distinct\ncategories present in the input. The results of this paper, particularly the\nmechanism for moving persistent states, are not limited to numerical cognition\nbut can be applied to a wide range of tasks involving similar representations.", "published": "2025-03-20 12:28:08", "link": "http://arxiv.org/abs/2503.16085v1", "categories": ["q-bio.NC", "cs.AI"], "primary_category": "q-bio.NC"}
{"title": "3-D Image-to-Image Fusion in Lightsheet Microscopy by Two-Step Adversarial Network: Contribution to the FuseMyCells Challenge", "abstract": "Lightsheet microscopy is a powerful 3-D imaging technique that addresses\nlimitations of traditional optical and confocal microscopy but suffers from a\nlow penetration depth and reduced image quality at greater depths. Multiview\nlightsheet microscopy improves 3-D resolution by combining multiple views but\nsimultaneously increasing the complexity and the photon budget, leading to\npotential photobleaching and phototoxicity. The FuseMyCells challenge,\norganized in conjunction with the IEEE ISBI 2025 conference, aims to benchmark\ndeep learning-based solutions for fusing high-quality 3-D volumes from single\n3-D views, potentially simplifying procedures and conserving the photon budget.\nIn this work, we propose a contribution to the FuseMyCells challenge based on a\ntwo-step procedure. The first step processes a downsampled version of the image\nto capture the entire region of interest, while the second step uses a\npatch-based approach for high-resolution inference, incorporating adversarial\nloss to enhance visual outcomes. This method addresses challenges related to\nhigh data resolution, the necessity of global context, and the preservation of\nhigh-frequency details. Experimental results demonstrate the effectiveness of\nour approach, highlighting its potential to improve 3-D image fusion quality\nand extend the capabilities of lightsheet microscopy. The average SSIM for the\nnucleus and membranes is greater than 0.85 and 0.91, respectively.", "published": "2025-03-20 12:12:01", "link": "http://arxiv.org/abs/2503.16075v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "eess.IV"}
{"title": "PromptHash: Affinity-Prompted Collaborative Cross-Modal Learning for Adaptive Hashing Retrieval", "abstract": "Cross-modal hashing is a promising approach for efficient data retrieval and\nstorage optimization. However, contemporary methods exhibit significant\nlimitations in semantic preservation, contextual integrity, and information\nredundancy, which constrains retrieval efficacy. We present PromptHash, an\ninnovative framework leveraging affinity prompt-aware collaborative learning\nfor adaptive cross-modal hashing. We propose an end-to-end framework for\naffinity-prompted collaborative hashing, with the following fundamental\ntechnical contributions: (i) a text affinity prompt learning mechanism that\npreserves contextual information while maintaining parameter efficiency, (ii)\nan adaptive gated selection fusion architecture that synthesizes State Space\nModel with Transformer network for precise cross-modal feature integration, and\n(iii) a prompt affinity alignment strategy that bridges modal heterogeneity\nthrough hierarchical contrastive learning. To the best of our knowledge, this\nstudy presents the first investigation into affinity prompt awareness within\ncollaborative cross-modal adaptive hash learning, establishing a paradigm for\nenhanced semantic consistency across modalities. Through comprehensive\nevaluation on three benchmark multi-label datasets, PromptHash demonstrates\nsubstantial performance improvements over existing approaches. Notably, on the\nNUS-WIDE dataset, our method achieves significant gains of 18.22% and 18.65% in\nimage-to-text and text-to-image retrieval tasks, respectively. The code is\npublicly available at https://github.com/ShiShuMo/PromptHash.", "published": "2025-03-20 11:56:27", "link": "http://arxiv.org/abs/2503.16064v1", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Expert Race: A Flexible Routing Strategy for Scaling Diffusion Transformer with Mixture of Experts", "abstract": "Diffusion models have emerged as mainstream framework in visual generation.\nBuilding upon this success, the integration of Mixture of Experts (MoE) methods\nhas shown promise in enhancing model scalability and performance. In this\npaper, we introduce Race-DiT, a novel MoE model for diffusion transformers with\na flexible routing strategy, Expert Race. By allowing tokens and experts to\ncompete together and select the top candidates, the model learns to dynamically\nassign experts to critical tokens. Additionally, we propose per-layer\nregularization to address challenges in shallow layer learning, and router\nsimilarity loss to prevent mode collapse, ensuring better expert utilization.\nExtensive experiments on ImageNet validate the effectiveness of our approach,\nshowcasing significant performance gains while promising scaling properties.", "published": "2025-03-20 11:45:08", "link": "http://arxiv.org/abs/2503.16057v2", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Temporal-Spatial Attention Network (TSAN) for DoS Attack Detection in Network Traffic", "abstract": "Denial-of-Service (DoS) attacks remain a critical threat to network security,\ndisrupting services and causing significant economic losses. Traditional\ndetection methods, including statistical and rule-based models, struggle to\nadapt to evolving attack patterns. To address this challenge, we propose a\nnovel Temporal-Spatial Attention Network (TSAN) architecture for detecting\nDenial of Service (DoS) attacks in network traffic. By leveraging both temporal\nand spatial features of network traffic, our approach captures complex traffic\npatterns and anomalies that traditional methods might miss. The TSAN model\nincorporates transformer-based temporal encoding, convolutional spatial\nencoding, and a cross-attention mechanism to fuse these complementary feature\nspaces. Additionally, we employ multi-task learning with auxiliary tasks to\nenhance the model's robustness. Experimental results on the NSL-KDD dataset\ndemonstrate that TSAN outperforms state-of-the-art models, achieving superior\naccuracy, precision, recall, and F1-score while maintaining computational\nefficiency for real-time deployment. The proposed architecture offers an\noptimal balance between detection accuracy and computational overhead, making\nit highly suitable for real-world network security applications.", "published": "2025-03-20 11:31:45", "link": "http://arxiv.org/abs/2503.16047v2", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "Open Science and Artificial Intelligence for supporting the sustainability of the SRC Network: The espSRC case", "abstract": "The SKA Observatory (SKAO), a landmark project in radio astronomy, seeks to\naddress fundamental questions in astronomy. To process its immense data output,\napproximately 700 PB/year, a global network of SKA Regional Centres (SR-CNet)\nwill provide the infrastructure, tools, computational power needed for\nscientific analysis and scientific support. The Spanish SRC (espSRC) focuses on\nensuring the sustainability of this network by reducing its environmental\nimpact, integrating green practices into data platforms, and developing Open\nScience technologies to enable reproducible research. This paper discusses and\nsummarizes part of the research and development activities that the team is\nconducting to reduce the SRC energy consumption at the espSRC and SRCNet. The\npaper also discusses fundamental research on trusted repositories to support\nOpen Science practices.", "published": "2025-03-20 11:29:00", "link": "http://arxiv.org/abs/2503.16045v1", "categories": ["astro-ph.IM", "cs.AI"], "primary_category": "astro-ph.IM"}
{"title": "GreenIQ: A Deep Search Platform for Comprehensive Carbon Market Analysis and Automated Report Generation", "abstract": "This study introduces GreenIQ, an AI-powered deep search platform designed to\nrevolutionise carbon market intelligence through autonomous analysis and\nautomated report generation. Carbon markets operate across diverse regulatory\nlandscapes, generating vast amounts of heterogeneous data from policy\ndocuments, industry reports, academic literature, and real-time trading\nplatforms. Traditional research approaches remain labour-intensive, slow, and\ndifficult to scale. GreenIQ addresses these limitations through a multi-agent\narchitecture powered by Large Language Models (LLMs), integrating five\nspecialised AI agents: a Main Researcher Agent for intelligent information\nretrieval, a Report Writing Agent for structured synthesis, a Final Reviewer\nAgent for accuracy verification, a Data Visualisation Agent for enhanced\ninterpretability, and a Translator Agent for multilingual adaptation. The\nsystem achieves seamless integration of structured and unstructured information\nwith AI-driven citation verification, ensuring high transparency and\nreliability. GreenIQ delivers a 99.2\\% reduction in processing time and a\n99.7\\% cost reduction compared to traditional research methodologies. A novel\nAI persona-based evaluation framework involving 16 domain-specific AI personas\nhighlights its superior cross-jurisdictional analytical capabilities and\nregulatory insight generation. GreenIQ sets new standards in AI-driven research\nsynthesis, policy analysis, and sustainability finance by streamlining carbon\nmarket research. It offers an efficient and scalable framework for\nenvironmental and financial intelligence, enabling more accurate, timely, and\ncost-effective decision-making in complex regulatory landscapes", "published": "2025-03-20 11:19:43", "link": "http://arxiv.org/abs/2503.16041v2", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Single Image Iterative Subject-driven Generation and Editing", "abstract": "Personalizing image generation and editing is particularly challenging when\nwe only have a few images of the subject, or even a single image. A common\napproach to personalization is concept learning, which can integrate the\nsubject into existing models relatively quickly, but produces images whose\nquality tends to deteriorate quickly when the number of subject images is\nsmall. Quality can be improved by pre-training an encoder, but training\nrestricts generation to the training distribution, and is time consuming. It is\nstill an open hard challenge to personalize image generation and editing from a\nsingle image without training. Here, we present SISO, a novel, training-free\napproach based on optimizing a similarity score with an input subject image.\nMore specifically, SISO iteratively generates images and optimizes the model\nbased on loss of similarity with the given subject image until a satisfactory\nlevel of similarity is achieved, allowing plug-and-play optimization to any\nimage generator. We evaluated SISO in two tasks, image editing and image\ngeneration, using a diverse data set of personal subjects, and demonstrate\nsignificant improvements over existing methods in image quality, subject\nfidelity, and background preservation.", "published": "2025-03-20 10:45:04", "link": "http://arxiv.org/abs/2503.16025v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Unifying EEG and Speech for Emotion Recognition: A Two-Step Joint Learning Framework for Handling Missing EEG Data During Inference", "abstract": "Computer interfaces are advancing towards using multi-modalities to enable\nbetter human-computer interactions. The use of automatic emotion recognition\n(AER) can make the interactions natural and meaningful thereby enhancing the\nuser experience. Though speech is the most direct and intuitive modality for\nAER, it is not reliable because it can be intentionally faked by humans. On the\nother hand, physiological modalities like EEG, are more reliable and impossible\nto fake. However, use of EEG is infeasible for realistic scenarios usage\nbecause of the need for specialized recording setup. In this paper, one of our\nprimary aims is to ride on the reliability of the EEG modality to facilitate\nrobust AER on the speech modality. Our approach uses both the modalities during\ntraining to reliably identify emotion at the time of inference, even in the\nabsence of the more reliable EEG modality. We propose, a two-step joint\nmulti-modal learning approach (JMML) that exploits both the intra- and inter-\nmodal characteristics to construct emotion embeddings that enrich the\nperformance of AER. In the first step, using JEC-SSL, intra-modal learning is\ndone independently on the individual modalities. This is followed by an\ninter-modal learning using the proposed extended variant of deep canonically\ncorrelated cross-modal autoencoder (E-DCC-CAE). The approach learns the joint\nproperties of both the modalities by mapping them into a common representation\nspace, such that the modalities are maximally correlated. These emotion\nembeddings, hold properties of both the modalities there by enhancing the\nperformance of ML classifier used for AER. Experimental results show the\nefficacy of the proposed approach. To best of our knowledge, this is the first\nattempt to combine speech and EEG with joint multi-modal learning approach for\nreliable AER.", "published": "2025-03-20 10:26:49", "link": "http://arxiv.org/abs/2503.18964v1", "categories": ["cs.SD", "cs.AI"], "primary_category": "cs.SD"}
{"title": "Feature selection strategies for optimized heart disease diagnosis using ML and DL models", "abstract": "Heart disease remains one of the leading causes of morbidity and mortality\nworldwide, necessitating the development of effective diagnostic tools to\nenable early diagnosis and clinical decision-making. This study evaluates the\nimpact of feature selection techniques Mutual Information (MI), Analysis of\nVariance (ANOVA), and Chi-Square on the predictive performance of various\nmachine learning (ML) and deep learning (DL) models using a dataset of clinical\nindicators for heart disease. Eleven ML/DL models were assessed using metrics\nsuch as precision, recall, AUC score, F1-score, and accuracy. Results indicate\nthat MI outperformed other methods, particularly for advanced models like\nneural networks, achieving the highest accuracy of 82.3% and recall score of\n0.94. Logistic regression (accuracy 82.1%) and random forest (accuracy 80.99%)\nalso demonstrated improved performance with MI. Simpler models such as Naive\nBayes and decision trees achieved comparable results with ANOVA and Chi-Square,\nyielding accuracies of 76.45% and 75.99%, respectively, making them\ncomputationally efficient alternatives. Conversely, k Nearest Neighbors (KNN)\nand Support Vector Machines (SVM) exhibited lower performance, with accuracies\nranging between 51.52% and 54.43%, regardless of the feature selection method.\nThis study provides a comprehensive comparison of feature selection methods for\nheart disease prediction, demonstrating the critical role of feature selection\nin optimizing model performance. The results offer practical guidance for\nselecting appropriate feature selection techniques based on the chosen\nclassification algorithm, contributing to the development of more accurate and\nefficient diagnostic tools for enhanced clinical decision-making in cardiology.", "published": "2025-03-20 09:59:01", "link": "http://arxiv.org/abs/2503.16577v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Exploring the Reliability of Self-explanation and its Relationship with Classification in Language Model-driven Financial Analysis", "abstract": "Language models (LMs) have exhibited exceptional versatility in reasoning and\nin-depth financial analysis through their proprietary information processing\ncapabilities. Previous research focused on evaluating classification\nperformance while often overlooking explainability or pre-conceived that\nrefined explanation corresponds to higher classification accuracy. Using a\npublic dataset in finance domain, we quantitatively evaluated self-explanations\nby LMs, focusing on their factuality and causality. We identified the\nstatistically significant relationship between the accuracy of classifications\nand the factuality or causality of self-explanations. Our study built an\nempirical foundation for approximating classification confidence through\nself-explanations and for optimizing classification via proprietary reasoning.", "published": "2025-03-20 09:33:59", "link": "http://arxiv.org/abs/2503.15985v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "AUV Acceleration Prediction Using DVL and Deep Learning", "abstract": "Autonomous underwater vehicles (AUVs) are essential for various applications,\nincluding oceanographic surveys, underwater mapping, and infrastructure\ninspections. Accurate and robust navigation are critical to completing these\ntasks. To this end, a Doppler velocity log (DVL) and inertial sensors are fused\ntogether. Recently, a model-based approach demonstrated the ability to extract\nthe vehicle acceleration vector from DVL velocity measurements. Motivated by\nthis advancement, in this paper we present an end-to-end deep learning approach\nto estimate the AUV acceleration vector based on past DVL velocity\nmeasurements. Based on recorded data from sea experiments, we demonstrate that\nthe proposed method improves acceleration vector estimation by more than 65%\ncompared to the model-based approach by using data-driven techniques. As a\nresult of our data-driven approach, we can enhance navigation accuracy and\nreliability in AUV applications, contributing to more efficient and effective\nunderwater missions through improved accuracy and reliability.", "published": "2025-03-20 09:33:47", "link": "http://arxiv.org/abs/2503.16573v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "DIPLI: Deep Image Prior Lucky Imaging for Blind Astronomical Image Restoration", "abstract": "Contemporary image restoration and super-resolution techniques effectively\nharness deep neural networks, markedly outperforming traditional methods.\nHowever, astrophotography presents unique challenges for deep learning due to\nlimited training data. This work explores hybrid strategies, such as the Deep\nImage Prior (DIP) model, which facilitates blind training but is susceptible to\noverfitting, artifact generation, and instability when handling noisy images.\nWe propose enhancements to the DIP model's baseline performance through several\nadvanced techniques. First, we refine the model to process multiple frames\nconcurrently, employing the Back Projection method and the TVNet model. Next,\nwe adopt a Markov approach incorporating Monte Carlo estimation, Langevin\ndynamics, and a variational input technique to achieve unbiased estimates with\nminimal variance and counteract overfitting effectively. Collectively, these\nmodifications reduce the likelihood of noise learning and mitigate loss\nfunction fluctuations during training, enhancing result stability. We validated\nour algorithm across multiple image sets of astronomical and celestial objects,\nachieving performance that not only mitigates limitations of Lucky Imaging, a\nclassical computer vision technique that remains a standard in astronomical\nimage reconstruction but surpasses the original DIP model, state of the art\ntransformer- and diffusion-based models, underscoring the significance of our\nimprovements.", "published": "2025-03-20 09:33:16", "link": "http://arxiv.org/abs/2503.15984v1", "categories": ["cs.CV", "astro-ph.IM", "cs.AI", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Beyond the Visible: Multispectral Vision-Language Learning for Earth Observation", "abstract": "Vision-language models for Earth observation (EO) typically rely on the\nvisual spectrum of data as the only model input, thus failing to leverage the\nrich spectral information available in the multispectral channels recorded by\nsatellites. Therefore, in this paper, we introduce Llama3-MS-CLIP, the first\nvision-language model pre-trained with contrastive learning on a large-scale\nmultispectral dataset and report on the performance gains due to the extended\nspectral range. Furthermore, we present the largest-to-date image-caption\ndataset for multispectral data, consisting of one million Sentinel-2 samples\nand corresponding textual descriptions generated with Llama3-LLaVA-Next and\nOverture Maps data. We develop a scalable captioning pipeline, which is\nvalidated by domain experts. We evaluate Llama3-MS-CLIP on multispectral\nzero-shot image classification and retrieval using three datasets of varying\ncomplexity. Our results demonstrate that Llama3-MS-CLIP significantly\noutperforms other RGB-based approaches, improving classification accuracy by\n6.77% on average and retrieval performance by 4.63% mAP compared to the\nsecond-best model. Our results emphasize the relevance of multispectral\nvision-language learning. We release the image-caption dataset, code, and model\nweights under an open-source license.", "published": "2025-03-20 09:13:31", "link": "http://arxiv.org/abs/2503.15969v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Efficient ANN-Guided Distillation: Aligning Rate-based Features of Spiking Neural Networks through Hybrid Block-wise Replacement", "abstract": "Spiking Neural Networks (SNNs) have garnered considerable attention as a\npotential alternative to Artificial Neural Networks (ANNs). Recent studies have\nhighlighted SNNs' potential on large-scale datasets. For SNN training, two main\napproaches exist: direct training and ANN-to-SNN (ANN2SNN) conversion. To fully\nleverage existing ANN models in guiding SNN learning, either direct ANN-to-SNN\nconversion or ANN-SNN distillation training can be employed. In this paper, we\npropose an ANN-SNN distillation framework from the ANN-to-SNN perspective,\ndesigned with a block-wise replacement strategy for ANN-guided learning. By\ngenerating intermediate hybrid models that progressively align SNN feature\nspaces to those of ANN through rate-based features, our framework naturally\nincorporates rate-based backpropagation as a training method. Our approach\nachieves results comparable to or better than state-of-the-art SNN distillation\nmethods, showing both training and learning efficiency.", "published": "2025-03-20 09:04:38", "link": "http://arxiv.org/abs/2503.16572v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "GAN-enhanced Simulation-driven DNN Testing in Absence of Ground Truth", "abstract": "The generation of synthetic inputs via simulators driven by search algorithms\nis essential for cost-effective testing of Deep Neural Network (DNN) components\nfor safety-critical systems. However, in many applications, simulators are\nunable to produce the ground-truth data needed for automated test oracles and\nto guide the search process.\n  To tackle this issue, we propose an approach for the generation of inputs for\ncomputer vision DNNs that integrates a generative network to ensure simulator\nfidelity and employs heuristic-based search fitnesses that leverage\ntransformation consistency, noise resistance, surprise adequacy, and\nuncertainty estimation. We compare the performance of our fitnesses with that\nof a traditional fitness function leveraging ground truth; further, we assess\nhow the integration of a GAN not leveraging the ground truth impacts on test\nand retraining effectiveness.\n  Our results suggest that leveraging transformation consistency is the best\noption to generate inputs for both DNN testing and retraining; it maximizes\ninput diversity, spots the inputs leading to worse DNN performance, and leads\nto best DNN performance after retraining. Besides enabling simulator-based\ntesting in the absence of ground truth, our findings pave the way for testing\nsolutions that replace costly simulators with diffusion and large language\nmodels, which might be more affordable than simulators, but cannot generate\nground-truth data.", "published": "2025-03-20 08:49:10", "link": "http://arxiv.org/abs/2503.15953v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE"}
{"title": "Unreal-MAP: Unreal-Engine-Based General Platform for Multi-Agent Reinforcement Learning", "abstract": "In this paper, we propose Unreal Multi-Agent Playground (Unreal-MAP), an MARL\ngeneral platform based on the Unreal-Engine (UE). Unreal-MAP allows users to\nfreely create multi-agent tasks using the vast visual and physical resources\navailable in the UE community, and deploy state-of-the-art (SOTA) MARL\nalgorithms within them. Unreal-MAP is user-friendly in terms of deployment,\nmodification, and visualization, and all its components are open-source. We\nalso develop an experimental framework compatible with algorithms ranging from\nrule-based to learning-based provided by third-party frameworks. Lastly, we\ndeploy several SOTA algorithms in example tasks developed via Unreal-MAP, and\nconduct corresponding experimental analyses. We believe Unreal-MAP can play an\nimportant role in the MARL field by closely integrating existing algorithms\nwith user-customized tasks, thus advancing the field of MARL.", "published": "2025-03-20 08:40:41", "link": "http://arxiv.org/abs/2503.15947v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical Deployment", "abstract": "We propose V-Droid, a mobile GUI task automation agent. Unlike previous\nmobile agents that utilize Large Language Models (LLMs) as generators to\ndirectly generate actions at each step, V-Droid employs LLMs as verifiers to\nevaluate candidate actions before making final decisions. To realize this novel\nparadigm, we introduce a comprehensive framework for constructing\nverifier-driven mobile agents: the discretized action space construction\ncoupled with the prefilling-only workflow to accelerate the verification\nprocess, the pair-wise progress preference training to significantly enhance\nthe verifier's decision-making capabilities, and the scalable human-agent joint\nannotation scheme to efficiently collect the necessary data at scale. V-Droid\nsets a new state-of-the-art task success rate across several public mobile task\nautomation benchmarks: 59.5% on AndroidWorld, 38.3% on AndroidLab, and 49% on\nMobileAgentBench, surpassing existing agents by 9.5%, 2.1%, and 9%,\nrespectively. Furthermore, V-Droid achieves an impressively low latency of 0.7\nseconds per step, making it the first mobile agent capable of delivering\nnear-real-time, effective decision-making capabilities.", "published": "2025-03-20 08:25:00", "link": "http://arxiv.org/abs/2503.15937v2", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Denoising-based Contractive Imitation Learning", "abstract": "A fundamental challenge in imitation learning is the \\emph{covariate shift}\nproblem. Existing methods to mitigate covariate shift often require additional\nexpert interactions, access to environment dynamics, or complex adversarial\ntraining, which may not be practical in real-world applications. In this paper,\nwe propose a simple yet effective method (DeCIL) to mitigate covariate shift by\nincorporating a denoising mechanism that enhances the contraction properties of\nthe state transition mapping. Our approach involves training two neural\nnetworks: a dynamics model ( f ) that predicts the next state from the current\nstate, and a joint state-action denoising policy network ( d ) that refines\nthis state prediction via denoising and outputs the corresponding action. We\nprovide theoretical analysis showing that the denoising network acts as a local\ncontraction mapping, reducing the error propagation of the state transition and\nimproving stability. Our method is straightforward to implement and can be\neasily integrated with existing imitation learning frameworks without requiring\nadditional expert data or complex modifications to the training procedure.\nEmpirical results demonstrate that our approach effectively improves success\nrate of various imitation learning tasks under noise perturbation.", "published": "2025-03-20 07:52:19", "link": "http://arxiv.org/abs/2503.15918v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "No Thing, Nothing: Highlighting Safety-Critical Classes for Robust LiDAR Semantic Segmentation in Adverse Weather", "abstract": "Existing domain generalization methods for LiDAR semantic segmentation under\nadverse weather struggle to accurately predict \"things\" categories compared to\n\"stuff\" categories. In typical driving scenes, \"things\" categories can be\ndynamic and associated with higher collision risks, making them crucial for\nsafe navigation and planning. Recognizing the importance of \"things\"\ncategories, we identify their performance drop as a serious bottleneck in\nexisting approaches. We observed that adverse weather induces degradation of\nsemantic-level features and both corruption of local features, leading to a\nmisprediction of \"things\" as \"stuff\". To mitigate these corruptions, we suggest\nour method, NTN - segmeNt Things for No-accident. To address semantic-level\nfeature corruption, we bind each point feature to its superclass, preventing\nthe misprediction of things classes into visually dissimilar categories.\nAdditionally, to enhance robustness against local corruption caused by adverse\nweather, we define each LiDAR beam as a local region and propose a\nregularization term that aligns the clean data with its corrupted counterpart\nin feature space. NTN achieves state-of-the-art performance with a +2.6 mIoU\ngain on the SemanticKITTI-to-SemanticSTF benchmark and +7.9 mIoU on the\nSemanticPOSS-to-SemanticSTF benchmark. Notably, NTN achieves a +4.8 and +7.9\nmIoU improvement on \"things\" classes, respectively, highlighting its\neffectiveness.", "published": "2025-03-20 07:40:24", "link": "http://arxiv.org/abs/2503.15910v2", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Enhancing Close-up Novel View Synthesis via Pseudo-labeling", "abstract": "Recent methods, such as Neural Radiance Fields (NeRF) and 3D Gaussian\nSplatting (3DGS), have demonstrated remarkable capabilities in novel view\nsynthesis. However, despite their success in producing high-quality images for\nviewpoints similar to those seen during training, they struggle when generating\ndetailed images from viewpoints that significantly deviate from the training\nset, particularly in close-up views. The primary challenge stems from the lack\nof specific training data for close-up views, leading to the inability of\ncurrent methods to render these views accurately. To address this issue, we\nintroduce a novel pseudo-label-based learning strategy. This approach leverages\npseudo-labels derived from existing training data to provide targeted\nsupervision across a wide range of close-up viewpoints. Recognizing the absence\nof benchmarks for this specific challenge, we also present a new dataset\ndesigned to assess the effectiveness of both current and future methods in this\narea. Our extensive experiments demonstrate the efficacy of our approach.", "published": "2025-03-20 07:27:46", "link": "http://arxiv.org/abs/2503.15908v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Jasmine: Harnessing Diffusion Prior for Self-supervised Depth Estimation", "abstract": "In this paper, we propose Jasmine, the first Stable Diffusion (SD)-based\nself-supervised framework for monocular depth estimation, which effectively\nharnesses SD's visual priors to enhance the sharpness and generalization of\nunsupervised prediction. Previous SD-based methods are all supervised since\nadapting diffusion models for dense prediction requires high-precision\nsupervision. In contrast, self-supervised reprojection suffers from inherent\nchallenges (e.g., occlusions, texture-less regions, illumination variance), and\nthe predictions exhibit blurs and artifacts that severely compromise SD's\nlatent priors. To resolve this, we construct a novel surrogate task of hybrid\nimage reconstruction. Without any additional supervision, it preserves the\ndetail priors of SD models by reconstructing the images themselves while\npreventing depth estimation from degradation. Furthermore, to address the\ninherent misalignment between SD's scale and shift invariant estimation and\nself-supervised scale-invariant depth estimation, we build the Scale-Shift GRU.\nIt not only bridges this distribution gap but also isolates the fine-grained\ntexture of SD output against the interference of reprojection loss. Extensive\nexperiments demonstrate that Jasmine achieves SoTA performance on the KITTI\nbenchmark and exhibits superior zero-shot generalization across multiple\ndatasets.", "published": "2025-03-20 07:15:49", "link": "http://arxiv.org/abs/2503.15905v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "A multi-model approach using XAI and anomaly detection to predict asteroid hazards", "abstract": "The potential for catastrophic collision makes near-Earth asteroids (NEAs) a\nserious concern. Planetary defense depends on accurately classifying\npotentially hazardous asteroids (PHAs), however the complexity of the data\nhampers conventional techniques. This work offers a sophisticated method for\naccurately predicting hazards by combining machine learning, deep learning,\nexplainable AI (XAI), and anomaly detection. Our approach extracts essential\nparameters like size, velocity, and trajectory from historical and real-time\nasteroid data. A hybrid algorithm improves prediction accuracy by combining\nseveral cutting-edge models. A forecasting module predicts future asteroid\nbehavior, and Monte Carlo simulations evaluate the likelihood of collisions.\nTimely mitigation is made possible by a real-time alarm system that notifies\nworldwide monitoring stations. This technique enhances planetary defense\nefforts by combining real-time alarms with sophisticated predictive modeling.", "published": "2025-03-20 07:00:01", "link": "http://arxiv.org/abs/2503.15901v1", "categories": ["astro-ph.EP", "astro-ph.IM", "cs.AI", "cs.LG"], "primary_category": "astro-ph.EP"}
{"title": "Time After Time: Deep-Q Effect Estimation for Interventions on When and What to do", "abstract": "Problems in fields such as healthcare, robotics, and finance requires\nreasoning about the value both of what decision or action to take and when to\ntake it. The prevailing hope is that artificial intelligence will support such\ndecisions by estimating the causal effect of policies such as how to treat\npatients or how to allocate resources over time. However, existing methods for\nestimating the effect of a policy struggle with \\emph{irregular time}. They\neither discretize time, or disregard the effect of timing policies. We present\na new deep-Q algorithm that estimates the effect of both when and what to do\ncalled Earliest Disagreement Q-Evaluation (EDQ). EDQ makes use of recursion for\nthe Q-function that is compatible with flexible sequence models, such as\ntransformers. EDQ provides accurate estimates under standard assumptions. We\nvalidate the approach through experiments on survival time and tumor growth\ntasks.", "published": "2025-03-20 06:27:35", "link": "http://arxiv.org/abs/2503.15890v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "LeanTTA: A Backpropagation-Free and Stateless Approach to Quantized Test-Time Adaptation on Edge Devices", "abstract": "While there are many advantages to deploying machine learning models on edge\ndevices, the resource constraints of mobile platforms, the dynamic nature of\nthe environment, and differences between the distribution of training versus\nin-the-wild data make such deployments challenging. Current test-time\nadaptation methods are often memory-intensive and not designed to be\nquantization-compatible or deployed on low-resource devices. To address these\nchallenges, we present LeanTTA, a novel backpropagation-free and stateless\nframework for quantized test-time adaptation tailored to edge devices. Our\napproach minimizes computational costs by dynamically updating normalization\nstatistics without backpropagation, which frees LeanTTA from the common pitfall\nof relying on large batches and historical data, making our method robust to\nrealistic deployment scenarios. Our approach is the first to enable further\ncomputational gains by combining partial adaptation with quantized module\nfusion. We validate our framework across sensor modalities, demonstrating\nsignificant improvements over state-of-the-art TTA methods, including a 15.7%\nerror reduction, peak memory usage of only 11.2MB for ResNet18, and fast\nadaptation within an order-of-magnitude of normal inference speeds on-device.\nLeanTTA provides a robust solution for achieving the right trade offs between\naccuracy and system efficiency in edge deployments, addressing the unique\nchallenges posed by limited data and varied operational conditions.", "published": "2025-03-20 06:27:09", "link": "http://arxiv.org/abs/2503.15889v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "DeepPsy-Agent: A Stage-Aware and Deep-Thinking Emotional Support Agent System", "abstract": "This paper introduces DeepPsy-Agent, an innovative psychological support\nsystem that combines the three-stage helping theory in psychology with deep\nlearning techniques. The system consists of two core components: (1) a\nmulti-stage response-capable dialogue model (\\textit{deeppsy-chat}), which\nenhances reasoning capabilities through stage-awareness and deep-thinking\nanalysis to generate high-quality responses; and (2) a real-time stage\ntransition detection model that identifies contextual shifts to guide the\ndialogue towards more effective intervention stages. Based on 30,000 real\npsychological hotline conversations, we employ AI-simulated dialogues and\nexpert re-annotation strategies to construct a high-quality multi-turn dialogue\ndataset. Experimental results demonstrate that DeepPsy-Agent outperforms\ngeneral-purpose large language models (LLMs) in key metrics such as problem\nexposure completeness, cognitive restructuring success rate, and action\nadoption rate. Ablation studies further validate the effectiveness of\nstage-awareness and deep-thinking modules, showing that stage information\ncontributes 42.3\\% to performance, while the deep-thinking module increases\nroot-cause identification by 58.3\\% and reduces ineffective suggestions by\n72.1\\%. This system addresses critical challenges in AI-based psychological\nsupport through dynamic dialogue management and deep reasoning, advancing\nintelligent mental health services.", "published": "2025-03-20 05:59:29", "link": "http://arxiv.org/abs/2503.15876v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "TruthLens: Explainable DeepFake Detection for Face Manipulated and Fully Synthetic Data", "abstract": "Detecting DeepFakes has become a crucial research area as the widespread use\nof AI image generators enables the effortless creation of face-manipulated and\nfully synthetic content, yet existing methods are often limited to binary\nclassification (real vs. fake) and lack interpretability. To address these\nchallenges, we propose TruthLens, a novel and highly generalizable framework\nfor DeepFake detection that not only determines whether an image is real or\nfake but also provides detailed textual reasoning for its predictions. Unlike\ntraditional methods, TruthLens effectively handles both face-manipulated\nDeepFakes and fully AI-generated content while addressing fine-grained queries\nsuch as \"Does the eyes/nose/mouth look real or fake?\"\n  The architecture of TruthLens combines the global contextual understanding of\nmultimodal large language models like PaliGemma2 with the localized feature\nextraction capabilities of vision-only models like DINOv2. This hybrid design\nleverages the complementary strengths of both models, enabling robust detection\nof subtle manipulations while maintaining interpretability. Extensive\nexperiments on diverse datasets demonstrate that TruthLens outperforms\nstate-of-the-art methods in detection accuracy (by 2-14%) and explainability,\nin both in-domain and cross-data settings, generalizing effectively across\ntraditional and emerging manipulation techniques.", "published": "2025-03-20 05:40:42", "link": "http://arxiv.org/abs/2503.15867v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Active management of battery degradation in wireless sensor network using deep reinforcement learning for group battery replacement", "abstract": "Wireless sensor networks (WSNs) have become a promising solution for\nstructural health monitoring (SHM), especially in hard-to-reach or remote\nlocations. Battery-powered WSNs offer various advantages over wired systems,\nhowever limited battery life has always been one of the biggest obstacles in\npractical use of the WSNs, regardless of energy harvesting methods. While\nvarious methods have been studied for battery health management, existing\nmethods exclusively aim to extend lifetime of individual batteries, lacking a\nsystem level view. A consequence of applying such methods is that batteries in\na WSN tend to fail at different times, posing significant difficulty on\nplanning and scheduling of battery replacement trip. This study investigate a\ndeep reinforcement learning (DRL) method for active battery degradation\nmanagement by optimizing duty cycle of WSNs at the system level. This active\nmanagement strategy effectively reduces earlier failure of battery individuals\nwhich enable group replacement without sacrificing WSN performances. A\nsimulated environment based on a real-world WSN setup was developed to train a\nDRL agent and learn optimal duty cycle strategies. The performance of the\nstrategy was validated in a long-term setup with various network sizes,\ndemonstrating its efficiency and scalability.", "published": "2025-03-20 05:36:33", "link": "http://arxiv.org/abs/2503.15865v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting Generation with Flexible Pose and Multi-View Joint Modeling", "abstract": "We propose VideoRFSplat, a direct text-to-3D model leveraging a video\ngeneration model to generate realistic 3D Gaussian Splatting (3DGS) for\nunbounded real-world scenes. To generate diverse camera poses and unbounded\nspatial extent of real-world scenes, while ensuring generalization to arbitrary\ntext prompts, previous methods fine-tune 2D generative models to jointly model\ncamera poses and multi-view images. However, these methods suffer from\ninstability when extending 2D generative models to joint modeling due to the\nmodality gap, which necessitates additional models to stabilize training and\ninference. In this work, we propose an architecture and a sampling strategy to\njointly model multi-view images and camera poses when fine-tuning a video\ngeneration model. Our core idea is a dual-stream architecture that attaches a\ndedicated pose generation model alongside a pre-trained video generation model\nvia communication blocks, generating multi-view images and camera poses through\nseparate streams. This design reduces interference between the pose and image\nmodalities. Additionally, we propose an asynchronous sampling strategy that\ndenoises camera poses faster than multi-view images, allowing rapidly denoised\nposes to condition multi-view generation, reducing mutual ambiguity and\nenhancing cross-modal consistency. Trained on multiple large-scale real-world\ndatasets (RealEstate10K, MVImgNet, DL3DV-10K, ACID), VideoRFSplat outperforms\nexisting text-to-3D direct generation methods that heavily depend on post-hoc\nrefinement via score distillation sampling, achieving superior results without\nsuch refinement.", "published": "2025-03-20 05:26:09", "link": "http://arxiv.org/abs/2503.15855v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Beyond Local Selection: Global Cut Selection for Enhanced Mixed-Integer Programming", "abstract": "In mixed-integer programming (MIP) solvers, cutting planes are essential for\nBranch-and-Cut (B&C) algorithms as they reduce the search space and accelerate\nthe solving process. Traditional methods rely on hard-coded heuristics for cut\nplane selection but fail to leverage problem-specific structural features.\nRecent machine learning approaches use neural networks for cut selection but\nfocus narrowly on the efficiency of single-node within the B&C algorithm,\nwithout considering the broader contextual information. To address this, we\npropose Global Cut Selection (GCS), which uses a bipartite graph to represent\nthe search tree and combines graph neural networks with reinforcement learning\nto develop cut selection strategies. Unlike prior methods, GCS applies cutting\nplanes across all nodes, incorporating richer contextual information.\nExperiments show GCS significantly improves solving efficiency for synthetic\nand large-scale real-world MIPs compared to traditional and learning-based\nmethods.", "published": "2025-03-20 04:59:18", "link": "http://arxiv.org/abs/2503.15847v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Computation-Efficient and Recognition-Friendly 3D Point Cloud Privacy Protection", "abstract": "3D point cloud has been widely used in applications such as self-driving\ncars, robotics, CAD models, etc. To the best of our knowledge, these\napplications raised the issue of privacy leakage in 3D point clouds, which has\nnot been studied well. Different from the 2D image privacy, which is related to\ntexture and 2D geometric structure, the 3D point cloud is texture-less and only\nrelevant to 3D geometric structure. In this work, we defined the 3D point cloud\nprivacy problem and proposed an efficient privacy-preserving framework named\nPointFlowGMM that can support downstream classification and segmentation tasks\nwithout seeing the original data. Using a flow-based generative model, the\npoint cloud is projected into a latent Gaussian mixture distributed subspace.\nWe further designed a novel angular similarity loss to obfuscate the original\ngeometric structure and reduce the model size from 767MB to 120MB without a\ndecrease in recognition performance. The projected point cloud in the latent\nspace is orthogonally rotated randomly to further protect the original\ngeometric structure, the class-to-class relationship is preserved after\nrotation, thus, the protected point cloud can support the recognition task. We\nevaluated our model on multiple datasets and achieved comparable recognition\nresults on encrypted point clouds compared to the original point clouds.", "published": "2025-03-20 03:09:44", "link": "http://arxiv.org/abs/2503.15818v2", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Ranking Counterfactual Explanations", "abstract": "AI-driven outcomes can be challenging for end-users to understand.\nExplanations can address two key questions: \"Why this outcome?\" (factual) and\n\"Why not another?\" (counterfactual). While substantial efforts have been made\nto formalize factual explanations, a precise and comprehensive study of\ncounterfactual explanations is still lacking. This paper proposes a formal\ndefinition of counterfactual explanations, proving some properties they\nsatisfy, and examining the relationship with factual explanations. Given that\nmultiple counterfactual explanations generally exist for a specific case, we\nalso introduce a rigorous method to rank these counterfactual explanations,\ngoing beyond a simple minimality condition, and to identify the optimal ones.\nOur experiments with 12 real-world datasets highlight that, in most cases, a\nsingle optimal counterfactual explanation emerges. We also demonstrate, via\nthree metrics, that the selected optimal explanation exhibits higher\nrepresentativeness and can explain a broader range of elements than a random\nminimal counterfactual. This result highlights the effectiveness of our\napproach in identifying more robust and comprehensive counterfactual\nexplanations.", "published": "2025-03-20 03:04:05", "link": "http://arxiv.org/abs/2503.15817v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Attention Pruning: Automated Fairness Repair of Language Models via Surrogate Simulated Annealing", "abstract": "This paper explores pruning attention heads as a post-processing bias\nmitigation method for large language models (LLMs). Modern AI systems such as\nLLMs are expanding into sensitive social contexts where fairness concerns\nbecome especially crucial. Since LLMs develop decision-making patterns by\ntraining on massive datasets of human-generated content, they naturally encode\nand perpetuate societal biases. While modifying training datasets and\nalgorithms is expensive and requires significant resources; post-processing\ntechniques-such as selectively deactivating neurons and attention heads in\npre-trained LLMs-can provide feasible and effective approaches to improve\nfairness. However, identifying the optimal subset of parameters to prune\npresents a combinatorial challenge within LLMs' immense parameter space,\nrequiring solutions that efficiently balance competing objectives across the\nfrontiers of model fairness and utility.\n  To address the computational challenges, we explore a search-based program\nrepair approach via randomized simulated annealing. Given the prohibitive\nevaluation costs in billion-parameter LLMs, we develop surrogate deep neural\nnetworks that efficiently model the relationship between attention head states\n(active/inactive) and their corresponding fairness/utility metrics. This allows\nus to perform optimization over the surrogate models and efficiently identify\noptimal subsets of attention heads for selective pruning rather than directly\nsearching through the LLM parameter space. This paper introduces Attention\nPruning, a fairness-aware surrogate simulated annealing approach to prune\nattention heads in LLMs that disproportionately contribute to bias while\nminimally impacting overall model utility. Our experiments show that Attention\nPruning achieves up to $40\\%$ reduction in gender bias and outperforms the\nstate-of-the-art bias mitigation strategies.", "published": "2025-03-20 03:02:32", "link": "http://arxiv.org/abs/2503.15815v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Video-VoT-R1: An efficient video inference model integrating image packing and AoE architecture", "abstract": "In the field of video-language pretraining, existing models face numerous\nchallenges in terms of inference efficiency and multimodal data processing.\nThis paper proposes a KunLunBaize-VoT-R1 video inference model based on a\nlong-sequence image encoder, along with its training and application methods.\nBy integrating image packing technology, the Autonomy-of-Experts (AoE)\narchitecture, and combining the video of Thought (VoT), a large language model\n(LLM) trained with large-scale reinforcement learning, and multiple training\ntechniques, the efficiency and accuracy of the model in video inference tasks\nare effectively improved. Experiments show that this model performs\noutstandingly in multiple tests, providing a new solution for video-language\nunderstanding.", "published": "2025-03-20 02:50:57", "link": "http://arxiv.org/abs/2503.15807v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Blend the Separated: Mixture of Synergistic Experts for Data-Scarcity Drug-Target Interaction Prediction", "abstract": "Drug-target interaction prediction (DTI) is essential in various applications\nincluding drug discovery and clinical application. There are two perspectives\nof input data widely used in DTI prediction: Intrinsic data represents how\ndrugs or targets are constructed, and extrinsic data represents how drugs or\ntargets are related to other biological entities. However, any of the two\nperspectives of input data can be scarce for some drugs or targets, especially\nfor those unpopular or newly discovered. Furthermore, ground-truth labels for\nspecific interaction types can also be scarce. Therefore, we propose the first\nmethod to tackle DTI prediction under input data and/or label scarcity. To make\nour model functional when only one perspective of input data is available, we\ndesign two separate experts to process intrinsic and extrinsic data\nrespectively and fuse them adaptively according to different samples.\nFurthermore, to make the two perspectives complement each other and remedy\nlabel scarcity, two experts synergize with each other in a mutually supervised\nway to exploit the enormous unlabeled data. Extensive experiments on 3\nreal-world datasets under different extents of input data scarcity and/or label\nscarcity demonstrate our model outperforms states of the art significantly and\nsteadily, with a maximum improvement of 53.53%. We also test our model without\nany data scarcity and it still outperforms current methods.", "published": "2025-03-20 02:27:16", "link": "http://arxiv.org/abs/2503.15796v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "MobiFuse: Learning Universal Human Mobility Patterns through Cross-domain Data Fusion", "abstract": "Human mobility modeling is critical for urban planning and transportation\nmanagement, yet existing datasets often lack the resolution and semantic\nrichness required for comprehensive analysis. To address this, we proposed a\ncross-domain data fusion framework that integrates multi-modal data of distinct\nnature and spatio-temporal resolution, including geographical, mobility,\nsocio-demographic, and traffic information, to construct a privacy-preserving\nand semantically enriched human travel trajectory dataset. This framework is\ndemonstrated through two case studies in Los Angeles (LA) and Egypt, where a\ndomain adaptation algorithm ensures its transferability across diverse urban\ncontexts. Quantitative evaluation shows that the generated synthetic dataset\naccurately reproduces mobility patterns observed in empirical data. Moreover,\nlarge-scale traffic simulations for LA County based on the generated synthetic\ndemand align well with observed traffic. On California's I-405 corridor, the\nsimulation yields a Mean Absolute Percentage Error of 5.85% for traffic volume\nand 4.36% for speed compared to Caltrans PeMS observations.", "published": "2025-03-20 01:41:28", "link": "http://arxiv.org/abs/2503.15779v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Detecting LLM-Written Peer Reviews", "abstract": "Editors of academic journals and program chairs of conferences require peer\nreviewers to write their own reviews. However, there is growing concern about\nthe rise of lazy reviewing practices, where reviewers use large language models\n(LLMs) to generate reviews instead of writing them independently. Existing\ntools for detecting LLM-generated content are not designed to differentiate\nbetween fully LLM-generated reviews and those merely polished by an LLM. In\nthis work, we employ a straightforward approach to identify LLM-generated\nreviews - doing an indirect prompt injection via the paper PDF to ask the LLM\nto embed a watermark. Our focus is on presenting watermarking schemes and\nstatistical tests that maintain a bounded family-wise error rate, when a venue\nevaluates multiple reviews, with a higher power as compared to standard methods\nlike Bonferroni correction. These guarantees hold without relying on any\nassumptions about human-written reviews. We also consider various methods for\nprompt injection including font embedding and jailbreaking. We evaluate the\neffectiveness and various tradeoffs of these methods, including different\nreviewer defenses. We find a high success rate in the embedding of our\nwatermarks in LLM-generated reviews across models. We also find that our\napproach is resilient to common reviewer defenses, and that the bounds on error\nrates in our statistical tests hold in practice while having the power to flag\nLLM-generated reviews, while Bonferroni correction is infeasible.", "published": "2025-03-20 01:11:35", "link": "http://arxiv.org/abs/2503.15772v1", "categories": ["cs.DL", "cs.AI", "cs.CR"], "primary_category": "cs.DL"}
{"title": "Advancing Problem-Based Learning in Biomedical Engineering in the Era of Generative AI", "abstract": "Problem-Based Learning (PBL) has significantly impacted biomedical\nengineering (BME) education since its introduction in the early 2000s,\neffectively enhancing critical thinking and real-world knowledge application\namong students. With biomedical engineering rapidly converging with artificial\nintelligence (AI), integrating effective AI education into established\ncurricula has become challenging yet increasingly necessary. Recent\nadvancements, including AI's recognition by the 2024 Nobel Prize, have\nhighlighted the importance of training students comprehensively in biomedical\nAI. However, effective biomedical AI education faces substantial obstacles,\nsuch as diverse student backgrounds, limited personalized mentoring,\nconstrained computational resources, and difficulties in safely scaling\nhands-on practical experiments due to privacy and ethical concerns associated\nwith biomedical data. To overcome these issues, we conducted a three-year\n(2021-2023) case study implementing an advanced PBL framework tailored\nspecifically for biomedical AI education, involving 92 undergraduate and 156\ngraduate students from the joint Biomedical Engineering program of Georgia\nInstitute of Technology and Emory University. Our approach emphasizes\ncollaborative, interdisciplinary problem-solving through authentic biomedical\nAI challenges. The implementation led to measurable improvements in learning\noutcomes, evidenced by high research productivity (16 student-authored\npublications), consistently positive peer evaluations, and successful\ndevelopment of innovative computational methods addressing real biomedical\nchallenges. Additionally, we examined the role of generative AI both as a\nteaching subject and an educational support tool within the PBL framework. Our\nstudy presents a practical and scalable roadmap for biomedical engineering\ndepartments aiming to integrate robust AI education into their curricula.", "published": "2025-03-20 00:52:02", "link": "http://arxiv.org/abs/2503.16558v1", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY"}
{"title": "Towards Agentic AI Networking in 6G: A Generative Foundation Model-as-Agent Approach", "abstract": "The promising potential of AI and network convergence in improving networking\nperformance and enabling new service capabilities has recently attracted\nsignificant interest. Existing network AI solutions, while powerful, are mainly\nbuilt based on the close-loop and passive learning framework, resulting in\nmajor limitations in autonomous solution finding and dynamic environmental\nadaptation. Agentic AI has recently been introduced as a promising solution to\naddress the above limitations and pave the way for true generally intelligent\nand beneficial AI systems. The key idea is to create a networking ecosystem to\nsupport a diverse range of autonomous and embodied AI agents in fulfilling\ntheir goals. In this paper, we focus on the novel challenges and requirements\nof agentic AI networking. We propose AgentNet, a novel framework for supporting\ninteraction, collaborative learning, and knowledge transfer among AI agents. We\nintroduce a general architectural framework of AgentNet and then propose a\ngenerative foundation model (GFM)-based implementation in which multiple\nGFM-as-agents have been created as an interactive knowledge-base to bootstrap\nthe development of embodied AI agents according to different task requirements\nand environmental features. We consider two application scenarios,\ndigital-twin-based industrial automation and metaverse-based infotainment\nsystem, to describe how to apply AgentNet for supporting efficient task-driven\ncollaboration and interaction among AI agents.", "published": "2025-03-20 00:48:44", "link": "http://arxiv.org/abs/2503.15764v1", "categories": ["cs.NI", "cs.AI"], "primary_category": "cs.NI"}
{"title": "Dialogic Learning in Child-Robot Interaction: A Hybrid Approach to Personalized Educational Content Generation", "abstract": "Dialogic learning fosters motivation and deeper understanding in education\nthrough purposeful and structured dialogues. Foundational models offer a\ntransformative potential for child-robot interactions, enabling the design of\npersonalized, engaging, and scalable interactions. However, their integration\ninto educational contexts presents challenges in terms of ensuring\nage-appropriate and safe content and alignment with pedagogical goals. We\nintroduce a hybrid approach to designing personalized educational dialogues in\nchild-robot interactions. By combining rule-based systems with LLMs for\nselective offline content generation and human validation, the framework\nensures educational quality and developmental appropriateness. We illustrate\nthis approach through a project aimed at enhancing reading motivation, in which\na robot facilitated book-related dialogues.", "published": "2025-03-20 00:46:10", "link": "http://arxiv.org/abs/2503.15762v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "ATTENTION2D: Communication Efficient Distributed Self-Attention Mechanism", "abstract": "Transformer-based models have emerged as a leading architecture for natural\nlanguage processing, natural language generation, and image generation tasks. A\nfundamental element of the transformer architecture is self-attention, which\nallows the model to capture intricate dependencies within the data. However,\nthe self-attention mechanism also incurs significant computational and memory\ncosts, particularly for long sequences.\n  In this paper, we introduce ATTENTION2D, a novel approach that exploits\nparallelism along two dimensions - query and key/value - of the self-attention\noperation. This method enables efficient distribution and parallelization of\ncomputations across multiple devices. Our approach facilitates asymptotically\nfaster training and inference phases compared to previous methods, without\nrelying on approximations or incurring additional computational or memory\noverheads. Furthermore, unlike existing techniques that struggle to scale with\nan increasing number of processing units, our approach effectively scales with\nadditional processing units.\n  Our experimental results confirm the effectiveness of our method in improving\ncommunication efficiency and scalability. Compared to Ring Attention, our\napproach demonstrated up to a 5x performance boost on a GPT-3-like model using\n64 NVIDIA A100 GPUs across 16 nodes, and up to a 9.4x performance boost on 64\nNVIDIA H100 GPUs across 64 nodes.", "published": "2025-03-20 00:25:44", "link": "http://arxiv.org/abs/2503.15758v1", "categories": ["cs.LG", "cs.AI", "cs.DC"], "primary_category": "cs.LG"}
{"title": "AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration", "abstract": "As large language models (LLMs) become increasingly capable, security and\nsafety evaluation are crucial. While current red teaming approaches have made\nstrides in assessing LLM vulnerabilities, they often rely heavily on human\ninput and lack comprehensive coverage of emerging attack vectors. This paper\nintroduces AutoRedTeamer, a novel framework for fully automated, end-to-end red\nteaming against LLMs. AutoRedTeamer combines a multi-agent architecture with a\nmemory-guided attack selection mechanism to enable continuous discovery and\nintegration of new attack vectors. The dual-agent framework consists of a red\nteaming agent that can operate from high-level risk categories alone to\ngenerate and execute test cases and a strategy proposer agent that autonomously\ndiscovers and implements new attacks by analyzing recent research. This modular\ndesign allows AutoRedTeamer to adapt to emerging threats while maintaining\nstrong performance on existing attack vectors. We demonstrate AutoRedTeamer's\neffectiveness across diverse evaluation settings, achieving 20% higher attack\nsuccess rates on HarmBench against Llama-3.1-70B while reducing computational\ncosts by 46% compared to existing approaches. AutoRedTeamer also matches the\ndiversity of human-curated benchmarks in generating test cases, providing a\ncomprehensive, scalable, and continuously evolving framework for evaluating the\nsecurity of AI systems.", "published": "2025-03-20 00:13:04", "link": "http://arxiv.org/abs/2503.15754v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "Using Language Models to Decipher the Motivation Behind Human Behaviors", "abstract": "AI presents a novel tool for deciphering the motivations behind human\nbehaviors. We show that by varying prompts to a large language model, we can\nelicit a full range of human behaviors in a variety of different scenarios in\nterms of classic economic games. Then by analyzing which prompts are needed to\nelicit which behaviors, we can infer (decipher) the motivations behind the\nhuman behaviors. We also show how one can analyze the prompts to reveal\nrelationships between the classic economic games, providing new insight into\nwhat different economic scenarios induce people to think about. We also show\nhow this deciphering process can be used to understand differences in the\nbehavioral tendencies of different populations.", "published": "2025-03-20 00:07:06", "link": "http://arxiv.org/abs/2503.15752v3", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "ContextGNN goes to Elliot: Towards Benchmarking Relational Deep Learning for Static Link Prediction (aka Personalized Item Recommendation)", "abstract": "Relational deep learning (RDL) settles among the most exciting advances in\nmachine learning for relational databases, leveraging the representational\npower of message passing graph neural networks (GNNs) to derive useful\nknowledge and run predicting tasks on tables connected through\nprimary-to-foreign key links. The RDL paradigm has been successfully applied to\nrecommendation lately, through its most recent representative deep learning\narchitecture namely, ContextGNN. While acknowledging ContextGNN's improved\nperformance on real-world recommendation datasets and tasks, preliminary tests\nfor the more traditional static link prediction task (aka personalized item\nrecommendation) on the popular Amazon Book dataset have demonstrated how\nContextGNN has still room for improvement compared to other state-of-the-art\nGNN-based recommender systems. To this end, with this paper, we integrate\nContextGNN within Elliot, a popular framework for reproducibility and\nbenchmarking analyses, counting around 50 state-of-the-art recommendation\nmodels from the literature to date. On such basis, we run preliminary\nexperiments on three standard recommendation datasets and against six\nstate-of-the-art GNN-based recommender systems, confirming similar trends to\nthose observed by the authors in their original paper. The code is publicly\navailable on GitHub:\nhttps://github.com/danielemalitesta/Rel-DeepLearning-RecSys.", "published": "2025-03-20 19:17:09", "link": "http://arxiv.org/abs/2503.16661v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Informative Path Planning to Explore and Map Unknown Planetary Surfaces with Gaussian Processes", "abstract": "Many environments, such as unvisited planetary surfaces and oceanic regions,\nremain unexplored due to a lack of prior knowledge. Autonomous vehicles must\nsample upon arrival, process data, and either transmit findings to a\nteleoperator or decide where to explore next. Teleoperation is suboptimal, as\nhuman intuition lacks mathematical guarantees for optimality. This study\nevaluates an informative path planning algorithm for mapping a scalar variable\ndistribution while minimizing travel distance and ensuring model convergence.\nWe compare traditional open loop coverage methods (e.g., Boustrophedon, Spiral)\nwith information-theoretic approaches using Gaussian processes, which update\nmodels iteratively with confidence metrics. The algorithm's performance is\ntested on three surfaces, a parabola, Townsend function, and lunar crater\nhydration map, to assess noise, convexity, and function behavior. Results\ndemonstrate that information-driven methods significantly outperform naive\nexploration in reducing model error and travel distance while improving\nconvergence potential.", "published": "2025-03-20 18:10:13", "link": "http://arxiv.org/abs/2503.16613v1", "categories": ["cs.RO", "cs.IR", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Iterative Optimal Attention and Local Model for Single Image Rain Streak Removal", "abstract": "High-fidelity imaging is crucial for the successful safety supervision and\nintelligent deployment of vision-based measurement systems (VBMS). It ensures\nhigh-quality imaging in VBMS, which is fundamental for reliable visual\nmeasurement and analysis. However, imaging quality can be significantly\nimpaired by adverse weather conditions, particularly rain, leading to blurred\nimages and reduced contrast. Such impairments increase the risk of inaccurate\nevaluations and misinterpretations in VBMS. To address these limitations, we\npropose an Expectation Maximization Reconstruction Transformer (EMResformer)\nfor single image rain streak removal. The EMResformer retains the key\nself-attention values for feature aggregation, enhancing local features to\nproduce superior image reconstruction. Specifically, we propose an Expectation\nMaximization Block seamlessly integrated into the single image rain streak\nremoval network, enhancing its ability to eliminate superfluous information and\nrestore a cleaner background image. Additionally, to further enhance local\ninformation for improved detail rendition, we introduce a Local Model Residual\nBlock, which integrates two local model blocks along with a sequence of\nconvolutions and activation functions. This integration synergistically\nfacilitates the extraction of more pertinent features for enhanced single image\nrain streak removal. Extensive experiments validate that our proposed\nEMResformer surpasses current state-of-the-art single image rain streak removal\nmethods on both synthetic and real-world datasets, achieving an improved\nbalance between model complexity and single image deraining performance.\nFurthermore, we evaluate the effectiveness of our method in VBMS scenarios,\ndemonstrating that high-quality imaging significantly improves the accuracy and\nreliability of VBMS tasks.", "published": "2025-03-20 14:06:53", "link": "http://arxiv.org/abs/2503.16165v1", "categories": ["cs.CV", "cs.IR"], "primary_category": "cs.CV"}
{"title": "OThink-MR1: Stimulating multimodal generalized reasoning capabilities via dynamic reinforcement learning", "abstract": "Multimodal Large Language Models (MLLMs) have gained significant traction for\ntheir ability to process diverse input data types and generate coherent,\ncontextually relevant outputs across various applications. While supervised\nfine-tuning (SFT) has been the predominant approach to enhance MLLM\ncapabilities in task-specific optimization, it often falls short in fostering\ncrucial generalized reasoning abilities. Although reinforcement learning (RL)\nholds great promise in overcoming these limitations, it encounters two\nsignificant challenges: (1) its generalized capacities in multimodal tasks\nremain largely unexplored, and (2) its training constraints, including the\nconstant Kullback-Leibler divergence or the clamp strategy, often result in\nsuboptimal bottlenecks. To address these challenges, we propose OThink-MR1, an\nadvanced MLLM equipped with profound comprehension and reasoning capabilities\nacross multimodal tasks. Specifically, we introduce Group Relative Policy\nOptimization with a dynamic Kullback-Leibler strategy (GRPO-D), which markedly\nenhances reinforcement learning (RL) performance. For Qwen2-VL-2B-Instruct,\nGRPO-D achieves a relative improvement of more than 5.72% over SFT and more\nthan 13.59% over GRPO in same-task evaluation on two adapted datasets.\nFurthermore, GRPO-D demonstrates remarkable cross-task generalization\ncapabilities, with an average relative improvement of more than 61.63% over SFT\nin cross-task evaluation. These results highlight that the MLLM trained with\nGRPO-D on one multimodal task can be effectively transferred to another task,\nunderscoring the superior generalized reasoning capabilities of our proposed\nOThink-MR1 model.", "published": "2025-03-20 12:22:18", "link": "http://arxiv.org/abs/2503.16081v2", "categories": ["cs.LG", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Integrating Notch Filtering and Statistical Methods for Improved Cardiac Diagnostics Using MATLAB", "abstract": "A Notch Filter is essential in ECG signal processing to eliminate narrowband\nnoise, especially powerline interference at 50 Hz or 60 Hz. This interference\noverlaps with vital ECG signal features, affecting the accuracy of downstream\nclassification tasks (e.g., arrhythmia detection). A properly designed notch\nfilter enhances signal quality, preserves essential ECG components (P, QRS, T\nwaves), and improves the performance of machine learning or deep learning\nmodels used for ECG classification.", "published": "2025-03-20 09:35:39", "link": "http://arxiv.org/abs/2504.02847v1", "categories": ["eess.SP", "cs.IR", "eess.IV"], "primary_category": "eess.SP"}
{"title": "UAV-Relay Assisted RSMA Fluid Antenna System: Outage Probability Analysis", "abstract": "This letter studies the impact of fluid antenna system (FAS) technology on\nthe performance of unmanned aerial vehicle (UAV)-assisted multiuser\ncommunication networks. Specifically, we consider a scenario where a\nfixed-position antenna (FPA) base station (BS) serves K FAS-equipped users with\nthe assistance of a UAV acting as an aerial relay. The BS employs\nrate-splitting multiple access (RSMA), while the UAV operates in half-duplex\n(HD) mode using the decode-and-forward (DF) strategy. For this system, we\nderive a compact analytical expression for the outage probability (OP) and its\nasymptotic behavior in the high signal-to-noise ratio (SNR) regime, leveraging\nthe multivariate t-distribution. Our results show how deploying FAS at ground\nusers (GUs) in UAV-aided communications improves overall system performance\ncompared to using FPA GUs.", "published": "2025-03-20 23:48:24", "link": "http://arxiv.org/abs/2503.16751v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Leveraging Code Structure to Improve Soft Output for GRAND, GCD, OSD, and SCL", "abstract": "In addition to a proposed codeword, error correction decoders that provide\nblockwise soft output (SO) return an estimate of the likelihood that the\ndecoding is correct. Following Forney, such estimates are traditionally only\npossible for list decoders where the soft output is the likelihood that a\ndecoding is correct given it is assumed to be in the list. Recently, it has\nbeen established that Guessing Random Additive Noise Decoding (GRAND), Guessing\nCodeword Decoding (GCD), Ordered Statistics Decoding (OSD), and Successive\nCancellation List (SCL) decoding can provide more accurate soft output, even\nwithout list decoding. Central to the improvement is a per-decoding estimate of\nthe likelihood that a decoding has not been found that can be readily\ncalculated during the decoding process. Here we explore how linear codebook\nconstraints can be employed to further enhance the precision of such SO. We\nevaluate performance by adapting a forecasting statistic called the Brier\nScore. Results indicate that the SO generated by the approach is essentially as\naccurate as the maximum a posteriori estimate.", "published": "2025-03-20 19:50:55", "link": "http://arxiv.org/abs/2503.16677v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Transformer-based Wireless Symbol Detection Over Fading Channels", "abstract": "Pre-trained Transformers, through in-context learning (ICL), have\ndemonstrated exceptional capabilities to adapt to new tasks using example\nprompts without model update. Transformer-based wireless receivers, where\nprompts consist of the pilot data in the form of transmitted and received\nsignal pairs, have shown high detection accuracy when pilot data are abundant.\nHowever, pilot information is often costly and limited in practice. In this\nwork, we propose the DEcision Feedback INcontExt Detection (DEFINED) solution\nas a new wireless receiver design, which bypasses channel estimation and\ndirectly performs symbol detection using the (sometimes extremely) limited\npilot data. The key innovation in DEFINED is the proposed decision feedback\nmechanism in ICL, where we sequentially incorporate the detected symbols into\nthe prompts as pseudo-labels to improve the detection for subsequent symbols.\nFurthermore, we proposed another detection method where we combine ICL with\nSemi-Supervised Learning (SSL) to extract information from both labeled and\nunlabeled data during inference, thus avoiding the errors propagated during the\ndecision feedback process of the original DEFINED. Extensive experiments across\na broad range of wireless communication settings demonstrate that a small\nTransformer trained with DEFINED or IC-SSL achieves significant performance\nimprovements over conventional methods, in some cases only needing a single\npilot pair to achieve similar performance of the latter with more than 4 pilot\npairs.", "published": "2025-03-20 17:57:01", "link": "http://arxiv.org/abs/2503.16594v1", "categories": ["cs.IT", "cs.LG", "eess.SP", "math.IT", "stat.ML"], "primary_category": "cs.IT"}
{"title": "Dispersion is (Almost) Optimal under (A)synchrony", "abstract": "The dispersion problem has received much attention recently in the\ndistributed computing literature. In this problem, $k\\leq n$ agents placed\ninitially arbitrarily on the nodes of an $n$-node, $m$-edge anonymous graph of\nmaximum degree $\\Delta$ have to reposition autonomously to reach a\nconfiguration in which each agent is on a distinct node of the graph.\nDispersion is interesting as well as important due to its connections to many\nfundamental coordination problems by mobile agents on graphs, such as\nexploration, scattering, load balancing, relocation of self-driven electric\ncars (robots) to recharge stations (nodes), etc. The objective has been to\nprovide a solution that optimizes simultaneously time and memory complexities.\nThere exist graphs for which the lower bound on time complexity is $\\Omega(k)$.\nMemory complexity is $\\Omega(\\log k)$ per agent independent of graph topology.\nThe state-of-the-art algorithms have (i) time complexity $O(k\\log^2k)$ and\nmemory complexity $O(\\log(k+\\Delta))$ under the synchronous setting [DISC'24]\nand (ii) time complexity $O(\\min\\{m,k\\Delta\\})$ and memory complexity\n$O(\\log(k+\\Delta))$ under the asynchronous setting [OPODIS'21]. In this paper,\nwe improve substantially on this state-of-the-art. Under the synchronous\nsetting as in [DISC'24], we present the first optimal $O(k)$ time algorithm\nkeeping memory complexity $O(\\log (k+\\Delta))$. Under the asynchronous setting\nas in [OPODIS'21], we present the first algorithm with time complexity $O(k\\log\nk)$ keeping memory complexity $O(\\log (k+\\Delta))$, which is time-optimal\nwithin an $O(\\log k)$ factor despite asynchrony. Both results were obtained\nthrough novel techniques to quickly find empty nodes to settle agents, which\nmay be of independent interest.", "published": "2025-03-20 15:09:05", "link": "http://arxiv.org/abs/2503.16216v1", "categories": ["cs.DC", "cs.DS", "cs.MA", "cs.RO"], "primary_category": "cs.DC"}
{"title": "Consensus Tracking Control of Multi-agent Systems with A Time-varying Reference State under Binary-valued Communication", "abstract": "This paper investigates the problem of consensus tracking control of discrete\ntime multi-agent systems under binary-valued communication. Different from most\nexisting studies on consensus tracking, the transmitted information between\nagents is the binary-valued. Parameter identification with binary-valued\nobservations is applied to the estimation of neighbors'states and the tracking\ncontrol is designed based on the estimation. Two Lyapunov functions are\nconstructed to deal with the strong coupling of estimation and control.\nCompared with consensus problems under binary-valued communication, a reference\nstate is required for consensus tracking control. Two scenarios of the\ntime-varying reference state are studied respectively. (1) The reference state\nis asymptotically convergent. An online algorithm that performs estimation and\ncontrol simultaneously is proposed, in which the estimation step size and the\ncontrol gain are decreasing with time. By this algorithm, the multi-agent\nsystem is proved to achieve consensus tracking with convergence rate\nO(1/k^{\\epsilon} ) under certain conditions. (2) The reference state is\nbounded, which is less conservative than that in the first case. In this case,\nthe estimation step size and control gain are designed to be constant. By this\nalgorithm, all the followers can reach to a neighborhood of the leader with an\nexponential rate. Finally, simulations are given to demonstrate theoretical\nresults.", "published": "2025-03-20 08:50:15", "link": "http://arxiv.org/abs/2503.15955v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "Data Spatial Programming", "abstract": "We introduce a novel programming model, Data Spatial Programming, which\nextends the semantics of Object-Oriented Programming (OOP) by introducing new\nclass-like constructs called archetypes. These archetypes encapsulate the\ntopological relationships between data entities and the execution flow in a\nstructured manner, enabling more expressive and semantically rich computations\nover interconnected data structures or finite states. By formalizing the\nrelationships between data elements in this topological space, our approach\nallows for more intuitive modeling of complex systems where a topology of\nconnections is formed for the underlying computational model. This paradigm\naddresses limitations in traditional OOP when representing a wide range of\nproblems in computer science such as agent-based systems, social networks,\nprocessing on relational data, neural networks, distributed systems, finite\nstate machines, and other spatially-oriented computational problems.", "published": "2025-03-20 02:55:40", "link": "http://arxiv.org/abs/2503.15812v4", "categories": ["cs.PL", "cs.MA", "cs.SE"], "primary_category": "cs.PL"}
{"title": "Universal approximation property of neural stochastic differential equations", "abstract": "We identify various classes of neural networks that are able to approximate\ncontinuous functions locally uniformly subject to fixed global linear growth\nconstraints. For such neural networks the associated neural stochastic\ndifferential equations can approximate general stochastic differential\nequations, both of It\\^o diffusion type, arbitrarily well. Moreover,\nquantitative error estimates are derived for stochastic differential equations\nwith sufficiently regular coefficients.", "published": "2025-03-20 20:34:23", "link": "http://arxiv.org/abs/2503.16696v1", "categories": ["math.PR", "cs.LG", "math.FA", "q-fin.MF", "stat.ML", "41A29, 60H10, 68T07, 91G80"], "primary_category": "math.PR"}
{"title": "UniSync: A Unified Framework for Audio-Visual Synchronization", "abstract": "Precise audio-visual synchronization in speech videos is crucial for content\nquality and viewer comprehension. Existing methods have made significant\nstrides in addressing this challenge through rule-based approaches and\nend-to-end learning techniques. However, these methods often rely on limited\naudio-visual representations and suboptimal learning strategies, potentially\nconstraining their effectiveness in more complex scenarios. To address these\nlimitations, we present UniSync, a novel approach for evaluating audio-visual\nsynchronization using embedding similarities. UniSync offers broad\ncompatibility with various audio representations (e.g., Mel spectrograms,\nHuBERT) and visual representations (e.g., RGB images, face parsing maps, facial\nlandmarks, 3DMM), effectively handling their significant dimensional\ndifferences. We enhance the contrastive learning framework with a margin-based\nloss component and cross-speaker unsynchronized pairs, improving discriminative\ncapabilities. UniSync outperforms existing methods on standard datasets and\ndemonstrates versatility across diverse audio-visual representations. Its\nintegration into talking face generation frameworks enhances synchronization\nquality in both natural and AI-generated content.", "published": "2025-03-20 17:16:03", "link": "http://arxiv.org/abs/2503.16357v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "How Robust Are Router-LLMs? Analysis of the Fragility of LLM Routing\n  Capabilities", "abstract": "Large language model (LLM) routing has emerged as a crucial strategy for\nbalancing computational costs with performance by dynamically assigning queries\nto the most appropriate model based on query complexity. Despite recent\nadvances showing that preference-data-based routers can outperform traditional\nmethods, current evaluation benchmarks remain limited. They largely focus on\ngeneral model capabilities while overlooking task-specific behaviors and\ncritical concerns such as privacy, safety, and potential backdoor\nvulnerabilities introduced through preference data. In response, we propose the\nDSC benchmark: Diverse, Simple, and Categorized, an evaluation framework that\ncategorizes router performance across a broad spectrum of query types,\nincluding coding, translation, mathematics, human instructions, general\nknowledge, and LLM jailbreaking. Additionally, it integrates privacy and safety\nassessments to reveal hidden risks. Our experiments on three preference-based\nrouters and two commercial counterparts demonstrate that while these systems\nimprove efficiency, they often make suboptimal, category-driven decisions. For\ninstance, a BERT-based router directs all coding and mathematics queries to the\nmost powerful LLM even when simpler models would suffice, while routing\njailbreaking attempts to weaker models, thereby elevating safety risks.", "published": "2025-03-20 19:52:30", "link": "http://arxiv.org/abs/2504.07113v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
