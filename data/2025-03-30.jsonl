{"title": "On Gottschalk's surjunctivity conjecture for non-uniform cellular automata", "abstract": "Gottschalk's surjunctivity conjecture for a group $G$ states that it is\nimpossible for cellular automata (CA) over the universe $G$ with finite\nalphabet to produce strict embeddings of the full shift into itself. A group\nuniverse $G$ satisfying Gottschalk's surjunctivity conjecture is called a\nsurjunctive group. The surjunctivity theorem of Gromov and Weiss shows that\nevery sofic group is surjunctive. In this paper, we study the surjunctivity of\nlocal perturbations of CA and more generally of non-uniform cellular automata\n(NUCA) with finite memory and uniformly bounded singularity over surjunctive\ngroup universes. In particular, we show that such a NUCA must be invertible\nwhenever it is reversible. We also obtain similar results which extend to the\nclass of NUCA a certain dual-surjunctivity theorem of Capobianco, Kari, and\nTaati for CA.", "published": "2025-03-30 13:26:52", "link": "http://arxiv.org/abs/2503.23435v1", "categories": ["math.DS", "cs.DM", "math.GR", "nlin.CG"], "primary_category": "math.DS"}
{"title": "Multi-Pass Streaming Lower Bounds for Approximating Max-Cut", "abstract": "In the Max-Cut problem in the streaming model, an algorithm is given the\nedges of an unknown graph $G = (V,E)$ in some fixed order, and its goal is to\napproximate the size of the largest cut in $G$. Improving upon an earlier\nresult of Kapralov, Khanna and Sudan, it was shown by Kapralov and Krachun that\nfor all $\\varepsilon>0$, no $o(n)$ memory streaming algorithm can achieve a\n$(1/2+\\varepsilon)$-approximation for Max-Cut. Their result holds for\nsingle-pass streams, i.e.~the setting in which the algorithm only views the\nstream once, and it was open whether multi-pass access may help. The\nstate-of-the-art result along these lines, due to Assadi and N, rules out\narbitrarily good approximation algorithms with constantly many passes and\n$n^{1-\\delta}$ space for any $\\delta>0$.\n  We improve upon this state-of-the-art result, showing that any non-trivial\napproximation algorithm for Max-Cut requires either polynomially many passes or\npolynomially large space. More specifically, we show that for all\n$\\varepsilon>0$, a $k$-pass streaming $(1/2+\\varepsilon)$-approximation\nalgorithm for Max-Cut requires $\\Omega_{\\varepsilon}\\left(n^{1/3}/k\\right)$\nspace. This result leads to a similar lower bound for the Maximum Directed Cut\nproblem, showing the near optimality of the algorithm of [Saxena, Singer,\nSudan, Velusamy, SODA 2025].\n  Our lower bounds proceed by showing a communication complexity lower bound\nfor the Distributional Implicit Hidden Partition (DIHP) Problem, introduced by\nKapralov and Krachun. While a naive application of the discrepancy method\nfails, we identify a property of protocols called ``globalness'', and show that\n(1) any protocol for DIHP can be turned into a global protocol, (2) the\ndiscrepancy of a global protocol must be small. The second step is the more\ntechnically involved step in the argument, and therein we use global\nhypercontractive inequalities.", "published": "2025-03-30 11:22:59", "link": "http://arxiv.org/abs/2503.23404v1", "categories": ["cs.DS", "cs.CC", "cs.DM"], "primary_category": "cs.DS"}
{"title": "A Constrained Multi-Agent Reinforcement Learning Approach to Autonomous Traffic Signal Control", "abstract": "Traffic congestion in modern cities is exacerbated by the limitations of\ntraditional fixed-time traffic signal systems, which fail to adapt to dynamic\ntraffic patterns. Adaptive Traffic Signal Control (ATSC) algorithms have\nemerged as a solution by dynamically adjusting signal timing based on real-time\ntraffic conditions. However, the main limitation of such methods is that they\nare not transferable to environments under real-world constraints, such as\nbalancing efficiency, minimizing collisions, and ensuring fairness across\nintersections. In this paper, we view the ATSC problem as a constrained\nmulti-agent reinforcement learning (MARL) problem and propose a novel algorithm\nnamed Multi-Agent Proximal Policy Optimization with Lagrange Cost Estimator\n(MAPPO-LCE) to produce effective traffic signal control policies. Our approach\nintegrates the Lagrange multipliers method to balance rewards and constraints,\nwith a cost estimator for stable adjustment. We also introduce three\nconstraints on the traffic network: GreenTime, GreenSkip, and PhaseSkip, which\npenalize traffic policies that do not conform to real-world scenarios. Our\nexperimental results on three real-world datasets demonstrate that MAPPO-LCE\noutperforms three baseline MARL algorithms by across all environments and\ntraffic constraints (improving on MAPPO by 12.60%, IPPO by 10.29%, and QTRAN by\n13.10%). Our results show that constrained MARL is a valuable tool for traffic\nplanners to deploy scalable and efficient ATSC methods in real-world traffic\nnetworks. We provide code at https://github.com/Asatheesh6561/MAPPO-LCE.", "published": "2025-03-30 23:29:48", "link": "http://arxiv.org/abs/2503.23626v1", "categories": ["cs.MA", "cs.LG"], "primary_category": "cs.MA"}
{"title": "VFlow: Discovering Optimal Agentic Workflows for Verilog Generation", "abstract": "Hardware design automation faces challenges in generating high-quality\nVerilog code efficiently. This paper introduces VFlow, an automated framework\nthat optimizes agentic workflows for Verilog code generation. Unlike existing\napproaches that rely on pre-defined prompting strategies, VFlow leverages Monte\nCarlo Tree Search (MCTS) to discover effective sequences of Large Language\nModels invocations that maximize code quality while minimizing computational\ncosts. VFlow extends the AFLOW methodology with domain-specific operators\naddressing hardware design requirements, including syntax validation,\nsimulation-based verification, and synthesis optimization. Experimental\nevaluation on the VerilogEval benchmark demonstrates VFlow's superiority,\nachieving an 83.6% average pass@1 rate-a 6.1\\% improvement over\nstate-of-the-art PromptV and a 36.9\\% gain compared to direct LLM invocation.\nMost significantly, VFlow enhances the capabilities of smaller models, enabling\nDeepSeek-V3 to achieve 141.2\\% of GPT-4o's performance while reducing API costs\nto just 13\\%. These findings indicate that intelligently optimized workflows\nenable cost-efficient LLMs to outperform larger models on hardware design\ntasks, potentially democratizing access to advanced digital circuit development\ntools and accelerating innovation in the semiconductor industry", "published": "2025-03-30 15:44:22", "link": "http://arxiv.org/abs/2504.03723v1", "categories": ["cs.AR", "cs.MA"], "primary_category": "cs.AR"}
{"title": "SPIO: Ensemble and Selective Strategies via LLM-Based Multi-Agent Planning in Automated Data Science", "abstract": "Large Language Models (LLMs) have revolutionized automated data analytics and\nmachine learning by enabling dynamic reasoning and adaptability. While recent\napproaches have advanced multi-stage pipelines through multi-agent systems,\nthey typically rely on rigid, single-path workflows that limit the exploration\nand integration of diverse strategies, often resulting in suboptimal\npredictions. To address these challenges, we propose SPIO (Sequential Plan\nIntegration and Optimization), a novel framework that leverages LLM-driven\ndecision-making to orchestrate multi-agent planning across four key modules:\ndata preprocessing, feature engineering, modeling, and hyperparameter tuning.\nIn each module, dedicated planning agents independently generate candidate\nstrategies that cascade into subsequent stages, fostering comprehensive\nexploration. A plan optimization agent refines these strategies by suggesting\nseveral optimized plans. We further introduce two variants: SPIO-S, which\nselects a single best solution path as determined by the LLM, and SPIO-E, which\nselects the top k candidate plans and ensembles them to maximize predictive\nperformance. Extensive experiments on Kaggle and OpenML datasets demonstrate\nthat SPIO significantly outperforms state-of-the-art methods, providing a\nrobust and scalable solution for automated data science task.", "published": "2025-03-30 04:45:32", "link": "http://arxiv.org/abs/2503.23314v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "A first-order DirAC-based parametric Ambisonic coder for immersive communications", "abstract": "Directional Audio Coding (DirAC) is a proven method for parametrically\nrepresenting a 3D audio scene in B-format and is capable of reproducing it on\narbitrary loudspeaker layouts. Although such a method seems well suited for low\nbitrate Ambisonic transmission, little work has been done on the feasibility of\nbuilding a real system upon it. In this paper, we present a DirAC-based coding\nfor Higher-Order Ambisonics (HOA), developed as part of a standardisation\neffort to extend the 3GPP EVS codec to immersive communications. Starting from\nthe first-order DirAC model, we show how to reduce algorithmic delay, the\nbitrate required for the parameters and complexity by bringing the full\nsynthesis in the spherical harmonic domain. The evaluation of the proposed\ntechnique for coding 3\\textsuperscript{rd} order Ambisonics at bitrates from 32\nto 128 kbps shows the relevance of the parametric approach compared with\nexisting solutions.", "published": "2025-03-30 20:47:10", "link": "http://arxiv.org/abs/2503.23586v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Evaluation of the Pronunciation of Tajweed Rules Based on DNN as a Step Towards Interactive Recitation Learning", "abstract": "Proper recitation of the Quran, adhering to the rules of Tajweed, is crucial\nfor preventing mistakes during recitation and requires significant effort to\nmaster. Traditional methods of teaching these rules are limited by the\navailability of qualified instructors and time constraints. Automatic\nevaluation of recitation can address these challenges by providing prompt\nfeedback and supporting independent practice. This study focuses on developing\na deep learning model to classify three Tajweed rules - separate stretching (Al\nMad), tight noon (Ghunnah), and hide (Ikhfaa) - using the publicly available\nQDAT dataset, which contains over 1,500 audio recordings. The input data\nconsisted of audio recordings from this dataset, transformed into normalized\nmel-spectrograms. For classification, the EfficientNet-B0 architecture was\nused, enhanced with a Squeeze-and-Excitation attention mechanism. The developed\nmodel achieved accuracy rates of 95.35%, 99.34%, and 97.01% for the respective\nrules. An analysis of the learning curves confirmed the model's robustness and\nabsence of overfitting. The proposed approach demonstrates high efficiency and\npaves the way for developing interactive educational systems for Tajweed study.", "published": "2025-03-30 15:03:02", "link": "http://arxiv.org/abs/2503.23470v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speculative End-Turn Detector for Efficient Speech Chatbot Assistant", "abstract": "Spoken dialogue systems powered by large language models have demonstrated\nremarkable abilities in understanding human speech and generating appropriate\nspoken responses. However, these systems struggle with end-turn detection (ETD)\n-- the ability to distinguish between user turn completion and hesitation. This\nlimitation often leads to premature or delayed responses, disrupting the flow\nof spoken conversations. In this paper, we introduce the ETD Dataset, the first\npublic dataset for end-turn detection. The ETD dataset consists of both\nsynthetic speech data generated with text-to-speech models and real-world\nspeech data collected from web sources. We also propose SpeculativeETD, a novel\ncollaborative inference framework that balances efficiency and accuracy to\nimprove real-time ETD in resource-constrained environments. Our approach\njointly employs a lightweight GRU-based model, which rapidly detects the\nnon-speaking units in real-time on local devices, and a high-performance\nWav2vec-based model running on the server to make a more challenging\nclassification of distinguishing turn ends from mere pauses. Experiments\ndemonstrate that the proposed SpeculativeETD significantly improves ETD\naccuracy while keeping the required computations low. Datasets and code will be\navailable after the review.", "published": "2025-03-30 13:34:23", "link": "http://arxiv.org/abs/2503.23439v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Scaling Auditory Cognition via Test-Time Compute in Audio Language Models", "abstract": "Large language models (LLMs) have shown exceptional versatility in natural\nlanguage processing, prompting recent efforts to extend their multimodal\ncapabilities to speech processing through the development of audio large\nlanguage models (Audio LLMs). While Audio LLMs excel in tasks such as speech\nrecognition and synthesis, it remains unclear how they perform when faced with\nthe auditory cognitive challenges posed by real-world environments, such as\naudio comprehension and listening recall, particularly in the presence of\nbackground noise or overlapping speech. Unlike text-based LLMs, which have\naccess to vast amounts of text data for pre-training, retraining Audio LLMs\nwith diverse auditory cognitive scenes is difficult due to the limited datasets\nthat simulate real-world auditory cognitive scenarios and the challenge of\nacquiring auditory cognitive labels for training. While test-time compute (TTC)\nmethods have been shown to enhance the capabilities of text-based LLMs during\ninference, a key challenge lies in designing these TTC methods to improve the\nauditory capabilities of Audio LLMs. This study aims to address these two\nresearch gaps by: i) exploring the auditory cognitive capabilities of Audio\nLLMs, and ii) enhancing their capabilities using TTC approaches. We have\ninvestigated five different Audio LLMs for auditory cognition using a\n\\textit{self-collected} database and have proposed five TTC approaches to\nenhance auditory cognitive capabilities during inference. Our findings reveal\nthat Audio LLMs performance decreases in more challenging auditory cognitive\ntasks. The proposed TTC approaches significantly enhance cognitive auditory\ncapabilities, advancing the development of more adaptable and resilient Audio\nLLMs for practical applications such as assistive listening devices,\nvoice-based AI assistants, and communication technologies.", "published": "2025-03-30 11:04:18", "link": "http://arxiv.org/abs/2503.23395v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "D3-Guard: Acoustic-based Drowsy Driving Detection Using Smartphones", "abstract": "Since the number of cars has grown rapidly in recent years, driving safety\ndraws more and more public attention. Drowsy driving is one of the biggest\nthreatens to driving safety. Therefore, a simple but robust system that can\ndetect drowsy driving with commercial off-the-shelf devices (such as\nsmartphones) is very necessary. With this motivation, we explore the\nfeasibility of purely using acoustic sensors embedded in smartphones to detect\ndrowsy driving. We first study characteristics of drowsy driving, and find some\nunique patterns of Doppler shift caused by three typical drowsy behaviors, i.e.\nnodding, yawning and operating steering wheel. We then validate our important\nfindings through empirical analysis of the driving data collected from real\ndriving environments. We further propose a real-time Drowsy Driving Detection\nsystem (D3-Guard) based on audio devices embedded in smartphones. In order to\nimprove the performance of our system, we adopt an effective feature extraction\nmethod based on undersampling technique and FFT, and carefully design a\nhigh-accuracy detector based on LSTM networks for the early detection of drowsy\ndriving. Through extensive experiments with 5 volunteer drivers in real driving\nenvironments, our system can distinguish drowsy driving actions with an average\ntotal accuracy of 93.31% in real-time. Over 80% drowsy driving actions can be\ndetected within first 70% of action duration.", "published": "2025-03-30 10:52:01", "link": "http://arxiv.org/abs/2503.23393v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "HearSmoking: Smoking Detection in Driving Environment via Acoustic Sensing on Smartphones", "abstract": "Driving safety has drawn much public attention in recent years due to the\nfast-growing number of cars. Smoking is one of the threats to driving safety\nbut is often ignored by drivers. Existing works on smoking detection either\nwork in contact manner or need additional devices. This motivates us to explore\nthe practicability of using smartphones to detect smoking events in driving\nenvironment. In this paper, we propose a cigarette smoking detection system,\nnamed HearSmoking, which only uses acoustic sensors on smartphones to improve\ndriving safety. After investigating typical smoking habits of drivers,\nincluding hand movement and chest fluctuation, we design an acoustic signal to\nbe emitted by the speaker and received by the microphone. We calculate Relative\nCorrelation Coefficient of received signals to obtain movement patterns of\nhands and chest. The processed data is sent into a trained Convolutional Neural\nNetwork for classification of hand movement. We also design a method to detect\nrespiration at the same time. To improve system performance, we further analyse\nthe periodicity of the composite smoking motion. Through extensive experiments\nin real driving environments, HearSmoking detects smoking events with an\naverage total accuracy of 93.44 percent in real-time.", "published": "2025-03-30 10:40:44", "link": "http://arxiv.org/abs/2503.23391v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "HearFit+: Personalized Fitness Monitoring via Audio Signals on Smart Speakers", "abstract": "Fitness can help to strengthen muscles, increase resistance to diseases, and\nimprove body shape. Nowadays, a great number of people choose to exercise at\nhome/office rather than at the gym due to lack of time. However, it is\ndifficult for them to get good fitness effects without professional guidance.\nMotivated by this, we propose the first personalized fitness monitoring system,\nHearFit+, using smart speakers at home/office. We explore the feasibility of\nusing acoustic sensing to monitor fitness. We design a fitness detection method\nbased on Doppler shift and adopt the short time energy to segment fitness\nactions. Based on deep learning, HearFit+ can perform fitness classification\nand user identification at the same time. Combined with incremental learning,\nusers can easily add new actions. We design 4 evaluation metrics (i.e.,\nduration, intensity, continuity, and smoothness) to help users to improve\nfitness effects. Through extensive experiments including over 9,000 actions of\n10 types of fitness from 12 volunteers, HearFit+ can achieve an average\naccuracy of 96.13% on fitness classification and 91% accuracy for user\nidentification. All volunteers confirm that HearFit+ can help improve the\nfitness effect in various environments.", "published": "2025-03-30 10:31:55", "link": "http://arxiv.org/abs/2503.23387v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "JavisDiT: Joint Audio-Video Diffusion Transformer with Hierarchical Spatio-Temporal Prior Synchronization", "abstract": "This paper introduces JavisDiT, a novel Joint Audio-Video Diffusion\nTransformer designed for synchronized audio-video generation (JAVG). Built upon\nthe powerful Diffusion Transformer (DiT) architecture, JavisDiT is able to\ngenerate high-quality audio and video content simultaneously from open-ended\nuser prompts. To ensure optimal synchronization, we introduce a fine-grained\nspatio-temporal alignment mechanism through a Hierarchical Spatial-Temporal\nSynchronized Prior (HiST-Sypo) Estimator. This module extracts both global and\nfine-grained spatio-temporal priors, guiding the synchronization between the\nvisual and auditory components. Furthermore, we propose a new benchmark,\nJavisBench, consisting of 10,140 high-quality text-captioned sounding videos\nspanning diverse scenes and complex real-world scenarios. Further, we\nspecifically devise a robust metric for evaluating the synchronization between\ngenerated audio-video pairs in real-world complex content. Experimental results\ndemonstrate that JavisDiT significantly outperforms existing methods by\nensuring both high-quality generation and precise synchronization, setting a\nnew standard for JAVG tasks. Our code, model, and dataset will be made publicly\navailable at https://javisdit.github.io/.", "published": "2025-03-30 09:40:42", "link": "http://arxiv.org/abs/2503.23377v1", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
