{"title": "From Speech-to-Speech Translation to Automatic Dubbing", "abstract": "We present enhancements to a speech-to-speech translation pipeline in order\nto perform automatic dubbing. Our architecture features neural machine\ntranslation generating output of preferred length, prosodic alignment of the\ntranslation with the original speech segments, neural text-to-speech with fine\ntuning of the duration of each utterance, and, finally, audio rendering to\nenriches text-to-speech output with background noise and reverberation\nextracted from the original audio. We report on a subjective evaluation of\nautomatic dubbing of excerpts of TED Talks from English into Italian, which\nmeasures the perceived naturalness of automatic dubbing and the relative\nimportance of each proposed enhancement.", "published": "2020-01-19 07:03:05", "link": "http://arxiv.org/abs/2001.06785v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A multimodal deep learning approach for named entity recognition from\n  social media", "abstract": "Named Entity Recognition (NER) from social media posts is a challenging task.\nUser generated content that forms the nature of social media, is noisy and\ncontains grammatical and linguistic errors. This noisy content makes it much\nharder for tasks such as named entity recognition. We propose two novel deep\nlearning approaches utilizing multimodal deep learning and Transformers. Both\nof our approaches use image features from short social media posts to provide\nbetter results on the NER task. On the first approach, we extract image\nfeatures using InceptionV3 and use fusion to combine textual and image\nfeatures. This presents more reliable name entity recognition when the images\nrelated to the entities are provided by the user. On the second approach, we\nuse image features combined with text and feed it into a BERT like Transformer.\nThe experimental results, namely, the precision, recall and F1 score metrics\nshow the superiority of our work compared to other state-of-the-art NER\nsolutions.", "published": "2020-01-19 19:37:45", "link": "http://arxiv.org/abs/2001.06888v3", "categories": ["cs.CL", "cs.LG", "cs.MM", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Deep Learning for Hindi Text Classification: A Comparison", "abstract": "Natural Language Processing (NLP) and especially natural language text\nanalysis have seen great advances in recent times. Usage of deep learning in\ntext processing has revolutionized the techniques for text processing and\nachieved remarkable results. Different deep learning architectures like CNN,\nLSTM, and very recent Transformer have been used to achieve state of the art\nresults variety on NLP tasks. In this work, we survey a host of deep learning\narchitectures for text classification tasks. The work is specifically concerned\nwith the classification of Hindi text. The research in the classification of\nmorphologically rich and low resource Hindi language written in Devanagari\nscript has been limited due to the absence of large labeled corpus. In this\nwork, we used translated versions of English data-sets to evaluate models based\non CNN, LSTM and Attention. Multilingual pre-trained sentence embeddings based\non BERT and LASER are also compared to evaluate their effectiveness for the\nHindi language. The paper also serves as a tutorial for popular text\nclassification techniques.", "published": "2020-01-19 09:29:12", "link": "http://arxiv.org/abs/2001.10340v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
