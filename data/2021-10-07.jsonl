{"title": "A Comparative Study of Transformer-Based Language Models on Extractive\n  Question Answering", "abstract": "Question Answering (QA) is a task in natural language processing that has\nseen considerable growth after the advent of transformers. There has been a\nsurge in QA datasets that have been proposed to challenge natural language\nprocessing models to improve human and existing model performance. Many\npre-trained language models have proven to be incredibly effective at the task\nof extractive question answering. However, generalizability remains as a\nchallenge for the majority of these models. That is, some datasets require\nmodels to reason more than others. In this paper, we train various pre-trained\nlanguage models and fine-tune them on multiple question answering datasets of\nvarying levels of difficulty to determine which of the models are capable of\ngeneralizing the most comprehensively across different datasets. Further, we\npropose a new architecture, BERT-BiLSTM, and compare it with other language\nmodels to determine if adding more bidirectionality can improve model\nperformance. Using the F1-score as our metric, we find that the RoBERTa and\nBART pre-trained models perform the best across all datasets and that our\nBERT-BiLSTM model outperforms the baseline BERT model.", "published": "2021-10-07 02:23:19", "link": "http://arxiv.org/abs/2110.03142v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transliteration of Foreign Words in Burmese", "abstract": "This manuscript provides general descriptions on transliteration of foreign\nwords in the Burmese language. Phenomena caused by phonetic and orthographic\nissues are discussed. Based on this work, we expect to gradually establish\nprescriptive guidelines to normalize the transliteration on modern words in\nBurmese.", "published": "2021-10-07 03:36:02", "link": "http://arxiv.org/abs/2110.03163v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HowSumm: A Multi-Document Summarization Dataset Derived from WikiHow\n  Articles", "abstract": "We present HowSumm, a novel large-scale dataset for the task of query-focused\nmulti-document summarization (qMDS), which targets the use-case of generating\nactionable instructions from a set of sources. This use-case is different from\nthe use-cases covered in existing multi-document summarization (MDS) datasets\nand is applicable to educational and industrial scenarios. We employed\nautomatic methods, and leveraged statistics from existing human-crafted qMDS\ndatasets, to create HowSumm from wikiHow website articles and the sources they\ncite. We describe the creation of the dataset and discuss the unique features\nthat distinguish it from other summarization corpora. Automatic and human\nevaluations of both extractive and abstractive summarization models on the\ndataset reveal that there is room for improvement.", "published": "2021-10-07 04:44:32", "link": "http://arxiv.org/abs/2110.03179v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Influence Tuning: Demoting Spurious Correlations via Instance\n  Attribution and Instance-Driven Updates", "abstract": "Among the most critical limitations of deep learning NLP models are their\nlack of interpretability, and their reliance on spurious correlations. Prior\nwork proposed various approaches to interpreting the black-box models to unveil\nthe spurious correlations, but the research was primarily used in\nhuman-computer interaction scenarios. It still remains underexplored whether or\nhow such model interpretations can be used to automatically \"unlearn\"\nconfounding features. In this work, we propose influence tuning--a procedure\nthat leverages model interpretations to update the model parameters towards a\nplausible interpretation (rather than an interpretation that relies on spurious\npatterns in the data) in addition to learning to predict the task labels. We\nshow that in a controlled setup, influence tuning can help deconfounding the\nmodel from spurious patterns in data, significantly outperforming baseline\nmethods that use adversarial training.", "published": "2021-10-07 06:59:46", "link": "http://arxiv.org/abs/2110.03212v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Layer-wise Pruning of Transformer Attention Heads for Efficient Language\n  Modeling", "abstract": "While Transformer-based models have shown impressive language modeling\nperformance, the large computation cost is often prohibitive for practical use.\nAttention head pruning, which removes unnecessary attention heads in the\nmultihead attention, is a promising technique to solve this problem. However,\nit does not evenly reduce the overall load because the heavy feedforward module\nis not affected by head pruning. In this paper, we apply layer-wise attention\nhead pruning on All-attention Transformer so that the entire computation and\nthe number of parameters can be reduced proportionally to the number of pruned\nheads. While the architecture has the potential to fully utilize head pruning,\nwe propose three training methods that are especially helpful to minimize\nperformance degradation and stabilize the pruning process. Our pruned model\nshows consistently lower perplexity within a comparable parameter size than\nTransformer-XL on WikiText-103 language modeling benchmark.", "published": "2021-10-07 08:19:26", "link": "http://arxiv.org/abs/2110.03252v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-tasking Dialogue Comprehension with Discourse Parsing", "abstract": "Multi-party dialogue machine reading comprehension (MRC) raises an even more\nchallenging understanding goal on dialogue with more than two involved\nspeakers, compared with the traditional plain passage style MRC. To accurately\nperform the question-answering (QA) task according to such multi-party\ndialogue, models have to handle fundamentally different discourse relationships\nfrom common non-dialogue plain text, where discourse relations are supposed to\nconnect two far apart utterances in a linguistics-motivated way.To further\nexplore the role of such unusual discourse structure on the correlated QA task\nin terms of MRC, we propose the first multi-task model for jointly performing\nQA and discourse parsing (DP) on the multi-party dialogue MRC task. Our\nproposed model is evaluated on the latest benchmark Molweni, whose results\nindicate that training with complementary tasks indeed benefits not only QA\ntask, but also DP task itself. We further find that the joint model is\ndistinctly stronger when handling longer dialogues which again verifies the\nnecessity of DP in the related MRC.", "published": "2021-10-07 08:51:49", "link": "http://arxiv.org/abs/2110.03269v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Logic-Based Framework for Natural Language Inference in Dutch", "abstract": "We present a framework for deriving inference relations between Dutch\nsentence pairs. The proposed framework relies on logic-based reasoning to\nproduce inspectable proofs leading up to inference labels; its judgements are\ntherefore transparent and formally verifiable. At its core, the system is\npowered by two ${\\lambda}$-calculi, used as syntactic and semantic theories,\nrespectively. Sentences are first converted to syntactic proofs and terms of\nthe linear ${\\lambda}$-calculus using a choice of two parsers: an Alpino-based\npipeline, and Neural Proof Nets. The syntactic terms are then converted to\nsemantic terms of the simply typed ${\\lambda}$-calculus, via a set of hand\ndesigned type- and term-level transformations. Pairs of semantic terms are then\nfed to an automated theorem prover for natural logic which reasons with them\nwhile using the lexical relations found in the Open Dutch WordNet. We evaluate\nthe reasoning pipeline on the recently created Dutch natural language inference\ndataset, and achieve promising results, remaining only within a $1.1-3.2{\\%}$\nperformance margin to strong neural baselines. To the best of our knowledge,\nthe reasoning pipeline is the first logic-based system for Dutch.", "published": "2021-10-07 10:34:46", "link": "http://arxiv.org/abs/2110.03323v3", "categories": ["cs.CL", "F.4.1; I.2.7"], "primary_category": "cs.CL"}
{"title": "Cross-Language Learning for Entity Matching", "abstract": "Transformer-based entity matching methods have significantly moved the state\nof the art for less-structured matching tasks such as matching product offers\nin e-commerce. In order to excel at these tasks, Transformer-based matching\nmethods require a decent amount of training pairs. Providing enough training\ndata can be challenging, especially if a matcher for non-English product\ndescriptions should be learned. This poster explores along the use case of\nmatching product offers from different e-shops to which extent it is possible\nto improve the performance of Transformer-based matchers by complementing a\nsmall set of training pairs in the target language, German in our case, with a\nlarger set of English-language training pairs. Our experiments using different\nTransformers show that extending the German set with English pairs improves the\nmatching performance in all cases. The impact of adding the English pairs is\nespecially high in low-resource settings in which only a rather small number of\nnon-English pairs is available. As it is often possible to automatically gather\nEnglish training pairs from the Web by exploiting schema.org annotations, our\nresults are relevant for many product matching scenarios targeting low-resource\nlanguages.", "published": "2021-10-07 11:08:31", "link": "http://arxiv.org/abs/2110.03338v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridge to Target Domain by Prototypical Contrastive Learning and Label\n  Confusion: Re-explore Zero-Shot Learning for Slot Filling", "abstract": "Zero-shot cross-domain slot filling alleviates the data dependence in the\ncase of data scarcity in the target domain, which has aroused extensive\nresearch. However, as most of the existing methods do not achieve effective\nknowledge transfer to the target domain, they just fit the distribution of the\nseen slot and show poor performance on unseen slot in the target domain. To\nsolve this, we propose a novel approach based on prototypical contrastive\nlearning with a dynamic label confusion strategy for zero-shot slot filling.\nThe prototypical contrastive learning aims to reconstruct the semantic\nconstraints of labels, and we introduce the label confusion strategy to\nestablish the label dependence between the source domains and the target domain\non-the-fly. Experimental results show that our model achieves significant\nimprovement on the unseen slots, while also set new state-of-the-arts on slot\nfilling task.", "published": "2021-10-07 15:50:56", "link": "http://arxiv.org/abs/2110.03572v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Retriever-Ranker for dense text retrieval", "abstract": "Current dense text retrieval models face two typical challenges. First, they\nadopt a siamese dual-encoder architecture to encode queries and documents\nindependently for fast indexing and searching, while neglecting the\nfiner-grained term-wise interactions. This results in a sub-optimal recall\nperformance. Second, their model training highly relies on a negative sampling\ntechnique to build up the negative documents in their contrastive losses. To\naddress these challenges, we present Adversarial Retriever-Ranker (AR2), which\nconsists of a dual-encoder retriever plus a cross-encoder ranker. The two\nmodels are jointly optimized according to a minimax adversarial objective: the\nretriever learns to retrieve negative documents to cheat the ranker, while the\nranker learns to rank a collection of candidates including both the\nground-truth and the retrieved ones, as well as providing progressive direct\nfeedback to the dual-encoder retriever. Through this adversarial game, the\nretriever gradually produces harder negative documents to train a better\nranker, whereas the cross-encoder ranker provides progressive feedback to\nimprove retriever. We evaluate AR2 on three benchmarks. Experimental results\nshow that AR2 consistently and significantly outperforms existing dense\nretriever methods and achieves new state-of-the-art results on all of them.\nThis includes the improvements on Natural Questions R@5 to 77.9%(+2.1%),\nTriviaQA R@5 to 78.2%(+1.4), and MS-MARCO MRR@10 to 39.5%(+1.3%). Code and\nmodels are available at https://github.com/microsoft/AR2.", "published": "2021-10-07 16:41:15", "link": "http://arxiv.org/abs/2110.03611v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UoB at SemEval-2021 Task 5: Extending Pre-Trained Language Models to\n  Include Task and Domain-Specific Information for Toxic Span Prediction", "abstract": "Toxicity is pervasive in social media and poses a major threat to the health\nof online communities. The recent introduction of pre-trained language models,\nwhich have achieved state-of-the-art results in many NLP tasks, has transformed\nthe way in which we approach natural language processing. However, the inherent\nnature of pre-training means that they are unlikely to capture task-specific\nstatistical information or learn domain-specific knowledge. Additionally, most\nimplementations of these models typically do not employ conditional random\nfields, a method for simultaneous token classification. We show that these\nmodifications can improve model performance on the Toxic Spans Detection task\nat SemEval-2021 to achieve a score within 4 percentage points of the top\nperforming team.", "published": "2021-10-07 18:29:06", "link": "http://arxiv.org/abs/2110.03730v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GNN is a Counter? Revisiting GNN for Question Answering", "abstract": "Question Answering (QA) has been a long-standing research topic in AI and NLP\nfields, and a wealth of studies have been conducted to attempt to equip QA\nsystems with human-level reasoning capability. To approximate the complicated\nhuman reasoning process, state-of-the-art QA systems commonly use pre-trained\nlanguage models (LMs) to access knowledge encoded in LMs together with\nelaborately designed modules based on Graph Neural Networks (GNNs) to perform\nreasoning over knowledge graphs (KGs). However, many problems remain open\nregarding the reasoning functionality of these GNN-based modules. Can these\nGNN-based modules really perform a complex reasoning process? Are they under-\nor over-complicated for QA? To open the black box of GNN and investigate these\nproblems, we dissect state-of-the-art GNN modules for QA and analyze their\nreasoning capability. We discover that even a very simple graph neural counter\ncan outperform all the existing GNN modules on CommonsenseQA and OpenBookQA,\ntwo popular QA benchmark datasets which heavily rely on knowledge-aware\nreasoning. Our work reveals that existing knowledge-aware GNN modules may only\ncarry out some simple reasoning such as counting. It remains a challenging open\nproblem to build comprehensive reasoning modules for knowledge-powered QA.", "published": "2021-10-07 05:44:52", "link": "http://arxiv.org/abs/2110.03192v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Towards Continual Knowledge Learning of Language Models", "abstract": "Large Language Models (LMs) are known to encode world knowledge in their\nparameters as they pretrain on a vast amount of web corpus, which is often\nutilized for performing knowledge-dependent downstream tasks such as question\nanswering, fact-checking, and open dialogue. In real-world scenarios, the world\nknowledge stored in the LMs can quickly become outdated as the world changes,\nbut it is non-trivial to avoid catastrophic forgetting and reliably acquire new\nknowledge while preserving invariant knowledge. To push the community towards\nbetter maintenance of ever-changing LMs, we formulate a new continual learning\n(CL) problem called Continual Knowledge Learning (CKL). We construct a new\nbenchmark and metric to quantify the retention of time-invariant world\nknowledge, the update of outdated knowledge, and the acquisition of new\nknowledge. We adopt applicable recent methods from literature to create several\nstrong baselines. Through extensive experiments, we find that CKL exhibits\nunique challenges that are not addressed in previous CL setups, where parameter\nexpansion is necessary to reliably retain and learn knowledge simultaneously.\nBy highlighting the critical causes of knowledge forgetting, we show that CKL\nis a challenging and important problem that helps us better understand and\ntrain ever-changing LMs. The benchmark datasets, evaluation script, and\nbaseline code to reproduce our results are available at\nhttps://github.com/joeljang/continual-knowledge-learning.", "published": "2021-10-07 07:00:57", "link": "http://arxiv.org/abs/2110.03215v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Situated Dialogue Learning through Procedural Environment Generation", "abstract": "We teach goal-driven agents to interactively act and speak in situated\nenvironments by training on generated curriculums. Our agents operate in LIGHT\n(Urbanek et al. 2019) -- a large-scale crowd-sourced fantasy text adventure\ngame wherein an agent perceives and interacts with the world through textual\nnatural language. Goals in this environment take the form of character-based\nquests, consisting of personas and motivations. We augment LIGHT by learning to\nprocedurally generate additional novel textual worlds and quests to create a\ncurriculum of steadily increasing difficulty for training agents to achieve\nsuch goals. In particular, we measure curriculum difficulty in terms of the\nrarity of the quest in the original training distribution -- an easier\nenvironment is one that is more likely to have been found in the unaugmented\ndataset. An ablation study shows that this method of learning from the tail of\na distribution results in significantly higher generalization abilities as\nmeasured by zero-shot performance on never-before-seen quests.", "published": "2021-10-07 08:36:36", "link": "http://arxiv.org/abs/2110.03262v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VisualTTS: TTS with Accurate Lip-Speech Synchronization for Automatic\n  Voice Over", "abstract": "In this paper, we formulate a novel task to synthesize speech in sync with a\nsilent pre-recorded video, denoted as automatic voice over (AVO). Unlike\ntraditional speech synthesis, AVO seeks to generate not only human-sounding\nspeech, but also perfect lip-speech synchronization. A natural solution to AVO\nis to condition the speech rendering on the temporal progression of lip\nsequence in the video. We propose a novel text-to-speech model that is\nconditioned on visual input, named VisualTTS, for accurate lip-speech\nsynchronization. The proposed VisualTTS adopts two novel mechanisms that are 1)\ntextual-visual attention, and 2) visual fusion strategy during acoustic\ndecoding, which both contribute to forming accurate alignment between the input\ntext content and lip motion in input lip sequence. Experimental results show\nthat VisualTTS achieves accurate lip-speech synchronization and outperforms all\nbaseline systems.", "published": "2021-10-07 11:25:25", "link": "http://arxiv.org/abs/2110.03342v3", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "WenetSpeech: A 10000+ Hours Multi-domain Mandarin Corpus for Speech\n  Recognition", "abstract": "In this paper, we present WenetSpeech, a multi-domain Mandarin corpus\nconsisting of 10000+ hours high-quality labeled speech, 2400+ hours weakly\nlabeled speech, and about 10000 hours unlabeled speech, with 22400+ hours in\ntotal. We collect the data from YouTube and Podcast, which covers a variety of\nspeaking styles, scenarios, domains, topics, and noisy conditions. An optical\ncharacter recognition (OCR) based method is introduced to generate the\naudio/text segmentation candidates for the YouTube data on its corresponding\nvideo captions, while a high-quality ASR transcription system is used to\ngenerate audio/text pair candidates for the Podcast data. Then we propose a\nnovel end-to-end label error detection approach to further validate and filter\nthe candidates. We also provide three manually labelled high-quality test sets\nalong with WenetSpeech for evaluation -- Dev for cross-validation purpose in\ntraining, Test_Net, collected from Internet for matched test, and\nTest\\_Meeting, recorded from real meetings for more challenging mismatched\ntest. Baseline systems trained with WenetSpeech are provided for three popular\nspeech recognition toolkits, namely Kaldi, ESPnet, and WeNet, and recognition\nresults on the three test sets are also provided as benchmarks. To the best of\nour knowledge, WenetSpeech is the current largest open-sourced Mandarin speech\ncorpus with transcriptions, which benefits research on production-level speech\nrecognition.", "published": "2021-10-07 12:05:29", "link": "http://arxiv.org/abs/2110.03370v5", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
{"title": "Advancing the dimensionality reduction of speaker embeddings for speaker\n  diarisation: disentangling noise and informing speech activity", "abstract": "The objective of this work is to train noise-robust speaker embeddings\nadapted for speaker diarisation. Speaker embeddings play a crucial role in the\nperformance of diarisation systems, but they often capture spurious information\nsuch as noise, adversely affecting performance. Our previous work has proposed\nan auto-encoder-based dimensionality reduction module to help remove the\nredundant information. However, they do not explicitly separate such\ninformation and have also been found to be sensitive to hyper-parameter values.\nTo this end, we propose two contributions to overcome these issues: (i) a novel\ndimensionality reduction framework that can disentangle spurious information\nfrom the speaker embeddings; (ii) the use of speech activity vector to prevent\nthe speaker code from representing the background noise. Through a range of\nexperiments conducted on four datasets, our approach consistently demonstrates\nthe state-of-the-art performance among models without system fusion.", "published": "2021-10-07 12:19:09", "link": "http://arxiv.org/abs/2110.03380v3", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
{"title": "Beam Search with Bidirectional Strategies for Neural Response Generation", "abstract": "Sequence-to-sequence neural networks have been widely used in language-based\napplications as they have flexible capabilities to learn various language\nmodels. However, when seeking for the optimal language response through trained\nneural networks, current existing approaches such as beam-search decoder\nstrategies are still not able reaching to promising performances. Instead of\ndeveloping various decoder strategies based on a \"regular sentence order\"\nneural network (a trained model by outputting sentences from left-to-right\norder), we leveraged \"reverse\" order as additional language model (a trained\nmodel by outputting sentences from right-to-left order) which can provide\ndifferent perspectives for the path finding problems. In this paper, we propose\nbidirectional strategies in searching paths by combining two networks\n(left-to-right and right-to-left language models) making a bidirectional beam\nsearch possible. Besides, our solution allows us using any similarity measure\nin our sentence selection criterion. Our approaches demonstrate better\nperformance compared to the unidirectional beam search strategy.", "published": "2021-10-07 12:27:31", "link": "http://arxiv.org/abs/2110.03389v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mandarin-English Code-switching Speech Recognition with Self-supervised\n  Speech Representation Models", "abstract": "Code-switching (CS) is common in daily conversations where more than one\nlanguage is used within a sentence. The difficulties of CS speech recognition\nlie in alternating languages and the lack of transcribed data. Therefore, this\npaper uses the recently successful self-supervised learning (SSL) methods to\nleverage many unlabeled speech data without CS. We show that hidden\nrepresentations of SSL models offer frame-level language identity even if the\nmodels are trained with English speech only. Jointly training CTC and language\nidentification modules with self-supervised speech representations improves CS\nspeech recognition performance. Furthermore, using multilingual speech data for\npre-training obtains the best CS speech recognition.", "published": "2021-10-07 14:43:35", "link": "http://arxiv.org/abs/2110.03504v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "mRAT-SQL+GAP:A Portuguese Text-to-SQL Transformer", "abstract": "The translation of natural language questions to SQL queries has attracted\ngrowing attention, in particular in connection with transformers and similar\nlanguage models. A large number of techniques are geared towards the English\nlanguage; in this work, we thus investigated translation to SQL when input\nquestions are given in the Portuguese language. To do so, we properly adapted\nstate-of-the-art tools and resources. We changed the RAT-SQL+GAP system by\nrelying on a multilingual BART model (we report tests with other language\nmodels), and we produced a translated version of the Spider dataset. Our\nexperiments expose interesting phenomena that arise when non-English languages\nare targeted; in particular, it is better to train with original and translated\ntraining datasets together, even if a single target language is desired. This\nmultilingual BART model fine-tuned with a double-size training dataset (English\nand Portuguese) achieved 83% of the baseline, making inferences for the\nPortuguese test dataset. This investigation can help other researchers to\nproduce results in Machine Learning in a language different from English. Our\nmultilingual ready version of RAT-SQL+GAP and the data are available,\nopen-sourced as mRAT-SQL+GAP at: https://github.com/C4AI/gap-text2sql", "published": "2021-10-07 15:08:24", "link": "http://arxiv.org/abs/2110.03546v2", "categories": ["cs.CL", "cs.AI", "68T07, 68T50", "I.2.7; H.3.3"], "primary_category": "cs.CL"}
{"title": "GeSERA: General-domain Summary Evaluation by Relevance Analysis", "abstract": "We present GeSERA, an open-source improved version of SERA for evaluating\nautomatic extractive and abstractive summaries from the general domain. SERA is\nbased on a search engine that compares candidate and reference summaries\n(called queries) against an information retrieval document base (called index).\nSERA was originally designed for the biomedical domain only, where it showed a\nbetter correlation with manual methods than the widely used lexical-based ROUGE\nmethod. In this paper, we take out SERA from the biomedical domain to the\ngeneral one by adapting its content-based method to successfully evaluate\nsummaries from the general domain. First, we improve the query reformulation\nstrategy with POS Tags analysis of general-domain corpora. Second, we replace\nthe biomedical index used in SERA with two article collections from AQUAINT-2\nand Wikipedia. We conduct experiments with TAC2008, TAC2009, and CNNDM\ndatasets. Results show that, in most cases, GeSERA achieves higher correlations\nwith manual evaluation methods than SERA, while it reduces its gap with ROUGE\nfor general-domain summary evaluation. GeSERA even surpasses ROUGE in two cases\nof TAC2009. Finally, we conduct extensive experiments and provide a\ncomprehensive study of the impact of human annotators and the index size on\nsummary evaluation with SERA and GeSERA.", "published": "2021-10-07 15:41:05", "link": "http://arxiv.org/abs/2110.03567v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Transcribe-to-Diarize: Neural Speaker Diarization for Unlimited Number\n  of Speakers using End-to-End Speaker-Attributed ASR", "abstract": "This paper presents Transcribe-to-Diarize, a new approach for neural speaker\ndiarization that uses an end-to-end (E2E) speaker-attributed automatic speech\nrecognition (SA-ASR). The E2E SA-ASR is a joint model that was recently\nproposed for speaker counting, multi-talker speech recognition, and speaker\nidentification from monaural audio that contains overlapping speech. Although\nthe E2E SA-ASR model originally does not estimate any time-related information,\nwe show that the start and end times of each word can be estimated with\nsufficient accuracy from the internal state of the E2E SA-ASR by adding a small\nnumber of learnable parameters. Similar to the target-speaker voice activity\ndetection (TS-VAD)-based diarization method, the E2E SA-ASR model is applied to\nestimate speech activity of each speaker while it has the advantages of (i)\nhandling unlimited number of speakers, (ii) leveraging linguistic information\nfor speaker diarization, and (iii) simultaneously generating speaker-attributed\ntranscriptions. Experimental results on the LibriCSS and AMI corpora show that\nthe proposed method achieves significantly better diarization error rate than\nvarious existing speaker diarization methods when the number of speakers is\nunknown, and achieves a comparable performance to TS-VAD when the number of\nspeakers is given in advance. The proposed method simultaneously generates\nspeaker-attributed transcription with state-of-the-art accuracy.", "published": "2021-10-07 02:48:49", "link": "http://arxiv.org/abs/2110.03151v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Detecting Autism Spectrum Disorders with Machine Learning Models Using\n  Speech Transcripts", "abstract": "Autism spectrum disorder (ASD) can be defined as a neurodevelopmental\ndisorder that affects how children interact, communicate and socialize with\nothers. This disorder can occur in a broad spectrum of symptoms, with varying\neffects and severity. While there is no permanent cure for ASD, early detection\nand proactive treatment can substantially improve the lives of many children.\nCurrent methods to accurately diagnose ASD are invasive, time-consuming, and\ntedious. They can also be subjective perspectives of a number of clinicians\ninvolved, including pediatricians, speech pathologists, psychologists, and\npsychiatrists. New technologies are rapidly emerging that include machine\nlearning models using speech, computer vision from facial, retinal, and brain\nMRI images of patients to accurately and timely detect this disorder. Our\nresearch focuses on computational linguistics and machine learning using speech\ndata from TalkBank, the world's largest spoken language database. We used data\nof both ASD and Typical Development (TD) in children from TalkBank to develop\nmachine learning models to accurately predict ASD. More than 50 features were\nused from specifically two datasets in TalkBank to run our experiments using\nfive different classifiers. Logistic Regression and Random Forest models were\nfound to be the most effective for each of these two main datasets, with an\naccuracy of 0.75. These experiments confirm that while significant\nopportunities exist for improving the accuracy, machine learning models can\nreliably predict ASD status in children for effective diagnosis.", "published": "2021-10-07 09:10:15", "link": "http://arxiv.org/abs/2110.03281v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "End-to-End Supermask Pruning: Learning to Prune Image Captioning Models", "abstract": "With the advancement of deep models, research work on image captioning has\nled to a remarkable gain in raw performance over the last decade, along with\nincreasing model complexity and computational cost. However, surprisingly works\non compression of deep networks for image captioning task has received little\nto no attention. For the first time in image captioning research, we provide an\nextensive comparison of various unstructured weight pruning methods on three\ndifferent popular image captioning architectures, namely Soft-Attention,\nUp-Down and Object Relation Transformer. Following this, we propose a novel\nend-to-end weight pruning method that performs gradual sparsification based on\nweight sensitivity to the training loss. The pruning schemes are then extended\nwith encoder pruning, where we show that conducting both decoder pruning and\ntraining simultaneously prior to the encoder pruning provides good overall\nperformance. Empirically, we show that an 80% to 95% sparse network (up to 75%\nreduction in model size) can either match or outperform its dense counterpart.\nThe code and pre-trained models for Up-Down and Object Relation Transformer\nthat are capable of achieving CIDEr scores >120 on the MS-COCO dataset but with\nonly 8.7 MB and 14.5 MB in model size (size reduction of 96% and 94%\nrespectively against dense versions) are publicly available at\nhttps://github.com/jiahuei/sparse-image-captioning.", "published": "2021-10-07 09:34:00", "link": "http://arxiv.org/abs/2110.03298v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "On the Latent Holes of VAEs for Text Generation", "abstract": "In this paper, we provide the first focused study on the discontinuities\n(aka. holes) in the latent space of Variational Auto-Encoders (VAEs), a\nphenomenon which has been shown to have a detrimental effect on model capacity.\nWhen investigating latent holes, existing works are exclusively centred around\nthe encoder network and they merely explore the existence of holes. We tackle\nthese limitations by proposing a highly efficient Tree-based Decoder-Centric\n(TDC) algorithm for latent hole identification, with a focal point on the text\ndomain. In contrast to past studies, our approach pays attention to the decoder\nnetwork, as a decoder has a direct impact on the model's output quality.\nFurthermore, we provide, for the first time, in-depth empirical analysis of the\nlatent hole phenomenon, investigating several important aspects such as how the\nholes impact VAE algorithms' performance on text generation, and how the holes\nare distributed in the latent space.", "published": "2021-10-07 10:22:04", "link": "http://arxiv.org/abs/2110.03318v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Back from the future: bidirectional CTC decoding using future\n  information in speech recognition", "abstract": "In this paper, we propose a simple but effective method to decode the output\nof Connectionist Temporal Classifier (CTC) model using a bi-directional neural\nlanguage model. The bidirectional language model uses the future as well as the\npast information in order to predict the next output in the sequence. The\nproposed method based on bi-directional beam search takes advantage of the CTC\ngreedy decoding output to represent the noisy future information. Experiments\non the Librispeechdataset demonstrate the superiority of our proposed method\ncompared to baselines using unidirectional decoding. In particular, the boost\ninaccuracy is most apparent at the start of a sequence which is the most\nerroneous part for existing systems based on unidirectional decoding.", "published": "2021-10-07 10:42:02", "link": "http://arxiv.org/abs/2110.03326v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Noisy Text Data: Achilles' Heel of popular transformer based NLP models", "abstract": "In the last few years, the ML community has created a number of new NLP\nmodels based on transformer architecture. These models have shown great\nperformance for various NLP tasks on benchmark datasets, often surpassing SOTA\nresults. Buoyed with this success, one often finds industry practitioners\nactively experimenting with fine-tuning these models to build NLP applications\nfor industry use cases. However, for most datasets that are used by\npractitioners to build industrial NLP applications, it is hard to guarantee the\npresence of any noise in the data. While most transformer based NLP models have\nperformed exceedingly well in transferring the learnings from one dataset to\nanother, it remains unclear how these models perform when fine-tuned on noisy\ntext. We address the open question by Kumar et al. (2020) to explore the\nsensitivity of popular transformer based NLP models to noise in the text data.\nWe continue working with the noise as defined by them -- spelling mistakes &\ntypos (which are the most commonly occurring noise). We show (via experimental\nresults) that these models perform badly on most common NLP tasks namely text\nclassification, textual similarity, NER, question answering, text summarization\non benchmark datasets. We further show that as the noise in data increases, the\nperformance degrades. Our findings suggest that one must be vary of the\npresence of noise in their datasets while fine-tuning popular transformer based\nNLP models.", "published": "2021-10-07 11:45:31", "link": "http://arxiv.org/abs/2110.03353v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pretrained Language Models are Symbolic Mathematics Solvers too!", "abstract": "Solving symbolic mathematics has always been of in the arena of human\ningenuity that needs compositional reasoning and recurrence. However, recent\nstudies have shown that large-scale language models such as transformers are\nuniversal and surprisingly can be trained as a sequence-to-sequence task to\nsolve complex mathematical equations. These large transformer models need\nhumongous amounts of training data to generalize to unseen symbolic mathematics\nproblems. In this paper, we present a sample efficient way of solving the\nsymbolic tasks by first pretraining the transformer model with language\ntranslation and then fine-tuning the pretrained transformer model to solve the\ndownstream task of symbolic mathematics. We achieve comparable accuracy on the\nintegration task with our pretrained model while using around $1.5$ orders of\nmagnitude less number of training samples with respect to the state-of-the-art\ndeep learning for symbolic mathematics. The test accuracy on differential\nequation tasks is considerably lower comparing with integration as they need\nhigher order recursions that are not present in language translations. We\npropose the generalizability of our pretrained language model from Anna\nKarenina Principle (AKP). We pretrain our model with different pairs of\nlanguage translations. Our results show language bias in solving symbolic\nmathematics tasks. Finally, we study the robustness of the fine-tuned model on\nsymbolic math tasks against distribution shift, and our approach generalizes\nbetter in distribution shift scenarios for the function integration.", "published": "2021-10-07 14:37:06", "link": "http://arxiv.org/abs/2110.03501v3", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Magic dust for cross-lingual adaptation of monolingual wav2vec-2.0", "abstract": "We propose a simple and effective cross-lingual transfer learning method to\nadapt monolingual wav2vec-2.0 models for Automatic Speech Recognition (ASR) in\nresource-scarce languages. We show that a monolingual wav2vec-2.0 is a good\nfew-shot ASR learner in several languages. We improve its performance further\nvia several iterations of Dropout Uncertainty-Driven Self-Training (DUST) by\nusing a moderate-sized unlabeled speech dataset in the target language. A key\nfinding of this work is that the adapted monolingual wav2vec-2.0 achieves\nsimilar performance as the topline multilingual XLSR model, which is trained on\nfifty-three languages, on the target language ASR task.", "published": "2021-10-07 15:29:22", "link": "http://arxiv.org/abs/2110.03560v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Applying Phonological Features in Multilingual Text-To-Speech", "abstract": "This study investigates whether phonological features can be applied in\ntext-to-speech systems to generate native and non-native speech in English and\nMandarin. We present a mapping of ARPABET/pinyin to SAMPA/SAMPA-SC and then to\nphonological features. We tested whether this mapping could lead to the\nsuccessful generation of native, non-native, and code-switched speech in the\ntwo languages. We ran two experiments, one with a small dataset and one with a\nlarger dataset. The results proved that phonological features could be used as\na feasible input system, although further investigation is needed to improve\nmodel performance. The accented output generated by the TTS models also helps\nwith understanding human second language acquisition processes.", "published": "2021-10-07 16:37:01", "link": "http://arxiv.org/abs/2110.03609v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Causal Direction of Data Collection Matters: Implications of Causal and\n  Anticausal Learning for NLP", "abstract": "The principle of independent causal mechanisms (ICM) states that generative\nprocesses of real world data consist of independent modules which do not\ninfluence or inform each other. While this idea has led to fruitful\ndevelopments in the field of causal inference, it is not widely-known in the\nNLP community. In this work, we argue that the causal direction of the data\ncollection process bears nontrivial implications that can explain a number of\npublished NLP findings, such as differences in semi-supervised learning (SSL)\nand domain adaptation (DA) performance across different settings. We categorize\ncommon NLP tasks according to their causal direction and empirically assay the\nvalidity of the ICM principle for text data using minimum description length.\nWe conduct an extensive meta-analysis of over 100 published SSL and 30 DA\nstudies, and find that the results are consistent with our expectations based\non causal insights. This work presents the first attempt to analyze the ICM\nprinciple in NLP, and provides constructive suggestions for future modeling\nchoices. Code available at https://github.com/zhijing-jin/icm4nlp", "published": "2021-10-07 16:56:17", "link": "http://arxiv.org/abs/2110.03618v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Contextual Sentence Classification: Detecting Sustainability Initiatives\n  in Company Reports", "abstract": "We introduce the novel task of detecting sustainability initiatives in\ncompany reports. Given a full report, the aim is to automatically identify\nmentions of practical activities that a company has performed in order to\ntackle specific societal issues. New methods for identifying continuous\nsentence spans need to be developed for capturing the multi-sentence structure\nof individual sustainability initiatives. We release a new dataset of company\nreports in which the text has been manually annotated with sustainability\ninitiatives. We also evaluate different models for initiative detection,\nintroducing a novel aggregation and evaluation methodology. Our proposed\narchitecture uses sequences of consecutive sentences to account for contextual\ninformation when making classification decisions at the individual sentence\nlevel.", "published": "2021-10-07 18:28:03", "link": "http://arxiv.org/abs/2110.03727v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Sonorant spectra and coarticulation distinguish speakers with different\n  dialects", "abstract": "The aim of this study is to determine the effect of language varieties on the\nspectral distribution of stressed and unstressed sonorants (nasals /m, n/,\nlateral approximants /l/, and rhotics /r/) and on their coarticulatory effects\non adjacent sounds. To quantify the shape of the spectral distribution, we\ncalculated the spectral moments from the sonorant spectra of nasals /m, n/,\nlateral approximants /l/, and rhotics /r/ produced by Athenian Greek and\nCypriot Greek speakers. To estimate the co-articulatory effects of sonorants on\nthe adjacent vowels' F1 - F4 formant frequencies, we developed polynomial\nmodels of the adjacent vowel's formant contours. We found significant effects\nof language variety (sociolinguistic information) on the spectral moments of\neach sonorant /m/, /n/, /l/, /r/ (except between /m/ and /n/) and on the\nformant contours of the adjacent vowel. All sonorants (including /m/ and /n/)\nhad distinct effects on adjacent vowel's formant contours, especially for F3\nand F4. The study highlights that the combination of spectral moments and\ncoarticulatory effects of sonorants determines linguistic (stress and phonemic\ncategory) and sociolinguistic (language variety) characteristics of sonorants.\nIt also provides the first comparative acoustic analysis of Athenian Greek and\nCypriot Greek sonorants.", "published": "2021-10-07 19:18:18", "link": "http://arxiv.org/abs/2110.03756v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Streaming Transformer Transducer Based Speech Recognition Using\n  Non-Causal Convolution", "abstract": "This paper improves the streaming transformer transducer for speech\nrecognition by using non-causal convolution. Many works apply the causal\nconvolution to improve streaming transformer ignoring the lookahead context. We\npropose to use non-causal convolution to process the center block and lookahead\ncontext separately. This method leverages the lookahead context in convolution\nand maintains similar training and decoding efficiency. Given the similar\nlatency, using the non-causal convolution with lookahead context gives better\naccuracy than causal convolution, especially for open-domain dictation\nscenarios. Besides, this paper applies talking-head attention and a novel\nhistory context compression scheme to further improve the performance. The\ntalking-head attention improves the multi-head self-attention by transferring\ninformation among different heads. The history context compression method\nintroduces more extended history context compactly. On our in-house data, the\nproposed methods improve a small Emformer baseline with lookahead context by\nrelative WERR 5.1\\%, 14.5\\%, 8.4\\% on open-domain dictation, assistant general\nscenarios, and assistant calling scenarios, respectively.", "published": "2021-10-07 21:36:48", "link": "http://arxiv.org/abs/2110.05241v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Improving Bird Classification with Unsupervised Sound Separation", "abstract": "This paper addresses the problem of species classification in bird song\nrecordings. The massive amount of available field recordings of birds presents\nan opportunity to use machine learning to automatically track bird populations.\nHowever, it also poses a problem: such field recordings typically contain\nsignificant environmental noise and overlapping vocalizations that interfere\nwith classification. The widely available training datasets for species\nidentification also typically leave background species unlabeled. This leads\nclassifiers to ignore vocalizations with a low signal-to-noise ratio. However,\nrecent advances in unsupervised sound separation, such as \\emph{mixture\ninvariant training} (MixIT), enable high quality separation of bird songs to be\nlearned from such noisy recordings. In this paper, we demonstrate improved\nseparation quality when training a MixIT model specifically for birdsong data,\noutperforming a general audio separation model by over 5 dB in SI-SNR\nimprovement of reconstructed mixtures. We also demonstrate precision\nimprovements with a downstream multi-species bird classifier across three\nindependent datasets. The best classifier performance is achieved by taking the\nmaximum model activations over the separated channels and original audio.\nFinally, we document additional classifier improvements, including taxonomic\nclassification, augmentation by random low-pass filters, and additional channel\nnormalization.", "published": "2021-10-07 06:48:48", "link": "http://arxiv.org/abs/2110.03209v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Temporal Dynamic Convolutional Neural Network for Text-Independent\n  Speaker Verification and Phonemetic Analysis", "abstract": "In the field of text-independent speaker recognition, dynamic models that\nadapt along the time axis have been proposed to consider the phoneme-varying\ncharacteristics of speech. However, a detailed analysis of how dynamic models\nwork depending on phonemes is insufficient. In this paper, we propose temporal\ndynamic CNN (TDY-CNN) that considers temporal variation of phonemes by applying\nkernels optimally adapting to each time bin. These kernels adapt to time bins\nby applying weighted sum of trained basis kernels. Then, an analysis of how\nadaptive kernels work on different phonemes in various layers is carried out.\nTDY-ResNet-38(x0.5) using six basis kernels improved an equal error rate (EER),\nthe speaker verification performance, by 17.3% compared to the baseline model\nResNet-38(x0.5). In addition, we showed that adaptive kernels depend on phoneme\ngroups and are more phoneme-specific at early layers. The temporal dynamic\nmodel adapts itself to phonemes without explicitly given phoneme information\nduring training, and results show the necessity to consider phoneme variation\nwithin utterances for more accurate and robust text-independent speaker\nverification.", "published": "2021-10-07 07:00:22", "link": "http://arxiv.org/abs/2110.03213v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "FilterAugment: An Acoustic Environmental Data Augmentation Method", "abstract": "Acoustic environments affect acoustic characteristics of sound to be\nrecognized by physically interacting with sound wave propagation. Thus,\ntraining acoustic models for audio and speech tasks requires regularization on\nvarious acoustic environments in order to achieve robust performance in real\nlife applications. We propose FilterAugment, a data augmentation method for\nregularization of acoustic models on various acoustic environments.\nFilterAugment mimics acoustic filters by applying different weights on\nfrequency bands, therefore enables model to extract relevant information from\nwider frequency region. It is an improved version of frequency masking which\nmasks information on random frequency bands. FilterAugment improved sound event\ndetection (SED) model performance by 6.50% while frequency masking only\nimproved 2.13% in terms of polyphonic sound detection score (PSDS). It achieved\nequal error rate (EER) of 1.22% when applied to a text-independent speaker\nverification model, outperforming model used frequency masking with EER of\n1.26%. Prototype of FilterAugment was applied in our participation in DCASE\n2021 challenge task 4, and played a major role in achieving the 3rd rank.", "published": "2021-10-07 09:11:06", "link": "http://arxiv.org/abs/2110.03282v4", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Experimental investigation on STFT phase representations for deep\n  learning-based dysarthric speech detection", "abstract": "Mainstream deep learning-based dysarthric speech detection approaches\ntypically rely on processing the magnitude spectrum of the short-time Fourier\ntransform of input signals, while ignoring the phase spectrum. Although\nconsiderable insight about the structure of a signal can be obtained from the\nmagnitude spectrum, the phase spectrum also contains inherent structures which\nare not immediately apparent due to phase discontinuity. To reveal meaningful\nphase structures, alternative phase representations such as the modified group\ndelay (MGD) and instantaneous frequency (IF) spectra have been investigated in\nseveral applications. The objective of this paper is to investigate the\napplicability of the unprocessed phase, MGD, and IF spectra for dysarthric\nspeech detection. Experimental results show that dysarthric cues are present in\nall considered phase representations. Further, it is shown that using phase\nrepresentations as complementary features to the magnitude spectrum is\nbeneficial for deep learning-based dysarthric speech detection, with the\ncombination of magnitude and IF spectra yielding a high performance. The\npresented results should raise awareness in the research community about the\npotential of the phase spectrum for dysarthric speech detection and motivate\nresearch into novel architectures which optimally exploit magnitude and phase\ninformation.", "published": "2021-10-07 09:11:08", "link": "http://arxiv.org/abs/2110.03283v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Explaining deep learning models for spoofing and deepfake detection with\n  SHapley Additive exPlanations", "abstract": "Substantial progress in spoofing and deepfake detection has been made in\nrecent years. Nonetheless, the community has yet to make notable inroads in\nproviding an explanation for how a classifier produces its output. The\ndominance of black box spoofing detection solutions is at further odds with the\ndrive toward trustworthy, explainable artificial intelligence. This paper\ndescribes our use of SHapley Additive exPlanations (SHAP) to gain new insights\nin spoofing detection. We demonstrate use of the tool in revealing unexpected\nclassifier behaviour, the artefacts that contribute most to classifier outputs\nand differences in the behaviour of competing spoofing detection models. The\ntool is both efficient and flexible, being readily applicable to a host of\ndifferent architecture models in addition to related, different applications.\nAll results reported in the paper are reproducible using open-source software.", "published": "2021-10-07 10:00:04", "link": "http://arxiv.org/abs/2110.03309v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Knowledge Distillation for Neural Transducers from Large Self-Supervised\n  Pre-trained Models", "abstract": "Self-supervised pre-training is an effective approach to leveraging a large\namount of unlabelled data to reduce word error rates (WERs) of automatic speech\nrecognition (ASR) systems. Since it is impractical to use large pre-trained\nmodels for many real-world ASR applications, it is desirable to have a much\nsmaller model while retaining the performance of the pre-trained model. In this\npaper, we propose a simple knowledge distillation (KD) loss function for neural\ntransducers that focuses on the one-best path in the output probability lattice\nunder both streaming and non-streaming setups, which allows a small student\nmodel to approach the performance of the large pre-trained teacher model.\nExperiments on the LibriSpeech dataset show that despite being 10 times smaller\nthan the teacher model, the proposed loss results in relative WER reductions\n(WERRs) of 11.5% and 6.8% on the test-other set for non-streaming and streaming\nstudent models compared to the baseline transducers trained without KD using\nthe labelled 100-hour clean data. With an additional 860 hours of unlabelled\ndata for KD, the WERRs increase to 48.2% and 38.5% for non-streaming and\nstreaming students. If language model shallow fusion is used for producing\ndistillation targets, a further improvement in the student model is observed.", "published": "2021-10-07 11:03:43", "link": "http://arxiv.org/abs/2110.03334v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Enhanced Memory Network: The novel network structure for Symbolic Music\n  Generation", "abstract": "Symbolic melodies generation is one of the essential tasks for automatic\nmusic generation. Recently, models based on neural networks have had a\nsignificant influence on generating symbolic melodies. However, the musical\ncontext structure is complicated to capture through deep neural networks.\nAlthough long short-term memory (LSTM) is attempted to solve this problem\nthrough learning order dependence in the musical sequence, it is not capable of\ncapturing musical context with only one note as input for each time step of\nLSTM. In this paper, we propose a novel Enhanced Memory Network (EMN) with\nseveral recurrent units, named Enhanced Memory Unit (EMU), to explicitly modify\nthe internal architecture of LSTM for containing music beat information and\nreinforces the memory of the latest musical beat through aggregating beat\ninside the memory gate. In addition, to increase the diversity of generated\nmusical notes, cosine distance among adjacent time steps of hidden states is\nconsidered as part of loss functions to avoid a high similarity score that\nharms the diversity of generated notes. Objective and subjective evaluation\nresults show that the proposed method achieves state-of-the-art performance.\nCode and music demo are available at https://github.com/qrqrqrqr/EMU", "published": "2021-10-07 12:30:01", "link": "http://arxiv.org/abs/2110.03392v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Analyzing the Robustness of Unsupervised Speech Recognition", "abstract": "Unsupervised speech recognition (unsupervised ASR) aims to learn the ASR\nsystem with non-parallel speech and text corpus only. Wav2vec-U has shown\npromising results in unsupervised ASR by self-supervised speech representations\ncoupled with Generative Adversarial Network (GAN) training, but the robustness\nof the unsupervised ASR framework is unknown. In this work, we further analyze\nthe training robustness of unsupervised ASR on the domain mismatch scenarios in\nwhich the domains of unpaired speech and text are different. Three domain\nmismatch scenarios include: (1) using speech and text from different datasets,\n(2) utilizing noisy/spontaneous speech, and (3) adjusting the amount of speech\nand text data. We also quantify the degree of the domain mismatch by\ncalculating the JS-divergence of phoneme n-gram between the transcription of\nspeech and text. This metric correlates with the performance highly.\nExperimental results show that domain mismatch leads to inferior performance,\nbut a self-supervised model pre-trained on the targeted speech domain can\nextract better representation to alleviate the performance drop.", "published": "2021-10-07 14:46:10", "link": "http://arxiv.org/abs/2110.03509v5", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Accent-Robust Automatic Speech Recognition Using Supervised and\n  Unsupervised Wav2vec Embeddings", "abstract": "Speech recognition models often obtain degraded performance when tested on\nspeech with unseen accents. Domain-adversarial training (DAT) and multi-task\nlearning (MTL) are two common approaches for building accent-robust ASR models.\nASR models using accent embeddings is another approach for improving robustness\nto accents. In this study, we perform systematic comparisons of DAT and MTL\napproaches using a large volume of English accent corpus (4000 hours of US\nEnglish speech and 1244 hours of 20 non-US-English accents speech). We explore\nembeddings trained under supervised and unsupervised settings: a separate\nembedding matrix trained using accent labels, and embeddings extracted from a\nfine-tuned wav2vec model. We find that our DAT model trained with supervised\nembeddings achieves the best performance overall and consistently provides\nbenefits for all testing datasets, and our MTL model trained with wav2vec\nembeddings are helpful learning accent-invariant features and improving\nnovel/unseen accents. We also illustrate that wav2vec embeddings have more\nadvantages for building accent-robust ASR when no accent labels are available\nfor training supervised embeddings.", "published": "2021-10-07 14:51:46", "link": "http://arxiv.org/abs/2110.03520v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Minimum word error training for non-autoregressive Transformer-based\n  code-switching ASR", "abstract": "Non-autoregressive end-to-end ASR framework might be potentially appropriate\nfor code-switching recognition task thanks to its inherent property that\npresent output token being independent of historical ones. However, it still\nunder-performs the state-of-the-art autoregressive ASR frameworks. In this\npaper, we propose various approaches to boosting the performance of a\nCTC-mask-based nonautoregressive Transformer under code-switching ASR scenario.\nTo begin with, we attempt diversified masking method that are closely related\nwith code-switching point, yielding an improved baseline model. More\nimportantly, we employ MinimumWord Error (MWE) criterion to train the model.\nOne of the challenges is how to generate a diversified hypothetical space, so\nas to obtain the average loss for a given ground truth. To address such a\nchallenge, we explore different approaches to yielding desired N-best-based\nhypothetical space. We demonstrate the efficacy of the proposed methods on\nSEAME corpus, a challenging English-Mandarin code-switching corpus for\nSoutheast Asia community. Compared with the crossentropy-trained strong\nbaseline, the proposed MWE training method achieves consistent performance\nimprovement on the test sets.", "published": "2021-10-07 15:53:08", "link": "http://arxiv.org/abs/2110.03573v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Mixer-TTS: non-autoregressive, fast and compact text-to-speech model\n  conditioned on language model embeddings", "abstract": "This paper describes Mixer-TTS, a non-autoregressive model for\nmel-spectrogram generation. The model is based on the MLP-Mixer architecture\nadapted for speech synthesis. The basic Mixer-TTS contains pitch and duration\npredictors, with the latter being trained with an unsupervised TTS alignment\nframework. Alongside the basic model, we propose the extended version which\nadditionally uses token embeddings from a pre-trained language model. Basic\nMixer-TTS and its extended version achieve a mean opinion score (MOS) of 4.05\nand 4.11, respectively, compared to a MOS of 4.27 of original LJSpeech samples.\nBoth versions have a small number of parameters and enable much faster speech\nsynthesis compared to the models with similar quality.", "published": "2021-10-07 16:07:58", "link": "http://arxiv.org/abs/2110.03584v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Optimization of a Fixed Virtual Sensing Feedback ANC Controller for\n  In-Ear Headphones with Multiple Loudspeakers", "abstract": "In this paper we consider an in-ear headphone equipped with an inner\nmicrophone and multiple loudspeakers and we propose an optimization procedure\nwith a convex objective function to derive a fixed multi-loudspeaker ANC\ncontroller aiming at minimizing the sound pressure at the ear drum. Based on\nthe virtual microphone arrangement (VMA) technique and measured acoustic paths\nbetween the loudspeakers and the ear drum, the FIR filters of the ANC\ncontroller are jointly optimized to minimize the power spectral density at the\near drum, subject to design and stability constraints. For an in-ear headphone\nwith two loudspeakers, the proposed multi-loudspeaker VMA controller is\ncompared to two single-loudspeaker VMA controllers. Simulation results with\ndiffuse noise show that the multi-loudspeaker VMA controller effectively\nimproves the attenuation by up to about 10 dB for frequencies below 300 Hz when\ncompared to both single-loudspeaker VMA controllers.", "published": "2021-10-07 16:09:45", "link": "http://arxiv.org/abs/2110.03586v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "On audio enhancement via online non-negative matrix factorization", "abstract": "We propose a method for noise reduction, the task of producing a clean audio\nsignal from a recording corrupted by additive noise. Many common approaches to\nthis problem are based upon applying non-negative matrix factorization to\nspectrogram measurements. These methods use a noiseless recording, which is\nbelieved to be similar in structure to the signal of interest, and a pure-noise\nrecording to learn dictionaries for the true signal and the noise.\n  One may then construct an approximation of the true signal by projecting the\ncorrupted recording on to the clean dictionary. In this work, we build upon\nthese methods by proposing the use of \\emph{online} non-negative matrix\nfactorization for this problem. This method is more memory efficient than\ntraditional non-negative matrix factorization and also has potential\napplications to real-time denoising.", "published": "2021-10-07 00:10:54", "link": "http://arxiv.org/abs/2110.03114v1", "categories": ["eess.AS", "cs.SD", "94A12"], "primary_category": "eess.AS"}
{"title": "A Cough-based deep learning framework for detecting COVID-19", "abstract": "This paper presents a deep learning framework for detecting COVID-19 positive\nsubjects from their cough sounds. In particular, the proposed approach\ncomprises two main steps. In the first step, we generate a feature representing\nthe cough sound by combining an embedding extracted from a pre-trained model\nand handcrafted features extracted from draw audio recording, referred to as\nthe front-end feature extraction. Then, the combined features are fed into\ndifferent back-end classification models for detecting COVID-19 positive\nsubjects in the second step. Our experiments on the Track-2 dataset of the\nSecond 2021 DiCOVA Challenge achieved the second top ranking with an AUC score\nof 81.21 and the top F1 score of 53.21 on a Blind Test set, improving the\nchallenge baseline by 8.43% and 23.4% respectively and showing deployability,\nrobustness and competitiveness with the state-of-the-art systems.", "published": "2021-10-07 08:19:12", "link": "http://arxiv.org/abs/2110.03251v4", "categories": ["cs.SD", "eess.AS", "92-05, 68Txx", "J.3; I.5.4; I.5.2; H.5.5; C.3; K.5"], "primary_category": "cs.SD"}
{"title": "Improving Confidence Estimation on Out-of-Domain Data for End-to-End\n  Speech Recognition", "abstract": "As end-to-end automatic speech recognition (ASR) models reach promising\nperformance, various downstream tasks rely on good confidence estimators for\nthese systems. Recent research has shown that model-based confidence estimators\nhave a significant advantage over using the output softmax probabilities. If\nthe input data to the speech recogniser is from mismatched acoustic and\nlinguistic conditions, the ASR performance and the corresponding confidence\nestimators may exhibit severe degradation. Since confidence models are often\ntrained on the same in-domain data as the ASR, generalising to out-of-domain\n(OOD) scenarios is challenging. By keeping the ASR model untouched, this paper\nproposes two approaches to improve the model-based confidence estimators on OOD\ndata: using pseudo transcriptions and an additional OOD language model. With an\nASR model trained on LibriSpeech, experiments show that the proposed methods\ncan greatly improve the confidence metrics on TED-LIUM and Switchboard datasets\nwhile preserving in-domain performance. Furthermore, the improved confidence\nestimators are better calibrated on OOD data and can provide a much more\nreliable criterion for data selection.", "published": "2021-10-07 10:44:27", "link": "http://arxiv.org/abs/2110.03327v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Towards Universal Neural Vocoding with a Multi-band Excited WaveNet", "abstract": "This paper introduces the Multi-Band Excited WaveNet a neural vocoder for\nspeaking and singing voices. It aims to advance the state of the art towards an\nuniversal neural vocoder, which is a model that can generate voice signals from\narbitrary mel spectrograms extracted from voice signals. Following the success\nof the DDSP model and following the development of the recently proposed\nexcitation vocoders we propose a vocoder structure consisting of multiple\nspecialized DNN that are combined with dedicated signal processing components.\nAll components are implemented as differentiable operators and therefore allow\njoined optimization of the model parameters. To prove the capacity of the model\nto reproduce high quality voice signals we evaluate the model on single and\nmulti speaker/singer datasets. We conduct a subjective evaluation demonstrating\nthat the models support a wide range of domain variations (unseen voices,\nlanguages, expressivity) achieving perceptive quality that compares with a\nstate of the art universal neural vocoder, however using significantly smaller\ntraining datasets and significantly less parameters. We also demonstrate\nremaining limits of the universality of neural vocoders e.g. the creation of\nsaturated singing voices.", "published": "2021-10-07 10:47:03", "link": "http://arxiv.org/abs/2110.03329v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-scale speaker embedding-based graph attention networks for speaker\n  diarisation", "abstract": "The objective of this work is effective speaker diarisation using multi-scale\nspeaker embeddings. Typically, there is a trade-off between the ability to\nrecognise short speaker segments and the discriminative power of the embedding,\naccording to the segment length used for embedding extraction. To this end,\nrecent works have proposed the use of multi-scale embeddings where segments\nwith varying lengths are used. However, the scores are combined using a\nweighted summation scheme where the weights are fixed after the training phase,\nwhereas the importance of segment lengths can differ with in a single session.\nTo address this issue, we present three key contributions in this paper: (1) we\npropose graph attention networks for multi-scale speaker diarisation; (2) we\ndesign scale indicators to utilise scale information of each embedding; (3) we\nadapt the attention-based aggregation to utilise a pre-computed affinity matrix\nfrom multi-scale embeddings. We demonstrate the effectiveness of our method in\nvarious datasets where the speaker confusion which constitutes the primary\nmetric drops over 10% in average relative compared to the baseline.", "published": "2021-10-07 11:59:02", "link": "http://arxiv.org/abs/2110.03361v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Light-SERNet: A lightweight fully convolutional neural network for\n  speech emotion recognition", "abstract": "Detecting emotions directly from a speech signal plays an important role in\neffective human-computer interactions. Existing speech emotion recognition\nmodels require massive computational and storage resources, making them hard to\nimplement concurrently with other machine-interactive tasks in embedded\nsystems. In this paper, we propose an efficient and lightweight fully\nconvolutional neural network for speech emotion recognition in systems with\nlimited hardware resources. In the proposed FCNN model, various feature maps\nare extracted via three parallel paths with different filter sizes. This helps\ndeep convolution blocks to extract high-level features, while ensuring\nsufficient separability. The extracted features are used to classify the\nemotion of the input speech segment. While our model has a smaller size than\nthat of the state-of-the-art models, it achieves higher performance on the\nIEMOCAP and EMO-DB datasets.", "published": "2021-10-07 13:16:31", "link": "http://arxiv.org/abs/2110.03435v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Towards Faster Continuous Multi-Channel HRTF Measurements Based on\n  Learning System Models", "abstract": "Measuring personal head-related transfer functions (HRTFs) is essential in\nbinaural audio. Personal HRTFs are not only required for binaural rendering and\nfor loudspeaker-based binaural reproduction using crosstalk cancellation, but\nthey also serve as a basis for data-driven HRTF individualization techniques\nand psychoacoustic experiments. Although many attempts have been made to\nexpedite HRTF measurements, the rotational velocities in today's measurement\nsystems remain lower than those in natural head movements. To cope with faster\nrotations, we present a novel continuous HRTF measurement method. This method\nestimates the HRTFs offline using a Kalman smoother and learns state-space\nparameters, including the system model, on short signal segments, utilizing the\nexpectation maximization algorithm. We evaluated our method in simulated\nsingle-channel and multi-channel measurements using a rigid sphere HRTF model.\nComparing with conventional methods, we found that the system distances are\nimproved by up to 30 dB.", "published": "2021-10-07 17:18:41", "link": "http://arxiv.org/abs/2110.03630v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Voice Reenactment with F0 and timing constraints and adversarial\n  learning of conversions", "abstract": "This paper introduces voice reenactement as the task of voice conversion (VC)\nin which the expressivity of the source speaker is preserved during conversion\nwhile the identity of a target speaker is transferred. To do so, an original\nneural- VC architecture is proposed based on sequence-to-sequence voice\nconversion (S2S-VC) in which the speech prosody of the source speaker is\npreserved during conversion. First, the S2S-VC architecture is modified so as\nto synchronize the converted speech with the source speech by mean of phonetic\nduration encoding; second, the decoder is conditioned on the desired sequence\nof F0- values and an explicit F0-loss is formulated between the F0 of the\nsource speaker and the one of the converted speech. Besides, an adversarial\nlearning of conversions is integrated within the S2S-VC architecture so as to\nexploit both advantages of reconstruction of original speech and converted\nspeech with manipulated attributes during training and then reducing the\ninconsistency between training and conversion. An experimental evaluation on\nthe VCTK speech database shows that the speech prosody can be efficiently\npreserved during conversion, and that the proposed adversarial learning\nconsistently improves the conversion and the naturalness of the reenacted\nspeech.", "published": "2021-10-07 18:56:13", "link": "http://arxiv.org/abs/2110.03744v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Effect of surface treatment on vibration energy transfer of ultrasonic\n  sonotrode", "abstract": "In this paper, two kinds of ultrasonic radiation rod with surface treatment\n(ion nitriding and vacuum carburizing) are selected to carry out finite element\nanalysis on ultrasonic vibration system and casting system, and explore the\ninfluence of surface treatment on vibration energy transmission of radiation\nrod. The cavitation field of radiation rod with different surface treatment in\nwater was obtained through the cavitation erosion area of aluminum foil in\nwater by using the aluminum foil cavitation experiment, so as to verify the\nsimulation results of sound pressure field in aluminum melt. The results show\nthat the surface treatment weakens the vibration response of the radiating rod,\nreduces the longitudinal amplitude of the radiating rod, and reduces the\namplitude of sound pressure transmitted into the aluminum melt.", "published": "2021-10-07 10:54:26", "link": "http://arxiv.org/abs/2110.12842v1", "categories": ["cond-mat.mtrl-sci", "eess.AS"], "primary_category": "cond-mat.mtrl-sci"}
{"title": "StrengthNet: Deep Learning-based Emotion Strength Assessment for\n  Emotional Speech Synthesis", "abstract": "Recently, emotional speech synthesis has achieved remarkable performance. The\nemotion strength of synthesized speech can be controlled flexibly using a\nstrength descriptor, which is obtained by an emotion attribute ranking\nfunction. However, a trained ranking function on specific data has poor\ngeneralization, which limits its applicability for more realistic cases. In\nthis paper, we propose a deep learning based emotion strength assessment\nnetwork for strength prediction that is referred to as StrengthNet. Our model\nconforms to a multi-task learning framework with a structure that includes an\nacoustic encoder, a strength predictor and an auxiliary emotion predictor. A\ndata augmentation strategy was utilized to improve the model generalization.\nExperiments show that the predicted emotion strength of the proposed\nStrengthNet are highly correlated with ground truth scores for seen and unseen\nspeech. Our codes are available at: https://github.com/ttslr/StrengthNet.", "published": "2021-10-07 03:16:15", "link": "http://arxiv.org/abs/2110.03156v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Transferring Voice Knowledge for Acoustic Event Detection: An Empirical\n  Study", "abstract": "Detection of common events and scenes from audio is useful for extracting and\nunderstanding human contexts in daily life. Prior studies have shown that\nleveraging knowledge from a relevant domain is beneficial for a target acoustic\nevent detection (AED) process. Inspired by the observation that many\nhuman-centered acoustic events in daily life involve voice elements, this paper\ninvestigates the potential of transferring high-level voice representations\nextracted from a public speaker dataset to enrich an AED pipeline. Towards this\nend, we develop a dual-branch neural network architecture for the joint\nlearning of voice and acoustic features during an AED process and conduct\nthorough empirical studies to examine the performance on the public AudioSet\n[1] with different types of inputs. Our main observations are that: 1) Joint\nlearning of audio and voice inputs improves the AED performance (mean average\nprecision) for both a CNN baseline (0.292 vs 0.134 mAP) and a TALNet [2]\nbaseline (0.361 vs 0.351 mAP); 2) Augmenting the extra voice features is\ncritical to maximize the model performance with dual inputs.", "published": "2021-10-07 04:03:21", "link": "http://arxiv.org/abs/2110.03174v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "End-To-End Label Uncertainty Modeling for Speech-based Arousal\n  Recognition Using Bayesian Neural Networks", "abstract": "Emotions are subjective constructs. Recent end-to-end speech emotion\nrecognition systems are typically agnostic to the subjective nature of\nemotions, despite their state-of-the-art performance. In this work, we\nintroduce an end-to-end Bayesian neural network architecture to capture the\ninherent subjectivity in the arousal dimension of emotional expressions. To the\nbest of our knowledge, this work is the first to use Bayesian neural networks\nfor speech emotion recognition. At training, the network learns a distribution\nof weights to capture the inherent uncertainty related to subjective arousal\nannotations. To this end, we introduce a loss term that enables the model to be\nexplicitly trained on a distribution of annotations, rather than training them\nexclusively on mean or gold-standard labels. We evaluate the proposed approach\non the AVEC'16 dataset. Qualitative and quantitative analysis of the results\nreveals that the proposed model can aptly capture the distribution of\nsubjective arousal annotations, with state-of-the-art results in mean and\nstandard deviation estimations for uncertainty modeling.", "published": "2021-10-07 09:34:28", "link": "http://arxiv.org/abs/2110.03299v4", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cloning one's voice using very limited data in the wild", "abstract": "With the increasing popularity of speech synthesis products, the industry has\nput forward more requirements for personalized speech synthesis: (1) How to use\nlow-resource, easily accessible data to clone a person's voice. (2) How to\nclone a person's voice while controlling the style and prosody. To solve the\nabove two problems, we proposed the Hieratron model framework in which the\nprosody and timbre are modeled separately using two modules, therefore, the\nindependent control of timbre and the other characteristics of audio can be\nachieved while generating speech. The practice shows that, for very limited\ntarget speaker data in the wild, Hieratron has obvious advantages over the\ntraditional method, in addition to controlling the style and language of the\ngenerated speech, the mean opinion score on speech quality of the generated\nspeech has also been improved by more than 0.2 points.", "published": "2021-10-07 11:40:08", "link": "http://arxiv.org/abs/2110.03347v2", "categories": ["eess.AS", "cs.HC", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SERAB: A multi-lingual benchmark for speech emotion recognition", "abstract": "Recent developments in speech emotion recognition (SER) often leverage deep\nneural networks (DNNs). Comparing and benchmarking different DNN models can\noften be tedious due to the use of different datasets and evaluation protocols.\nTo facilitate the process, here, we present the Speech Emotion Recognition\nAdaptation Benchmark (SERAB), a framework for evaluating the performance and\ngeneralization capacity of different approaches for utterance-level SER. The\nbenchmark is composed of nine datasets for SER in six languages. Since the\ndatasets have different sizes and numbers of emotional classes, the proposed\nsetup is particularly suitable for estimating the generalization capacity of\npre-trained DNN-based feature extractors. We used the proposed framework to\nevaluate a selection of standard hand-crafted feature sets and state-of-the-art\nDNN representations. The results highlight that using only a subset of the data\nincluded in SERAB can result in biased evaluation, while compliance with the\nproposed protocol can circumvent this issue.", "published": "2021-10-07 13:01:34", "link": "http://arxiv.org/abs/2110.03414v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Peer Collaborative Learning for Polyphonic Sound Event Detection", "abstract": "This paper describes that semi-supervised learning called peer collaborative\nlearning (PCL) can be applied to the polyphonic sound event detection (PSED)\ntask, which is one of the tasks in the Detection and Classification of Acoustic\nScenes and Events (DCASE) challenge. Many deep learning models have been\nstudied to find out what kind of sound events occur where and for how long in a\ngiven audio clip. The characteristic of PCL used in this paper is the\ncombination of ensemble-based knowledge distillation into sub-networks and\nstudent-teacher model-based knowledge distillation, which can train a robust\nPSED model from a small amount of strongly labeled data, weakly labeled data,\nand a large amount of unlabeled data. We evaluated the proposed PCL model using\nthe DCASE 2019 Task 4 datasets and achieved an F1-score improvement of about\n10% compared to the baseline model.", "published": "2021-10-07 14:47:11", "link": "http://arxiv.org/abs/2110.03511v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Direct design of biquad filter cascades with deep learning by sampling\n  random polynomials", "abstract": "Designing infinite impulse response filters to match an arbitrary magnitude\nresponse requires specialized techniques. Methods like modified Yule-Walker are\nrelatively efficient, but may not be sufficiently accurate in matching high\norder responses. On the other hand, iterative optimization techniques often\nenable superior performance, but come at the cost of longer run-times and are\nsensitive to initial conditions, requiring manual tuning. In this work, we\naddress some of these limitations by learning a direct mapping from the target\nmagnitude response to the filter coefficient space with a neural network\ntrained on millions of random filters. We demonstrate our approach enables both\nfast and accurate estimation of filter coefficients given a desired response.\nWe investigate training with different families of random filters, and find\ntraining with a variety of filter families enables better generalization when\nestimating real-world filters, using head-related transfer functions and guitar\ncabinets as case studies. We compare our method against existing methods\nincluding modified Yule-Walker and gradient descent and show our approach is,\non average, both faster and more accurate.", "published": "2021-10-07 17:58:08", "link": "http://arxiv.org/abs/2110.03691v2", "categories": ["eess.SP", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "PEAF: Learnable Power Efficient Analog Acoustic Features for Audio\n  Recognition", "abstract": "At the end of Moore's law, new computing paradigms are required to prolong\nthe battery life of wearable and IoT smart audio devices. Theoretical analysis\nand physical validation have shown that analog signal processing (ASP) can be\nmore power-efficient than its digital counterpart in the realm of low-to-medium\nsignal-to-noise ratio applications. In addition, ASP allows a direct interface\nwith an analog microphone without a power-hungry analog-to-digital converter.\nHere, we present power-efficient analog acoustic features (PEAF) that are\nvalidated by fabricated CMOS chips for running audio recognition. Linear,\nnon-linear, and learnable PEAF variants are evaluated on two speech processing\ntasks that are demanded in many battery-operated devices: wake word detection\n(WWD) and keyword spotting (KWS). Compared to digital acoustic features, higher\npower efficiency with competitive classification accuracy can be obtained. A\nnovel theoretical framework based on information theory is established to\nanalyze the information flow in each individual stage of the feature extraction\npipeline. The analysis identifies the information bottleneck and helps improve\nthe KWS accuracy by up to 7%. This work may pave the way to building more\npower-efficient smart audio devices with best-in-class inference performance.", "published": "2021-10-07 18:05:09", "link": "http://arxiv.org/abs/2110.03715v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Wake-Cough: cough spotting and cougher identification for personalised\n  long-term cough monitoring", "abstract": "We present `wake-cough', an application of wake-word spotting to coughs using\na Resnet50 and the identification of coughers using i-vectors, for the purpose\nof a long-term, personalised cough monitoring system. Coughs, recorded in a\nquiet (73$\\pm$5 dB) and noisy (34$\\pm$17 dB) environment, were used to extract\ni-vectors, x-vectors and d-vectors, used as features to the classifiers. The\nsystem achieves 90.02\\% accuracy when using an MLP to discriminate between 51\ncoughers using 2-sec long cough segments in the noisy environment. When\ndiscriminating between 5 and 14 coughers using longer (100 sec) segments in the\nquiet environment, this accuracy improves to 99.78% and 98.39% respectively.\nUnlike speech, i-vectors outperform x-vectors and d-vectors in identifying\ncoughers. These coughs were added as an extra class to the Google Speech\nCommands dataset and features were extracted by preserving the end-to-end\ntime-domain information in a trigger phrase. The highest accuracy of 88.58% is\nachieved in spotting coughs among 35 other trigger phrases using a Resnet50.\nThus, wake-cough represents a personalised, non-intrusive cough monitoring\nsystem, which is power-efficient as on-device wake-word detection can keep a\nsmartphone-based monitoring device mostly dormant. This makes wake-cough\nextremely attractive in multi-bed ward environments to monitor patients'\nlong-term recovery from lung ailments such as tuberculosis (TB) and COVID-19.", "published": "2021-10-07 20:10:20", "link": "http://arxiv.org/abs/2110.03771v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FAST-RIR: Fast neural diffuse room impulse response generator", "abstract": "We present a neural-network-based fast diffuse room impulse response\ngenerator (FAST-RIR) for generating room impulse responses (RIRs) for a given\nacoustic environment. Our FAST-RIR takes rectangular room dimensions, listener\nand speaker positions, and reverberation time as inputs and generates specular\nand diffuse reflections for a given acoustic environment. Our FAST-RIR is\ncapable of generating RIRs for a given input reverberation time with an average\nerror of 0.02s. We evaluate our generated RIRs in automatic speech recognition\n(ASR) applications using Google Speech API, Microsoft Speech API, and Kaldi\ntools. We show that our proposed FAST-RIR with batch size 1 is 400 times faster\nthan a state-of-the-art diffuse acoustic simulator (DAS) on a CPU and gives\nsimilar performance to DAS in ASR experiments. Our FAST-RIR is 12 times faster\nthan an existing GPU-based RIR generator (gpuRIR). We show that our FAST-RIR\noutperforms gpuRIR by 2.5% in an AMI far-field ASR benchmark.", "published": "2021-10-07 05:21:01", "link": "http://arxiv.org/abs/2110.04057v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Attention is All You Need? Good Embeddings with Statistics are\n  enough:Large Scale Audio Understanding without Transformers/ Convolutions/\n  BERTs/ Mixers/ Attention/ RNNs or ....", "abstract": "This paper presents a way of doing large scale audio understanding without\ntraditional state of the art neural architectures. Ever since the introduction\nof deep learning for understanding audio signals in the past decade,\nconvolutional architectures have been able to achieve state of the art results\nsurpassing traditional hand-crafted features. In the recent past, there has\nbeen a similar shift away from traditional convolutional and recurrent neural\nnetworks towards purely end-to-end Transformer architectures. We, in this work,\nexplore an approach, based on Bag-of-Words model. Our approach does not have\nany convolutions, recurrence, attention, transformers or other approaches such\nas BERT. We utilize micro and macro level clustered vanilla embeddings, and use\na MLP head for classification. We only use feed-forward encoder-decoder models\nto get the bottlenecks of spectral envelops, spectral patches and slices as\nwell as multi-resolution spectra. A classification head (a feed-forward layer),\nsimilar to the approach in SimCLR is trained on a learned representation. Using\nsimple codes learned on latent representations, we show how we surpass\ntraditional convolutional neural network architectures, and come strikingly\nclose to outperforming powerful Transformer architectures. This work hopefully\nwould pave way for exciting advancements in the field of representation\nlearning without massive, end-to-end neural architectures.", "published": "2021-10-07 05:00:26", "link": "http://arxiv.org/abs/2110.03183v5", "categories": ["cs.SD", "cs.AI", "cs.IR", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
