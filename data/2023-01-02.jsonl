{"title": "Statistical Machine Translation for Indic Languages", "abstract": "Machine Translation (MT) system generally aims at automatic representation of\nsource language into target language retaining the originality of context using\nvarious Natural Language Processing (NLP) techniques. Among various NLP\nmethods, Statistical Machine Translation(SMT). SMT uses probabilistic and\nstatistical techniques to analyze information and conversion. This paper\ncanvasses about the development of bilingual SMT models for translating English\nto fifteen low-resource Indian Languages (ILs) and vice versa. At the outset,\nall 15 languages are briefed with a short description related to our\nexperimental need. Further, a detailed analysis of Samanantar and OPUS dataset\nfor model building, along with standard benchmark dataset (Flores-200) for\nfine-tuning and testing, is done as a part of our experiment. Different\npreprocessing approaches are proposed in this paper to handle the noise of the\ndataset. To create the system, MOSES open-source SMT toolkit is explored.\nDistance reordering is utilized with the aim to understand the rules of grammar\nand context-dependent adjustments through a phrase reordering categorization\nframework. In our experiment, the quality of the translation is evaluated using\nstandard metrics such as BLEU, METEOR, and RIBES", "published": "2023-01-02 06:23:12", "link": "http://arxiv.org/abs/2301.00539v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Russia-Ukraine war: Modeling and Clustering the Sentiments Trends of\n  Various Countries", "abstract": "With Twitter's growth and popularity, a huge number of views are shared by\nusers on various topics, making this platform a valuable information source on\nvarious political, social, and economic issues. This paper investigates English\ntweets on the Russia-Ukraine war to analyze trends reflecting users' opinions\nand sentiments regarding the conflict. The tweets' positive and negative\nsentiments are analyzed using a BERT-based model, and the time series\nassociated with the frequency of positive and negative tweets for various\ncountries is calculated. Then, we propose a method based on the neighborhood\naverage for modeling and clustering the time series of countries. The\nclustering results provide valuable insight into public opinion regarding this\nconflict. Among other things, we can mention the similar thoughts of users from\nthe United States, Canada, the United Kingdom, and most Western European\ncountries versus the shared views of Eastern European, Scandinavian, Asian, and\nSouth American nations toward the conflict.", "published": "2023-01-02 11:32:47", "link": "http://arxiv.org/abs/2301.00604v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Active Learning Methods to Strategically Select Essays for\n  Automated Scoring", "abstract": "Research on automated essay scoring has become increasing important because\nit serves as a method for evaluating students' written-responses at scale.\nScalable methods for scoring written responses are needed as students migrate\nto online learning environments resulting in the need to evaluate large numbers\nof written-response assessments. The purpose of this study is to describe and\nevaluate three active learning methods than can be used to minimize the number\nof essays that must be scored by human raters while still providing the data\nneeded to train a modern automated essay scoring system. The three active\nlearning methods are the uncertainty-based, the topological-based, and the\nhybrid method. These three methods were used to select essays included as part\nof the Automated Student Assessment Prize competition that were then classified\nusing a scoring model that was training with the bidirectional encoder\nrepresentations from transformer language model. All three active learning\nmethods produced strong results, with the topological-based method producing\nthe most efficient classification. Growth rate accuracy was also evaluated. The\nactive learning methods produced different levels of efficiency under different\nsample size allocations but, overall, all three methods were highly efficient\nand produced classifications that were similar to one another.", "published": "2023-01-02 12:46:10", "link": "http://arxiv.org/abs/2301.00628v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Follow the Timeline! Generating Abstractive and Extractive Timeline\n  Summary in Chronological Order", "abstract": "Nowadays, time-stamped web documents related to a general news query floods\nspread throughout the Internet, and timeline summarization targets concisely\nsummarizing the evolution trajectory of events along the timeline. Unlike\ntraditional document summarization, timeline summarization needs to model the\ntime series information of the input events and summarize important events in\nchronological order. To tackle this challenge, in this paper, we propose a\nUnified Timeline Summarizer (UTS) that can generate abstractive and extractive\ntimeline summaries in time order. Concretely, in the encoder part, we propose a\ngraph-based event encoder that relates multiple events according to their\ncontent dependency and learns a global representation of each event. In the\ndecoder part, to ensure the chronological order of the abstractive summary, we\npropose to extract the feature of event-level attention in its generation\nprocess with sequential information remained and use it to simulate the\nevolutionary attention of the ground truth summary. The event-level attention\ncan also be used to assist in extracting summary, where the extracted summary\nalso comes in time sequence. We augment the previous Chinese large-scale\ntimeline summarization dataset and collect a new English timeline dataset.\nExtensive experiments conducted on these datasets and on the out-of-domain\nTimeline 17 dataset show that UTS achieves state-of-the-art performance in\nterms of both automatic and human evaluations.", "published": "2023-01-02 20:29:40", "link": "http://arxiv.org/abs/2301.00867v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MAUD: An Expert-Annotated Legal NLP Dataset for Merger Agreement\n  Understanding", "abstract": "Reading comprehension of legal text can be a particularly challenging task\ndue to the length and complexity of legal clauses and a shortage of\nexpert-annotated datasets. To address this challenge, we introduce the Merger\nAgreement Understanding Dataset (MAUD), an expert-annotated reading\ncomprehension dataset based on the American Bar Association's 2021 Public\nTarget Deal Points Study, with over 39,000 examples and over 47,000 total\nannotations. Our fine-tuned Transformer baselines show promising results, with\nmodels performing well above random on most questions. However, on a large\nsubset of questions, there is still room for significant improvement. As the\nonly expert-annotated merger agreement dataset, MAUD is valuable as a benchmark\nfor both the legal profession and the NLP community.", "published": "2023-01-02 21:08:27", "link": "http://arxiv.org/abs/2301.00876v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Political Polarisation using Language Models: A dataset\n  and method", "abstract": "Our paper aims to analyze political polarization in US political system using\nLanguage Models, and thereby help candidates make an informed decision. The\navailability of this information will help voters understand their candidates\nviews on the economy, healthcare, education and other social issues. Our main\ncontributions are a dataset extracted from Wikipedia that spans the past 120\nyears and a Language model based method that helps analyze how polarized a\ncandidate is. Our data is divided into 2 parts, background information and\npolitical information about a candidate, since our hypothesis is that the\npolitical views of a candidate should be based on reason and be independent of\nfactors such as birthplace, alma mater, etc. We further split this data into 4\nphases chronologically, to help understand if and how the polarization amongst\ncandidates changes. This data has been cleaned to remove biases. To understand\nthe polarization we begin by showing results from some classical language\nmodels in Word2Vec and Doc2Vec. And then use more powerful techniques like the\nLongformer, a transformer based encoder, to assimilate more information and\nfind the nearest neighbors of each candidate based on their political view and\ntheir background.", "published": "2023-01-02 22:15:04", "link": "http://arxiv.org/abs/2301.00891v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using meaning instead of words to track topics", "abstract": "The ability to monitor the evolution of topics over time is extremely\nvaluable for businesses. Currently, all existing topic tracking methods use\nlexical information by matching word usage. However, no studies has ever\nexperimented with the use of semantic information for tracking topics. Hence,\nwe explore a novel semantic-based method using word embeddings. Our results\nshow that a semantic-based approach to topic tracking is on par with the\nlexical approach but makes different mistakes. This suggest that both methods\nmay complement each other.", "published": "2023-01-02 08:55:55", "link": "http://arxiv.org/abs/2301.00565v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Undesirable Dependence on Frequency of Gender Bias Metrics Based on\n  Word Embeddings", "abstract": "Numerous works use word embedding-based metrics to quantify societal biases\nand stereotypes in texts. Recent studies have found that word embeddings can\ncapture semantic similarity but may be affected by word frequency. In this work\nwe study the effect of frequency when measuring female vs. male gender bias\nwith word embedding-based bias quantification methods. We find that Skip-gram\nwith negative sampling and GloVe tend to detect male bias in high frequency\nwords, while GloVe tends to return female bias in low frequency words. We show\nthese behaviors still exist when words are randomly shuffled. This proves that\nthe frequency-based effect observed in unshuffled corpora stems from properties\nof the metric rather than from word associations. The effect is spurious and\nproblematic since bias metrics should depend exclusively on word co-occurrences\nand not individual word frequencies. Finally, we compare these results with the\nones obtained with an alternative metric based on Pointwise Mutual Information.\nWe find that this metric does not show a clear dependence on frequency, even\nthough it is slightly skewed towards male bias across all frequencies.", "published": "2023-01-02 18:27:10", "link": "http://arxiv.org/abs/2301.00792v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transformer Based Geocoding", "abstract": "In this paper, we formulate the problem of predicting a geolocation from free\ntext as a sequence-to-sequence problem. Using this formulation, we obtain a\ngeocoding model by training a T5 encoder-decoder transformer model using free\ntext as an input and geolocation as an output. The geocoding model was trained\non geo-tagged wikidump data with adaptive cell partitioning for the geolocation\nrepresentation. All of the code including Rest-based application, dataset and\nmodel checkpoints used in this work are publicly available.", "published": "2023-01-02 10:13:32", "link": "http://arxiv.org/abs/2301.01170v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Analysing Discrete Self Supervised Speech Representation for Spoken\n  Language Modeling", "abstract": "This work profoundly analyzes discrete self-supervised speech representations\n(units) through the eyes of Generative Spoken Language Modeling (GSLM).\nFollowing the findings of such an analysis, we propose practical improvements\nto the discrete unit for the GSLM. First, we start comprehending these units by\nanalyzing them in three axes: interpretation, visualization, and resynthesis.\nOur analysis finds a high correlation between the speech units to phonemes and\nphoneme families, while their correlation with speaker or gender is weaker.\nAdditionally, we found redundancies in the extracted units and claim that one\nreason may be the units' context. Following this analysis, we propose a new,\nunsupervised metric to measure unit redundancies. Finally, we use this metric\nto develop new methods that improve the robustness of units' clustering and\nshow significant improvement considering zero-resource speech metrics such as\nABX. Code and analysis tools are available under the following link:\nhttps://github.com/slp-rl/SLM-Discrete-Representations", "published": "2023-01-02 10:36:40", "link": "http://arxiv.org/abs/2301.00591v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Design and analysis of tweet-based election models for the 2021 Mexican\n  legislative election", "abstract": "Modelling and forecasting real-life human behaviour using online social media\nis an active endeavour of interest in politics, government, academia, and\nindustry. Since its creation in 2006, Twitter has been proposed as a potential\nlaboratory that could be used to gauge and predict social behaviour. During the\nlast decade, the user base of Twitter has been growing and becoming more\nrepresentative of the general population. Here we analyse this user base in the\ncontext of the 2021 Mexican Legislative Election. To do so, we use a dataset of\n15 million election-related tweets in the six months preceding election day. We\nexplore different election models that assign political preference to either\nthe ruling parties or the opposition. We find that models using data with\ngeographical attributes determine the results of the election with better\nprecision and accuracy than conventional polling methods. These results\ndemonstrate that analysis of public online data can outperform conventional\npolling methods, and that political analysis and general forecasting would\nlikely benefit from incorporating such data in the immediate future. Moreover,\nthe same Twitter dataset with geographical attributes is positively correlated\nwith results from official census data on population and internet usage in\nMexico. These findings suggest that we have reached a period in time when\nonline activity, appropriately curated, can provide an accurate representation\nof offline behaviour.", "published": "2023-01-02 12:40:05", "link": "http://arxiv.org/abs/2301.00626v2", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "Tsetlin Machine Embedding: Representing Words Using Logical Expressions", "abstract": "Embedding words in vector space is a fundamental first step in\nstate-of-the-art natural language processing (NLP). Typical NLP solutions\nemploy pre-defined vector representations to improve generalization by\nco-locating similar words in vector space. For instance, Word2Vec is a\nself-supervised predictive model that captures the context of words using a\nneural network. Similarly, GLoVe is a popular unsupervised model incorporating\ncorpus-wide word co-occurrence statistics. Such word embedding has\nsignificantly boosted important NLP tasks, including sentiment analysis,\ndocument classification, and machine translation. However, the embeddings are\ndense floating-point vectors, making them expensive to compute and difficult to\ninterpret. In this paper, we instead propose to represent the semantics of\nwords with a few defining words that are related using propositional logic. To\nproduce such logical embeddings, we introduce a Tsetlin Machine-based\nautoencoder that learns logical clauses self-supervised. The clauses consist of\ncontextual words like \"black,\" \"cup,\" and \"hot\" to define other words like\n\"coffee,\" thus being human-understandable. We evaluate our embedding approach\non several intrinsic and extrinsic benchmarks, outperforming GLoVe on six\nclassification tasks. Furthermore, we investigate the interpretability of our\nembedding using the logical representations acquired during training. We also\nvisualize word clusters in vector space, demonstrating how our logical\nembedding co-locate similar words.", "published": "2023-01-02 15:02:45", "link": "http://arxiv.org/abs/2301.00709v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "IRT2: Inductive Linking and Ranking in Knowledge Graphs of Varying Scale", "abstract": "We address the challenge of building domain-specific knowledge models for\nindustrial use cases, where labelled data and taxonomic information is\ninitially scarce. Our focus is on inductive link prediction models as a basis\nfor practical tools that support knowledge engineers with exploring text\ncollections and discovering and linking new (so-called open-world) entities to\nthe knowledge graph. We argue that - though neural approaches to text mining\nhave yielded impressive results in the past years - current benchmarks do not\nreflect the typical challenges encountered in the industrial wild properly.\nTherefore, our first contribution is an open benchmark coined IRT2 (inductive\nreasoning with text) that (1) covers knowledge graphs of varying sizes\n(including very small ones), (2) comes with incidental, low-quality text\nmentions, and (3) includes not only triple completion but also ranking, which\nis relevant for supporting experts with discovery tasks.\n  We investigate two neural models for inductive link prediction, one based on\nend-to-end learning and one that learns from the knowledge graph and text data\nin separate steps. These models compete with a strong bag-of-words baseline.\nThe results show a significant advance in performance for the neural approaches\nas soon as the available graph data decreases for linking. For ranking, the\nresults are promising, and the neural approaches outperform the sparse\nretriever by a wide margin.", "published": "2023-01-02 15:19:21", "link": "http://arxiv.org/abs/2301.00716v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Concept Knowledge Graph for User Next Intent Prediction at Alipay", "abstract": "This paper illustrates the technologies of user next intent prediction with a\nconcept knowledge graph. The system has been deployed on the Web at Alipay,\nserving more than 100 million daily active users. To explicitly characterize\nuser intent, we propose AlipayKG, which is an offline concept knowledge graph\nin the Life-Service domain modeling the historical behaviors of users, the rich\ncontent interacted by users and the relations between them. We further\nintroduce a Transformer-based model which integrates expert rules from the\nknowledge graph to infer the online user's next intent. Experimental results\ndemonstrate that the proposed system can effectively enhance the performance of\nthe downstream tasks while retaining explainability.", "published": "2023-01-02 02:10:18", "link": "http://arxiv.org/abs/2301.00503v3", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EmoGator: A New Open Source Vocal Burst Dataset with Baseline Machine\n  Learning Classification Methodologies", "abstract": "Vocal Bursts -- short, non-speech vocalizations that convey emotions, such as\nlaughter, cries, sighs, moans, and groans -- are an often-overlooked aspect of\nspeech emotion recognition, but an important aspect of human vocal\ncommunication. One barrier to study of these interesting vocalizations is a\nlack of large datasets. I am pleased to introduce the EmoGator dataset, which\nconsists of 32,130 samples from 357 speakers, 16.9654 hours of audio; each\nsample classified into one of 30 distinct emotion categories by the speaker.\nSeveral different approaches to construct classifiers to identify emotion\ncategories will be discussed, and directions for future research will be\nsuggested. Data set is available for download from\nhttps://github.com/fredbuhl/EmoGator.", "published": "2023-01-02 03:02:10", "link": "http://arxiv.org/abs/2301.00508v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Voice Reconstruction from EEG during Imagined Speech", "abstract": "Translating imagined speech from human brain activity into voice is a\nchallenging and absorbing research issue that can provide new means of human\ncommunication via brain signals. Endeavors toward reconstructing speech from\nbrain activity have shown their potential using invasive measures of spoken\nspeech data, however, have faced challenges in reconstructing imagined speech.\nIn this paper, we propose NeuroTalk, which converts non-invasive brain signals\nof imagined speech into the user's own voice. Our model was trained with spoken\nspeech EEG which was generalized to adapt to the domain of imagined speech,\nthus allowing natural correspondence between the imagined speech and the voice\nas a ground truth. In our framework, automatic speech recognition decoder\ncontributed to decomposing the phonemes of generated speech, thereby displaying\nthe potential of voice reconstruction from unseen words. Our results imply the\npotential of speech synthesis from human EEG signals, not only from spoken\nspeech but also from the brain signals of imagined speech.", "published": "2023-01-02 05:10:31", "link": "http://arxiv.org/abs/2301.07173v1", "categories": ["eess.AS", "cs.HC", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
