{"title": "Towards Leaving No Indic Language Behind: Building Monolingual Corpora,\n  Benchmark and Models for Indic Languages", "abstract": "Building Natural Language Understanding (NLU) capabilities for Indic\nlanguages, which have a collective speaker base of more than one billion\nspeakers is absolutely crucial. In this work, we aim to improve the NLU\ncapabilities of Indic languages by making contributions along 3 important axes\n(i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on\nIndic languages. Specifically, we curate the largest monolingual corpora,\nIndicCorp, with 20.9B tokens covering 24 languages from 4 language families - a\n2.3x increase over prior work, while supporting 12 additional languages. Next,\nwe create a human-supervised benchmark, IndicXTREME, consisting of nine diverse\nNLU tasks covering 20 languages. Across languages and tasks, IndicXTREME\ncontains a total of 105 evaluation sets, of which 52 are new contributions to\nthe literature. To the best of our knowledge, this is the first effort towards\ncreating a standard benchmark for Indic languages that aims to test the\nmultilingual zero-shot capabilities of pretrained language models. Finally, we\ntrain IndicBERT v2, a state-of-the-art model supporting all the languages.\nAveraged across languages and tasks, the model achieves an absolute improvement\nof 2 points over a strong baseline. The data and models are available at\nhttps://github.com/AI4Bharat/IndicBERT.", "published": "2022-12-11 04:45:50", "link": "http://arxiv.org/abs/2212.05409v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Feature-Level Debiased Natural Language Understanding", "abstract": "Natural language understanding (NLU) models often rely on dataset biases\nrather than intended task-relevant features to achieve high performance on\nspecific datasets. As a result, these models perform poorly on datasets outside\nthe training distribution. Some recent studies address this issue by reducing\nthe weights of biased samples during the training process. However, these\nmethods still encode biased latent features in representations and neglect the\ndynamic nature of bias, which hinders model prediction. We propose an NLU\ndebiasing method, named debiasing contrastive learning (DCT), to simultaneously\nalleviate the above problems based on contrastive learning. We devise a\ndebiasing, positive sampling strategy to mitigate biased latent features by\nselecting the least similar biased positive samples. We also propose a dynamic\nnegative sampling strategy to capture the dynamic influence of biases by\nemploying a bias-only model to dynamically select the most similar biased\nnegative samples. We conduct experiments on three NLU benchmark datasets.\nExperimental results show that DCT outperforms state-of-the-art baselines on\nout-of-distribution datasets while maintaining in-distribution performance. We\nalso verify that DCT can reduce biased latent features from the model's\nrepresentation.", "published": "2022-12-11 06:16:14", "link": "http://arxiv.org/abs/2212.05421v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FastClass: A Time-Efficient Approach to Weakly-Supervised Text\n  Classification", "abstract": "Weakly-supervised text classification aims to train a classifier using only\nclass descriptions and unlabeled data. Recent research shows that\nkeyword-driven methods can achieve state-of-the-art performance on various\ntasks. However, these methods not only rely on carefully-crafted class\ndescriptions to obtain class-specific keywords but also require substantial\namount of unlabeled data and takes a long time to train. This paper proposes\nFastClass, an efficient weakly-supervised classification approach. It uses\ndense text representation to retrieve class-relevant documents from external\nunlabeled corpus and selects an optimal subset to train a classifier. Compared\nto keyword-driven methods, our approach is less reliant on initial class\ndescriptions as it no longer needs to expand each class description into a set\nof class-specific keywords. Experiments on a wide range of classification tasks\nshow that the proposed approach frequently outperforms keyword-driven models in\nterms of classification accuracy and often enjoys orders-of-magnitude faster\ntraining speed.", "published": "2022-12-11 13:43:22", "link": "http://arxiv.org/abs/2212.05506v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Associations Between Natural Language Processing (NLP) Enriched Social\n  Determinants of Health and Suicide Death among US Veterans", "abstract": "Importance: Social determinants of health (SDOH) are known to be associated\nwith increased risk of suicidal behaviors, but few studies utilized SDOH from\nunstructured electronic health record (EHR) notes.\n  Objective: To investigate associations between suicide and recent SDOH,\nidentified using structured and unstructured data.\n  Design: Nested case-control study.\n  Setting: EHR data from the US Veterans Health Administration (VHA).\n  Participants: 6,122,785 Veterans who received care in the US VHA between\nOctober 1, 2010, and September 30, 2015.\n  Exposures: Occurrence of SDOH over a maximum span of two years compared with\nno occurrence of SDOH.\n  Main Outcomes and Measures: Cases of suicide deaths were matched with 4\ncontrols on birth year, cohort entry date, sex, and duration of follow-up. We\ndeveloped an NLP system to extract SDOH from unstructured notes. Structured\ndata, NLP on unstructured data, and combining them yielded six, eight and nine\nSDOH respectively. Adjusted odds ratios (aORs) and 95% confidence intervals\n(CIs) were estimated using conditional logistic regression.\n  Results: In our cohort, 8,821 Veterans committed suicide during 23,725,382\nperson-years of follow-up (incidence rate 37.18/100,000 person-years). Our\ncohort was mostly male (92.23%) and white (76.99%). Across the five common SDOH\nas covariates, NLP-extracted SDOH, on average, covered 80.03% of all SDOH\noccurrences. All SDOH, measured by structured data and NLP, were significantly\nassociated with increased risk of suicide. The SDOH with the largest effects\nwas legal problems (aOR=2.66, 95% CI=.46-2.89), followed by violence (aOR=2.12,\n95% CI=1.98-2.27). NLP-extracted and structured SDOH were also associated with\nsuicide.\n  Conclusions and Relevance: NLP-extracted SDOH were always significantly\nassociated with increased risk of suicide among Veterans, suggesting the\npotential of NLP in public health studies.", "published": "2022-12-11 17:15:02", "link": "http://arxiv.org/abs/2212.05546v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MORTY: Structured Summarization for Targeted Information Extraction from\n  Scholarly Articles", "abstract": "Information extraction from scholarly articles is a challenging task due to\nthe sizable document length and implicit information hidden in text, figures,\nand citations. Scholarly information extraction has various applications in\nexploration, archival, and curation services for digital libraries and\nknowledge management systems. We present MORTY, an information extraction\ntechnique that creates structured summaries of text from scholarly articles.\nOur approach condenses the article's full-text to property-value pairs as a\nsegmented text snippet called structured summary. We also present a sizable\nscholarly dataset combining structured summaries retrieved from a scholarly\nknowledge graph and corresponding publicly available scientific articles, which\nwe openly publish as a resource for the research community. Our results show\nthat structured summarization is a suitable approach for targeted information\nextraction that complements other commonly used methods such as question\nanswering and named entity recognition.", "published": "2022-12-11 06:49:29", "link": "http://arxiv.org/abs/2212.05429v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Extending TrOCR for Text Localization-Free OCR of Full-Page Scanned\n  Receipt Images", "abstract": "Digitization of scanned receipts aims to extract text from receipt images and\nsave it into structured documents. This is usually split into two sub-tasks:\ntext localization and optical character recognition (OCR). Most existing OCR\nmodels only focus on the cropped text instance images, which require the\nbounding box information provided by a text region detection model. Introducing\nan additional detector to identify the text instance images in advance adds\ncomplexity, however instance-level OCR models have very low accuracy when\nprocessing the whole image for the document-level OCR, such as receipt images\ncontaining multiple text lines arranged in various layouts. To this end, we\npropose a localization-free document-level OCR model for transcribing all the\ncharacters in a receipt image into an ordered sequence end-to-end.\nSpecifically, we finetune the pretrained instance-level model TrOCR with\nrandomly cropped image chunks, and gradually increase the image chunk size to\ngeneralize the recognition ability from instance images to full-page images. In\nour experiments on the SROIE receipt OCR dataset, the model finetuned with our\nstrategy achieved 64.4 F1-score and a 22.8% character error rate (CER),\nrespectively, which outperforms the baseline results with 48.5 F1-score and\n50.6% CER. The best model, which splits the full image into 15 equally sized\nchunks, gives 87.8 F1-score and 4.98% CER with minimal additional pre or\npost-processing of the output. Moreover, the characters in the generated\ndocument-level sequences are arranged in the reading order, which is practical\nfor real-world applications.", "published": "2022-12-11 15:45:26", "link": "http://arxiv.org/abs/2212.05525v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A Study of Slang Representation Methods", "abstract": "Considering the large amount of content created online by the minute,\nslang-aware automatic tools are critically needed to promote social good, and\nassist policymakers and moderators in restricting the spread of offensive\nlanguage, abuse, and hate speech. Despite the success of large language models\nand the spontaneous emergence of slang dictionaries, it is unclear how far\ntheir combination goes in terms of slang understanding for downstream social\ngood tasks. In this paper, we provide a framework to study different\ncombinations of representation learning models and knowledge resources for a\nvariety of downstream tasks that rely on slang understanding. Our experiments\nshow the superiority of models that have been pre-trained on social media data,\nwhile the impact of dictionaries is positive only for static word embeddings.\nOur error analysis identifies core challenges for slang representation\nlearning, including out-of-vocabulary words, polysemy, variance, and annotation\ndisagreements, which can be traced to characteristics of slang as a quickly\nevolving and highly subjective language.", "published": "2022-12-11 21:56:44", "link": "http://arxiv.org/abs/2212.05613v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "End-to-End Speech Translation of Arabic to English Broadcast News", "abstract": "Speech translation (ST) is the task of directly translating acoustic speech\nsignals in a source language into text in a foreign language. ST task has been\naddressed, for a long time, using a pipeline approach with two modules : first\nan Automatic Speech Recognition (ASR) in the source language followed by a\ntext-to-text Machine translation (MT). In the past few years, we have seen a\nparadigm shift towards the end-to-end approaches using sequence-to-sequence\ndeep neural network models. This paper presents our efforts towards the\ndevelopment of the first Broadcast News end-to-end Arabic to English speech\ntranslation system. Starting from independent ASR and MT LDC releases, we were\nable to identify about 92 hours of Arabic audio recordings for which the manual\ntranscription was also translated into English at the segment level. These data\nwas used to train and compare pipeline and end-to-end speech translation\nsystems under multiple scenarios including transfer learning and data\naugmentation techniques.", "published": "2022-12-11 11:35:46", "link": "http://arxiv.org/abs/2212.05479v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multimodal and Explainable Internet Meme Classification", "abstract": "In the current context where online platforms have been effectively\nweaponized in a variety of geo-political events and social issues, Internet\nmemes make fair content moderation at scale even more difficult. Existing work\non meme classification and tracking has focused on black-box methods that do\nnot explicitly consider the semantics of the memes or the context of their\ncreation. In this paper, we pursue a modular and explainable architecture for\nInternet meme understanding. We design and implement multimodal classification\nmethods that perform example- and prototype-based reasoning over training\ncases, while leveraging both textual and visual SOTA models to represent the\nindividual cases. We study the relevance of our modular and explainable models\nin detecting harmful memes on two existing tasks: Hate Speech Detection and\nMisogyny Classification. We compare the performance between example- and\nprototype-based methods, and between text, vision, and multimodal models,\nacross different categories of harmfulness (e.g., stereotype and\nobjectification). We devise a user-friendly interface that facilitates the\ncomparative analysis of examples retrieved by all of our models for any given\nmeme, informing the community about the strengths and limitations of these\nexplainable methods.", "published": "2022-12-11 21:52:21", "link": "http://arxiv.org/abs/2212.05612v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "MAViC: Multimodal Active Learning for Video Captioning", "abstract": "A large number of annotated video-caption pairs are required for training\nvideo captioning models, resulting in high annotation costs. Active learning\ncan be instrumental in reducing these annotation requirements. However, active\nlearning for video captioning is challenging because multiple semantically\nsimilar captions are valid for a video, resulting in high entropy outputs even\nfor less-informative samples. Moreover, video captioning algorithms are\nmultimodal in nature with a visual encoder and language decoder. Further, the\nsequential and combinatorial nature of the output makes the problem even more\nchallenging. In this paper, we introduce MAViC which leverages our proposed\nMultimodal Semantics Aware Sequential Entropy (M-SASE) based acquisition\nfunction to address the challenges of active learning approaches for video\ncaptioning. Our approach integrates semantic similarity and uncertainty of both\nvisual and language dimensions in the acquisition function. Our detailed\nexperiments empirically demonstrate the efficacy of M-SASE for active learning\nfor video captioning and improve on the baselines by a large margin.", "published": "2022-12-11 18:51:57", "link": "http://arxiv.org/abs/2212.11109v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Religion and Spirituality on Social Media in the Aftermath of the Global\n  Pandemic", "abstract": "During the COVID-19 pandemic, the Church closed its physical doors for the\nfirst time in about 800 years, which is, arguably, a cataclysmic event. Other\nreligions have found themselves in a similar situation, and they were\npractically forced to move online, which is an unprecedented occasion. In this\npaper, we analyse this sudden change in religious activities twofold: we create\nand deliver a questionnaire, as well as analyse Twitter data, to understand\npeople's perceptions and activities related to religious activities online.\nImportantly, we also analyse the temporal variations in this process by\nanalysing a period of 3 months: July-September 2020. Additionally to the\nseparate analysis of the two data sources, we also discuss the implications\nfrom triangulating the results.", "published": "2022-12-11 18:41:02", "link": "http://arxiv.org/abs/2212.11121v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.CY"}
{"title": "MnTTS2: An Open-Source Multi-Speaker Mongolian Text-to-Speech Synthesis\n  Dataset", "abstract": "Text-to-Speech (TTS) synthesis for low-resource languages is an attractive\nresearch issue in academia and industry nowadays. Mongolian is the official\nlanguage of the Inner Mongolia Autonomous Region and a representative\nlow-resource language spoken by over 10 million people worldwide. However,\nthere is a relative lack of open-source datasets for Mongolian TTS. Therefore,\nwe make public an open-source multi-speaker Mongolian TTS dataset, named\nMnTTS2, for the benefit of related researchers. In this work, we prepare the\ntranscription from various topics and invite three professional Mongolian\nannouncers to form a three-speaker TTS dataset, in which each announcer records\n10 hours of speeches in Mongolian, resulting 30 hours in total. Furthermore, we\nbuild the baseline system based on the state-of-the-art FastSpeech2 model and\nHiFi-GAN vocoder. The experimental results suggest that the constructed MnTTS2\ndataset is sufficient to build robust multi-speaker TTS models for real-world\napplications. The MnTTS2 dataset, training recipe, and pretrained models are\nreleased at: \\url{https://github.com/ssmlkl/MnTTS2}", "published": "2022-12-11 14:55:02", "link": "http://arxiv.org/abs/2301.00657v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Can REF output quality scores be assigned by AI? Experimental evidence", "abstract": "This document describes strategies for using Artificial Intelligence (AI) to\npredict some journal article scores in future research assessment exercises.\nFive strategies have been assessed.", "published": "2022-12-11 18:32:00", "link": "http://arxiv.org/abs/2212.08041v1", "categories": ["cs.CY", "cs.CL", "cs.GL", "cs.IR", "cs.LG"], "primary_category": "cs.CY"}
{"title": "BASPRO: a balanced script producer for speech corpus collection based on\n  the genetic algorithm", "abstract": "The performance of speech-processing models is heavily influenced by the\nspeech corpus that is used for training and evaluation. In this study, we\npropose BAlanced Script PROducer (BASPRO) system, which can automatically\nconstruct a phonetically balanced and rich set of Chinese sentences for\ncollecting Mandarin Chinese speech data. First, we used pretrained natural\nlanguage processing systems to extract ten-character candidate sentences from a\nlarge corpus of Chinese news texts. Then, we applied a genetic algorithm-based\nmethod to select 20 phonetically balanced sentence sets, each containing 20\nsentences, from the candidate sentences. Using BASPRO, we obtained a recording\nscript called TMNews, which contains 400 ten-character sentences. TMNews covers\n84% of the syllables used in the real world. Moreover, the syllable\ndistribution has 0.96 cosine similarity to the real-world syllable\ndistribution. We converted the script into a speech corpus using two\ntext-to-speech systems. Using the designed speech corpus, we tested the\nperformances of speech enhancement (SE) and automatic speech recognition (ASR),\nwhich are one of the most important regression- and classification-based speech\nprocessing tasks, respectively. The experimental results show that the SE and\nASR models trained on the designed speech corpus outperform their counterparts\ntrained on a randomly composed speech corpus.", "published": "2022-12-11 02:05:30", "link": "http://arxiv.org/abs/2301.04120v1", "categories": ["cs.NE", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.NE"}
