{"title": "MetaPAD: Meta Pattern Discovery from Massive Text Corpora", "abstract": "Mining textual patterns in news, tweets, papers, and many other kinds of text\ncorpora has been an active theme in text mining and NLP research. Previous\nstudies adopt a dependency parsing-based pattern discovery approach. However,\nthe parsing results lose rich context around entities in the patterns, and the\nprocess is costly for a corpus of large scale. In this study, we propose a\nnovel typed textual pattern structure, called meta pattern, which is extended\nto a frequent, informative, and precise subsequence pattern in certain context.\nWe propose an efficient framework, called MetaPAD, which discovers meta\npatterns from massive corpora with three techniques: (1) it develops a\ncontext-aware segmentation method to carefully determine the boundaries of\npatterns with a learnt pattern quality assessment function, which avoids costly\ndependency parsing and generates high-quality patterns; (2) it identifies and\ngroups synonymous meta patterns from multiple facets---their types, contexts,\nand extractions; and (3) it examines type distributions of entities in the\ninstances extracted by each group of patterns, and looks for appropriate type\nlevels to make discovered patterns precise. Experiments demonstrate that our\nproposed framework discovers high-quality typed textual patterns efficiently\nfrom different genres of massive corpora and facilitates information\nextraction.", "published": "2017-03-13 01:06:19", "link": "http://arxiv.org/abs/1703.04213v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Story Cloze Ending Selection Baselines and Data Examination", "abstract": "This paper describes two supervised baseline systems for the Story Cloze Test\nShared Task (Mostafazadeh et al., 2016a). We first build a classifier using\nfeatures based on word embeddings and semantic similarity computation. We\nfurther implement a neural LSTM system with different encoding strategies that\ntry to model the relation between the story and the provided endings. Our\nexperiments show that a model using representation features based on average\nword embedding vectors over the given story words and the candidate ending\nsentences words, joint with similarity features between the story and candidate\nending representations performed better than the neural models. Our best model\nachieves an accuracy of 72.42, ranking 3rd in the official evaluation.", "published": "2017-03-13 11:03:40", "link": "http://arxiv.org/abs/1703.04330v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Nematus: a Toolkit for Neural Machine Translation", "abstract": "We present Nematus, a toolkit for Neural Machine Translation. The toolkit\nprioritizes high translation accuracy, usability, and extensibility. Nematus\nhas been used to build top-performing submissions to shared translation tasks\nat WMT and IWSLT, and has been used to train systems for production\nenvironments.", "published": "2017-03-13 12:28:03", "link": "http://arxiv.org/abs/1703.04357v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DRAGNN: A Transition-based Framework for Dynamically Connected Neural\n  Networks", "abstract": "In this work, we present a compact, modular framework for constructing novel\nrecurrent neural architectures. Our basic module is a new generic unit, the\nTransition Based Recurrent Unit (TBRU). In addition to hidden layer\nactivations, TBRUs have discrete state dynamics that allow network connections\nto be built dynamically as a function of intermediate activations. By\nconnecting multiple TBRUs, we can extend and combine commonly used\narchitectures such as sequence-to-sequence, attention mechanisms, and\nre-cursive tree-structured models. A TBRU can also serve as both an encoder for\ndownstream tasks and as a decoder for its own task simultaneously, resulting in\nmore accurate multi-task learning. We call our approach Dynamic Recurrent\nAcyclic Graphical Neural Networks, or DRAGNN. We show that DRAGNN is\nsignificantly more accurate and efficient than seq2seq with attention for\nsyntactic dependency parsing and yields more accurate multi-task learning for\nextractive summarization tasks.", "published": "2017-03-13 16:36:38", "link": "http://arxiv.org/abs/1703.04474v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Geometrical morphology", "abstract": "We explore inflectional morphology as an example of the relationship of the\ndiscrete and the continuous in linguistics. The grammar requests a form of a\nlexeme by specifying a set of feature values, which corresponds to a corner M\nof a hypercube in feature value space. The morphology responds to that request\nby providing a morpheme, or a set of morphemes, whose vector sum is\ngeometrically closest to the corner M. In short, the chosen morpheme $\\mu$ is\nthe morpheme (or set of morphemes) that maximizes the inner product of $\\mu$\nand M.", "published": "2017-03-13 16:50:36", "link": "http://arxiv.org/abs/1703.04481v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DeepFM: A Factorization-Machine based Neural Network for CTR Prediction", "abstract": "Learning sophisticated feature interactions behind user behaviors is critical\nin maximizing CTR for recommender systems. Despite great progress, existing\nmethods seem to have a strong bias towards low- or high-order interactions, or\nrequire expertise feature engineering. In this paper, we show that it is\npossible to derive an end-to-end learning model that emphasizes both low- and\nhigh-order feature interactions. The proposed model, DeepFM, combines the power\nof factorization machines for recommendation and deep learning for feature\nlearning in a new neural network architecture. Compared to the latest Wide \\&\nDeep model from Google, DeepFM has a shared input to its \"wide\" and \"deep\"\nparts, with no need of feature engineering besides raw features. Comprehensive\nexperiments are conducted to demonstrate the effectiveness and efficiency of\nDeepFM over the existing models for CTR prediction, on both benchmark data and\ncommercial data.", "published": "2017-03-13 04:55:19", "link": "http://arxiv.org/abs/1703.04247v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Visual Representation of Wittgenstein's Tractatus Logico-Philosophicus", "abstract": "In this paper we present a data visualization method together with its\npotential usefulness in digital humanities and philosophy of language. We\ncompile a multilingual parallel corpus from different versions of\nWittgenstein's Tractatus Logico-Philosophicus, including the original in German\nand translations into English, Spanish, French, and Russian. Using this corpus,\nwe compute a similarity measure between propositions and render a visual\nnetwork of relations for different languages.", "published": "2017-03-13 11:19:56", "link": "http://arxiv.org/abs/1703.04336v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "El Lenguaje Natural como Lenguaje Formal", "abstract": "Formal languages theory is useful for the study of natural language. In\nparticular, it is of interest to study the adequacy of the grammatical\nformalisms to express syntactic phenomena present in natural language. First,\nit helps to draw hypothesis about the nature and complexity of the\nspeaker-hearer linguistic competence, a fundamental question in linguistics and\nother cognitive sciences. Moreover, from an engineering point of view, it\nallows the knowledge of practical limitations of applications based on those\nformalisms. In this article I introduce the adequacy problem of grammatical\nformalisms for natural language, also introducing some formal language theory\nconcepts required for this discussion. Then, I review the formalisms that have\nbeen proposed in history, and the arguments that have been given to support or\nreject their adequacy.\n  -----\n  La teor\\'ia de lenguajes formales es \\'util para el estudio de los lenguajes\nnaturales. En particular, resulta de inter\\'es estudiar la adecuaci\\'on de los\nformalismos gramaticales para expresar los fen\\'omenos sint\\'acticos presentes\nen el lenguaje natural. Primero, ayuda a trazar hip\\'otesis acerca de la\nnaturaleza y complejidad de las competencias ling\\\"u\\'isticas de los\nhablantes-oyentes del lenguaje, un interrogante fundamental de la\nling\\\"u\\'istica y otras ciencias cognitivas. Adem\\'as, desde el punto de vista\nde la ingenier\\'ia, permite conocer limitaciones pr\\'acticas de las\naplicaciones basadas en dichos formalismos. En este art\\'iculo hago una\nintroducci\\'on al problema de la adecuaci\\'on de los formalismos gramaticales\npara el lenguaje natural, introduciendo tambi\\'en algunos conceptos de la\nteor\\'ia de lenguajes formales necesarios para esta discusi\\'on. Luego, hago un\nrepaso de los formalismos que han sido propuestos a lo largo de la historia, y\nde los argumentos que se han dado para sostener o refutar su adecuaci\\'on.", "published": "2017-03-13 14:34:23", "link": "http://arxiv.org/abs/1703.04417v1", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
{"title": "Reinforcement Learning for Transition-Based Mention Detection", "abstract": "This paper describes an application of reinforcement learning to the mention\ndetection task. We define a novel action-based formulation for the mention\ndetection task, in which a model can flexibly revise past labeling decisions by\ngrouping together tokens and assigning partial mention labels. We devise a\nmethod to create mention-level episodes and we train a model by rewarding\ncorrectly labeled complete mentions, irrespective of the inner structure\ncreated. The model yields results which are on par with a competitive\nsupervised counterpart while being more flexible in terms of achieving targeted\nbehavior through reward modeling and generating internal mention structure,\nespecially on longer mentions.", "published": "2017-03-13 17:13:51", "link": "http://arxiv.org/abs/1703.04489v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "High-Throughput and Language-Agnostic Entity Disambiguation and Linking\n  on User Generated Data", "abstract": "The Entity Disambiguation and Linking (EDL) task matches entity mentions in\ntext to a unique Knowledge Base (KB) identifier such as a Wikipedia or Freebase\nid. It plays a critical role in the construction of a high quality information\nnetwork, and can be further leveraged for a variety of information retrieval\nand NLP tasks such as text categorization and document tagging. EDL is a\ncomplex and challenging problem due to ambiguity of the mentions and real world\ntext being multi-lingual. Moreover, EDL systems need to have high throughput\nand should be lightweight in order to scale to large datasets and run on\noff-the-shelf machines. More importantly, these systems need to be able to\nextract and disambiguate dense annotations from the data in order to enable an\nInformation Retrieval or Extraction task running on the data to be more\nefficient and accurate. In order to address all these challenges, we present\nthe Lithium EDL system and algorithm - a high-throughput, lightweight,\nlanguage-agnostic EDL system that extracts and correctly disambiguates 75% more\nentities than state-of-the-art EDL systems and is significantly faster than\nthem.", "published": "2017-03-13 17:34:18", "link": "http://arxiv.org/abs/1703.04498v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
