{"title": "Incremental Comprehension of Garden-Path Sentences by Large Language\n  Models: Semantic Interpretation, Syntactic Re-Analysis, and Attention", "abstract": "When reading temporarily ambiguous garden-path sentences, misinterpretations\nsometimes linger past the point of disambiguation. This phenomenon has\ntraditionally been studied in psycholinguistic experiments using online\nmeasures such as reading times and offline measures such as comprehension\nquestions. Here, we investigate the processing of garden-path sentences and the\nfate of lingering misinterpretations using four large language models (LLMs):\nGPT-2, LLaMA-2, Flan-T5, and RoBERTa. The overall goal is to evaluate whether\nhumans and LLMs are aligned in their processing of garden-path sentences and in\nthe lingering misinterpretations past the point of disambiguation, especially\nwhen extra-syntactic information (e.g., a comma delimiting a clause boundary)\nis present to guide processing. We address this goal using 24 garden-path\nsentences that have optional transitive and reflexive verbs leading to\ntemporary ambiguities. For each sentence, there are a pair of comprehension\nquestions corresponding to the misinterpretation and the correct\ninterpretation. In three experiments, we (1) measure the dynamic semantic\ninterpretations of LLMs using the question-answering task; (2) track whether\nthese models shift their implicit parse tree at the point of disambiguation (or\nby the end of the sentence); and (3) visualize the model components that attend\nto disambiguating information when processing the question probes. These\nexperiments show promising alignment between humans and LLMs in the processing\nof garden-path sentences, especially when extra-syntactic information is\navailable to guide processing.", "published": "2024-05-25 03:36:13", "link": "http://arxiv.org/abs/2405.16042v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Keypoint-based Progressive Chain-of-Thought Distillation for LLMs", "abstract": "Chain-of-thought distillation is a powerful technique for transferring\nreasoning abilities from large language models (LLMs) to smaller student\nmodels. Previous methods typically require the student to mimic the\nstep-by-step rationale produced by LLMs, often facing the following challenges:\n(i) Tokens within a rationale vary in significance, and treating them equally\nmay fail to accurately mimic keypoint tokens, leading to reasoning errors. (ii)\nThey usually distill knowledge by consistently predicting all the steps in a\nrationale, which falls short in distinguishing the learning order of step\ngeneration. This diverges from the human cognitive progression of starting with\neasy tasks and advancing to harder ones, resulting in sub-optimal outcomes. To\nthis end, we propose a unified framework, called KPOD, to address these issues.\nSpecifically, we propose a token weighting module utilizing mask learning to\nencourage accurate mimicry of keypoint tokens by the student during\ndistillation. Besides, we develop an in-rationale progressive distillation\nstrategy, starting with training the student to generate the final reasoning\nsteps and gradually extending to cover the entire rationale. To accomplish\nthis, a weighted token generation loss is proposed to assess step reasoning\ndifficulty, and a value function is devised to schedule the progressive\ndistillation by considering both step difficulty and question diversity.\nExtensive experiments on four reasoning benchmarks illustrate our KPOD\noutperforms previous methods by a large margin.", "published": "2024-05-25 05:27:38", "link": "http://arxiv.org/abs/2405.16064v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "iREL at SemEval-2024 Task 9: Improving Conventional Prompting Methods\n  for Brain Teasers", "abstract": "This paper describes our approach for SemEval-2024 Task 9: BRAINTEASER: A\nNovel Task Defying Common Sense. The BRAINTEASER task comprises multiple-choice\nQuestion Answering designed to evaluate the models' lateral thinking\ncapabilities. It consists of Sentence Puzzle and Word Puzzle subtasks that\nrequire models to defy default common-sense associations and exhibit\nunconventional thinking. We propose a unique strategy to improve the\nperformance of pre-trained language models, notably the Gemini 1.0 Pro Model,\nin both subtasks. We employ static and dynamic few-shot prompting techniques\nand introduce a model-generated reasoning strategy that utilizes the LLM's\nreasoning capabilities to improve performance. Our approach demonstrated\nsignificant improvements, showing that it performed better than the baseline\nmodels by a considerable margin but fell short of performing as well as the\nhuman annotators, thus highlighting the efficacy of the proposed strategies.", "published": "2024-05-25 08:50:51", "link": "http://arxiv.org/abs/2405.16129v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "5W1H Extraction With Large Language Models", "abstract": "The extraction of essential news elements through the 5W1H framework\n(\\textit{What}, \\textit{When}, \\textit{Where}, \\textit{Why}, \\textit{Who}, and\n\\textit{How}) is critical for event extraction and text summarization. The\nadvent of Large language models (LLMs) such as ChatGPT presents an opportunity\nto address language-related tasks through simple prompts without fine-tuning\nmodels with much time. While ChatGPT has encountered challenges in processing\nlonger news texts and analyzing specific attributes in context, especially\nanswering questions about \\textit{What}, \\textit{Why}, and \\textit{How}. The\neffectiveness of extraction tasks is notably dependent on high-quality\nhuman-annotated datasets. However, the absence of such datasets for the 5W1H\nextraction increases the difficulty of fine-tuning strategies based on\nopen-source LLMs. To address these limitations, first, we annotate a\nhigh-quality 5W1H dataset based on four typical news corpora\n(\\textit{CNN/DailyMail}, \\textit{XSum}, \\textit{NYT}, \\textit{RA-MDS}); second,\nwe design several strategies from zero-shot/few-shot prompting to efficient\nfine-tuning to conduct 5W1H aspects extraction from the original news\ndocuments. The experimental results demonstrate that the performance of the\nfine-tuned models on our labelled dataset is superior to the performance of\nChatGPT. Furthermore, we also explore the domain adaptation capability by\ntesting the source-domain (e.g. NYT) models on the target domain corpus (e.g.\nCNN/DailyMail) for the task of 5W1H extraction.", "published": "2024-05-25 09:42:58", "link": "http://arxiv.org/abs/2405.16150v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Multi-lingual Alignment Through Soft Contrastive Learning", "abstract": "Making decent multi-lingual sentence representations is critical to achieve\nhigh performances in cross-lingual downstream tasks. In this work, we propose a\nnovel method to align multi-lingual embeddings based on the similarity of\nsentences measured by a pre-trained mono-lingual embedding model. Given\ntranslation sentence pairs, we train a multi-lingual model in a way that the\nsimilarity between cross-lingual embeddings follows the similarity of sentences\nmeasured at the mono-lingual teacher model. Our method can be considered as\ncontrastive learning with soft labels defined as the similarity between\nsentences. Our experimental results on five languages show that our contrastive\nloss with soft labels far outperforms conventional contrastive loss with hard\nlabels in various benchmarks for bitext mining tasks and STS tasks. In\naddition, our method outperforms existing multi-lingual embeddings including\nLaBSE, for Tatoeba dataset. The code is available at\nhttps://github.com/YAI12xLinq-B/IMASCL", "published": "2024-05-25 09:46:07", "link": "http://arxiv.org/abs/2405.16155v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Accelerating Inference of Retrieval-Augmented Generation via Sparse\n  Context Selection", "abstract": "Large language models (LLMs) augmented with retrieval exhibit robust\nperformance and extensive versatility by incorporating external contexts.\nHowever, the input length grows linearly in the number of retrieved documents,\ncausing a dramatic increase in latency. In this paper, we propose a novel\nparadigm named Sparse RAG, which seeks to cut computation costs through\nsparsity. Specifically, Sparse RAG encodes retrieved documents in parallel,\nwhich eliminates latency introduced by long-range attention of retrieved\ndocuments. Then, LLMs selectively decode the output by only attending to highly\nrelevant caches auto-regressively, which are chosen via prompting LLMs with\nspecial control tokens. It is notable that Sparse RAG combines the assessment\nof each individual document and the generation of the response into a single\nprocess. The designed sparse mechanism in a RAG system can facilitate the\nreduction of the number of documents loaded during decoding for accelerating\nthe inference of the RAG system. Additionally, filtering out undesirable\ncontexts enhances the model's focus on relevant context, inherently improving\nits generation quality. Evaluation results of two datasets show that Sparse RAG\ncan strike an optimal balance between generation quality and computational\nefficiency, demonstrating its generalizability across both short- and long-form\ngeneration tasks.", "published": "2024-05-25 11:10:04", "link": "http://arxiv.org/abs/2405.16178v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConStat: Performance-Based Contamination Detection in Large Language\n  Models", "abstract": "Public benchmarks play an essential role in the evaluation of large language\nmodels. However, data contamination can lead to inflated performance, rendering\nthem unreliable for model comparison. It is therefore crucial to detect\ncontamination and estimate its impact on measured performance. Unfortunately,\nexisting detection methods can be easily evaded and fail to quantify\ncontamination. To overcome these limitations, we propose a novel definition of\ncontamination as artificially inflated and non-generalizing benchmark\nperformance instead of the inclusion of benchmark samples in the training data.\nThis perspective enables us to detect any model with inflated performance,\ni.e., performance that does not generalize to rephrased samples, synthetic\nsamples from the same distribution, or different benchmarks for the same task.\nBased on this insight, we develop ConStat, a statistical method that reliably\ndetects and quantifies contamination by comparing performance between a primary\nand reference benchmark relative to a set of reference models. We demonstrate\nthe effectiveness of ConStat in an extensive evaluation of diverse model\narchitectures, benchmarks, and contamination scenarios and find high levels of\ncontamination in multiple popular models including Mistral, Llama, Yi, and the\ntop-3 Open LLM Leaderboard models.", "published": "2024-05-25 15:36:37", "link": "http://arxiv.org/abs/2405.16281v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InstructPatentGPT: Training patent language models to follow\n  instructions with human feedback", "abstract": "In this research, patent prosecution is conceptualized as a system of\nreinforcement learning from human feedback. The objective of the system is to\nincrease the likelihood for a language model to generate patent claims that\nhave a higher chance of being granted. To showcase the controllability of the\nlanguage model, the system learns from granted patents and pre-grant\napplications with different rewards. The status of \"granted\" and \"pre-grant\"\nare perceived as labeled human feedback implicitly. In addition, specific to\npatent drafting, the experiments in this research demonstrate the model's\ncapability to learn from adjusting claim length and inclusion of limiting terms\nfor narrowing claim scope. As proof of concept, the experiments focus on claim\nones only and the training data originates from a patent dataset tailored\nspecifically for artificial intelligence. Although the available human feedback\nin patent prosecution are limited and the quality of generated patent text\nrequires improvement, the experiments following the 3-stage reinforcement\nlearning from human feedback have demonstrated that generative language models\nare capable of reflecting the human feedback or intent in patent prosecution.\nTo enhance the usability of language models, the implementation in this\nresearch utilizes modern techniques that enable execution on a single\nconsumer-grade GPU. The demonstrated proof of concept, which reduces hardware\nrequirements, will prove valuable in the future as more human feedback in\npatent prosecution become available for broader use, either within patent\noffices or in the public domain.", "published": "2024-05-25 11:48:50", "link": "http://arxiv.org/abs/2406.16897v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large\n  Language Models", "abstract": "Large Language Models (LLMs) have become pivotal in advancing the field of\nartificial intelligence, yet their immense sizes pose significant challenges\nfor both fine-tuning and deployment. Current post-training pruning methods,\nwhile reducing the sizes of LLMs, often fail to maintain their original\nperformance. To address these challenges, this paper introduces SPP, a\nSparsity-Preserved Parameter-efficient fine-tuning method. Different from\nexisting post-training pruning approaches that struggle with performance\nretention, SPP proposes to employ lightweight learnable column and row matrices\nto optimize sparse LLM weights, keeping the structure and sparsity of pruned\npre-trained models intact. By element-wise multiplication and residual\naddition, SPP ensures the consistency of model sparsity pattern and ratio\nduring both training and weight-merging processes. We demonstrate the\neffectiveness of SPP by applying it to the LLaMA and LLaMA-2 model families\nwith recent post-training pruning methods. Our results show that SPP\nsignificantly enhances the performance of models with different sparsity\npatterns (i.e. unstructured and N:M sparsity), especially for those with high\nsparsity ratios (e.g. 75%), making it a promising solution for the efficient\nfine-tuning of sparse LLMs. Code will be made available at\nhttps://github.com/Lucky-Lance/SPP.", "published": "2024-05-25 04:55:27", "link": "http://arxiv.org/abs/2405.16057v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Completeness-Oriented Tool Retrieval for Large Language Models", "abstract": "Recently, integrating external tools with Large Language Models (LLMs) has\ngained significant attention as an effective strategy to mitigate the\nlimitations inherent in their pre-training data. However, real-world systems\noften incorporate a wide array of tools, making it impractical to input all\ntools into LLMs due to length limitations and latency constraints. Therefore,\nto fully exploit the potential of tool-augmented LLMs, it is crucial to develop\nan effective tool retrieval system. Existing tool retrieval methods primarily\nfocus on semantic matching between user queries and tool descriptions,\nfrequently leading to the retrieval of redundant, similar tools. Consequently,\nthese methods fail to provide a complete set of diverse tools necessary for\naddressing the multifaceted problems encountered by LLMs. In this paper, we\npropose a novel modelagnostic COllaborative Learning-based Tool Retrieval\napproach, COLT, which captures not only the semantic similarities between user\nqueries and tool descriptions but also takes into account the collaborative\ninformation of tools. Specifically, we first fine-tune the PLM-based retrieval\nmodels to capture the semantic relationships between queries and tools in the\nsemantic learning stage. Subsequently, we construct three bipartite graphs\namong queries, scenes, and tools and introduce a dual-view graph collaborative\nlearning framework to capture the intricate collaborative relationships among\ntools during the collaborative learning stage. Extensive experiments on both\nthe open benchmark and the newly introduced ToolLens dataset show that COLT\nachieves superior performance. Notably, the performance of BERT-mini (11M) with\nour proposed model framework outperforms BERT-large (340M), which has 30 times\nmore parameters. Furthermore, we will release ToolLens publicly to facilitate\nfuture research on tool retrieval.", "published": "2024-05-25 06:41:23", "link": "http://arxiv.org/abs/2405.16089v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "SNOBERT: A Benchmark for clinical notes entity linking in the SNOMED CT\n  clinical terminology", "abstract": "The extraction and analysis of insights from medical data, primarily stored\nin free-text formats by healthcare workers, presents significant challenges due\nto its unstructured nature. Medical coding, a crucial process in healthcare,\nremains minimally automated due to the complexity of medical ontologies and\nrestricted access to medical texts for training Natural Language Processing\nmodels. In this paper, we proposed a method, \"SNOBERT,\" of linking text spans\nin clinical notes to specific concepts in the SNOMED CT using BERT-based\nmodels. The method consists of two stages: candidate selection and candidate\nmatching. The models were trained on one of the largest publicly available\ndataset of labeled clinical notes. SNOBERT outperforms other classical methods\nbased on deep learning, as confirmed by the results of a challenge in which it\nwas applied.", "published": "2024-05-25 08:00:44", "link": "http://arxiv.org/abs/2405.16115v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How Well Do Deep Learning Models Capture Human Concepts? The Case of the\n  Typicality Effect", "abstract": "How well do representations learned by ML models align with those of humans?\nHere, we consider concept representations learned by deep learning models and\nevaluate whether they show a fundamental behavioral signature of human\nconcepts, the typicality effect. This is the finding that people judge some\ninstances (e.g., robin) of a category (e.g., Bird) to be more typical than\nothers (e.g., penguin). Recent research looking for human-like typicality\neffects in language and vision models has focused on models of a single\nmodality, tested only a small number of concepts, and found only modest\ncorrelations with human typicality ratings. The current study expands this\nbehavioral evaluation of models by considering a broader range of language (N =\n8) and vision (N = 10) model architectures. It also evaluates whether the\ncombined typicality predictions of vision + language model pairs, as well as a\nmultimodal CLIP-based model, are better aligned with human typicality judgments\nthan those of models of either modality alone. Finally, it evaluates the models\nacross a broader range of concepts (N = 27) than prior studies. There were\nthree important findings. First, language models better align with human\ntypicality judgments than vision models. Second, combined language and vision\nmodels (e.g., AlexNet + MiniLM) better predict the human typicality data than\nthe best-performing language model (i.e., MiniLM) or vision model (i.e.,\nViT-Huge) alone. Third, multimodal models (i.e., CLIP ViT) show promise for\nexplaining human typicality judgments. These results advance the\nstate-of-the-art in aligning the conceptual representations of ML models and\nhumans. A methodological contribution is the creation of a new image set for\ntesting the conceptual alignment of vision models.", "published": "2024-05-25 08:38:30", "link": "http://arxiv.org/abs/2405.16128v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "GeneAgent: Self-verification Language Agent for Gene Set Knowledge\n  Discovery using Domain Databases", "abstract": "Gene set knowledge discovery is essential for advancing human functional\ngenomics. Recent studies have shown promising performance by harnessing the\npower of Large Language Models (LLMs) on this task. Nonetheless, their results\nare subject to several limitations common in LLMs such as hallucinations. In\nresponse, we present GeneAgent, a first-of-its-kind language agent featuring\nself-verification capability. It autonomously interacts with various biological\ndatabases and leverages relevant domain knowledge to improve accuracy and\nreduce hallucination occurrences. Benchmarking on 1,106 gene sets from\ndifferent sources, GeneAgent consistently outperforms standard GPT-4 by a\nsignificant margin. Moreover, a detailed manual review confirms the\neffectiveness of the self-verification module in minimizing hallucinations and\ngenerating more reliable analytical narratives. To demonstrate its practical\nutility, we apply GeneAgent to seven novel gene sets derived from mouse B2905\nmelanoma cell lines, with expert evaluations showing that GeneAgent offers\nnovel insights into gene functions and subsequently expedites knowledge\ndiscovery.", "published": "2024-05-25 12:35:15", "link": "http://arxiv.org/abs/2405.16205v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "No Two Devils Alike: Unveiling Distinct Mechanisms of Fine-tuning\n  Attacks", "abstract": "The existing safety alignment of Large Language Models (LLMs) is found\nfragile and could be easily attacked through different strategies, such as\nthrough fine-tuning on a few harmful examples or manipulating the prefix of the\ngeneration results. However, the attack mechanisms of these strategies are\nstill underexplored. In this paper, we ask the following question:\n\\textit{while these approaches can all significantly compromise safety, do\ntheir attack mechanisms exhibit strong similarities?} To answer this question,\nwe break down the safeguarding process of an LLM when encountered with harmful\ninstructions into three stages: (1) recognizing harmful instructions, (2)\ngenerating an initial refusing tone, and (3) completing the refusal response.\nAccordingly, we investigate whether and how different attack strategies could\ninfluence each stage of this safeguarding process. We utilize techniques such\nas logit lens and activation patching to identify model components that drive\nspecific behavior, and we apply cross-model probing to examine representation\nshifts after an attack. In particular, we analyze the two most representative\ntypes of attack approaches: Explicit Harmful Attack (EHA) and Identity-Shifting\nAttack (ISA). Surprisingly, we find that their attack mechanisms diverge\ndramatically. Unlike ISA, EHA tends to aggressively target the harmful\nrecognition stage. While both EHA and ISA disrupt the latter two stages, the\nextent and mechanisms of their attacks differ significantly. Our findings\nunderscore the importance of understanding LLMs' internal safeguarding process\nand suggest that diverse defense mechanisms are required to effectively cope\nwith various types of attacks.", "published": "2024-05-25 13:38:40", "link": "http://arxiv.org/abs/2405.16229v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "AutoManual: Constructing Instruction Manuals by LLM Agents via\n  Interactive Environmental Learning", "abstract": "Large Language Models (LLM) based agents have shown promise in autonomously\ncompleting tasks across various domains, e.g., robotics, games, and web\nnavigation. However, these agents typically require elaborate design and expert\nprompts to solve tasks in specific domains, which limits their adaptability. We\nintroduce AutoManual, a framework enabling LLM agents to autonomously build\ntheir understanding through interaction and adapt to new environments.\nAutoManual categorizes environmental knowledge into diverse rules and optimizes\nthem in an online fashion by two agents: 1) The Planner codes actionable plans\nbased on current rules for interacting with the environment. 2) The Builder\nupdates the rules through a well-structured rule system that facilitates online\nrule management and essential detail retention. To mitigate hallucinations in\nmanaging rules, we introduce a *case-conditioned prompting* strategy for the\nBuilder. Finally, the Formulator agent compiles these rules into a\ncomprehensive manual. The self-generated manual can not only improve the\nadaptability but also guide the planning of smaller LLMs while being\nhuman-readable. Given only one simple demonstration, AutoManual significantly\nimproves task success rates, achieving 97.4\\% with GPT-4-turbo and 86.2\\% with\nGPT-3.5-turbo on ALFWorld benchmark tasks. The code is available at\nhttps://github.com/minghchen/automanual.", "published": "2024-05-25 14:11:44", "link": "http://arxiv.org/abs/2405.16247v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Generating clickbait spoilers with an ensemble of large language models", "abstract": "Clickbait posts are a widespread problem in the webspace. The generation of\nspoilers, i.e. short texts that neutralize clickbait by providing information\nthat satisfies the curiosity induced by it, is one of the proposed solutions to\nthe problem. Current state-of-the-art methods are based on passage retrieval or\nquestion answering approaches and are limited to generating spoilers only in\nthe form of a phrase or a passage. In this work, we propose an ensemble of\nfine-tuned large language models for clickbait spoiler generation. Our approach\nis not limited to phrase or passage spoilers, but is also able to generate\nmultipart spoilers that refer to several non-consecutive parts of text.\nExperimental evaluation demonstrates that the proposed ensemble model\noutperforms the baselines in terms of BLEU, METEOR and BERTScore metrics.", "published": "2024-05-25 15:49:08", "link": "http://arxiv.org/abs/2405.16284v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Comparative Analysis of Open-Source Language Models in Summarizing\n  Medical Text Data", "abstract": "Unstructured text in medical notes and dialogues contains rich information.\nRecent advancements in Large Language Models (LLMs) have demonstrated superior\nperformance in question answering and summarization tasks on unstructured text\ndata, outperforming traditional text analysis approaches. However, there is a\nlack of scientific studies in the literature that methodically evaluate and\nreport on the performance of different LLMs, specifically for domain-specific\ndata such as medical chart notes. We propose an evaluation approach to analyze\nthe performance of open-source LLMs such as Llama2 and Mistral for medical\nsummarization tasks, using GPT-4 as an assessor. Our innovative approach to\nquantitative evaluation of LLMs can enable quality control, support the\nselection of effective LLMs for specific tasks, and advance knowledge discovery\nin digital health.", "published": "2024-05-25 16:16:22", "link": "http://arxiv.org/abs/2405.16295v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to Reason via Program Generation, Emulation, and Search", "abstract": "Program synthesis with language models (LMs) has unlocked a large set of\nreasoning abilities; code-tuned LMs have proven adept at generating programs\nthat solve a wide variety of algorithmic symbolic manipulation tasks (e.g. word\nconcatenation). However, not all reasoning tasks are easily expressible as\ncode, e.g. tasks involving commonsense reasoning, moral decision-making, and\nsarcasm understanding. Our goal is to extend an LM's program synthesis skills\nto such tasks and evaluate the results via pseudo-programs, namely Python\nprograms where some leaf function calls are left undefined. To that end, we\npropose, Code Generation and Emulated EXecution (CoGEX). CoGEX works by (1)\ntraining LMs to generate pseudo-programs, (2) teaching them to emulate their\ngenerated program's execution, including those leaf functions, allowing the\nLM's knowledge to fill in the execution gaps; and (3) using them to search over\nmany programs to find an optimal one. To adapt the CoGEX model to a new task,\nwe introduce a method for performing program search to find a single program\nwhose pseudo-execution yields optimal performance when applied to all the\ninstances of a given dataset. We show that our approach yields large\nimprovements compared to standard in-context learning approaches on a battery\nof tasks, both algorithmic and soft reasoning. This result thus demonstrates\nthat code synthesis can be applied to a much broader class of problems than\npreviously considered. Our released dataset, fine-tuned models, and\nimplementation can be found at \\url{https://github.com/nweir127/CoGEX}.", "published": "2024-05-25 19:40:50", "link": "http://arxiv.org/abs/2405.16337v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "STRIDE: A Tool-Assisted LLM Agent Framework for Strategic and\n  Interactive Decision-Making", "abstract": "Large Language Models (LLMs) like GPT-4 have revolutionized natural language\nprocessing, showing remarkable linguistic proficiency and reasoning\ncapabilities. However, their application in strategic multi-agent\ndecision-making environments is hampered by significant limitations including\npoor mathematical reasoning, difficulty in following instructions, and a\ntendency to generate incorrect information. These deficiencies hinder their\nperformance in strategic and interactive tasks that demand adherence to nuanced\ngame rules, long-term planning, exploration in unknown environments, and\nanticipation of opponents' moves. To overcome these obstacles, this paper\npresents a novel LLM agent framework equipped with memory and specialized tools\nto enhance their strategic decision-making capabilities. We deploy the tools in\na number of economically important environments, in particular bilateral\nbargaining and multi-agent and dynamic mechanism design. We employ quantitative\nmetrics to assess the framework's performance in various strategic\ndecision-making problems. Our findings establish that our enhanced framework\nsignificantly improves the strategic decision-making capability of LLMs. While\nwe highlight the inherent limitations of current LLM models, we demonstrate the\nimprovements through targeted enhancements, suggesting a promising direction\nfor future developments in LLM applications for interactive environments.", "published": "2024-05-25 23:25:10", "link": "http://arxiv.org/abs/2405.16376v2", "categories": ["cs.CL", "cs.GT"], "primary_category": "cs.CL"}
{"title": "Retrieval-Augmented Conversational Recommendation with Prompt-based\n  Semi-Structured Natural Language State Tracking", "abstract": "Conversational recommendation (ConvRec) systems must understand rich and\ndiverse natural language (NL) expressions of user preferences and intents,\noften communicated in an indirect manner (e.g., \"I'm watching my weight\"). Such\ncomplex utterances make retrieving relevant items challenging, especially if\nonly using often incomplete or out-of-date metadata. Fortunately, many domains\nfeature rich item reviews that cover standard metadata categories and offer\ncomplex opinions that might match a user's interests (e.g., \"classy joint for a\ndate\"). However, only recently have large language models (LLMs) let us unlock\nthe commonsense connections between user preference utterances and complex\nlanguage in user-generated reviews. Further, LLMs enable novel paradigms for\nsemi-structured dialogue state tracking, complex intent and preference\nunderstanding, and generating recommendations, explanations, and question\nanswers. We thus introduce a novel technology RA-Rec, a Retrieval-Augmented,\nLLM-driven dialogue state tracking system for ConvRec, showcased with a video,\nopen source GitHub repository, and interactive Google Colab notebook.", "published": "2024-05-25 15:41:26", "link": "http://arxiv.org/abs/2406.00033v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Unlocking Insights from Logbooks Using AI", "abstract": "Electronic logbooks contain valuable information about activities and events\nconcerning their associated particle accelerator facilities. However, the\nhighly technical nature of logbook entries can hinder their usability and\nautomation. As natural language processing (NLP) continues advancing, it offers\nopportunities to address various challenges that logbooks present. This work\nexplores jointly testing a tailored Retrieval Augmented Generation (RAG) model\nfor enhancing the usability of particle accelerator logbooks at institutes like\nDESY, BESSY, Fermilab, BNL, SLAC, LBNL, and CERN. The RAG model uses a corpus\nbuilt on logbook contributions and aims to unlock insights from these logbooks\nby leveraging retrieval over facility datasets, including discussion about\npotential multimodal sources. Our goals are to increase the FAIR-ness\n(findability, accessibility, interoperability, and reusability) of logbooks by\nexploiting their information content to streamline everyday use, enable\nmacro-analysis for root cause analysis, and facilitate problem-solving\nautomation.", "published": "2024-05-25 13:38:15", "link": "http://arxiv.org/abs/2406.12881v1", "categories": ["physics.acc-ph", "cs.CL"], "primary_category": "physics.acc-ph"}
{"title": "Theoretical Analysis of Weak-to-Strong Generalization", "abstract": "Strong student models can learn from weaker teachers: when trained on the\npredictions of a weaker model, a strong pretrained student can learn to correct\nthe weak model's errors and generalize to examples where the teacher is not\nconfident, even when these examples are excluded from training. This enables\nlearning from cheap, incomplete, and possibly incorrect label information, such\nas coarse logical rules or the generations of a language model. We show that\nexisting weak supervision theory fails to account for both of these effects,\nwhich we call pseudolabel correction and coverage expansion, respectively. We\ngive a new bound based on expansion properties of the data distribution and\nstudent hypothesis class that directly accounts for pseudolabel correction and\ncoverage expansion. Our bounds capture the intuition that weak-to-strong\ngeneralization occurs when the strong model is unable to fit the mistakes of\nthe weak teacher without incurring additional error. We show that these\nexpansion properties can be checked from finite data and give empirical\nevidence that they hold in practice.", "published": "2024-05-25 03:48:12", "link": "http://arxiv.org/abs/2405.16043v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Prompt Optimization with EASE? Efficient Ordering-aware Automated\n  Selection of Exemplars", "abstract": "Large language models (LLMs) have shown impressive capabilities in real-world\napplications. The capability of in-context learning (ICL) allows us to adapt an\nLLM to downstream tasks by including input-label exemplars in the prompt\nwithout model fine-tuning. However, the quality of these exemplars in the\nprompt greatly impacts performance, highlighting the need for an effective\nautomated exemplar selection method. Recent studies have explored\nretrieval-based approaches to select exemplars tailored to individual test\nqueries, which can be undesirable due to extra test-time computation and an\nincreased risk of data exposure. Moreover, existing methods fail to adequately\naccount for the impact of exemplar ordering on the performance. On the other\nhand, the impact of the instruction, another essential component in the prompt\ngiven to the LLM, is often overlooked in existing exemplar selection methods.\nTo address these challenges, we propose a novel method named EASE, which\nleverages the hidden embedding from a pre-trained language model to represent\nordered sets of exemplars and uses a neural bandit algorithm to optimize the\nsets of exemplars while accounting for exemplar ordering. Our EASE can\nefficiently find an ordered set of exemplars that performs well for all test\nqueries from a given task, thereby eliminating test-time computation.\nImportantly, EASE can be readily extended to jointly optimize both the\nexemplars and the instruction. Through extensive empirical evaluations\n(including novel tasks), we demonstrate the superiority of EASE over existing\nmethods, and reveal practical insights about the impact of exemplar selection\non ICL, which may be of independent interest. Our code is available at\nhttps://github.com/ZhaoxuanWu/EASE-Prompt-Optimization.", "published": "2024-05-25 08:23:05", "link": "http://arxiv.org/abs/2405.16122v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.AI"}
{"title": "DefSent+: Improving sentence embeddings of language models by projecting\n  definition sentences into a quasi-isotropic or isotropic vector space of\n  unlimited dictionary entries", "abstract": "This paper presents a significant improvement on the previous conference\npaper known as DefSent. The prior study seeks to improve sentence embeddings of\nlanguage models by projecting definition sentences into the vector space of\ndictionary entries. We discover that this approach is not fully explored due to\nthe methodological limitation of using word embeddings of language models to\nrepresent dictionary entries. This leads to two hindrances. First, dictionary\nentries are constrained by the single-word vocabulary, and thus cannot be fully\nexploited. Second, semantic representations of language models are known to be\nanisotropic, but pre-processing word embeddings for DefSent is not allowed\nbecause its weight is frozen during training and tied to the prediction layer.\nIn this paper, we propose a novel method to progressively build entry\nembeddings not subject to the limitations. As a result, definition sentences\ncan be projected into a quasi-isotropic or isotropic vector space of unlimited\ndictionary entries, so that sentence embeddings of noticeably better quality\nare attainable. We abbreviate our approach as DefSent+ (a plus version of\nDefSent), involving the following strengths: 1) the task performance on\nmeasuring sentence similarities is significantly improved compared to DefSent;\n2) when DefSent+ is used to further train data-augmented models like SIMCSE,\nSNCSE, and SynCSE, state-of-the-art performance on measuring sentence\nsimilarities can be achieved among the approaches without using manually\nlabeled datasets; 3) DefSent+ is also competitive in feature-based transfer for\nNLP downstream tasks.", "published": "2024-05-25 09:43:38", "link": "http://arxiv.org/abs/2405.16153v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bi-reachability in Petri nets with data", "abstract": "We investigate Petri nets with data, an extension of plain Petri nets where\ntokens carry values from an infinite data domain, and executability of\ntransitions is conditioned by equalities between data values. We provide a\ndecision procedure for the bi-reachability problem: given a Petri net and its\ntwo configurations, we ask if each of the configurations is reachable from the\nother. This pushes forward the decidability borderline, as the bi-reachability\nproblem subsumes the coverability problem (which is known to be decidable) and\nis subsumed by the reachability problem (whose decidability status is unknown).", "published": "2024-05-25 11:00:44", "link": "http://arxiv.org/abs/2405.16176v2", "categories": ["cs.CL", "cs.FL", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Picturing Ambiguity: A Visual Twist on the Winograd Schema Challenge", "abstract": "Large Language Models (LLMs) have demonstrated remarkable success in tasks\nlike the Winograd Schema Challenge (WSC), showcasing advanced textual\ncommon-sense reasoning. However, applying this reasoning to multimodal domains,\nwhere understanding text and images together is essential, remains a\nsubstantial challenge. To address this, we introduce WinoVis, a novel dataset\nspecifically designed to probe text-to-image models on pronoun disambiguation\nwithin multimodal contexts. Utilizing GPT-4 for prompt generation and Diffusion\nAttentive Attribution Maps (DAAM) for heatmap analysis, we propose a novel\nevaluation framework that isolates the models' ability in pronoun\ndisambiguation from other visual processing challenges. Evaluation of\nsuccessive model versions reveals that, despite incremental advancements,\nStable Diffusion 2.0 achieves a precision of 56.7% on WinoVis, only marginally\nsurpassing random guessing. Further error analysis identifies important areas\nfor future research aimed at advancing text-to-image models in their ability to\ninterpret and interact with the complex visual world.", "published": "2024-05-25 15:28:22", "link": "http://arxiv.org/abs/2405.16277v3", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Confidence Under the Hood: An Investigation into the\n  Confidence-Probability Alignment in Large Language Models", "abstract": "As the use of Large Language Models (LLMs) becomes more widespread,\nunderstanding their self-evaluation of confidence in generated responses\nbecomes increasingly important as it is integral to the reliability of the\noutput of these models. We introduce the concept of Confidence-Probability\nAlignment, that connects an LLM's internal confidence, quantified by token\nprobabilities, to the confidence conveyed in the model's response when\nexplicitly asked about its certainty. Using various datasets and prompting\ntechniques that encourage model introspection, we probe the alignment between\nmodels' internal and expressed confidence. These techniques encompass using\nstructured evaluation scales to rate confidence, including answer options when\nprompting, and eliciting the model's confidence level for outputs it does not\nrecognize as its own. Notably, among the models analyzed, OpenAI's GPT-4 showed\nthe strongest confidence-probability alignment, with an average Spearman's\n$\\hat{\\rho}$ of 0.42, across a wide range of tasks. Our work contributes to the\nongoing efforts to facilitate risk assessment in the application of LLMs and to\nfurther our understanding of model trustworthiness.", "published": "2024-05-25 15:42:04", "link": "http://arxiv.org/abs/2405.16282v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Paths of A Million People: Extracting Life Trajectories from Wikipedia", "abstract": "The life trajectories of notable people have been studied to pinpoint the\ntimes and places of significant events such as birth, death, education,\nmarriage, competition, work, speeches, scientific discoveries, artistic\nachievements, and battles. Understanding how these individuals interact with\nothers provides valuable insights for broader research into human dynamics.\nHowever, the scarcity of trajectory data in terms of volume, density, and\ninter-person interactions, limits relevant studies from being comprehensive and\ninteractive. We mine millions of biography pages from Wikipedia and tackle the\ngeneralization problem stemming from the variety and heterogeneity of the\ntrajectory descriptions. Our ensemble model COSMOS, which combines the idea of\nsemi-supervised learning and contrastive learning, achieves an F1 score of\n85.95%. For this task, we also create a hand-curated dataset,\nWikiLifeTrajectory, consisting of 8,852 (person, time, location) triplets as\nground truth. Besides, we perform an empirical analysis on the trajectories of\n8,272 historians to demonstrate the validity of the extracted results. To\nfacilitate the research on trajectory extractions and help the analytical\nstudies to construct grand narratives, we make our code, the million-level\nextracted trajectories, and the WikiLifeTrajectory dataset publicly available.", "published": "2024-05-25 06:57:33", "link": "http://arxiv.org/abs/2406.00032v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "C3LLM: Conditional Multimodal Content Generation Using Large Language\n  Models", "abstract": "We introduce C3LLM (Conditioned-on-Three-Modalities Large Language Models), a\nnovel framework combining three tasks of video-to-audio, audio-to-text, and\ntext-to-audio together. C3LLM adapts the Large Language Model (LLM) structure\nas a bridge for aligning different modalities, synthesizing the given\nconditional information, and making multimodal generation in a discrete manner.\nOur contributions are as follows. First, we adapt a hierarchical structure for\naudio generation tasks with pre-trained audio codebooks. Specifically, we train\nthe LLM to generate audio semantic tokens from the given conditions, and\nfurther use a non-autoregressive transformer to generate different levels of\nacoustic tokens in layers to better enhance the fidelity of the generated\naudio. Second, based on the intuition that LLMs were originally designed for\ndiscrete tasks with the next-word prediction method, we use the discrete\nrepresentation for audio generation and compress their semantic meanings into\nacoustic tokens, similar to adding \"acoustic vocabulary\" to LLM. Third, our\nmethod combines the previous tasks of audio understanding, video-to-audio\ngeneration, and text-to-audio generation together into one unified model,\nproviding more versatility in an end-to-end fashion. Our C3LLM achieves\nimproved results through various automated evaluation metrics, providing better\nsemantic alignment compared to previous methods.", "published": "2024-05-25 09:10:12", "link": "http://arxiv.org/abs/2405.16136v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Carnatic Raga Identification System using Rigorous Time-Delay Neural\n  Network", "abstract": "Large scale machine learning-based Raga identification continues to be a\nnontrivial issue in the computational aspects behind Carnatic music. Each raga\nconsists of many unique and intrinsic melodic patterns that can be used to\neasily identify them from others. These ragas can also then be used to cluster\nsongs within the same raga, as well as identify songs in other closely related\nragas. In this case, the input sound is analyzed using a combination of steps\nincluding using a Discrete Fourier transformation and using Triangular\nFiltering to create custom bins of possible notes, extracting features from the\npresence of particular notes or lack thereof. Using a combination of Neural\nNetworks including 1D Convolutional Neural Networks conventionally known as\nTime-Delay Neural Networks) and Long Short-Term Memory (LSTM), which are a form\nof Recurrent Neural Networks, the backbone of the classification strategy to\nbuild the model can be created. In addition, to help with variations in shruti,\na long-time attention-based mechanism will be implemented to determine the\nrelative changes in frequency rather than the absolute differences. This will\nprovide a much more meaningful data point when training audio clips in\ndifferent shrutis. To evaluate the accuracy of the classifier, a dataset of 676\nrecordings is used. The songs are distributed across the list of ragas. The\ngoal of this program is to be able to effectively and efficiently label a much\nwider range of audio clips in more shrutis, ragas, and with more background\nnoise.", "published": "2024-05-25 01:31:58", "link": "http://arxiv.org/abs/2405.16000v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
