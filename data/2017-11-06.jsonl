{"title": "Authorship Analysis of Xenophon's Cyropaedia", "abstract": "In the past several decades, many authorship attribution studies have used\ncomputational methods to determine the authors of disputed texts. Disputed\nauthorship is a common problem in Classics, since little information about\nancient documents has survived the centuries. Many scholars have questioned the\nauthenticity of the final chapter of Xenophon's Cyropaedia, a 4th century B.C.\nhistorical text. In this study, we use N-grams frequency vectors with a cosine\nsimilarity function and word frequency vectors with Naive Bayes Classifiers\n(NBC) and Support Vector Machines (SVM) to analyze the authorship of the\nCyropaedia. Although the N-gram analysis shows that the epilogue of the\nCyropaedia differs slightly from the rest of the work, comparing the analysis\nof Xenophon with analyses of Aristotle and Plato suggests that this difference\nis not significant. Both NBC and SVM analyses of word frequencies show that the\nfinal chapter of the Cyropaedia is closely related to the other chapters of the\nCyropaedia. Therefore, this analysis suggests that the disputed chapter was\nwritten by Xenophon. This information can help scholars better understand the\nCyropaedia and also demonstrates the usefulness of applying modern authorship\nanalysis techniques to classical literature.", "published": "2017-11-06 00:39:31", "link": "http://arxiv.org/abs/1711.01684v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distributed Representation for Traditional Chinese Medicine Herb via\n  Deep Learning Models", "abstract": "Traditional Chinese Medicine (TCM) has accumulated a big amount of precious\nresource in the long history of development. TCM prescriptions that consist of\nTCM herbs are an important form of TCM treatment, which are similar to natural\nlanguage documents, but in a weakly ordered fashion. Directly adapting language\nmodeling style methods to learn the embeddings of the herbs can be problematic\nas the herbs are not strictly in order, the herbs in the front of the\nprescription can be connected to the very last ones. In this paper, we propose\nto represent TCM herbs with distributed representations via Prescription Level\nLanguage Modeling (PLLM). In one of our experiments, the correlation between\nour calculated similarity between medicines and the judgment of professionals\nachieves a Spearman score of 55.35 indicating a strong correlation, which\nsurpasses human beginners (TCM related field bachelor student) by a big margin\n(over 10%).", "published": "2017-11-06 03:05:05", "link": "http://arxiv.org/abs/1711.01701v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Dialogue Systems: Recent Advances and New Frontiers", "abstract": "Dialogue systems have attracted more and more attention. Recent advances on\ndialogue systems are overwhelmingly contributed by deep learning techniques,\nwhich have been employed to enhance a wide range of big data applications such\nas computer vision, natural language processing, and recommender systems. For\ndialogue systems, deep learning can leverage a massive amount of data to learn\nmeaningful feature representations and response generation strategies, while\nrequiring a minimum amount of hand-crafting. In this article, we give an\noverview to these recent advances on dialogue systems from various perspectives\nand discuss some possible research directions. In particular, we generally\ndivide existing dialogue systems into task-oriented and non-task-oriented\nmodels, then detail how deep learning techniques help them with representative\nalgorithms and finally discuss some appealing research directions that can\nbring the dialogue system research into a new frontier.", "published": "2017-11-06 05:20:54", "link": "http://arxiv.org/abs/1711.01731v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of Croatian Word Embeddings", "abstract": "Croatian is poorly resourced and highly inflected language from Slavic\nlanguage family. Nowadays, research is focusing mostly on English. We created a\nnew word analogy corpus based on the original English Word2vec word analogy\ncorpus and added some of the specific linguistic aspects from Croatian\nlanguage. Next, we created Croatian WordSim353 and RG65 corpora for a basic\nevaluation of word similarities. We compared created corpora on two popular\nword representation models, based on Word2Vec tool and fastText tool. Models\nhas been trained on 1.37B tokens training data corpus and tested on a new\nrobust Croatian word analogy corpus. Results show that models are able to\ncreate meaningful word representation. This research has shown that free word\norder and the higher morphological complexity of Croatian language influences\nthe quality of resulting word embeddings.", "published": "2017-11-06 09:40:41", "link": "http://arxiv.org/abs/1711.01804v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Speed Reading via Skim-RNN", "abstract": "Inspired by the principles of speed reading, we introduce Skim-RNN, a\nrecurrent neural network (RNN) that dynamically decides to update only a small\nfraction of the hidden state for relatively unimportant input tokens. Skim-RNN\ngives computational advantage over an RNN that always updates the entire hidden\nstate. Skim-RNN uses the same input and output interfaces as a standard RNN and\ncan be easily used instead of RNNs in existing models. In our experiments, we\nshow that Skim-RNN can achieve significantly reduced computational cost without\nlosing accuracy compared to standard RNNs across five different natural\nlanguage tasks. In addition, we demonstrate that the trade-off between accuracy\nand speed of Skim-RNN can be dynamically controlled during inference time in a\nstable manner. Our analysis also shows that Skim-RNN running on a single CPU\noffers lower latency compared to standard RNNs on GPUs.", "published": "2017-11-06 18:58:46", "link": "http://arxiv.org/abs/1711.02085v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TAMU at KBP 2017: Event Nugget Detection and Coreference Resolution", "abstract": "In this paper, we describe TAMU's system submitted to the TAC KBP 2017 event\nnugget detection and coreference resolution task. Our system builds on the\nstatistical and empirical observations made on training and development data.\nWe found that modifiers of event nuggets tend to have unique syntactic\ndistribution. Their parts-of-speech tags and dependency relations provides them\nessential characteristics that are useful in identifying their span and also\ndefining their types and realis status. We further found that the joint\nmodeling of event span detection and realis status identification performs\nbetter than the individual models for both tasks. Our simple system designed\nusing minimal features achieved the micro-average F1 scores of 57.72, 44.27 and\n42.47 for event span detection, type identification and realis status\nclassification tasks respectively. Also, our system achieved the CoNLL F1 score\nof 27.20 in event coreference resolution task.", "published": "2017-11-06 20:30:50", "link": "http://arxiv.org/abs/1711.02162v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Language-Universal End-to-End Speech Recognition", "abstract": "Building speech recognizers in multiple languages typically involves\nreplicating a monolingual training recipe for each language, or utilizing a\nmulti-task learning approach where models for different languages have separate\noutput labels but share some internal parameters. In this work, we exploit\nrecent progress in end-to-end speech recognition to create a single\nmultilingual speech recognition system capable of recognizing any of the\nlanguages seen in training. To do so, we propose the use of a universal\ncharacter set that is shared among all languages. We also create a\nlanguage-specific gating mechanism within the network that can modulate the\nnetwork's internal representations in a language-specific way. We evaluate our\nproposed approach on the Microsoft Cortana task across three languages and show\nthat our system outperforms both the individual monolingual systems and systems\nbuilt with a multi-task learning approach. We also show that this model can be\nused to initialize a monolingual speech recognizer, and can be used to create a\nbilingual model for use in code-switching scenarios.", "published": "2017-11-06 22:48:55", "link": "http://arxiv.org/abs/1711.02207v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved training for online end-to-end speech recognition systems", "abstract": "Achieving high accuracy with end-to-end speech recognizers requires careful\nparameter initialization prior to training. Otherwise, the networks may fail to\nfind a good local optimum. This is particularly true for online networks, such\nas unidirectional LSTMs. Currently, the best strategy to train such systems is\nto bootstrap the training from a tied-triphone system. However, this is time\nconsuming, and more importantly, is impossible for languages without a\nhigh-quality pronunciation lexicon. In this work, we propose an initialization\nstrategy that uses teacher-student learning to transfer knowledge from a large,\nwell-trained, offline end-to-end speech recognition model to an online\nend-to-end model, eliminating the need for a lexicon or any other linguistic\nresources. We also explore curriculum learning and label smoothing and show how\nthey can be combined with the proposed teacher-student learning for further\nimprovements. We evaluate our methods on a Microsoft Cortana personal assistant\ntask and show that the proposed method results in a 19 % relative improvement\nin word error rate compared to a randomly-initialized baseline system.", "published": "2017-11-06 22:59:48", "link": "http://arxiv.org/abs/1711.02212v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structure Regularized Bidirectional Recurrent Convolutional Neural\n  Network for Relation Classification", "abstract": "Relation classification is an important semantic processing task in the field\nof natural language processing (NLP). In this paper, we present a novel model,\nStructure Regularized Bidirectional Recurrent Convolutional Neural\nNetwork(SR-BRCNN), to classify the relation of two entities in a sentence, and\nthe new dataset of Chinese Sanwen for named entity recognition and relation\nclassification. Some state-of-the-art systems concentrate on modeling the\nshortest dependency path (SDP) between two entities leveraging convolutional or\nrecurrent neural networks. We further explore how to make full use of the\ndependency relations information in the SDP and how to improve the model by the\nmethod of structure regularization. We propose a structure regularized model to\nlearn relation representations along the SDP extracted from the forest formed\nby the structure regularized dependency tree, which benefits reducing the\ncomplexity of the whole model and helps improve the $F_{1}$ score by 10.3.\nExperimental results show that our method outperforms the state-of-the-art\napproaches on the Chinese Sanwen task and performs as well on the SemEval-2010\nTask 8 dataset\\footnote{The Chinese Sanwen corpus this paper developed and used\nwill be released in the further.", "published": "2017-11-06 06:21:01", "link": "http://arxiv.org/abs/1711.02509v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Weighted Transformer Network for Machine Translation", "abstract": "State-of-the-art results on neural machine translation often use attentional\nsequence-to-sequence models with some form of convolution or recursion. Vaswani\net al. (2017) propose a new architecture that avoids recurrence and convolution\ncompletely. Instead, it uses only self-attention and feed-forward layers. While\nthe proposed architecture achieves state-of-the-art results on several machine\ntranslation tasks, it requires a large number of parameters and training\niterations to converge. We propose Weighted Transformer, a Transformer with\nmodified attention layers, that not only outperforms the baseline network in\nBLEU score but also converges 15-40% faster. Specifically, we replace the\nmulti-head attention by multiple self-attention branches that the model learns\nto combine during the training process. Our model improves the state-of-the-art\nperformance by 0.5 BLEU points on the WMT 2014 English-to-German translation\ntask and by 0.4 on the English-to-French translation task.", "published": "2017-11-06 19:35:00", "link": "http://arxiv.org/abs/1711.02132v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Synthetic and Natural Noise Both Break Neural Machine Translation", "abstract": "Character-based neural machine translation (NMT) models alleviate\nout-of-vocabulary issues, learn morphology, and move us closer to completely\nend-to-end translation systems. Unfortunately, they are also very brittle and\neasily falter when presented with noisy data. In this paper, we confront NMT\nmodels with synthetic and natural sources of noise. We find that\nstate-of-the-art models fail to translate even moderately noisy texts that\nhumans have no trouble comprehending. We explore two approaches to increase\nmodel robustness: structure-invariant word representations and robust training\non noisy texts. We find that a model based on a character convolutional neural\nnetwork is able to simultaneously learn representations robust to multiple\nkinds of noise.", "published": "2017-11-06 20:59:58", "link": "http://arxiv.org/abs/1711.02173v2", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Multilingual Speech Recognition With A Single End-To-End Model", "abstract": "Training a conventional automatic speech recognition (ASR) system to support\nmultiple languages is challenging because the sub-word unit, lexicon and word\ninventories are typically language specific. In contrast, sequence-to-sequence\nmodels are well suited for multilingual ASR because they encapsulate an\nacoustic, pronunciation and language model jointly in a single network. In this\nwork we present a single sequence-to-sequence ASR model trained on 9 different\nIndian languages, which have very little overlap in their scripts.\nSpecifically, we take a union of language-specific grapheme sets and train a\ngrapheme-based sequence-to-sequence model jointly on data from all languages.\nWe find that this model, which is not explicitly given any information about\nlanguage identity, improves recognition performance by 21% relative compared to\nanalogous sequence-to-sequence models trained on each language individually. By\nmodifying the model to accept a language identifier as an additional input\nfeature, we further improve performance by an additional 7% relative and\neliminate confusion between different languages.", "published": "2017-11-06 01:55:45", "link": "http://arxiv.org/abs/1711.01694v2", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "$A^{4}NT$: Author Attribute Anonymity by Adversarial Training of Neural\n  Machine Translation", "abstract": "Text-based analysis methods allow to reveal privacy relevant author\nattributes such as gender, age and identify of the text's author. Such methods\ncan compromise the privacy of an anonymous author even when the author tries to\nremove privacy sensitive content. In this paper, we propose an automatic\nmethod, called Adversarial Author Attribute Anonymity Neural Translation\n($A^4NT$), to combat such text-based adversaries. We combine\nsequence-to-sequence language models used in machine translation and generative\nadversarial networks to obfuscate author attributes. Unlike machine translation\ntechniques which need paired data, our method can be trained on unpaired\ncorpora of text containing different authors. Importantly, we propose and\nevaluate techniques to impose constraints on our $A^4NT$ to preserve the\nsemantics of the input text. $A^4NT$ learns to make minimal changes to the\ninput text to successfully fool author attribute classifiers, while aiming to\nmaintain the meaning of the input. We show through experiments on two different\ndatasets and three settings that our proposed method is effective in fooling\nthe author attribute classifiers and thereby improving the anonymity of\nauthors.", "published": "2017-11-06 14:54:56", "link": "http://arxiv.org/abs/1711.01921v3", "categories": ["cs.CR", "cs.CL", "cs.CY", "cs.SI", "stat.ML"], "primary_category": "cs.CR"}
{"title": "Minimum-Phase HRTF Modeling of Pinna Spectral Notches using Group Delay\n  Decomposition", "abstract": "Accurate reconstruction of HRTFs is important in the development of high\nquality binaural sound synthesis systems. Conventionally, minimum phase HRTF\nmodel development for reconstruction of HRTFs has been limited to minimum\nphase-pure delay models which ignore the all pass component of the HRTF. In\nthis paper, a novel method for minimum phase HRTF modelling of Pinna Spectral\nNotches (PSNs) using group delay decomposition is proposed. The proposed model\ncaptures the PSNs contributed by both the minimum phase and all pass component\nof HRTF thus facilitating an accurate reconstruction of HRTFs. The purely\nminimum phase HRTF components and their corresponding spatial angles are first\nidentified using Fourier Bessel Series method that ensures a continuous\nevolution of the PSNs. The minimum phase-pure delay model is used to\nreconstruct HRTF for these spatial angles. Subsequently, the spatial angles\nwhich require both the minimum phase and all pass components are modelled using\nan all-pass filter cascaded with minimum-phase pure-delay model. Performance of\nthe proposed model is evaluated by conducting experiments on PSN extraction,\ncross coherence analysis, and binaural synthesis. Both objective and subjective\nevaluation results are used to indicate the significance of the proposed model\nin binaural sound synthesis.", "published": "2017-11-06 13:13:37", "link": "http://arxiv.org/abs/1711.01872v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Mandarin tone modeling using recurrent neural networks", "abstract": "We propose an Encoder-Classifier framework to model the Mandarin tones using\nrecurrent neural networks (RNN). In this framework, extracted frames of\nfeatures for tone classification are fed in to the RNN and casted into a fixed\ndimensional vector (tone embedding) and then classified into tone types using a\nsoftmax layer along with other auxiliary inputs. We investigate various\nconfigurations that help to improve the model, including pooling, feature\nsplicing and utilization of syllable-level tone embeddings. Besides, tone\nembeddings and durations of the contextual syllables are exploited to\nfacilitate tone classification. Experimental results on Mandarin tone\nclassification show the proposed network setups improve tone classification\naccuracy. The results indicate that the RNN encoder-classifier based tone model\nflexibly accommodates heterogeneous inputs (sequential and segmental) and hence\nhas the advantages from both the sequential classification tone models and\nsegmental classification tone models.", "published": "2017-11-06 15:17:47", "link": "http://arxiv.org/abs/1711.01946v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unsupervised Learning of Semantic Audio Representations", "abstract": "Even in the absence of any explicit semantic annotation, vast collections of\naudio recordings provide valuable information for learning the categorical\nstructure of sounds. We consider several class-agnostic semantic constraints\nthat apply to unlabeled nonspeech audio: (i) noise and translations in time do\nnot change the underlying sound category, (ii) a mixture of two sound events\ninherits the categories of the constituents, and (iii) the categories of events\nin close temporal proximity are likely to be the same or related. Without\nlabels to ground them, these constraints are incompatible with classification\nloss functions. However, they may still be leveraged to identify geometric\ninequalities needed for triplet loss-based training of convolutional neural\nnetworks. The result is low-dimensional embeddings of the input spectrograms\nthat recover 41% and 84% of the performance of their fully-supervised\ncounterparts when applied to downstream query-by-example sound retrieval and\nsound event classification tasks, respectively. Moreover, in\nlimited-supervision settings, our unsupervised embeddings double the\nstate-of-the-art classification performance.", "published": "2017-11-06 22:54:01", "link": "http://arxiv.org/abs/1711.02209v1", "categories": ["cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
