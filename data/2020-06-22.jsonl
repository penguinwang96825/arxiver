{"title": "Efficient text generation of user-defined topic using generative\n  adversarial networks", "abstract": "This study focused on efficient text generation using generative adversarial\nnetworks (GAN). Assuming that the goal is to generate a paragraph of a\nuser-defined topic and sentimental tendency, conventionally the whole network\nhas to be re-trained to obtain new results each time when a user changes the\ntopic. This would be time-consuming and impractical. Therefore, we propose a\nUser-Defined GAN (UD-GAN) with two-level discriminators to solve this problem.\nThe first discriminator aims to guide the generator to learn paragraph-level\ninformation and sentence syntactic structure, which is constructed by\nmultiple-LSTMs. The second one copes with higher-level information, such as the\nuser-defined sentiment and topic for text generation. The cosine similarity\nbased on TF-IDF and length penalty are adopted to determine the relevance of\nthe topic. Then, the second discriminator is re-trained with the generator if\nthe topic or sentiment for text generation is modified. The system evaluations\nare conducted to compare the performance of the proposed method with other\nGAN-based ones. The objective results showed that the proposed method is\ncapable of generating texts with less time than others and the generated text\nis related to the user-defined topic and sentiment. We will further investigate\nthe possibility of incorporating more detailed paragraph information such as\nsemantics into text generation to enhance the result.", "published": "2020-06-22 04:49:47", "link": "http://arxiv.org/abs/2006.12005v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clinical Predictive Keyboard using Statistical and Neural Language\n  Modeling", "abstract": "A language model can be used to predict the next word during authoring, to\ncorrect spelling or to accelerate writing (e.g., in sms or emails). Language\nmodels, however, have only been applied in a very small scale to assist\nphysicians during authoring (e.g., discharge summaries or radiology reports).\nBut along with the assistance to the physician, computer-based systems which\nexpedite the patient's exit also assist in decreasing the hospital infections.\nWe employed statistical and neural language modeling to predict the next word\nof a clinical text and assess all the models in terms of accuracy and keystroke\ndiscount in two datasets with radiology reports. We show that a neural language\nmodel can achieve as high as 51.3% accuracy in radiology reports (one out of\ntwo words predicted correctly). We also show that even when the models are\nemployed only for frequent words, the physician can save valuable time.", "published": "2020-06-22 07:20:20", "link": "http://arxiv.org/abs/2006.12040v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Non-Taxonomic Relations for Measuring Semantic Similarity and\n  Relatedness in WordNet", "abstract": "Various applications in the areas of computational linguistics and artificial\nintelligence employ semantic similarity to solve challenging tasks, such as\nword sense disambiguation, text classification, information retrieval, machine\ntranslation, and document clustering. Previous work on semantic similarity\nfollowed a mono-relational approach using mostly the taxonomic relation \"ISA\".\nThis paper explores the benefits of using all types of non-taxonomic relations\nin large linked data, such as WordNet knowledge graph, to enhance existing\nsemantic similarity and relatedness measures. We propose a holistic\npoly-relational approach based on a new relation-based information content and\nnon-taxonomic-based weighted paths to devise a comprehensive semantic\nsimilarity and relatedness measure. To demonstrate the benefits of exploiting\nnon-taxonomic relations in a knowledge graph, we used three strategies to\ndeploy non-taxonomic relations at different granularity levels. We conducted\nexperiments on four well-known gold standard datasets, and the results\ndemonstrated the robustness and scalability of the proposed semantic similarity\nand relatedness measure, which significantly improves existing similarity\nmeasures.", "published": "2020-06-22 09:59:39", "link": "http://arxiv.org/abs/2006.12106v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ReCO: A Large Scale Chinese Reading Comprehension Dataset on Opinion", "abstract": "This paper presents the ReCO, a human-curated ChineseReading Comprehension\ndataset on Opinion. The questions in ReCO are opinion based queries issued to\nthe commercial search engine. The passages are provided by the crowdworkers who\nextract the support snippet from the retrieved documents. Finally, an\nabstractive yes/no/uncertain answer was given by the crowdworkers. The release\nof ReCO consists of 300k questions that to our knowledge is the largest in\nChinese reading comprehension. A prominent characteristic of ReCO is that in\naddition to the original context paragraph, we also provided the support\nevidence that could be directly used to answer the question. Quality analysis\ndemonstrates the challenge of ReCO that requires various types of reasoning\nskills, such as causal inference, logical reasoning, etc. Current QA models\nthat perform very well on many question answering problems, such as BERT, only\nachieve 77% accuracy on this dataset, a large margin behind humans nearly 92%\nperformance, indicating ReCO presents a good challenge for machine reading\ncomprehension. The codes, datasets are freely available at\nhttps://github.com/benywon/ReCO.", "published": "2020-06-22 11:18:26", "link": "http://arxiv.org/abs/2006.12146v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shared Task on Evaluating Accuracy in Natural Language Generation", "abstract": "We propose a shared task on methodologies and algorithms for evaluating the\naccuracy of generated texts. Participants will measure the accuracy of\nbasketball game summaries produced by NLG systems from basketball box score\ndata.", "published": "2020-06-22 13:30:35", "link": "http://arxiv.org/abs/2006.12234v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dirichlet-Smoothed Word Embeddings for Low-Resource Settings", "abstract": "Nowadays, classical count-based word embeddings using positive pointwise\nmutual information (PPMI) weighted co-occurrence matrices have been widely\nsuperseded by machine-learning-based methods like word2vec and GloVe. But these\nmethods are usually applied using very large amounts of text data. In many\ncases, however, there is not much text data available, for example for specific\ndomains or low-resource languages. This paper revisits PPMI by adding Dirichlet\nsmoothing to correct its bias towards rare words. We evaluate on standard word\nsimilarity data sets and compare to word2vec and the recent state of the art\nfor low-resource settings: Positive and Unlabeled (PU) Learning for word\nembeddings. The proposed method outperforms PU-Learning for low-resource\nsettings and obtains competitive results for Maltese and Luxembourgish.", "published": "2020-06-22 16:43:34", "link": "http://arxiv.org/abs/2006.12414v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Step Towards Interpretable Authorship Verification", "abstract": "A central problem that has been researched for many years in the field of\ndigital text forensics is the question whether two documents were written by\nthe same author. Authorship verification (AV) is a research branch in this\nfield that deals with this question. Over the years, research activities in the\ncontext of AV have steadily increased, which has led to a variety of approaches\ntrying to solve this problem. Many of these approaches, however, make use of\nfeatures that are related to or influenced by the topic of the documents.\nTherefore, it may accidentally happen that their verification results are based\nnot on the writing style (the actual focus of AV), but on the topic of the\ndocuments. To address this problem, we propose an alternative AV approach that\nconsiders only topic-agnostic features in its classification decision. In\naddition, we present a post-hoc interpretation method that allows to understand\nwhich particular features have contributed to the prediction of the proposed AV\nmethod. To evaluate the performance of our AV method, we compared it with ten\ncompeting baselines (including the current state of the art) on four\nchallenging data sets. The results show that our approach outperforms all\nbaselines in two cases (with a maximum accuracy of 84%), while in the other two\ncases it performs as well as the strongest baseline.", "published": "2020-06-22 16:44:26", "link": "http://arxiv.org/abs/2006.12418v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Students Need More Attention: BERT-based AttentionModel for Small Data\n  with Application to AutomaticPatient Message Triage", "abstract": "Small and imbalanced datasets commonly seen in healthcare represent a\nchallenge when training classifiers based on deep learning models. So\nmotivated, we propose a novel framework based on BioBERT (Bidirectional Encoder\nRepresentations from Transformers forBiomedical TextMining). Specifically, (i)\nwe introduce Label Embeddings for Self-Attention in each layer of BERT, which\nwe call LESA-BERT, and (ii) by distilling LESA-BERT to smaller variants, we aim\nto reduce overfitting and model size when working on small datasets. As an\napplication, our framework is utilized to build a model for patient portal\nmessage triage that classifies the urgency of a message into three categories:\nnon-urgent, medium and urgent. Experiments demonstrate that our approach can\noutperform several strong baseline classifiers by a significant margin of 4.3%\nin terms of macro F1 score. The code for this project is publicly available at\n\\url{https://github.com/shijing001/text_classifiers}.", "published": "2020-06-22 03:39:00", "link": "http://arxiv.org/abs/2006.11991v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Examination of Community Sentiment Dynamics due to COVID-19 Pandemic: A\n  Case Study from A State in Australia", "abstract": "The outbreak of the novel Coronavirus Disease 2019 (COVID-19) has caused\nunprecedented impacts to people's daily life around the world. Various measures\nand policies such as lockdown and social-distancing are implemented by\ngovernments to combat the disease during the pandemic period. These measures\nand policies as well as virus itself may cause different mental health issues\nto people such as depression, anxiety, sadness, etc. In this paper, we exploit\nthe massive text data posted by Twitter users to analyse the sentiment dynamics\nof people living in the state of New South Wales (NSW) in Australia during the\npandemic period. Different from the existing work that mostly focuses the\ncountry-level and static sentiment analysis, we analyse the sentiment dynamics\nat the fine-grained local government areas (LGAs). Based on the analysis of\naround 94 million tweets that posted by around 183 thousand users located at\ndifferent LGAs in NSW in five months, we found that people in NSW showed an\noverall positive sentimental polarity and the COVID-19 pandemic decreased the\noverall positive sentimental polarity during the pandemic period. The\nfine-grained analysis of sentiment in LGAs found that despite the dominant\npositive sentiment most of days during the study period, some LGAs experienced\nsignificant sentiment changes from positive to negative. This study also\nanalysed the sentimental dynamics delivered by the hot topics in Twitter such\nas government policies (e.g. the Australia's JobKeeper program, lockdown,\nsocial-distancing) as well as the focused social events (e.g. the Ruby Princess\nCruise). The results showed that the policies and events did affect people's\noverall sentiment, and they affected people's overall sentiment differently at\ndifferent stages.", "published": "2020-06-22 12:30:42", "link": "http://arxiv.org/abs/2006.12185v3", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "MedLatinEpi and MedLatinLit: Two Datasets for the Computational\n  Authorship Analysis of Medieval Latin Texts", "abstract": "We present and make available MedLatinEpi and MedLatinLit, two datasets of\nmedieval Latin texts to be used in research on computational authorship\nanalysis. MedLatinEpi and MedLatinLit consist of 294 and 30 curated texts,\nrespectively, labelled by author; MedLatinEpi texts are of epistolary nature,\nwhile MedLatinLit texts consist of literary comments and treatises about\nvarious subjects. As such, these two datasets lend themselves to supporting\nresearch in authorship analysis tasks, such as authorship attribution,\nauthorship verification, or same-author verification. Along with the datasets\nwe provide experimental results, obtained on these datasets, for the authorship\nverification task, i.e., the task of predicting whether a text of unknown\nauthorship was written by a candidate author or not. We also make available the\nsource code of the authorship verification system we have used, thus allowing\nour experiments to be reproduced, and to be used as baselines, by other\nresearchers. We also describe the application of the above authorship\nverification system, using these datasets as training data, for investigating\nthe authorship of two medieval epistles whose authorship has been disputed by\nscholars.", "published": "2020-06-22 14:22:47", "link": "http://arxiv.org/abs/2006.12289v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Open-Domain Conversational Agents: Current Progress, Open Problems, and\n  Future Directions", "abstract": "We present our view of what is necessary to build an engaging open-domain\nconversational agent: covering the qualities of such an agent, the pieces of\nthe puzzle that have been built so far, and the gaping holes we have not filled\nyet. We present a biased view, focusing on work done by our own group, while\nciting related work in each area. In particular, we discuss in detail the\nproperties of continual learning, providing engaging content, and being\nwell-behaved -- and how to measure success in providing them. We end with a\ndiscussion of our experience and learnings, and our recommendations to the\ncommunity.", "published": "2020-06-22 17:23:47", "link": "http://arxiv.org/abs/2006.12442v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Self-Attention Network based Node Embedding Model", "abstract": "Despite several signs of progress have been made recently, limited research\nhas been conducted for an inductive setting where embeddings are required for\nnewly unseen nodes -- a setting encountered commonly in practical applications\nof deep learning for graph networks. This significantly affects the\nperformances of downstream tasks such as node classification, link prediction\nor community extraction. To this end, we propose SANNE -- a novel unsupervised\nembedding model -- whose central idea is to employ a transformer self-attention\nnetwork to iteratively aggregate vector representations of nodes in random\nwalks. Our SANNE aims to produce plausible embeddings not only for present\nnodes, but also for newly unseen nodes. Experimental results show that the\nproposed SANNE obtains state-of-the-art results for the node classification\ntask on well-known benchmark datasets.", "published": "2020-06-22 09:46:10", "link": "http://arxiv.org/abs/2006.12100v1", "categories": ["cs.LG", "cs.CL", "cs.SI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Self-Supervised Representations Improve End-to-End Speech Translation", "abstract": "End-to-end speech-to-text translation can provide a simpler and smaller\nsystem but is facing the challenge of data scarcity. Pre-training methods can\nleverage unlabeled data and have been shown to be effective on data-scarce\nsettings. In this work, we explore whether self-supervised pre-trained speech\nrepresentations can benefit the speech translation task in both high- and\nlow-resource settings, whether they can transfer well to other languages, and\nwhether they can be effectively combined with other common methods that help\nimprove low-resource end-to-end speech translation such as using a pre-trained\nhigh-resource speech recognition system. We demonstrate that self-supervised\npre-trained features can consistently improve the translation performance, and\ncross-lingual transfer allows to extend to a variety of languages without or\nwith little tuning.", "published": "2020-06-22 10:28:38", "link": "http://arxiv.org/abs/2006.12124v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Using Company Specific Headlines and Convolutional Neural Networks to\n  Predict Stock Fluctuations", "abstract": "This work presents a Convolutional Neural Network (CNN) for the prediction of\nnext-day stock fluctuations using company-specific news headlines. Experiments\nto evaluate model performance using various configurations of word-embeddings\nand convolutional filter widths are reported. The total number of convolutional\nfilters used is far fewer than is common, reducing the dimensionality of the\ntask without loss of accuracy. Furthermore, multiple hidden layers with\ndecreasing dimensionality are employed. A classification accuracy of 61.7\\% is\nachieved using pre-learned embeddings, that are fine-tuned during training to\nrepresent the specific context of this task. Multiple filter widths are also\nimplemented to detect different length phrases that are key for classification.\nTrading simulations are conducted using the presented classification results.\nInitial investments are more than tripled over a 838 day testing period using\nthe optimal classification configuration and a simple trading strategy. Two\nnovel methods are presented to reduce the risk of the trading simulations.\nAdjustment of the sigmoid class threshold and re-labelling headlines using\nmultiple classes form the basis of these methods. A combination of these\napproaches is found to more than double the Average Trade Profit (ATP) achieved\nduring baseline simulations.", "published": "2020-06-22 16:53:26", "link": "http://arxiv.org/abs/2006.12426v1", "categories": ["cs.CL", "cs.LG", "q-fin.ST", "I.2.7"], "primary_category": "cs.CL"}
{"title": "The Depth-to-Width Interplay in Self-Attention", "abstract": "Self-attention architectures, which are rapidly pushing the frontier in\nnatural language processing, demonstrate a surprising depth-inefficient\nbehavior: previous works indicate that increasing the internal representation\n(network width) is just as useful as increasing the number of self-attention\nlayers (network depth). We theoretically predict a width-dependent transition\nbetween depth-efficiency and depth-inefficiency in self-attention. We conduct\nsystematic empirical ablations on networks of depths 6 to 48 that clearly\nreveal the theoretically predicted behaviors, and provide explicit quantitative\nsuggestions regarding the optimal depth-to-width allocation for a given\nself-attention network size. The race towards beyond 1-Trillion parameter\nlanguage models renders informed guidelines for increasing self-attention depth\nand width in tandem an essential ingredient. Our guidelines elucidate the\ndepth-to-width trade-off in self-attention networks of sizes up to the scale of\nGPT3 (which we project to be too deep for its size), and beyond, marking an\nunprecedented width of 30K as optimal for a 1-Trillion parameter network.", "published": "2020-06-22 17:47:09", "link": "http://arxiv.org/abs/2006.12467v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Exploring Software Naturalness through Neural Language Models", "abstract": "The Software Naturalness hypothesis argues that programming languages can be\nunderstood through the same techniques used in natural language processing. We\nexplore this hypothesis through the use of a pre-trained transformer-based\nlanguage model to perform code analysis tasks. Present approaches to code\nanalysis depend heavily on features derived from the Abstract Syntax Tree (AST)\nwhile our transformer-based language models work on raw source code. This work\nis the first to investigate whether such language models can discover AST\nfeatures automatically. To achieve this, we introduce a sequence labeling task\nthat directly probes the language models understanding of AST. Our results show\nthat transformer based language models achieve high accuracy in the AST tagging\ntask. Furthermore, we evaluate our model on a software vulnerability\nidentification task. Importantly, we show that our approach obtains\nvulnerability identification results comparable to graph based approaches that\nrely heavily on compilers for feature extraction.", "published": "2020-06-22 21:56:14", "link": "http://arxiv.org/abs/2006.12641v2", "categories": ["cs.CL", "cs.LG", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Mental representations of objects reflect the ways in which we interact\n  with them", "abstract": "In order to interact with objects in our environment, humans rely on an\nunderstanding of the actions that can be performed on them, as well as their\nproperties. When considering concrete motor actions, this knowledge has been\ncalled the object affordance. Can this notion be generalized to any type of\ninteraction that one can have with an object? In this paper we introduce a\nmethod to represent objects in a space where each dimension corresponds to a\nbroad mode of interaction, based on verb selectional preferences in text\ncorpora. This object embedding makes it possible to predict human judgments of\nverb applicability to objects better than a variety of alternative approaches.\nFurthermore, we show that the dimensions in this space can be used to predict\ncategorical and functional dimensions in a state-of-the-art mental\nrepresentation of objects, derived solely from human judgements of object\nsimilarity. These results suggest that interaction knowledge accounts for a\nlarge part of mental representations of objects.", "published": "2020-06-22 22:05:56", "link": "http://arxiv.org/abs/2007.04245v2", "categories": ["cs.CL", "cs.CV", "cs.LG", "q-bio.NC", "68U15", "J.4; I.5.4; I.2.7"], "primary_category": "cs.CL"}
{"title": "Sound Event Localization and Detection Using Activity-Coupled Cartesian\n  DOA Vector and RD3net", "abstract": "Our systems submitted to the DCASE2020 task~3: Sound Event Localization and\nDetection (SELD) are described in this report. We consider two systems: a\nsingle-stage system that solve sound event localization~(SEL) and sound event\ndetection~(SED) simultaneously, and a two-stage system that first handles the\nSED and SEL tasks individually and later combines those results. As the\nsingle-stage system, we propose a unified training framework that uses an\nactivity-coupled Cartesian DOA vector~(ACCDOA) representation as a single\ntarget for both the SED and SEL tasks. To efficiently estimate sound event\nlocations and activities, we further propose RD3Net, which incorporates\nrecurrent and convolution layers with dense skip connections and dilation. To\ngeneralize the models, we apply three data augmentation techniques: equalized\nmixture data augmentation~(EMDA), rotation of first-order Ambisonic~(FOA)\nsingals, and multichannel extension of SpecAugment. Our systems demonstrate a\nsignificant improvement over the baseline system.", "published": "2020-06-22 05:43:53", "link": "http://arxiv.org/abs/2006.12014v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Musical Smart City: Perspectives on Ubiquitous Sonification", "abstract": "Smart cities are urban areas with sensor networks that collect data used\ntowards efficient management. As a source of ubiquitous data, smart city\ninitiatives present opportunities to enhance inhabitants' urban awareness.\nHowever, making sense of smart city data is challenging and there is a gap\nbetween available data and end-user applications. Sonification emerges as a\npromising method for the interpretation of smart city data and the production\nof novel musical experiences. In this paper, we first present the smart city\nparadigm. We then cover the topics of ubiquitous and mobile music, followed by\nan overview of sonification research. Finally, we propose an approach entitled\nubiquitous sonification and present the initial design of a speculative use\ncase for musical smart city systems, leveraging user and urban data to inform\nbehaviour.", "published": "2020-06-22 14:37:43", "link": "http://arxiv.org/abs/2006.12305v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Articulatory-WaveNet: Autoregressive Model For Acoustic-to-Articulatory\n  Inversion", "abstract": "This paper presents Articulatory-WaveNet, a new approach for\nacoustic-to-articulator inversion. The proposed system uses the WaveNet speech\nsynthesis architecture, with dilated causal convolutional layers using previous\nvalues of the predicted articulatory trajectories conditioned on acoustic\nfeatures. The system was trained and evaluated on the ElectroMagnetic\nArticulography corpus of Mandarin Accented English (EMA-MAE),consisting of 39\nspeakers including both native English speakers and native Mandarin speakers\nspeaking English. Results show significant improvement in both correlation and\nRMSE between the generated and true articulatory trajectories for the new\nmethod, with an average correlation of 0.83, representing a 36% relative\nimprovement over the 0.61 correlation obtained with a baseline Hidden Markov\nModel (HMM)-Gaussian Mixture Model (GMM) inversion framework. To the best of\nour knowledge, this paper presents the first application of a point-by-point\nwaveform synthesis approach to the problem of acoustic-to-articulatory\ninversion and the results show improved performance compared to previous\nmethods for speaker dependent acoustic to articulatory inversion.", "published": "2020-06-22 20:10:35", "link": "http://arxiv.org/abs/2006.12594v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
