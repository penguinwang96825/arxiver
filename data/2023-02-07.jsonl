{"title": "Capturing Topic Framing via Masked Language Modeling", "abstract": "Differential framing of issues can lead to divergent world views on important\nissues. This is especially true in domains where the information presented can\nreach a large audience, such as traditional and social media. Scalable and\nreliable measurement of such differential framing is an important first step in\naddressing them. In this work, based on the intuition that framing affects the\ntone and word choices in written language, we propose a framework for modeling\nthe differential framing of issues through masked token prediction via\nlarge-scale fine-tuned language models (LMs). Specifically, we explore three\nkey factors for our framework: 1) prompt generation methods for the masked\ntoken prediction; 2) methods for normalizing the output of fine-tuned LMs; 3)\nrobustness to the choice of pre-trained LMs used for fine-tuning. Through\nexperiments on a dataset of articles from traditional media outlets covering\nfive diverse and politically polarized topics, we show that our framework can\ncapture differential framing of these topics with high reliability.", "published": "2023-02-07 01:23:14", "link": "http://arxiv.org/abs/2302.03183v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UDApter -- Efficient Domain Adaptation Using Adapters", "abstract": "We propose two methods to make unsupervised domain adaptation (UDA) more\nparameter efficient using adapters, small bottleneck layers interspersed with\nevery layer of the large-scale pre-trained language model (PLM). The first\nmethod deconstructs UDA into a two-step process: first by adding a domain\nadapter to learn domain-invariant information and then by adding a task adapter\nthat uses domain-invariant information to learn task representations in the\nsource domain. The second method jointly learns a supervised classifier while\nreducing the divergence measure. Compared to strong baselines, our simple\nmethods perform well in natural language inference (MNLI) and the cross-domain\nsentiment classification task. We even outperform unsupervised domain\nadaptation methods such as DANN and DSN in sentiment classification, and we are\nwithin 0.85% F1 for natural language inference task, by fine-tuning only a\nfraction of the full model parameters. We release our code at\nhttps://github.com/declare-lab/domadapter", "published": "2023-02-07 02:04:17", "link": "http://arxiv.org/abs/2302.03194v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Benefits of Training Expert Language Models over\n  Instruction Tuning", "abstract": "Recently, Language Models (LMs) instruction-tuned on multiple tasks, also\nknown as multitask-prompted fine-tuning (MT), have shown the capability to\ngeneralize to unseen tasks. Previous work has shown that scaling the number of\ntraining tasks is the key component in making stronger MT LMs. In this work, we\nreport an unexpected finding that an expert LM fine-tuned on just a single task\ncan outperform an MT LM trained with 300+ different tasks on 11 different\nunseen datasets and on 13 datasets of the BIG-bench benchmark by a mean\naccuracy of 3.20% and 1.29%, respectively. This finding casts doubt on the\npreviously held belief that simply scaling the number of tasks makes stronger\nMT LMs. Leveraging this finding, we further show that this distributed approach\nof training a separate expert LM per training task instead of a single MT LM\nfor zero-shot inference possesses many benefits including (1) avoiding negative\ntask transfer that often occurs during instruction tuning, (2) being able to\ncontinually learn new tasks without having to re-train on previous tasks to\navoid catastrophic forgetting, and (3) showing compositional capabilities when\nmerging individual experts together. The code is available at\nhttps://github.com/joeljang/ELM.", "published": "2023-02-07 02:24:30", "link": "http://arxiv.org/abs/2302.03202v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An entity-guided text summarization framework with relational\n  heterogeneous graph neural network", "abstract": "Two crucial issues for text summarization to generate faithful summaries are\nto make use of knowledge beyond text and to make use of cross-sentence\nrelations in text. Intuitive ways for the two issues are Knowledge Graph (KG)\nand Graph Neural Network (GNN) respectively. Entities are semantic units in\ntext and in KG. This paper focuses on both issues by leveraging entities\nmentioned in text to connect GNN and KG for summarization. Firstly, entities\nare leveraged to construct a sentence-entity graph with weighted multi-type\nedges to model sentence relations, and a relational heterogeneous GNN for\nsummarization is proposed to calculate node encodings. Secondly, entities are\nleveraged to link the graph to KG to collect knowledge. Thirdly, entities guide\na two-step summarization framework defining a multi-task selector to select\nsalient sentences and entities, and using an entity-focused abstractor to\ncompress the sentences. GNN is connected with KG by constructing\nsentence-entity graphs where entity-entity edges are built based on KG,\ninitializing entity embeddings on KG, and training entity embeddings using\nentity-entity edges. The relational heterogeneous GNN utilizes both edge\nweights and edge types in GNN to calculate graphs with weighted multi-type\nedges. Experiments show the proposed method outperforms extractive baselines\nincluding the HGNN-based HGNNSum and abstractive baselines including the\nentity-driven SENECA on CNN/DM, and outperforms most baselines on NYT50.\nExperiments on sub-datasets show the density of sentence-entity edges greatly\ninfluences the performance of the proposed method. The greater the density, the\nbetter the performance. Ablations show effectiveness of the method.", "published": "2023-02-07 02:27:21", "link": "http://arxiv.org/abs/2302.03205v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bringing the State-of-the-Art to Customers: A Neural Agent Assistant\n  Framework for Customer Service Support", "abstract": "Building Agent Assistants that can help improve customer service support\nrequires inputs from industry users and their customers, as well as knowledge\nabout state-of-the-art Natural Language Processing (NLP) technology. We combine\nexpertise from academia and industry to bridge the gap and build\ntask/domain-specific Neural Agent Assistants (NAA) with three high-level\ncomponents for: (1) Intent Identification, (2) Context Retrieval, and (3)\nResponse Generation. In this paper, we outline the pipeline of the NAA's core\nsystem and also present three case studies in which three industry partners\nsuccessfully adapt the framework to find solutions to their unique challenges.\nOur findings suggest that a collaborative process is instrumental in spurring\nthe development of emerging NLP models for Conversational AI tasks in industry.\nThe full reference implementation code and results are available at\n\\url{https://github.com/VectorInstitute/NAA}", "published": "2023-02-07 03:11:06", "link": "http://arxiv.org/abs/2302.03222v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AutoWS: Automated Weak Supervision Framework for Text Classification", "abstract": "Creating large, good quality labeled data has become one of the major\nbottlenecks for developing machine learning applications. Multiple techniques\nhave been developed to either decrease the dependence of labeled data\n(zero/few-shot learning, weak supervision) or to improve the efficiency of\nlabeling process (active learning). Among those, Weak Supervision has been\nshown to reduce labeling costs by employing hand crafted labeling functions\ndesigned by domain experts. We propose AutoWS -- a novel framework for\nincreasing the efficiency of weak supervision process while decreasing the\ndependency on domain experts. Our method requires a small set of labeled\nexamples per label class and automatically creates a set of labeling functions\nto assign noisy labels to numerous unlabeled data. Noisy labels can then be\naggregated into probabilistic labels used by a downstream discriminative\nclassifier. Our framework is fully automatic and requires no hyper-parameter\nspecification by users. We compare our approach with different state-of-the-art\nwork on weak supervision and noisy training. Experimental results show that our\nmethod outperforms competitive baselines.", "published": "2023-02-07 07:12:05", "link": "http://arxiv.org/abs/2302.03297v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Manner of Execution from Partial Corrections", "abstract": "Some actions must be executed in different ways depending on the context. For\nexample, wiping away marker requires vigorous force while wiping away almonds\nrequires more gentle force. In this paper we provide a model where an agent\nlearns which manner of action execution to use in which context, drawing on\nevidence from trial and error and verbal corrections when it makes a mistake\n(e.g., ``no, gently''). The learner starts out with a domain model that lacks\nthe concepts denoted by the words in the teacher's feedback; both the words\ndescribing the context (e.g., marker) and the adverbs like ``gently''. We show\nthat through the the semantics of coherence, our agent can perform the symbol\ngrounding that's necessary for exploiting the teacher's feedback so as to solve\nits domain-level planning problem: to perform its actions in the current\ncontext in the right way.", "published": "2023-02-07 09:25:58", "link": "http://arxiv.org/abs/2302.03338v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What do Language Models know about word senses? Zero-Shot WSD with\n  Language Models and Domain Inventories", "abstract": "Language Models are the core for almost any Natural Language Processing\nsystem nowadays. One of their particularities is their contextualized\nrepresentations, a game changer feature when a disambiguation between word\nsenses is necessary. In this paper we aim to explore to what extent language\nmodels are capable of discerning among senses at inference time. We performed\nthis analysis by prompting commonly used Languages Models such as BERT or\nRoBERTa to perform the task of Word Sense Disambiguation (WSD). We leverage the\nrelation between word senses and domains, and cast WSD as a textual entailment\nproblem, where the different hypothesis refer to the domains of the word\nsenses. Our results show that this approach is indeed effective, close to\nsupervised systems.", "published": "2023-02-07 09:55:07", "link": "http://arxiv.org/abs/2302.03353v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity-Aware Dual Co-Attention Network for Fake News Detection", "abstract": "Fake news and misinformation spread rapidly on the Internet. How to identify\nit and how to interpret the identification results have become important\nissues. In this paper, we propose a Dual Co-Attention Network (Dual-CAN) for\nfake news detection, which takes news content, social media replies, and\nexternal knowledge into consideration. Our experimental results support that\nthe proposed Dual-CAN outperforms current representative models in two\nbenchmark datasets. We further make in-depth discussions by comparing how\nmodels work in both datasets with empirical analysis of attention weights.", "published": "2023-02-07 14:00:40", "link": "http://arxiv.org/abs/2302.03475v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Arabic Named Entity Recognition: Past, Recent Advances, and\n  Future Trends", "abstract": "As more and more Arabic texts emerged on the Internet, extracting important\ninformation from these Arabic texts is especially useful. As a fundamental\ntechnology, Named entity recognition (NER) serves as the core component in\ninformation extraction technology, while also playing a critical role in many\nother Natural Language Processing (NLP) systems, such as question answering and\nknowledge graph building. In this paper, we provide a comprehensive review of\nthe development of Arabic NER, especially the recent advances in deep learning\nand pre-trained language model. Specifically, we first introduce the background\nof Arabic NER, including the characteristics of Arabic and existing resources\nfor Arabic NER. Then, we systematically review the development of Arabic NER\nmethods. Traditional Arabic NER systems focus on feature engineering and\ndesigning domain-specific rules. In recent years, deep learning methods achieve\nsignificant progress by representing texts via continuous vector\nrepresentations. With the growth of pre-trained language model, Arabic NER\nyields better performance. Finally, we conclude the method gap between Arabic\nNER and NER methods from other languages, which helps outline future directions\nfor Arabic NER.", "published": "2023-02-07 14:56:52", "link": "http://arxiv.org/abs/2302.03512v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficiently Upgrading Multilingual Machine Translation Models to Support\n  More Languages", "abstract": "With multilingual machine translation (MMT) models continuing to grow in size\nand number of supported languages, it is natural to reuse and upgrade existing\nmodels to save computation as data becomes available in more languages.\nHowever, adding new languages requires updating the vocabulary, which\ncomplicates the reuse of embeddings. The question of how to reuse existing\nmodels while also making architectural changes to provide capacity for both old\nand new languages has also not been closely studied. In this work, we introduce\nthree techniques that help speed up effective learning of the new languages and\nalleviate catastrophic forgetting despite vocabulary and architecture\nmismatches. Our results show that by (1) carefully initializing the network,\n(2) applying learning rate scaling, and (3) performing data up-sampling, it is\npossible to exceed the performance of a same-sized baseline model with 30%\ncomputation and recover the performance of a larger model trained from scratch\nwith over 50% reduction in computation. Furthermore, our analysis reveals that\nthe introduced techniques help learn the new directions more effectively and\nalleviate catastrophic forgetting at the same time. We hope our work will guide\nresearch into more efficient approaches to growing languages for these MMT\nmodels and ultimately maximize the reuse of existing models.", "published": "2023-02-07 15:20:13", "link": "http://arxiv.org/abs/2302.03528v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CALaMo: a Constructionist Assessment of Language Models", "abstract": "This paper presents a novel framework for evaluating Neural Language Models'\nlinguistic abilities using a constructionist approach. Not only is the\nusage-based model in line with the underlying stochastic philosophy of neural\narchitectures, but it also allows the linguist to keep meaning as a determinant\nfactor in the analysis. We outline the framework and present two possible\nscenarios for its application.", "published": "2023-02-07 16:56:48", "link": "http://arxiv.org/abs/2302.03589v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploitation and exploration in text evolution. Quantifying planning and\n  translation flows during writing", "abstract": "Writing is a complex process at the center of much of modern human activity.\nDespite it appears to be a linear process, writing conceals many highly\nnon-linear processes. Previous research has focused on three phases of writing:\nplanning, translation and transcription, and revision. While research has shown\nthese are non-linear, they are often treated linearly when measured. Here, we\nintroduce measures to detect and quantify subcycles of planning (exploration)\nand translation (exploitation) during the writing process. We apply these to a\nnovel dataset that recorded the creation of a text in all its phases, from\nearly attempts to the finishing touches on a final version. This dataset comes\nfrom a series of writing workshops in which, through innovative versioning\nsoftware, we were able to record all the steps in the construction of a text.\nMore than 60 junior researchers in science wrote a scientific essay intended\nfor a general readership. We recorded each essay as a writing cloud, defined as\na complex topological structure capturing the history of the essay itself.\nThrough this unique dataset of writing clouds, we expose a representation of\nthe writing process that quantifies its complexity and the writer's efforts\nthroughout the draft and through time. Interestingly, this representation\nhighlights the phases of \"translation flow\", where authors improve existing\nideas, and exploration, where creative deviations appear as the writer returns\nto the planning phase. These turning points between translation and exploration\nbecome rarer as the writing process progresses and the author approaches the\nfinal version. Our results and the new measures introduced have the potential\nto foster the discussion about the non-linear nature of writing and support the\ndevelopment of tools that can support more creative and impactful writing\nprocesses.", "published": "2023-02-07 17:52:33", "link": "http://arxiv.org/abs/2302.03645v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mining Effective Features Using Quantum Entropy for Humor Recognition", "abstract": "Humor recognition has been extensively studied with different methods in the\npast years. However, existing studies on humor recognition do not understand\nthe mechanisms that generate humor. In this paper, inspired by the incongruity\ntheory, any joke can be divided into two components (the setup and the\npunchline). Both components have multiple possible semantics, and there is an\nincongruous relationship between them. We use density matrices to represent the\nsemantic uncertainty of the setup and the punchline, respectively, and design\nQE-Uncertainty and QE-Incongruity with the help of quantum entropy as features\nfor humor recognition. The experimental results on the SemEval2021 Task 7\ndataset show that the proposed features are more effective than the baselines\nfor recognizing humorous and non-humorous texts.", "published": "2023-02-07 19:09:09", "link": "http://arxiv.org/abs/2302.03716v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories", "abstract": "In this paper we improve the zero-shot generalization ability of language\nmodels via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves\naugmentation documents from multiple information corpora (\"external memories\"),\nwith the option to \"plug in\" new memory at inference time. We develop a joint\nlearning mechanism that trains the augmentation component with latent labels\nderived from the end retrieval task, paired with hard negatives from the memory\nmixture. We instantiate the model in a zero-shot dense retrieval setting by\naugmenting a strong T5-based retriever with MoMA. Our model, MoMA, obtains\nstrong zero-shot retrieval accuracy on the eighteen tasks included in the\nstandard BEIR benchmark. It outperforms systems that seek generalization from\nincreased model parameters and computation steps. Our analysis further\nillustrates the necessity of augmenting with mixture-of-memory for robust\ngeneralization, the benefits of augmentation learning, and how MoMA utilizes\nthe plug-in memory at inference time without changing its parameters. We plan\nto open source our code.", "published": "2023-02-07 20:59:31", "link": "http://arxiv.org/abs/2302.03754v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer-based Models for Long-Form Document Matching: Challenges and\n  Empirical Analysis", "abstract": "Recent advances in the area of long document matching have primarily focused\non using transformer-based models for long document encoding and matching.\nThere are two primary challenges associated with these models. Firstly, the\nperformance gain provided by transformer-based models comes at a steep cost -\nboth in terms of the required training time and the resource (memory and\nenergy) consumption. The second major limitation is their inability to handle\nmore than a pre-defined input token length at a time. In this work, we\nempirically demonstrate the effectiveness of simple neural models (such as\nfeed-forward networks, and CNNs) and simple embeddings (like GloVe, and\nParagraph Vector) over transformer-based models on the task of document\nmatching. We show that simple models outperform the more complex BERT-based\nmodels while taking significantly less training time, energy, and memory. The\nsimple models are also more robust to variations in document length and text\nperturbations.", "published": "2023-02-07 21:51:05", "link": "http://arxiv.org/abs/2302.03765v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Applying BERT and ChatGPT for Sentiment Analysis of Lyme Disease in\n  Scientific Literature", "abstract": "This chapter presents a practical guide for conducting Sentiment Analysis\nusing Natural Language Processing (NLP) techniques in the domain of tick-borne\ndisease text. The aim is to demonstrate the process of how the presence of bias\nin the discourse surrounding chronic manifestations of the disease can be\nevaluated. The goal is to use a dataset of 5643 abstracts collected from\nscientific journals on the topic of chronic Lyme disease to demonstrate using\nPython, the steps for conducting sentiment analysis using pre-trained language\nmodels and the process of validating the preliminary results using both\ninterpretable machine learning tools, as well as a novel methodology of using\nemerging state-of-the-art large language models like ChatGPT. This serves as a\nuseful resource for researchers and practitioners interested in using NLP\ntechniques for sentiment analysis in the medical domain.", "published": "2023-02-07 01:15:05", "link": "http://arxiv.org/abs/2302.06474v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Translation Quality Evaluation on Low Resource Languages from\n  Large Language Models", "abstract": "Learned metrics such as BLEURT have in recent years become widely employed to\nevaluate the quality of machine translation systems. Training such metrics\nrequires data which can be expensive and difficult to acquire, particularly for\nlower-resource languages. We show how knowledge can be distilled from Large\nLanguage Models (LLMs) to improve upon such learned metrics without requiring\nhuman annotators, by creating synthetic datasets which can be mixed into\nexisting datasets, requiring only a corpus of text in the target language. We\nshow that the performance of a BLEURT-like model on lower resource languages\ncan be improved in this way.", "published": "2023-02-07 14:35:35", "link": "http://arxiv.org/abs/2302.03491v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cluster-Level Contrastive Learning for Emotion Recognition in\n  Conversations", "abstract": "A key challenge for Emotion Recognition in Conversations (ERC) is to\ndistinguish semantically similar emotions. Some works utilise Supervised\nContrastive Learning (SCL) which uses categorical emotion labels as supervision\nsignals and contrasts in high-dimensional semantic space. However, categorical\nlabels fail to provide quantitative information between emotions. ERC is also\nnot equally dependent on all embedded features in the semantic space, which\nmakes the high-dimensional SCL inefficient. To address these issues, we propose\na novel low-dimensional Supervised Cluster-level Contrastive Learning (SCCL)\nmethod, which first reduces the high-dimensional SCL space to a\nthree-dimensional affect representation space Valence-Arousal-Dominance (VAD),\nthen performs cluster-level contrastive learning to incorporate measurable\nemotion prototypes. To help modelling the dialogue and enriching the context,\nwe leverage the pre-trained knowledge adapters to infuse linguistic and factual\nknowledge. Experiments show that our method achieves new state-of-the-art\nresults with 69.81% on IEMOCAP, 65.7% on MELD, and 62.51% on DailyDialog\ndatasets. The analysis also proves that the VAD space is not only suitable for\nERC but also interpretable, with VAD prototypes enhancing its performance and\nstabilising the training of SCCL. In addition, the pre-trained knowledge\nadapters benefit the performance of the utterance encoder and SCCL. Our code is\navailable at: https://github.com/SteveKGYang/SCCL", "published": "2023-02-07 14:49:20", "link": "http://arxiv.org/abs/2302.03508v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt\n  Tuning and Discovery", "abstract": "The strength of modern generative models lies in their ability to be\ncontrolled through text-based prompts. Typical \"hard\" prompts are made from\ninterpretable words and tokens, and must be hand-crafted by humans. There are\nalso \"soft\" prompts, which consist of continuous feature vectors. These can be\ndiscovered using powerful optimization methods, but they cannot be easily\ninterpreted, re-used across models, or plugged into a text-based interface.\n  We describe an approach to robustly optimize hard text prompts through\nefficient gradient-based optimization. Our approach automatically generates\nhard text-based prompts for both text-to-image and text-to-text applications.\nIn the text-to-image setting, the method creates hard prompts for diffusion\nmodels, allowing API users to easily generate, discover, and mix and match\nimage concepts without prior knowledge on how to prompt the model. In the\ntext-to-text setting, we show that hard prompts can be automatically discovered\nthat are effective in tuning LMs for classification.", "published": "2023-02-07 18:40:18", "link": "http://arxiv.org/abs/2302.03668v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "What Matters In The Structured Pruning of Generative Language Models?", "abstract": "Auto-regressive large language models such as GPT-3 require enormous\ncomputational resources to use. Traditionally, structured pruning methods are\nemployed to reduce resource usage. However, their application to and efficacy\nfor generative language models is heavily under-explored. In this paper we\nconduct an comprehensive evaluation of common structured pruning methods,\nincluding magnitude, random, and movement pruning on the feed-forward layers in\nGPT-type models. Unexpectedly, random pruning results in performance that is\ncomparable to the best established methods, across multiple natural language\ngeneration tasks. To understand these results, we provide a framework for\nmeasuring neuron-level redundancy of models pruned by different methods, and\ndiscover that established structured pruning methods do not take into account\nthe distinctiveness of neurons, leaving behind excess redundancies. In view of\nthis, we introduce Globally Unique Movement (GUM) to improve the uniqueness of\nneurons in pruned models. We then discuss the effects of our techniques on\ndifferent redundancy metrics to explain the improved performance.", "published": "2023-02-07 22:05:55", "link": "http://arxiv.org/abs/2302.03773v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reliable Natural Language Understanding with Large Language Models and\n  Answer Set Programming", "abstract": "Humans understand language by extracting information (meaning) from\nsentences, combining it with existing commonsense knowledge, and then\nperforming reasoning to draw conclusions. While large language models (LLMs)\nsuch as GPT-3 and ChatGPT are able to leverage patterns in the text to solve a\nvariety of NLP tasks, they fall short in problems that require reasoning. They\nalso cannot reliably explain the answers generated for a given question. In\norder to emulate humans better, we propose STAR, a framework that combines LLMs\nwith Answer Set Programming (ASP). We show how LLMs can be used to effectively\nextract knowledge -- represented as predicates -- from language. Goal-directed\nASP is then employed to reliably reason over this knowledge. We apply the STAR\nframework to three different NLU tasks requiring reasoning: qualitative\nreasoning, mathematical reasoning, and goal-directed conversation. Our\nexperiments reveal that STAR is able to bridge the gap of reasoning in NLU\ntasks, leading to significant performance improvements, especially for smaller\nLLMs, i.e., LLMs with a smaller number of parameters. NLU applications\ndeveloped using the STAR framework are also explainable: along with the\npredicates generated, a justification in the form of a proof tree can be\nproduced for a given output.", "published": "2023-02-07 22:37:21", "link": "http://arxiv.org/abs/2302.03780v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ZipLM: Inference-Aware Structured Pruning of Language Models", "abstract": "The breakthrough performance of large language models (LLMs) comes with major\ncomputational footprints and high deployment costs. In this paper, we progress\ntowards resolving this problem by proposing a novel structured compression\napproach for LLMs, called ZipLM. ZipLM achieves state-of-the-art\naccuracy-vs-speedup, while matching a set of desired target runtime speedups in\nany given inference environment. Specifically, given a model, a dataset, an\ninference environment, as well as a set of speedup targets, ZipLM iteratively\nidentifies and removes components with the worst loss-runtime trade-off. Unlike\nprior methods that specialize in either the post-training/one-shot or the\ngradual compression setting, and only for specific families of models such as\nBERT (encoder) or GPT (decoder), ZipLM produces state-of-the-art compressed\nmodels across all these settings. Furthermore, ZipLM achieves superior results\nfor a fraction of the computational cost relative to prior distillation and\npruning techniques, making it a cost-effective approach for generating an\nentire family of smaller, faster, and highly accurate models, guaranteed to\nmeet the desired inference specifications. In particular, ZipLM outperforms all\nprior BERT-base distillation and pruning techniques, such as CoFi, MiniLM, and\nTinyBERT. Moreover, it matches the performance of the heavily optimized\nMobileBERT model, obtained via extensive architecture search, by simply pruning\nthe baseline BERT-large model. When compressing GPT2, ZipLM outperforms\nDistilGPT2 while being 60% smaller and 30% faster. Our code is available at:\nhttps://github.com/IST-DASLab/ZipLM.", "published": "2023-02-07 18:55:28", "link": "http://arxiv.org/abs/2302.04089v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Real-Word Error Correction with Trigrams: Correcting Multiple Errors in\n  a Sentence", "abstract": "Spelling correction is a fundamental task in Text Mining. In this study, we\nassess the real-word error correction model proposed by Mays, Damerau and\nMercer and describe several drawbacks of the model. We propose a new variation\nwhich focuses on detecting and correcting multiple real-word errors in a\nsentence, by manipulating a Probabilistic Context-Free Grammar (PCFG) to\ndiscriminate between items in the search space. We test our approach on the\nWall Street Journal corpus and show that it outperforms Hirst and Budanitsky's\nWordNet-based method and Wilcox-O'Hearn, Hirst, and Budanitsky's fixed windows\nsize method.-O'Hearn, Hirst, and Budanitsky's fixed windows size method.", "published": "2023-02-07 13:52:14", "link": "http://arxiv.org/abs/2302.04096v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Continual Pre-training of Language Models", "abstract": "Language models (LMs) have been instrumental for the rapid advance of natural\nlanguage processing. This paper studies continual pre-training of LMs, in\nparticular, continual domain-adaptive pre-training (or continual DAP-training).\nExisting research has shown that further pre-training an LM using a domain\ncorpus to adapt the LM to the domain can improve the end-task performance in\nthe domain. This paper proposes a novel method to continually DAP-train an LM\nwith a sequence of unlabeled domain corpora to adapt the LM to these domains to\nimprove their end-task performances. The key novelty of our method is a\nsoft-masking mechanism that directly controls the update to the LM. A novel\nproxy is also proposed to preserve the general knowledge in the original LM.\nAdditionally, it contrasts the representations of the previously learned domain\nknowledge (including the general knowledge in the pre-trained LM) and the\nknowledge from the current full network to achieve knowledge integration. The\nmethod not only overcomes catastrophic forgetting, but also achieves knowledge\ntransfer to improve end-task performances. Empirical evaluation demonstrates\nthe effectiveness of the proposed method.", "published": "2023-02-07 03:57:55", "link": "http://arxiv.org/abs/2302.03241v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "PLACES: Prompting Language Models for Social Conversation Synthesis", "abstract": "Collecting high quality conversational data can be very expensive for most\napplications and infeasible for others due to privacy, ethical, or similar\nconcerns. A promising direction to tackle this problem is to generate synthetic\ndialogues by prompting large language models. In this work, we use a small set\nof expert-written conversations as in-context examples to synthesize a social\nconversation dataset using prompting. We perform several thorough evaluations\nof our synthetic conversations compared to human-collected conversations. This\nincludes various dimensions of conversation quality with human evaluation\ndirectly on the synthesized conversations, and interactive human evaluation of\nchatbots fine-tuned on the synthetically generated dataset. We additionally\ndemonstrate that this prompting approach is generalizable to multi-party\nconversations, providing potential to create new synthetic data for multi-party\ntasks. Our synthetic multi-party conversations were rated more favorably across\nall measured dimensions compared to conversation excerpts sampled from a\nhuman-collected multi-party dataset.", "published": "2023-02-07 05:48:16", "link": "http://arxiv.org/abs/2302.03269v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "The Effect of Metadata on Scientific Literature Tagging: A Cross-Field\n  Cross-Model Study", "abstract": "Due to the exponential growth of scientific publications on the Web, there is\na pressing need to tag each paper with fine-grained topics so that researchers\ncan track their interested fields of study rather than drowning in the whole\nliterature. Scientific literature tagging is beyond a pure multi-label text\nclassification task because papers on the Web are prevalently accompanied by\nmetadata information such as venues, authors, and references, which may serve\nas additional signals to infer relevant tags. Although there have been studies\nmaking use of metadata in academic paper classification, their focus is often\nrestricted to one or two scientific fields (e.g., computer science and\nbiomedicine) and to one specific model. In this work, we systematically study\nthe effect of metadata on scientific literature tagging across 19 fields. We\nselect three representative multi-label classifiers (i.e., a bag-of-words\nmodel, a sequence-based model, and a pre-trained language model) and explore\ntheir performance change in scientific literature tagging when metadata are fed\nto the classifiers as additional features. We observe some ubiquitous patterns\nof metadata's effects across all fields (e.g., venues are consistently\nbeneficial to paper tagging in almost all cases), as well as some unique\npatterns in fields other than computer science and biomedicine, which are not\nexplored in previous studies.", "published": "2023-02-07 09:34:41", "link": "http://arxiv.org/abs/2302.03341v1", "categories": ["cs.DL", "cs.CL", "cs.LG"], "primary_category": "cs.DL"}
{"title": "Natural Language Processing for Policymaking", "abstract": "Language is the medium for many political activities, from campaigns to news\nreports. Natural language processing (NLP) uses computational tools to parse\ntext into key information that is needed for policymaking. In this chapter, we\nintroduce common methods of NLP, including text classification, topic modeling,\nevent extraction, and text scaling. We then overview how these methods can be\nused for policymaking through four major applications including data collection\nfor evidence-based policymaking, interpretation of political decisions, policy\ncommunication, and investigation of policy effects. Finally, we highlight some\npotential limitations and ethical concerns when using NLP for policymaking.\n  This text is from Chapter 7 (pages 141-162) of the Handbook of Computational\nSocial Science for Policy (2023). Open Access on Springer:\nhttps://doi.org/10.1007/978-3-031-16624-2", "published": "2023-02-07 14:34:39", "link": "http://arxiv.org/abs/2302.03490v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Auditing Gender Presentation Differences in Text-to-Image Models", "abstract": "Text-to-image models, which can generate high-quality images based on textual\ninput, have recently enabled various content-creation tools. Despite\nsignificantly affecting a wide range of downstream applications, the\ndistributions of these generated images are still not fully understood,\nespecially when it comes to the potential stereotypical attributes of different\ngenders. In this work, we propose a paradigm (Gender Presentation Differences)\nthat utilizes fine-grained self-presentation attributes to study how gender is\npresented differently in text-to-image models. By probing gender indicators in\nthe input text (e.g., \"a woman\" or \"a man\"), we quantify the frequency\ndifferences of presentation-centric attributes (e.g., \"a shirt\" and \"a dress\")\nthrough human annotation and introduce a novel metric: GEP. Furthermore, we\npropose an automatic method to estimate such differences. The automatic GEP\nmetric based on our approach yields a higher correlation with human annotations\nthan that based on existing CLIP scores, consistently across three\nstate-of-the-art text-to-image models. Finally, we demonstrate the\ngeneralization ability of our metrics in the context of gender stereotypes\nrelated to occupations.", "published": "2023-02-07 18:52:22", "link": "http://arxiv.org/abs/2302.03675v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.CV"}
{"title": "Concept Algebra for (Score-Based) Text-Controlled Generative Models", "abstract": "This paper concerns the structure of learned representations in text-guided\ngenerative models, focusing on score-based models. A key property of such\nmodels is that they can compose disparate concepts in a `disentangled' manner.\nThis suggests these models have internal representations that encode concepts\nin a `disentangled' manner. Here, we focus on the idea that concepts are\nencoded as subspaces of some representation space. We formalize what this\nmeans, show there's a natural choice for the representation, and develop a\nsimple method for identifying the part of the representation corresponding to a\ngiven concept. In particular, this allows us to manipulate the concepts\nexpressed by the model through algebraic manipulation of the representation. We\ndemonstrate the idea with examples using Stable Diffusion. Code in\nhttps://github.com/zihao12/concept-algebra-code", "published": "2023-02-07 20:43:48", "link": "http://arxiv.org/abs/2302.03693v6", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Characterizing Financial Market Coverage using Artificial Intelligence", "abstract": "This paper scrutinizes a database of over 4900 YouTube videos to characterize\nfinancial market coverage. Financial market coverage generates a large number\nof videos. Therefore, watching these videos to derive actionable insights could\nbe challenging and complex. In this paper, we leverage Whisper, a\nspeech-to-text model from OpenAI, to generate a text corpus of market coverage\nvideos from Bloomberg and Yahoo Finance. We employ natural language processing\nto extract insights regarding language use from the market coverage. Moreover,\nwe examine the prominent presence of trending topics and their evolution over\ntime, and the impacts that some individuals and organizations have on the\nfinancial market. Our characterization highlights the dynamics of the financial\nmarket coverage and provides valuable insights reflecting broad discussions\nregarding recent financial events and the world economy.", "published": "2023-02-07 16:03:33", "link": "http://arxiv.org/abs/2302.03694v1", "categories": ["q-fin.ST", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal\n  Supervision", "abstract": "We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can\nbe trained with minimal supervision. By combining two types of discrete speech\nrepresentations, we cast TTS as a composition of two sequence-to-sequence\ntasks: from text to high-level semantic tokens (akin to \"reading\") and from\nsemantic tokens to low-level acoustic tokens (\"speaking\"). Decoupling these two\ntasks enables training of the \"speaking\" module using abundant audio-only data,\nand unlocks the highly efficient combination of pretraining and backtranslation\nto reduce the need for parallel data when training the \"reading\" component. To\ncontrol the speaker identity, we adopt example prompting, which allows\nSPEAR-TTS to generalize to unseen speakers using only a short sample of 3\nseconds, without any explicit speaker representation or speaker-id labels. Our\nexperiments demonstrate that SPEAR-TTS achieves a character error rate that is\ncompetitive with state-of-the-art methods using only 15 minutes of parallel\ndata, while matching ground-truth speech in terms of naturalness and acoustic\nquality, as measured in subjective tests.", "published": "2023-02-07 15:48:31", "link": "http://arxiv.org/abs/2302.03540v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Revisiting Pre-training in Audio-Visual Learning", "abstract": "Pre-training technique has gained tremendous success in enhancing model\nperformance on various tasks, but found to perform worse than training from\nscratch in some uni-modal situations. This inspires us to think: are the\npre-trained models always effective in the more complex multi-modal scenario,\nespecially for the heterogeneous modalities such as audio and visual ones? We\nfind that the answer is No. Specifically, we explore the effects of pre-trained\nmodels on two audio-visual learning scenarios: cross-modal initialization and\nmulti-modal joint learning. When cross-modal initialization is applied, the\nphenomena of \"dead channel\" caused by abnormal Batchnorm parameters hinders the\nutilization of model capacity. Thus, we propose Adaptive Batchnorm\nRe-initialization (ABRi) to better exploit the capacity of pre-trained models\nfor target tasks. In multi-modal joint learning, we find a strong pre-trained\nuni-modal encoder would bring negative effects on the encoder of another\nmodality. To alleviate such problem, we introduce a two-stage Fusion Tuning\nstrategy, taking better advantage of the pre-trained knowledge while making the\nuni-modal encoders cooperate with an adaptive masking method. The experiment\nresults show that our methods could further exploit pre-trained models'\npotential and boost performance in audio-visual learning.", "published": "2023-02-07 15:34:14", "link": "http://arxiv.org/abs/2302.03533v2", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
