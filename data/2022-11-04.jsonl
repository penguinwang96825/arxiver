{"title": "Late Fusion with Triplet Margin Objective for Multimodal Ideology\n  Prediction and Analysis", "abstract": "Prior work on ideology prediction has largely focused on single modalities,\ni.e., text or images. In this work, we introduce the task of multimodal\nideology prediction, where a model predicts binary or five-point scale\nideological leanings, given a text-image pair with political content. We first\ncollect five new large-scale datasets with English documents and images along\nwith their ideological leanings, covering news articles from a wide range of US\nmainstream media and social media posts from Reddit and Twitter. We conduct\nin-depth analyses of news articles and reveal differences in image content and\nusage across the political spectrum. Furthermore, we perform extensive\nexperiments and ablation studies, demonstrating the effectiveness of targeted\npretraining objectives on different model components. Our best-performing\nmodel, a late-fusion architecture pretrained with a triplet objective over\nmultimodal content, outperforms the state-of-the-art text-only model by almost\n4% and a strong multimodal baseline with no pretraining by over 3%.", "published": "2022-11-04 05:45:26", "link": "http://arxiv.org/abs/2211.02269v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MultiWOZ-DF -- A Dataflow implementation of the MultiWOZ dataset", "abstract": "Semantic Machines (SM) have introduced the use of the dataflow (DF) paradigm\nto dialogue modelling, using computational graphs to hierarchically represent\nuser requests, data, and the dialogue history [Semantic Machines et al. 2020].\nAlthough the main focus of that paper was the SMCalFlow dataset (to date, the\nonly dataset with \"native\" DF annotations), they also reported some results of\nan experiment using a transformed version of the commonly used MultiWOZ dataset\n[Budzianowski et al. 2018] into a DF format. In this paper, we expand the\nexperiments using DF for the MultiWOZ dataset, exploring some additional\nexperimental set-ups. The code and instructions to reproduce the experiments\nreported here have been released. The contributions of this paper are: 1.) A DF\nimplementation capable of executing MultiWOZ dialogues; 2.) Several versions of\nconversion of MultiWOZ into a DF format are presented; 3.) Experimental results\non state match and translation accuracy.", "published": "2022-11-04 08:09:33", "link": "http://arxiv.org/abs/2211.02303v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CLSE: Corpus of Linguistically Significant Entities", "abstract": "One of the biggest challenges of natural language generation (NLG) is the\nproper handling of named entities. Named entities are a common source of\ngrammar mistakes such as wrong prepositions, wrong article handling, or\nincorrect entity inflection. Without factoring linguistic representation, such\nerrors are often underrepresented when evaluating on a small set of arbitrarily\npicked argument values, or when translating a dataset from a linguistically\nsimpler language, like English, to a linguistically complex language, like\nRussian. However, for some applications, broadly precise grammatical\ncorrectness is critical -- native speakers may find entity-related grammar\nerrors silly, jarring, or even offensive.\n  To enable the creation of more linguistically diverse NLG datasets, we\nrelease a Corpus of Linguistically Significant Entities (CLSE) annotated by\nlinguist experts. The corpus includes 34 languages and covers 74 different\nsemantic types to support various applications from airline ticketing to video\ngames. To demonstrate one possible use of CLSE, we produce an augmented version\nof the Schema-Guided Dialog Dataset, SGD-CLSE. Using the CLSE's entities and a\nsmall number of human translations, we create a linguistically representative\nNLG evaluation benchmark in three languages: French (high-resource), Marathi\n(low-resource), and Russian (highly inflected language). We establish quality\nbaselines for neural, template-based, and hybrid NLG systems and discuss the\nstrengths and weaknesses of each approach.", "published": "2022-11-04 12:56:12", "link": "http://arxiv.org/abs/2211.02423v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dealing with Abbreviations in the Slovenian Biographical Lexicon", "abstract": "Abbreviations present a significant challenge for NLP systems because they\ncause tokenization and out-of-vocabulary errors. They can also make the text\nless readable, especially in reference printed books, where they are\nextensively used. Abbreviations are especially problematic in low-resource\nsettings, where systems are less robust to begin with. In this paper, we\npropose a new method for addressing the problems caused by a high density of\ndomain-specific abbreviations in a text. We apply this method to the case of a\nSlovenian biographical lexicon and evaluate it on a newly developed\ngold-standard dataset of 51 Slovenian biographies. Our abbreviation\nidentification method performs significantly better than commonly used ad-hoc\nsolutions, especially at identifying unseen abbreviations. We also propose and\npresent the results of a method for expanding the identified abbreviations in\ncontext.", "published": "2022-11-04 13:09:02", "link": "http://arxiv.org/abs/2211.02429v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Continuous Prompt Tuning Based Textual Entailment Model for E-commerce\n  Entity Typing", "abstract": "The explosion of e-commerce has caused the need for processing and analysis\nof product titles, like entity typing in product titles. However, the rapid\nactivity in e-commerce has led to the rapid emergence of new entities, which is\ndifficult to be solved by general entity typing. Besides, product titles in\ne-commerce have very different language styles from text data in general\ndomain. In order to handle new entities in product titles and address the\nspecial language styles problem of product titles in e-commerce domain, we\npropose our textual entailment model with continuous prompt tuning based\nhypotheses and fusion embeddings for e-commerce entity typing. First, we\nreformulate the entity typing task into a textual entailment problem to handle\nnew entities that are not present during training. Second, we design a model to\nautomatically generate textual entailment hypotheses using a continuous prompt\ntuning method, which can generate better textual entailment hypotheses without\nmanual design. Third, we utilize the fusion embeddings of BERT embedding and\nCharacterBERT embedding with a two-layer MLP classifier to solve the problem\nthat the language styles of product titles in e-commerce are different from\nthat of general domain. To analyze the effect of each contribution, we compare\nthe performance of entity typing and textual entailment model, and conduct\nablation studies on continuous prompt tuning and fusion embeddings. We also\nevaluate the impact of different prompt template initialization for the\ncontinuous prompt tuning. We show our proposed model improves the average F1\nscore by around 2% compared to the baseline BERT entity typing model.", "published": "2022-11-04 14:20:40", "link": "http://arxiv.org/abs/2211.02483v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "1Cademy @ Causal News Corpus 2022: Leveraging Self-Training in Causality\n  Classification of Socio-Political Event Data", "abstract": "This paper details our participation in the Challenges and Applications of\nAutomated Extraction of Socio-political Events from Text (CASE) workshop @\nEMNLP 2022, where we take part in Subtask 1 of Shared Task 3. We approach the\ngiven task of event causality detection by proposing a self-training pipeline\nthat follows a teacher-student classifier method. More specifically, we\ninitially train a teacher model on the true, original task data, and use that\nteacher model to self-label data to be used in the training of a separate\nstudent model for the final task prediction. We test how restricting the number\nof positive or negative self-labeled examples in the self-training process\naffects classification performance. Our final results show that using\nself-training produces a comprehensive performance improvement across all\nmodels and self-labeled training sets tested within the task of event causality\nsequence classification. On top of that, we find that self-training performance\ndid not diminish even when restricting either positive/negative examples used\nin training. Our code is be publicly available at\nhttps://github.com/Gzhang-umich/1CademyTeamOfCASE.", "published": "2022-11-04 20:02:48", "link": "http://arxiv.org/abs/2211.02729v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Miko Team: Deep Learning Approach for Legal Question Answering in ALQAC\n  2022", "abstract": "We introduce efficient deep learning-based methods for legal document\nprocessing including Legal Document Retrieval and Legal Question Answering\ntasks in the Automated Legal Question Answering Competition (ALQAC 2022). In\nthis competition, we achieve 1\\textsuperscript{st} place in the first task and\n3\\textsuperscript{rd} place in the second task. Our method is based on the\nXLM-RoBERTa model that is pre-trained from a large amount of unlabeled corpus\nbefore fine-tuning to the specific tasks. The experimental results showed that\nour method works well in legal retrieval information tasks with limited labeled\ndata. Besides, this method can be applied to other information retrieval tasks\nin low-resource languages.", "published": "2022-11-04 00:50:20", "link": "http://arxiv.org/abs/2211.02200v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SMAuC -- The Scientific Multi-Authorship Corpus", "abstract": "The rapidly growing volume of scientific publications offers an interesting\nchallenge for research on methods for analyzing the authorship of documents\nwith one or more authors. However, most existing datasets lack scientific\ndocuments or the necessary metadata for constructing new experiments and test\ncases. We introduce SMAuC, a comprehensive, metadata-rich corpus tailored to\nscientific authorship analysis. Comprising over 3 million publications across\nvarious disciplines from over 5 million authors, SMAuC is the largest openly\naccessible corpus for this purpose. It encompasses scientific texts from\nhumanities and natural sciences, accompanied by extensive, curated metadata,\nincluding unambiguous author IDs. SMAuC aims to significantly advance the\ndomain of authorship analysis in scientific texts.", "published": "2022-11-04 14:07:17", "link": "http://arxiv.org/abs/2211.02477v2", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "BERT for Long Documents: A Case Study of Automated ICD Coding", "abstract": "Transformer models have achieved great success across many NLP problems.\nHowever, previous studies in automated ICD coding concluded that these models\nfail to outperform some of the earlier solutions such as CNN-based models. In\nthis paper we challenge this conclusion. We present a simple and scalable\nmethod to process long text with the existing transformer models such as BERT.\nWe show that this method significantly improves the previous results reported\nfor transformer models in ICD coding, and is able to outperform one of the\nprominent CNN-based methods.", "published": "2022-11-04 15:24:19", "link": "http://arxiv.org/abs/2211.02519v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generation of Chinese classical poetry based on pre-trained model", "abstract": "In order to test whether artificial intelligence can create qualified\nclassical poetry like humans, the author proposes a study of Chinese classical\npoetry generation based on a pre-trained model. This paper mainly tries to use\nBART and other pre training models, proposes FS2TEXT and RR2TEXT to generate\nmetrical poetry text and even specific style poetry text, and solves the\nproblem that the user's writing intention gradually reduces the relevance of\nthe generated poetry text.\n  In order to test the model's results, the authors selected ancient poets, by\ncombining it with BART's poetic model work, developed a set of AI poetry Turing\nproblems, it was reviewed by a group of poets and poetry writing researchers.\nThere were more than 600 participants, and the final results showed that,\nhigh-level poetry lovers can't distinguish between AI activity and human\nactivity, this indicates that the author's working methods are not\nsignificantly different from human activities. The model of poetry generation\nstudied by the author generalizes works that cannot be distinguished from those\nof advanced scholars.\n  The number of modern Chinese poets has reached 5 million. However, many\nmodern Chinese poets lack language ability and skills as a result of their\nchildhood learning. However, many modern poets have no creative inspiration,\nand the author's model can help them. They can look at this model when they\nchoose words and phrases and they can write works based on the poems they\nalready have, and they can write their own poems. The importance of poetry lies\nin the author's thoughts and reflections. It doesn't matter how good AI poetry\nis. The only thing that matters is for people to see and inspire them.", "published": "2022-11-04 16:05:31", "link": "http://arxiv.org/abs/2211.02541v1", "categories": ["cs.CL", "cs.AI", "J.5; I.2.7"], "primary_category": "cs.CL"}
{"title": "A Comparison of SVM against Pre-trained Language Models (PLMs) for Text\n  Classification Tasks", "abstract": "The emergence of pre-trained language models (PLMs) has shown great success\nin many Natural Language Processing (NLP) tasks including text classification.\nDue to the minimal to no feature engineering required when using these models,\nPLMs are becoming the de facto choice for any NLP task. However, for\ndomain-specific corpora (e.g., financial, legal, and industrial), fine-tuning a\npre-trained model for a specific task has shown to provide a performance\nimprovement. In this paper, we compare the performance of four different PLMs\non three public domain-free datasets and a real-world dataset containing\ndomain-specific words, against a simple SVM linear classifier with TFIDF\nvectorized text. The experimental results on the four datasets show that using\nPLMs, even fine-tuned, do not provide significant gain over the linear SVM\nclassifier. Hence, we recommend that for text classification tasks, traditional\nSVM along with careful feature engineering can pro-vide a cheaper and superior\nperformance than PLMs.", "published": "2022-11-04 16:28:40", "link": "http://arxiv.org/abs/2211.02563v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The 'Problem' of Human Label Variation: On Ground Truth in Data,\n  Modeling and Evaluation", "abstract": "Human variation in labeling is often considered noise. Annotation projects\nfor machine learning (ML) aim at minimizing human label variation, with the\nassumption to maximize data quality and in turn optimize and maximize machine\nlearning metrics. However, this conventional practice assumes that there exists\na ground truth, and neglects that there exists genuine human variation in\nlabeling due to disagreement, subjectivity in annotation or multiple plausible\nanswers. In this position paper, we argue that this big open problem of human\nlabel variation persists and critically needs more attention to move our field\nforward. This is because human label variation impacts all stages of the ML\npipeline: data, modeling and evaluation. However, few works consider all of\nthese dimensions jointly; and existing research is fragmented. We reconcile\ndifferent previously proposed notions of human label variation, provide a\nrepository of publicly-available datasets with un-aggregated labels, depict\napproaches proposed so far, identify gaps and suggest ways forward. As datasets\nare becoming increasingly available, we hope that this synthesized view on the\n'problem' will lead to an open discussion on possible strategies to devise\nfundamentally new directions.", "published": "2022-11-04 16:38:09", "link": "http://arxiv.org/abs/2211.02570v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Transformer Architecture for Online Gesture Recognition of\n  Mathematical Expressions", "abstract": "The Transformer architecture is shown to provide a powerful framework as an\nend-to-end model for building expression trees from online handwritten gestures\ncorresponding to glyph strokes. In particular, the attention mechanism was\nsuccessfully used to encode, learn and enforce the underlying syntax of\nexpressions creating latent representations that are correctly decoded to the\nexact mathematical expression tree, providing robustness to ablated inputs and\nunseen glyphs. For the first time, the encoder is fed with spatio-temporal data\ntokens potentially forming an infinitely large vocabulary, which finds\napplications beyond that of online gesture recognition. A new supervised\ndataset of online handwriting gestures is provided for training models on\ngeneric handwriting recognition tasks and a new metric is proposed for the\nevaluation of the syntactic correctness of the output expression trees. A small\nTransformer model suitable for edge inference was successfully trained to an\naverage normalised Levenshtein accuracy of 94%, resulting in valid postfix RPN\ntree representation for 94% of predictions.", "published": "2022-11-04 17:55:55", "link": "http://arxiv.org/abs/2211.02643v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Intriguing Properties of Compression on Multilingual Models", "abstract": "Multilingual models are often particularly dependent on scaling to generalize\nto a growing number of languages. Compression techniques are widely relied upon\nto reconcile the growth in model size with real world resource constraints, but\ncompression can have a disparate effect on model performance for low-resource\nlanguages. It is thus crucial to understand the trade-offs between scale,\nmultilingualism, and compression. In this work, we propose an experimental\nframework to characterize the impact of sparsifying multilingual pre-trained\nlanguage models during fine-tuning. Applying this framework to mBERT named\nentity recognition models across 40 languages, we find that compression confers\nseveral intriguing and previously unknown generalization properties. In\ncontrast to prior findings, we find that compression may improve model\nrobustness over dense models. We additionally observe that under certain\nsparsification regimes compression may aid, rather than disproportionately\nimpact the performance of low-resource languages.", "published": "2022-11-04 20:28:01", "link": "http://arxiv.org/abs/2211.02738v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KGLM: Integrating Knowledge Graph Structure in Language Models for Link\n  Prediction", "abstract": "The ability of knowledge graphs to represent complex relationships at scale\nhas led to their adoption for various needs including knowledge representation,\nquestion-answering, and recommendation systems. Knowledge graphs are often\nincomplete in the information they represent, necessitating the need for\nknowledge graph completion tasks. Pre-trained and fine-tuned language models\nhave shown promise in these tasks although these models ignore the intrinsic\ninformation encoded in the knowledge graph, namely the entity and relation\ntypes. In this work, we propose the Knowledge Graph Language Model (KGLM)\narchitecture, where we introduce a new entity/relation embedding layer that\nlearns to differentiate distinctive entity and relation types, therefore\nallowing the model to learn the structure of the knowledge graph. In this work,\nwe show that further pre-training the language models with this additional\nembedding layer using the triples extracted from the knowledge graph, followed\nby the standard fine-tuning phase sets a new state-of-the-art performance for\nthe link prediction task on the benchmark datasets.", "published": "2022-11-04 20:38:12", "link": "http://arxiv.org/abs/2211.02744v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Federated Multilingual Models for Medical Transcript Analysis", "abstract": "Federated Learning (FL) is a novel machine learning approach that allows the\nmodel trainer to access more data samples, by training the model across\nmultiple decentralized data sources, while data access constraints are in\nplace. Such trained models can achieve significantly higher performance beyond\nwhat can be done when trained on a single data source. As part of FL's\npromises, none of the training data is ever transmitted to any central\nlocation, ensuring that sensitive data remains local and private. These\ncharacteristics make FL perfectly suited for large-scale applications in\nhealthcare, where a variety of compliance constraints restrict how data may be\nhandled, processed, and stored. Despite the apparent benefits of federated\nlearning, the heterogeneity in the local data distributions pose significant\nchallenges, and such challenges are even more pronounced in the case of\nmultilingual data providers. In this paper we present a federated learning\nsystem for training a large-scale multi-lingual model suitable for fine-tuning\non downstream tasks such as medical entity tagging. Our work represents one of\nthe first such production-scale systems, capable of training across multiple\nhighly heterogeneous data providers, and achieving levels of accuracy that\ncould not be otherwise achieved by using central training with public data.\nFinally, we show that the global model performance can be further improved by a\ntraining step performed locally.", "published": "2022-11-04 01:07:54", "link": "http://arxiv.org/abs/2211.09722v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Extending Logic Explained Networks to Text Classification", "abstract": "Recently, Logic Explained Networks (LENs) have been proposed as\nexplainable-by-design neural models providing logic explanations for their\npredictions. However, these models have only been applied to vision and tabular\ndata, and they mostly favour the generation of global explanations, while local\nones tend to be noisy and verbose. For these reasons, we propose LENp,\nimproving local explanations by perturbing input words, and we test it on text\nclassification. Our results show that (i) LENp provides better local\nexplanations than LIME in terms of sensitivity and faithfulness, and (ii) logic\nexplanations are more useful and user-friendly than feature scoring provided by\nLIME as attested by a human survey.", "published": "2022-11-04 16:12:03", "link": "http://arxiv.org/abs/2211.09732v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BERT-Deep CNN: State-of-the-Art for Sentiment Analysis of COVID-19\n  Tweets", "abstract": "The free flow of information has been accelerated by the rapid development of\nsocial media technology. There has been a significant social and psychological\nimpact on the population due to the outbreak of Coronavirus disease (COVID-19).\nThe COVID-19 pandemic is one of the current events being discussed on social\nmedia platforms. In order to safeguard societies from this pandemic, studying\npeople's emotions on social media is crucial. As a result of their particular\ncharacteristics, sentiment analysis of texts like tweets remains challenging.\nSentiment analysis is a powerful text analysis tool. It automatically detects\nand analyzes opinions and emotions from unstructured data. Texts from a wide\nrange of sources are examined by a sentiment analysis tool, which extracts\nmeaning from them, including emails, surveys, reviews, social media posts, and\nweb articles. To evaluate sentiments, natural language processing (NLP) and\nmachine learning techniques are used, which assign weights to entities, topics,\nthemes, and categories in sentences or phrases. Machine learning tools learn\nhow to detect sentiment without human intervention by examining examples of\nemotions in text. In a pandemic situation, analyzing social media texts to\nuncover sentimental trends can be very helpful in gaining a better\nunderstanding of society's needs and predicting future trends. We intend to\nstudy society's perception of the COVID-19 pandemic through social media using\nstate-of-the-art BERT and Deep CNN models. The superiority of BERT models over\nother deep models in sentiment analysis is evident and can be concluded from\nthe comparison of the various research studies mentioned in this article.", "published": "2022-11-04 14:35:56", "link": "http://arxiv.org/abs/2211.09733v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Spectral Regularization: an Inductive Bias for Sequence Modeling", "abstract": "Various forms of regularization in learning tasks strive for different\nnotions of simplicity. This paper presents a spectral regularization technique,\nwhich attaches a unique inductive bias to sequence modeling based on an\nintuitive concept of simplicity defined in the Chomsky hierarchy. From\nfundamental connections between Hankel matrices and regular grammars, we\npropose to use the trace norm of the Hankel matrix, the tightest convex\nrelaxation of its rank, as the spectral regularizer. To cope with the fact that\nthe Hankel matrix is bi-infinite, we propose an unbiased stochastic estimator\nfor its trace norm. Ultimately, we demonstrate experimental results on Tomita\ngrammars, which exhibit the potential benefits of spectral regularization and\nvalidate the proposed stochastic estimator.", "published": "2022-11-04 04:07:05", "link": "http://arxiv.org/abs/2211.02255v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Once-for-All Sequence Compression for Self-Supervised Speech Models", "abstract": "The sequence length along the time axis is often the dominant factor of the\ncomputation in speech processing. Works have been proposed to reduce the\nsequence length for lowering the computational cost in self-supervised speech\nmodels. However, different downstream tasks have different tolerance of\nsequence compressing, so a model that produces a fixed compressing rate may not\nfit all tasks. In this work, we introduce a once-for-all (OFA) sequence\ncompression framework for self-supervised speech models that supports a\ncontinuous range of operating compressing rates. The framework is evaluated on\nvarious tasks, showing marginal degradation compared to the fixed compressing\nrate variants with a smooth performance-efficiency trade-off. We further\nexplore adaptive compressing rate learning, demonstrating the ability to select\ntask-specific preferred frame periods without needing a grid search.", "published": "2022-11-04 09:19:13", "link": "http://arxiv.org/abs/2211.02332v4", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Minimum Latency Training of Sequence Transducers for Streaming\n  End-to-End Speech Recognition", "abstract": "Sequence transducers, such as the RNN-T and the Conformer-T, are one of the\nmost promising models of end-to-end speech recognition, especially in streaming\nscenarios where both latency and accuracy are important. Although various\nmethods, such as alignment-restricted training and FastEmit, have been studied\nto reduce the latency, latency reduction is often accompanied with a\nsignificant degradation in accuracy. We argue that this suboptimal performance\nmight be caused because none of the prior methods explicitly model and reduce\nthe latency. In this paper, we propose a new training method to explicitly\nmodel and reduce the latency of sequence transducer models. First, we define\nthe expected latency at each diagonal line on the lattice, and show that its\ngradient can be computed efficiently within the forward-backward algorithm.\nThen we augment the transducer loss with this expected latency, so that an\noptimal trade-off between latency and accuracy is achieved. Experimental\nresults on the WSJ dataset show that the proposed minimum latency training\nreduces the latency of causal Conformer-T from 220 ms to 27 ms within a WER\ndegradation of 0.7%, and outperforms conventional alignment-restricted training\n(110 ms) and FastEmit (67 ms) methods.", "published": "2022-11-04 09:19:59", "link": "http://arxiv.org/abs/2211.02333v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multilingual Name Entity Recognition and Intent Classification Employing\n  Deep Learning Architectures", "abstract": "Named Entity Recognition and Intent Classification are among the most\nimportant subfields of the field of Natural Language Processing. Recent\nresearch has lead to the development of faster, more sophisticated and\nefficient models to tackle the problems posed by those two tasks. In this work\nwe explore the effectiveness of two separate families of Deep Learning networks\nfor those tasks: Bidirectional Long Short-Term networks and Transformer-based\nnetworks. The models were trained and tested on the ATIS benchmark dataset for\nboth English and Greek languages. The purpose of this paper is to present a\ncomparative study of the two groups of networks for both languages and showcase\nthe results of our experiments. The models, being the current state-of-the-art,\nyielded impressive results and achieved high performance.", "published": "2022-11-04 12:42:29", "link": "http://arxiv.org/abs/2211.02415v1", "categories": ["cs.CL", "cs.LG", "cs.MA", "I.2.7; I.2.1; I.2.11"], "primary_category": "cs.CL"}
{"title": "A Weakly-Supervised Streaming Multilingual Speech Model with Truly\n  Zero-Shot Capability", "abstract": "In this paper, we introduce our work of building a Streaming Multilingual\nSpeech Model (SM2), which can transcribe or translate multiple spoken languages\ninto texts of the target language. The backbone of SM2 is Transformer\nTransducer, which has high streaming capability. Instead of human labeled\nspeech translation (ST) data, SM2 models are trained using weakly supervised\ndata generated by converting the transcriptions in speech recognition corpora\nwith a machine translation service. With 351 thousand hours of anonymized\nspeech training data from 25 languages, SM2 models achieve comparable or even\nbetter ST quality than some recent popular large-scale non-streaming speech\nmodels. More importantly, we show that SM2 has the truly zero-shot capability\nwhen expanding to new target languages, yielding high quality ST results for\n{source-speech, target-text} pairs that are not seen during training.", "published": "2022-11-04 14:59:55", "link": "http://arxiv.org/abs/2211.02499v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Transformer-Based Substitute Recommendation Model Incorporating Weakly\n  Supervised Customer Behavior Data", "abstract": "The substitute-based recommendation is widely used in E-commerce to provide\nbetter alternatives to customers. However, existing research typically uses the\ncustomer behavior signals like co-view and view-but-purchase-another to capture\nthe substitute relationship. Despite its intuitive soundness, we find that such\nan approach might ignore the functionality and characteristics of products. In\nthis paper, we adapt substitute recommendation into language matching problem\nby taking product title description as model input to consider product\nfunctionality. We design a new transformation method to de-noise the signals\nderived from production data. In addition, we consider multilingual support\nfrom the engineering point of view. Our proposed end-to-end transformer-based\nmodel achieves both successes from offline and online experiments. The proposed\nmodel has been deployed in a large-scale E-commerce website for 11 marketplaces\nin 6 languages. Our proposed model is demonstrated to increase revenue by 19%\nbased on an online A/B experiment.", "published": "2022-11-04 15:57:19", "link": "http://arxiv.org/abs/2211.02533v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Biased Self-supervised learning for ASR", "abstract": "Self-supervised learning via masked prediction pre-training (MPPT) has shown\nimpressive performance on a range of speech-processing tasks. This paper\nproposes a method to bias self-supervised learning towards a specific task. The\ncore idea is to slightly finetune the model that is used to obtain the target\nsequence. This leads to better performance and a substantial increase in\ntraining speed. Furthermore, this paper proposes a variant of MPPT that allows\nlow-footprint streaming models to be trained effectively by computing the MPPT\nloss on masked and unmasked frames. These approaches are evaluated for\nautomatic speech recognition on the Librispeech corpus, where 100 hours of data\nserved as the labelled data and 860 hours as the unlabelled data. The biased\ntraining outperforms the unbiased training by 15.5% after 250k updates and\n23.8% after 100k updates on test-other. For the streaming models, the\npre-training approach yields a reduction in word error rate of 44.1%.", "published": "2022-11-04 15:57:59", "link": "http://arxiv.org/abs/2211.02536v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Evaluating and Improving Factuality in Multimodal Abstractive\n  Summarization", "abstract": "Current metrics for evaluating factuality for abstractive document\nsummarization have achieved high correlations with human judgment, but they do\nnot account for the vision modality and thus are not adequate for\nvision-and-language summarization. We propose CLIPBERTScore, a simple weighted\ncombination of CLIPScore and BERTScore to leverage the robustness and strong\nfactuality detection performance between image-summary and document-summary,\nrespectively. Next, due to the lack of meta-evaluation benchmarks to evaluate\nthe quality of multimodal factuality metrics, we collect human judgments of\nfactuality with respect to documents and images. We show that this simple\ncombination of two metrics in the zero-shot setting achieves higher\ncorrelations than existing factuality metrics for document summarization,\noutperforms an existing multimodal summarization metric, and performs\ncompetitively with strong multimodal factuality metrics specifically fine-tuned\nfor the task. Our thorough analysis demonstrates the robustness and high\ncorrelation of CLIPBERTScore and its components on four factuality\nmetric-evaluation benchmarks. Finally, we demonstrate two practical downstream\napplications of our CLIPBERTScore metric: for selecting important images to\nfocus on during training, and as a reward for reinforcement learning to improve\nfactuality of multimodal summary generation w.r.t automatic and human\nevaluation. Our data and code are publicly available at\nhttps://github.com/meetdavidwan/faithful-multimodal-summ", "published": "2022-11-04 16:50:40", "link": "http://arxiv.org/abs/2211.02580v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Measuring Progress on Scalable Oversight for Large Language Models", "abstract": "Developing safe and useful general-purpose AI systems will require us to make\nprogress on scalable oversight: the problem of supervising systems that\npotentially outperform us on most skills relevant to the task at hand.\nEmpirical work on this problem is not straightforward, since we do not yet have\nsystems that broadly exceed our abilities. This paper discusses one of the\nmajor ways we think about this problem, with a focus on ways it can be studied\nempirically. We first present an experimental design centered on tasks for\nwhich human specialists succeed but unaided humans and current general AI\nsystems fail. We then present a proof-of-concept experiment meant to\ndemonstrate a key feature of this experimental design and show its viability\nwith two question-answering tasks: MMLU and time-limited QuALITY. On these\ntasks, we find that human participants who interact with an unreliable\nlarge-language-model dialog assistant through chat -- a trivial baseline\nstrategy for scalable oversight -- substantially outperform both the model\nalone and their own unaided performance. These results are an encouraging sign\nthat scalable oversight will be tractable to study with present models and\nbolster recent findings that large language models can productively assist\nhumans with difficult tasks.", "published": "2022-11-04 17:03:49", "link": "http://arxiv.org/abs/2211.03540v2", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "An approach to standardize, automate omni-channel and AI transactional\n  digital service creation", "abstract": "Our work is at the crossroads of two categories of technologies. On the one\nhand, omnichannel digit services, to address the needs of users in the most\nseamless way. On the other hand, low code approaches, to build simply even\ncomplex software applications. In this twofold context, we propose DSUL\n(Digital Service Universal Language). It allows to build omnichannel services\nwith minimal work from their designers. We describe precisely how DSUL\noperates, and its innovation in regard to the state of the art. We also\nconsider the various methods to evaluate this framework.", "published": "2022-11-04 10:30:12", "link": "http://arxiv.org/abs/2211.03543v1", "categories": ["cs.SE", "cs.CL", "cs.PL"], "primary_category": "cs.SE"}
{"title": "Stutter-TTS: Controlled Synthesis and Improved Recognition of Stuttered\n  Speech", "abstract": "Stuttering is a speech disorder where the natural flow of speech is\ninterrupted by blocks, repetitions or prolongations of syllables, words and\nphrases. The majority of existing automatic speech recognition (ASR) interfaces\nperform poorly on utterances with stutter, mainly due to lack of matched\ntraining data. Synthesis of speech with stutter thus presents an opportunity to\nimprove ASR for this type of speech. We describe Stutter-TTS, an end-to-end\nneural text-to-speech model capable of synthesizing diverse types of stuttering\nutterances. We develop a simple, yet effective prosody-control strategy whereby\nadditional tokens are introduced into source text during training to represent\nspecific stuttering characteristics. By choosing the position of the stutter\ntokens, Stutter-TTS allows word-level control of where stuttering occurs in the\nsynthesized utterance. We are able to synthesize stutter events with high\naccuracy (F1-scores between 0.63 and 0.84, depending on stutter type). By\nfine-tuning an ASR model on synthetic stuttered speech we are able to reduce\nword error by 5.7% relative on stuttered utterances, with only minor (<0.2%\nrelative) degradation for fluent utterances.", "published": "2022-11-04 23:45:31", "link": "http://arxiv.org/abs/2211.09731v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "The Path to Autonomous Learners", "abstract": "In this paper, we present a new theoretical approach for enabling domain\nknowledge acquisition by intelligent systems. We introduce a hybrid model that\nstarts with minimal input knowledge in the form of an upper ontology of\nconcepts, stores and reasons over this knowledge through a knowledge graph\ndatabase and learns new information through a Logic Neural Network. We study\nthe behavior of this architecture when handling new data and show that the\nfinal system is capable of enriching its current knowledge as well as extending\nit to new domains.", "published": "2022-11-04 12:18:58", "link": "http://arxiv.org/abs/2211.02403v1", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG", "math.LO"], "primary_category": "stat.ML"}
{"title": "Self-Supervised Learning for Speech Enhancement through Synthesis", "abstract": "Modern speech enhancement (SE) networks typically implement noise suppression\nthrough time-frequency masking, latent representation masking, or\ndiscriminative signal prediction. In contrast, some recent works explore SE via\ngenerative speech synthesis, where the system's output is synthesized by a\nneural vocoder after an inherently lossy feature-denoising step. In this paper,\nwe propose a denoising vocoder (DeVo) approach, where a vocoder accepts noisy\nrepresentations and learns to directly synthesize clean speech. We leverage\nrich representations from self-supervised learning (SSL) speech models to\ndiscover relevant features. We conduct a candidate search across 15 potential\nSSL front-ends and subsequently train our vocoder adversarially with the best\nSSL configuration. Additionally, we demonstrate a causal version capable of\nrunning on streaming audio with 10ms latency and minimal performance\ndegradation. Finally, we conduct both objective evaluations and subjective\nlistening studies to show our system improves objective metrics and outperforms\nan existing state-of-the-art SE model subjectively.", "published": "2022-11-04 16:06:56", "link": "http://arxiv.org/abs/2211.02542v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Integrated Parameter-Efficient Tuning for General-Purpose Audio Models", "abstract": "The advent of hyper-scale and general-purpose pre-trained models is shifting\nthe paradigm of building task-specific models for target tasks. In the field of\naudio research, task-agnostic pre-trained models with high transferability and\nadaptability have achieved state-of-the-art performances through fine-tuning\nfor downstream tasks. Nevertheless, re-training all the parameters of these\nmassive models entails an enormous amount of time and cost, along with a huge\ncarbon footprint. To overcome these limitations, the present study explores and\napplies efficient transfer learning methods in the audio domain. We also\npropose an integrated parameter-efficient tuning (IPET) framework by\naggregating the embedding prompt (a prompt-based learning approach), and the\nadapter (an effective transfer learning method). We demonstrate the efficacy of\nthe proposed framework using two backbone pre-trained audio models with\ndifferent characteristics: the audio spectrogram transformer and wav2vec 2.0.\nThe proposed IPET framework exhibits remarkable performance compared to\nfine-tuning method with fewer trainable parameters in four downstream tasks:\nsound event classification, music genre classification, keyword spotting, and\nspeaker verification. Furthermore, the authors identify and analyze the\nshortcomings of the IPET framework, providing lessons and research directions\nfor parameter efficient tuning in the audio domain.", "published": "2022-11-04 02:21:15", "link": "http://arxiv.org/abs/2211.02227v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CochlScene: Acquisition of acoustic scene data using crowdsourcing", "abstract": "This paper describes a pipeline for collecting acoustic scene data by using\ncrowdsourcing. The detailed process of crowdsourcing is explained, including\nplanning, validation criteria, and actual user interfaces. As a result of data\ncollection, we present CochlScene, a novel dataset for acoustic scene\nclassification. Our dataset consists of 76k samples collected from 831\nparticipants in 13 acoustic scenes. We also propose a manual data split of\ntraining, validation, and test sets to increase the reliability of the\nevaluation results. Finally, we provide a baseline system for future research.", "published": "2022-11-04 07:01:16", "link": "http://arxiv.org/abs/2211.02289v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Speech Prosody of Audiobook Text-to-Speech Synthesis with\n  Acoustic and Textual Contexts", "abstract": "We present a multi-speaker Japanese audiobook text-to-speech (TTS) system\nthat leverages multimodal context information of preceding acoustic context and\nbilateral textual context to improve the prosody of synthetic speech. Previous\nwork either uses unilateral or single-modality context, which does not fully\nrepresent the context information. The proposed method uses an acoustic context\nencoder and a textual context encoder to aggregate context information and\nfeeds it to the TTS model, which enables the model to predict context-dependent\nprosody. We conducted comprehensive objective and subjective evaluations on a\nmulti-speaker Japanese audiobook dataset. Experimental results demonstrate that\nthe proposed method significantly outperforms two previous works. Additionally,\nwe present insights about the different choices of context - modalities,\nlateral information and length - for audiobook TTS that have never been\ndiscussed in the literature before.", "published": "2022-11-04 09:23:02", "link": "http://arxiv.org/abs/2211.02336v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NoreSpeech: Knowledge Distillation based Conditional Diffusion Model for\n  Noise-robust Expressive TTS", "abstract": "Expressive text-to-speech (TTS) can synthesize a new speaking style by\nimiating prosody and timbre from a reference audio, which faces the following\nchallenges: (1) The highly dynamic prosody information in the reference audio\nis difficult to extract, especially, when the reference audio contains\nbackground noise. (2) The TTS systems should have good generalization for\nunseen speaking styles. In this paper, we present a\n\\textbf{no}ise-\\textbf{r}obust \\textbf{e}xpressive TTS model (NoreSpeech),\nwhich can robustly transfer speaking style in a noisy reference utterance to\nsynthesized speech. Specifically, our NoreSpeech includes several components:\n(1) a novel DiffStyle module, which leverages powerful probabilistic denoising\ndiffusion models to learn noise-agnostic speaking style features from a teacher\nmodel by knowledge distillation; (2) a VQ-VAE block, which maps the style\nfeatures into a controllable quantized latent space for improving the\ngeneralization of style transfer; and (3) a straight-forward but effective\nparameter-free text-style alignment module, which enables NoreSpeech to\ntransfer style to a textual input from a length-mismatched reference utterance.\nExperiments demonstrate that NoreSpeech is more effective than previous\nexpressive TTS models in noise environments. Audio samples and code are\navailable at:\n\\href{http://dongchaoyang.top/NoreSpeech\\_demo/}{http://dongchaoyang.top/NoreSpeech\\_demo/}", "published": "2022-11-04 13:32:58", "link": "http://arxiv.org/abs/2211.02448v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sampling Rate Offset Estimation and Compensation for Distributed\n  Adaptive Node-Specific Signal Estimation in Wireless Acoustic Sensor Networks", "abstract": "Sampling rate offsets (SROs) between devices in a heterogeneous wireless\nacoustic sensor network (WASN) can hinder the ability of distributed adaptive\nalgorithms to perform as intended when they rely on coherent signal processing.\nIn this paper, we present an SRO estimation and compensation method to allow\nthe deployment of the distributed adaptive node-specific signal estimation\n(DANSE) algorithm in WASNs composed of asynchronous devices. The signals\navailable at each node are first utilised in a coherence-drift-based method to\nblindly estimate SROs which are then compensated for via phase shifts in the\nfrequency domain. A modification of the weighted overlap-add (WOLA)\nimplementation of DANSE is introduced to account for SRO-induced full-sample\ndrifts, permitting per-sample signal transmission via an approximation of the\nWOLA process as a time-domain convolution. The performance of the proposed\nalgorithm is evaluated in the context of distributed noise reduction for the\nestimation of a target speech signal in an asynchronous WASN.", "published": "2022-11-04 14:38:14", "link": "http://arxiv.org/abs/2211.02489v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Neural Feature Predictor and Discriminative Residual Coding for\n  Low-Bitrate Speech Coding", "abstract": "Low and ultra-low-bitrate neural speech coding achieves unprecedented coding\ngain by generating speech signals from compact speech features. This paper\nintroduces additional coding efficiency in neural speech coding by reducing the\ntemporal redundancy existing in the frame-level feature sequence via a\nrecurrent neural predictor. The prediction can achieve a low-entropy residual\nrepresentation, which we discriminatively code based on their contribution to\nthe signal reconstruction. The harmonization of feature prediction and\ndiscriminative coding results in a dynamic bit allocation algorithm that spends\nmore bits on unpredictable but rare events. As a result, we develop a scalable,\nlightweight, low-latency, and low-bitrate neural speech coding system. We\ndemonstrate the advantage of the proposed methods using the LPCNet as a neural\nvocoder. While the proposed method guarantees causality in its prediction, the\nsubjective tests and feature space analysis show that our model achieves\nsuperior coding efficiency compared to LPCNet and Lyra V2 in the very low\nbitrates.", "published": "2022-11-04 15:07:42", "link": "http://arxiv.org/abs/2211.02506v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cold Diffusion for Speech Enhancement", "abstract": "Diffusion models have recently shown promising results for difficult\nenhancement tasks such as the conditional and unconditional restoration of\nnatural images and audio signals. In this work, we explore the possibility of\nleveraging a recently proposed advanced iterative diffusion model, namely cold\ndiffusion, to recover clean speech signals from noisy signals. The unique\nmathematical properties of the sampling process from cold diffusion could be\nutilized to restore high-quality samples from arbitrary degradations. Based on\nthese properties, we propose an improved training algorithm and objective to\nhelp the model generalize better during the sampling process. We verify our\nproposed framework by investigating two model architectures. Experimental\nresults on benchmark speech enhancement dataset VoiceBank-DEMAND demonstrate\nthe strong performance of the proposed approach compared to representative\ndiscriminative models and diffusion-based enhancement models.", "published": "2022-11-04 15:41:01", "link": "http://arxiv.org/abs/2211.02527v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech enhancement using ego-noise references with a microphone array\n  embedded in an unmanned aerial vehicle", "abstract": "A method is proposed for performing speech enhancement using ego-noise\nreferences with a microphone array embedded in an unmanned aerial vehicle\n(UAV). The ego-noise reference signals are captured with microphones located\nnear the UAV's propellers and used in the prior knowledge multichannel Wiener\nfilter (PK-MWF) to obtain the speech correlation matrix estimate. Speech\npresence probability (SPP) can be estimated for detecting speech activity from\nan external microphone near the speech source, providing a performance\nbenchmark, or from one of the embedded microphones, assuming a more realistic\nscenario. Experimental measurements are performed in a semi-anechoic chamber,\nwith a UAV mounted on a stand and a loudspeaker playing a speech signal, while\nsetting three distinct and fixed propeller rotation speeds, resulting in three\ndifferent signal-to-noise ratios (SNRs). The recordings obtained and made\navailable online are used to compare the proposed method to the use of the\nstandard multichannel Wiener filter (MWF) estimated with and without the\npropellers' microphones being used in its formulation. Results show that\ncompared to those, the use of PK-MWF achieves higher levels of improvement in\nspeech intelligibility and quality, measured by STOI and PESQ, while the SNR\nimprovement is similar.", "published": "2022-11-04 18:15:21", "link": "http://arxiv.org/abs/2211.02690v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Real-Time Joint Personalized Speech Enhancement and Acoustic Echo\n  Cancellation", "abstract": "Personalized speech enhancement (PSE) is a real-time SE approach utilizing a\nspeaker embedding of a target person to remove background noise, reverberation,\nand interfering voices. To deploy a PSE model for full duplex communications,\nthe model must be combined with acoustic echo cancellation (AEC), although such\na combination has been less explored. This paper proposes a series of methods\nthat are applicable to various model architectures to develop efficient causal\nmodels that can handle the tasks of PSE, AEC, and joint PSE-AEC. We present\nextensive evaluation results using both simulated data and real recordings,\ncovering various acoustic conditions and evaluation metrics. The results show\nthe effectiveness of the proposed methods for two different model\narchitectures. Our best joint PSE-AEC model comes close to the expert models\noptimized for individual tasks of PSE and AEC in their respective scenarios and\nsignificantly outperforms the expert models for the combined PSE-AEC task.", "published": "2022-11-04 22:29:00", "link": "http://arxiv.org/abs/2211.02773v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Music Mixing Style Transfer: A Contrastive Learning Approach to\n  Disentangle Audio Effects", "abstract": "We propose an end-to-end music mixing style transfer system that converts the\nmixing style of an input multitrack to that of a reference song. This is\nachieved with an encoder pre-trained with a contrastive objective to extract\nonly audio effects related information from a reference music recording. All\nour models are trained in a self-supervised manner from an already-processed\nwet multitrack dataset with an effective data preprocessing method that\nalleviates the data scarcity of obtaining unprocessed dry data. We analyze the\nproposed encoder for the disentanglement capability of audio effects and also\nvalidate its performance for mixing style transfer through both objective and\nsubjective evaluations. From the results, we show the proposed system not only\nconverts the mixing style of multitrack audio close to a reference but is also\nrobust with mixture-wise style transfer upon using a music source separation\nmodel.", "published": "2022-11-04 03:45:17", "link": "http://arxiv.org/abs/2211.02247v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Real-Time Target Sound Extraction", "abstract": "We present the first neural network model to achieve real-time and streaming\ntarget sound extraction. To accomplish this, we propose Waveformer, an\nencoder-decoder architecture with a stack of dilated causal convolution layers\nas the encoder, and a transformer decoder layer as the decoder. This hybrid\narchitecture uses dilated causal convolutions for processing large receptive\nfields in a computationally efficient manner while also leveraging the\ngeneralization performance of transformer-based architectures. Our evaluations\nshow as much as 2.2-3.3 dB improvement in SI-SNRi compared to the prior models\nfor this task while having a 1.2-4x smaller model size and a 1.5-2x lower\nruntime. We provide code, dataset, and audio samples:\nhttps://waveformer.cs.washington.edu/.", "published": "2022-11-04 03:51:23", "link": "http://arxiv.org/abs/2211.02250v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Wireless Deep Speech Semantic Transmission", "abstract": "In this paper, we propose a new class of high-efficiency semantic coded\ntransmission methods for end-to-end speech transmission over wireless channels.\nWe name the whole system as deep speech semantic transmission (DSST).\nSpecifically, we introduce a nonlinear transform to map the speech source to\nsemantic latent space and feed semantic features into source-channel encoder to\ngenerate the channel-input sequence. Guided by the variational modeling idea,\nwe build an entropy model on the latent space to estimate the importance\ndiversity among semantic feature embeddings. Accordingly, these semantic\nfeatures of different importance can be allocated with different coding rates\nreasonably, which maximizes the system coding gain. Furthermore, we introduce a\nchannel signal-to-noise ratio (SNR) adaptation mechanism such that a single\nmodel can be applied over various channel states. The end-to-end optimization\nof our model leads to a flexible rate-distortion (RD) trade-off, supporting\nversatile wireless speech semantic transmission. Experimental results verify\nthat our DSST system clearly outperforms current engineered speech transmission\nsystems on both objective and subjective metrics. Compared with existing neural\nspeech semantic transmission methods, our model saves up to 75% of channel\nbandwidth costs when achieving the same quality. An intuitive comparison of\naudio demos can be found at https://ximoo123.github.io/DSST.", "published": "2022-11-04 06:49:42", "link": "http://arxiv.org/abs/2211.02283v1", "categories": ["cs.SD", "cs.IT", "eess.AS", "math.IT"], "primary_category": "cs.SD"}
{"title": "Binaural Rendering of Ambisonic Signals by Neural Networks", "abstract": "Binaural rendering of ambisonic signals is of broad interest to virtual\nreality and immersive media. Conventional methods often require manually\nmeasured Head-Related Transfer Functions (HRTFs). To address this issue, we\ncollect a paired ambisonic-binaural dataset and propose a deep learning\nframework in an end-to-end manner. Experimental results show that neural\nnetworks outperform the conventional method in objective metrics and achieve\ncomparable subjective metrics. To validate the proposed framework, we\nexperimentally explore different settings of the input features, model\nstructures, output features, and loss functions. Our proposed system achieves\nan SDR of 7.32 and MOSs of 3.83, 3.58, 3.87, 3.58 in quality, timbre,\nlocalization, and immersion dimensions.", "published": "2022-11-04 07:57:37", "link": "http://arxiv.org/abs/2211.02301v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SPEAKER VGG CCT: Cross-corpus Speech Emotion Recognition with Speaker\n  Embedding and Vision Transformers", "abstract": "In recent years, Speech Emotion Recognition (SER) has been investigated\nmainly transforming the speech signal into spectrograms that are then\nclassified using Convolutional Neural Networks pretrained on generic images and\nfine tuned with spectrograms. In this paper, we start from the general idea\nabove and develop a new learning solution for SER, which is based on Compact\nConvolutional Transformers (CCTs) combined with a speaker embedding. With CCTs,\nthe learning power of Vision Transformers (ViT) is combined with a diminished\nneed for large volume of data as made possible by the convolution. This is\nimportant in SER, where large corpora of data are usually not available. The\nspeaker embedding allows the network to extract an identity representation of\nthe speaker, which is then integrated by means of a self-attention mechanism\nwith the features that the CCT extracts from the spectrogram. Overall, the\nsolution is capable of operating in real-time showing promising results in a\ncross-corpus scenario, where training and test datasets are kept separate.\nExperiments have been performed on several benchmarks in a cross-corpus setting\nas rarely used in the literature, with results that are comparable or superior\nto those obtained with state-of-the-art network architectures. Our code is\navailable at https://github.com/JabuMlDev/Speaker-VGG-CCT.", "published": "2022-11-04 10:49:44", "link": "http://arxiv.org/abs/2211.02366v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Analysing Diffusion-based Generative Approaches versus Discriminative\n  Approaches for Speech Restoration", "abstract": "Diffusion-based generative models have had a high impact on the computer\nvision and speech processing communities these past years. Besides data\ngeneration tasks, they have also been employed for data restoration tasks like\nspeech enhancement and dereverberation. While discriminative models have\ntraditionally been argued to be more powerful e.g. for speech enhancement,\ngenerative diffusion approaches have recently been shown to narrow this\nperformance gap considerably. In this paper, we systematically compare the\nperformance of generative diffusion models and discriminative approaches on\ndifferent speech restoration tasks. For this, we extend our prior contributions\non diffusion-based speech enhancement in the complex time-frequency domain to\nthe task of bandwith extension. We then compare it to a discriminatively\ntrained neural network with the same network architecture on three restoration\ntasks, namely speech denoising, dereverberation and bandwidth extension. We\nobserve that the generative approach performs globally better than its\ndiscriminative counterpart on all tasks, with the strongest benefit for\nnon-additive distortion models, like in dereverberation and bandwidth\nextension. Code and audio examples can be found online at\nhttps://uhh.de/inf-sp-sgmsemultitask", "published": "2022-11-04 12:06:14", "link": "http://arxiv.org/abs/2211.02397v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Spatially Selective Deep Non-linear Filters for Speaker Extraction", "abstract": "In a scenario with multiple persons talking simultaneously, the spatial\ncharacteristics of the signals are the most distinct feature for extracting the\ntarget signal. In this work, we develop a deep joint spatial-spectral\nnon-linear filter that can be steered in an arbitrary target direction. For\nthis we propose a simple and effective conditioning mechanism, which sets the\ninitial state of the filter's recurrent layers based on the target direction.\nWe show that this scheme is more effective than the baseline approach and\nincreases the flexibility of the filter at no performance cost. The resulting\nspatially selective non-linear filters can also be used for speech separation\nof an arbitrary number of speakers and enable very accurate multi-speaker\nlocalization as we demonstrate in this paper.", "published": "2022-11-04 12:54:06", "link": "http://arxiv.org/abs/2211.02420v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CCATMos: Convolutional Context-aware Transformer Network for\n  Non-intrusive Speech Quality Assessment", "abstract": "Speech quality assessment has been a critical component in many voice\ncommunication related applications such as telephony and online conferencing.\nTraditional intrusive speech quality assessment requires the clean reference of\nthe degraded utterance to provide an accurate quality measurement. This\nrequirement limits the usability of these methods in real-world scenarios. On\nthe other hand, non-intrusive subjective measurement is the ``golden standard\"\nin evaluating speech quality as human listeners can intrinsically evaluate the\nquality of any degraded speech with ease. In this paper, we propose a novel\nend-to-end model structure called Convolutional Context-Aware Transformer\n(CCAT) network to predict the mean opinion score (MOS) of human raters. We\nevaluate our model on three MOS-annotated datasets spanning multiple languages\nand distortion types and submit our results to the ConferencingSpeech 2022\nChallenge. Our experiments show that CCAT provides promising MOS predictions\ncompared to current state-of-art non-intrusive speech assessment models with\naverage Pearson correlation coefficient (PCC) increasing from 0.530 to 0.697\nand average RMSE decreasing from 0.768 to 0.570 compared to the baseline model\non the challenge evaluation test set.", "published": "2022-11-04 16:46:11", "link": "http://arxiv.org/abs/2211.02577v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Resource-Efficient Transfer Learning From Speech Foundation Model Using\n  Hierarchical Feature Fusion", "abstract": "Self-supervised pre-training of a speech foundation model, followed by\nsupervised fine-tuning, has shown impressive quality improvements on automatic\nspeech recognition (ASR) tasks. Fine-tuning separate foundation models for many\ndownstream tasks are expensive since the foundation model is usually very big.\nParameter-efficient fine-tuning methods (e.g. adapter, sparse update methods)\noffer an alternative paradigm where a small set of parameters are updated to\nadapt the foundation model to new tasks. However, these methods still suffer\nfrom a high computational memory cost and slow training speed because they\nrequire backpropagation through the entire neural network at each step. In the\npaper, we analyze the performance of features at different layers of a\nfoundation model on the speech recognition task and propose a novel\nhierarchical feature fusion method for resource-efficient transfer learning\nfrom speech foundation models. Experimental results show that the proposed\nmethod can achieve better performance on speech recognition task than existing\nalgorithms with fewer number of trainable parameters, less computational memory\ncost and faster training speed. After combining with Adapters at all layers,\nthe proposed method can achieve the same performance as fine-tuning the whole\nmodel with $97\\%$ fewer trainable encoder parameters and $53\\%$ faster training\nspeed.", "published": "2022-11-04 19:03:45", "link": "http://arxiv.org/abs/2211.02712v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "SAMO: Speaker Attractor Multi-Center One-Class Learning for Voice\n  Anti-Spoofing", "abstract": "Voice anti-spoofing systems are crucial auxiliaries for automatic speaker\nverification (ASV) systems. A major challenge is caused by unseen attacks\nempowered by advanced speech synthesis technologies. Our previous research on\none-class learning has improved the generalization ability to unseen attacks by\ncompacting the bona fide speech in the embedding space. However, such\ncompactness lacks consideration of the diversity of speakers. In this work, we\npropose speaker attractor multi-center one-class learning (SAMO), which\nclusters bona fide speech around a number of speaker attractors and pushes away\nspoofing attacks from all the attractors in a high-dimensional embedding space.\nFor training, we propose an algorithm for the co-optimization of bona fide\nspeech clustering and bona fide/spoof classification. For inference, we propose\nstrategies to enable anti-spoofing for speakers without enrollment. Our\nproposed system outperforms existing state-of-the-art single systems with a\nrelative improvement of 38% on equal error rate (EER) on the ASVspoof2019 LA\nevaluation set.", "published": "2022-11-04 19:31:33", "link": "http://arxiv.org/abs/2211.02718v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Multi-blank Transducers for Speech Recognition", "abstract": "This paper proposes a modification to RNN-Transducer (RNN-T) models for\nautomatic speech recognition (ASR). In standard RNN-T, the emission of a blank\nsymbol consumes exactly one input frame; in our proposed method, we introduce\nadditional blank symbols, which consume two or more input frames when emitted.\nWe refer to the added symbols as big blanks, and the method multi-blank RNN-T.\nFor training multi-blank RNN-Ts, we propose a novel logit under-normalization\nmethod in order to prioritize emissions of big blanks. With experiments on\nmultiple languages and datasets, we show that multi-blank RNN-T methods could\nbring relative speedups of over +90%/+139% to model inference for English\nLibrispeech and German Multilingual Librispeech datasets, respectively. The\nmulti-blank RNN-T method also improves ASR accuracy consistently. We will\nrelease our implementation of the method in the NeMo\n(https://github.com/NVIDIA/NeMo) toolkit.", "published": "2022-11-04 16:24:46", "link": "http://arxiv.org/abs/2211.03541v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
