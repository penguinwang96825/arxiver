{"title": "Be Concise and Precise: Synthesizing Open-Domain Entity Descriptions\n  from Facts", "abstract": "Despite being vast repositories of factual information, cross-domain\nknowledge graphs, such as Wikidata and the Google Knowledge Graph, only\nsparsely provide short synoptic descriptions for entities. Such descriptions\nthat briefly identify the most discernible features of an entity provide\nreaders with a near-instantaneous understanding of what kind of entity they are\nbeing presented. They can also aid in tasks such as named entity\ndisambiguation, ontological type determination, and answering entity queries.\nGiven the rapidly increasing numbers of entities in knowledge graphs, a fully\nautomated synthesis of succinct textual descriptions from underlying factual\ninformation is essential. To this end, we propose a novel fact-to-sequence\nencoder-decoder model with a suitable copy mechanism to generate concise and\nprecise textual descriptions of entities. In an in-depth evaluation, we\ndemonstrate that our method significantly outperforms state-of-the-art\nalternatives.", "published": "2019-04-16 01:30:00", "link": "http://arxiv.org/abs/1904.07391v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Positional Encoding to Control Output Sequence Length", "abstract": "Neural encoder-decoder models have been successful in natural language\ngeneration tasks. However, real applications of abstractive summarization must\nconsider additional constraint that a generated summary should not exceed a\ndesired length. In this paper, we propose a simple but effective extension of a\nsinusoidal positional encoding (Vaswani et al., 2017) to enable neural\nencoder-decoder model to preserves the length constraint. Unlike in previous\nstudies where that learn embeddings representing each length, the proposed\nmethod can generate a text of any length even if the target length is not\npresent in training data. The experimental results show that the proposed\nmethod can not only control the generation length but also improve the ROUGE\nscores.", "published": "2019-04-16 02:48:11", "link": "http://arxiv.org/abs/1904.07418v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Subjective Assessment of Text Complexity: A Dataset for German Language", "abstract": "This paper presents TextComplexityDE, a dataset consisting of 1000 sentences\nin German language taken from 23 Wikipedia articles in 3 different\narticle-genres to be used for developing text-complexity predictor models and\nautomatic text simplification in German language. The dataset includes\nsubjective assessment of different text-complexity aspects provided by German\nlearners in level A and B. In addition, it contains manual simplification of\n250 of those sentences provided by native speakers and subjective assessment of\nthe simplified sentences by participants from the target group. The subjective\nratings were collected using both laboratory studies and crowdsourcing\napproach.", "published": "2019-04-16 14:39:21", "link": "http://arxiv.org/abs/1904.07733v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UTFPR at SemEval-2019 Task 5: Hate Speech Identification with Recurrent\n  Neural Networks", "abstract": "In this paper we revisit the problem of automatically identifying hate speech\nin posts from social media. We approach the task using a system based on\nminimalistic compositional Recurrent Neural Networks (RNN). We tested our\napproach on the SemEval-2019 Task 5: Multilingual Detection of Hate Speech\nAgainst Immigrants and Women in Twitter (HatEval) shared task dataset. The\ndataset made available by the HatEval organizers contained English and Spanish\nposts retrieved from Twitter annotated with respect to the presence of hateful\ncontent and its target. In this paper we present the results obtained by our\nsystem in comparison to the other entries in the shared task. Our system\nachieved competitive performance ranking 7th in sub-task A out of 62 systems in\nthe English track.", "published": "2019-04-16 17:41:49", "link": "http://arxiv.org/abs/1904.07839v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating the Impact of Speech Recognition Errors on Spoken Question\n  Answering by Adversarial Domain Adaptation", "abstract": "Spoken question answering (SQA) is challenging due to complex reasoning on\ntop of the spoken documents. The recent studies have also shown the\ncatastrophic impact of automatic speech recognition (ASR) errors on SQA.\nTherefore, this work proposes to mitigate the ASR errors by aligning the\nmismatch between ASR hypotheses and their corresponding reference\ntranscriptions. An adversarial model is applied to this domain adaptation task,\nwhich forces the model to learn domain-invariant features the QA model can\neffectively utilize in order to improve the SQA results. The experiments\nsuccessfully demonstrate the effectiveness of our proposed model, and the\nresults are better than the previous best model by 2% EM score.", "published": "2019-04-16 18:13:39", "link": "http://arxiv.org/abs/1904.07904v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Characteristics of Schizophrenic Speech", "abstract": "Natural language processing tools are used to automatically detect\ndisturbances in transcribed speech of schizophrenia inpatients who speak\nHebrew. We measure topic mutation over time and show that controls maintain\nmore cohesive speech than inpatients. We also examine differences in how\ninpatients and controls use adjectives and adverbs to describe content words\nand show that the ones used by controls are more common than the those of\ninpatients. We provide experimental results and show their potential for\nautomatically detecting schizophrenia in patients by means only of their speech\npatterns.", "published": "2019-04-16 20:09:41", "link": "http://arxiv.org/abs/1904.07953v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Systematic Study of Leveraging Subword Information for Learning Word\n  Representations", "abstract": "The use of subword-level information (e.g., characters, character n-grams,\nmorphemes) has become ubiquitous in modern word representation learning. Its\nimportance is attested especially for morphologically rich languages which\ngenerate a large number of rare words. Despite a steadily increasing interest\nin such subword-informed word representations, their systematic comparative\nanalysis across typologically diverse languages and different tasks is still\nmissing. In this work, we deliver such a study focusing on the variation of two\ncrucial components required for subword-level integration into word\nrepresentation models: 1) segmentation of words into subword units, and 2)\nsubword composition functions to obtain final word representations. We propose\na general framework for learning subword-informed word representations that\nallows for easy experimentation with different segmentation and composition\ncomponents, also including more advanced techniques based on position\nembeddings and self-attention. Using the unified framework, we run experiments\nover a large number of subword-informed word representation configurations (60\nin total) on 3 tasks (general and rare word similarity, dependency parsing,\nfine-grained entity typing) for 5 languages representing 3 language types. Our\nmain results clearly indicate that there is no \"one-sizefits-all\"\nconfiguration, as performance is both language- and task-dependent. We also\nshow that configurations based on unsupervised segmentation (e.g., BPE,\nMorfessor) are sometimes comparable to or even outperform the ones based on\nsupervised word segmentation.", "published": "2019-04-16 21:50:32", "link": "http://arxiv.org/abs/1904.07994v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding the Behaviors of BERT in Ranking", "abstract": "This paper studies the performances and behaviors of BERT in ranking tasks.\nWe explore several different ways to leverage the pre-trained BERT and\nfine-tune it on two ranking tasks: MS MARCO passage reranking and TREC Web\nTrack ad hoc document ranking. Experimental results on MS MARCO demonstrate the\nstrong effectiveness of BERT in question-answering focused passage ranking\ntasks, as well as the fact that BERT is a strong interaction-based seq2seq\nmatching model. Experimental results on TREC show the gaps between the BERT\npre-trained on surrounding contexts and the needs of ad hoc document ranking.\nAnalyses illustrate how BERT allocates its attentions between query-document\ntokens in its Transformer layers, how it prefers semantic matches between\nparaphrase tokens, and how that differs with the soft match patterns learned by\na click-trained neural ranker.", "published": "2019-04-16 08:30:31", "link": "http://arxiv.org/abs/1904.07531v4", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial\n  Event Extraction", "abstract": "Most existing event extraction (EE) methods merely extract event arguments\nwithin the sentence scope. However, such sentence-level EE methods struggle to\nhandle soaring amounts of documents from emerging applications, such as\nfinance, legislation, health, etc., where event arguments always scatter across\ndifferent sentences, and even multiple such event mentions frequently co-exist\nin the same document. To address these challenges, we propose a novel\nend-to-end model, Doc2EDAG, which can generate an entity-based directed acyclic\ngraph to fulfill the document-level EE (DEE) effectively. Moreover, we\nreformalize a DEE task with the no-trigger-words design to ease the\ndocument-level event labeling. To demonstrate the effectiveness of Doc2EDAG, we\nbuild a large-scale real-world dataset consisting of Chinese financial\nannouncements with the challenges mentioned above. Extensive experiments with\ncomprehensive analyses illustrate the superiority of Doc2EDAG over\nstate-of-the-art methods. Data and codes can be found at\nhttps://github.com/dolphin-zs/Doc2EDAG.", "published": "2019-04-16 08:39:06", "link": "http://arxiv.org/abs/1904.07535v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sameness Entices, but Novelty Enchants in Fanfiction Online", "abstract": "Cultural evolution is driven by how we choose what to consume and share with\nothers. A common belief is that the cultural artifacts that succeed are ones\nthat balance novelty and conventionality. This balance theory suggests that\npeople prefer works that are familiar, but not so familiar as to be boring;\nnovel, but not so novel as to violate the expectations of their genre. We test\nthis idea using a large dataset of fanfiction. We apply a multiple regression\nmodel and a generalized additive model to examine how the recognition a work\nreceives varies with its novelty, estimated through a Latent Dirichlet\nAllocation topic model, in the context of existing works. We find the opposite\npattern of what the balance theory predicts$\\unicode{x2014}$overall success\ndecline almost monotonically with novelty and exhibits a U-shaped, instead of\nan inverse U-shaped, curve. This puzzle is resolved by teasing out two\ncompeting forces: sameness attracts the mass whereas novelty provides\nenjoyment. Taken together, even though the balance theory holds in terms of\nexpressed enjoyment, the overall success can show the opposite pattern due to\nthe dominant role of sameness to attract the audience. Under these two forces,\ncultural evolution may have to work against inertia$\\unicode{x2014}$the\nappetite for consuming the familiar$\\unicode{x2014}$and may resemble a\npunctuated equilibrium, marked by occasional leaps.", "published": "2019-04-16 14:50:09", "link": "http://arxiv.org/abs/1904.07741v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Discovery of Multimodal Links in Multi-image,\n  Multi-sentence Documents", "abstract": "Images and text co-occur constantly on the web, but explicit links between\nimages and sentences (or other intra-document textual units) are often not\npresent. We present algorithms that discover image-sentence relationships\nwithout relying on explicit multimodal annotation in training. We experiment on\nseven datasets of varying difficulty, ranging from documents consisting of\ngroups of images captioned post hoc by crowdworkers to naturally-occurring\nuser-generated multimodal documents. We find that a structured training\nobjective based on identifying whether collections of images and sentences\nco-occur in documents can suffice to predict links between specific sentences\nand specific images within the same document at test time.", "published": "2019-04-16 17:07:20", "link": "http://arxiv.org/abs/1904.07826v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Query Expansion for Cross-Language Question Re-Ranking", "abstract": "Community question-answering (CQA) platforms have become very popular forums\nfor asking and answering questions daily. While these forums are rich\nrepositories of community knowledge, they present challenges for finding\nrelevant answers and similar questions, due to the open-ended nature of\ninformal discussions. Further, if the platform allows questions and answers in\nmultiple languages, we are faced with the additional challenge of matching\ncross-lingual information. In this work, we focus on the cross-language\nquestion re-ranking shared task, which aims to find existing questions that may\nbe written in different languages. Our contribution is an exploration of query\nexpansion techniques for this problem. We investigate expansions based on Word\nEmbeddings, DBpedia concepts linking, and Hypernym, and show that they\noutperform existing state-of-the-art methods.", "published": "2019-04-16 20:55:59", "link": "http://arxiv.org/abs/1904.07982v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "I4U Submission to NIST SRE 2018: Leveraging from a Decade of Shared\n  Experiences", "abstract": "The I4U consortium was established to facilitate a joint entry to NIST\nspeaker recognition evaluations (SRE). The latest edition of such joint\nsubmission was in SRE 2018, in which the I4U submission was among the\nbest-performing systems. SRE'18 also marks the 10-year anniversary of I4U\nconsortium into NIST SRE series of evaluation. The primary objective of the\ncurrent paper is to summarize the results and lessons learned based on the\ntwelve sub-systems and their fusion submitted to SRE'18. It is also our\nintention to present a shared view on the advancements, progresses, and major\nparadigm shifts that we have witnessed as an SRE participant in the past decade\nfrom SRE'08 to SRE'18. In this regard, we have seen, among others, a paradigm\nshift from supervector representation to deep speaker embedding, and a switch\nof research challenge from channel compensation to domain adaptation.", "published": "2019-04-16 00:55:20", "link": "http://arxiv.org/abs/1904.07386v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised acoustic unit discovery for speech synthesis using discrete\n  latent-variable neural networks", "abstract": "For our submission to the ZeroSpeech 2019 challenge, we apply discrete\nlatent-variable neural networks to unlabelled speech and use the discovered\nunits for speech synthesis. Unsupervised discrete subword modelling could be\nuseful for studies of phonetic category learning in infants or in low-resource\nspeech technology requiring symbolic input. We use an autoencoder (AE)\narchitecture with intermediate discretisation. We decouple acoustic unit\ndiscovery from speaker modelling by conditioning the AE's decoder on the\ntraining speaker identity. At test time, unit discovery is performed on speech\nfrom an unseen speaker, followed by unit decoding conditioned on a known target\nspeaker to obtain reconstructed filterbanks. This output is fed to a neural\nvocoder to synthesise speech in the target speaker's voice. For discretisation,\ncategorical variational autoencoders (CatVAEs), vector-quantised VAEs (VQ-VAEs)\nand straight-through estimation are compared at different compression levels on\ntwo languages. Our final model uses convolutional encoding, VQ-VAE\ndiscretisation, deconvolutional decoding and an FFTNet vocoder. We show that\ndecoupled speaker conditioning intrinsically improves discrete acoustic\nrepresentations, yielding competitive synthesis quality compared to the\nchallenge baseline.", "published": "2019-04-16 09:38:01", "link": "http://arxiv.org/abs/1904.07556v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Causality Extraction based on Self-Attentive BiLSTM-CRF with Transferred\n  Embeddings", "abstract": "Causality extraction from natural language texts is a challenging open\nproblem in artificial intelligence. Existing methods utilize patterns,\nconstraints, and machine learning techniques to extract causality, heavily\ndepending on domain knowledge and requiring considerable human effort and time\nfor feature engineering. In this paper, we formulate causality extraction as a\nsequence labeling problem based on a novel causality tagging scheme. On this\nbasis, we propose a neural causality extractor with the BiLSTM-CRF model as the\nbackbone, named SCITE (Self-attentive BiLSTM-CRF wIth Transferred Embeddings),\nwhich can directly extract cause and effect without extracting candidate causal\npairs and identifying their relations separately. To address the problem of\ndata insufficiency, we transfer contextual string embeddings, also known as\nFlair embeddings, which are trained on a large corpus in our task. In addition,\nto improve the performance of causality extraction, we introduce a multihead\nself-attention mechanism into SCITE to learn the dependencies between causal\nwords. We evaluate our method on a public dataset, and experimental results\ndemonstrate that our method achieves significant and consistent improvement\ncompared to baselines.", "published": "2019-04-16 12:54:00", "link": "http://arxiv.org/abs/1904.07629v6", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Analysis of Speech Enhancement and Recognition Losses in Limited\n  Resources Multi-talker Single Channel Audio-Visual ASR", "abstract": "In this paper, we analyzed how audio-visual speech enhancement can help to\nperform the ASR task in a cocktail party scenario. Therefore we considered two\nsimple end-to-end LSTM-based models that perform single-channel audio-visual\nspeech enhancement and phone recognition respectively. Then, we studied how the\ntwo models interact, and how to train them jointly affects the final result. We\nanalyzed different training strategies that reveal some interesting and\nunexpected behaviors. The experiments show that during optimization of the ASR\ntask the speech enhancement capability of the model significantly decreases and\nvice-versa. Nevertheless the joint optimization of the two tasks shows a\nremarkable drop of the Phone Error Rate (PER) compared to the audio-visual\nbaseline models trained only to perform phone recognition. We analyzed the\nbehaviors of the proposed models by using two limited-size datasets, and in\nparticular we used the mixed-speech versions of GRID and TCD-TIMIT.", "published": "2019-04-16 14:43:19", "link": "http://arxiv.org/abs/1904.08248v2", "categories": ["eess.AS", "cs.CL", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "An Empirical Evaluation of Text Representation Schemes on Multilingual\n  Social Web to Filter the Textual Aggression", "abstract": "This paper attempt to study the effectiveness of text representation schemes\non two tasks namely: User Aggression and Fact Detection from the social media\ncontents. In User Aggression detection, The aim is to identify the level of\naggression from the contents generated in the Social media and written in the\nEnglish, Devanagari Hindi and Romanized Hindi. Aggression levels are\ncategorized into three predefined classes namely: `Non-aggressive`, `Overtly\nAggressive`, and `Covertly Aggressive`. During the disaster-related incident,\nSocial media like, Twitter is flooded with millions of posts. In such emergency\nsituations, identification of factual posts is important for organizations\ninvolved in the relief operation. We anticipated this problem as a combination\nof classification and Ranking problem. This paper presents a comparison of\nvarious text representation scheme based on BoW techniques, distributed\nword/sentence representation, transfer learning on classifiers. Weighted $F_1$\nscore is used as a primary evaluation metric. Results show that text\nrepresentation using BoW performs better than word embedding on machine\nlearning classifiers. While pre-trained Word embedding techniques perform\nbetter on classifiers based on deep neural net. Recent transfer learning model\nlike ELMO, ULMFiT are fine-tuned for the Aggression classification task.\nHowever, results are not at par with pre-trained word embedding model. Overall,\nword embedding using fastText produce best weighted $F_1$-score than Word2Vec\nand Glove. Results are further improved using pre-trained vector model.\nStatistical significance tests are employed to ensure the significance of the\nclassification results. In the case of lexically different test Dataset, other\nthan training Dataset, deep neural models are more robust and perform\nsubstantially better than machine learning classifiers.", "published": "2019-04-16 17:10:52", "link": "http://arxiv.org/abs/1904.08770v1", "categories": ["cs.IR", "cs.CL", "cs.SI", "I.2.7; I.2.6"], "primary_category": "cs.IR"}
{"title": "Spoof detection using time-delay shallow neural network and feature\n  switching", "abstract": "Detecting spoofed utterances is a fundamental problem in voice-based\nbiometrics. Spoofing can be performed either by logical accesses like speech\nsynthesis, voice conversion or by physical accesses such as replaying the\npre-recorded utterance. Inspired by the state-of-the-art \\emph{x}-vector based\nspeaker verification approach, this paper proposes a time-delay shallow neural\nnetwork (TD-SNN) for spoof detection for both logical and physical access. The\nnovelty of the proposed TD-SNN system vis-a-vis conventional DNN systems is\nthat it can handle variable length utterances during testing. Performance of\nthe proposed TD-SNN systems and the baseline Gaussian mixture models (GMMs) is\nanalyzed on the ASV-spoof-2019 dataset. The performance of the systems is\nmeasured in terms of the minimum normalized tandem detection cost function\n(min-t-DCF). When studied with individual features, the TD-SNN system\nconsistently outperforms the GMM system for physical access. For logical\naccess, GMM surpasses TD-SNN systems for certain individual features. When\ncombined with the decision-level feature switching (DLFS) paradigm, the best\nTD-SNN system outperforms the best baseline GMM system on evaluation data with\na relative improvement of 48.03\\% and 49.47\\% for both logical and physical\naccess, respectively.", "published": "2019-04-16 04:22:26", "link": "http://arxiv.org/abs/1904.07453v2", "categories": ["eess.AS", "cs.CR", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Denoising by Accumulating Per-Frequency Modeling Fluctuations", "abstract": "We present a method for audio denoising that combines processing done in both\nthe time domain and the time-frequency domain. Given a noisy audio clip, the\nmethod trains a deep neural network to fit this signal. Since the fitting is\nonly partly successful and is able to better capture the underlying clean\nsignal than the noise, the output of the network helps to disentangle the clean\naudio from the rest of the signal. This is done by accumulating a fitting score\nper time-frequency bin and applying the time-frequency domain filtering based\non the obtained scores. The method is completely unsupervised and only trains\non the specific audio clip that is being denoised. Our experiments demonstrate\nfavorable performance in comparison to the literature methods. Our code and\nsamples are available at github.com/mosheman5/DNP and as supplementary. Index\nTerms: Audio denoising; Unsupervised learning", "published": "2019-04-16 12:06:58", "link": "http://arxiv.org/abs/1904.07612v3", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Co-Separating Sounds of Visual Objects", "abstract": "Learning how objects sound from video is challenging, since they often\nheavily overlap in a single audio channel. Current methods for visually-guided\naudio source separation sidestep the issue by training with artificially mixed\nvideo clips, but this puts unwieldy restrictions on training data collection\nand may even prevent learning the properties of \"true\" mixed sounds. We\nintroduce a co-separation training paradigm that permits learning object-level\nsounds from unlabeled multi-source videos. Our novel training objective\nrequires that the deep neural network's separated audio for similar-looking\nobjects be consistently identifiable, while simultaneously reproducing accurate\nvideo-level audio tracks for each source training pair. Our approach\ndisentangles sounds in realistic test videos, even in cases where an object was\nnot observed individually during training. We obtain state-of-the-art results\non visually-guided audio source separation and audio denoising for the MUSIC,\nAudioSet, and AV-Bench datasets.", "published": "2019-04-16 15:07:50", "link": "http://arxiv.org/abs/1904.07750v2", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Improved Speech Separation with Time-and-Frequency Cross-domain Joint\n  Embedding and Clustering", "abstract": "Speech separation has been very successful with deep learning techniques.\nSubstantial effort has been reported based on approaches over spectrogram,\nwhich is well known as the standard time-and-frequency cross-domain\nrepresentation for speech signals. It is highly correlated to the phonetic\nstructure of speech, or \"how the speech sounds\" when perceived by human, but\nprimarily frequency domain features carrying temporal behaviour. Very\nimpressive work achieving speech separation over time domain was reported\nrecently, probably because waveforms in time domain may describe the different\nrealizations of speech in a more precise way than spectrogram. In this paper,\nwe propose a framework properly integrating the above two directions, hoping to\nachieve both purposes. We construct a time-and-frequency feature map by\nconcatenating the 1-dim convolution encoded feature map (for time domain) and\nthe spectrogram (for frequency domain), which was then processed by an\nembedding network and clustering approaches very similar to those used in time\nand frequency domain prior works. In this way, the information in the time and\nfrequency domains, as well as the interactions between them, can be jointly\nconsidered during embedding and clustering. Very encouraging results\n(state-of-the-art to our knowledge) were obtained with WSJ0-2mix dataset in\npreliminary experiments.", "published": "2019-04-16 17:48:59", "link": "http://arxiv.org/abs/1904.07845v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Audio-Visual Model Distillation Using Acoustic Images", "abstract": "In this paper, we investigate how to learn rich and robust feature\nrepresentations for audio classification from visual data and acoustic images,\na novel audio data modality. Former models learn audio representations from raw\nsignals or spectral data acquired by a single microphone, with remarkable\nresults in classification and retrieval. However, such representations are not\nso robust towards variable environmental sound conditions. We tackle this\ndrawback by exploiting a new multimodal labeled action recognition dataset\nacquired by a hybrid audio-visual sensor that provides RGB video, raw audio\nsignals, and spatialized acoustic data, also known as acoustic images, where\nthe visual and acoustic images are aligned in space and synchronized in time.\nUsing this richer information, we train audio deep learning models in a\nteacher-student fashion. In particular, we distill knowledge into audio\nnetworks from both visual and acoustic image teachers. Our experiments suggest\nthat the learned representations are more powerful and have better\ngeneralization capabilities than the features learned from models trained using\njust single-microphone audio data.", "published": "2019-04-16 19:15:00", "link": "http://arxiv.org/abs/1904.07933v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Expediting TTS Synthesis with Adversarial Vocoding", "abstract": "Recent approaches in text-to-speech (TTS) synthesis employ neural network\nstrategies to vocode perceptually-informed spectrogram representations directly\ninto listenable waveforms. Such vocoding procedures create a computational\nbottleneck in modern TTS pipelines. We propose an alternative approach which\nutilizes generative adversarial networks (GANs) to learn mappings from\nperceptually-informed spectrograms to simple magnitude spectrograms which can\nbe heuristically vocoded. Through a user study, we show that our approach\nsignificantly outperforms na\\\"ive vocoding strategies while being hundreds of\ntimes faster than neural network vocoders used in state-of-the-art TTS systems.\nWe also show that our method can be used to achieve state-of-the-art results in\nunsupervised synthesis of individual words of speech.", "published": "2019-04-16 19:42:43", "link": "http://arxiv.org/abs/1904.07944v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
