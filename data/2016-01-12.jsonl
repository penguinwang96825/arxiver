{"title": "Comparison and Adaptation of Automatic Evaluation Metrics for Quality\n  Assessment of Re-Speaking", "abstract": "Re-speaking is a mechanism for obtaining high quality subtitles for use in\nlive broadcast and other public events. Because it relies on humans performing\nthe actual re-speaking, the task of estimating the quality of the results is\nnon-trivial. Most organisations rely on humans to perform the actual quality\nassessment, but purely automatic methods have been developed for other similar\nproblems, like Machine Translation. This paper will try to compare several of\nthese methods: BLEU, EBLEU, NIST, METEOR, METEOR-PL, TER and RIBES. These will\nthen be matched to the human-derived NER metric, commonly used in re-speaking.", "published": "2016-01-12 10:06:52", "link": "http://arxiv.org/abs/1601.02789v1", "categories": ["cs.CL", "stat.AP", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Learning Hidden Unit Contributions for Unsupervised Acoustic Model\n  Adaptation", "abstract": "This work presents a broad study on the adaptation of neural network acoustic\nmodels by means of learning hidden unit contributions (LHUC) -- a method that\nlinearly re-combines hidden units in a speaker- or environment-dependent manner\nusing small amounts of unsupervised adaptation data. We also extend LHUC to a\nspeaker adaptive training (SAT) framework that leads to a more adaptable DNN\nacoustic model, working both in a speaker-dependent and a speaker-independent\nmanner, without the requirements to maintain auxiliary speaker-dependent\nfeature extractors or to introduce significant speaker-dependent changes to the\nDNN structure. Through a series of experiments on four different speech\nrecognition benchmarks (TED talks, Switchboard, AMI meetings, and Aurora4)\ncomprising 270 test speakers, we show that LHUC in both its test-only and SAT\nvariants results in consistent word error rate reductions ranging from 5% to\n23% relative depending on the task and the degree of mismatch between training\nand test data. In addition, we have investigated the effect of the amount of\nadaptation data per speaker, the quality of unsupervised adaptation targets,\nthe complementarity to other adaptation techniques, one-shot adaptation, and an\nextension to adapting DNNs trained in a sequence discriminative manner.", "published": "2016-01-12 12:33:56", "link": "http://arxiv.org/abs/1601.02828v2", "categories": ["cs.CL", "cs.LG", "cs.SD"], "primary_category": "cs.CL"}
