{"title": "Annotating Derivations: A New Evaluation Strategy and Dataset for\n  Algebra Word Problems", "abstract": "We propose a new evaluation for automatic solvers for algebra word problems,\nwhich can identify mistakes that existing evaluations overlook. Our proposal is\nto evaluate such solvers using derivations, which reflect how an equation\nsystem was constructed from the word problem. To accomplish this, we develop an\nalgorithm for checking the equivalence between two derivations, and show how\nderivation an- notations can be semi-automatically added to existing datasets.\nTo make our experiments more comprehensive, we include the derivation\nannotation for DRAW-1K, a new dataset containing 1000 general algebra word\nproblems. In our experiments, we found that the annotated derivations enable a\nmore accurate evaluation of automatic solvers than previously used metrics. We\nrelease derivation annotations for over 2300 algebra word problems for future\nevaluations.", "published": "2016-09-23 00:38:59", "link": "http://arxiv.org/abs/1609.07197v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Multi-Task Learning with Shared Memory", "abstract": "Neural network based models have achieved impressive results on various\nspecific tasks. However, in previous works, most models are learned separately\nbased on single-task supervised objectives, which often suffer from\ninsufficient training data. In this paper, we propose two deep architectures\nwhich can be trained jointly on multiple related tasks. More specifically, we\naugment neural model with an external memory, which is shared by several tasks.\nExperiments on two groups of text classification tasks show that our proposed\narchitectures can improve the performance of a task with the help of other\nrelated tasks.", "published": "2016-09-23 03:35:27", "link": "http://arxiv.org/abs/1609.07222v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AMR-to-text generation as a Traveling Salesman Problem", "abstract": "The task of AMR-to-text generation is to generate grammatical text that\nsustains the semantic meaning for a given AMR graph. We at- tack the task by\nfirst partitioning the AMR graph into smaller fragments, and then generating\nthe translation for each fragment, before finally deciding the order by solving\nan asymmetric generalized traveling salesman problem (AGTSP). A Maximum Entropy\nclassifier is trained to estimate the traveling costs, and a TSP solver is used\nto find the optimized solution. The final model reports a BLEU score of 22.44\non the SemEval-2016 Task8 dataset.", "published": "2016-09-23 18:12:12", "link": "http://arxiv.org/abs/1609.07451v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Relation Paths in Neural Relation Extraction", "abstract": "Distantly supervised relation extraction has been widely used to find novel\nrelational facts from plain text. To predict the relation between a pair of two\ntarget entities, existing methods solely rely on those direct sentences\ncontaining both entities. In fact, there are also many sentences containing\nonly one of the target entities, which provide rich and useful information for\nrelation extraction. To address this issue, we build inference chains between\ntwo target entities via intermediate entities, and propose a path-based neural\nrelation extraction model to encode the relational semantics from both direct\nsentences and inference chains. Experimental results on real-world datasets\nshow that, our model can make full use of those sentences containing only one\ntarget entity, and achieves significant and consistent improvements on relation\nextraction as compared with baselines. The source code of this paper can be\nobtained from https: //github.com/thunlp/PathNRE.", "published": "2016-09-23 19:59:51", "link": "http://arxiv.org/abs/1609.07479v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A New Statistic Feature of the Short-Time Amplitude Spectrum Values for\n  Human's Unvoiced Pronunciation", "abstract": "In this paper, a new statistic feature of the discrete short-time amplitude\nspectrum is discovered by experiments for the signals of unvoiced\npronunciation. For the random-varying short-time spectrum, this feature reveals\nthe relationship between the amplitude's average and its standard for every\nfrequency component. On the other hand, the association between the amplitude\ndistributions for different frequency components is also studied. A new model\nrepresenting such association is inspired by the normalized histogram of\namplitude. By mathematical analysis, the new statistic feature discovered is\nproved to be necessary evidence which supports the proposed model, and also can\nbe direct evidence for the widely used hypothesis of \"identical distribution of\namplitude for all frequencies\".", "published": "2016-09-23 07:03:32", "link": "http://arxiv.org/abs/1609.07245v2", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
{"title": "Language as a Latent Variable: Discrete Generative Models for Sentence\n  Compression", "abstract": "In this work we explore deep generative models of text in which the latent\nrepresentation of a document is itself drawn from a discrete language model\ndistribution. We formulate a variational auto-encoder for inference in this\nmodel and apply it to the task of compressing sentences. In this application\nthe generative model first draws a latent summary sentence from a background\nlanguage model, and then subsequently draws the observed sentence conditioned\non this latent summary. In our empirical evaluation we show that generative\nformulations of both abstractive and extractive compression yield\nstate-of-the-art results when trained on a large amount of supervised data.\nFurther, we explore semi-supervised compression scenarios where we show that it\nis possible to achieve performance competitive with previously proposed\nsupervised models while training on a fraction of the supervised data.", "published": "2016-09-23 11:25:41", "link": "http://arxiv.org/abs/1609.07317v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Speaker Recognition for Children's Speech", "abstract": "This paper presents results on Speaker Recognition (SR) for children's\nspeech, using the OGI Kids corpus and GMM-UBM and GMM-SVM SR systems. Regions\nof the spectrum containing important speaker information for children are\nidentified by conducting SR experiments over 21 frequency bands. As for adults,\nthe spectrum can be split into four regions, with the first (containing primary\nvocal tract resonance information) and third (corresponding to high frequency\nspeech sounds) being most useful for SR. However, the frequencies at which\nthese regions occur are from 11% to 38% higher for children. It is also noted\nthat subband SR rates are lower for younger children. Finally results are\npresented of SR experiments to identify a child in a class (30 children,\nsimilar age) and school (288 children, varying ages). Class performance depends\non age, with accuracy varying from 90% for young children to 99% for older\nchildren. The identification rate achieved for a child in a school is 81%.", "published": "2016-09-23 20:03:14", "link": "http://arxiv.org/abs/1609.07498v1", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
