{"title": "Improving Non-autoregressive Translation Quality with Pretrained\n  Language Model, Embedding Distillation and Upsampling Strategy for CTC", "abstract": "Non-autoregressive approaches aim to improve the inference speed of\ntranslation models, particularly those that generate output in a one-pass\nforward manner. However, these approaches often suffer from a significant drop\nin translation quality compared to autoregressive models. This paper introduces\na series of innovative techniques to enhance the translation quality of\nNon-Autoregressive Translation (NAT) models while maintaining a substantial\nacceleration in inference speed. We propose fine-tuning Pretrained Multilingual\nLanguage Models (PMLMs) with the CTC loss to train NAT models effectively.\nFurthermore, we adopt the MASK insertion scheme for up-sampling instead of\ntoken duplication, and we present an embedding distillation method to further\nenhance performance. In our experiments, our model outperforms the baseline\nautoregressive model (Transformer \\textit{base}) on multiple datasets,\nincluding WMT'14 DE$\\leftrightarrow$EN, WMT'16 RO$\\leftrightarrow$EN, and\nIWSLT'14 DE$\\leftrightarrow$EN. Notably, our model achieves better performance\nthan the baseline autoregressive model on the IWSLT'14 En$\\leftrightarrow$De\nand WMT'16 En$\\leftrightarrow$Ro datasets, even without using distillation data\nduring training. It is worth highlighting that on the IWSLT'14\nDE$\\rightarrow$EN dataset, our model achieves an impressive BLEU score of\n39.59, setting a new state-of-the-art performance. Additionally, our model\nexhibits a remarkable speed improvement of 16.35 times compared to the\nautoregressive model.", "published": "2023-06-10 05:24:29", "link": "http://arxiv.org/abs/2306.06345v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Review of State-of-The-Art Methods for Java Code\n  Generation from Natural Language Text", "abstract": "Java Code Generation consists in generating automatically Java code from a\nNatural Language Text. This NLP task helps in increasing programmers'\nproductivity by providing them with immediate solutions to the simplest and\nmost repetitive tasks. Code generation is a challenging task because of the\nhard syntactic rules and the necessity of a deep understanding of the semantic\naspect of the programming language. Many works tried to tackle this task using\neither RNN-based, or Transformer-based models. The latter achieved remarkable\nadvancement in the domain and they can be divided into three groups: (1)\nencoder-only models, (2) decoder-only models, and (3) encoder-decoder models.\nIn this paper, we provide a comprehensive review of the evolution and progress\nof deep learning models in Java code generation task. We focus on the most\nimportant methods and present their merits and limitations, as well as the\nobjective functions used by the community. In addition, we provide a detailed\ndescription of datasets and evaluation metrics used in the literature. Finally,\nwe discuss results of different models on CONCODE dataset, then propose some\nfuture directions.", "published": "2023-06-10 07:27:51", "link": "http://arxiv.org/abs/2306.06371v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation", "abstract": "Neural machine translation has achieved promising results on many translation\ntasks. However, previous studies have shown that neural models induce a\nnon-smooth representation space, which harms its generalization results.\nRecently, kNN-MT has provided an effective paradigm to smooth the prediction\nbased on neighbor representations during inference. Despite promising results,\nkNN-MT usually requires large inference overhead. We propose an effective\ntraining framework INK to directly smooth the representation space via\nadjusting representations of kNN neighbors with a small number of new\nparameters. The new parameters are then used to refresh the whole\nrepresentation datastore to get new kNN knowledge asynchronously. This loop\nkeeps running until convergence. Experiments on four benchmark datasets show\nthat \\method achieves average gains of 1.99 COMET and 1.0 BLEU, outperforming\nthe state-of-the-art kNN-MT system with 0.02x memory space and 1.9x inference\nspeedup.", "published": "2023-06-10 08:39:16", "link": "http://arxiv.org/abs/2306.06381v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Training For Low-Resource Disfluency Correction", "abstract": "Disfluencies commonly occur in conversational speech. Speech with\ndisfluencies can result in noisy Automatic Speech Recognition (ASR)\ntranscripts, which affects downstream tasks like machine translation. In this\npaper, we propose an adversarially-trained sequence-tagging model for\nDisfluency Correction (DC) that utilizes a small amount of labeled real\ndisfluent data in conjunction with a large amount of unlabeled data. We show\nthe benefit of our proposed technique, which crucially depends on synthetically\ngenerated disfluent data, by evaluating it for DC in three Indian languages-\nBengali, Hindi, and Marathi (all from the Indo-Aryan family). Our technique\nalso performs well in removing stuttering disfluencies in ASR transcripts\nintroduced by speech impairments. We achieve an average 6.15 points improvement\nin F1-score over competitive baselines across all three languages mentioned. To\nthe best of our knowledge, we are the first to utilize adversarial training for\nDC and use it to correct stuttering disfluencies in English, establishing a new\nbenchmark for this task.", "published": "2023-06-10 08:58:53", "link": "http://arxiv.org/abs/2306.06384v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Boosting Language Models Reasoning with Chain-of-Knowledge Prompting", "abstract": "Recently, Chain-of-Thought (CoT) prompting has delivered success on complex\nreasoning tasks, which aims at designing a simple prompt like ``Let's think\nstep by step'' or multiple in-context exemplars with well-designed rationales\nto elicit Large Language Models (LLMs) to generate intermediate reasoning\nsteps. However, the generated rationales often come with mistakes, making\nunfactual and unfaithful reasoning chains. To mitigate this brittleness, we\npropose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting\nLLMs to generate explicit pieces of knowledge evidence in the form of structure\ntriple. This is inspired by our human behaviors, i.e., we can draw a mind map\nor knowledge map as the reasoning evidence in the brain before answering a\ncomplex question. Benefiting from CoK, we additionally introduce a\nF^2-Verification method to estimate the reliability of the reasoning chains in\nterms of factuality and faithfulness. For the unreliable response, the wrong\nevidence can be indicated to prompt the LLM to rethink. Extensive experiments\ndemonstrate that our method can further improve the performance of commonsense,\nfactual, symbolic, and arithmetic reasoning tasks.", "published": "2023-06-10 12:42:36", "link": "http://arxiv.org/abs/2306.06427v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ORGAN: Observation-Guided Radiology Report Generation via Tree Reasoning", "abstract": "This paper explores the task of radiology report generation, which aims at\ngenerating free-text descriptions for a set of radiographs. One significant\nchallenge of this task is how to correctly maintain the consistency between the\nimages and the lengthy report. Previous research explored solving this issue\nthrough planning-based methods, which generate reports only based on high-level\nplans. However, these plans usually only contain the major observations from\nthe radiographs (e.g., lung opacity), lacking much necessary information, such\nas the observation characteristics and preliminary clinical diagnoses. To\naddress this problem, the system should also take the image information into\naccount together with the textual plan and perform stronger reasoning during\nthe generation process. In this paper, we propose an observation-guided\nradiology report generation framework (ORGAN). It first produces an observation\nplan and then feeds both the plan and radiographs for report generation, where\nan observation graph and a tree reasoning mechanism are adopted to precisely\nenrich the plan information by capturing the multi-formats of each observation.\nExperimental results demonstrate that our framework outperforms previous\nstate-of-the-art methods regarding text quality and clinical efficacy", "published": "2023-06-10 15:36:04", "link": "http://arxiv.org/abs/2306.06466v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Structural Similarities between Documents for Coherence\n  Assessment with Graph Convolutional Networks", "abstract": "Coherence is an important aspect of text quality, and various approaches have\nbeen applied to coherence modeling. However, existing methods solely focus on a\nsingle document's coherence patterns, ignoring the underlying correlation\nbetween documents. We investigate a GCN-based coherence model that is capable\nof capturing structural similarities between documents. Our model first creates\na graph structure for each document, from where we mine different subgraph\npatterns. We then construct a heterogeneous graph for the training corpus,\nconnecting documents based on their shared subgraphs. Finally, a GCN is applied\nto the heterogeneous graph to model the connectivity relationships. We evaluate\nour method on two tasks, assessing discourse coherence and automated essay\nscoring. Results show that our GCN-based model outperforms all baselines,\nachieving a new state-of-the-art on both tasks.", "published": "2023-06-10 16:08:47", "link": "http://arxiv.org/abs/2306.06472v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Annotation-Inspired Implicit Discourse Relation Classification with\n  Auxiliary Discourse Connective Generation", "abstract": "Implicit discourse relation classification is a challenging task due to the\nabsence of discourse connectives. To overcome this issue, we design an\nend-to-end neural model to explicitly generate discourse connectives for the\ntask, inspired by the annotation process of PDTB. Specifically, our model\njointly learns to generate discourse connectives between arguments and predict\ndiscourse relations based on the arguments and the generated connectives. To\nprevent our relation classifier from being misled by poor connectives generated\nat the early stage of training while alleviating the discrepancy between\ntraining and inference, we adopt Scheduled Sampling to the joint learning. We\nevaluate our method on three benchmarks, PDTB 2.0, PDTB 3.0, and PCC. Results\nshow that our joint model significantly outperforms various baselines on three\ndatasets, demonstrating its superiority for the task.", "published": "2023-06-10 16:38:46", "link": "http://arxiv.org/abs/2306.06480v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Arabic Multimodal Dataset for Sentiment Analysis", "abstract": "Multimodal Sentiment Analysis (MSA) has recently become a centric research\ndirection for many real-world applications. This proliferation is due to the\nfact that opinions are central to almost all human activities and are key\ninfluencers of our behaviors. In addition, the recent deployment of Deep\nLearning-based (DL) models has proven their high efficiency for a wide range of\nWestern languages. In contrast, Arabic DL-based multimodal sentiment analysis\n(MSA) is still in its infantile stage due, mainly, to the lack of standard\ndatasets. In this paper, our investigation is twofold. First, we design a\npipeline that helps building our Arabic Multimodal dataset leveraging both\nstate-of-the-art transformers and feature extraction tools within word\nalignment techniques. Thereafter, we validate our dataset using\nstate-of-the-art transformer-based model dealing with multimodality. Despite\nthe small size of the outcome dataset, experiments show that Arabic\nmultimodality is very promising", "published": "2023-06-10 00:13:09", "link": "http://arxiv.org/abs/2306.06322v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Investigating the Effectiveness of ChatGPT in Mathematical Reasoning and\n  Problem Solving: Evidence from the Vietnamese National High School Graduation\n  Examination", "abstract": "This study offers a complete analysis of ChatGPT's mathematics abilities in\nresponding to multiple-choice questions for the Vietnamese National High School\nGraduation Examination (VNHSGE) on a range of subjects and difficulty levels.\nThe dataset included 250 questions divided into four levels: knowledge (K),\ncomprehension (C), application (A), and high application (H), and it included\nten themes that covered diverse mathematical concepts. The outcomes demonstrate\nthat ChatGPT's performance varies depending on the difficulty level and\nsubject. It performed best on questions at Level (K), with an accuracy rate of\n$83\\%$; but, as the difficulty level rose, it scored poorly, with an accuracy\nrate of $10\\%$. The study has also shown that ChatGPT significantly succeeds in\nproviding responses to questions on subjects including exponential and\nlogarithmic functions, geometric progression, and arithmetic progression. The\nstudy found that ChatGPT had difficulty correctly answering questions on topics\nincluding derivatives and applications, spatial geometry, and Oxyz spatial\ncalculus. Additionally, this study contrasted ChatGPT outcomes with Vietnamese\nstudents in VNHSGE and in other math competitions. ChatGPT dominated in the SAT\nMath competition with a success rate of $70\\%$, followed by VNHSGE mathematics\n($58.8\\%)$. However, its success rates were lower on other exams, such as AP\nStatistics, the GRE Quantitative, AMC 10, AMC 12, and AP Calculus BC. These\nresults suggest that ChatGPT has the potential to be an effective teaching tool\nfor mathematics, but more work is needed to enhance its handling of graphical\ndata and address the challenges presented by questions that are getting more\nchallenging.", "published": "2023-06-10 02:01:02", "link": "http://arxiv.org/abs/2306.06331v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OpenSR: Open-Modality Speech Recognition via Maintaining Multi-Modality\n  Alignment", "abstract": "Speech Recognition builds a bridge between the multimedia streaming\n(audio-only, visual-only or audio-visual) and the corresponding text\ntranscription. However, when training the specific model of new domain, it\noften gets stuck in the lack of new-domain utterances, especially the labeled\nvisual utterances. To break through this restriction, we attempt to achieve\nzero-shot modality transfer by maintaining the multi-modality alignment in\nphoneme space learned with unlabeled multimedia utterances in the high resource\ndomain during the pre-training \\cite{shi2022learning}, and propose a training\nsystem Open-modality Speech Recognition (\\textbf{OpenSR}) that enables the\nmodels trained on a single modality (e.g., audio-only) applicable to more\nmodalities (e.g., visual-only and audio-visual). Furthermore, we employ a\ncluster-based prompt tuning strategy to handle the domain shift for the\nscenarios with only common words in the new domain utterances. We demonstrate\nthat OpenSR enables modality transfer from one to any in three different\nsettings (zero-, few- and full-shot), and achieves highly competitive zero-shot\nperformance compared to the existing few-shot and full-shot lip-reading\nmethods. To the best of our knowledge, OpenSR achieves the state-of-the-art\nperformance of word error rate in LRS2 on audio-visual speech recognition and\nlip-reading with 2.7\\% and 25.0\\%, respectively. The code and demo are\navailable at https://github.com/Exgc/OpenSR.", "published": "2023-06-10 11:04:10", "link": "http://arxiv.org/abs/2306.06410v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Modality Influence in Multimodal Machine Learning", "abstract": "Multimodal Machine Learning has emerged as a prominent research direction\nacross various applications such as Sentiment Analysis, Emotion Recognition,\nMachine Translation, Hate Speech Recognition, and Movie Genre Classification.\nThis approach has shown promising results by utilizing modern deep learning\narchitectures. Despite the achievements made, challenges remain in data\nrepresentation, alignment techniques, reasoning, generation, and quantification\nwithin multimodal learning. Additionally, assumptions about the dominant role\nof textual modality in decision-making have been made. However, limited\ninvestigations have been conducted on the influence of different modalities in\nMultimodal Machine Learning systems. This paper aims to address this gap by\nstudying the impact of each modality on multimodal learning tasks. The research\nfocuses on verifying presumptions and gaining insights into the usage of\ndifferent modalities. The main contribution of this work is the proposal of a\nmethodology to determine the effect of each modality on several Multimodal\nMachine Learning models and datasets from various tasks. Specifically, the\nstudy examines Multimodal Sentiment Analysis, Multimodal Emotion Recognition,\nMultimodal Hate Speech Recognition, and Multimodal Disease Detection. The study\nobjectives include training SOTA MultiModal Machine Learning models with masked\nmodalities to evaluate their impact on performance. Furthermore, the research\naims to identify the most influential modality or set of modalities for each\ntask and draw conclusions for diverse multimodal classification tasks. By\nundertaking these investigations, this research contributes to a better\nunderstanding of the role of individual modalities in multi-modal learning and\nprovides valuable insights for future advancements in this field.", "published": "2023-06-10 16:28:52", "link": "http://arxiv.org/abs/2306.06476v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Low Resource NER Using Assisting Language And Transfer\n  Learning", "abstract": "Named Entity Recognition (NER) is a fundamental task in NLP that is used to\nlocate the key information in text and is primarily applied in conversational\nand search systems. In commercial applications, NER or comparable slot-filling\nmethods have been widely deployed for popular languages. NER is used in\napplications such as human resources, customer service, search engines, content\nclassification, and academia. In this paper, we draw focus on identifying name\nentities for low-resource Indian languages that are closely related, like Hindi\nand Marathi. We use various adaptations of BERT such as baseBERT, AlBERT, and\nRoBERTa to train a supervised NER model. We also compare multilingual models\nwith monolingual models and establish a baseline. In this work, we show the\nassisting capabilities of the Hindi and Marathi languages for the NER task. We\nshow that models trained using multiple languages perform better than a single\nlanguage. However, we also observe that blind mixing of all datasets doesn't\nnecessarily provide improvements and data selection methods may be required.", "published": "2023-06-10 16:31:04", "link": "http://arxiv.org/abs/2306.06477v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Human-in-the-Loop through Chain-of-Thought", "abstract": "While the emergence of powerful language models along with Chain-of-thought\nprompting has made automation more and more omnipresent, it sometimes\ndemonstrates its weakness in long-term or multi-step logical reasoning. For\nexample, users don't always get desirable answers for complex mathematical\nproblems without human involvement. Against this background, we present the\nManual Correction System (MCS) -- a human-in-the-loop system enhanced by\nChain-of-Thought prompting, which explores how manual correction of sub-logics\nin rationales can improve LLM's reasoning performance. Moving one step forward,\nconsidering a system with human-in-the-loop involves more than having humans\nimprove performance but also controlling the cost. Therefore, we post a\nCost-utility Analysis Model for Human-in-the-Loop systems (CAMLOP) based on\nclassical economics theory to analyze, quantify and balance the utility and the\ncorresponding cost. We conduct experiments of MCS and CAMLOP with twelve\ndatasets. A significant advantage w.r.t cost and utility proves its superiority\nover strong baselines.", "published": "2023-06-10 04:31:57", "link": "http://arxiv.org/abs/2306.07932v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Universal Language Modelling agent", "abstract": "Large Language Models are designed to understand complex Human Language. Yet,\nUnderstanding of animal language has long intrigued researchers striving to\nbridge the communication gap between humans and other species. This research\npaper introduces a novel approach that draws inspiration from the linguistic\nconcepts found in the Quran, a revealed Holy Arabic scripture dating back 1400\nyears. By exploring the linguistic structure of the Quran, specifically the\ncomponents of ism, fil, and harf, we aim to unlock the underlying intentions\nand meanings embedded within animal conversations using audio data. To unravel\nthe intricate complexities of animal language, we employ word embedding\ntechniques to analyze each distinct frequency component. This methodology\nenables the identification of potential correlations and the extraction of\nmeaningful insights from the data. Furthermore, we leverage a bioacoustics\nmodel to generate audio, which serves as a valuable resource for training\nnatural language processing (NLP) techniques. This Paper aims to find the\nintention* behind animal language rather than having each word translation.", "published": "2023-06-10 21:09:16", "link": "http://arxiv.org/abs/2306.06521v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What Can an Accent Identifier Learn? Probing Phonetic and Prosodic\n  Information in a Wav2vec2-based Accent Identification Model", "abstract": "This study is focused on understanding and quantifying the change in phoneme\nand prosody information encoded in the Self-Supervised Learning (SSL) model,\nbrought by an accent identification (AID) fine-tuning task. This problem is\naddressed based on model probing. Specifically, we conduct a systematic\nlayer-wise analysis of the representations of the Transformer layers on a\nphoneme correlation task, and a novel word-level prosody prediction task. We\ncompare the probing performance of the pre-trained and fine-tuned SSL models.\nResults show that the AID fine-tuning task steers the top 2 layers to learn\nricher phoneme and prosody representation. These changes share some\nsimilarities with the effects of fine-tuning with an Automatic Speech\nRecognition task. In addition, we observe strong accent-specific phoneme\nrepresentations in layer 9. To sum up, this study provides insights into the\nunderstanding of SSL features and their interactions with fine-tuning tasks.", "published": "2023-06-10 21:20:47", "link": "http://arxiv.org/abs/2306.06524v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AutoTAMP: Autoregressive Task and Motion Planning with LLMs as\n  Translators and Checkers", "abstract": "For effective human-robot interaction, robots need to understand, plan, and\nexecute complex, long-horizon tasks described by natural language. Recent\nadvances in large language models (LLMs) have shown promise for translating\nnatural language into robot action sequences for complex tasks. However,\nexisting approaches either translate the natural language directly into robot\ntrajectories or factor the inference process by decomposing language into task\nsub-goals and relying on a motion planner to execute each sub-goal. When\ncomplex environmental and temporal constraints are involved, inference over\nplanning tasks must be performed jointly with motion plans using traditional\ntask-and-motion planning (TAMP) algorithms, making factorization into subgoals\nuntenable. Rather than using LLMs to directly plan task sub-goals, we instead\nperform few-shot translation from natural language task descriptions to an\nintermediate task representation that can then be consumed by a TAMP algorithm\nto jointly solve the task and motion plan. To improve translation, we\nautomatically detect and correct both syntactic and semantic errors via\nautoregressive re-prompting, resulting in significant improvements in task\ncompletion. We show that our approach outperforms several methods using LLMs as\nplanners in complex task domains. See our project website\nhttps://yongchao98.github.io/MIT-REALM-AutoTAMP/ for prompts, videos, and code.", "published": "2023-06-10 21:58:29", "link": "http://arxiv.org/abs/2306.06531v3", "categories": ["cs.RO", "cs.CL", "cs.HC"], "primary_category": "cs.RO"}
{"title": "Medical Data Augmentation via ChatGPT: A Case Study on Medication\n  Identification and Medication Event Classification", "abstract": "The identification of key factors such as medications, diseases, and\nrelationships within electronic health records and clinical notes has a wide\nrange of applications in the clinical field. In the N2C2 2022 competitions,\nvarious tasks were presented to promote the identification of key factors in\nelectronic health records (EHRs) using the Contextualized Medication Event\nDataset (CMED). Pretrained large language models (LLMs) demonstrated\nexceptional performance in these tasks. This study aims to explore the\nutilization of LLMs, specifically ChatGPT, for data augmentation to overcome\nthe limited availability of annotated data for identifying the key factors in\nEHRs. Additionally, different pre-trained BERT models, initially trained on\nextensive datasets like Wikipedia and MIMIC, were employed to develop models\nfor identifying these key variables in EHRs through fine-tuning on augmented\ndatasets. The experimental results of two EHR analysis tasks, namely medication\nidentification and medication event classification, indicate that data\naugmentation based on ChatGPT proves beneficial in improving performance for\nboth medication identification and medication event classification.", "published": "2023-06-10 20:55:21", "link": "http://arxiv.org/abs/2306.07297v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semi-supervsied Learning-based Sound Event Detection using Freuqency\n  Dynamic Convolution with Large Kernel Attention for DCASE Challenge 2023 Task\n  4", "abstract": "This report proposes a frequency dynamic convolution (FDY) with a large\nkernel attention (LKA)-convolutional recurrent neural network (CRNN) with a\npre-trained bidirectional encoder representation from audio transformers\n(BEATs) embedding-based sound event detection (SED) model that employs a\nmean-teacher and pseudo-label approach to address the challenge of limited\nlabeled data for DCASE 2023 Task 4. The proposed FDY with LKA integrates the\nFDY and LKA module to effectively capture time-frequency patterns, long-term\ndependencies, and high-level semantic information in audio signals. The\nproposed FDY with LKA-CRNN with a BEATs embedding network is initially trained\non the entire DCASE 2023 Task 4 dataset using the mean-teacher approach,\ngenerating pseudo-labels for weakly labeled, unlabeled, and the AudioSet.\nSubsequently, the proposed SED model is retrained using the same pseudo-label\napproach. A subset of these models is selected for submission, demonstrating\nsuperior F1-scores and polyphonic SED score performance on the DCASE 2023\nChallenge Task 4 validation dataset.", "published": "2023-06-10 15:05:11", "link": "http://arxiv.org/abs/2306.06461v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio-Visual Speech Enhancement With Selective Off-Screen Speech\n  Extraction", "abstract": "This paper describes an audio-visual speech enhancement (AV-SE) method that\nestimates from noisy input audio a mixture of the speech of the speaker\nappearing in an input video (on-screen target speech) and of a selected speaker\nnot appearing in the video (off-screen target speech). Although conventional\nAV-SE methods have suppressed all off-screen sounds, it is necessary to listen\nto a specific pre-known speaker's speech (e.g., family member's voice and\nannouncements in stations) in future applications of AV-SE (e.g., hearing\naids), even when users' sight does not capture the speaker. To overcome this\nlimitation, we extract a visual clue for the on-screen target speech from the\ninput video and a voiceprint clue for the off-screen one from a pre-recorded\nspeech of the speaker. Two clues from different domains are integrated as an\naudio-visual clue, and the proposed model directly estimates the target\nmixture. To improve the estimation accuracy, we introduce a temporal attention\nmechanism for the voiceprint clue and propose a training strategy called the\nmuting strategy. Experimental results show that our method outperforms a\nbaseline method that uses the state-of-the-art AV-SE and speaker extraction\nmethods individually in terms of estimation accuracy and computational\nefficiency.", "published": "2023-06-10 17:37:22", "link": "http://arxiv.org/abs/2306.06495v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Vocoder-Free Non-Parallel Conversion of Whispered Speech With Masked\n  Cycle-Consistent Generative Adversarial Networks", "abstract": "Cycle-consistent generative adversarial networks have been widely used in\nnon-parallel voice conversion (VC). Their ability to learn mappings between\nsource and target features without relying on parallel training data eliminates\nthe need for temporal alignments. However, most methods decouple the conversion\nof acoustic features from synthesizing the audio signal by using separate\nmodels for conversion and waveform synthesis. This work unifies conversion and\nsynthesis into a single model, thereby eliminating the need for a separate\nvocoder. By leveraging cycle-consistent training and a self-supervised\nauxiliary training task, our model is able to efficiently generate converted\nhigh-quality raw audio waveforms. Subjective listening tests show that our\nmethod outperforms the baseline in whispered speech conversion (up to 6.7%\nrelative improvement), and mean opinion score predictions yield competitive\nresults in conventional VC (between 0.5% and 2.4% relative improvement).", "published": "2023-06-10 19:33:05", "link": "http://arxiv.org/abs/2306.06514v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
