{"title": "A Masked Segmental Language Model for Unsupervised Natural Language\n  Segmentation", "abstract": "Segmentation remains an important preprocessing step both in languages where\n\"words\" or other important syntactic/semantic units (like morphemes) are not\nclearly delineated by white space, as well as when dealing with continuous\nspeech data, where there is often no meaningful pause between words.\nNear-perfect supervised methods have been developed for use in resource-rich\nlanguages such as Chinese, but many of the world's languages are both\nmorphologically complex, and have no large dataset of \"gold\" segmentations into\nmeaningful units. To solve this problem, we propose a new type of Segmental\nLanguage Model (Sun and Deng, 2018; Kawakami et al., 2019; Wang et al., 2021)\nfor use in both unsupervised and lightly supervised segmentation tasks. We\nintroduce a Masked Segmental Language Model (MSLM) built on a span-masking\ntransformer architecture, harnessing the power of a bi-directional masked\nmodeling context and attention. In a series of experiments, our model\nconsistently outperforms Recurrent SLMs on Chinese (PKU Corpus) in segmentation\nquality, and performs similarly to the Recurrent model on English (PTB). We\nconclude by discussing the different challenges posed in segmenting\nphonemic-type writing systems.", "published": "2021-04-16 00:00:05", "link": "http://arxiv.org/abs/2104.07829v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Human-like informative conversations: Better acknowledgements using\n  conditional mutual information", "abstract": "This work aims to build a dialogue agent that can weave new factual content\ninto conversations as naturally as humans. We draw insights from linguistic\nprinciples of conversational analysis and annotate human-human conversations\nfrom the Switchboard Dialog Act Corpus to examine humans strategies for\nacknowledgement, transition, detail selection and presentation. When current\nchatbots (explicitly provided with new factual content) introduce facts into a\nconversation, their generated responses do not acknowledge the prior turns.\nThis is because models trained with two contexts - new factual content and\nconversational history - generate responses that are non-specific w.r.t. one of\nthe contexts, typically the conversational history. We show that specificity\nw.r.t. conversational history is better captured by Pointwise Conditional\nMutual Information ($\\text{pcmi}_h$) than by the established use of Pointwise\nMutual Information ($\\text{pmi}$). Our proposed method, Fused-PCMI, trades off\n$\\text{pmi}$ for $\\text{pcmi}_h$ and is preferred by humans for overall quality\nover the Max-PMI baseline 60% of the time. Human evaluators also judge\nresponses with higher $\\text{pcmi}_h$ better at acknowledgement 74% of the\ntime. The results demonstrate that systems mimicking human conversational\ntraits (in this case acknowledgement) improve overall quality and more broadly\nillustrate the utility of linguistic principles in improving dialogue agents.", "published": "2021-04-16 00:13:57", "link": "http://arxiv.org/abs/2104.07831v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tracing Topic Transitions with Temporal Graph Clusters", "abstract": "Twitter serves as a data source for many Natural Language Processing (NLP)\ntasks. It can be challenging to identify topics on Twitter due to continuous\nupdating data stream. In this paper, we present an unsupervised graph based\nframework to identify the evolution of sub-topics within two weeks of\nreal-world Twitter data. We first employ a Markov Clustering Algorithm (MCL)\nwith a node removal method to identify optimal graph clusters from temporal\nGraph-of-Words (GoW). Subsequently, we model the clustering transitions between\nthe temporal graphs to identify the topic evolution. Finally, the transition\nflows generated from both computational approach and human annotations are\ncompared to ensure the validity of our framework.", "published": "2021-04-16 00:55:31", "link": "http://arxiv.org/abs/2104.07836v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Entity Alignment with Adversarial Kernel Embedding and\n  Adversarial Knowledge Translation", "abstract": "Cross-lingual entity alignment, which aims to precisely connect the same\nentities in different monolingual knowledge bases (KBs) together, often suffers\nchallenges from feature inconsistency to sequence context unawareness. This\npaper presents a dual adversarial learning framework for cross-lingual entity\nalignment, DAEA, with two original contributions. First, in order to address\nthe structural and attribute feature inconsistency between entities in two\nknowledge graphs (KGs), an adversarial kernel embedding technique is proposed\nto extract graph-invariant information in an unsupervised manner, and project\ntwo KGs into the common embedding space. Second, in order to further improve\nsuccessful rate of entity alignment, we propose to produce multiple random\nwalks through each entity to be aligned and mask these entities in random\nwalks. With the guidance of known aligned entities in the context of multiple\nrandom walks, an adversarial knowledge translation model is developed to fill\nand translate masked entities in pairwise random walks from two KGs. Extensive\nexperiments performed on real-world datasets show that DAEA can well solve the\nfeature inconsistency and sequence context unawareness issues and significantly\noutperforms thirteen state-of-the-art entity alignment methods.", "published": "2021-04-16 00:57:28", "link": "http://arxiv.org/abs/2104.07837v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Failures of Automatic Translation in the Case of\n  Unambiguous Gender", "abstract": "Transformer based models are the modern work horses for neural machine\ntranslation (NMT), reaching state of the art across several benchmarks. Despite\ntheir impressive accuracy, we observe a systemic and rudimentary class of\nerrors made by transformer based models with regards to translating from a\nlanguage that doesn't mark gender on nouns into others that do. We find that\neven when the surrounding context provides unambiguous evidence of the\nappropriate grammatical gender marking, no transformer based model we tested\nwas able to accurately gender occupation nouns systematically. We release an\nevaluation scheme and dataset for measuring the ability of transformer based\nNMT models to translate gender morphology correctly in unambiguous contexts\nacross syntactically diverse sentences. Our dataset translates from an English\nsource into 20 languages from several different language families. With the\navailability of this dataset, our hope is that the NMT community can iterate on\nsolutions for this class of especially egregious errors.", "published": "2021-04-16 00:57:36", "link": "http://arxiv.org/abs/2104.07838v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Classes Clusters?", "abstract": "Sentence embedding models aim to provide general purpose embeddings for\nsentences. Most of the models studied in this paper claim to perform well on\nSTS tasks - but they do not report on their suitability for clustering. This\npaper looks at four recent sentence embedding models (Universal Sentence\nEncoder (Cer et al., 2018), Sentence-BERT (Reimers and Gurevych, 2019), LASER\n(Artetxe and Schwenk, 2019), and DeCLUTR (Giorgi et al., 2020)). It gives a\nbrief overview of the ideas behind their implementations. It then investigates\nhow well topic classes in two text classification datasets (Amazon Reviews (Ni\net al., 2019) and News Category Dataset (Misra, 2018)) map to clusters in their\ncorresponding sentence embedding space. While the performance of the resulting\nclassification model is far from perfect, it is better than random. This is\ninteresting because the classification model has been constructed in an\nunsupervised way. The topic classes in these real life topic classification\ndatasets can be partly reconstructed by clustering the corresponding sentence\nembeddings.", "published": "2021-04-16 01:16:30", "link": "http://arxiv.org/abs/2104.07840v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multivalent Entailment Graphs for Question Answering", "abstract": "Drawing inferences between open-domain natural language predicates is a\nnecessity for true language understanding. There has been much progress in\nunsupervised learning of entailment graphs for this purpose. We make three\ncontributions: (1) we reinterpret the Distributional Inclusion Hypothesis to\nmodel entailment between predicates of different valencies, like DEFEAT(Biden,\nTrump) entails WIN(Biden); (2) we actualize this theory by learning\nunsupervised Multivalent Entailment Graphs of open-domain predicates; and (3)\nwe demonstrate the capabilities of these graphs on a novel question answering\ntask. We show that directional entailment is more helpful for inference than\nbidirectional similarity on questions of fine-grained semantics. We also show\nthat drawing on evidence across valencies answers more questions than by using\nonly the same valency evidence.", "published": "2021-04-16 01:45:40", "link": "http://arxiv.org/abs/2104.07846v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparison of Grammatical Error Correction Using Back-Translation Models", "abstract": "Grammatical error correction (GEC) suffers from a lack of sufficient parallel\ndata. Therefore, GEC studies have developed various methods to generate pseudo\ndata, which comprise pairs of grammatical and artificially produced\nungrammatical sentences. Currently, a mainstream approach to generate pseudo\ndata is back-translation (BT). Most previous GEC studies using BT have employed\nthe same architecture for both GEC and BT models. However, GEC models have\ndifferent correction tendencies depending on their architectures. Thus, in this\nstudy, we compare the correction tendencies of the GEC models trained on pseudo\ndata generated by different BT models, namely, Transformer, CNN, and LSTM. The\nresults confirm that the correction tendencies for each error type are\ndifferent for every BT model. Additionally, we examine the correction\ntendencies when using a combination of pseudo data generated by different BT\nmodels. As a result, we find that the combination of different BT models\nimproves or interpolates the F_0.5 scores of each error type compared with that\nof single BT models with different seeds.", "published": "2021-04-16 01:58:44", "link": "http://arxiv.org/abs/2104.07848v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Segmenting Subtitles for Correcting ASR Segmentation Errors", "abstract": "Typical ASR systems segment the input audio into utterances using purely\nacoustic information, which may not resemble the sentence-like units that are\nexpected by conventional machine translation (MT) systems for Spoken Language\nTranslation. In this work, we propose a model for correcting the acoustic\nsegmentation of ASR models for low-resource languages to improve performance on\ndownstream tasks. We propose the use of subtitles as a proxy dataset for\ncorrecting ASR acoustic segmentation, creating synthetic acoustic utterances by\nmodeling common error modes. We train a neural tagging model for correcting ASR\nacoustic segmentation and show that it improves downstream performance on MT\nand audio-document cross-language information retrieval (CLIR).", "published": "2021-04-16 03:04:10", "link": "http://arxiv.org/abs/2104.07868v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing Across Time: What Does RoBERTa Know and When?", "abstract": "Models of language trained on very large corpora have been demonstrated\nuseful for NLP. As fixed artifacts, they have become the object of intense\nstudy, with many researchers \"probing\" the extent to which linguistic\nabstractions, factual and commonsense knowledge, and reasoning abilities they\nacquire and readily demonstrate. Building on this line of work, we consider a\nnew question: for types of knowledge a language model learns, when during\n(pre)training are they acquired? We plot probing performance across iterations,\nusing RoBERTa as a case study. Among our findings: linguistic knowledge is\nacquired fast, stably, and robustly across domains. Facts and commonsense are\nslower and more domain-sensitive. Reasoning abilities are, in general, not\nstably acquired. As new datasets, pretraining protocols, and probes emerge, we\nbelieve that probing-across-time analyses can help researchers understand the\ncomplex, intermingled learning that these models undergo and guide us toward\nmore efficient approaches that accomplish necessary learning faster.", "published": "2021-04-16 04:26:39", "link": "http://arxiv.org/abs/2104.07885v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comparative Study on Collecting High-Quality Implicit Reasonings at a\n  Large-scale", "abstract": "Explicating implicit reasoning (i.e. warrants) in arguments is a\nlong-standing challenge for natural language understanding systems. While\nrecent approaches have focused on explicating warrants via crowdsourcing or\nexpert annotations, the quality of warrants has been questionable due to the\nextreme complexity and subjectivity of the task. In this paper, we tackle the\ncomplex task of warrant explication and devise various methodologies for\ncollecting warrants. We conduct an extensive study with trained experts to\nevaluate the resulting warrants of each methodology and find that our\nmethodologies allow for high-quality warrants to be collected. We construct a\npreliminary dataset of 6,000 warrants annotated over 600 arguments for 3\ndebatable topics. To facilitate research in related downstream tasks, we\nrelease our guidelines and preliminary dataset.", "published": "2021-04-16 07:03:08", "link": "http://arxiv.org/abs/2104.07924v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimal Size-Performance Tradeoffs: Weighing PoS Tagger Models", "abstract": "Improvement in machine learning-based NLP performance are often presented\nwith bigger models and more complex code. This presents a trade-off: better\nscores come at the cost of larger tools; bigger models tend to require more\nduring training and inference time. We present multiple methods for measuring\nthe size of a model, and for comparing this with the model's performance.\n  In a case study over part-of-speech tagging, we then apply these techniques\nto taggers for eight languages and present a novel analysis identifying which\ntaggers are size-performance optimal. Results indicate that some classical\ntaggers place on the size-performance skyline across languages. Further,\nalthough the deep models have highest performance for multiple scores, it is\noften not the most complex of these that reach peak performance.", "published": "2021-04-16 08:02:56", "link": "http://arxiv.org/abs/2104.07951v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ProphetNet-X: Large-Scale Pre-training Models for English, Chinese,\n  Multi-lingual, Dialog, and Code Generation", "abstract": "Now, the pre-training technique is ubiquitous in natural language processing\nfield. ProphetNet is a pre-training based natural language generation method\nwhich shows powerful performance on English text summarization and question\ngeneration tasks. In this paper, we extend ProphetNet into other domains and\nlanguages, and present the ProphetNet family pre-training models, named\nProphetNet-X, where X can be English, Chinese, Multi-lingual, and so on. We\npre-train a cross-lingual generation model ProphetNet-Multi, a Chinese\ngeneration model ProphetNet-Zh, two open-domain dialog generation models\nProphetNet-Dialog-En and ProphetNet-Dialog-Zh. And also, we provide a PLG\n(Programming Language Generation) model ProphetNet-Code to show the generation\nperformance besides NLG (Natural Language Generation) tasks. In our\nexperiments, ProphetNet-X models achieve new state-of-the-art performance on 10\nbenchmarks. All the models of ProphetNet-X share the same model structure,\nwhich allows users to easily switch between different models. We make the code\nand models publicly available, and we will keep updating more pre-training\nmodels and finetuning scripts.", "published": "2021-04-16 10:00:43", "link": "http://arxiv.org/abs/2104.08006v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cost-effective End-to-end Information Extraction for Semi-structured\n  Document Images", "abstract": "A real-world information extraction (IE) system for semi-structured document\nimages often involves a long pipeline of multiple modules, whose complexity\ndramatically increases its development and maintenance cost. One can instead\nconsider an end-to-end model that directly maps the input to the target output\nand simplify the entire process. However, such generation approach is known to\nlead to unstable performance if not designed carefully. Here we present our\nrecent effort on transitioning from our existing pipeline-based IE system to an\nend-to-end system focusing on practical challenges that are associated with\nreplacing and deploying the system in real, large-scale production. By\ncarefully formulating document IE as a sequence generation task, we show that a\nsingle end-to-end IE system can be built and still achieve competent\nperformance.", "published": "2021-04-16 11:37:39", "link": "http://arxiv.org/abs/2104.08041v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Zero-Shot Multi-Lingual Entity Linking", "abstract": "Entity linking -- the task of identifying references in free text to relevant\nknowledge base representations -- often focuses on single languages. We\nconsider multilingual entity linking, where a single model is trained to link\nreferences to same-language knowledge bases in several languages. We propose a\nneural ranker architecture, which leverages multilingual transformer\nrepresentations of text to be easily applied to a multilingual setting. We then\nexplore how a neural ranker trained in one language (e.g. English) transfers to\nan unseen language (e.g. Chinese), and find that while there is a consistent\nbut not large drop in performance. How can this drop in performance be\nalleviated? We explore adding an adversarial objective to force our model to\nlearn language-invariant representations. We find that using this approach\nimproves recall in several datasets, often matching the in-language\nperformance, thus alleviating some of the performance loss occurring from\nzero-shot transfer.", "published": "2021-04-16 12:50:07", "link": "http://arxiv.org/abs/2104.08082v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LU-BZU at SemEval-2021 Task 2: Word2Vec and Lemma2Vec performance in\n  Arabic Word-in-Context disambiguation", "abstract": "This paper presents a set of experiments to evaluate and compare between the\nperformance of using CBOW Word2Vec and Lemma2Vec models for Arabic\nWord-in-Context (WiC) disambiguation without using sense inventories or sense\nembeddings. As part of the SemEval-2021 Shared Task 2 on WiC disambiguation, we\nused the dev.ar-ar dataset (2k sentence pairs) to decide whether two words in a\ngiven sentence pair carry the same meaning. We used two Word2Vec models:\nWiki-CBOW, a pre-trained model on Arabic Wikipedia, and another model we\ntrained on large Arabic corpora of about 3 billion tokens. Two Lemma2Vec models\nwas also constructed based on the two Word2Vec models. Each of the four models\nwas then used in the WiC disambiguation task, and then evaluated on the\nSemEval-2021 test.ar-ar dataset. At the end, we reported the performance of\ndifferent models and compared between using lemma-based and word-based models.", "published": "2021-04-16 13:38:35", "link": "http://arxiv.org/abs/2104.08110v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Temporal Adaptation of BERT and Performance on Downstream Document\n  Classification: Insights from Social Media", "abstract": "Language use differs between domains and even within a domain, language use\nchanges over time. For pre-trained language models like BERT, domain adaptation\nthrough continued pre-training has been shown to improve performance on\nin-domain downstream tasks. In this article, we investigate whether temporal\nadaptation can bring additional benefits. For this purpose, we introduce a\ncorpus of social media comments sampled over three years. It contains\nunlabelled data for adaptation and evaluation on an upstream masked language\nmodelling task as well as labelled data for fine-tuning and evaluation on a\ndownstream document classification task. We find that temporality matters for\nboth tasks: temporal adaptation improves upstream and temporal fine-tuning\ndownstream task performance. Time-specific models generally perform better on\npast than on future test sets, which matches evidence on the bursty usage of\ntopical words. However, adapting BERT to time and domain does not improve\nperformance on the downstream task over only adapting to domain. Token-level\nanalysis shows that temporal adaptation captures event-driven changes in\nlanguage use in the downstream task, but not those changes that are actually\nrelevant to task performance. Based on our findings, we discuss when temporal\nadaptation may be more effective.", "published": "2021-04-16 13:48:53", "link": "http://arxiv.org/abs/2104.08116v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Variable-Length Textual Adversarial Attacks", "abstract": "Adversarial attacks have shown the vulnerability of machine learning models,\nhowever, it is non-trivial to conduct textual adversarial attacks on natural\nlanguage processing tasks due to the discreteness of data. Most previous\napproaches conduct attacks with the atomic \\textit{replacement} operation,\nwhich usually leads to fixed-length adversarial examples and therefore limits\nthe exploration on the decision space. In this paper, we propose\nvariable-length textual adversarial attacks~(VL-Attack) and integrate three\natomic operations, namely \\textit{insertion}, \\textit{deletion} and\n\\textit{replacement}, into a unified framework, by introducing and manipulating\na special \\textit{blank} token while attacking. In this way, our approach is\nable to more comprehensively find adversarial examples around the decision\nboundary and effectively conduct adversarial attacks. Specifically, our method\ndrops the accuracy of IMDB classification by $96\\%$ with only editing $1.3\\%$\ntokens while attacking a pre-trained BERT model. In addition, fine-tuning the\nvictim model with generated adversarial samples can improve the robustness of\nthe model without hurting the performance, especially for length-sensitive\nmodels. On the task of non-autoregressive machine translation, our method can\nachieve $33.18$ BLEU score on IWSLT14 German-English translation, achieving an\nimprovement of $1.47$ over the baseline model.", "published": "2021-04-16 14:37:27", "link": "http://arxiv.org/abs/2104.08139v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Back to Square One: Artifact Detection, Training and Commonsense\n  Disentanglement in the Winograd Schema", "abstract": "The Winograd Schema (WS) has been proposed as a test for measuring\ncommonsense capabilities of models. Recently, pre-trained language model-based\napproaches have boosted performance on some WS benchmarks but the source of\nimprovement is still not clear. This paper suggests that the apparent progress\non WS may not necessarily reflect progress in commonsense reasoning. To support\nthis claim, we first show that the current evaluation method of WS is\nsub-optimal and propose a modification that uses twin sentences for evaluation.\nWe also propose two new baselines that indicate the existence of artifacts in\nWS benchmarks. We then develop a method for evaluating WS-like sentences in a\nzero-shot setting to account for the commonsense reasoning abilities acquired\nduring the pretraining and observe that popular language models perform\nrandomly in this setting when using our more strict evaluation. We conclude\nthat the observed progress is mostly due to the use of supervision in training\nWS models, which is not likely to successfully support all the required\ncommonsense reasoning skills and knowledge.", "published": "2021-04-16 15:17:23", "link": "http://arxiv.org/abs/2104.08161v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word2rate: training and evaluating multiple word embeddings as\n  statistical transitions", "abstract": "Using pretrained word embeddings has been shown to be a very effective way in\nimproving the performance of natural language processing tasks. In fact almost\nany natural language tasks that can be thought of has been improved by these\npretrained embeddings. These tasks range from sentiment analysis, translation,\nsequence prediction amongst many others. One of the most successful word\nembeddings is the Word2vec CBOW model proposed by Mikolov trained by the\nnegative sampling technique. Mai et al. modifies this objective to train CMOW\nembeddings that are sensitive to word order. We used a modified version of the\nnegative sampling objective for our context words, modelling the context\nembeddings as a Taylor series of rate matrices. We show that different modes of\nthe Taylor series produce different types of embeddings. We compare these\nembeddings to their similar counterparts like CBOW and CMOW and show that they\nachieve comparable performance. We also introduce a novel left-right context\nsplit objective that improves performance for tasks sensitive to word order.\nOur Word2rate model is grounded in a statistical foundation using rate matrices\nwhile being competitive in variety of language tasks.", "published": "2021-04-16 15:31:29", "link": "http://arxiv.org/abs/2104.08173v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural\n  Language Generation", "abstract": "Natural language generation (NLG) benchmarks provide an important avenue to\nmeasure progress and develop better NLG systems. Unfortunately, the lack of\npublicly available NLG benchmarks for low-resource languages poses a\nchallenging barrier for building NLG systems that work well for languages with\nlimited amounts of data. Here we introduce IndoNLG, the first benchmark to\nmeasure natural language generation (NLG) progress in three low-resource -- yet\nwidely spoken -- languages of Indonesia: Indonesian, Javanese, and Sundanese.\nAltogether, these languages are spoken by more than 100 million native\nspeakers, and hence constitute an important use case of NLG systems today.\nConcretely, IndoNLG covers six tasks: summarization, question answering,\nchit-chat, and three different pairs of machine translation (MT) tasks. We\ncollate a clean pretraining corpus of Indonesian, Sundanese, and Javanese\ndatasets, Indo4B-Plus, which is used to pretrain our models: IndoBART and\nIndoGPT. We show that IndoBART and IndoGPT achieve competitive performance on\nall tasks -- despite using only one-fifth the parameters of a larger\nmultilingual model, mBART-LARGE (Liu et al., 2020). This finding emphasizes the\nimportance of pretraining on closely related, local languages to achieve more\nefficient learning and faster inference for very low-resource languages like\nJavanese and Sundanese.", "published": "2021-04-16 16:16:44", "link": "http://arxiv.org/abs/2104.08200v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "$Q^{2}$: Evaluating Factual Consistency in Knowledge-Grounded Dialogues\n  via Question Generation and Question Answering", "abstract": "Neural knowledge-grounded generative models for dialogue often produce\ncontent that is factually inconsistent with the knowledge they rely on, making\nthem unreliable and limiting their applicability. Inspired by recent work on\nevaluating factual consistency in abstractive summarization, we propose an\nautomatic evaluation metric for factual consistency in knowledge-grounded\ndialogue using automatic question generation and question answering. Our\nmetric, denoted $Q^2$, compares answer spans using natural language inference\n(NLI), instead of token-based matching as done in previous work. To foster\nproper evaluation, we curate a novel dataset of dialogue system outputs for the\nWizard-of-Wikipedia dataset, manually annotated for factual consistency. We\nperform a thorough meta-evaluation of $Q^2$ against other metrics using this\ndataset and two others, where it consistently shows higher correlation with\nhuman judgements.", "published": "2021-04-16 16:21:16", "link": "http://arxiv.org/abs/2104.08202v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Open-Vocabulary Translation from Visual Text Representations", "abstract": "Machine translation models have discrete vocabularies and commonly use\nsubword segmentation techniques to achieve an 'open vocabulary.' This approach\nrelies on consistent and correct underlying unicode sequences, and makes models\nsusceptible to degradation from common types of noise and variation. Motivated\nby the robustness of human language processing, we propose the use of visual\ntext representations, which dispense with a finite set of text embeddings in\nfavor of continuous vocabularies created by processing visually rendered text\nwith sliding windows. We show that models using visual text representations\napproach or match performance of traditional text models on small and larger\ndatasets. More importantly, models with visual embeddings demonstrate\nsignificant robustness to varied types of noise, achieving e.g., 25.9 BLEU on a\ncharacter permuted German-English task where subword models degrade to 1.9.", "published": "2021-04-16 16:37:13", "link": "http://arxiv.org/abs/2104.08211v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distantly Supervised Relation Extraction with Sentence Reconstruction\n  and Knowledge Base Priors", "abstract": "We propose a multi-task, probabilistic approach to facilitate distantly\nsupervised relation extraction by bringing closer the representations of\nsentences that contain the same Knowledge Base pairs. To achieve this, we bias\nthe latent space of sentences via a Variational Autoencoder (VAE) that is\ntrained jointly with a relation classifier. The latent code guides the pair\nrepresentations and influences sentence reconstruction. Experimental results on\ntwo datasets created via distant supervision indicate that multi-task learning\nresults in performance benefits. Additional exploration of employing Knowledge\nBase priors into the VAE reveals that the sentence space can be shifted towards\nthat of the Knowledge Base, offering interpretability and further improving\nresults.", "published": "2021-04-16 17:10:19", "link": "http://arxiv.org/abs/2104.08225v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Adversarially-Learned Turing Test for Dialog Generation Models", "abstract": "The design of better automated dialogue evaluation metrics offers the\npotential of accelerate evaluation research on conversational AI. However,\nexisting trainable dialogue evaluation models are generally restricted to\nclassifiers trained in a purely supervised manner, which suffer a significant\nrisk from adversarial attacking (e.g., a nonsensical response that enjoys a\nhigh classification score). To alleviate this risk, we propose an adversarial\ntraining approach to learn a robust model, ATT (Adversarial Turing Test), that\ndiscriminates machine-generated responses from human-written replies. In\ncontrast to previous perturbation-based methods, our discriminator is trained\nby iteratively generating unrestricted and diverse adversarial examples using\nreinforcement learning. The key benefit of this unrestricted adversarial\ntraining approach is allowing the discriminator to improve robustness in an\niterative attack-defense game. Our discriminator shows high accuracy on strong\nattackers including DialoGPT and GPT-3.", "published": "2021-04-16 17:13:14", "link": "http://arxiv.org/abs/2104.08231v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What to Pre-Train on? Efficient Intermediate Task Selection", "abstract": "Intermediate task fine-tuning has been shown to culminate in large transfer\ngains across many NLP tasks. With an abundance of candidate datasets as well as\npre-trained language models, it has become infeasible to run the cross-product\nof all combinations to find the best transfer setting. In this work we first\nestablish that similar sequential fine-tuning gains can be achieved in adapter\nsettings, and subsequently consolidate previously proposed methods that\nefficiently identify beneficial tasks for intermediate transfer learning. We\nexperiment with a diverse set of 42 intermediate and 11 target English\nclassification, multiple choice, question answering, and sequence tagging\ntasks. Our results show that efficient embedding based methods that rely solely\non the respective datasets outperform computational expensive few-shot\nfine-tuning approaches. Our best methods achieve an average Regret@3 of less\nthan 1% across all target tasks, demonstrating that we are able to efficiently\nidentify the best datasets for intermediate training.", "published": "2021-04-16 17:31:18", "link": "http://arxiv.org/abs/2104.08247v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "proScript: Partially Ordered Scripts Generation via Pre-trained Language\n  Models", "abstract": "Scripts - standardized event sequences describing typical everyday activities\n- have been shown to help understand narratives by providing expectations,\nresolving ambiguity, and filling in unstated information. However, to date they\nhave proved hard to author or extract from text. In this work, we demonstrate\nfor the first time that pre-trained neural language models (LMs) can be be\nfinetuned to generate high-quality scripts, at varying levels of granularity,\nfor a wide range of everyday scenarios (e.g., bake a cake). To do this, we\ncollected a large (6.4k), crowdsourced partially ordered scripts (named\nproScript), which is substantially larger than prior datasets, and developed\nmodels that generate scripts with combining language generation and structure\nprediction. We define two complementary tasks: (i) edge prediction: given a\nscenario and unordered events, organize the events into a valid (possibly\npartial-order) script, and (ii) script generation: given only a scenario,\ngenerate events and organize them into a (possibly partial-order) script. Our\nexperiments show that our models perform well (e.g., F1=75.7 in task (i)),\nillustrating a new approach to overcoming previous barriers to script\ncollection. We also show that there is still significant room for improvement\ntoward human level performance. Together, our tasks, dataset, and models offer\na new research direction for learning script knowledge.", "published": "2021-04-16 17:35:10", "link": "http://arxiv.org/abs/2104.08251v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-Adaptive Document-Level Neural Machine Translation", "abstract": "Most existing document-level neural machine translation (NMT) models leverage\na fixed number of the previous or all global source sentences to handle the\ncontext-independent problem in standard NMT. However, the translating of each\nsource sentence benefits from various sizes of context, and inappropriate\ncontext may harm the translation performance. In this work, we introduce a\ndata-adaptive method that enables the model to adopt the necessary and useful\ncontext. Specifically, we introduce a light predictor into two document-level\ntranslation models to select the explicit context. Experiments demonstrate the\nproposed approach can significantly improve the performance over the previous\nmethods with a gain up to 1.99 BLEU points.", "published": "2021-04-16 17:43:58", "link": "http://arxiv.org/abs/2104.08259v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Reason for Text Generation from Scientific Tables", "abstract": "In this paper, we introduce SciGen, a new challenge dataset for the task of\nreasoning-aware data-to-text generation consisting of tables from scientific\narticles and their corresponding descriptions. Describing scientific tables\ngoes beyond the surface realization of the table content and requires reasoning\nover table values. The unique properties of SciGen are that (1) tables mostly\ncontain numerical values, and (2) the corresponding descriptions require\narithmetic reasoning. SciGen is therefore the first dataset that assesses the\narithmetic reasoning capabilities of generation models on complex input\nstructures, i.e., tables from scientific articles. We study the effectiveness\nof state-of-the-art data-to-text generation models on SciGen and evaluate the\nresults using common metrics as well as human evaluation. Our results and\nanalyses show that (a) while humans like to reason for describing scientific\ntables, the ability of state-of-the-art models is severely limited on this\ntask, (b) while adding more training data improves the results, it is not the\nsolution for reasoning-aware text generation, and (c) one of the main\nbottlenecks for this task is the lack of proper automatic evaluation metrics.\nThe data, code, and annotations for human evaluation will be available at\nhttps://github.com/UKPLab/SciGen. SciGen opens new avenues for future research\nin reasoning-aware text generation and evaluation.", "published": "2021-04-16 18:01:36", "link": "http://arxiv.org/abs/2104.08296v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Membership Inference Attack Susceptibility of Clinical Language Models", "abstract": "Deep Neural Network (DNN) models have been shown to have high empirical\nprivacy leakages. Clinical language models (CLMs) trained on clinical data have\nbeen used to improve performance in biomedical natural language processing\ntasks. In this work, we investigate the risks of training-data leakage through\nwhite-box or black-box access to CLMs. We design and employ membership\ninference attacks to estimate the empirical privacy leaks for model\narchitectures like BERT and GPT2. We show that membership inference attacks on\nCLMs lead to non-trivial privacy leakages of up to 7%. Our results show that\nsmaller models have lower empirical privacy leakages than larger ones, and\nmasked LMs have lower leakages than auto-regressive LMs. We further show that\ndifferentially private CLMs can have improved model utility on clinical domain\nwhile ensuring low empirical privacy leakage. Lastly, we also study the effects\nof group-level membership inference and disease rarity on CLM privacy leakages.", "published": "2021-04-16 18:29:58", "link": "http://arxiv.org/abs/2104.08305v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Surface Form Competition: Why the Highest Probability Answer Isn't\n  Always Right", "abstract": "Large language models have shown promising results in zero-shot settings\n(Brown et al.,2020; Radford et al., 2019). For example, they can perform\nmultiple choice tasks simply by conditioning on a question and selecting the\nanswer with the highest probability.\n  However, ranking by string probability can be problematic due to surface form\ncompetition-wherein different surface forms compete for probability mass, even\nif they represent the same underlying concept, e.g. \"computer\" and \"PC.\" Since\nprobability mass is finite, this lowers the probability of the correct answer,\ndue to competition from other strings that are valid answers (but not one of\nthe multiple choice options).\n  We introduce Domain Conditional Pointwise Mutual Information, an alternative\nscoring function that directly compensates for surface form competition by\nsimply reweighing each option according to a term that is proportional to its a\npriori likelihood within the context of the specific zero-shot task. It\nachieves consistent gains in zero-shot performance over both calibrated (Zhao\net al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models\nover a variety of multiple choice datasets.", "published": "2021-04-16 18:57:19", "link": "http://arxiv.org/abs/2104.08315v9", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Importance of Effectively Adapting Pretrained Language Models for\n  Active Learning", "abstract": "Recent Active Learning (AL) approaches in Natural Language Processing (NLP)\nproposed using off-the-shelf pretrained language models (LMs). In this paper,\nwe argue that these LMs are not adapted effectively to the downstream task\nduring AL and we explore ways to address this issue. We suggest to first adapt\nthe pretrained LM to the target task by continuing training with all the\navailable unlabeled data and then use it for AL. We also propose a simple yet\neffective fine-tuning method to ensure that the adapted LM is properly trained\nin both low and high resource scenarios during AL. Our experiments demonstrate\nthat our approach provides substantial data efficiency improvements compared to\nthe standard fine-tuning approach, suggesting that a poor training strategy can\nbe catastrophic for AL.", "published": "2021-04-16 19:07:31", "link": "http://arxiv.org/abs/2104.08320v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ESTER: A Machine Reading Comprehension Dataset for Event Semantic\n  Relation Reasoning", "abstract": "Understanding how events are semantically related to each other is the\nessence of reading comprehension. Recent event-centric reading comprehension\ndatasets focus mostly on event arguments or temporal relations. While these\ntasks partially evaluate machines' ability of narrative understanding,\nhuman-like reading comprehension requires the capability to process event-based\ninformation beyond arguments and temporal reasoning. For example, to understand\ncausality between events, we need to infer motivation or purpose; to establish\nevent hierarchy, we need to understand the composition of events. To facilitate\nthese tasks, we introduce ESTER, a comprehensive machine reading comprehension\n(MRC) dataset for Event Semantic Relation Reasoning. The dataset leverages\nnatural language queries to reason about the five most common event semantic\nrelations, provides more than 6K questions and captures 10.1K event relation\npairs. Experimental results show that the current SOTA systems achieve 22.1%,\n63.3%, and 83.5% for token-based exact-match, F1, and event-based HIT@1 scores,\nwhich are all significantly below human performances (36.0%, 79.6%, 100%\nrespectively), highlighting our dataset as a challenging benchmark.", "published": "2021-04-16 19:59:26", "link": "http://arxiv.org/abs/2104.08350v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Concadia: Towards Image-Based Text Generation with a Purpose", "abstract": "Current deep learning models often achieve excellent results on benchmark\nimage-to-text datasets but fail to generate texts that are useful in practice.\nWe argue that to close this gap, it is vital to distinguish descriptions from\ncaptions based on their distinct communicative roles. Descriptions focus on\nvisual features and are meant to replace an image (often to increase\naccessibility), whereas captions appear alongside an image to supply additional\ninformation. To motivate this distinction and help people put it into practice,\nwe introduce the publicly available Wikipedia-based dataset Concadia consisting\nof 96,918 images with corresponding English-language descriptions, captions,\nand surrounding context. Using insights from Concadia, models trained on it,\nand a preregistered human-subjects experiment with human- and model-generated\ntexts, we characterize the commonalities and differences between descriptions\nand captions. In addition, we show that, for generating both descriptions and\ncaptions, it is useful to augment image-to-text models with representations of\nthe textual context in which the image appeared.", "published": "2021-04-16 21:25:00", "link": "http://arxiv.org/abs/2104.08376v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Extractive Summarization by Human Memory Simulation", "abstract": "Summarization systems face the core challenge of identifying and selecting\nimportant information. In this paper, we tackle the problem of content\nselection in unsupervised extractive summarization of long, structured\ndocuments. We introduce a wide range of heuristics that leverage cognitive\nrepresentations of content units and how these are retained or forgotten in\nhuman memory. We find that properties of these representations of human memory\ncan be exploited to capture relevance of content units in scientific articles.\nExperiments show that our proposed heuristics are effective at leveraging\ncognitive structures and the organization of the document (i.e.\\ sections of an\narticle), and automatic and human evaluations provide strong evidence that\nthese heuristics extract more summary-worthy content units.", "published": "2021-04-16 22:49:38", "link": "http://arxiv.org/abs/2104.08392v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Re-TACRED: Addressing Shortcomings of the TACRED Dataset", "abstract": "TACRED is one of the largest and most widely used sentence-level relation\nextraction datasets. Proposed models that are evaluated using this dataset\nconsistently set new state-of-the-art performance. However, they still exhibit\nlarge error rates despite leveraging external knowledge and unsupervised\npretraining on large text corpora. A recent study suggested that this may be\ndue to poor dataset quality. The study observed that over 50% of the most\nchallenging sentences from the development and test sets are incorrectly\nlabeled and account for an average drop of 8% f1-score in model performance.\nHowever, this study was limited to a small biased sample of 5k (out of a total\nof 106k) sentences, substantially restricting the generalizability and broader\nimplications of its findings. In this paper, we address these shortcomings by:\n(i) performing a comprehensive study over the whole TACRED dataset, (ii)\nproposing an improved crowdsourcing strategy and deploying it to re-annotate\nthe whole dataset, and (iii) performing a thorough analysis to understand how\ncorrecting the TACRED annotations affects previously published results. After\nverification, we observed that 23.9% of TACRED labels are incorrect. Moreover,\nevaluating several models on our revised dataset yields an average f1-score\nimprovement of 14.3% and helps uncover significant relationships between the\ndifferent models (rather than simply offsetting or scaling their scores by a\nconstant factor). Finally, aside from our analysis we also release Re-TACRED, a\nnew completely re-annotated version of the TACRED dataset that can be used to\nperform reliable evaluation of relation extraction models.", "published": "2021-04-16 22:55:11", "link": "http://arxiv.org/abs/2104.08398v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structure-Aware Abstractive Conversation Summarization via Discourse and\n  Action Graphs", "abstract": "Abstractive conversation summarization has received much attention recently.\nHowever, these generated summaries often suffer from insufficient, redundant,\nor incorrect content, largely due to the unstructured and complex\ncharacteristics of human-human interactions. To this end, we propose to\nexplicitly model the rich structures in conversations for more precise and\naccurate conversation summarization, by first incorporating discourse relations\nbetween utterances and action triples (\"who-doing-what\") in utterances through\nstructured graphs to better encode conversations, and then designing a\nmulti-granularity decoder to generate summaries by combining all levels of\ninformation. Experiments show that our proposed models outperform\nstate-of-the-art methods and generalize well in other domains in terms of both\nautomatic evaluations and human judgments. We have publicly released our code\nat https://github.com/GT-SALT/Structure-Aware-BART.", "published": "2021-04-16 23:04:52", "link": "http://arxiv.org/abs/2104.08400v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Matching-oriented Product Quantization For Ad-hoc Retrieval", "abstract": "Product quantization (PQ) is a widely used technique for ad-hoc retrieval.\nRecent studies propose supervised PQ, where the embedding and quantization\nmodels can be jointly trained with supervised learning. However, there is a\nlack of appropriate formulation of the joint training objective; thus, the\nimprovements over previous non-supervised baselines are limited in reality. In\nthis work, we propose the Matching-oriented Product Quantization (MoPQ), where\na novel objective Multinoulli Contrastive Loss (MCL) is formulated. With the\nminimization of MCL, we are able to maximize the matching probability of query\nand ground-truth key, which contributes to the optimal retrieval accuracy.\nGiven that the exact computation of MCL is intractable due to the demand of\nvast contrastive samples, we further propose the Differentiable Cross-device\nSampling (DCS), which significantly augments the contrastive samples for\nprecise approximation of MCL. We conduct extensive experimental studies on four\nreal-world datasets, whose results verify the effectiveness of MoPQ. The code\nis available at https://github.com/microsoft/MoPQ.", "published": "2021-04-16 02:25:46", "link": "http://arxiv.org/abs/2104.07858v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Translational NLP: A New Paradigm and General Principles for Natural\n  Language Processing Research", "abstract": "Natural language processing (NLP) research combines the study of universal\nprinciples, through basic science, with applied science targeting specific use\ncases and settings. However, the process of exchange between basic NLP and\napplications is often assumed to emerge naturally, resulting in many\ninnovations going unapplied and many important questions left unstudied. We\ndescribe a new paradigm of Translational NLP, which aims to structure and\nfacilitate the processes by which basic and applied NLP research inform one\nanother. Translational NLP thus presents a third research paradigm, focused on\nunderstanding the challenges posed by application needs and how these\nchallenges can drive innovation in basic science and technology design. We show\nthat many significant advances in NLP research have emerged from the\nintersection of basic principles with application needs, and present a\nconceptual framework outlining the stakeholders and key questions in\ntranslational research. Our framework provides a roadmap for developing\nTranslational NLP as a dedicated research area, and identifies general\ntranslational principles to facilitate exchange between basic and applied\nresearch.", "published": "2021-04-16 03:46:10", "link": "http://arxiv.org/abs/2104.07874v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Faithful and Plausible Explanations of Medical Code Predictions", "abstract": "Machine learning models that offer excellent predictive performance often\nlack the interpretability necessary to support integrated human machine\ndecision-making. In clinical medicine and other high-risk settings, domain\nexperts may be unwilling to trust model predictions without explanations. Work\nin explainable AI must balance competing objectives along two different axes:\n1) Explanations must balance faithfulness to the model's decision-making with\ntheir plausibility to a domain expert. 2) Domain experts desire local\nexplanations of individual predictions and global explanations of behavior in\naggregate. We propose to train a proxy model that mimics the behavior of the\ntrained model and provides fine-grained control over these trade-offs. We\nevaluate our approach on the task of assigning ICD codes to clinical notes to\ndemonstrate that explanations from the proxy model are faithful and replicate\nthe trained model behavior.", "published": "2021-04-16 05:13:36", "link": "http://arxiv.org/abs/2104.07894v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Generating Bug-Fixes Using Pretrained Transformers", "abstract": "Detecting and fixing bugs are two of the most important yet frustrating parts\nof the software development cycle. Existing bug detection tools are based\nmainly on static analyzers, which rely on mathematical logic and symbolic\nreasoning about the program execution to detect common types of bugs. Fixing\nbugs is typically left out to the developer. In this work we introduce\nDeepDebug: a data-driven program repair approach which learns to detect and fix\nbugs in Java methods mined from real-world GitHub repositories. We frame\nbug-patching as a sequence-to-sequence learning task consisting of two steps:\n(i) denoising pretraining, and (ii) supervised finetuning on the target\ntranslation task. We show that pretraining on source code programs improves the\nnumber of patches found by 33% as compared to supervised training from scratch,\nwhile domain-adaptive pretraining from natural language to code further\nimproves the accuracy by another 32%. We refine the standard accuracy\nevaluation metric into non-deletion and deletion-only fixes, and show that our\nbest model generates 75% more non-deletion fixes than the previous state of the\nart. In contrast to prior work, we attain our best results when generating raw\ncode, as opposed to working with abstracted code that tends to only benefit\nsmaller capacity models. Finally, we observe a subtle improvement from adding\nsyntax embeddings along with the standard positional embeddings, as well as\nwith adding an auxiliary task to predict each token's syntactic class. Despite\nfocusing on Java, our approach is language agnostic, requiring only a\ngeneral-purpose parser such as tree-sitter.", "published": "2021-04-16 05:27:04", "link": "http://arxiv.org/abs/2104.07896v2", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL"}
{"title": "MetaXL: Meta Representation Transformation for Low-resource\n  Cross-lingual Learning", "abstract": "The combination of multilingual pre-trained representations and cross-lingual\ntransfer learning is one of the most effective methods for building functional\nNLP systems for low-resource languages. However, for extremely low-resource\nlanguages without large-scale monolingual corpora for pre-training or\nsufficient annotated data for fine-tuning, transfer learning remains an\nunder-studied and challenging task. Moreover, recent work shows that\nmultilingual representations are surprisingly disjoint across languages,\nbringing additional challenges for transfer onto extremely low-resource\nlanguages. In this paper, we propose MetaXL, a meta-learning based framework\nthat learns to transform representations judiciously from auxiliary languages\nto a target one and brings their representation spaces closer for effective\ntransfer. Extensive experiments on real-world low-resource languages - without\naccess to large-scale monolingual corpora or large amounts of labeled data -\nfor tasks like cross-lingual sentiment analysis and named entity recognition\nshow the effectiveness of our approach. Code for MetaXL is publicly available\nat github.com/microsoft/MetaXL.", "published": "2021-04-16 06:15:52", "link": "http://arxiv.org/abs/2104.07908v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of Extrapolation in Text Generation with Scalar\n  Control", "abstract": "We conduct an empirical evaluation of extrapolation performance when\nconditioning on scalar control inputs like desired output length, desired edit\nfrom an input sentence, and desired sentiment across three text generation\ntasks. Specifically, we examine a zero-shot setting where models are asked to\ngeneralize to ranges of control values not seen during training. We focus on\nevaluating popular embedding methods for scalar inputs, including both\nlearnable and sinusoidal embeddings, as well as simpler approaches.\nSurprisingly, our findings indicate that the simplest strategy of using scalar\ninputs directly, without further encoding, most reliably allows for successful\nextrapolation.", "published": "2021-04-16 06:22:24", "link": "http://arxiv.org/abs/2104.07910v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Broccoli: Sprinkling Lightweight Vocabulary Learning into Everyday\n  Information Diets", "abstract": "The learning of a new language remains to this date a cognitive task that\nrequires considerable diligence and willpower, recent advances and tools\nnotwithstanding. In this paper, we propose Broccoli, a new paradigm aimed at\nreducing the required effort by seamlessly embedding vocabulary learning into\nusers' everyday information diets. This is achieved by inconspicuously\nswitching chosen words encountered by the user for their translation in the\ntarget language. Thus, by seeing words in context, the user can assimilate new\nvocabulary without much conscious effort. We validate our approach in a careful\nuser study, finding that the efficacy of the lightweight Broccoli approach is\ncompetitive with traditional, memorization-based vocabulary learning. The low\ncognitive overhead is manifested in a pronounced decrease in learners' usage of\nmnemonic learning strategies, as compared to traditional learning. Finally, we\nestablish that language patterns in typical information diets are compatible\nwith spaced-repetition strategies, thus enabling an efficient use of the\nBroccoli paradigm. Overall, our work establishes the feasibility of a novel and\npowerful \"install-and-forget\" approach for embedded language acquisition.", "published": "2021-04-16 07:38:05", "link": "http://arxiv.org/abs/2104.07941v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "A Million Tweets Are Worth a Few Points: Tuning Transformers for\n  Customer Service Tasks", "abstract": "In online domain-specific customer service applications, many companies\nstruggle to deploy advanced NLP models successfully, due to the limited\navailability of and noise in their datasets. While prior research demonstrated\nthe potential of migrating large open-domain pretrained models for\ndomain-specific tasks, the appropriate (pre)training strategies have not yet\nbeen rigorously evaluated in such social media customer service settings,\nespecially under multilingual conditions. We address this gap by collecting a\nmultilingual social media corpus containing customer service conversations\n(865k tweets), comparing various pipelines of pretraining and finetuning\napproaches, applying them on 5 different end tasks. We show that pretraining a\ngeneric multilingual transformer model on our in-domain dataset, before\nfinetuning on specific end tasks, consistently boosts performance, especially\nin non-English settings.", "published": "2021-04-16 07:45:04", "link": "http://arxiv.org/abs/2104.07944v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Models are Few-Shot Butlers", "abstract": "Pretrained language models demonstrate strong performance in most NLP tasks\nwhen fine-tuned on small task-specific datasets. Hence, these autoregressive\nmodels constitute ideal agents to operate in text-based environments where\nlanguage understanding and generative capabilities are essential. Nonetheless,\ncollecting expert demonstrations in such environments is a time-consuming\nendeavour. We introduce a two-stage procedure to learn from a small set of\ndemonstrations and further improve by interacting with an environment. We show\nthat language models fine-tuned with only 1.2% of the expert demonstrations and\na simple reinforcement learning algorithm achieve a 51% absolute improvement in\nsuccess rate over existing methods in the ALFWorld environment.", "published": "2021-04-16 08:47:07", "link": "http://arxiv.org/abs/2104.07972v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BERT2Code: Can Pretrained Language Models be Leveraged for Code Search?", "abstract": "Millions of repetitive code snippets are submitted to code repositories every\nday. To search from these large codebases using simple natural language queries\nwould allow programmers to ideate, prototype, and develop easier and faster.\nAlthough the existing methods have shown good performance in searching codes\nwhen the natural language description contains keywords from the code, they are\nstill far behind in searching codes based on the semantic meaning of the\nnatural language query and semantic structure of the code. In recent years,\nboth natural language and programming language research communities have\ncreated techniques to embed them in vector spaces. In this work, we leverage\nthe efficacy of these embedding models using a simple, lightweight 2-layer\nneural network in the task of semantic code search. We show that our model\nlearns the inherent relationship between the embedding spaces and further\nprobes into the scope of improvement by empirically analyzing the embedding\nmethods. In this analysis, we show that the quality of the code embedding model\nis the bottleneck for our model's performance, and discuss future directions of\nstudy in this area.", "published": "2021-04-16 10:28:27", "link": "http://arxiv.org/abs/2104.08017v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Effect of Visual Extensions on Natural Language Understanding in\n  Vision-and-Language Models", "abstract": "A method for creating a vision-and-language (V&L) model is to extend a\nlanguage model through structural modifications and V&L pre-training. Such an\nextension aims to make a V&L model inherit the capability of natural language\nunderstanding (NLU) from the original language model. To see how well this is\nachieved, we propose to evaluate V&L models using an NLU benchmark (GLUE). We\ncompare five V&L models, including single-stream and dual-stream models,\ntrained with the same pre-training. Dual-stream models, with their higher\nmodality independence achieved by approximately doubling the number of\nparameters, are expected to preserve the NLU capability better. Our main\nfinding is that the dual-stream scores are not much different than the\nsingle-stream scores, contrary to expectation. Further analysis shows that\npre-training causes the performance drop in NLU tasks with few exceptions.\nThese results suggest that adopting a single-stream structure and devising the\npre-training could be an effective method for improving the maintenance of\nlanguage knowledge in V&L extensions.", "published": "2021-04-16 12:28:50", "link": "http://arxiv.org/abs/2104.08066v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "To Share or not to Share: Predicting Sets of Sources for Model Transfer\n  Learning", "abstract": "In low-resource settings, model transfer can help to overcome a lack of\nlabeled data for many tasks and domains. However, predicting useful transfer\nsources is a challenging problem, as even the most similar sources might lead\nto unexpected negative transfer results. Thus, ranking methods based on task\nand text similarity -- as suggested in prior work -- may not be sufficient to\nidentify promising sources. To tackle this problem, we propose a new approach\nto automatically determine which and how many sources should be exploited. For\nthis, we study the effects of model transfer on sequence labeling across\nvarious domains and tasks and show that our methods based on model similarity\nand support vector machines are able to predict promising sources, resulting in\nperformance increases of up to 24 F1 points.", "published": "2021-04-16 12:44:40", "link": "http://arxiv.org/abs/2104.08078v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Citations are not opinions: a corpus linguistics approach to\n  understanding how citations are made", "abstract": "Citation content analysis seeks to understand citations based on the language\nused during the making of a citation. A key issue in citation content analysis\nis looking for linguistic structures that characterize distinct classes of\ncitations for the purposes of understanding the intent and function of a\ncitation. Previous works have focused on modeling linguistic features first and\ndrawn conclusions on the language structures unique to each class of citation\nfunction based on the performance of a classification task or inter-annotator\nagreement. In this study, we start with a large sample of a pre-classified\ncitation corpus, 2 million citations from each class of the scite Smart\nCitation dataset (supporting, disputing, and mentioning citations), and analyze\nits corpus linguistics in order to reveal the unique and statistically\nsignificant language structures belonging to each type of citation. By\ngenerating comparison tables for each citation type we present a number of\ninteresting linguistic features that uniquely characterize citation type. What\nwe find is that within citation collocates, there is very low correlation\nbetween citation type and sentiment. Additionally, we find that the\nsubjectivity of citation collocates across classes is very low. These findings\nsuggest that the sentiment of collocates is not a predictor of citation\nfunction and that due to their low subjectivity, an opinion-expressing mode of\nunderstanding citations, implicit in previous citation sentiment analysis\nliterature, is inappropriate. Instead, we suggest that citations can be better\nunderstood as claims-making devices where the citation type can be explained by\nunderstanding how two claims are being compared. By presenting this approach,\nwe hope to inspire similar corpus linguistic studies on citations that derive a\nmore robust theory of citation from an empirical basis using citation corpora", "published": "2021-04-16 12:52:27", "link": "http://arxiv.org/abs/2104.08087v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Cross-Modal Retrieval Augmentation for Multi-Modal Classification", "abstract": "Recent advances in using retrieval components over external knowledge sources\nhave shown impressive results for a variety of downstream tasks in natural\nlanguage processing. Here, we explore the use of unstructured external\nknowledge sources of images and their corresponding captions for improving\nvisual question answering (VQA). First, we train a novel alignment model for\nembedding images and captions in the same space, which achieves substantial\nimprovement in performance on image-caption retrieval w.r.t. similar methods.\nSecond, we show that retrieval-augmented multi-modal transformers using the\ntrained alignment model improve results on VQA over strong baselines. We\nfurther conduct extensive experiments to establish the promise of this\napproach, and examine novel applications for inference time such as\nhot-swapping indices.", "published": "2021-04-16 13:27:45", "link": "http://arxiv.org/abs/2104.08108v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Supervising Model Attention with Human Explanations for Robust Natural\n  Language Inference", "abstract": "Natural Language Inference (NLI) models are known to learn from biases and\nartefacts within their training data, impacting how well they generalise to\nother unseen datasets. Existing de-biasing approaches focus on preventing the\nmodels from learning these biases, which can result in restrictive models and\nlower performance. We instead investigate teaching the model how a human would\napproach the NLI task, in order to learn features that will generalise better\nto previously unseen examples. Using natural language explanations, we\nsupervise the model's attention weights to encourage more attention to be paid\nto the words present in the explanations, significantly improving model\nperformance. Our experiments show that the in-distribution improvements of this\nmethod are also accompanied by out-of-distribution improvements, with the\nsupervised models learning from features that generalise better to other NLI\ndatasets. Analysis of the model indicates that human explanations encourage\nincreased attention on the important words, with more attention paid to words\nin the premise and less attention paid to punctuation and stop-words.", "published": "2021-04-16 14:45:35", "link": "http://arxiv.org/abs/2104.08142v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Counter-Interference Adapter for Multilingual Machine Translation", "abstract": "Developing a unified multilingual model has long been a pursuit for machine\ntranslation. However, existing approaches suffer from performance degradation\n-- a single multilingual model is inferior to separately trained bilingual ones\non rich-resource languages. We conjecture that such a phenomenon is due to\ninterference caused by joint training with multiple languages. To accommodate\nthe issue, we propose CIAT, an adapted Transformer model with a small parameter\noverhead for multilingual machine translation. We evaluate CIAT on multiple\nbenchmark datasets, including IWSLT, OPUS-100, and WMT. Experiments show that\nCIAT consistently outperforms strong multilingual baselines on 64 of total 66\nlanguage directions, 42 of which see above 0.5 BLEU improvement. Our code is\navailable at \\url{https://github.com/Yaoming95/CIAT}~.", "published": "2021-04-16 14:58:28", "link": "http://arxiv.org/abs/2104.08154v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Probing artificial neural networks: insights from neuroscience", "abstract": "A major challenge in both neuroscience and machine learning is the\ndevelopment of useful tools for understanding complex information processing\nsystems. One such tool is probes, i.e., supervised models that relate features\nof interest to activation patterns arising in biological or artificial neural\nnetworks. Neuroscience has paved the way in using such models through numerous\nstudies conducted in recent decades. In this work, we draw insights from\nneuroscience to help guide probing research in machine learning. We highlight\ntwo important design choices for probes $-$ direction and expressivity $-$ and\nrelate these choices to research goals. We argue that specific research goals\nplay a paramount role when designing a probe and encourage future probing\nstudies to be explicit in stating these goals.", "published": "2021-04-16 16:13:23", "link": "http://arxiv.org/abs/2104.08197v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Flexible Instance-Specific Rationalization of NLP Models", "abstract": "Recent research on model interpretability in natural language processing\nextensively uses feature scoring methods for identifying which parts of the\ninput are the most important for a model to make a prediction (i.e. explanation\nor rationale). However, previous research has shown that there is no clear best\nscoring method across various text classification tasks while practitioners\ntypically have to make several other ad-hoc choices regarding the length and\nthe type of the rationale (e.g. short or long, contiguous or not). Inspired by\nthis, we propose a simple yet effective and flexible method that allows\nselecting optimally for each data instance: (1) a feature scoring method; (2)\nthe length; and (3) the type of the rationale. Our method is inspired by input\nerasure approaches to interpretability which assume that the most faithful\nrationale for a prediction should be the one with the highest difference\nbetween the model's output distribution using the full text and the text after\nremoving the rationale as input respectively. Evaluation on four standard text\nclassification datasets shows that our proposed method provides more faithful,\ncomprehensive and highly sufficient explanations compared to using a fixed\nfeature scoring method, rationale length and type. More importantly, we\ndemonstrate that a practitioner is not required to make any ad-hoc choices in\norder to extract faithful rationales using our approach.", "published": "2021-04-16 16:53:48", "link": "http://arxiv.org/abs/2104.08219v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Condenser: a Pre-training Architecture for Dense Retrieval", "abstract": "Pre-trained Transformer language models (LM) have become go-to text\nrepresentation encoders. Prior research fine-tunes deep LMs to encode text\nsequences such as sentences and passages into single dense vector\nrepresentations for efficient text comparison and retrieval. However, dense\nencoders require a lot of data and sophisticated techniques to effectively\ntrain and suffer in low data situations. This paper finds a key reason is that\nstandard LMs' internal attention structure is not ready-to-use for dense\nencoders, which needs to aggregate text information into the dense\nrepresentation. We propose to pre-train towards dense encoder with a novel\nTransformer architecture, Condenser, where LM prediction CONditions on DENSE\nRepresentation. Our experiments show Condenser improves over standard LM by\nlarge margins on various text retrieval and similarity tasks.", "published": "2021-04-16 17:36:44", "link": "http://arxiv.org/abs/2104.08253v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Data Augmentation for Voice-Assistant NLU using BERT-based\n  Interchangeable Rephrase", "abstract": "We introduce a data augmentation technique based on byte pair encoding and a\nBERT-like self-attention model to boost performance on spoken language\nunderstanding tasks. We compare and evaluate this method with a range of\naugmentation techniques encompassing generative models such as VAEs and\nperformance-boosting techniques such as synonym replacement and\nback-translation. We show our method performs strongly on domain and intent\nclassification tasks for a voice assistant and in a user-study focused on\nutterance naturalness and semantic similarity.", "published": "2021-04-16 17:53:58", "link": "http://arxiv.org/abs/2104.08268v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Membership Inference Attacks on Knowledge Graphs", "abstract": "Membership inference attacks (MIAs) infer whether a specific data record is\nused for target model training. MIAs have provoked many discussions in the\ninformation security community since they give rise to severe data privacy\nissues, especially for private and sensitive datasets. Knowledge Graphs (KGs),\nwhich describe domain-specific subjects and relationships among them, are\nvaluable and sensitive, such as medical KGs constructed from electronic health\nrecords. However, the privacy threat to knowledge graphs is critical but rarely\nexplored. In this paper, we conduct the first empirical evaluation of privacy\nthreats to knowledge graphs triggered by knowledge graph embedding methods\n(KGEs). We propose three types of membership inference attacks: transfer\nattacks (TAs), prediction loss-based attacks (PLAs), and prediction\ncorrectness-based attacks (PCAs), according to attack difficulty levels. In the\nexperiments, we conduct three inference attacks against four standard KGE\nmethods over three benchmark datasets. In addition, we also propose the attacks\nagainst medical KG and financial KG. The results demonstrate that the proposed\nattack methods can easily explore the privacy leakage of knowledge graphs.", "published": "2021-04-16 17:56:48", "link": "http://arxiv.org/abs/2104.08273v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Text2App: A Framework for Creating Android Apps from Text Descriptions", "abstract": "We present Text2App -- a framework that allows users to create functional\nAndroid applications from natural language specifications. The conventional\nmethod of source code generation tries to generate source code directly, which\nis impractical for creating complex software. We overcome this limitation by\ntransforming natural language into an abstract intermediate formal language\nrepresenting an application with a substantially smaller number of tokens. The\nintermediate formal representation is then compiled into target source codes.\nThis abstraction of programming details allows seq2seq networks to learn\ncomplex application structures with less overhead. In order to train sequence\nmodels, we introduce a data synthesis method grounded in a human survey. We\ndemonstrate that Text2App generalizes well to unseen combination of app\ncomponents and it is capable of handling noisy natural language instructions.\nWe explore the possibility of creating applications from highly abstract\ninstructions by coupling our system with GPT-3 -- a large pretrained language\nmodel. We perform an extensive human evaluation and identify the capabilities\nand limitations of our system. The source code, a ready-to-run demo notebook,\nand a demo video are publicly available at\n\\url{https://github.com/text2app/Text2App}.", "published": "2021-04-16 18:13:10", "link": "http://arxiv.org/abs/2104.08301v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Capturing Row and Column Semantics in Transformer Based Question\n  Answering over Tables", "abstract": "Transformer based architectures are recently used for the task of answering\nquestions over tables. In order to improve the accuracy on this task,\nspecialized pre-training techniques have been developed and applied on millions\nof open-domain web tables. In this paper, we propose two novel approaches\ndemonstrating that one can achieve superior performance on table QA task\nwithout even using any of these specialized pre-training techniques. The first\nmodel, called RCI interaction, leverages a transformer based architecture that\nindependently classifies rows and columns to identify relevant cells. While\nthis model yields extremely high accuracy at finding cell values on recent\nbenchmarks, a second model we propose, called RCI representation, provides a\nsignificant efficiency advantage for online QA systems over tables by\nmaterializing embeddings for existing tables. Experiments on recent benchmarks\nprove that the proposed methods can effectively locate cell values on tables\n(up to ~98% Hit@1 accuracy on WikiSQL lookup questions). Also, the interaction\nmodel outperforms the state-of-the-art transformer based approaches,\npre-trained on very large table corpora (TAPAS and TaBERT), achieving ~3.4% and\n~18.86% additional precision improvement on the standard WikiSQL benchmark.", "published": "2021-04-16 18:22:30", "link": "http://arxiv.org/abs/2104.08303v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "\"Wikily\" Supervised Neural Translation Tailored to Cross-Lingual Tasks", "abstract": "We present a simple but effective approach for leveraging Wikipedia for\nneural machine translation as well as cross-lingual tasks of image captioning\nand dependency parsing without using any direct supervision from external\nparallel data or supervised models in the target language. We show that first\nsentences and titles of linked Wikipedia pages, as well as cross-lingual image\ncaptions, are strong signals for a seed parallel data to extract bilingual\ndictionaries and cross-lingual word embeddings for mining parallel text from\nWikipedia. Our final model achieves high BLEU scores that are close to or\nsometimes higher than strong supervised baselines in low-resource languages;\ne.g. supervised BLEU of 4.0 versus 12.1 from our model in English-to-Kazakh.\nMoreover, we tailor our wikily supervised translation models to unsupervised\nimage captioning, and cross-lingual dependency parser transfer. In image\ncaptioning, we train a multi-tasking machine translation and image captioning\npipeline for Arabic and English from which the Arabic training data is a\ntranslated version of the English captioning data, using our wikily-supervised\ntranslation models. Our captioning results on Arabic are slightly better than\nthat of its supervised model. In dependency parsing, we translate a large\namount of monolingual text, and use it as artificial training data in an\nannotation projection framework. We show that our model outperforms recent work\non cross-lingual transfer of dependency parsers.", "published": "2021-04-16 21:49:12", "link": "http://arxiv.org/abs/2104.08384v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Neural String Edit Distance", "abstract": "We propose the neural string edit distance model for string-pair matching and\nstring transduction based on learnable string edit distance. We modify the\noriginal expectation-maximization learned edit distance algorithm into a\ndifferentiable loss function, allowing us to integrate it into a neural network\nproviding a contextual representation of the input. We evaluate on cognate\ndetection, transliteration, and grapheme-to-phoneme conversion, and show that\nwe can trade off between performance and interpretability in a single\nframework. Using contextual representations, which are difficult to interpret,\nwe match the performance of state-of-the-art string-pair matching models. Using\nstatic embeddings and a slightly different loss function, we force\ninterpretability, at the expense of an accuracy drop.", "published": "2021-04-16 22:16:47", "link": "http://arxiv.org/abs/2104.08388v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enriching a Model's Notion of Belief using a Persistent Memory", "abstract": "Although pretrained language models (PTLMs) have been shown to contain\nsignificant amounts of world knowledge, they can still produce inconsistent\nanswers to questions when probed, even after using specialized training\ntechniques to reduce inconsistency. As a result, it can be hard to identify\nwhat the model actually \"believes\" about the world. Our goal is to reduce this\nproblem, so systems are more globally consistent and accurate in their answers.\nOur approach is to add a memory component -- a BeliefBank -- that records a\nmodel's answers, and two mechanisms that use it to improve consistency among\nbeliefs. First, a reasoning component -- a weighted SAT solver -- improves\nconsistency by flipping answers that significantly clash with others. Second, a\nfeedback component re-queries the model but using known beliefs as context. We\nshow that, in a controlled experimental setting, these two mechanisms improve\nboth accuracy and consistency. This is significant as it is a first step\ntowards endowing models with an evolving memory, allowing them to construct a\nmore coherent picture of the world.", "published": "2021-04-16 23:09:11", "link": "http://arxiv.org/abs/2104.08401v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DEUX: An Attribute-Guided Framework for Sociable Recommendation Dialog\n  Systems", "abstract": "It is important for sociable recommendation dialog systems to perform as both\non-task content and social content to engage users and gain their favor. In\naddition to understand the user preferences and provide a satisfying\nrecommendation, such systems must be able to generate coherent and natural\nsocial conversations to the user. Traditional dialog state tracking cannot be\napplied to such systems because it does not track the attributes in the social\ncontent. To address this challenge, we propose DEUX, a novel attribute-guided\nframework to create better user experiences while accomplishing a movie\nrecommendation task. DEUX has a module that keeps track of the movie attributes\n(e.g., favorite genres, actors,etc.) in both user utterances and system\nresponses. This allows the system to introduce new movie attributes in its\nsocial content. Then, DEUX has multiple values for the same attribute type\nwhich suits the recommendation task since a user may like multiple genres, for\ninstance. Experiments suggest that DEUX outperforms all the baselines on being\nmore consistent, fitting the user preferences better, and providing a more\nengaging chat experience. Our approach can be used for any similar problems of\nsociable task-oriented dialog system.", "published": "2021-04-16 12:12:26", "link": "http://arxiv.org/abs/2105.00825v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WhatTheWikiFact: Fact-Checking Claims Against Wikipedia", "abstract": "The rise of Internet has made it a major source of information.\nUnfortunately, not all information online is true, and thus a number of\nfact-checking initiatives have been launched, both manual and automatic, to\ndeal with the problem. Here, we present our contribution in this regard:\n\\emph{WhatTheWikiFact}, a system for automatic claim verification using\nWikipedia. The system can predict the veracity of an input claim, and it\nfurther shows the evidence it has retrieved as part of the verification\nprocess. It shows confidence scores and a list of relevant Wikipedia articles,\ntogether with detailed information about each article, including the phrase\nused to retrieve it, the most relevant sentences extracted from it and their\nstance with respect to the input claim, as well as the associated\nprobabilities. The system supports several languages: Bulgarian, English, and\nRussian.", "published": "2021-04-16 12:23:56", "link": "http://arxiv.org/abs/2105.00826v2", "categories": ["cs.CL", "cs.IR", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Memorisation versus Generalisation in Pre-trained Language Models", "abstract": "State-of-the-art pre-trained language models have been shown to memorise\nfacts and perform well with limited amounts of training data. To gain a better\nunderstanding of how these models learn, we study their generalisation and\nmemorisation capabilities in noisy and low-resource scenarios. We find that the\ntraining of these models is almost unaffected by label noise and that it is\npossible to reach near-optimal results even on extremely noisy datasets.\nHowever, our experiments also show that they mainly learn from high-frequency\npatterns and largely fail when tested on low-resource tasks such as few-shot\nlearning and rare entity recognition. To mitigate such limitations, we propose\nan extension based on prototypical networks that improves performance in\nlow-resource named entity recognition tasks.", "published": "2021-04-16 18:53:19", "link": "http://arxiv.org/abs/2105.00828v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VGNMN: Video-grounded Neural Module Network to Video-Grounded Language\n  Tasks", "abstract": "Neural module networks (NMN) have achieved success in image-grounded tasks\nsuch as Visual Question Answering (VQA) on synthetic images. However, very\nlimited work on NMN has been studied in the video-grounded dialogue tasks.\nThese tasks extend the complexity of traditional visual tasks with the\nadditional visual temporal variance and language cross-turn dependencies.\nMotivated by recent NMN approaches on image-grounded tasks, we introduce\nVideo-grounded Neural Module Network (VGNMN) to model the information retrieval\nprocess in video-grounded language tasks as a pipeline of neural modules. VGNMN\nfirst decomposes all language components in dialogues to explicitly resolve any\nentity references and detect corresponding action-based inputs from the\nquestion. The detected entities and actions are used as parameters to\ninstantiate neural module networks and extract visual cues from the video. Our\nexperiments show that VGNMN can achieve promising performance on a challenging\nvideo-grounded dialogue benchmark as well as a video QA benchmark.", "published": "2021-04-16 06:47:41", "link": "http://arxiv.org/abs/2104.07921v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Fast, Effective, and Self-Supervised: Transforming Masked Language\n  Models into Universal Lexical and Sentence Encoders", "abstract": "Pretrained Masked Language Models (MLMs) have revolutionised NLP in recent\nyears. However, previous work has indicated that off-the-shelf MLMs are not\neffective as universal lexical or sentence encoders without further\ntask-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks\nusing annotated task data. In this work, we demonstrate that it is possible to\nturn MLMs into effective universal lexical and sentence encoders even without\nany additional data and without any supervision. We propose an extremely\nsimple, fast and effective contrastive learning technique, termed Mirror-BERT,\nwhich converts MLMs (e.g., BERT and RoBERTa) into such encoders in 20-30\nseconds without any additional external knowledge. Mirror-BERT relies on fully\nidentical or slightly modified string pairs as positive (i.e., synonymous)\nfine-tuning examples, and aims to maximise their similarity during identity\nfine-tuning. We report huge gains over off-the-shelf MLMs with Mirror-BERT in\nboth lexical-level and sentence-level tasks, across different domains and\ndifferent languages. Notably, in the standard sentence semantic similarity\n(STS) tasks, our self-supervised Mirror-BERT model even matches the performance\nof the task-tuned Sentence-BERT models from prior work. Finally, we delve\ndeeper into the inner workings of MLMs, and suggest some evidence on why this\nsimple approach can yield effective universal lexical and sentence encoders.", "published": "2021-04-16 10:49:56", "link": "http://arxiv.org/abs/2104.08027v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Editing Factual Knowledge in Language Models", "abstract": "The factual knowledge acquired during pre-training and stored in the\nparameters of Language Models (LMs) can be useful in downstream tasks (e.g.,\nquestion answering or textual inference). However, some facts can be\nincorrectly induced or become obsolete over time. We present KnowledgeEditor, a\nmethod which can be used to edit this knowledge and, thus, fix 'bugs' or\nunexpected predictions without the need for expensive re-training or\nfine-tuning. Besides being computationally efficient, KnowledgeEditordoes not\nrequire any modifications in LM pre-training (e.g., the use of meta-learning).\nIn our approach, we train a hyper-network with constrained optimization to\nmodify a fact without affecting the rest of the knowledge; the trained\nhyper-network is then used to predict the weight update at test time. We show\nKnowledgeEditor's efficacy with two popular architectures and\nknowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and\nii) a sequence-to-sequence BART model for question answering. With our method,\nchanging a prediction on the specific wording of a query tends to result in a\nconsistent change in predictions also for its paraphrases. We show that this\ncan be further encouraged by exploiting (e.g., automatically-generated)\nparaphrases during training. Interestingly, our hyper-network can be regarded\nas a 'probe' revealing which components need to be changed to manipulate\nfactual knowledge; our analysis shows that the updates tend to be concentrated\non a small subset of components. Source code available at\nhttps://github.com/nicola-decao/KnowledgeEditor", "published": "2021-04-16 15:24:42", "link": "http://arxiv.org/abs/2104.08164v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Modeling Fuzzy Cluster Transitions for Topic Tracing", "abstract": "Twitter can be viewed as a data source for Natural Language Processing (NLP)\ntasks. The continuously updating data streams on Twitter make it challenging to\ntrace real-time topic evolution. In this paper, we propose a framework for\nmodeling fuzzy transitions of topic clusters. We extend our previous work on\ncrisp cluster transitions by incorporating fuzzy logic in order to enrich the\nunderlying structures identified by the framework. We apply the methodology to\nboth computer generated clusters of nouns from tweets and human tweet\nannotations. The obtained fuzzy transitions are compared with the crisp\ntransitions, on both computer generated clusters and human labeled topic sets.", "published": "2021-04-16 17:41:16", "link": "http://arxiv.org/abs/2104.08258v1", "categories": ["cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Learning Evolved Combinatorial Symbols with a Neuro-symbolic Generative\n  Model", "abstract": "Humans have the ability to rapidly understand rich combinatorial concepts\nfrom limited data. Here we investigate this ability in the context of auditory\nsignals, which have been evolved in a cultural transmission experiment to study\nthe emergence of combinatorial structure in language. We propose a\nneuro-symbolic generative model which combines the strengths of previous\napproaches to concept learning. Our model performs fast inference drawing on\nneural network methods, while still retaining the interpretability and\ngeneralization from limited data seen in structured generative approaches. This\nmodel outperforms a purely neural network-based approach on classification as\nevaluated against both ground truth and human experimental classification\npreferences, and produces superior reproductions of observed signals as well.\nOur results demonstrate the power of flexible combined neural-symbolic\narchitectures for human-like generalization in raw perceptual domains and\noffers a step towards developing precise computational models of inductive\nbiases in language evolution.", "published": "2021-04-16 17:57:51", "link": "http://arxiv.org/abs/2104.08274v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Does language help generalization in vision models?", "abstract": "Vision models trained on multimodal datasets can benefit from the wide\navailability of large image-caption datasets. A recent model (CLIP) was found\nto generalize well in zero-shot and transfer learning settings. This could\nimply that linguistic or \"semantic grounding\" confers additional generalization\nabilities to the visual feature space. Here, we systematically evaluate various\nmultimodal architectures and vision-only models in terms of unsupervised\nclustering, few-shot learning, transfer learning and adversarial robustness. In\neach setting, multimodal training produced no additional generalization\ncapability compared to standard supervised visual training. We conclude that\nwork is still required for semantic grounding to help improve vision models.", "published": "2021-04-16 18:54:14", "link": "http://arxiv.org/abs/2104.08313v3", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "LAMPRET: Layout-Aware Multimodal PreTraining for Document Understanding", "abstract": "Document layout comprises both structural and visual (eg. font-sizes)\ninformation that is vital but often ignored by machine learning models. The few\nexisting models which do use layout information only consider textual contents,\nand overlook the existence of contents in other modalities such as images.\nAdditionally, spatial interactions of presented contents in a layout were never\nreally fully exploited. To bridge this gap, we parse a document into content\nblocks (eg. text, table, image) and propose a novel layout-aware multimodal\nhierarchical framework, LAMPreT, to model the blocks and the whole document.\nOur LAMPreT encodes each block with a multimodal transformer in the lower-level\nand aggregates the block-level representations and connections utilizing a\nspecifically designed transformer at the higher-level. We design hierarchical\npretraining objectives where the lower-level model is trained similarly to\nmultimodal grounding models, and the higher-level model is trained with our\nproposed novel layout-aware objectives. We evaluate the proposed model on two\nlayout-aware tasks -- text block filling and image suggestion and show the\neffectiveness of our proposed hierarchical architecture as well as pretraining\ntechniques.", "published": "2021-04-16 23:27:39", "link": "http://arxiv.org/abs/2104.08405v1", "categories": ["cs.CL", "cs.CV", "cs.IR"], "primary_category": "cs.CL"}
{"title": "AMMU : A Survey of Transformer-based Biomedical Pretrained Language\n  Models", "abstract": "Transformer-based pretrained language models (PLMs) have started a new era in\nmodern natural language processing (NLP). These models combine the power of\ntransformers, transfer learning, and self-supervised learning (SSL). Following\nthe success of these models in the general domain, the biomedical research\ncommunity has developed various in-domain PLMs starting from BioBERT to the\nlatest BioELECTRA and BioALBERT models. We strongly believe there is a need for\na survey paper that can provide a comprehensive survey of various\ntransformer-based biomedical pretrained language models (BPLMs). In this\nsurvey, we start with a brief overview of foundational concepts like\nself-supervised learning, embedding layer and transformer encoder layers. We\ndiscuss core concepts of transformer-based PLMs like pretraining methods,\npretraining tasks, fine-tuning methods, and various embedding types specific to\nbiomedical domain. We introduce a taxonomy for transformer-based BPLMs and then\ndiscuss all the models. We discuss various challenges and present possible\nsolutions. We conclude by highlighting some of the open issues which will drive\nthe research community to further improve transformer-based BPLMs.", "published": "2021-04-16 18:09:51", "link": "http://arxiv.org/abs/2105.00827v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Keyword Spotting by capturing long-range interactions with\n  Temporal Lambda Networks", "abstract": "Models based on attention mechanisms have shown unprecedented speech\nrecognition performance. However, they are computationally expensive and\nunnecessarily complex for keyword spotting, a task targeted to small-footprint\ndevices. This work explores the application of Lambda networks, an alternative\nframework for capturing long-range interactions without attention, for the\nkeyword spotting task. We propose a novel \\textit{ResNet}-based model by\nswapping the residual blocks by temporal Lambda layers. Furthermore, the\nproposed architecture is built upon uni-dimensional temporal convolutions that\nfurther reduce its complexity. The presented model does not only reach\nstate-of-the-art accuracies on the Google Speech Commands dataset, but it is\n85% and 65% lighter than its Transformer-based (KWT) and convolutional (Res15)\ncounterparts while being up to 100 times faster. To the best of our knowledge,\nthis is the first attempt to explore the Lambda framework within the speech\ndomain and therefore, we unravel further research of new interfaces based on\nthis architecture.", "published": "2021-04-16 12:51:57", "link": "http://arxiv.org/abs/2104.08086v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "TalkNet 2: Non-Autoregressive Depth-Wise Separable Convolutional Model\n  for Speech Synthesis with Explicit Pitch and Duration Prediction", "abstract": "We propose TalkNet, a non-autoregressive convolutional neural model for\nspeech synthesis with explicit pitch and duration prediction. The model\nconsists of three feed-forward convolutional networks. The first network\npredicts grapheme durations. An input text is expanded by repeating each symbol\naccording to the predicted duration. The second network predicts pitch value\nfor every mel frame. The third network generates a mel-spectrogram from the\nexpanded text conditioned on predicted pitch. All networks are based on 1D\ndepth-wise separable convolutional architecture. The explicit duration\nprediction eliminates word skipping and repeating. The quality of the generated\nspeech nearly matches the best auto-regressive models - TalkNet trained on the\nLJSpeech dataset got MOS 4.08. The model has only 13.2M parameters, almost 2x\nless than the present state-of-the-art text-to-speech models. The\nnon-autoregressive architecture allows for fast training and inference. The\nsmall model size and fast inference make the TalkNet an attractive candidate\nfor embedded speech synthesis.", "published": "2021-04-16 15:58:46", "link": "http://arxiv.org/abs/2104.08189v3", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
