{"title": "Simple Yet Effective Synthetic Dataset Construction for Unsupervised\n  Opinion Summarization", "abstract": "Opinion summarization provides an important solution for summarizing opinions\nexpressed among a large number of reviews. However, generating aspect-specific\nand general summaries is challenging due to the lack of annotated data. In this\nwork, we propose two simple yet effective unsupervised approaches to generate\nboth aspect-specific and general opinion summaries by training on synthetic\ndatasets constructed with aspect-related review contents. Our first approach,\nSeed Words Based Leave-One-Out (SW-LOO), identifies aspect-related portions of\nreviews simply by exact-matching aspect seed words and outperforms existing\nmethods by 3.4 ROUGE-L points on SPACE and 0.5 ROUGE-1 point on OPOSUM+ for\naspect-specific opinion summarization. Our second approach, Natural Language\nInference Based Leave-One-Out (NLI-LOO) identifies aspect-related sentences\nutilizing an NLI model in a more general setting without using seed words and\noutperforms existing approaches by 1.2 ROUGE-L points on SPACE for\naspect-specific opinion summarization and remains competitive on other metrics.", "published": "2023-03-21 08:08:04", "link": "http://arxiv.org/abs/2303.11660v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Open-domain Paradox for Chatbots: Common Ground as the Basis for\n  Human-like Dialogue", "abstract": "There is a surge in interest in the development of open-domain chatbots,\ndriven by the recent advancements of large language models. The \"openness\" of\nthe dialogue is expected to be maximized by providing minimal information to\nthe users about the common ground they can expect, including the presumed joint\nactivity. However, evidence suggests that the effect is the opposite. Asking\nusers to \"just chat about anything\" results in a very narrow form of dialogue,\nwhich we refer to as the \"open-domain paradox\". In this position paper, we\nexplain this paradox through the theory of common ground as the basis for\nhuman-like communication. Furthermore, we question the assumptions behind\nopen-domain chatbots and identify paths forward for enabling common ground in\nhuman-computer dialogue.", "published": "2023-03-21 10:01:49", "link": "http://arxiv.org/abs/2303.11708v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LEAPT: Learning Adaptive Prefix-to-prefix Translation For Simultaneous\n  Machine Translation", "abstract": "Simultaneous machine translation, which aims at a real-time translation, is\nuseful in many live scenarios but very challenging due to the trade-off between\naccuracy and latency. To achieve the balance for both, the model needs to wait\nfor appropriate streaming text (READ policy) and then generates its translation\n(WRITE policy). However, WRITE policies of previous work either are specific to\nthe method itself due to the end-to-end training or suffer from the input\nmismatch between training and decoding for the non-end-to-end training.\nTherefore, it is essential to learn a generic and better WRITE policy for\nsimultaneous machine translation. Inspired by strategies utilized by human\ninterpreters and \"wait\" policies, we propose a novel adaptive prefix-to-prefix\ntraining policy called LEAPT, which allows our machine translation model to\nlearn how to translate source sentence prefixes and make use of the future\ncontext. Experiments show that our proposed methods greatly outperform\ncompetitive baselines and achieve promising results.", "published": "2023-03-21 11:17:37", "link": "http://arxiv.org/abs/2303.11750v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chinese Intermediate English Learners outdid ChatGPT in deep cohesion:\n  Evidence from English narrative writing", "abstract": "ChatGPT is a publicly available chatbot that can quickly generate texts on\ngiven topics, but it is unknown whether the chatbot is really superior to human\nwriters in all aspects of writing and whether its writing quality can be\nprominently improved on the basis of updating commands. Consequently, this\nstudy compared the writing performance on a narrative topic by ChatGPT and\nChinese intermediate English (CIE) learners so as to reveal the chatbot's\nadvantage and disadvantage in writing. The data were analyzed in terms of five\ndiscourse components using Coh-Metrix (a special instrument for analyzing\nlanguage discourses), and the results revealed that ChatGPT performed better\nthan human writers in narrativity, word concreteness, and referential cohesion,\nbut worse in syntactic simplicity and deep cohesion in its initial version.\nAfter more revision commands were updated, while the resulting version was\nfacilitated in syntactic simplicity, yet it is still lagged far behind CIE\nlearners' writing in deep cohesion. In addition, the correlation analysis of\nthe discourse components suggests that narrativity was correlated with\nreferential cohesion in both ChatGPT and human writers, but the correlations\nvaried within each group.", "published": "2023-03-21 12:55:54", "link": "http://arxiv.org/abs/2303.11812v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understand Legal Documents with Contextualized Large Language Models", "abstract": "The growth of pending legal cases in populous countries, such as India, has\nbecome a major issue. Developing effective techniques to process and understand\nlegal documents is extremely useful in resolving this problem. In this paper,\nwe present our systems for SemEval-2023 Task 6: understanding legal texts (Modi\net al., 2023). Specifically, we first develop the Legal-BERT-HSLN model that\nconsiders the comprehensive context information in both intra- and\ninter-sentence levels to predict rhetorical roles (subtask A) and then train a\nLegal-LUKE model, which is legal-contextualized and entity-aware, to recognize\nlegal entities (subtask B). Our evaluations demonstrate that our designed\nmodels are more accurate than baselines, e.g., with an up to 15.0% better F1\nscore in subtask B. We achieved notable performance in the task leaderboard,\ne.g., 0.834 micro F1 score, and ranked No.5 out of 27 teams in subtask A.", "published": "2023-03-21 18:48:11", "link": "http://arxiv.org/abs/2303.12135v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Unified Taxonomy of Deep Syntactic Relations", "abstract": "This paper analyzes multiple deep-syntactic frameworks with the goal of\ncreating a proposal for a set of universal semantic role labels. The proposal\nexamines various theoretic linguistic perspectives and focuses on Meaning-Text\nTheory and Functional Generative Description frameworks.\n  For the purpose of this research, data from four languages is used -- Spanish\nand Catalan (Taule et al., 2011), Czech (Hajic et al., 2017), and English\n(Hajic et al., 2012). This proposal is oriented towards Universal Dependencies\n(de Marneffe et al., 2021) with a further intention of applying the universal\nsemantic role labels to the UD data.", "published": "2023-03-21 22:43:41", "link": "http://arxiv.org/abs/2303.12220v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Heterogeneous-Branch Collaborative Learning for Dialogue Generation", "abstract": "With the development of deep learning, advanced dialogue generation methods\nusually require a greater amount of computational resources. One promising\napproach to obtaining a high-performance and lightweight model is knowledge\ndistillation, which relies heavily on the pre-trained powerful teacher.\nCollaborative learning, also known as online knowledge distillation, is an\neffective way to conduct one-stage group distillation in the absence of a\nwell-trained large teacher model. However, previous work has a severe branch\nhomogeneity problem due to the same training objective and the independent\nidentical training sets. To alleviate this problem, we consider the dialogue\nattributes in the training of network branches. Each branch learns the\nattribute-related features based on the selected subset. Furthermore, we\npropose a dual group-based knowledge distillation method, consisting of\npositive distillation and negative distillation, to further diversify the\nfeatures of different branches in a steadily and interpretable way. The\nproposed approach significantly improves branch heterogeneity and outperforms\nstate-of-the-art collaborative learning methods on two widely used open-domain\ndialogue datasets.", "published": "2023-03-21 06:41:50", "link": "http://arxiv.org/abs/2303.11621v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Logical Reasoning over Natural Language as Knowledge Representation: A\n  Survey", "abstract": "Logical reasoning is central to human cognition and intelligence. It includes\ndeductive, inductive, and abductive reasoning. Past research of logical\nreasoning within AI uses formal language as knowledge representation and\nsymbolic reasoners. However, reasoning with formal language has proved\nchallenging (e.g., brittleness and knowledge-acquisition bottleneck). This\npaper provides a comprehensive overview on a new paradigm of logical reasoning,\nwhich uses natural language as knowledge representation and pretrained language\nmodels as reasoners, including philosophical definition and categorization of\nlogical reasoning, advantages of the new paradigm, benchmarks and methods,\nchallenges of the new paradigm, possible future directions, and relation to\nrelated NLP fields. This new paradigm is promising since it not only alleviates\nmany challenges of formal representation but also has advantages over\nend-to-end neural methods. This survey focus on transformer-based LLMs\nexplicitly working on deductive, inductive, and abductive reasoning over\nEnglish representation.", "published": "2023-03-21 16:56:05", "link": "http://arxiv.org/abs/2303.12023v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "cTBLS: Augmenting Large Language Models with Conversational Tables", "abstract": "Optimizing accuracy and performance while eliminating hallucinations of\nopen-domain conversational large language models (LLMs) is an open research\nchallenge. A particularly promising direction is to augment and ground LLMs\nwith information from structured sources. This paper introduces Conversational\nTables (cTBLS), a three-step architecture to retrieve and generate dialogue\nresponses grounded on retrieved tabular information. cTBLS uses Transformer\nencoder embeddings for Dense Table Retrieval and obtains up to 125% relative\nimprovement over the retriever in the previous state-of-the-art system on the\nHyrbiDialogue dataset. cTBLS then uses a shared process between encoder and\ndecoder models to perform a coarse+fine tabular knowledge (e.g., cell) ranking\ncombined with a GPT-3.5 LLM response generator to yield a 2x relative\nimprovement in ROUGE scores. Finally, human evaluators prefer cTBLs +80% of the\ntime (coherency, fluency) and judge informativeness to be 4x better than the\nprevious state-of-the-art.", "published": "2023-03-21 17:04:44", "link": "http://arxiv.org/abs/2303.12024v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Wearing Masks Implies Refuting Trump?: Towards Target-specific User\n  Stance Prediction across Events in COVID-19 and US Election 2020", "abstract": "People who share similar opinions towards controversial topics could form an\necho chamber and may share similar political views toward other topics as well.\nThe existence of such connections, which we call connected behavior, gives\nresearchers a unique opportunity to predict how one would behave for a future\nevent given their past behaviors. In this work, we propose a framework to\nconduct connected behavior analysis. Neural stance detection models are trained\non Twitter data collected on three seemingly independent topics, i.e., wearing\na mask, racial equality, and Trump, to detect people's stance, which we\nconsider as their online behavior in each topic-related event. Our results\nreveal a strong connection between the stances toward the three topical events\nand demonstrate the power of past behaviors in predicting one's future\nbehavior.", "published": "2023-03-21 17:14:04", "link": "http://arxiv.org/abs/2303.12029v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Large Language Models Can Be Used to Estimate the Latent Positions of\n  Politicians", "abstract": "Existing approaches to estimating politicians' latent positions along\nspecific dimensions often fail when relevant data is limited. We leverage the\nembedded knowledge in generative large language models (LLMs) to address this\nchallenge and measure lawmakers' positions along specific political or policy\ndimensions. We prompt an instruction/dialogue-tuned LLM to pairwise compare\nlawmakers and then scale the resulting graph using the Bradley-Terry model. We\nestimate novel measures of U.S. senators' positions on liberal-conservative\nideology, gun control, and abortion. Our liberal-conservative scale, used to\nvalidate LLM-driven scaling, strongly correlates with existing measures and\noffsets interpretive gaps, suggesting LLMs synthesize relevant data from\ninternet and digitized media rather than memorizing existing measures. Our gun\ncontrol and abortion measures -- the first of their kind -- differ from the\nliberal-conservative scale in face-valid ways and predict interest group\nratings and legislator votes better than ideology alone. Our findings suggest\nLLMs hold promise for solving complex social science measurement problems.", "published": "2023-03-21 17:48:00", "link": "http://arxiv.org/abs/2303.12057v4", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "VideoXum: Cross-modal Visual and Textural Summarization of Videos", "abstract": "Video summarization aims to distill the most important information from a\nsource video to produce either an abridged clip or a textual narrative.\nTraditionally, different methods have been proposed depending on whether the\noutput is a video or text, thus ignoring the correlation between the two\nsemantically related tasks of visual summarization and textual summarization.\nWe propose a new joint video and text summarization task. The goal is to\ngenerate both a shortened video clip along with the corresponding textual\nsummary from a long video, collectively referred to as a cross-modal summary.\nThe generated shortened video clip and text narratives should be semantically\nwell aligned. To this end, we first build a large-scale human-annotated dataset\n-- VideoXum (X refers to different modalities). The dataset is reannotated\nbased on ActivityNet. After we filter out the videos that do not meet the\nlength requirements, 14,001 long videos remain in our new dataset. Each video\nin our reannotated dataset has human-annotated video summaries and the\ncorresponding narrative summaries. We then design a novel end-to-end model --\nVTSUM-BILP to address the challenges of our proposed task. Moreover, we propose\na new metric called VT-CLIPScore to help evaluate the semantic consistency of\ncross-modality summary. The proposed model achieves promising performance on\nthis new task and establishes a benchmark for future research.", "published": "2023-03-21 17:51:23", "link": "http://arxiv.org/abs/2303.12060v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ChatGPT and a New Academic Reality: Artificial Intelligence-Written\n  Research Papers and the Ethics of the Large Language Models in Scholarly\n  Publishing", "abstract": "This paper discusses OpenAIs ChatGPT, a generative pre-trained transformer,\nwhich uses natural language processing to fulfill text-based user requests\n(i.e., a chatbot). The history and principles behind ChatGPT and similar models\nare discussed. This technology is then discussed in relation to its potential\nimpact on academia and scholarly research and publishing. ChatGPT is seen as a\npotential model for the automated preparation of essays and other types of\nscholarly manuscripts. Potential ethical issues that could arise with the\nemergence of large language models like GPT-3, the underlying technology behind\nChatGPT, and its usage by academics and researchers, are discussed and situated\nwithin the context of broader advancements in artificial intelligence, machine\nlearning, and natural language processing for research and scholarly\npublishing.", "published": "2023-03-21 14:35:07", "link": "http://arxiv.org/abs/2303.13367v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Fine-tuning ClimateBert transformer with ClimaText for the disclosure\n  analysis of climate-related financial risks", "abstract": "In recent years there has been a growing demand from financial agents,\nespecially from particular and institutional investors, for companies to report\non climate-related financial risks. A vast amount of information, in text\nformat, can be expected to be disclosed in the short term by firms in order to\nidentify these types of risks in their financial and non financial reports,\nparticularly in response to the growing regulation that is being passed on the\nmatter. To this end, this paper applies state-of-the-art NLP techniques to\nachieve the detection of climate change in text corpora. We use transfer\nlearning to fine-tune two transformer models, BERT and ClimateBert -a recently\npublished DistillRoBERTa-based model that has been specifically tailored for\nclimate text classification-. These two algorithms are based on the transformer\narchitecture which enables learning the contextual relationships between words\nin a text. We carry out the fine-tuning process of both models on the novel\nClima-Text database, consisting of data collected from Wikipedia, 10K Files\nReports and web-based claims. Our text classification model obtained from the\nClimateBert fine-tuning process on ClimaText, outperforms the models created\nwith BERT and the current state-of-the-art transformer in this particular\nproblem. Our study is the first one to implement on the ClimaText database the\nrecently published ClimateBert algorithm. Based on our results, it can be said\nthat ClimateBert fine-tuned on ClimaText is an outstanding tool within the NLP\npre-trained transformer models that may and should be used by investors,\ninstitutional agents and companies themselves to monitor the disclosure of\nclimate risk in financial reports. In addition, our transfer learning\nmethodology is cheap in computational terms, thus allowing any organization to\nperform it.", "published": "2023-03-21 07:25:36", "link": "http://arxiv.org/abs/2303.13373v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The African Stopwords project: curating stopwords for African languages", "abstract": "Stopwords are fundamental in Natural Language Processing (NLP) techniques for\ninformation retrieval. One of the common tasks in preprocessing of text data is\nthe removal of stopwords. Currently, while high-resource languages like English\nbenefit from the availability of several stopwords, low-resource languages,\nsuch as those found in the African continent, have none that are standardized\nand available for use in NLP packages. Stopwords in the context of African\nlanguages are understudied and can reveal information about the crossover\nbetween languages. The \\textit{African Stopwords} project aims to study and\ncurate stopwords for African languages. In this paper, we present our current\nprogress on ten African languages as well as future plans for the project.", "published": "2023-03-21 17:32:01", "link": "http://arxiv.org/abs/2304.12155v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training\n  Efficiency", "abstract": "Recent research has focused on weight sparsity in deep neural network\ntraining to reduce FLOPs, aiming for improved efficiency (test accuracy w.r.t\ntraining FLOPs). However, sparse weight training often compromises accuracy,\nrequiring extended training schedules to attain the accuracy of dense models.\nIn contrast, our approach, Sparse Iso-FLOP Transformations (Sparse-IFT), uses\nsparsity to improve accuracy while maintaining dense model FLOPs. Using a\nsingle hyperparameter (i.e., the sparsity level), Sparse-IFTs efficiently\nreplace dense layers, expanding the search space for optimal sparse masks. In\naddition, dynamic sparse training (DST) with Sparse-IFT models effectively\nnavigate this larger sparse mask-weight space, which is evidenced by a spectral\nanalysis using Ramanujan graph properties. Our study reveals a robust\ncorrelation among mask topology, weights, and final performance. Notably,\nwithout adjusting any training hyperparameters, replacing dense layers with\nSparse-IFT yields significant improvements, such as a +3.5% boost for ResNet-18\non ImageNet and +0.9% for GPT-3 Small on the Open LLM leaderboard. To the best\nof our knowledge, this is the first work to demonstrate the use of sparsity for\nimproving the accuracy of dense models through a set of simple-to-use sparse\ntransformations. Code is available at:\nhttps://github.com/CerebrasResearch/Sparse-IFT.", "published": "2023-03-21 01:06:37", "link": "http://arxiv.org/abs/2303.11525v4", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Difficulty in chirality recognition for Transformer architectures\n  learning chemical structures from string", "abstract": "Recent years have seen rapid development of descriptor generation based on\nrepresentation learning of extremely diverse molecules, especially those that\napply natural language processing (NLP) models to SMILES, a literal\nrepresentation of molecular structure. However, little research has been done\non how these models understand chemical structure. To address this black box,\nwe investigated the relationship between the learning progress of SMILES and\nchemical structure using a representative NLP model, the Transformer. We show\nthat while the Transformer learns partial structures of molecules quickly, it\nrequires extended training to understand overall structures. Consistently, the\naccuracy of molecular property predictions using descriptors generated from\nmodels at different learning steps was similar from the beginning to the end of\ntraining. Furthermore, we found that the Transformer requires particularly long\ntraining to learn chirality and sometimes stagnates with low performance due to\nmisunderstanding of enantiomers. These findings are expected to deepen the\nunderstanding of NLP models in chemistry.", "published": "2023-03-21 04:47:45", "link": "http://arxiv.org/abs/2303.11593v4", "categories": ["cs.LG", "cs.CL", "physics.chem-ph", "q-bio.BM", "J.2; I.2.7"], "primary_category": "cs.LG"}
{"title": "Transformers in Speech Processing: A Survey", "abstract": "The remarkable success of transformers in the field of natural language\nprocessing has sparked the interest of the speech-processing community, leading\nto an exploration of their potential for modeling long-range dependencies\nwithin speech sequences. Recently, transformers have gained prominence across\nvarious speech-related domains, including automatic speech recognition, speech\nsynthesis, speech translation, speech para-linguistics, speech enhancement,\nspoken dialogue systems, and numerous multimodal applications. In this paper,\nwe present a comprehensive survey that aims to bridge research studies from\ndiverse subfields within speech technology. By consolidating findings from\nacross the speech technology landscape, we provide a valuable resource for\nresearchers interested in harnessing the power of transformers to advance the\nfield. We identify the challenges encountered by transformers in speech\nprocessing while also offering insights into potential solutions to address\nthese issues.", "published": "2023-03-21 06:00:39", "link": "http://arxiv.org/abs/2303.11607v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Improving Content Retrievability in Search with Controllable Query\n  Generation", "abstract": "An important goal of online platforms is to enable content discovery, i.e.\nallow users to find a catalog entity they were not familiar with. A\npre-requisite to discover an entity, e.g. a book, with a search engine is that\nthe entity is retrievable, i.e. there are queries for which the system will\nsurface such entity in the top results. However, machine-learned search engines\nhave a high retrievability bias, where the majority of the queries return the\nsame entities. This happens partly due to the predominance of narrow intent\nqueries, where users create queries using the title of an already known entity,\ne.g. in book search 'harry potter'. The amount of broad queries where users\nwant to discover new entities, e.g. in music search 'chill lyrical electronica\nwith an atmospheric feeling to it', and have a higher tolerance to what they\nmight find, is small in comparison. We focus here on two factors that have a\nnegative impact on the retrievability of the entities (I) the training data\nused for dense retrieval models and (II) the distribution of narrow and broad\nintent queries issued in the system. We propose CtrlQGen, a method that\ngenerates queries for a chosen underlying intent-narrow or broad. We can use\nCtrlQGen to improve factor (I) by generating training data for dense retrieval\nmodels comprised of diverse synthetic queries. CtrlQGen can also be used to\ndeal with factor (II) by suggesting queries with broader intents to users. Our\nresults on datasets from the domains of music, podcasts, and books reveal that\nwe can significantly decrease the retrievability bias of a dense retrieval\nmodel when using CtrlQGen. First, by using the generated queries as training\ndata for dense models we make 9% of the entities retrievable (go from zero to\nnon-zero retrievability). Second, by suggesting broader queries to users, we\ncan make 12% of the entities retrievable in the best case.", "published": "2023-03-21 07:46:57", "link": "http://arxiv.org/abs/2303.11648v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Positive-Augmented Contrastive Learning for Image and Video Captioning\n  Evaluation", "abstract": "The CLIP model has been recently proven to be very effective for a variety of\ncross-modal tasks, including the evaluation of captions generated from\nvision-and-language architectures. In this paper, we propose a new recipe for a\ncontrastive-based evaluation metric for image captioning, namely\nPositive-Augmented Contrastive learning Score (PAC-S), that in a novel way\nunifies the learning of a contrastive visual-semantic space with the addition\nof generated images and text on curated data. Experiments spanning several\ndatasets demonstrate that our new metric achieves the highest correlation with\nhuman judgments on both images and videos, outperforming existing\nreference-based metrics like CIDEr and SPICE and reference-free metrics like\nCLIP-Score. Finally, we test the system-level correlation of the proposed\nmetric when considering popular image captioning approaches, and assess the\nimpact of employing different cross-modal features. Our source code and trained\nmodels are publicly available at: https://github.com/aimagelab/pacscore.", "published": "2023-03-21 18:03:14", "link": "http://arxiv.org/abs/2303.12112v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Fundamentals of Generative Large Language Models and Perspectives in\n  Cyber-Defense", "abstract": "Generative Language Models gained significant attention in late 2022 / early\n2023, notably with the introduction of models refined to act consistently with\nusers' expectations of interactions with AI (conversational models). Arguably\nthe focal point of public attention has been such a refinement of the GPT3\nmodel -- the ChatGPT and its subsequent integration with auxiliary\ncapabilities, including search as part of Microsoft Bing. Despite extensive\nprior research invested in their development, their performance and\napplicability to a range of daily tasks remained unclear and niche. However,\ntheir wider utilization without a requirement for technical expertise, made in\nlarge part possible through conversational fine-tuning, revealed the extent of\ntheir true capabilities in a real-world environment. This has garnered both\npublic excitement for their potential applications and concerns about their\ncapabilities and potential malicious uses. This review aims to provide a brief\noverview of the history, state of the art, and implications of Generative\nLanguage Models in terms of their principles, abilities, limitations, and\nfuture prospects -- especially in the context of cyber-defense, with a focus on\nthe Swiss operational environment.", "published": "2023-03-21 18:45:09", "link": "http://arxiv.org/abs/2303.12132v1", "categories": ["cs.CL", "cs.CR", "cs.LG", "I.2.7; I.2.1; K.6.5; K.4.2; J.7"], "primary_category": "cs.CL"}
{"title": "MAGVLT: Masked Generative Vision-and-Language Transformer", "abstract": "While generative modeling on multimodal image-text data has been actively\ndeveloped with large-scale paired datasets, there have been limited attempts to\ngenerate both image and text data by a single model rather than a generation of\none fixed modality conditioned on the other modality. In this paper, we explore\na unified generative vision-and-language (VL) model that can produce both\nimages and text sequences. Especially, we propose a generative VL transformer\nbased on the non-autoregressive mask prediction, named MAGVLT, and compare it\nwith an autoregressive generative VL transformer (ARGVLT). In comparison to\nARGVLT, the proposed MAGVLT enables bidirectional context encoding, fast\ndecoding by parallel token predictions in an iterative refinement, and extended\nediting capabilities such as image and text infilling. For rigorous training of\nour MAGVLT with image-text pairs from scratch, we combine the image-to-text,\ntext-to-image, and joint image-and-text mask prediction tasks. Moreover, we\ndevise two additional tasks based on the step-unrolled mask prediction and the\nselective prediction on the mixture of two image-text pairs. Experimental\nresults on various downstream generation tasks of VL benchmarks show that our\nMAGVLT outperforms ARGVLT by a large margin even with significant inference\nspeedup. Particularly, MAGVLT achieves competitive results on both zero-shot\nimage-to-text and text-to-image generation tasks from MS-COCO by one\nmoderate-sized model (fewer than 500M parameters) even without the use of\nmonomodal data and networks.", "published": "2023-03-21 21:49:39", "link": "http://arxiv.org/abs/2303.12208v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining\n  on Visual Language Understanding", "abstract": "Most humans use visual imagination to understand and reason about language,\nbut models such as BERT reason about language using knowledge acquired during\ntext-only pretraining. In this work, we investigate whether vision-and-language\npretraining can improve performance on text-only tasks that involve implicit\nvisual reasoning, focusing primarily on zero-shot probing methods. We propose a\nsuite of visual language understanding (VLU) tasks for probing the visual\nreasoning abilities of text encoder models, as well as various non-visual\nnatural language understanding (NLU) tasks for comparison. We also contribute a\nnovel zero-shot knowledge probing method, Stroop probing, for applying models\nsuch as CLIP to text-only tasks without needing a prediction head such as the\nmasked language modelling head of models like BERT. We show that SOTA\nmultimodally trained text encoders outperform unimodally trained text encoders\non the VLU tasks while being underperformed by them on the NLU tasks, lending\nnew context to previously mixed results regarding the NLU capabilities of\nmultimodal models. We conclude that exposure to images during pretraining\naffords inherent visual reasoning knowledge that is reflected in language-only\ntasks that require implicit visual reasoning. Our findings bear importance in\nthe broader context of multimodal learning, providing principled guidelines for\nthe choice of text encoders used in such contexts.", "published": "2023-03-21 17:30:40", "link": "http://arxiv.org/abs/2303.12513v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "In-depth analysis of music structure as a text network", "abstract": "Music, enchanting and poetic, permeates every corner of human civilization.\nAlthough music is not unfamiliar to people, our understanding of its essence\nremains limited, and there is still no universally accepted scientific\ndescription. This is primarily due to music being regarded as a product of both\nreason and emotion, making it difficult to define. In this article, we focus on\nthe fundamental elements of music and construct an evolutionary network from\nthe perspective of music as a natural language, aligning with the statistical\ncharacteristics of texts. Through this approach, we aim to comprehend the\nstructural differences in music across different periods, enabling a more\nscientific exploration of music. Relying on the advantages of structuralism, we\ncan concentrate on the relationships and order between the physical elements of\nmusic, rather than getting entangled in the blurred boundaries of science and\nphilosophy. The scientific framework we present not only conforms to past\nconclusions in music, but also serves as a bridge that connects music to\nnatural language processing and knowledge graphs.", "published": "2023-03-21 08:39:56", "link": "http://arxiv.org/abs/2303.13631v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Optical Character Recognition and Transcription of Berber Signs from\n  Images in a Low-Resource Language Amazigh", "abstract": "The Berber, or Amazigh language family is a low-resource North African\nvernacular language spoken by the indigenous Berber ethnic group. It has its\nown unique alphabet called Tifinagh used across Berber communities in Morocco,\nAlgeria, and others. The Afroasiatic language Berber is spoken by 14 million\npeople, yet lacks adequate representation in education, research, web\napplications etc. For instance, there is no option of translation to or from\nAmazigh / Berber on Google Translate, which hosts over 100 languages today.\nConsequently, we do not find specialized educational apps, L2 (2nd language\nlearner) acquisition, automated language translation, and remote-access\nfacilities enabled in Berber. Motivated by this background, we propose a\nsupervised approach called DaToBS for Detection and Transcription of Berber\nSigns. The DaToBS approach entails the automatic recognition and transcription\nof Tifinagh characters from signs in photographs of natural environments. This\nis achieved by self-creating a corpus of 1862 pre-processed character images;\ncurating the corpus with human-guided annotation; and feeding it into an OCR\nmodel via the deployment of CNN for deep learning based on computer vision\nmodels. We deploy computer vision modeling (rather than language models)\nbecause there are pictorial symbols in this alphabet, this deployment being a\nnovel aspect of our work. The DaToBS experimentation and analyses yield over 92\npercent accuracy in our research. To the best of our knowledge, ours is among\nthe first few works in the automated transcription of Berber signs from\nroadside images with deep learning, yielding high accuracy. This can pave the\nway for developing pedagogical applications in the Berber language, thereby\naddressing an important goal of outreach to underrepresented communities via AI\nin education.", "published": "2023-03-21 21:38:44", "link": "http://arxiv.org/abs/2303.13549v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "eess.IV", "I.2.1; I.2.7; J.5; K.3.1"], "primary_category": "cs.CV"}
{"title": "Self-Supervised Representations for Singing Voice Conversion", "abstract": "A singing voice conversion model converts a song in the voice of an arbitrary\nsource singer to the voice of a target singer. Recently, methods that leverage\nself-supervised audio representations such as HuBERT and Wav2Vec 2.0 have\nhelped further the state-of-the-art. Though these methods produce more natural\nand melodic singing outputs, they often rely on confusion and disentanglement\nlosses to render the self-supervised representations speaker and\npitch-invariant. In this paper, we circumvent disentanglement training and\npropose a new model that leverages ASR fine-tuned self-supervised\nrepresentations as inputs to a HiFi-GAN neural vocoder for singing voice\nconversion. We experiment with different f0 encoding schemes and show that an\nf0 harmonic generation module that uses a parallel bank of transposed\nconvolutions (PBTC) alongside ASR fine-tuned Wav2Vec 2.0 features results in\nthe best singing voice conversion quality. Additionally, the model is capable\nof making a spoken voice sing. We also show that a simple f0 shifting scheme\nduring inference helps retain singer identity and bolsters the performance of\nour singing voice conversion model. Our results are backed up by extensive MOS\nstudies that compare different ablations and baselines.", "published": "2023-03-21 21:04:03", "link": "http://arxiv.org/abs/2303.12197v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "ICASSP 2023 Deep Noise Suppression Challenge", "abstract": "Deep Speech Enhancement Challenge is the 5th edition of deep noise\nsuppression (DNS) challenges organized at ICASSP 2023 Signal Processing Grand\nChallenges. DNS challenges were organized during 2019-2023 to stimulate\nresearch in deep speech enhancement (DSE). Previous DNS challenges were\norganized at INTERSPEECH 2020, ICASSP 2021, INTERSPEECH 2021, and ICASSP 2022.\nFrom prior editions, we learnt that improving signal quality (SIG) is\nchallenging particularly in presence of simultaneously active interfering\ntalkers and noise. This challenge aims to develop models for joint denosing,\ndereverberation and suppression of interfering talkers. When primary talker\nwears a headphone, certain acoustic properties of their speech such as\ndirect-to-reverberation (DRR), signal to noise ratio (SNR) etc. make it\npossible to suppress neighboring talkers even without enrollment data for\nprimary talker. This motivated us to create two tracks for this challenge: (i)\nTrack-1 Headset; (ii) Track-2 Speakerphone. Both tracks has fullband (48kHz)\ntraining data and testset, and each testclips has a corresponding enrollment\ndata (10-30s duration) for primary talker. Each track invited submissions of\npersonalized and non-personalized models all of which are evaluated through\nsame subjective evaluation. Most models submitted to challenge were\npersonalized models, same team is winner in both tracks where the best models\nhas improvement of 0.145 and 0.141 in challenge's Score as compared to noisy\nblind testset.", "published": "2023-03-21 00:11:47", "link": "http://arxiv.org/abs/2303.11510v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Personalized Lightweight Text-to-Speech: Voice Cloning with Adaptive\n  Structured Pruning", "abstract": "Personalized TTS is an exciting and highly desired application that allows\nusers to train their TTS voice using only a few recordings. However, TTS\ntraining typically requires many hours of recording and a large model, making\nit unsuitable for deployment on mobile devices. To overcome this limitation,\nrelated works typically require fine-tuning a pre-trained TTS model to preserve\nits ability to generate high-quality audio samples while adapting to the target\nspeaker's voice. This process is commonly referred to as ``voice cloning.''\nAlthough related works have achieved significant success in changing the TTS\nmodel's voice, they are still required to fine-tune from a large pre-trained\nmodel, resulting in a significant size for the voice-cloned model. In this\npaper, we propose applying trainable structured pruning to voice cloning. By\ntraining the structured pruning masks with voice-cloning data, we can produce a\nunique pruned model for each target speaker. Our experiments demonstrate that\nusing learnable structured pruning, we can compress the model size to 7 times\nsmaller while achieving comparable voice-cloning performance.", "published": "2023-03-21 12:59:46", "link": "http://arxiv.org/abs/2303.11816v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ByteCover3: Accurate Cover Song Identification on Short Queries", "abstract": "Deep learning based methods have become a paradigm for cover song\nidentification (CSI) in recent years, where the ByteCover systems have achieved\nstate-of-the-art results on all the mainstream datasets of CSI. However, with\nthe burgeon of short videos, many real-world applications require matching\nshort music excerpts to full-length music tracks in the database, which is\nstill under-explored and waiting for an industrial-level solution. In this\npaper, we upgrade the previous ByteCover systems to ByteCover3 that utilizes\nlocal features to further improve the identification performance of short music\nqueries. ByteCover3 is designed with a local alignment loss (LAL) module and a\ntwo-stage feature retrieval pipeline, allowing the system to perform CSI in a\nmore precise and efficient way. We evaluated ByteCover3 on multiple datasets\nwith different benchmark settings, where ByteCover3 beat all the compared\nmethods including its previous versions.", "published": "2023-03-21 09:27:40", "link": "http://arxiv.org/abs/2303.11692v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "End-to-End Integration of Speech Separation and Voice Activity Detection\n  for Low-Latency Diarization of Telephone Conversations", "abstract": "Recent works show that speech separation guided diarization (SSGD) is an\nincreasingly promising direction, mainly thanks to the recent progress in\nspeech separation. It performs diarization by first separating the speakers and\nthen applying voice activity detection (VAD) on each separated stream. In this\nwork we conduct an in-depth study of SSGD in the conversational telephone\nspeech (CTS) domain, focusing mainly on low-latency streaming diarization\napplications. We consider three state-of-the-art speech separation (SSep)\nalgorithms and study their performance both in online and offline scenarios,\nconsidering non-causal and causal implementations as well as continuous SSep\n(CSS) windowed inference. We compare different SSGD algorithms on two widely\nused CTS datasets: CALLHOME and Fisher Corpus (Part 1 and 2) and evaluate both\nseparation and diarization performance. To improve performance, a novel, causal\nand computationally efficient leakage removal algorithm is proposed, which\nsignificantly decreases false alarms. We also explore, for the first time,\nfully end-to-end SSGD integration between SSep and VAD modules. Crucially, this\nenables fine-tuning on real-world data for which oracle speakers sources are\nnot available. In particular, our best model achieves 8.8% DER on CALLHOME,\nwhich outperforms the current state-of-the-art end-to-end neural diarization\nmodel, despite being trained on an order of magnitude less data and having\nsignificantly lower latency, i.e., 0.1 vs. 1 s. Finally, we also show that the\nseparated signals can be readily used also for automatic speech recognition,\nreaching performance close to using oracle sources in some configurations.", "published": "2023-03-21 16:33:56", "link": "http://arxiv.org/abs/2303.12002v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ModEFormer: Modality-Preserving Embedding for Audio-Video\n  Synchronization using Transformers", "abstract": "Lack of audio-video synchronization is a common problem during television\nbroadcasts and video conferencing, leading to an unsatisfactory viewing\nexperience. A widely accepted paradigm is to create an error detection\nmechanism that identifies the cases when audio is leading or lagging. We\npropose ModEFormer, which independently extracts audio and video embeddings\nusing modality-specific transformers. Different from the other\ntransformer-based approaches, ModEFormer preserves the modality of the input\nstreams which allows us to use a larger batch size with more negative audio\nsamples for contrastive learning. Further, we propose a trade-off between the\nnumber of negative samples and number of unique samples in a batch to\nsignificantly exceed the performance of previous methods. Experimental results\nshow that ModEFormer achieves state-of-the-art performance, 94.5% for LRS2 and\n90.9% for LRS3. Finally, we demonstrate how ModEFormer can be used for offset\ndetection for test clips.", "published": "2023-03-21 02:37:46", "link": "http://arxiv.org/abs/2303.11551v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
