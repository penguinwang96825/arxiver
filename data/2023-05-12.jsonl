{"title": "Asymmetric feature interaction for interpreting model predictions", "abstract": "In natural language processing (NLP), deep neural networks (DNNs) could model\ncomplex interactions between context and have achieved impressive results on a\nrange of NLP tasks. Prior works on feature interaction attribution mainly focus\non studying symmetric interaction that only explains the additional influence\nof a set of words in combination, which fails to capture asymmetric influence\nthat contributes to model prediction. In this work, we propose an asymmetric\nfeature interaction attribution explanation model that aims to explore\nasymmetric higher-order feature interactions in the inference of deep neural\nNLP models. By representing our explanation with an directed interaction graph,\nwe experimentally demonstrate interpretability of the graph to discover\nasymmetric feature interactions. Experimental results on two sentiment\nclassification datasets show the superiority of our model against the\nstate-of-the-art feature interaction attribution methods in identifying\ninfluential features for model predictions. Our code is available at\nhttps://github.com/StillLu/ASIV.", "published": "2023-05-12 03:31:24", "link": "http://arxiv.org/abs/2305.07224v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When Giant Language Brains Just Aren't Enough! Domain Pizzazz with\n  Knowledge Sparkle Dust", "abstract": "Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing, with GPT models at the forefront. While their remarkable\nperformance spans a range of tasks, adapting LLMs for real-world business\nscenarios still poses challenges warranting further investigation. This paper\npresents an empirical analysis aimed at bridging the gap in adapting LLMs to\npractical use cases. To do that, we select the question answering (QA) task of\ninsurance as a case study due to its challenge of reasoning. Based on the task\nwe design a new model relied on LLMs which are empowered by additional\nknowledge extracted from insurance policy rulebooks and DBpedia. The additional\nknowledge helps LLMs to understand new concepts of insurance for domain\nadaptation. Preliminary results on two QA datasets show that knowledge\nenhancement significantly improves the reasoning ability of GPT-3.5 (55.80% and\n57.83% in terms of accuracy). The analysis also indicates that existing public\nknowledge bases, e.g., DBPedia is beneficial for knowledge enhancement. Our\nfindings reveal that the inherent complexity of business scenarios often\nnecessitates the incorporation of domain-specific knowledge and external\nresources for effective problem-solving.", "published": "2023-05-12 03:49:59", "link": "http://arxiv.org/abs/2305.07230v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Harvesting Event Schemas from Large Language Models", "abstract": "Event schema provides a conceptual, structural and formal language to\nrepresent events and model the world event knowledge. Unfortunately, it is\nchallenging to automatically induce high-quality and high-coverage event\nschemas due to the open nature of real-world events, the diversity of event\nexpressions, and the sparsity of event knowledge. In this paper, we propose a\nnew paradigm for event schema induction -- knowledge harvesting from\nlarge-scale pre-trained language models, which can effectively resolve the\nabove challenges by discovering, conceptualizing and structuralizing event\nschemas from PLMs. And an Event Schema Harvester (ESHer) is designed to\nautomatically induce high-quality event schemas via in-context generation-based\nconceptualization, confidence-aware schema structuralization and graph-based\nschema aggregation. Empirical results show that ESHer can induce high-quality\nand high-coverage event schemas on varying domains.", "published": "2023-05-12 06:51:05", "link": "http://arxiv.org/abs/2305.07280v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open-WikiTable: Dataset for Open Domain Question Answering with Complex\n  Reasoning over Table", "abstract": "Despite recent interest in open domain question answering (ODQA) over tables,\nmany studies still rely on datasets that are not truly optimal for the task\nwith respect to utilizing structural nature of table. These datasets assume\nanswers reside as a single cell value and do not necessitate exploring over\nmultiple cells such as aggregation, comparison, and sorting. Thus, we release\nOpen-WikiTable, the first ODQA dataset that requires complex reasoning over\ntables. Open-WikiTable is built upon WikiSQL and WikiTableQuestions to be\napplicable in the open-domain setting. As each question is coupled with both\ntextual answers and SQL queries, Open-WikiTable opens up a wide range of\npossibilities for future research, as both reader and parser methods can be\napplied. The dataset and code are publicly available.", "published": "2023-05-12 07:24:16", "link": "http://arxiv.org/abs/2305.07288v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RepCL: Exploring Effective Representation for Continual Text\n  Classification", "abstract": "Continual learning (CL) aims to constantly learn new knowledge over time\nwhile avoiding catastrophic forgetting on old tasks. In this work, we focus on\ncontinual text classification under the class-incremental setting. Recent CL\nstudies find that the representations learned in one task may not be effective\nfor other tasks, namely representation bias problem. For the first time we\nformally analyze representation bias from an information bottleneck perspective\nand suggest that exploiting representations with more class-relevant\ninformation could alleviate the bias. To this end, we propose a novel\nreplay-based continual text classification method, RepCL. Our approach utilizes\ncontrastive and generative representation learning objectives to capture more\nclass-relevant features. In addition, RepCL introduces an adversarial replay\nstrategy to alleviate the overfitting problem of replay. Experiments\ndemonstrate that RepCL effectively alleviates forgetting and achieves\nstate-of-the-art performance on three text classification tasks.", "published": "2023-05-12 07:32:00", "link": "http://arxiv.org/abs/2305.07289v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MedGPTEval: A Dataset and Benchmark to Evaluate Responses of Large\n  Language Models in Medicine", "abstract": "METHODS: First, a set of evaluation criteria is designed based on a\ncomprehensive literature review. Second, existing candidate criteria are\noptimized for using a Delphi method by five experts in medicine and\nengineering. Third, three clinical experts design a set of medical datasets to\ninteract with LLMs. Finally, benchmarking experiments are conducted on the\ndatasets. The responses generated by chatbots based on LLMs are recorded for\nblind evaluations by five licensed medical experts. RESULTS: The obtained\nevaluation criteria cover medical professional capabilities, social\ncomprehensive capabilities, contextual capabilities, and computational\nrobustness, with sixteen detailed indicators. The medical datasets include\ntwenty-seven medical dialogues and seven case reports in Chinese. Three\nchatbots are evaluated, ChatGPT by OpenAI, ERNIE Bot by Baidu Inc., and Doctor\nPuJiang (Dr. PJ) by Shanghai Artificial Intelligence Laboratory. Experimental\nresults show that Dr. PJ outperforms ChatGPT and ERNIE Bot in both\nmultiple-turn medical dialogue and case report scenarios.", "published": "2023-05-12 09:37:13", "link": "http://arxiv.org/abs/2305.07340v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ZARA: Improving Few-Shot Self-Rationalization for Small Language Models", "abstract": "Language models (LMs) that jointly generate end-task answers as well as\nfree-text rationales are known as self-rationalization models. Recent works\ndemonstrate great performance gain for self-rationalization by few-shot\nprompting LMs with rationale-augmented exemplars. However, the ability to\nbenefit from explanations only emerges with large-scale LMs, which have poor\naccessibility. In this work, we explore the less-studied setting of leveraging\nexplanations for small LMs to improve few-shot self-rationalization. We first\nrevisit the relationship between rationales and answers. Inspired by the\nimplicit mental process of how human beings assess explanations, we present a\nnovel approach, Zero-shot Augmentation of Rationale-Answer pairs (ZARA), to\nautomatically construct pseudo-parallel data for self-training by reducing the\nproblem of plausibility judgement to natural language inference. Experimental\nresults show ZARA achieves SOTA performance on the FEB benchmark, for both the\ntask accuracy and the explanation metric. In addition, we conduct human and\nquantitative evaluation validating ZARA's ability to automatically identify\nplausible and accurate rationale-answer pairs.", "published": "2023-05-12 10:07:12", "link": "http://arxiv.org/abs/2305.07355v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Versatile and Efficient Visual Knowledge Integration into\n  Pre-trained Language Models with Cross-Modal Adapters", "abstract": "Humans learn language via multi-modal knowledge. However, due to the\ntext-only pre-training scheme, most existing pre-trained language models (PLMs)\nare hindered from the multi-modal information.\n  To inject visual knowledge into PLMs, existing methods incorporate either the\ntext or image encoder of vision-language models (VLMs) to encode the visual\ninformation and update all the original parameters of PLMs for knowledge\nfusion.\n  In this paper, we propose a new plug-and-play module, X-adapter, to flexibly\nleverage the aligned visual and textual knowledge learned in pre-trained VLMs\nand efficiently inject them into PLMs.\n  Specifically, we insert X-adapters into PLMs, and only the added parameters\nare updated during adaptation.\n  To fully exploit the potential in VLMs, X-adapters consist of two\nsub-modules, V-expert and T-expert, to fuse VLMs' image and text\nrepresentations, respectively.\n  We can opt for activating different sub-modules depending on the downstream\ntasks.\n  Experimental results show that our method can significantly improve the\nperformance on object-color reasoning and natural language understanding (NLU)\ntasks compared with PLM baselines.", "published": "2023-05-12 10:08:46", "link": "http://arxiv.org/abs/2305.07358v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt Learning to Mitigate Catastrophic Forgetting in Cross-lingual\n  Transfer for Open-domain Dialogue Generation", "abstract": "Dialogue systems for non-English languages have long been under-explored. In\nthis paper, we take the first step to investigate few-shot cross-lingual\ntransfer learning (FS-XLT) and multitask learning (MTL) in the context of\nopen-domain dialogue generation for non-English languages with limited data. We\nobserved catastrophic forgetting in both FS-XLT and MTL for all 6 languages in\nour preliminary experiments. To mitigate the issue, we propose a simple yet\neffective prompt learning approach that can preserve the multilinguality of\nmultilingual pre-trained language model (mPLM) in FS-XLT and MTL by bridging\nthe gap between pre-training and fine-tuning with Fixed-prompt LM Tuning and\nour hand-crafted prompts. Experimental results on all 6 languages in terms of\nboth automatic and human evaluations demonstrate the effectiveness of our\napproach. Our code is available at https://github.com/JeremyLeiLiu/XLinguDial.", "published": "2023-05-12 11:41:16", "link": "http://arxiv.org/abs/2305.07393v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Perturbation-based QE: An Explainable, Unsupervised Word-level Quality\n  Estimation Method for Blackbox Machine Translation", "abstract": "Quality Estimation (QE) is the task of predicting the quality of Machine\nTranslation (MT) system output, without using any gold-standard translation\nreferences. State-of-the-art QE models are supervised: they require\nhuman-labeled quality of some MT system output on some datasets for training,\nmaking them domain-dependent and MT-system-dependent. There has been research\non unsupervised QE, which requires glass-box access to the MT systems, or\nparallel MT data to generate synthetic errors for training QE models. In this\npaper, we present Perturbation-based QE - a word-level Quality Estimation\napproach that works simply by analyzing MT system output on perturbed input\nsource sentences. Our approach is unsupervised, explainable, and can evaluate\nany type of blackbox MT systems, including the currently prominent large\nlanguage models (LLMs) with opaque internal processes. For language directions\nwith no labeled QE data, our approach has similar or better performance than\nthe zero-shot supervised approach on the WMT21 shared task. Our approach is\nbetter at detecting gender bias and word-sense-disambiguation errors in\ntranslation than supervised QE, indicating its robustness to out-of-domain\nusage. The performance gap is larger when detecting errors on a nontraditional\ntranslation-prompting LLM, indicating that our approach is more generalizable\nto different MT systems. We give examples demonstrating our approach's\nexplainability power, where it shows which input source words have influence on\na certain MT output word.", "published": "2023-05-12 13:10:57", "link": "http://arxiv.org/abs/2305.07457v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Comprehensive Solution Program Centric Pretraining for Table-and-Text\n  Hybrid Numerical Reasoning", "abstract": "Numerical reasoning over table-and-text hybrid passages, such as financial\nreports, poses significant challenges and has numerous potential applications.\nNoise and irrelevant variables in the model input have been a hindrance to its\nperformance. Additionally, coarse-grained supervision of the whole solution\nprogram has impeded the model's ability to learn the underlying numerical\nreasoning process. In this paper, we propose three pretraining tasks that\noperate at both the whole program and sub-program level: Variable Integrity\nRanking, which guides the model to focus on useful variables; Variable Operator\nPrediction, which decomposes the supervision into fine-grained single operator\nprediction; and Variable Keyphrase Masking, which encourages the model to\nidentify key evidence that sub-programs are derived from. Experimental results\ndemonstrate the effectiveness of our proposed methods, surpassing\ntransformer-based model baselines.", "published": "2023-05-12 13:44:40", "link": "http://arxiv.org/abs/2305.07475v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Analysis of Adapter Efficiency", "abstract": "Adapters have been positioned as a parameter-efficient fine-tuning (PEFT)\napproach, whereby a minimal number of parameters are added to the model and\nfine-tuned. However, adapters have not been sufficiently analyzed to understand\nif PEFT translates to benefits in training/deployment efficiency and\nmaintainability/extensibility. Through extensive experiments on many adapters,\ntasks, and languages in supervised and cross-lingual zero-shot settings, we\nclearly show that for Natural Language Understanding (NLU) tasks, the parameter\nefficiency in adapters does not translate to efficiency gains compared to full\nfine-tuning of models. More precisely, adapters are relatively expensive to\ntrain and have slightly higher deployment latency. Furthermore, the\nmaintainability/extensibility benefits of adapters can be achieved with simpler\napproaches like multi-task training via full fine-tuning, which also provide\nrelatively faster training times. We, therefore, recommend that for moderately\nsized models for NLU tasks, practitioners should rely on full fine-tuning or\nmulti-task training rather than using adapters. Our code is available at\nhttps://github.com/AI4Bharat/adapter-efficiency.", "published": "2023-05-12 14:05:45", "link": "http://arxiv.org/abs/2305.07491v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LeXFiles and LegalLAMA: Facilitating English Multinational Legal\n  Language Model Development", "abstract": "In this work, we conduct a detailed analysis on the performance of\nlegal-oriented pre-trained language models (PLMs). We examine the interplay\nbetween their original objective, acquired knowledge, and legal language\nunderstanding capacities which we define as the upstream, probing, and\ndownstream performance, respectively. We consider not only the models' size but\nalso the pre-training corpora used as important dimensions in our study. To\nthis end, we release a multinational English legal corpus (LeXFiles) and a\nlegal knowledge probing benchmark (LegalLAMA) to facilitate training and\ndetailed analysis of legal-oriented PLMs. We release two new legal PLMs trained\non LeXFiles and evaluate them alongside others on LegalLAMA and LexGLUE. We\nfind that probing performance strongly correlates with upstream performance in\nrelated legal topics. On the other hand, downstream performance is mainly\ndriven by the model's size and prior legal knowledge which can be estimated by\nupstream and probing performance. Based on these findings, we can conclude that\nboth dimensions are important for those seeking the development of\ndomain-specific PLMs.", "published": "2023-05-12 14:21:38", "link": "http://arxiv.org/abs/2305.07507v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What are the Desired Characteristics of Calibration Sets? Identifying\n  Correlates on Long Form Scientific Summarization", "abstract": "Summarization models often generate text that is poorly calibrated to quality\nmetrics because they are trained to maximize the likelihood of a single\nreference (MLE). To address this, recent work has added a calibration step,\nwhich exposes a model to its own ranked outputs to improve relevance or, in a\nseparate line of work, contrasts positive and negative sets to improve\nfaithfulness. While effective, much of this work has focused on how to generate\nand optimize these sets. Less is known about why one setup is more effective\nthan another. In this work, we uncover the underlying characteristics of\neffective sets. For each training instance, we form a large, diverse pool of\ncandidates and systematically vary the subsets used for calibration\nfine-tuning. Each selection strategy targets distinct aspects of the sets, such\nas lexical diversity or the size of the gap between positive and negatives. On\nthree diverse scientific long-form summarization datasets (spanning biomedical,\nclinical, and chemical domains), we find, among others, that faithfulness\ncalibration is optimal when the negative sets are extractive and more likely to\nbe generated, whereas for relevance calibration, the metric margin between\ncandidates should be maximized and surprise--the disagreement between model and\nmetric defined candidate rankings--minimized. Code to create, select, and\noptimize calibration sets is available at\nhttps://github.com/griff4692/calibrating-summaries", "published": "2023-05-12 17:08:47", "link": "http://arxiv.org/abs/2305.07615v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parallel Tree Kernel Computation", "abstract": "Tree kernels are fundamental tools that have been leveraged in many\napplications, particularly those based on machine learning for Natural Language\nProcessing tasks. In this paper, we devise a parallel implementation of the\nsequential algorithm for the computation of some tree kernels of two finite\nsets of trees (Ouali-Sebti, 2015). Our comparison is narrowed on a sequential\nimplementation of SubTree kernel computation. This latter is mainly reduced to\nan intersection of weighted tree automata. Our approach relies on the nature of\nthe data parallelism source inherent in this computation by deploying the\nMapReduce paradigm. One of the key benefits of our approach is its versatility\nin being adaptable to a wide range of substructure tree kernel-based learning\nmethods. To evaluate the efficacy of our parallel approach, we conducted a\nseries of experiments that compared it against the sequential version using a\ndiverse set of synthetic tree language datasets that were manually crafted for\nour analysis. The reached results clearly demonstrate that the proposed\nparallel algorithm outperforms the sequential one in terms of latency.", "published": "2023-05-12 18:16:45", "link": "http://arxiv.org/abs/2305.07717v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NL2TL: Transforming Natural Languages to Temporal Logics using Large\n  Language Models", "abstract": "Temporal Logic (TL) can be used to rigorously specify complex high-level\nspecification for systems in many engineering applications. The translation\nbetween natural language (NL) and TL has been under-explored due to the lack of\ndataset and generalizable model across different application domains. In this\npaper, we propose an accurate and generalizable transformation framework of\nEnglish instructions from NL to TL, exploring the use of Large Language Models\n(LLMs) at multiple stages. Our contributions are twofold. First, we develop a\nframework to create a dataset of NL-TL pairs combining LLMs and human\nannotation. We publish a dataset with 28K NL-TL pairs. Then, we finetune T5\nmodels on the lifted versions (i.e., the specific Atomic Propositions (AP) are\nhidden) of the NL and TL. The enhanced generalizability originates from two\naspects: 1) Usage of lifted NL-TL characterizes common logical structures,\nwithout constraints of specific domains. 2) Application of LLMs in dataset\ncreation largely enhances corpus richness. We test the generalization of\ntrained models on five varied domains. To achieve full NL-TL transformation, we\neither combine the lifted model with AP recognition task or do the further\nfinetuning on each specific domain. During the further finetuning, our model\nachieves higher accuracy (>95%) using only <10% training data, compared with\nthe baseline sequence to sequence (Seq2Seq) model.", "published": "2023-05-12 21:22:08", "link": "http://arxiv.org/abs/2305.07766v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constructing Holistic Measures for Social Biases in Masked Language\n  Models", "abstract": "Masked Language Models (MLMs) have been successful in many natural language\nprocessing tasks. However, real-world stereotype biases are likely to be\nreflected in MLMs due to their learning from large text corpora. Most of the\nevaluation metrics proposed in the past adopt different masking strategies,\ndesigned with the log-likelihood of MLMs. They lack holistic considerations\nsuch as variance for stereotype bias and anti-stereotype bias samples. In this\npaper, the log-likelihoods of stereotype bias and anti-stereotype bias samples\noutput by MLMs are considered Gaussian distributions. Two evaluation metrics,\nKullback Leibler Divergence Score (KLDivS) and Jensen Shannon Divergence Score\n(JSDivS) are proposed to evaluate social biases in MLMs The experimental\nresults on the public datasets StereoSet and CrowS-Pairs demonstrate that\nKLDivS and JSDivS are more stable and interpretable compared to the metrics\nproposed in the past.", "published": "2023-05-12 23:09:06", "link": "http://arxiv.org/abs/2305.07795v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ACCENT: An Automatic Event Commonsense Evaluation Metric for Open-Domain\n  Dialogue Systems", "abstract": "Commonsense reasoning is omnipresent in human communications and thus is an\nimportant feature for open-domain dialogue systems. However, evaluating\ncommonsense in dialogue systems is still an open challenge. We take the first\nstep by focusing on event commonsense that considers events and their\nrelations, and is crucial in both dialogues and general commonsense reasoning.\nWe propose ACCENT, an event commonsense evaluation metric empowered by\ncommonsense knowledge bases (CSKBs). ACCENT first extracts event-relation\ntuples from a dialogue, and then evaluates the response by scoring the tuples\nin terms of their compatibility with the CSKB. To evaluate ACCENT, we construct\nthe first public event commonsense evaluation dataset for open-domain\ndialogues. Our experiments show that ACCENT is an efficient metric for event\ncommonsense evaluation, which achieves higher correlations with human judgments\nthan existing baselines.", "published": "2023-05-12 23:11:48", "link": "http://arxiv.org/abs/2305.07797v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gaussian Prior Reinforcement Learning for Nested Named Entity\n  Recognition", "abstract": "Named Entity Recognition (NER) is a well and widely studied task in natural\nlanguage processing. Recently, the nested NER has attracted more attention\nsince its practicality and difficulty. Existing works for nested NER ignore the\nrecognition order and boundary position relation of nested entities. To address\nthese issues, we propose a novel seq2seq model named GPRL, which formulates the\nnested NER task as an entity triplet sequence generation process. GPRL adopts\nthe reinforcement learning method to generate entity triplets decoupling the\nentity order in gold labels and expects to learn a reasonable recognition order\nof entities via trial and error. Based on statistics of boundary distance for\nnested entities, GPRL designs a Gaussian prior to represent the boundary\ndistance distribution between nested entities and adjust the output probability\ndistribution of nested boundary tokens. Experiments on three nested NER\ndatasets demonstrate that GPRL outperforms previous nested NER models.", "published": "2023-05-12 05:55:34", "link": "http://arxiv.org/abs/2305.07266v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Relational Hyperbolic Word Embeddings from Natural Language\n  Definitions", "abstract": "Natural language definitions possess a recursive, self-explanatory semantic\nstructure that can support representation learning methods able to preserve\nexplicit conceptual relations and constraints in the latent space. This paper\npresents a multi-relational model that explicitly leverages such a structure to\nderive word embeddings from definitions. By automatically extracting the\nrelations linking defined and defining terms from dictionaries, we demonstrate\nhow the problem of learning word embeddings can be formalised via a\ntranslational framework in Hyperbolic space and used as a proxy to capture the\nglobal semantic structure of definitions. An extensive empirical analysis\ndemonstrates that the framework can help imposing the desired structural\nconstraints while preserving the semantic mapping required for controllable and\ninterpretable traversal. Moreover, the experiments reveal the superiority of\nthe Hyperbolic word embeddings over the Euclidean counterparts and demonstrate\nthat the multi-relational approach can obtain competitive results when compared\nto state-of-the-art neural models, with the advantage of being intrinsically\nmore efficient and interpretable.", "published": "2023-05-12 08:16:06", "link": "http://arxiv.org/abs/2305.07303v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Zero-shot Multilingual Neural Machine Translation by\n  Leveraging Cross-lingual Consistency Regularization", "abstract": "The multilingual neural machine translation (NMT) model has a promising\ncapability of zero-shot translation, where it could directly translate between\nlanguage pairs unseen during training. For good transfer performance from\nsupervised directions to zero-shot directions, the multilingual NMT model is\nexpected to learn universal representations across different languages. This\npaper introduces a cross-lingual consistency regularization, CrossConST, to\nbridge the representation gap among different languages and boost zero-shot\ntranslation performance. The theoretical analysis shows that CrossConST\nimplicitly maximizes the probability distribution for zero-shot translation,\nand the experimental results on both low-resource and high-resource benchmarks\nshow that CrossConST consistently improves the translation performance. The\nexperimental analysis also proves that CrossConST could close the sentence\nrepresentation gap and better align the representation space. Given the\nuniversality and simplicity of CrossConST, we believe it can serve as a strong\nbaseline for future multilingual NMT research.", "published": "2023-05-12 08:32:18", "link": "http://arxiv.org/abs/2305.07310v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving the Quality of Neural Machine Translation Through Proper\n  Translation of Name Entities", "abstract": "In this paper, we have shown a method of improving the quality of neural\nmachine translation by translating/transliterating name entities as a\npreprocessing step. Through experiments we have shown the performance gain of\nour system. For evaluation we considered three types of name entities viz\nperson names, location names and organization names. The system was able to\ncorrectly translate mostly all the name entities. For person names the accuracy\nwas 99.86%, for location names the accuracy was 99.63% and for organization\nnames the accuracy was 99.05%. Overall, the accuracy of the system was 99.52%", "published": "2023-05-12 10:20:13", "link": "http://arxiv.org/abs/2305.07360v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Transliteration between Sindhi Scripts from Devanagari to\n  Perso-Arabic", "abstract": "In this paper, we have shown a script conversion (transliteration) technique\nthat converts Sindhi text in the Devanagari script to the Perso-Arabic script.\nWe showed this by incorporating a hybrid approach where some part of the text\nis converted using a rule base and in case an ambiguity arises then a\nprobabilistic model is used to resolve the same. Using this approach, the\nsystem achieved an overall accuracy of 99.64%.", "published": "2023-05-12 10:29:37", "link": "http://arxiv.org/abs/2305.07365v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Interactive Text-to-SQL Generation via Editable Step-by-Step\n  Explanations", "abstract": "Relational databases play an important role in business, science, and more.\nHowever, many users cannot fully unleash the analytical power of relational\ndatabases, because they are not familiar with database languages such as SQL.\nMany techniques have been proposed to automatically generate SQL from natural\nlanguage, but they suffer from two issues: (1) they still make many mistakes,\nparticularly for complex queries, and (2) they do not provide a flexible way\nfor non-expert users to validate and refine incorrect queries. To address these\nissues, we introduce a new interaction mechanism that allows users to directly\nedit a step-by-step explanation of a query to fix errors. Our experiments on\nmultiple datasets, as well as a user study with 24 participants, demonstrate\nthat our approach can achieve better performance than multiple SOTA approaches.\nOur code and datasets are available at https://github.com/magic-YuanTian/STEPS.", "published": "2023-05-12 10:45:29", "link": "http://arxiv.org/abs/2305.07372v5", "categories": ["cs.DB", "cs.CL", "I.2.7"], "primary_category": "cs.DB"}
{"title": "Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation", "abstract": "Causal reasoning ability is crucial for numerous NLP applications. Despite\nthe impressive emerging ability of ChatGPT in various NLP tasks, it is unclear\nhow well ChatGPT performs in causal reasoning. In this paper, we conduct the\nfirst comprehensive evaluation of the ChatGPT's causal reasoning capabilities.\nExperiments show that ChatGPT is not a good causal reasoner, but a good causal\nexplainer. Besides, ChatGPT has a serious hallucination on causal reasoning,\npossibly due to the reporting biases between causal and non-causal\nrelationships in natural language, as well as ChatGPT's upgrading processes,\nsuch as RLHF. The In-Context Learning (ICL) and Chain-of-Thought (CoT)\ntechniques can further exacerbate such causal hallucination. Additionally, the\ncausal reasoning ability of ChatGPT is sensitive to the words used to express\nthe causal concept in prompts, and close-ended prompts perform better than\nopen-ended prompts. For events in sentences, ChatGPT excels at capturing\nexplicit causality rather than implicit causality, and performs better in\nsentences with lower event density and smaller lexical distance between events.\nThe code is available on https://github.com/ArrogantL/ChatGPT4CausalReasoning .", "published": "2023-05-12 10:54:13", "link": "http://arxiv.org/abs/2305.07375v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Synergistic Interplay between Search and Large Language Models for\n  Information Retrieval", "abstract": "Information retrieval (IR) plays a crucial role in locating relevant\nresources from vast amounts of data, and its applications have evolved from\ntraditional knowledge bases to modern retrieval models (RMs). The emergence of\nlarge language models (LLMs) has further revolutionized the IR field by\nenabling users to interact with search systems in natural languages. In this\npaper, we explore the advantages and disadvantages of LLMs and RMs,\nhighlighting their respective strengths in understanding user-issued queries\nand retrieving up-to-date information. To leverage the benefits of both\nparadigms while circumventing their limitations, we propose InteR, a novel\nframework that facilitates information refinement through synergy between RMs\nand LLMs. InteR allows RMs to expand knowledge in queries using LLM-generated\nknowledge collections and enables LLMs to enhance prompt formulation using\nretrieved documents. This iterative refinement process augments the inputs of\nRMs and LLMs, leading to more accurate retrieval. Experiments on large-scale\nretrieval benchmarks involving web search and low-resource retrieval tasks\ndemonstrate that InteR achieves overall superior zero-shot retrieval\nperformance compared to state-of-the-art methods, even those using relevance\njudgment. Source code is available at https://github.com/Cyril-JZ/InteR", "published": "2023-05-12 11:58:15", "link": "http://arxiv.org/abs/2305.07402v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Instance Smoothed Contrastive Learning for Unsupervised Sentence\n  Embedding", "abstract": "Contrastive learning-based methods, such as unsup-SimCSE, have achieved\nstate-of-the-art (SOTA) performances in learning unsupervised sentence\nembeddings. However, in previous studies, each embedding used for contrastive\nlearning only derived from one sentence instance, and we call these embeddings\ninstance-level embeddings. In other words, each embedding is regarded as a\nunique class of its own, whichmay hurt the generalization performance. In this\nstudy, we propose IS-CSE (instance smoothing contrastive sentence embedding) to\nsmooth the boundaries of embeddings in the feature space. Specifically, we\nretrieve embeddings from a dynamic memory buffer according to the semantic\nsimilarity to get a positive embedding group. Then embeddings in the group are\naggregated by a self-attention operation to produce a smoothed instance\nembedding for further analysis. We evaluate our method on standard semantic\ntext similarity (STS) tasks and achieve an average of 78.30%, 79.47%, 77.73%,\nand 79.42% Spearman's correlation on the base of BERT-base, BERT-large,\nRoBERTa-base, and RoBERTa-large respectively, a 2.05%, 1.06%, 1.16% and 0.52%\nimprovement compared to unsup-SimCSE.", "published": "2023-05-12 12:46:13", "link": "http://arxiv.org/abs/2305.07424v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ArtGPT-4: Towards Artistic-understanding Large Vision-Language Models\n  with Enhanced Adapter", "abstract": "The success of large language models (LLMs) has inspired an emerging research\nfield of multimodal learning. However, a grand challenge of exploiting LLMs for\nmultimodal learning is the size of pre-trained LLMs which are always with\nbillions of parameters. To tackle this challenge, models such as MiniGPT-4 and\nLLaVA have been developed to fine-tune the pre-trained models using fewer\nparameters. Despite their promising performance, these models remain limited in\ntheir understanding of artistic imagery. To facilitate better\nartistic-understanding, in this paper, we propose ArtGPT-4, a pioneering large\nvision-language model tailored to address the limitations of existing models in\nartistic comprehension. The key innovation of ArtGPT-4 lies in its craft for\nthe sophisticated challenge of artistic image comprehension, setting it apart\nfrom other models that overlook fine details for broader themes. Specifically,\nit works by integrating some specialized adapter layers into the LLM, enabling\nthe model to more efficiently and effectively parse and interpret complex\nvisual tokens, instead of fine-tuning the whole LLM as in the existing method.\nArtGPT-4 has demonstrated its outstanding performance on the efficiency:\nutilizing a Tesla A100 device, its training can be completed in mere 2 hours\nwith an image-text pair dataset comprising approximately 0.52M entries.\nAdditionally, ArtGPT-4 has also achieved state-of-the-art performance on the\nArtEmis and ArtEmis-v2.0 datasets as well as the benchmarks established in this\nwork, lagging behind professional artists' descriptions by a negligible 0.15\npoints on a 6-point scale. The outstanding performance of ArtGPT-4 shows that\nit can render images with an artistic-understanding and convey the emotions\nthey inspire, mirroring human interpretation. The code and the pre-trained\nmodel are accessible in \\url{https://github.com/DLYuanGod/ArtGPT-4}.", "published": "2023-05-12 14:04:30", "link": "http://arxiv.org/abs/2305.07490v6", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Measuring Progress in Fine-grained Vision-and-Language Understanding", "abstract": "While pretraining on large-scale image-text data from the Web has facilitated\nrapid progress on many vision-and-language (V&L) tasks, recent work has\ndemonstrated that pretrained models lack \"fine-grained\" understanding, such as\nthe ability to recognise relationships, verbs, and numbers in images. This has\nresulted in an increased interest in the community to either develop new\nbenchmarks or models for such capabilities. To better understand and quantify\nprogress in this direction, we investigate four competitive V&L models on four\nfine-grained benchmarks. Through our analysis, we find that X-VLM (Zeng et al.,\n2022) consistently outperforms other baselines, and that modelling innovations\ncan impact performance more than scaling Web data, which even degrades\nperformance sometimes. Through a deeper investigation of X-VLM, we highlight\nthe importance of both novel losses and rich data sources for learning\nfine-grained skills. Finally, we inspect training dynamics, and discover that\nfor some tasks, performance peaks early in training or significantly\nfluctuates, never converging.", "published": "2023-05-12 15:34:20", "link": "http://arxiv.org/abs/2305.07558v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Multimodal Sentiment Analysis: A Survey", "abstract": "Multimodal sentiment analysis has become an important research area in the\nfield of artificial intelligence. With the latest advances in deep learning,\nthis technology has reached new heights. It has great potential for both\napplication and research, making it a popular research topic. This review\nprovides an overview of the definition, background, and development of\nmultimodal sentiment analysis. It also covers recent datasets and advanced\nmodels, emphasizing the challenges and future prospects of this technology.\nFinally, it looks ahead to future research directions. It should be noted that\nthis review provides constructive suggestions for promising research directions\nand building better performing multimodal sentiment analysis models, which can\nhelp researchers in this field.", "published": "2023-05-12 16:56:13", "link": "http://arxiv.org/abs/2305.07611v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "NevIR: Negation in Neural Information Retrieval", "abstract": "Negation is a common everyday phenomena and has been a consistent area of\nweakness for language models (LMs). Although the Information Retrieval (IR)\ncommunity has adopted LMs as the backbone of modern IR architectures, there has\nbeen little to no research in understanding how negation impacts neural IR. We\ntherefore construct a straightforward benchmark on this theme: asking IR models\nto rank two documents that differ only by negation. We show that the results\nvary widely according to the type of IR architecture: cross-encoders perform\nbest, followed by late-interaction models, and in last place are bi-encoder and\nsparse neural architectures. We find that most information retrieval models\n(including SOTA ones) do not consider negation, performing the same or worse\nthan a random ranking. We show that although the obvious approach of continued\nfine-tuning on a dataset of contrastive documents containing negations\nincreases performance (as does model size), there is still a large gap between\nmachine and human performance.", "published": "2023-05-12 17:05:54", "link": "http://arxiv.org/abs/2305.07614v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Knowledge Authoring for Rules and Actions", "abstract": "Knowledge representation and reasoning (KRR) systems describe and reason with\ncomplex concepts and relations in the form of facts and rules. Unfortunately,\nwide deployment of KRR systems runs into the problem that domain experts have\ngreat difficulty constructing correct logical representations of their domain\nknowledge. Knowledge engineers can help with this construction process, but\nthere is a deficit of such specialists. The earlier Knowledge Authoring Logic\nMachine (KALM) based on Controlled Natural Language (CNL) was shown to have\nvery high accuracy for authoring facts and questions. More recently, KALMFL, a\nsuccessor of KALM, replaced CNL with factual English, which is much less\nrestrictive and requires very little training from users. However, KALMFL has\nlimitations in representing certain types of knowledge, such as authoring rules\nfor multi-step reasoning or understanding actions with timestamps. To address\nthese limitations, we propose KALMRA to enable authoring of rules and actions.\nOur evaluation using the UTI guidelines benchmark shows that KALMRA achieves a\nhigh level of correctness (100%) on rule authoring. When used for authoring and\nreasoning with actions, KALMRA achieves more than 99.3% correctness on the bAbI\nbenchmark, demonstrating its effectiveness in more sophisticated KRR jobs.\nFinally, we illustrate the logical reasoning capabilities of KALMRA by drawing\nattention to the problems faced by the recently made famous AI, ChatGPT.", "published": "2023-05-12 21:08:35", "link": "http://arxiv.org/abs/2305.07763v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HPE:Answering Complex Questions over Text by Hybrid Question Parsing and\n  Execution", "abstract": "The dominant paradigm of textual question answering systems is based on\nend-to-end neural networks, which excels at answering natural language\nquestions but falls short on complex ones. This stands in contrast to the broad\nadaptation of semantic parsing approaches over structured data sources (e.g.,\nrelational database, knowledge graphs), that convert natural language questions\nto logical forms and execute them with query engines. Towards combining the\nstrengths of neural and symbolic methods, we propose a framework of question\nparsing and execution on textual QA. It comprises two central pillars: (1) We\nparse the question of varying complexity into an intermediate representation,\nnamed H-expression, which is composed of simple questions as the primitives and\nsymbolic operations representing the relationships among them; (2) To execute\nthe resulting H-expressions, we design a hybrid executor, which integrates the\ndeterministic rules to translate the symbolic operations with a drop-in neural\nreader network to answer each decomposed simple question. Hence, the proposed\nframework can be viewed as a top-down question parsing followed by a bottom-up\nanswer backtracking. The resulting H-expressions closely guide the execution\nprocess, offering higher precision besides better interpretability while still\npreserving the advantages of the neural readers for resolving its primitive\nelements. Our extensive experiments on MuSiQue, 2WikiQA, HotpotQA, and NQ show\nthat the proposed parsing and hybrid execution framework outperforms existing\napproaches in supervised, few-shot, and zero-shot settings, while also\neffectively exposing its underlying reasoning process.", "published": "2023-05-12 22:37:06", "link": "http://arxiv.org/abs/2305.07789v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Small Language Models on PubMedQA via Generative Data\n  Augmentation", "abstract": "Large Language Models (LLMs) have made remarkable advancements in the field\nof natural language processing. However, their increasing size poses challenges\nin terms of computational cost. On the other hand, Small Language Models (SLMs)\nare known for their efficiency, but they often struggle with limited capacity\nand training data, especially in specific domains. In this paper, we introduce\na novel method aimed at improving SLMs in the medical domain using LLM-based\ngenerative data augmentation. The objective of our approach is to develop more\nefficient and capable models that are specifically tailored for specialized\napplications. Through experiments conducted on the PubMedQA dataset, we\ndemonstrate the effectiveness of LLMs in refining and diversifying existing\nquestion-answer pairs. This refinement process leads to improved performance in\na significantly smaller model after fine-tuning. Notably, our best SLM, with\nunder 1.6 billion parameters, outperforms the few-shot GPT-4 on the PubMedQA\ndataset. Our code and generated data are publicly available to facilitate\nfurther explorations.", "published": "2023-05-12 23:49:23", "link": "http://arxiv.org/abs/2305.07804v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Better speech synthesis through scaling", "abstract": "In recent years, the field of image generation has been revolutionized by the\napplication of autoregressive transformers and DDPMs. These approaches model\nthe process of image generation as a step-wise probabilistic processes and\nleverage large amounts of compute and data to learn the image distribution.\nThis methodology of improving performance need not be confined to images. This\npaper describes a way to apply advances in the image generative domain to\nspeech synthesis. The result is TorToise -- an expressive, multi-voice\ntext-to-speech system.\n  All model code and trained weights have been open-sourced at\nhttps://github.com/neonbjb/tortoise-tts.", "published": "2023-05-12 04:19:49", "link": "http://arxiv.org/abs/2305.07243v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Model-based Programming: Redefining the Atomic Unit of Programming for\n  the Deep Learning Era", "abstract": "This paper introduces and explores a new programming paradigm, Model-based\nProgramming, designed to address the challenges inherent in applying deep\nlearning models to real-world applications. Despite recent significant\nsuccesses of deep learning models across a range of tasks, their deployment in\nreal business scenarios remains fraught with difficulties, such as complex\nmodel training, large computational resource requirements, and integration\nissues with existing programming languages. To ameliorate these challenges, we\npropose the concept of 'Model-based Programming' and present a novel\nprogramming language - M Language, tailored to a prospective model-centered\nprogramming paradigm. M Language treats models as basic computational units,\nenabling developers to concentrate more on crucial tasks such as model loading,\nfine-tuning, evaluation, and deployment, thereby enhancing the efficiency of\ncreating deep learning applications. We posit that this innovative programming\nparadigm will stimulate the extensive application and advancement of deep\nlearning technology and provide a robust foundation for a model-driven future.", "published": "2023-05-12 09:38:11", "link": "http://arxiv.org/abs/2305.07341v1", "categories": ["cs.LG", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "Implications of Deep Circuits in Improving Quality of Quantum Question\n  Answering", "abstract": "Question Answering (QA) has proved to be an arduous challenge in the area of\nnatural language processing (NLP) and artificial intelligence (AI). Many\nattempts have been made to develop complete solutions for QA as well as\nimproving significant sub-modules of the QA systems to improve the overall\nperformance through the course of time. Questions are the most important piece\nof QA, because knowing the question is equivalent to knowing what counts as an\nanswer (Harrah in Philos Sci, 1961 [1]). In this work, we have attempted to\nunderstand questions in a better way by using Quantum Machine Learning (QML).\nThe properties of Quantum Computing (QC) have enabled classically intractable\ndata processing. So, in this paper, we have performed question classification\non questions from two classes of SelQA (Selection-based Question Answering)\ndataset using quantum-based classifier algorithms-quantum support vector\nmachine (QSVM) and variational quantum classifier (VQC) from Qiskit (Quantum\nInformation Science toolKIT) for Python. We perform classification with both\nclassifiers in almost similar environments and study the effects of circuit\ndepths while comparing the results of both classifiers. We also use these\nclassification results with our own rule-based QA system and observe\nsignificant performance improvement. Hence, this experiment has helped in\nimproving the quality of QA in general.", "published": "2023-05-12 10:52:13", "link": "http://arxiv.org/abs/2305.07374v1", "categories": ["cs.CL", "cs.AI", "quant-ph"], "primary_category": "cs.CL"}
{"title": "Surfacing Biases in Large Language Models using Contrastive Input\n  Decoding", "abstract": "Ensuring that large language models (LMs) are fair, robust and useful\nrequires an understanding of how different modifications to their inputs impact\nthe model's behaviour. In the context of open-text generation tasks, however,\nsuch an evaluation is not trivial. For example, when introducing a model with\nan input text and a perturbed, \"contrastive\" version of it, meaningful\ndifferences in the next-token predictions may not be revealed with standard\ndecoding strategies. With this motivation in mind, we propose Contrastive Input\nDecoding (CID): a decoding algorithm to generate text given two inputs, where\nthe generated text is likely given one input but unlikely given the other. In\nthis way, the contrastive generations can highlight potentially subtle\ndifferences in how the LM output differs for the two inputs in a simple and\ninterpretable manner. We use CID to highlight context-specific biases that are\nhard to detect with standard decoding strategies and quantify the effect of\ndifferent input perturbations.", "published": "2023-05-12 11:09:49", "link": "http://arxiv.org/abs/2305.07378v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Investigating the Sensitivity of Automatic Speech Recognition Systems to\n  Phonetic Variation in L2 Englishes", "abstract": "Automatic Speech Recognition (ASR) systems exhibit the best performance on\nspeech that is similar to that on which it was trained. As such,\nunderrepresented varieties including regional dialects, minority-speakers, and\nlow-resource languages, see much higher word error rates (WERs) than those\nvarieties seen as 'prestigious', 'mainstream', or 'standard'. This can act as a\nbarrier to incorporating ASR technology into the annotation process for\nlarge-scale linguistic research since the manual correction of the erroneous\nautomated transcripts can be just as time and resource consuming as manual\ntranscriptions. A deeper understanding of the behaviour of an ASR system is\nthus beneficial from a speech technology standpoint, in terms of improving ASR\naccuracy, and from an annotation standpoint, where knowing the likely errors\nmade by an ASR system can aid in this manual correction. This work demonstrates\na method of probing an ASR system to discover how it handles phonetic variation\nacross a number of L2 Englishes. Specifically, how particular phonetic\nrealisations which were rare or absent in the system's training data can lead\nto phoneme level misrecognitions and contribute to higher WERs. It is\ndemonstrated that the behaviour of the ASR is systematic and consistent across\nspeakers with similar spoken varieties (in this case the same L1) and phoneme\nsubstitution errors are typically in agreement with human annotators. By\nidentifying problematic productions specific weaknesses can be addressed by\nsourcing such realisations for training and fine-tuning thus making the system\nmore robust to pronunciation variation.", "published": "2023-05-12 11:29:13", "link": "http://arxiv.org/abs/2305.07389v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Two-in-One: A Model Hijacking Attack Against Text Generation Models", "abstract": "Machine learning has progressed significantly in various applications ranging\nfrom face recognition to text generation. However, its success has been\naccompanied by different attacks. Recently a new attack has been proposed which\nraises both accountability and parasitic computing risks, namely the model\nhijacking attack. Nevertheless, this attack has only focused on image\nclassification tasks. In this work, we broaden the scope of this attack to\ninclude text generation and classification models, hence showing its broader\napplicability. More concretely, we propose a new model hijacking attack, Ditto,\nthat can hijack different text classification tasks into multiple generation\nones, e.g., language translation, text summarization, and language modeling. We\nuse a range of text benchmark datasets such as SST-2, TweetEval, AGnews, QNLI,\nand IMDB to evaluate the performance of our attacks. Our results show that by\nusing Ditto, an adversary can successfully hijack text generation models\nwithout jeopardizing their utility.", "published": "2023-05-12 12:13:27", "link": "http://arxiv.org/abs/2305.07406v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Improving Cascaded Unsupervised Speech Translation with Denoising\n  Back-translation", "abstract": "Most of the speech translation models heavily rely on parallel data, which is\nhard to collect especially for low-resource languages. To tackle this issue, we\npropose to build a cascaded speech translation system without leveraging any\nkind of paired data. We use fully unpaired data to train our unsupervised\nsystems and evaluate our results on CoVoST 2 and CVSS. The results show that\nour work is comparable with some other early supervised methods in some\nlanguage pairs. While cascaded systems always suffer from severe error\npropagation problems, we proposed denoising back-translation (DBT), a novel\napproach to building robust unsupervised neural machine translation (UNMT). DBT\nsuccessfully increases the BLEU score by 0.7--0.9 in all three translation\ndirections. Moreover, we simplified the pipeline of our cascaded system to\nreduce inference latency and conducted a comprehensive analysis of every part\nof our work. We also demonstrate our unsupervised speech translation results on\nthe established website.", "published": "2023-05-12 13:07:51", "link": "http://arxiv.org/abs/2305.07455v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Memory Model for Question Answering from Streaming Data Supported by\n  Rehearsal and Anticipation of Coreference Information", "abstract": "Existing question answering methods often assume that the input content\n(e.g., documents or videos) is always accessible to solve the task.\nAlternatively, memory networks were introduced to mimic the human process of\nincremental comprehension and compression of the information in a\nfixed-capacity memory. However, these models only learn how to maintain memory\nby backpropagating errors in the answers through the entire network. Instead,\nit has been suggested that humans have effective mechanisms to boost their\nmemorization capacities, such as rehearsal and anticipation. Drawing\ninspiration from these, we propose a memory model that performs rehearsal and\nanticipation while processing inputs to memorize important information for\nsolving question answering tasks from streaming data. The proposed mechanisms\nare applied self-supervised during training through masked modeling tasks\nfocused on coreference information. We validate our model on a short-sequence\n(bAbI) dataset as well as large-sequence textual (NarrativeQA) and video\n(ActivityNet-QA) question answering datasets, where it achieves substantial\nimprovements over previous memory network approaches. Furthermore, our ablation\nstudy confirms the proposed mechanisms' importance for memory models.", "published": "2023-05-12 15:46:36", "link": "http://arxiv.org/abs/2305.07565v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large\n  Language Model Recommendation", "abstract": "The remarkable achievements of Large Language Models (LLMs) have led to the\nemergence of a novel recommendation paradigm -- Recommendation via LLM\n(RecLLM). Nevertheless, it is important to note that LLMs may contain social\nprejudices, and therefore, the fairness of recommendations made by RecLLM\nrequires further investigation. To avoid the potential risks of RecLLM, it is\nimperative to evaluate the fairness of RecLLM with respect to various sensitive\nattributes on the user side. Due to the differences between the RecLLM paradigm\nand the traditional recommendation paradigm, it is problematic to directly use\nthe fairness benchmark of traditional recommendation. To address the dilemma,\nwe propose a novel benchmark called Fairness of Recommendation via LLM\n(FaiRLLM). This benchmark comprises carefully crafted metrics and a dataset\nthat accounts for eight sensitive attributes1 in two recommendation scenarios:\nmusic and movies. By utilizing our FaiRLLM benchmark, we conducted an\nevaluation of ChatGPT and discovered that it still exhibits unfairness to some\nsensitive attributes when generating recommendations. Our code and dataset can\nbe found at https://github.com/jizhi-zhang/FaiRLLM.", "published": "2023-05-12 16:54:36", "link": "http://arxiv.org/abs/2305.07609v3", "categories": ["cs.IR", "cs.CL", "cs.CY"], "primary_category": "cs.IR"}
{"title": "PALR: Personalization Aware LLMs for Recommendation", "abstract": "Large language models (LLMs) have recently received significant attention for\ntheir exceptional capabilities. Despite extensive efforts in developing\ngeneral-purpose LLMs that can be utilized in various natural language\nprocessing (NLP) tasks, there has been less research exploring their potential\nin recommender systems. In this paper, we propose a novel framework, named\nPALR, which aiming to combine user history behaviors (such as clicks,\npurchases, ratings, etc.) with LLMs to generate user preferred items.\nSpecifically, we first use user/item interactions as guidance for candidate\nretrieval. Then we adopt a LLM-based ranking model to generate recommended\nitems. Unlike existing approaches that typically adopt general-purpose LLMs for\nzero/few-shot recommendation testing or training on small-sized language models\n(with less than 1 billion parameters), which cannot fully elicit LLMs'\nreasoning abilities and leverage rich item side parametric knowledge, we\nfine-tune a 7 billion parameters LLM for the ranking purpose. This model takes\nretrieval candidates in natural language format as input, with instruction\nwhich explicitly asking to select results from input candidates during\ninference. Our experimental results demonstrate that our solution outperforms\nstate-of-the-art models on various sequential recommendation tasks.", "published": "2023-05-12 17:21:33", "link": "http://arxiv.org/abs/2305.07622v3", "categories": ["cs.IR", "cs.AI", "cs.CL", "I.2.6; I.2.7"], "primary_category": "cs.IR"}
{"title": "Text2Cohort: Facilitating Intuitive Access to Biomedical Data with\n  Natural Language Cohort Discovery", "abstract": "The Imaging Data Commons (IDC) is a cloud-based database that provides\nresearchers with open access to cancer imaging data, with the goal of\nfacilitating collaboration. However, cohort discovery within the IDC database\nhas a significant technical learning curve. Recently, large language models\n(LLM) have demonstrated exceptional utility for natural language processing\ntasks. We developed Text2Cohort, a LLM-powered toolkit to facilitate\nuser-friendly natural language cohort discovery in the IDC. Our method\ntranslates user input into IDC queries using grounding techniques and returns\nthe query's response. We evaluate Text2Cohort on 50 natural language inputs,\nfrom information extraction to cohort discovery. Our toolkit successfully\ngenerated responses with an 88% accuracy and 0.94 F1 score. We demonstrate that\nText2Cohort can enable researchers to discover and curate cohorts on IDC with\nhigh levels of accuracy using natural language in a more intuitive and\nuser-friendly way.", "published": "2023-05-12 17:46:06", "link": "http://arxiv.org/abs/2305.07637v3", "categories": ["cs.LG", "cs.CL", "cs.HC", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Using Language Models to Detect Alarming Student Responses", "abstract": "This article details the advances made to a system that uses artificial\nintelligence to identify alarming student responses. This system is built into\nour assessment platform to assess whether a student's response indicates they\nare a threat to themselves or others. Such responses may include details\nconcerning threats of violence, severe depression, suicide risks, and\ndescriptions of abuse. Driven by advances in natural language processing, the\nlatest model is a fine-tuned language model trained on a large corpus\nconsisting of student responses and supplementary texts. We demonstrate that\nthe use of a language model delivers a substantial improvement in accuracy over\nthe previous iterations of this system.", "published": "2023-05-12 18:07:00", "link": "http://arxiv.org/abs/2305.07709v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent\n  English?", "abstract": "Language models (LMs) are powerful tools for natural language processing, but\nthey often struggle to produce coherent and fluent text when they are small.\nModels with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can\nrarely generate coherent and consistent English text beyond a few words even\nafter extensive training. This raises the question of whether the emergence of\nthe ability to produce coherent English text only occurs at larger scales (with\nhundreds of millions of parameters or more) and complex architectures (with\nmany layers of global attention).\n  In this work, we introduce TinyStories, a synthetic dataset of short stories\nthat only contain words that a typical 3 to 4-year-olds usually understand,\ngenerated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train\nand evaluate LMs that are much smaller than the state-of-the-art models (below\n10 million total parameters), or have much simpler architectures (with only one\ntransformer block), yet still produce fluent and consistent stories with\nseveral paragraphs that are diverse and have almost perfect grammar, and\ndemonstrate reasoning capabilities.\n  We also introduce a new paradigm for the evaluation of language models: We\nsuggest a framework which uses GPT-4 to grade the content generated by these\nmodels as if those were stories written by students and graded by a (human)\nteacher. This new paradigm overcomes the flaws of standard benchmarks which\noften requires the model's output to be very structures, and moreover provides\na multidimensional score for the model, providing scores for different\ncapabilities such as grammar, creativity and consistency.\n  We hope that TinyStories can facilitate the development, analysis and\nresearch of LMs, especially for low-resource or specialized domains, and shed\nlight on the emergence of language capabilities in LMs.", "published": "2023-05-12 20:56:48", "link": "http://arxiv.org/abs/2305.07759v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Melody-Guided Lyrics Generation", "abstract": "Automatic song writing is a topic of significant practical interest. However,\nits research is largely hindered by the lack of training data due to copyright\nconcerns and challenged by its creative nature. Most noticeably, prior works\noften fall short of modeling the cross-modal correlation between melody and\nlyrics due to limited parallel data, hence generating lyrics that are less\nsingable. Existing works also lack effective mechanisms for content control, a\nmuch desired feature for democratizing song creation for people with limited\nmusic background. In this work, we propose to generate pleasantly listenable\nlyrics without training on melody-lyric aligned data. Instead, we design a\nhierarchical lyric generation framework that disentangles training (based\npurely on text) from inference (melody-guided text generation). At inference\ntime, we leverage the crucial alignments between melody and lyrics and compile\nthe given melody into constraints to guide the generation process. Evaluation\nresults show that our model can generate high-quality lyrics that are more\nsingable, intelligible, coherent, and in rhyme than strong baselines including\nthose supervised on parallel data.", "published": "2023-05-12 20:57:20", "link": "http://arxiv.org/abs/2305.07760v2", "categories": ["cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.AI"}
{"title": "IMAGINATOR: Pre-Trained Image+Text Joint Embeddings using Word-Level\n  Grounding of Images", "abstract": "Word embeddings, i.e., semantically meaningful vector representation of\nwords, are largely influenced by the distributional hypothesis \"You shall know\na word by the company it keeps\" (Harris, 1954), whereas modern prediction-based\nneural network embeddings rely on design choices and hyperparameter\noptimization. Word embeddings like Word2Vec, GloVe etc. well capture the\ncontextuality and real-world analogies but contemporary convolution-based image\nembeddings such as VGGNet, AlexNet, etc. do not capture contextual knowledge.\nThe popular king-queen analogy does not hold true for most commonly used vision\nembeddings.\n  In this paper, we introduce a pre-trained joint embedding (JE), named\nIMAGINATOR, trained on 21K distinct image objects level from 1M image+text\npairs. JE is a way to encode multimodal data into a vector space where the text\nmodality serves as the ground-ing key, which the complementary modality (in\nthis case, the image) is anchored with. IMAGINATOR encapsulates three\nindividual representations: (i) object-object co-location, (ii) word-object\nco-location, and (iii) word-object correlation. These three ways capture\ncomplementary aspects of the two modalities which are further combined to\nobtain the final JEs.\n  Generated JEs are intrinsically evaluated to assess how well they capture the\ncontextuality and real-world analogies. We also evaluate pre-trained IMAGINATOR\nJEs on three downstream tasks: (i) image captioning, (ii) Image2Tweet, and\n(iii) text-based image retrieval. IMAGINATOR establishes a new standard on the\naforementioned down-stream tasks by outperforming the current SoTA on all the\nselected tasks. IMAGINATOR will be made publicly available. The codes are\navailable at https://github.com/varunakk/IMAGINATOR", "published": "2023-05-12 05:34:52", "link": "http://arxiv.org/abs/2305.10438v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Semantic-aware Digital Twin for Metaverse: A Comprehensive Review", "abstract": "To facilitate the deployment of digital twins in Metaverse, the paradigm with\nsemantic awareness has been proposed as a means for enabling accurate and\ntask-oriented information extraction with inherent intelligence. However, this\nframework requires all devices in the Metaverse environment to be directly\nlinked with the semantic model to enable faithful interpretation of messages.\nIn contrast, this article introduces the digital twin framework, considering a\nsmart industrial application, which enables semantic communication in\nconjugation with the Metaverse enabling technologies. The fundamentals of this\nframework are demonstrated on an industrial shopfloor management use case with\na digital twin so as to improve its performance through semantic communication.\nAn overview of semantic communication, Metaverse, and digital twins is\npresented. Integration of these technologies with the basic architecture as\nwell as the impact on future industrial applications is presented. In a\nnutshell, this article showcases how semantic awareness can be an effective\ncandidate in the implementation of digital twins for Metaverse applications.", "published": "2023-05-12 09:19:30", "link": "http://arxiv.org/abs/2305.18304v1", "categories": ["cs.CY", "cs.CL", "cs.IR", "cs.MM", "A.1; H.5; I.6; J.7; F.4"], "primary_category": "cs.CY"}
{"title": "Multi-level Temporal-channel Speaker Retrieval for Zero-shot Voice\n  Conversion", "abstract": "Zero-shot voice conversion (VC) converts source speech into the voice of any\ndesired speaker using only one utterance of the speaker without requiring\nadditional model updates. Typical methods use a speaker representation from a\npre-trained speaker verification (SV) model or learn speaker representation\nduring VC training to achieve zero-shot VC. However, existing speaker modeling\nmethods overlook the variation of speaker information richness in temporal and\nfrequency channel dimensions of speech. This insufficient speaker modeling\nhampers the ability of the VC model to accurately represent unseen speakers who\nare not in the training dataset. In this study, we present a robust zero-shot\nVC model with multi-level temporal-channel retrieval, referred to as MTCR-VC.\nSpecifically, to flexibly adapt to the dynamic-variant speaker characteristic\nin the temporal and channel axis of the speech, we propose a novel fine-grained\nspeaker modeling method, called temporal-channel retrieval (TCR), to find out\nwhen and where speaker information appears in speech. It retrieves\nvariable-length speaker representation from both temporal and channel\ndimensions under the guidance of a pre-trained SV model. Besides, inspired by\nthe hierarchical process of human speech production, the MTCR speaker module\nstacks several TCR blocks to extract speaker representations from\nmulti-granularity levels. Furthermore, to achieve better speech disentanglement\nand reconstruction, we introduce a cycle-based training strategy to simulate\nzero-shot inference recurrently. We adopt perpetual constraints on three\naspects, including content, style, and speaker, to drive this process.\nExperiments demonstrate that MTCR-VC is superior to the previous zero-shot VC\nmethods in modeling speaker timbre while maintaining good speech naturalness.", "published": "2023-05-12 02:36:04", "link": "http://arxiv.org/abs/2305.07204v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Continual Learning for End-to-End ASR by Averaging Domain Experts", "abstract": "Continual learning for end-to-end automatic speech recognition has to contend\nwith a number of difficulties. Fine-tuning strategies tend to lose performance\non data already seen, a process known as catastrophic forgetting. On the other\nhand, strategies that freeze parameters and append tunable parameters must\nmaintain multiple models. We suggest a strategy that maintains only a single\nmodel for inference and avoids catastrophic forgetting.\n  Our experiments show that a simple linear interpolation of several models'\nparameters, each fine-tuned from the same generalist model, results in a single\nmodel that performs well on all tested data. For our experiments we selected\ntwo open-source end-to-end speech recognition models pre-trained on large\ndatasets and fine-tuned them on 3 separate datasets: SGPISpeech, CORAAL, and\nDiPCo. The proposed average of domain experts model performs well on all tested\ndata, and has almost no loss in performance on data from the domain of original\ntraining.", "published": "2023-05-12 16:19:30", "link": "http://arxiv.org/abs/2305.09681v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Versatile audio-visual learning for emotion recognition", "abstract": "Most current audio-visual emotion recognition models lack the flexibility\nneeded for deployment in practical applications. We envision a multimodal\nsystem that works even when only one modality is available and can be\nimplemented interchangeably for either predicting emotional attributes or\nrecognizing categorical emotions. Achieving such flexibility in a multimodal\nemotion recognition system is difficult due to the inherent challenges in\naccurately interpreting and integrating varied data sources. It is also a\nchallenge to robustly handle missing or partial information while allowing\ndirect switch between regression or classification tasks. This study proposes a\nversatile audio-visual learning (VAVL) framework for handling unimodal and\nmultimodal systems for emotion regression or emotion classification tasks. We\nimplement an audio-visual framework that can be trained even when audio and\nvisual paired data is not available for part of the training set (i.e., audio\nonly or only video is present). We achieve this effective representation\nlearning with audio-visual shared layers, residual connections over shared\nlayers, and a unimodal reconstruction task. Our experimental results reveal\nthat our architecture significantly outperforms strong baselines on the\nCREMA-D, MSP-IMPROV, and CMU-MOSEI corpora. Notably, VAVL attains a new\nstate-of-the-art performance in the emotional attribute prediction task on the\nMSP-IMPROV corpus.", "published": "2023-05-12 03:13:37", "link": "http://arxiv.org/abs/2305.07216v2", "categories": ["cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Transavs: End-To-End Audio-Visual Segmentation With Transformer", "abstract": "Audio-Visual Segmentation (AVS) is a challenging task, which aims to segment\nsounding objects in video frames by exploring audio signals. Generally AVS\nfaces two key challenges: (1) Audio signals inherently exhibit a high degree of\ninformation density, as sounds produced by multiple objects are entangled\nwithin the same audio stream; (2) Objects of the same category tend to produce\nsimilar audio signals, making it difficult to distinguish between them and thus\nleading to unclear segmentation results. Toward this end, we propose TransAVS,\nthe first Transformer-based end-to-end framework for AVS task. Specifically,\nTransAVS disentangles the audio stream as audio queries, which will interact\nwith images and decode into segmentation masks with full transformer\narchitectures. This scheme not only promotes comprehensive audio-image\ncommunication but also explicitly excavates instance cues encapsulated in the\nscene. Meanwhile, to encourage these audio queries to capture distinctive\nsounding objects instead of degrading to be homogeneous, we devise two\nself-supervised loss functions at both query and mask levels, allowing the\nmodel to capture distinctive features within similar audio data and achieve\nmore precise segmentation. Our experiments demonstrate that TransAVS achieves\nstate-of-the-art results on the AVSBench dataset, highlighting its\neffectiveness in bridging the gap between audio and visual modalities.", "published": "2023-05-12 03:31:04", "link": "http://arxiv.org/abs/2305.07223v2", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Music Rearrangement Using Hierarchical Segmentation", "abstract": "Music rearrangement involves reshuffling, deleting, and repeating sections of\na music piece with the goal of producing a standalone version that has a\ndifferent duration. It is a creative and time-consuming task commonly performed\nby an expert music engineer. In this paper, we propose a method for\nautomatically rearranging music recordings that takes into account the\nhierarchical structure of the recording. Previous approaches focus solely on\nidentifying cut-points in the audio that could result in smooth transitions. We\ninstead utilize deep audio representations to hierarchically segment the piece\nand define a cut-point search subject to the boundaries and musical functions\nof the segments. We score suitable entry- and exit-point pairs based on their\nsimilarity and the segments they belong to, and define an optimal path search.\nExperimental results demonstrate the selected cut-points are most commonly\nimperceptible by listeners and result in more consistent musical development\nwith less distracting repetitions.", "published": "2023-05-12 09:50:54", "link": "http://arxiv.org/abs/2305.07347v1", "categories": ["cs.SD", "cs.IR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Benchmarks and leaderboards for sound demixing tasks", "abstract": "Music demixing is the task of separating different tracks from the given\nsingle audio signal into components, such as drums, bass, and vocals from the\nrest of the accompaniment. Separation of sources is useful for a range of\nareas, including entertainment and hearing aids. In this paper, we introduce\ntwo new benchmarks for the sound source separation tasks and compare popular\nmodels for sound demixing, as well as their ensembles, on these benchmarks. For\nthe models' assessments, we provide the leaderboard at\nhttps://mvsep.com/quality_checker/, giving a comparison for a range of models.\nThe new benchmark datasets are available for download. We also develop a novel\napproach for audio separation, based on the ensembling of different models that\nare suited best for the particular stem. The proposed solution was evaluated in\nthe context of the Music Demixing Challenge 2023 and achieved top results in\ndifferent tracks of the challenge. The code and the approach are open-sourced\non GitHub.", "published": "2023-05-12 14:00:26", "link": "http://arxiv.org/abs/2305.07489v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Device-Robust Acoustic Scene Classification via Impulse Response\n  Augmentation", "abstract": "The ability to generalize to a wide range of recording devices is a crucial\nperformance factor for audio classification models. The characteristics of\ndifferent types of microphones introduce distributional shifts in the digitized\naudio signals due to their varying frequency responses. If this domain shift is\nnot taken into account during training, the model's performance could degrade\nseverely when it is applied to signals recorded by unseen devices. In\nparticular, training a model on audio signals recorded with a small number of\ndifferent microphones can make generalization to unseen devices difficult. To\ntackle this problem, we convolve audio signals in the training set with\npre-recorded device impulse responses (DIRs) to artificially increase the\ndiversity of recording devices. We systematically study the effect of DIR\naugmentation on the task of Acoustic Scene Classification using CNNs and Audio\nSpectrogram Transformers. The results show that DIR augmentation in isolation\nperforms similarly to the state-of-the-art method Freq-MixStyle. However, we\nalso show that DIR augmentation and Freq-MixStyle are complementary, achieving\na new state-of-the-art performance on signals recorded by devices unseen during\ntraining.", "published": "2023-05-12 14:12:56", "link": "http://arxiv.org/abs/2305.07499v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
