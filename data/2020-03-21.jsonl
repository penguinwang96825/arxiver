{"title": "Probing Word Translations in the Transformer and Trading Decoder for\n  Encoder Layers", "abstract": "Due to its effectiveness and performance, the Transformer translation model\nhas attracted wide attention, most recently in terms of probing-based\napproaches. Previous work focuses on using or probing source linguistic\nfeatures in the encoder. To date, the way word translation evolves in\nTransformer layers has not yet been investigated. Naively, one might assume\nthat encoder layers capture source information while decoder layers translate.\nIn this work, we show that this is not quite the case: translation already\nhappens progressively in encoder layers and even in the input embeddings. More\nsurprisingly, we find that some of the lower decoder layers do not actually do\nthat much decoding. We show all of this in terms of a probing approach where we\nproject representations of the layer analyzed to the final trained and frozen\nclassifier level of the Transformer decoder to measure word translation\naccuracy. Our findings motivate and explain a Transformer configuration change:\nif translation already happens in the encoder layers, perhaps we can increase\nthe number of encoder layers, while decreasing the number of decoder layers,\nboosting decoding speed, without loss in translation quality? Our experiments\nshow that this is indeed the case: we can increase speed by up to a factor 2.3\nwith small gains in translation quality, while an 18-4 deep encoder\nconfiguration boosts translation quality by +1.42 BLEU (En-De) at a speed-up of\n1.4.", "published": "2020-03-21 06:12:14", "link": "http://arxiv.org/abs/2003.09586v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Joint Approach to Compound Splitting and Idiomatic Compound Detection", "abstract": "Applications such as machine translation, speech recognition, and information\nretrieval require efficient handling of noun compounds as they are one of the\npossible sources for out-of-vocabulary (OOV) words. In-depth processing of noun\ncompounds requires not only splitting them into smaller components (or even\nroots) but also the identification of instances that should remain unsplitted\nas they are of idiomatic nature. We develop a two-fold deep learning-based\napproach of noun compound splitting and idiomatic compound detection for the\nGerman language that we train using a newly collected corpus of annotated\nGerman compounds. Our neural noun compound splitter operates on a sub-word\nlevel and outperforms the current state of the art by about 5%.", "published": "2020-03-21 09:00:52", "link": "http://arxiv.org/abs/2003.09606v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis in Drug Reviews using Supervised Machine Learning\n  Algorithms", "abstract": "Sentiment Analysis is an important algorithm in Natural Language Processing\nwhich is used to detect sentiment within some text. In our project, we had\nchosen to work on analyzing reviews of various drugs which have been reviewed\nin form of texts and have also been given a rating on a scale from 1-10. We had\nobtained this data set from the UCI machine learning repository which had 2\ndata sets: train and test (split as 75-25\\%). We had split the number rating\nfor the drug into three classes in general: positive (7-10), negative (1-4) or\nneutral(4-7). There are multiple reviews for the drugs that belong to a similar\ncondition and we decided to investigate how the reviews for different\nconditions use different words impact the ratings of the drugs. Our intention\nwas mainly to implement supervised machine learning classification algorithms\nthat predict the class of the rating using the textual review. We had primarily\nimplemented different embeddings such as Term Frequency Inverse Document\nFrequency (TFIDF) and the Count Vectors (CV). We had trained models on the most\npopular conditions such as \"Birth Control\", \"Depression\" and \"Pain\" within the\ndata set and obtained good results while predicting the test data sets.", "published": "2020-03-21 20:13:11", "link": "http://arxiv.org/abs/2003.11643v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Deep Generative Variational Autoencoding for Replay Spoof Detection in\n  Automatic Speaker Verification", "abstract": "Automatic speaker verification (ASV) systems are highly vulnerable to\npresentation attacks, also called spoofing attacks. Replay is among the\nsimplest attacks to mount - yet difficult to detect reliably. The\ngeneralization failure of spoofing countermeasures (CMs) has driven the\ncommunity to study various alternative deep learning CMs. The majority of them\nare supervised approaches that learn a human-spoof discriminator. In this\npaper, we advocate a different, deep generative approach that leverages from\npowerful unsupervised manifold learning in classification. The potential\nbenefits include the possibility to sample new data, and to obtain insights to\nthe latent features of genuine and spoofed speech. To this end, we propose to\nuse variational autoencoders (VAEs) as an alternative backend for replay attack\ndetection, via three alternative models that differ in their\nclass-conditioning. The first one, similar to the use of Gaussian mixture\nmodels (GMMs) in spoof detection, is to train independently two VAEs - one for\neach class. The second one is to train a single conditional model (C-VAE) by\ninjecting a one-hot class label vector to the encoder and decoder networks. Our\nfinal proposal integrates an auxiliary classifier to guide the learning of the\nlatent space. Our experimental results using constant-Q cepstral coefficient\n(CQCC) features on the ASVspoof 2017 and 2019 physical access subtask datasets\nindicate that the C-VAE offers substantial improvement in comparison to\ntraining two separate VAEs for each class. On the 2019 dataset, the C-VAE\noutperforms the VAE and the baseline GMM by an absolute 9 - 10% in both equal\nerror rate (EER) and tandem detection cost function (t-DCF) metrics. Finally,\nwe propose VAE residuals - the absolute difference of the original input and\nthe reconstruction as features for spoofing detection.", "published": "2020-03-21 00:56:05", "link": "http://arxiv.org/abs/2003.09542v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Quantum Vocal Theory of Sound", "abstract": "Concepts and formalism from acoustics are often used to exemplify quantum\nmechanics. Conversely, quantum mechanics could be used to achieve a new\nperspective on acoustics, as shown by Gabor studies. Here, we focus in\nparticular on the study of human voice, considered as a probe to investigate\nthe world of sounds. We present a theoretical framework that is based on\nobservables of vocal production, and on some measurement apparati that can be\nused both for analysis and synthesis. In analogy to the description of spin\nstates of a particle, the quantum-mechanical formalism is used to describe the\nrelations between the fundamental states associated with phonetic labels such\nas phonation, turbulence, and supraglottal myoelastic vibrations. The\nintermingling of these states, and their temporal evolution, can still be\ninterpreted in the Fourier/Gabor plane, and effective extractors can be\nimplemented. The bases for a Quantum Vocal Theory of Sound, with implications\nin sound analysis and design, are presented.", "published": "2020-03-21 11:14:05", "link": "http://arxiv.org/abs/2003.09632v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
