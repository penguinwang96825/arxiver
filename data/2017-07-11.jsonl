{"title": "Refining Raw Sentence Representations for Textual Entailment Recognition\n  via Attention", "abstract": "In this paper we present the model used by the team Rivercorners for the 2017\nRepEval shared task. First, our model separately encodes a pair of sentences\ninto variable-length representations by using a bidirectional LSTM. Later, it\ncreates fixed-length raw representations by means of simple aggregation\nfunctions, which are then refined using an attention mechanism. Finally it\ncombines the refined representations of both sentences into a single vector to\nbe used for classification. With this model we obtained test accuracies of\n72.057% and 72.055% in the matched and mismatched evaluation tracks\nrespectively, outperforming the LSTM baseline, and obtaining performances\nsimilar to a model that relies on shared information between sentences (ESIM).\nWhen using an ensemble both accuracies increased to 72.247% and 72.827%\nrespectively.", "published": "2017-07-11 02:02:09", "link": "http://arxiv.org/abs/1707.03103v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dataset for a Neural Natural Language Interface for Databases (NNLIDB)", "abstract": "Progress in natural language interfaces to databases (NLIDB) has been slow\nmainly due to linguistic issues (such as language ambiguity) and domain\nportability. Moreover, the lack of a large corpus to be used as a standard\nbenchmark has made data-driven approaches difficult to develop and compare. In\nthis paper, we revisit the problem of NLIDBs and recast it as a sequence\ntranslation problem. To this end, we introduce a large dataset extracted from\nthe Stack Exchange Data Explorer website, which can be used for training neural\nnatural language interfaces for databases. We also report encouraging baseline\nresults on a smaller manually annotated test corpus, obtained using an\nattention-based sequence-to-sequence neural network.", "published": "2017-07-11 08:33:55", "link": "http://arxiv.org/abs/1707.03172v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A non-projective greedy dependency parser with bidirectional LSTMs", "abstract": "The LyS-FASTPARSE team presents BIST-COVINGTON, a neural implementation of\nthe Covington (2001) algorithm for non-projective dependency parsing. The\nbidirectional LSTM approach by Kipperwasser and Goldberg (2016) is used to\ntrain a greedy parser with a dynamic oracle to mitigate error propagation. The\nmodel participated in the CoNLL 2017 UD Shared Task. In spite of not using any\nensemble methods and using the baseline segmentation and PoS tagging, the\nparser obtained good results on both macro-average LAS and UAS in the big\ntreebanks category (55 languages), ranking 7th out of 33 teams. In the all\ntreebanks category (LAS and UAS) we ranked 16th and 12th. The gap between the\nall and big categories is mainly due to the poor performance on four parallel\nPUD treebanks, suggesting that some `suffixed' treebanks (e.g. Spanish-AnCora)\nperform poorly on cross-treebank settings, which does not occur with the\ncorresponding `unsuffixed' treebank (e.g. Spanish). By changing that, we obtain\nthe 11th best LAS among all runs (official and unofficial). The code is made\navailable at https://github.com/CoNLL-UD-2017/LyS-FASTPARSE", "published": "2017-07-11 11:44:31", "link": "http://arxiv.org/abs/1707.03228v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leipzig Corpus Miner - A Text Mining Infrastructure for Qualitative Data\n  Analysis", "abstract": "This paper presents the \"Leipzig Corpus Miner\", a technical infrastructure\nfor supporting qualitative and quantitative content analysis. The\ninfrastructure aims at the integration of 'close reading' procedures on\nindividual documents with procedures of 'distant reading', e.g. lexical\ncharacteristics of large document collections. Therefore information retrieval\nsystems, lexicometric statistics and machine learning procedures are combined\nin a coherent framework which enables qualitative data analysts to make use of\nstate-of-the-art Natural Language Processing techniques on very large document\ncollections. Applicability of the framework ranges from social sciences to\nmedia studies and market research. As an example we introduce the usage of the\nframework in a political science study on post-democracy and neoliberalism.", "published": "2017-07-11 13:04:15", "link": "http://arxiv.org/abs/1707.03253v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling the dynamics of domain specific terminology in diachronic\n  corpora", "abstract": "In terminology work, natural language processing, and digital humanities,\nseveral studies address the analysis of variations in context and meaning of\nterms in order to detect semantic change and the evolution of terms. We\ndistinguish three different approaches to describe contextual variations:\nmethods based on the analysis of patterns and linguistic clues, methods\nexploring the latent semantic space of single words, and methods for the\nanalysis of topic membership. The paper presents the notion of context\nvolatility as a new measure for detecting semantic change and applies it to key\nterm extraction in a political science case study. The measure quantifies the\ndynamics of a term's contextual variation within a diachronic corpus to\nidentify periods of time that are characterised by intense controversial\ndebates or substantial semantic transformations.", "published": "2017-07-11 13:11:45", "link": "http://arxiv.org/abs/1707.03255v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A simple but tough-to-beat baseline for the Fake News Challenge stance\n  detection task", "abstract": "Identifying public misinformation is a complicated and challenging task. An\nimportant part of checking the veracity of a specific claim is to evaluate the\nstance different news sources take towards the assertion. Automatic stance\nevaluation, i.e. stance detection, would arguably facilitate the process of\nfact checking. In this paper, we present our stance detection system which\nclaimed third place in Stage 1 of the Fake News Challenge. Despite our\nstraightforward approach, our system performs at a competitive level with the\ncomplex ensembles of the top two winning teams. We therefore propose our system\nas the 'simple but tough-to-beat baseline' for the Fake News Challenge stance\ndetection task.", "published": "2017-07-11 13:44:51", "link": "http://arxiv.org/abs/1707.03264v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multiple Context-Free Tree Grammars: Lexicalization and Characterization", "abstract": "Multiple (simple) context-free tree grammars are investigated, where \"simple\"\nmeans \"linear and nondeleting\". Every multiple context-free tree grammar that\nis finitely ambiguous can be lexicalized; i.e., it can be transformed into an\nequivalent one (generating the same tree language) in which each rule of the\ngrammar contains a lexical symbol. Due to this transformation, the rank of the\nnonterminals increases at most by 1, and the multiplicity (or fan-out) of the\ngrammar increases at most by the maximal rank of the lexical symbols; in\nparticular, the multiplicity does not increase when all lexical symbols have\nrank 0. Multiple context-free tree grammars have the same tree generating power\nas multi-component tree adjoining grammars (provided the latter can use a\nroot-marker). Moreover, every multi-component tree adjoining grammar that is\nfinitely ambiguous can be lexicalized. Multiple context-free tree grammars have\nthe same string generating power as multiple context-free (string) grammars and\npolynomial time parsing algorithms. A tree language can be generated by a\nmultiple context-free tree grammar if and only if it is the image of a regular\ntree language under a deterministic finite-copying macro tree transducer.\nMultiple context-free tree grammars can be used as a synchronous translation\ndevice.", "published": "2017-07-11 20:48:57", "link": "http://arxiv.org/abs/1707.03457v1", "categories": ["cs.FL", "cs.CL", "68Q45", "F.4.2; F.4.3"], "primary_category": "cs.FL"}
{"title": "Look Who's Talking: Bipartite Networks as Representations of a Topic\n  Model of New Zealand Parliamentary Speeches", "abstract": "Quantitative methods to measure the participation to parliamentary debate and\ndiscourse of elected Members of Parliament (MPs) and the parties they belong to\nare lacking. This is an exploratory study in which we propose the development\nof a new approach for a quantitative analysis of such participation. We utilize\nthe New Zealand government's digital Hansard database to construct a topic\nmodel of parliamentary speeches consisting of nearly 40 million words in the\nperiod 2003-2016. A Latent Dirichlet Allocation topic model is implemented in\norder to reveal the thematic structure of our set of documents. This generative\nstatistical model enables the detection of major themes or topics that are\npublicly discussed in the New Zealand parliament, as well as permitting their\nclassification by MP. Information on topic proportions is subsequently analyzed\nusing a combination of statistical methods. We observe patterns arising from\ntime-series analysis of topic frequencies which can be related to specific\nsocial, economic and legislative events. We then construct a bipartite network\nrepresentation, linking MPs to topics, for each of four parliamentary terms in\nthis time frame. We build projected networks (onto the set of nodes represented\nby MPs) and proceed to the study of the dynamical changes of their topology,\nincluding community structure. By performing this longitudinal network\nanalysis, we can observe the evolution of the New Zealand parliamentary topic\nnetwork and its main parties in the period studied.", "published": "2017-07-11 01:25:31", "link": "http://arxiv.org/abs/1707.03095v3", "categories": ["cs.CL", "cs.DL", "cs.SI", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Detecting Policy Preferences and Dynamics in the UN General Debate with\n  Neural Word Embeddings", "abstract": "Foreign policy analysis has been struggling to find ways to measure policy\npreferences and paradigm shifts in international political systems. This paper\npresents a novel, potential solution to this challenge, through the application\nof a neural word embedding (Word2vec) model on a dataset featuring speeches by\nheads of state or government in the United Nations General Debate. The paper\nprovides three key contributions based on the output of the Word2vec model.\nFirst, it presents a set of policy attention indices, synthesizing the semantic\nproximity of political speeches to specific policy themes. Second, it\nintroduces country-specific semantic centrality indices, based on topological\nanalyses of countries' semantic positions with respect to each other. Third, it\ntests the hypothesis that there exists a statistical relation between the\nsemantic content of political speeches and UN voting behavior, falsifying it\nand suggesting that political speeches contain information of different nature\nthen the one behind voting outcomes. The paper concludes with a discussion of\nthe practical use of its results and consequences for foreign policy analysis,\npublic accountability, and transparency.", "published": "2017-07-11 23:16:20", "link": "http://arxiv.org/abs/1707.03490v1", "categories": ["cs.CL", "cs.AI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "On the letter frequencies and entropy of written Marathi", "abstract": "We carry out a comprehensive analysis of letter frequencies in contemporary\nwritten Marathi. We determine sets of letters which statistically predominate\nany large generic Marathi text, and use these sets to estimate the entropy of\nMarathi.", "published": "2017-07-11 19:52:56", "link": "http://arxiv.org/abs/1707.08209v1", "categories": ["cs.IT", "cs.CL", "math.IT", "H.1.1; E.4"], "primary_category": "cs.IT"}
