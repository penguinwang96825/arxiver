{"title": "The Narayana Morphism and Related Words", "abstract": "The Narayana morphism $\\nu$ maps $0 \\rightarrow 01$, $1 \\rightarrow 2$, $2\n\\rightarrow 0$ and has a fixed point $\\mathbf{n} = n_0 n_1 n_2 \\cdots = {\\tt\n0120010120120}\\cdots$. In this paper we study the properties of this word and\nrelated words using automata theory.", "published": "2025-03-02 21:19:18", "link": "http://arxiv.org/abs/2503.01026v3", "categories": ["math.CO", "cs.DM", "cs.FL", "math.NT"], "primary_category": "math.CO"}
{"title": "Soft Barycentric Refinement", "abstract": "The soft Barycentric refinement preserves manifolds with or without boundary.\nIn every dimension larger than one, there is a universal spectral central\nlimiting measure that has affinities with the Barycentric limiting measure one\ndimension lower. Ricci type quantities like the length of the dual sphere of\nco-dimension-2 simplex stay invariant under soft refinements. We prove that the\ndual graphs of any manifold can be colored with 3 colors, which is in the\n2-dimensional case a special case of the Groetzsch theorem. It follows that the\nvertices of a soft Barycentric refined q-manifold G' can be colored by q+1 or\nq+2 colors.", "published": "2025-03-02 14:21:17", "link": "http://arxiv.org/abs/2503.00909v1", "categories": ["math.CO", "cs.DM", "05C15, 05C50, 15-xx"], "primary_category": "math.CO"}
{"title": "$K_{2,3}$-induced minor-free graphs admit quasi-isometry with additive distortion to graphs of tree-width at most two", "abstract": "A graph $H$ is an induced minor of a graph $G$ if $H$ can be obtained from\n$G$ by a sequence of edge contractions and vertex deletions. Otherwise, $G$ is\n$H$-induced minor-free. In this paper, we prove that $K_{2,3}$-induced\nminor-free graphs admit a quasi-isometry with additive distortion to graphs\nwith tree-width at most two. Our result implies that a recent conjecture of\nNguyen et al. [Coarse tree-width (2025)] holds for $K_{2,3}$-induced minor-free\ngraphs.", "published": "2025-03-02 08:49:10", "link": "http://arxiv.org/abs/2503.00798v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "Interval H-graphs : Recognition and forbidden obstructions", "abstract": "We introduce the class of interval $H$-graphs, which is the generalization of\ninterval graphs, particularly interval bigraphs. For a fixed graph $H$ with\nvertices $a_1,a_2,\\dots,a_k$, we say that an input graph $G$ with given\npartition $V_1,\\dots,V_k$ of its vertices is an interval $H$-graph if each\nvertex $v \\in G$ can be represented by an interval $I_v$ from a real line so\nthat $u \\in V_i$ and $v \\in V_j$ are adjacent if and only if $a_ia_j$ is an\nedge of $H$ and intervals $I_u$ and $I_v$ intersect. $G$ is called interval\n$k$-graph if $H$ is a complete graph on $k$ vertices. and interval bigraph when\n$k=2$. We study the ordering characterization and forbidden obstructions of\ninterval $k$-graphs and present a polynomial-time recognition algorithm for\nthem. Additionally, we discuss how this algorithm can be extended to recognize\ngeneral interval $H$-graphs. Special cases of interval $k$-graphs, particularly\ncomparability interval $k$-graphs, were previously studied in [2], where the\ncomplexity interval $k$-graph recognition was posed as an open problem.", "published": "2025-03-02 00:10:46", "link": "http://arxiv.org/abs/2503.00672v1", "categories": ["cs.DM", "cs.DS", "math.CO"], "primary_category": "cs.DM"}
{"title": "Pricing time-capped American options using Least Squares Monte Carlo method", "abstract": "In this paper, we adopt the least squares Monte Carlo (LSMC) method to price\ntime-capped American options. The aforementioned cap can be an independent\nrandom variable or dependent on asset price at random time. We allow various\ntime caps. In particular, we give an algorithm for pricing the American options\ncapped by the first drawdown epoch. We focus on the geometric L\\'evy market. We\nprove that our estimator converges to the true price as one takes the\ndiscretisation step tending to zero and the number of trajectories going to\ninfinity.", "published": "2025-03-02 22:07:54", "link": "http://arxiv.org/abs/2503.01040v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Liquidity-adjusted Return and Volatility, and Autoregressive Models", "abstract": "We construct liquidity-adjusted return and volatility using purposely\ndesigned liquidity metrics (liquidity jump and liquidity diffusion) that\nincorporate additional liquidity information. Based on these measures, we\nintroduce a liquidity-adjusted ARMA-GARCH framework to address the limitations\nof traditional ARMA-GARCH models, which are not effectively in modeling\nilliquid assets with high liquidity variability, such as cryptocurrencies. We\ndemonstrate that the liquidity-adjusted model improves model fit for\ncryptocurrencies, with greater volatility sensitivity to past shocks and\nreduced volatility persistence of erratic past volatility. Our model is\nvalidated by the empirical evidence that the liquidity-adjusted mean-variance\n(LAMV) portfolios outperform the traditional mean-variance (TMV) portfolios.", "published": "2025-03-02 13:53:19", "link": "http://arxiv.org/abs/2503.08693v1", "categories": ["q-fin.ST", "q-fin.RM", "q-fin.TR"], "primary_category": "q-fin.ST"}
{"title": "Unmasking Digital Falsehoods: A Comparative Analysis of LLM-Based\n  Misinformation Detection Strategies", "abstract": "The proliferation of misinformation on social media has raised significant\nsocietal concerns, necessitating robust detection mechanisms. Large Language\nModels such as GPT-4 and LLaMA2 have been envisioned as possible tools for\ndetecting misinformation based on their advanced natural language understanding\nand reasoning capabilities. This paper conducts a comparison of LLM-based\napproaches to detecting misinformation between text-based, multimodal, and\nagentic approaches. We evaluate the effectiveness of fine-tuned models,\nzero-shot learning, and systematic fact-checking mechanisms in detecting\nmisinformation across different topic domains like public health, politics, and\nfinance. We also discuss scalability, generalizability, and explainability of\nthe models and recognize key challenges such as hallucination, adversarial\nattacks on misinformation, and computational resources. Our findings point\ntowards the importance of hybrid approaches that pair structured verification\nprotocols with adaptive learning techniques to enhance detection accuracy and\nexplainability. The paper closes by suggesting potential avenues of future\nwork, including real-time tracking of misinformation, federated learning, and\ncross-platform detection models.", "published": "2025-03-02 04:31:42", "link": "http://arxiv.org/abs/2503.00724v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Personalized Tool-Augmented LLMs from the Perspectives of\n  Personalization and Proactivity", "abstract": "Personalized tool utilization is essential for aligning large language models\n(LLMs) with user preference in interaction scenarios with various tools.\nHowever, most of the current benchmarks primarily focus on either\npersonalization of text generation or direct tool-utilizing, without\nconsidering both. In this work, we introduce a novel benchmark ETAPP for\nevaluating personalized tool invocation, establishing a sandbox environment,\nand a comprehensive dataset of 800 testing cases covering diverse user\nprofiles. To improve the accuracy of our evaluation, we propose a\nkey-point-based LLM evaluation method, mitigating biases in the LLM-as-a-judge\nsystem by manually annotating key points for each test case and providing them\nto LLM as the reference. Additionally, we evaluate the excellent LLMs and\nprovide an in-depth analysis. Furthermore, we investigate the impact of\ndifferent tool-invoking strategies on LLMs' personalization performance and the\neffects of fine-tuning in our task. The effectiveness of our preference-setting\nand key-point-based evaluation method is also validated. Our findings offer\ninsights into improving personalized LLM agents. Our Code is available at\nhttps://github.com/hypasd-art/ETAPP.", "published": "2025-03-02 07:36:22", "link": "http://arxiv.org/abs/2503.00771v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DuoDecoding: Hardware-aware Heterogeneous Speculative Decoding with\n  Dynamic Multi-Sequence Drafting", "abstract": "Large language models (LLMs) exhibit exceptional performance across a wide\nrange of tasks; however, their token-by-token autoregressive generation process\nsignificantly hinders inference speed. Speculative decoding presents a\npromising draft-then-verify framework that reduces generation latency while\nmaintaining output distribution fidelity. Nevertheless, the draft model\nintroduces additional computational overhead, becoming a performance bottleneck\nand increasing the time to first token (TTFT). Previous approaches to mitigate\ndraft model overhead have primarily relied on heuristics and generally failed\nto match the quality of the draft language models. To address these challenges,\nwe propose DuoDecoding, a novel approach that strategically deploys the draft\nand target models on the CPU and GPU respectively, enabling parallel decoding\nwhile preserving draft quality. Our method incorporates a hardware-aware\noptimal draft budget to minimize idle times and employs dynamic multi-sequence\ndrafting to enhance draft quality. Extensive experiments across seven tasks\nshow that DuoDecoding achieves up to 2.61x speedup in generation latency, while\nreducing TTFT to 83% of that in conventional speculative decoding. The Code is\navailable at https://github.com/KaiLv69/DuoDecoding.", "published": "2025-03-02 08:27:48", "link": "http://arxiv.org/abs/2503.00784v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predictive Data Selection: The Data That Predicts Is the Data That\n  Teaches", "abstract": "Language model pretraining involves training on extensive corpora, where data\nquality plays a pivotal role. In this work, we aim to directly estimate the\ncontribution of data during pretraining and select pretraining data in an\nefficient manner. Specifically, we draw inspiration from recent findings\nshowing that compression efficiency (i.e., the normalized loss) of diverse\nmodels on certain text correlates strongly with their downstream performance,\nwhen the text domain aligns with the downstream benchmarks(Huang et al., 2024).\nBuilding on this observation, we hypothesize that data on which model losses\nare predictive of downstream abilities also contribute effectively to learning.\nTo leverage this insight, we introduce predictive data selection (PreSelect), a\nlightweight and efficient data selection method that requires training and\ndeploying only a fastText-based scorer. Through comprehensive experiments with\n1B and 3B parameter models, we demonstrate that models trained on 30B tokens\nselected with PreSelect surpass the performance of the vanilla baseline trained\non 300B tokens, achieving a 10x reduction in compute requirements. Furthermore,\nPreSelect significantly outperforms other competitive data selection baselines,\nsuch as DCLM and FineWeb-Edu on a scale of 3B models trained on 100B tokens. We\nopen-source our trained data selection scorer along with the curated datasets\nat https://github.com/hkust-nlp/PreSelect.", "published": "2025-03-02 09:21:28", "link": "http://arxiv.org/abs/2503.00808v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Waste Not, Want Not; Recycled Gumbel Noise Improves Consistency in\n  Natural Language Generation", "abstract": "Consistency in the output of language models is critical for their\nreliability and practical utility. Due to their training objective, language\nmodels learn to model the full space of possible continuations, leading to\noutputs that can vary significantly in style and content, even for similar or\nrepeated inputs. To address this, we propose a novel decoding algorithm that\nenhances response consistency across different prompts with no degradation in\nresponse quality. By incorporating a latent variable into the next-token\nsampling process based on the Gumbel reparametrisation trick, our method\noutperforms standard sampling by up to 10% across semantic and stylistic\nconsistency benchmarks. Additionally, our approach integrates seamlessly with\nexisting sampling methods with negligible computational overhead, providing a\npractical solution for improving the reliability of language model outputs.", "published": "2025-03-02 10:08:51", "link": "http://arxiv.org/abs/2503.00831v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Argument Summarization and its Evaluation in the Era of Large Language\n  Models", "abstract": "Large Language Models (LLMs) have revolutionized various Natural Language\nGeneration (NLG) tasks, including Argument Summarization (ArgSum), a key\nsubfield of Argument Mining (AM). This paper investigates the integration of\nstate-of-the-art LLMs into ArgSum, including for its evaluation. In particular,\nwe propose a novel prompt-based evaluation scheme, and validate it through a\nnovel human benchmark dataset. Our work makes three main contributions: (i) the\nintegration of LLMs into existing ArgSum frameworks, (ii) the development of a\nnew LLM-based ArgSum system, benchmarked against prior methods, and (iii) the\nintroduction of an advanced LLM-based evaluation scheme. We demonstrate that\nthe use of LLMs substantially improves both the generation and evaluation of\nargument summaries, achieving state-of-the-art results and advancing the field\nof ArgSum.", "published": "2025-03-02 10:49:10", "link": "http://arxiv.org/abs/2503.00847v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DUAL: Diversity and Uncertainty Active Learning for Text Summarization", "abstract": "With the rise of large language models, neural text summarization has\nadvanced significantly in recent years. However, even state-of-the-art models\ncontinue to rely heavily on high-quality human-annotated data for training and\nevaluation. Active learning is frequently used as an effective way to collect\nsuch datasets, especially when annotation resources are scarce. Active learning\nmethods typically prioritize either uncertainty or diversity but have shown\nlimited effectiveness in summarization, often being outperformed by random\nsampling. We present Diversity and Uncertainty Active Learning (DUAL), a novel\nalgorithm that combines uncertainty and diversity to iteratively select and\nannotate samples that are both representative of the data distribution and\nchallenging for the current model. DUAL addresses the selection of noisy\nsamples in uncertainty-based methods and the limited exploration scope of\ndiversity-based methods. Through extensive experiments with different\nsummarization models and benchmark datasets, we demonstrate that DUAL\nconsistently matches or outperforms the best performing strategies. Using\nvisualizations and quantitative metrics, we provide valuable insights into the\neffectiveness and robustness of different active learning strategies, in an\nattempt to understand why these strategies haven't performed consistently in\ntext summarization. Finally, we show that DUAL strikes a good balance between\ndiversity and robustness.", "published": "2025-03-02 12:06:16", "link": "http://arxiv.org/abs/2503.00867v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Instruct-of-Reflection: Enhancing Large Language Models Iterative\n  Reflection Capabilities via Dynamic-Meta Instruction", "abstract": "Self-reflection for Large Language Models (LLMs) has gained significant\nattention. Existing approaches involve models iterating and improving their\nprevious responses based on LLMs' internal reflection ability or external\nfeedback. However, recent research has raised doubts about whether intrinsic\nself-correction without external feedback may even degrade performance. Based\non our empirical evidence, we find that current static reflection methods may\nlead to redundant, drift, and stubborn issues. To mitigate this, we introduce\nInstruct-of-Reflection (IoRT), a novel and general reflection framework that\nleverages dynamic-meta instruction to enhance the iterative reflection\ncapability of LLMs. Specifically, we propose the instructor driven by the\nmeta-thoughts and self-consistency classifier, generates various instructions,\nincluding refresh, stop, and select, to guide the next reflection iteration.\nOur experiments demonstrate that IoRT achieves an average improvement of 10.1%\nover established baselines in mathematical and commonsense reasoning tasks,\nhighlighting its efficacy and applicability.", "published": "2025-03-02 14:02:03", "link": "http://arxiv.org/abs/2503.00902v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Layered Insights: Generalizable Analysis of Authorial Style by\n  Leveraging All Transformer Layers", "abstract": "We propose a new approach for the authorship attribution task that leverages\nthe various linguistic representations learned at different layers of\npre-trained transformer-based models. We evaluate our approach on three\ndatasets, comparing it to a state-of-the-art baseline in in-domain and\nout-of-domain scenarios. We found that utilizing various transformer layers\nimproves the robustness of authorship attribution models when tested on\nout-of-domain data, resulting in new state-of-the-art results. Our analysis\ngives further insights into how our model's different layers get specialized in\nrepresenting certain stylistic features that benefit the model when tested out\nof the domain.", "published": "2025-03-02 16:47:31", "link": "http://arxiv.org/abs/2503.00958v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Text Editing for Grammatical Error Correction: Arabic as a\n  Case Study", "abstract": "Text editing frames grammatical error correction (GEC) as a sequence tagging\nproblem, where edit tags are assigned to input tokens, and applying these edits\nresults in the corrected text. This approach has gained attention for its\nefficiency and interpretability. However, while extensively explored for\nEnglish, text editing remains largely underexplored for morphologically rich\nlanguages like Arabic. In this paper, we introduce a text editing approach that\nderives edit tags directly from data, eliminating the need for\nlanguage-specific edits. We demonstrate its effectiveness on Arabic, a\ndiglossic and morphologically rich language, and investigate the impact of\ndifferent edit representations on model performance. Our approach achieves SOTA\nresults on two Arabic GEC benchmarks and performs on par with SOTA on two\nothers. Additionally, our models are over six times faster than existing Arabic\nGEC systems, making our approach more practical for real-world applications.\nFinally, we explore ensemble models, demonstrating how combining different\nmodels leads to further performance improvements. We make our code, data, and\npretrained models publicly available.", "published": "2025-03-02 18:48:50", "link": "http://arxiv.org/abs/2503.00985v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Polish linguistic and cultural competency in large language\n  models", "abstract": "Large language models (LLMs) are becoming increasingly proficient in\nprocessing and generating multilingual texts, which allows them to address\nreal-world problems more effectively. However, language understanding is a far\nmore complex issue that goes beyond simple text analysis. It requires\nfamiliarity with cultural context, including references to everyday life,\nhistorical events, traditions, folklore, literature, and pop culture. A lack of\nsuch knowledge can lead to misinterpretations and subtle, hard-to-detect\nerrors. To examine language models' knowledge of the Polish cultural context,\nwe introduce the Polish linguistic and cultural competency benchmark,\nconsisting of 600 manually crafted questions. The benchmark is divided into six\ncategories: history, geography, culture & tradition, art & entertainment,\ngrammar, and vocabulary. As part of our study, we conduct an extensive\nevaluation involving over 30 open-weight and commercial LLMs. Our experiments\nprovide a new perspective on Polish competencies in language models, moving\npast traditional natural language processing tasks and general knowledge\nassessment.", "published": "2025-03-02 19:27:10", "link": "http://arxiv.org/abs/2503.00995v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language-agnostic, automated assessment of listeners' speech recall\n  using large language models", "abstract": "Speech-comprehension difficulties are common among older people. Standard\nspeech tests do not fully capture such difficulties because the tests poorly\nresemble the context-rich, story-like nature of ongoing conversation and are\ntypically available only in a country's dominant/official language (e.g.,\nEnglish), leading to inaccurate scores for native speakers of other languages.\nAssessments for naturalistic, story speech in multiple languages require\naccurate, time-efficient scoring. The current research leverages modern large\nlanguage models (LLMs) in native English speakers and native speakers of 10\nother languages to automate the generation of high-quality, spoken stories and\nscoring of speech recall in different languages. Participants listened to and\nfreely recalled short stories (in quiet/clear and in babble noise) in their\nnative language. LLM text-embeddings and LLM prompt engineering with semantic\nsimilarity analyses to score speech recall revealed sensitivity to known\neffects of temporal order, primacy/recency, and background noise, and high\nsimilarity of recall scores across languages. The work overcomes limitations\nassociated with simple speech materials and testing of closed native-speaker\ngroups because recall data of varying length and details can be mapped across\nlanguages with high accuracy. The full automation of speech generation and\nrecall scoring provides an important step towards comprehension assessments of\nnaturalistic speech with clinical applicability.", "published": "2025-03-02 22:28:41", "link": "http://arxiv.org/abs/2503.01045v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AI-Invented Tonal Languages: Preventing a Machine Lingua Franca Beyond\n  Human Understanding", "abstract": "This paper investigates the potential for large language models (LLMs) to\ndevelop private tonal languages for machine-to-machine (M2M) communication.\nInspired by cryptophasia in human twins (affecting up to 50% of twin births)\nand natural tonal languages like Mandarin and Vietnamese, we implement a\nprecise character-to-frequency mapping system that encodes the full ASCII\ncharacter set (32-126) using musical semitones. Each character is assigned a\nunique frequency, creating a logarithmic progression beginning with space (220\nHz) and ending with tilde (50,175.42 Hz). This spans approximately 7.9 octaves,\nwith higher characters deliberately mapped to ultrasonic frequencies beyond\nhuman perception (>20 kHz). Our implemented software prototype demonstrates\nthis encoding through visualization, auditory playback, and ABC musical\nnotation, allowing for analysis of information density and transmission speed.\nTesting reveals that tonal encoding can achieve information rates exceeding\nhuman speech while operating partially outside human perceptual boundaries.\nThis work responds directly to concerns about AI systems catastrophically\ndeveloping private languages within the next five years, providing a concrete\nprototype software example of how such communication might function and the\ntechnical foundation required for its emergence, detection, and governance.", "published": "2025-03-02 23:59:52", "link": "http://arxiv.org/abs/2503.01063v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RAPID: Efficient Retrieval-Augmented Long Text Generation with Writing\n  Planning and Information Discovery", "abstract": "Generating knowledge-intensive and comprehensive long texts, such as\nencyclopedia articles, remains significant challenges for Large Language\nModels. It requires not only the precise integration of facts but also the\nmaintenance of thematic coherence throughout the article. Existing methods,\nsuch as direct generation and multi-agent discussion, often struggle with\nissues like hallucinations, topic incoherence, and significant latency. To\naddress these challenges, we propose RAPID, an efficient retrieval-augmented\nlong text generation framework. RAPID consists of three main modules: (1)\nRetrieval-augmented preliminary outline generation to reduce hallucinations,\n(2) Attribute-constrained search for efficient information discovery, (3)\nPlan-guided article generation for enhanced coherence. Extensive experiments on\nour newly compiled benchmark dataset, FreshWiki-2024, demonstrate that RAPID\nsignificantly outperforms state-of-the-art methods across a wide range of\nevaluation metrics (e.g. long-text generation, outline quality, latency, etc).\nOur work provides a robust and efficient solution to the challenges of\nautomated long-text generation.", "published": "2025-03-02 06:11:29", "link": "http://arxiv.org/abs/2503.00751v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Babel: Open Multilingual Large Language Models Serving Over 90% of\n  Global Speakers", "abstract": "Large language models (LLMs) have revolutionized natural language processing\n(NLP), yet open-source multilingual LLMs remain scarce, with existing models\noften limited in language coverage. Such models typically prioritize\nwell-resourced languages, while widely spoken but under-resourced languages are\noften overlooked. To address this disparity, we introduce $\\texttt{Babel}$, an\nopen multilingual LLM that covers the top 25 languages by number of speakers,\nsupports over 90% of the global population, and includes many languages\nneglected by other open multilingual LLMs. Unlike traditional continue\npretraining approaches, Babel expands its parameter count through a layer\nextension technique that elevates Babel's performance ceiling. We introduce two\nvariants: $\\texttt{Babel-9B}$, designed for efficient inference and\nfine-tuning, and $\\texttt{Babel-83B}$, which sets a new standard for open\nmultilingual LLMs. Extensive evaluations on multilingual tasks demonstrate its\nsuperior performance compared to open LLMs of comparable size. In addition,\nusing open-source supervised fine-tuning datasets, Babel achieves remarkable\nperformance, with Babel-9B-Chat leading among 10B-sized LLMs and Babel-83B-Chat\nsetting a new standard for multilingual tasks, reaching the same level of\ncommercial models.", "published": "2025-03-02 11:53:55", "link": "http://arxiv.org/abs/2503.00865v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HiBench: Benchmarking LLMs Capability on Hierarchical Structure\n  Reasoning", "abstract": "Structure reasoning is a fundamental capability of large language models\n(LLMs), enabling them to reason about structured commonsense and answer\nmulti-hop questions. However, existing benchmarks for structure reasoning\nmainly focus on horizontal and coordinate structures (\\emph{e.g.} graphs),\noverlooking the hierarchical relationships within them. Hierarchical structure\nreasoning is crucial for human cognition, particularly in memory organization\nand problem-solving. It also plays a key role in various real-world tasks, such\nas information extraction and decision-making. To address this gap, we propose\nHiBench, the first framework spanning from initial structure generation to\nfinal proficiency assessment, designed to benchmark the hierarchical reasoning\ncapabilities of LLMs systematically. HiBench encompasses six representative\nscenarios, covering both fundamental and practical aspects, and consists of 30\ntasks with varying hierarchical complexity, totaling 39,519 queries. To\nevaluate LLMs comprehensively, we develop five capability dimensions that\ndepict different facets of hierarchical structure understanding. Through\nextensive evaluation of 20 LLMs from 10 model families, we reveal key insights\ninto their capabilities and limitations: 1) existing LLMs show proficiency in\nbasic hierarchical reasoning tasks; 2) they still struggle with more complex\nstructures and implicit hierarchical representations, especially in structural\nmodification and textual reasoning. Based on these findings, we create a small\nyet well-designed instruction dataset, which enhances LLMs' performance on\nHiBench by an average of 88.84\\% (Llama-3.1-8B) and 31.38\\% (Qwen2.5-7B) across\nall tasks. The HiBench dataset and toolkit are available here,\nhttps://github.com/jzzzzh/HiBench, to encourage evaluation.", "published": "2025-03-02 14:25:37", "link": "http://arxiv.org/abs/2503.00912v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SemViQA: A Semantic Question Answering System for Vietnamese Information\n  Fact-Checking", "abstract": "The rise of misinformation, exacerbated by Large Language Models (LLMs) like\nGPT and Gemini, demands robust fact-checking solutions, especially for\nlow-resource languages like Vietnamese. Existing methods struggle with semantic\nambiguity, homonyms, and complex linguistic structures, often trading accuracy\nfor efficiency. We introduce SemViQA, a novel Vietnamese fact-checking\nframework integrating Semantic-based Evidence Retrieval (SER) and Two-step\nVerdict Classification (TVC). Our approach balances precision and speed,\nachieving state-of-the-art results with 78.97\\% strict accuracy on ISE-DSC01\nand 80.82\\% on ViWikiFC, securing 1st place in the UIT Data Science Challenge.\nAdditionally, SemViQA Faster improves inference speed 7x while maintaining\ncompetitive accuracy. SemViQA sets a new benchmark for Vietnamese fact\nverification, advancing the fight against misinformation. The source code is\navailable at: https://github.com/DAVID-NGUYEN-S16/SemViQA.", "published": "2025-03-02 16:22:46", "link": "http://arxiv.org/abs/2503.00955v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Models Predict Empathy Gaps Between Social In-groups and\n  Out-groups", "abstract": "Studies of human psychology have demonstrated that people are more motivated\nto extend empathy to in-group members than out-group members (Cikara et al.,\n2011). In this study, we investigate how this aspect of intergroup relations in\nhumans is replicated by LLMs in an emotion intensity prediction task. In this\ntask, the LLM is given a short description of an experience a person had that\ncaused them to feel a particular emotion; the LLM is then prompted to predict\nthe intensity of the emotion the person experienced on a numerical scale. By\nmanipulating the group identities assigned to the LLM's persona (the\n\"perceiver\") and the person in the narrative (the \"experiencer\"), we measure\nhow predicted emotion intensities differ between in-group and out-group\nsettings. We observe that LLMs assign higher emotion intensity scores to\nin-group members than out-group members. This pattern holds across all three\ntypes of social groupings we tested: race/ethnicity, nationality, and religion.\nWe perform an in-depth analysis on Llama-3.1-8B, the model which exhibited\nstrongest intergroup bias among those tested.", "published": "2025-03-02 21:31:14", "link": "http://arxiv.org/abs/2503.01030v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Variance reduction in output from generative AI", "abstract": "Generative AI models, such as ChatGPT, will increasingly replace humans in\nproducing output for a variety of important tasks. While much prior work has\nmostly focused on the improvement in the average performance of generative AI\nmodels relative to humans' performance, much less attention has been paid to\nthe significant reduction of variance in output produced by generative AI\nmodels. In this Perspective, we demonstrate that generative AI models are\ninherently prone to the phenomenon of \"regression toward the mean\" whereby\nvariance in output tends to shrink relative to that in real-world\ndistributions. We discuss potential social implications of this phenomenon\nacross three levels-societal, group, and individual-and two dimensions-material\nand non-material. Finally, we discuss interventions to mitigate negative\neffects, considering the roles of both service providers and users. Overall,\nthis Perspective aims to raise awareness of the importance of output variance\nin generative AI and to foster collaborative efforts to meet the challenges\nposed by the reduction of variance in output generated by AI models.", "published": "2025-03-02 21:34:10", "link": "http://arxiv.org/abs/2503.01033v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "NCL-UoR at SemEval-2025 Task 3: Detecting Multilingual Hallucination and\n  Related Observable Overgeneration Text Spans with Modified RefChecker and\n  Modified SeflCheckGPT", "abstract": "SemEval-2025 Task 3 (Mu-SHROOM) focuses on detecting hallucinations in\ncontent generated by various large language models (LLMs) across multiple\nlanguages. This task involves not only identifying the presence of\nhallucinations but also pinpointing their specific occurrences. To tackle this\nchallenge, this study introduces two methods: modified RefChecker and modified\nSelfCheckGPT. The modified RefChecker integrates prompt-based factual\nverification into References, structuring them as claim-based tests rather than\nsingle external knowledge sources. The modified SelfCheckGPT incorporates\nexternal knowledge to overcome its reliance on internal knowledge. In addition,\nboth methods' original prompt designs are enhanced to identify hallucinated\nwords within LLM-generated texts. Experimental results demonstrate the\neffectiveness of the approach, achieving a high ranking on the test dataset in\ndetecting hallucinations across various languages, with an average IoU of\n0.5310 and an average COR of 0.5669.", "published": "2025-03-02 04:21:33", "link": "http://arxiv.org/abs/2503.01921v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Output Length Effect on DeepSeek-R1's Safety in Forced Thinking", "abstract": "Large Language Models (LLMs) have demonstrated strong reasoning capabilities,\nbut their safety under adversarial conditions remains a challenge. This study\nexamines the impact of output length on the robustness of DeepSeek-R1,\nparticularly in Forced Thinking scenarios. We analyze responses across various\nadversarial prompts and find that while longer outputs can improve safety\nthrough self-correction, certain attack types exploit extended generations. Our\nfindings suggest that output length should be dynamically controlled to balance\nreasoning effectiveness and security. We propose reinforcement learning-based\npolicy adjustments and adaptive token length regulation to enhance LLM safety.", "published": "2025-03-02 06:29:22", "link": "http://arxiv.org/abs/2503.01923v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unnatural Languages Are Not Bugs but Features for LLMs", "abstract": "Large Language Models (LLMs) have been observed to process non-human-readable\ntext sequences, such as jailbreak prompts, often viewed as a bug for aligned\nLLMs. In this work, we present a systematic investigation challenging this\nperception, demonstrating that unnatural languages - strings that appear\nincomprehensible to humans but maintain semantic meanings for LLMs - contain\nlatent features usable by models. Notably, unnatural languages possess latent\nfeatures that can be generalized across different models and tasks during\ninference. Furthermore, models fine-tuned on unnatural versions of instruction\ndatasets perform on-par with those trained on natural language, achieving 49.71\nwin rates in Length-controlled AlpacaEval 2.0 in average across various base\nmodels. In addition, through comprehensive analysis, we demonstrate that LLMs\nprocess unnatural languages by filtering noise and inferring contextual meaning\nfrom filtered words.", "published": "2025-03-02 12:10:17", "link": "http://arxiv.org/abs/2503.01926v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The\n  Balancing Act of Selective Unlearning in Large Language Models", "abstract": "Large Language Models (LLMs) face significant challenges in maintaining\nprivacy, ethics, and compliance, when sensitive or obsolete data must be\nselectively removed. Retraining these models from scratch is computationally\ninfeasible, necessitating efficient alternatives. As part of the SemEval 2025\nTask 4, this work focuses on the application of selective unlearning in LLMs to\naddress this challenge. In this paper, we present our experiments and findings,\nprimarily leveraging global weight modification to achieve an equilibrium\nbetween effectiveness of unlearning, knowledge retention, and target model's\npost-unlearning utility. We also detail the task-specific evaluation mechanism,\nresults, and challenges. Our algorithms have achieved an aggregate score of\n0.409 and 0.389 on the test set for 7B and 1B target models, respectively,\ndemonstrating promising results in verifiable LLM unlearning.", "published": "2025-03-02 07:58:08", "link": "http://arxiv.org/abs/2503.04795v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Parallel Corpora for Machine Translation in Low-resource Indic\n  Languages: A Comprehensive Review", "abstract": "Parallel corpora play an important role in training machine translation (MT)\nmodels, particularly for low-resource languages where high-quality bilingual\ndata is scarce. This review provides a comprehensive overview of available\nparallel corpora for Indic languages, which span diverse linguistic families,\nscripts, and regional variations. We categorize these corpora into\ntext-to-text, code-switched, and various categories of multimodal datasets,\nhighlighting their significance in the development of robust multilingual MT\nsystems. Beyond resource enumeration, we critically examine the challenges\nfaced in corpus creation, including linguistic diversity, script variation,\ndata scarcity, and the prevalence of informal textual content.We also discuss\nand evaluate these corpora in various terms such as alignment quality and\ndomain representativeness. Furthermore, we address open challenges such as data\nimbalance across Indic languages, the trade-off between quality and quantity,\nand the impact of noisy, informal, and dialectal data on MT performance.\nFinally, we outline future directions, including leveraging cross-lingual\ntransfer learning, expanding multilingual datasets, and integrating multimodal\nresources to enhance translation quality. To the best of our knowledge, this\npaper presents the first comprehensive review of parallel corpora specifically\ntailored for low-resource Indic languages in the context of machine\ntranslation.", "published": "2025-03-02 21:22:53", "link": "http://arxiv.org/abs/2503.04797v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How Diversely Can Language Models Solve Problems? Exploring the\n  Algorithmic Diversity of Model-Generated Code", "abstract": "Language models (LMs) have exhibited impressive abilities in generating code\nfrom natural language requirements. In this work, we highlight the diversity of\ncode generated by LMs as a critical criterion for evaluating their code\ngeneration capabilities. There is a lack of studies focused on assessing the\ndiversity of generated code, which overlooks its importance in code LMs.\nTherefore, we propose a systematic approach to evaluate code diversity,\nintroducing various metrics with inter-code similarity. Specifically, we\nintroduce code clustering methods that leverages LMs' capabilities in code\nunderstanding and reasoning, resulting in a set of metrics that represent the\nnumber of algorithms in model-generated solutions. We extensively investigate\nthe property of model-generated solutions by contrasting them with\nhuman-written ones and quantifying the impact of various factors on code\ndiversity: model size, temperature, instruction tuning, and problem complexity.\nOur analysis demonstrates that model-generated solutions exhibit low\nalgorithmic diversity, which was neglected by the research community. Moreover,\nwe explore methods to increase code diversity by combining solutions from\ndifferent models and increasing sampling temperatures. Our findings highlight\nthat code diversity can be enhanced with the help of heterogeneous models and\nsetting temperature beyond 1.0 that has not been fully explored due to the\nfunctional correctness degradation. To facilitate our research direction, we\npublicly share our code and datasets through open-source repositories.", "published": "2025-03-02 02:04:58", "link": "http://arxiv.org/abs/2503.00691v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Towards hyperparameter-free optimization with differential privacy", "abstract": "Differential privacy (DP) is a privacy-preserving paradigm that protects the\ntraining data when training deep learning models. Critically, the performance\nof models is determined by the training hyperparameters, especially those of\nthe learning rate schedule, thus requiring fine-grained hyperparameter tuning\non the data. In practice, it is common to tune the learning rate\nhyperparameters through the grid search that (1) is computationally expensive\nas multiple runs are needed, and (2) increases the risk of data leakage as the\nselection of hyperparameters is data-dependent. In this work, we adapt the\nautomatic learning rate schedule to DP optimization for any models and\noptimizers, so as to significantly mitigate or even eliminate the cost of\nhyperparameter tuning when applied together with automatic per-sample gradient\nclipping. Our hyperparameter-free DP optimization is almost as computationally\nefficient as the standard non-DP optimization, and achieves state-of-the-art DP\nperformance on various language and vision tasks.", "published": "2025-03-02 02:59:52", "link": "http://arxiv.org/abs/2503.00703v1", "categories": ["cs.LG", "cs.CL", "cs.CR", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Causal Inference on Outcomes Learned from Text", "abstract": "We propose a machine-learning tool that yields causal inference on text in\nrandomized trials. Based on a simple econometric framework in which text may\ncapture outcomes of interest, our procedure addresses three questions: First,\nis the text affected by the treatment? Second, which outcomes is the effect on?\nAnd third, how complete is our description of causal effects? To answer all\nthree questions, our approach uses large language models (LLMs) that suggest\nsystematic differences across two groups of text documents and then provides\nvalid inference based on costly validation. Specifically, we highlight the need\nfor sample splitting to allow for statistical validation of LLM outputs, as\nwell as the need for human labeling to validate substantive claims about how\ndocuments differ across groups. We illustrate the tool in a proof-of-concept\napplication using abstracts of academic manuscripts.", "published": "2025-03-02 04:36:27", "link": "http://arxiv.org/abs/2503.00725v1", "categories": ["econ.EM", "cs.CL", "cs.LG", "stat.ME"], "primary_category": "econ.EM"}
{"title": "UniWav: Towards Unified Pre-training for Speech Representation Learning\n  and Generation", "abstract": "Pre-training and representation learning have been playing an increasingly\nimportant role in modern speech processing. Nevertheless, different\napplications have been relying on different foundation models, since\npredominant pre-training techniques are either designed for discriminative\ntasks or generative tasks. In this work, we make the first attempt at building\na unified pre-training framework for both types of tasks in speech. We show\nthat with the appropriate design choices for pre-training, one can jointly\nlearn a representation encoder and generative audio decoder that can be applied\nto both types of tasks. We propose UniWav, an encoder-decoder framework\ndesigned to unify pre-training representation learning and generative tasks. On\nspeech recognition, text-to-speech, and speech tokenization, UniWav achieves\ncomparable performance to different existing foundation models, each trained on\na specific task. Our findings suggest that a single general-purpose foundation\nmodel for speech can be built to replace different foundation models, reducing\nthe overhead and cost of pre-training.", "published": "2025-03-02 05:15:40", "link": "http://arxiv.org/abs/2503.00733v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Rewarding Graph Reasoning Process makes LLMs more Generalized Reasoners", "abstract": "Despite significant advancements in Large Language Models (LLMs), developing\nadvanced reasoning capabilities in LLMs remains a key challenge. Process Reward\nModels (PRMs) have demonstrated exceptional promise in enhancing reasoning by\nproviding step-wise feedback, particularly in the context of mathematical\nreasoning. However, their application to broader reasoning domains remains\nunderstudied, largely due to the high costs associated with manually creating\nstep-level supervision. In this work, we explore the potential of PRMs in graph\nreasoning problems - a domain that demands sophisticated multi-step reasoning\nand offers opportunities for automated step-level data generation using\nestablished graph algorithms. We introduce GraphSILO, the largest dataset for\ngraph reasoning problems with fine-grained step-wise labels, built using\nautomated Task-oriented Trajectories and Monte Carlo Tree Search (MCTS) to\ngenerate detailed reasoning steps with step-wise labels. Building upon this\ndataset, we train GraphPRM, the first PRM designed for graph reasoning\nproblems, and evaluate its effectiveness in two key settings: inference-time\nscaling and reinforcement learning via Direct Preference Optimization (DPO).\nExperimental results show that GraphPRM significantly improves LLM performance\nacross 13 graph reasoning tasks, delivering a 9% gain for Qwen2.5-7B and\ndemonstrating transferability to new graph reasoning datasets and new reasoning\ndomains like mathematical problem-solving. Notably, GraphPRM enhances LLM\nperformance on GSM8K and Math500, underscoring the cross-domain applicability\nof graph-based reasoning rewards. Our findings highlight the potential of PRMs\nin advancing reasoning across diverse domains, paving the way for more\nversatile and effective LLMs.", "published": "2025-03-02 10:39:40", "link": "http://arxiv.org/abs/2503.00845v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unveiling Biases while Embracing Sustainability: Assessing the Dual\n  Challenges of Automatic Speech Recognition Systems", "abstract": "In this paper, we present a bias and sustainability focused investigation of\nAutomatic Speech Recognition (ASR) systems, namely Whisper and Massively\nMultilingual Speech (MMS), which have achieved state-of-the-art (SOTA)\nperformances. Despite their improved performance in controlled settings, there\nremains a critical gap in understanding their efficacy and equity in real-world\nscenarios. We analyze ASR biases w.r.t. gender, accent, and age group, as well\nas their effect on downstream tasks. In addition, we examine the environmental\nimpact of ASR systems, scrutinizing the use of large acoustic models on carbon\nemission and energy consumption. We also provide insights into our empirical\nanalyses, offering a valuable contribution to the claims surrounding bias and\nsustainability in ASR systems.", "published": "2025-03-02 14:17:11", "link": "http://arxiv.org/abs/2503.00907v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs", "abstract": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment.", "published": "2025-03-02 18:12:50", "link": "http://arxiv.org/abs/2503.00979v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evidence of conceptual mastery in the application of rules by Large\n  Language Models", "abstract": "In this paper we leverage psychological methods to investigate LLMs'\nconceptual mastery in applying rules. We introduce a novel procedure to match\nthe diversity of thought generated by LLMs to that observed in a human sample.\nWe then conducted two experiments comparing rule-based decision-making in\nhumans and LLMs. Study 1 found that all investigated LLMs replicated human\npatterns regardless of whether they are prompted with scenarios created before\nor after their training cut-off. Moreover, we found unanticipated differences\nbetween the two sets of scenarios among humans. Surprisingly, even these\ndifferences were replicated in LLM responses. Study 2 turned to a contextual\nfeature of human rule application: under forced time delay, human samples rely\nmore heavily on a rule's text than on other considerations such as a rule's\npurpose.. Our results revealed that some models (Gemini Pro and Claude 3)\nresponded in a human-like manner to a prompt describing either forced delay or\ntime pressure, while others (GPT-4o and Llama 3.2 90b) did not. We argue that\nthe evidence gathered suggests that LLMs have mastery over the concept of rule,\nwith implications for both legal decision making and philosophical inquiry.", "published": "2025-03-02 19:23:46", "link": "http://arxiv.org/abs/2503.00992v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "A Semantic Search Pipeline for Causality-driven Adhoc Information\n  Retrieval", "abstract": "We present a unsupervised semantic search pipeline for the Causality-driven\nAdhoc Information Retrieval (CAIR-2021) shared task. The CAIR shared task\nexpands traditional information retrieval to support the retrieval of documents\ncontaining the likely causes of a query event. A successful system must be able\nto distinguish between topical documents and documents containing causal\ndescriptions of events that are causally related to the query event. Our\napproach involves aggregating results from multiple query strategies over a\nsemantic and lexical index. The proposed approach leads the CAIR-2021\nleaderboard and outperformed both traditional IR and pure semantic\nembedding-based approaches.", "published": "2025-03-02 19:59:41", "link": "http://arxiv.org/abs/2503.01003v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Optimizing Multi-Hop Document Retrieval Through Intermediate\n  Representations", "abstract": "Retrieval-augmented generation (RAG) encounters challenges when addressing\ncomplex queries, particularly multi-hop questions. While several methods tackle\nmulti-hop queries by iteratively generating internal queries and retrieving\nexternal documents, these approaches are computationally expensive. In this\npaper, we identify a three-stage information processing pattern in LLMs during\nlayer-by-layer reasoning, consisting of extraction, processing, and subsequent\nextraction steps. This observation suggests that the representations in\nintermediate layers contain richer information compared to those in other\nlayers. Building on this insight, we propose Layer-wise RAG (L-RAG). Unlike\nprior methods that focus on generating new internal queries, L-RAG leverages\nintermediate representations from the middle layers, which capture next-hop\ninformation, to retrieve external knowledge. L-RAG achieves performance\ncomparable to multi-step approaches while maintaining inference overhead\nsimilar to that of standard RAG. Experimental results show that L-RAG\noutperforms existing RAG methods on open-domain multi-hop question-answering\ndatasets, including MuSiQue, HotpotQA, and 2WikiMultiHopQA. The code is\navailable in https://anonymous.4open.science/r/L-RAG-ADD5/", "published": "2025-03-02 11:33:22", "link": "http://arxiv.org/abs/2503.04796v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "The Reliability of LLMs for Medical Diagnosis: An Examination of\n  Consistency, Manipulation, and Contextual Awareness", "abstract": "Universal healthcare access is critically needed, especially in\nresource-limited settings. Large Language Models (LLMs) offer promise for\ndemocratizing healthcare with advanced diagnostics, but their reliability\nrequires thorough evaluation, especially in trust-dependent environments. This\nstudy assesses LLMs' diagnostic reliability focusing on consistency,\nmanipulation resilience, and contextual integration, crucial for safe and\nethical use in universal healthcare.\n  We evaluated leading LLMs using 52 patient cases, expanded into variants with\ndemographic changes, symptom rewordings, and exam modifications, while keeping\ncore diagnoses constant. Manipulation susceptibility was tested by inserting\nmisleading narratives and irrelevant details. Contextual awareness was\nrvaluated by comparing diagnoses with and without patient history. We analyzed\ndiagnostic change rates and response patterns across manipulations.\n  LLMs showed perfect diagnostic consistency for identical data but significant\nmanipulation susceptibility. Gemini had a 40% diagnosis change rate and ChatGPT\n30% with irrelevant details. ChatGPT had a higher context influence rate (77.8%\nvs. Gemini's 55.6%), but both showed limited nuanced contextual integration,\nexhibiting anchoring bias by prioritizing salient data over context.\n  LLMs' vulnerability to manipulation and limited contextual awareness pose\nchallenges in clinical use. Unlike clinicians, they may overstate diagnostic\ncertainty without validation. Safeguards and domain-specific designs are\ncrucial for reliable healthcare applications. Broad clinical use without\noversight is premature and risky. LLMs can enhance diagnostics with responsible\nuse, but future research is needed to improve manipulation resistance and\ncontextual understanding for safe healthcare democratization.", "published": "2025-03-02 11:50:16", "link": "http://arxiv.org/abs/2503.10647v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "ER-RAG: Enhance RAG with ER-Based Unified Modeling of Heterogeneous Data\n  Sources", "abstract": "Large language models (LLMs) excel in question-answering (QA) tasks, and\nretrieval-augmented generation (RAG) enhances their precision by incorporating\nexternal evidence from diverse sources like web pages, databases, and knowledge\ngraphs. However, current RAG methods rely on agent-specific strategies for\nindividual data sources, posing challenges low-resource or black-box\nenvironments and complicates operations when evidence is fragmented across\nsources. To address these limitations, we propose ER-RAG, a framework that\nunifies evidence integration across heterogeneous data sources using the\nEntity-Relationship (ER) model. ER-RAG standardizes entity retrieval and\nrelationship querying through ER-based APIs with GET and JOIN operations. It\nemploys a two-stage generation process: first, a preference optimization module\nselects optimal sources; second, another module constructs API chains based on\nsource schemas. This unified approach allows efficient fine-tuning and seamless\nintegration across diverse data sources. ER-RAG demonstrated its effectiveness\nby winning all three tracks of the 2024 KDDCup CRAG Challenge, achieving\nperformance on par with commercial RAG pipelines using an 8B LLM backbone. It\noutperformed hybrid competitors by 3.1% in LLM score and accelerated retrieval\nby 5.5X.", "published": "2025-03-02 06:21:56", "link": "http://arxiv.org/abs/2504.06271v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Acoustic Anomaly Detection on UAM Propeller Defect with Acoustic dataset\n  for Crack of drone Propeller (ADCP)", "abstract": "The imminent commercialization of UAM requires stable, AI-based maintenance\nsystems to ensure safety for both passengers and pedestrians. This paper\npresents a methodology for non-destructively detecting cracks in UAM propellers\nusing drone propeller sound datasets. Normal operating sounds were recorded,\nand abnormal sounds (categorized as ripped and broken) were differentiated by\nvarying the microphone-propeller angle and throttle power. Our novel approach\nintegrates FFT and STFT preprocessing techniques to capture both global\nfrequency patterns and local time-frequency variations, thereby enhancing\nanomaly detection performance. The constructed Acoustic Dataset for Crack of\nDrone Propeller (ADCP) demonstrates the potential for detecting propeller\ncracks and lays the groundwork for future UAM maintenance applications.", "published": "2025-03-02 08:40:23", "link": "http://arxiv.org/abs/2503.00790v1", "categories": ["cs.SD", "cs.ET", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploiting Vulnerabilities in Speech Translation Systems through\n  Targeted Adversarial Attacks", "abstract": "As speech translation (ST) systems become increasingly prevalent,\nunderstanding their vulnerabilities is crucial for ensuring robust and reliable\ncommunication. However, limited work has explored this issue in depth. This\npaper explores methods of compromising these systems through imperceptible\naudio manipulations. Specifically, we present two innovative approaches: (1)\nthe injection of perturbation into source audio, and (2) the generation of\nadversarial music designed to guide targeted translation, while also conducting\nmore practical over-the-air attacks in the physical world. Our experiments\nreveal that carefully crafted audio perturbations can mislead translation\nmodels to produce targeted, harmful outputs, while adversarial music achieve\nthis goal more covertly, exploiting the natural imperceptibility of music.\nThese attacks prove effective across multiple languages and translation models,\nhighlighting a systemic vulnerability in current ST architectures. The\nimplications of this research extend beyond immediate security concerns,\nshedding light on the interpretability and robustness of neural speech\nprocessing systems. Our findings underscore the need for advanced defense\nmechanisms and more resilient architectures in the realm of audio systems. More\ndetails and samples can be found at https://adv-st.github.io.", "published": "2025-03-02 16:38:16", "link": "http://arxiv.org/abs/2503.00957v2", "categories": ["cs.SD", "cs.AI", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CBW: Towards Dataset Ownership Verification for Speaker Verification via\n  Clustering-based Backdoor Watermarking", "abstract": "With the increasing adoption of deep learning in speaker verification,\nlarge-scale speech datasets have become valuable intellectual property. To\naudit and prevent the unauthorized usage of these valuable released datasets,\nespecially in commercial or open-source scenarios, we propose a novel dataset\nownership verification method. Our approach introduces a clustering-based\nbackdoor watermark (CBW), enabling dataset owners to determine whether a\nsuspicious third-party model has been trained on a protected dataset under a\nblack-box setting. The CBW method consists of two key stages: dataset\nwatermarking and ownership verification. During watermarking, we implant\nmultiple trigger patterns in the dataset to make similar samples (measured by\ntheir feature similarities) close to the same trigger while dissimilar samples\nare near different triggers. This ensures that any model trained on the\nwatermarked dataset exhibits specific misclassification behaviors when exposed\nto trigger-embedded inputs. To verify dataset ownership, we design a\nhypothesis-test-based framework that statistically evaluates whether a\nsuspicious model exhibits the expected backdoor behavior. We conduct extensive\nexperiments on benchmark datasets, verifying the effectiveness and robustness\nof our method against potential adaptive attacks. The code for reproducing main\nexperiments is available at https://github.com/Radiant0726/CBW", "published": "2025-03-02 02:02:57", "link": "http://arxiv.org/abs/2503.05794v3", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
