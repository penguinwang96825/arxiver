{"title": "Unsupervised Neural Machine Translation with SMT as Posterior\n  Regularization", "abstract": "Without real bilingual corpus available, unsupervised Neural Machine\nTranslation (NMT) typically requires pseudo parallel data generated with the\nback-translation method for the model training. However, due to weak\nsupervision, the pseudo data inevitably contain noises and errors that will be\naccumulated and reinforced in the subsequent training process, leading to bad\ntranslation performance. To address this issue, we introduce phrase based\nStatistic Machine Translation (SMT) models which are robust to noisy data, as\nposterior regularizations to guide the training of unsupervised NMT models in\nthe iterative back-translation process. Our method starts from SMT models built\nwith pre-trained language models and word-level translation tables inferred\nfrom cross-lingual embeddings. Then SMT and NMT models are optimized jointly\nand boost each other incrementally in a unified EM framework. In this way, (1)\nthe negative effect caused by errors in the iterative back-translation process\ncan be alleviated timely by SMT filtering noises from its phrase tables;\nmeanwhile, (2) NMT can compensate for the deficiency of fluency inherent in\nSMT. Experiments conducted on en-fr and en-de translation tasks show that our\nmethod outperforms the strong baseline and achieves new state-of-the-art\nunsupervised machine translation performance.", "published": "2019-01-14 03:34:27", "link": "http://arxiv.org/abs/1901.04112v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Using Context-Dependent Symbols in CTC Without State-Tying\n  Decision Trees", "abstract": "Deep neural acoustic models benefit from context-dependent (CD) modeling of\noutput symbols. We consider direct training of CTC networks with CD outputs,\nand identify two issues. The first one is frame-level normalization of\nprobabilities in CTC, which induces strong language modeling behavior that\nleads to overfitting and interference with external language models. The second\none is poor generalization in the presence of numerous lexical units like\ntriphones or tri-chars. We mitigate the former with utterance-level\nnormalization of probabilities. The latter typically requires reducing the CD\nsymbol inventory with state-tying decision trees, which have to be transferred\nfrom classical GMM-HMM systems. We replace the trees with a CD symbol embedding\nnetwork, which saves parameters and ensures generalization to unseen and\nundersampled CD symbols. The embedding network is trained together with the\nrest of the acoustic model and removes one of the last cases in which neural\nsystems have to be bootstrapped from GMM-HMM ones.", "published": "2019-01-14 16:23:35", "link": "http://arxiv.org/abs/1901.04379v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Human few-shot learning of compositional instructions", "abstract": "People learn in fast and flexible ways that have not been emulated by\nmachines. Once a person learns a new verb \"dax,\" he or she can effortlessly\nunderstand how to \"dax twice,\" \"walk and dax,\" or \"dax vigorously.\" There have\nbeen striking recent improvements in machine learning for natural language\nprocessing, yet the best algorithms require vast amounts of experience and\nstruggle to generalize new concepts in compositional ways. To better understand\nthese distinctively human abilities, we study the compositional skills of\npeople through language-like instruction learning tasks. Our results show that\npeople can learn and use novel functional concepts from very few examples\n(few-shot learning), successfully applying familiar functions to novel inputs.\nPeople can also compose concepts in complex ways that go beyond the provided\ndemonstrations. Two additional experiments examined the assumptions and\ninductive biases that people make when solving these tasks, revealing three\nbiases: mutual exclusivity, one-to-one mappings, and iconic concatenation. We\ndiscuss the implications for cognitive modeling and the potential for building\nmachines with more human-like language learning capabilities.", "published": "2019-01-14 22:19:35", "link": "http://arxiv.org/abs/1901.04587v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Albanian Language Identification in Text Documents", "abstract": "In this work we investigate the accuracy of standard and state-of-the-art\nlanguage identification methods in identifying Albanian in written text\ndocuments. A dataset consisting of news articles written in Albanian has been\nconstructed for this purpose. We noticed a considerable decrease of accuracy\nwhen using test documents that miss the Albanian alphabet letters \" \\\"E \" and \"\n\\c{C} \" and created a custom training corpus that solved this problem by\nachieving an accuracy of more than 99%. Based on our experiments, the most\nperforming language identification methods for Albanian use a na\\\"ive Bayes\nclassifier and n-gram based classification features.", "published": "2019-01-14 10:05:52", "link": "http://arxiv.org/abs/1901.04216v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Image Based Review Text Generation with Emotional Guidance", "abstract": "In the current field of computer vision, automatically generating texts from\ngiven images has been a fully worked technique. Up till now, most works of this\narea focus on image content describing, namely image-captioning. However, rare\nresearches focus on generating product review texts, which is ubiquitous in the\nonline shopping malls and is crucial for online shopping selection and\nevaluation. Different from content describing, review texts include more\nsubjective information of customers, which may bring difference to the results.\nTherefore, we aimed at a new field concerning generating review text from\ncustomers based on images together with the ratings of online shopping\nproducts, which appear as non-image attributes. We made several adjustments to\nthe existing image-captioning model to fit our task, in which we should also\ntake non-image features into consideration. We also did experiments based on\nour model and get effective primary results.", "published": "2019-01-14 05:42:51", "link": "http://arxiv.org/abs/1901.04140v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Exploring Transfer Learning for Low Resource Emotional TTS", "abstract": "During the last few years, spoken language technologies have known a big\nimprovement thanks to Deep Learning. However Deep Learning-based algorithms\nrequire amounts of data that are often difficult and costly to gather.\nParticularly, modeling the variability in speech of different speakers,\ndifferent styles or different emotions with few data remains challenging. In\nthis paper, we investigate how to leverage fine-tuning on a pre-trained Deep\nLearning-based TTS model to synthesize speech with a small dataset of another\nspeaker. Then we investigate the possibility to adapt this model to have\nemotional TTS by fine-tuning the neutral TTS model with a small emotional\ndataset.", "published": "2019-01-14 13:05:48", "link": "http://arxiv.org/abs/1901.04276v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Machine learning for the recognition of emotion in the speech of couples\n  in psychotherapy using the Stanford Suppes Brain Lab Psychotherapy Dataset", "abstract": "The automatic recognition of emotion in speech can inform our understanding\nof language, emotion, and the brain. It also has practical application to\nhuman-machine interactive systems. This paper examines the recognition of\nemotion in naturally occurring speech, where there are no constraints on what\nis said or the emotions expressed. This task is more difficult than that using\ndata collected in scripted, experimentally controlled settings, and fewer\nresults are published. Our data come from couples in psychotherapy. Video and\naudio recordings were made of three couples (A, B, C) over 18 hour-long therapy\nsessions. This paper describes the method used to code the audio recordings for\nthe four emotions of Anger, Sadness, Joy and Tension, plus Neutral, also\ncovering our approach to managing the unbalanced samples that a naturally\noccurring emotional speech dataset produces. Three groups of acoustic features\nwere used in our analysis: filter-bank, frequency, and voice-quality features.\nThe random forests model classified the features. Recognition rates are\nreported for each individual, the result of the speaker-dependent models that\nwe built. In each case, the best recognition rates were achieved using the\nfilter-bank features alone. For Couple A, these rates were 90% for the female\nand 87% for the male for the recognition of three emotions plus Neutral. For\nCouple B, the rates were 84% for the female and 78% for the male for the\nrecognition of all four emotions plus Neutral. For Couple C, a rate of 88% was\nachieved for the female for the recognition of the four emotions plus Neutral\nand 95% for the male for three emotions plus Neutral. For pairwise recognition,\nthe rates ranged from 76% to 99% across the three couples. Our results show\nthat couple therapy is a rich context for the study of emotion in naturally\noccurring speech.", "published": "2019-01-14 02:21:08", "link": "http://arxiv.org/abs/1901.04110v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Music Artist Classification with Convolutional Recurrent Neural Networks", "abstract": "Previous attempts at music artist classification use frame level audio\nfeatures which summarize frequency content within short intervals of time.\nComparatively, more recent music information retrieval tasks take advantage of\ntemporal structure in audio spectrograms using deep convolutional and recurrent\nmodels. This paper revisits artist classification with this new framework and\nempirically explores the impacts of incorporating temporal structure in the\nfeature representation. To this end, an established classification\narchitecture, a Convolutional Recurrent Neural Network (CRNN), is applied to\nthe artist20 music artist identification dataset under a comprehensive set of\nconditions. These include audio clip length, which is a novel contribution in\nthis work, and previously identified considerations such as dataset split and\nfeature level. Our results improve upon baseline works, verify the influence of\nthe producer effect on classification performance and demonstrate the\ntrade-offs between audio length and training set size. The best performing\nmodel achieves an average F1 score of 0.937 across three independent trials\nwhich is a substantial improvement over the corresponding baseline under\nsimilar conditions. Additionally, to showcase the effectiveness of the CRNN's\nfeature extraction capabilities, we visualize audio samples at the model's\nbottleneck layer demonstrating that learned representations segment into\nclusters belonging to their respective artists.", "published": "2019-01-14 20:33:44", "link": "http://arxiv.org/abs/1901.04555v2", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
