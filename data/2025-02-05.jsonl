{"title": "OrderFusion: Encoding Orderbook for Probabilistic Intraday Price Prediction", "abstract": "Efficient and reliable probabilistic prediction of intraday electricity\nprices is essential to manage market uncertainties and support robust trading\nstrategies. However, current methods often suffer from parameter\ninefficiencies, as they fail to fully exploit the potential of modeling\ninterdependencies between bids and offers in the orderbook, requiring a large\nnumber of parameters for representation learning. Furthermore, these methods\nface the quantile crossing issue, where upper quantiles fall below the lower\nquantiles, resulting in unreliable probabilistic predictions. To address these\ntwo challenges, we propose an encoding method called OrderFusion and design a\nhierarchical multi-quantile head. The OrderFusion encodes the orderbook into a\n2.5D representation, which is processed by a tailored jump cross-attention\nbackbone to capture the interdependencies of bids and offers, enabling\nparameter-efficient learning. The head sets the median quantile as an anchor\nand predicts multiple quantiles hierarchically, ensuring reliability by\nenforcing monotonicity between quantiles through non-negative functions.\nExtensive experiments and ablation studies are conducted on four price indices:\n60-min ID3, 60-min ID1, 15-min ID3, and 15-min ID1 using the German orderbook\nover three years to ensure a fair evaluation. The results confirm that our\ndesign choices improve overall performance, offering a parameter-efficient and\nreliable solution for probabilistic intraday price prediction.", "published": "2025-02-05 15:37:21", "link": "http://arxiv.org/abs/2502.06830v1", "categories": ["q-fin.CP", "cs.AI", "cs.LG"], "primary_category": "q-fin.CP"}
{"title": "FactorGCL: A Hypergraph-Based Factor Model with Temporal Residual Contrastive Learning for Stock Returns Prediction", "abstract": "As a fundamental method in economics and finance, the factor model has been\nextensively utilized in quantitative investment. In recent years, there has\nbeen a paradigm shift from traditional linear models with expert-designed\nfactors to more flexible nonlinear machine learning-based models with\ndata-driven factors, aiming to enhance the effectiveness of these factor\nmodels. However, due to the low signal-to-noise ratio in market data, mining\neffective factors in data-driven models remains challenging. In this work, we\npropose a hypergraph-based factor model with temporal residual contrastive\nlearning (FactorGCL) that employs a hypergraph structure to better capture\nhigh-order nonlinear relationships among stock returns and factors. To mine\nhidden factors that supplement human-designed prior factors for predicting\nstock returns, we design a cascading residual hypergraph architecture, in which\nthe hidden factors are extracted from the residual information after removing\nthe influence of prior factors. Additionally, we propose a temporal residual\ncontrastive learning method to guide the extraction of effective and\ncomprehensive hidden factors by contrasting stock-specific residual information\nover different time periods. Our extensive experiments on real stock market\ndata demonstrate that FactorGCL not only outperforms existing state-of-the-art\nmethods but also mines effective hidden factors for predicting stock returns.", "published": "2025-02-05 12:37:15", "link": "http://arxiv.org/abs/2502.05218v1", "categories": ["q-fin.ST", "cs.AI", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "Efficient Triangular Arbitrage Detection via Graph Neural Networks", "abstract": "Triangular arbitrage is a profitable trading strategy in financial markets\nthat exploits discrepancies in currency exchange rates. Traditional methods for\ndetecting triangular arbitrage opportunities, such as exhaustive search\nalgorithms and linear programming solvers, often suffer from high computational\ncomplexity and may miss potential opportunities in dynamic markets. In this\npaper, we propose a novel approach to triangular arbitrage detection using\nGraph Neural Networks (GNNs). By representing the currency exchange network as\na graph, we leverage the powerful representation and learning capabilities of\nGNNs to identify profitable arbitrage opportunities more efficiently.\nSpecifically, we formulate the triangular arbitrage problem as a graph-based\noptimization task and design a GNN architecture that captures the complex\nrelationships between currencies and exchange rates. We introduce a relaxed\nloss function to enable more flexible learning and integrate Deep Q-Learning\nprinciples to optimize the expected returns. Our experiments on a synthetic\ndataset demonstrate that the proposed GNN-based method achieves a higher\naverage yield with significantly reduced computational time compared to\ntraditional methods. This work highlights the potential of using GNNs for\nsolving optimization problems in finance and provides a promising approach for\nreal-time arbitrage detection in dynamic financial markets.", "published": "2025-02-05 14:13:31", "link": "http://arxiv.org/abs/2502.03194v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "Consistent Client Simulation for Motivational Interviewing-based\n  Counseling", "abstract": "Simulating human clients in mental health counseling is crucial for training\nand evaluating counselors (both human or simulated) in a scalable manner.\nNevertheless, past research on client simulation did not focus on complex\nconversation tasks such as mental health counseling. In these tasks, the\nchallenge is to ensure that the client's actions (i.e., interactions with the\ncounselor) are consistent with with its stipulated profiles and negative\nbehavior settings. In this paper, we propose a novel framework that supports\nconsistent client simulation for mental health counseling. Our framework tracks\nthe mental state of a simulated client, controls its state transitions, and\ngenerates for each state behaviors consistent with the client's motivation,\nbeliefs, preferred plan to change, and receptivity. By varying the client\nprofile and receptivity, we demonstrate that consistent simulated clients for\ndifferent counseling scenarios can be effectively created. Both our automatic\nand expert evaluations on the generated counseling sessions also show that our\nclient simulation method achieves higher consistency than previous methods.", "published": "2025-02-05 00:58:30", "link": "http://arxiv.org/abs/2502.02802v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CAMI: A Counselor Agent Supporting Motivational Interviewing through\n  State Inference and Topic Exploration", "abstract": "Conversational counselor agents have become essential tools for addressing\nthe rising demand for scalable and accessible mental health support. This paper\nintroduces CAMI, a novel automated counselor agent grounded in Motivational\nInterviewing (MI) -- a client-centered counseling approach designed to address\nambivalence and facilitate behavior change. CAMI employs a novel STAR\nframework, consisting of client's state inference, motivation topic\nexploration, and response generation modules, leveraging large language models\n(LLMs). These components work together to evoke change talk, aligning with MI\nprinciples and improving counseling outcomes for clients from diverse\nbackgrounds. We evaluate CAMI's performance through both automated and manual\nevaluations, utilizing simulated clients to assess MI skill competency,\nclient's state inference accuracy, topic exploration proficiency, and overall\ncounseling success. Results show that CAMI not only outperforms several\nstate-of-the-art methods but also shows more realistic counselor-like behavior.\nAdditionally, our ablation study underscores the critical roles of state\ninference and topic exploration in achieving this performance.", "published": "2025-02-05 01:09:09", "link": "http://arxiv.org/abs/2502.02807v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Achieving Operational Universality through a Turing Complete Chemputer", "abstract": "The most fundamental abstraction underlying all modern computers is the\nTuring Machine, that is if any modern computer can simulate a Turing Machine,\nan equivalence which is called Turing completeness, it is theoretically\npossible to achieve any task that can be algorithmically described by executing\na series of discrete unit operations. In chemistry, the ability to program\nchemical processes is demanding because it is hard to ensure that the process\ncan be understood at a high level of abstraction, and then reduced to practice.\nHerein we exploit the concept of Turing completeness applied to robotic\nplatforms for chemistry that can be used to synthesise complex molecules\nthrough unit operations that execute chemical processes using a\nchemically-aware programming language, XDL. We leverage the concept of\ncomputability by computers to synthesizability of chemical compounds by\nautomated synthesis machines. The results of an interactive demonstration of\nTuring completeness using the colour gamut and conditional logic are presented\nand examples of chemical use-cases are discussed. Over 16.7 million\ncombinations of Red, Green, Blue (RGB) colour space were binned into 5 discrete\nvalues and measured over 10 regions of interest (ROIs), affording 78 million\npossible states per step and served as a proxy for conceptual, chemical space\nexploration. This formal description establishes a formal framework in future\nchemical programming languages to ensure complex logic operations are expressed\nand executed correctly, with the possibility of error correction, in the\nautomated and autonomous pursuit of increasingly complex molecules.", "published": "2025-02-05 04:06:32", "link": "http://arxiv.org/abs/2502.02872v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lowering the Barrier of Machine Learning: Achieving Zero Manual Labeling\n  in Review Classification Using LLMs", "abstract": "With the internet's evolution, consumers increasingly rely on online reviews\nfor service or product choices, necessitating that businesses analyze extensive\ncustomer feedback to enhance their offerings. While machine learning-based\nsentiment classification shows promise in this realm, its technical complexity\noften bars small businesses and individuals from leveraging such advancements,\nwhich may end up making the competitive gap between small and large businesses\neven bigger in terms of improving customer satisfaction. This paper introduces\nan approach that integrates large language models (LLMs), specifically\nGenerative Pre-trained Transformer (GPT) and Bidirectional Encoder\nRepresentations from Transformers (BERT)-based models, making it accessible to\na wider audience. Our experiments across various datasets confirm that our\napproach retains high classification accuracy without the need for manual\nlabeling, expert knowledge in tuning and data annotation, or substantial\ncomputational power. By significantly lowering the barriers to applying\nsentiment classification techniques, our methodology enhances competitiveness\nand paves the way for making machine learning technology accessible to a\nbroader audience.", "published": "2025-02-05 05:31:54", "link": "http://arxiv.org/abs/2502.02893v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLaVAC: Fine-tuning LLaVA as a Multimodal Sentiment Classifier", "abstract": "We present LLaVAC, a method for constructing a classifier for multimodal\nsentiment analysis. This method leverages fine-tuning of the Large Language and\nVision Assistant (LLaVA) to predict sentiment labels across both image and text\nmodalities. Our approach involves designing a structured prompt that\nincorporates both unimodal and multimodal labels to fine-tune LLaVA, enabling\nit to perform sentiment classification effectively. Experiments on the\nMVSA-Single dataset demonstrate that LLaVAC outperforms existing methods in\nmultimodal sentiment analysis across three data processing procedures. The\nimplementation of LLaVAC is publicly available at\nhttps://github.com/tchayintr/llavac.", "published": "2025-02-05 07:10:04", "link": "http://arxiv.org/abs/2502.02938v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Position: Editing Large Language Models Poses Serious Safety Risks", "abstract": "Large Language Models (LLMs) contain large amounts of facts about the world.\nThese facts can become outdated over time, which has led to the development of\nknowledge editing methods (KEs) that can change specific facts in LLMs with\nlimited side effects. This position paper argues that editing LLMs poses\nserious safety risks that have been largely overlooked. First, we note the fact\nthat KEs are widely available, computationally inexpensive, highly performant,\nand stealthy makes them an attractive tool for malicious actors. Second, we\ndiscuss malicious use cases of KEs, showing how KEs can be easily adapted for a\nvariety of malicious purposes. Third, we highlight vulnerabilities in the AI\necosystem that allow unrestricted uploading and downloading of updated models\nwithout verification. Fourth, we argue that a lack of social and institutional\nawareness exacerbates this risk, and discuss the implications for different\nstakeholders. We call on the community to (i) research tamper-resistant models\nand countermeasures against malicious model editing, and (ii) actively engage\nin securing the AI ecosystem.", "published": "2025-02-05 07:51:32", "link": "http://arxiv.org/abs/2502.02958v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DOLFIN -- Document-Level Financial test set for Machine Translation", "abstract": "Despite the strong research interest in document-level Machine Translation\n(MT), the test sets dedicated to this task are still scarce. The existing test\nsets mainly cover topics from the general domain and fall short on specialised\ndomains, such as legal and financial. Also, in spite of their document-level\naspect, they still follow a sentence-level logic that does not allow for\nincluding certain linguistic phenomena such as information reorganisation. In\nthis work, we aim to fill this gap by proposing a novel test set: DOLFIN. The\ndataset is built from specialised financial documents, and it makes a step\ntowards true document-level MT by abandoning the paradigm of perfectly aligned\nsentences, presenting data in units of sections rather than sentences. The test\nset consists of an average of 1950 aligned sections for five language pairs. We\npresent a detailed data collection pipeline that can serve as inspiration for\naligning new document-level datasets. We demonstrate the usefulness and quality\nof this test set by evaluating a number of models. Our results show that the\ntest set is able to discriminate between context-sensitive and context-agnostic\nmodels and shows the weaknesses when models fail to accurately translate\nfinancial texts. The test set is made public for the community.", "published": "2025-02-05 10:30:40", "link": "http://arxiv.org/abs/2502.03053v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IAO Prompting: Making Knowledge Flow Explicit in LLMs through Structured\n  Reasoning Templates", "abstract": "While Large Language Models (LLMs) demonstrate impressive reasoning\ncapabilities, understanding and validating their knowledge utilization remains\nchallenging. Chain-of-thought (CoT) prompting partially addresses this by\nrevealing intermediate reasoning steps, but the knowledge flow and application\nremain implicit. We introduce IAO (Input-Action-Output) prompting, a structured\ntemplate-based method that explicitly models how LLMs access and apply their\nknowledge during complex reasoning tasks. IAO decomposes problems into\nsequential steps, each clearly identifying the input knowledge being used, the\naction being performed, and the resulting output. This structured decomposition\nenables us to trace knowledge flow, verify factual consistency, and identify\npotential knowledge gaps or misapplications. Through experiments across diverse\nreasoning tasks, we demonstrate that IAO not only improves zero-shot\nperformance but also provides transparency in how LLMs leverage their stored\nknowledge. Human evaluation confirms that this structured approach enhances our\nability to verify knowledge utilization and detect potential hallucinations or\nreasoning errors. Our findings provide insights into both knowledge\nrepresentation within LLMs and methods for more reliable knowledge application.", "published": "2025-02-05 11:14:20", "link": "http://arxiv.org/abs/2502.03080v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structured Token Retention and Computational Memory Paths in Large\n  Language Models", "abstract": "Memory retention mechanisms play a central role in determining the efficiency\nof computational architectures designed for processing extended sequences.\nConventional methods for token management often impose fixed retention\nthresholds or rely on uniform attention weight distributions, leading to\ninefficient memory utilization and premature information loss in extended\nsequence modeling. Structured Token Retention (STR) introduces a probabilistic\nselection framework that dynamically adjusts token persistence based on\ncontextual significance, ensuring that computational resources are allocated to\nsemantically relevant elements. Computational Memory Paths (CMP) extend this\nframework through hierarchical memory allocation, refining retention efficiency\nthrough structured reallocation of token embeddings. Comparative assessments\nagainst baseline models demonstrate that STR and CMP improve token survival\nrates across long input sequences while reducing cumulative error propagation\nacross processing layers. Experimental results further indicate reductions in\ncomputational overhead, improving inference speed without degrading contextual\ncoherence. Token distribution analyses reveal that structured memory allocation\nprevents excessive redundancy in attention weight calculations, optimizing\ninformation retrieval efficiency in large-scale generative architectures. The\nintegration of STR and CMP into an open-source model illustrates the\nadaptability of structured memory retention methodologies, highlighting their\napplicability in generative text processing, long-context comprehension, and\nscalable sequence modeling.", "published": "2025-02-05 11:59:22", "link": "http://arxiv.org/abs/2502.03102v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Language Bias in Cross-Lingual Job Retrieval: A Recruitment\n  Platform Perspective", "abstract": "Understanding the textual components of resumes and job postings is critical\nfor improving job-matching accuracy and optimizing job search systems in online\nrecruitment platforms. However, existing works primarily focus on analyzing\nindividual components within this information, requiring multiple specialized\ntools to analyze each aspect. Such disjointed methods could potentially hinder\noverall generalizability in recruitment-related text processing. Therefore, we\npropose a unified sentence encoder that utilized multi-task dual-encoder\nframework for jointly learning multiple component into the unified sentence\nencoder. The results show that our method outperforms other state-of-the-art\nmodels, despite its smaller model size. Moreover, we propose a novel metric,\nLanguage Bias Kullback-Leibler Divergence (LBKL), to evaluate language bias in\nthe encoder, demonstrating significant bias reduction and superior\ncross-lingual performance.", "published": "2025-02-05 14:38:56", "link": "http://arxiv.org/abs/2502.03220v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A scale of conceptual orality and literacy: Automatic text\n  categorization in the tradition of \"N\u00e4he und Distanz\"", "abstract": "Koch and Oesterreicher's model of \"N\\\"ahe und Distanz\" (N\\\"ahe = immediacy,\nconceptual orality; Distanz = distance, conceptual literacy) is constantly used\nin German linguistics. However, there is no statistical foundation for use in\ncorpus linguistic analyzes, while it is increasingly moving into empirical\ncorpus linguistics. Theoretically, it is stipulated, among other things, that\nwritten texts can be rated on a scale of conceptual orality and literacy by\nlinguistic features. This article establishes such a scale based on PCA and\ncombines it with automatic analysis. Two corpora of New High German serve as\nexamples. When evaluating established features, a central finding is that\nfeatures of conceptual orality and literacy must be distinguished in order to\nrank texts in a differentiated manner. The scale is also discussed with a view\nto its use in corpus compilation and as a guide for analyzes in larger corpora.\nWith a theory-driven starting point and as a \"tailored\" dimension, the approach\ncompared to Biber's Dimension 1 is particularly suitable for these supporting,\ncontrolling tasks.", "published": "2025-02-05 15:08:37", "link": "http://arxiv.org/abs/2502.03252v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How do Humans and Language Models Reason About Creativity? A Comparative\n  Analysis", "abstract": "Creativity assessment in science and engineering is increasingly based on\nboth human and AI judgment, but the cognitive processes and biases behind these\nevaluations remain poorly understood. We conducted two experiments examining\nhow including example solutions with ratings impact creativity evaluation,\nusing a finegrained annotation protocol where raters were tasked with\nexplaining their originality scores and rating for the facets of remoteness\n(whether the response is \"far\" from everyday ideas), uncommonness (whether the\nresponse is rare), and cleverness. In Study 1, we analyzed creativity ratings\nfrom 72 experts with formal science or engineering training, comparing those\nwho received example solutions with ratings (example) to those who did not (no\nexample). Computational text analysis revealed that, compared to experts with\nexamples, no-example experts used more comparative language (e.g.,\n\"better/worse\") and emphasized solution uncommonness, suggesting they may have\nrelied more on memory retrieval for comparisons. In Study 2, parallel analyses\nwith state-of-the-art LLMs revealed that models prioritized uncommonness and\nremoteness of ideas when rating originality, suggesting an evaluative process\nrooted around the semantic similarity of ideas. In the example condition, while\nLLM accuracy in predicting the true originality scores improved, the\ncorrelations of remoteness, uncommonness, and cleverness with originality also\nincreased substantially - to upwards of 0.99 - suggesting a homogenization in\nthe LLMs evaluation of the individual facets. These findings highlight\nimportant implications for how humans and AI reason about creativity and\nsuggest diverging preferences for what different populations prioritize when\nrating.", "published": "2025-02-05 15:08:43", "link": "http://arxiv.org/abs/2502.03253v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Minerva: A Programmable Memory Test Benchmark for Language Models", "abstract": "How effectively can LLM-based AI assistants utilize their memory (context) to\nperform various tasks? Traditional data benchmarks, which are often manually\ncrafted, suffer from several limitations: they are static, susceptible to\noverfitting, difficult to interpret, and lack actionable insights--failing to\npinpoint the specific capabilities a model lacks when it does not pass a test.\nIn this paper, we present a framework for automatically generating a\ncomprehensive set of tests to evaluate models' abilities to use their memory\neffectively. Our framework extends the range of capability tests beyond the\ncommonly explored (passkey, key-value, needle in the haystack) search, a\ndominant focus in the literature. Specifically, we evaluate models on atomic\ntasks such as searching, recalling, editing, matching, comparing information in\ncontext memory, and performing basic operations when inputs are structured into\ndistinct blocks, simulating real-world data. Additionally, we design composite\ntests to investigate the models' ability to maintain state while operating on\nmemory. Our benchmark enables an interpretable, detailed assessment of memory\ncapabilities of LLMs.", "published": "2025-02-05 16:53:45", "link": "http://arxiv.org/abs/2502.03358v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Integrating automatic speech recognition into remote healthcare\n  interpreting: A pilot study of its impact on interpreting quality", "abstract": "This paper reports on the results from a pilot study investigating the impact\nof automatic speech recognition (ASR) technology on interpreting quality in\nremote healthcare interpreting settings. Employing a within-subjects experiment\ndesign with four randomised conditions, this study utilises scripted medical\nconsultations to simulate dialogue interpreting tasks. It involves four trainee\ninterpreters with a language combination of Chinese and English. It also\ngathers participants' experience and perceptions of ASR support through cued\nretrospective reports and semi-structured interviews. Preliminary data suggest\nthat the availability of ASR, specifically the access to full ASR transcripts\nand to ChatGPT-generated summaries based on ASR, effectively improved\ninterpreting quality. Varying types of ASR output had different impacts on the\ndistribution of interpreting error types. Participants reported similar\ninteractive experiences with the technology, expressing their preference for\nfull ASR transcripts. This pilot study shows encouraging results of applying\nASR to dialogue-based healthcare interpreting and offers insights into the\noptimal ways to present ASR output to enhance interpreter experience and\nperformance. However, it should be emphasised that the main purpose of this\nstudy was to validate the methodology and that further research with a larger\nsample size is necessary to confirm these findings.", "published": "2025-02-05 17:17:29", "link": "http://arxiv.org/abs/2502.03381v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Think or Step-by-Step? UnZIPping the Black Box in Zero-Shot Prompts", "abstract": "Zero-shot prompting techniques have significantly improved the performance of\nLarge Language Models (LLMs). However, we lack a clear understanding of why\nzero-shot prompts are so effective. For example, in the prompt \"Let's think\nstep-by-step,\" is \"think\" or \"step-by-step\" more crucial to its success?\nExisting interpretability methods, such as gradient-based and attention-based\napproaches, are computationally intensive and restricted to open-source models.\nWe introduce the ZIP score (Zero-shot Importance of Perturbation score), a\nversatile metric applicable to both open and closed-source models, based on\nsystematic input word perturbations. Our experiments across four recent LLMs,\nseven widely-used prompts, and several tasks, reveal interesting patterns in\nword importance. For instance, while both 'step-by-step' and 'think' show high\nZIP scores, which one is more influential depends on the model and task. We\nvalidate our method using controlled experiments and compare our results with\nhuman judgments, finding that proprietary models align more closely with human\nintuition regarding word significance. These findings enhance our understanding\nof LLM behavior and contribute to developing more effective zero-shot prompts\nand improved model analysis.", "published": "2025-02-05 18:04:29", "link": "http://arxiv.org/abs/2502.03418v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sorting the Babble in Babel: Assessing the Performance of Language\n  Detection Algorithms on the OpenAlex Database", "abstract": "This project aims to compare various language classification procedures,\nprocedures combining various Python language detection algorithms and\nmetadata-based corpora extracted from manually-annotated articles sampled from\nthe OpenAlex database. Following an analysis of precision and recall\nperformance for each algorithm, corpus, and language as well as of processing\nspeeds recorded for each algorithm and corpus type, overall procedure\nperformance at the database level was simulated using probabilistic confusion\nmatrices for each algorithm, corpus, and language as well as a probabilistic\nmodel of relative article language frequencies for the whole OpenAlex database.\nResults show that procedure performance strongly depends on the importance\ngiven to each of the measures implemented: for contexts where precision is\npreferred, using the LangID algorithm on the greedy corpus gives the best\nresults; however, for all cases where recall is considered at least slightly\nmore important than precision or as soon as processing times are given any kind\nof consideration, the procedure combining the FastSpell algorithm and the\nTitles corpus outperforms all other alternatives. Given the lack of truly\nmultilingual, large-scale bibliographic databases, it is hoped that these\nresults help confirm and foster the unparalleled potential of the OpenAlex\ndatabase for cross-linguistic, bibliometric-based research and analysis.", "published": "2025-02-05 21:29:09", "link": "http://arxiv.org/abs/2502.03627v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-Preserving Gradient Modulation for Large Language Models: A\n  Novel Approach to Semantic Consistency in Long-Form Text Generation", "abstract": "Maintaining semantic consistency over extended text sequences remains a\nfundamental challenge in long-form text generation, where conventional training\nmethodologies often struggle to prevent contextual drift and coherence\ndegradation. A novel gradient modulation approach is introduced, designed to\nadjust parameter updates dynamically in response to contextual relevance,\nensuring that generated text remains aligned with prior discourse. By\nintegrating a modulation function that selectively amplifies or attenuates\ngradients based on learned contextual dependencies, the proposed method\nenhances the stability of model-generated narratives without imposing\nsignificant computational overhead. Comparative evaluations against baseline\nmodels reveal improvements in coherence, contextual retention, and long-range\ndependency tracking, demonstrating the effectiveness of modifying the learning\nprocess at the gradient level. The results indicate that sentence structure\nvariability and lexical diversity benefit from this approach, mitigating\nrepetitive phrasing and improving adaptability across diverse linguistic\ncontexts. Statistical validation of coherence metrics further substantiates the\nobserved enhancements, with a significant reduction in inconsistencies emerging\nas a direct consequence of the modulation mechanism. Computational efficiency\nassessments confirm that the framework achieves these gains without requiring\nsubstantial modifications to the underlying architecture, ensuring\ncompatibility with existing optimization workflows.", "published": "2025-02-05 22:13:06", "link": "http://arxiv.org/abs/2502.03643v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speculative Prefill: Turbocharging TTFT with Lightweight and\n  Training-Free Token Importance Estimation", "abstract": "Improving time-to-first-token (TTFT) is an essentially important objective in\nmodern large language model (LLM) inference engines. Because optimizing TTFT\ndirectly results in higher maximal QPS and meets the requirements of many\ncritical applications. However, boosting TTFT is notoriously challenging since\nit is purely compute-bounded and the performance bottleneck shifts from the\nself-attention to the MLP part. We present SpecPrefill, a training free\nframework that accelerates the inference TTFT for both long and medium context\nqueries based on the following insight: LLMs are generalized enough to still\npreserve the quality given only a carefully chosen subset of prompt tokens. At\nits core, SpecPrefill leverages a lightweight model to speculate locally\nimportant tokens based on the context. These tokens, along with the necessary\npositional information, are then sent to the main model for processing. We\nevaluate SpecPrefill with a diverse set of tasks, followed by a comprehensive\nbenchmarking of performance improvement both in a real end-to-end setting and\nablation studies. SpecPrefill manages to serve Llama-3.1-405B-Instruct-FP8 with\nup to $7\\times$ maximal end-to-end QPS on real downstream tasks and\n$7.66\\times$ TTFT improvement during benchmarking.", "published": "2025-02-05 00:22:06", "link": "http://arxiv.org/abs/2502.02789v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging the true depth of LLMs", "abstract": "Large Language Models demonstrate remarkable capabilities at the cost of high\ncompute requirements. While recent research has shown that intermediate layers\ncan be removed or have their order shuffled without impacting performance\nsignificantly, these findings have not been employed to reduce the\ncomputational cost of inference. We investigate several potential ways to\nreduce the depth of pre-trained LLMs without significantly affecting\nperformance. Leveraging our insights, we present a novel approach that exploits\nthis decoupling between layers by grouping some of them into pairs that can be\nevaluated in parallel.\n  This modification of the computational graph -- through better parallelism --\nresults in an average improvement of around 1.20x on the number of tokens\ngenerated per second, without re-training nor fine-tuning, while retaining\n95%-99% of the original accuracy. Empirical evaluation demonstrates that this\napproach significantly improves serving efficiency while maintaining model\nperformance, offering a practical improvement for large-scale LLM deployment.", "published": "2025-02-05 00:26:27", "link": "http://arxiv.org/abs/2502.02790v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Position: Multimodal Large Language Models Can Significantly Advance\n  Scientific Reasoning", "abstract": "Scientific reasoning, the process through which humans apply logic, evidence,\nand critical thinking to explore and interpret scientific phenomena, is\nessential in advancing knowledge reasoning across diverse fields. However,\ndespite significant progress, current scientific reasoning models still\nstruggle with generalization across domains and often fall short of multimodal\nperception. Multimodal Large Language Models (MLLMs), which integrate text,\nimages, and other modalities, present an exciting opportunity to overcome these\nlimitations and enhance scientific reasoning. Therefore, this position paper\nargues that MLLMs can significantly advance scientific reasoning across\ndisciplines such as mathematics, physics, chemistry, and biology. First, we\npropose a four-stage research roadmap of scientific reasoning capabilities, and\nhighlight the current state of MLLM applications in scientific reasoning,\nnoting their ability to integrate and reason over diverse data types. Second,\nwe summarize the key challenges that remain obstacles to achieving MLLM's full\npotential. To address these challenges, we propose actionable insights and\nsuggestions for the future. Overall, our work offers a novel perspective on\nMLLM integration with scientific reasoning, providing the LLM community with a\nvaluable vision for achieving Artificial General Intelligence (AGI).", "published": "2025-02-05 04:05:27", "link": "http://arxiv.org/abs/2502.02871v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Benchmark for the Detection of Metalinguistic Disagreements between\n  LLMs and Knowledge Graphs", "abstract": "Evaluating large language models (LLMs) for tasks like fact extraction in\nsupport of knowledge graph construction frequently involves computing accuracy\nmetrics using a ground truth benchmark based on a knowledge graph (KG). These\nevaluations assume that errors represent factual disagreements. However, human\ndiscourse frequently features metalinguistic disagreement, where agents differ\nnot on facts but on the meaning of the language used to express them. Given the\ncomplexity of natural language processing and generation using LLMs, we ask: do\nmetalinguistic disagreements occur between LLMs and KGs? Based on an\ninvestigation using the T-REx knowledge alignment dataset, we hypothesize that\nmetalinguistic disagreement does in fact occur between LLMs and KGs, with\npotential relevance for the practice of knowledge graph engineering. We propose\na benchmark for evaluating the detection of factual and metalinguistic\ndisagreements between LLMs and KGs. An initial proof of concept of such a\nbenchmark is available on Github.", "published": "2025-02-05 05:37:26", "link": "http://arxiv.org/abs/2502.02896v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM-KT: Aligning Large Language Models with Knowledge Tracing using a\n  Plug-and-Play Instruction", "abstract": "The knowledge tracing (KT) problem is an extremely important topic in\npersonalized education, which aims to predict whether students can correctly\nanswer the next question based on their past question-answer records. Prior\nwork on this task mainly focused on learning the sequence of behaviors based on\nthe IDs or textual information. However, these studies usually fail to capture\nstudents' sufficient behavioral patterns without reasoning with rich world\nknowledge about questions. In this paper, we propose a large language models\n(LLMs)-based framework for KT, named \\texttt{\\textbf{LLM-KT}}, to integrate the\nstrengths of LLMs and traditional sequence interaction models. For task-level\nalignment, we design Plug-and-Play instruction to align LLMs with KT,\nleveraging LLMs' rich knowledge and powerful reasoning capacity. For\nmodality-level alignment, we design the plug-in context and sequence to\nintegrate multiple modalities learned by traditional methods. To capture the\nlong context of history records, we present a plug-in context to flexibly\ninsert the compressed context embedding into LLMs using question-specific and\nconcept-specific tokens. Furthermore, we introduce a plug-in sequence to\nenhance LLMs with sequence interaction behavior representation learned by\ntraditional sequence models using a sequence adapter. Extensive experiments\nshow that \\texttt{\\textbf{LLM-KT}} obtains state-of-the-art performance on four\ntypical datasets by comparing it with approximately 20 strong baselines.", "published": "2025-02-05 07:21:49", "link": "http://arxiv.org/abs/2502.02945v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ReachAgent: Enhancing Mobile Agent via Page Reaching and Operation", "abstract": "Recently, mobile AI agents have gained increasing attention. Given a task,\nmobile AI agents can interact with mobile devices in multiple steps and finally\nform a GUI flow that solves the task. However, existing agents tend to focus on\nmost task-relevant elements at each step, leading to local optimal solutions\nand ignoring the overall GUI flow. To address this issue, we constructed a\ntraining dataset called MobileReach, which breaks the task into page reaching\nand operation subtasks. Furthermore, we propose ReachAgent, a two-stage\nframework that focuses on improving its task-completion abilities. It utilizes\nthe page reaching and page operation subtasks, along with reward-based\npreference GUI flows, to further enhance the agent. Experimental results show\nthat ReachAgent significantly improves the IoU Acc and Text Acc by 7.12% and\n7.69% on the step-level and 4.72% and 4.63% on the task-level compared to the\nSOTA agent. Our data and code will be released upon acceptance.", "published": "2025-02-05 07:35:23", "link": "http://arxiv.org/abs/2502.02955v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MedBioLM: Optimizing Medical and Biological QA with Fine-Tuned Large\n  Language Models and Retrieval-Augmented Generation", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nnatural language processing tasks. However, their application to specialized\ndomains such as medicine and biology requires further optimization to ensure\nfactual accuracy, reliability, and contextual depth. We introduce MedBioLM, a\ndomain-adapted biomedical question-answering model designed to enhance both\nshort-form and long-form queries. By integrating fine-tuning and\nretrieval-augmented generation (RAG), MedBioLM dynamically incorporates\ndomain-specific knowledge, improving reasoning abilities and factual accuracy.\nTo evaluate its effectiveness, we fine-tuned the model on diverse biomedical QA\ndatasets, covering structured multiple-choice assessments and complex clinical\nreasoning tasks. Fine-tuning significantly improves accuracy on benchmark\ndatasets, while RAG enhances factual consistency. These results highlight the\npotential of domain-optimized LLMs in advancing biomedical research, medical\neducation, and clinical decision support.", "published": "2025-02-05 08:58:35", "link": "http://arxiv.org/abs/2502.03004v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Scaling Laws for Upcycling Mixture-of-Experts Language Models", "abstract": "Pretraining large language models (LLMs) is resource-intensive, often\nrequiring months of training time even with high-end GPU clusters. There are\ntwo approaches of mitigating such computational demands: reusing smaller models\nto train larger ones (upcycling), and training computationally efficient models\nlike mixture-of-experts (MoE). In this paper, we study the upcycling of LLMs to\nMoE models, of which the scaling behavior remains underexplored. Through\nextensive experiments, we identify empirical scaling laws that describe how\nperformance depends on dataset size and model configuration. Particularly, we\nshow that, while scaling these factors improves performance, there is a novel\ninteraction term between the dense and upcycled training dataset that limits\nthe efficiency of upcycling at large computational budgets. Based on these\nfindings, we provide guidance to scale upcycling, and establish conditions\nunder which upcycling outperforms from-scratch trainings within budget\nconstraints.", "published": "2025-02-05 09:11:13", "link": "http://arxiv.org/abs/2502.03009v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Analyze Feature Flow to Enhance Interpretation and Steering in Language\n  Models", "abstract": "We introduce a new approach to systematically map features discovered by\nsparse autoencoder across consecutive layers of large language models,\nextending earlier work that examined inter-layer feature links. By using a\ndata-free cosine similarity technique, we trace how specific features persist,\ntransform, or first appear at each stage. This method yields granular flow\ngraphs of feature evolution, enabling fine-grained interpretability and\nmechanistic insights into model computations. Crucially, we demonstrate how\nthese cross-layer feature maps facilitate direct steering of model behavior by\namplifying or suppressing chosen features, achieving targeted thematic control\nin text generation. Together, our findings highlight the utility of a causal,\ncross-layer interpretability framework that not only clarifies how features\ndevelop through forward passes but also provides new means for transparent\nmanipulation of large language models.", "published": "2025-02-05 09:39:34", "link": "http://arxiv.org/abs/2502.03032v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Knowledge Distillation from Large Language Models for Household Energy\n  Modeling", "abstract": "Machine learning (ML) is increasingly vital for smart-grid research, yet\nrestricted access to realistic, diverse data - often due to privacy concerns -\nslows progress and fuels doubts within the energy sector about adopting\nML-based strategies. We propose integrating Large Language Models (LLMs) in\nenergy modeling to generate realistic, culturally sensitive, and\nbehavior-specific data for household energy usage across diverse geographies.\nIn this study, we employ and compare five different LLMs to systematically\nproduce family structures, weather patterns, and daily consumption profiles for\nhouseholds in six distinct countries. A four-stage methodology synthesizes\ncontextual daily data, including culturally nuanced activities, realistic\nweather ranges, HVAC operations, and distinct `energy signatures' that capture\nunique consumption footprints. Additionally, we explore an alternative strategy\nwhere external weather datasets can be directly integrated, bypassing\nintermediate weather modeling stages while ensuring physically consistent data\ninputs. The resulting dataset provides insights into how cultural, climatic,\nand behavioral factors converge to shape carbon emissions, offering a\ncost-effective avenue for scenario-based energy optimization. This approach\nunderscores how prompt engineering, combined with knowledge distillation, can\nadvance sustainable energy research and climate mitigation efforts. Source code\nis available at\nhttps://github.com/Singularity-AI-Lab/LLM-Energy-Knowledge-Distillation .", "published": "2025-02-05 09:43:14", "link": "http://arxiv.org/abs/2502.03034v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Teaching Large Language Models Number-Focused Headline Generation With\n  Key Element Rationales", "abstract": "Number-focused headline generation is a summarization task requiring both\nhigh textual quality and precise numerical accuracy, which poses a unique\nchallenge for Large Language Models (LLMs). Existing studies in the literature\nfocus only on either textual quality or numerical reasoning and thus are\ninadequate to address this challenge. In this paper, we propose a novel\nchain-of-thought framework for using rationales comprising key elements of the\nTopic, Entities, and Numerical reasoning (TEN) in news articles to enhance the\ncapability for LLMs to generate topic-aligned high-quality texts with precise\nnumerical accuracy. Specifically, a teacher LLM is employed to generate TEN\nrationales as supervision data, which are then used to teach and fine-tune a\nstudent LLM. Our approach teaches the student LLM automatic generation of\nrationales with enhanced capability for numerical reasoning and topic-aligned\nnumerical headline generation. Experiments show that our approach achieves\nsuperior performance in both textual quality and numerical accuracy.", "published": "2025-02-05 12:39:07", "link": "http://arxiv.org/abs/2502.03129v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scalable In-Context Learning on Tabular Data via Retrieval-Augmented\n  Large Language Models", "abstract": "Recent studies have shown that large language models (LLMs), when customized\nwith post-training on tabular data, can acquire general tabular in-context\nlearning (TabICL) capabilities. These models are able to transfer effectively\nacross diverse data schemas and different task domains. However, existing\nLLM-based TabICL approaches are constrained to few-shot scenarios due to the\nsequence length limitations of LLMs, as tabular instances represented in plain\ntext consume substantial tokens. To address this limitation and enable scalable\nTabICL for any data size, we propose retrieval-augmented LLMs tailored to\ntabular data. Our approach incorporates a customized retrieval module, combined\nwith retrieval-guided instruction-tuning for LLMs. This enables LLMs to\neffectively leverage larger datasets, achieving significantly improved\nperformance across 69 widely recognized datasets and demonstrating promising\nscaling behavior. Extensive comparisons with state-of-the-art tabular models\nreveal that, while LLM-based TabICL still lags behind well-tuned numeric models\nin overall performance, it uncovers powerful algorithms under limited contexts,\nenhances ensemble diversity, and excels on specific datasets. These unique\nproperties underscore the potential of language as a universal and accessible\ninterface for scalable tabular data learning.", "published": "2025-02-05 13:16:41", "link": "http://arxiv.org/abs/2502.03147v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Euska\u00f1olDS: A Naturally Sourced Corpus for Basque-Spanish\n  Code-Switching", "abstract": "Code-switching (CS) remains a significant challenge in Natural Language\nProcessing (NLP), mainly due a lack of relevant data. In the context of the\ncontact between the Basque and Spanish languages in the north of the Iberian\nPeninsula, CS frequently occurs in both formal and informal spontaneous\ninteractions. However, resources to analyse this phenomenon and support the\ndevelopment and evaluation of models capable of understanding and generating\ncode-switched language for this language pair are almost non-existent. We\nintroduce a first approach to develop a naturally sourced corpus for\nBasque-Spanish code-switching. Our methodology consists of identifying CS texts\nfrom previously available corpora using language identification models, which\nare then manually validated to obtain a reliable subset of CS instances. We\npresent the properties of our corpus and make it available under the name\nEuska\\~nolDS.", "published": "2025-02-05 14:04:42", "link": "http://arxiv.org/abs/2502.03188v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large\n  Language Models", "abstract": "Despite their impressive capacities, Large language models (LLMs) often\nstruggle with the hallucination issue of generating inaccurate or fabricated\ncontent even when they possess correct knowledge. In this paper, we extend the\nexploration of the correlation between hidden-state prediction changes and\noutput factuality into a deeper, token-wise level. Based on the insights , we\npropose cross-layer Entropy eNhanced Decoding (END), a decoding method that\nmitigates hallucinations without requiring extra training. END leverages inner\nprobability changes across layers to individually quantify the factual\nknowledge required for each candidate token, and adjusts the final predicting\ndistribution to prioritize tokens with higher factuality. Experiments on both\nhallucination and QA benchmarks demonstrate that END significantly enhances the\ntruthfulness and informativeness of generated content while maintaining robust\nQA accuracy. Moreover, our work provides a deeper perspective on understanding\nthe correlations between inherent knowledge and output factuality.", "published": "2025-02-05 14:19:52", "link": "http://arxiv.org/abs/2502.03199v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient extraction of medication information from clinical notes: an\n  evaluation in two languages", "abstract": "Objective: To evaluate the accuracy, computational cost and portability of a\nnew Natural Language Processing (NLP) method for extracting medication\ninformation from clinical narratives. Materials and Methods: We propose an\noriginal transformer-based architecture for the extraction of entities and\ntheir relations pertaining to patients' medication regimen. First, we used this\napproach to train and evaluate a model on French clinical notes, using a newly\nannotated corpus from H\\^opitaux Universitaires de Strasbourg. Second, the\nportability of the approach was assessed by conducting an evaluation on\nclinical documents in English from the 2018 n2c2 shared task. Information\nextraction accuracy and computational cost were assessed by comparison with an\navailable method using transformers. Results: The proposed architecture\nachieves on the task of relation extraction itself performance that are\ncompetitive with the state-of-the-art on both French and English (F-measures\n0.82 and 0.96 vs 0.81 and 0.95), but reduce the computational cost by 10.\nEnd-to-end (Named Entity recognition and Relation Extraction) F1 performance is\n0.69 and 0.82 for French and English corpus. Discussion: While an existing\nsystem developed for English notes was deployed in a French hospital setting\nwith reasonable effort, we found that an alternative architecture offered\nend-to-end drug information extraction with comparable extraction performance\nand lower computational impact for both French and English clinical text\nprocessing, respectively. Conclusion: The proposed architecture can be used to\nextract medication information from clinical text with high performance and low\ncomputational cost and consequently suits with usually limited hospital IT\nresources", "published": "2025-02-05 15:13:08", "link": "http://arxiv.org/abs/2502.03257v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "ECM: A Unified Electronic Circuit Model for Explaining the Emergence of\n  In-Context Learning and Chain-of-Thought in Large Language Model", "abstract": "Recent advancements in large language models (LLMs) have led to significant\nsuccesses across various applications, where the most noticeable is to a series\nof emerging capabilities, particularly in the areas of In-Context Learning\n(ICL) and Chain-of-Thought (CoT). To better understand and control model\nperformance, many studies have begun investigating the underlying causes of\nthese phenomena and their impact on task outcomes. However, existing\nexplanatory frameworks predominantly focus on isolating and explaining ICL and\nCoT independently, leading to an incomplete understanding of their combined\ninfluence on model performance. To address this gap, we propose the Electronic\nCircuit Model (ECM), which provides a foundation for developing scalable,\nlearnable policies and improving the management of AI-generated content.\nSpecifically, ECM conceptualizes model behavior as an electronic circuit: ICL\nis represented as semantic magnetic field to providing an additional voltage\nfollowing Faraday's Law, while CoT is modeled as series resistors to constrain\nthe model output performance following Ohm's Law. Experimental results\ndemonstrate that the ECM effectively predicts and explains LLM performance\nacross a variety of prompting strategies. Furthermore, we apply ECM to advanced\nreasoning strategy optimization on a series of tasks, such as the International\nOlympiad in Informatics (IOI) and the International Mathematical Olympiad\n(IMO), achieving competitive performance that surpasses nearly 80% of top human\ncompetitors.", "published": "2025-02-05 16:22:33", "link": "http://arxiv.org/abs/2502.03325v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Demystifying Long Chain-of-Thought Reasoning in LLMs", "abstract": "Scaling inference compute enhances reasoning in large language models (LLMs),\nwith long chains-of-thought (CoTs) enabling strategies like backtracking and\nerror correction. Reinforcement learning (RL) has emerged as a crucial method\nfor developing these capabilities, yet the conditions under which long CoTs\nemerge remain unclear, and RL training requires careful design choices. In this\nstudy, we systematically investigate the mechanics of long CoT reasoning,\nidentifying the key factors that enable models to generate long CoT\ntrajectories. Through extensive supervised fine-tuning (SFT) and RL\nexperiments, we present four main findings: (1) While SFT is not strictly\nnecessary, it simplifies training and improves efficiency; (2) Reasoning\ncapabilities tend to emerge with increased training compute, but their\ndevelopment is not guaranteed, making reward shaping crucial for stabilizing\nCoT length growth; (3) Scaling verifiable reward signals is critical for RL. We\nfind that leveraging noisy, web-extracted solutions with filtering mechanisms\nshows strong potential, particularly for out-of-distribution (OOD) tasks such\nas STEM reasoning; and (4) Core abilities like error correction are inherently\npresent in base models, but incentivizing these skills effectively for complex\ntasks via RL demands significant compute, and measuring their emergence\nrequires a nuanced approach. These insights provide practical guidance for\noptimizing training strategies to enhance long CoT reasoning in LLMs. Our code\nis available at: https://github.com/eddycmu/demystify-long-cot.", "published": "2025-02-05 17:13:32", "link": "http://arxiv.org/abs/2502.03373v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LIMO: Less is More for Reasoning", "abstract": "We present a fundamental discovery that challenges our understanding of how\ncomplex reasoning emerges in large language models. While conventional wisdom\nsuggests that sophisticated reasoning tasks demand extensive training data\n(>100,000 examples), we demonstrate that complex mathematical reasoning\nabilities can be effectively elicited with surprisingly few examples. Through\ncomprehensive experiments, our proposed model LIMO demonstrates unprecedented\nperformance in mathematical reasoning. With merely 817 curated training\nsamples, LIMO achieves 57.1% accuracy on AIME and 94.8% on MATH, improving from\nprevious SFT-based models' 6.5% and 59.2% respectively, while only using 1% of\nthe training data required by previous approaches. LIMO demonstrates\nexceptional out-of-distribution generalization, achieving 40.5% absolute\nimprovement across 10 diverse benchmarks, outperforming models trained on 100x\nmore data, challenging the notion that SFT leads to memorization rather than\ngeneralization. Based on these results, we propose the Less-Is-More Reasoning\nHypothesis (LIMO Hypothesis): In foundation models where domain knowledge has\nbeen comprehensively encoded during pre-training, sophisticated reasoning\ncapabilities can emerge through minimal but precisely orchestrated\ndemonstrations of cognitive processes. This hypothesis posits that the\nelicitation threshold for complex reasoning is determined by two key factors:\n(1) the completeness of the model's encoded knowledge foundation during\npre-training, and (2) the effectiveness of post-training examples as \"cognitive\ntemplates\" that show the model how to utilize its knowledge base to solve\ncomplex reasoning tasks. To facilitate reproducibility and future research in\ndata-efficient reasoning, we release LIMO as a comprehensive open-source suite\nat https://github.com/GAIR-NLP/LIMO.", "published": "2025-02-05 17:23:45", "link": "http://arxiv.org/abs/2502.03387v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SPRI: Aligning Large Language Models with Context-Situated Principles", "abstract": "Aligning Large Language Models to integrate and reflect human values,\nespecially for tasks that demand intricate human oversight, is arduous since it\nis resource-intensive and time-consuming to depend on human expertise for\ncontext-specific guidance. Prior work has utilized predefined sets of rules or\nprinciples to steer the behavior of models (Bai et al., 2022; Sun et al.,\n2023). However, these principles tend to be generic, making it challenging to\nadapt them to each individual input query or context. In this work, we present\nSituated-PRInciples (SPRI), a framework requiring minimal or no human effort\nthat is designed to automatically generate guiding principles in real-time for\neach input query and utilize them to align each response. We evaluate SPRI on\nthree tasks, and show that 1) SPRI can derive principles in a complex\ndomain-specific task that leads to on-par performance as expert-crafted ones;\n2) SPRI-generated principles lead to instance-specific rubrics that outperform\nprior LLM-as-a-judge frameworks; 3) using SPRI to generate synthetic SFT data\nleads to substantial improvement on truthfulness. We release our code and model\ngenerations at https://github.com/honglizhan/SPRI-public.", "published": "2025-02-05 17:32:29", "link": "http://arxiv.org/abs/2502.03397v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On Fairness of Unified Multimodal Large Language Model for Image\n  Generation", "abstract": "Unified multimodal large language models (U-MLLMs) have demonstrated\nimpressive performance in visual understanding and generation in an end-to-end\npipeline. Compared with generation-only models (e.g., Stable Diffusion),\nU-MLLMs may raise new questions about bias in their outputs, which can be\naffected by their unified capabilities. This gap is particularly concerning\ngiven the under-explored risk of propagating harmful stereotypes. In this\npaper, we benchmark the latest U-MLLMs and find that most exhibit significant\ndemographic biases, such as gender and race bias. To better understand and\nmitigate this issue, we propose a locate-then-fix strategy, where we audit and\nshow how the individual model component is affected by bias. Our analysis shows\nthat bias originates primarily from the language model. More interestingly, we\nobserve a \"partial alignment\" phenomenon in U-MLLMs, where understanding bias\nappears minimal, but generation bias remains substantial. Thus, we propose a\nnovel balanced preference model to balance the demographic distribution with\nsynthetic data. Experiments demonstrate that our approach reduces demographic\nbias while preserving semantic fidelity. We hope our findings underscore the\nneed for more holistic interpretation and debiasing strategies of U-MLLMs in\nthe future.", "published": "2025-02-05 18:21:03", "link": "http://arxiv.org/abs/2502.03429v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do Large Language Model Benchmarks Test Reliability?", "abstract": "When deploying large language models (LLMs), it is important to ensure that\nthese models are not only capable, but also reliable. Many benchmarks have been\ncreated to track LLMs' growing capabilities, however there has been no similar\nfocus on measuring their reliability. To understand the potential ramifications\nof this gap, we investigate how well current benchmarks quantify model\nreliability. We find that pervasive label errors can compromise these\nevaluations, obscuring lingering model failures and hiding unreliable behavior.\n  Motivated by this gap in the evaluation of reliability, we then propose the\nconcept of so-called platinum benchmarks, i.e., benchmarks carefully curated to\nminimize label errors and ambiguity. As a first attempt at constructing such\nbenchmarks, we revise examples from fifteen existing popular benchmarks. We\nevaluate a wide range of models on these platinum benchmarks and find that,\nindeed, frontier LLMs still exhibit failures on simple tasks such as\nelementary-level math word problems. Analyzing these failures further reveals\npreviously unidentified patterns of problems on which frontier models\nconsistently struggle. We provide code at\nhttps://github.com/MadryLab/platinum-benchmarks", "published": "2025-02-05 18:58:19", "link": "http://arxiv.org/abs/2502.03461v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Can Cross Encoders Produce Useful Sentence Embeddings?", "abstract": "Cross encoders (CEs) are trained with sentence pairs to detect relatedness.\nAs CEs require sentence pairs at inference, the prevailing view is that they\ncan only be used as re-rankers in information retrieval pipelines. Dual\nencoders (DEs) are instead used to embed sentences, where sentence pairs are\nencoded by two separate encoders with shared weights at training, and a loss\nfunction that ensures the pair's embeddings lie close in vector space if the\nsentences are related. DEs however, require much larger datasets to train, and\nare less accurate than CEs. We report a curious finding that embeddings from\nearlier layers of CEs can in fact be used within an information retrieval\npipeline. We show how to exploit CEs to distill a lighter-weight DE, with a\n5.15x speedup in inference time.", "published": "2025-02-05 19:09:53", "link": "http://arxiv.org/abs/2502.03552v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Looking for the Inner Music: Probing LLMs' Understanding of Literary\n  Style", "abstract": "Recent work has demonstrated that language models can be trained to identify\nthe author of much shorter literary passages than has been thought feasible for\ntraditional stylometry. We replicate these results for authorship and extend\nthem to a new dataset measuring novel genre. We find that LLMs are able to\ndistinguish authorship and genre, but they do so in different ways. Some models\nseem to rely more on memorization, while others benefit more from training to\nlearn author/genre characteristics. We then use three methods to probe one\nhigh-performing LLM for features that define style. These include direct\nsyntactic ablations to input text as well as two methods that look at model\ninternals. We find that authorial style is easier to define than genre-level\nstyle and is more impacted by minor syntactic decisions and contextual word\nusage. However, some traits like pronoun usage and word order prove significant\nfor defining both kinds of literary style.", "published": "2025-02-05 22:20:17", "link": "http://arxiv.org/abs/2502.03647v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Advancing Reasoning in Large Language Models: Promising Methods and\n  Approaches", "abstract": "Large Language Models (LLMs) have succeeded remarkably in various natural\nlanguage processing (NLP) tasks, yet their reasoning capabilities remain a\nfundamental challenge. While LLMs exhibit impressive fluency and factual\nrecall, their ability to perform complex reasoning-spanning logical deduction,\nmathematical problem-solving, commonsense inference, and multi-step\nreasoning-often falls short of human expectations. This survey provides a\ncomprehensive review of emerging techniques enhancing reasoning in LLMs. We\ncategorize existing methods into key approaches, including prompting strategies\n(e.g., Chain-of-Thought reasoning, Self-Consistency, and Tree-of-Thought\nreasoning), architectural innovations (e.g., retrieval-augmented models,\nmodular reasoning networks, and neuro-symbolic integration), and learning\nparadigms (e.g., fine-tuning with reasoning-specific datasets, reinforcement\nlearning, and self-supervised reasoning objectives). Additionally, we explore\nevaluation frameworks used to assess reasoning in LLMs and highlight open\nchallenges, such as hallucinations, robustness, and reasoning generalization\nacross diverse tasks. By synthesizing recent advancements, this survey aims to\nprovide insights into promising directions for future research and practical\napplications of reasoning-augmented LLMs.", "published": "2025-02-05 23:31:39", "link": "http://arxiv.org/abs/2502.03671v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMs can be easily Confused by Instructional Distractions", "abstract": "Despite the fact that large language models (LLMs) show exceptional skill in\ninstruction following tasks, this strength can turn into a vulnerability when\nthe models are required to disregard certain instructions.\nInstruction-following tasks typically involve a clear task description and\ninput text containing the target data to be processed. However, when the input\nitself resembles an instruction, confusion may arise, even if there is explicit\nprompting to distinguish between the task instruction and the input. We refer\nto this phenomenon as instructional distraction. In this paper, we introduce a\nnovel benchmark, named DIM-Bench, specifically designed to assess LLMs'\nperformance under instructional distraction. The benchmark categorizes\nreal-world instances of instructional distraction and evaluates LLMs across\nfour instruction tasks: rewriting, proofreading, translation, and style\ntransfer -- alongside five input tasks: reasoning, code generation,\nmathematical reasoning, bias detection, and question answering. Our\nexperimental results reveal that even the most advanced LLMs are susceptible to\ninstructional distraction, often failing to accurately follow user intent in\nsuch cases.", "published": "2025-02-05 04:52:57", "link": "http://arxiv.org/abs/2502.04362v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Analysis for Reasoning Bias of Language Models with Small\n  Initialization", "abstract": "Transformer-based Large Language Models (LLMs) have revolutionized Natural\nLanguage Processing by demonstrating exceptional performance across diverse\ntasks. This study investigates the impact of the parameter initialization scale\non the training behavior and task preferences of LLMs. We discover that smaller\ninitialization scales encourage models to favor reasoning tasks, whereas larger\ninitialization scales lead to a preference for memorization tasks. We validate\nthis reasoning bias via real datasets and meticulously designed anchor\nfunctions. Further analysis of initial training dynamics suggests that specific\nmodel components, particularly the embedding space and self-attention\nmechanisms, play pivotal roles in shaping these learning biases. We provide a\ntheoretical framework from the perspective of model training dynamics to\nexplain these phenomena. Additionally, experiments on real-world language tasks\ncorroborate our theoretical insights. This work enhances our understanding of\nhow initialization strategies influence LLM performance on reasoning tasks and\noffers valuable guidelines for training models.", "published": "2025-02-05 15:23:26", "link": "http://arxiv.org/abs/2502.04375v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf", "abstract": "In contemporary workplaces, meetings are essential for exchanging ideas and\nensuring team alignment but often face challenges such as time consumption,\nscheduling conflicts, and inefficient participation. Recent advancements in\nLarge Language Models (LLMs) have demonstrated their strong capabilities in\nnatural language generation and reasoning, prompting the question: can LLMs\neffectively delegate participants in meetings? To explore this, we develop a\nprototype LLM-powered meeting delegate system and create a comprehensive\nbenchmark using real meeting transcripts. Our evaluation reveals that GPT-4/4o\nmaintain balanced performance between active and cautious engagement\nstrategies. In contrast, Gemini 1.5 Pro tends to be more cautious, while Gemini\n1.5 Flash and Llama3-8B/70B display more active tendencies. Overall, about 60\\%\nof responses address at least one key point from the ground-truth. However,\nimprovements are needed to reduce irrelevant or repetitive content and enhance\ntolerance for transcription errors commonly found in real-world settings.\nAdditionally, we implement the system in practical settings and collect\nreal-world feedback from demos. Our findings underscore the potential and\nchallenges of utilizing LLMs as meeting delegates, offering valuable insights\ninto their practical application for alleviating the burden of meetings.", "published": "2025-02-05 16:25:43", "link": "http://arxiv.org/abs/2502.04376v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Limitations of Large Language Models in Clinical Problem-Solving Arising\n  from Inflexible Reasoning", "abstract": "Large Language Models (LLMs) have attained human-level accuracy on medical\nquestion-answer (QA) benchmarks. However, their limitations in navigating\nopen-ended clinical scenarios have recently been shown, raising concerns about\nthe robustness and generalizability of LLM reasoning across diverse, real-world\nmedical tasks. To probe potential LLM failure modes in clinical\nproblem-solving, we present the medical abstraction and reasoning corpus\n(M-ARC). M-ARC assesses clinical reasoning through scenarios designed to\nexploit the Einstellung effect -- the fixation of thought arising from prior\nexperience, targeting LLM inductive biases toward inflexible pattern matching\nfrom their training data rather than engaging in flexible reasoning. We find\nthat LLMs, including current state-of-the-art o1 and Gemini models, perform\npoorly compared to physicians on M-ARC, often demonstrating lack of commonsense\nmedical reasoning and a propensity to hallucinate. In addition, uncertainty\nestimation analyses indicate that LLMs exhibit overconfidence in their answers,\ndespite their limited accuracy. The failure modes revealed by M-ARC in LLM\nmedical reasoning underscore the need to exercise caution when deploying these\nmodels in clinical settings.", "published": "2025-02-05 18:14:27", "link": "http://arxiv.org/abs/2502.04381v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FedP$^2$EFT: Federated Learning to Personalize Parameter Efficient\n  Fine-Tuning for Multilingual LLMs", "abstract": "Federated learning (FL) has enabled the training of multilingual large\nlanguage models (LLMs) on diverse and decentralized multilingual data,\nespecially on low-resource languages. To improve client-specific performance,\npersonalization via the use of parameter-efficient fine-tuning (PEFT) modules\nsuch as LoRA is common. This involves a personalization strategy (PS), such as\nthe design of the PEFT adapter structures (e.g., in which layers to add LoRAs\nand what ranks) and choice of hyperparameters (e.g., learning rates) for\nfine-tuning. Instead of manual PS configuration, we propose FedP$^2$EFT, a\nfederated learning-to-personalize method for multilingual LLMs in cross-device\nFL settings. Unlike most existing PEFT structure selection methods, which are\nprone to overfitting low-data regimes, FedP$^2$EFT collaboratively learns the\noptimal personalized PEFT structure for each client via Bayesian sparse rank\nselection. Evaluations on both simulated and real-world multilingual FL\nbenchmarks demonstrate that FedP$^2$EFT largely outperforms existing\npersonalized fine-tuning methods, while complementing a range of existing FL\nmethods.", "published": "2025-02-05 21:36:21", "link": "http://arxiv.org/abs/2502.04387v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sovereign Large Language Models: Advantages, Strategy and Regulations", "abstract": "This report analyzes key trends, challenges, risks, and opportunities\nassociated with the development of Large Language Models (LLMs) globally. It\nexamines national experiences in developing LLMs and assesses the feasibility\nof investment in this sector. Additionally, the report explores strategies for\nimplementing, regulating, and financing AI projects at the state level.", "published": "2025-02-05 23:16:58", "link": "http://arxiv.org/abs/2503.04745v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "OPTIC: Optimizing Patient-Provider Triaging & Improving Communications\n  in Clinical Operations using GPT-4 Data Labeling and Model Distillation", "abstract": "The COVID-19 pandemic has accelerated the adoption of telemedicine and\npatient messaging through electronic medical portals (patient medical advice\nrequests, or PMARs). While these platforms enhance patient access to\nhealthcare, they have also increased the burden on healthcare providers due to\nthe surge in PMARs. This study seeks to develop an efficient tool for message\ntriaging to reduce physician workload and improve patient-provider\ncommunication. We developed OPTIC (Optimizing Patient-Provider Triaging &\nImproving Communications in Clinical Operations), a powerful message triaging\ntool that utilizes GPT-4 for data labeling and BERT for model distillation. The\nstudy used a dataset of 405,487 patient messaging encounters from Johns Hopkins\nMedicine between January and June 2020. High-quality labeled data was generated\nthrough GPT-4-based prompt engineering, which was then used to train a BERT\nmodel to classify messages as \"Admin\" or \"Clinical.\" The BERT model achieved\n88.85% accuracy on the test set validated by GPT-4 labeling, with a sensitivity\nof 88.29%, specificity of 89.38%, and an F1 score of 0.8842. BERTopic analysis\nidentified 81 distinct topics within the test data, with over 80% accuracy in\nclassifying 58 topics. The system was successfully deployed through Epic's\nNebula Cloud Platform, demonstrating its practical effectiveness in healthcare\nsettings.", "published": "2025-02-05 05:49:34", "link": "http://arxiv.org/abs/2503.05701v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SimMark: A Robust Sentence-Level Similarity-Based Watermarking Algorithm\n  for Large Language Models", "abstract": "The rapid proliferation of large language models (LLMs) has created an urgent\nneed for reliable methods to detect whether a text is generated by such models.\nIn this paper, we propose SimMark, a posthoc watermarking algorithm that makes\nLLMs' outputs traceable without requiring access to the model's internal\nlogits, enabling compatibility with a wide range of LLMs, including API-only\nmodels. By leveraging the similarity of semantic sentence embeddings and\nrejection sampling to impose detectable statistical patterns imperceptible to\nhumans, and employing a soft counting mechanism, SimMark achieves robustness\nagainst paraphrasing attacks. Experimental results demonstrate that SimMark\nsets a new benchmark for robust watermarking of LLM-generated content,\nsurpassing prior sentence-level watermarking techniques in robustness, sampling\nefficiency, and applicability across diverse domains, all while preserving the\ntext quality.", "published": "2025-02-05 00:21:01", "link": "http://arxiv.org/abs/2502.02787v1", "categories": ["cs.CL", "cs.CR", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What is in a name? Mitigating Name Bias in Text Embeddings via\n  Anonymization", "abstract": "Text-embedding models often exhibit biases arising from the data on which\nthey are trained. In this paper, we examine a hitherto unexplored bias in\ntext-embeddings: bias arising from the presence of $\\textit{names}$ such as\npersons, locations, organizations etc. in the text. Our study shows how the\npresence of $\\textit{name-bias}$ in text-embedding models can potentially lead\nto erroneous conclusions in assessment of thematic similarity.Text-embeddings\ncan mistakenly indicate similarity between texts based on names in the text,\neven when their actual semantic content has no similarity or indicate\ndissimilarity simply because of the names in the text even when the texts match\nsemantically. We first demonstrate the presence of name bias in different\ntext-embedding models and then propose $\\textit{text-anonymization}$ during\ninference which involves removing references to names, while preserving the\ncore theme of the text. The efficacy of the anonymization approach is\ndemonstrated on two downstream NLP tasks, achieving significant performance\ngains. Our simple and training-optimization-free approach offers a practical\nand easily implementable solution to mitigate name bias.", "published": "2025-02-05 05:54:49", "link": "http://arxiv.org/abs/2502.02903v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ScholaWrite: A Dataset of End-to-End Scholarly Writing Process", "abstract": "Writing is a cognitively demanding task involving continuous decision-making,\nheavy use of working memory, and frequent switching between multiple\nactivities. Scholarly writing is particularly complex as it requires authors to\ncoordinate many pieces of multiform knowledge. To fully understand writers'\ncognitive thought process, one should fully decode the end-to-end writing data\n(from individual ideas to final manuscript) and understand their complex\ncognitive mechanisms in scholarly writing. We introduce ScholaWrite dataset, a\nfirst-of-its-kind keystroke corpus of an end-to-end scholarly writing process\nfor complete manuscripts, with thorough annotations of cognitive writing\nintentions behind each keystroke. Our dataset includes LaTeX-based keystroke\ndata from five preprints with nearly 62K total text changes and annotations\nacross 4 months of paper writing. ScholaWrite shows promising usability and\napplications (e.g., iterative self-writing), demonstrating the importance of\ncollection of end-to-end writing data, rather than the final manuscript, for\nthe development of future writing assistants to support the cognitive thinking\nprocess of scientists. Our de-identified data examples and code are available\non our project page.", "published": "2025-02-05 05:57:37", "link": "http://arxiv.org/abs/2502.02904v3", "categories": ["cs.HC", "cs.CL", "q-bio.NC"], "primary_category": "cs.HC"}
{"title": "SPARC: Subspace-Aware Prompt Adaptation for Robust Continual Learning in\n  LLMs", "abstract": "We propose SPARC, a lightweight continual learning framework for large\nlanguage models (LLMs) that enables efficient task adaptation through prompt\ntuning in a lower-dimensional space. By leveraging principal component analysis\n(PCA), we identify a compact subspace of the training data. Optimizing prompts\nin this lower-dimensional space enhances training efficiency, as it focuses\nupdates on the most relevant features while reducing computational overhead.\nFurthermore, since the model's internal structure remains unaltered, the\nextensive knowledge gained from pretraining is fully preserved, ensuring that\npreviously learned information is not compromised during adaptation. Our method\nachieves high knowledge retention in both task-incremental and\ndomain-incremental continual learning setups while fine-tuning only 0.04% of\nthe model's parameters. Additionally, by integrating LoRA, we enhance\nadaptability to computational constraints, allowing for a tradeoff between\naccuracy and training cost. Experiments on the SuperGLUE benchmark demonstrate\nthat our PCA-based prompt tuning combined with LoRA maintains full knowledge\nretention while improving accuracy, utilizing only 1% of the model's\nparameters. These results establish our approach as a scalable and\nresource-efficient solution for continual learning in LLMs.", "published": "2025-02-05 06:11:55", "link": "http://arxiv.org/abs/2502.02909v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Training an LLM-as-a-Judge Model: Pipeline, Insights, and Practical\n  Lessons", "abstract": "The rapid advancement of large language models (LLMs) has opened new\npossibilities for their adoption as evaluative judges. This paper introduces\nThemis, a fine-tuned LLM judge that delivers sophisticated context-aware\nevaluations. We provide a comprehensive overview of the development pipeline\nfor Themis, highlighting its scenario-dependent evaluation prompts and two\nnovel methods for controlled instruction generation. These designs enable\nThemis to effectively distill evaluative skills from teacher models, while\nretaining flexibility for continuous development. We introduce two\nhuman-labeled benchmarks for meta-evaluation, demonstrating that Themis can\nachieve high alignment with human preferences in an economical manner.\nAdditionally, we explore insights into the LLM-as-a-judge paradigm, revealing\nnuances in performance and the varied effects of reference answers. Notably, we\nobserve that pure knowledge distillation from strong LLMs, though common, does\nnot guarantee performance improvement through scaling. We propose a mitigation\nstrategy based on instruction-following difficulty. Furthermore, we provide\npractical guidelines covering data balancing, prompt customization,\nmulti-objective training, and metric aggregation. We aim for our method and\nfindings, along with the fine-tuning data, benchmarks, and model checkpoints,\nto support future research and development in this area.", "published": "2025-02-05 08:35:55", "link": "http://arxiv.org/abs/2502.02988v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Policies and Evaluation for Online Meeting Summarization", "abstract": "With more and more meetings moving to a digital domain, meeting summarization\nhas recently gained interest in both academic and commercial research. However,\nprior academic research focuses on meeting summarization as an offline task,\nperformed after the meeting concludes. In this paper, we perform the first\nsystematic study of online meeting summarization. For this purpose, we propose\nseveral policies for conducting online summarization. We discuss the unique\nchallenges of this task compared to the offline setting and define novel\nmetrics to evaluate latency and partial summary quality. The experiments on the\nAutoMin dataset show that 1) online models can produce strong summaries, 2) our\nmetrics allow a detailed analysis of different systems' quality-latency\ntrade-off, also taking into account intermediate outputs and 3) adaptive\npolicies perform better than fixed scheduled ones. These findings provide a\nstarting point for the wider research community to explore this important task.", "published": "2025-02-05 12:15:00", "link": "http://arxiv.org/abs/2502.03111v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "iVISPAR -- An Interactive Visual-Spatial Reasoning Benchmark for VLMs", "abstract": "Vision-Language Models (VLMs) are known to struggle with spatial reasoning\nand visual alignment. To help overcome these limitations, we introduce iVISPAR,\nan interactive multi-modal benchmark designed to evaluate the spatial reasoning\ncapabilities of VLMs acting as agents. iVISPAR is based on a variant of the\nsliding tile puzzle-a classic problem that demands logical planning, spatial\nawareness, and multi-step reasoning. The benchmark supports visual 2D, 3D, and\ntext-based input modalities, enabling comprehensive assessments of VLMs'\nplanning and reasoning skills. We evaluate a broad suite of state-of-the-art\nopen-source and closed-source VLMs, comparing their performance while also\nproviding optimal path solutions and a human baseline to assess the task's\ncomplexity and feasibility for humans. Results indicate that while some VLMs\nperform well on simple spatial tasks, they encounter difficulties with more\ncomplex configurations and problem properties. Notably, while VLMs generally\nperform better in 2D vision compared to 3D or text-based representations, they\nconsistently fall short of human performance, illustrating the persistent\nchallenge of visual alignment. This highlights critical gaps in current VLM\ncapabilities, highlighting their limitations in achieving human-level\ncognition.", "published": "2025-02-05 14:29:01", "link": "http://arxiv.org/abs/2502.03214v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Token Assorted: Mixing Latent and Text Tokens for Improved Language\n  Model Reasoning", "abstract": "Large Language Models (LLMs) excel at reasoning and planning when trained on\nchainof-thought (CoT) data, where the step-by-step thought process is\nexplicitly outlined by text tokens. However, this results in lengthy inputs\nwhere many words support textual coherence rather than core reasoning\ninformation, and processing these inputs consumes substantial computation\nresources. In this work, we propose a hybrid representation of the reasoning\nprocess, where we partially abstract away the initial reasoning steps using\nlatent discrete tokens generated by VQ-VAE, significantly reducing the length\nof reasoning traces. We explore the use of latent trace abstractions in two\nscenarios: 1) training the model from scratch for the Keys-Finding Maze\nproblem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary\nincluding unseen latent tokens, for both logical and mathematical reasoning\nproblems. To facilitate effective learning, we introduce a simple training\nprocedure that randomly mixes latent and text tokens, which enables fast\nadaptation to new latent tokens. Our approach consistently outperforms the\nbaselines methods in various benchmarks.", "published": "2025-02-05 15:33:00", "link": "http://arxiv.org/abs/2502.03275v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO"], "primary_category": "cs.CL"}
{"title": "SymAgent: A Neural-Symbolic Self-Learning Agent Framework for Complex\n  Reasoning over Knowledge Graphs", "abstract": "Recent advancements have highlighted that Large Language Models (LLMs) are\nprone to hallucinations when solving complex reasoning problems, leading to\nerroneous results. To tackle this issue, researchers incorporate Knowledge\nGraphs (KGs) to improve the reasoning ability of LLMs. However, existing\nmethods face two limitations: 1) they typically assume that all answers to the\nquestions are contained in KGs, neglecting the incompleteness issue of KGs, and\n2) they treat the KG as a static repository and overlook the implicit logical\nreasoning structures inherent in KGs. In this paper, we introduce SymAgent, an\ninnovative neural-symbolic agent framework that achieves collaborative\naugmentation between KGs and LLMs. We conceptualize KGs as dynamic environments\nand transform complex reasoning tasks into a multi-step interactive process,\nenabling KGs to participate deeply in the reasoning process. SymAgent consists\nof two modules: Agent-Planner and Agent-Executor. The Agent-Planner leverages\nLLM's inductive reasoning capability to extract symbolic rules from KGs,\nguiding efficient question decomposition. The Agent-Executor autonomously\ninvokes predefined action tools to integrate information from KGs and external\ndocuments, addressing the issues of KG incompleteness. Furthermore, we design a\nself-learning framework comprising online exploration and offline iterative\npolicy updating phases, enabling the agent to automatically synthesize\nreasoning trajectories and improve performance. Experimental results\ndemonstrate that SymAgent with weak LLM backbones (i.e., 7B series) yields\nbetter or comparable performance compared to various strong baselines. Further\nanalysis reveals that our agent can identify missing triples, facilitating\nautomatic KG updates.", "published": "2025-02-05 15:37:05", "link": "http://arxiv.org/abs/2502.03283v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "ALPET: Active Few-shot Learning for Citation Worthiness Detection in\n  Low-Resource Wikipedia Languages", "abstract": "Citation Worthiness Detection (CWD) consists in determining which sentences,\nwithin an article or collection, should be backed up with a citation to\nvalidate the information it provides. This study, introduces ALPET, a framework\ncombining Active Learning (AL) and Pattern-Exploiting Training (PET), to\nenhance CWD for languages with limited data resources. Applied to Catalan,\nBasque, and Albanian Wikipedia datasets, ALPET outperforms the existing CCW\nbaseline while reducing the amount of labeled data in some cases above 80\\%.\nALPET's performance plateaus after 300 labeled samples, showing it suitability\nfor low-resource scenarios where large, labeled datasets are not common. While\nspecific active learning query strategies, like those employing K-Means\nclustering, can offer advantages, their effectiveness is not universal and\noften yields marginal gains over random sampling, particularly with smaller\ndatasets. This suggests that random sampling, despite its simplicity, remains a\nstrong baseline for CWD in constraint resource environments. Overall, ALPET's\nability to achieve high performance with fewer labeled samples makes it a\npromising tool for enhancing the verifiability of online content in\nlow-resource language settings.", "published": "2025-02-05 15:49:41", "link": "http://arxiv.org/abs/2502.03292v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MeDiSumQA: Patient-Oriented Question-Answer Generation from Discharge\n  Letters", "abstract": "While increasing patients' access to medical documents improves medical care,\nthis benefit is limited by varying health literacy levels and complex medical\nterminology. Large language models (LLMs) offer solutions by simplifying\nmedical information. However, evaluating LLMs for safe and patient-friendly\ntext generation is difficult due to the lack of standardized evaluation\nresources. To fill this gap, we developed MeDiSumQA. MeDiSumQA is a dataset\ncreated from MIMIC-IV discharge summaries through an automated pipeline\ncombining LLM-based question-answer generation with manual quality checks. We\nuse this dataset to evaluate various LLMs on patient-oriented\nquestion-answering. Our findings reveal that general-purpose LLMs frequently\nsurpass biomedical-adapted models, while automated metrics correlate with human\njudgment. By releasing MeDiSumQA on PhysioNet, we aim to advance the\ndevelopment of LLMs to enhance patient understanding and ultimately improve\ncare outcomes.", "published": "2025-02-05 15:56:37", "link": "http://arxiv.org/abs/2502.03298v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient\n  Zeroth-order LLM Fine-tuning", "abstract": "Large language models (LLMs) excel across various tasks, but standard\nfirst-order (FO) fine-tuning demands considerable memory, significantly\nlimiting real-world deployment. Recently, zeroth-order (ZO) optimization stood\nout as a promising memory-efficient training paradigm, avoiding backward passes\nand relying solely on forward passes for gradient estimation, making it\nattractive for resource-constrained scenarios. However, ZO method lags far\nbehind FO method in both convergence speed and accuracy. To bridge the gap, we\nintroduce a novel layer-wise divergence analysis that uncovers the distinct\nupdate pattern of FO and ZO optimization. Aiming to resemble the learning\ncapacity of FO method from the findings, we propose \\textbf{Di}vergence-driven\n\\textbf{Z}eroth-\\textbf{O}rder (\\textbf{DiZO}) optimization. DiZO conducts\ndivergence-driven layer adaptation by incorporating projections to ZO updates,\ngenerating diverse-magnitude updates precisely scaled to layer-wise individual\noptimization needs. Our results demonstrate that DiZO significantly reduces the\nneeded iterations for convergence without sacrificing throughput, cutting\ntraining GPU hours by up to 48\\% on various datasets. Moreover, DiZO\nconsistently outperforms the representative ZO baselines in fine-tuning\nRoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some\ncases, even surpasses memory-intensive FO fine-tuning.", "published": "2025-02-05 16:03:17", "link": "http://arxiv.org/abs/2502.03304v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Out-of-Distribution Detection using Synthetic Data Generation", "abstract": "Distinguishing in- and out-of-distribution (OOD) inputs is crucial for\nreliable deployment of classification systems. However, OOD data is typically\nunavailable or difficult to collect, posing a significant challenge for\naccurate OOD detection. In this work, we present a method that harnesses the\ngenerative capabilities of Large Language Models (LLMs) to create high-quality\nsynthetic OOD proxies, eliminating the dependency on any external OOD data\nsource. We study the efficacy of our method on classical text classification\ntasks such as toxicity detection and sentiment classification as well as\nclassification tasks arising in LLM development and deployment, such as\ntraining a reward model for RLHF and detecting misaligned generations.\nExtensive experiments on nine InD-OOD dataset pairs and various model sizes\nshow that our approach dramatically lowers false positive rates (achieving a\nperfect zero in some cases) while maintaining high accuracy on in-distribution\ntasks, outperforming baseline methods by a significant margin.", "published": "2025-02-05 16:22:09", "link": "http://arxiv.org/abs/2502.03323v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "High-Fidelity Simultaneous Speech-To-Speech Translation", "abstract": "We introduce Hibiki, a decoder-only model for simultaneous speech\ntranslation. Hibiki leverages a multistream language model to synchronously\nprocess source and target speech, and jointly produces text and audio tokens to\nperform speech-to-text and speech-to-speech translation. We furthermore address\nthe fundamental challenge of simultaneous interpretation, which unlike its\nconsecutive counterpart, where one waits for the end of the source utterance to\nstart translating, adapts its flow to accumulate just enough context to produce\na correct translation in real-time, chunk by chunk. To do so, we introduce a\nweakly-supervised method that leverages the perplexity of an off-the-shelf text\ntranslation system to identify optimal delays on a per-word basis and create\naligned synthetic data. After supervised training, Hibiki performs adaptive,\nsimultaneous speech translation with vanilla temperature sampling. On a\nFrench-English simultaneous speech translation task, Hibiki demonstrates\nstate-of-the-art performance in translation quality, speaker fidelity and\nnaturalness. Moreover, the simplicity of its inference process makes it\ncompatible with batched translation and even real-time on-device deployment. We\nprovide examples as well as models and inference code.", "published": "2025-02-05 17:18:55", "link": "http://arxiv.org/abs/2502.03382v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language\n  Model Training", "abstract": "Small language models (SLMs) have attracted considerable attention from both\nacademia and industry due to their broad range of applications in edge devices.\nTo obtain SLMs with strong performance, conventional approaches either\npre-train the models from scratch, which incurs substantial computational\ncosts, or compress/prune existing large language models (LLMs), which results\nin performance drops and falls short in comparison to pre-training. In this\npaper, we investigate the family of acceleration methods that involve both\nstructured pruning and model training. We found 1) layer-wise adaptive pruning\n(Adapt-Pruner) is extremely effective in LLMs and yields significant\nimprovements over existing pruning techniques, 2) adaptive pruning equipped\nwith further training leads to models comparable to those pre-training from\nscratch, 3) incremental pruning brings non-trivial performance gain by\ninterleaving pruning with training and only removing a small portion of neurons\n($\\sim$5%) at a time. Experimental results on LLaMA-3.1-8B demonstrate that\nAdapt-Pruner outperforms conventional pruning methods, such as LLM-Pruner,\nFLAP, and SliceGPT, by an average of 1%-7% in accuracy on commonsense\nbenchmarks. Additionally, Adapt-Pruner restores the performance of\nMobileLLM-125M to 600M on the MMLU benchmark with 200$\\times$ fewer tokens via\npruning from its larger counterparts, and discovers a new 1B model that\nsurpasses LLaMA-3.2-1B in multiple benchmarks.", "published": "2025-02-05 18:57:40", "link": "http://arxiv.org/abs/2502.03460v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Teaching Language Models to Critique via Reinforcement Learning", "abstract": "Teaching large language models (LLMs) to critique and refine their outputs is\ncrucial for building systems that can iteratively improve, yet it is\nfundamentally limited by the ability to provide accurate judgments and\nactionable suggestions. In this work, we study LLM critics for code generation\nand propose $\\texttt{CTRL}$, a framework for $\\texttt{C}$ritic\n$\\texttt{T}$raining via $\\texttt{R}$einforcement $\\texttt{L}$earning, which\ntrains a critic model to generate feedback that maximizes correction\nperformance for a fixed generator model without human supervision. Our results\ndemonstrate that critics trained with $\\texttt{CTRL}$ significantly enhance\npass rates and mitigate compounding errors across both base and stronger\ngenerator models. Furthermore, we show that these critic models act as accurate\ngenerative reward models and enable test-time scaling through iterative\ncritique-revision, achieving up to 106.1% relative improvements across\nchallenging code generation benchmarks.", "published": "2025-02-05 02:18:46", "link": "http://arxiv.org/abs/2502.03492v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "An Empirical Exploration of ChatGPT's Ability to Support Problem\n  Formulation Tasks for Mission Engineering and a Documentation of its\n  Performance Variability", "abstract": "Systems engineering (SE) is evolving with the availability of generative\nartificial intelligence (AI) and the demand for a systems-of-systems\nperspective, formalized under the purview of mission engineering (ME) in the US\nDepartment of Defense. Formulating ME problems is challenging because they are\nopen-ended exercises that involve translation of ill-defined problems into\nwell-defined ones that are amenable for engineering development. It remains to\nbe seen to which extent AI could assist problem formulation objectives. To that\nend, this paper explores the quality and consistency of multi-purpose Large\nLanguage Models (LLM) in supporting ME problem formulation tasks, specifically\nfocusing on stakeholder identification. We identify a relevant reference\nproblem, a NASA space mission design challenge, and document ChatGPT-3.5's\nability to perform stakeholder identification tasks. We execute multiple\nparallel attempts and qualitatively evaluate LLM outputs, focusing on both\ntheir quality and variability. Our findings portray a nuanced picture. We find\nthat the LLM performs well in identifying human-focused stakeholders but poorly\nin recognizing external systems and environmental factors, despite explicit\nefforts to account for these. Additionally, LLMs struggle with preserving the\ndesired level of abstraction and exhibit a tendency to produce solution\nspecific outputs that are inappropriate for problem formulation. More\nimportantly, we document great variability among parallel threads, highlighting\nthat LLM outputs should be used with caution, ideally by adopting a stochastic\nview of their abilities. Overall, our findings suggest that, while ChatGPT\ncould reduce some expert workload, its lack of consistency and domain\nunderstanding may limit its reliability for problem formulation tasks.", "published": "2025-02-05 17:58:23", "link": "http://arxiv.org/abs/2502.03511v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "REALEDIT: Reddit Edits As a Large-scale Empirical Dataset for Image\n  Transformations", "abstract": "Existing image editing models struggle to meet real-world demands. Despite\nexcelling in academic benchmarks, they have yet to be widely adopted for real\nuser needs. Datasets that power these models use artificial edits, lacking the\nscale and ecological validity necessary to address the true diversity of user\nrequests. We introduce REALEDIT, a large-scale image editing dataset with\nauthentic user requests and human-made edits sourced from Reddit. REALEDIT\nincludes a test set of 9300 examples to evaluate models on real user requests.\nOur results show that existing models fall short on these tasks, highlighting\nthe need for realistic training data. To address this, we introduce 48K\ntraining examples and train our REALEDIT model, achieving substantial gains -\noutperforming competitors by up to 165 Elo points in human judgment and 92\npercent relative improvement on the automated VIEScore metric. We deploy our\nmodel on Reddit, testing it on new requests, and receive positive feedback.\nBeyond image editing, we explore REALEDIT's potential in detecting edited\nimages by partnering with a deepfake detection non-profit. Finetuning their\nmodel on REALEDIT data improves its F1-score by 14 percentage points,\nunderscoring the dataset's value for broad applications.", "published": "2025-02-05 21:35:48", "link": "http://arxiv.org/abs/2502.03629v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Reflection-Window Decoding: Text Generation with Selective Refinement", "abstract": "The autoregressive decoding for text generation in large language models\n(LLMs), while widely used, is inherently suboptimal due to the lack of a\nbuilt-in mechanism to perform refinement and/or correction of the generated\ncontent. In this paper, we consider optimality in terms of the joint\nprobability over the generated response, when jointly considering all tokens at\nthe same time. We theoretically characterize the potential deviation of the\nautoregressively generated response from its globally optimal counterpart that\nis of the same length. Our analysis suggests that we need to be cautious when\nnoticeable uncertainty arises during text generation, which may signal the\nsub-optimality of the generation history. To address the pitfall of\nautoregressive decoding for text generation, we propose an approach that\nincorporates a sliding reflection window and a pausing criterion, such that\nrefinement and generation can be carried out interchangeably as the decoding\nproceeds. Our selective refinement framework strikes a balance between\nefficiency and optimality, and our extensive experimental results demonstrate\nthe effectiveness of our approach.", "published": "2025-02-05 23:53:08", "link": "http://arxiv.org/abs/2502.03678v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MARAGE: Transferable Multi-Model Adversarial Attack for\n  Retrieval-Augmented Generation Data Extraction", "abstract": "Retrieval-Augmented Generation (RAG) offers a solution to mitigate\nhallucinations in Large Language Models (LLMs) by grounding their outputs to\nknowledge retrieved from external sources. The use of private resources and\ndata in constructing these external data stores can expose them to risks of\nextraction attacks, in which attackers attempt to steal data from these private\ndatabases. Existing RAG extraction attacks often rely on manually crafted\nprompts, which limit their effectiveness. In this paper, we introduce a\nframework called MARAGE for optimizing an adversarial string that, when\nappended to user queries submitted to a target RAG system, causes outputs\ncontaining the retrieved RAG data verbatim. MARAGE leverages a continuous\noptimization scheme that integrates gradients from multiple models with\ndifferent architectures simultaneously to enhance the transferability of the\noptimized string to unseen models. Additionally, we propose a strategy that\nemphasizes the initial tokens in the target RAG data, further improving the\nattack's generalizability. Evaluations show that MARAGE consistently\noutperforms both manual and optimization-based baselines across multiple LLMs\nand RAG datasets, while maintaining robust transferability to previously unseen\nmodels. Moreover, we conduct probing tasks to shed light on the reasons why\nMARAGE is more effective compared to the baselines and to analyze the impact of\nour approach on the model's internal state.", "published": "2025-02-05 00:17:01", "link": "http://arxiv.org/abs/2502.04360v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Contrastive Token-level Explanations for Graph-based Rumour Detection", "abstract": "The widespread use of social media has accelerated the dissemination of\ninformation, but it has also facilitated the spread of harmful rumours, which\ncan disrupt economies, influence political outcomes, and exacerbate public\nhealth crises, such as the COVID-19 pandemic. While Graph Neural Network\n(GNN)-based approaches have shown significant promise in automated rumour\ndetection, they often lack transparency, making their predictions difficult to\ninterpret. Existing graph explainability techniques fall short in addressing\nthe unique challenges posed by the dependencies among feature dimensions in\nhigh-dimensional text embeddings used in GNN-based models. In this paper, we\nintroduce Contrastive Token Layerwise Relevance Propagation (CT-LRP), a novel\nframework designed to enhance the explainability of GNN-based rumour detection.\nCT-LRP extends current graph explainability methods by providing token-level\nexplanations that offer greater granularity and interpretability. We evaluate\nthe effectiveness of CT-LRP across multiple GNN models trained on three\npublicly available rumour detection datasets, demonstrating that it\nconsistently produces high-fidelity, meaningful explanations, paving the way\nfor more robust and trustworthy rumour detection systems.", "published": "2025-02-05 07:14:11", "link": "http://arxiv.org/abs/2502.04366v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DreamDPO: Aligning Text-to-3D Generation with Human Preferences via\n  Direct Preference Optimization", "abstract": "Text-to-3D generation automates 3D content creation from textual\ndescriptions, which offers transformative potential across various fields.\nHowever, existing methods often struggle to align generated content with human\npreferences, limiting their applicability and flexibility. To address these\nlimitations, in this paper, we propose DreamDPO, an optimization-based\nframework that integrates human preferences into the 3D generation process,\nthrough direct preference optimization. Practically, DreamDPO first constructs\npairwise examples, then compare their alignment with human preferences using\nreward or large multimodal models, and lastly optimizes the 3D representation\nwith a preference-driven loss function. By leveraging pairwise comparison to\nreflect preferences, DreamDPO reduces reliance on precise pointwise quality\nevaluations while enabling fine-grained controllability through\npreference-guided optimization. Experiments demonstrate that DreamDPO achieves\ncompetitive results, and provides higher-quality and more controllable 3D\ncontent compared to existing methods. The code and models will be open-sourced.", "published": "2025-02-05 11:03:08", "link": "http://arxiv.org/abs/2502.04370v1", "categories": ["cs.CL", "cs.GR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PerPO: Perceptual Preference Optimization via Discriminative Rewarding", "abstract": "This paper presents Perceptual Preference Optimization (PerPO), a perception\nalignment method aimed at addressing the visual discrimination challenges in\ngenerative pre-trained multimodal large language models (MLLMs). To align MLLMs\nwith human visual perception process, PerPO employs discriminative rewarding to\ngather diverse negative samples, followed by listwise preference optimization\nto rank them.By utilizing the reward as a quantitative margin for ranking, our\nmethod effectively bridges generative preference optimization and\ndiscriminative empirical risk minimization. PerPO significantly enhances MLLMs'\nvisual discrimination capabilities while maintaining their generative\nstrengths, mitigates image-unconditional reward hacking, and ensures consistent\nperformance across visual tasks. This work marks a crucial step towards more\nperceptually aligned and versatile MLLMs. We also hope that PerPO will\nencourage the community to rethink MLLM alignment strategies.", "published": "2025-02-05 11:28:11", "link": "http://arxiv.org/abs/2502.04371v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Mining Unstructured Medical Texts With Conformal Active Learning", "abstract": "The extraction of relevant data from Electronic Health Records (EHRs) is\ncrucial to identifying symptoms and automating epidemiological surveillance\nprocesses. By harnessing the vast amount of unstructured text in EHRs, we can\ndetect patterns that indicate the onset of disease outbreaks, enabling faster,\nmore targeted public health responses. Our proposed framework provides a\nflexible and efficient solution for mining data from unstructured texts,\nsignificantly reducing the need for extensive manual labeling by specialists.\nExperiments show that our framework achieving strong performance with as few as\n200 manually labeled texts, even for complex classification problems.\nAdditionally, our approach can function with simple lightweight models,\nachieving competitive and occasionally even better results compared to more\nresource-intensive deep learning models. This capability not only accelerates\nprocessing times but also preserves patient privacy, as the data can be\nprocessed on weaker on-site hardware rather than being transferred to external\nsystems. Our methodology, therefore, offers a practical, scalable, and\nprivacy-conscious approach to real-time epidemiological monitoring, equipping\nhealth institutions to respond rapidly and effectively to emerging health\nthreats.", "published": "2025-02-05 12:59:15", "link": "http://arxiv.org/abs/2502.04372v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Capture Video Game Engagement?", "abstract": "Can out-of-the-box pretrained Large Language Models (LLMs) detect human\naffect successfully when observing a video? To address this question, for the\nfirst time, we evaluate comprehensively the capacity of popular LLMs to\nannotate and successfully predict continuous affect annotations of videos when\nprompted by a sequence of text and video frames in a multimodal fashion.\nParticularly in this paper, we test LLMs' ability to correctly label changes of\nin-game engagement in 80 minutes of annotated videogame footage from 20\nfirst-person shooter games of the GameVibe corpus. We run over 2,400\nexperiments to investigate the impact of LLM architecture, model size, input\nmodality, prompting strategy, and ground truth processing method on engagement\nprediction. Our findings suggest that while LLMs rightfully claim human-like\nperformance across multiple domains, they generally fall behind capturing\ncontinuous experience annotations provided by humans. We examine some of the\nunderlying causes for the relatively poor overall performance, highlight the\ncases where LLMs exceed expectations, and draw a roadmap for the further\nexploration of automated emotion labelling via LLMs.", "published": "2025-02-05 17:14:47", "link": "http://arxiv.org/abs/2502.04379v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.CV"}
{"title": "Diversity as a Reward: Fine-Tuning LLMs on a Mixture of\n  Domain-Undetermined Data", "abstract": "Fine-tuning large language models (LLMs) using diverse datasets is crucial\nfor enhancing their overall performance across various domains. In practical\nscenarios, existing methods based on modeling the mixture proportions of data\ncomposition often struggle with data whose domain labels are missing, imprecise\nor non-normalized, while methods based on data selection usually encounter\ndifficulties in balancing multi-domain performance. To address these\nchallenges, in this paper, we study the role of data diversity in enhancing the\noverall abilities of LLMs by empirically constructing contrastive data pools\nand theoretically deriving explanations for both inter- and intra-diversity.\nBuilding upon the insights gained, we propose a new method that gives the LLM a\ndual identity: an output model to cognitively probe and select data based on\ndiversity reward, as well as an input model to be tuned with the selected data.\nExtensive experiments show that the proposed method notably boosts performance\nacross domain-undetermined data and a series of foundational downstream tasks\nwhen applied to various advanced LLMs. We release our code and hope this study\ncan shed light on the understanding of data diversity and advance\nfeedback-driven data-model co-development for LLMs.", "published": "2025-02-05 17:21:01", "link": "http://arxiv.org/abs/2502.04380v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sparse Autoencoders for Hypothesis Generation", "abstract": "We describe HypotheSAEs, a general method to hypothesize interpretable\nrelationships between text data (e.g., headlines) and a target variable (e.g.,\nclicks). HypotheSAEs has three steps: (1) train a sparse autoencoder on text\nembeddings to produce interpretable features describing the data distribution,\n(2) select features that predict the target variable, and (3) generate a\nnatural language interpretation of each feature (e.g., \"mentions being\nsurprised or shocked\") using an LLM. Each interpretation serves as a hypothesis\nabout what predicts the target variable. Compared to baselines, our method\nbetter identifies reference hypotheses on synthetic datasets (at least +0.06 in\nF1) and produces more predictive hypotheses on real datasets (~twice as many\nsignificant findings), despite requiring 1-2 orders of magnitude less compute\nthan recent LLM-based methods. HypotheSAEs also produces novel discoveries on\ntwo well-studied tasks: explaining partisan differences in Congressional\nspeeches and identifying drivers of engagement with online headlines.", "published": "2025-02-05 18:58:02", "link": "http://arxiv.org/abs/2502.04382v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "In Praise of Stubbornness: The Case for Cognitive-Dissonance-Aware\n  Knowledge Updates in LLMs", "abstract": "Despite remarkable capabilities, large language models (LLMs) struggle to\ncontinually update their knowledge without catastrophic forgetting. In\ncontrast, humans effortlessly integrate new information, detect conflicts with\nexisting beliefs, and selectively update their mental models. This paper\nintroduces a cognitive-inspired investigation paradigm to study continual\nknowledge updating in LLMs. We implement two key components inspired by human\ncognition: (1) Dissonance and Familiarity Awareness, analyzing model behavior\nto classify information as novel, familiar, or dissonant; and (2) Targeted\nNetwork Updates, which track neural activity to identify frequently used\n(stubborn) and rarely used (plastic) neurons. Through carefully designed\nexperiments in controlled settings, we uncover a number of empirical findings\ndemonstrating the potential of this approach. First, dissonance detection is\nfeasible using simple activation and gradient features, suggesting potential\nfor cognitive-inspired training. Second, we find that non-dissonant updates\nlargely preserve prior knowledge regardless of targeting strategy, revealing\ninherent robustness in LLM knowledge integration. Most critically, we discover\nthat dissonant updates prove catastrophically destructive to the model's\nknowledge base, indiscriminately affecting even information unrelated to the\ncurrent updates. This suggests fundamental limitations in how neural networks\nhandle contradictions and motivates the need for new approaches to knowledge\nupdating that better mirror human cognitive mechanisms.", "published": "2025-02-05 23:49:33", "link": "http://arxiv.org/abs/2502.04390v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "KDA: A Knowledge-Distilled Attacker for Generating Diverse Prompts to\n  Jailbreak LLMs", "abstract": "Jailbreak attacks exploit specific prompts to bypass LLM safeguards, causing\nthe LLM to generate harmful, inappropriate, and misaligned content. Current\njailbreaking methods rely heavily on carefully designed system prompts and\nnumerous queries to achieve a single successful attack, which is costly and\nimpractical for large-scale red-teaming. To address this challenge, we propose\nto distill the knowledge of an ensemble of SOTA attackers into a single\nopen-source model, called Knowledge-Distilled Attacker (KDA), which is\nfinetuned to automatically generate coherent and diverse attack prompts without\nthe need for meticulous system prompt engineering. Compared to existing\nattackers, KDA achieves higher attack success rates and greater cost-time\nefficiency when targeting multiple SOTA open-source and commercial black-box\nLLMs. Furthermore, we conducted a quantitative diversity analysis of prompts\ngenerated by baseline methods and KDA, identifying diverse and ensemble attacks\nas key factors behind KDA's effectiveness and efficiency.", "published": "2025-02-05 21:50:34", "link": "http://arxiv.org/abs/2502.05223v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "DiffListener: Discrete Diffusion Model for Listener Generation", "abstract": "The listener head generation (LHG) task aims to generate natural nonverbal\nlistener responses based on the speaker's multimodal cues. While prior work\neither rely on limited modalities (e.g. audio and facial information) or employ\nautoregressive approaches which have limitations such as accumulating\nprediction errors. To address these limitations, we propose DiffListener, a\ndiscrete diffusion based approach for non-autoregressive listener head\ngeneration. Our model takes the speaker's facial information, audio, and text\nas inputs, additionally incorporating facial differential information to\nrepresent the temporal dynamics of expressions and movements. With this\nexplicit modeling of facial dynamics, DiffListener can generate coherent\nreaction sequences in a non-autoregressive manner. Through comprehensive\nexperiments, DiffListener demonstrates state-of-the-art performance in both\nquantitative and qualitative evaluations. The user study shows that\nDiffListener generates natural context-aware listener reactions that are well\nsynchronized with the speaker. The code and demo videos are available in\nhttps://siyeoljung.github.io/DiffListener", "published": "2025-02-05 07:57:15", "link": "http://arxiv.org/abs/2502.06822v1", "categories": ["cs.LG", "cs.CL", "cs.GR"], "primary_category": "cs.LG"}
{"title": "Entropy Adaptive Decoding: Dynamic Model Switching for Efficient\n  Inference", "abstract": "We present Entropy Adaptive Decoding (EAD), a novel approach for efficient\nlanguage model inference that dynamically switches between different-sized\nmodels based on prediction uncertainty. By monitoring rolling entropy in model\nlogit distributions, our method identifies text regions where a smaller model\nsuffices and switches to a larger model only when prediction uncertainty\nexceeds a threshold. Unlike speculative decoding approaches that maintain\nperfect output fidelity through verification, EAD accepts controlled output\ndivergence in exchange for computational efficiency. Our experiments on the\nMATH benchmark demonstrate remarkable efficiency gains across different model\nfamilies. Using the LLaMA family, we maintain 96.7\\% of the 11B model's\nperformance (50.4\\% vs 52.1\\%) while using it for only 43\\% of tokens,\ndecreasing computational cost by 41.5\\%. These gains become more pronounced\nwith larger size differentials in the Qwen family, where we achieve 92.9\\% of\nthe 14B model's performance (74.3\\% vs 80.0\\%) while using it for just 25\\% of\ntokens, decreasing computational cost by 67\\%. The consistency of these results\nacross model pairs suggests that language model computation can be\nsignificantly optimized by selectively deploying model capacity based on local\ngeneration complexity. Our findings indicate that current approaches to model\ninference may be unnecessarily conservative in their pursuit of perfect output\nfidelity, and that accepting minor performance trade-offs can enable dramatic\nreductions in computational costs.", "published": "2025-02-05 22:15:21", "link": "http://arxiv.org/abs/2502.06833v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multimodal Transformer Models for Turn-taking Prediction: Effects on\n  Conversational Dynamics of Human-Agent Interaction during Cooperative\n  Gameplay", "abstract": "This study investigates multimodal turn-taking prediction within human-agent\ninteractions (HAI), particularly focusing on cooperative gaming environments.\nIt comprises both model development and subsequent user study, aiming to refine\nour understanding and improve conversational dynamics in spoken dialogue\nsystems (SDSs). For the modeling phase, we introduce a novel transformer-based\ndeep learning (DL) model that simultaneously integrates multiple modalities -\ntext, vision, audio, and contextual in-game data to predict turn-taking events\nin real-time. Our model employs a Crossmodal Transformer architecture to\neffectively fuse information from these diverse modalities, enabling more\ncomprehensive turn-taking predictions. The model demonstrates superior\nperformance compared to baseline models, achieving 87.3% accuracy and 83.0%\nmacro F1 score. A human user study was then conducted to empirically evaluate\nthe turn-taking DL model in an interactive scenario with a virtual avatar while\nplaying the game \"Dont Starve Together\", comparing a control condition without\nturn-taking prediction (n=20) to an experimental condition with our model\ndeployed (n=40). Both conditions included a mix of English and Korean speakers,\nsince turn-taking cues are known to vary by culture. We then analyzed the\ninteraction quality, examining aspects such as utterance counts, interruption\nfrequency, and participant perceptions of the avatar. Results from the user\nstudy suggest that our multimodal turn-taking model not only enhances the\nfluidity and naturalness of human-agent conversations, but also maintains a\nbalanced conversational dynamic without significantly altering dialogue\nfrequency. The study provides in-depth insights into the influence of\nturn-taking abilities on user perceptions and interaction quality, underscoring\nthe potential for more contextually adaptive and responsive conversational\nagents.", "published": "2025-02-05 23:00:49", "link": "http://arxiv.org/abs/2503.16432v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Enhancing Reasoning to Adapt Large Language Models for Domain-Specific\n  Applications", "abstract": "This paper presents SOLOMON, a novel Neuro-inspired Large Language Model\n(LLM) Reasoning Network architecture that enhances the adaptability of\nfoundation models for domain-specific applications. Through a case study in\nsemiconductor layout design, we demonstrate how SOLOMON enables swift\nadaptation of general-purpose LLMs to specialized tasks by leveraging Prompt\nEngineering and In-Context Learning techniques. Our experiments reveal the\nchallenges LLMs face in spatial reasoning and applying domain knowledge to\npractical problems. Results show that SOLOMON instances significantly\noutperform their baseline LLM counterparts and achieve performance comparable\nto state-of-the-art reasoning model, o1-preview. We discuss future research\ndirections for developing more adaptive AI systems that can continually learn,\nadapt, and evolve in response to new information and changing requirements.", "published": "2025-02-05 19:27:24", "link": "http://arxiv.org/abs/2502.04384v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SY", "eess.SY", "68T09, 68T35, 68T45, 94C30", "I.2.7; I.2.11; B.7.2"], "primary_category": "cs.CL"}
{"title": "Cache-Craft: Managing Chunk-Caches for Efficient Retrieval-Augmented\n  Generation", "abstract": "Retrieval-Augmented Generation (RAG) is often used with Large Language Models\n(LLMs) to infuse domain knowledge or user-specific information. In RAG, given a\nuser query, a retriever extracts chunks of relevant text from a knowledge base.\nThese chunks are sent to an LLM as part of the input prompt. Typically, any\ngiven chunk is repeatedly retrieved across user questions. However, currently,\nfor every question, attention-layers in LLMs fully compute the key values (KVs)\nrepeatedly for the input chunks, as state-of-the-art methods cannot reuse\nKV-caches when chunks appear at arbitrary locations with arbitrary contexts.\nNaive reuse leads to output quality degradation. This leads to potentially\nredundant computations on expensive GPUs and increases latency. In this work,\nwe propose Cache-Craft, a system for managing and reusing precomputed KVs\ncorresponding to the text chunks (we call chunk-caches) in RAG-based systems.\nWe present how to identify chunk-caches that are reusable, how to efficiently\nperform a small fraction of recomputation to fix the cache to maintain output\nquality, and how to efficiently store and evict chunk-caches in the hardware\nfor maximizing reuse while masking any overheads. With real production\nworkloads as well as synthetic datasets, we show that Cache-Craft reduces\nredundant computation by 51% over SOTA prefix-caching and 75% over full\nrecomputation. Additionally, with continuous batching on a real production\nworkload, we get a 1.6X speed up in throughput and a 2X reduction in end-to-end\nresponse latency over prefix-caching while maintaining quality, for both the\nLLaMA-3-8B and LLaMA-3-70B models.", "published": "2025-02-05 14:12:33", "link": "http://arxiv.org/abs/2502.15734v1", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG", "cs.OS"], "primary_category": "cs.DC"}
{"title": "GenSE: Generative Speech Enhancement via Language Models using\n  Hierarchical Modeling", "abstract": "Semantic information refers to the meaning conveyed through words, phrases,\nand contextual relationships within a given linguistic structure. Humans can\nleverage semantic information, such as familiar linguistic patterns and\ncontextual cues, to reconstruct incomplete or masked speech signals in noisy\nenvironments. However, existing speech enhancement (SE) approaches often\noverlook the rich semantic information embedded in speech, which is crucial for\nimproving intelligibility, speaker consistency, and overall quality of enhanced\nspeech signals. To enrich the SE model with semantic information, we employ\nlanguage models as an efficient semantic learner and propose a comprehensive\nframework tailored for language model-based speech enhancement, called\n\\textit{GenSE}. Specifically, we approach SE as a conditional language modeling\ntask rather than a continuous signal regression problem defined in existing\nworks. This is achieved by tokenizing speech signals into semantic tokens using\na pre-trained self-supervised model and into acoustic tokens using a\ncustom-designed single-quantizer neural codec model. To improve the stability\nof language model predictions, we propose a hierarchical modeling method that\ndecouples the generation of clean semantic tokens and clean acoustic tokens\ninto two distinct stages. Moreover, we introduce a token chain prompting\nmechanism during the acoustic token generation stage to ensure timbre\nconsistency throughout the speech enhancement process. Experimental results on\nbenchmark datasets demonstrate that our proposed approach outperforms\nstate-of-the-art SE systems in terms of speech quality and generalization\ncapability.", "published": "2025-02-05 07:14:39", "link": "http://arxiv.org/abs/2502.02942v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fine-grained Preference Optimization Improves Zero-shot Text-to-Speech", "abstract": "Integrating human feedback to align text-to-speech (TTS) system outputs with\nhuman preferences has proven to be an effective approach for enhancing the\nrobustness of language model-based TTS systems. Current approaches primarily\nfocus on using preference data annotated at the utterance level. However,\nfrequent issues that affect the listening experience often only arise in\nspecific segments of audio samples, while other segments are well-generated. In\nthis study, we propose a fine-grained preference optimization approach (FPO) to\nenhance the robustness of TTS systems. FPO focuses on addressing localized\nissues in generated samples rather than uniformly optimizing the entire\nutterance. Specifically, we first analyze the types of issues in generated\nsamples, categorize them into two groups, and propose a selective training loss\nstrategy to optimize preferences based on fine-grained labels for each issue\ntype. Experimental results show that FPO enhances the robustness of zero-shot\nTTS systems by effectively addressing local issues, significantly reducing the\nbad case ratio, and improving intelligibility. Furthermore, FPO exhibits\nsuperior data efficiency compared with baseline systems, achieving similar\nperformance with fewer training samples.", "published": "2025-02-05 07:27:00", "link": "http://arxiv.org/abs/2502.02950v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Leveraging Broadcast Media Subtitle Transcripts for Automatic Speech\n  Recognition and Subtitling", "abstract": "The recent advancement of speech recognition technology has been driven by\nlarge-scale datasets and attention-based architectures, but many challenges\nstill remain, especially for low-resource languages and dialects. This paper\nexplores the integration of weakly supervised transcripts from TV subtitles\ninto automatic speech recognition (ASR) systems, aiming to improve both\nverbatim transcriptions and automatically generated subtitles. To this end,\nverbatim data and subtitles are regarded as different domains or languages, due\nto their distinct characteristics. We propose and compare several end-to-end\narchitectures that are designed to jointly model both modalities with separate\nor shared encoders and decoders. The proposed methods are able to jointly\ngenerate a verbatim transcription and a subtitle. Evaluation on Flemish\n(Belgian Dutch) demonstrates that a model with cascaded encoders and separate\ndecoders allows to represent the differences between the two data types most\nefficiently while improving on both domains. Despite differences in domain and\nlinguistic variations, combining verbatim transcripts with subtitle data leads\nto notable ASR improvements without the need for extensive preprocessing.\nAdditionally, experiments with a large-scale subtitle dataset show the\nscalability of the proposed approach. The methods not only improve ASR accuracy\nbut also generate subtitles that closely match standard written text, offering\nseveral potential applications.", "published": "2025-02-05 14:26:58", "link": "http://arxiv.org/abs/2502.03212v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Should Audio Front-ends be Adaptive? Comparing Learnable and Adaptive\n  Front-ends", "abstract": "Hand-crafted features, such as Mel-filterbanks, have traditionally been the\nchoice for many audio processing applications. Recently, there has been a\ngrowing interest in learnable front-ends that extract representations directly\nfrom the raw audio waveform. \\textcolor{black}{However, both hand-crafted\nfilterbanks and current learnable front-ends lead to fixed computation graphs\nat inference time, failing to dynamically adapt to varying acoustic\nenvironments, a key feature of human auditory systems.} To this end, we explore\nthe question of whether audio front-ends should be adaptive by comparing the\nAda-FE front-end (a recently developed adaptive front-end that employs a neural\nadaptive feedback controller to dynamically adjust the Q-factors of its\nspectral decomposition filters) to established learnable front-ends.\nSpecifically, we systematically investigate learnable front-ends and Ada-FE\nacross two commonly used back-end backbones and a wide range of audio\nbenchmarks including speech, sound event, and music. The comprehensive results\nshow that our Ada-FE outperforms advanced learnable front-ends, and more\nimportantly, it exhibits impressive stability or robustness on test samples\nover various training epochs.", "published": "2025-02-05 15:16:52", "link": "http://arxiv.org/abs/2502.03260v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Comprehensive Layer-wise Analysis of SSL Models for Audio Deepfake\n  Detection", "abstract": "This paper conducts a comprehensive layer-wise analysis of self-supervised\nlearning (SSL) models for audio deepfake detection across diverse contexts,\nincluding multilingual datasets (English, Chinese, Spanish), partial, song, and\nscene-based deepfake scenarios. By systematically evaluating the contributions\nof different transformer layers, we uncover critical insights into model\nbehavior and performance. Our findings reveal that lower layers consistently\nprovide the most discriminative features, while higher layers capture less\nrelevant information. Notably, all models achieve competitive equal error rate\n(EER) scores even when employing a reduced number of layers. This indicates\nthat we can reduce computational costs and increase the inference speed of\ndetecting deepfakes by utilizing only a few lower layers. This work enhances\nour understanding of SSL models in deepfake detection, offering valuable\ninsights applicable across varied linguistic and contextual settings. Our\ntrained models and code are publicly available:\nhttps://github.com/Yaselley/SSL_Layerwise_Deepfake.", "published": "2025-02-05 19:17:24", "link": "http://arxiv.org/abs/2502.03559v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AudioMiXR: Spatial Audio Object Manipulation with 6DoF for Sound Design\n  in Augmented Reality", "abstract": "We present AudioMiXR, an augmented reality (AR) interface intended to assess\nhow users manipulate virtual audio objects situated in their physical space\nusing six degrees of freedom (6DoF) deployed on a head-mounted display (Apple\nVision Pro) for 3D sound design. Existing tools for 3D sound design are\ntypically constrained to desktop displays, which may limit spatial awareness of\nmixing within the execution environment. Utilizing an XR HMD to create\nsoundscapes may provide a real-time test environment for 3D sound design, as\nmodern HMDs can provide precise spatial localization assisted by cross-modal\ninteractions. However, there is no research on design guidelines specific to\nsound design with six degrees of freedom (6DoF) in XR. To provide a first step\ntoward identifying design-related research directions in this space, we\nconducted an exploratory study where we recruited 27 participants, consisting\nof expert and non-expert sound designers. The goal was to assess design lessons\nthat can be used to inform future research venues in 3D sound design. We ran a\nwithin-subjects study where users designed both a music and cinematic\nsoundscapes. After thematically analyzing participant data, we constructed two\ndesign lessons: 1. Proprioception for AR Sound Design, and 2. Balancing\nAudio-Visual Modalities in AR GUIs. Additionally, we provide application\ndomains that can benefit most from 6DoF sound design based on our results.", "published": "2025-02-05 06:46:40", "link": "http://arxiv.org/abs/2502.02929v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Metis: A Foundation Speech Generation Model with Masked Generative\n  Pre-training", "abstract": "We introduce Metis, a foundation model for unified speech generation. Unlike\nprevious task-specific or multi-task models, Metis follows a pre-training and\nfine-tuning paradigm. It is pre-trained on large-scale unlabeled speech data\nusing masked generative modeling and then fine-tuned to adapt to diverse speech\ngeneration tasks. Specifically, 1) Metis utilizes two discrete speech\nrepresentations: SSL tokens derived from speech self-supervised learning (SSL)\nfeatures, and acoustic tokens directly quantized from waveforms. 2) Metis\nperforms masked generative pre-training on SSL tokens, utilizing 300K hours of\ndiverse speech data, without any additional condition. 3) Through fine-tuning\nwith task-specific conditions, Metis achieves efficient adaptation to various\nspeech generation tasks while supporting multimodal input, even when using\nlimited data and trainable parameters. Experiments demonstrate that Metis can\nserve as a foundation model for unified speech generation: Metis outperforms\nstate-of-the-art task-specific or multi-task systems across five speech\ngeneration tasks, including zero-shot text-to-speech, voice conversion, target\nspeaker extraction, speech enhancement, and lip-to-speech, even with fewer than\n20M trainable parameters or 300 times less training data. Audio samples are are\navailable at https://metis-demo.github.io/.", "published": "2025-02-05 12:36:21", "link": "http://arxiv.org/abs/2502.03128v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
