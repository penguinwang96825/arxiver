{"title": "Determining Semantic Textual Similarity using Natural Deduction Proofs", "abstract": "Determining semantic textual similarity is a core research subject in natural\nlanguage processing. Since vector-based models for sentence representation\noften use shallow information, capturing accurate semantics is difficult. By\ncontrast, logical semantic representations capture deeper levels of sentence\nsemantics, but their symbolic nature does not offer graded notions of textual\nsimilarity. We propose a method for determining semantic textual similarity by\ncombining shallow features with features extracted from natural deduction\nproofs of bidirectional entailment relations between sentence pairs. For the\nnatural deduction proofs, we use ccg2lambda, a higher-order automatic inference\nsystem, which converts Combinatory Categorial Grammar (CCG) derivation trees\ninto semantic representations and conducts natural deduction proofs.\nExperiments show that our system was able to outperform other logic-based\nsystems and that features derived from the proofs are effective for learning\ntextual similarity.", "published": "2017-07-27 05:49:51", "link": "http://arxiv.org/abs/1707.08713v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective Inference for Generative Neural Parsing", "abstract": "Generative neural models have recently achieved state-of-the-art results for\nconstituency parsing. However, without a feasible search procedure, their use\nhas so far been limited to reranking the output of external parsers in which\ndecoding is more tractable. We describe an alternative to the conventional\naction-level beam search used for discriminative neural models that enables us\nto decode directly in these generative models. We then show that by improving\nour basic candidate selection strategy and using a coarse pruning function, we\ncan improve accuracy while exploring significantly less of the search space.\nApplied to the model of Choe and Charniak (2016), our inference procedure\nobtains 92.56 F1 on section 23 of the Penn Treebank, surpassing prior\nstate-of-the-art results for single-model systems.", "published": "2017-07-27 18:01:18", "link": "http://arxiv.org/abs/1707.08976v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting Sequence Models for Sentence Correction", "abstract": "In a controlled experiment of sequence-to-sequence approaches for the task of\nsentence correction, we find that character-based models are generally more\neffective than word-based models and models that encode subword information via\nconvolutions, and that modeling the output data as a series of diffs improves\neffectiveness over standard approaches. Our strongest sequence-to-sequence\nmodel improves over our strongest phrase-based statistical machine translation\nmodel, with access to the same data, by 6 M2 (0.5 GLEU) points. Additionally,\nin the data environment of the standard CoNLL-2014 setup, we demonstrate that\nmodeling (and tuning against) diffs yields similar or better M2 scores with\nsimpler models and/or significantly less data than previous\nsequence-to-sequence approaches.", "published": "2017-07-27 22:50:55", "link": "http://arxiv.org/abs/1707.09067v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysis of Italian Word Embeddings", "abstract": "In this work we analyze the performances of two of the most used word\nembeddings algorithms, skip-gram and continuous bag of words on Italian\nlanguage. These algorithms have many hyper-parameter that have to be carefully\ntuned in order to obtain accurate word representation in vectorial space. We\nprovide an accurate analysis and an evaluation, showing what are the best\nconfiguration of parameters for specific tasks.", "published": "2017-07-27 08:56:29", "link": "http://arxiv.org/abs/1707.08783v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deep Residual Learning for Weakly-Supervised Relation Extraction", "abstract": "Deep residual learning (ResNet) is a new method for training very deep neural\nnetworks using identity map-ping for shortcut connections. ResNet has won the\nImageNet ILSVRC 2015 classification task, and achieved state-of-the-art\nperformances in many computer vision tasks. However, the effect of residual\nlearning on noisy natural language processing tasks is still not well\nunderstood. In this paper, we design a novel convolutional neural network (CNN)\nwith residual learning, and investigate its impacts on the task of distantly\nsupervised noisy relation extraction. In contradictory to popular beliefs that\nResNet only works well for very deep networks, we found that even with 9 layers\nof CNNs, using identity mapping could significantly improve the performance for\ndistantly-supervised relation extraction.", "published": "2017-07-27 13:56:36", "link": "http://arxiv.org/abs/1707.08866v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Shared Task on Bandit Learning for Machine Translation", "abstract": "We introduce and describe the results of a novel shared task on bandit\nlearning for machine translation. The task was organized jointly by Amazon and\nHeidelberg University for the first time at the Second Conference on Machine\nTranslation (WMT 2017). The goal of the task is to encourage research on\nlearning machine translation from weak user feedback instead of human\nreferences or post-edits. On each of a sequence of rounds, a machine\ntranslation system is required to propose a translation for an input, and\nreceives a real-valued estimate of the quality of the proposed translation for\nlearning. This paper describes the shared task's learning and evaluation setup,\nusing services hosted on Amazon Web Services (AWS), the data and evaluation\nmetrics, and the results of various machine translation architectures and\nlearning protocols.", "published": "2017-07-27 21:16:41", "link": "http://arxiv.org/abs/1707.09050v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Detecting and Explaining Causes From Text For a Time Series Event", "abstract": "Explaining underlying causes or effects about events is a challenging but\nvaluable task. We define a novel problem of generating explanations of a time\nseries event by (1) searching cause and effect relationships of the time series\nwith textual data and (2) constructing a connecting chain between them to\ngenerate an explanation. To detect causal features from text, we propose a\nnovel method based on the Granger causality of time series between features\nextracted from text such as N-grams, topics, sentiments, and their composition.\nThe generation of the sequence of causal entities requires a commonsense\ncausative knowledge base with efficient reasoning. To ensure good\ninterpretability and appropriate lexical usage we combine symbolic and neural\nrepresentations, using a neural reasoning algorithm trained on commonsense\ncausal tuples to predict the next cause step. Our quantitative and human\nanalysis show empirical evidence that our method successfully extracts\nmeaningful causality relationships between time series with textual features\nand generates appropriate explanation between them.", "published": "2017-07-27 13:14:57", "link": "http://arxiv.org/abs/1707.08852v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
