{"title": "FutureTOD: Teaching Future Knowledge to Pre-trained Language Model for\n  Task-Oriented Dialogue", "abstract": "Pre-trained language models based on general text enable huge success in the\nNLP scenario. But the intrinsical difference of linguistic patterns between\ngeneral text and task-oriented dialogues makes existing pre-trained language\nmodels less useful in practice. Current dialogue pre-training methods rely on a\ncontrastive framework and face the challenges of both selecting true positives\nand hard negatives. In this paper, we propose a novel dialogue pre-training\nmodel, FutureTOD, which distills future knowledge to the representation of the\nprevious dialogue context using a self-training framework. Our intuition is\nthat a good dialogue representation both learns local context information and\npredicts future information. Extensive experiments on diverse downstream\ndialogue tasks demonstrate the effectiveness of our model, especially the\ngeneralization, robustness, and learning discriminative dialogue\nrepresentations capabilities.", "published": "2023-06-17 10:40:07", "link": "http://arxiv.org/abs/2306.10315v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Seen to Unseen: Exploring Compositional Generalization of\n  Multi-Attribute Controllable Dialogue Generation", "abstract": "Existing controllable dialogue generation work focuses on the\nsingle-attribute control and lacks generalization capability to\nout-of-distribution multiple attribute combinations. In this paper, we explore\nthe compositional generalization for multi-attribute controllable dialogue\ngeneration where a model can learn from seen attribute values and generalize to\nunseen combinations. We propose a prompt-based disentangled controllable\ndialogue generation model, DCG. It learns attribute concept composition by\ngenerating attribute-oriented prompt vectors and uses a disentanglement loss to\ndisentangle different attributes for better generalization. Besides, we design\na unified reference-free evaluation framework for multiple attributes with\ndifferent levels of granularities. Experiment results on two benchmarks prove\nthe effectiveness of our method and the evaluation metric.", "published": "2023-06-17 10:50:19", "link": "http://arxiv.org/abs/2306.10317v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Persian Semantic Role Labeling Using Transfer Learning and BERT-Based\n  Models", "abstract": "Semantic role labeling (SRL) is the process of detecting the\npredicate-argument structure of each predicate in a sentence. SRL plays a\ncrucial role as a pre-processing step in many NLP applications such as topic\nand concept extraction, question answering, summarization, machine translation,\nsentiment analysis, and text mining. Recently, in many languages, unified SRL\ndragged lots of attention due to its outstanding performance, which is the\nresult of overcoming the error propagation problem. However, regarding the\nPersian language, all previous works have focused on traditional methods of SRL\nleading to a drop in accuracy and imposing expensive feature extraction steps\nin terms of financial resources, time and energy consumption. In this work, we\npresent an end-to-end SRL method that not only eliminates the need for feature\nextraction but also outperforms existing methods in facing new samples in\npractical situations. The proposed method does not employ any auxiliary\nfeatures and shows more than 16 (83.16) percent improvement in accuracy against\nprevious methods in similar circumstances.", "published": "2023-06-17 12:50:09", "link": "http://arxiv.org/abs/2306.10339v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KEST: Kernel Distance Based Efficient Self-Training for Improving\n  Controllable Text Generation", "abstract": "Self-training (ST) has come to fruition in language understanding tasks by\nproducing pseudo labels, which reduces the labeling bottleneck of language\nmodel fine-tuning. Nevertheless, in facilitating semi-supervised controllable\nlanguage generation, ST faces two key challenges. First, augmented by\nself-generated pseudo text, generation models tend to over-exploit the\npreviously learned text distribution, suffering from mode collapse and poor\ngeneration diversity. Second, generating pseudo text in each iteration is\ntime-consuming, severely decelerating the training process. In this work, we\npropose KEST, a novel and efficient self-training framework to handle these\nproblems. KEST utilizes a kernel-based loss, rather than standard cross\nentropy, to learn from the soft pseudo text produced by a shared\nnon-autoregressive generator. We demonstrate both theoretically and empirically\nthat KEST can benefit from more diverse pseudo text in an efficient manner,\nwhich allows not only refining and exploiting the previously fitted\ndistribution but also enhanced exploration towards a larger potential text\nspace, providing a guarantee of improved performance. Experiments on three\ncontrollable generation tasks demonstrate that KEST significantly improves\ncontrol accuracy while maintaining comparable text fluency and generation\ndiversity against several strong baselines.", "published": "2023-06-17 19:40:57", "link": "http://arxiv.org/abs/2306.10414v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Multiword Expression Identification Using Lateral\n  Inhibition and Domain Adaptation", "abstract": "Correctly identifying multiword expressions (MWEs) is an important task for\nmost natural language processing systems since their misidentification can\nresult in ambiguity and misunderstanding of the underlying text. In this work,\nwe evaluate the performance of the mBERT model for MWE identification in a\nmultilingual context by training it on all 14 languages available in version\n1.2 of the PARSEME corpus. We also incorporate lateral inhibition and language\nadversarial training into our methodology to create language-independent\nembeddings and improve its capabilities in identifying multiword expressions.\nThe evaluation of our models shows that the approach employed in this work\nachieves better results compared to the best system of the PARSEME 1.2\ncompetition, MTLB-STRUCT, on 11 out of 14 languages for global MWE\nidentification and on 12 out of 14 languages for unseen MWE identification.\nAdditionally, averaged across all languages, our best approach outperforms the\nMTLB-STRUCT system by 1.23% on global MWE identification and by 4.73% on unseen\nglobal MWE identification.", "published": "2023-06-17 20:28:32", "link": "http://arxiv.org/abs/2306.10419v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Snowman: A Million-scale Chinese Commonsense Knowledge Graph Distilled\n  from Foundation Model", "abstract": "Constructing commonsense knowledge graphs (CKGs) has attracted wide research\nattention due to its significant importance in cognitive intelligence.\nNevertheless, existing CKGs are typically oriented to English, limiting the\nresearch in non-English languages. Meanwhile, the emergence of foundation\nmodels like ChatGPT and GPT-4 has shown promising intelligence with the help of\nreinforcement learning from human feedback. Under the background, in this\npaper, we utilize foundation models to construct a Chinese CKG, named Snowman.\nSpecifically, we distill different types of commonsense head items from\nChatGPT, and continue to use it to collect tail items with respect to the head\nitems and pre-defined relations. Based on the preliminary analysis, we find the\nnegative commonsense knowledge distilled by ChatGPT achieves lower human\nacceptance compared to other knowledge. Therefore, we design a simple yet\neffective self-instruct filtering strategy to filter out invalid negative\ncommonsense. Overall, the constructed Snowman covers more than ten million\nChinese commonsense triples, making it the largest Chinese CKG. Moreover, human\nstudies show the acceptance of Snowman achieves 90.6\\%, indicating the\nhigh-quality triples distilled by the cutting-edge foundation model. We also\nconduct experiments on commonsense knowledge models to show the usability and\neffectiveness of our Snowman.", "published": "2023-06-17 02:51:33", "link": "http://arxiv.org/abs/2306.10241v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Generative AI Models for Telecom: The Next Big Thing?", "abstract": "The evolution of generative artificial intelligence (GenAI) constitutes a\nturning point in reshaping the future of technology in different aspects.\nWireless networks in particular, with the blooming of self-evolving networks,\nrepresent a rich field for exploiting GenAI and reaping several benefits that\ncan fundamentally change the way how wireless networks are designed and\noperated nowadays. To be specific, large GenAI models are envisioned to open up\na new era of autonomous wireless networks, in which multi-modal GenAI models\ntrained over various Telecom data, can be fine-tuned to perform several\ndownstream tasks, eliminating the need for building and training dedicated AI\nmodels for each specific task and paving the way for the realization of\nartificial general intelligence (AGI)-empowered wireless networks. In this\narticle, we aim to unfold the opportunities that can be reaped from integrating\nlarge GenAI models into the Telecom domain. In particular, we first highlight\nthe applications of large GenAI models in future wireless networks, defining\npotential use-cases and revealing insights on the associated theoretical and\npractical challenges. Furthermore, we unveil how 6G can open up new\nopportunities through connecting multiple on-device large GenAI models, and\nhence, paves the way to the collective intelligence paradigm. Finally, we put a\nforward-looking vision on how large GenAI models will be the key to realize\nself-evolving networks.", "published": "2023-06-17 03:45:00", "link": "http://arxiv.org/abs/2306.10249v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Typo-Robust Representation Learning for Dense Retrieval", "abstract": "Dense retrieval is a basic building block of information retrieval\napplications. One of the main challenges of dense retrieval in real-world\nsettings is the handling of queries containing misspelled words. A popular\napproach for handling misspelled queries is minimizing the representations\ndiscrepancy between misspelled queries and their pristine ones. Unlike the\nexisting approaches, which only focus on the alignment between misspelled and\npristine queries, our method also improves the contrast between each misspelled\nquery and its surrounding queries. To assess the effectiveness of our proposed\nmethod, we compare it against the existing competitors using two benchmark\ndatasets and two base encoders. Our method outperforms the competitors in all\ncases with misspelled queries. Our code and models are available at\nhttps://github. com/panuthept/DST-DenseRetrieval.", "published": "2023-06-17 13:48:30", "link": "http://arxiv.org/abs/2306.10348v1", "categories": ["cs.IR", "cs.CL", "I.2.7"], "primary_category": "cs.IR"}
{"title": "LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event\n  Boundary Captioning", "abstract": "Our winning entry for the CVPR 2023 Generic Event Boundary Captioning (GEBC)\ncompetition is detailed in this paper. Unlike conventional video captioning\ntasks, GEBC demands that the captioning model possess an understanding of\nimmediate changes in status around the designated video boundary, making it a\ndifficult task. This paper proposes an effective model LLMVA-GEBC (Large\nLanguage Model with Video Adapter for Generic Event Boundary Captioning): (1)\nWe utilize a pretrained LLM for generating human-like captions with high\nquality. (2) To adapt the model to the GEBC task, we take the video Q-former as\nan adapter and train it with the frozen visual feature extractors and LLM. Our\nproposed method achieved a 76.14 score on the test set and won the first place\nin the challenge. Our code is available at\nhttps://github.com/zjr2000/LLMVA-GEBC .", "published": "2023-06-17 13:55:54", "link": "http://arxiv.org/abs/2306.10354v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Empowering NLG: Offline Reinforcement Learning for Informal\n  Summarization in Online Domains", "abstract": "Our research introduces an innovative Natural Language Generation (NLG)\napproach that aims to optimize user experience and alleviate the workload of\nhuman customer support agents. Our primary objective is to generate informal\nsummaries for online articles and posts using an offline reinforcement learning\ntechnique. In our study, we compare our proposed method with existing\napproaches to text generation and provide a comprehensive overview of our\narchitectural design, which incorporates crawling, reinforcement learning, and\ntext generation modules. By presenting this original approach, our paper makes\na valuable contribution to the field of NLG by offering a fresh perspective on\ngenerating natural language summaries for online content. Through the\nimplementation of Empowering NLG, we are able to generate higher-quality\nreplies in the online domain. The experimental results demonstrate a\nsignificant improvement in the average \"like\" score, increasing from 0.09954378\nto 0.5000152. This advancement has the potential to enhance the efficiency and\neffectiveness of customer support services and elevate the overall user\nexperience when consuming online content.", "published": "2023-06-17 13:00:54", "link": "http://arxiv.org/abs/2306.17174v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RECAP-KG: Mining Knowledge Graphs from Raw GP Notes for Remote COVID-19\n  Assessment in Primary Care", "abstract": "Clinical decision-making is a fundamental stage in delivering appropriate\ncare to patients. In recent years several decision-making systems designed to\naid the clinician in this process have been developed. However, technical\nsolutions currently in use are based on simple regression models and are only\nable to take into account simple pre-defined multiple-choice features, such as\npatient age, pre-existing conditions, smoker status, etc. One particular source\nof patient data, that available decision-making systems are incapable of\nprocessing is the collection of patient consultation GP notes. These contain\ncrucial signs and symptoms - the information used by clinicians in order to\nmake a final decision and direct the patient to the appropriate care.\nExtracting information from GP notes is a technically challenging problem, as\nthey tend to include abbreviations, typos, and incomplete sentences.\n  This paper addresses this open challenge. We present a framework that\nperforms knowledge graph construction from raw GP medical notes written during\nor after patient consultations. By relying on support phrases mined from the\nSNOMED ontology, as well as predefined supported facts from values used in the\nRECAP (REmote COVID-19 Assessment in Primary Care) patient risk prediction\ntool, our graph generative framework is able to extract structured knowledge\ngraphs from the highly unstructured and inconsistent format that consultation\nnotes are written in. Our knowledge graphs include information about existing\npatient symptoms, their duration, and their severity.\n  We apply our framework to consultation notes of COVID-19 patients in the UK\nCOVID-19 Clinical Assesment Servcie (CCAS) patient dataset. We provide a\nquantitative evaluation of the performance of our framework, demonstrating that\nour approach has better accuracy than traditional NLP methods when answering\nquestions about patients.", "published": "2023-06-17 23:35:51", "link": "http://arxiv.org/abs/2306.17175v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GLIMMER: generalized late-interaction memory reranker", "abstract": "Memory-augmentation is a powerful approach for efficiently incorporating\nexternal information into language models, but leads to reduced performance\nrelative to retrieving text. Recent work introduced LUMEN, a memory-retrieval\nhybrid that partially pre-computes memory and updates memory representations on\nthe fly with a smaller live encoder.\n  We propose GLIMMER, which improves on this approach through 1) exploiting\nfree access to the powerful memory representations by applying a shallow\nreranker on top of memory to drastically improve retrieval quality at low cost,\nand 2) incorporating multi-task training to learn a general and higher quality\nmemory and live encoder. GLIMMER achieves strong gains in performance at faster\nspeeds compared to LUMEN and FiD on the KILT benchmark of knowledge-intensive\ntasks.", "published": "2023-06-17 01:54:25", "link": "http://arxiv.org/abs/2306.10231v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Old and New Minimalism: a Hopf algebra comparison", "abstract": "In this paper we compare some old formulations of Minimalism, in particular\nStabler's computational minimalism, and Chomsky's new formulation of Merge and\nMinimalism, from the point of view of their mathematical description in terms\nof Hopf algebras. We show that the newer formulation has a clear advantage\npurely in terms of the underlying mathematical structure. More precisely, in\nthe case of Stabler's computational minimalism, External Merge can be described\nin terms of a partially defined operated algebra with binary operation, while\nInternal Merge determines a system of right-ideal coideals of the Loday-Ronco\nHopf algebra and corresponding right-module coalgebra quotients. This\nmathematical structure shows that Internal and External Merge have\nsignificantly different roles in the old formulations of Minimalism, and they\nare more difficult to reconcile as facets of a single algebraic operation, as\ndesirable linguistically. On the other hand, we show that the newer formulation\nof Minimalism naturally carries a Hopf algebra structure where Internal and\nExternal Merge directly arise from the same operation. We also compare, at the\nlevel of algebraic properties, the externalization model of the new Minimalism\nwith proposals for assignments of planar embeddings based on heads of trees.", "published": "2023-06-17 06:31:36", "link": "http://arxiv.org/abs/2306.10270v1", "categories": ["cs.CL", "math.QA", "math.RA", "68Q70, 16T05"], "primary_category": "cs.CL"}
{"title": "CorNav: Autonomous Agent with Self-Corrected Planning for Zero-Shot\n  Vision-and-Language Navigation", "abstract": "Understanding and following natural language instructions while navigating\nthrough complex, real-world environments poses a significant challenge for\ngeneral-purpose robots. These environments often include obstacles and\npedestrians, making it essential for autonomous agents to possess the\ncapability of self-corrected planning to adjust their actions based on feedback\nfrom the surroundings. However, the majority of existing vision-and-language\nnavigation (VLN) methods primarily operate in less realistic simulator settings\nand do not incorporate environmental feedback into their decision-making\nprocesses. To address this gap, we introduce a novel zero-shot framework called\nCorNav, utilizing a large language model for decision-making and comprising two\nkey components: 1) incorporating environmental feedback for refining future\nplans and adjusting its actions, and 2) multiple domain experts for parsing\ninstructions, scene understanding, and refining predicted actions. In addition\nto the framework, we develop a 3D simulator that renders realistic scenarios\nusing Unreal Engine 5. To evaluate the effectiveness and generalization of\nnavigation agents in a zero-shot multi-task setting, we create a benchmark\ncalled NavBench. Extensive experiments demonstrate that CorNav consistently\noutperforms all baselines by a significant margin across all tasks. On average,\nCorNav achieves a success rate of 28.1\\%, surpassing the best baseline's\nperformance of 20.5\\%.", "published": "2023-06-17 11:44:04", "link": "http://arxiv.org/abs/2306.10322v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Reliability and repeatability of ISO 3382-3 metrics based on repeated\n  acoustic measurements in open-plan offices", "abstract": "This paper investigates variability in the key ISO 3382-3:2012 metrics, based\nprimarily on the repeatability and reliability of these metrics, using repeated\nmeasurements in open-plan offices. Two types of repeated measurements were\nperformed in offices, Type1 (n=36), where the same path over workstations was\nmeasured from opposite ends, and Type2 (n=7), where two different measurement\npaths were measured. Overall, most of the Type1 results seem reasonable\nconsidering repeats were conducted in complicated room acoustic environments,\nwhile Type2 repeats would benefit from larger sample sizes in future studies.\nSome recommendations are outlined for the ISO 3382-3 methodology vis-a-vis\nType1 and Type2 repeats, including future research directions that go beyond\nincreased sample sizes. (This is an abridged version of the abstract. Please\nsee the paper for the full abstract)", "published": "2023-06-17 06:10:52", "link": "http://arxiv.org/abs/2306.10268v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Two simultaneous talkers distract more than one in simulated\n  multi-talker environments, regardless of overall sound levels typical of\n  open-plan offices", "abstract": "The irrelevant speech effect (ISE) characterizes detriment to cognitive task\nperformance in the presence of irrelevant speech. This paper examines whether\nthe ISE varies due to the number of simultaneously active nearby talkers (for\nup to two talkers), or the overall sound level, within the context of a\nsimulated open-plan office. Two experiments were conducted within a\nclimate-controlled chamber that was set-up as a medium-sized open-plan office.\nThe cognitive tasks performed by the participants included the digit recall\ntask, and a writing task, within a room acoustic simulation of realistic\nmulti-talker speech from spatially separated talkers. Within Experiment 1\n(n=60), an increase in the number of talkers from none (T0) to one (T1), and\nfrom one to two (T2) simultaneous talkers resulted in statistically significant\ndecline in the digit recall task performances, with effect sizes of 24% (i.e.,\nT1 vs. T0), and 12% (i.e., T2 vs. T1), respectively. The pauses between words\nduring the writing task were similar for T0 and T1, but showed a statistically\nsignificant increase within T2 vs. T1, with an effect size of 12%. The findings\nof Experiment 1 are inconsistent with the maximally distracting status\nattributed to T1 in some studies, but is consistent with findings in other\nstudies. Within Experiment 2 (n = 62), the cognitive performance in T2 remained\nlargely invariant between 45 and 57 dB (A-weighted sound pressure levels),\nwhich represents a typical range of levels within open-plan offices. In\ngeneral, these findings have relevance for characterizing auditory distraction\nwithin complex multi-talker environments; both in laboratory studies and actual\nopen-plan offices. (Abridged version; please see the paper for the full\nabstract)", "published": "2023-06-17 06:14:21", "link": "http://arxiv.org/abs/2306.10269v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Autophonic Loudness of Singers in Simulated Room Acoustic Environments", "abstract": "This paper aims to study the effect of room acoustics and phonemes on the\nperception of loudness of one's own voice (autophonic loudness) for a group of\ntrained singers. For a set of five phonemes, 20 singers vocalized over several\nautophonic loudness ratios, while maintaining pitch constancy over extreme\nvoice levels, within five simulated rooms. There were statistically significant\ndifferences in the slope of the autophonic loudness function (logarithm of\nautophonic loudness as a function of voice sound pressure level) for the five\nphonemes, with slopes ranging from 1.3 (/a:/) to 2.0 (/z:/). There was no\nsignificant variation in the autophonic loudness function slopes with\nvariations in room acoustics. The autophonic room response, which represents a\nsystematic decrease in voice levels with increasing levels of room reflections,\nwas also studied, with some evidence found in support. Overall, the average\nslope of the autophonic room response for the three corner vowels (/a:/, /i:/,\nand /u:/) was -1.4 for medium autophonic loudness. The findings relating to the\nslope of the autophonic loudness function are in agreement with the findings of\nprevious studies where the sensorimotor mechanisms in regulating voice were\nshown to be more important in the perception of autophonic loudness than\nhearing of room acoustics. However, the role of room acoustics, in terms of the\nautophonic room response, is shown to be more complicated, requiring further\ninquiry. Overall, it is shown that autophonic loudness grows at more than twice\nthe rate of loudness growth for sounds created outside the human body.", "published": "2023-06-17 06:34:05", "link": "http://arxiv.org/abs/2306.10271v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Musico-acoustic Depictions of Laminar and Turbulent Flows in Ligeti\n  Piano Etude No. 9 and a Novel Method of Analysis", "abstract": "The relationship between musical material and physical phenomena has become a\ntopic in the musicological literature over the last several decades,\nparticularly concerning elements of the musical system itself, and\nconstructions found in the work of contemporary classical composers such as\nGyorgy Ligeti and Iannis Xenakis. Most scholars, who adopt this approach,\nexplore the physical phenomena of fractals in the analysis of musical works,\nbut fluid mechanical frameworks, such as laminar and turbulent flows, offer a\nnew avenue to be explored. In this paper I will propose a novel method of\nmusical analysis for examining musical structures in terms of fluid-like\nbehaviour such that Ligeti etude no. 9 serves as a model, whereby the metaphors\nof laminar and turbulent flows take precedence. The methodological design\nincludes the utility of converting terms (by proposing correlations between\nphysical concepts and the acoustic properties of music), theoretical frameworks\nfor musicological application, and scatter plots, which provide central\nanalytic support to demonstrating the fluid-like tendencies in musical\nmaterials, for they capture a formal development over time.", "published": "2023-06-17 19:06:41", "link": "http://arxiv.org/abs/2306.10093v1", "categories": ["cs.SD", "eess.AS", "physics.flu-dyn"], "primary_category": "cs.SD"}
{"title": "Neural Fast Full-Rank Spatial Covariance Analysis for Blind Source\n  Separation", "abstract": "This paper describes an efficient unsupervised learning method for a neural\nsource separation model that utilizes a probabilistic generative model of\nobserved multichannel mixtures proposed for blind source separation (BSS). For\nthis purpose, amortized variational inference (AVI) has been used for directly\nsolving the inverse problem of BSS with full-rank spatial covariance analysis\n(FCA). Although this unsupervised technique called neural FCA is in principle\nfree from the domain mismatch problem, it is computationally demanding due to\nthe full rankness of the spatial model in exchange for robustness against\nrelatively short reverberations. To reduce the model complexity without\nsacrificing performance, we propose neural FastFCA based on the\njointly-diagonalizable yet full-rank spatial model. Our neural separation model\nintroduced for AVI alternately performs neural network blocks and single steps\nof an efficient iterative algorithm called iterative source steering. This\nalternating architecture enables the separation model to quickly separate the\nmixture spectrogram by leveraging both the deep neural network and the\nmultichannel optimization algorithm. The training objective with AVI is derived\nto maximize the marginalized likelihood of the observed mixtures. The\nexperiment using mixture signals of two to four sound sources shows that neural\nFastFCA outperforms conventional BSS methods and reduces the computational time\nto about 2% of that for the neural FCA.", "published": "2023-06-17 02:50:17", "link": "http://arxiv.org/abs/2306.10240v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Text-Driven Foley Sound Generation With Latent Diffusion Model", "abstract": "Foley sound generation aims to synthesise the background sound for multimedia\ncontent. Previous models usually employ a large development set with labels as\ninput (e.g., single numbers or one-hot vector). In this work, we propose a\ndiffusion model based system for Foley sound generation with text conditions.\nTo alleviate the data scarcity issue, our model is initially pre-trained with\nlarge-scale datasets and fine-tuned to this task via transfer learning using\nthe contrastive language-audio pertaining (CLAP) technique. We have observed\nthat the feature embedding extracted by the text encoder can significantly\naffect the performance of the generation model. Hence, we introduce a trainable\nlayer after the encoder to improve the text embedding produced by the encoder.\nIn addition, we further refine the generated waveform by generating multiple\ncandidate audio clips simultaneously and selecting the best one, which is\ndetermined in terms of the similarity score between the embedding of the\ncandidate clips and the embedding of the target text label. Using the proposed\nmethod, our system ranks ${1}^{st}$ among the systems submitted to DCASE\nChallenge 2023 Task 7. The results of the ablation studies illustrate that the\nproposed techniques significantly improve sound generation performance. The\ncodes for implementing the proposed system are available online.", "published": "2023-06-17 14:16:24", "link": "http://arxiv.org/abs/2306.10359v5", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
