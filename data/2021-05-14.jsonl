{"title": "Adversarial Learning for Zero-Shot Stance Detection on Social Media", "abstract": "Stance detection on social media can help to identify and understand slanted\nnews or commentary in everyday life. In this work, we propose a new model for\nzero-shot stance detection on Twitter that uses adversarial learning to\ngeneralize across topics. Our model achieves state-of-the-art performance on a\nnumber of unseen test topics with minimal computational costs. In addition, we\nextend zero-shot stance detection to new topics, highlighting future directions\nfor zero-shot transfer.", "published": "2021-05-14 01:08:48", "link": "http://arxiv.org/abs/2105.06603v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Multi-Branch Layers for On-Device Neural Machine Translation", "abstract": "With the rapid development of artificial intelligence (AI), there is a trend\nin moving AI applications, such as neural machine translation (NMT), from cloud\nto mobile devices. Constrained by limited hardware resources and battery, the\nperformance of on-device NMT systems is far from satisfactory. Inspired by\nconditional computation, we propose to improve the performance of on-device NMT\nsystems with dynamic multi-branch layers. Specifically, we design a layer-wise\ndynamic multi-branch network with only one branch activated during training and\ninference. As not all branches are activated during training, we propose\nshared-private reparameterization to ensure sufficient training for each\nbranch. At almost the same computational cost, our method achieves improvements\nof up to 1.7 BLEU points on the WMT14 English-German translation task and 1.8\nBLEU points on the WMT20 Chinese-English translation task over the Transformer\nmodel, respectively. Compared with a strong baseline that also uses multiple\nbranches, the proposed method is up to 1.5 times faster with the same number of\nparameters.", "published": "2021-05-14 07:32:53", "link": "http://arxiv.org/abs/2105.06679v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DaLAJ - a dataset for linguistic acceptability judgments for Swedish:\n  Format, baseline, sharing", "abstract": "We present DaLAJ 1.0, a Dataset for Linguistic Acceptability Judgments for\nSwedish, comprising 9 596 sentences in its first version; and the initial\nexperiment using it for the binary classification task. DaLAJ is based on the\nSweLL second language learner data, consisting of essays at different levels of\nproficiency. To make sure the dataset can be freely available despite the GDPR\nregulations, we have sentence-scrambled learner essays and removed part of the\nmetadata about learners, keeping for each sentence only information about the\nmother tongue and the level of the course where the essay has been written. We\nuse the normalized version of learner language as the basis for the DaLAJ\nsentences, and keep only one error per sentence. We repeat the same sentence\nfor each individual correction tag used in the sentence. For DaLAJ 1.0 we have\nused four error categories (out of 35 available in SweLL), all connected to\nlexical or word-building choices. Our baseline results for the binary\nclassification show an accuracy of 58% for DaLAJ 1.0 using BERT embeddings. The\ndataset is included in the SwedishGlue (Swe. SuperLim) benchmark. Below, we\ndescribe the format of the dataset, first experiments, our insights and the\nmotivation for the chosen approach to data sharing.", "published": "2021-05-14 07:37:38", "link": "http://arxiv.org/abs/2105.06681v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Classifying Long Clinical Documents with Pre-trained Transformers", "abstract": "Automatic phenotyping is a task of identifying cohorts of patients that match\na predefined set of criteria. Phenotyping typically involves classifying long\nclinical documents that contain thousands of tokens. At the same time, recent\nstate-of-art transformer-based pre-trained language models limit the input to a\nfew hundred tokens (e.g. 512 tokens for BERT). We evaluate several strategies\nfor incorporating pre-trained sentence encoders into document-level\nrepresentations of clinical text, and find that hierarchical transformers\nwithout pre-training are competitive with task pre-trained models.", "published": "2021-05-14 10:24:58", "link": "http://arxiv.org/abs/2105.06752v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Locate and Label: A Two-stage Identifier for Nested Named Entity\n  Recognition", "abstract": "Named entity recognition (NER) is a well-studied task in natural language\nprocessing. Traditional NER research only deals with flat entities and ignores\nnested entities. The span-based methods treat entity recognition as a span\nclassification task. Although these methods have the innate ability to handle\nnested NER, they suffer from high computational cost, ignorance of boundary\ninformation, under-utilization of the spans that partially match with entities,\nand difficulties in long entity recognition. To tackle these issues, we propose\na two-stage entity identifier. First we generate span proposals by filtering\nand boundary regression on the seed spans to locate the entities, and then\nlabel the boundary-adjusted span proposals with the corresponding categories.\nOur method effectively utilizes the boundary information of entities and\npartially matched spans during training. Through boundary regression, entities\nof any length can be covered theoretically, which improves the ability to\nrecognize long entities. In addition, many low-quality seed spans are filtered\nout in the first stage, which reduces the time complexity of inference.\nExperiments on nested NER datasets demonstrate that our proposed method\noutperforms previous state-of-the-art models.", "published": "2021-05-14 12:52:34", "link": "http://arxiv.org/abs/2105.06804v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empathetic Dialog Generation with Fine-Grained Intents", "abstract": "Empathetic dialog generation aims at generating coherent responses following\nprevious dialog turns and, more importantly, showing a sense of caring and a\ndesire to help. Existing models either rely on pre-defined emotion labels to\nguide the response generation, or use deterministic rules to decide the emotion\nof the response. With the advent of advanced language models, it is possible to\nlearn subtle interactions directly from the dataset, providing that the emotion\ncategories offer sufficient nuances and other non-emotional but emotional\nregulating intents are included. In this paper, we describe how to incorporate\na taxonomy of 32 emotion categories and 8 additional emotion regulating intents\nto succeed the task of empathetic response generation. To facilitate the\ntraining, we also curated a large-scale emotional dialog dataset from movie\nsubtitles. Through a carefully designed crowdsourcing experiment, we evaluated\nand demonstrated how our model produces more empathetic dialogs compared with\nits baselines.", "published": "2021-05-14 13:45:40", "link": "http://arxiv.org/abs/2105.06829v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Navigation by Reasoning over Spatial Configurations", "abstract": "We deal with the navigation problem where the agent follows natural language\ninstructions while observing the environment. Focusing on language\nunderstanding, we show the importance of spatial semantics in grounding\nnavigation instructions into visual perceptions. We propose a neural agent that\nuses the elements of spatial configurations and investigate their influence on\nthe navigation agent's reasoning ability. Moreover, we model the sequential\nexecution order and align visual objects with spatial configurations in the\ninstruction. Our neural agent improves strong baselines on the seen\nenvironments and shows competitive performance on the unseen environments.\nAdditionally, the experimental results demonstrate that explicit modeling of\nspatial semantic elements in the instructions can improve the grounding and\nspatial reasoning of the model.", "published": "2021-05-14 14:04:23", "link": "http://arxiv.org/abs/2105.06839v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Thank you BART! Rewarding Pre-Trained Models Improves Formality Style\n  Transfer", "abstract": "Scarcity of parallel data causes formality style transfer models to have\nscarce success in preserving content. We show that fine-tuning pre-trained\nlanguage (GPT-2) and sequence-to-sequence (BART) models boosts content\npreservation, and that this is possible even with limited amounts of parallel\ndata. Augmenting these models with rewards that target style and content -- the\ntwo core aspects of the task -- we achieve a new state-of-the-art.", "published": "2021-05-14 16:39:22", "link": "http://arxiv.org/abs/2105.06947v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Counterfactual Interventions Reveal the Causal Effect of Relative Clause\n  Representations on Agreement Prediction", "abstract": "When language models process syntactically complex sentences, do they use\ntheir representations of syntax in a manner that is consistent with the grammar\nof the language? We propose AlterRep, an intervention-based method to address\nthis question. For any linguistic feature of a given sentence, AlterRep\ngenerates counterfactual representations by altering how the feature is\nencoded, while leaving intact all other aspects of the original representation.\nBy measuring the change in a model's word prediction behavior when these\ncounterfactual representations are substituted for the original ones, we can\ndraw conclusions about the causal effect of the linguistic feature in question\non the model's behavior. We apply this method to study how BERT models of\ndifferent sizes process relative clauses (RCs). We find that BERT variants use\nRC boundary information during word prediction in a manner that is consistent\nwith the rules of English grammar; this RC boundary information generalizes to\na considerable extent across different RC types, suggesting that BERT\nrepresents RCs as an abstract linguistic category.", "published": "2021-05-14 17:11:55", "link": "http://arxiv.org/abs/2105.06965v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EASE: Extractive-Abstractive Summarization with Explanations", "abstract": "Current abstractive summarization systems outperform their extractive\ncounterparts, but their widespread adoption is inhibited by the inherent lack\nof interpretability. To achieve the best of both worlds, we propose EASE, an\nextractive-abstractive framework for evidence-based text generation and apply\nit to document summarization. We present an explainable summarization system\nbased on the Information Bottleneck principle that is jointly trained for\nextraction and abstraction in an end-to-end fashion. Inspired by previous\nresearch that humans use a two-stage framework to summarize long documents\n(Jing and McKeown, 2000), our framework first extracts a pre-defined amount of\nevidence spans as explanations and then generates a summary using only the\nevidence. Using automatic and human evaluations, we show that explanations from\nour framework are more relevant than simple baselines, without substantially\nsacrificing the quality of the generated summary.", "published": "2021-05-14 17:45:06", "link": "http://arxiv.org/abs/2105.06982v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT Busters: Outlier Dimensions that Disrupt Transformers", "abstract": "Multiple studies have shown that Transformers are remarkably robust to\npruning. Contrary to this received wisdom, we demonstrate that pre-trained\nTransformer encoders are surprisingly fragile to the removal of a very small\nnumber of features in the layer outputs (<0.0001% of model weights). In case of\nBERT and other pre-trained encoder Transformers, the affected component is the\nscaling factors and biases in the LayerNorm. The outliers are high-magnitude\nnormalization parameters that emerge early in pre-training and show up\nconsistently in the same dimensional position throughout the model. We show\nthat disabling them significantly degrades both the MLM loss and the downstream\ntask performance. This effect is observed across several BERT-family models and\nother popular pre-trained Transformer architectures, including BART, XLNet and\nELECTRA; we also show a similar effect in GPT-2.", "published": "2021-05-14 17:54:28", "link": "http://arxiv.org/abs/2105.06990v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RetGen: A Joint framework for Retrieval and Grounded Text Generation\n  Modeling", "abstract": "Recent advances in large-scale pre-training such as GPT-3 allow seemingly\nhigh quality text to be generated from a given prompt. However, such generation\nsystems often suffer from problems of hallucinated facts, and are not\ninherently designed to incorporate useful external information. Grounded\ngeneration models appear to offer remedies, but their training typically relies\non rarely-available parallel data where information-relevant documents are\nprovided for context. We propose a framework that alleviates this data\nconstraint by jointly training a grounded generator and document retriever on\nthe language model signal. The model learns to reward retrieval of the\ndocuments with the highest utility in generation, and attentively combines them\nusing a Mixture-of-Experts (MoE) ensemble to generate follow-on text. We\ndemonstrate that both generator and retriever can take advantage of this joint\ntraining and work synergistically to produce more informative and relevant text\nin both prose and dialogue generation.", "published": "2021-05-14 00:11:38", "link": "http://arxiv.org/abs/2105.06597v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neural-Symbolic Commonsense Reasoner with Relation Predictors", "abstract": "Commonsense reasoning aims to incorporate sets of commonsense facts,\nretrieved from Commonsense Knowledge Graphs (CKG), to draw conclusion about\nordinary situations. The dynamic nature of commonsense knowledge postulates\nmodels capable of performing multi-hop reasoning over new situations. This\nfeature also results in having large-scale sparse Knowledge Graphs, where such\nreasoning process is needed to predict relations between new events. However,\nexisting approaches in this area are limited by considering CKGs as a limited\nset of facts, thus rendering them unfit for reasoning over new unseen\nsituations and events. In this paper, we present a neural-symbolic reasoner,\nwhich is capable of reasoning over large-scale dynamic CKGs. The logic rules\nfor reasoning over CKGs are learned during training by our model. In addition\nto providing interpretable explanation, the learned logic rules help to\ngeneralise prediction to newly introduced events. Experimental results on the\ntask of link prediction on CKGs prove the effectiveness of our model by\noutperforming the state-of-the-art models.", "published": "2021-05-14 08:54:25", "link": "http://arxiv.org/abs/2105.06717v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Out-of-Manifold Regularization in Contextual Embedding Space for Text\n  Classification", "abstract": "Recent studies on neural networks with pre-trained weights (i.e., BERT) have\nmainly focused on a low-dimensional subspace, where the embedding vectors\ncomputed from input words (or their contexts) are located. In this work, we\npropose a new approach to finding and regularizing the remainder of the space,\nreferred to as out-of-manifold, which cannot be accessed through the words.\nSpecifically, we synthesize the out-of-manifold embeddings based on two\nembeddings obtained from actually-observed words, to utilize them for\nfine-tuning the network. A discriminator is trained to detect whether an input\nembedding is located inside the manifold or not, and simultaneously, a\ngenerator is optimized to produce new embeddings that can be easily identified\nas out-of-manifold by the discriminator. These two modules successfully\ncollaborate in a unified and end-to-end manner for regularizing the\nout-of-manifold. Our extensive evaluation on various text classification\nbenchmarks demonstrates the effectiveness of our approach, as well as its good\ncompatibility with existing data augmentation techniques which aim to enhance\nthe manifold.", "published": "2021-05-14 10:17:59", "link": "http://arxiv.org/abs/2105.06750v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DialogSum: A Real-Life Scenario Dialogue Summarization Dataset", "abstract": "Proposal of large-scale datasets has facilitated research on deep neural\nmodels for news summarization. Deep learning can also be potentially useful for\nspoken dialogue summarization, which can benefit a range of real-life scenarios\nincluding customer service management and medication tracking. To this end, we\npropose DialogSum, a large-scale labeled dialogue summarization dataset. We\nconduct empirical analysis on DialogSum using state-of-the-art neural\nsummarizers. Experimental results show unique challenges in dialogue\nsummarization, such as spoken terms, special discourse structures, coreferences\nand ellipsis, pragmatics and social common sense, which require specific\nrepresentation learning technologies to better deal with.", "published": "2021-05-14 11:12:40", "link": "http://arxiv.org/abs/2105.06762v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Plot and Rework: Modeling Storylines for Visual Storytelling", "abstract": "Writing a coherent and engaging story is not easy. Creative writers use their\nknowledge and worldview to put disjointed elements together to form a coherent\nstoryline, and work and rework iteratively toward perfection. Automated visual\nstorytelling (VIST) models, however, make poor use of external knowledge and\niterative generation when attempting to create stories. This paper introduces\nPR-VIST, a framework that represents the input image sequence as a story graph\nin which it finds the best path to form a storyline. PR-VIST then takes this\npath and learns to generate the final story via an iterative training process.\nThis framework produces stories that are superior in terms of diversity,\ncoherence, and humanness, per both automatic and human evaluations. An ablation\nstudy shows that both plotting and reworking contribute to the model's\nsuperiority.", "published": "2021-05-14 16:41:29", "link": "http://arxiv.org/abs/2105.06950v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A cost-benefit analysis of cross-lingual transfer methods", "abstract": "An effective method for cross-lingual transfer is to fine-tune a bilingual or\nmultilingual model on a supervised dataset in one language and evaluating it on\nanother language in a zero-shot manner. Translating examples at training time\nor inference time are also viable alternatives. However, there are costs\nassociated with these methods that are rarely addressed in the literature. In\nthis work, we analyze cross-lingual methods in terms of their effectiveness\n(e.g., accuracy), development and deployment costs, as well as their latencies\nat inference time. Our experiments on three tasks indicate that the best\ncross-lingual method is highly task-dependent. Finally, by combining zero-shot\nand translation methods, we achieve the state-of-the-art in two of the three\ndatasets used in this work. Based on these results, we question the need for\nmanually labeled training data in a target language. Code and translated\ndatasets are available at https://github.com/unicamp-dl/cross-lingual-analysis", "published": "2021-05-14 13:21:12", "link": "http://arxiv.org/abs/2105.06813v4", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "QAConv: Question Answering on Informative Conversations", "abstract": "This paper introduces QAConv, a new question answering (QA) dataset that uses\nconversations as a knowledge source. We focus on informative conversations,\nincluding business emails, panel discussions, and work channels. Unlike\nopen-domain and task-oriented dialogues, these conversations are usually long,\ncomplex, asynchronous, and involve strong domain knowledge. In total, we\ncollect 34,608 QA pairs from 10,259 selected conversations with both\nhuman-written and machine-generated questions. We use a question generator and\na dialogue summarizer as auxiliary tools to collect and recommend questions.\nThe dataset has two testing scenarios: chunk mode and full mode, depending on\nwhether the grounded partial conversation is provided or retrieved.\nExperimental results show that state-of-the-art pretrained QA systems have\nlimited zero-shot performance and tend to predict our questions as\nunanswerable. Our dataset provides a new training and evaluation testbed to\nfacilitate QA on conversations research.", "published": "2021-05-14 15:53:05", "link": "http://arxiv.org/abs/2105.06912v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Do Context-Aware Translation Models Pay the Right Attention?", "abstract": "Context-aware machine translation models are designed to leverage contextual\ninformation, but often fail to do so. As a result, they inaccurately\ndisambiguate pronouns and polysemous words that require context for resolution.\nIn this paper, we ask several questions: What contexts do human translators use\nto resolve ambiguous words? Are models paying large amounts of attention to the\nsame context? What if we explicitly train them to do so? To answer these\nquestions, we introduce SCAT (Supporting Context for Ambiguous Translations), a\nnew English-French dataset comprising supporting context words for 14K\ntranslations that professional translators found useful for pronoun\ndisambiguation. Using SCAT, we perform an in-depth analysis of the context used\nto disambiguate, examining positional and lexical characteristics of the\nsupporting words. Furthermore, we measure the degree of alignment between the\nmodel's attention scores and the supporting context from SCAT, and apply a\nguided attention strategy to encourage agreement between the two.", "published": "2021-05-14 17:32:24", "link": "http://arxiv.org/abs/2105.06977v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Listen with Intent: Improving Speech Recognition with Audio-to-Intent\n  Front-End", "abstract": "Comprehending the overall intent of an utterance helps a listener recognize\nthe individual words spoken. Inspired by this fact, we perform a novel study of\nthe impact of explicitly incorporating intent representations as additional\ninformation to improve a recurrent neural network-transducer (RNN-T) based\nautomatic speech recognition (ASR) system. An audio-to-intent (A2I) model\nencodes the intent of the utterance in the form of embeddings or posteriors,\nand these are used as auxiliary inputs for RNN-T training and inference.\nExperimenting with a 50k-hour far-field English speech corpus, this study shows\nthat when running the system in non-streaming mode, where intent representation\nis extracted from the entire utterance and then used to bias streaming RNN-T\nsearch from the start, it provides a 5.56% relative word error rate reduction\n(WERR). On the other hand, a streaming system using per-frame intent posteriors\nas extra inputs for the RNN-T ASR system yields a 3.33% relative WERR. A\nfurther detailed analysis of the streaming system indicates that our proposed\nmethod brings especially good gain on media-playing related intents (e.g. 9.12%\nrelative WERR on PlayMusicIntent).", "published": "2021-05-14 21:19:30", "link": "http://arxiv.org/abs/2105.07071v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Benefit Of Temporally-Strong Labels In Audio Event Classification", "abstract": "To reveal the importance of temporal precision in ground truth audio event\nlabels, we collected precise (~0.1 sec resolution) \"strong\" labels for a\nportion of the AudioSet dataset. We devised a temporally strong evaluation set\n(including explicit negatives of varying difficulty) and a small strong-labeled\ntraining subset of 67k clips (compared to the original dataset's 1.8M clips\nlabeled at 10 sec resolution). We show that fine-tuning with a mix of weak and\nstrongly labeled data can substantially improve classifier performance, even\nwhen evaluated using only the original weak labels. For a ResNet50\narchitecture, d' on the strong evaluation data including explicit negatives\nimproves from 1.13 to 1.41. The new labels are available as an update to\nAudioSet.", "published": "2021-05-14 18:48:20", "link": "http://arxiv.org/abs/2105.07031v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Streaming Transformer for Hardware Efficient Voice Trigger Detection and\n  False Trigger Mitigation", "abstract": "We present a unified and hardware efficient architecture for two stage voice\ntrigger detection (VTD) and false trigger mitigation (FTM) tasks. Two stage VTD\nsystems of voice assistants can get falsely activated to audio segments\nacoustically similar to the trigger phrase of interest. FTM systems cancel such\nactivations by using post trigger audio context. Traditional FTM systems rely\non automatic speech recognition lattices which are computationally expensive to\nobtain on device. We propose a streaming transformer (TF) encoder architecture,\nwhich progressively processes incoming audio chunks and maintains audio context\nto perform both VTD and FTM tasks using only acoustic features. The proposed\njoint model yields an average 18% relative reduction in false reject rate (FRR)\nfor the VTD task at a given false alarm rate. Moreover, our model suppresses\n95% of the false triggers with an additional one second of post-trigger audio.\nFinally, on-device measurements show 32% reduction in runtime memory and 56%\nreduction in inference time compared to non-streaming version of the model.", "published": "2021-05-14 00:41:42", "link": "http://arxiv.org/abs/2105.06598v1", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sound Pressure Minimization at the Ear Drum for In-ear ANC Headphones\n  using a Fixed Feedforward Remote Microphone Technique", "abstract": "In this paper we consider an in-ear headphone equipped with an external\nmicrophone and aim to minimize the sound pressure at the ear drum by means of a\nfixed feedforward ANC controller. Based on measured acoustic paths to predict\nthe sound pressure generated by external sources and the headphone at the ear\ndrum, the FIR filter coefficients of the ANC controller are optimized for\ndifferent sound fields. Due to the acoustic feedback path between the\nloudspeaker and the microphone, a stability constraint based on the Nyquist\nstability criterion is introduced. Performance degradations due to reinsertions\nof the headphone and intra-subject variations are addressed by simultaneously\noptimizing the controller for several measurement repetitions of the acoustic\npaths. Simulations show that the controller optimized for an ipsilateral\nexcitation produces an attenuation of at least -10 dB that extends\napproximately to +45{\\deg} and -65{\\deg} from the ipsilateral DoA. The\ncontroller optimized for a diffuse-field excitation achieves an attenuation of\nat least -10 dB over a wider range of DoAs on the ipsilateral side, namely\n+90{\\deg} to -90{\\deg}. Optimizing the controller for several measurement\nrepetitions is shown to be effective against performance degradations due to\nreinsertions and intra-subject variations.", "published": "2021-05-14 15:29:38", "link": "http://arxiv.org/abs/2105.06894v1", "categories": ["eess.AS", "cs.SD", "cs.SY", "eess.SY"], "primary_category": "eess.AS"}
{"title": "Chord Recognition- Music and Audio Information Retrieval", "abstract": "Music Information Retrieval (MIR) is a collaborative scientific study that\nhelp to build innovative information research themes, novel frameworks, and\ndeveloping connected delivery mechanisms in addition to making the world's\nmassive collection of music open for everyone. Modern rock music proved to be\ndifficult to estimate tempo and chord recognition did not work. All of the\nfindings indicate that modern rock and metal music can be analysed, despite its\ncomplexity, but that further research is needed in this area to make it useful.\nUsing a neural network has been one of the simplest ways of dealing with it.\nThe pitch class profile vector is used in the neural network method. Because\nthe vector only contains 12 elements of semi-tone values, it is enough for\nchord recognition. Of course, there are other ways of achieving this work, most\nof them depend on pitch class profiling to transform the chord into a type that\ncan be recognised, but the recognition process is time-consuming centred on\nextremely complicated and memory-intensive methods.", "published": "2021-05-14 18:14:53", "link": "http://arxiv.org/abs/2105.07019v2", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Predicting speech intelligibility from EEG in a non-linear\n  classification paradigm", "abstract": "Objective: Currently, only behavioral speech understanding tests are\navailable, which require active participation of the person being tested. As\nthis is infeasible for certain populations, an objective measure of speech\nintelligibility is required. Recently, brain imaging data has been used to\nestablish a relationship between stimulus and brain response. Linear models\nhave been successfully linked to speech intelligibility but require per-subject\ntraining. We present a deep-learning-based model incorporating dilated\nconvolutions that operates in a match/mismatch paradigm. The accuracy of the\nmodel's match/mismatch predictions can be used as a proxy for speech\nintelligibility without subject-specific (re)training. Approach: We evaluated\nthe performance of the model as a function of input segment length, EEG\nfrequency band and receptive field size while comparing it to multiple baseline\nmodels. Next, we evaluated performance on held-out data and finetuning.\nFinally, we established a link between the accuracy of our model and the\nstate-of-the-art behavioral MATRIX test. Main results: The dilated\nconvolutional model significantly outperformed the baseline models for every\ninput segment length, for all EEG frequency bands except the delta and theta\nband, and receptive field sizes between 250 and 500 ms. Additionally,\nfinetuning significantly increased the accuracy on a held-out dataset. Finally,\na significant correlation (r=0.59, p=0.0154) was found between the speech\nreception threshold estimated using the behavioral MATRIX test and our\nobjective method. Significance: Our method is the first to predict the speech\nreception threshold from EEG for unseen subjects, contributing to objective\nmeasures of speech intelligibility.", "published": "2021-05-14 14:12:52", "link": "http://arxiv.org/abs/2105.06844v4", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP", "q-bio.QM"], "primary_category": "eess.AS"}
{"title": "Nonuniform Sampling Rate Conversion: An Efficient Approach", "abstract": "We present a discrete-time algorithm for nonuniform sampling rate conversion\nthat presents low computational complexity and memory requirements. It\ngeneralizes arbitrary sampling rate conversion by accommodating time-varying\nconversion ratios, i.e., it can efficiently adapt to instantaneous changes of\nthe input and output sampling rates. This approach is based on appropriately\nfactorizing the time-varying discrete-time filter used for the conversion.\nCommon filters that satisfy this factorization property are those where the\nunderlying continuous-time filter consists of linear combinations of\nexponentials, e.g., those described by linear constant-coefficient differential\nequations. This factorization separates the computation into two parts: one\nconsisting of a factor solely depending on the output sampling instants and the\nother consists of a summation -- that can be computed recursively -- whose\nterms depend solely on the input sampling instants and its number of terms is\ngiven by a relationship between input and output sampling instants. Thus,\nnonuniform sampling rates can be accommodated by updating the factors involved\nand adjusting the number of terms added. When the impulse response consists of\nexponentials, computing the factors can be done recursively in an efficient\nmanner.", "published": "2021-05-14 08:25:58", "link": "http://arxiv.org/abs/2105.06700v1", "categories": ["eess.SP", "cs.IT", "cs.SD", "eess.AS", "eess.IV", "math.IT"], "primary_category": "eess.SP"}
