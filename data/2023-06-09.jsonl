{"title": "Word sense extension", "abstract": "Humans often make creative use of words to express novel senses. A\nlong-standing effort in natural language processing has been focusing on word\nsense disambiguation (WSD), but little has been explored about how the sense\ninventory of a word may be extended toward novel meanings. We present a\nparadigm of word sense extension (WSE) that enables words to spawn new senses\ntoward novel context. We develop a framework that simulates novel word sense\nextension by first partitioning a polysemous word type into two pseudo-tokens\nthat mark its different senses, and then inferring whether the meaning of a\npseudo-token can be extended to convey the sense denoted by the token\npartitioned from the same word type. Our framework combines cognitive models of\nchaining with a learning scheme that transforms a language model embedding\nspace to support various types of word sense extension. We evaluate our\nframework against several competitive baselines and show that it is superior in\npredicting plausible novel senses for over 7,500 English words. Furthermore, we\nshow that our WSE framework improves performance over a range of\ntransformer-based WSD models in predicting rare word senses with few or zero\nmentions in the training data.", "published": "2023-06-09 00:54:21", "link": "http://arxiv.org/abs/2306.05609v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WSPAlign: Word Alignment Pre-training via Large-Scale Weakly Supervised\n  Span Prediction", "abstract": "Most existing word alignment methods rely on manual alignment datasets or\nparallel corpora, which limits their usefulness. Here, to mitigate the\ndependence on manual data, we broaden the source of supervision by relaxing the\nrequirement for correct, fully-aligned, and parallel sentences. Specifically,\nwe make noisy, partially aligned, and non-parallel paragraphs. We then use such\na large-scale weakly-supervised dataset for word alignment pre-training via\nspan prediction. Extensive experiments with various settings empirically\ndemonstrate that our approach, which is named WSPAlign, is an effective and\nscalable way to pre-train word aligners without manual data. When fine-tuned on\nstandard benchmarks, WSPAlign has set a new state-of-the-art by improving upon\nthe best-supervised baseline by 3.3~6.1 points in F1 and 1.5~6.1 points in AER.\nFurthermore, WSPAlign also achieves competitive performance compared with the\ncorresponding baselines in few-shot, zero-shot and cross-lingual tests, which\ndemonstrates that WSPAlign is potentially more practical for low-resource\nlanguages than existing methods.", "published": "2023-06-09 03:11:42", "link": "http://arxiv.org/abs/2306.05644v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "I run as fast as a rabbit, can you? A Multilingual Simile Dialogue\n  Dataset", "abstract": "A simile is a figure of speech that compares two different things (called the\ntenor and the vehicle) via shared properties. The tenor and the vehicle are\nusually connected with comparator words such as \"like\" or \"as\". The simile\nphenomena are unique and complex in a real-life dialogue scene where the tenor\nand the vehicle can be verbal phrases or sentences, mentioned by different\nspeakers, exist in different sentences, or occur in reversed order. However,\nthe current simile research usually focuses on similes in a triplet tuple\n(tenor, property, vehicle) or a single sentence where the tenor and vehicle are\nusually entities or noun phrases, which could not reflect complex simile\nphenomena in real scenarios. In this paper, we propose a novel and high-quality\nmultilingual simile dialogue (MSD) dataset to facilitate the study of complex\nsimile phenomena. The MSD is the largest manually annotated simile data\n($\\sim$20K) and it contains both English and Chinese data. Meanwhile, the MSD\ndata can also be used on dialogue tasks to test the ability of dialogue systems\nwhen using similes. We design 3 simile tasks (recognition, interpretation, and\ngeneration) and 2 dialogue tasks (retrieval and generation) with MSD. For each\ntask, we provide experimental results from strong pre-trained or\nstate-of-the-art models. The experiments demonstrate the challenge of MSD and\nwe have released the data/code on GitHub.", "published": "2023-06-09 05:04:13", "link": "http://arxiv.org/abs/2306.05672v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge\n  Evaluation", "abstract": "New Natural Langauge Process~(NLP) benchmarks are urgently needed to align\nwith the rapid development of large language models (LLMs). We present Xiezhi,\nthe most comprehensive evaluation suite designed to assess holistic domain\nknowledge. Xiezhi comprises multiple-choice questions across 516 diverse\ndisciplines ranging from 13 different subjects with 249,587 questions and\naccompanied by Xiezhi-Specialty and Xiezhi-Interdiscipline, both with 15k\nquestions. We conduct evaluation of the 47 cutting-edge LLMs on Xiezhi. Results\nindicate that LLMs exceed average performance of humans in science,\nengineering, agronomy, medicine, and art, but fall short in economics,\njurisprudence, pedagogy, literature, history, and management. We anticipate\nXiezhi will help analyze important strengths and shortcomings of LLMs, and the\nbenchmark is released in~\\url{https://github.com/MikeGu721/XiezhiBenchmark}.", "published": "2023-06-09 09:52:05", "link": "http://arxiv.org/abs/2306.05783v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards the Exploitation of LLM-based Chatbot for Providing Legal\n  Support to Palestinian Cooperatives", "abstract": "With the ever-increasing utilization of natural language processing (NLP), we\nstarted to witness over the past few years a significant transformation in our\ninteraction with legal texts. This technology has advanced the analysis and\nenhanced the understanding of complex legal terminology and contexts. The\ndevelopment of recent large language models (LLMs), particularly ChatGPT, has\nalso introduced a revolutionary contribution to the way that legal texts can be\nprocessed and comprehended. In this paper, we present our work on a\ncooperative-legal question-answering LLM-based chatbot, where we developed a\nset of legal questions about Palestinian cooperatives, associated with their\nregulations and compared the auto-generated answers by the chatbot to their\ncorrespondences that are designed by a legal expert. To evaluate the proposed\nchatbot, we have used 50 queries generated by the legal expert and compared the\nanswers produced by the chart to their relevance judgments. Finding\ndemonstrated that an overall accuracy rate of 82% has been achieved when\nanswering the queries, while exhibiting an F1 score equivalent to 79%.", "published": "2023-06-09 11:57:57", "link": "http://arxiv.org/abs/2306.05827v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards a Robust Detection of Language Model Generated Text: Is ChatGPT\n  that Easy to Detect?", "abstract": "Recent advances in natural language processing (NLP) have led to the\ndevelopment of large language models (LLMs) such as ChatGPT. This paper\nproposes a methodology for developing and evaluating ChatGPT detectors for\nFrench text, with a focus on investigating their robustness on out-of-domain\ndata and against common attack schemes. The proposed method involves\ntranslating an English dataset into French and training a classifier on the\ntranslated data. Results show that the detectors can effectively detect\nChatGPT-generated text, with a degree of robustness against basic attack\ntechniques in in-domain settings. However, vulnerabilities are evident in\nout-of-domain contexts, highlighting the challenge of detecting adversarial\ntext. The study emphasizes caution when applying in-domain testing results to a\nwider variety of content. We provide our translated datasets and models as\nopen-source resources. https://gitlab.inria.fr/wantoun/robust-chatgpt-detection", "published": "2023-06-09 13:03:53", "link": "http://arxiv.org/abs/2306.05871v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Good, but not always Fair: An Evaluation of Gender Bias for three\n  commercial Machine Translation Systems", "abstract": "Machine Translation (MT) continues to make significant strides in quality and\nis increasingly adopted on a larger scale. Consequently, analyses have been\nredirected to more nuanced aspects, intricate phenomena, as well as potential\nrisks that may arise from the widespread use of MT tools. Along this line, this\npaper offers a meticulous assessment of three commercial MT systems - Google\nTranslate, DeepL, and Modern MT - with a specific focus on gender translation\nand bias. For three language pairs (English/Spanish, English/Italian, and\nEnglish/French), we scrutinize the behavior of such systems at several levels\nof granularity and on a variety of naturally occurring gender phenomena in\ntranslation. Our study takes stock of the current state of online MT tools, by\nrevealing significant discrepancies in the gender translation of the three\nsystems, with each system displaying varying degrees of bias despite their\noverall translation quality.", "published": "2023-06-09 13:24:27", "link": "http://arxiv.org/abs/2306.05882v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models Can Learn Exceptions to Syntactic Rules", "abstract": "Artificial neural networks can generalize productively to novel contexts. Can\nthey also learn exceptions to those productive rules? We explore this question\nusing the case of restrictions on English passivization (e.g., the fact that\n\"The vacation lasted five days\" is grammatical, but \"*Five days was lasted by\nthe vacation\" is not). We collect human acceptability judgments for passive\nsentences with a range of verbs, and show that the probability distribution\ndefined by GPT-2, a language model, matches the human judgments with high\ncorrelation. We also show that the relative acceptability of a verb in the\nactive vs. passive voice is positively correlated with the relative frequency\nof its occurrence in those voices. These results provide preliminary support\nfor the entrenchment hypothesis, according to which learners track and uses the\ndistributional properties of their input to learn negative exceptions to rules.\nAt the same time, this hypothesis fails to explain the magnitude of\nunpassivizability demonstrated by certain individual verbs, suggesting that\nother cues to exceptionality are available in the linguistic input.", "published": "2023-06-09 15:35:11", "link": "http://arxiv.org/abs/2306.05969v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Labeling of German Chest X-Ray Radiology Reports using Deep\n  Learning", "abstract": "Radiologists are in short supply globally, and deep learning models offer a\npromising solution to address this shortage as part of clinical\ndecision-support systems. However, training such models often requires\nexpensive and time-consuming manual labeling of large datasets. Automatic label\nextraction from radiology reports can reduce the time required to obtain\nlabeled datasets, but this task is challenging due to semantically similar\nwords and missing annotated data. In this work, we explore the potential of\nweak supervision of a deep learning-based label prediction model, using a\nrule-based labeler. We propose a deep learning-based CheXpert label prediction\nmodel, pre-trained on reports labeled by a rule-based German CheXpert model and\nfine-tuned on a small dataset of manually labeled reports. Our results\ndemonstrate the effectiveness of our approach, which significantly outperformed\nthe rule-based model on all three tasks. Our findings highlight the benefits of\nemploying deep learning-based models even in scenarios with sparse data and the\nuse of the rule-based labeler as a tool for weak supervision.", "published": "2023-06-09 16:08:35", "link": "http://arxiv.org/abs/2306.05997v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assisting Language Learners: Automated Trans-Lingual Definition\n  Generation via Contrastive Prompt Learning", "abstract": "The standard definition generation task requires to automatically produce\nmono-lingual definitions (e.g., English definitions for English words), but\nignores that the generated definitions may also consist of unfamiliar words for\nlanguage learners. In this work, we propose a novel task of Trans-Lingual\nDefinition Generation (TLDG), which aims to generate definitions in another\nlanguage, i.e., the native speaker's language. Initially, we explore the\nunsupervised manner of this task and build up a simple implementation of\nfine-tuning the multi-lingual machine translation model. Then, we develop two\nnovel methods, Prompt Combination and Contrastive Prompt Learning, for further\nenhancing the quality of the generation. Our methods are evaluated against the\nbaseline Pipeline method in both rich- and low-resource settings, and we\nempirically establish its superiority in generating higher-quality\ntrans-lingual definitions.", "published": "2023-06-09 17:32:45", "link": "http://arxiv.org/abs/2306.06058v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mind2Web: Towards a Generalist Agent for the Web", "abstract": "We introduce Mind2Web, the first dataset for developing and evaluating\ngeneralist agents for the web that can follow language instructions to complete\ncomplex tasks on any website. Existing datasets for web agents either use\nsimulated websites or only cover a limited set of websites and tasks, thus not\nsuitable for generalist web agents. With over 2,000 open-ended tasks collected\nfrom 137 websites spanning 31 domains and crowdsourced action sequences for the\ntasks, Mind2Web provides three necessary ingredients for building generalist\nweb agents: 1) diverse domains, websites, and tasks, 2) use of real-world\nwebsites instead of simulated and simplified ones, and 3) a broad spectrum of\nuser interaction patterns. Based on Mind2Web, we conduct an initial exploration\nof using large language models (LLMs) for building generalist web agents. While\nthe raw HTML of real-world websites are often too large to be fed to LLMs, we\nshow that first filtering it with a small LM significantly improves the\neffectiveness and efficiency of LLMs. Our solution demonstrates a decent level\nof performance, even on websites or entire domains the model has never seen\nbefore, but there is still a substantial room to improve towards truly\ngeneralizable agents. We open-source our dataset, model implementation, and\ntrained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further\nresearch on building a generalist agent for the web.", "published": "2023-06-09 17:44:31", "link": "http://arxiv.org/abs/2306.06070v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Dialogue Relation Extraction by Relating Explainable Triggers\n  and Relation Names", "abstract": "Developing dialogue relation extraction (DRE) systems often requires a large\namount of labeled data, which can be costly and time-consuming to annotate. In\norder to improve scalability and support diverse, unseen relation extraction,\nthis paper proposes a method for leveraging the ability to capture triggers and\nrelate them to previously unseen relation names. Specifically, we introduce a\nmodel that enables zero-shot dialogue relation extraction by utilizing\ntrigger-capturing capabilities. Our experiments on a benchmark DialogRE dataset\ndemonstrate that the proposed model achieves significant improvements for both\nseen and unseen relations. Notably, this is the first attempt at zero-shot\ndialogue relation extraction using trigger-capturing capabilities, and our\nresults suggest that this approach is effective for inferring previously unseen\nrelation types. Overall, our findings highlight the potential for this method\nto enhance the scalability and practicality of DRE systems.", "published": "2023-06-09 07:10:01", "link": "http://arxiv.org/abs/2306.06141v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Morphosyntactic probing of multilingual BERT models", "abstract": "We introduce an extensive dataset for multilingual probing of morphological\ninformation in language models (247 tasks across 42 languages from 10\nfamilies), each consisting of a sentence with a target word and a morphological\ntag as the desired label, derived from the Universal Dependencies treebanks. We\nfind that pre-trained Transformer models (mBERT and XLM-RoBERTa) learn features\nthat attain strong performance across these tasks. We then apply two methods to\nlocate, for each probing task, where the disambiguating information resides in\nthe input. The first is a new perturbation method that masks various parts of\ncontext; the second is the classical method of Shapley values. The most\nintriguing finding that emerges is a strong tendency for the preceding context\nto hold more information relevant to the prediction than the following context.", "published": "2023-06-09 19:15:20", "link": "http://arxiv.org/abs/2306.06205v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conformalizing Machine Translation Evaluation", "abstract": "Several uncertainty estimation methods have been recently proposed for\nmachine translation evaluation. While these methods can provide a useful\nindication of when not to trust model predictions, we show in this paper that\nthe majority of them tend to underestimate model uncertainty, and as a result\nthey often produce misleading confidence intervals that do not cover the ground\ntruth. We propose as an alternative the use of conformal prediction, a\ndistribution-free method to obtain confidence intervals with a theoretically\nestablished guarantee on coverage. First, we demonstrate that split conformal\nprediction can ``correct'' the confidence intervals of previous methods to\nyield a desired coverage level. Then, we highlight biases in estimated\nconfidence intervals, both in terms of the translation language pairs and the\nquality of translations. We apply conditional conformal prediction techniques\nto obtain calibration subsets for each data subgroup, leading to equalized\ncoverage.", "published": "2023-06-09 19:36:18", "link": "http://arxiv.org/abs/2306.06221v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Record Deduplication for Entity Distribution Modeling in ASR Transcripts", "abstract": "Voice digital assistants must keep up with trending search queries. We rely\non a speech recognition model using contextual biasing with a rapidly updated\nset of entities, instead of frequent model retraining, to keep up with trends.\nThere are several challenges with this approach: (1) the entity set must be\nfrequently reconstructed, (2) the entity set is of limited size due to latency\nand accuracy trade-offs, and (3) finding the true entity distribution for\nbiasing is complicated by ASR misrecognition. We address these challenges and\ndefine an entity set by modeling customers true requested entity distribution\nfrom ASR output in production using record deduplication, a technique from the\nfield of entity resolution. Record deduplication resolves or deduplicates\ncoreferences, including misrecognitions, of the same latent entity. Our method\nsuccessfully retrieves 95% of misrecognized entities and when used for\ncontextual biasing shows an estimated 5% relative word error rate reduction.", "published": "2023-06-09 20:42:11", "link": "http://arxiv.org/abs/2306.06246v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Unified Generative Approach to Product Attribute-Value Identification", "abstract": "Product attribute-value identification (PAVI) has been studied to link\nproducts on e-commerce sites with their attribute values (e.g., <Material,\nCotton>) using product text as clues. Technical demands from real-world\ne-commerce platforms require PAVI methods to handle unseen values,\nmulti-attribute values, and canonicalized values, which are only partly\naddressed in existing extraction- and classification-based approaches.\nMotivated by this, we explore a generative approach to the PAVI task. We\nfinetune a pre-trained generative model, T5, to decode a set of attribute-value\npairs as a target sequence from the given product text. Since the attribute\nvalue pairs are unordered set elements, how to linearize them will matter; we,\nthus, explore methods of composing an attribute-value pair and ordering the\npairs for the task. Experimental results confirm that our generation-based\napproach outperforms the existing extraction and classification-based methods\non large-scale real-world datasets meant for those methods.", "published": "2023-06-09 00:33:30", "link": "http://arxiv.org/abs/2306.05605v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in\n  Language Models", "abstract": "Prompt-based learning has been proved to be an effective way in pre-trained\nlanguage models (PLMs), especially in low-resource scenarios like few-shot\nsettings. However, the trustworthiness of PLMs is of paramount significance and\npotential vulnerabilities have been shown in prompt-based templates that could\nmislead the predictions of language models, causing serious security concerns.\nIn this paper, we will shed light on some vulnerabilities of PLMs, by proposing\na prompt-based adversarial attack on manual templates in black box scenarios.\nFirst of all, we design character-level and word-level heuristic approaches to\nbreak manual templates separately. Then we present a greedy algorithm for the\nattack based on the above heuristic destructive approaches. Finally, we\nevaluate our approach with the classification tasks on three variants of BERT\nseries models and eight datasets. And comprehensive experimental results\njustify the effectiveness of our approach in terms of attack success rate and\nattack speed.", "published": "2023-06-09 03:53:42", "link": "http://arxiv.org/abs/2306.05659v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena", "abstract": "Evaluating large language model (LLM) based chat assistants is challenging\ndue to their broad capabilities and the inadequacy of existing benchmarks in\nmeasuring human preferences. To address this, we explore using strong LLMs as\njudges to evaluate these models on more open-ended questions. We examine the\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\nself-enhancement biases, as well as limited reasoning ability, and propose\nsolutions to mitigate some of them. We then verify the agreement between LLM\njudges and human preferences by introducing two benchmarks: MT-bench, a\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\ncrowdsourced human preferences well, achieving over 80% agreement, the same\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\nexplainable way to approximate human preferences, which are otherwise very\nexpensive to obtain. Additionally, we show our benchmark and traditional\nbenchmarks complement each other by evaluating several variants of LLaMA and\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\nhuman preferences are publicly available at\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.", "published": "2023-06-09 05:55:52", "link": "http://arxiv.org/abs/2306.05685v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transformer-based Time-to-Event Prediction for Chronic Kidney Disease\n  Deterioration", "abstract": "Deep-learning techniques, particularly the transformer model, have shown\ngreat potential in enhancing the prediction performance of longitudinal health\nrecords. While previous methods have mainly focused on fixed-time risk\nprediction, time-to-event prediction (also known as survival analysis) is often\nmore appropriate for clinical scenarios. Here, we present a novel deep-learning\narchitecture we named STRAFE, a generalizable survival analysis\ntransformer-based architecture for electronic health records. The performance\nof STRAFE was evaluated using a real-world claim dataset of over 130,000\nindividuals with stage 3 chronic kidney disease (CKD) and was found to\noutperform other time-to-event prediction algorithms in predicting the exact\ntime of deterioration to stage 5. Additionally, STRAFE was found to outperform\nbinary outcome algorithms in predicting fixed-time risk, possibly due to its\nability to train on censored data. We show that STRAFE predictions can improve\nthe positive predictive value of high-risk patients by 3-fold, demonstrating\npossible usage to improve targeting for intervention programs. Finally, we\nsuggest a novel visualization approach to predictions on a per-patient basis.\nIn conclusion, STRAFE is a cutting-edge time-to-event prediction algorithm that\nhas the potential to enhance risk predictions in large claims datasets.", "published": "2023-06-09 09:46:38", "link": "http://arxiv.org/abs/2306.05779v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "HiTZ@Antidote: Argumentation-driven Explainable Artificial Intelligence\n  for Digital Medicine", "abstract": "Providing high quality explanations for AI predictions based on machine\nlearning is a challenging and complex task. To work well it requires, among\nother factors: selecting a proper level of generality/specificity of the\nexplanation; considering assumptions about the familiarity of the explanation\nbeneficiary with the AI task under consideration; referring to specific\nelements that have contributed to the decision; making use of additional\nknowledge (e.g. expert evidence) which might not be part of the prediction\nprocess; and providing evidence supporting negative hypothesis. Finally, the\nsystem needs to formulate the explanation in a clearly interpretable, and\npossibly convincing, way. Given these considerations, ANTIDOTE fosters an\nintegrated vision of explainable AI, where low-level characteristics of the\ndeep learning process are combined with higher level schemes proper of the\nhuman argumentation capacity. ANTIDOTE will exploit cross-disciplinary\ncompetences in deep learning and argumentation to support a broader and\ninnovative view of explainable AI, where the need for high-quality explanations\nfor clinical cases deliberation is critical. As a first result of the project,\nwe publish the Antidote CasiMedicos dataset to facilitate research on\nexplainable AI in general, and argumentation in the medical domain in\nparticular.", "published": "2023-06-09 16:50:02", "link": "http://arxiv.org/abs/2306.06029v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Trapping LLM Hallucinations Using Tagged Context Prompts", "abstract": "Recent advances in large language models (LLMs), such as ChatGPT, have led to\nhighly sophisticated conversation agents. However, these models suffer from\n\"hallucinations,\" where the model generates false or fabricated information.\nAddressing this challenge is crucial, particularly with AI-driven platforms\nbeing adopted across various sectors. In this paper, we propose a novel method\nto recognize and flag instances when LLMs perform outside their domain\nknowledge, and ensuring users receive accurate information.\n  We find that the use of context combined with embedded tags can successfully\ncombat hallucinations within generative language models. To do this, we\nbaseline hallucination frequency in no-context prompt-response pairs using\ngenerated URLs as easily-tested indicators of fabricated data. We observed a\nsignificant reduction in overall hallucination when context was supplied along\nwith question prompts for tested generative engines. Lastly, we evaluated how\nplacing tags within contexts impacted model responses and were able to\neliminate hallucinations in responses with 98.88% effectiveness.", "published": "2023-06-09 17:48:54", "link": "http://arxiv.org/abs/2306.06085v1", "categories": ["cs.CL", "cs.AI", "I.2.7; K.4.2"], "primary_category": "cs.CL"}
{"title": "SentiGOLD: A Large Bangla Gold Standard Multi-Domain Sentiment Analysis\n  Dataset and its Evaluation", "abstract": "This study introduces SentiGOLD, a Bangla multi-domain sentiment analysis\ndataset. Comprising 70,000 samples, it was created from diverse sources and\nannotated by a gender-balanced team of linguists. SentiGOLD adheres to\nestablished linguistic conventions agreed upon by the Government of Bangladesh\nand a Bangla linguistics committee. Unlike English and other languages, Bangla\nlacks standard sentiment analysis datasets due to the absence of a national\nlinguistics framework. The dataset incorporates data from online video\ncomments, social media posts, blogs, news, and other sources while maintaining\ndomain and class distribution rigorously. It spans 30 domains (e.g., politics,\nentertainment, sports) and includes 5 sentiment classes (strongly negative,\nweakly negative, neutral, and strongly positive). The annotation scheme,\napproved by the national linguistics committee, ensures a robust Inter\nAnnotator Agreement (IAA) with a Fleiss' kappa score of 0.88. Intra- and\ncross-dataset evaluation protocols are applied to establish a standard\nclassification system. Cross-dataset evaluation on the noisy SentNoB dataset\npresents a challenging test scenario. Additionally, zero-shot experiments\ndemonstrate the generalizability of SentiGOLD. The top model achieves a macro\nf1 score of 0.62 (intra-dataset) across 5 classes, setting a benchmark, and\n0.61 (cross-dataset from SentNoB) across 3 classes, comparable to the\nstate-of-the-art. Fine-tuned sentiment analysis model can be accessed at\nhttps://sentiment.bangla.gov.bd.", "published": "2023-06-09 12:07:10", "link": "http://arxiv.org/abs/2306.06147v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "$FastDoc$: Domain-Specific Fast Continual Pre-training Technique using\n  Document-Level Metadata and Taxonomy", "abstract": "In this paper, we propose $FastDoc$ (Fast Continual Pre-training Technique\nusing Document Level Metadata and Taxonomy), a novel, compute-efficient\nframework that utilizes Document metadata and Domain-Specific Taxonomy as\nsupervision signals to continually pre-train transformer encoder on a\ndomain-specific corpus. The main innovation is that during domain-specific\npretraining, an open-domain encoder is continually pre-trained using\nsentence-level embeddings as inputs (to accommodate long documents), however,\nfine-tuning is done with token-level embeddings as inputs to this encoder. We\nperform such domain-specific pre-training on three different domains namely\ncustomer support, scientific, and legal domains, and compare performance on 6\ndifferent downstream tasks and 9 different datasets. The novel use of\ndocument-level supervision along with sentence-level embedding input for\npre-training reduces pre-training compute by around $1,000$, $4,500$, and $500$\ntimes compared to MLM and/or NSP in Customer Support, Scientific, and Legal\nDomains, respectively. The reduced training time does not lead to a\ndeterioration in performance. In fact we show that $FastDoc$ either outperforms\nor performs on par with several competitive transformer-based baselines in\nterms of character-level F1 scores and other automated metrics in the Customer\nSupport, Scientific, and Legal Domains. Moreover, reduced training aids in\nmitigating the risk of catastrophic forgetting. Thus, unlike baselines,\n$FastDoc$ shows a negligible drop in performance on open domain.", "published": "2023-06-09 18:42:19", "link": "http://arxiv.org/abs/2306.06190v3", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Reliability Check: An Analysis of GPT-3's Response to Sensitive Topics\n  and Prompt Wording", "abstract": "Large language models (LLMs) have become mainstream technology with their\nversatile use cases and impressive performance. Despite the countless\nout-of-the-box applications, LLMs are still not reliable. A lot of work is\nbeing done to improve the factual accuracy, consistency, and ethical standards\nof these models through fine-tuning, prompting, and Reinforcement Learning with\nHuman Feedback (RLHF), but no systematic analysis of the responses of these\nmodels to different categories of statements, or on their potential\nvulnerabilities to simple prompting changes is available. In this work, we\nanalyze what confuses GPT-3: how the model responds to certain sensitive topics\nand what effects the prompt wording has on the model response. We find that\nGPT-3 correctly disagrees with obvious Conspiracies and Stereotypes but makes\nmistakes with common Misconceptions and Controversies. The model responses are\ninconsistent across prompts and settings, highlighting GPT-3's unreliability.\nDataset and code of our analysis is available in\nhttps://github.com/tanny411/GPT3-Reliability-Check.", "published": "2023-06-09 19:07:31", "link": "http://arxiv.org/abs/2306.06199v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using Foundation Models to Detect Policy Violations with Minimal\n  Supervision", "abstract": "Foundation models, i.e. large neural networks pre-trained on large text\ncorpora, have revolutionized NLP. They can be instructed directly (e.g.\n(arXiv:2005.14165)) - this is called hard prompting - and they can be tuned\nusing very little data (e.g. (arXiv:2104.08691)) - this technique is called\nsoft prompting. We seek to leverage their capabilities to detect policy\nviolations. Our contributions are: We identify a hard prompt that adapts\nchain-of-thought prompting to policy violation tasks. This prompt produces\npolicy violation classifications, along with extractive explanations that\njustify the classification. We compose the hard-prompts with soft prompt tuning\nto produce a classifier that attains high accuracy with very little\nsupervision; the same classifier also produces explanations. Though the\nsupervision only acts on the classifications, we find that the modified\nexplanations remain consistent with the (tuned) model's response. Along the\nway, we identify several unintuitive aspects of foundation models. For\ninstance, adding an example from a specific class can actually reduce\npredictions of that class, and separately, the effects of tokenization on\nscoring etc. Based on our technical results, we identify a simple workflow for\nproduct teams to quickly develop effective policy violation detectors.", "published": "2023-06-09 20:08:48", "link": "http://arxiv.org/abs/2306.06234v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Measuring and Modifying Factual Knowledge in Large Language Models", "abstract": "Large Language Models (LLMs) store an extensive amount of factual knowledge\nobtained from vast collections of text. To effectively utilize these models for\ndownstream tasks, it is crucial to have reliable methods for measuring their\nknowledge. However, existing approaches for knowledge measurement have certain\nlimitations, and despite recent efforts, they fail to provide accurate\nmeasurements and the necessary insights for modifying the knowledge within\nLLMs. In this work, we employ information theory-based measurements to provide\na framework estimating the factual knowledge contained within large language\nmodels. More specifically, we measure knowledge by analyzing the LLM's\nprediction probability distribution before and after instilling the target\nknowledge, employing metrics such as entropy and KL-divergence. Introducing our\nmetrics, we first assess their accuracy in comparison to previous ranking-based\nmethods, surpassing them by over $35\\%$ in a synthetic experiment. Then, we\nexplore two prominent methods of knowledge instillation, discovering that LLMs\nexhibit limitations in capturing new knowledge under specific circumstances for\none of these methods. Lastly, we demonstrate the applicability of our methods\nin extracting unlearned and mislearned facts in LLMs through their application\nto in-context learning. We make code and data for all methods and experiments\nin this paper publicly available.", "published": "2023-06-09 21:25:48", "link": "http://arxiv.org/abs/2306.06264v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Protect Your Prompts: Protocols for IP Protection in LLM Applications", "abstract": "With the rapid adoption of AI in the form of large language models (LLMs),\nthe potential value of carefully engineered prompts has become significant.\nHowever, to realize this potential, prompts should be tradable on an open\nmarket. Since prompts are, at present, generally economically non-excludable,\nby virtue of their nature as text, no general competitive market has yet been\nestablished. This note discusses two protocols intended to provide protection\nof prompts, elevating their status as intellectual property, thus confirming\nthe intellectual property rights of prompt engineers, and potentially\nsupporting the flourishing of an open market for LLM prompts.", "published": "2023-06-09 23:23:26", "link": "http://arxiv.org/abs/2306.06297v1", "categories": ["cs.CL", "cs.AI", "91D10, 68T10, 03D40", "I.2.6; K.6.5; F.3.2"], "primary_category": "cs.CL"}
{"title": "Implementing BERT and fine-tuned RobertA to detect AI generated news by\n  ChatGPT", "abstract": "The abundance of information on social media has increased the necessity of\naccurate real-time rumour detection. Manual techniques of identifying and\nverifying fake news generated by AI tools are impracticable and time-consuming\ngiven the enormous volume of information generated every day. This has sparked\nan increase in interest in creating automated systems to find fake news on the\nInternet. The studies in this research demonstrate that the BERT and RobertA\nmodels with fine-tuning had the best success in detecting AI generated news.\nWith a score of 98%, tweaked RobertA in particular showed excellent precision.\nIn conclusion, this study has shown that neural networks can be used to\nidentify bogus news AI generation news created by ChatGPT. The RobertA and BERT\nmodels' excellent performance indicates that these models can play a critical\nrole in the fight against misinformation.", "published": "2023-06-09 17:53:19", "link": "http://arxiv.org/abs/2306.07401v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models Are Semi-Parametric Reinforcement Learning Agents", "abstract": "Inspired by the insights in cognitive science with respect to human memory\nand reasoning mechanism, a novel evolvable LLM-based (Large Language Model)\nagent framework is proposed as REMEMBERER. By equipping the LLM with a\nlong-term experience memory, REMEMBERER is capable of exploiting the\nexperiences from the past episodes even for different task goals, which excels\nan LLM-based agent with fixed exemplars or equipped with a transient working\nmemory. We further introduce Reinforcement Learning with Experience Memory\n(RLEM) to update the memory. Thus, the whole system can learn from the\nexperiences of both success and failure, and evolve its capability without\nfine-tuning the parameters of the LLM. In this way, the proposed REMEMBERER\nconstitutes a semi-parametric RL agent. Extensive experiments are conducted on\ntwo RL task sets to evaluate the proposed framework. The average results with\ndifferent initialization and training sets exceed the prior SOTA by 4% and 2%\nfor the success rate on two task sets and demonstrate the superiority and\nrobustness of REMEMBERER.", "published": "2023-06-09 08:08:18", "link": "http://arxiv.org/abs/2306.07929v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Understanding Telecom Language Through Large Language Models", "abstract": "The recent progress of artificial intelligence (AI) opens up new frontiers in\nthe possibility of automating many tasks involved in Telecom networks design,\nimplementation, and deployment. This has been further pushed forward with the\nevolution of generative artificial intelligence (AI), including the emergence\nof large language models (LLMs), which is believed to be the cornerstone toward\nrealizing self-governed, interactive AI agents. Motivated by this, in this\npaper, we aim to adapt the paradigm of LLMs to the Telecom domain. In\nparticular, we fine-tune several LLMs including BERT, distilled BERT, RoBERTa\nand GPT-2, to the Telecom domain languages, and demonstrate a use case for\nidentifying the 3rd Generation Partnership Project (3GPP) standard working\ngroups. We consider training the selected models on 3GPP technical documents\n(Tdoc) pertinent to years 2009-2019 and predict the Tdoc categories in years\n2020-2023. The results demonstrate that fine-tuning BERT and RoBERTa model\nachieves 84.6% accuracy, while GPT-2 model achieves 83% in identifying 3GPP\nworking groups. The distilled BERT model with around 50% less parameters\nachieves similar performance as others. This corroborates that fine-tuning\npretrained LLM can effectively identify the categories of Telecom language. The\ndeveloped framework shows a stepping stone towards realizing intent-driven and\nself-evolving wireless networks from Telecom languages, and paves the way for\nthe implementation of generative AI in the Telecom domain.", "published": "2023-06-09 15:44:41", "link": "http://arxiv.org/abs/2306.07933v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GPT-Calls: Enhancing Call Segmentation and Tagging by Generating\n  Synthetic Conversations via Large Language Models", "abstract": "Transcriptions of phone calls are of significant value across diverse fields,\nsuch as sales, customer service, healthcare, and law enforcement. Nevertheless,\nthe analysis of these recorded conversations can be an arduous and\ntime-intensive process, especially when dealing with extended or multifaceted\ndialogues. In this work, we propose a novel method, GPT-distilled Calls\nSegmentation and Tagging (GPT-Calls), for efficient and accurate call\nsegmentation and topic extraction. GPT-Calls is composed of offline and online\nphases. The offline phase is applied once to a given list of topics and\ninvolves generating a distribution of synthetic sentences for each topic using\na GPT model and extracting anchor vectors. The online phase is applied to every\ncall separately and scores the similarity between the transcripted conversation\nand the topic anchors found in the offline phase. Then, time domain analysis is\napplied to the similarity scores to group utterances into segments and tag them\nwith topics. The proposed paradigm provides an accurate and efficient method\nfor call segmentation and topic extraction that does not require labeled data,\nthus making it a versatile approach applicable to various domains. Our\nalgorithm operates in production under Dynamics 365 Sales Conversation\nIntelligence, and our research is based on real sales conversations gathered\nfrom various Dynamics 365 Sales tenants.", "published": "2023-06-09 15:47:22", "link": "http://arxiv.org/abs/2306.07941v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Low-rank Adaptation Method for Wav2vec2-based Fake Audio Detection", "abstract": "Self-supervised speech models are a rapidly developing research topic in fake\naudio detection. Many pre-trained models can serve as feature extractors,\nlearning richer and higher-level speech features. However,when fine-tuning\npre-trained models, there is often a challenge of excessively long training\ntimes and high memory consumption, and complete fine-tuning is also very\nexpensive. To alleviate this problem, we apply low-rank adaptation(LoRA) to the\nwav2vec2 model, freezing the pre-trained model weights and injecting a\ntrainable rank-decomposition matrix into each layer of the transformer\narchitecture, greatly reducing the number of trainable parameters for\ndownstream tasks. Compared with fine-tuning with Adam on the wav2vec2 model\ncontaining 317M training parameters, LoRA achieved similar performance by\nreducing the number of trainable parameters by 198 times.", "published": "2023-06-09 01:43:41", "link": "http://arxiv.org/abs/2306.05617v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Customizing General-Purpose Foundation Models for Medical Report\n  Generation", "abstract": "Medical caption prediction which can be regarded as a task of medical report\ngeneration (MRG), requires the automatic generation of coherent and accurate\ncaptions for the given medical images. However, the scarcity of labelled\nmedical image-report pairs presents great challenges in the development of deep\nand large-scale neural networks capable of harnessing the potential artificial\ngeneral intelligence power like large language models (LLMs). In this work, we\npropose customizing off-the-shelf general-purpose large-scale pre-trained\nmodels, i.e., foundation models (FMs), in computer vision and natural language\nprocessing with a specific focus on medical report generation. Specifically,\nfollowing BLIP-2, a state-of-the-art vision-language pre-training approach, we\nintroduce our encoder-decoder-based MRG model. This model utilizes a\nlightweight query Transformer to connect two FMs: the giant vision Transformer\nEVA-ViT-g and a bilingual LLM trained to align with human intentions (referred\nto as ChatGLM-6B). Furthermore, we conduct ablative experiments on the\ntrainable components of the model to identify the crucial factors for effective\ntransfer learning. Our findings demonstrate that unfreezing EVA-ViT-g to learn\nmedical image representations, followed by parameter-efficient training of\nChatGLM-6B to capture the writing styles of medical reports, is essential for\nachieving optimal results. Our best attempt (PCLmed Team) achieved the 4th and\nthe 2nd, respectively, out of 13 participating teams, based on the BERTScore\nand ROUGE-1 metrics, in the ImageCLEFmedical Caption 2023 Caption Prediction\nTask competition.", "published": "2023-06-09 03:02:36", "link": "http://arxiv.org/abs/2306.05642v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "Privacy Aware Question-Answering System for Online Mental Health Risk\n  Assessment", "abstract": "Social media platforms have enabled individuals suffering from mental\nillnesses to share their lived experiences and find the online support\nnecessary to cope. However, many users fail to receive genuine clinical\nsupport, thus exacerbating their symptoms. Screening users based on what they\npost online can aid providers in administering targeted healthcare and minimize\nfalse positives. Pre-trained Language Models (LMs) can assess users' social\nmedia data and classify them in terms of their mental health risk. We propose a\nQuestion-Answering (QA) approach to assess mental health risk using the\nUnified-QA model on two large mental health datasets. To protect user data, we\nextend Unified-QA by anonymizing the model training process using differential\nprivacy. Our results demonstrate the effectiveness of modeling risk assessment\nas a QA task, specifically for mental health use cases. Furthermore, the\nmodel's performance decreases by less than 1% with the inclusion of\ndifferential privacy. The proposed system's performance is indicative of a\npromising research direction that will lead to the development of privacy-aware\ndiagnostic systems.", "published": "2023-06-09 03:37:49", "link": "http://arxiv.org/abs/2306.05652v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Learning Emotional Representations from Imbalanced Speech Data for\n  Speech Emotion Recognition and Emotional Text-to-Speech", "abstract": "Effective speech emotional representations play a key role in Speech Emotion\nRecognition (SER) and Emotional Text-To-Speech (TTS) tasks. However, emotional\nspeech samples are more difficult and expensive to acquire compared with\nNeutral style speech, which causes one issue that most related works\nunfortunately neglect: imbalanced datasets. Models might overfit to the\nmajority Neutral class and fail to produce robust and effective emotional\nrepresentations. In this paper, we propose an Emotion Extractor to address this\nissue. We use augmentation approaches to train the model and enable it to\nextract effective and generalizable emotional representations from imbalanced\ndatasets. Our empirical results show that (1) for the SER task, the proposed\nEmotion Extractor surpasses the state-of-the-art baseline on three imbalanced\ndatasets; (2) the produced representations from our Emotion Extractor benefit\nthe TTS model, and enable it to synthesize more expressive speech.", "published": "2023-06-09 07:04:56", "link": "http://arxiv.org/abs/2306.05709v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Causality between Sentiment and Cryptocurrency Prices", "abstract": "This study investigates the relationship between narratives conveyed through\nmicroblogging platforms, namely Twitter, and the value of crypto assets. Our\nstudy provides a unique technique to build narratives about cryptocurrency by\ncombining topic modelling of short texts with sentiment analysis. First, we\nused an unsupervised machine learning algorithm to discover the latent topics\nwithin the massive and noisy textual data from Twitter, and then we revealed\n4-5 cryptocurrency-related narratives, including financial investment,\ntechnological advancement related to crypto, financial and political\nregulations, crypto assets, and media coverage. In a number of situations, we\nnoticed a strong link between our narratives and crypto prices. Our work\nconnects the most recent innovation in economics, Narrative Economics, to a new\narea of study that combines topic modelling and sentiment analysis to relate\nconsumer behaviour to narratives.", "published": "2023-06-09 10:40:22", "link": "http://arxiv.org/abs/2306.05803v1", "categories": ["q-fin.CP", "cs.CL", "cs.LG", "I.2.7"], "primary_category": "q-fin.CP"}
{"title": "Can Large Language Models Infer Causation from Correlation?", "abstract": "Causal inference is one of the hallmarks of human intelligence. While the\nfield of CausalNLP has attracted much interest in the recent years, existing\ncausal inference datasets in NLP primarily rely on discovering causality from\nempirical knowledge (e.g., commonsense knowledge). In this work, we propose the\nfirst benchmark dataset to test the pure causal inference skills of large\nlanguage models (LLMs). Specifically, we formulate a novel task Corr2Cause,\nwhich takes a set of correlational statements and determines the causal\nrelationship between the variables. We curate a large-scale dataset of more\nthan 200K samples, on which we evaluate seventeen existing LLMs. Through our\nexperiments, we identify a key shortcoming of LLMs in terms of their causal\ninference skills, and show that these models achieve almost close to random\nperformance on the task. This shortcoming is somewhat mitigated when we try to\nre-purpose LLMs for this skill via finetuning, but we find that these models\nstill fail to generalize -- they can only perform causal inference in\nin-distribution settings when variable names and textual expressions used in\nthe queries are similar to those in the training set, but fail in\nout-of-distribution settings generated by perturbing these queries. Corr2Cause\nis a challenging task for LLMs, and would be helpful in guiding future research\non improving LLMs' pure reasoning skills and generalizability. Our data is at\nhttps://huggingface.co/datasets/causalnlp/corr2cause. Our code is at\nhttps://github.com/causalNLP/corr2cause.", "published": "2023-06-09 12:09:15", "link": "http://arxiv.org/abs/2306.05836v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Encoder-Decoder and Dual-Path Conformer for Comprehensive\n  Feature Learning in Speech Enhancement", "abstract": "Current speech enhancement (SE) research has largely neglected channel\nattention and spatial attention, and encoder-decoder architecture-based\nnetworks have not adequately considered how to provide efficient inputs to the\nintermediate enhancement layer. To address these issues, this paper proposes a\ntime-frequency (T-F) domain SE network (DPCFCS-Net) that incorporates improved\ndensely connected blocks, dual-path modules, convolution-augmented transformers\n(conformers), channel attention, and spatial attention. Compared with previous\nmodels, our proposed model has a more efficient encoder-decoder and can learn\ncomprehensive features. Experimental results on the VCTK+DEMAND dataset\ndemonstrate that our method outperforms existing techniques in SE performance.\nFurthermore, the improved densely connected block and two dimensions attention\nmodule developed in this work are highly adaptable and easily integrated into\nexisting networks.", "published": "2023-06-09 12:52:01", "link": "http://arxiv.org/abs/2306.05861v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Efficient Speech Separation Network Based on Recurrent Fusion Dilated\n  Convolution and Channel Attention", "abstract": "We present an efficient speech separation neural network, ARFDCN, which\ncombines dilated convolutions, multi-scale fusion (MSF), and channel attention\nto overcome the limited receptive field of convolution-based networks and the\nhigh computational cost of transformer-based networks. The suggested network\narchitecture is encoder-decoder based. By using dilated convolutions with\ngradually increasing dilation value to learn local and global features and\nfusing them at adjacent stages, the model can learn rich feature content.\nMeanwhile, by adding channel attention modules to the network, the model can\nextract channel weights, learn more important features, and thus improve its\nexpressive power and robustness. Experimental results indicate that the model\nachieves a decent balance between performance and computational efficiency,\nmaking it a promising alternative to current mainstream models for practical\napplications.", "published": "2023-06-09 13:30:27", "link": "http://arxiv.org/abs/2306.05887v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FinGPT: Open-Source Financial Large Language Models", "abstract": "Large language models (LLMs) have shown the potential of revolutionizing\nnatural language processing tasks in diverse domains, sparking great interest\nin finance. Accessing high-quality financial data is the first challenge for\nfinancial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken\nadvantage of their unique data accumulation, such privileged access calls for\nan open-source alternative to democratize Internet-scale financial data.\n  In this paper, we present an open-source large language model, FinGPT, for\nthe finance sector. Unlike proprietary models, FinGPT takes a data-centric\napproach, providing researchers and practitioners with accessible and\ntransparent resources to develop their FinLLMs. We highlight the importance of\nan automatic data curation pipeline and the lightweight low-rank adaptation\ntechnique in building FinGPT. Furthermore, we showcase several potential\napplications as stepping stones for users, such as robo-advising, algorithmic\ntrading, and low-code development. Through collaborative efforts within the\nopen-source AI4Finance community, FinGPT aims to stimulate innovation,\ndemocratize FinLLMs, and unlock new opportunities in open finance. Two\nassociated code repos are \\url{https://github.com/AI4Finance-Foundation/FinGPT}\nand \\url{https://github.com/AI4Finance-Foundation/FinNLP}", "published": "2023-06-09 16:52:00", "link": "http://arxiv.org/abs/2306.06031v1", "categories": ["q-fin.ST", "cs.CL", "cs.LG", "q-fin.TR"], "primary_category": "q-fin.ST"}
{"title": "Developing Speech Processing Pipelines for Police Accountability", "abstract": "Police body-worn cameras have the potential to improve accountability and\ntransparency in policing. Yet in practice, they result in millions of hours of\nfootage that is never reviewed. We investigate the potential of large\npre-trained speech models for facilitating reviews, focusing on ASR and officer\nspeech detection in footage from traffic stops. Our proposed pipeline includes\ntraining data alignment and filtering, fine-tuning with resource constraints,\nand combining officer speech detection with ASR for a fully automated approach.\nWe find that (1) fine-tuning strongly improves ASR performance on officer\nspeech (WER=12-13%), (2) ASR on officer speech is much more accurate than on\ncommunity member speech (WER=43.55-49.07%), (3) domain-specific tasks like\nofficer speech detection and diarization remain challenging. Our work offers\npractical applications for reviewing body camera footage and general guidance\nfor adapting pre-trained speech models to noisy multi-speaker domains.", "published": "2023-06-09 17:48:58", "link": "http://arxiv.org/abs/2306.06086v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models for Scalable Vector Graphics-Driven\n  Image Understanding", "abstract": "Large language models (LLMs) have made significant advancements in natural\nlanguage understanding. However, through that enormous semantic representation\nthat the LLM has learnt, is it somehow possible for it to understand images as\nwell? This work investigates this question. To enable the LLM to process\nimages, we convert them into a representation given by Scalable Vector Graphics\n(SVG). To study what the LLM can do with this XML-based textual description of\nimages, we test the LLM on three broad computer vision tasks: (i) visual\nreasoning and question answering, (ii) image classification under distribution\nshift, few-shot learning, and (iii) generating new images using visual\nprompting. Even though we do not naturally associate LLMs with any visual\nunderstanding capabilities, our results indicate that the LLM can often do a\ndecent job in many of these tasks, potentially opening new avenues for research\ninto LLMs' ability to understand image data. Our code, data, and models can be\nfound here https://github.com/mu-cai/svg-llm.", "published": "2023-06-09 17:57:01", "link": "http://arxiv.org/abs/2306.06094v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Probing self-supervised speech models for phonetic and phonemic\n  information: a case study in aspiration", "abstract": "Textless self-supervised speech models have grown in capabilities in recent\nyears, but the nature of the linguistic information they encode has not yet\nbeen thoroughly examined. We evaluate the extent to which these models' learned\nrepresentations align with basic representational distinctions made by humans,\nfocusing on a set of phonetic (low-level) and phonemic (more abstract)\ncontrasts instantiated in word-initial stops. We find that robust\nrepresentations of both phonetic and phonemic distinctions emerge in early\nlayers of these models' architectures, and are preserved in the principal\ncomponents of deeper layer representations. Our analyses suggest two sources\nfor this success: some can only be explained by the optimization of the models\non speech data, while some can be attributed to these models' high-dimensional\narchitectures. Our findings show that speech-trained HuBERT derives a low-noise\nand low-dimensional subspace corresponding to abstract phonological\ndistinctions.", "published": "2023-06-09 20:07:22", "link": "http://arxiv.org/abs/2306.06232v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Theory of Unsupervised Speech Recognition", "abstract": "Unsupervised speech recognition (ASR-U) is the problem of learning automatic\nspeech recognition (ASR) systems from unpaired speech-only and text-only\ncorpora. While various algorithms exist to solve this problem, a theoretical\nframework is missing from studying their properties and addressing such issues\nas sensitivity to hyperparameters and training instability. In this paper, we\nproposed a general theoretical framework to study the properties of ASR-U\nsystems based on random matrix theory and the theory of neural tangent kernels.\nSuch a framework allows us to prove various learnability conditions and sample\ncomplexity bounds of ASR-U. Extensive ASR-U experiments on synthetic languages\nwith three classes of transition graphs provide strong empirical evidence for\nour theory (code available at cactuswiththoughts/UnsupASRTheory.git).", "published": "2023-06-09 08:12:27", "link": "http://arxiv.org/abs/2306.07926v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Leveraging text data for causal inference using electronic health\n  records", "abstract": "In studies that rely on data from electronic health records (EHRs),\nunstructured text data such as clinical progress notes offer a rich source of\ninformation about patient characteristics and care that may be missing from\nstructured data. Despite the prevalence of text in clinical research, these\ndata are often ignored for the purposes of quantitative analysis due their\ncomplexity. This paper presents a unified framework for leveraging text data to\nsupport causal inference with electronic health data at multiple stages of\nanalysis. In particular, we consider how natural language processing and\nstatistical text analysis can be combined with standard inferential techniques\nto address common challenges due to missing data, confounding bias, and\ntreatment effect heterogeneity. Through an application to a recent EHR study\ninvestigating the effects of a non-randomized medical intervention on patient\noutcomes, we show how incorporating text data in a traditional matching\nanalysis can help strengthen the validity of an estimated treatment effect and\nidentify patient subgroups that may benefit most from treatment. We believe\nthese methods have the potential to expand the scope of secondary analysis of\nclinical data to domains where structured EHR data is limited, such as in\ndeveloping countries. To this end, we provide code and open-source replication\nmaterials to encourage adoption and broader exploration of these techniques in\nclinical research.", "published": "2023-06-09 16:06:02", "link": "http://arxiv.org/abs/2307.03687v2", "categories": ["cs.CL", "stat.AP", "stat.ME"], "primary_category": "cs.CL"}
{"title": "Exploring the Responses of Large Language Models to Beginner\n  Programmers' Help Requests", "abstract": "Background and Context: Over the past year, large language models (LLMs) have\ntaken the world by storm. In computing education, like in other walks of life,\nmany opportunities and threats have emerged as a consequence.\n  Objectives: In this article, we explore such opportunities and threats in a\nspecific area: responding to student programmers' help requests. More\nspecifically, we assess how good LLMs are at identifying issues in problematic\ncode that students request help on.\n  Method: We collected a sample of help requests and code from an online\nprogramming course. We then prompted two different LLMs (OpenAI Codex and\nGPT-3.5) to identify and explain the issues in the students' code and assessed\nthe LLM-generated answers both quantitatively and qualitatively.\n  Findings: GPT-3.5 outperforms Codex in most respects. Both LLMs frequently\nfind at least one actual issue in each student program (GPT-3.5 in 90% of the\ncases). Neither LLM excels at finding all the issues (GPT-3.5 finding them 57%\nof the time). False positives are common (40% chance for GPT-3.5). The advice\nthat the LLMs provide on the issues is often sensible. The LLMs perform better\non issues involving program logic rather than on output formatting. Model\nsolutions are frequently provided even when the LLM is prompted not to. LLM\nresponses to prompts in a non-English language are only slightly worse than\nresponses to English prompts.\n  Implications: Our results continue to highlight the utility of LLMs in\nprogramming education. At the same time, the results highlight the\nunreliability of LLMs: LLMs make some of the same mistakes that students do,\nperhaps especially when formatting output as required by automated assessment\nsystems. Our study informs teachers interested in using LLMs as well as future\nefforts to customize LLMs for the needs of programming education.", "published": "2023-06-09 07:19:43", "link": "http://arxiv.org/abs/2306.05715v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.SE"], "primary_category": "cs.CY"}
{"title": "Challenges and Opportunities for the Design of Smart Speakers", "abstract": "Advances in voice technology and voice user interfaces (VUIs) -- such as\nAlexa, Siri, and Google Home -- have opened up the potential for many new types\nof interaction. However, despite the potential of these devices reflected by\nthe growing market and body of VUI research, there is a lingering sense that\nthe technology is still underused. In this paper, we conducted a systematic\nliterature review of 35 papers to identify and synthesize 127 VUI design\nguidelines into five themes. Additionally, we conducted semi-structured\ninterviews with 15 smart speaker users to understand their use and non-use of\nthe technology. From the interviews, we distill four design challenges that\ncontribute the most to non-use. Based on their (non-)use, we identify four\nopportunity spaces for designers to explore such as focusing on information\nsupport while multitasking (cooking, driving, childcare, etc), incorporating\nusers' mental models for smart speakers, and integrating calm design\nprinciples.", "published": "2023-06-09 08:18:58", "link": "http://arxiv.org/abs/2306.05741v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.RO"], "primary_category": "cs.HC"}
{"title": "Acoustic Scene Clustering Using Joint Optimization of Deep Embedding\n  Learning and Clustering Iteration", "abstract": "Recent efforts have been made on acoustic scene classification in the audio\nsignal processing community. In contrast, few studies have been conducted on\nacoustic scene clustering, which is a newly emerging problem. Acoustic scene\nclustering aims at merging the audio recordings of the same class of acoustic\nscene into a single cluster without using prior information and training\nclassifiers. In this study, we propose a method for acoustic scene clustering\nthat jointly optimizes the procedures of feature learning and clustering\niteration. In the proposed method, the learned feature is a deep embedding that\nis extracted from a deep convolutional neural network (CNN), while the\nclustering algorithm is the agglomerative hierarchical clustering (AHC). We\nformulate a unified loss function for integrating and optimizing these two\nprocedures. Various features and methods are compared. The experimental results\ndemonstrate that the proposed method outperforms other unsupervised methods in\nterms of the normalized mutual information and the clustering accuracy. In\naddition, the deep embedding outperforms many state-of-the-art features.", "published": "2023-06-09 01:51:47", "link": "http://arxiv.org/abs/2306.05621v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Domestic Activities Classification from Audio Recordings Using\n  Multi-scale Dilated Depthwise Separable Convolutional Network", "abstract": "Domestic activities classification (DAC) from audio recordings aims at\nclassifying audio recordings into pre-defined categories of domestic\nactivities, which is an effective way for estimation of daily activities\nperformed in home environment. In this paper, we propose a method for DAC from\naudio recordings using a multi-scale dilated depthwise separable convolutional\nnetwork (DSCN). The DSCN is a lightweight neural network with small size of\nparameters and thus suitable to be deployed in portable terminals with limited\ncomputing resources. To expand the receptive field with the same size of DSCN's\nparameters, dilated convolution, instead of normal convolution, is used in the\nDSCN for further improving the DSCN's performance. In addition, the embeddings\nof various scales learned by the dilated DSCN are concatenated as a multi-scale\nembedding for representing property differences among various classes of\ndomestic activities. Evaluated on a public dataset of the Task 5 of the 2018\nchallenge on Detection and Classification of Acoustic Scenes and Events\n(DCASE-2018), the results show that: both dilated convolution and multi-scale\nembedding contribute to the performance improvement of the proposed method; and\nthe proposed method outperforms the methods based on state-of-the-art\nlightweight network in terms of classification accuracy.", "published": "2023-06-09 02:15:33", "link": "http://arxiv.org/abs/2306.05624v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker Embeddings as Individuality Proxy for Voice Stress Detection", "abstract": "Since the mental states of the speaker modulate speech, stress introduced by\ncognitive or physical loads could be detected in the voice. The existing voice\nstress detection benchmark has shown that the audio embeddings extracted from\nthe Hybrid BYOL-S self-supervised model perform well. However, the benchmark\nonly evaluates performance separately on each dataset, but does not evaluate\nperformance across the different types of stress and different languages.\nMoreover, previous studies found strong individual differences in stress\nsusceptibility. This paper presents the design and development of voice stress\ndetection, trained on more than 100 speakers from 9 language groups and five\ndifferent types of stress. We address individual variabilities in voice stress\nanalysis by adding speaker embeddings to the hybrid BYOL-S features. The\nproposed method significantly improves voice stress detection performance with\nan input audio length of only 3-5 seconds.", "published": "2023-06-09 14:11:07", "link": "http://arxiv.org/abs/2306.05915v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Boosting Fast and High-Quality Speech Synthesis with Linear Diffusion", "abstract": "Denoising Diffusion Probabilistic Models have shown extraordinary ability on\nvarious generative tasks. However, their slow inference speed renders them\nimpractical in speech synthesis. This paper proposes a linear diffusion model\n(LinDiff) based on an ordinary differential equation to simultaneously reach\nfast inference and high sample quality. Firstly, we employ linear interpolation\nbetween the target and noise to design a diffusion sequence for training, while\npreviously the diffusion path that links the noise and target is a curved\nsegment. When decreasing the number of sampling steps (i.e., the number of line\nsegments used to fit the path), the ease of fitting straight lines compared to\ncurves allows us to generate higher quality samples from a random noise with\nfewer iterations. Secondly, to reduce computational complexity and achieve\neffective global modeling of noisy speech, LinDiff employs a patch-based\nprocessing approach that partitions the input signal into small patches. The\npatch-wise token leverages Transformer architecture for effective modeling of\nglobal information. Adversarial training is used to further improve the sample\nquality with decreased sampling steps. We test proposed method with speech\nsynthesis conditioned on acoustic feature (Mel-spectrograms). Experimental\nresults verify that our model can synthesize high-quality speech even with only\none diffusion step. Both subjective and objective evaluations demonstrate that\nour model can synthesize speech of a quality comparable to that of\nautoregressive models with faster synthesis speed (3 diffusion steps).", "published": "2023-06-09 07:02:43", "link": "http://arxiv.org/abs/2306.05708v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Reconstructing Human Expressiveness in Piano Performances with a\n  Transformer Network", "abstract": "Capturing intricate and subtle variations in human expressiveness in music\nperformance using computational approaches is challenging. In this paper, we\npropose a novel approach for reconstructing human expressiveness in piano\nperformance with a multi-layer bi-directional Transformer encoder. To address\nthe needs for large amounts of accurately captured and score-aligned\nperformance data in training neural networks, we use transcribed scores\nobtained from an existing transcription model to train our model. We integrate\npianist identities to control the sampling process and explore the ability of\nour system to model variations in expressiveness for different pianists. The\nsystem is evaluated through statistical analysis of generated expressive\nperformances and a listening test. Overall, the results suggest that our method\nachieves state-of-the-art in generating human-like piano performances from\ntranscribed scores, while fully and consistently reconstructing human\nexpressiveness poses further challenges.", "published": "2023-06-09 17:05:53", "link": "http://arxiv.org/abs/2306.06040v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Everybody Compose: Deep Beats To Music", "abstract": "This project presents a deep learning approach to generate monophonic\nmelodies based on input beats, allowing even amateurs to create their own music\ncompositions. Three effective methods - LSTM with Full Attention, LSTM with\nLocal Attention, and Transformer with Relative Position Representation - are\nproposed for this novel task, providing great variation, harmony, and structure\nin the generated music. This project allows anyone to compose their own music\nby tapping their keyboards or ``recoloring'' beat sequences from existing\nworks.", "published": "2023-06-09 22:24:05", "link": "http://arxiv.org/abs/2306.06284v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Frame-level Classifier for Word Timings with Non-peaky CTC in\n  End-to-End Automatic Speech Recognition", "abstract": "End-to-end (E2E) systems have shown comparable performance to hybrid systems\nfor automatic speech recognition (ASR). Word timings, as a by-product of ASR,\nare essential in many applications, especially for subtitling and\ncomputer-aided pronunciation training. In this paper, we improve the\nframe-level classifier for word timings in E2E system by introducing label\npriors in connectionist temporal classification (CTC) loss, which is adopted\nfrom prior works, and combining low-level Mel-scale filter banks with\nhigh-level ASR encoder output as input feature. On the internal Chinese corpus,\nthe proposed method achieves 95.68%/94.18% compared to the hybrid system\n93.0%/90.22% on the word timing accuracy metrics. It also surpass a previous\nE2E approach with an absolute increase of 4.80%/8.02% on the metrics on 7\nlanguages. In addition, we further improve word timing accuracy by delaying CTC\npeaks with frame-wise knowledge distillation, though only experimenting on\nLibriSpeech.", "published": "2023-06-09 03:36:00", "link": "http://arxiv.org/abs/2306.07949v1", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
{"title": "HRTF upsampling with a generative adversarial network using a gnomonic\n  equiangular projection", "abstract": "An individualised head-related transfer function (HRTF) is very important for\ncreating realistic virtual reality (VR) and augmented reality (AR)\nenvironments. However, acoustically measuring high-quality HRTFs requires\nexpensive equipment and an acoustic lab setting. To overcome these limitations\nand to make this measurement more efficient HRTF upsampling has been exploited\nin the past where a high-resolution HRTF is created from a low-resolution one.\nThis paper demonstrates how generative adversarial networks (GANs) can be\napplied to HRTF upsampling. We propose a novel approach that transforms the\nHRTF data for direct use with a convolutional super-resolution generative\nadversarial network (SRGAN). This new approach is benchmarked against three\nbaselines: barycentric upsampling, spherical harmonic (SH) upsampling and an\nHRTF selection approach. Experimental results show that the proposed method\noutperforms all three baselines in terms of log-spectral distortion (LSD) and\nlocalisation performance using perceptual models when the input HRTF is sparse\n(less than 20 measured positions).", "published": "2023-06-09 11:05:09", "link": "http://arxiv.org/abs/2306.05812v2", "categories": ["eess.AS", "cs.CV", "cs.HC", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
