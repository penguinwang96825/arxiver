{"title": "Revisiting Transformer-based Models for Long Document Classification", "abstract": "The recent literature in text classification is biased towards short text\nsequences (e.g., sentences or paragraphs). In real-world applications,\nmulti-page multi-paragraph documents are common and they cannot be efficiently\nencoded by vanilla Transformer-based models. We compare different\nTransformer-based Long Document Classification (TrLDC) approaches that aim to\nmitigate the computational overhead of vanilla transformers to encode much\nlonger text, namely sparse attention and hierarchical encoding methods. We\nexamine several aspects of sparse attention (e.g., size of local attention\nwindow, use of global attention) and hierarchical (e.g., document splitting\nstrategy) transformers on four document classification datasets covering\ndifferent domains. We observe a clear benefit from being able to process longer\ntext, and, based on our results, we derive practical advice of applying\nTransformer-based models on long document classification tasks.", "published": "2022-04-14 00:44:36", "link": "http://arxiv.org/abs/2204.06683v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model", "abstract": "We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language\nmodel trained on the Pile, whose weights will be made freely and openly\navailable to the public through a permissive license. It is, to the best of our\nknowledge, the largest dense autoregressive model that has publicly available\nweights at the time of submission. In this work, we describe \\model{}'s\narchitecture and training and evaluate its performance on a range of\nlanguage-understanding, mathematics, and knowledge-based tasks. We find that\nGPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in\nperformance when evaluated five-shot than similarly sized GPT-3 and FairSeq\nmodels. We open-source the training and evaluation code, as well as the model\nweights, at https://github.com/EleutherAI/gpt-neox.", "published": "2022-04-14 04:00:27", "link": "http://arxiv.org/abs/2204.06745v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Generalize to More: Continuous Semantic Augmentation for\n  Neural Machine Translation", "abstract": "The principal task in supervised neural machine translation (NMT) is to learn\nto generate target sentences conditioned on the source inputs from a set of\nparallel sentence pairs, and thus produce a model capable of generalizing to\nunseen instances. However, it is commonly observed that the generalization\nperformance of the model is highly influenced by the amount of parallel data\nused in training. Although data augmentation is widely used to enrich the\ntraining data, conventional methods with discrete manipulations fail to\ngenerate diverse and faithful training samples. In this paper, we present a\nnovel data augmentation paradigm termed Continuous Semantic Augmentation\n(CsaNMT), which augments each training instance with an adjacency semantic\nregion that could cover adequate variants of literal expression under the same\nmeaning. We conduct extensive experiments on both rich-resource and\nlow-resource settings involving various language pairs, including WMT14\nEnglish-{German,French}, NIST Chinese-English and multiple low-resource IWSLT\ntranslation tasks. The provided empirical evidences show that CsaNMT sets a new\nlevel of performance among existing augmentation techniques, improving on the\nstate-of-the-art by a large margin. The core codes are contained in Appendix E.", "published": "2022-04-14 08:16:28", "link": "http://arxiv.org/abs/2204.06812v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Gender Debiasing Affects Internal Model Representations, and Why It\n  Matters", "abstract": "Common studies of gender bias in NLP focus either on extrinsic bias measured\nby model performance on a downstream task or on intrinsic bias found in models'\ninternal representations. However, the relationship between extrinsic and\nintrinsic bias is relatively unknown. In this work, we illuminate this\nrelationship by measuring both quantities together: we debias a model during\ndownstream fine-tuning, which reduces extrinsic bias, and measure the effect on\nintrinsic bias, which is operationalized as bias extractability with\ninformation-theoretic probing. Through experiments on two tasks and multiple\nbias metrics, we show that our intrinsic bias metric is a better indicator of\ndebiasing than (a contextual adaptation of) the standard WEAT metric, and can\nalso expose cases of superficial debiasing. Our framework provides a\ncomprehensive perspective on bias in NLP models, which can be applied to deploy\nNLP systems in a more informed manner. Our code and model checkpoints are\npublicly available.", "published": "2022-04-14 08:54:15", "link": "http://arxiv.org/abs/2204.06827v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Does BERT really agree ? Fine-grained Analysis of Lexical Dependence on\n  a Syntactic Task", "abstract": "Although transformer-based Neural Language Models demonstrate impressive\nperformance on a variety of tasks, their generalization abilities are not well\nunderstood. They have been shown to perform strongly on subject-verb number\nagreement in a wide array of settings, suggesting that they learned to track\nsyntactic dependencies during their training even without explicit supervision.\nIn this paper, we examine the extent to which BERT is able to perform\nlexically-independent subject-verb number agreement (NA) on targeted syntactic\ntemplates. To do so, we disrupt the lexical patterns found in naturally\noccurring stimuli for each targeted structure in a novel fine-grained analysis\nof BERT's behavior. Our results on nonce sentences suggest that the model\ngeneralizes well for simple templates, but fails to perform\nlexically-independent syntactic generalization when as little as one attractor\nis present.", "published": "2022-04-14 11:33:15", "link": "http://arxiv.org/abs/2204.06889v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Challenges for Open-domain Targeted Sentiment Analysis", "abstract": "Since previous studies on open-domain targeted sentiment analysis are limited\nin dataset domain variety and sentence level, we propose a novel dataset\nconsisting of 6,013 human-labeled data to extend the data domains in topics of\ninterest and document level. Furthermore, we offer a nested target annotation\nschema to extract the complete sentiment information in documents, boosting the\npracticality and effectiveness of open-domain targeted sentiment analysis.\nMoreover, we leverage the pre-trained model BART in a sequence-to-sequence\ngeneration method for the task. Benchmark results show that there exists large\nroom for improvement of open-domain targeted sentiment analysis. Meanwhile,\nexperiments have shown that challenges remain in the effective use of\nopen-domain data, long documents, the complexity of target structure, and\ndomain variances.", "published": "2022-04-14 11:44:02", "link": "http://arxiv.org/abs/2204.06893v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Visual Dialogue Models Do Scorekeeping? Exploring How Dialogue\n  Representations Incrementally Encode Shared Knowledge", "abstract": "Cognitively plausible visual dialogue models should keep a mental scoreboard\nof shared established facts in the dialogue context. We propose a theory-based\nevaluation method for investigating to what degree models pretrained on the\nVisDial dataset incrementally build representations that appropriately do\nscorekeeping. Our conclusion is that the ability to make the distinction\nbetween shared and privately known statements along the dialogue is moderately\npresent in the analysed models, but not always incrementally consistent, which\nmay partially be due to the limited need for grounding interactions in the\noriginal task.", "published": "2022-04-14 13:52:11", "link": "http://arxiv.org/abs/2204.06970v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XLMRQA: Open-Domain Question Answering on Vietnamese Wikipedia-based\n  Textual Knowledge Source", "abstract": "Question answering (QA) is a natural language understanding task within the\nfields of information retrieval and information extraction that has attracted\nmuch attention from the computational linguistics and artificial intelligence\nresearch community in recent years because of the strong development of machine\nreading comprehension-based models. A reader-based QA system is a high-level\nsearch engine that can find correct answers to queries or questions in\nopen-domain or domain-specific texts using machine reading comprehension (MRC)\ntechniques. The majority of advancements in data resources and machine-learning\napproaches in the MRC and QA systems especially are developed significantly in\ntwo resource-rich languages such as English and Chinese. A low-resource\nlanguage like Vietnamese has witnessed a scarcity of research on QA systems.\nThis paper presents XLMRQA, the first Vietnamese QA system using a supervised\ntransformer-based reader on the Wikipedia-based textual knowledge source (using\nthe UIT-ViQuAD corpus), outperforming the two robust QA systems using deep\nneural network models: DrQA and BERTserini with 24.46% and 6.28%, respectively.\nFrom the results obtained on the three systems, we analyze the influence of\nquestion types on the performance of the QA systems.", "published": "2022-04-14 14:54:33", "link": "http://arxiv.org/abs/2204.07002v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Anti-Asian Hate Speech Detection via Data Augmented Semantic Relation\n  Inference", "abstract": "With the spreading of hate speech on social media in recent years, automatic\ndetection of hate speech is becoming a crucial task and has attracted attention\nfrom various communities. This task aims to recognize online posts (e.g.,\ntweets) that contain hateful information. The peculiarities of languages in\nsocial media, such as short and poorly written content, lead to the difficulty\nof learning semantics and capturing discriminative features of hate speech.\nPrevious studies have utilized additional useful resources, such as sentiment\nhashtags, to improve the performance of hate speech detection. Hashtags are\nadded as input features serving either as sentiment-lexicons or extra context\ninformation. However, our close investigation shows that directly leveraging\nthese features without considering their context may introduce noise to\nclassifiers. In this paper, we propose a novel approach to leverage sentiment\nhashtags to enhance hate speech detection in a natural language inference\nframework. We design a novel framework SRIC that simultaneously performs two\ntasks: (1) semantic relation inference between online posts and sentiment\nhashtags, and (2) sentiment classification on these posts. The semantic\nrelation inference aims to encourage the model to encode sentiment-indicative\ninformation into representations of online posts. We conduct extensive\nexperiments on two real-world datasets and demonstrate the effectiveness of our\nproposed framework compared with state-of-the-art representation learning\nmodels.", "published": "2022-04-14 15:03:35", "link": "http://arxiv.org/abs/2204.07010v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Label Semantic Aware Pre-training for Few-shot Text Classification", "abstract": "In text classification tasks, useful information is encoded in the label\nnames. Label semantic aware systems have leveraged this information for\nimproved text classification performance during fine-tuning and prediction.\nHowever, use of label-semantics during pre-training has not been extensively\nexplored. We therefore propose Label Semantic Aware Pre-training (LSAP) to\nimprove the generalization and data efficiency of text classification systems.\nLSAP incorporates label semantics into pre-trained generative models (T5 in our\ncase) by performing secondary pre-training on labeled sentences from a variety\nof domains. As domain-general pre-training requires large amounts of data, we\ndevelop a filtering and labeling pipeline to automatically create\nsentence-label pairs from unlabeled text. We perform experiments on intent\n(ATIS, Snips, TOPv2) and topic classification (AG News, Yahoo! Answers). LSAP\nobtains significant accuracy improvements over state-of-the-art models for\nfew-shot text classification while maintaining performance comparable to state\nof the art in high-resource settings.", "published": "2022-04-14 17:33:34", "link": "http://arxiv.org/abs/2204.07128v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FREDA: Flexible Relation Extraction Data Annotation", "abstract": "To effectively train accurate Relation Extraction models, sufficient and\nproperly labeled data is required. Adequately labeled data is difficult to\nobtain and annotating such data is a tricky undertaking. Previous works have\nshown that either accuracy has to be sacrificed or the task is extremely\ntime-consuming, if done accurately. We are proposing an approach in order to\nproduce high-quality datasets for the task of Relation Extraction quickly.\nNeural models, trained to do Relation Extraction on the created datasets,\nachieve very good results and generalize well to other datasets. In our study,\nwe were able to annotate 10,022 sentences for 19 relations in a reasonable\namount of time, and trained a commonly used baseline model for each relation.", "published": "2022-04-14 17:57:53", "link": "http://arxiv.org/abs/2204.07150v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Fake News Detection: Are current models \"fact-checking\" or\n  \"gut-checking\"?", "abstract": "Automatic fake news detection models are ostensibly based on logic, where the\ntruth of a claim made in a headline can be determined by supporting or refuting\nevidence found in a resulting web query. These models are believed to be\nreasoning in some way; however, it has been shown that these same results, or\nbetter, can be achieved without considering the claim at all -- only the\nevidence. This implies that other signals are contained within the examined\nevidence, and could be based on manipulable factors such as emotion, sentiment,\nor part-of-speech (POS) frequencies, which are vulnerable to adversarial\ninputs. We neutralize some of these signals through multiple forms of both\nneural and non-neural pre-processing and style transfer, and find that this\nflattening of extraneous indicators can induce the models to actually require\nboth claims and evidence to perform well. We conclude with the construction of\na model using emotion vectors built off a lexicon and passed through an\n\"emotional attention\" mechanism to appropriately weight certain emotions. We\nprovide quantifiable results that prove our hypothesis that manipulable\nfeatures are being used for fact-checking.", "published": "2022-04-14 21:05:37", "link": "http://arxiv.org/abs/2204.07229v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constructing Open Cloze Tests Using Generation and Discrimination\n  Capabilities of Transformers", "abstract": "This paper presents the first multi-objective transformer model for\nconstructing open cloze tests that exploits generation and discrimination\ncapabilities to improve performance. Our model is further enhanced by tweaking\nits loss function and applying a post-processing re-ranking algorithm that\nimproves overall test structure. Experiments using automatic and human\nevaluation show that our approach can achieve up to 82% accuracy according to\nexperts, outperforming previous work and baselines. We also release a\ncollection of high-quality open cloze tests along with sample system output and\nhuman annotations that can serve as a future benchmark.", "published": "2022-04-14 21:16:05", "link": "http://arxiv.org/abs/2204.07237v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Schema Graph Fusion Network for Multi-Domain Dialogue State\n  Tracking", "abstract": "Dialogue State Tracking (DST) aims to keep track of users' intentions during\nthe course of a conversation. In DST, modelling the relations among domains and\nslots is still an under-studied problem. Existing approaches that have\nconsidered such relations generally fall short in: (1) fusing prior slot-domain\nmembership relations and dialogue-aware dynamic slot relations explicitly, and\n(2) generalizing to unseen domains. To address these issues, we propose a novel\n\\textbf{D}ynamic \\textbf{S}chema \\textbf{G}raph \\textbf{F}usion\n\\textbf{Net}work (\\textbf{DSGFNet}), which generates a dynamic schema graph to\nexplicitly fuse the prior slot-domain membership relations and dialogue-aware\ndynamic slot relations. It also uses the schemata to facilitate knowledge\ntransfer to new domains. DSGFNet consists of a dialogue utterance encoder, a\nschema graph encoder, a dialogue-aware schema graph evolving network, and a\nschema graph enhanced dialogue state decoder. Empirical results on benchmark\ndatasets (i.e., SGD, MultiWOZ2.1, and MultiWOZ2.2), show that DSGFNet\noutperforms existing methods.", "published": "2022-04-14 00:04:13", "link": "http://arxiv.org/abs/2204.06677v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Multi-label topic classification for COVID-19 literature with Bioformer", "abstract": "We describe Bioformer team's participation in the multi-label topic\nclassification task for COVID-19 literature (track 5 of BioCreative VII). Topic\nclassification is performed using different BERT models (BioBERT, PubMedBERT,\nand Bioformer). We formulate the topic classification task as a sentence pair\nclassification problem, where the title is the first sentence, and the abstract\nis the second sentence. Our results show that Bioformer outperforms BioBERT and\nPubMedBERT in this task. Compared to the baseline results, our best model\nincreased micro, macro, and instance-based F1 score by 8.8%, 15.5%, 7.4%,\nrespectively. Bioformer achieved the highest micro F1 and macro F1 scores in\nthis challenge. In post-challenge experiments, we found that pretraining of\nBioformer on COVID-19 articles further improves the performance.", "published": "2022-04-14 05:24:54", "link": "http://arxiv.org/abs/2204.06758v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Unified Multi-task Learning Framework for Multi-goal Conversational\n  Recommender Systems", "abstract": "Recent years witnessed several advances in developing multi-goal\nconversational recommender systems (MG-CRS) that can proactively attract users'\ninterests and naturally lead user-engaged dialogues with multiple\nconversational goals and diverse topics. Four tasks are often involved in\nMG-CRS, including Goal Planning, Topic Prediction, Item Recommendation, and\nResponse Generation. Most existing studies address only some of these tasks. To\nhandle the whole problem of MG-CRS, modularized frameworks are adopted where\neach task is tackled independently without considering their interdependencies.\nIn this work, we propose a novel Unified MultI-goal conversational recommeNDer\nsystem, namely UniMIND. In specific, we unify these four tasks with different\nformulations into the same sequence-to-sequence (Seq2Seq) paradigm.\nPrompt-based learning strategies are investigated to endow the unified model\nwith the capability of multi-task learning. Finally, the overall learning and\ninference procedure consists of three stages, including multi-task learning,\nprompt-based tuning, and inference. Experimental results on two MG-CRS\nbenchmarks (DuRecDial and TG-ReDial) show that UniMIND achieves\nstate-of-the-art performance on all tasks with a unified model. Extensive\nanalyses and discussions are provided for shedding some new perspectives for\nMG-CRS.", "published": "2022-04-14 12:31:27", "link": "http://arxiv.org/abs/2204.06923v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Handling sign language transcription system with the computer-friendly\n  numerical multilabels", "abstract": "This paper presents our recent developments in the automatic processing of\nsign language corpora using the Hamburg Sign Language Annotation System\n(HamNoSys). We designed an automated tool to convert HamNoSys annotations into\nnumerical labels for defined initial features of body and hand positions. Our\nproposed numerical multilabels greatly simplify annotations' structure without\nsignificant loss of gloss meaning. These numerical multilabels can potentially\nbe used to feed the machine learning models, which would accelerate the\ndevelopment of vision-based sign language recognition. In addition, this tool\ncan assist experts in the annotation process and help identify semantic errors.\nThe code and sample annotations are publicly available at\n\\url{https://github.com/hearai/parse-hamnosys}.", "published": "2022-04-14 12:33:33", "link": "http://arxiv.org/abs/2204.06924v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rows from Many Sources: Enriching row completions from Wikidata with a\n  pre-trained Language Model", "abstract": "Row completion is the task of augmenting a given table of text and numbers\nwith additional, relevant rows. The task divides into two steps: subject\nsuggestion, the task of populating the main column; and gap filling, the task\nof populating the remaining columns. We present state-of-the-art results for\nsubject suggestion and gap filling measured on a standard benchmark\n(WikiTables). Our idea is to solve this task by harmoniously combining\nknowledge base table interpretation and free text generation. We interpret the\ntable using the knowledge base to suggest new rows and generate metadata like\nheaders through property linking. To improve candidate diversity, we synthesize\nadditional rows using free text generation via GPT-3, and crucially, we exploit\nthe metadata we interpret to produce better prompts for text generation.\nFinally, we verify that the additional synthesized content can be linked to the\nknowledge base or a trusted web source such as Wikipedia.", "published": "2022-04-14 15:11:52", "link": "http://arxiv.org/abs/2204.07014v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dialogue Strategy Adaptation to New Action Sets Using Multi-dimensional\n  Modelling", "abstract": "A major bottleneck for building statistical spoken dialogue systems for new\ndomains and applications is the need for large amounts of training data. To\naddress this problem, we adopt the multi-dimensional approach to dialogue\nmanagement and evaluate its potential for transfer learning. Specifically, we\nexploit pre-trained task-independent policies to speed up training for an\nextended task-specific action set, in which the single summary action for\nrequesting a slot is replaced by multiple slot-specific request actions. Policy\noptimisation and evaluation experiments using an agenda-based user simulator\nshow that with limited training data, much better performance levels can be\nachieved when using the proposed multi-dimensional adaptation method. We\nconfirm this improvement in a crowd-sourced human user evaluation of our spoken\ndialogue system, comparing partially trained policies. The multi-dimensional\nsystem (with adaptation on limited training data in the target scenario)\noutperforms the one-dimensional baseline (without adaptation on the same amount\nof training data) by 7% perceived success rate.", "published": "2022-04-14 16:26:22", "link": "http://arxiv.org/abs/2204.07082v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A* shortest string decoding for non-idempotent semirings", "abstract": "The single shortest path algorithm is undefined for weighted finite-state\nautomata over non-idempotent semirings because such semirings do not guarantee\nthe existence of a shortest path. However, in non-idempotent semirings\nadmitting an order satisfying a monotonicity condition (such as the plus-times\nor log semirings), the notion of shortest string is well-defined. We describe\nan algorithm which finds the shortest string for a weighted non-deterministic\nautomaton over such semirings using the backwards shortest distance of an\nequivalent deterministic automaton (DFA) as a heuristic for A* search performed\nover a companion idempotent semiring, which is proven to return the shortest\nstring. While there may be exponentially more states in the DFA, this algorithm\nneeds to visit only a small fraction of them if determinization is performed\n\"on the fly\".", "published": "2022-04-14 21:15:34", "link": "http://arxiv.org/abs/2204.07236v2", "categories": ["cs.FL", "cs.CL"], "primary_category": "cs.FL"}
{"title": "The Art of Prompting: Event Detection based on Type Specific Prompts", "abstract": "We compare various forms of prompts to represent event types and develop a\nunified framework to incorporate the event type specific prompts for\nsupervised, few-shot, and zero-shot event detection. The experimental results\ndemonstrate that a well-defined and comprehensive event type prompt can\nsignificantly improve the performance of event detection, especially when the\nannotated data is scarce (few-shot event detection) or not available (zero-shot\nevent detection). By leveraging the semantics of event types, our unified\nframework shows up to 24.3\\% F-score gain over the previous state-of-the-art\nbaselines.", "published": "2022-04-14 21:28:50", "link": "http://arxiv.org/abs/2204.07241v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Qtrade AI at SemEval-2022 Task 11: An Unified Framework for Multilingual\n  NER Task", "abstract": "This paper describes our system, which placed third in the Multilingual Track\n(subtask 11), fourth in the Code-Mixed Track (subtask 12), and seventh in the\nChinese Track (subtask 9) in the SemEval 2022 Task 11: MultiCoNER Multilingual\nComplex Named Entity Recognition. Our system's key contributions are as\nfollows: 1) For multilingual NER tasks, we offer an unified framework with\nwhich one can easily execute single-language or multilingual NER tasks, 2) for\nlow-resource code-mixed NER task, one can easily enhance his or her dataset\nthrough implementing several simple data augmentation methods and 3) for\nChinese tasks, we propose a model that can capture Chinese lexical semantic,\nlexical border, and lexical graph structural information. Finally, our system\nachieves macro-f1 scores of 77.66, 84.35, and 74.00 on subtasks 11, 12, and 9,\nrespectively, during the testing phase.", "published": "2022-04-14 07:51:36", "link": "http://arxiv.org/abs/2204.07459v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Usage-based learning of grammatical categories", "abstract": "Human languages use a wide range of grammatical categories to constrain which\nwords or phrases can fill certain slots in grammatical patterns and to express\nadditional meanings, such as tense or aspect, through morpho-syntactic means.\nThese grammatical categories, which are most often language-specific and\nchanging over time, are difficult to define and learn. This paper raises the\nquestion how these categories can be acquired and where they have come from. We\nexplore a usage-based approach. This means that categories and grammatical\nconstructions are selected and aligned by their success in language\ninteractions. We report on a multi-agent experiment in which agents are endowed\nwith mechanisms for understanding and producing utterances as well as\nmechanisms for expanding their inventories using a meta-level learning process\nbased on pro- and anti-unification. We show that a categorial type network\nwhich has scores based on the success in a language interaction leads to the\nspontaneous formation of grammatical categories in tandem with the formation of\ngrammatical patterns.", "published": "2022-04-14 07:44:25", "link": "http://arxiv.org/abs/2204.10201v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Top-K Decoding for Non-Autoregressive Semantic Parsing via\n  Intent Conditioning", "abstract": "Semantic parsing (SP) is a core component of modern virtual assistants like\nGoogle Assistant and Amazon Alexa. While sequence-to-sequence-based\nauto-regressive (AR) approaches are common for conversational semantic parsing,\nrecent studies employ non-autoregressive (NAR) decoders and reduce inference\nlatency while maintaining competitive parsing quality. However, a major\ndrawback of NAR decoders is the difficulty of generating top-k (i.e., k-best)\noutputs with approaches such as beam search. To address this challenge, we\npropose a novel NAR semantic parser that introduces intent conditioning on the\ndecoder. Inspired by the traditional intent and slot tagging parsers, we\ndecouple the top-level intent prediction from the rest of a parse. As the\ntop-level intent largely governs the syntax and semantics of a parse, the\nintent conditioning allows the model to better control beam search and improves\nthe quality and diversity of top-k outputs. We introduce a hybrid\nteacher-forcing approach to avoid training and inference mismatch. We evaluate\nthe proposed NAR on conversational SP datasets, TOP & TOPv2. Like the existing\nNAR models, we maintain the O(1) decoding time complexity while generating more\ndiverse outputs and improving the top-3 exact match (EM) by 2.4 points. In\ncomparison with AR models, our model speeds up beam search inference by 6.7\ntimes on CPU with competitive top-k EM.", "published": "2022-04-14 04:06:39", "link": "http://arxiv.org/abs/2204.06748v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Shedding New Light on the Language of the Dark Web", "abstract": "The hidden nature and the limited accessibility of the Dark Web, combined\nwith the lack of public datasets in this domain, make it difficult to study its\ninherent characteristics such as linguistic properties. Previous works on text\nclassification of Dark Web domain have suggested that the use of deep neural\nmodels may be ineffective, potentially due to the linguistic differences\nbetween the Dark and Surface Webs. However, not much work has been done to\nuncover the linguistic characteristics of the Dark Web. This paper introduces\nCoDA, a publicly available Dark Web dataset consisting of 10000 web documents\ntailored towards text-based Dark Web analysis. By leveraging CoDA, we conduct a\nthorough linguistic analysis of the Dark Web and examine the textual\ndifferences between the Dark Web and the Surface Web. We also assess the\nperformance of various methods of Dark Web page classification. Finally, we\ncompare CoDA with an existing public Dark Web dataset and evaluate their\nsuitability for various use cases.", "published": "2022-04-14 11:17:22", "link": "http://arxiv.org/abs/2204.06885v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Latent Aspect Detection from Online Unsolicited Customer Reviews", "abstract": "Within the context of review analytics, aspects are the features of products\nand services at which customers target their opinions and sentiments. Aspect\ndetection helps product owners and service providers to identify shortcomings\nand prioritize customers' needs, and hence, maintain revenues and mitigate\ncustomer churn. Existing methods focus on detecting the surface form of an\naspect by training supervised learning methods that fall short when aspects are\nlatent in reviews. In this paper, we propose an unsupervised method to extract\nlatent occurrences of aspects. Specifically, we assume that a customer\nundergoes a two-stage hypothetical generative process when writing a review:\n(1) deciding on an aspect amongst the set of aspects available for the product\nor service, and (2) writing the opinion words that are more interrelated to the\nchosen aspect from the set of all words available in a language. We employ\nlatent Dirichlet allocation to learn the latent aspects distributions for\ngenerating the reviews. Experimental results on benchmark datasets show that\nour proposed method is able to improve the state of the art when the aspects\nare latent with no surface form in reviews.", "published": "2022-04-14 13:46:25", "link": "http://arxiv.org/abs/2204.06964v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.7; H.3.3"], "primary_category": "cs.CL"}
{"title": "Exploring Dual Encoder Architectures for Question Answering", "abstract": "Dual encoders have been used for question-answering (QA) and information\nretrieval (IR) tasks with good results. Previous research focuses on two major\ntypes of dual encoders, Siamese Dual Encoder (SDE), with parameters shared\nacross two encoders, and Asymmetric Dual Encoder (ADE), with two distinctly\nparameterized encoders. In this work, we explore different ways in which the\ndual encoder can be structured, and evaluate how these differences can affect\ntheir efficacy in terms of QA retrieval tasks. By evaluating on MS MARCO, open\ndomain NQ and the MultiReQA benchmarks, we show that SDE performs significantly\nbetter than ADE. We further propose three different improved versions of ADEs\nby sharing or freezing parts of the architectures between two encoder towers.\nWe find that sharing parameters in projection layers would enable ADEs to\nperform competitively with or outperform SDEs. We further explore and explain\nwhy parameter sharing in projection layer significantly improves the efficacy\nof the dual encoders, by directly probing the embedding spaces of the two\nencoder towers with t-SNE algorithm.", "published": "2022-04-14 17:21:14", "link": "http://arxiv.org/abs/2204.07120v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scalable and Robust Self-Learning for Skill Routing in Large-Scale\n  Conversational AI Systems", "abstract": "Skill routing is an important component in large-scale conversational\nsystems. In contrast to traditional rule-based skill routing, state-of-the-art\nsystems use a model-based approach to enable natural conversations. To provide\nsupervision signal required to train such models, ideas such as human\nannotation, replication of a rule-based system, relabeling based on user\nparaphrases, and bandit-based learning were suggested. However, these\napproaches: (a) do not scale in terms of the number of skills and skill\non-boarding, (b) require a very costly expert annotation/rule-design, (c)\nintroduce risks in the user experience with each model update. In this paper,\nwe present a scalable self-learning approach to explore routing alternatives\nwithout causing abrupt policy changes that break the user experience, learn\nfrom the user interaction, and incrementally improve the routing via frequent\nmodel refreshes. To enable such robust frequent model updates, we suggest a\nsimple and effective approach that ensures controlled policy updates for\nindividual domains, followed by an off-policy evaluation for making deployment\ndecisions without any need for lengthy A/B experimentation. We conduct various\noffline and online A/B experiments on a commercial large-scale conversational\nsystem to demonstrate the effectiveness of the proposed method in real-world\nproduction settings.", "published": "2022-04-14 17:46:14", "link": "http://arxiv.org/abs/2204.07135v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.LG"}
{"title": "CLUES: A Benchmark for Learning Classifiers using Natural Language\n  Explanations", "abstract": "Supervised learning has traditionally focused on inductive learning by\nobserving labeled examples of a task. In contrast, humans have the ability to\nlearn new concepts from language. Here, we explore training zero-shot\nclassifiers for structured data purely from language. For this, we introduce\nCLUES, a benchmark for Classifier Learning Using natural language ExplanationS,\nconsisting of a range of classification tasks over structured data along with\nnatural language supervision in the form of explanations. CLUES consists of 36\nreal-world and 144 synthetic classification tasks. It contains crowdsourced\nexplanations describing real-world tasks from multiple teachers and\nprogrammatically generated explanations for the synthetic tasks. To model the\ninfluence of explanations in classifying an example, we develop ExEnt, an\nentailment-based model that learns classifiers using explanations. ExEnt\ngeneralizes up to 18% better (relative) on novel tasks than a baseline that\ndoes not use explanations. We delineate key challenges for automated learning\nfrom explanations, addressing which can lead to progress on CLUES in the\nfuture. Code and datasets are available at: https://clues-benchmark.github.io.", "published": "2022-04-14 17:54:46", "link": "http://arxiv.org/abs/2204.07142v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analysing similarities between legal court documents using natural\n  language processing approaches based on Transformers", "abstract": "Recent advances in Artificial Intelligence (AI) have leveraged promising\nresults in solving complex problems in the area of Natural Language Processing\n(NLP), being an important tool to help in the expeditious resolution of\njudicial proceedings in the legal area. In this context, this work targets the\nproblem of detecting the degree of similarity between judicial documents that\ncan be achieved in the inference group, by applying six NLP techniques based on\nthe transformers architecture to a case study of legal proceedings in the\nBrazilian judicial system. The NLP transformer-based models, namely BERT, GPT-2\nand RoBERTa, were pre-trained using a general purpose corpora of the Brazilian\nPortuguese language, and then were fine-tuned and specialised for the legal\nsector using 210,000 legal proceedings. Vector representations of each legal\ndocument were calculated based on their embeddings, which were used to cluster\nthe lawsuits, calculating the quality of each model based on the cosine of the\ndistance between the elements of the group to its centroid. We noticed that\nmodels based on transformers presented better performance when compared to\nprevious traditional NLP techniques, with the RoBERTa model specialised for the\nBrazilian Portuguese language presenting the best results. This methodology can\nbe also applied to other case studies for different languages, making it\npossible to advance in the current state of the art in the area of NLP applied\nto the legal sector.", "published": "2022-04-14 18:25:56", "link": "http://arxiv.org/abs/2204.07182v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Applying Feature Underspecified Lexicon Phonological Features in\n  Multilingual Text-to-Speech", "abstract": "This study investigates whether the phonological features derived from the\nFeaturally Underspecified Lexicon model can be applied in text-to-speech\nsystems to generate native and non-native speech in English and Mandarin. We\npresent a mapping of ARPABET/pinyin to SAMPA/SAMPA-SC and then to phonological\nfeatures. This mapping was tested for whether it could lead to the successful\ngeneration of native, non-native, and code-switched speech in the two\nlanguages. We ran two experiments, one with a small dataset and one with a\nlarger dataset. The results supported that phonological features could be used\nas a feasible input system for languages in or not in the train data, although\nfurther investigation is needed to improve model performance. The results lend\nsupport to FUL by presenting successfully synthesised output, and by having the\noutput carrying a source-language accent when synthesising a language not in\nthe training data. The TTS process stimulated human second language acquisition\nprocess and thus also confirm FUL's ability to account for acquisition.", "published": "2022-04-14 21:04:55", "link": "http://arxiv.org/abs/2204.07228v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Brainish: Formalizing A Multimodal Language for Intelligence and\n  Consciousness", "abstract": "Having a rich multimodal inner language is an important component of human\nintelligence that enables several necessary core cognitive functions such as\nmultimodal prediction, translation, and generation. Building upon the Conscious\nTuring Machine (CTM), a machine model for consciousness proposed by Blum and\nBlum (2021), we describe the desiderata of a multimodal language called\nBrainish, comprising words, images, audio, and sensations combined in\nrepresentations that the CTM's processors use to communicate with each other.\nWe define the syntax and semantics of Brainish before operationalizing this\nlanguage through the lens of multimodal artificial intelligence, a vibrant\nresearch area studying the computational tools necessary for processing and\nrelating information from heterogeneous signals. Our general framework for\nlearning Brainish involves designing (1) unimodal encoders to segment and\nrepresent unimodal data, (2) a coordinated representation space that relates\nand composes unimodal features to derive holistic meaning across multimodal\ninputs, and (3) decoders to map multimodal representations into predictions\n(for fusion) or raw data (for translation or generation). Through discussing\nhow Brainish is crucial for communication and coordination in order to achieve\nconsciousness in the CTM, and by implementing a simple version of Brainish and\nevaluating its capability of demonstrating intelligence on multimodal\nprediction and retrieval tasks on several real-world image, text, and audio\ndatasets, we argue that such an inner language will be important for advances\nin machine models of intelligence and consciousness.", "published": "2022-04-14 00:35:52", "link": "http://arxiv.org/abs/2205.00001v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Lombard Effect for Bilingual Speakers in Cantonese and English:\n  importance of spectro-temporal features", "abstract": "For a better understanding of the mechanisms underlying speech perception and\nthe contribution of different signal features, computational models of speech\nrecognition have a long tradition in hearing research. Due to the diverse range\nof situations in which speech needs to be recognized, these models need to be\ngeneralizable across many acoustic conditions, speakers, and languages. This\ncontribution examines the importance of different features for speech\nrecognition predictions of plain and Lombard speech for English in comparison\nto Cantonese in stationary and modulated noise. While Cantonese is a tonal\nlanguage that encodes information in spectro-temporal features, the Lombard\neffect is known to be associated with spectral changes in the speech signal.\nThese contrasting properties of tonal languages and the Lombard effect form an\ninteresting basis for the assessment of speech recognition models. Here, an\nautomatic speech recognition-based ASR model using spectral or spectro-temporal\nfeatures is evaluated with empirical data. The results indicate that\nspectro-temporal features are crucial in order to predict the speaker-specific\nspeech recognition threshold SRT$_{50}$ in both Cantonese and English as well\nas to account for the improvement of speech recognition in modulated noise,\nwhile effects due to Lombard speech can already be predicted by spectral\nfeatures.", "published": "2022-04-14 12:06:59", "link": "http://arxiv.org/abs/2204.06907v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Streamable Neural Audio Synthesis With Non-Causal Convolutions", "abstract": "Deep learning models are mostly used in an offline inference fashion.\nHowever, this strongly limits the use of these models inside audio generation\nsetups, as most creative workflows are based on real-time digital signal\nprocessing. Although approaches based on recurrent networks can be naturally\nadapted to this buffer-based computation, the use of convolutions still poses\nsome serious challenges. To tackle this issue, the use of causal streaming\nconvolutions have been proposed. However, this requires specific complexified\ntraining and can impact the resulting audio quality.\n  In this paper, we introduce a new method allowing to produce non-causal\nstreaming models. This allows to make any convolutional model compatible with\nreal-time buffer-based processing. As our method is based on a post-training\nreconfiguration of the model, we show that it is able to transform models\ntrained without causal constraints into a streaming model. We show how our\nmethod can be adapted to fit complex architectures with parallel branches. To\nevaluate our method, we apply it on the recent RAVE model, which provides\nhigh-quality real-time audio synthesis. We test our approach on multiple music\nand speech datasets and show that it is faster than overlap-add methods, while\nhaving no impact on the generation quality. Finally, we introduce two\nopen-source implementation of our work as Max/MSP and PureData externals, and\nas a VST audio plugin. This allows to endow traditional digital audio\nworkstation with real-time neural audio synthesis on a laptop CPU.", "published": "2022-04-14 16:00:32", "link": "http://arxiv.org/abs/2204.07064v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Learning and controlling the source-filter representation of speech with\n  a variational autoencoder", "abstract": "Understanding and controlling latent representations in deep generative\nmodels is a challenging yet important problem for analyzing, transforming and\ngenerating various types of data. In speech processing, inspiring from the\nanatomical mechanisms of phonation, the source-filter model considers that\nspeech signals are produced from a few independent and physically meaningful\ncontinuous latent factors, among which the fundamental frequency $f_0$ and the\nformants are of primary importance. In this work, we start from a variational\nautoencoder (VAE) trained in an unsupervised manner on a large dataset of\nunlabeled natural speech signals, and we show that the source-filter model of\nspeech production naturally arises as orthogonal subspaces of the VAE latent\nspace. Using only a few seconds of labeled speech signals generated with an\nartificial speech synthesizer, we propose a method to identify the latent\nsubspaces encoding $f_0$ and the first three formant frequencies, we show that\nthese subspaces are orthogonal, and based on this orthogonality, we develop a\nmethod to accurately and independently control the source-filter speech factors\nwithin the latent subspaces. Without requiring additional information such as\ntext or human-labeled data, this results in a deep generative model of speech\nspectrograms that is conditioned on $f_0$ and the formant frequencies, and\nwhich is applied to the transformation speech signals. Finally, we also propose\na robust $f_0$ estimation method that exploits the projection of a speech\nsignal onto the learned latent subspace associated with $f_0$.", "published": "2022-04-14 16:13:06", "link": "http://arxiv.org/abs/2204.07075v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RadioSES: mmWave-Based Audioradio Speech Enhancement and Separation\n  System", "abstract": "Speech enhancement and separation have been a long-standing problem,\nespecially with the recent advances using a single microphone. Although\nmicrophones perform well in constrained settings, their performance for speech\nseparation decreases in noisy conditions. In this work, we propose RadioSES, an\naudioradio speech enhancement and separation system that overcomes inherent\nproblems in audio-only systems. By fusing a complementary radio modality,\nRadioSES can estimate the number of speakers, solve source association problem,\nseparate and enhance noisy mixture speeches, and improve both intelligibility\nand perceptual quality. We perform millimeter-wave sensing to detect and\nlocalize speakers, and introduce an audioradio deep learning framework to fuse\nthe separate radio features with the mixed audio features. Extensive\nexperiments using commercial off-the-shelf devices show that RadioSES\noutperforms a variety of state-of-the-art baselines, with consistent\nperformance gains in different environmental settings. Compared with the\naudiovisual methods, RadioSES provides similar improvements (e.g., ~3 dB gains\nin SiSDR), along with the benefits of lower computational complexity and being\nless privacy concerning.", "published": "2022-04-14 16:33:29", "link": "http://arxiv.org/abs/2204.07092v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "From Environmental Sound Representation to Robustness of 2D CNN Models\n  Against Adversarial Attacks", "abstract": "This paper investigates the impact of different standard environmental sound\nrepresentations (spectrograms) on the recognition performance and adversarial\nattack robustness of a victim residual convolutional neural network, namely\nResNet-18. Our main motivation for focusing on such a front-end classifier\nrather than other complex architectures is balancing recognition accuracy and\nthe total number of training parameters. Herein, we measure the impact of\ndifferent settings required for generating more informative Mel-frequency\ncepstral coefficient (MFCC), short-time Fourier transform (STFT), and discrete\nwavelet transform (DWT) representations on our front-end model. This\nmeasurement involves comparing the classification performance over the\nadversarial robustness. We demonstrate an inverse relationship between\nrecognition accuracy and model robustness against six benchmarking attack\nalgorithms on the balance of average budgets allocated by the adversary and the\nattack cost. Moreover, our experimental results have shown that while the\nResNet-18 model trained on DWT spectrograms achieves a high recognition\naccuracy, attacking this model is relatively more costly for the adversary than\nother 2D representations. We also report some results on different\nconvolutional neural network architectures such as ResNet-34, ResNet-56,\nAlexNet, and GoogLeNet, SB-CNN, and LSTM-based.", "published": "2022-04-14 15:14:08", "link": "http://arxiv.org/abs/2204.07018v1", "categories": ["cs.SD", "cs.CR", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
