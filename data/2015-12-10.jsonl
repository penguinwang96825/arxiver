{"title": "Mined Semantic Analysis: A New Concept Space Model for Semantic\n  Representation of Textual Data", "abstract": "Mined Semantic Analysis (MSA) is a novel concept space model which employs\nunsupervised learning to generate semantic representations of text. MSA\nrepresents textual structures (terms, phrases, documents) as a Bag of Concepts\n(BoC) where concepts are derived from concept rich encyclopedic corpora.\nTraditional concept space models exploit only target corpus content to\nconstruct the concept space. MSA, alternatively, uncovers implicit relations\nbetween concepts by mining for their associations (e.g., mining Wikipedia's\n\"See also\" link graph). We evaluate MSA's performance on benchmark datasets for\nmeasuring semantic relatedness of words and sentences. Empirical results show\ncompetitive performance of MSA compared to prior state-of-the-art methods.\nAdditionally, we introduce the first analytical study to examine statistical\nsignificance of results reported by different semantic relatedness methods. Our\nstudy shows that, the nuances of results across top performing methods could be\nstatistically insignificant. The study positions MSA as one of state-of-the-art\nmethods for measuring semantic relatedness, besides the inherent\ninterpretability and simplicity of the generated semantic representation.", "published": "2015-12-10 22:15:10", "link": "http://arxiv.org/abs/1512.03465v3", "categories": ["cs.CL", "H.3.1"], "primary_category": "cs.CL"}
{"title": "Neural Self Talk: Image Understanding via Continuous Questioning and\n  Answering", "abstract": "In this paper we consider the problem of continuously discovering image\ncontents by actively asking image based questions and subsequently answering\nthe questions being asked. The key components include a Visual Question\nGeneration (VQG) module and a Visual Question Answering module, in which\nRecurrent Neural Networks (RNN) and Convolutional Neural Network (CNN) are\nused. Given a dataset that contains images, questions and their answers, both\nmodules are trained at the same time, with the difference being VQG uses the\nimages as input and the corresponding questions as output, while VQA uses\nimages and questions as input and the corresponding answers as output. We\nevaluate the self talk process subjectively using Amazon Mechanical Turk, which\nshow effectiveness of the proposed method.", "published": "2015-12-10 21:58:46", "link": "http://arxiv.org/abs/1512.03460v1", "categories": ["cs.CV", "cs.CL", "cs.RO", "I.2.10"], "primary_category": "cs.CV"}
