{"title": "A Publicly Available Cross-Platform Lemmatizer for Bulgarian", "abstract": "Our dictionary-based lemmatizer for the Bulgarian language presented here is\ndistributed as free software, publicly available to download and use under the\nGPL v3 license. The presented software is written entirely in Java and is\ndistributed as a GATE plugin. To our best knowledge, at the time of writing\nthis article, there are not any other free lemmatization tools specifically\ntargeting the Bulgarian language. The presented lemmatizer is a work in\nprogress and currently yields an accuracy of about 95% in comparison to the\nmanually annotated corpus BulTreeBank-Morph, which contains 273933 tokens.", "published": "2015-06-13 05:47:50", "link": "http://arxiv.org/abs/1506.04228v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of the Accuracy of the BGLemmatizer", "abstract": "This paper reveals the results of an analysis of the accuracy of developed\nsoftware for automatic lemmatization for the Bulgarian language. This\nlemmatization software is written entirely in Java and is distributed as a GATE\nplugin. Certain statistical methods are used to define the accuracy of this\nsoftware. The results of the analysis show 95% lemmatization accuracy.", "published": "2015-06-13 05:53:57", "link": "http://arxiv.org/abs/1506.04229v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Bayesian Model for Generative Transition-based Dependency Parsing", "abstract": "We propose a simple, scalable, fully generative model for transition-based\ndependency parsing with high accuracy. The model, parameterized by Hierarchical\nPitman-Yor Processes, overcomes the limitations of previous generative models\nby allowing fast and accurate inference. We propose an efficient decoding\nalgorithm based on particle filtering that can adapt the beam size to the\nuncertainty in the model while jointly predicting POS tags and parse trees. The\nUAS of the parser is on par with that of a greedy discriminative baseline. As a\nlanguage model, it obtains better perplexity than a n-gram model by performing\nsemi-supervised learning over a large unlabelled corpus. We show that the model\nis able to generate locally and syntactically coherent sentences, opening the\ndoor to further applications in language generation.", "published": "2015-06-13 23:39:09", "link": "http://arxiv.org/abs/1506.04334v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
