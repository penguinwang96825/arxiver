{"title": "ERNetCL: A novel emotion recognition network in textual conversation\n  based on curriculum learning strategy", "abstract": "Emotion recognition in conversation (ERC) has emerged as a research hotspot\nin domains such as conversational robots and question-answer systems. How to\nefficiently and adequately retrieve contextual emotional cues has been one of\nthe key challenges in the ERC task. Existing efforts do not fully model the\ncontext and employ complex network structures, resulting in limited performance\ngains. In this paper, we propose a novel emotion recognition network based on\ncurriculum learning strategy (ERNetCL). The proposed ERNetCL primarily consists\nof temporal encoder (TE), spatial encoder (SE), and curriculum learning (CL)\nloss. We utilize TE and SE to combine the strengths of previous methods in a\nsimplistic manner to efficiently capture temporal and spatial contextual\ninformation in the conversation. To ease the harmful influence resulting from\nemotion shift and simulate the way humans learn curriculum from easy to hard,\nwe apply the idea of CL to the ERC task to progressively optimize the network\nparameters. At the beginning of training, we assign lower learning weights to\ndifficult samples. As the epoch increases, the learning weights for these\nsamples are gradually raised. Extensive experiments on four datasets exhibit\nthat our proposed method is effective and dramatically beats other baseline\nmodels.", "published": "2023-08-12 03:05:44", "link": "http://arxiv.org/abs/2308.06450v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Demonstration-based learning for few-shot biomedical named entity\n  recognition under machine reading comprehension", "abstract": "Although deep learning techniques have shown significant achievements, they\nfrequently depend on extensive amounts of hand-labeled data and tend to perform\ninadequately in few-shot scenarios. The objective of this study is to devise a\nstrategy that can improve the model's capability to recognize biomedical\nentities in scenarios of few-shot learning. By redefining biomedical named\nentity recognition (BioNER) as a machine reading comprehension (MRC) problem,\nwe propose a demonstration-based learning method to address few-shot BioNER,\nwhich involves constructing appropriate task demonstrations. In assessing our\nproposed method, we compared the proposed method with existing advanced methods\nusing six benchmark datasets, including BC4CHEMD, BC5CDR-Chemical,\nBC5CDR-Disease, NCBI-Disease, BC2GM, and JNLPBA. We examined the models'\nefficacy by reporting F1 scores from both the 25-shot and 50-shot learning\nexperiments. In 25-shot learning, we observed 1.1% improvements in the average\nF1 scores compared to the baseline method, reaching 61.7%, 84.1%, 69.1%, 70.1%,\n50.6%, and 59.9% on six datasets, respectively. In 50-shot learning, we further\nimproved the average F1 scores by 1.0% compared to the baseline method,\nreaching 73.1%, 86.8%, 76.1%, 75.6%, 61.7%, and 65.4%, respectively. We\nreported that in the realm of few-shot learning BioNER, MRC-based language\nmodels are much more proficient in recognizing biomedical entities compared to\nthe sequence labeling approach. Furthermore, our MRC-language models can\ncompete successfully with fully-supervised learning methodologies that rely\nheavily on the availability of abundant annotated data. These results highlight\npossible pathways for future advancements in few-shot BioNER methodologies.", "published": "2023-08-12 03:23:09", "link": "http://arxiv.org/abs/2308.06454v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher", "abstract": "Safety lies at the core of the development of Large Language Models (LLMs).\nThere is ample work on aligning LLMs with human ethics and preferences,\nincluding data filtering in pretraining, supervised fine-tuning, reinforcement\nlearning from human feedback, and red teaming, etc. In this study, we discover\nthat chat in cipher can bypass the safety alignment techniques of LLMs, which\nare mainly conducted in natural languages. We propose a novel framework\nCipherChat to systematically examine the generalizability of safety alignment\nto non-natural languages -- ciphers. CipherChat enables humans to chat with\nLLMs through cipher prompts topped with system role descriptions and few-shot\nenciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs,\nincluding ChatGPT and GPT-4 for different representative human ciphers across\n11 safety domains in both English and Chinese. Experimental results show that\ncertain ciphers succeed almost 100% of the time to bypass the safety alignment\nof GPT-4 in several safety domains, demonstrating the necessity of developing\nsafety alignment for non-natural languages. Notably, we identify that LLMs seem\nto have a ''secret cipher'', and propose a novel SelfCipher that uses only role\nplay and several demonstrations in natural language to evoke this capability.\nSelfCipher surprisingly outperforms existing human ciphers in almost all cases.\nOur code and data will be released at https://github.com/RobustNLP/CipherChat.", "published": "2023-08-12 04:05:57", "link": "http://arxiv.org/abs/2308.06463v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NewsDialogues: Towards Proactive News Grounded Conversation", "abstract": "Hot news is one of the most popular topics in daily conversations. However,\nnews grounded conversation has long been stymied by the lack of well-designed\ntask definition and scarce data. In this paper, we propose a novel task,\nProactive News Grounded Conversation, in which a dialogue system can\nproactively lead the conversation based on some key topics of the news. In\naddition, both information-seeking and chit-chat scenarios are included\nrealistically, where the user may ask a series of questions about the news\ndetails or express their opinions and be eager to chat. To further develop this\nnovel task, we collect a human-to-human Chinese dialogue dataset\n\\ts{NewsDialogues}, which includes 1K conversations with a total of 14.6K\nutterances and detailed annotations for target topics and knowledge spans.\nFurthermore, we propose a method named Predict-Generate-Rank, consisting of a\ngenerator for grounded knowledge prediction and response generation, and a\nranker for the ranking of multiple responses to alleviate the exposure bias. We\nconduct comprehensive experiments to demonstrate the effectiveness of the\nproposed method and further present several key findings and challenges to\nprompt future research.", "published": "2023-08-12 08:33:42", "link": "http://arxiv.org/abs/2308.06501v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AutoConv: Automatically Generating Information-seeking Conversations\n  with Large Language Models", "abstract": "Information-seeking conversation, which aims to help users gather information\nthrough conversation, has achieved great progress in recent years. However, the\nresearch is still stymied by the scarcity of training data. To alleviate this\nproblem, we propose AutoConv for synthetic conversation generation, which takes\nadvantage of the few-shot learning ability and generation capacity of large\nlanguage models (LLM). Specifically, we formulate the conversation generation\nproblem as a language modeling task, then finetune an LLM with a few human\nconversations to capture the characteristics of the information-seeking process\nand use it for generating synthetic conversations with high quality.\nExperimental results on two frequently-used datasets verify that AutoConv has\nsubstantial improvements over strong baselines and alleviates the dependence on\nhuman annotation. In addition, we also provide several analysis studies to\npromote future research.", "published": "2023-08-12 08:52:40", "link": "http://arxiv.org/abs/2308.06507v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "With a Little Help from the Authors: Reproducing Human Evaluation of an\n  MT Error Detector", "abstract": "This work presents our efforts to reproduce the results of the human\nevaluation experiment presented in the paper of Vamvas and Sennrich (2022),\nwhich evaluated an automatic system detecting over- and undertranslations\n(translations containing more or less information than the original) in machine\ntranslation (MT) outputs. Despite the high quality of the documentation and\ncode provided by the authors, we discuss some problems we found in reproducing\nthe exact experimental setup and offer recommendations for improving\nreproducibility. Our replicated results generally confirm the conclusions of\nthe original study, but in some cases, statistically significant differences\nwere observed, suggesting a high variability of human annotation.", "published": "2023-08-12 11:00:59", "link": "http://arxiv.org/abs/2308.06527v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MT4CrossOIE: Multi-stage Tuning for Cross-lingual Open Information\n  Extraction", "abstract": "Cross-lingual open information extraction aims to extract structured\ninformation from raw text across multiple languages. Previous work uses a\nshared cross-lingual pre-trained model to handle the different languages but\nunderuses the potential of the language-specific representation. In this paper,\nwe propose an effective multi-stage tuning framework called MT4CrossIE,\ndesigned for enhancing cross-lingual open information extraction by injecting\nlanguage-specific knowledge into the shared model. Specifically, the\ncross-lingual pre-trained model is first tuned in a shared semantic space\n(e.g., embedding matrix) in the fixed encoder and then other components are\noptimized in the second stage. After enough training, we freeze the pre-trained\nmodel and tune the multiple extra low-rank language-specific modules using\nmixture-of-LoRAs for model-based cross-lingual transfer. In addition, we\nleverage two-stage prompting to encourage the large language model (LLM) to\nannotate the multi-lingual raw data for data-based cross-lingual transfer. The\nmodel is trained with multi-lingual objectives on our proposed dataset\nOpenIE4++ by combing the model-based and data-based transfer techniques.\nExperimental results on various benchmarks emphasize the importance of\naggregating multiple plug-in-and-play language-specific modules and demonstrate\nthe effectiveness of MT4CrossIE in cross-lingual\nOIE\\footnote{\\url{https://github.com/CSJianYang/Multilingual-Multimodal-NLP}}.", "published": "2023-08-12 12:38:10", "link": "http://arxiv.org/abs/2308.06552v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text-to-Video: a Two-stage Framework for Zero-shot Identity-agnostic\n  Talking-head Generation", "abstract": "The advent of ChatGPT has introduced innovative methods for information\ngathering and analysis. However, the information provided by ChatGPT is limited\nto text, and the visualization of this information remains constrained.\nPrevious research has explored zero-shot text-to-video (TTV) approaches to\ntransform text into videos. However, these methods lacked control over the\nidentity of the generated audio, i.e., not identity-agnostic, hindering their\neffectiveness. To address this limitation, we propose a novel two-stage\nframework for person-agnostic video cloning, specifically focusing on TTV\ngeneration. In the first stage, we leverage pretrained zero-shot models to\nachieve text-to-speech (TTS) conversion. In the second stage, an audio-driven\ntalking head generation method is employed to produce compelling videos\nprivided the audio generated in the first stage. This paper presents a\ncomparative analysis of different TTS and audio-driven talking head generation\nmethods, identifying the most promising approach for future research and\ndevelopment. Some audio and videos samples can be found in the following link:\nhttps://github.com/ZhichaoWang970201/Text-to-Video/tree/main.", "published": "2023-08-12 03:30:49", "link": "http://arxiv.org/abs/2308.06457v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Generating Faithful Text From a Knowledge Graph with Noisy Reference\n  Text", "abstract": "Knowledge Graph (KG)-to-Text generation aims at generating fluent\nnatural-language text that accurately represents the information of a given\nknowledge graph. While significant progress has been made in this task by\nexploiting the power of pre-trained language models (PLMs) with appropriate\ngraph structure-aware modules, existing models still fall short of generating\nfaithful text, especially when the ground-truth natural-language text contains\nadditional information that is not present in the graph. In this paper, we\ndevelop a KG-to-text generation model that can generate faithful\nnatural-language text from a given graph, in the presence of noisy reference\ntext. Our framework incorporates two core ideas: Firstly, we utilize\ncontrastive learning to enhance the model's ability to differentiate between\nfaithful and hallucinated information in the text, thereby encouraging the\ndecoder to generate text that aligns with the input graph. Secondly, we empower\nthe decoder to control the level of hallucination in the generated text by\nemploying a controllable text generation technique. We evaluate our model's\nperformance through the standard quantitative metrics as well as a\nChatGPT-based quantitative and qualitative analysis. Our evaluation\ndemonstrates the superior performance of our model over state-of-the-art\nKG-to-text models on faithfulness.", "published": "2023-08-12 07:12:45", "link": "http://arxiv.org/abs/2308.06488v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Three Ways of Using Large Language Models to Evaluate Chat", "abstract": "This paper describes the systems submitted by team6 for ChatEval, the DSTC 11\nTrack 4 competition. We present three different approaches to predicting\nturn-level qualities of chatbot responses based on large language models\n(LLMs). We report improvement over the baseline using dynamic few-shot examples\nfrom a vector store for the prompts for ChatGPT. We also analyze the\nperformance of the other two approaches and report needed improvements for\nfuture work. We developed the three systems over just two weeks, showing the\npotential of LLMs for this task. An ablation study conducted after the\nchallenge deadline shows that the new Llama 2 models are closing the\nperformance gap between ChatGPT and open-source LLMs. However, we find that the\nLlama 2 models do not benefit from few-shot examples in the same way as\nChatGPT.", "published": "2023-08-12 08:34:15", "link": "http://arxiv.org/abs/2308.06502v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HyperFormer: Enhancing Entity and Relation Interaction for\n  Hyper-Relational Knowledge Graph Completion", "abstract": "Hyper-relational knowledge graphs (HKGs) extend standard knowledge graphs by\nassociating attribute-value qualifiers to triples, which effectively represent\nadditional fine-grained information about its associated triple.\nHyper-relational knowledge graph completion (HKGC) aims at inferring unknown\ntriples while considering its qualifiers. Most existing approaches to HKGC\nexploit a global-level graph structure to encode hyper-relational knowledge\ninto the graph convolution message passing process. However, the addition of\nmulti-hop information might bring noise into the triple prediction process. To\naddress this problem, we propose HyperFormer, a model that considers\nlocal-level sequential information, which encodes the content of the entities,\nrelations and qualifiers of a triple. More precisely, HyperFormer is composed\nof three different modules: an entity neighbor aggregator module allowing to\nintegrate the information of the neighbors of an entity to capture different\nperspectives of it; a relation qualifier aggregator module to integrate\nhyper-relational knowledge into the corresponding relation to refine the\nrepresentation of relational content; a convolution-based bidirectional\ninteraction module based on a convolutional operation, capturing pairwise\nbidirectional interactions of entity-relation, entity-qualifier, and\nrelation-qualifier. realize the depth perception of the content related to the\ncurrent statement. Furthermore, we introduce a Mixture-of-Experts strategy into\nthe feed-forward layers of HyperFormer to strengthen its representation\ncapabilities while reducing the amount of model parameters and computation.\nExtensive experiments on three well-known datasets with four different\nconditions demonstrate HyperFormer's effectiveness. Datasets and code are\navailable at https://github.com/zhiweihu1103/HKGC-HyperFormer.", "published": "2023-08-12 09:31:43", "link": "http://arxiv.org/abs/2308.06512v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "MC-DRE: Multi-Aspect Cross Integration for Drug Event/Entity Extraction", "abstract": "Extracting meaningful drug-related information chunks, such as adverse drug\nevents (ADE), is crucial for preventing morbidity and saving many lives. Most\nADEs are reported via an unstructured conversation with the medical context, so\napplying a general entity recognition approach is not sufficient enough. In\nthis paper, we propose a new multi-aspect cross-integration framework for drug\nentity/event detection by capturing and aligning different\ncontext/language/knowledge properties from drug-related documents. We first\nconstruct multi-aspect encoders to describe semantic, syntactic, and medical\ndocument contextual information by conducting those slot tagging tasks, main\ndrug entity/event detection, part-of-speech tagging, and general medical named\nentity recognition. Then, each encoder conducts cross-integration with other\ncontextual information in three ways: the key-value cross, attention cross, and\nfeedforward cross, so the multi-encoders are integrated in depth. Our model\noutperforms all SOTA on two widely used tasks, flat entity detection and\ndiscontinuous event extraction.", "published": "2023-08-12 12:03:41", "link": "http://arxiv.org/abs/2308.06546v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bio-SIEVE: Exploring Instruction Tuning Large Language Models for\n  Systematic Review Automation", "abstract": "Medical systematic reviews can be very costly and resource intensive. We\nexplore how Large Language Models (LLMs) can support and be trained to perform\nliterature screening when provided with a detailed set of selection criteria.\nSpecifically, we instruction tune LLaMA and Guanaco models to perform abstract\nscreening for medical systematic reviews. Our best model, Bio-SIEVE,\noutperforms both ChatGPT and trained traditional approaches, and generalises\nbetter across medical domains. However, there remains the challenge of adapting\nthe model to safety-first scenarios. We also explore the impact of multi-task\ntraining with Bio-SIEVE-Multi, including tasks such as PICO extraction and\nexclusion reasoning, but find that it is unable to match single-task\nBio-SIEVE's performance. We see Bio-SIEVE as an important step towards\nspecialising LLMs for the biomedical systematic review process and explore its\nfuture developmental opportunities. We release our models, code and a list of\nDOIs to reconstruct our dataset for reproducibility.", "published": "2023-08-12 16:56:55", "link": "http://arxiv.org/abs/2308.06610v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Emergent communication for AR", "abstract": "Mobile augmented reality (MAR) is widely acknowledged as one of the\nubiquitous interfaces to the digital twin and Metaverse, demanding unparalleled\nlevels of latency, computational power, and energy efficiency. The existing\nsolutions for realizing MAR combine multiple technologies like edge, cloud\ncomputing, and fifth-generation (5G) networks. However, the inherent\ncommunication latency of visual data imposes apparent limitations on the\nquality of experience (QoE). To address the challenge, we propose an emergent\nsemantic communication framework to learn the communication protocols in MAR.\nSpecifically, we train two agents through a modified Lewis signaling game to\nemerge a discrete communication protocol spontaneously. Based on this protocol,\ntwo agents can communicate about the abstract idea of visual data through\nmessages with extremely small data sizes in a noisy channel, which leads to\nmessage errors. To better simulate real-world scenarios, we incorporate channel\nuncertainty into our training process. Experiments have shown that the proposed\nscheme has better generalization on unseen objects than traditional object\nrecognition used in MAR and can effectively enhance communication efficiency\nthrough the utilization of small-size messages.", "published": "2023-08-12 16:45:39", "link": "http://arxiv.org/abs/2308.07342v1", "categories": ["eess.SP", "cs.CL"], "primary_category": "eess.SP"}
{"title": "Performance Prediction for Multi-hop Questions", "abstract": "We study the problem of Query Performance Prediction (QPP) for open-domain\nmulti-hop Question Answering (QA), where the task is to estimate the difficulty\nof evaluating a multi-hop question over a corpus. Despite the extensive\nresearch on predicting the performance of ad-hoc and QA retrieval models, there\nhas been a lack of study on the estimation of the difficulty of multi-hop\nquestions. The problem is challenging due to the multi-step nature of the\nretrieval process, potential dependency of the steps and the reasoning\ninvolved. To tackle this challenge, we propose multHP, a novel pre-retrieval\nmethod for predicting the performance of open-domain multi-hop questions. Our\nextensive evaluation on the largest multi-hop QA dataset using several modern\nQA systems shows that the proposed model is a strong predictor of the\nperformance, outperforming traditional single-hop QPP models. Additionally, we\ndemonstrate that our approach can be effectively used to optimize the\nparameters of QA systems, such as the number of documents to be retrieved,\nresulting in improved overall retrieval performance.", "published": "2023-08-12 01:34:41", "link": "http://arxiv.org/abs/2308.06431v1", "categories": ["cs.CL", "cs.DB", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Alternative Pseudo-Labeling for Semi-Supervised Automatic Speech\n  Recognition", "abstract": "When labeled data is insufficient, semi-supervised learning with the\npseudo-labeling technique can significantly improve the performance of\nautomatic speech recognition. However, pseudo-labels are often noisy,\ncontaining numerous incorrect tokens. Taking noisy labels as ground-truth in\nthe loss function results in suboptimal performance. Previous works attempted\nto mitigate this issue by either filtering out the nosiest pseudo-labels or\nimproving the overall quality of pseudo-labels. While these methods are\neffective to some extent, it is unrealistic to entirely eliminate incorrect\ntokens in pseudo-labels. In this work, we propose a novel framework named\nalternative pseudo-labeling to tackle the issue of noisy pseudo-labels from the\nperspective of the training objective. The framework comprises several\ncomponents. Firstly, a generalized CTC loss function is introduced to handle\nnoisy pseudo-labels by accepting alternative tokens in the positions of\nincorrect tokens. Applying this loss function in pseudo-labeling requires\ndetecting incorrect tokens in the predicted pseudo-labels. In this work, we\nadopt a confidence-based error detection method that identifies the incorrect\ntokens by comparing their confidence scores with a given threshold, thus\nnecessitating the confidence score to be discriminative. Hence, the second\nproposed technique is the contrastive CTC loss function that widens the\nconfidence gap between the correctly and incorrectly predicted tokens, thereby\nimproving the error detection ability. Additionally, obtaining satisfactory\nperformance with confidence-based error detection typically requires extensive\nthreshold tuning. Instead, we propose an automatic thresholding method that\nuses labeled data as a proxy for determining the threshold, thus saving the\npain of manual tuning.", "published": "2023-08-12 12:13:52", "link": "http://arxiv.org/abs/2308.06547v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "VisIT-Bench: A Benchmark for Vision-Language Instruction Following\n  Inspired by Real-World Use", "abstract": "We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for\nevaluation of instruction-following vision-language models for real-world use.\nOur starting point is curating 70 'instruction families' that we envision\ninstruction tuned vision-language models should be able to address. Extending\nbeyond evaluations like VQAv2 and COCO, tasks range from basic recognition to\ngame playing and creative generation. Following curation, our dataset comprises\n592 test queries, each with a human-authored instruction-conditioned caption.\nThese descriptions surface instruction-specific factors, e.g., for an\ninstruction asking about the accessibility of a storefront for wheelchair\nusers, the instruction-conditioned caption describes ramps/potential obstacles.\nThese descriptions enable 1) collecting human-verified reference outputs for\neach instance; and 2) automatic evaluation of candidate multimodal generations\nusing a text-only LLM, aligning with human judgment. We quantify quality gaps\nbetween models and references using both human and automatic evaluations; e.g.,\nthe top-performing instruction-following model wins against the GPT-4 reference\nin just 27% of the comparison. VisIT-Bench is dynamic to participate,\npractitioners simply submit their model's response on the project website;\nData, code and leaderboard is available at visit-bench.github.io.", "published": "2023-08-12 15:27:51", "link": "http://arxiv.org/abs/2308.06595v4", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Neural Latent Aligner: Cross-trial Alignment for Learning\n  Representations of Complex, Naturalistic Neural Data", "abstract": "Understanding the neural implementation of complex human behaviors is one of\nthe major goals in neuroscience. To this end, it is crucial to find a true\nrepresentation of the neural data, which is challenging due to the high\ncomplexity of behaviors and the low signal-to-ratio (SNR) of the signals. Here,\nwe propose a novel unsupervised learning framework, Neural Latent Aligner\n(NLA), to find well-constrained, behaviorally relevant neural representations\nof complex behaviors. The key idea is to align representations across repeated\ntrials to learn cross-trial consistent information. Furthermore, we propose a\nnovel, fully differentiable time warping model (TWM) to resolve the temporal\nmisalignment of trials. When applied to intracranial electrocorticography\n(ECoG) of natural speaking, our model learns better representations for\ndecoding behaviors than the baseline models, especially in lower dimensional\nspace. The TWM is empirically validated by measuring behavioral coherence\nbetween aligned trials. The proposed framework learns more cross-trial\nconsistent representations than the baselines, and when visualized, the\nmanifold reveals shared neural trajectories across trials.", "published": "2023-08-12 02:35:24", "link": "http://arxiv.org/abs/2308.06443v1", "categories": ["cs.LG", "eess.AS"], "primary_category": "cs.LG"}
{"title": "BigWavGAN: A Wave-To-Wave Generative Adversarial Network for Music\n  Super-Resolution", "abstract": "Generally, Deep Neural Networks (DNNs) are expected to have high performance\nwhen their model size is large. However, large models failed to produce\nhigh-quality results commensurate with their scale in music Super-Resolution\n(SR). We attribute this to that DNNs cannot learn information commensurate with\ntheir size from standard mean square error losses. To unleash the potential of\nlarge DNN models in music SR, we propose BigWavGAN, which incorporates Demucs,\na large-scale wave-to-wave model, with State-Of-The-Art (SOTA) discriminators\nand adversarial training strategies. Our discriminator consists of Multi-Scale\nDiscriminator (MSD) and Multi-Resolution Discriminator (MRD). During inference,\nsince only the generator is utilized, there are no additional parameters or\ncomputational resources required compared to the baseline model Demucs.\nObjective evaluation affirms the effectiveness of BigWavGAN in music SR.\nSubjective evaluations indicate that BigWavGAN can generate music with\nsignificantly high perceptual quality over the baseline model. Notably,\nBigWavGAN surpasses the SOTA music SR model in both simulated and real-world\nscenarios. Moreover, BigWavGAN represents its superior generalization ability\nto address out-of-distribution data. The conducted ablation study reveals the\nimportance of our discriminators and training strategies. Samples are available\non the demo page.", "published": "2023-08-12 06:40:46", "link": "http://arxiv.org/abs/2308.06483v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Flexible Keyword Spotting based on Homogeneous Audio-Text Embedding", "abstract": "Spotting user-defined/flexible keywords represented in text frequently uses\nan expensive text encoder for joint analysis with an audio encoder in an\nembedding space, which can suffer from heterogeneous modality representation\n(i.e., large mismatch) and increased complexity. In this work, we propose a\nnovel architecture to efficiently detect arbitrary keywords based on an\naudio-compliant text encoder which inherently has homogeneous representation\nwith audio embedding, and it is also much smaller than a compatible text\nencoder. Our text encoder converts the text to phonemes using a\ngrapheme-to-phoneme (G2P) model, and then to an embedding using representative\nphoneme vectors, extracted from the paired audio encoder on rich speech\ndatasets. We further augment our method with confusable keyword generation to\ndevelop an audio-text embedding verifier with strong discriminative power.\nExperimental results show that our scheme outperforms the state-of-the-art\nresults on Libriphrase hard dataset, increasing Area Under the ROC Curve (AUC)\nmetric from 84.21% to 92.7% and reducing Equal-Error-Rate (EER) metric from\n23.36% to 14.4%.", "published": "2023-08-12 05:41:15", "link": "http://arxiv.org/abs/2308.06472v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
