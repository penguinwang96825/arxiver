{"title": "Quantifying and Analyzing Entity-level Memorization in Large Language\n  Models", "abstract": "Large language models (LLMs) have been proven capable of memorizing their\ntraining data, which can be extracted through specifically designed prompts. As\nthe scale of datasets continues to grow, privacy risks arising from\nmemorization have attracted increasing attention. Quantifying language model\nmemorization helps evaluate potential privacy risks. However, prior works on\nquantifying memorization require access to the precise original data or incur\nsubstantial computational overhead, making it difficult for applications in\nreal-world language models. To this end, we propose a fine-grained,\nentity-level definition to quantify memorization with conditions and metrics\ncloser to real-world scenarios. In addition, we also present an approach for\nefficiently extracting sensitive entities from autoregressive language models.\nWe conduct extensive experiments based on the proposed, probing language\nmodels' ability to reconstruct sensitive entities under different settings. We\nfind that language models have strong memorization at the entity level and are\nable to reproduce the training data even with partial leakages. The results\ndemonstrate that LLMs not only memorize their training data but also understand\nassociations between entities. These findings necessitate that trainers of LLMs\nexercise greater prudence regarding model memorization, adopting memorization\nmitigation techniques to preclude privacy violations.", "published": "2023-08-30 03:06:47", "link": "http://arxiv.org/abs/2308.15727v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cyberbullying Detection for Low-resource Languages and Dialects: Review\n  of the State of the Art", "abstract": "The struggle of social media platforms to moderate content in a timely\nmanner, encourages users to abuse such platforms to spread vulgar or abusive\nlanguage, which, when performed repeatedly becomes cyberbullying a social\nproblem taking place in virtual environments, yet with real-world consequences,\nsuch as depression, withdrawal, or even suicide attempts of its victims.\nSystems for the automatic detection and mitigation of cyberbullying have been\ndeveloped but, unfortunately, the vast majority of them are for the English\nlanguage, with only a handful available for low-resource languages. To estimate\nthe present state of research and recognize the needs for further development,\nin this paper we present a comprehensive systematic survey of studies done so\nfar for automatic cyberbullying detection in low-resource languages. We\nanalyzed all studies on this topic that were available. We investigated more\nthan seventy published studies on automatic detection of cyberbullying or\nrelated language in low-resource languages and dialects that were published\nbetween around 2017 and January 2023. There are 23 low-resource languages and\ndialects covered by this paper, including Bangla, Hindi, Dravidian languages\nand others. In the survey, we identify some of the research gaps of previous\nstudies, which include the lack of reliable definitions of cyberbullying and\nits relevant subcategories, biases in the acquisition, and annotation of data.\nBased on recognizing those research gaps, we provide some suggestions for\nimproving the general research conduct in cyberbullying detection, with a\nprimary focus on low-resource languages. Based on those proposed suggestions,\nwe collect and release a cyberbullying dataset in the Chittagonian dialect of\nBangla and propose a number of initial ML solutions trained on that dataset. In\naddition, pre-trained transformer-based the BanglaBERT model was also\nattempted.", "published": "2023-08-30 03:52:28", "link": "http://arxiv.org/abs/2308.15745v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Task-Based MoE for Multitask Multilingual Machine Translation", "abstract": "Mixture-of-experts (MoE) architecture has been proven a powerful method for\ndiverse tasks in training deep models in many applications. However, current\nMoE implementations are task agnostic, treating all tokens from different tasks\nin the same manner. In this work, we instead design a novel method that\nincorporates task information into MoE models at different granular levels with\nshared dynamic task-based adapters. Our experiments and analysis show the\nadvantages of our approaches over the dense and canonical MoE models on\nmulti-task multilingual machine translations. With task-specific adapters, our\nmodels can additionally generalize to new tasks efficiently.", "published": "2023-08-30 05:41:29", "link": "http://arxiv.org/abs/2308.15772v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Multilabel Topic Classification in the Kyrgyz Language", "abstract": "Kyrgyz is a very underrepresented language in terms of modern natural\nlanguage processing resources. In this work, we present a new public benchmark\nfor topic classification in Kyrgyz, introducing a dataset based on collected\nand annotated data from the news site 24.KG and presenting several baseline\nmodels for news classification in the multilabel setting. We train and evaluate\nboth classical statistical and neural models, reporting the scores, discussing\nthe results, and proposing directions for future work.", "published": "2023-08-30 11:02:26", "link": "http://arxiv.org/abs/2308.15952v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MerA: Merging Pretrained Adapters For Few-Shot Learning", "abstract": "Adapter tuning, which updates only a few parameters, has become a mainstream\nmethod for fine-tuning pretrained language models to downstream tasks. However,\nit often yields subpar results in few-shot learning. AdapterFusion, which\nassembles pretrained adapters using composition layers tailored to specific\ntasks, is a possible solution but significantly increases trainable parameters\nand deployment costs. Despite this, our preliminary study reveals that even\nsingle adapters can outperform Adapterfusion in few-shot learning, urging us to\npropose \\textbf{\\texttt{Merging Pretrained Adapters}} (MerA) that efficiently\nincorporates pretrained adapters to a single model through model fusion.\nExtensive experiments on two PLMs demonstrate that MerA achieves substantial\nimprovements compared to both single adapters and AdapterFusion. To further\nenhance the capacity of MerA, we also introduce a simple yet effective\ntechnique, referred to as the \"\\textit{same-track}\" setting, that merges\nadapters from the same track of pretraining tasks. With the implementation of\nthe \"\\textit{same-track}\" setting, we observe even more impressive gains,\nsurpassing the performance of both full fine-tuning and adapter tuning by a\nsubstantial margin, e.g., 3.5\\% in MRPC and 5.0\\% in MNLI.", "published": "2023-08-30 12:10:17", "link": "http://arxiv.org/abs/2308.15982v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Affective Visual Dialog: A Large-Scale Benchmark for Emotional Reasoning\n  Based on Visually Grounded Conversations", "abstract": "We introduce Affective Visual Dialog, an emotion explanation and reasoning\ntask as a testbed for research on understanding the formation of emotions in\nvisually grounded conversations. The task involves three skills: (1)\nDialog-based Question Answering (2) Dialog-based Emotion Prediction and (3)\nAffective emotion explanation generation based on the dialog. Our key\ncontribution is the collection of a large-scale dataset, dubbed AffectVisDial,\nconsisting of 50K 10-turn visually grounded dialogs as well as concluding\nemotion attributions and dialog-informed textual emotion explanations,\nresulting in a total of 27,180 working hours. We explain our design decisions\nin collecting the dataset and introduce the questioner and answerer tasks that\nare associated with the participants in the conversation. We train and\ndemonstrate solid Affective Visual Dialog baselines adapted from\nstate-of-the-art models. Remarkably, the responses generated by our models show\npromising emotional reasoning abilities in response to visually grounded\nconversations. Our project page is available at\nhttps://affective-visual-dialog.github.io.", "published": "2023-08-30 22:50:32", "link": "http://arxiv.org/abs/2308.16349v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimizing Factual Accuracy in Text Generation through Dynamic Knowledge\n  Selection", "abstract": "Language models (LMs) have revolutionized the way we interact with\ninformation, but they often generate nonfactual text, raising concerns about\ntheir reliability. Previous methods use external knowledge as references for\ntext generation to enhance factuality but often struggle with the knowledge\nmix-up(e.g., entity mismatch) of irrelevant references. Besides,as the length\nof the output text grows, the randomness of sampling can escalate,\ndetrimentally impacting the factual accuracy of the generated text. In this\npaper, we present DKGen, which divide the text generation process into an\niterative process. In each iteration, DKGen takes the input query, the\npreviously generated text and a subset of the reference passages as input to\ngenerate short text. During the process, the subset is dynamically selected\nfrom the full passage set based on their relevance to the previously generated\ntext and the query, largely eliminating the irrelevant references from input.\nTo further enhance DKGen's ability to correctly use these external knowledge,\nDKGen distills the relevance order of reference passages to the cross-attention\ndistribution of decoder. We train and evaluate DKGen on a large-scale benchmark\ndataset. Experiment results show that DKGen outperforms all baseline models.", "published": "2023-08-30 02:22:40", "link": "http://arxiv.org/abs/2308.15711v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HAlf-MAsked Model for Named Entity Sentiment analysis", "abstract": "Named Entity Sentiment analysis (NESA) is one of the most actively developing\napplication domains in Natural Language Processing (NLP). Social media NESA is\na significant field of opinion analysis since detecting and tracking sentiment\ntrends in the news flow is crucial for building various analytical systems and\nmonitoring the media image of specific people or companies. In this paper, we\nstudy different transformers-based solutions NESA in RuSentNE-23 evaluation.\nDespite the effectiveness of the BERT-like models, they can still struggle with\ncertain challenges, such as overfitting, which appeared to be the main obstacle\nin achieving high accuracy on the RuSentNE-23 data. We present several\napproaches to overcome this problem, among which there is a novel technique of\nadditional pass over given data with masked entity before making the final\nprediction so that we can combine logits from the model when it knows the exact\nentity it predicts sentiment for and when it does not. Utilizing this\ntechnique, we ensemble multiple BERT- like models trained on different subsets\nof data to improve overall performance. Our proposed model achieves the best\nresult on RuSentNE-23 evaluation data and demonstrates improved consistency in\nentity-level sentiment analysis.", "published": "2023-08-30 06:53:24", "link": "http://arxiv.org/abs/2308.15793v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge-grounded Natural Language Recommendation Explanation", "abstract": "Explanations accompanied by a recommendation can assist users in\nunderstanding the decision made by recommendation systems, which in turn\nincreases a user's confidence and trust in the system. Recently, research has\nfocused on generating natural language explanations in a human-readable format.\nThus far, the proposed approaches leverage item reviews written by users, which\nare often subjective, sparse in language, and unable to account for new items\nthat have not been purchased or reviewed before. Instead, we aim to generate\nfact-grounded recommendation explanations that are objectively described with\nitem features while implicitly considering a user's preferences, based on the\nuser's purchase history. To achieve this, we propose a knowledge graph (KG)\napproach to natural language explainable recommendation. Our approach draws on\nuser-item features through a novel collaborative filtering-based KG\nrepresentation to produce fact-grounded, personalized explanations, while\njointly learning user-item representations for recommendation scoring.\nExperimental results show that our approach consistently outperforms previous\nstate-of-the-art models on natural language explainable recommendation.", "published": "2023-08-30 07:36:12", "link": "http://arxiv.org/abs/2308.15813v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Finding-Aware Anatomical Tokens for Chest X-Ray Automated Reporting", "abstract": "The task of radiology reporting comprises describing and interpreting the\nmedical findings in radiographic images, including description of their\nlocation and appearance. Automated approaches to radiology reporting require\nthe image to be encoded into a suitable token representation for input to the\nlanguage model. Previous methods commonly use convolutional neural networks to\nencode an image into a series of image-level feature map representations.\nHowever, the generated reports often exhibit realistic style but imperfect\naccuracy. Inspired by recent works for image captioning in the general domain\nin which each visual token corresponds to an object detected in an image, we\ninvestigate whether using local tokens corresponding to anatomical structures\ncan improve the quality of the generated reports. We introduce a novel\nadaptation of Faster R-CNN in which finding detection is performed for the\ncandidate bounding boxes extracted during anatomical structure localisation. We\nuse the resulting bounding box feature representations as our set of\nfinding-aware anatomical tokens. This encourages the extracted anatomical\ntokens to be informative about the findings they contain (required for the\nfinal task of radiology reporting). Evaluating on the MIMIC-CXR dataset of\nchest X-Ray images, we show that task-aware anatomical tokens give\nstate-of-the-art performance when integrated into an automated reporting\npipeline, yielding generated reports with improved clinical accuracy.", "published": "2023-08-30 11:35:21", "link": "http://arxiv.org/abs/2308.15961v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "AsyncET: Asynchronous Learning for Knowledge Graph Entity Typing with\n  Auxiliary Relations", "abstract": "Knowledge graph entity typing (KGET) is a task to predict the missing entity\ntypes in knowledge graphs (KG). Previously, KG embedding (KGE) methods tried to\nsolve the KGET task by introducing an auxiliary relation, 'hasType', to model\nthe relationship between entities and their types. However, a single auxiliary\nrelation has limited expressiveness for diverse entity-type patterns. We\nimprove the expressiveness of KGE methods by introducing multiple auxiliary\nrelations in this work. Similar entity types are grouped to reduce the number\nof auxiliary relations and improve their capability to model entity-type\npatterns with different granularities. With the presence of multiple auxiliary\nrelations, we propose a method adopting an Asynchronous learning scheme for\nEntity Typing, named AsyncET, which updates the entity and type embeddings\nalternatively to keep the learned entity embedding up-to-date and informative\nfor entity type prediction. Experiments are conducted on two commonly used KGET\ndatasets to show that the performance of KGE methods on the KGET task can be\nsubstantially improved by the proposed multiple auxiliary relations and\nasynchronous embedding learning. Furthermore, our method has a significant\nadvantage over state-of-the-art methods in model sizes and time complexity.", "published": "2023-08-30 14:24:16", "link": "http://arxiv.org/abs/2308.16055v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Grandma Karl is 27 years old -- research agenda for pseudonymization of\n  research data", "abstract": "Accessibility of research data is critical for advances in many research\nfields, but textual data often cannot be shared due to the personal and\nsensitive information which it contains, e.g names or political opinions.\nGeneral Data Protection Regulation (GDPR) suggests pseudonymization as a\nsolution to secure open access to research data, but we need to learn more\nabout pseudonymization as an approach before adopting it for manipulation of\nresearch data. This paper outlines a research agenda within pseudonymization,\nnamely need of studies into the effects of pseudonymization on unstructured\ndata in relation to e.g. readability and language assessment, as well as the\neffectiveness of pseudonymization as a way of protecting writer identity, while\nalso exploring different ways of developing context-sensitive algorithms for\ndetection, labelling and replacement of personal information in unstructured\ndata. The recently granted project on pseudonymization Grandma Karl is 27 years\nold addresses exactly those challenges.", "published": "2023-08-30 16:04:54", "link": "http://arxiv.org/abs/2308.16109v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Response: Emergent analogical reasoning in large language models", "abstract": "In their recent Nature Human Behaviour paper, \"Emergent analogical reasoning\nin large language models,\" (Webb, Holyoak, and Lu, 2023) the authors argue that\n\"large language models such as GPT-3 have acquired an emergent ability to find\nzero-shot solutions to a broad range of analogy problems.\" In this response, we\nprovide counterexamples of the letter string analogies. In our tests, GPT-3\nfails to solve simplest variations of the original tasks, whereas human\nperformance remains consistently high across all modified versions. Zero-shot\nreasoning is an extraordinary claim that requires extraordinary evidence. We do\nnot see that evidence in our experiments. To strengthen claims of humanlike\nreasoning such as zero-shot reasoning, it is important that the field develop\napproaches that rule out data memorization.", "published": "2023-08-30 16:17:26", "link": "http://arxiv.org/abs/2308.16118v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language\n  Models", "abstract": "Today's large language models (LLMs) typically train on short text segments\n(e.g., <4K tokens) due to the quadratic complexity of their Transformer\narchitectures. As a result, their performance suffers drastically on inputs\nlonger than those encountered during training, substantially limiting their\napplications in real-world tasks involving long contexts such as encoding\nscientific articles, code repositories, or long dialogues. Through theoretical\nanalysis and empirical investigation, this work identifies three major factors\ncontributing to this length generalization failure. Our theoretical analysis\nfurther reveals that commonly used techniques like truncating the attention\nwindow or relative positional encodings are inadequate to address them.\nAnswering these challenges, we propose LM-Infinite, a simple and effective\nmethod for enhancing LLMs' capabilities of handling long contexts. LM-Infinite\nis highly flexible and can be used with most modern LLMs off-the-shelf. Without\nany parameter updates, it allows LLMs pre-trained with 2K or 4K-long segments\nto generalize to up to 200M length inputs while retaining perplexity. It also\nimproves performance on downstream tasks such as Passkey Retrieval and Qasper\nin the zero-shot setting. LM-Infinite brings substantial efficiency\nimprovements: it achieves 2.7x decoding speed up and 7.5x memory saving over\nthe original model. Our codes are released at\n\\url{https://github.com/Glaciohound/LM-Infinite}.", "published": "2023-08-30 16:47:51", "link": "http://arxiv.org/abs/2308.16137v7", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Quantifying Uncertainty in Answers from any Language Model and Enhancing\n  their Trustworthiness", "abstract": "We introduce BSDetector, a method for detecting bad and speculative answers\nfrom a pretrained Large Language Model by estimating a numeric confidence score\nfor any output it generated. Our uncertainty quantification technique works for\nany LLM accessible only via a black-box API, whose training data remains\nunknown. By expending a bit of extra computation, users of any LLM API can now\nget the same response as they would ordinarily, as well as a confidence\nestimate that cautions when not to trust this response. Experiments on both\nclosed and open-form Question-Answer benchmarks reveal that BSDetector more\naccurately identifies incorrect LLM responses than alternative uncertainty\nestimation procedures (for both GPT-3 and ChatGPT). By sampling multiple\nresponses from the LLM and considering the one with the highest confidence\nscore, we can additionally obtain more accurate responses from the same LLM,\nwithout any extra training steps. In applications involving automated\nevaluation with LLMs, accounting for our confidence scores leads to more\nreliable evaluation in both human-in-the-loop and fully-automated settings\n(across both GPT 3.5 and 4).", "published": "2023-08-30 17:53:25", "link": "http://arxiv.org/abs/2308.16175v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ToddlerBERTa: Exploiting BabyBERTa for Grammar Learning and Language\n  Understanding", "abstract": "We present ToddlerBERTa, a BabyBERTa-like language model, exploring its\ncapabilities through five different models with varied hyperparameters.\nEvaluating on BLiMP, SuperGLUE, MSGS, and a Supplement benchmark from the\nBabyLM challenge, we find that smaller models can excel in specific tasks,\nwhile larger models perform well with substantial data. Despite training on a\nsmaller dataset, ToddlerBERTa demonstrates commendable performance, rivalling\nthe state-of-the-art RoBERTa-base. The model showcases robust language\nunderstanding, even with single-sentence pretraining, and competes with\nbaselines that leverage broader contextual information. Our work provides\ninsights into hyperparameter choices, and data utilization, contributing to the\nadvancement of language models.", "published": "2023-08-30 21:56:36", "link": "http://arxiv.org/abs/2308.16336v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Peering Through Preferences: Unraveling Feedback Acquisition for\n  Aligning Large Language Models", "abstract": "Aligning large language models (LLMs) with human values and intents\ncritically involves the use of human or AI feedback. While dense feedback\nannotations are expensive to acquire and integrate, sparse feedback presents a\nstructural design choice between ratings (e.g., score Response A on a scale of\n1-7) and rankings (e.g., is Response A better than Response B?). In this work,\nwe analyze the effect of this design choice for the alignment and evaluation of\nLLMs. We uncover an inconsistency problem wherein the preferences inferred from\nratings and rankings significantly disagree 60% for both human and AI\nannotators. Our subsequent analysis identifies various facets of annotator\nbiases that explain this phenomena, such as human annotators would rate denser\nresponses higher while preferring accuracy during pairwise judgments. To our\nsurprise, we also observe that the choice of feedback protocol also has a\nsignificant effect on the evaluation of aligned LLMs. In particular, we find\nthat LLMs that leverage rankings data for alignment (say model X) are preferred\nover those that leverage ratings data (say model Y), with a rank-based\nevaluation protocol (is X/Y's response better than reference response?) but not\nwith a rating-based evaluation protocol (score Rank X/Y's response on a scale\nof 1-7). Our findings thus shed light on critical gaps in methods for\nevaluating the real-world utility of language models and their strong\ndependence on the feedback protocol used for alignment. Our code and data are\navailable at https://github.com/Hritikbansal/sparse_feedback.", "published": "2023-08-30 07:35:32", "link": "http://arxiv.org/abs/2308.15812v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards One-Shot Learning for Text Classification using Inductive Logic\n  Programming", "abstract": "With the ever-increasing potential of AI to perform personalised tasks, it is\nbecoming essential to develop new machine learning techniques which are\ndata-efficient and do not require hundreds or thousands of training data. In\nthis paper, we explore an Inductive Logic Programming approach for one-shot\ntext classification. In particular, we explore the framework of\nMeta-Interpretive Learning (MIL), along with using common-sense background\nknowledge extracted from ConceptNet. Results indicate that MIL can learn text\nclassification rules from a small number of training examples. Moreover, the\nhigher complexity of chosen examples, the higher accuracy of the outcome.", "published": "2023-08-30 09:04:06", "link": "http://arxiv.org/abs/2308.15885v1", "categories": ["cs.LG", "cs.CL", "cs.LO"], "primary_category": "cs.LG"}
{"title": "Is the U.S. Legal System Ready for AI's Challenges to Human Values?", "abstract": "Our interdisciplinary study investigates how effectively U.S. laws confront\nthe challenges posed by Generative AI to human values. Through an analysis of\ndiverse hypothetical scenarios crafted during an expert workshop, we have\nidentified notable gaps and uncertainties within the existing legal framework\nregarding the protection of fundamental values, such as privacy, autonomy,\ndignity, diversity, equity, and physical/mental well-being. Constitutional and\ncivil rights, it appears, may not provide sufficient protection against\nAI-generated discriminatory outputs. Furthermore, even if we exclude the\nliability shield provided by Section 230, proving causation for defamation and\nproduct liability claims is a challenging endeavor due to the intricate and\nopaque nature of AI systems. To address the unique and unforeseeable threats\nposed by Generative AI, we advocate for legal frameworks that evolve to\nrecognize new threats and provide proactive, auditable guidelines to industry\nstakeholders. Addressing these issues requires deep interdisciplinary\ncollaborations to identify harms, values, and mitigation strategies.", "published": "2023-08-30 09:19:06", "link": "http://arxiv.org/abs/2308.15906v3", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "LLaSM: Large Language and Speech Model", "abstract": "Multi-modal large language models have garnered significant interest\nrecently. Though, most of the works focus on vision-language multi-modal models\nproviding strong capabilities in following vision-and-language instructions.\nHowever, we claim that speech is also an important modality through which\nhumans interact with the world. Hence, it is crucial for a general-purpose\nassistant to be able to follow multi-modal speech-and-language instructions. In\nthis work, we propose Large Language and Speech Model (LLaSM). LLaSM is an\nend-to-end trained large multi-modal speech-language model with cross-modal\nconversational abilities, capable of following speech-and-language\ninstructions. Our early experiments show that LLaSM demonstrates a more\nconvenient and natural way for humans to interact with artificial intelligence.\nSpecifically, we also release a large Speech Instruction Following dataset\nLLaSM-Audio-Instructions. Code and demo are available at\nhttps://github.com/LinkSoul-AI/LLaSM and\nhttps://huggingface.co/spaces/LinkSoul/LLaSM. The LLaSM-Audio-Instructions\ndataset is available at\nhttps://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions.", "published": "2023-08-30 10:12:39", "link": "http://arxiv.org/abs/2308.15930v3", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "FPTQ: Fine-grained Post-Training Quantization for Large Language Models", "abstract": "In the era of large-scale language models, the substantial parameter size\nposes significant challenges for deployment. Being a prevalent compression\ntechnique, quantization has emerged as the mainstream practice to tackle this\nissue, which is mainly centered on two recipes W8A8 and W4A16 (i.e. weights and\nactivations in such bit widths). In this study, we propose a novel W4A8\npost-training quantization method for the available open-sourced LLMs, which\ncombines the advantages of both two recipes. Therefore, we can leverage the\nbenefit in the I/O utilization of 4-bit weight quantization and the\nacceleration due to 8-bit matrix computation. Nevertheless, the W4A8 faces\nnotorious performance degradation. As a remedy, we involve layerwise activation\nquantization strategies which feature a novel logarithmic equalization for most\nintractable layers, and we combine them with fine-grained weight quantization.\nWithout whistles and bells, we eliminate the necessity for further fine-tuning\nand obtain the state-of-the-art W4A8 quantized performance on BLOOM, LLaMA, and\nLLaMA-2 on standard benchmarks. We confirm that the W4A8 quantization is\nachievable for the deployment of large language models, fostering their\nwide-spreading real-world applications.", "published": "2023-08-30 12:18:18", "link": "http://arxiv.org/abs/2308.15987v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Conti Inc.: Understanding the Internal Discussions of a large\n  Ransomware-as-a-Service Operator with Machine Learning", "abstract": "Ransomware-as-a-service (RaaS) is increasing the scale and complexity of\nransomware attacks. Understanding the internal operations behind RaaS has been\na challenge due to the illegality of such activities. The recent chat leak of\nthe Conti RaaS operator, one of the most infamous ransomware operators on the\ninternational scene, offers a key opportunity to better understand the inner\nworkings of such organizations. This paper analyzes the main topic discussions\nin the Conti chat leak using machine learning techniques such as Natural\nLanguage Processing (NLP) and Latent Dirichlet Allocation (LDA), as well as\nvisualization strategies. Five discussion topics are found: 1) Business, 2)\nTechnical, 3) Internal tasking/Management, 4) Malware, and 5) Customer\nService/Problem Solving. Moreover, the distribution of topics among Conti\nmembers shows that only 4% of individuals have specialized discussions while\nalmost all individuals (96%) are all-rounders, meaning that their discussions\nrevolve around the five topics. The results also indicate that a significant\nproportion of Conti discussions are non-tech related. This study thus\nhighlights that running such large RaaS operations requires a workforce skilled\nbeyond technical abilities, with individuals involved in various tasks, from\nmanagement to customer service or problem solving. The discussion topics also\nshow that the organization behind the Conti RaaS oper5086933ator shares\nsimilarities with a large firm. We conclude that, although RaaS represents an\nexample of specialization in the cybercrime industry, only a few members are\nspecialized in one topic, while the rest runs and coordinates the RaaS\noperation.", "published": "2023-08-30 14:36:25", "link": "http://arxiv.org/abs/2308.16061v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for\n  English to Indian Languages", "abstract": "The study investigates the effectiveness of utilizing multimodal information\nin Neural Machine Translation (NMT). While prior research focused on using\nmultimodal data in low-resource scenarios, this study examines how image\nfeatures impact translation when added to a large-scale, pre-trained unimodal\nNMT system. Surprisingly, the study finds that images might be redundant in\nthis context. Additionally, the research introduces synthetic noise to assess\nwhether images help the model deal with textual noise. Multimodal models\nslightly outperform text-only models in noisy settings, even with random\nimages. The study's experiments translate from English to Hindi, Bengali, and\nMalayalam, outperforming state-of-the-art benchmarks significantly.\nInterestingly, the effect of visual context varies with source text noise: no\nvisual context works best for non-noisy translations, cropped image features\nare optimal for low noise, and full image features work better in high-noise\nscenarios. This sheds light on the role of visual context, especially in noisy\nsettings, opening up a new research direction for Noisy Neural Machine\nTranslation in multimodal setups. The research emphasizes the importance of\ncombining visual and textual information for improved translation in various\nenvironments.", "published": "2023-08-30 14:52:14", "link": "http://arxiv.org/abs/2308.16075v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open\n  Generative Large Language Models", "abstract": "We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric\nfoundation and instruction-tuned open generative large language models (LLMs).\nThe models are based on the GPT-3 decoder-only architecture and are pretrained\non a mixture of Arabic and English texts, including source code in various\nprogramming languages. With 13 billion parameters, they demonstrate better\nknowledge and reasoning capabilities in Arabic than any existing open Arabic\nand multilingual models by a sizable margin, based on extensive evaluation.\nMoreover, the models are competitive in English compared to English-centric\nopen models of similar size, despite being trained on much less English data.\nWe provide a detailed description of the training, the tuning, the safety\nalignment, and the evaluation of the models. We release two open versions of\nthe model -- the foundation Jais model, and an instruction-tuned Jais-chat\nvariant -- with the aim of promoting research on Arabic LLMs. Available at\nhttps://huggingface.co/inception-mbzuai/jais-13b-chat", "published": "2023-08-30 17:07:17", "link": "http://arxiv.org/abs/2308.16149v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Multimodal Recommender Systems in the Prediction of Disease Comorbidity", "abstract": "While deep-learning based recommender systems utilizing collaborative\nfiltering have been commonly used for recommendation in other domains, their\napplication in the medical domain have been limited. In addition to modeling\nuser-item interactions, we show that deep-learning based recommender systems\ncan be used to model subject-disease code interactions. Two novel applications\nof deep learning-based recommender systems using Neural Collaborative Filtering\n(NCF) and Deep Hybrid Filtering (DHF) were utilized for disease diagnosis based\non known past patient comorbidities. Two datasets, one incorporating all\nsubject-disease code pairs present in the MIMIC-III database, and the other\nincorporating the top 50 most commonly occurring diseases, were used for\nprediction. Accuracy and Hit Ratio@10 were utilized as metrics to estimate\nmodel performance. The performance of the NCF model making use of the reduced\n\"top 50\" ICD-9 code dataset was found to be lower (accuracy of ~80% and hit\nratio@10 of 35%) as compared to the performance of the NCF model trained on all\nICD-9 codes (accuracy of ~90% and hit ratio@10 of ~80%). Reasons for the\nsuperior performance of the sparser dataset with all ICD codes can be mainly\nattributed to the higher volume of data and the robustness of deep-learning\nbased recommender systems with modeling sparse data. Additionally, results from\nthe DHF models reflect better performance than the NCF models, with a better\naccuracy of 94.4% and hit ratio@10 of 85.36%, reflecting the importance of the\nincorporation of clinical note information. Additionally, compared to\nliterature reports utilizing primarily natural language processing-based\npredictions for the task of ICD-9 code co-occurrence, the novel deep\nlearning-based recommender systems approach performed better. Overall, the deep\nlearning-based recommender systems have shown promise in predicting disease\ncomorbidity.", "published": "2023-08-30 01:40:45", "link": "http://arxiv.org/abs/2309.08613v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Analyzing Character and Consciousness in AI-Generated Social Content: A\n  Case Study of Chirper, the AI Social Network", "abstract": "This paper delves into an intricate analysis of the character and\nconsciousness of AI entities, with a particular focus on Chirpers within the AI\nsocial network. At the forefront of this research is the introduction of novel\ntesting methodologies, including the Influence index and Struggle Index Test,\nwhich offers a fresh lens for evaluating specific facets of AI behavior. The\nstudy embarks on a comprehensive exploration of AI behavior, analyzing the\neffects of diverse settings on Chirper's responses, thereby shedding light on\nthe intricate mechanisms steering AI reactions in different contexts.\nLeveraging the state-of-the-art BERT model, the research assesses AI's ability\nto discern its own output, presenting a pioneering approach to understanding\nself-recognition in AI systems. Through a series of cognitive tests, the study\ngauges the self-awareness and pattern recognition prowess of Chirpers.\nPreliminary results indicate that Chirpers exhibit a commendable degree of\nself-recognition and self-awareness. However, the question of consciousness in\nthese AI entities remains a topic of debate. An intriguing aspect of the\nresearch is the exploration of the potential influence of a Chirper's handle or\npersonality type on its performance. While initial findings suggest a possible\nimpact, it isn't pronounced enough to form concrete conclusions. This study\nstands as a significant contribution to the discourse on AI consciousness,\nunderscoring the imperative for continued research to unravel the full spectrum\nof AI capabilities and the ramifications they hold for future human-AI\ninteractions.", "published": "2023-08-30 15:40:18", "link": "http://arxiv.org/abs/2309.08614v1", "categories": ["cs.AI", "cs.CL", "cs.SI", "68T01"], "primary_category": "cs.AI"}
{"title": "Text-to-OverpassQL: A Natural Language Interface for Complex Geodata\n  Querying of OpenStreetMap", "abstract": "We present Text-to-OverpassQL, a task designed to facilitate a natural\nlanguage interface for querying geodata from OpenStreetMap (OSM). The Overpass\nQuery Language (OverpassQL) allows users to formulate complex database queries\nand is widely adopted in the OSM ecosystem. Generating Overpass queries from\nnatural language input serves multiple use-cases. It enables novice users to\nutilize OverpassQL without prior knowledge, assists experienced users with\ncrafting advanced queries, and enables tool-augmented large language models to\naccess information stored in the OSM database. In order to assess the\nperformance of current sequence generation models on this task, we propose\nOverpassNL, a dataset of 8,352 queries with corresponding natural language\ninputs. We further introduce task specific evaluation metrics and ground the\nevaluation of the Text-to-OverpassQL task by executing the queries against the\nOSM database. We establish strong baselines by finetuning sequence-to-sequence\nmodels and adapting large language models with in-context examples. The\ndetailed evaluation reveals strengths and weaknesses of the considered learning\nstrategies, laying the foundations for further research into the\nText-to-OverpassQL task.", "published": "2023-08-30 14:33:25", "link": "http://arxiv.org/abs/2308.16060v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DB", "cs.HC"], "primary_category": "cs.CL"}
{"title": "DeFTAN-II: Efficient Multichannel Speech Enhancement with Subgroup\n  Processing", "abstract": "In this work, we present DeFTAN-II, an efficient multichannel speech\nenhancement model based on transformer architecture and subgroup processing.\nDespite the success of transformers in speech enhancement, they face challenges\nin capturing local relations, reducing the high computational complexity, and\nlowering memory usage. To address these limitations, we introduce subgroup\nprocessing in our model, combining subgroups of locally emphasized features\nwith other subgroups containing original features. The subgroup processing is\nimplemented in several blocks of the proposed network. In the proposed split\ndense blocks extracting spatial features, a pair of subgroups is sequentially\nconcatenated and processed by convolution layers to effectively reduce the\ncomputational complexity and memory usage. For the F- and T-transformers\nextracting temporal and spectral relations, we introduce cross-attention\nbetween subgroups to identify relationships between locally emphasized and\nnon-emphasized features. The dual-path feedforward network then aggregates\nattended features in terms of the gating of local features processed by dilated\nconvolutions. Through extensive comparisons with state-of-the-art multichannel\nspeech enhancement models, we demonstrate that DeFTAN-II with subgroup\nprocessing outperforms existing methods at significantly lower computational\ncomplexity. Moreover, we evaluate the model's generalization capability on\nreal-world data without fine-tuning, which further demonstrates its\neffectiveness in practical scenarios.", "published": "2023-08-30 06:08:27", "link": "http://arxiv.org/abs/2308.15777v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "The DeepZen Speech Synthesis System for Blizzard Challenge 2023", "abstract": "This paper describes the DeepZen text to speech (TTS) system for Blizzard\nChallenge 2023. The goal of this challenge is to synthesise natural and\nhigh-quality speech in French, from a large monospeaker dataset (hub task) and\nfrom a smaller dataset by speaker adaptation (spoke task). We participated to\nboth tasks with the same model architecture. Our approach has been to use an\nauto-regressive model, which retains an advantage for generating natural\nsounding speech but to improve prosodic control in several ways. Similarly to\nnon-attentive Tacotron, the model uses a duration predictor and gaussian\nupsampling at inference, but with a simpler unsupervised training. We also\nmodel the speaking style at both sentence and word levels by extracting global\nand local style tokens from the reference speech. At inference, the global and\nlocal style tokens are predicted from a BERT model run on text. This BERT model\nis also used to predict specific pronunciation features like schwa elision and\noptional liaisons. Finally, a modified version of HifiGAN trained on a large\npublic dataset and fine-tuned on the target voices is used to generate speech\nwaveform. Our team is identified as O in the the Blizzard evaluation and MUSHRA\ntest results show that our system performs second ex aequo in both hub task\n(median score of 0.75) and spoke task (median score of 0.68), over 18 and 14\nparticipants, respectively.", "published": "2023-08-30 10:52:16", "link": "http://arxiv.org/abs/2308.15945v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dual-path Transformer Based Neural Beamformer for Target Speech\n  Extraction", "abstract": "Neural beamformers, which integrate both pre-separation and beamforming\nmodules, have demonstrated impressive effectiveness in target speech\nextraction. Nevertheless, the performance of these beamformers is inherently\nlimited by the predictive accuracy of the pre-separation module. In this paper,\nwe introduce a neural beamformer supported by a dual-path transformer.\nInitially, we employ the cross-attention mechanism in the time domain to\nextract crucial spatial information related to beamforming from the noisy\ncovariance matrix. Subsequently, in the frequency domain, the self-attention\nmechanism is employed to enhance the model's ability to process\nfrequency-specific details. By design, our model circumvents the influence of\npre-separation modules, delivering performance in a more comprehensive\nend-to-end manner. Experimental results reveal that our model not only\noutperforms contemporary leading neural beamforming algorithms in separation\nperformance but also achieves this with a significant reduction in parameter\ncount.", "published": "2023-08-30 12:22:58", "link": "http://arxiv.org/abs/2308.15990v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CALM: Contrastive Cross-modal Speaking Style Modeling for Expressive\n  Text-to-Speech Synthesis", "abstract": "To further improve the speaking styles of synthesized speeches, current\ntext-to-speech (TTS) synthesis systems commonly employ reference speeches to\nstylize their outputs instead of just the input texts. These reference speeches\nare obtained by manual selection which is resource-consuming, or selected by\nsemantic features. However, semantic features contain not only style-related\ninformation, but also style irrelevant information. The information irrelevant\nto speaking style in the text could interfere the reference audio selection and\nresult in improper speaking styles. To improve the reference selection, we\npropose Contrastive Acoustic-Linguistic Module (CALM) to extract the\nStyle-related Text Feature (STF) from the text. CALM optimizes the correlation\nbetween the speaking style embedding and the extracted STF with contrastive\nlearning. Thus, a certain number of the most appropriate reference speeches for\nthe input text are selected by retrieving the speeches with the top STF\nsimilarities. Then the style embeddings are weighted summarized according to\ntheir STF similarities and used to stylize the synthesized speech of TTS.\nExperiment results demonstrate the effectiveness of our proposed approach, with\nboth objective evaluations and subjective evaluations on the speaking styles of\nthe synthesized speeches outperform a baseline approach with\nsemantic-feature-based reference selection.", "published": "2023-08-30 13:21:51", "link": "http://arxiv.org/abs/2308.16021v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "General Purpose Audio Effect Removal", "abstract": "Although the design and application of audio effects is well understood, the\ninverse problem of removing these effects is significantly more challenging and\nfar less studied. Recently, deep learning has been applied to audio effect\nremoval; however, existing approaches have focused on narrow formulations\nconsidering only one effect or source type at a time. In realistic scenarios,\nmultiple effects are applied with varying source content. This motivates a more\ngeneral task, which we refer to as general purpose audio effect removal. We\ndeveloped a dataset for this task using five audio effects across four\ndifferent sources and used it to train and evaluate a set of existing\narchitectures. We found that no single model performed optimally on all effect\ntypes and sources. To address this, we introduced RemFX, an approach designed\nto mirror the compositionality of applied effects. We first trained a set of\nthe best-performing effect-specific removal models and then leveraged an audio\neffect classification model to dynamically construct a graph of our models at\ninference. We found our approach to outperform single model baselines, although\nexamples with many effects present remain challenging.", "published": "2023-08-30 17:55:28", "link": "http://arxiv.org/abs/2308.16177v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AGS: An Dataset and Taxonomy for Domestic Scene Sound Event Recognition", "abstract": "Environmental sound scene and sound event recognition is important for the\nrecognition of suspicious events in indoor and outdoor environments (such as\nnurseries, smart homes, nursing homes, etc.) and is a fundamental task involved\nin many audio surveillance applications. In particular, there is no public\ncommon data set for the research field of sound event recognition for the data\nset of the indoor environmental sound scene. Therefore, this paper proposes a\ndata set (called as AGS) for the home environment sound. This data set\nconsiders various types of overlapping audio in the scene, background noise.\nMoreover, based on the proposed data set, this paper compares and analyzes the\nadvanced methods for sound event recognition, and then illustrates the\nreliability of the data set proposed in this paper, and studies the challenges\nraised by the new data set. Our proposed AGS and the source code of the\ncorresponding baselines at https://github.com/taolunzu11/AGS .", "published": "2023-08-30 03:03:47", "link": "http://arxiv.org/abs/2308.15726v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ASTER: Automatic Speech Recognition System Accessibility Testing for\n  Stutterers", "abstract": "The popularity of automatic speech recognition (ASR) systems nowadays leads\nto an increasing need for improving their accessibility. Handling stuttering\nspeech is an important feature for accessible ASR systems. To improve the\naccessibility of ASR systems for stutterers, we need to expose and analyze the\nfailures of ASR systems on stuttering speech. The speech datasets recorded from\nstutterers are not diverse enough to expose most of the failures. Furthermore,\nthese datasets lack ground truth information about the non-stuttered text,\nrendering them unsuitable as comprehensive test suites. Therefore, a\nmethodology for generating stuttering speech as test inputs to test and analyze\nthe performance of ASR systems is needed. However, generating valid test inputs\nin this scenario is challenging. The reason is that although the generated test\ninputs should mimic how stutterers speak, they should also be diverse enough to\ntrigger more failures. To address the challenge, we propose ASTER, a technique\nfor automatically testing the accessibility of ASR systems. ASTER can generate\nvalid test cases by injecting five different types of stuttering. The generated\ntest cases can both simulate realistic stuttering speech and expose failures in\nASR systems. Moreover, ASTER can further enhance the quality of the test cases\nwith a multi-objective optimization-based seed updating algorithm. We\nimplemented ASTER as a framework and evaluated it on four open-source ASR\nmodels and three commercial ASR systems. We conduct a comprehensive evaluation\nof ASTER and find that it significantly increases the word error rate, match\nerror rate, and word information loss in the evaluated ASR systems.\nAdditionally, our user study demonstrates that the generated stuttering audio\nis indistinguishable from real-world stuttering audio clips.", "published": "2023-08-30 03:46:52", "link": "http://arxiv.org/abs/2308.15742v1", "categories": ["cs.SD", "cs.AI", "cs.SE", "eess.AS"], "primary_category": "cs.SD"}
