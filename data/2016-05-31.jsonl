{"title": "Implementing a Reverse Dictionary, based on word definitions, using a\n  Node-Graph Architecture", "abstract": "In this paper, we outline an approach to build graph-based reverse\ndictionaries using word definitions. A reverse dictionary takes a phrase as an\ninput and outputs a list of words semantically similar to that phrase. It is a\nsolution to the Tip-of-the-Tongue problem. We use a distance-based similarity\nmeasure, computed on a graph, to assess the similarity between a word and the\ninput phrase. We compare the performance of our approach with the Onelook\nReverse Dictionary and a distributional semantics method based on word2vec, and\nshow that our approach is much better than the distributional semantics method,\nand as good as Onelook, on a 3k lexicon. This simple approach sets a new\nperformance baseline for reverse dictionaries.", "published": "2016-05-31 20:09:59", "link": "http://arxiv.org/abs/1606.00025v5", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Hierarchical Question-Image Co-Attention for Visual Question Answering", "abstract": "A number of recent works have proposed attention models for Visual Question\nAnswering (VQA) that generate spatial maps highlighting image regions relevant\nto answering the question. In this paper, we argue that in addition to modeling\n\"where to look\" or visual attention, it is equally important to model \"what\nwords to listen to\" or question attention. We present a novel co-attention\nmodel for VQA that jointly reasons about image and question attention. In\naddition, our model reasons about the question (and consequently the image via\nthe co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional\nconvolution neural networks (CNN). Our model improves the state-of-the-art on\nthe VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA\ndataset. By using ResNet, the performance is further improved to 62.1% for VQA\nand 65.4% for COCO-QA.", "published": "2016-05-31 22:02:01", "link": "http://arxiv.org/abs/1606.00061v5", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Attention Correctness in Neural Image Captioning", "abstract": "Attention mechanisms have recently been introduced in deep learning for\nvarious tasks in natural language processing and computer vision. But despite\ntheir popularity, the \"correctness\" of the implicitly-learned attention maps\nhas only been assessed qualitatively by visualization of several examples. In\nthis paper we focus on evaluating and improving the correctness of attention in\nneural image captioning models. Specifically, we propose a quantitative\nevaluation metric for the consistency between the generated attention maps and\nhuman annotations, using recently released datasets with alignment between\nregions in images and entities in captions. We then propose novel models with\ndifferent levels of explicit supervision for learning attention maps during\ntraining. The supervision can be strong when alignment between regions and\ncaption entities are available, or weak when only object segments and\ncategories are provided. We show on the popular Flickr30k and COCO datasets\nthat introducing supervision of attention maps during training solidly improves\nboth attention correctness and caption quality, showing the promise of making\nmachine perception more human-like.", "published": "2016-05-31 10:04:20", "link": "http://arxiv.org/abs/1605.09553v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Determining the Characteristic Vocabulary for a Specialized Dictionary\n  using Word2vec and a Directed Crawler", "abstract": "Specialized dictionaries are used to understand concepts in specific domains,\nespecially where those concepts are not part of the general vocabulary, or\nhaving meanings that differ from ordinary languages. The first step in creating\na specialized dictionary involves detecting the characteristic vocabulary of\nthe domain in question. Classical methods for detecting this vocabulary involve\ngathering a domain corpus, calculating statistics on the terms found there, and\nthen comparing these statistics to a background or general language corpus.\nTerms which are found significantly more often in the specialized corpus than\nin the background corpus are candidates for the characteristic vocabulary of\nthe domain. Here we present two tools, a directed crawler, and a distributional\nsemantics package, that can be used together, circumventing the need of a\nbackground corpus. Both tools are available on the web.", "published": "2016-05-31 10:31:16", "link": "http://arxiv.org/abs/1605.09564v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
