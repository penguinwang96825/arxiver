{"title": "A Simple and Effective Approach to the Story Cloze Test", "abstract": "In the Story Cloze Test, a system is presented with a 4-sentence prompt to a\nstory, and must determine which one of two potential endings is the 'right'\nending to the story. Previous work has shown that ignoring the training set and\ntraining a model on the validation set can achieve high accuracy on this task\ndue to stylistic differences between the story endings in the training set and\nvalidation and test sets. Following this approach, we present a simpler\nfully-neural approach to the Story Cloze Test using skip-thought embeddings of\nthe stories in a feed-forward network that achieves close to state-of-the-art\nperformance on this task without any feature engineering. We also find that\nconsidering just the last sentence of the prompt instead of the whole prompt\nyields higher accuracy with our approach.", "published": "2018-03-15 00:16:43", "link": "http://arxiv.org/abs/1803.05547v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advancing Connectionist Temporal Classification With Attention Modeling", "abstract": "In this study, we propose advancing all-neural speech recognition by directly\nincorporating attention modeling within the Connectionist Temporal\nClassification (CTC) framework. In particular, we derive new context vectors\nusing time convolution features to model attention as part of the CTC network.\nTo further improve attention modeling, we utilize content information extracted\nfrom a network representing an implicit language model. Finally, we introduce\nvector based attention weights that are applied on context vectors across both\ntime and their individual components. We evaluate our system on a 3400 hours\nMicrosoft Cortana voice assistant task and demonstrate that our proposed model\nconsistently outperforms the baseline model achieving about 20% relative\nreduction in word error rates.", "published": "2018-03-15 01:19:21", "link": "http://arxiv.org/abs/1803.05563v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advancing Acoustic-to-Word CTC Model", "abstract": "The acoustic-to-word model based on the connectionist temporal classification\n(CTC) criterion was shown as a natural end-to-end (E2E) model directly\ntargeting words as output units. However, the word-based CTC model suffers from\nthe out-of-vocabulary (OOV) issue as it can only model limited number of words\nin the output layer and maps all the remaining words into an OOV output node.\nHence, such a word-based CTC model can only recognize the frequent words\nmodeled by the network output nodes. Our first attempt to improve the\nacoustic-to-word model is a hybrid CTC model which consults a letter-based CTC\nwhen the word-based CTC model emits OOV tokens during testing time. Then, we\npropose a much better solution by training a mixed-unit CTC model which\ndecomposes all the OOV words into sequences of frequent words and multi-letter\nunits. Evaluated on a 3400 hours Microsoft Cortana voice assistant task, the\nfinal acoustic-to-word solution improves the baseline word-based CTC by\nrelative 12.09% word error rate (WER) reduction when combined with our proposed\nattention CTC. Such an E2E model without using any language model (LM) or\ncomplex decoder outperforms the traditional context-dependent phoneme CTC which\nhas strong LM and decoder by relative 6.79%.", "published": "2018-03-15 01:25:17", "link": "http://arxiv.org/abs/1803.05566v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Achieving Human Parity on Automatic Chinese to English News Translation", "abstract": "Machine translation has made rapid advances in recent years. Millions of\npeople are using it today in online translation systems and mobile applications\nin order to communicate across language barriers. The question naturally arises\nwhether such systems can approach or achieve parity with human translations. In\nthis paper, we first address the problem of how to define and accurately\nmeasure human parity in translation. We then describe Microsoft's machine\ntranslation system and measure the quality of its translations on the widely\nused WMT 2017 news translation task from Chinese to English. We find that our\nlatest neural machine translation system has reached a new state-of-the-art,\nand that the translation quality is at human parity when compared to\nprofessional human translations. We also find that it significantly exceeds the\nquality of crowd-sourced non-professional translations.", "published": "2018-03-15 01:30:58", "link": "http://arxiv.org/abs/1803.05567v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word2Bits - Quantized Word Vectors", "abstract": "Word vectors require significant amounts of memory and storage, posing issues\nto resource limited devices like mobile phones and GPUs. We show that high\nquality quantized word vectors using 1-2 bits per parameter can be learned by\nintroducing a quantization function into Word2Vec. We furthermore show that\ntraining with the quantization function acts as a regularizer. We train word\nvectors on English Wikipedia (2017) and evaluate them on standard word\nsimilarity and analogy tasks and on question answering (SQuAD). Our quantized\nword vectors not only take 8-16x less space than full precision (32 bit) word\nvectors but also outperform them on word similarity tasks and question\nanswering.", "published": "2018-03-15 09:21:34", "link": "http://arxiv.org/abs/1803.05651v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HFL-RC System at SemEval-2018 Task 11: Hybrid Multi-Aspects Model for\n  Commonsense Reading Comprehension", "abstract": "This paper describes the system which got the state-of-the-art results at\nSemEval-2018 Task 11: Machine Comprehension using Commonsense Knowledge. In\nthis paper, we present a neural network called Hybrid Multi-Aspects (HMA)\nmodel, which mimic the human's intuitions on dealing with the multiple-choice\nreading comprehension. In this model, we aim to produce the predictions in\nmultiple aspects by calculating attention among the text, question and choices,\nand combine these results for final predictions. Experimental results show that\nour HMA model could give substantial improvements over the baseline system and\ngot the first place on the final test set leaderboard with the accuracy of\n84.13%.", "published": "2018-03-15 09:30:12", "link": "http://arxiv.org/abs/1803.05655v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structure Regularized Neural Network for Entity Relation Classification\n  for Chinese Literature Text", "abstract": "Relation classification is an important semantic processing task in the field\nof natural language processing. In this paper, we propose the task of relation\nclassification for Chinese literature text. A new dataset of Chinese literature\ntext is constructed to facilitate the study in this task. We present a novel\nmodel, named Structure Regularized Bidirectional Recurrent Convolutional Neural\nNetwork (SR-BRCNN), to identify the relation between entities. The proposed\nmodel learns relation representations along the shortest dependency path (SDP)\nextracted from the structure regularized dependency tree, which has the\nbenefits of reducing the complexity of the whole model. Experimental results\nshow that the proposed method significantly improves the F1 score by 10.3, and\noutperforms the state-of-the-art approaches on Chinese literature text.", "published": "2018-03-15 09:45:58", "link": "http://arxiv.org/abs/1803.05662v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RUSSE'2018: A Shared Task on Word Sense Induction for the Russian\n  Language", "abstract": "The paper describes the results of the first shared task on word sense\ninduction (WSI) for the Russian language. While similar shared tasks were\nconducted in the past for some Romance and Germanic languages, we explore the\nperformance of sense induction and disambiguation methods for a Slavic language\nthat shares many features with other Slavic languages, such as rich morphology\nand virtually free word order. The participants were asked to group contexts of\na given word in accordance with its senses that were not provided beforehand.\nFor instance, given a word \"bank\" and a set of contexts for this word, e.g.\n\"bank is a financial institution that accepts deposits\" and \"river bank is a\nslope beside a body of water\", a participant was asked to cluster such contexts\nin the unknown in advance number of clusters corresponding to, in this case,\nthe \"company\" and the \"area\" senses of the word \"bank\". For the purpose of this\nevaluation campaign, we developed three new evaluation datasets based on sense\ninventories that have different sense granularity. The contexts in these\ndatasets were sampled from texts of Wikipedia, the academic corpus of Russian,\nand an explanatory dictionary of Russian. Overall, 18 teams participated in the\ncompetition submitting 383 models. Multiple teams managed to substantially\noutperform competitive state-of-the-art baselines from the previous years based\non sense embeddings.", "published": "2018-03-15 15:08:36", "link": "http://arxiv.org/abs/1803.05795v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RUSSE: The First Workshop on Russian Semantic Similarity", "abstract": "The paper gives an overview of the Russian Semantic Similarity Evaluation\n(RUSSE) shared task held in conjunction with the Dialogue 2015 conference.\nThere exist a lot of comparative studies on semantic similarity, yet no\nanalysis of such measures was ever performed for the Russian language.\nExploring this problem for the Russian language is even more interesting,\nbecause this language has features, such as rich morphology and free word\norder, which make it significantly different from English, German, and other\nwell-studied languages. We attempt to bridge this gap by proposing a shared\ntask on the semantic similarity of Russian nouns. Our key contribution is an\nevaluation methodology based on four novel benchmark datasets for the Russian\nlanguage. Our analysis of the 105 submissions from 19 teams reveals that\nsuccessful approaches for English, such as distributional and skip-gram models,\nare directly applicable to Russian as well. On the one hand, the best results\nin the contest were obtained by sophisticated supervised models that combine\nevidence from different sources. On the other hand, completely unsupervised\napproaches, such as a skip-gram model estimated on a large-scale corpus, were\nable score among the top 5 systems.", "published": "2018-03-15 15:50:58", "link": "http://arxiv.org/abs/1803.05820v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enriching Frame Representations with Distributionally Induced Senses", "abstract": "We introduce a new lexical resource that enriches the Framester knowledge\ngraph, which links Framnet, WordNet, VerbNet and other resources, with semantic\nfeatures from text corpora. These features are extracted from distributionally\ninduced sense inventories and subsequently linked to the manually-constructed\nframe representations to boost the performance of frame disambiguation in\ncontext. Since Framester is a frame-based knowledge graph, which enables\nfull-fledged OWL querying and reasoning, our resource paves the way for the\ndevelopment of novel, deeper semantic-aware applications that could benefit\nfrom the combination of knowledge from text and complex symbolic\nrepresentations of events and participants. Together with the resource we also\nprovide the software we developed for the evaluation in the task of Word Frame\nDisambiguation (WFD).", "published": "2018-03-15 16:06:33", "link": "http://arxiv.org/abs/1803.05829v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RankME: Reliable Human Ratings for Natural Language Generation", "abstract": "Human evaluation for natural language generation (NLG) often suffers from\ninconsistent user ratings. While previous research tends to attribute this\nproblem to individual user preferences, we show that the quality of human\njudgements can also be improved by experimental design. We present a novel\nrank-based magnitude estimation method (RankME), which combines the use of\ncontinuous scales and relative assessments. We show that RankME significantly\nimproves the reliability and consistency of human ratings compared to\ntraditional evaluation methods. In addition, we show that it is possible to\nevaluate NLG systems according to multiple, distinct criteria, which is\nimportant for error analysis. Finally, we demonstrate that RankME, in\ncombination with Bayesian estimation of system quality, is a cost-effective\nalternative for ranking multiple NLG systems.", "published": "2018-03-15 18:10:45", "link": "http://arxiv.org/abs/1803.05928v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Study of Recent Contributions on Information Extraction", "abstract": "This paper reports on modern approaches in Information Extraction (IE) and\nits two main sub-tasks of Named Entity Recognition (NER) and Relation\nExtraction (RE). Basic concepts and the most recent approaches in this area are\nreviewed, which mainly include Machine Learning (ML) based approaches and the\nmore recent trend to Deep Learning (DL) based methods.", "published": "2018-03-15 10:04:27", "link": "http://arxiv.org/abs/1803.05667v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Neural Text Generation: Past, Present and Beyond", "abstract": "This paper presents a systematic survey on recent development of neural text\ngeneration models. Specifically, we start from recurrent neural network\nlanguage models with the traditional maximum likelihood estimation training\nscheme and point out its shortcoming for text generation. We thus introduce the\nrecently proposed methods for text generation based on reinforcement learning,\nre-parametrization tricks and generative adversarial nets (GAN) techniques. We\ncompare different properties of these models and the corresponding techniques\nto handle their common problems such as gradient vanishing and generation\ndiversity. Finally, we conduct a benchmarking experiment with different types\nof neural text generation models on two well-known datasets and discuss the\nempirical results along with the aforementioned model properties.", "published": "2018-03-15 07:54:30", "link": "http://arxiv.org/abs/1803.07133v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Underspread/Overspread Classification of Random Processes", "abstract": "We study the impact of the recently introduced underspread/overspread\nclassificationon the spectra of processes with square-integrable covariance\nfunctions. We briefly review the most prominent definitions of a time-varying\npower spectrum and point out their limited applicability for {\\em general}\nnonstationary processes. The time-frequency-parametrized approximation of the\nnonstationary Wiener filter provides an excellent example for the main\nconclusion: It is the class of underspread processeswhere a time--varying power\nspectrum can be used in the same manner as the time--invariant power spectrum\nof stationary processes.", "published": "2018-03-15 03:40:13", "link": "http://arxiv.org/abs/1803.05582v1", "categories": ["stat.ME", "eess.AS", "eess.SP", "math.PR"], "primary_category": "stat.ME"}
