{"title": "Leveraging Sentence-level Information with Encoder LSTM for Semantic\n  Slot Filling", "abstract": "Recurrent Neural Network (RNN) and one of its specific architectures, Long\nShort-Term Memory (LSTM), have been widely used for sequence labeling. In this\npaper, we first enhance LSTM-based sequence labeling to explicitly model label\ndependencies. Then we propose another enhancement to incorporate the global\ninformation spanning over the whole input sequence. The latter proposed method,\nencoder-labeler LSTM, first encodes the whole input sequence into a fixed\nlength vector with the encoder LSTM, and then uses this encoded vector as the\ninitial state of another LSTM for sequence labeling. Combining these methods,\nwe can predict the label sequence with considering label dependencies and\ninformation of whole input sequence. In the experiments of a slot filling task,\nwhich is an essential component of natural language understanding, with using\nthe standard ATIS corpus, we achieved the state-of-the-art F1-score of 95.66%.", "published": "2016-01-07 13:32:31", "link": "http://arxiv.org/abs/1601.01530v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Word Embeddings to Item Recommendation", "abstract": "Social network platforms can use the data produced by their users to serve\nthem better. One of the services these platforms provide is recommendation\nservice. Recommendation systems can predict the future preferences of users\nusing their past preferences. In the recommendation systems literature there\nare various techniques, such as neighborhood based methods, machine-learning\nbased methods and matrix-factorization based methods. In this work, a set of\nwell known methods from natural language processing domain, namely Word2Vec, is\napplied to recommendation systems domain. Unlike previous works that use\nWord2Vec for recommendation, this work uses non-textual features, the\ncheck-ins, and it recommends venues to visit/check-in to the target users. For\nthe experiments, a Foursquare check-in dataset is used. The results show that\nuse of continuous vector space representations of items modeled by techniques\nof Word2Vec is promising for making recommendations.", "published": "2016-01-07 00:09:37", "link": "http://arxiv.org/abs/1601.01356v3", "categories": ["cs.LG", "cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Learning to Compose Neural Networks for Question Answering", "abstract": "We describe a question answering model that applies to both images and\nstructured knowledge bases. The model uses natural language strings to\nautomatically assemble neural networks from a collection of composable modules.\nParameters for these modules are learned jointly with network-assembly\nparameters via reinforcement learning, with only (world, question, answer)\ntriples as supervision. Our approach, which we term a dynamic neural model\nnetwork, achieves state-of-the-art results on benchmark datasets in both visual\nand structured domains.", "published": "2016-01-07 21:21:59", "link": "http://arxiv.org/abs/1601.01705v4", "categories": ["cs.CL", "cs.CV", "cs.NE"], "primary_category": "cs.CL"}
