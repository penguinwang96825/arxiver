{"title": "Fashion Recommendation: Outfit Compatibility using GNN", "abstract": "Numerous industries have benefited from the use of machine learning and\nfashion in industry is no exception. By gaining a better understanding of what\nmakes a good outfit, companies can provide useful product recommendations to\ntheir users. In this project, we follow two existing approaches that employ\ngraphs to represent outfits and use modified versions of the Graph neural\nnetwork (GNN) frameworks. Both Node-wise Graph Neural Network (NGNN) and\nHypergraph Neural Network aim to score a set of items according to the outfit\ncompatibility of items. The data used is the Polyvore Dataset which consists of\ncurated outfits with product images and text descriptions for each product in\nan outfit. We recreate the analysis on a subset of this data and compare the\ntwo existing models on their performance on two tasks Fill in the blank (FITB):\nfinding an item that completes an outfit, and Compatibility prediction:\nestimating compatibility of different items grouped as an outfit. We can\nreplicate the results directionally and find that HGNN does have a slightly\nbetter performance on both tasks. On top of replicating the results of the two\npapers we also tried to use embeddings generated from a vision transformer and\nwitness enhanced prediction accuracy across the board", "published": "2024-04-28 00:57:17", "link": "http://arxiv.org/abs/2404.18040v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient LLM Inference with Kcache", "abstract": "Large Language Models(LLMs) have had a profound impact on AI applications,\nparticularly in the domains of long-text comprehension and generation. KV Cache\ntechnology is one of the most widely used techniques in the industry. It\nensures efficient sequence generation by caching previously computed KV states.\nHowever, it also introduces significant memory overhead. We discovered that KV\nCache is not necessary and proposed a novel KCache technique to alleviate the\nmemory bottleneck issue during the LLMs inference process. KCache can be used\ndirectly for inference without any training process, Our evaluations show that\nKCache improves the throughput of popular LLMs by 40% with the baseline, while\nkeeping accuracy.", "published": "2024-04-28 03:11:42", "link": "http://arxiv.org/abs/2404.18057v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual Spelling Correction with Language Model for Low-resource\n  Setting", "abstract": "The task of Spell Correction(SC) in low-resource languages presents a\nsignificant challenge due to the availability of only a limited corpus of data\nand no annotated spelling correction datasets. To tackle these challenges a\nsmall-scale word-based transformer LM is trained to provide the SC model with\ncontextual understanding. Further, the probabilistic error rules are extracted\nfrom the corpus in an unsupervised way to model the tendency of error\nhappening(error model). Then the combination of LM and error model is used to\ndevelop the SC model through the well-known noisy channel framework. The\neffectiveness of this approach is demonstrated through experiments on the\nNepali language where there is access to just an unprocessed corpus of textual\ndata.", "published": "2024-04-28 05:29:35", "link": "http://arxiv.org/abs/2404.18072v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CRE-LLM: A Domain-Specific Chinese Relation Extraction Framework with\n  Fine-tuned Large Language Model", "abstract": "Domain-Specific Chinese Relation Extraction (DSCRE) aims to extract relations\nbetween entities from domain-specific Chinese text. Despite the rapid\ndevelopment of PLMs in recent years, especially LLMs, DSCRE still faces three\ncore challenges: complex network structure design, poor awareness, and high\nconsumption of fine-tuning. Given the impressive performance of large language\nmodels (LLMs) in natural language processing, we propose a new framework called\nCRE-LLM. This framework is based on fine-tuning open-source LLMs, such as\nLlama-2, ChatGLM2, and Baichuan2. CRE-LLM enhances the logic-awareness and\ngenerative capabilities of the model by constructing an appropriate prompt and\nutilizing open-source LLMs for instruction-supervised fine-tuning. And then it\ndirectly extracts the relations of the given entities in the input textual\ndata, which improving the CRE approach. To demonstrate the effectiveness of the\nproposed framework, we conducted extensive experiments on two domain-specific\nCRE datasets, FinRE and SanWen. The experimental results show that CRE-LLM is\nsignificantly superior and robust, achieving state-of-the-art (SOTA)\nperformance on the FinRE dataset. This paper introduces a novel approach to\ndomain-specific relation extraction (DSCRE) tasks that are semantically more\ncomplex by combining LLMs with triples. Our code is publicly available.", "published": "2024-04-28 06:27:15", "link": "http://arxiv.org/abs/2404.18085v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EkoHate: Abusive Language and Hate Speech Detection for Code-switched\n  Political Discussions on Nigerian Twitter", "abstract": "Nigerians have a notable online presence and actively discuss political and\ntopical matters. This was particularly evident throughout the 2023 general\nelection, where Twitter was used for campaigning, fact-checking and\nverification, and even positive and negative discourse. However, little or none\nhas been done in the detection of abusive language and hate speech in Nigeria.\nIn this paper, we curated code-switched Twitter data directed at three\nmusketeers of the governorship election on the most populous and economically\nvibrant state in Nigeria; Lagos state, with the view to detect offensive speech\nin political discussions. We developed EkoHate -- an abusive language and hate\nspeech dataset for political discussions between the three candidates and their\nfollowers using a binary (normal vs offensive) and fine-grained four-label\nannotation scheme. We analysed our dataset and provided an empirical evaluation\nof state-of-the-art methods across both supervised and cross-lingual transfer\nlearning settings. In the supervised setting, our evaluation results in both\nbinary and four-label annotation schemes show that we can achieve 95.1 and 70.3\nF1 points respectively. Furthermore, we show that our dataset adequately\ntransfers very well to three publicly available offensive datasets (OLID,\nHateUS2020, and FountaHate), generalizing to political discussions in other\nregions like the US.", "published": "2024-04-28 13:25:11", "link": "http://arxiv.org/abs/2404.18180v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparing LLM prompting with Cross-lingual transfer performance on\n  Indigenous and Low-resource Brazilian Languages", "abstract": "Large Language Models are transforming NLP for a variety of tasks. However,\nhow LLMs perform NLP tasks for low-resource languages (LRLs) is less explored.\nIn line with the goals of the AmericasNLP workshop, we focus on 12 LRLs from\nBrazil, 2 LRLs from Africa and 2 high-resource languages (HRLs) (e.g., English\nand Brazilian Portuguese). Our results indicate that the LLMs perform worse for\nthe part of speech (POS) labeling of LRLs in comparison to HRLs. We explain the\nreasons behind this failure and provide an error analysis through examples\nobserved in our data set.", "published": "2024-04-28 19:24:28", "link": "http://arxiv.org/abs/2404.18286v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer Learning and Transformer Architecture for Financial Sentiment\n  Analysis", "abstract": "Financial sentiment analysis allows financial institutions like Banks and\nInsurance Companies to better manage the credit scoring of their customers in a\nbetter way. Financial domain uses specialized mechanisms which makes sentiment\nanalysis difficult. In this paper, we propose a pre-trained language model\nwhich can help to solve this problem with fewer labelled data. We extend on the\nprinciples of Transfer learning and Transformation architecture principles and\nalso take into consideration recent outbreak of pandemics like COVID. We apply\nthe sentiment analysis to two different sets of data. We also take smaller\ntraining set and fine tune the same as part of the model.", "published": "2024-04-28 17:15:07", "link": "http://arxiv.org/abs/2405.01586v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Perplexity Predict Fine-Tuning Performance? An Investigation of\n  Tokenization Effects on Sequential Language Models for Nepali", "abstract": "Recent language models use subwording mechanisms to handle\nOut-of-Vocabulary(OOV) words seen during test time and, their generation\ncapacity is generally measured using perplexity, an intrinsic metric. It is\nknown that increasing the subword granularity results in a decrease of\nperplexity value. However, the study of how subwording affects the\nunderstanding capacity of language models has been very few and only limited to\na handful of languages. To reduce this gap we used 6 different tokenization\nschemes to pretrain relatively small language models in Nepali and used the\nrepresentations learned to finetune on several downstream tasks. Although\nbyte-level BPE algorithm has been used in recent models like GPT, RoBERTa we\nshow that on average they are sub-optimal in comparison to algorithms such as\nSentencePiece in finetuning performances for Nepali. Additionally, similar\nrecent studies have focused on the Bert-based language model. We, however,\npretrain and finetune sequential transformer-based language models.", "published": "2024-04-28 05:26:12", "link": "http://arxiv.org/abs/2404.18071v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Logic Agent: Enhancing Validity with Logic Rule Invocation", "abstract": "Chain-of-Thought (CoT) prompting has emerged as a pivotal technique for\naugmenting the inferential capabilities of language models during reasoning\ntasks. Despite its advancements, CoT often grapples with challenges in\nvalidating reasoning validity and ensuring informativeness. Addressing these\nlimitations, this paper introduces the Logic Agent (LA), an agent-based\nframework aimed at enhancing the validity of reasoning processes in Large\nLanguage Models (LLMs) through strategic logic rule invocation. Unlike\nconventional approaches, LA transforms LLMs into logic agents that dynamically\napply propositional logic rules, initiating the reasoning process by converting\nnatural language inputs into structured logic forms. The logic agent leverages\na comprehensive set of predefined functions to systematically navigate the\nreasoning process. This methodology not only promotes the structured and\ncoherent generation of reasoning constructs but also significantly improves\ntheir interpretability and logical coherence. Through extensive\nexperimentation, we demonstrate LA's capacity to scale effectively across\nvarious model sizes, markedly improving the precision of complex reasoning\nacross diverse tasks.", "published": "2024-04-28 10:02:28", "link": "http://arxiv.org/abs/2404.18130v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "L3Cube-MahaNews: News-based Short Text and Long Document Classification\n  Datasets in Marathi", "abstract": "The availability of text or topic classification datasets in the low-resource\nMarathi language is limited, typically consisting of fewer than 4 target\nlabels, with some achieving nearly perfect accuracy. In this work, we introduce\nL3Cube-MahaNews, a Marathi text classification corpus that focuses on News\nheadlines and articles. This corpus stands out as the largest supervised\nMarathi Corpus, containing over 1.05L records classified into a diverse range\nof 12 categories. To accommodate different document lengths, MahaNews comprises\nthree supervised datasets specifically designed for short text, long documents,\nand medium paragraphs. The consistent labeling across these datasets\nfacilitates document length-based analysis. We provide detailed data statistics\nand baseline results on these datasets using state-of-the-art pre-trained BERT\nmodels. We conduct a comparative analysis between monolingual and multilingual\nBERT models, including MahaBERT, IndicBERT, and MuRIL. The monolingual MahaBERT\nmodel outperforms all others on every dataset. These resources also serve as\nMarathi topic classification datasets or models and are publicly available at\nhttps://github.com/l3cube-pune/MarathiNLP .", "published": "2024-04-28 15:20:45", "link": "http://arxiv.org/abs/2404.18216v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TextGram: Towards a better domain-adaptive pretraining", "abstract": "For green AI, it is crucial to measure and reduce the carbon footprint\nemitted during the training of large language models. In NLP, performing\npre-training on Transformer models requires significant computational\nresources. This pre-training involves using a large amount of text data to gain\nprior knowledge for performing downstream tasks. Thus, it is important that we\nselect the correct data in the form of domain-specific data from this vast\ncorpus to achieve optimum results aligned with our domain-specific tasks. While\ntraining on large unsupervised data is expensive, it can be optimized by\nperforming a data selection step before pretraining. Selecting important data\nreduces the space overhead and the substantial amount of time required to\npre-train the model while maintaining constant accuracy. We investigate the\nexisting selection strategies and propose our own domain-adaptive data\nselection method - TextGram - that effectively selects essential data from\nlarge corpora. We compare and evaluate the results of finetuned models for text\nclassification task with and without data selection. We show that the proposed\nstrategy works better compared to other selection methods.", "published": "2024-04-28 15:44:57", "link": "http://arxiv.org/abs/2404.18228v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "From Persona to Personalization: A Survey on Role-Playing Language\n  Agents", "abstract": "Recent advancements in large language models (LLMs) have significantly\nboosted the rise of Role-Playing Language Agents (RPLAs), i.e., specialized AI\nsystems designed to simulate assigned personas. By harnessing multiple advanced\nabilities of LLMs, including in-context learning, instruction following, and\nsocial intelligence, RPLAs achieve a remarkable sense of human likeness and\nvivid role-playing performance. RPLAs can mimic a wide range of personas,\nranging from historical figures and fictional characters to real-life\nindividuals. Consequently, they have catalyzed numerous AI applications, such\nas emotional companions, interactive video games, personalized assistants and\ncopilots, and digital clones. In this paper, we conduct a comprehensive survey\nof this field, illustrating the evolution and recent progress in RPLAs\nintegrating with cutting-edge LLM technologies. We categorize personas into\nthree types: 1) Demographic Persona, which leverages statistical stereotypes;\n2) Character Persona, focused on well-established figures; and 3)\nIndividualized Persona, customized through ongoing user interactions for\npersonalized services. We begin by presenting a comprehensive overview of\ncurrent methodologies for RPLAs, followed by the details for each persona type,\ncovering corresponding data sourcing, agent construction, and evaluation.\nAfterward, we discuss the fundamental risks, existing limitations, and future\nprospects of RPLAs. Additionally, we provide a brief review of RPLAs in AI\napplications, which reflects practical user demands that shape and drive RPLA\nresearch. Through this work, we aim to establish a clear taxonomy of RPLA\nresearch and applications, and facilitate future research in this critical and\never-evolving field, and pave the way for a future where humans and RPLAs\ncoexist in harmony.", "published": "2024-04-28 15:56:41", "link": "http://arxiv.org/abs/2404.18231v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SOUL: Unlocking the Power of Second-Order Optimization for LLM\n  Unlearning", "abstract": "Large Language Models (LLMs) have highlighted the necessity of effective\nunlearning mechanisms to comply with data regulations and ethical AI practices.\nLLM unlearning aims at removing undesired data influences and associated model\ncapabilities without compromising utility beyond the scope of unlearning. While\ninterest in studying LLM unlearning is growing, the impact of the optimizer\nchoice for LLM unlearning remains unexplored. In this work, we shed light on\nthe significance of optimizer selection in LLM unlearning for the first time,\nestablishing a clear connection between second-order optimization and influence\nunlearning (a classical approach using influence functions to update the model\nfor data influence removal). This insight propels us to develop a second-order\noptimization-based LLM unlearning framework, termed Second-Order UnLearning\n(SOUL), which extends the static, one-shot model update using influence\nunlearning to a dynamic, iterative unlearning process. Our extensive\nexperiments show that SOUL consistently outperforms conventional first-order\nmethods across various unlearning tasks, models, and metrics, indicating that\nsecond-order optimization offers an effective and broadly applicable solution\nfor LLM unlearning. Codes are available at https://github.com/OPTML-Group/SOUL.", "published": "2024-04-28 16:31:32", "link": "http://arxiv.org/abs/2404.18239v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "PatentGPT: A Large Language Model for Intellectual Property", "abstract": "In recent years, large language models(LLMs) have attracted significant\nattention due to their exceptional performance across a multitude of natural\nlanguage process tasks, and have been widely applied in various fields.\nHowever, the application of large language models in the Intellectual Property\n(IP) domain is challenging due to the strong need for specialized knowledge,\nprivacy protection, processing of extremely long text in this field. In this\ntechnical report, we present for the first time a low-cost, standardized\nprocedure for training IP-oriented LLMs, meeting the unique requirements of the\nIP domain. Using this standard process, we have trained the PatentGPT series\nmodels based on open-source pretrained models. By evaluating them on the\nopen-source IP-oriented benchmark MOZIP, our domain-specific LLMs outperforms\nGPT-4, indicating the effectiveness of the proposed training procedure and the\nexpertise of the PatentGPT models in the IP domain. Remarkably, our model\nsurpassed GPT-4 on the 2019 China Patent Agent Qualification Examination,\nscoring 65 and matching human expert levels. Additionally, the PatentGPT model,\nwhich utilizes the SMoE architecture, achieves performance comparable to that\nof GPT-4 in the IP domain and demonstrates a better cost-performance ratio on\nlong-text tasks, potentially serving as an alternative to GPT-4 within the IP\ndomain.", "published": "2024-04-28 17:36:43", "link": "http://arxiv.org/abs/2404.18255v5", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Mapping 'when'-clauses in Latin American and Caribbean languages: an\n  experiment in subtoken-based typology", "abstract": "Languages can encode temporal subordination lexically, via subordinating\nconjunctions, and morphologically, by marking the relation on the predicate.\nSystematic cross-linguistic variation among the former can be studied using\nwell-established token-based typological approaches to token-aligned parallel\ncorpora. Variation among different morphological means is instead much harder\nto tackle and therefore more poorly understood, despite being predominant in\nseveral language groups. This paper explores variation in the expression of\ngeneric temporal subordination ('when'-clauses) among the languages of Latin\nAmerica and the Caribbean, where morphological marking is particularly common.\nIt presents probabilistic semantic maps computed on the basis of the languages\nof the region, thus avoiding bias towards the many world's languages that\nexclusively use lexified connectors, incorporating associations between\ncharacter $n$-grams and English $when$. The approach allows capturing\nmorphological clause-linkage devices in addition to lexified connectors, paving\nthe way for larger-scale, strategy-agnostic analyses of typological variation\nin temporal subordination.", "published": "2024-04-28 17:43:24", "link": "http://arxiv.org/abs/2404.18257v1", "categories": ["cs.CL", "cs.IR", "68T50, 68U15, 68T35, (Primary), 86A32, 15A03 (Secondary)", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Modeling Orthographic Variation Improves NLP Performance for Nigerian\n  Pidgin", "abstract": "Nigerian Pidgin is an English-derived contact language and is traditionally\nan oral language, spoken by approximately 100 million people. No orthographic\nstandard has yet been adopted, and thus the few available Pidgin datasets that\nexist are characterised by noise in the form of orthographic variations. This\ncontributes to under-performance of models in critical NLP tasks. The current\nwork is the first to describe various types of orthographic variations commonly\nfound in Nigerian Pidgin texts, and model this orthographic variation. The\nvariations identified in the dataset form the basis of a phonetic-theoretic\nframework for word editing, which is used to generate orthographic variations\nto augment training data. We test the effect of this data augmentation on two\ncritical NLP tasks: machine translation and sentiment analysis. The proposed\nvariation generation framework augments the training data with new orthographic\nvariants which are relevant for the test set but did not occur in the training\nset originally. Our results demonstrate the positive effect of augmenting the\ntraining data with a combination of real texts from other corpora as well as\nsynthesized orthographic variation, resulting in performance improvements of\n2.1 points in sentiment analysis and 1.4 BLEU points in translation to English.", "published": "2024-04-28 18:07:13", "link": "http://arxiv.org/abs/2404.18264v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Tuning Large Language Models for Graph\n  Representation Learning", "abstract": "Text-rich graphs, which exhibit rich textual information on nodes and edges,\nare prevalent across a wide range of real-world business applications. Large\nLanguage Models (LLMs) have demonstrated remarkable abilities in understanding\ntext, which also introduced the potential for more expressive modeling in\ntext-rich graphs. Despite these capabilities, efficiently applying LLMs to\nrepresentation learning on graphs presents significant challenges. Recently,\nparameter-efficient fine-tuning methods for LLMs have enabled efficient new\ntask generalization with minimal time and memory consumption. Inspired by this,\nwe introduce Graph-aware Parameter-Efficient Fine-Tuning - GPEFT, a novel\napproach for efficient graph representation learning with LLMs on text-rich\ngraphs. Specifically, we utilize a graph neural network (GNN) to encode\nstructural information from neighboring nodes into a graph prompt. This prompt\nis then inserted at the beginning of the text sequence. To improve the quality\nof graph prompts, we pre-trained the GNN to assist the frozen LLM in predicting\nthe next token in the node text. Compared with existing joint GNN and LMs, our\nmethod directly generate the node embeddings from large language models with an\naffordable fine-tuning cost. We validate our approach through comprehensive\nexperiments conducted on 8 different text-rich graphs, observing an average\nimprovement of 2% in hit@1 and Mean Reciprocal Rank (MRR) in link prediction\nevaluations. Our results demonstrate the efficacy and efficiency of our model,\nshowing that it can be smoothly integrated with various large language models,\nincluding OPT, LLaMA and Falcon.", "published": "2024-04-28 18:36:59", "link": "http://arxiv.org/abs/2404.18271v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bias Neutralization Framework: Measuring Fairness in Large Language\n  Models with Bias Intelligence Quotient (BiQ)", "abstract": "The burgeoning influence of Large Language Models (LLMs) in shaping public\ndiscourse and decision-making underscores the imperative to address inherent\nbiases within these AI systems. In the wake of AI's expansive integration\nacross sectors, addressing racial bias in LLMs has never been more critical.\nThis paper introduces a novel framework called Comprehensive Bias\nNeutralization Framework (CBNF) which embodies an innovative approach to\nquantifying and mitigating biases within LLMs. Our framework combines the Large\nLanguage Model Bias Index (LLMBI) [Oketunji, A., Anas, M., Saina, D., (2023)]\nand Bias removaL with No Demographics (BLIND) [Orgad, H., Belinkov, Y. (2023)]\nmethodologies to create a new metric called Bias Intelligence Quotient\n(BiQ)which detects, measures, and mitigates racial bias in LLMs without\nreliance on demographic annotations.\n  By introducing a new metric called BiQ that enhances LLMBI with additional\nfairness metrics, CBNF offers a multi-dimensional metric for bias assessment,\nunderscoring the necessity of a nuanced approach to fairness in AI [Mehrabi et\nal., 2021]. This paper presents a detailed analysis of Latimer AI (a language\nmodel incrementally trained on black history and culture) in comparison to\nChatGPT 3.5, illustrating Latimer AI's efficacy in detecting racial, cultural,\nand gender biases through targeted training and refined bias mitigation\nstrategies [Latimer & Bender, 2023].", "published": "2024-04-28 18:47:14", "link": "http://arxiv.org/abs/2404.18276v1", "categories": ["cs.CL", "cs.AI", "D.1; I.2"], "primary_category": "cs.CL"}
{"title": "Utilizing Large Language Models for Information Extraction from Real\n  Estate Transactions", "abstract": "Real estate sales contracts contain crucial information for property\ntransactions, but manual data extraction can be time-consuming and error-prone.\nThis paper explores the application of large language models, specifically\ntransformer-based architectures, for automated information extraction from real\nestate contracts. We discuss challenges, techniques, and future directions in\nleveraging these models to improve efficiency and accuracy in real estate\ncontract analysis. We generated synthetic contracts using the real-world\ntransaction dataset, thereby fine-tuning the large-language model and achieving\nsignificant metrics improvements and qualitative improvements in information\nretrieval and reasoning tasks.", "published": "2024-04-28 01:38:38", "link": "http://arxiv.org/abs/2404.18043v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "USAT: A Universal Speaker-Adaptive Text-to-Speech Approach", "abstract": "Conventional text-to-speech (TTS) research has predominantly focused on\nenhancing the quality of synthesized speech for speakers in the training\ndataset. The challenge of synthesizing lifelike speech for unseen,\nout-of-dataset speakers, especially those with limited reference data, remains\na significant and unresolved problem. While zero-shot or few-shot\nspeaker-adaptive TTS approaches have been explored, they have many limitations.\nZero-shot approaches tend to suffer from insufficient generalization\nperformance to reproduce the voice of speakers with heavy accents. While\nfew-shot methods can reproduce highly varying accents, they bring a significant\nstorage burden and the risk of overfitting and catastrophic forgetting. In\naddition, prior approaches only provide either zero-shot or few-shot\nadaptation, constraining their utility across varied real-world scenarios with\ndifferent demands. Besides, most current evaluations of speaker-adaptive TTS\nare conducted only on datasets of native speakers, inadvertently neglecting a\nvast portion of non-native speakers with diverse accents. Our proposed\nframework unifies both zero-shot and few-shot speaker adaptation strategies,\nwhich we term as \"instant\" and \"fine-grained\" adaptations based on their\nmerits. To alleviate the insufficient generalization performance observed in\nzero-shot speaker adaptation, we designed two innovative discriminators and\nintroduced a memory mechanism for the speech decoder. To prevent catastrophic\nforgetting and reduce storage implications for few-shot speaker adaptation, we\ndesigned two adapters and a unique adaptation procedure.", "published": "2024-04-28 06:50:55", "link": "http://arxiv.org/abs/2404.18094v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Explaining vague language", "abstract": "Why is language vague? Vagueness may be explained and rationalized if it can\nbe shown that vague language is more useful to speaker and hearer than precise\nlanguage. In a well-known paper, Lipman proposes a game-theoretic account of\nvagueness in terms of mixed strategy that leads to a puzzle: vagueness cannot\nbe strictly better than precision at equilibrium. More recently, \\'Egr\\'e,\nSpector, Mortier and Verheyen have put forward a Bayesian account of vagueness\nestablishing that using vague words can be strictly more informative than using\nprecise words. This paper proposes to compare both results and to explain why\nthey are not in contradiction. Lipman's definition of vagueness relies\nexclusively on a property of signaling strategies, without making any\nassumptions about the lexicon, whereas \\'Egr\\'e et al.'s involves a layer of\nsemantic content. We argue that the semantic account of vagueness is needed,\nand more adequate and explanatory of vagueness.", "published": "2024-04-28 12:11:34", "link": "http://arxiv.org/abs/2404.18154v1", "categories": ["cs.CL", "cs.GT", "cs.IT", "math.IT", "91A86", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Ranked List Truncation for Large Language Model-based Re-Ranking", "abstract": "We study ranked list truncation (RLT) from a novel \"retrieve-then-re-rank\"\nperspective, where we optimize re-ranking by truncating the retrieved list\n(i.e., trim re-ranking candidates). RLT is crucial for re-ranking as it can\nimprove re-ranking efficiency by sending variable-length candidate lists to a\nre-ranker on a per-query basis. It also has the potential to improve re-ranking\neffectiveness. Despite its importance, there is limited research into applying\nRLT methods to this new perspective. To address this research gap, we reproduce\nexisting RLT methods in the context of re-ranking, especially newly emerged\nlarge language model (LLM)-based re-ranking. In particular, we examine to what\nextent established findings on RLT for retrieval are generalizable to the\n\"retrieve-then-re-rank\" setup from three perspectives: (i) assessing RLT\nmethods in the context of LLM-based re-ranking with lexical first-stage\nretrieval, (ii) investigating the impact of different types of first-stage\nretrievers on RLT methods, and (iii) investigating the impact of different\ntypes of re-rankers on RLT methods. We perform experiments on the TREC 2019 and\n2020 deep learning tracks, investigating 8 RLT methods for pipelines involving\n3 retrievers and 2 re-rankers. We reach new insights into RLT methods in the\ncontext of re-ranking.", "published": "2024-04-28 13:39:33", "link": "http://arxiv.org/abs/2404.18185v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "H.3.3"], "primary_category": "cs.IR"}
{"title": "Towards Incremental Learning in Large Language Models: A Critical Review", "abstract": "Incremental learning is the ability of systems to acquire knowledge over\ntime, enabling their adaptation and generalization to novel tasks. It is a\ncritical ability for intelligent, real-world systems, especially when data\nchanges frequently or is limited. This review provides a comprehensive analysis\nof incremental learning in Large Language Models. It synthesizes the\nstate-of-the-art incremental learning paradigms, including continual learning,\nmeta-learning, parameter-efficient learning, and mixture-of-experts learning.\nWe demonstrate their utility for incremental learning by describing specific\nachievements from these related topics and their critical factors. An important\nfinding is that many of these approaches do not update the core model, and none\nof them update incrementally in real-time. The paper highlights current\nproblems and challenges for future research in the field. By consolidating the\nlatest relevant research developments, this review offers a comprehensive\nunderstanding of incremental learning and its implications for designing and\ndeveloping LLM-based learning systems.", "published": "2024-04-28 20:44:53", "link": "http://arxiv.org/abs/2404.18311v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Learnable Linguistic Watermarks for Tracing Model Extraction Attacks on\n  Large Language Models", "abstract": "In the rapidly evolving domain of artificial intelligence, safeguarding the\nintellectual property of Large Language Models (LLMs) is increasingly crucial.\nCurrent watermarking techniques against model extraction attacks, which rely on\nsignal insertion in model logits or post-processing of generated text, remain\nlargely heuristic. We propose a novel method for embedding learnable linguistic\nwatermarks in LLMs, aimed at tracing and preventing model extraction attacks.\nOur approach subtly modifies the LLM's output distribution by introducing\ncontrolled noise into token frequency distributions, embedding an statistically\nidentifiable controllable watermark.We leverage statistical hypothesis testing\nand information theory, particularly focusing on Kullback-Leibler Divergence,\nto differentiate between original and modified distributions effectively. Our\nwatermarking method strikes a delicate well balance between robustness and\noutput quality, maintaining low false positive/negative rates and preserving\nthe LLM's original performance.", "published": "2024-04-28 14:45:53", "link": "http://arxiv.org/abs/2405.01509v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Lightweight Conceptual Dictionary Learning for Text Classification Using\n  Information Compression", "abstract": "We propose a novel, lightweight supervised dictionary learning framework for\ntext classification based on data compression and representation. This\ntwo-phase algorithm initially employs the Lempel-Ziv-Welch (LZW) algorithm to\nconstruct a dictionary from text datasets, focusing on the conceptual\nsignificance of dictionary elements. Subsequently, dictionaries are refined\nconsidering label data, optimizing dictionary atoms to enhance discriminative\npower based on mutual information and class distribution. This process\ngenerates discriminative numerical representations, facilitating the training\nof simple classifiers such as SVMs and neural networks. We evaluate our\nalgorithm's information-theoretic performance using information bottleneck\nprinciples and introduce the information plane area rank (IPAR) as a novel\nmetric to quantify the information-theoretic performance. Tested on six\nbenchmark text datasets, our algorithm competes closely with top models,\nespecially in limited-vocabulary contexts, using significantly fewer\nparameters. \\review{Our algorithm closely matches top-performing models,\ndeviating by only ~2\\% on limited-vocabulary datasets, using just 10\\% of their\nparameters. However, it falls short on diverse-vocabulary datasets, likely due\nto the LZW algorithm's constraints with low-repetition data. This contrast\nhighlights its efficiency and limitations across different dataset types.", "published": "2024-04-28 10:11:52", "link": "http://arxiv.org/abs/2405.01584v1", "categories": ["cs.CL", "cs.LG", "eess.SP"], "primary_category": "cs.CL"}
{"title": "Tabular Embedding Model (TEM): Finetuning Embedding Models For Tabular\n  RAG Applications", "abstract": "In recent times Large Language Models have exhibited tremendous capabilities,\nespecially in the areas of mathematics, code generation and general-purpose\nreasoning. However for specialized domains especially in applications that\nrequire parsing and analyzing large chunks of numeric or tabular data even\nstate-of-the-art (SOTA) models struggle. In this paper, we introduce a new\napproach to solving domain-specific tabular data analysis tasks by presenting a\nunique RAG workflow that mitigates the scalability issues of existing tabular\nLLM solutions. Specifically, we present Tabular Embedding Model (TEM), a novel\napproach to fine-tune embedding models for tabular Retrieval-Augmentation\nGeneration (RAG) applications. Embedding models form a crucial component in the\nRAG workflow and even current SOTA embedding models struggle as they are\npredominantly trained on textual datasets and thus underperform in scenarios\ninvolving complex tabular data. The evaluation results showcase that our\napproach not only outperforms current SOTA embedding models in this domain but\nalso does so with a notably smaller and more efficient model structure.", "published": "2024-04-28 14:58:55", "link": "http://arxiv.org/abs/2405.01585v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Improve Academic Query Resolution through BERT-based Question Extraction\n  from Images", "abstract": "Providing fast and accurate resolution to the student's query is an essential\nsolution provided by Edtech organizations. This is generally provided with a\nchat-bot like interface to enable students to ask their doubts easily. One\npreferred format for student queries is images, as it allows students to\ncapture and post questions without typing complex equations and information.\nHowever, this format also presents difficulties, as images may contain multiple\nquestions or textual noise that lowers the accuracy of existing single-query\nanswering solutions. In this paper, we propose a method for extracting\nquestions from text or images using a BERT-based deep learning model and\ncompare it to the other rule-based and layout-based methods. Our method aims to\nimprove the accuracy and efficiency of student query resolution in Edtech\norganizations.", "published": "2024-04-28 19:11:08", "link": "http://arxiv.org/abs/2405.01587v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CLARINET: Augmenting Language Models to Ask Clarification Questions for\n  Retrieval", "abstract": "Users often make ambiguous requests that require clarification. We study the\nproblem of asking clarification questions in an information retrieval setting,\nwhere systems often face ambiguous search queries and it is challenging to turn\nthe uncertainty in the retrieval model into a natural language question. We\npresent CLARINET, a system that asks informative clarification questions by\nchoosing questions whose answers would maximize certainty in the correct\ncandidate. Our approach works by augmenting a large language model (LLM) to\ncondition on a retrieval distribution, finetuning end-to-end to generate the\nquestion that would have maximized the rank of the true candidate at each turn.\nWhen evaluated on a real-world retrieval dataset of users searching for books,\nour system outperforms traditional heuristics such as information gain on\nretrieval success by 17% and vanilla-prompted LLMs by 39% relative.", "published": "2024-04-28 18:21:31", "link": "http://arxiv.org/abs/2405.15784v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Leveraging Prompts in LLMs to Overcome Imbalances in Complex Educational\n  Text Data", "abstract": "In this paper, we explore the potential of Large Language Models (LLMs) with\nassertions to mitigate imbalances in educational datasets. Traditional models\noften fall short in such contexts, particularly due to the complexity and\nnuanced nature of the data. This issue is especially prominent in the education\nsector, where cognitive engagement levels among students show significant\nvariation in their open responses. To test our hypothesis, we utilized an\nexisting technology for assertion-based prompt engineering through an\n'Iterative - ICL PE Design Process' comparing traditional Machine Learning (ML)\nmodels against LLMs augmented with assertions (N=135). Further, we conduct a\nsensitivity analysis on a subset (n=27), examining the variance in model\nperformance concerning classification metrics and cognitive engagement levels\nin each iteration. Our findings reveal that LLMs with assertions significantly\noutperform traditional ML models, particularly in cognitive engagement levels\nwith minority representation, registering up to a 32% increase in F1-score.\nAdditionally, our sensitivity study indicates that incorporating targeted\nassertions into the LLM tested on the subset enhances its performance by\n11.94%. This improvement primarily addresses errors stemming from the model's\nlimitations in understanding context and resolving lexical ambiguities in\nstudent responses.", "published": "2024-04-28 00:24:08", "link": "http://arxiv.org/abs/2407.01551v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Exploring the Robustness of In-Context Learning with Noisy Labels", "abstract": "Recently, the mysterious In-Context Learning (ICL) ability exhibited by\nTransformer architectures, especially in large language models (LLMs), has\nsparked significant research interest. However, the resilience of Transformers'\nin-context learning capabilities in the presence of noisy samples, prevalent in\nboth training corpora and prompt demonstrations, remains underexplored. In this\npaper, inspired by prior research that studies ICL ability using simple\nfunction classes, we take a closer look at this problem by investigating the\nrobustness of Transformers against noisy labels. Specifically, we first conduct\na thorough evaluation and analysis of the robustness of Transformers against\nnoisy labels during in-context learning and show that they exhibit notable\nresilience against diverse types of noise in demonstration labels. Furthermore,\nwe delve deeper into this problem by exploring whether introducing noise into\nthe training set, akin to a form of data augmentation, enhances such robustness\nduring inference, and find that such noise can indeed improve the robustness of\nICL. Overall, our fruitful analysis and findings provide a comprehensive\nunderstanding of the resilience of Transformer models against label noises\nduring ICL and provide valuable insights into the research on Transformers in\nnatural language processing. Our code is available at\nhttps://github.com/InezYu0928/in-context-learning.", "published": "2024-04-28 14:05:23", "link": "http://arxiv.org/abs/2404.18191v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG", "math.OC"], "primary_category": "cs.CL"}
{"title": "LEGENT: Open Platform for Embodied Agents", "abstract": "Despite advancements in Large Language Models (LLMs) and Large Multimodal\nModels (LMMs), their integration into language-grounded, human-like embodied\nagents remains incomplete, hindering complex real-life task performance in\nphysical environments. Existing integrations often feature limited open\nsourcing, challenging collective progress in this field. We introduce LEGENT,\nan open, scalable platform for developing embodied agents using LLMs and LMMs.\nLEGENT offers a dual approach: a rich, interactive 3D environment with\ncommunicable and actionable agents, paired with a user-friendly interface, and\na sophisticated data generation pipeline utilizing advanced algorithms to\nexploit supervision from simulated worlds at scale. In our experiments, an\nembryonic vision-language-action model trained on LEGENT-generated data\nsurpasses GPT-4V in embodied tasks, showcasing promising generalization\ncapabilities.", "published": "2024-04-28 16:50:12", "link": "http://arxiv.org/abs/2404.18243v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "ComposerX: Multi-Agent Symbolic Music Composition with LLMs", "abstract": "Music composition represents the creative side of humanity, and itself is a\ncomplex task that requires abilities to understand and generate information\nwith long dependency and harmony constraints. While demonstrating impressive\ncapabilities in STEM subjects, current LLMs easily fail in this task,\ngenerating ill-written music even when equipped with modern techniques like\nIn-Context-Learning and Chain-of-Thoughts. To further explore and enhance LLMs'\npotential in music composition by leveraging their reasoning ability and the\nlarge knowledge base in music history and theory, we propose ComposerX, an\nagent-based symbolic music generation framework. We find that applying a\nmulti-agent approach significantly improves the music composition quality of\nGPT-4. The results demonstrate that ComposerX is capable of producing coherent\npolyphonic music compositions with captivating melodies, while adhering to user\ninstructions.", "published": "2024-04-28 06:17:42", "link": "http://arxiv.org/abs/2404.18081v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
