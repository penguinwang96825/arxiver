{"title": "Discourse Structures Guided Fine-grained Propaganda Identification", "abstract": "Propaganda is a form of deceptive narratives that instigate or mislead the\npublic, usually with a political purpose. In this paper, we aim to identify\npropaganda in political news at two fine-grained levels: sentence-level and\ntoken-level. We observe that propaganda content is more likely to be embedded\nin sentences that attribute causality or assert contrast to nearby sentences,\nas well as seen in opinionated evaluation, speculation and discussions of\nfuture expectation. Hence, we propose to incorporate both local and global\ndiscourse structures for propaganda discovery and construct two teacher models\nfor identifying PDTB-style discourse relations between nearby sentences and\ncommon discourse roles of sentences in a news article respectively. We further\ndevise two methods to incorporate the two types of discourse structures for\npropaganda identification by either using teacher predicted probabilities as\nadditional features or soliciting guidance in a knowledge distillation\nframework. Experiments on the benchmark dataset demonstrate that leveraging\nguidance from discourse structures can significantly improve both precision and\nrecall of propaganda content identification.", "published": "2023-10-28 00:18:19", "link": "http://arxiv.org/abs/2310.18544v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Conspiracy Theories News based on Event Relation Graph", "abstract": "Conspiracy theories, as a type of misinformation, are narratives that\nexplains an event or situation in an irrational or malicious manner. While most\nprevious work examined conspiracy theory in social media short texts, limited\nattention was put on such misinformation in long news documents. In this paper,\nwe aim to identify whether a news article contains conspiracy theories. We\nobserve that a conspiracy story can be made up by mixing uncorrelated events\ntogether, or by presenting an unusual distribution of relations between events.\nAchieving a contextualized understanding of events in a story is essential for\ndetecting conspiracy theories. Thus, we propose to incorporate an event\nrelation graph for each article, in which events are nodes, and four common\ntypes of event relations, coreference, temporal, causal, and subevent\nrelations, are considered as edges. Then, we integrate the event relation graph\ninto conspiracy theory identification in two ways: an event-aware language\nmodel is developed to augment the basic language model with the knowledge of\nevents and event relations via soft labels; further, a heterogeneous graph\nattention network is designed to derive a graph embedding based on hard labels.\nExperiments on a large benchmark dataset show that our approach based on event\nrelation graph improves both precision and recall of conspiracy theory\nidentification, and generalizes well for new unseen media sources.", "published": "2023-10-28 00:27:21", "link": "http://arxiv.org/abs/2310.18545v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Accelerating LLaMA Inference by Enabling Intermediate Layer Decoding via\n  Instruction Tuning with LITE", "abstract": "Large Language Models (LLMs) have achieved remarkable performance across a\nwide variety of natural language tasks; however, their large size makes their\ninference slow and computationally expensive. Focusing on this problem, we\npropose to instruction tune LLMs with additional explicit losses from the\nintermediate layers (LITE) and show that it enables these layers to acquire\n'good' generation ability without affecting the generation ability of the final\nlayer. We perform 'dynamic confidence-based early exiting' at token level from\nthe intermediate layers which improves the efficiency of text generation\nwithout compromising the quality of the generation. We conduct comprehensive\nexperiments by instruction tuning LLaMA-2 models on the Alpaca dataset and\nholistically evaluate on four different human-instruction test sets. We show\nthat dynamic early exiting achieves consistent and considerable inference\ncomputation cost improvements (37.86% for 7B and 46.35% for 13B model) while\nmaintaining the generation quality of the responses. We further conduct a\nthorough analysis of the results over several important aspects, such as\ncomparing the semantic similarity of the outputs and dissecting the efficiency\nimprovements by comparing the number of tokens generated in the output. In\nsummary, our work contributes to improving the efficiency of LLM inference\nwhile maintaining the generation quality, a crucial step en route to enabling\ntheir widespread adoption.", "published": "2023-10-28 04:07:58", "link": "http://arxiv.org/abs/2310.18581v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Anaphor Assisted Document-Level Relation Extraction", "abstract": "Document-level relation extraction (DocRE) involves identifying relations\nbetween entities distributed in multiple sentences within a document. Existing\nmethods focus on building a heterogeneous document graph to model the internal\nstructure of an entity and the external interaction between entities. However,\nthere are two drawbacks in existing methods. On one hand, anaphor plays an\nimportant role in reasoning to identify relations between entities but is\nignored by these methods. On the other hand, these methods achieve\ncross-sentence entity interactions implicitly by utilizing a document or\nsentences as intermediate nodes. Such an approach has difficulties in learning\nfine-grained interactions between entities across different sentences,\nresulting in sub-optimal performance. To address these issues, we propose an\nAnaphor-Assisted (AA) framework for DocRE tasks. Experimental results on the\nwidely-used datasets demonstrate that our model achieves a new state-of-the-art\nperformance.", "published": "2023-10-28 06:11:18", "link": "http://arxiv.org/abs/2310.18604v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ASTormer: An AST Structure-aware Transformer Decoder for Text-to-SQL", "abstract": "Text-to-SQL aims to generate an executable SQL program given the user\nutterance and the corresponding database schema. To ensure the well-formedness\nof output SQLs, one prominent approach adopts a grammar-based recurrent decoder\nto produce the equivalent SQL abstract syntax tree (AST). However, previous\nmethods mainly utilize an RNN-series decoder, which 1) is time-consuming and\ninefficient and 2) introduces very few structure priors. In this work, we\npropose an AST structure-aware Transformer decoder (ASTormer) to replace\ntraditional RNN cells. The structural knowledge, such as node types and\npositions in the tree, is seamlessly incorporated into the decoder via both\nabsolute and relative position embeddings. Besides, the proposed framework is\ncompatible with different traversing orders even considering adaptive node\nselection. Extensive experiments on five text-to-SQL benchmarks demonstrate the\neffectiveness and efficiency of our structured decoder compared to competitive\nbaselines.", "published": "2023-10-28 10:21:40", "link": "http://arxiv.org/abs/2310.18662v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When Reviewers Lock Horn: Finding Disagreement in Scientific Peer\n  Reviews", "abstract": "To this date, the efficacy of the scientific publishing enterprise\nfundamentally rests on the strength of the peer review process. The journal\neditor or the conference chair primarily relies on the expert reviewers'\nassessment, identify points of agreement and disagreement and try to reach a\nconsensus to make a fair and informed decision on whether to accept or reject a\npaper. However, with the escalating number of submissions requiring review,\nespecially in top-tier Artificial Intelligence (AI) conferences, the\neditor/chair, among many other works, invests a significant, sometimes\nstressful effort to mitigate reviewer disagreements. Here in this work, we\nintroduce a novel task of automatically identifying contradictions among\nreviewers on a given article. To this end, we introduce ContraSciView, a\ncomprehensive review-pair contradiction dataset on around 8.5k papers (with\naround 28k review pairs containing nearly 50k review pair comments) from the\nopen review-based ICLR and NeurIPS conferences. We further propose a baseline\nmodel that detects contradictory statements from the review pairs. To the best\nof our knowledge, we make the first attempt to identify disagreements among\npeer reviewers automatically. We make our dataset and code public for further\ninvestigations.", "published": "2023-10-28 11:57:51", "link": "http://arxiv.org/abs/2310.18685v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Crossing the Aisle: Unveiling Partisan and Counter-Partisan Events in\n  News Reporting", "abstract": "News media is expected to uphold unbiased reporting. Yet they may still\naffect public opinion by selectively including or omitting events that support\nor contradict their ideological positions. Prior work in NLP has only studied\nmedia bias via linguistic style and word usage. In this paper, we study to\nwhich degree media balances news reporting and affects consumers through event\ninclusion or omission. We first introduce the task of detecting both partisan\nand counter-partisan events: events that support or oppose the author's\npolitical ideology. To conduct our study, we annotate a high-quality dataset,\nPAC, containing 8,511 (counter-)partisan event annotations in 304 news articles\nfrom ideologically diverse media outlets. We benchmark PAC to highlight the\nchallenges of this task. Our findings highlight both the ways in which the news\nsubtly shapes opinion and the need for large language models that better\nunderstand events within a broader context. Our dataset can be found at\nhttps://github.com/launchnlp/Partisan-Event-Dataset.", "published": "2023-10-28 17:50:13", "link": "http://arxiv.org/abs/2310.18768v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ProMap: Effective Bilingual Lexicon Induction via Language Model\n  Prompting", "abstract": "Bilingual Lexicon Induction (BLI), where words are translated between two\nlanguages, is an important NLP task. While noticeable progress on BLI in rich\nresource languages using static word embeddings has been achieved. The word\ntranslation performance can be further improved by incorporating information\nfrom contextualized word embeddings. In this paper, we introduce ProMap, a\nnovel approach for BLI that leverages the power of prompting pretrained\nmultilingual and multidialectal language models to address these challenges. To\novercome the employment of subword tokens in these models, ProMap relies on an\neffective padded prompting of language models with a seed dictionary that\nachieves good performance when used independently. We also demonstrate the\neffectiveness of ProMap in re-ranking results from other BLI methods such as\nwith aligned static word embeddings. When evaluated on both rich-resource and\nlow-resource languages, ProMap consistently achieves state-of-the-art results.\nFurthermore, ProMap enables strong performance in few-shot scenarios (even with\nless than 10 training examples), making it a valuable tool for low-resource\nlanguage translation. Overall, we believe our method offers both exciting and\npromising direction for BLI in general and low-resource languages in\nparticular. ProMap code and data are available at\n\\url{https://github.com/4mekki4/promap}.", "published": "2023-10-28 18:33:24", "link": "http://arxiv.org/abs/2310.18778v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are NLP Models Good at Tracing Thoughts: An Overview of Narrative\n  Understanding", "abstract": "Narrative understanding involves capturing the author's cognitive processes,\nproviding insights into their knowledge, intentions, beliefs, and desires.\nAlthough large language models (LLMs) excel in generating grammatically\ncoherent text, their ability to comprehend the author's thoughts remains\nuncertain. This limitation hinders the practical applications of narrative\nunderstanding. In this paper, we conduct a comprehensive survey of narrative\nunderstanding tasks, thoroughly examining their key features, definitions,\ntaxonomy, associated datasets, training objectives, evaluation metrics, and\nlimitations. Furthermore, we explore the potential of expanding the\ncapabilities of modularized LLMs to address novel narrative understanding\ntasks. By framing narrative understanding as the retrieval of the author's\nimaginative cues that outline the narrative structure, our study introduces a\nfresh perspective on enhancing narrative comprehension.", "published": "2023-10-28 18:47:57", "link": "http://arxiv.org/abs/2310.18783v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Translating away Translationese without Parallel Data", "abstract": "Translated texts exhibit systematic linguistic differences compared to\noriginal texts in the same language, and these differences are referred to as\ntranslationese. Translationese has effects on various cross-lingual natural\nlanguage processing tasks, potentially leading to biased results. In this\npaper, we explore a novel approach to reduce translationese in translated\ntexts: translation-based style transfer. As there are no parallel\nhuman-translated and original data in the same language, we use a\nself-supervised approach that can learn from comparable (rather than parallel)\nmono-lingual original and translated data. However, even this self-supervised\napproach requires some parallel data for validation. We show how we can\neliminate the need for parallel validation data by combining the\nself-supervised loss with an unsupervised loss. This unsupervised loss\nleverages the original language model loss over the style-transferred output\nand a semantic similarity loss between the input and style-transferred output.\nWe evaluate our approach in terms of original vs. translationese binary\nclassification in addition to measuring content preservation and target-style\nfluency. The results show that our approach is able to reduce translationese\nclassifier accuracy to a level of a random classifier after style transfer\nwhile adequately preserving the content and fluency in the target original\nstyle.", "published": "2023-10-28 22:11:25", "link": "http://arxiv.org/abs/2310.18830v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs-Healthcare : Current Applications and Challenges of Large Language\n  Models in various Medical Specialties", "abstract": "We aim to present a comprehensive overview of the latest advancements in\nutilizing Large Language Models (LLMs) within the healthcare sector,\nemphasizing their transformative impact across various medical domains. LLMs\nhave become pivotal in supporting healthcare, including physicians, healthcare\nproviders, and patients. Our review provides insight into the applications of\nLarge Language Models (LLMs) in healthcare, specifically focusing on diagnostic\nand treatment-related functionalities. We shed light on how LLMs are applied in\ncancer care, dermatology, dental care, neurodegenerative disorders, and mental\nhealth, highlighting their innovative contributions to medical diagnostics and\npatient care. Throughout our analysis, we explore the challenges and\nopportunities associated with integrating LLMs in healthcare, recognizing their\npotential across various medical specialties despite existing limitations.\nAdditionally, we offer an overview of handling diverse data types within the\nmedical field.", "published": "2023-10-28 01:01:30", "link": "http://arxiv.org/abs/2311.12882v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MILDSum: A Novel Benchmark Dataset for Multilingual Summarization of\n  Indian Legal Case Judgments", "abstract": "Automatic summarization of legal case judgments is a practically important\nproblem that has attracted substantial research efforts in many countries. In\nthe context of the Indian judiciary, there is an additional complexity --\nIndian legal case judgments are mostly written in complex English, but a\nsignificant portion of India's population lacks command of the English\nlanguage. Hence, it is crucial to summarize the legal documents in Indian\nlanguages to ensure equitable access to justice. While prior research primarily\nfocuses on summarizing legal case judgments in their source languages, this\nstudy presents a pioneering effort toward cross-lingual summarization of\nEnglish legal documents into Hindi, the most frequently spoken Indian language.\nWe construct the first high-quality legal corpus comprising of 3,122 case\njudgments from prominent Indian courts in English, along with their summaries\nin both English and Hindi, drafted by legal practitioners. We benchmark the\nperformance of several diverse summarization approaches on our corpus and\ndemonstrate the need for further research in cross-lingual summarization in the\nlegal domain.", "published": "2023-10-28 05:51:57", "link": "http://arxiv.org/abs/2310.18600v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive\n  Learning for Code Generation", "abstract": "With the rise of powerful closed-sourced LLMs (ChatGPT, GPT-4), there are\nincreasing interests in distilling the capabilies of close-sourced LLMs to\nsmaller open-sourced LLMs. Previous distillation methods usually prompt ChatGPT\nto generate a set of instructions and answers, for the student model to learn.\nHowever, such standard distillation approach neglects the merits and conditions\nof the student model. Inspired by modern teaching principles, we design a\npersonalised distillation process, in which the student attempts to solve a\ntask first, then the teacher provides an adaptive refinement for the student to\nimprove. Instead of feeding the student with teacher's prior, personalised\ndistillation enables personalised learning for the student model, as it only\nlearns on examples it makes mistakes upon and learns to improve its own\nsolution. On code generation, personalised distillation consistently\noutperforms standard distillation with only one third of the data. With only\n2.5-3K personalised examples that incur a data-collection cost of 4-6$, we\nboost CodeGen-mono-16B by 7% to achieve 36.4% pass@1 and StarCoder by 12.2% to\nachieve 45.8% pass@1 on HumanEval.", "published": "2023-10-28 07:54:39", "link": "http://arxiv.org/abs/2310.18628v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to\n  Determinacy", "abstract": "Recent advances in large language models (LLMs) have revolutionized the\nlandscape of reasoning tasks. To enhance the capabilities of LLMs to emulate\nhuman reasoning, prior studies have focused on modeling reasoning steps using\nvarious thought structures like chains, trees, or graphs. However, LLM-based\nreasoning still encounters the following challenges: (1) Limited adaptability\nof preset structures to diverse tasks; (2) Insufficient precision in exploiting\nknown conditions to derive new ones; and (3) Inadequate consideration of\nhistorical reasoning experiences for subsequent reasoning steps. To this end,\nwe propose DetermLR, a novel perspective that rethinks the reasoning process as\nan evolution from indeterminacy to determinacy. First, we categorize known\nconditions into two types: determinate and indeterminate premises This provides\nan oveall direction for the reasoning process and guides LLMs in converting\nindeterminate data into progressively determinate insights. Subsequently, we\nleverage quantitative measurements to prioritize more relevant premises to\nexplore new insights. Furthermore, we automate the storage and extraction of\navailable premises and reasoning paths with reasoning memory, preserving\nhistorical reasoning details for subsequent reasoning steps. Comprehensive\nexperimental results demonstrate that DetermLR surpasses all baselines on\nvarious logical reasoning benchmarks: LogiQA, ProofWriter, FOLIO, PrOntoQA, and\nLogicalDeduction. Compared to previous multi-step reasoning methods, DetermLR\nachieves higher accuracy with fewer reasoning steps, highlighting its superior\nefficiency and effectiveness in solving logical reasoning tasks.", "published": "2023-10-28 10:05:51", "link": "http://arxiv.org/abs/2310.18659v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "TLM: Token-Level Masking for Transformers", "abstract": "Structured dropout approaches, such as attention dropout and DropHead, have\nbeen investigated to regularize the multi-head attention mechanism in\nTransformers. In this paper, we propose a new regularization scheme based on\ntoken-level rather than structure-level to reduce overfitting. Specifically, we\ndevise a novel Token-Level Masking (TLM) training strategy for Transformers to\nregularize the connections of self-attention, which consists of two masking\ntechniques that are effective and easy to implement. The underlying idea is to\nmanipulate the connections between tokens in the multi-head attention via\nmasking, where the networks are forced to exploit partial neighbors'\ninformation to produce a meaningful representation. The generality and\neffectiveness of TLM are thoroughly evaluated via extensive experiments on 4\ndiversified NLP tasks across 18 datasets, including natural language\nunderstanding benchmark GLUE, ChineseGLUE, Chinese Grammatical Error\nCorrection, and data-to-text generation. The results indicate that TLM can\nconsistently outperform attention dropout and DropHead, e.g., it increases by\n0.5 points relative to DropHead with BERT-large on GLUE. Moreover, TLM can\nestablish a new record on the data-to-text benchmark Rotowire (18.93 BLEU). Our\ncode will be publicly available at https://github.com/Young1993/tlm.", "published": "2023-10-28 15:42:47", "link": "http://arxiv.org/abs/2310.18738v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sequence-Level Certainty Reduces Hallucination In Knowledge-Grounded\n  Dialogue Generation", "abstract": "In this work, we propose sequence-level certainty as a common theme over\nhallucination in Knowledge Grounded Dialogue Generation (KGDG). We explore the\ncorrelation between the level of hallucination in model responses and two types\nof sequence-level certainty: probabilistic certainty and semantic certainty.\nEmpirical results reveal that higher levels of both types of certainty in model\nresponses are correlated with lower levels of hallucination. We further propose\nCertainty-based Response Ranking (CRR), a decoding-time hallucination\nmitigation method that samples several response candidates, ranks them based on\nsequence-level certainty, and outputs the response with the highest certainty\nlevel. Aligning with our definitions of sequence-level certainty, we design 2\ntypes of CRR approaches: Probabilistic CRR (P-CRR) and Semantic CRR (S-CRR).\nP-CRR ranks individually sampled model responses using the arithmetic mean\nlog-probability of the entire sequence. S-CRR approaches certainty estimation\nfrom meaning-space, and ranks model response candidates based on their semantic\ncertainty level as measured by an entailment-based Agreement Score (AS).\nThrough extensive experiments across 3 KGDG datasets, 3 decoding methods, and 4\nKGDG models, we validate the effectiveness of CRR for reducing hallucination in\nKGDG task.", "published": "2023-10-28 19:42:28", "link": "http://arxiv.org/abs/2310.18794v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "All Things Considered: Detecting Partisan Events from News Media with\n  Cross-Article Comparison", "abstract": "Public opinion is shaped by the information news media provide, and that\ninformation in turn may be shaped by the ideological preferences of media\noutlets. But while much attention has been devoted to media bias via overt\nideological language or topic selection, a more unobtrusive way in which the\nmedia shape opinion is via the strategic inclusion or omission of partisan\nevents that may support one side or the other. We develop a latent\nvariable-based framework to predict the ideology of news articles by comparing\nmultiple articles on the same story and identifying partisan events whose\ninclusion or omission reveals ideology. Our experiments first validate the\nexistence of partisan event selection, and then show that article alignment and\ncross-document comparison detect partisan events and article ideology better\nthan competitive baselines. Our results reveal the high-level form of media\nbias, which is present even among mainstream media with strong norms of\nobjectivity and nonpartisanship. Our codebase and dataset are available at\nhttps://github.com/launchnlp/ATC.", "published": "2023-10-28 21:53:23", "link": "http://arxiv.org/abs/2310.18827v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dense Retrieval as Indirect Supervision for Large-space Decision Making", "abstract": "Many discriminative natural language understanding (NLU) tasks have large\nlabel spaces. Learning such a process of large-space decision making is\nparticularly challenging due to the lack of training instances per label and\nthe difficulty of selection among many fine-grained labels. Inspired by dense\nretrieval methods for passage finding in open-domain QA, we propose a\nreformulation of large-space discriminative NLU tasks as a learning-to-retrieve\ntask, leading to a novel solution named Dense Decision Retrieval (DDR ).\nInstead of predicting fine-grained decisions as logits, DDR adopts a\ndual-encoder architecture that learns to predict by retrieving from a decision\nthesaurus. This approach not only leverages rich indirect supervision signals\nfrom easy-to-consume learning resources for dense retrieval, it also leads to\nenhanced prediction generalizability with a semantically meaningful\nrepresentation of the large decision space. When evaluated on tasks with\ndecision spaces ranging from hundreds to hundred-thousand scales, DDR\noutperforms strong baselines greatly by 27.54% in P@1 on two extreme\nmulti-label classification tasks, 1.17% in F1 score ultra-fine entity typing,\nand 1.26% in accuracy on three few-shot intent classification tasks on average.\nCode and resources are available at https://github.com/luka-group/DDR", "published": "2023-10-28 07:00:28", "link": "http://arxiv.org/abs/2310.18619v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Setting the Trap: Capturing and Defeating Backdoors in Pretrained\n  Language Models through Honeypots", "abstract": "In the field of natural language processing, the prevalent approach involves\nfine-tuning pretrained language models (PLMs) using local samples. Recent\nresearch has exposed the susceptibility of PLMs to backdoor attacks, wherein\nthe adversaries can embed malicious prediction behaviors by manipulating a few\ntraining samples. In this study, our objective is to develop a\nbackdoor-resistant tuning procedure that yields a backdoor-free model, no\nmatter whether the fine-tuning dataset contains poisoned samples. To this end,\nwe propose and integrate a honeypot module into the original PLM, specifically\ndesigned to absorb backdoor information exclusively. Our design is motivated by\nthe observation that lower-layer representations in PLMs carry sufficient\nbackdoor features while carrying minimal information about the original tasks.\nConsequently, we can impose penalties on the information acquired by the\nhoneypot module to inhibit backdoor creation during the fine-tuning process of\nthe stem network. Comprehensive experiments conducted on benchmark datasets\nsubstantiate the effectiveness and robustness of our defensive strategy.\nNotably, these results indicate a substantial reduction in the attack success\nrate ranging from 10\\% to 40\\% when compared to prior state-of-the-art methods.", "published": "2023-10-28 08:21:16", "link": "http://arxiv.org/abs/2310.18633v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "EHRXQA: A Multi-Modal Question Answering Dataset for Electronic Health\n  Records with Chest X-ray Images", "abstract": "Electronic Health Records (EHRs), which contain patients' medical histories\nin various multi-modal formats, often overlook the potential for joint\nreasoning across imaging and table modalities underexplored in current EHR\nQuestion Answering (QA) systems. In this paper, we introduce EHRXQA, a novel\nmulti-modal question answering dataset combining structured EHRs and chest\nX-ray images. To develop our dataset, we first construct two uni-modal\nresources: 1) The MIMIC-CXR-VQA dataset, our newly created medical visual\nquestion answering (VQA) benchmark, specifically designed to augment the\nimaging modality in EHR QA, and 2) EHRSQL (MIMIC-IV), a refashioned version of\na previously established table-based EHR QA dataset. By integrating these two\nuni-modal resources, we successfully construct a multi-modal EHR QA dataset\nthat necessitates both uni-modal and cross-modal reasoning. To address the\nunique challenges of multi-modal questions within EHRs, we propose a\nNeuralSQL-based strategy equipped with an external VQA API. This pioneering\nendeavor enhances engagement with multi-modal EHR sources and we believe that\nour dataset can catalyze advances in real-world medical scenarios such as\nclinical decision-making and research. EHRXQA is available at\nhttps://github.com/baeseongsu/ehrxqa.", "published": "2023-10-28 09:42:04", "link": "http://arxiv.org/abs/2310.18652v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "N-Critics: Self-Refinement of Large Language Models with Ensemble of\n  Critics", "abstract": "We propose a self-correction mechanism for Large Language Models (LLMs) to\nmitigate issues such as toxicity and fact hallucination. This method involves\nrefining model outputs through an ensemble of critics and the model's own\nfeedback. Drawing inspiration from human behavior, we explore whether LLMs can\nemulate the self-correction process observed in humans who often engage in\nself-reflection and seek input from others to refine their understanding of\ncomplex topics. Our approach is model-agnostic and can be applied across\nvarious domains to enhance trustworthiness by addressing fairness, bias, and\nrobustness concerns. We consistently observe performance improvements in LLMs\nfor reducing toxicity and correcting factual errors.", "published": "2023-10-28 11:22:22", "link": "http://arxiv.org/abs/2310.18679v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Probing LLMs for Joint Encoding of Linguistic Categories", "abstract": "Large Language Models (LLMs) exhibit impressive performance on a range of NLP\ntasks, due to the general-purpose linguistic knowledge acquired during\npretraining. Existing model interpretability research (Tenney et al., 2019)\nsuggests that a linguistic hierarchy emerges in the LLM layers, with lower\nlayers better suited to solving syntactic tasks and higher layers employed for\nsemantic processing. Yet, little is known about how encodings of different\nlinguistic phenomena interact within the models and to what extent processing\nof linguistically-related categories relies on the same, shared model\nrepresentations. In this paper, we propose a framework for testing the joint\nencoding of linguistic categories in LLMs. Focusing on syntax, we find evidence\nof joint encoding both at the same (related part-of-speech (POS) classes) and\ndifferent (POS classes and related syntactic dependency relations) levels of\nlinguistic hierarchy. Our cross-lingual experiments show that the same patterns\nhold across languages in multilingual LLMs.", "published": "2023-10-28 12:46:40", "link": "http://arxiv.org/abs/2310.18696v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using Large Language Models to Support Thematic Analysis in Empirical\n  Legal Studies", "abstract": "Thematic analysis and other variants of inductive coding are widely used\nqualitative analytic methods within empirical legal studies (ELS). We propose a\nnovel framework facilitating effective collaboration of a legal expert with a\nlarge language model (LLM) for generating initial codes (phase 2 of thematic\nanalysis), searching for themes (phase 3), and classifying the data in terms of\nthe themes (to kick-start phase 4). We employed the framework for an analysis\nof a dataset (n=785) of facts descriptions from criminal court opinions\nregarding thefts. The goal of the analysis was to discover classes of typical\nthefts. Our results show that the LLM, namely OpenAI's GPT-4, generated\nreasonable initial codes, and it was capable of improving the quality of the\ncodes based on expert feedback. They also suggest that the model performed well\nin zero-shot classification of facts descriptions in terms of the themes.\nFinally, the themes autonomously discovered by the LLM appear to map fairly\nwell to the themes arrived at by legal experts. These findings can be leveraged\nby legal researchers to guide their decisions in integrating LLMs into their\nthematic analyses, as well as other inductive coding projects.", "published": "2023-10-28 15:20:44", "link": "http://arxiv.org/abs/2310.18729v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Open Visual Knowledge Extraction via Relation-Oriented Multimodality\n  Model Prompting", "abstract": "Images contain rich relational knowledge that can help machines understand\nthe world. Existing methods on visual knowledge extraction often rely on the\npre-defined format (e.g., sub-verb-obj tuples) or vocabulary (e.g., relation\ntypes), restricting the expressiveness of the extracted knowledge. In this\nwork, we take a first exploration to a new paradigm of open visual knowledge\nextraction. To achieve this, we present OpenVik which consists of an open\nrelational region detector to detect regions potentially containing relational\nknowledge and a visual knowledge generator that generates format-free knowledge\nby prompting the large multimodality model with the detected region of\ninterest. We also explore two data enhancement techniques for diversifying the\ngenerated format-free visual knowledge. Extensive knowledge quality evaluations\nhighlight the correctness and uniqueness of the extracted open visual knowledge\nby OpenVik. Moreover, integrating our extracted knowledge across various visual\nreasoning applications shows consistent improvements, indicating the real-world\napplicability of OpenVik.", "published": "2023-10-28 20:09:29", "link": "http://arxiv.org/abs/2310.18804v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Emotion-Oriented Behavior Model Using Deep Learning", "abstract": "Emotions, as a fundamental ingredient of any social interaction, lead to\nbehaviors that represent the effectiveness of the interaction through facial\nexpressions and gestures in humans. Hence an agent must possess the social and\ncognitive abilities to understand human social parameters and behave\naccordingly. However, no such emotion-oriented behavior model is presented yet\nin the existing research. The emotion prediction may generate appropriate\nagents' behaviors for effective interaction using conversation modality.\nConsidering the importance of emotions, and behaviors, for an agent's social\ninteraction, an Emotion-based Behavior model is presented in this paper for\nSocio-cognitive artificial agents. The proposed model is implemented using\ntweets data trained on multiple models like Long Short-Term Memory (LSTM),\nConvolution Neural Network (CNN) and Bidirectional Encoder Representations from\nTransformers (BERT) for emotion prediction with an average accuracy of 92%, and\n55% respectively. Further, using emotion predictions from CNN-LSTM, the\nbehavior module responds using facial expressions and gestures using Behavioral\nMarkup Language (BML). The accuracy of emotion-based behavior predictions is\nstatistically validated using the 2-tailed Pearson correlation on the data\ncollected from human users through questionnaires. Analysis shows that all\nemotion-based behaviors accurately depict human-like gestures and facial\nexpressions based on the significant correlation at the 0.01 and 0.05 levels.\nThis study is a steppingstone to a multi-faceted artificial agent interaction\nbased on emotion-oriented behaviors. Cognition has significance regarding\nsocial interaction among humans.", "published": "2023-10-28 17:27:59", "link": "http://arxiv.org/abs/2311.14674v1", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Audio-Visual Instance Segmentation", "abstract": "In this paper, we propose a new multi-modal task, termed audio-visual\ninstance segmentation (AVIS), which aims to simultaneously identify, segment\nand track individual sounding object instances in audible videos. To facilitate\nthis research, we introduce a high-quality benchmark named AVISeg, containing\nover 90K instance masks from 26 semantic categories in 926 long videos.\nAdditionally, we propose a strong baseline model for this task. Our model first\nlocalizes sound source within each frame, and condenses object-specific\ncontexts into concise tokens. Then it builds long-range audio-visual\ndependencies between these tokens using window-based attention, and tracks\nsounding objects among the entire video sequences. Extensive experiments reveal\nthat our method performs best on AVISeg, surpassing the existing methods from\nrelated tasks. We further conduct the evaluation on several multi-modal large\nmodels. Unfortunately, they exhibits subpar performance on instance-level sound\nsource localization and temporal perception. We expect that AVIS will inspire\nthe community towards a more comprehensive multi-modal understanding. Dataset\nand code is available at https://github.com/ruohaoguo/avis.", "published": "2023-10-28 13:37:52", "link": "http://arxiv.org/abs/2310.18709v4", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
