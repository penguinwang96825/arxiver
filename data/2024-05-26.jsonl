{"title": "DarijaBanking: A New Resource for Overcoming Language Barriers in\n  Banking Intent Detection for Moroccan Arabic Speakers", "abstract": "Navigating the complexities of language diversity is a central challenge in\ndeveloping robust natural language processing systems, especially in\nspecialized domains like banking. The Moroccan Dialect (Darija) serves as the\ncommon language that blends cultural complexities, historical impacts, and\nregional differences. The complexities of Darija present a special set of\nchallenges for language models, as it differs from Modern Standard Arabic with\nstrong influence from French, Spanish, and Tamazight, it requires a specific\napproach for effective communication. To tackle these challenges, this paper\nintroduces \\textbf{DarijaBanking}, a novel Darija dataset aimed at enhancing\nintent classification in the banking domain, addressing the critical need for\nautomatic banking systems (e.g., chatbots) that communicate in the native\nlanguage of Moroccan clients. DarijaBanking comprises over 1,800 parallel\nhigh-quality queries in Darija, Modern Standard Arabic (MSA), English, and\nFrench, organized into 24 intent classes. We experimented with various intent\nclassification methods, including full fine-tuning of monolingual and\nmultilingual models, zero-shot learning, retrieval-based approaches, and Large\nLanguage Model prompting. One of the main contributions of this work is\nBERTouch, our BERT-based language model for intent classification in Darija.\nBERTouch achieved F1-scores of 0.98 for Darija and 0.96 for MSA on\nDarijaBanking, outperforming the state-of-the-art alternatives including GPT-4\nshowcasing its effectiveness in the targeted application.", "published": "2024-05-26 08:33:28", "link": "http://arxiv.org/abs/2405.16482v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tool Learning in the Wild: Empowering Language Models as Automatic Tool\n  Agents", "abstract": "Augmenting large language models (LLMs) with external tools has emerged as a\npromising approach to extend their utility, enabling them to solve practical\ntasks. Previous methods manually parse tool documentation and create in-context\ndemonstrations, transforming tools into structured formats for LLMs to use in\ntheir step-by-step reasoning. However, this manual process requires domain\nexpertise and struggles to scale to large toolsets. Additionally, these methods\nrely heavily on ad-hoc inference techniques or special tokens to integrate\nfree-form LLM generation with tool-calling actions, limiting the LLM's\nflexibility in handling diverse tool specifications and integrating multiple\ntools.\n  In this work, we propose AutoTools, a framework that enables LLMs to automate\nthe tool-use workflow. Specifically, the LLM automatically transforms tool\ndocumentation into callable functions, verifying syntax and runtime\ncorrectness. Then, the LLM integrates these functions into executable programs\nto solve practical tasks, flexibly grounding tool-use actions into its\nreasoning processes. Extensive experiments on existing and newly collected,\nmore challenging benchmarks illustrate the superiority of our framework.\nInspired by these promising results, we further investigate how to improve the\nexpertise of LLMs, especially open-source LLMs with fewer parameters, within\nAutoTools. Thus, we propose the AutoTools-learning approach, training the LLMs\nwith three learning tasks on 34k instances of high-quality synthetic data,\nincluding documentation understanding, relevance learning, and function\nprogramming. Fine-grained results validate the effectiveness of our overall\ntraining approach and each individual task. Our methods are an important step\ntowards the use of LLMs for solving real-world tasks with external tools.", "published": "2024-05-26 11:40:58", "link": "http://arxiv.org/abs/2405.16533v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Preliminary Empirical Study on Prompt-based Unsupervised Keyphrase\n  Extraction", "abstract": "Pre-trained large language models can perform natural language processing\ndownstream tasks by conditioning on human-designed prompts. However, a\nprompt-based approach often requires \"prompt engineering\" to design different\nprompts, primarily hand-crafted through laborious trial and error, requiring\nhuman intervention and expertise. It is a challenging problem when constructing\na prompt-based keyphrase extraction method. Therefore, we investigate and study\nthe effectiveness of different prompts on the keyphrase extraction task to\nverify the impact of the cherry-picked prompts on the performance of extracting\nkeyphrases. Extensive experimental results on six benchmark keyphrase\nextraction datasets and different pre-trained large language models demonstrate\nthat (1) designing complex prompts may not necessarily be more effective than\ndesigning simple prompts; (2) individual keyword changes in the designed\nprompts can affect the overall performance; (3) designing complex prompts\nachieve better performance than designing simple prompts when facing long\ndocuments.", "published": "2024-05-26 13:37:57", "link": "http://arxiv.org/abs/2405.16571v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatically Generating Numerous Context-Driven SFT Data for LLMs\n  across Diverse Granularity", "abstract": "Constructing high-quality query-response pairs from custom corpus is crucial\nfor supervised fine-tuning (SFT) large language models (LLMs) in many\napplications, like creating domain-specific AI assistants or roleplaying\nagents. However, sourcing this data through human annotation is costly, and\nexisting automated methods often fail to capture the diverse range of\ncontextual granularity and tend to produce homogeneous data. To tackle these\nissues, we introduce a novel method named AugCon, capable of automatically\ngenerating context-driven SFT data across multiple levels of granularity with\nhigh diversity, quality and fidelity. AugCon begins by generating queries using\nthe Context-Split-Tree (CST), an innovative approach for recursively deriving\nqueries and splitting context to cover full granularity. Then, we train a\nscorer through contrastive learning to collaborate with CST to rank and refine\nqueries. Finally, a synergistic integration of self-alignment and\nself-improving is introduced to obtain high-fidelity responses.\n  Extensive experiments are conducted incorporating both human and automatic\nevaluations, encompassing a test scenario and four widely-used benchmarks in\nEnglish and Chinese. The results highlight the significant advantages of AugCon\nin producing high diversity, quality, and fidelity SFT data against several\nstate-of-the-art methods. All of our code, dataset, and fine-tuned model will\nbe available at: https://github.com/quanshr/AugCon.", "published": "2024-05-26 14:14:18", "link": "http://arxiv.org/abs/2405.16579v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MentalManip: A Dataset For Fine-grained Analysis of Mental Manipulation\n  in Conversations", "abstract": "Mental manipulation, a significant form of abuse in interpersonal\nconversations, presents a challenge to identify due to its context-dependent\nand often subtle nature. The detection of manipulative language is essential\nfor protecting potential victims, yet the field of Natural Language Processing\n(NLP) currently faces a scarcity of resources and research on this topic. Our\nstudy addresses this gap by introducing a new dataset, named ${\\rm M{\\small\nental}M{\\small anip}}$, which consists of $4,000$ annotated movie dialogues.\nThis dataset enables a comprehensive analysis of mental manipulation,\npinpointing both the techniques utilized for manipulation and the\nvulnerabilities targeted in victims. Our research further explores the\neffectiveness of leading-edge models in recognizing manipulative dialogue and\nits components through a series of experiments with various configurations. The\nresults demonstrate that these models inadequately identify and categorize\nmanipulative content. Attempts to improve their performance by fine-tuning with\nexisting datasets on mental health and toxicity have not overcome these\nlimitations. We anticipate that ${\\rm M{\\small ental}M{\\small anip}}$ will\nstimulate further research, leading to progress in both understanding and\nmitigating the impact of mental manipulation in conversations.", "published": "2024-05-26 14:27:48", "link": "http://arxiv.org/abs/2405.16584v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compressing Lengthy Context With UltraGist", "abstract": "Compressing lengthy context is a critical but technically challenging\nproblem. In this paper, we propose a new method called UltraGist, which is\ndistinguished for its high-quality compression of lengthy context due to the\ninnovative design of the compression and learning algorithm. UltraGist brings\nforth the following important benefits. Firstly, it notably contributes to the\nflexibility of compression, as it can be effectively learned to support a broad\nrange of context lengths and compression ratios. Secondly, it helps to produce\nfine-grained compression for the lengthy context, where each small segment of\nthe context is progressively processed on top of a tailored cross-attention\nmechanism. Thirdly, it makes the training process sample-efficient and thus\nmaximizes the use of training data. Finally, it facilitates the efficient\nrunning of compression for dynamic context, as the compression result can be\nprogressively generated and hence incrementally updated. UltraGist is evaluated\non a wide variety of tasks associated with lengthy context, such as document QA\nand summarization, few-shot learning, multi-session conversation, et al. Whilst\nthe existing methods fail to handle these challenging scenarios, our approach\nis able to preserve a near-lossless compression performance throughout all the\nevaluations. Our data, model, and code have been released at\n\\url{https://github.com/namespace-Pt/UltraGist}.", "published": "2024-05-26 17:23:56", "link": "http://arxiv.org/abs/2405.16635v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Triple Preference Optimization: Achieving Better Alignment using a\n  Single Step Optimization", "abstract": "Reinforcement Learning with Human Feedback (RLHF) enhances the alignment of\nLarge Language Models (LLMs). However, its limitations have led to the\ndevelopment of Direct Preference Optimization (DPO), an RL-free approach\ndesigned to overcome these shortcomings. While studies have shown that DPO\nimproves instruction-following capabilities, it negatively impacts the\nreasoning ability of LLMs. Additionally, DPO is highly sensitive to judgment\nnoise in preference datasets and the size of the training set. Although several\nmodifications to DPO have been proposed, they still fail to fully resolve these\nissues. To address these limitations, we propose Triple Preference Optimization\n(TPO), a new preference learning method designed to enhance both reasoning and\ninstruction-following abilities through one-step optimization. We compare TPO\nagainst DPO and its recent variants using state-of-the-art training setups,\nincluding both base and instruction-tuned models such as Mistral and Llama 3.\nOur evaluation covers a comprehensive range of chat-based and reasoning\nbenchmarks. The results demonstrate that TPO achieves significant improvements\nover existing methods without substantially increasing response length across\ndifferent dataset sizes. Specifically, TPO outperforms DPO and SimPO by up to\n7.0% and 7.3% points on Arena-Hard, 12.2% and 13.3% points on MixEval-Hard,\n10.4% and 10.1% points on MMLU-Pro, and 19.0% and 19.2% points on GSM8K,\nrespectively. Furthermore, TPO achieves these improvements while requiring less\ndata than DPO.", "published": "2024-05-26 20:18:11", "link": "http://arxiv.org/abs/2405.16681v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Accurate and Nuanced Open-QA Evaluation Through Textual Entailment", "abstract": "Open-domain question answering (Open-QA) is a common task for evaluating\nlarge language models (LLMs). However, current Open-QA evaluations are\ncriticized for the ambiguity in questions and the lack of semantic\nunderstanding in evaluators. Complex evaluators, powered by foundation models\nor LLMs and pertaining to semantic equivalence, still deviate from human\njudgments by a large margin. We propose to study the entailment relations of\nanswers to identify more informative and more general system answers, offering\na much closer evaluation to human judgment on both NaturalQuestions and\nTriviaQA while being learning-free. The entailment-based evaluation we propose\nallows the assignment of bonus or partial marks by quantifying the inference\ngap between answers, enabling a nuanced ranking of answer correctness that has\nhigher AUC than current methods.", "published": "2024-05-26 21:33:27", "link": "http://arxiv.org/abs/2405.16702v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Scale Knowledge Washing", "abstract": "Large language models show impressive abilities in memorizing world\nknowledge, which leads to concerns regarding memorization of private\ninformation, toxic or sensitive knowledge, and copyrighted content. We\nintroduce the problem of Large Scale Knowledge Washing, focusing on unlearning\nan extensive amount of factual knowledge. Previous unlearning methods usually\ndefine the reverse loss and update the model via backpropagation, which may\naffect the model's fluency and reasoning ability or even destroy the model due\nto extensive training with the reverse loss. Existing works introduce\nadditional data from downstream tasks to prevent the model from losing\ncapabilities, which requires downstream task awareness. Controlling the\ntradeoff of unlearning and maintaining existing capabilities is also\nchallenging. To this end, we propose LAW (Large Scale Washing) to update the\nMLP layers in decoder-only large language models to perform knowledge washing,\nas inspired by model editing methods and based on the hypothesis that knowledge\nand reasoning are disentanglable. We derive a new objective with the knowledge\nto be unlearned to update the weights of certain MLP layers. Experimental\nresults demonstrate the effectiveness of LAW in forgetting target knowledge\nwhile maintaining reasoning ability. The code will be open-sourced at\nhttps://github.com/wangyu-ustc/LargeScaleWashing.", "published": "2024-05-26 23:29:49", "link": "http://arxiv.org/abs/2405.16720v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Reference Preference Optimization for Large Language Models", "abstract": "How can Large Language Models (LLMs) be aligned with human intentions and\nvalues? A typical solution is to gather human preference on model outputs and\nfinetune the LLMs accordingly while ensuring that updates do not deviate too\nfar from a reference model. Recent approaches, such as direct preference\noptimization (DPO), have eliminated the need for unstable and sluggish\nreinforcement learning optimization by introducing close-formed supervised\nlosses. However, a significant limitation of the current approach is its design\nfor a single reference model only, neglecting to leverage the collective power\nof numerous pretrained LLMs. To overcome this limitation, we introduce a novel\nclosed-form formulation for direct preference optimization using multiple\nreference models. The resulting algorithm, Multi-Reference Preference\nOptimization (MRPO), leverages broader prior knowledge from diverse reference\nmodels, substantially enhancing preference learning capabilities compared to\nthe single-reference DPO. Our experiments demonstrate that LLMs finetuned with\nMRPO generalize better in various preference data, regardless of data scarcity\nor abundance. Furthermore, MRPO effectively finetunes LLMs to exhibit superior\nperformance in several downstream natural language processing tasks such as\nGSM8K and TruthfulQA.", "published": "2024-05-26 00:29:04", "link": "http://arxiv.org/abs/2405.16388v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Assessing Empathy in Large Language Models with Real-World\n  Physician-Patient Interactions", "abstract": "The integration of Large Language Models (LLMs) into the healthcare domain\nhas the potential to significantly enhance patient care and support through the\ndevelopment of empathetic, patient-facing chatbots. This study investigates an\nintriguing question Can ChatGPT respond with a greater degree of empathy than\nthose typically offered by physicians? To answer this question, we collect a\nde-identified dataset of patient messages and physician responses from Mayo\nClinic and generate alternative replies using ChatGPT. Our analyses incorporate\nnovel empathy ranking evaluation (EMRank) involving both automated metrics and\nhuman assessments to gauge the empathy level of responses. Our findings\nindicate that LLM-powered chatbots have the potential to surpass human\nphysicians in delivering empathetic communication, suggesting a promising\navenue for enhancing patient care and reducing professional burnout. The study\nnot only highlights the importance of empathy in patient interactions but also\nproposes a set of effective automatic empathy ranking metrics, paving the way\nfor the broader adoption of LLMs in healthcare.", "published": "2024-05-26 01:58:57", "link": "http://arxiv.org/abs/2405.16402v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge", "abstract": "Knowledge Graph Embedding (KGE) techniques are crucial in learning compact\nrepresentations of entities and relations within a knowledge graph,\nfacilitating efficient reasoning and knowledge discovery. While existing\nmethods typically focus either on training KGE models solely based on graph\nstructure or fine-tuning pre-trained language models with classification data\nin KG, KG-FIT leverages LLM-guided refinement to construct a semantically\ncoherent hierarchical structure of entity clusters. By incorporating this\nhierarchical knowledge along with textual information during the fine-tuning\nprocess, KG-FIT effectively captures both global semantics from the LLM and\nlocal semantics from the KG. Extensive experiments on the benchmark datasets\nFB15K-237, YAGO3-10, and PrimeKG demonstrate the superiority of KG-FIT over\nstate-of-the-art pre-trained language model-based methods, achieving\nimprovements of 14.4%, 13.5%, and 11.9% in the Hits@10 metric for the link\nprediction task, respectively. Furthermore, KG-FIT yields substantial\nperformance gains of 12.6%, 6.7%, and 17.7% compared to the structure-based\nbase models upon which it is built. These results highlight the effectiveness\nof KG-FIT in incorporating open-world knowledge from LLMs to significantly\nenhance the expressiveness and informativeness of KG embeddings.", "published": "2024-05-26 03:04:26", "link": "http://arxiv.org/abs/2405.16412v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "M-RAG: Reinforcing Large Language Model Performance through\n  Retrieval-Augmented Generation with Multiple Partitions", "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nretrieving relevant memories from an external database. However, existing RAG\nmethods typically organize all memories in a whole database, potentially\nlimiting focus on crucial memories and introducing noise. In this paper, we\nintroduce a multiple partition paradigm for RAG (called M-RAG), where each\ndatabase partition serves as a basic unit for RAG execution. Based on this\nparadigm, we propose a novel framework that leverages LLMs with Multi-Agent\nReinforcement Learning to optimize different language generation tasks\nexplicitly. Through comprehensive experiments conducted on seven datasets,\nspanning three language generation tasks and involving three distinct language\nmodel architectures, we confirm that M-RAG consistently outperforms various\nbaseline methods, achieving improvements of 11%, 8%, and 12% for text\nsummarization, machine translation, and dialogue generation, respectively.", "published": "2024-05-26 04:03:13", "link": "http://arxiv.org/abs/2405.16420v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Development of an open education resources (OER) system: a comparative\n  analysis and implementation approach", "abstract": "Several institutions are collaborating on the development of a new web-based\nOpen Education Resources (OER) system designed exclusively for non-commercial\neducational purposes. This initiative is underpinned by meticulous research\naimed at constructing an OER system that optimizes user experiences across\ndiverse user profiles. A significant emphasis is placed on utilizing\nopen-source tools, frameworks, and technologies. The project includes a\ncomparative analysis of the top five open-source Learning Management Systems\n(LMS), providing critical insights to inform the development process. The\nprimary objective is to create a web-based system that facilitates the sharing\nof educational resources for non-commercial users, leveraging information and\ncommunication technologies. The project is structured around two key teams: a\nresearch team and a development team. This comprehensive approach is intended\nto establish a robust, user-centric OER system, informed by insights from\nexisting platforms and the latest advancements in open education resource\ndevelopment.", "published": "2024-05-26 05:58:45", "link": "http://arxiv.org/abs/2405.16442v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "LoQT: Low-Rank Adapters for Quantized Pretraining", "abstract": "Despite advances using low-rank adapters and quantization, pretraining of\nlarge models on consumer hardware has not been possible without model sharding,\noffloading during training, or per-layer gradient updates. To address these\nlimitations, we propose Low-Rank Adapters for Quantized Training (LoQT), a\nmethod for efficiently training quantized models. LoQT uses gradient-based\ntensor factorization to initialize low-rank trainable weight matrices that are\nperiodically merged into quantized full-rank weight matrices. Our approach is\nsuitable for both pretraining and fine-tuning models. We demonstrate this for\nlanguage modeling and downstream task adaptation, finding that LoQT enables\nefficient training of models up to 7B parameters on a 24GB GPU. We also\ndemonstrate the feasibility of training a 13B model using per-layer gradient\nupdates on the same hardware.", "published": "2024-05-26 11:29:57", "link": "http://arxiv.org/abs/2405.16528v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Cocktail: A Comprehensive Information Retrieval Benchmark with\n  LLM-Generated Documents Integration", "abstract": "The proliferation of Large Language Models (LLMs) has led to an influx of\nAI-generated content (AIGC) on the internet, transforming the corpus of\nInformation Retrieval (IR) systems from solely human-written to a coexistence\nwith LLM-generated content. The impact of this surge in AIGC on IR systems\nremains an open question, with the primary challenge being the lack of a\ndedicated benchmark for researchers. In this paper, we introduce Cocktail, a\ncomprehensive benchmark tailored for evaluating IR models in this mixed-sourced\ndata landscape of the LLM era. Cocktail consists of 16 diverse datasets with\nmixed human-written and LLM-generated corpora across various text retrieval\ntasks and domains. Additionally, to avoid the potential bias from previously\nincluded dataset information in LLMs, we also introduce an up-to-date dataset,\nnamed NQ-UTD, with queries derived from recent events. Through conducting over\n1,000 experiments to assess state-of-the-art retrieval models against the\nbenchmarked datasets in Cocktail, we uncover a clear trade-off between ranking\nperformance and source bias in neural retrieval models, highlighting the\nnecessity for a balanced approach in designing future IR systems. We hope\nCocktail can serve as a foundational resource for IR research in the LLM era,\nwith all data and code publicly available at\n\\url{https://github.com/KID-22/Cocktail}.", "published": "2024-05-26 12:30:20", "link": "http://arxiv.org/abs/2405.16546v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "SED: Self-Evaluation Decoding Enhances Large Language Models for Better\n  Generation", "abstract": "Existing Large Language Models (LLMs) generate text through unidirectional\nautoregressive decoding methods to respond to various user queries. These\nmethods tend to consider token selection in a simple sequential manner, making\nit easy to fall into suboptimal options when encountering uncertain tokens,\nreferred to as chaotic points in our work. Many chaotic points exist in texts\ngenerated by LLMs, and they often significantly affect the quality of\nsubsequently generated tokens, which can interfere with LLMs' generation. This\npaper proposes Self-Evaluation Decoding, SED, a decoding method for enhancing\nmodel generation. Analogous to the human decision-making process, SED\nintegrates speculation and evaluation steps into the decoding process, allowing\nLLMs to make more careful decisions and thus optimize token selection at\nchaotic points. Experimental results across various tasks using different LLMs\ndemonstrate SED's effectiveness.", "published": "2024-05-26 12:43:18", "link": "http://arxiv.org/abs/2405.16552v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Low-resourced Languages and Online Knowledge Repositories: A\n  Need-Finding Study", "abstract": "Online Knowledge Repositories (OKRs) like Wikipedia offer communities a way\nto share and preserve information about themselves and their ways of living.\nHowever, for communities with low-resourced languages -- including most African\ncommunities -- the quality and volume of content available are often\ninadequate. One reason for this lack of adequate content could be that many\nOKRs embody Western ways of knowledge preservation and sharing, requiring many\nlow-resourced language communities to adapt to new interactions. To understand\nthe challenges faced by low-resourced language contributors on the popular OKR\nWikipedia, we conducted (1) a thematic analysis of Wikipedia forum discussions\nand (2) a contextual inquiry study with 14 novice contributors. We focused on\nthree Ethiopian languages: Afan Oromo, Amharic, and Tigrinya. Our analysis\nrevealed several recurring themes; for example, contributors struggle to find\nresources to corroborate their articles in low-resourced languages, and\nlanguage technology support, like translation systems and spellcheck, result in\nseveral errors that waste contributors' time. We hope our study will support\ndesigners in making online knowledge repositories accessible to low-resourced\nlanguage speakers.", "published": "2024-05-26 19:20:26", "link": "http://arxiv.org/abs/2405.16669v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "gzip Predicts Data-dependent Scaling Laws", "abstract": "Past work has established scaling laws that predict the performance of a\nneural language model (LM) as a function of its parameter count and the number\nof tokens it's trained on, enabling optimal allocation of a fixed compute\nbudget. Are these scaling laws agnostic to training data as some prior work\nsuggests? We generate training datasets of varying complexities by modulating\nthe syntactic properties of a PCFG, finding that 1) scaling laws are sensitive\nto differences in data complexity and that 2) gzip, a compression algorithm, is\nan effective predictor of how data complexity impacts scaling properties. We\npropose a new data-dependent scaling law for LM's that accounts for the\ntraining data's gzip-compressibility; its compute-optimal frontier increases in\ndataset size preference (over parameter count preference) as training data\nbecomes harder to compress.", "published": "2024-05-26 20:33:08", "link": "http://arxiv.org/abs/2405.16684v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Predicting Rental Price of Lane Houses in Shanghai with Machine Learning\n  Methods and Large Language Models", "abstract": "Housing has emerged as a crucial concern among young individuals residing in\nmajor cities, including Shanghai. Given the unprecedented surge in property\nprices in this metropolis, young people have increasingly resorted to the\nrental market to address their housing needs. This study utilizes five\ntraditional machine learning methods: multiple linear regression (MLR), ridge\nregression (RR), lasso regression (LR), decision tree (DT), and random forest\n(RF), along with a Large Language Model (LLM) approach using ChatGPT, for\npredicting the rental prices of lane houses in Shanghai. It applies these\nmethods to examine a public data sample of about 2,609 lane house rental\ntransactions in 2021 in Shanghai, and then compares the results of these\nmethods. In terms of predictive power, RF has achieved the best performance\namong the traditional methods. However, the LLM approach, particularly in the\n10-shot scenario, shows promising results that surpass traditional methods in\nterms of R-Squared value. The three performance metrics: mean squared error\n(MSE), mean absolute error (MAE), and R-Squared, are used to evaluate the\nmodels. Our conclusion is that while traditional machine learning models offer\nrobust techniques for rental price prediction, the integration of LLM such as\nChatGPT holds significant potential for enhancing predictive accuracy.", "published": "2024-05-26 07:01:33", "link": "http://arxiv.org/abs/2405.17505v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Adaptive Activation Steering: A Tuning-Free LLM Truthfulness Improvement\n  Method for Diverse Hallucinations Categories", "abstract": "Recent studies have indicated that Large Language Models (LLMs) harbor an\ninherent understanding of truthfulness, yet often fail to consistently express\nit and generate false statements. This gap between \"knowing\" and \"telling\"\nposes a challenge for ensuring the truthfulness of generated content. Inspired\nby recent work on the practice of encoding human-interpretable concepts\nlinearly within large language models, we treat truthfulness as a specially\nlinearly encoded concept within LLMs, and introduce Adaptive Activation\nSteering (ACT), a tuning-free method that adaptively shifts LLM's activations\nin the \"truthful\" direction during inference. ACT addresses diverse categories\nof hallucinations by utilizing diverse truthfulness-related steering vectors\nand adjusting the steering intensity adaptively. Applied as an add-on across\nvarious models, ACT significantly improves truthfulness in LLaMA ($\\uparrow$\n142%), LLaMA2 ($\\uparrow$ 24%), Alpaca ($\\uparrow$ 36%), Vicuna ($\\uparrow$\n28%), LLaMA2-Chat ($\\uparrow$ 19%), and LLaMA3($\\uparrow$ 34%). Furthermore, we\nverify ACT's scalability across larger models (13B, 33B, 65B), underscoring the\nadaptability of ACT to large-scale language models. Our code is available at\nhttps://github.com/tianlwang/ACT.", "published": "2024-05-26 21:39:53", "link": "http://arxiv.org/abs/2406.00034v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SpinQuant: LLM quantization with learned rotations", "abstract": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\nCode is available at https://github.com/facebookresearch/SpinQuant.", "published": "2024-05-26 02:15:49", "link": "http://arxiv.org/abs/2405.16406v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Tensor Attention Training: Provably Efficient Learning of Higher-order\n  Transformers", "abstract": "Tensor Attention, a multi-view attention that is able to capture high-order\ncorrelations among multiple modalities, can overcome the representational\nlimitations of classical matrix attention. However, the $O(n^3)$ time\ncomplexity of tensor attention poses a significant obstacle to its utilization\nin transformers, where $n$ is the input sequence length. In this work, we prove\nthat the backward gradient of tensor attention training can be computed in\nalmost linear time $n^{1+o(1)}$, the same complexity as its forward computation\nunder the bounded entries assumption. We provide a closed-form solution for the\ngradient and propose a fast computation method utilizing polynomial\napproximation methods and tensor algebraic techniques. Furthermore, we prove\nthe necessity and tightness of our assumption through hardness analysis,\nshowing that slightly weakening it renders the gradient problem unsolvable in\ntruly subcubic time. Our theoretical results establish the feasibility of\nefficient higher-order transformer training and may facilitate practical\napplications of tensor attention architectures.", "published": "2024-05-26 02:59:13", "link": "http://arxiv.org/abs/2405.16411v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Augmented Risk Prediction for the Onset of Alzheimer's Disease from\n  Electronic Health Records with Large Language Models", "abstract": "Alzheimer's disease (AD) is the fifth-leading cause of death among Americans\naged 65 and older. Screening and early detection of AD and related dementias\n(ADRD) are critical for timely intervention and for identifying clinical trial\nparticipants. The widespread adoption of electronic health records (EHRs)\noffers an important resource for developing ADRD screening tools such as\nmachine learning based predictive models. Recent advancements in large language\nmodels (LLMs) demonstrate their unprecedented capability of encoding knowledge\nand performing reasoning, which offers them strong potential for enhancing risk\nprediction. This paper proposes a novel pipeline that augments risk prediction\nby leveraging the few-shot inference power of LLMs to make predictions on cases\nwhere traditional supervised learning methods (SLs) may not excel.\nSpecifically, we develop a collaborative pipeline that combines SLs and LLMs\nvia a confidence-driven decision-making mechanism, leveraging the strengths of\nSLs in clear-cut cases and LLMs in more complex scenarios. We evaluate this\npipeline using a real-world EHR data warehouse from Oregon Health \\& Science\nUniversity (OHSU) Hospital, encompassing EHRs from over 2.5 million patients\nand more than 20 million patient encounters. Our results show that our proposed\napproach effectively combines the power of SLs and LLMs, offering significant\nimprovements in predictive performance. This advancement holds promise for\nrevolutionizing ADRD screening and early detection practices, with potential\nimplications for better strategies of patient management and thus improving\nhealthcare.", "published": "2024-05-26 03:05:10", "link": "http://arxiv.org/abs/2405.16413v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.AP"], "primary_category": "cs.AI"}
{"title": "AI-Generated Text Detection and Classification Based on BERT Deep\n  Learning Algorithm", "abstract": "AI-generated text detection plays an increasingly important role in various\nfields. In this study, we developed an efficient AI-generated text detection\nmodel based on the BERT algorithm, which provides new ideas and methods for\nsolving related problems. In the data preprocessing stage, a series of steps\nwere taken to process the text, including operations such as converting to\nlowercase, word splitting, removing stop words, stemming extraction, removing\ndigits, and eliminating redundant spaces, to ensure data quality and accuracy.\nBy dividing the dataset into a training set and a test set in the ratio of 60%\nand 40%, and observing the changes in the accuracy and loss values during the\ntraining process, we found that the model performed well during the training\nprocess. The accuracy increases steadily from the initial 94.78% to 99.72%,\nwhile the loss value decreases from 0.261 to 0.021 and converges gradually,\nwhich indicates that the BERT model is able to detect AI-generated text with\nhigh accuracy and the prediction results are gradually approaching the real\nclassification results. Further analysis of the results of the training and\ntest sets reveals that in terms of loss value, the average loss of the training\nset is 0.0565, while the average loss of the test set is 0.0917, showing a\nslightly higher loss value. As for the accuracy, the average accuracy of the\ntraining set reaches 98.1%, while the average accuracy of the test set is\n97.71%, which is not much different from each other, indicating that the model\nhas good generalisation ability. In conclusion, the AI-generated text detection\nmodel based on the BERT algorithm proposed in this study shows high accuracy\nand stability in experiments, providing an effective solution for related\nfields.", "published": "2024-05-26 04:26:07", "link": "http://arxiv.org/abs/2405.16422v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and\n  Evaluation Framework for Chinese Psychological Counseling", "abstract": "Using large language models (LLMs) to assist psychological counseling is a\nsignificant but challenging task at present. Attempts have been made on\nimproving empathetic conversations or acting as effective assistants in the\ntreatment with LLMs. However, the existing datasets lack consulting knowledge,\nresulting in LLMs lacking professional consulting competence. Moreover, how to\nautomatically evaluate multi-turn dialogues within the counseling process\nremains an understudied area. To bridge the gap, we propose CPsyCoun, a\nreport-based multi-turn dialogue reconstruction and evaluation framework for\nChinese psychological counseling. To fully exploit psychological counseling\nreports, a two-phase approach is devised to construct high-quality dialogues\nwhile a comprehensive evaluation benchmark is developed for the effective\nautomatic evaluation of multi-turn psychological consultations. Competitive\nexperimental results demonstrate the effectiveness of our proposed framework in\npsychological counseling. We open-source the datasets and model for future\nresearch at https://github.com/CAS-SIAT-XinHai/CPsyCoun", "published": "2024-05-26 05:18:00", "link": "http://arxiv.org/abs/2405.16433v3", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "The Importance of Directional Feedback for LLM-based Optimizers", "abstract": "We study the potential of using large language models (LLMs) as an\ninteractive optimizer for solving maximization problems in a text space using\nnatural language and numerical feedback. Inspired by the classical optimization\nliterature, we classify the natural language feedback into directional and\nnon-directional, where the former is a generalization of the first-order\nfeedback to the natural language space. We find that LLMs are especially\ncapable of optimization when they are provided with {directional feedback}.\nBased on this insight, we design a new LLM-based optimizer that synthesizes\ndirectional feedback from the historical optimization trace to achieve reliable\nimprovement over iterations. Empirically, we show our LLM-based optimizer is\nmore stable and efficient in solving optimization problems, from maximizing\nmathematical functions to optimizing prompts for writing poems, compared with\nexisting techniques.", "published": "2024-05-26 05:22:35", "link": "http://arxiv.org/abs/2405.16434v2", "categories": ["cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.AI"}
{"title": "M$^3$CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal\n  Chain-of-Thought", "abstract": "Multi-modal Chain-of-Thought (MCoT) requires models to leverage knowledge\nfrom both textual and visual modalities for step-by-step reasoning, which gains\nincreasing attention. Nevertheless, the current MCoT benchmark still faces some\nchallenges: (1) absence of visual modal reasoning, (2) single-step visual modal\nreasoning, and (3) Domain missing, thereby hindering the development of MCoT.\nMotivated by this, we introduce a novel benchmark (M$^3$CoT) to address the\nabove challenges, advancing the multi-domain, multi-step, and multi-modal CoT.\nAdditionally, we conduct a thorough evaluation involving abundant MCoT\napproaches on Vision Large Language Models (VLLMs). In addition, we highlight\nthat the current VLLMs still struggle to correctly reason in M$^3$CoT and there\nremains a large gap between existing VLLMs and human performance in M$^3$CoT,\ndespite their superior results on previous MCoT benchmarks. To our knowledge,\nwe take the first meaningful step toward the multi-domain, multi-step, and\nmulti-modal scenario in MCoT. We hope that M$^3$CoT can serve as a valuable\nresource, providing a pioneering foundation in multi-domain, multi-step,\nmulti-modal chain-of-thought research.", "published": "2024-05-26 07:56:30", "link": "http://arxiv.org/abs/2405.16473v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Planning with Multi-Constraints via Collaborative Language Agents", "abstract": "The rapid advancement of neural language models has sparked a new surge of\nintelligent agent research. Unlike traditional agents, large language\nmodel-based agents (LLM agents) have emerged as a promising paradigm for\nachieving artificial general intelligence (AGI) due to their superior reasoning\nand generalization capabilities. Effective planning is crucial for the success\nof LLM agents in real-world tasks, making it a highly pursued topic in the\ncommunity. Current planning methods typically translate tasks into executable\naction sequences. However, determining a feasible or optimal sequence for\ncomplex tasks with multiple constraints at fine granularity, which often\nrequires compositing long chains of heterogeneous actions, remains challenging.\nThis paper introduces Planning with Multi-Constraints (PMC), a zero-shot\nmethodology for collaborative LLM-based multi-agent systems that simplifies\ncomplex task planning with constraints by decomposing it into a hierarchy of\nsubordinate tasks. Each subtask is then mapped into executable actions. PMC was\nassessed on two constraint-intensive benchmarks, TravelPlanner and API-Bank.\nNotably, PMC achieved an average 42.68% success rate on TravelPlanner,\nsignificantly higher than GPT-4 (2.92%), and outperforming GPT-4 with ReAct on\nAPI-Bank by 13.64%, showing the immense potential of integrating LLM with\nmulti-agent systems. We also show that PMC works with small LLM as the planning\ncore, e.g., LLaMA-3.1-8B.", "published": "2024-05-26 10:33:17", "link": "http://arxiv.org/abs/2405.16510v4", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Let Silence Speak: Enhancing Fake News Detection with Generated Comments\n  from Large Language Models", "abstract": "Fake news detection plays a crucial role in protecting social media users and\nmaintaining a healthy news ecosystem. Among existing works, comment-based fake\nnews detection methods are empirically shown as promising because comments\ncould reflect users' opinions, stances, and emotions and deepen models'\nunderstanding of fake news. Unfortunately, due to exposure bias and users'\ndifferent willingness to comment, it is not easy to obtain diverse comments in\nreality, especially for early detection scenarios. Without obtaining the\ncomments from the ``silent'' users, the perceived opinions may be incomplete,\nsubsequently affecting news veracity judgment. In this paper, we explore the\npossibility of finding an alternative source of comments to guarantee the\navailability of diverse comments, especially those from silent users.\nSpecifically, we propose to adopt large language models (LLMs) as a user\nsimulator and comment generator, and design GenFEND, a generated\nfeedback-enhanced detection framework, which generates comments by prompting\nLLMs with diverse user profiles and aggregating generated comments from\nmultiple subpopulation groups. Experiments demonstrate the effectiveness of\nGenFEND and further analysis shows that the generated comments cover more\ndiverse users and could even be more effective than actual comments.", "published": "2024-05-26 17:09:23", "link": "http://arxiv.org/abs/2405.16631v1", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "A Survey of Multimodal Large Language Model from A Data-centric\n  Perspective", "abstract": "Multimodal large language models (MLLMs) enhance the capabilities of standard\nlarge language models by integrating and processing data from multiple\nmodalities, including text, vision, audio, video, and 3D environments. Data\nplays a pivotal role in the development and refinement of these models. In this\nsurvey, we comprehensively review the literature on MLLMs from a data-centric\nperspective. Specifically, we explore methods for preparing multimodal data\nduring the pretraining and adaptation phases of MLLMs. Additionally, we analyze\nthe evaluation methods for the datasets and review the benchmarks for\nevaluating MLLMs. Our survey also outlines potential future research\ndirections. This work aims to provide researchers with a detailed understanding\nof the data-driven aspects of MLLMs, fostering further exploration and\ninnovation in this field.", "published": "2024-05-26 17:31:21", "link": "http://arxiv.org/abs/2405.16640v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.AI"}
{"title": "RLSF: Reinforcement Learning via Symbolic Feedback", "abstract": "Reinforcement Learning with Human Feedback (RLHF) is considered a standard\napproach to fine-tuning Large Language Models (LLMs). However, such methods\noften face limitations such as unsound black-box reward models, difficulties in\ncollecting human preference data, and the reliance on sparse scalar rewards.\nThese methods often fall short when applied to tasks that require complex\ndomain-specific understanding.\n  To address these challenges, we propose a new fine-tuning paradigm we refer\nto as Reinforcement Learning via Symbolic Feedback (RLSF), which aims to\nimprove domain-specific understanding of LLMs more effectively than traditional\nreward signals. In the RLSF setting, the LLM being fine-tuned is considered an\nRL agent, while the environment is allowed access to reasoning or domain\nknowledge tools (e.g., solvers, provers, algebra systems, or knowledge bases).\nCrucially, in RLSF, these reasoning tools can provide feedback to the LLMs via\npoly-sized certificates (e.g., proofs), that characterize errors in the\nLLM-generated object with respect to some correctness specification. As a\nbonus, our RLSF approach does not require the reasoning systems we use to be\ndifferentiable. The ability of RLSF-based fine-tuning to leverage\ncertificate-generating symbolic tools enables sound fine-grained (token-level)\nreward signals to LLMs, and thus addresses the limitations of traditional\nreward models mentioned above.\n  Via extensive evaluations, we show that our RLSF-based fine-tuning of LLMs\noutperforms traditional approaches on five different applications, namely,\nprogram synthesis from natural language pseudo-code to programming language,\nthree chemistry tasks, and solving the Game of 24. A takeaway is that\nfine-tuning via RLSF enables relatively smaller LLMs to significantly\noutperform closed-source models that are orders of magnitude larger (e.g.,\nGPT-4).", "published": "2024-05-26 18:49:59", "link": "http://arxiv.org/abs/2405.16661v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Conjunctive categorial grammars and Lambek grammars with additives", "abstract": "A new family of categorial grammars is proposed, defined by enriching basic\ncategorial grammars with a conjunction operation. It is proved that the\nformalism obtained in this way has the same expressive power as conjunctive\ngrammars, that is, context-free grammars enhanced with conjunction. It is also\nshown that categorial grammars with conjunction can be naturally embedded into\nthe Lambek calculus with conjunction and disjunction operations. This further\nimplies that a certain NP-complete set can be defined in the Lambek calculus\nwith conjunction. We also show how to handle some subtle issues connected with\nthe empty string. Finally, we prove that a language generated by a conjunctive\ngrammar can be described by a Lambek grammar with disjunction (but without\nconjunction).", "published": "2024-05-26 18:53:56", "link": "http://arxiv.org/abs/2405.16662v1", "categories": ["cs.LO", "cs.CL", "math.LO", "03D05", "F.4.2; F.4.1"], "primary_category": "cs.LO"}
{"title": "Crossmodal ASR Error Correction with Discrete Speech Units", "abstract": "ASR remains unsatisfactory in scenarios where the speaking style diverges\nfrom that used to train ASR systems, resulting in erroneous transcripts. To\naddress this, ASR Error Correction (AEC), a post-ASR processing approach, is\nrequired. In this work, we tackle an understudied issue: the Low-Resource\nOut-of-Domain (LROOD) problem, by investigating crossmodal AEC on very limited\ndownstream data with 1-best hypothesis transcription. We explore pre-training\nand fine-tuning strategies and uncover an ASR domain discrepancy phenomenon,\nshedding light on appropriate training schemes for LROOD data. Moreover, we\npropose the incorporation of discrete speech units to align with and enhance\nthe word embeddings for improving AEC quality. Results from multiple corpora\nand several evaluation metrics demonstrate the feasibility and efficacy of our\nproposed AEC approach on LROOD data as well as its generalizability and\nsuperiority on large-scale data. Finally, a study on speech emotion recognition\nconfirms that our model produces ASR error-robust transcripts suitable for\ndownstream applications.", "published": "2024-05-26 19:58:38", "link": "http://arxiv.org/abs/2405.16677v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Systematic Review of Federated Generative Models", "abstract": "Federated Learning (FL) has emerged as a solution for distributed systems\nthat allow clients to train models on their data and only share models instead\nof local data. Generative Models are designed to learn the distribution of a\ndataset and generate new data samples that are similar to the original data.\nMany prior works have tried proposing Federated Generative Models. Using\nFederated Learning and Generative Models together can be susceptible to\nattacks, and designing the optimal architecture remains challenging.\n  This survey covers the growing interest in the intersection of FL and\nGenerative Models by comprehensively reviewing research conducted from 2019 to\n2024. We systematically compare nearly 100 papers, focusing on their FL and\nGenerative Model methods and privacy considerations. To make this field more\naccessible to newcomers, we highlight the state-of-the-art advancements and\nidentify unresolved challenges, offering insights for future research in this\nevolving field.", "published": "2024-05-26 20:20:44", "link": "http://arxiv.org/abs/2405.16682v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Implicit Multimodal Alignment: On the Generalization of Frozen LLMs to\n  Multimodal Inputs", "abstract": "Large Language Models (LLMs) have demonstrated impressive performance on\nmultimodal tasks, without any multimodal finetuning. They are the building\nblock for Large Multimodal Models, yet, we still lack a proper understanding of\ntheir success. In this work, we expose frozen LLMs to image, video, audio and\ntext inputs and analyse their internal representation aiming to understand\ntheir generalization beyond textual inputs.\n  Findings. Perceptual tokens (1) are easily distinguishable from textual ones\ninside LLMs, with significantly different representations, and complete\ntranslation to textual tokens does not exist. Yet, (2) both perceptual and\ntextual tokens activate similar LLM weights. Despite being different, (3)\nperceptual and textual tokens are implicitly aligned inside LLMs, we call this\nthe implicit multimodal alignment (IMA), and argue that this is linked to\narchitectural design, helping LLMs to generalize. This provide more evidence to\nbelieve that the generalization of LLMs to multimodal inputs is mainly due to\ntheir architecture.\n  Implications. (1) We find a positive correlation between the implicit\nalignment score and the task performance, suggesting that this could act as a\nproxy metric for model evaluation and selection. (2) A negative correlation\nexists regarding hallucinations, revealing that this problem is mainly due to\nmisalignment between the internal perceptual and textual representations. (3)\nPerceptual tokens change slightly throughout the model, thus, we propose\ndifferent approaches to skip computations (e.g. in FFN layers), and\nsignificantly reduce the inference cost. (4) Due to the slowly changing\nembeddings across layers, and the high overlap between textual and multimodal\nactivated weights, we compress LLMs by keeping only 1 subnetwork that works\nwell across a wide range of multimodal tasks. Paper code:\nhttps://github.com/mshukor/ima-lmms.", "published": "2024-05-26 21:31:59", "link": "http://arxiv.org/abs/2405.16700v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Zamba: A Compact 7B SSM Hybrid Model", "abstract": "In this technical report, we present Zamba, a novel 7B SSM-transformer hybrid\nmodel which achieves competitive performance against leading open-weight models\nat a comparable scale. Zamba is trained on 1T tokens from openly available\ndatasets and is the best non-transformer model at this scale. Zamba pioneers a\nunique architecture combining a Mamba backbone with a single shared attention\nmodule, thus obtaining the benefits of attention at minimal parameter cost. Due\nto its architecture, Zamba is significantly faster at inference than comparable\ntransformer models and requires substantially less memory for generation of\nlong sequences. Zamba is pretrained in two phases: the first phase is based on\nexisting web datasets, while the second one consists of annealing the model\nover high-quality instruct and synthetic datasets, and is characterized by a\nrapid learning rate decay. We open-source the weights and all checkpoints for\nZamba, through both phase 1 and annealing phases.", "published": "2024-05-26 22:23:02", "link": "http://arxiv.org/abs/2405.16712v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Crafting Interpretable Embeddings by Asking LLMs Questions", "abstract": "Large language models (LLMs) have rapidly improved text embeddings for a\ngrowing array of natural-language processing tasks. However, their opaqueness\nand proliferation into scientific domains such as neuroscience have created a\ngrowing need for interpretability. Here, we ask whether we can obtain\ninterpretable embeddings through LLM prompting. We introduce question-answering\nembeddings (QA-Emb), embeddings where each feature represents an answer to a\nyes/no question asked to an LLM. Training QA-Emb reduces to selecting a set of\nunderlying questions rather than learning model weights.\n  We use QA-Emb to flexibly generate interpretable models for predicting fMRI\nvoxel responses to language stimuli. QA-Emb significantly outperforms an\nestablished interpretable baseline, and does so while requiring very few\nquestions. This paves the way towards building flexible feature spaces that can\nconcretize and evaluate our understanding of semantic brain representations. We\nadditionally find that QA-Emb can be effectively approximated with an efficient\nmodel, and we explore broader applications in simple NLP tasks.", "published": "2024-05-26 22:30:29", "link": "http://arxiv.org/abs/2405.16714v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Code Repair with LLMs gives an Exploration-Exploitation Tradeoff", "abstract": "Iteratively improving and repairing source code with large language models\n(LLMs), known as refinement, has emerged as a popular way of generating\nprograms that would be too complex to construct in one shot. Given a bank of\ntest cases, together with a candidate program, an LLM can improve that program\nby being prompted with failed test cases. But it remains an open question how\nto best iteratively refine code, with prior work employing simple greedy or\nbreadth-first strategies. We show here that refinement exposes an\nexplore-exploit tradeoff: exploit by refining the program that passes the most\ntest cases, or explore by refining a lesser considered program. We frame this\nas an arm-acquiring bandit problem, which we solve with Thompson Sampling. The\nresulting LLM-based program synthesis algorithm is broadly applicable: Across\nloop invariant synthesis, visual reasoning puzzles, and competition programming\nproblems, we find that our new method can solve more problems using fewer\nlanguage model calls.", "published": "2024-05-26 04:00:30", "link": "http://arxiv.org/abs/2405.17503v3", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL"], "primary_category": "cs.SE"}
{"title": "Medical MLLM is Vulnerable: Cross-Modality Jailbreak and Mismatched\n  Attacks on Medical Multimodal Large Language Models", "abstract": "Security concerns related to Large Language Models (LLMs) have been\nextensively explored, yet the safety implications for Multimodal Large Language\nModels (MLLMs), particularly in medical contexts (MedMLLMs), remain\ninsufficiently studied. This paper delves into the underexplored security\nvulnerabilities of MedMLLMs, especially when deployed in clinical environments\nwhere the accuracy and relevance of question-and-answer interactions are\ncritically tested against complex medical challenges. By combining existing\nclinical medical data with atypical natural phenomena, we define the mismatched\nmalicious attack (2M-attack) and introduce its optimized version, known as the\noptimized mismatched malicious attack (O2M-attack or 2M-optimization). Using\nthe voluminous 3MAD dataset that we construct, which covers a wide range of\nmedical image modalities and harmful medical scenarios, we conduct a\ncomprehensive analysis and propose the MCM optimization method, which\nsignificantly enhances the attack success rate on MedMLLMs. Evaluations with\nthis dataset and attack methods, including white-box attacks on LLaVA-Med and\ntransfer attacks (black-box) on four other SOTA models, indicate that even\nMedMLLMs designed with enhanced security features remain vulnerable to security\nbreaches. Our work underscores the urgent need for a concerted effort to\nimplement robust security measures and enhance the safety and efficacy of\nopen-source MedMLLMs, particularly given the potential severity of jailbreak\nattacks and other malicious or clinically significant exploits in medical\nsettings. Our code is available at https://github.com/dirtycomputer/O2M_attack.", "published": "2024-05-26 19:11:21", "link": "http://arxiv.org/abs/2405.20775v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CR"}
{"title": "SymTax: Symbiotic Relationship and Taxonomy Fusion for Effective\n  Citation Recommendation", "abstract": "Citing pertinent literature is pivotal to writing and reviewing a scientific\ndocument. Existing techniques mainly focus on the local context or the global\ncontext for recommending citations but fail to consider the actual human\ncitation behaviour. We propose SymTax, a three-stage recommendation\narchitecture that considers both the local and the global context, and\nadditionally the taxonomical representations of query-candidate tuples and the\nSymbiosis prevailing amongst them. SymTax learns to embed the infused\ntaxonomies in the hyperbolic space and uses hyperbolic separation as a latent\nfeature to compute query-candidate similarity. We build a novel and large\ndataset ArSyTa containing 8.27 million citation contexts and describe the\ncreation process in detail. We conduct extensive experiments and ablation\nstudies to demonstrate the effectiveness and design choice of each module in\nour framework. Also, combinatorial analysis from our experiments shed light on\nthe choice of language models (LMs) and fusion embedding, and the inclusion of\nsection heading as a signal. Our proposed module that captures the symbiotic\nrelationship solely leads to performance gains of 26.66% and 39.25% in Recall@5\nw.r.t. SOTA on ACL-200 and RefSeer datasets, respectively. The complete\nframework yields a gain of 22.56% in Recall@5 wrt SOTA on our proposed dataset.\nThe code and dataset are available at https://github.com/goyalkaraniit/SymTax", "published": "2024-05-26 21:51:58", "link": "http://arxiv.org/abs/2406.01606v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "The global landscape of academic guidelines for generative AI and Large\n  Language Models", "abstract": "The integration of Generative Artificial Intelligence (GAI) and Large\nLanguage Models (LLMs) in academia has spurred a global discourse on their\npotential pedagogical benefits and ethical considerations. Positive reactions\nhighlight some potential, such as collaborative creativity, increased access to\neducation, and empowerment of trainers and trainees. However, negative\nreactions raise concerns about ethical complexities, balancing innovation and\nacademic integrity, unequal access, and misinformation risks. Through a\nsystematic survey and text-mining-based analysis of global and national\ndirectives, insights from independent research, and eighty university-level\nguidelines, this study provides a nuanced understanding of the opportunities\nand challenges posed by GAI and LLMs in education. It emphasizes the importance\nof balanced approaches that harness the benefits of these technologies while\naddressing ethical considerations and ensuring equitable access and educational\noutcomes. The paper concludes with recommendations for fostering responsible\ninnovation and ethical practices to guide the integration of GAI and LLMs in\nacademia.", "published": "2024-05-26 15:28:24", "link": "http://arxiv.org/abs/2406.18842v3", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Reconstructing the Charlie Parker Omnibook using an audio-to-score\n  automatic transcription pipeline", "abstract": "The Charlie Parker Omnibook is a cornerstone of jazz music education,\ndescribed by pianist Ethan Iverson as \"the most important jazz education text\never published\". In this work we propose a new transcription pipeline and\nexplore the extent to which state of the art music technology is able to\nreconstruct these scores directly from the audio without human intervention.\nOur pipeline includes: a newly trained source separation model for saxophone, a\nnew MIDI transcription model for solo saxophone and an adaptation of an\nexisting MIDI-to-score method for monophonic instruments.\n  To assess this pipeline we also provide an enhanced dataset of Charlie Parker\ntranscriptions as score-audio pairs with accurate MIDI alignments and downbeat\nannotations. This represents a challenging new benchmark for automatic\naudio-to-score transcription that we hope will advance research into areas\nbeyond transcribing audio-to-MIDI alone.\n  Together, these form another step towards producing scores that musicians can\nuse directly, without the need for onerous corrections or revisions. To\nfacilitate future research, all model checkpoints and data are made available\nto download along with code for the transcription pipeline. Improvements in our\nmodular pipeline could one day make the automatic transcription of complex jazz\nsolos a routine possibility, thereby enriching the resources available for\nmusic education and preservation.", "published": "2024-05-26 20:41:36", "link": "http://arxiv.org/abs/2405.16687v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
