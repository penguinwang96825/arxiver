{"title": "Arguments to Key Points Mapping with Prompt-based Learning", "abstract": "Handling and digesting a huge amount of information in an efficient manner\nhas been a long-term demand in modern society. Some solutions to map key points\n(short textual summaries capturing essential information and filtering\nredundancies) to a large number of arguments/opinions have been provided\nrecently (Bar-Haim et al., 2020). To complement the full picture of the\nargument-to-keypoint mapping task, we mainly propose two approaches in this\npaper. The first approach is to incorporate prompt engineering for fine-tuning\nthe pre-trained language models (PLMs). The second approach utilizes\nprompt-based learning in PLMs to generate intermediary texts, which are then\ncombined with the original argument-keypoint pairs and fed as inputs to a\nclassifier, thereby mapping them. Furthermore, we extend the experiments to\ncross/in-domain to conduct an in-depth analysis. In our evaluation, we find\nthat i) using prompt engineering in a more direct way (Approach 1) can yield\npromising results and improve the performance; ii) Approach 2 performs\nconsiderably worse than Approach 1 due to the negation issue of the PLM.", "published": "2022-11-28 01:48:29", "link": "http://arxiv.org/abs/2211.14995v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "STAGE: Span Tagging and Greedy Inference Scheme for Aspect Sentiment\n  Triplet Extraction", "abstract": "Aspect Sentiment Triplet Extraction (ASTE) has become an emerging task in\nsentiment analysis research, aiming to extract triplets of the aspect term, its\ncorresponding opinion term, and its associated sentiment polarity from a given\nsentence. Recently, many neural networks based models with different tagging\nschemes have been proposed, but almost all of them have their limitations:\nheavily relying on 1) prior assumption that each word is only associated with a\nsingle role (e.g., aspect term, or opinion term, etc. ) and 2) word-level\ninteractions and treating each opinion/aspect as a set of independent words.\nHence, they perform poorly on the complex ASTE task, such as a word associated\nwith multiple roles or an aspect/opinion term with multiple words. Hence, we\npropose a novel approach, Span TAgging and Greedy infErence (STAGE), to extract\nsentiment triplets in span-level, where each span may consist of multiple words\nand play different roles simultaneously. To this end, this paper formulates the\nASTE task as a multi-class span classification problem. Specifically, STAGE\ngenerates more accurate aspect sentiment triplet extractions via exploring\nspan-level information and constraints, which consists of two components,\nnamely, span tagging scheme and greedy inference strategy. The former tag all\npossible candidate spans based on a newly-defined tagging set. The latter\nretrieves the aspect/opinion term with the maximum length from the candidate\nsentiment snippet to output sentiment triplets. Furthermore, we propose a\nsimple but effective model based on the STAGE, which outperforms the\nstate-of-the-arts by a large margin on four widely-used datasets. Moreover, our\nSTAGE can be easily generalized to other pair/triplet extraction tasks, which\nalso demonstrates the superiority of the proposed scheme STAGE.", "published": "2022-11-28 02:07:03", "link": "http://arxiv.org/abs/2211.15003v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BJTU-WeChat's Systems for the WMT22 Chat Translation Task", "abstract": "This paper introduces the joint submission of the Beijing Jiaotong University\nand WeChat AI to the WMT'22 chat translation task for English-German. Based on\nthe Transformer, we apply several effective variants. In our experiments, we\nutilize the pre-training-then-fine-tuning paradigm. In the first pre-training\nstage, we employ data filtering and synthetic data generation (i.e.,\nback-translation, forward-translation, and knowledge distillation). In the\nsecond fine-tuning stage, we investigate speaker-aware in-domain data\ngeneration, speaker adaptation, prompt-based context modeling, target denoising\nfine-tuning, and boosted self-COMET-based model ensemble. Our systems achieve\n0.810 and 0.946 COMET scores. The COMET scores of English-German and\nGerman-English are the highest among all submissions.", "published": "2022-11-28 02:35:04", "link": "http://arxiv.org/abs/2211.15009v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Summer: WeChat Neural Machine Translation Systems for the WMT22\n  Biomedical Translation Task", "abstract": "This paper introduces WeChat's participation in WMT 2022 shared biomedical\ntranslation task on Chinese to English. Our systems are based on the\nTransformer, and use several different Transformer structures to improve the\nquality of translation. In our experiments, we employ data filtering, data\ngeneration, several variants of Transformer, fine-tuning and model ensemble.\nOur Chinese$\\to$English system, named Summer, achieves the highest BLEU score\namong all submissions.", "published": "2022-11-28 03:10:50", "link": "http://arxiv.org/abs/2211.15022v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SongRewriter: A Chinese Song Rewriting System with Controllable Content\n  and Rhyme Scheme", "abstract": "Although lyrics generation has achieved significant progress in recent years,\nit has limited practical applications because the generated lyrics cannot be\nperformed without composing compatible melodies. In this work, we bridge this\npractical gap by proposing a song rewriting system which rewrites the lyrics of\nan existing song such that the generated lyrics are compatible with the rhythm\nof the existing melody and thus singable. In particular, we propose\nSongRewriter,a controllable Chinese lyrics generation and editing system which\nassists users without prior knowledge of melody composition. The system is\ntrained by a randomized multi-level masking strategy which produces a unified\nmodel for generating entirely new lyrics or editing a few fragments. To improve\nthe controllabiliy of the generation process, we further incorporate a keyword\nprompt to control the lexical choices of the content and propose novel decoding\nconstraints and a vowel modeling task to enable flexible end and internal rhyme\nschemes. While prior rhyming metrics are mainly for rap lyrics, we propose\nthree novel rhyming evaluation metrics for song lyrics. Both automatic and\nhuman evaluations show that the proposed model performs better than the\nstate-of-the-art models in both contents and rhyming quality.", "published": "2022-11-28 03:52:05", "link": "http://arxiv.org/abs/2211.15037v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Pre-Trained Models with Extra-Large Vocabularies: A Contrastive\n  Analysis of Hebrew BERT Models and a New One to Outperform Them All", "abstract": "We present a new pre-trained language model (PLM) for modern Hebrew, termed\nAlephBERTGimmel, which employs a much larger vocabulary (128K items) than\nstandard Hebrew PLMs before. We perform a contrastive analysis of this model\nagainst all previous Hebrew PLMs (mBERT, heBERT, AlephBERT) and assess the\neffects of larger vocabularies on task performance. Our experiments show that\nlarger vocabularies lead to fewer splits, and that reducing splits is better\nfor model performance, across different tasks. All in all this new model\nachieves new SOTA on all available Hebrew benchmarks, including Morphological\nSegmentation, POS Tagging, Full Morphological Analysis, NER, and Sentiment\nAnalysis. Subsequently we advocate for PLMs that are larger not only in terms\nof number of layers or training data, but also in terms of their vocabulary. We\nrelease the new model publicly for unrestricted use.", "published": "2022-11-28 10:17:35", "link": "http://arxiv.org/abs/2211.15199v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HERDPhobia: A Dataset for Hate Speech against Fulani in Nigeria", "abstract": "Social media platforms allow users to freely share their opinions about\nissues or anything they feel like. However, they also make it easier to spread\nhate and abusive content. The Fulani ethnic group has been the victim of this\nunfortunate phenomenon. This paper introduces the HERDPhobia - the first\nannotated hate speech dataset on Fulani herders in Nigeria - in three\nlanguages: English, Nigerian-Pidgin, and Hausa. We present a benchmark\nexperiment using pre-trained languages models to classify the tweets as either\nhateful or non-hateful. Our experiment shows that the XML-T model provides\nbetter performance with 99.83% weighted F1. We released the dataset at\nhttps://github.com/hausanlp/HERDPhobia for further research.", "published": "2022-11-28 12:30:11", "link": "http://arxiv.org/abs/2211.15262v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "E2E Segmentation in a Two-Pass Cascaded Encoder ASR Model", "abstract": "We explore unifying a neural segmenter with two-pass cascaded encoder ASR\ninto a single model. A key challenge is allowing the segmenter (which runs in\nreal-time, synchronously with the decoder) to finalize the 2nd pass (which runs\n900 ms behind real-time) without introducing user-perceived latency or deletion\nerrors during inference. We propose a design where the neural segmenter is\nintegrated with the causal 1st pass decoder to emit a end-of-segment (EOS)\nsignal in real-time. The EOS signal is then used to finalize the non-causal 2nd\npass. We experiment with different ways to finalize the 2nd pass, and find that\na novel dummy frame injection strategy allows for simultaneous high quality 2nd\npass results and low finalization latency. On a real-world long-form captioning\ntask (YouTube), we achieve 2.4% relative WER and 140 ms EOS latency gains over\na baseline VAD-based segmenter with the same cascaded encoder.", "published": "2022-11-28 15:18:07", "link": "http://arxiv.org/abs/2211.15432v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatically Extracting Information in Medical Dialogue: Expert System\n  And Attention for Labelling", "abstract": "Medical dialogue information extraction is becoming an increasingly\nsignificant problem in modern medical care. It is difficult to extract key\ninformation from electronic medical records (EMRs) due to their large numbers.\nPreviously, researchers proposed attention-based models for retrieving features\nfrom EMRs, but their limitations were reflected in their inability to recognize\ndifferent categories in medical dialogues. In this paper, we propose a novel\nmodel, Expert System and Attention for Labelling (ESAL). We use mixture of\nexperts and pre-trained BERT to retrieve the semantics of different categories,\nenabling the model to fuse the differences between them. In our experiment,\nESAL was applied to a public dataset and the experimental results indicated\nthat ESAL significantly improved the performance of Medical Information\nClassification.", "published": "2022-11-28 16:49:13", "link": "http://arxiv.org/abs/2211.15544v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attack on Unfair ToS Clause Detection: A Case Study using Universal\n  Adversarial Triggers", "abstract": "Recent work has demonstrated that natural language processing techniques can\nsupport consumer protection by automatically detecting unfair clauses in the\nTerms of Service (ToS) Agreement. This work demonstrates that transformer-based\nToS analysis systems are vulnerable to adversarial attacks. We conduct\nexperiments attacking an unfair-clause detector with universal adversarial\ntriggers. Experiments show that a minor perturbation of the text can\nconsiderably reduce the detection performance. Moreover, to measure the\ndetectability of the triggers, we conduct a detailed human evaluation study by\ncollecting both answer accuracy and response time from the participants. The\nresults show that the naturalness of the triggers remains key to tricking\nreaders.", "published": "2022-11-28 17:01:19", "link": "http://arxiv.org/abs/2211.15556v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatically generating question-answer pairs for assessing basic\n  reading comprehension in Swedish", "abstract": "This paper presents an evaluation of the quality of automatically generated\nreading comprehension questions from Swedish text, using the Quinductor method.\nThis method is a light-weight, data-driven but non-neural method for automatic\nquestion generation (QG). The evaluation shows that Quinductor is a viable QG\nmethod that can provide a strong baseline for neural-network-based QG methods.", "published": "2022-11-28 17:27:14", "link": "http://arxiv.org/abs/2211.15568v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contrastive Novelty-Augmented Learning: Anticipating Outliers with Large\n  Language Models", "abstract": "In many task settings, text classification models are likely to encounter\nexamples from novel classes on which they cannot predict correctly. Selective\nprediction, in which models abstain on low-confidence examples, provides a\npossible solution, but existing models are often overly confident on unseen\nclasses. To remedy this overconfidence, we introduce Contrastive\nNovelty-Augmented Learning (CoNAL), a two-step method that generates OOD\nexamples representative of novel classes, then trains to decrease confidence on\nthem. First, we generate OOD examples by prompting a large language model\ntwice: we prompt it to enumerate relevant novel classes, then generate examples\nfrom each novel class matching the task format. Second, we train a classifier\nwith a novel contrastive objective that encourages lower confidence on\ngenerated OOD examples than training examples. When trained with CoNAL,\nclassifiers improve in their ability to detect and abstain on novel class\nexamples over prior methods by an average of 2.3% in terms of accuracy under\nthe accuracy-coverage curve (AUAC) and 5.5% AUROC across 4 NLP datasets, with\nno cost to in-distribution accuracy.", "published": "2022-11-28 19:03:35", "link": "http://arxiv.org/abs/2211.15718v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controlled Language Generation for Language Learning Items", "abstract": "This work aims to employ natural language generation (NLG) to rapidly\ngenerate items for English language learning applications: this requires both\nlanguage models capable of generating fluent, high-quality English, and to\ncontrol the output of the generation to match the requirements of the relevant\nitems. We experiment with deep pretrained models for this task, developing\nnovel methods for controlling items for factors relevant in language learning:\ndiverse sentences for different proficiency levels and argument structure to\ntest grammar. Human evaluation demonstrates high grammatically scores for all\nmodels (3.4 and above out of 4), and higher length (24%) and complexity (9%)\nover the baseline for the advanced proficiency model. Our results show that we\ncan achieve strong performance while adding additional control to ensure\ndiverse, tailored content for individual users.", "published": "2022-11-28 19:28:12", "link": "http://arxiv.org/abs/2211.15731v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Mathematically Modeling the Lexicon Entropy of Emergent Language", "abstract": "We formulate a stochastic process, FiLex, as a mathematical model of lexicon\nentropy in deep learning-based emergent language systems. Defining a model\nmathematically allows it to generate clear predictions which can be directly\nand decisively tested. We empirically verify across four different environments\nthat FiLex predicts the correct correlation between hyperparameters (training\nsteps, lexicon size, learning rate, rollout buffer size, and Gumbel-Softmax\ntemperature) and the emergent language's entropy in 20 out of 20\nenvironment-hyperparameter combinations. Furthermore, our experiments reveal\nthat different environments show diverse relationships between their\nhyperparameters and entropy which demonstrates the need for a model which can\nmake well-defined predictions at a precise level of granularity.", "published": "2022-11-28 21:35:24", "link": "http://arxiv.org/abs/2211.15783v2", "categories": ["cs.CL", "I.2.11; I.2.7; I.6.m"], "primary_category": "cs.CL"}
{"title": "Is it Required? Ranking the Skills Required for a Job-Title", "abstract": "In this paper, we describe our method for ranking the skills required for a\ngiven job title. Our analysis shows that important/relevant skills appear more\nfrequently in similar job titles. We train a Language-agnostic BERT Sentence\nEncoder (LaBSE) model to predict the importance of the skills using weak\nsupervision. We show the model can learn the importance of skills and perform\nwell in other languages. Furthermore, we show how the Inverse Document\nFrequency factor of skill boosts the specialised skills.", "published": "2022-11-28 10:27:11", "link": "http://arxiv.org/abs/2212.08553v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-tuning language models to find agreement among humans with diverse\n  preferences", "abstract": "Recent work in large language modeling (LLMs) has used fine-tuning to align\noutputs with the preferences of a prototypical user. This work assumes that\nhuman preferences are static and homogeneous across individuals, so that\naligning to a a single \"generic\" user will confer more general alignment. Here,\nwe embrace the heterogeneity of human preferences to consider a different\nchallenge: how might a machine help people with diverse views find agreement?\nWe fine-tune a 70 billion parameter LLM to generate statements that maximize\nthe expected approval for a group of people with potentially diverse opinions.\nHuman participants provide written opinions on thousands of questions touching\non moral and political issues (e.g., \"should we raise taxes on the rich?\"), and\nrate the LLM's generated candidate consensus statements for agreement and\nquality. A reward model is then trained to predict individual preferences,\nenabling it to quantify and rank consensus statements in terms of their appeal\nto the overall group, defined according to different aggregation (social\nwelfare) functions. The model produces consensus statements that are preferred\nby human users over those from prompted LLMs (>70%) and significantly\noutperforms a tight fine-tuned baseline that lacks the final ranking step.\nFurther, our best model's consensus statements are preferred over the best\nhuman-generated opinions (>65%). We find that when we silently constructed\nconsensus statements from only a subset of group members, those who were\nexcluded were more likely to dissent, revealing the sensitivity of the\nconsensus to individual contributions. These results highlight the potential to\nuse LLMs to help groups of humans align their values with one another.", "published": "2022-11-28 02:24:14", "link": "http://arxiv.org/abs/2211.15006v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Continuous diffusion for categorical data", "abstract": "Diffusion models have quickly become the go-to paradigm for generative\nmodelling of perceptual signals (such as images and sound) through iterative\nrefinement. Their success hinges on the fact that the underlying physical\nphenomena are continuous. For inherently discrete and categorical data such as\nlanguage, various diffusion-inspired alternatives have been proposed. However,\nthe continuous nature of diffusion models conveys many benefits, and in this\nwork we endeavour to preserve it. We propose CDCD, a framework for modelling\ncategorical data with diffusion models that are continuous both in time and\ninput space. We demonstrate its efficacy on several language modelling tasks.", "published": "2022-11-28 06:08:54", "link": "http://arxiv.org/abs/2211.15089v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scientific and Creative Analogies in Pretrained Language Models", "abstract": "This paper examines the encoding of analogy in large-scale pretrained\nlanguage models, such as BERT and GPT-2. Existing analogy datasets typically\nfocus on a limited set of analogical relations, with a high similarity of the\ntwo domains between which the analogy holds. As a more realistic setup, we\nintroduce the Scientific and Creative Analogy dataset (SCAN), a novel analogy\ndataset containing systematic mappings of multiple attributes and relational\nstructures across dissimilar domains. Using this dataset, we test the\nanalogical reasoning capabilities of several widely-used pretrained language\nmodels (LMs). We find that state-of-the-art LMs achieve low performance on\nthese complex analogy tasks, highlighting the challenges still posed by analogy\nunderstanding.", "published": "2022-11-28 12:49:44", "link": "http://arxiv.org/abs/2211.15268v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey on Conversational Search and Applications in Biomedicine", "abstract": "This paper aims to provide a radical rundown on Conversation Search\n(ConvSearch), an approach to enhance the information retrieval method where\nusers engage in a dialogue for the information-seeking tasks. In this survey,\nwe predominantly focused on the human interactive characteristics of the\nConvSearch systems, highlighting the operations of the action modules, likely\nthe Retrieval system, Question-Answering, and Recommender system. We labeled\nvarious ConvSearch research problems in knowledge bases, natural language\nprocessing, and dialogue management systems along with the action modules. We\nfurther categorized the framework to ConvSearch and the application is directed\ntoward biomedical and healthcare fields for the utilization of clinical social\ntechnology. Finally, we conclude by talking through the challenges and issues\nof ConvSearch, particularly in Bio-Medicine. Our main aim is to provide an\nintegrated and unified vision of the ConvSearch components from different\nfields, which benefit the information-seeking process in healthcare systems.", "published": "2022-11-28 14:08:53", "link": "http://arxiv.org/abs/2211.15328v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Considerations for meaningful sign language machine translation based on\n  glosses", "abstract": "Automatic sign language processing is gaining popularity in Natural Language\nProcessing (NLP) research (Yin et al., 2021). In machine translation (MT) in\nparticular, sign language translation based on glosses is a prominent approach.\nIn this paper, we review recent works on neural gloss translation. We find that\nlimitations of glosses in general and limitations of specific datasets are not\ndiscussed in a transparent manner and that there is no common standard for\nevaluation.\n  To address these issues, we put forward concrete recommendations for future\nresearch on gloss translation. Our suggestions advocate awareness of the\ninherent limitations of gloss-based approaches, realistic datasets, stronger\nbaselines and convincing evaluation.", "published": "2022-11-28 15:51:58", "link": "http://arxiv.org/abs/2211.15464v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "G^3: Geolocation via Guidebook Grounding", "abstract": "We demonstrate how language can improve geolocation: the task of predicting\nthe location where an image was taken. Here we study explicit knowledge from\nhuman-written guidebooks that describe the salient and class-discriminative\nvisual features humans use for geolocation. We propose the task of Geolocation\nvia Guidebook Grounding that uses a dataset of StreetView images from a diverse\nset of locations and an associated textual guidebook for GeoGuessr, a popular\ninteractive geolocation game. Our approach predicts a country for each image by\nattending over the clues automatically extracted from the guidebook.\nSupervising attention with country-level pseudo labels achieves the best\nperformance. Our approach substantially outperforms a state-of-the-art\nimage-only geolocation method, with an improvement of over 5% in Top-1\naccuracy. Our dataset and code can be found at\nhttps://github.com/g-luo/geolocation_via_guidebook_grounding.", "published": "2022-11-28 16:34:40", "link": "http://arxiv.org/abs/2211.15521v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Sentiment analysis and opinion mining on E-commerce site", "abstract": "Sentiment analysis or opinion mining help to illustrate the phrase NLP\n(Natural Language Processing). Sentiment analysis has been the most significant\ntopic in recent years. The goal of this study is to solve the sentiment\npolarity classification challenges in sentiment analysis. A broad technique for\ncategorizing sentiment opposition is presented, along with comprehensive\nprocess explanations. With the results of the analysis, both sentence-level\nclassification and review-level categorization are conducted. Finally, we\ndiscuss our plans for future sentiment analysis research.", "published": "2022-11-28 16:43:33", "link": "http://arxiv.org/abs/2211.15536v2", "categories": ["cs.CL", "cs.LG", "68T50"], "primary_category": "cs.CL"}
{"title": "GPT-Neo for commonsense reasoning -- a theoretical and practical lens", "abstract": "Recent work has demonstrated substantial gains in pre-training large-language\nmodels (LLMs) followed by supervised fine-tuning on the downstream task. In\nthis paper, we evaluate the performance of the GPT-neo model using $6$\ncommonsense reasoning benchmark tasks. We aim to examine the performance of\nsmaller models using the GPT-neo models against several larger model baselines\nsuch as GPT-$3$, Llama-$2$, MPT and Falcon. Upon fine-tuning with the\nappropriate set of hyperparameters, our model achieves competitive accuracy on\nseveral tasks. We also investigate and substantiate our results using\nattention-head visualization to better understand the model performance.\nFinally, we conduct various robustness tests using various methods to gauge the\nmodel performance under numerous settings.", "published": "2022-11-28 17:49:38", "link": "http://arxiv.org/abs/2211.15593v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Frustratingly Easy Label Projection for Cross-lingual Transfer", "abstract": "Translating training data into many languages has emerged as a practical\nsolution for improving cross-lingual transfer. For tasks that involve\nspan-level annotations, such as information extraction or question answering,\nan additional label projection step is required to map annotated spans onto the\ntranslated texts. Recently, a few efforts have utilized a simple\nmark-then-translate method to jointly perform translation and projection by\ninserting special markers around the labeled spans in the original sentence.\nHowever, as far as we are aware, no empirical analysis has been conducted on\nhow this approach compares to traditional annotation projection based on word\nalignment. In this paper, we present an extensive empirical study across 57\nlanguages and three tasks (QA, NER, and Event Extraction) to evaluate the\neffectiveness and limitations of both methods, filling an important gap in the\nliterature. Experimental results show that our optimized version of\nmark-then-translate, which we call EasyProject, is easily applied to many\nlanguages and works surprisingly well, outperforming the more complex word\nalignment-based methods. We analyze several key factors that affect the\nend-task performance, and show EasyProject works well because it can accurately\npreserve label span boundaries after translation. We will publicly release all\nour code and data.", "published": "2022-11-28 18:11:48", "link": "http://arxiv.org/abs/2211.15613v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Counting Datasets: A Survey of Multilingual Dataset Construction\n  and Necessary Resources", "abstract": "While the NLP community is generally aware of resource disparities among\nlanguages, we lack research that quantifies the extent and types of such\ndisparity. Prior surveys estimating the availability of resources based on the\nnumber of datasets can be misleading as dataset quality varies: many datasets\nare automatically induced or translated from English data. To provide a more\ncomprehensive picture of language resources, we examine the characteristics of\n156 publicly available NLP datasets. We manually annotate how they are created,\nincluding input text and label sources and tools used to build them, and what\nthey study, tasks they address and motivations for their creation. After\nquantifying the qualitative NLP resource gap across languages, we discuss how\nto improve data collection in low-resource languages. We survey\nlanguage-proficient NLP researchers and crowd workers per language, finding\nthat their estimated availability correlates with dataset availability. Through\ncrowdsourcing experiments, we identify strategies for collecting high-quality\nmultilingual data on the Mechanical Turk platform. We conclude by making macro\nand micro-level suggestions to the NLP community and individual researchers for\nfuture multilingual data development.", "published": "2022-11-28 18:54:33", "link": "http://arxiv.org/abs/2211.15649v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What learning algorithm is in-context learning? Investigations with\n  linear models", "abstract": "Neural sequence models, especially transformers, exhibit a remarkable\ncapacity for in-context learning. They can construct new predictors from\nsequences of labeled examples $(x, f(x))$ presented in the input without\nfurther parameter updates. We investigate the hypothesis that transformer-based\nin-context learners implement standard learning algorithms implicitly, by\nencoding smaller models in their activations, and updating these implicit\nmodels as new examples appear in the context. Using linear regression as a\nprototypical problem, we offer three sources of evidence for this hypothesis.\nFirst, we prove by construction that transformers can implement learning\nalgorithms for linear models based on gradient descent and closed-form ridge\nregression. Second, we show that trained in-context learners closely match the\npredictors computed by gradient descent, ridge regression, and exact\nleast-squares regression, transitioning between different predictors as\ntransformer depth and dataset noise vary, and converging to Bayesian estimators\nfor large widths and depths. Third, we present preliminary evidence that\nin-context learners share algorithmic features with these predictors: learners'\nlate layers non-linearly encode weight vectors and moment matrices. These\nresults suggest that in-context learning is understandable in algorithmic\nterms, and that (at least in the linear case) learners may rediscover standard\nestimation algorithms. Code and reference implementations are released at\nhttps://github.com/ekinakyurek/google-research/blob/master/incontext.", "published": "2022-11-28 18:59:51", "link": "http://arxiv.org/abs/2211.15661v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Handling and extracting key entities from customer conversations using\n  Speech recognition and Named Entity recognition", "abstract": "In this modern era of technology with e-commerce developing at a rapid pace,\nit is very important to understand customer requirements and details from a\nbusiness conversation. It is very crucial for customer retention and\nsatisfaction. Extracting key insights from these conversations is very\nimportant when it comes to developing their product or solving their issue.\nUnderstanding customer feedback, responses, and important details of the\nproduct are essential and it would be done using Named entity recognition\n(NER). For extracting the entities we would be converting the conversations to\ntext using the optimal speech-to-text model. The model would be a two-stage\nnetwork in which the conversation is converted to text. Then, suitable entities\nare extracted using robust techniques using a NER BERT transformer model. This\nwill aid in the enrichment of customer experience when there is an issue which\nis faced by them. If a customer faces a problem he will call and register his\ncomplaint. The model will then extract the key features from this conversation\nwhich will be necessary to look into the problem. These features would include\ndetails like the order number, and the exact problem. All these would be\nextracted directly from the conversation and this would reduce the effort of\ngoing through the conversation again.", "published": "2022-11-28 06:41:29", "link": "http://arxiv.org/abs/2211.17107v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Joint Multimodal Entity-Relation Extraction Based on Edge-enhanced Graph\n  Alignment Network and Word-pair Relation Tagging", "abstract": "Multimodal named entity recognition (MNER) and multimodal relation extraction\n(MRE) are two fundamental subtasks in the multimodal knowledge graph\nconstruction task. However, the existing methods usually handle two tasks\nindependently, which ignores the bidirectional interaction between them. This\npaper is the first to propose jointly performing MNER and MRE as a joint\nmultimodal entity-relation extraction task (JMERE). Besides, the current MNER\nand MRE models only consider aligning the visual objects with textual entities\nin visual and textual graphs but ignore the entity-entity relationships and\nobject-object relationships. To address the above challenges, we propose an\nedge-enhanced graph alignment network and a word-pair relation tagging (EEGA)\nfor JMERE task. Specifically, we first design a word-pair relation tagging to\nexploit the bidirectional interaction between MNER and MRE and avoid the error\npropagation. Then, we propose an edge-enhanced graph alignment network to\nenhance the JMERE task by aligning nodes and edges in the cross-graph. Compared\nwith previous methods, the proposed method can leverage the edge information to\nauxiliary alignment between objects and entities and find the correlations\nbetween entity-entity relationships and object-object relationships.\nExperiments are conducted to show the effectiveness of our model.", "published": "2022-11-28 03:23:54", "link": "http://arxiv.org/abs/2211.15028v2", "categories": ["cs.CL", "cs.AI", "cs.MM"], "primary_category": "cs.CL"}
{"title": "DiffusionBERT: Improving Generative Masked Language Models with\n  Diffusion Models", "abstract": "We present DiffusionBERT, a new generative masked language model based on\ndiscrete diffusion models. Diffusion models and many pre-trained language\nmodels have a shared training objective, i.e., denoising, making it possible to\ncombine the two powerful models and enjoy the best of both worlds. On the one\nhand, diffusion models offer a promising training strategy that helps improve\nthe generation quality. On the other hand, pre-trained denoising language\nmodels (e.g., BERT) can be used as a good initialization that accelerates\nconvergence. We explore training BERT to learn the reverse process of a\ndiscrete diffusion process with an absorbing state and elucidate several\ndesigns to improve it. First, we propose a new noise schedule for the forward\ndiffusion process that controls the degree of noise added at each step based on\nthe information of each token. Second, we investigate several designs of\nincorporating the time step into BERT. Experiments on unconditional text\ngeneration demonstrate that DiffusionBERT achieves significant improvement over\nexisting diffusion models for text (e.g., D3PM and Diffusion-LM) and previous\ngenerative masked language models in terms of perplexity and BLEU score.", "published": "2022-11-28 03:25:49", "link": "http://arxiv.org/abs/2211.15029v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Refined Semantic Enhancement towards Frequency Diffusion for Video\n  Captioning", "abstract": "Video captioning aims to generate natural language sentences that describe\nthe given video accurately. Existing methods obtain favorable generation by\nexploring richer visual representations in encode phase or improving the\ndecoding ability. However, the long-tailed problem hinders these attempts at\nlow-frequency tokens, which rarely occur but carry critical semantics, playing\na vital role in the detailed generation. In this paper, we introduce a novel\nRefined Semantic enhancement method towards Frequency Diffusion (RSFD), a\ncaptioning model that constantly perceives the linguistic representation of the\ninfrequent tokens. Concretely, a Frequency-Aware Diffusion (FAD) module is\nproposed to comprehend the semantics of low-frequency tokens to break through\ngeneration limitations. In this way, the caption is refined by promoting the\nabsorption of tokens with insufficient occurrence. Based on FAD, we design a\nDivergent Semantic Supervisor (DSS) module to compensate for the information\nloss of high-frequency tokens brought by the diffusion process, where the\nsemantics of low-frequency tokens is further emphasized to alleviate the\nlong-tailed problem. Extensive experiments indicate that RSFD outperforms the\nstate-of-the-art methods on two benchmark datasets, i.e., MSR-VTT and MSVD,\ndemonstrate that the enhancement of low-frequency tokens semantics can obtain a\ncompetitive generation effect. Code is available at\nhttps://github.com/lzp870/RSFD.", "published": "2022-11-28 05:45:17", "link": "http://arxiv.org/abs/2211.15076v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Generalized Category Discovery with Decoupled Prototypical Network", "abstract": "Generalized Category Discovery (GCD) aims to recognize both known and novel\ncategories from a set of unlabeled data, based on another dataset labeled with\nonly known categories. Without considering differences between known and novel\ncategories, current methods learn about them in a coupled manner, which can\nhurt model's generalization and discriminative ability. Furthermore, the\ncoupled training approach prevents these models transferring category-specific\nknowledge explicitly from labeled data to unlabeled data, which can lose\nhigh-level semantic information and impair model performance. To mitigate above\nlimitations, we present a novel model called Decoupled Prototypical Network\n(DPN). By formulating a bipartite matching problem for category prototypes, DPN\ncan not only decouple known and novel categories to achieve different training\ntargets effectively, but also align known categories in labeled and unlabeled\ndata to transfer category-specific knowledge explicitly and capture high-level\nsemantics. Furthermore, DPN can learn more discriminative features for both\nknown and novel categories through our proposed Semantic-aware Prototypical\nLearning (SPL). Besides capturing meaningful semantic information, SPL can also\nalleviate the noise of hard pseudo labels through semantic-weighted soft\nassignment. Extensive experiments show that DPN outperforms state-of-the-art\nmodels by a large margin on all evaluation metrics across multiple benchmark\ndatasets. Code and data are available at https://github.com/Lackel/DPN.", "published": "2022-11-28 08:05:45", "link": "http://arxiv.org/abs/2211.15115v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Long-tail Cross Modal Hashing", "abstract": "Existing Cross Modal Hashing (CMH) methods are mainly designed for balanced\ndata, while imbalanced data with long-tail distribution is more general in\nreal-world. Several long-tail hashing methods have been proposed but they can\nnot adapt for multi-modal data, due to the complex interplay between labels and\nindividuality and commonality information of multi-modal data. Furthermore, CMH\nmethods mostly mine the commonality of multi-modal data to learn hash codes,\nwhich may override tail labels encoded by the individuality of respective\nmodalities. In this paper, we propose LtCMH (Long-tail CMH) to handle\nimbalanced multi-modal data. LtCMH firstly adopts auto-encoders to mine the\nindividuality and commonality of different modalities by minimizing the\ndependency between the individuality of respective modalities and by enhancing\nthe commonality of these modalities. Then it dynamically combines the\nindividuality and commonality with direct features extracted from respective\nmodalities to create meta features that enrich the representation of tail\nlabels, and binaries meta features to generate hash codes. LtCMH significantly\noutperforms state-of-the-art baselines on long-tail datasets and holds a better\n(or comparable) performance on datasets with balanced labels.", "published": "2022-11-28 09:18:08", "link": "http://arxiv.org/abs/2211.15162v1", "categories": ["cs.IR", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Distance Metric Learning Loss Functions in Few-Shot Scenarios of\n  Supervised Language Models Fine-Tuning", "abstract": "This paper presents an analysis regarding an influence of the Distance Metric\nLearning (DML) loss functions on the supervised fine-tuning of the language\nmodels for classification tasks. We experimented with known datasets from\nSentEval Transfer Tasks.\n  Our experiments show that applying the DML loss function can increase\nperformance on downstream classification tasks of RoBERTa-large models in\nfew-shot scenarios. Models fine-tuned with the use of SoftTriple loss can\nachieve better results than models with a standard categorical cross-entropy\nloss function by about 2.89 percentage points from 0.04 to 13.48 percentage\npoints depending on the training dataset. Additionally, we accomplished a\ncomprehensive analysis with explainability techniques to assess the models'\nreliability and explain their results.", "published": "2022-11-28 10:05:58", "link": "http://arxiv.org/abs/2211.15195v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Revisiting Distance Metric Learning for Few-Shot Natural Language\n  Classification", "abstract": "Distance Metric Learning (DML) has attracted much attention in image\nprocessing in recent years. This paper analyzes its impact on supervised\nfine-tuning language models for Natural Language Processing (NLP)\nclassification tasks under few-shot learning settings. We investigated several\nDML loss functions in training RoBERTa language models on known SentEval\nTransfer Tasks datasets. We also analyzed the possibility of using proxy-based\nDML losses during model inference.\n  Our systematic experiments have shown that under few-shot learning settings,\nparticularly proxy-based DML losses can positively affect the fine-tuning and\ninference of a supervised language model. Models tuned with a combination of\nCCE (categorical cross-entropy loss) and ProxyAnchor Loss have, on average, the\nbest performance and outperform models with only CCE by about 3.27 percentage\npoints -- up to 10.38 percentage points depending on the training dataset.", "published": "2022-11-28 10:19:31", "link": "http://arxiv.org/abs/2211.15202v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mutual Exclusivity Training and Primitive Augmentation to Induce\n  Compositionality", "abstract": "Recent datasets expose the lack of the systematic generalization ability in\nstandard sequence-to-sequence models. In this work, we analyze this behavior of\nseq2seq models and identify two contributing factors: a lack of mutual\nexclusivity bias (i.e., a source sequence already mapped to a target sequence\nis less likely to be mapped to other target sequences), and the tendency to\nmemorize whole examples rather than separating structures from contents. We\npropose two techniques to address these two issues respectively: Mutual\nExclusivity Training that prevents the model from producing seen generations\nwhen facing novel, unseen examples via an unlikelihood-based loss; and\nprim2primX data augmentation that automatically diversifies the arguments of\nevery syntactic function to prevent memorizing and provide a compositional\ninductive bias without exposing test-set data. Combining these two techniques,\nwe show substantial empirical improvements using standard sequence-to-sequence\nmodels (LSTMs and Transformers) on two widely-used compositionality datasets:\nSCAN and COGS. Finally, we provide analysis characterizing the improvements as\nwell as the remaining challenges, and provide detailed ablations of our method.\nOur code is available at https://github.com/owenzx/met-primaug", "published": "2022-11-28 17:36:41", "link": "http://arxiv.org/abs/2211.15578v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Effectiveness of Parameter-Efficient Fine-Tuning", "abstract": "Fine-tuning pre-trained models has been ubiquitously proven to be effective\nin a wide range of NLP tasks. However, fine-tuning the whole model is parameter\ninefficient as it always yields an entirely new model for each task. Currently,\nmany research works propose to only fine-tune a small portion of the parameters\nwhile keeping most of the parameters shared across different tasks. These\nmethods achieve surprisingly good performance and are shown to be more stable\nthan their corresponding fully fine-tuned counterparts. However, such kind of\nmethods is still not well understood. Some natural questions arise: How does\nthe parameter sparsity lead to promising performance? Why is the model more\nstable than the fully fine-tuned models? How to choose the tunable parameters?\nIn this paper, we first categorize the existing methods into random approaches,\nrule-based approaches, and projection-based approaches based on how they choose\nwhich parameters to tune. Then, we show that all of the methods are actually\nsparse fine-tuned models and conduct a novel theoretical analysis of them. We\nindicate that the sparsity is actually imposing a regularization on the\noriginal model by controlling the upper bound of the stability. Such stability\nleads to better generalization capability which has been empirically observed\nin a lot of recent research works. Despite the effectiveness of sparsity\ngrounded by our theory, it still remains an open problem of how to choose the\ntunable parameters. To better choose the tunable parameters, we propose a novel\nSecond-order Approximation Method (SAM) which approximates the original problem\nwith an analytically solvable optimization function. The tunable parameters are\ndetermined by directly optimizing the approximation function. The experimental\nresults show that our proposed SAM model outperforms many strong baseline\nmodels and it also verifies our theoretical analysis.", "published": "2022-11-28 17:41:48", "link": "http://arxiv.org/abs/2211.15583v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SuS-X: Training-Free Name-Only Transfer of Vision-Language Models", "abstract": "Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet\neffective way to train large-scale vision-language models. CLIP demonstrates\nimpressive zero-shot classification and retrieval on diverse downstream tasks.\nHowever, to leverage its full potential, fine-tuning still appears to be\nnecessary. Fine-tuning the entire CLIP model can be resource-intensive and\nunstable. Moreover, recent methods that aim to circumvent this need for\nfine-tuning still require access to images from the target distribution. In\nthis paper, we pursue a different approach and explore the regime of\ntraining-free \"name-only transfer\" in which the only knowledge we possess about\nthe downstream task comprises the names of downstream target categories. We\npropose a novel method, SuS-X, consisting of two key building blocks -- SuS and\nTIP-X, that requires neither intensive fine-tuning nor costly labelled data.\nSuS-X achieves state-of-the-art zero-shot classification results on 19\nbenchmark datasets. We further show the utility of TIP-X in the training-free\nfew-shot setting, where we again achieve state-of-the-art results over strong\ntraining-free baselines. Code is available at\nhttps://github.com/vishaal27/SuS-X.", "published": "2022-11-28 16:48:41", "link": "http://arxiv.org/abs/2211.16198v4", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "On the Security Vulnerabilities of Text-to-SQL Models", "abstract": "Although it has been demonstrated that Natural Language Processing (NLP)\nalgorithms are vulnerable to deliberate attacks, the question of whether such\nweaknesses can lead to software security threats is under-explored. To bridge\nthis gap, we conducted vulnerability tests on Text-to-SQL systems that are\ncommonly used to create natural language interfaces to databases. We showed\nthat the Text-to-SQL modules within six commercial applications can be\nmanipulated to produce malicious code, potentially leading to data breaches and\nDenial of Service attacks. This is the first demonstration that NLP models can\nbe exploited as attack vectors in the wild. In addition, experiments using four\nopen-source language models verified that straightforward backdoor attacks on\nText-to-SQL systems achieve a 100% success rate without affecting their\nperformance. The aim of this work is to draw the community's attention to\npotential software security issues associated with NLP algorithms and encourage\nexploration of methods to mitigate against them.", "published": "2022-11-28 14:38:45", "link": "http://arxiv.org/abs/2211.15363v4", "categories": ["cs.CL", "cs.CR", "cs.DB", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Inter-KD: Intermediate Knowledge Distillation for CTC-Based Automatic\n  Speech Recognition", "abstract": "Recently, the advance in deep learning has brought a considerable improvement\nin the end-to-end speech recognition field, simplifying the traditional\npipeline while producing promising results. Among the end-to-end models, the\nconnectionist temporal classification (CTC)-based model has attracted research\ninterest due to its non-autoregressive nature. However, such CTC models require\na heavy computational cost to achieve outstanding performance. To mitigate the\ncomputational burden, we propose a simple yet effective knowledge distillation\n(KD) for the CTC framework, namely Inter-KD, that additionally transfers the\nteacher's knowledge to the intermediate CTC layers of the student network. From\nthe experimental results on the LibriSpeech, we verify that the Inter-KD shows\nbetter achievements compared to the conventional KD methods. Without using any\nlanguage model (LM) and data augmentation, Inter-KD improves the word error\nrate (WER) performance from 8.85 % to 6.30 % on the test-clean.", "published": "2022-11-28 05:23:59", "link": "http://arxiv.org/abs/2211.15075v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Automatic Transcription of Drum Strokes in Carnatic Music", "abstract": "The mridangam is a double-headed percussion instrument that plays a key role\nin Carnatic music concerts. This paper presents a novel automatic transcription\nalgorithm to classify the strokes played on the mridangam. Onset detection is\nfirst performed to segment the audio signal into individual strokes, and\nfeature vectors consisting of the DFT magnitude spectrum of the segmented\nsignal are generated. A multi-layer feedforward neural network is trained using\nthe feature vectors as inputs and the manual transcriptions as targets. Since\nthe mridangam is a tonal instrument tuned to a given tonic, tonic invariance is\nan important feature of the classifier. Tonic invariance is achieved by\naugmenting the dataset with pitch-shifted copies of the audio. This algorithm\nconsistently yields over 83% accuracy on a held-out test dataset.", "published": "2022-11-28 09:50:18", "link": "http://arxiv.org/abs/2211.15185v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Learnable Front Ends Based on Temporal Modulation for Music Tagging", "abstract": "While end-to-end systems are becoming popular in auditory signal processing\nincluding automatic music tagging, models using raw audio as input needs a\nlarge amount of data and computational resources without domain knowledge.\nInspired by the fact that temporal modulation is regarded as an essential\ncomponent in auditory perception, we introduce the Temporal Modulation Neural\nNetwork (TMNN) that combines Mel-like data-driven front ends and temporal\nmodulation filters with a simple ResNet back end. The structure includes a set\nof temporal modulation filters to capture long-term patterns in all frequency\nchannels. Experimental results show that the proposed front ends surpass\nstate-of-the-art (SOTA) methods on the MagnaTagATune dataset in automatic music\ntagging, and they are also helpful for keyword spotting on speech commands.\nMoreover, the model performance for each tag suggests that genre or instrument\ntags with complex rhythm and mood tags can especially be improved with temporal\nmodulation.", "published": "2022-11-28 12:17:14", "link": "http://arxiv.org/abs/2211.15254v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MuSFA: Improving Music Structural Function Analysis with Partially\n  Labeled Data", "abstract": "Music structure analysis (MSA) systems aim to segment a song recording into\nnon-overlapping sections with useful labels. Previous MSA systems typically\npredict abstract labels in a post-processing step and require the full context\nof the song. By contrast, we recently proposed a supervised framework, called\n\"Music Structural Function Analysis\" (MuSFA), that models and predicts\nmeaningful labels like 'verse' and 'chorus' directly from audio, without\nrequiring the full context of a song. However, the performance of this system\ndepends on the amount and quality of training data. In this paper, we propose\nto repurpose a public dataset, HookTheory Lead Sheet Dataset (HLSD), to improve\nthe performance. HLSD contains over 18K excerpts of music sections originally\ncollected for studying automatic melody harmonization. We treat each excerpt as\na partially labeled song and provide a label mapping, so that HLSD can be used\ntogether with other public datasets, such as SALAMI, RWC, and Isophonics. In\ncross-dataset evaluations, we find that including HLSD in training can improve\nstate-of-the-art boundary detection and section labeling scores by ~3% and ~1%\nrespectively.", "published": "2022-11-28 21:48:45", "link": "http://arxiv.org/abs/2211.15787v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automated Detection of Dolphin Whistles with Convolutional Networks and\n  Transfer Learning", "abstract": "Effective conservation of maritime environments and wildlife management of\nendangered species require the implementation of efficient, accurate and\nscalable solutions for environmental monitoring. Ecoacoustics offers the\nadvantages of non-invasive, long-duration sampling of environmental sounds and\nhas the potential to become the reference tool for biodiversity surveying.\nHowever, the analysis and interpretation of acoustic data is a time-consuming\nprocess that often requires a great amount of human supervision. This issue\nmight be tackled by exploiting modern techniques for automatic audio signal\nanalysis, which have recently achieved impressive performance thanks to the\nadvances in deep learning research. In this paper we show that convolutional\nneural networks can indeed significantly outperform traditional automatic\nmethods in a challenging detection task: identification of dolphin whistles\nfrom underwater audio recordings. The proposed system can detect signals even\nin the presence of ambient noise, at the same time consistently reducing the\nlikelihood of producing false positives and false negatives. Our results\nfurther support the adoption of artificial intelligence technology to improve\nthe automatic monitoring of marine ecosystems.", "published": "2022-11-28 15:06:46", "link": "http://arxiv.org/abs/2211.15406v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Probabilistic Modelling of Signal Mixtures with Differentiable\n  Dictionaries", "abstract": "We introduce a novel way to incorporate prior information into (semi-)\nsupervised non-negative matrix factorization, which we call differentiable\ndictionary search. It enables general, highly flexible and principled modelling\nof mixtures where non-linear sources are linearly mixed. We study its behavior\non an audio decomposition task, and conduct an extensive, highly controlled\nstudy of its modelling capabilities.", "published": "2022-11-28 15:27:53", "link": "http://arxiv.org/abs/2211.15439v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Differentiable Dictionary Search: Integrating Linear Mixing with Deep\n  Non-Linear Modelling for Audio Source Separation", "abstract": "This paper describes several improvements to a new method for signal\ndecomposition that we recently formulated under the name of Differentiable\nDictionary Search (DDS). The fundamental idea of DDS is to exploit a class of\npowerful deep invertible density estimators called normalizing flows, to model\nthe dictionary in a linear decomposition method such as NMF, effectively\ncreating a bijection between the space of dictionary elements and the\nassociated probability space, allowing a differentiable search through the\ndictionary space, guided by the estimated densities. As the initial formulation\nwas a proof of concept with some practical limitations, we will present several\nsteps towards making it scalable, hoping to improve both the computational\ncomplexity of the method and its signal decomposition capabilities. As a\ntestbed for experimental evaluation, we choose the task of frame-level piano\ntranscription, where the signal is to be decomposed into sources whose activity\nis attributed to individual piano notes. To highlight the impact of improved\nnon-linear modelling of sources, we compare variants of our method to a linear\novercomplete NMF baseline. Experimental results will show that even in the\nabsence of additional constraints, our models produce increasingly sparse and\nprecise decompositions, according to two pertinent evaluation measures.", "published": "2022-11-28 16:37:02", "link": "http://arxiv.org/abs/2211.15524v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
