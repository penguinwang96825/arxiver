{"title": "Predicting Prosodic Prominence from Text with Pre-trained Contextualized\n  Word Representations", "abstract": "In this paper we introduce a new natural language processing dataset and\nbenchmark for predicting prosodic prominence from written text. To our\nknowledge this will be the largest publicly available dataset with prosodic\nlabels. We describe the dataset construction and the resulting benchmark\ndataset in detail and train a number of different models ranging from\nfeature-based classifiers to neural network systems for the prediction of\ndiscretized prosodic prominence. We show that pre-trained contextualized word\nrepresentations from BERT outperform the other models even with less than 10%\nof the training data. Finally we discuss the dataset in light of the results\nand point to future research and plans for further improving both the dataset\nand methods of predicting prosodic prominence from text. The dataset and the\ncode for the models are publicly available.", "published": "2019-08-06 17:19:46", "link": "http://arxiv.org/abs/1908.02262v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Summarization in the Biomedical Domain", "abstract": "This chapter gives an overview of recent advances in the field of biomedical\ntext summarization. Different types of challenges are introduced, and methods\nare discussed concerning the type of challenge that they address. Biomedical\nliterature summarization is explored as a leading trend in the field, and some\nfuture lines of work are pointed out. Underlying methods of recent\nsummarization systems are briefly explained and the most significant evaluation\nresults are mentioned. The primary purpose of this chapter is to review the\nmost significant research efforts made in the current decade toward new methods\nof biomedical text summarization. As the main parts of this chapter, current\ntrends are discussed and new challenges are introduced.", "published": "2019-08-06 09:57:08", "link": "http://arxiv.org/abs/1908.02285v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clustering of Deep Contextualized Representations for Summarization of\n  Biomedical Texts", "abstract": "In recent years, summarizers that incorporate domain knowledge into the\nprocess of text summarization have outperformed generic methods, especially for\nsummarization of biomedical texts. However, construction and maintenance of\ndomain knowledge bases are resource-intense tasks requiring significant manual\nannotation. In this paper, we demonstrate that contextualized representations\nextracted from the pre-trained deep language model BERT, can be effectively\nused to measure the similarity between sentences and to quantify the\ninformative content. The results show that our BERT-based summarizer can\nimprove the performance of biomedical summarization. Although the summarizer\ndoes not use any sources of domain knowledge, it can capture the context of\nsentences more accurately than the comparison methods. The source code and data\nare available at https://github.com/BioTextSumm/BERT-based-Summ.", "published": "2019-08-06 10:18:20", "link": "http://arxiv.org/abs/1908.02286v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DpgMedia2019: A Dutch News Dataset for Partisanship Detection", "abstract": "We present a new Dutch news dataset with labeled partisanship. The dataset\ncontains more than 100K articles that are labeled on the publisher level and\n776 articles that were crowdsourced using an internal survey platform and\nlabeled on the article level. In this paper, we document our original\nmotivation, the collection and annotation process, limitations, and\napplications.", "published": "2019-08-06 18:50:45", "link": "http://arxiv.org/abs/1908.02322v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialog State Tracking: A Neural Reading Comprehension Approach", "abstract": "Dialog state tracking is used to estimate the current belief state of a\ndialog given all the preceding conversation. Machine reading comprehension, on\nthe other hand, focuses on building systems that read passages of text and\nanswer questions that require some understanding of passages. We formulate\ndialog state tracking as a reading comprehension task to answer the question\n$what\\ is\\ the\\ state\\ of\\ the\\ current\\ dialog?$ after reading conversational\ncontext. In contrast to traditional state tracking methods where the dialog\nstate is often predicted as a distribution over a closed set of all the\npossible slot values within an ontology, our method uses a simple\nattention-based neural network to point to the slot values within the\nconversation. Experiments on MultiWOZ-2.0 cross-domain dialog dataset show that\nour simple system can obtain similar accuracies compared to the previous more\ncomplex methods. By exploiting recent advances in contextual word embeddings,\nadding a model that explicitly tracks whether a slot value should be carried\nover to the next turn, and combining our method with a traditional joint state\ntracking method that relies on closed set vocabulary, we can obtain a\njoint-goal accuracy of $47.33\\%$ on the standard test split, exceeding current\nstate-of-the-art by $11.75\\%$**.", "published": "2019-08-06 04:01:42", "link": "http://arxiv.org/abs/1908.01946v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Balanced Dropout", "abstract": "Dropout is known as an effective way to reduce overfitting via preventing\nco-adaptations of units. In this paper, we theoretically prove that the\nco-adaptation problem still exists after using dropout due to the correlations\namong the inputs. Based on the proof, we further propose Self-Balanced Dropout,\na novel dropout method which uses a trainable variable to balance the influence\nof the input correlation on parameter update. We evaluate Self-Balanced Dropout\non a range of tasks with both simple and complex models. The experimental\nresults show that the mechanism can effectively solve the co-adaption problem\nto some extent and significantly improve the performance on all tasks.", "published": "2019-08-06 05:57:22", "link": "http://arxiv.org/abs/1908.01968v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Word Embedding for Response-To-Text Assessment of Evidence", "abstract": "Manually grading the Response to Text Assessment (RTA) is labor intensive.\nTherefore, an automatic method is being developed for scoring analytical\nwriting when the RTA is administered in large numbers of classrooms. Our\nlong-term goal is to also use this scoring method to provide formative feedback\nto students and teachers about students' writing quality. As a first step\ntowards this goal, interpretable features for automatically scoring the\nevidence rubric of the RTA have been developed. In this paper, we present a\nsimple but promising method for improving evidence scoring by employing the\nword embedding model. We evaluate our method on corpora of responses written by\nupper elementary students.", "published": "2019-08-06 05:58:06", "link": "http://arxiv.org/abs/1908.01969v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "eRevise: Using Natural Language Processing to Provide Formative Feedback\n  on Text Evidence Usage in Student Writing", "abstract": "Writing a good essay typically involves students revising an initial paper\ndraft after receiving feedback. We present eRevise, a web-based writing and\nrevising environment that uses natural language processing features generated\nfor rubric-based essay scoring to trigger formative feedback messages regarding\nstudents' use of evidence in response-to-text writing. By helping students\nunderstand the criteria for using text evidence during writing, eRevise\nempowers students to better revise their paper drafts. In a pilot deployment of\neRevise in 7 classrooms spanning grades 5 and 6, the quality of text evidence\nusage in writing improved after students received formative feedback then\nengaged in paper revision.", "published": "2019-08-06 07:24:14", "link": "http://arxiv.org/abs/1908.01992v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Co-Attention Based Neural Network for Source-Dependent Essay Scoring", "abstract": "This paper presents an investigation of using a co-attention based neural\nnetwork for source-dependent essay scoring. We use a co-attention mechanism to\nhelp the model learn the importance of each part of the essay more accurately.\nAlso, this paper shows that the co-attention based neural network model\nprovides reliable score prediction of source-dependent responses. We evaluate\nour model on two source-dependent response corpora. Results show that our model\noutperforms the baseline on both corpora. We also show that the attention of\nthe model is similar to the expert opinions with examples.", "published": "2019-08-06 07:28:43", "link": "http://arxiv.org/abs/1908.01993v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for\n  Vision-and-Language Tasks", "abstract": "We present ViLBERT (short for Vision-and-Language BERT), a model for learning\ntask-agnostic joint representations of image content and natural language. We\nextend the popular BERT architecture to a multi-modal two-stream model,\npro-cessing both visual and textual inputs in separate streams that interact\nthrough co-attentional transformer layers. We pretrain our model through two\nproxy tasks on the large, automatically collected Conceptual Captions dataset\nand then transfer it to multiple established vision-and-language tasks --\nvisual question answering, visual commonsense reasoning, referring expressions,\nand caption-based image retrieval -- by making only minor additions to the base\narchitecture. We observe significant improvements across tasks compared to\nexisting task-specific models -- achieving state-of-the-art on all four tasks.\nOur work represents a shift away from learning groundings between vision and\nlanguage only as part of task training and towards treating visual grounding as\na pretrainable and transferable capability.", "published": "2019-08-06 17:33:52", "link": "http://arxiv.org/abs/1908.02265v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Aligning Linguistic Words and Visual Semantic Units for Image Captioning", "abstract": "Image captioning attempts to generate a sentence composed of several\nlinguistic words, which are used to describe objects, attributes, and\ninteractions in an image, denoted as visual semantic units in this paper. Based\non this view, we propose to explicitly model the object interactions in\nsemantics and geometry based on Graph Convolutional Networks (GCNs), and fully\nexploit the alignment between linguistic words and visual semantic units for\nimage captioning. Particularly, we construct a semantic graph and a geometry\ngraph, where each node corresponds to a visual semantic unit, i.e., an object,\nan attribute, or a semantic (geometrical) interaction between two objects.\nAccordingly, the semantic (geometrical) context-aware embeddings for each unit\nare obtained through the corresponding GCN learning processers. At each time\nstep, a context gated attention module takes as inputs the embeddings of the\nvisual semantic units and hierarchically align the current word with these\nunits by first deciding which type of visual semantic unit (object, attribute,\nor interaction) the current word is about, and then finding the most correlated\nvisual semantic units under this type. Extensive experiments are conducted on\nthe challenging MS-COCO image captioning dataset, and superior results are\nreported when comparing to state-of-the-art approaches.", "published": "2019-08-06 13:19:24", "link": "http://arxiv.org/abs/1908.02127v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM", "I.5.4; I.4.9; I.4.10; I.2.7"], "primary_category": "cs.CV"}
{"title": "Triplet Based Embedding Distance and Similarity Learning for\n  Text-independent Speaker Verification", "abstract": "Speaker embeddings become growing popular in the text-independent speaker\nverification task. In this paper, we propose two improvements during the\ntraining stage. The improvements are both based on triplet cause the training\nstage and the evaluation stage of the baseline x-vector system focus on\ndifferent aims. Firstly, we introduce triplet loss for optimizing the Euclidean\ndistances between embeddings while minimizing the multi-class cross entropy\nloss. Secondly, we design an embedding similarity measurement network for\ncontrolling the similarity between the two selected embeddings. We further\njointly train the two new methods with the original network and achieve\nstate-of-the-art. The multi-task training synergies are shown with a 9%\nreduction equal error rate (EER) and detected cost function (DCF) on the 2016\nNIST Speaker Recognition Evaluation (SRE) Test Set.", "published": "2019-08-06 04:23:27", "link": "http://arxiv.org/abs/1908.02283v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Two-stage Training for Chinese Dialect Recognition", "abstract": "In this paper, we present a two-stage language identification (LID) system\nbased on a shallow ResNet14 followed by a simple 2-layer recurrent neural\nnetwork (RNN) architecture, which was used for Xunfei (iFlyTek) Chinese Dialect\nRecognition Challenge and won the first place among 110 teams. The system\ntrains an acoustic model (AM) firstly with connectionist temporal\nclassification (CTC) to recognize the given phonetic sequence annotation and\nthen train another RNN to classify dialect category by utilizing the\nintermediate features as inputs from the AM. Compared with a three-stage system\nwe further explore, our results show that the two-stage system can achieve high\naccuracy for Chinese dialects recognition under both short utterance and long\nutterance conditions with less training time.", "published": "2019-08-06 04:28:56", "link": "http://arxiv.org/abs/1908.02284v2", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Flexibly-Structured Model for Task-Oriented Dialogues", "abstract": "This paper proposes a novel end-to-end architecture for task-oriented\ndialogue systems. It is based on a simple and practical yet very effective\nsequence-to-sequence approach, where language understanding and state tracking\ntasks are modeled jointly with a structured copy-augmented sequential decoder\nand a multi-label decoder for each slot. The policy engine and language\ngeneration tasks are modeled jointly following that. The copy-augmented\nsequential decoder deals with new or unknown values in the conversation, while\nthe multi-label decoder combined with the sequential decoder ensures the\nexplicit assignment of values to slots. On the generation part, slot binary\nclassifiers are used to improve performance. This architecture is scalable to\nreal-world scenarios and is shown through an empirical evaluation to achieve\nstate-of-the-art performance on both the Cambridge Restaurant dataset and the\nStanford in-car assistant dataset\\footnote{The code is available at\n\\url{https://github.com/uber-research/FSDM}}", "published": "2019-08-06 23:56:25", "link": "http://arxiv.org/abs/1908.02402v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adversarially Trained End-to-end Korean Singing Voice Synthesis System", "abstract": "In this paper, we propose an end-to-end Korean singing voice synthesis system\nfrom lyrics and a symbolic melody using the following three novel approaches:\n1) phonetic enhancement masking, 2) local conditioning of text and pitch to the\nsuper-resolution network, and 3) conditional adversarial training. The proposed\nsystem consists of two main modules; a mel-synthesis network that generates a\nmel-spectrogram from the given input information, and a super-resolution\nnetwork that upsamples the generated mel-spectrogram into a linear-spectrogram.\nIn the mel-synthesis network, phonetic enhancement masking is applied to\ngenerate implicit formant masks solely from the input text, which enables a\nmore accurate phonetic control of singing voice. In addition, we show that two\nother proposed methods -- local conditioning of text and pitch, and conditional\nadversarial training -- are crucial for a realistic generation of the human\nsinging voice in the super-resolution process. Finally, both quantitative and\nqualitative evaluations are conducted, confirming the validity of all proposed\nmethods.", "published": "2019-08-06 01:17:44", "link": "http://arxiv.org/abs/1908.01919v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Acceleration of rank-constrained spatial covariance matrix estimation\n  for blind speech extraction", "abstract": "In this paper, we propose new accelerated update rules for rank-constrained\nspatial covariance model estimation, which efficiently extracts a directional\ntarget source in diffuse background noise.The naive updat e rule requires heavy\ncomputation such as matrix inversion or matrix multiplication. We resolve this\nproblem by expanding matrix inversion to reduce computational complexity; in\nthe parameter update step, we need neither matrix inversion nor multiplication.\nIn an experiment, we show that the proposed accelerated update rule achieves 87\ntimes faster calculation than the naive one.", "published": "2019-08-06 05:36:01", "link": "http://arxiv.org/abs/1908.01964v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Maximum likelihood convolutional beamformer for simultaneous denoising\n  and dereverberation", "abstract": "This article describes a probabilistic formulation of a Weighted Power\nminimization Distortionless response convolutional beamformer (WPD). The WPD\nunifies a weighted prediction error based dereverberation method (WPE) and a\nminimum power distortionless response beamformer (MPDR) into a single\nconvolutional beamformer, and achieves simultaneous dereverberation and\ndenoising in an optimal way. However, the optimization criterion is obtained\nsimply by combining existing criteria without any clear theoretical\njustification. This article presents a generative model and a probabilistic\nformulation of a WPD, and derives an optimization algorithm based on a maximum\nlikelihood estimation. We also describe a method for estimating the steering\nvector of the desired signal by utilizing WPE within the WPD framework to\nprovide an effective and efficient beamformer for denoising and\ndereverberation.", "published": "2019-08-06 10:19:55", "link": "http://arxiv.org/abs/1908.02710v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Practical Speech Recognition with HTK", "abstract": "The practical aspects of developing an Automatic Speech Recognition System\n(ASR) with HTK are reviewed. Steps are explained concerning hardware, software,\nlibraries, applications and computer programs used. The common procedure to\nrapidly apply speech recognition system is summarized. The procedure is\nillustrated, to implement a speech based electrical switch in home automation\nfor the Indonesian language. The main key of the procedure is to match the\nenvironment for training and testing using the training data recorded from the\ntesting program, HVite. Often the silence detector of HTK is wrongly triggered\nby noises because the microphone is too sensitive. This problem is mitigated by\nsimply scaling down the volume. In this sub-word phone-based speech\nrecognition, noise is included in the training database and labelled\nparticularly. Illustration of the procedure is applied to a home automation\napplication. Electrical switches are controlled by Indonesian speech\nrecognizer. The results show 100% command completion rate.", "published": "2019-08-06 13:12:57", "link": "http://arxiv.org/abs/1908.02119v1", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An End-to-End Text-independent Speaker Verification Framework with a\n  Keyword Adversarial Network", "abstract": "This paper presents an end-to-end text-independent speaker verification\nframework by jointly considering the speaker embedding (SE) network and\nautomatic speech recognition (ASR) network. The SE network learns to output an\nembedding vector which distinguishes the speaker characteristics of the input\nutterance, while the ASR network learns to recognize the phonetic context of\nthe input. In training our speaker verification framework, we consider both the\ntriplet loss minimization and adversarial gradient of the ASR network to obtain\nmore discriminative and text-independent speaker embedding vectors. With the\ntriplet loss, the distances between the embedding vectors of the same speaker\nare minimized while those of different speakers are maximized. Also, with the\nadversarial gradient of the ASR network, the text-dependency of the speaker\nembedding vector can be reduced. In the experiments, we evaluated our speaker\nverification framework using the LibriSpeech and CHiME 2013 dataset, and the\nevaluation results show that our speaker verification framework shows lower\nequal error rate and better text-independency compared to the other approaches.", "published": "2019-08-06 11:05:20", "link": "http://arxiv.org/abs/1908.02612v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
