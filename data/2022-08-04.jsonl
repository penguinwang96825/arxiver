{"title": "InitialGAN: A Language GAN with Completely Random Initialization", "abstract": "Text generative models trained via Maximum Likelihood Estimation (MLE) suffer\nfrom the notorious exposure bias problem, and Generative Adversarial Networks\n(GANs) are shown to have potential to tackle this problem. Existing language\nGANs adopt estimators like REINFORCE or continuous relaxations to model word\nprobabilities. The inherent limitations of such estimators lead current models\nto rely on pre-training techniques (MLE pre-training or pre-trained\nembeddings). Representation modeling methods which are free from those\nlimitations, however, are seldomly explored because of their poor performance\nin previous attempts. Our analyses reveal that invalid sampling methods and\nunhealthy gradients are the main contributors to such unsatisfactory\nperformance. In this work, we present two techniques to tackle these problems:\ndropout sampling and fully normalized LSTM. Based on these two techniques, we\npropose InitialGAN whose parameters are randomly initialized in full. Besides,\nwe introduce a new evaluation metric, Least Coverage Rate, to better evaluate\nthe quality of generated samples. The experimental results demonstrate that\nInitialGAN outperforms both MLE and other compared models. To the best of our\nknowledge, it is the first time a language GAN can outperform MLE without using\nany pre-training techniques.", "published": "2022-08-04 08:56:04", "link": "http://arxiv.org/abs/2208.02531v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt Tuning for Generative Multimodal Pretrained Models", "abstract": "Prompt tuning has become a new paradigm for model tuning and it has\ndemonstrated success in natural language pretraining and even vision\npretraining. In this work, we explore the transfer of prompt tuning to\nmultimodal pretraining, with a focus on generative multimodal pretrained\nmodels, instead of contrastive ones. Specifically, we implement prompt tuning\non the unified sequence-to-sequence pretrained model adaptive to both\nunderstanding and generation tasks. Experimental results demonstrate that the\nlight-weight prompt tuning can achieve comparable performance with finetuning\nand surpass other light-weight tuning methods. Besides, in comparison with\nfinetuned models, the prompt-tuned models demonstrate improved robustness\nagainst adversarial attacks. We further figure out that experimental factors,\nincluding the prompt length, prompt depth, and reparameteratization, have great\nimpacts on the model performance, and thus we empirically provide a\nrecommendation for the setups of prompt tuning. Despite the observed\nadvantages, we still find some limitations in prompt tuning, and we\ncorrespondingly point out the directions for future studies. Codes are\navailable at \\url{https://github.com/OFA-Sys/OFA}", "published": "2022-08-04 08:56:38", "link": "http://arxiv.org/abs/2208.02532v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Vocabulary Transfer for Biomedical Texts: Add Tokens if You Can Not Add\n  Data", "abstract": "Working within specific NLP subdomains presents significant challenges,\nprimarily due to a persistent deficit of data. Stringent privacy concerns and\nlimited data accessibility often drive this shortage. Additionally, the medical\ndomain demands high accuracy, where even marginal improvements in model\nperformance can have profound impacts. In this study, we investigate the\npotential of vocabulary transfer to enhance model performance in biomedical NLP\ntasks. Specifically, we focus on vocabulary extension, a technique that\ninvolves expanding the target vocabulary to incorporate domain-specific\nbiomedical terms. Our findings demonstrate that vocabulary extension, leads to\nmeasurable improvements in both downstream model performance and inference\ntime.", "published": "2022-08-04 09:53:22", "link": "http://arxiv.org/abs/2208.02554v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "N-best Response-based Analysis of Contradiction-awareness in Neural\n  Response Generation Models", "abstract": "Avoiding the generation of responses that contradict the preceding context is\na significant challenge in dialogue response generation. One feasible method is\npost-processing, such as filtering out contradicting responses from a resulting\nn-best response list. In this scenario, the quality of the n-best list\nconsiderably affects the occurrence of contradictions because the final\nresponse is chosen from this n-best list. This study quantitatively analyzes\nthe contextual contradiction-awareness of neural response generation models\nusing the consistency of the n-best lists. Particularly, we used polar\nquestions as stimulus inputs for concise and quantitative analyses. Our tests\nillustrate the contradiction-awareness of recent neural response generation\nmodels and methodologies, followed by a discussion of their properties and\nlimitations.", "published": "2022-08-04 10:58:32", "link": "http://arxiv.org/abs/2208.02578v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ATP: A holistic attention integrated approach to enhance ABSA", "abstract": "Aspect based sentiment analysis (ABSA) deals with the identification of the\nsentiment polarity of a review sentence towards a given aspect. Deep Learning\nsequential models like RNN, LSTM, and GRU are current state-of-the-art methods\nfor inferring the sentiment polarity. These methods work well to capture the\ncontextual relationship between the words of a review sentence. However, these\nmethods are insignificant in capturing long-term dependencies. Attention\nmechanism plays a significant role by focusing only on the most crucial part of\nthe sentence. In the case of ABSA, aspect position plays a vital role. Words\nnear to aspect contribute more while determining the sentiment towards the\naspect. Therefore, we propose a method that captures the position based\ninformation using dependency parsing tree and helps attention mechanism. Using\nthis type of position information over a simple word-distance-based position\nenhances the deep learning model's performance. We performed the experiments on\nSemEval'14 dataset to demonstrate the effect of dependency parsing\nrelation-based attention for ABSA.", "published": "2022-08-04 13:32:56", "link": "http://arxiv.org/abs/2208.02653v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fusing Sentence Embeddings Into LSTM-based Autoregressive Language\n  Models", "abstract": "Although masked language models are highly performant and widely adopted by\nNLP practitioners, they can not be easily used for autoregressive language\nmodelling (next word prediction and sequence probability estimation). We\npresent an LSTM-based autoregressive language model which uses prefix\nembeddings (from a pretrained masked language model) via fusion (e.g.\nconcatenation) to obtain a richer context representation for language\nmodelling. We find that fusion helps reliably in lowering the perplexity (16.74\n$\\rightarrow$ 15.80), which is even preserved after a transfer to a dataset\nfrom a different domain than the training data. We also evaluate the\nbest-performing fusion model by correlating its next word surprisal estimates\nwith human reading times. Contradicting our expectation, and despite the\nimprovement in perplexity overall, the correlation remains the same as for the\nbaseline model. Lastly, while we focus on language models pre-trained on text\nas the sources for the fusion, our approach can be possibly extended to fuse\nany information represented as a fixed-size vector into an auto-regressive\nlanguage model. These include e.g. sentence external information retrieved for\na knowledge base or representations of multi-modal encoders.", "published": "2022-08-04 02:13:03", "link": "http://arxiv.org/abs/2208.02402v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Act-Aware Slot-Value Predicting in Multi-Domain Dialogue State Tracking", "abstract": "As an essential component in task-oriented dialogue systems, dialogue state\ntracking (DST) aims to track human-machine interactions and generate state\nrepresentations for managing the dialogue. Representations of dialogue states\nare dependent on the domain ontology and the user's goals. In several\ntask-oriented dialogues with a limited scope of objectives, dialogue states can\nbe represented as a set of slot-value pairs. As the capabilities of dialogue\nsystems expand to support increasing naturalness in communication,\nincorporating dialogue act processing into dialogue model design becomes\nessential. The lack of such consideration limits the scalability of dialogue\nstate tracking models for dialogues having specific objectives and ontology. To\naddress this issue, we formulate and incorporate dialogue acts, and leverage\nrecent advances in machine reading comprehension to predict both categorical\nand non-categorical types of slots for multi-domain dialogue state tracking.\nExperimental results show that our models can improve the overall accuracy of\ndialogue state tracking on the MultiWOZ 2.1 dataset, and demonstrate that\nincorporating dialogue acts can guide dialogue state design for future\ntask-oriented dialogue systems.", "published": "2022-08-04 05:18:30", "link": "http://arxiv.org/abs/2208.02462v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Integrating Knowledge Graph embedding and pretrained Language Models in\n  Hypercomplex Spaces", "abstract": "Knowledge Graphs, such as Wikidata, comprise structural and textual knowledge\nin order to represent knowledge. For each of the two modalities dedicated\napproaches for graph embedding and language models learn patterns that allow\nfor predicting novel structural knowledge. Few approaches have integrated\nlearning and inference with both modalities and these existing ones could only\npartially exploit the interaction of structural and textual knowledge. In our\napproach, we build on existing strong representations of single modalities and\nwe use hypercomplex algebra to represent both, (i), single-modality embedding\nas well as, (ii), the interaction between different modalities and their\ncomplementary means of knowledge representation. More specifically, we suggest\nDihedron and Quaternion representations of 4D hypercomplex numbers to integrate\nfour modalities namely structural knowledge graph embedding, word-level\nrepresentations (e.g.\\ Word2vec, Fasttext), sentence-level representations\n(Sentence transformer), and document-level representations (sentence\ntransformer, Doc2vec). Our unified vector representation scores the\nplausibility of labelled edges via Hamilton and Dihedron products, thus\nmodeling pairwise interactions between different modalities. Extensive\nexperimental evaluation on standard benchmark datasets shows the superiority of\nour two new models using abundant textual information besides sparse structural\nknowledge to enhance performance in link prediction tasks.", "published": "2022-08-04 16:18:16", "link": "http://arxiv.org/abs/2208.02743v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adversarial Attacks on Image Generation With Made-Up Words", "abstract": "Text-guided image generation models can be prompted to generate images using\nnonce words adversarially designed to robustly evoke specific visual concepts.\nTwo approaches for such generation are introduced: macaronic prompting, which\ninvolves designing cryptic hybrid words by concatenating subword units from\ndifferent languages; and evocative prompting, which involves designing nonce\nwords whose broad morphological features are similar enough to that of existing\nwords to trigger robust visual associations. The two methods can also be\ncombined to generate images associated with more specific visual concepts. The\nimplications of these techniques for the circumvention of existing approaches\nto content moderation, and particularly the generation of offensive or harmful\nimages, are discussed.", "published": "2022-08-04 15:10:23", "link": "http://arxiv.org/abs/2208.04135v1", "categories": ["cs.CV", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "\"Yeah, it does have a...Windows `98 Vibe'': Usability Study of Security\n  Features in Programmable Logic Controllers", "abstract": "Programmable Logic Controllers (PLCs) drive industrial processes critical to\nsociety, e.g., water treatment and distribution, electricity and fuel networks.\nSearch engines (e.g., Shodan) have highlighted that Programmable Logic\nControllers (PLCs) are often left exposed to the Internet, one of the main\nreasons being the misconfigurations of security settings. This leads to the\nquestion -- why do these misconfigurations occur and, specifically, whether\nusability of security controls plays a part? To date, the usability of\nconfiguring PLC security mechanisms has not been studied. We present the first\ninvestigation through a task-based study and subsequent semi-structured\ninterviews (N=19). We explore the usability of PLC connection configurations\nand two key security mechanisms (i.e., access levels and user administration).\nWe find that the use of unfamiliar labels, layouts and misleading terminology\nexacerbates an already complex process of configuring security mechanisms. Our\nresults uncover various (mis-) perceptions about the security controls and how\ndesign constraints, e.g., safety and lack of regular updates (due to long term\nnature of such systems), provide significant challenges to realization of\nmodern HCI and usability principles. Based on these findings, we provide design\nrecommendations to bring usable security in industrial settings at par with its\nIT counterpart.", "published": "2022-08-04 07:20:00", "link": "http://arxiv.org/abs/2208.02500v1", "categories": ["cs.CR", "cs.CL", "cs.HC", "cs.SY", "eess.SY"], "primary_category": "cs.CR"}
{"title": "LATTE: LAnguage Trajectory TransformEr", "abstract": "Natural language is one of the most intuitive ways to express human intent.\nHowever, translating instructions and commands towards robotic motion\ngeneration and deployment in the real world is far from being an easy task. The\nchallenge of combining a robot's inherent low-level geometric and kinodynamic\nconstraints with a human's high-level semantic instructions traditionally is\nsolved using task-specific solutions with little generalizability between\nhardware platforms, often with the use of static sets of target actions and\ncommands. This work instead proposes a flexible language-based framework that\nallows a user to modify generic robotic trajectories. Our method leverages\npre-trained language models (BERT and CLIP) to encode the user's intent and\ntarget objects directly from a free-form text input and scene images, fuses\ngeometrical features generated by a transformer encoder network, and finally\noutputs trajectories using a transformer decoder, without the need of priors\nrelated to the task or robot information. We significantly extend our own\nprevious work presented in Bucker et al. by expanding the trajectory\nparametrization space to 3D and velocity as opposed to just XY movements. In\naddition, we now train the model to use actual images of the objects in the\nscene for context (as opposed to textual descriptions), and we evaluate the\nsystem in a diverse set of scenarios beyond manipulation, such as aerial and\nlegged robots. Our simulated and real-life experiments demonstrate that our\ntransformer model can successfully follow human intent, modifying the shape and\nspeed of trajectories within multiple environments. Codebase available at:\nhttps://github.com/arthurfenderbucker/LaTTe-Language-Trajectory-TransformEr.git", "published": "2022-08-04 22:43:21", "link": "http://arxiv.org/abs/2208.02918v3", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Domestic Activity Clustering from Audio via Depthwise Separable\n  Convolutional Autoencoder Network", "abstract": "Automatic estimation of domestic activities from audio can be used to solve\nmany problems, such as reducing the labor cost for nursing the elderly people.\nThis study focuses on solving the problem of domestic activity clustering from\naudio. The target of domestic activity clustering is to cluster audio clips\nwhich belong to the same category of domestic activity into one cluster in an\nunsupervised way. In this paper, we propose a method of domestic activity\nclustering using a depthwise separable convolutional autoencoder network. In\nthe proposed method, initial embeddings are learned by the depthwise separable\nconvolutional autoencoder, and a clustering-oriented loss is designed to\njointly optimize embedding refinement and cluster assignment. Different methods\nare evaluated on a public dataset (a derivative of the SINS dataset) used in\nthe challenge on Detection and Classification of Acoustic Scenes and Events\n(DCASE) in 2018. Our method obtains the normalized mutual information (NMI)\nscore of 54.46%, and the clustering accuracy (CA) score of 63.64%, and\noutperforms state-of-the-art methods in terms of NMI and CA. In addition, both\ncomputational complexity and memory requirement of our method is lower than\nthat of previous deep-model-based methods. Codes:\nhttps://github.com/vinceasvp/domestic-activity-clustering-from-audio", "published": "2022-08-04 02:20:30", "link": "http://arxiv.org/abs/2208.02406v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Attention and DCT based Global Context Modeling for Text-independent\n  Speaker Recognition", "abstract": "Learning an effective speaker representation is crucial for achieving\nreliable performance in speaker verification tasks. Speech signals are\nhigh-dimensional, long, and variable-length sequences containing diverse\ninformation at each time-frequency (TF) location. The standard convolutional\nlayer that operates on neighboring local regions often fails to capture the\ncomplex TF global information. Our motivation is to alleviate these challenges\nby increasing the modeling capacity, emphasizing significant information, and\nsuppressing possible redundancies. We aim to design a more robust and efficient\nspeaker recognition system by incorporating the benefits of attention\nmechanisms and Discrete Cosine Transform (DCT) based signal processing\ntechniques, to effectively represent the global information in speech signals.\nTo achieve this, we propose a general global time-frequency context modeling\nblock for speaker modeling. First, an attention-based context model is\nintroduced to capture the long-range and non-local relationship across\ndifferent time-frequency locations. Second, a 2D-DCT based context model is\nproposed to improve model efficiency and examine the benefits of signal\nmodeling. A multi-DCT attention mechanism is presented to improve modeling\npower with alternate DCT base forms. Finally, the global context information is\nused to recalibrate salient time-frequency locations by computing the\nsimilarity between the global context and local features. This effectively\nimproves the speaker verification performance compared to the standard ResNet\nmodel and Squeeze & Excitation block by a large margin. Our experimental\nresults show that the proposed global context modeling method can efficiently\nimprove the learned speaker representations by achieving channel-wise and\ntime-frequency feature recalibration.", "published": "2022-08-04 17:17:51", "link": "http://arxiv.org/abs/2208.02778v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Tokyo Kion-On: Query-Based Generative Sonification of Atmospheric Data", "abstract": "Amid growing environmental concerns, interactive displays of data constitute\nan important tool for exploring and understanding the impact of climate change\non the planet's ecosystemic integrity. This paper presents Tokyo kion-on, a\nquery-based sonification model of Tokyo's air temperature from 1876 to 2021.\nThe system uses a recurrent neural network architecture known as LSTM with\nattention trained on a small dataset of Japanese melodies and conditioned upon\nsaid atmospheric data. After describing the model's implementation, a brief\ncomparative illustration of the musical results is presented, along with a\ndiscussion on how the exposed hyper-parameters can promote active and\nnon-linear exploration of the data.", "published": "2022-08-04 06:56:06", "link": "http://arxiv.org/abs/2208.02494v1", "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Impact Makes a Sound and Sound Makes an Impact: Sound Guides\n  Representations and Explorations", "abstract": "Sound is one of the most informative and abundant modalities in the real\nworld while being robust to sense without contacts by small and cheap sensors\nthat can be placed on mobile devices. Although deep learning is capable of\nextracting information from multiple sensory inputs, there has been little use\nof sound for the control and learning of robotic actions. For unsupervised\nreinforcement learning, an agent is expected to actively collect experiences\nand jointly learn representations and policies in a self-supervised way. We\nbuild realistic robotic manipulation scenarios with physics-based sound\nsimulation and propose the Intrinsic Sound Curiosity Module (ISCM). The ISCM\nprovides feedback to a reinforcement learner to learn robust representations\nand to reward a more efficient exploration behavior. We perform experiments\nwith sound enabled during pre-training and disabled during adaptation, and show\nthat representations learned by ISCM outperform the ones by vision-only\nbaselines and pre-trained policies can accelerate the learning process when\napplied to downstream tasks.", "published": "2022-08-04 14:21:06", "link": "http://arxiv.org/abs/2208.02680v1", "categories": ["cs.RO", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.RO"}
{"title": "Keyword Spotting System and Evaluation of Pruning and Quantization\n  Methods on Low-power Edge Microcontrollers", "abstract": "Keyword spotting (KWS) is beneficial for voice-based user interactions with\nlow-power devices at the edge. The edge devices are usually always-on, so edge\ncomputing brings bandwidth savings and privacy protection. The devices\ntypically have limited memory spaces, computational performances, power and\ncosts, for example, Cortex-M based microcontrollers. The challenge is to meet\nthe high computation and low-latency requirements of deep learning on these\ndevices. This paper firstly shows our small-footprint KWS system running on\nSTM32F7 microcontroller with Cortex-M7 core @216MHz and 512KB static RAM. Our\nselected convolutional neural network (CNN) architecture has simplified number\nof operations for KWS to meet the constraint of edge devices. Our baseline\nsystem generates classification results for each 37ms including real-time audio\nfeature extraction part. This paper further evaluates the actual performance\nfor different pruning and quantization methods on microcontroller, including\ndifferent granularity of sparsity, skipping zero weights, weight-prioritized\nloop order, and SIMD instruction. The result shows that for microcontrollers,\nthere are considerable challenges for accelerate unstructured pruned models,\nand the structured pruning is more friendly than unstructured pruning. The\nresult also verified that the performance improvement for quantization and SIMD\ninstruction.", "published": "2022-08-04 16:49:45", "link": "http://arxiv.org/abs/2208.02765v1", "categories": ["cs.SD", "cs.AI", "cs.AR", "eess.AS"], "primary_category": "cs.SD"}
