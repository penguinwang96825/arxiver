{"title": "Reduce Meaningless Words for Joint Chinese Word Segmentation and\n  Part-of-speech Tagging", "abstract": "Conventional statistics-based methods for joint Chinese word segmentation and\npart-of-speech tagging (S&T) have generalization ability to recognize new words\nthat do not appear in the training data. An undesirable side effect is that a\nnumber of meaningless words will be incorrectly created. We propose an\neffective and efficient framework for S&T that introduces features to\nsignificantly reduce meaningless words generation. A general lexicon, Wikepedia\nand a large-scale raw corpus of 200 billion characters are used to generate\nword-based features for the wordhood. The word-lattice based framework consists\nof a character-based model and a word-based model in order to employ our\nword-based features. Experiments on Penn Chinese treebank 5 show that this\nmethod has a 62.9% reduction of meaningless word generation in comparison with\nthe baseline. As a result, the F1 measure for segmentation is increased to\n0.984.", "published": "2013-05-25 13:20:31", "link": "http://arxiv.org/abs/1305.5918v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
