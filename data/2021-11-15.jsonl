{"title": "Automatic Analysis of Linguistic Features in Journal Articles of\n  Different Academic Impacts with Feature Engineering Techniques", "abstract": "English research articles (RAs) are an essential genre in academia, so the\nattempts to employ NLP to assist the development of academic writing ability\nhave received considerable attention in the last two decades. However, there\nhas been no study employing feature engineering techniques to investigate the\nlinguistic features of RAs of different academic impacts (i.e., the papers of\nhigh/moderate citation times published in the journals of high/moderate impact\nfactors). This study attempts to extract micro-level linguistic features in\nhigh- and moderate-impact journal RAs, using feature engineering methods. We\nextracted 25 highly relevant features from the Corpus of English Journal\nArticles through feature selection methods. All papers in the corpus deal with\nCOVID-19 medical empirical studies. The selected features were then validated\nof the classification performance in terms of consistency and accuracy through\nsupervised machine learning methods. Results showed that 24 linguistic features\nsuch as the overlapping of content words between adjacent sentences, the use of\nthird-person pronouns, auxiliary verbs, tense, emotional words provide\nconsistent and accurate predictions for journal articles with different\nacademic impacts. Lastly, the random forest model is shown to be the best model\nto fit the relationship between these 24 features and journal articles with\nhigh and moderate impacts. These findings can be used to inform academic\nwriting courses and lay the foundation for developing automatic evaluation\nsystems for L2 graduate students.", "published": "2021-11-15 03:56:50", "link": "http://arxiv.org/abs/2111.07525v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysis of Data Augmentation Methods for Low-Resource Maltese ASR", "abstract": "Recent years have seen an increased interest in the computational speech\nprocessing of Maltese, but resources remain sparse. In this paper, we consider\ndata augmentation techniques for improving speech recognition for low-resource\nlanguages, focusing on Maltese as a test case. We consider three different\ntypes of data augmentation: unsupervised training, multilingual training and\nthe use of synthesized speech as training data. The goal is to determine which\nof these techniques, or combination of them, is the most effective to improve\nspeech recognition for languages where the starting point is a small corpus of\napproximately 7 hours of transcribed speech. Our results show that combining\nthe data augmentation techniques studied here lead us to an absolute WER\nimprovement of 15% without the use of a language model.", "published": "2021-11-15 14:28:21", "link": "http://arxiv.org/abs/2111.07793v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Choice of Knowledge Base in Automated Claim Checking", "abstract": "Automated claim checking is the task of determining the veracity of a claim\ngiven evidence found in a knowledge base of trustworthy facts. While previous\nwork has taken the knowledge base as given and optimized the claim-checking\npipeline, we take the opposite approach - taking the pipeline as given, we\nexplore the choice of knowledge base. Our first insight is that a\nclaim-checking pipeline can be transferred to a new domain of claims with\naccess to a knowledge base from the new domain. Second, we do not find a\n\"universally best\" knowledge base - higher domain overlap of a task dataset and\na knowledge base tends to produce better label accuracy. Third, combining\nmultiple knowledge bases does not tend to improve performance beyond using the\nclosest-domain knowledge base. Finally, we show that the claim-checking\npipeline's confidence score for selecting evidence can be used to assess\nwhether a knowledge base will perform well for a new set of claims, even in the\nabsence of ground-truth labels.", "published": "2021-11-15 14:30:03", "link": "http://arxiv.org/abs/2111.07795v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Metrics for Bias in Word Embeddings", "abstract": "Over the last years, word and sentence embeddings have established as text\npreprocessing for all kinds of NLP tasks and improved the performances\nsignificantly. Unfortunately, it has also been shown that these embeddings\ninherit various kinds of biases from the training data and thereby pass on\nbiases present in society to NLP solutions. Many papers attempted to quantify\nbias in word or sentence embeddings to evaluate debiasing methods or compare\ndifferent embedding models, usually with cosine-based metrics. However, lately\nsome works have raised doubts about these metrics showing that even though such\nmetrics report low biases, other tests still show biases. In fact, there is a\ngreat variety of bias metrics or tests proposed in the literature without any\nconsensus on the optimal solutions. Yet we lack works that evaluate bias\nmetrics on a theoretical level or elaborate the advantages and disadvantages of\ndifferent bias metrics. In this work, we will explore different cosine based\nbias metrics. We formalize a bias definition based on the ideas from previous\nworks and derive conditions for bias metrics. Furthermore, we thoroughly\ninvestigate the existing cosine-based metrics and their limitations to show why\nthese metrics can fail to report biases in some cases. Finally, we propose a\nnew metric, SAME, to address the shortcomings of existing metrics and\nmathematically prove that SAME behaves appropriately.", "published": "2021-11-15 16:07:15", "link": "http://arxiv.org/abs/2111.07864v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IIITT@Dravidian-CodeMix-FIRE2021: Transliterate or translate? Sentiment\n  analysis of code-mixed text in Dravidian languages", "abstract": "Sentiment analysis of social media posts and comments for various marketing\nand emotional purposes is gaining recognition. With the increasing presence of\ncode-mixed content in various native languages, there is a need for ardent\nresearch to produce promising results. This research paper bestows a tiny\ncontribution to this research in the form of sentiment analysis of code-mixed\nsocial media comments in the popular Dravidian languages Kannada, Tamil and\nMalayalam. It describes the work for the shared task conducted by\nDravidian-CodeMix at FIRE 2021 by employing pre-trained models like ULMFiT and\nmultilingual BERT fine-tuned on the code-mixed dataset, transliteration (TRAI)\nof the same, English translations (TRAA) of the TRAI data and the combination\nof all the three. The results are recorded in this research paper where the\nbest models stood 4th, 5th and 10th ranks in the Tamil, Kannada and Malayalam\ntasks respectively.", "published": "2021-11-15 16:57:59", "link": "http://arxiv.org/abs/2111.07906v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Question Answering-Based Signals into Abstractive\n  Summarization via Salient Span Selection", "abstract": "In this work, we propose a method for incorporating question-answering (QA)\nsignals into a summarization model. Our method identifies salient noun phrases\n(NPs) in the input document by automatically generating wh-questions that are\nanswered by the NPs and automatically determining whether those questions are\nanswered in the gold summaries. This QA-based signal is incorporated into a\ntwo-stage summarization model which first marks salient NPs in the input\ndocument using a classification model, then conditionally generates a summary.\nOur experiments demonstrate that the models trained using QA-based supervision\ngenerate higher-quality summaries than baseline methods of identifying salient\nspans on benchmark summarization datasets. Further, we show that the content of\nthe generated summaries can be controlled based on which NPs are marked in the\ninput document. Finally, we propose a method of augmenting the training data so\nthe gold summaries are more consistent with the marked input spans used during\ntraining and show how this results in models which learn to better exclude\nunmarked document content.", "published": "2021-11-15 17:36:41", "link": "http://arxiv.org/abs/2111.07935v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoLLIE: Continual Learning of Language Grounding from Language-Image\n  Embeddings", "abstract": "This paper presents CoLLIE: a simple, yet effective model for continual\nlearning of how language is grounded in vision. Given a pre-trained multimodal\nembedding model, where language and images are projected in the same semantic\nspace (in this case CLIP by OpenAI), CoLLIE learns a transformation function\nthat adjusts the language embeddings when needed to accommodate new language\nuse. This is done by predicting the difference vector that needs to be applied,\nas well as a scaling factor for this vector, so that the adjustment is only\napplied when needed. Unlike traditional few-shot learning, the model does not\njust learn new classes and labels, but can also generalize to similar language\nuse and leverage semantic compositionality. We verify the model's performance\non two different tasks of identifying the targets of referring expressions,\nwhere it has to learn new language use. The results show that the model can\nefficiently learn and generalize from only a few examples, with little\ninterference with the model's original zero-shot performance.", "published": "2021-11-15 18:54:58", "link": "http://arxiv.org/abs/2111.07993v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rationale production to support clinical decision-making", "abstract": "The development of neural networks for clinical artificial intelligence (AI)\nis reliant on interpretability, transparency, and performance. The need to\ndelve into the black-box neural network and derive interpretable explanations\nof model output is paramount. A task of high clinical importance is predicting\nthe likelihood of a patient being readmitted to hospital in the near future to\nenable efficient triage. With the increasing adoption of electronic health\nrecords (EHRs), there is great interest in applications of natural language\nprocessing (NLP) to clinical free-text contained within EHRs. In this work, we\napply InfoCal, the current state-of-the-art model that produces extractive\nrationales for its predictions, to the task of predicting hospital readmission\nusing hospital discharge notes. We compare extractive rationales produced by\nInfoCal to competitive transformer-based models pretrained on clinical text\ndata and for which the attention mechanism can be used for interpretation. We\nfind each presented model with selected interpretability or feature importance\nmethods yield varying results, with clinical language domain expertise and\npretraining critical to performance and subsequent interpretability.", "published": "2021-11-15 09:02:10", "link": "http://arxiv.org/abs/2111.07611v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Calculating Question Similarity is Enough: A New Method for KBQA Tasks", "abstract": "Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions with the help of an external knowledge base. The core idea is to find\nthe link between the internal knowledge behind questions and known triples of\nthe knowledge base. Traditional KBQA task pipelines contain several steps,\nincluding entity recognition, entity linking, answering selection, etc. In this\nkind of pipeline methods, errors in any procedure will inevitably propagate to\nthe final prediction. To address this challenge, this paper proposes a Corpus\nGeneration - Retrieve Method (CGRM) with Pre-training Language Model (PLM) for\nthe KBQA task. The major novelty lies in the design of the new method, wherein\nour approach, the knowledge enhanced T5 (kT5) model aims to generate natural\nlanguage QA pairs based on Knowledge Graph triples and directly solve the QA by\nretrieving the synthetic dataset. The new method can extract more information\nabout the entities from PLM to improve accuracy and simplify the processes. We\ntest our method on NLPCC-ICCPOL 2016 KBQA dataset, and the results show that\nour method improves the performance of KBQA and the out straight-forward method\nis competitive with the state-of-the-art.", "published": "2021-11-15 10:31:46", "link": "http://arxiv.org/abs/2111.07658v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis of Fashion Related Posts in Social Media", "abstract": "The role of social media in fashion industry has been blooming as the years\nhave continued on. In this work, we investigate sentiment analysis for fashion\nrelated posts in social media platforms. There are two main challenges of this\ntask. On the first place, information of different modalities must be jointly\nconsidered to make the final predictions. On the second place, some unique\nfashion related attributes should be taken into account. While most existing\nworks focus on traditional multimodal sentiment analysis, they always fail to\nexploit the fashion related attributes in this task. We propose a novel\nframework that jointly leverages the image vision, post text, as well as\nfashion attribute modality to determine the sentiment category. One\ncharacteristic of our model is that it extracts fashion attributes and\nintegrates them with the image vision information for effective representation.\nFurthermore, it exploits the mutual relationship between the fashion attributes\nand the post texts via a mutual attention mechanism. Since there is no existing\ndataset suitable for this task, we prepare a large-scale sentiment analysis\ndataset of over 12k fashion related social media posts. Extensive experiments\nare conducted to demonstrate the effectiveness of our model.", "published": "2021-11-15 14:58:09", "link": "http://arxiv.org/abs/2111.07815v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Annotators with Attitudes: How Annotator Beliefs And Identities Bias\n  Toxic Language Detection", "abstract": "The perceived toxicity of language can vary based on someone's identity and\nbeliefs, but this variation is often ignored when collecting toxic language\ndatasets, resulting in dataset and model biases. We seek to understand the who,\nwhy, and what behind biases in toxicity annotations. In two online studies with\ndemographically and politically diverse participants, we investigate the effect\nof annotator identities (who) and beliefs (why), drawing from social psychology\nresearch about hate speech, free speech, racist beliefs, political leaning, and\nmore. We disentangle what is annotated as toxic by considering posts with three\ncharacteristics: anti-Black language, African American English (AAE) dialect,\nand vulgarity. Our results show strong associations between annotator identity\nand beliefs and their ratings of toxicity. Notably, more conservative\nannotators and those who scored highly on our scale for racist beliefs were\nless likely to rate anti-Black language as toxic, but more likely to rate AAE\nas toxic. We additionally present a case study illustrating how a popular\ntoxicity detection system's ratings inherently reflect only specific beliefs\nand perspectives. Our findings call for contextualizing toxicity labels in\nsocial variables, which raises immense implications for toxic language\nannotation and detection.", "published": "2021-11-15 18:58:20", "link": "http://arxiv.org/abs/2111.07997v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Assessing gender bias in medical and scientific masked language models\n  with StereoSet", "abstract": "NLP systems use language models such as Masked Language Models (MLMs) that\nare pre-trained on large quantities of text such as Wikipedia create\nrepresentations of language. BERT is a powerful and flexible general-purpose\nMLM system developed using unlabeled text. Pre-training on large quantities of\ntext also has the potential to transparently embed the cultural and social\nbiases found in the source text into the MLM system. This study aims to compare\nbiases in general purpose and medical MLMs with the StereoSet bias assessment\ntool. The general purpose MLMs showed significant bias overall, with BERT\nscoring 57 and RoBERTa scoring 61. The category of gender bias is where the\nbest performances were found, with 63 for BERT and 73 for RoBERTa. Performances\nfor profession, race, and religion were similar to the overall bias scores for\nthe general-purpose MLMs.Medical MLMs showed more bias in all categories than\nthe general-purpose MLMs except for SciBERT, which showed a race bias score of\n55, which was superior to the race bias score of 53 for BERT. More gender\n(Medical 54-58 vs. General 63-73) and religious (46-54 vs. 58) biases were\nfound with medical MLMs. This evaluation of four medical MLMs for stereotyped\nassessments about race, gender, religion, and profession showed inferior\nperformance to general-purpose MLMs. These medically focused MLMs differ\nconsiderably in training source data, which is likely the root cause of the\ndifferences in ratings for stereotyped biases from the StereoSet tool.", "published": "2021-11-15 21:25:18", "link": "http://arxiv.org/abs/2111.08088v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Exploring Story Generation with Multi-task Objectives in Variational\n  Autoencoders", "abstract": "GPT-2 has been frequently adapted in story generation models as it provides\npowerful generative capability. However, it still fails to generate consistent\nstories and lacks diversity. Current story generation models leverage\nadditional information such as plots or commonsense into GPT-2 to guide the\ngeneration process. These approaches focus on improving generation quality of\nstories while our work look at both quality and diversity. We explore combining\nBERT and GPT-2 to build a variational autoencoder (VAE), and extend it by\nadding additional objectives to learn global features such as story topic and\ndiscourse relations. Our evaluations show our enhanced VAE can provide better\nquality and diversity trade off, generate less repetitive story content and\nlearn a more informative latent variable.", "published": "2021-11-15 23:07:19", "link": "http://arxiv.org/abs/2111.08133v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automated scholarly paper review: Concepts, technologies, and challenges", "abstract": "Peer review is a widely accepted mechanism for research evaluation, playing a\npivotal role in academic publishing. However, criticisms have long been leveled\nat this mechanism, mostly because of its poor efficiency and low\nreproducibility. Recent years have seen the application of artificial\nintelligence (AI) in assisting the peer review process. Nonetheless, with the\ninvolvement of humans, such limitations remain inevitable. In this paper, we\npropose the concept and pipeline of automated scholarly paper review (ASPR) and\nreview the relevant literature and technologies of achieving a full-scale\ncomputerized review process. On the basis of the review and discussion, we\nconclude that there is already corresponding research and preliminary\nimplementation at each stage of ASPR. We further look into the challenges in\nASPR with the existing technologies. The major difficulties lie in inadequate\ndata, imperfect document parsing and representation, defective\nhuman$\\unicode{x2013}$computer interaction, and flawed deep logical reasoning.\nMoreover, we point out the future directions and discuss the possible moral and\nethical issues of ASPR. In the foreseeable future, ASPR and peer review will\ncoexist in a reinforcing manner before ASPR is able to fully undertake the\nreviewing workload from humans.", "published": "2021-11-15 04:44:57", "link": "http://arxiv.org/abs/2111.07533v4", "categories": ["cs.AI", "cs.CL", "cs.DL"], "primary_category": "cs.AI"}
{"title": "Improving Prosody for Unseen Texts in Speech Synthesis by Utilizing\n  Linguistic Information and Noisy Data", "abstract": "Recent advancements in end-to-end speech synthesis have made it possible to\ngenerate highly natural speech. However, training these models typically\nrequires a large amount of high-fidelity speech data, and for unseen texts, the\nprosody of synthesized speech is relatively unnatural. To address these issues,\nwe propose to combine a fine-tuned BERT-based front-end with a pre-trained\nFastSpeech2-based acoustic model to improve prosody modeling. The pre-trained\nBERT is fine-tuned on the polyphone disambiguation task, the joint Chinese word\nsegmentation (CWS) and part-of-speech (POS) tagging task, and the prosody\nstructure prediction (PSP) task in a multi-task learning framework. FastSpeech\n2 is pre-trained on large-scale external data that are noisy but easier to\nobtain. Experimental results show that both the fine-tuned BERT model and the\npre-trained FastSpeech 2 can improve prosody, especially for those structurally\ncomplex sentences.", "published": "2021-11-15 05:58:29", "link": "http://arxiv.org/abs/2111.07549v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Adding more data does not always help: A study in medical conversation\n  summarization with PEGASUS", "abstract": "Medical conversation summarization is integral in capturing information\ngathered during interactions between patients and physicians. Summarized\nconversations are used to facilitate patient hand-offs between physicians, and\nas part of providing care in the future. Summaries, however, can be\ntime-consuming to produce and require domain expertise. Modern pre-trained NLP\nmodels such as PEGASUS have emerged as capable alternatives to human\nsummarization, reaching state-of-the-art performance on many summarization\nbenchmarks. However, many downstream tasks still require at least moderately\nsized datasets to achieve satisfactory performance. In this work we (1) explore\nthe effect of dataset size on transfer learning medical conversation\nsummarization using PEGASUS and (2) evaluate various iterative labeling\nstrategies in the low-data regime, following their success in the\nclassification setting. We find that model performance saturates with increase\nin dataset size and that the various active-learning strategies evaluated all\nshow equivalent performance consistent with simple dataset size increase. We\nalso find that naive iterative pseudo-labeling is on-par or slightly worse than\nno pseudo-labeling. Our work sheds light on the successes and challenges of\ntranslating low-data regime techniques in classification to medical\nconversation summarization and helps guides future work in this space. Relevant\ncode available at\n\\url{https://github.com/curai/curai-research/tree/main/medical-summarization-ML4H-2021}.", "published": "2021-11-15 07:27:35", "link": "http://arxiv.org/abs/2111.07564v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Say What? Collaborative Pop Lyric Generation Using Multitask Transfer\n  Learning", "abstract": "Lyric generation is a popular sub-field of natural language generation that\nhas seen growth in recent years. Pop lyrics are of unique interest due to the\ngenre's unique style and content, in addition to the high level of\ncollaboration that goes on behind the scenes in the professional pop\nsongwriting process. In this paper, we present a collaborative line-level lyric\ngeneration system that utilizes transfer-learning via the T5 transformer model,\nwhich, till date, has not been used to generate pop lyrics. By working and\ncommunicating directly with professional songwriters, we develop a model that\nis able to learn lyrical and stylistic tasks like rhyming, matching line beat\nrequirements, and ending lines with specific target words. Our approach\ncompares favorably to existing methods for multiple datasets and yields\npositive results from our online studies and interviews with industry\nsongwriters.", "published": "2021-11-15 08:13:26", "link": "http://arxiv.org/abs/2111.07592v1", "categories": ["cs.CL", "cs.HC", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Measuring Uncertainty in Translation Quality Evaluation (TQE)", "abstract": "From both human translators (HT) and machine translation (MT) researchers'\npoint of view, translation quality evaluation (TQE) is an essential task.\nTranslation service providers (TSPs) have to deliver large volumes of\ntranslations which meet customer specifications with harsh constraints of\nrequired quality level in tight time-frames and costs. MT researchers strive to\nmake their models better, which also requires reliable quality evaluation.\nWhile automatic machine translation evaluation (MTE) metrics and quality\nestimation (QE) tools are widely available and easy to access, existing\nautomated tools are not good enough, and human assessment from professional\ntranslators (HAP) are often chosen as the golden standard\n\\cite{han-etal-2021-TQA}. Human evaluations, however, are often accused of\nhaving low reliability and agreement. Is this caused by subjectivity or\nstatistics is at play? How to avoid the entire text to be checked and be more\nefficient with TQE from cost and efficiency perspectives, and what is the\noptimal sample size of the translated text, so as to reliably estimate the\ntranslation quality of the entire material? This work carries out such\nmotivated research to correctly estimate the confidence intervals\n\\cite{Brown_etal2001Interval} depending on the sample size of the translated\ntext, e.g. the amount of words or sentences, that needs to be processed on TQE\nworkflow step for confident and reliable evaluation of overall translation\nquality. The methodology we applied for this work is from Bernoulli Statistical\nDistribution Modelling (BSDM) and Monte Carlo Sampling Analysis (MCSA).", "published": "2021-11-15 12:09:08", "link": "http://arxiv.org/abs/2111.07699v1", "categories": ["cs.CL", "cs.NA", "math.NA", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Testing the Generalization of Neural Language Models for COVID-19\n  Misinformation Detection", "abstract": "A drastic rise in potentially life-threatening misinformation has been a\nby-product of the COVID-19 pandemic. Computational support to identify false\ninformation within the massive body of data on the topic is crucial to prevent\nharm. Researchers proposed many methods for flagging online misinformation\nrelated to COVID-19. However, these methods predominantly target specific\ncontent types (e.g., news) or platforms (e.g., Twitter). The methods'\ncapabilities to generalize were largely unclear so far. We evaluate fifteen\nTransformer-based models on five COVID-19 misinformation datasets that include\nsocial media posts, news articles, and scientific papers to fill this gap. We\nshow tokenizers and models tailored to COVID-19 data do not provide a\nsignificant advantage over general-purpose ones. Our study provides a realistic\nassessment of models for detecting COVID-19 misinformation. We expect that\nevaluating a broad spectrum of datasets and models will benefit future research\nin developing misinformation detection systems.", "published": "2021-11-15 15:01:55", "link": "http://arxiv.org/abs/2111.07819v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Triggerless Backdoor Attack for NLP Tasks with Clean Labels", "abstract": "Backdoor attacks pose a new threat to NLP models. A standard strategy to\nconstruct poisoned data in backdoor attacks is to insert triggers (e.g., rare\nwords) into selected sentences and alter the original label to a target label.\nThis strategy comes with a severe flaw of being easily detected from both the\ntrigger and the label perspectives: the trigger injected, which is usually a\nrare word, leads to an abnormal natural language expression, and thus can be\neasily detected by a defense model; the changed target label leads the example\nto be mistakenly labeled and thus can be easily detected by manual inspections.\nTo deal with this issue, in this paper, we propose a new strategy to perform\ntextual backdoor attacks which do not require an external trigger, and the\npoisoned samples are correctly labeled. The core idea of the proposed strategy\nis to construct clean-labeled examples, whose labels are correct but can lead\nto test label changes when fused with the training set. To generate poisoned\nclean-labeled examples, we propose a sentence generation model based on the\ngenetic algorithm to cater to the non-differentiable characteristic of text\ndata. Extensive experiments demonstrate that the proposed attacking strategy is\nnot only effective, but more importantly, hard to defend due to its triggerless\nand clean-labeled nature. Our work marks the first step towards developing\ntriggerless attacking strategies in NLP.", "published": "2021-11-15 18:36:25", "link": "http://arxiv.org/abs/2111.07970v2", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "LiT: Zero-Shot Transfer with Locked-image text Tuning", "abstract": "This paper presents contrastive-tuning, a simple method employing contrastive\ntraining to align image and text models while still taking advantage of their\npre-training. In our empirical study we find that locked pre-trained image\nmodels with unlocked text models work best. We call this instance of\ncontrastive-tuning \"Locked-image Tuning\" (LiT), which just teaches a text model\nto read out good representations from a pre-trained image model for new tasks.\nA LiT model gains the capability of zero-shot transfer to new vision tasks,\nsuch as image classification or retrieval. The proposed LiT is widely\napplicable; it works reliably with multiple pre-training methods (supervised\nand unsupervised) and across diverse architectures (ResNet, Vision Transformers\nand MLP-Mixer) using three different image-text datasets. With the\ntransformer-based pre-trained ViT-g/14 model, the LiT model achieves 85.2%\nzero-shot transfer accuracy on the ImageNet test set, and 82.5% on the\nchallenging out-of-distribution ObjectNet test set.", "published": "2021-11-15 18:53:48", "link": "http://arxiv.org/abs/2111.07991v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Joint Unsupervised and Supervised Training for Multilingual ASR", "abstract": "Self-supervised training has shown promising gains in pretraining models and\nfacilitating the downstream finetuning for speech recognition, like\nmultilingual ASR. Most existing methods adopt a 2-stage scheme where the\nself-supervised loss is optimized in the first pretraining stage, and the\nstandard supervised finetuning resumes in the second stage. In this paper, we\npropose an end-to-end (E2E) Joint Unsupervised and Supervised Training (JUST)\nmethod to combine the supervised RNN-T loss and the self-supervised contrastive\nand masked language modeling (MLM) losses. We validate its performance on the\npublic dataset Multilingual LibriSpeech (MLS), which includes 8 languages and\nis extremely imbalanced. On MLS, we explore (1) JUST trained from scratch, and\n(2) JUST finetuned from a pretrained checkpoint. Experiments show that JUST can\nconsistently outperform other existing state-of-the-art methods, and beat the\nmonolingual baseline by a significant margin, demonstrating JUST's capability\nof handling low-resource languages in multilingual ASR. Our average WER of all\nlanguages outperforms average monolingual baseline by 33.3%, and the\nstate-of-the-art 2-stage XLSR by 32%. On low-resource languages like Polish,\nour WER is less than half of the monolingual baseline and even beats the\nsupervised transfer learning method which uses external supervision.", "published": "2021-11-15 23:11:24", "link": "http://arxiv.org/abs/2111.08137v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Oil and Gas Pipeline Monitoring during COVID-19 Pandemic via Unmanned\n  Aerial Vehicle", "abstract": "The vast network of oil and gas transmission pipelines requires periodic\nmonitoring for maintenance and hazard inspection to avoid equipment failure and\npotential accidents. The severe COVID-19 pandemic situation forced the\ncompanies to shrink the size of their teams. One risk which is faced on-site is\nrepresented by the uncontrolled release of flammable oil and gas. Among many\ninspection methods, the unmanned aerial vehicle system contains flexibility and\nstability. Unmanned aerial vehicles can transfer data in real-time, while they\nare doing their monitoring tasks. The current article focuses on unmanned\naerial vehicles equipped with optical sensing and artificial intelligence,\nespecially image recognition with deep learning techniques for pipeline\nsurveillance. Unmanned aerial vehicles can be used for regular patrolling\nduties to identify and capture images and videos of the area of interest.\nPlaces that are hard to reach will be accessed faster, cheaper and with less\nrisk. The current paper is based on the idea of capturing video and images of\ndrone-based inspections, which can discover several potential hazardous\nproblems before they become dangerous. Damage can emerge as a weakening of the\ncladding on the external pipe insulation. There can also be the case when the\nthickness of piping through external corrosion can occur. The paper describes a\nsurvey completed by experts from the oil and gas industry done for finding the\nfunctional and non-functional requirements of the proposed system.", "published": "2021-11-15 16:44:16", "link": "http://arxiv.org/abs/2111.09155v1", "categories": ["cs.CV", "cs.CL", "cs.CY"], "primary_category": "cs.CV"}
{"title": "Speech Prediction using an Adaptive Recurrent Neural Network with\n  Application to Packet Loss Concealment", "abstract": "This paper proposes a novel approach for speech signal prediction based on a\nrecurrent neural network (RNN). Unlike existing RNN-based predictors, which\noperate on parametric features and are trained offline on a large collection of\nsuch features, the proposed predictor operates directly on speech samples and\nis trained online on the recent past of the speech signal. Optionally, the\nnetwork can be pre-trained offline to speed-up convergence at start-up. The\nproposed predictor is a single end-to-end network that captures all sorts of\ndependencies between samples, and therefore has the potential to outperform\nclassical linear/non-linear and short-term/long-term speech predictor\nstructures. We apply it to the packet loss concealment (PLC) problem and show\nthat it outperforms the standard ITU G.711 Appendix I PLC technique.", "published": "2021-11-15 22:33:48", "link": "http://arxiv.org/abs/2111.08116v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Time-Frequency Attention for Monaural Speech Enhancement", "abstract": "Most studies on speech enhancement generally don't consider the energy\ndistribution of speech in time-frequency (T-F) representation, which is\nimportant for accurate prediction of mask or spectra. In this paper, we present\na simple yet effective T-F attention (TFA) module, where a 2-D attention map is\nproduced to provide differentiated weights to the spectral components of T-F\nrepresentation. To validate the effectiveness of our proposed TFA module, we\nuse the residual temporal convolution network (ResTCN) as the backbone network\nand conduct extensive experiments on two commonly used training targets. Our\nexperiments demonstrate that applying our TFA module significantly improves the\nperformance in terms of five objective evaluation metrics with negligible\nparameter overhead. The evaluation results show that the proposed ResTCN with\nthe TFA module (ResTCN+TFA) consistently outperforms other baselines by a large\nmargin.", "published": "2021-11-15 03:43:04", "link": "http://arxiv.org/abs/2111.07518v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Monaural source separation: From anechoic to reverberant environments", "abstract": "Impressive progress in neural network-based single-channel speech source\nseparation has been made in recent years. But those improvements have been\nmostly reported on anechoic data, a situation that is hardly met in practice.\nTaking the SepFormer as a starting point, which achieves state-of-the-art\nperformance on anechoic mixtures, we gradually modify it to optimize its\nperformance on reverberant mixtures. Although this leads to a word error rate\nimprovement by 7 percentage points compared to the standard SepFormer\nimplementation, the system ends up with only marginally better performance than\na PIT-BLSTM separation system, that is optimized with rather straightforward\nmeans. This is surprising and at the same time sobering, challenging the\npractical usefulness of many improvements reported in recent years for monaural\nsource separation on nonreverberant data.", "published": "2021-11-15 07:51:28", "link": "http://arxiv.org/abs/2111.07578v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Investigating self-supervised front ends for speech spoofing\n  countermeasures", "abstract": "Self-supervised speech model is a rapid progressing research topic, and many\npre-trained models have been released and used in various down stream tasks.\nFor speech anti-spoofing, most countermeasures (CMs) use signal processing\nalgorithms to extract acoustic features for classification. In this study, we\nuse pre-trained self-supervised speech models as the front end of spoofing CMs.\nWe investigated different back end architectures to be combined with the\nself-supervised front end, the effectiveness of fine-tuning the front end, and\nthe performance of using different pre-trained self-supervised models. Our\nfindings showed that, when a good pre-trained front end was fine-tuned with\neither a shallow or a deep neural network-based back end on the ASVspoof 2019\nlogical access (LA) training set, the resulting CM not only achieved a low EER\nscore on the 2019 LA test set but also significantly outperformed the baseline\non the ASVspoof 2015, 2021 LA, and 2021 deepfake test sets. A sub-band analysis\nfurther demonstrated that the CM mainly used the information in a specific\nfrequency band to discriminate the bona fide and spoofed trials across the test\nsets.", "published": "2021-11-15 12:52:50", "link": "http://arxiv.org/abs/2111.07725v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Joint Far- and Near-End Speech Intelligibility Enhancement based on the\n  Approximated Speech Intelligibility Index", "abstract": "This paper considers speech enhancement of signals picked up in one noisy\nenvironment which must be presented to a listener in another noisy environment.\nRecently, it has been shown that an optimal solution to this problem requires\nthe consideration of the noise sources in both environments jointly. However,\nthe existing optimal mutual information based method requires a complicated\nsystem model that includes natural speech variations, and relies on\napproximations and assumptions of the underlying signal distributions. In this\npaper, we propose to use a simpler signal model and optimize speech\nintelligibility based on the Approximated Speech Intelligibility Index (ASII).\nWe derive a closed-form solution to the joint far- and near-end speech\nenhancement problem that is independent of the marginal distribution of signal\ncoefficients, and that achieves similar performance to existing work. In\naddition, we do not need to model or optimize for natural speech variations.", "published": "2021-11-15 13:57:44", "link": "http://arxiv.org/abs/2111.07759v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Biologically inspired speech emotion recognition", "abstract": "Conventional feature-based classification methods do not apply well to\nautomatic recognition of speech emotions, mostly because the precise set of\nspectral and prosodic features that is required to identify the emotional state\nof a speaker has not been determined yet. This paper presents a method that\noperates directly on the speech signal, thus avoiding the problematic step of\nfeature extraction. Furthermore, this method combines the strengths of the\nclassical source-filter model of human speech production with those of the\nrecently introduced liquid state machine (LSM), a biologically-inspired spiking\nneural network (SNN). The source and vocal tract components of the speech\nsignal are first separated and converted into perceptually relevant spectral\nrepresentations. These representations are then processed separately by two\nreservoirs of neurons. The output of each reservoir is reduced in\ndimensionality and fed to a final classifier. This method is shown to provide\nvery good classification performance on the Berlin Database of Emotional Speech\n(Emo-DB). This seems a very promising framework for solving efficiently many\nother problems in speech processing.", "published": "2021-11-15 22:28:05", "link": "http://arxiv.org/abs/2111.08112v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Symbolic Music Loop Generation with VQ-VAE", "abstract": "Music is a repetition of patterns and rhythms. It can be composed by\nrepeating a certain number of bars in a structured way. In this paper, the\nobjective is to generate a loop of 8 bars that can be used as a building block\nof music. Even considering musical diversity, we assume that music patterns\nfamiliar to humans can be defined in a finite set. With explicit rules to\nextract loops from music, we found that discrete representations are sufficient\nto model symbolic music sequences. Among VAE family, musical properties from\nVQ-VAE are better observed rather than other models. Further, to emphasize\nmusical structure, we have manipulated discrete latent features to be\nrepetitive so that the properties are more strengthened. Quantitative and\nqualitative experiments are extensively conducted to verify our assumptions.", "published": "2021-11-15 10:30:13", "link": "http://arxiv.org/abs/2111.07657v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Beyond Mono to Binaural: Generating Binaural Audio from Mono Audio with\n  Depth and Cross Modal Attention", "abstract": "Binaural audio gives the listener an immersive experience and can enhance\naugmented and virtual reality. However, recording binaural audio requires\nspecialized setup with a dummy human head having microphones in left and right\nears. Such a recording setup is difficult to build and setup, therefore mono\naudio has become the preferred choice in common devices. To obtain the same\nimpact as binaural audio, recent efforts have been directed towards lifting\nmono audio to binaural audio conditioned on the visual input from the scene.\nSuch approaches have not used an important cue for the task: the distance of\ndifferent sound producing objects from the microphones. In this work, we argue\nthat depth map of the scene can act as a proxy for inducing distance\ninformation of different objects in the scene, for the task of audio\nbinauralization. We propose a novel encoder-decoder architecture with a\nhierarchical attention mechanism to encode image, depth and audio feature\njointly. We design the network on top of state-of-the-art transformer networks\nfor image and depth representation. We show empirically that the proposed\nmethod outperforms state-of-the-art methods comfortably for two challenging\npublic datasets FAIR-Play and MUSIC-Stereo. We also demonstrate with\nqualitative results that the method is able to focus on the right information\nrequired for the task. The project details are available at\n\\url{https://krantiparida.github.io/projects/bmonobinaural.html}", "published": "2021-11-15 19:07:39", "link": "http://arxiv.org/abs/2111.08046v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Comparative Study of Speech Analysis Methods to Predict Parkinson's\n  Disease", "abstract": "One of the symptoms observed in the early stages of Parkinson's Disease (PD)\nis speech impairment. Speech disorders can be used to detect this disease\nbefore it degenerates. This work analyzes speech features and machine learning\napproaches to predict PD. Acoustic features such as shimmer and jitter\nvariants, and Mel Frequency Cepstral Coefficients (MFCC) are extracted from\nspeech signals. We use two datasets in this work: the MDVR-KCL and the Italian\nParkinson's Voice and Speech database. To separate PD and non-PD speech\nsignals, seven classification models were implemented: K-Nearest Neighbor,\nDecision Trees, Support Vector Machines, Naive Bayes, Logistic Regression,\nGradient Boosting, Random Forests. Three feature sets were used for each of the\nmodels: (a) Acoustic features only, (b) All the acoustic features and MFCC, (c)\nSelected subset of features from acoustic features and MFCC. Using all the\nacoustic features and MFCC, together with SVM produced the highest performance\nwith an accuracy of 98% and F1-Score of 99%. When compared with prior art, this\nshows a better performance. Our code and related documentation is available in\na public domain repository.", "published": "2021-11-15 04:29:51", "link": "http://arxiv.org/abs/2111.10207v1", "categories": ["eess.AS", "cs.LG", "q-bio.QM"], "primary_category": "eess.AS"}
{"title": "Attention based end to end Speech Recognition for Voice Search in Hindi\n  and English", "abstract": "We describe here our work with automatic speech recognition (ASR) in the\ncontext of voice search functionality on the Flipkart e-Commerce platform.\nStarting with the deep learning architecture of Listen-Attend-Spell (LAS), we\nbuild upon and expand the model design and attention mechanisms to incorporate\ninnovative approaches including multi-objective training, multi-pass training,\nand external rescoring using language models and phoneme based losses. We\nreport a relative WER improvement of 15.7% on top of state-of-the-art LAS\nmodels using these modifications. Overall, we report an improvement of 36.9%\nover the phoneme-CTC system. The paper also provides an overview of different\ncomponents that can be tuned in a LAS-based system.", "published": "2021-11-15 18:08:32", "link": "http://arxiv.org/abs/2111.10208v1", "categories": ["eess.AS", "cs.IR", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Metric-based multimodal meta-learning for human movement identification\n  via footstep recognition", "abstract": "We describe a novel metric-based learning approach that introduces a\nmultimodal framework and uses deep audio and geophone encoders in siamese\nconfiguration to design an adaptable and lightweight supervised model. This\nframework eliminates the need for expensive data labeling procedures and learns\ngeneral-purpose representations from low multisensory data obtained from\nomnipresent sensing systems. These sensing systems provide numerous\napplications and various use cases in activity recognition tasks. Here, we\nintend to explore the human footstep movements from indoor environments and\nanalyze representations from a small self-collected dataset of acoustic and\nvibration-based sensors. The core idea is to learn plausible similarities\nbetween two sensory traits and combining representations from audio and\ngeophone signals. We present a generalized framework to learn embeddings from\ntemporal and spatial features extracted from audio and geophone signals. We\nthen extract the representations in a shared space to maximize the learning of\na compatibility function between acoustic and geophone features. This, in turn,\ncan be used effectively to carry out a classification task from the learned\nmodel, as demonstrated by assigning high similarity to the pairs with a human\nfootstep movement and lower similarity to pairs containing no footstep\nmovement. Performance analyses show that our proposed multimodal framework\nachieves a 19.99\\% accuracy increase (in absolute terms) and avoided\noverfitting on the evaluation set when the training samples were increased from\n200 pairs to just 500 pairs while satisfactorily learning the audio and\ngeophone representations. Our results employ a metric-based contrastive\nlearning approach for multi-sensor data to mitigate the impact of data scarcity\nand perform human movement identification with limited data size.", "published": "2021-11-15 18:46:14", "link": "http://arxiv.org/abs/2111.07979v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.SY", "eess.AS", "eess.SY", "q-bio.NC"], "primary_category": "cs.SD"}
