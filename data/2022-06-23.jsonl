{"title": "Mining Error Templates for Grammatical Error Correction", "abstract": "Some grammatical error correction (GEC) systems incorporate hand-crafted\nrules and achieve positive results. However, manually defining rules is\ntime-consuming and laborious. In view of this, we propose a method to mine\nerror templates for GEC automatically. An error template is a regular\nexpression aiming at identifying text errors. We use the web crawler to acquire\nsuch error templates from the Internet. For each template, we further select\nthe corresponding corrective action by using the language model perplexity as a\ncriterion. We have accumulated 1,119 error templates for Chinese GEC based on\nthis method. Experimental results on the newly proposed CTC-2021 Chinese GEC\nbenchmark show that combing our error templates can effectively improve the\nperformance of a strong GEC system, especially on two error types with very\nlittle training data. Our error templates are available at\n\\url{https://github.com/HillZhang1999/gec_error_template}.", "published": "2022-06-23 09:29:52", "link": "http://arxiv.org/abs/2206.11569v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Theory-Grounded Measurement of U.S. Social Stereotypes in English\n  Language Models", "abstract": "NLP models trained on text have been shown to reproduce human stereotypes,\nwhich can magnify harms to marginalized groups when systems are deployed at\nscale. We adapt the Agency-Belief-Communion (ABC) stereotype model of Koch et\nal. (2016) from social psychology as a framework for the systematic study and\ndiscovery of stereotypic group-trait associations in language models (LMs). We\nintroduce the sensitivity test (SeT) for measuring stereotypical associations\nfrom language models. To evaluate SeT and other measures using the ABC model,\nwe collect group-trait judgments from U.S.-based subjects to compare with\nEnglish LM stereotypes. Finally, we extend this framework to measure LM\nstereotyping of intersectional identities.", "published": "2022-06-23 13:22:24", "link": "http://arxiv.org/abs/2206.11684v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards WinoQueer: Developing a Benchmark for Anti-Queer Bias in Large\n  Language Models", "abstract": "This paper presents exploratory work on whether and to what extent biases\nagainst queer and trans people are encoded in large language models (LLMs) such\nas BERT. We also propose a method for reducing these biases in downstream\ntasks: finetuning the models on data written by and/or about queer people. To\nmeasure anti-queer bias, we introduce a new benchmark dataset, WinoQueer,\nmodeled after other bias-detection benchmarks but addressing homophobic and\ntransphobic biases. We found that BERT shows significant homophobic bias, but\nthis bias can be mostly mitigated by finetuning BERT on a natural language\ncorpus written by members of the LGBTQ+ community.", "published": "2022-06-23 05:30:47", "link": "http://arxiv.org/abs/2206.11484v2", "categories": ["cs.CL", "cs.CY", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Constructing Cross-lingual Consumer Health Vocabulary with\n  Word-Embedding from Comparable User Generated Content", "abstract": "The online health community (OHC) is the primary channel for laypeople to\nshare health information. To analyze the health consumer-generated content\n(HCGC) from the OHCs, identifying the colloquial medical expressions used by\nlaypeople is a critical challenge. The open-access and collaborative consumer\nhealth vocabulary (OAC CHV) is the controlled vocabulary for addressing such a\nchallenge. Nevertheless, OAC CHV is only available in English, limiting its\napplicability to other languages. This research proposes a cross-lingual\nautomatic term recognition framework for extending the English CHV into a\ncross-lingual one. Our framework requires an English HCGC corpus and a\nnon-English (i.e., Chinese in this study) HCGC corpus as inputs. Two\nmonolingual word vector spaces are determined using the skip-gram algorithm so\nthat each space encodes common word associations from laypeople within a\nlanguage. Based on the isometry assumption, the framework aligns two\nmonolingual spaces into a bilingual word vector space, where we employ cosine\nsimilarity as a metric for identifying semantically similar words across\nlanguages. The experimental results demonstrate that our framework outperforms\nthe other two large language models in identifying CHV across languages. Our\nframework only requires raw HCGC corpora and a limited size of medical\ntranslations, reducing human efforts in compiling cross-lingual CHV.", "published": "2022-06-23 10:46:39", "link": "http://arxiv.org/abs/2206.11612v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Do Trajectories Encode Verb Meaning?", "abstract": "Distributional models learn representations of words from text, but are\ncriticized for their lack of grounding, or the linking of text to the\nnon-linguistic world. Grounded language models have had success in learning to\nconnect concrete categories like nouns and adjectives to the world via images\nand videos, but can struggle to isolate the meaning of the verbs themselves\nfrom the context in which they typically occur. In this paper, we investigate\nthe extent to which trajectories (i.e. the position and rotation of objects\nover time) naturally encode verb semantics. We build a procedurally generated\nagent-object-interaction dataset, obtain human annotations for the verbs that\noccur in this data, and compare several methods for representation learning\ngiven the trajectories. We find that trajectories correlate as-is with some\nverbs (e.g., fall), and that additional abstraction via self-supervised\npretraining can further capture nuanced differences in verb meaning (e.g., roll\nvs. slide).", "published": "2022-06-23 19:57:16", "link": "http://arxiv.org/abs/2206.11953v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Comparing informativeness of an NLG chatbot vs graphical app in\n  diet-information domain", "abstract": "Visual representation of data like charts and tables can be challenging to\nunderstand for readers. Previous work showed that combining visualisations with\ntext can improve the communication of insights in static contexts, but little\nis known about interactive ones. In this work we present an NLG chatbot that\nprocesses natural language queries and provides insights through a combination\nof charts and text. We apply it to nutrition, a domain communication quality is\ncritical. Through crowd-sourced evaluation we compare the informativeness of\nour chatbot against traditional, static diet-apps. We find that the\nconversational context significantly improved users' understanding of dietary\ndata in various tasks, and that users considered the chatbot as more useful and\nquick to use than traditional apps.", "published": "2022-06-23 07:15:58", "link": "http://arxiv.org/abs/2206.13435v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating Generative Patent Language Models", "abstract": "Generative language models are promising for assisting human writing in\nvarious domains. This manuscript aims to build generative language models in\nthe patent domain and evaluate model performance from a human-centric\nperspective. The perspective is to measure the ratio of keystrokes that can be\nsaved by autocompletion based on generative patent language models. A higher\nratio means a more effective model which can save more keystrokes. This metric\ncan be used to benchmark model performance. The metric is different from\nconventional machine-centric metrics that are token-based instead of\nkeystroke-based. In terms of model size, the largest model built in this\nmanuscript is 6B, which is state-of-the-art in the patent domain. Based on the\nmetric, it is found that the largest model is not necessarily the best for the\nhuman-centric metric. The finding means that keeping increasing model sizes in\nthe patent domain might be unnecessary if the purpose is to assist human\nwriting with autocompletion. Several patent language models are pre-trained\nfrom scratch in this research. The pre-trained models are released for future\nresearchers. Several visualization tools are also provided. The importance of\nbuilding a generative language model in the patent domain is the potential to\nfacilitate creativity and innovations in the future.", "published": "2022-06-23 08:58:05", "link": "http://arxiv.org/abs/2206.14578v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Temporal Extension of Latent Dirichlet Allocation for Unsupervised\n  Acoustic Unit Discovery", "abstract": "Latent Dirichlet allocation (LDA) is widely used for unsupervised topic\nmodelling on sets of documents. No temporal information is used in the model.\nHowever, there is often a relationship between the corresponding topics of\nconsecutive tokens. In this paper, we present an extension to LDA that uses a\nMarkov chain to model temporal information. We use this new model for acoustic\nunit discovery from speech. As input tokens, the model takes a discretised\nencoding of speech from a vector quantised (VQ) neural network with 512 codes.\nThe goal is then to map these 512 VQ codes to 50 phone-like units (topics) in\norder to more closely resemble true phones. In contrast to the base LDA, which\nonly considers how VQ codes co-occur within utterances (documents), the Markov\nchain LDA additionally captures how consecutive codes follow one another. This\nextension leads to an increase in cluster quality and phone segmentation\nresults compared to the base LDA. Compared to a recent vector quantised neural\nnetwork approach that also learns 50 units, the extended LDA model performs\nbetter in phone segmentation but worse in mutual information.", "published": "2022-06-23 13:53:59", "link": "http://arxiv.org/abs/2206.11706v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "eess.AS"}
{"title": "A Disability Lens towards Biases in GPT-3 Generated Open-Ended Languages", "abstract": "Language models (LM) are becoming prevalent in many language-based\napplication spaces globally. Although these LMs are improving our day-to-day\ninteractions with digital products, concerns remain whether open-ended\nlanguages or text generated from these models reveal any biases toward a\nspecific group of people, thereby risking the usability of a certain product.\nThere is a need to identify whether these models possess bias to improve the\nfairness in these models. This gap motivates our ongoing work, where we\nmeasured the two aspects of bias in GPT-3 generated text through a disability\nlens.", "published": "2022-06-23 21:57:08", "link": "http://arxiv.org/abs/2206.11993v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "QbyE-MLPMixer: Query-by-Example Open-Vocabulary Keyword Spotting using\n  MLPMixer", "abstract": "Current keyword spotting systems are typically trained with a large amount of\npre-defined keywords. Recognizing keywords in an open-vocabulary setting is\nessential for personalizing smart device interaction. Towards this goal, we\npropose a pure MLP-based neural network that is based on MLPMixer - an MLP\nmodel architecture that effectively replaces the attention mechanism in Vision\nTransformers. We investigate different ways of adapting the MLPMixer\narchitecture to the QbyE open-vocabulary keyword spotting task. Comparisons\nwith the state-of-the-art RNN and CNN models show that our method achieves\nbetter performance in challenging situations (10dB and 6dB environments) on\nboth the publicly available Hey-Snips dataset and a larger scale internal\ndataset with 400 speakers. Our proposed model also has a smaller number of\nparameters and MACs compared to the baseline models.", "published": "2022-06-23 18:18:44", "link": "http://arxiv.org/abs/2206.13231v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "The MuSe 2022 Multimodal Sentiment Analysis Challenge: Humor, Emotional\n  Reactions, and Stress", "abstract": "The Multimodal Sentiment Analysis Challenge (MuSe) 2022 is dedicated to\nmultimodal sentiment and emotion recognition. For this year's challenge, we\nfeature three datasets: (i) the Passau Spontaneous Football Coach Humor\n(Passau-SFCH) dataset that contains audio-visual recordings of German football\ncoaches, labelled for the presence of humour; (ii) the Hume-Reaction dataset in\nwhich reactions of individuals to emotional stimuli have been annotated with\nrespect to seven emotional expression intensities, and (iii) the Ulm-Trier\nSocial Stress Test (Ulm-TSST) dataset comprising of audio-visual data labelled\nwith continuous emotion values (arousal and valence) of people in stressful\ndispositions. Using the introduced datasets, MuSe 2022 2022 addresses three\ncontemporary affective computing problems: in the Humor Detection Sub-Challenge\n(MuSe-Humor), spontaneous humour has to be recognised; in the Emotional\nReactions Sub-Challenge (MuSe-Reaction), seven fine-grained `in-the-wild'\nemotions have to be predicted; and in the Emotional Stress Sub-Challenge\n(MuSe-Stress), a continuous prediction of stressed emotion values is featured.\nThe challenge is designed to attract different research communities,\nencouraging a fusion of their disciplines. Mainly, MuSe 2022 targets the\ncommunities of audio-visual emotion recognition, health informatics, and\nsymbolic sentiment analysis. This baseline paper describes the datasets as well\nas the feature sets extracted from them. A recurrent neural network with LSTM\ncells is used to set competitive baseline results on the test partitions for\neach sub-challenge. We report an Area Under the Curve (AUC) of .8480 for\nMuSe-Humor; .2801 mean (from 7-classes) Pearson's Correlations Coefficient for\nMuSe-Reaction, as well as .4931 Concordance Correlation Coefficient (CCC) and\n.4761 for valence and arousal in MuSe-Stress, respectively.", "published": "2022-06-23 13:34:33", "link": "http://arxiv.org/abs/2207.05691v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MM", "eess.AS"], "primary_category": "cs.LG"}
{"title": "AST-Probe: Recovering abstract syntax trees from hidden representations\n  of pre-trained language models", "abstract": "The objective of pre-trained language models is to learn contextual\nrepresentations of textual data. Pre-trained language models have become\nmainstream in natural language processing and code modeling. Using probes, a\ntechnique to study the linguistic properties of hidden vector spaces, previous\nworks have shown that these pre-trained language models encode simple\nlinguistic properties in their hidden representations. However, none of the\nprevious work assessed whether these models encode the whole grammatical\nstructure of a programming language. In this paper, we prove the existence of a\nsyntactic subspace, lying in the hidden representations of pre-trained language\nmodels, which contain the syntactic information of the programming language. We\nshow that this subspace can be extracted from the models' representations and\ndefine a novel probing method, the AST-Probe, that enables recovering the whole\nabstract syntax tree (AST) of an input code snippet. In our experimentations,\nwe show that this syntactic subspace exists in five state-of-the-art\npre-trained language models. In addition, we highlight that the middle layers\nof the models are the ones that encode most of the AST information. Finally, we\nestimate the optimal size of this syntactic subspace and show that its\ndimension is substantially lower than those of the models' representation\nspaces. This suggests that pre-trained language models use a small part of\ntheir representation spaces to encode syntactic information of the programming\nlanguages.", "published": "2022-06-23 14:09:05", "link": "http://arxiv.org/abs/2206.11719v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Frequency Dependent Sound Event Detection for DCASE 2022 Challenge Task\n  4", "abstract": "While many deep learning methods on other domains have been applied to sound\nevent detection (SED), differences between original domains of the methods and\nSED have not been appropriately considered so far. As SED uses audio data with\ntwo dimensions (time and frequency) for input, thorough comprehension on these\ntwo dimensions is essential for application of methods from other domains on\nSED. Previous works proved that methods those address on frequency dimension\nare especially powerful in SED. By applying FilterAugment and frequency dynamic\nconvolution those are frequency dependent methods proposed to enhance SED\nperformance, our submitted models achieved best PSDS1 of 0.4704 and best PSDS2\nof 0.8224.", "published": "2022-06-23 12:06:08", "link": "http://arxiv.org/abs/2206.11645v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Adversarial Multi-Task Learning for Disentangling Timbre and Pitch in\n  Singing Voice Synthesis", "abstract": "Recently, deep learning-based generative models have been introduced to\ngenerate singing voices. One approach is to predict the parametric vocoder\nfeatures consisting of explicit speech parameters. This approach has the\nadvantage that the meaning of each feature is explicitly distinguished. Another\napproach is to predict mel-spectrograms for a neural vocoder. However,\nparametric vocoders have limitations of voice quality and the mel-spectrogram\nfeatures are difficult to model because the timbre and pitch information are\nentangled. In this study, we propose a singing voice synthesis model with\nmulti-task learning to use both approaches -- acoustic features for a\nparametric vocoder and mel-spectrograms for a neural vocoder. By using the\nparametric vocoder features as auxiliary features, the proposed model can\nefficiently disentangle and control the timbre and pitch components of the\nmel-spectrogram. Moreover, a generative adversarial network framework is\napplied to improve the quality of singing voices in a multi-singer model.\nExperimental results demonstrate that our proposed model can generate more\nnatural singing voices than the single-task models, while performing better\nthan the conventional parametric vocoder-based model.", "published": "2022-06-23 09:11:02", "link": "http://arxiv.org/abs/2206.11558v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Two-pass Decoding and Cross-adaptation Based System Combination of\n  End-to-end Conformer and Hybrid TDNN ASR Systems", "abstract": "Fundamental modelling differences between hybrid and end-to-end (E2E)\nautomatic speech recognition (ASR) systems create large diversity and\ncomplementarity among them. This paper investigates multi-pass rescoring and\ncross adaptation based system combination approaches for hybrid TDNN and\nConformer E2E ASR systems. In multi-pass rescoring, state-of-the-art hybrid\nLF-MMI trained CNN-TDNN system featuring speed perturbation, SpecAugment and\nBayesian learning hidden unit contributions (LHUC) speaker adaptation was used\nto produce initial N-best outputs before being rescored by the speaker adapted\nConformer system using a 2-way cross system score interpolation. In cross\nadaptation, the hybrid CNN-TDNN system was adapted to the 1-best output of the\nConformer system or vice versa. Experiments on the 300-hour Switchboard corpus\nsuggest that the combined systems derived using either of the two system\ncombination approaches outperformed the individual systems. The best combined\nsystem obtained using multi-pass rescoring produced statistically significant\nword error rate (WER) reductions of 2.5% to 3.9% absolute (22.5% to 28.9%\nrelative) over the stand alone Conformer system on the NIST Hub5'00, Rt03 and\nRt02 evaluation data.", "published": "2022-06-23 10:17:13", "link": "http://arxiv.org/abs/2206.11596v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Formant Estimation and Tracking using Probabilistic Heat-Maps", "abstract": "Formants are the spectral maxima that result from acoustic resonances of the\nhuman vocal tract, and their accurate estimation is among the most fundamental\nspeech processing problems. Recent work has been shown that those frequencies\ncan accurately be estimated using deep learning techniques. However, when\npresented with a speech from a different domain than that in which they have\nbeen trained on, these methods exhibit a decline in performance, limiting their\nusage as generic tools.\n  The contribution of this paper is to propose a new network architecture that\nperforms well on a variety of different speaker and speech domains. Our\nproposed model is composed of a shared encoder that gets as input a spectrogram\nand outputs a domain-invariant representation. Then, multiple decoders further\nprocess this representation, each responsible for predicting a different\nformant while considering the lower formant predictions. An advantage of our\nmodel is that it is based on heatmaps that generate a probability distribution\nover formant predictions. Results suggest that our proposed model better\nrepresents the signal over various domains and leads to better formant\nfrequency tracking and estimation.", "published": "2022-06-23 11:39:08", "link": "http://arxiv.org/abs/2206.11632v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speaker-Independent Microphone Identification in Noisy Conditions", "abstract": "This work proposes a method for source device identification from speech\nrecordings that applies neural-network-based denoising, to mitigate the impact\nof counter-forensics attacks using noise injection. The method is evaluated by\ncomparing the impact of denoising on three state-of-the-art features for\nmicrophone classification, determining their discriminating power with and\nwithout denoising being applied. The proposed framework achieves a significant\nperformance increase for noisy material, and more generally, validates the\nusefulness of applying denoising prior to device identification for noisy\nrecordings.", "published": "2022-06-23 11:50:32", "link": "http://arxiv.org/abs/2206.11640v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The SJTU X-LANCE Lab System for CNSRC 2022", "abstract": "This technical report describes the SJTU X-LANCE Lab system for the three\ntracks in CNSRC 2022. In this challenge, we explored the speaker embedding\nmodeling ability of deep ResNet (Deeper r-vector). All the systems are only\ntrained on the Cnceleb training set and we use the same systems for the three\ntracks in CNSRC 2022. In this challenge, our system ranks the first place in\nthe fixed track of speaker verification task. Our best single system and fusion\nsystem achieve 0.3164 and 0.2975 minDCF respectively. Besides, we submit the\nresult of ResNet221 to the speaker retrieval track and achieve 0.4626 mAP. More\nimportantly, we have helped the wespeaker [1] toolkit reproduce our result:\nhttps://github.com/wenet-e2e/wespeaker.", "published": "2022-06-23 13:40:59", "link": "http://arxiv.org/abs/2206.11699v5", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards End-to-End Private Automatic Speaker Recognition", "abstract": "The development of privacy-preserving automatic speaker verification systems\nhas been the focus of a number of studies with the intent of allowing users to\nauthenticate themselves without risking the privacy of their voice. However,\ncurrent privacy-preserving methods assume that the template voice\nrepresentations (or speaker embeddings) used for authentication are extracted\nlocally by the user. This poses two important issues: first, knowledge of the\nspeaker embedding extraction model may create security and robustness\nliabilities for the authentication system, as this knowledge might help\nattackers in crafting adversarial examples able to mislead the system; second,\nfrom the point of view of a service provider the speaker embedding extraction\nmodel is arguably one of the most valuable components in the system and, as\nsuch, disclosing it would be highly undesirable. In this work, we show how\nspeaker embeddings can be extracted while keeping both the speaker's voice and\nthe service provider's model private, using Secure Multiparty Computation.\nFurther, we show that it is possible to obtain reasonable trade-offs between\nsecurity and computational cost. This work is complementary to those showing\nhow authentication may be performed privately, and thus can be considered as\nanother step towards fully private automatic speaker recognition.", "published": "2022-06-23 14:49:40", "link": "http://arxiv.org/abs/2206.11750v1", "categories": ["eess.AS", "cs.CR"], "primary_category": "eess.AS"}
{"title": "Comparing supervised and self-supervised embedding for ExVo Multi-Task\n  learning track", "abstract": "The ICML Expressive Vocalizations (ExVo) Multi-task challenge 2022, focuses\non understanding the emotional facets of the non-linguistic vocalizations\n(vocal bursts (VB)). The objective of this challenge is to predict emotional\nintensities for VB, being a multi-task challenge it also requires to predict\nspeakers' age and native-country. For this challenge we study and compare two\ndistinct embedding spaces namely, self-supervised learning (SSL) based\nembeddings and task-specific supervised learning based embeddings. Towards\nthat, we investigate feature representations obtained from several pre-trained\nSSL neural networks and task-specific supervised classification neural\nnetworks. Our studies show that the best performance is obtained with a hybrid\napproach, where predictions derived via both SSL and task-specific supervised\nlearning are used. Our best system on test-set surpasses the ComPARE baseline\n(harmonic mean of all sub-task scores i.e., $S_{MTL}$) by a relative $13\\%$\nmargin.", "published": "2022-06-23 20:32:09", "link": "http://arxiv.org/abs/2206.11968v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Restoring speech intelligibility for hearing aid users with deep\n  learning", "abstract": "Almost half a billion people world-wide suffer from disabling hearing loss.\nWhile hearing aids can partially compensate for this, a large proportion of\nusers struggle to understand speech in situations with background noise. Here,\nwe present a deep learning-based algorithm that selectively suppresses noise\nwhile maintaining speech signals. The algorithm restores speech intelligibility\nfor hearing aid users to the level of control subjects with normal hearing. It\nconsists of a deep network that is trained on a large custom database of noisy\nspeech signals and is further optimized by a neural architecture search, using\na novel deep learning-based metric for speech intelligibility. The network\nachieves state-of-the-art denoising on a range of human-graded assessments,\ngeneralizes across different noise categories and - in contrast to classic\nbeamforming approaches - operates on a single microphone. The system runs in\nreal time on a laptop, suggesting that large-scale deployment on hearing aid\nchips could be achieved within a few years. Deep learning-based denoising\ntherefore holds the potential to improve the quality of life of millions of\nhearing impaired people soon.", "published": "2022-06-23 09:26:45", "link": "http://arxiv.org/abs/2206.11567v1", "categories": ["cs.SD", "eess.AS", "q-bio.NC"], "primary_category": "cs.SD"}
{"title": "Towards Green ASR: Lossless 4-bit Quantization of a Hybrid TDNN System\n  on the 300-hr Switchboard Corpus", "abstract": "State of the art time automatic speech recognition (ASR) systems are becoming\nincreasingly complex and expensive for practical applications. This paper\npresents the development of a high performance and low-footprint 4-bit\nquantized LF-MMI trained factored time delay neural networks (TDNNs) based ASR\nsystem on the 300-hr Switchboard corpus. A key feature of the overall system\ndesign is to account for the fine-grained, varying performance sensitivity at\ndifferent model components to quantization errors. To this end, a set of neural\narchitectural compression and mixed precision quantization approaches were used\nto facilitate hidden layer level auto-configuration of optimal factored TDNN\nweight matrix subspace dimensionality and quantization bit-widths. The proposed\ntechniques were also used to produce 2-bit mixed precision quantized\nTransformer language models. Experiments conducted on the Switchboard data\nsuggest that the proposed neural architectural compression and mixed precision\nquantization techniques consistently outperform the uniform precision quantised\nbaseline systems of comparable bit-widths in terms of word error rate (WER). An\noverall \"lossless\" compression ratio of 13.6 was obtained over the baseline\nfull precision system including both the TDNN and Transformer components while\nincurring no statistically significant WER increase.", "published": "2022-06-23 12:02:33", "link": "http://arxiv.org/abs/2206.11643v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Efficient Transformer-based Speech Enhancement Using Long Frames and\n  STFT Magnitudes", "abstract": "The SepFormer architecture shows very good results in speech separation. Like\nother learned-encoder models, it uses short frames, as they have been shown to\nobtain better performance in these cases. This results in a large number of\nframes at the input, which is problematic; since the SepFormer is\ntransformer-based, its computational complexity drastically increases with\nlonger sequences. In this paper, we employ the SepFormer in a speech\nenhancement task and show that by replacing the learned-encoder features with a\nmagnitude short-time Fourier transform (STFT) representation, we can use long\nframes without compromising perceptual enhancement performance. We obtained\nequivalent quality and intelligibility evaluation scores while reducing the\nnumber of operations by a factor of approximately 8 for a 10-second utterance.", "published": "2022-06-23 13:50:31", "link": "http://arxiv.org/abs/2206.11703v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Conformer Based Elderly Speech Recognition System for Alzheimer's\n  Disease Detection", "abstract": "Early diagnosis of Alzheimer's disease (AD) is crucial in facilitating\npreventive care to delay further progression. This paper presents the\ndevelopment of a state-of-the-art Conformer based speech recognition system\nbuilt on the DementiaBank Pitt corpus for automatic AD detection. The baseline\nConformer system trained with speed perturbation and SpecAugment based data\naugmentation is significantly improved by incorporating a set of purposefully\ndesigned modeling features, including neural architecture search based\nauto-configuration of domain-specific Conformer hyper-parameters in addition to\nparameter fine-tuning; fine-grained elderly speaker adaptation using learning\nhidden unit contributions (LHUC); and two-pass cross-system rescoring based\ncombination with hybrid TDNN systems. An overall word error rate (WER)\nreduction of 13.6% absolute (34.8% relative) was obtained on the evaluation\ndata of 48 elderly speakers. Using the final systems' recognition outputs to\nextract textual features, the best-published speech recognition based AD\ndetection accuracy of 91.7% was obtained.", "published": "2022-06-23 12:50:55", "link": "http://arxiv.org/abs/2206.13232v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Pruned RNN-T for fast, memory-efficient ASR training", "abstract": "The RNN-Transducer (RNN-T) framework for speech recognition has been growing\nin popularity, particularly for deployed real-time ASR systems, because it\ncombines high accuracy with naturally streaming recognition. One of the\ndrawbacks of RNN-T is that its loss function is relatively slow to compute, and\ncan use a lot of memory. Excessive GPU memory usage can make it impractical to\nuse RNN-T loss in cases where the vocabulary size is large: for example, for\nChinese character-based ASR. We introduce a method for faster and more\nmemory-efficient RNN-T loss computation. We first obtain pruning bounds for the\nRNN-T recursion using a simple joiner network that is linear in the encoder and\ndecoder embeddings; we can evaluate this without using much memory. We then use\nthose pruning bounds to evaluate the full, non-linear joiner network.", "published": "2022-06-23 12:18:03", "link": "http://arxiv.org/abs/2206.13236v1", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
