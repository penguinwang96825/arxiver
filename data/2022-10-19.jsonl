{"title": "Type-supervised sequence labeling based on the heterogeneous star graph\n  for named entity recognition", "abstract": "Named entity recognition is a fundamental task in natural language\nprocessing, identifying the span and category of entities in unstructured\ntexts. The traditional sequence labeling methodology ignores the nested\nentities, i.e. entities included in other entity mentions. Many approaches\nattempt to address this scenario, most of which rely on complex structures or\nhave high computation complexity. The representation learning of the\nheterogeneous star graph containing text nodes and type nodes is investigated\nin this paper. In addition, we revise the graph attention mechanism into a\nhybrid form to address its unreasonableness in specific topologies. The model\nperforms the type-supervised sequence labeling after updating nodes in the\ngraph. The annotation scheme is an extension of the single-layer sequence\nlabeling and is able to cope with the vast majority of nested entities.\nExtensive experiments on public NER datasets reveal the effectiveness of our\nmodel in extracting both flat and nested entities. The method achieved\nstate-of-the-art performance on both flat and nested datasets. The significant\nimprovement in accuracy reflects the superiority of the multi-layer labeling\nstrategy.", "published": "2022-10-19 01:40:06", "link": "http://arxiv.org/abs/2210.10240v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Continued Pretraining for Better Zero- and Few-Shot Promptability", "abstract": "Recently introduced language model prompting methods can achieve high\naccuracy in zero- and few-shot settings while requiring few to no learned\ntask-specific parameters. Nevertheless, these methods still often trail behind\nfull model finetuning. In this work, we investigate if a dedicated continued\npretraining stage could improve \"promptability\", i.e., zero-shot performance\nwith natural language prompts or few-shot performance with prompt tuning. We\nreveal settings where existing continued pretraining methods lack\npromptability. We also identify current methodological gaps, which we fill with\nthorough large-scale experiments. We demonstrate that a simple recipe,\ncontinued pretraining that incorporates a trainable prompt during multi-task\nlearning, leads to improved promptability in both zero- and few-shot settings\ncompared to existing methods, up to 31% relative. On the other hand, we find\nthat continued pretraining using MAML-style meta-learning, a method that\ndirectly optimizes few-shot promptability, yields subpar performance. We\nvalidate our findings with two prompt tuning methods, and, based on our\nresults, we provide concrete recommendations to optimize promptability for\ndifferent use cases.", "published": "2022-10-19 02:41:51", "link": "http://arxiv.org/abs/2210.10258v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Entity Detection with Proposer and Regressor", "abstract": "Named entity recognition is a traditional task in natural language\nprocessing. In particular, nested entity recognition receives extensive\nattention for the widespread existence of the nesting scenario. The latest\nresearch migrates the well-established paradigm of set prediction in object\ndetection to cope with entity nesting. However, the manual creation of query\nvectors, which fail to adapt to the rich semantic information in the context,\nlimits these approaches. An end-to-end entity detection approach with proposer\nand regressor is presented in this paper to tackle the issues. First, the\nproposer utilizes the feature pyramid network to generate high-quality entity\nproposals. Then, the regressor refines the proposals for generating the final\nprediction. The model adopts encoder-only architecture and thus obtains the\nadvantages of the richness of query semantics, high precision of entity\nlocalization, and easiness of model training. Moreover, we introduce the novel\nspatially modulated attention and progressive refinement for further\nimprovement. Extensive experiments demonstrate that our model achieves advanced\nperformance in flat and nested NER, achieving a new state-of-the-art F1 score\nof 80.74 on the GENIA dataset and 72.38 on the WeiboNER dataset.", "published": "2022-10-19 02:42:46", "link": "http://arxiv.org/abs/2210.10260v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Aspect Sentiment Quad Prediction via Template-Order Data\n  Augmentation", "abstract": "Recently, aspect sentiment quad prediction (ASQP) has become a popular task\nin the field of aspect-level sentiment analysis. Previous work utilizes a\npredefined template to paraphrase the original sentence into a structure target\nsequence, which can be easily decoded as quadruplets of the form (aspect\ncategory, aspect term, opinion term, sentiment polarity). The template involves\nthe four elements in a fixed order. However, we observe that this solution\ncontradicts with the order-free property of the ASQP task, since there is no\nneed to fix the template order as long as the quadruplet is extracted\ncorrectly. Inspired by the observation, we study the effects of template orders\nand find that some orders help the generative model achieve better performance.\nIt is hypothesized that different orders provide various views of the\nquadruplet. Therefore, we propose a simple but effective method to identify the\nmost proper orders, and further combine multiple proper templates as data\naugmentation to improve the ASQP task. Specifically, we use the pre-trained\nlanguage model to select the orders with minimal entropy. By fine-tuning the\npre-trained language model with these template orders, our approach improves\nthe performance of quad prediction, and outperforms state-of-the-art methods\nsignificantly in low-resource settings.", "published": "2022-10-19 04:31:08", "link": "http://arxiv.org/abs/2210.10291v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Forging Multiple Training Objectives for Pre-trained Language Models via\n  Meta-Learning", "abstract": "Multiple pre-training objectives fill the vacancy of the understanding\ncapability of single-objective language modeling, which serves the ultimate\npurpose of pre-trained language models (PrLMs), generalizing well on a mass of\nscenarios. However, learning multiple training objectives in a single model is\nchallenging due to the unknown relative significance as well as the potential\ncontrariety between them. Empirical studies have shown that the current\nobjective sampling in an ad-hoc manual setting makes the learned language\nrepresentation barely converge to the desired optimum. Thus, we propose\n\\textit{MOMETAS}, a novel adaptive sampler based on meta-learning, which learns\nthe latent sampling pattern on arbitrary pre-training objectives. Such a design\nis lightweight with negligible additional training overhead. To validate our\napproach, we adopt five objectives and conduct continual pre-training with\nBERT-base and BERT-large models, where MOMETAS demonstrates universal\nperformance gain over other rule-based sampling strategies on 14 natural\nlanguage processing tasks.", "published": "2022-10-19 04:38:26", "link": "http://arxiv.org/abs/2210.10293v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Unified Neural Network Model for Readability Assessment with Feature\n  Projection and Length-Balanced Loss", "abstract": "For readability assessment, traditional methods mainly employ machine\nlearning classifiers with hundreds of linguistic features. Although the deep\nlearning model has become the prominent approach for almost all NLP tasks, it\nis less explored for readability assessment. In this paper, we propose a\nBERT-based model with feature projection and length-balanced loss (BERT-FP-LBL)\nfor readability assessment. Specially, we present a new difficulty knowledge\nguided semi-supervised method to extract topic features to complement the\ntraditional linguistic features. From the linguistic features, we employ\nprojection filtering to extract orthogonal features to supplement BERT\nrepresentations. Furthermore, we design a new length-balanced loss to handle\nthe greatly varying length distribution of data. Our model achieves\nstate-of-the-art performances on two English benchmark datasets and one dataset\nof Chinese textbooks, and also achieves the near-perfect accuracy of 99\\% on\none English dataset. Moreover, our proposed model obtains comparable results\nwith human experts in consistency test.", "published": "2022-10-19 05:33:27", "link": "http://arxiv.org/abs/2210.10305v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning from the Dictionary: Heterogeneous Knowledge Guided Fine-tuning\n  for Chinese Spell Checking", "abstract": "Chinese Spell Checking (CSC) aims to detect and correct Chinese spelling\nerrors. Recent researches start from the pretrained knowledge of language\nmodels and take multimodal information into CSC models to improve the\nperformance. However, they overlook the rich knowledge in the dictionary, the\nreference book where one can learn how one character should be pronounced,\nwritten, and used. In this paper, we propose the LEAD framework, which renders\nthe CSC model to learn heterogeneous knowledge from the dictionary in terms of\nphonetics, vision, and meaning. LEAD first constructs positive and negative\nsamples according to the knowledge of character phonetics, glyphs, and\ndefinitions in the dictionary. Then a unified contrastive learning-based\ntraining scheme is employed to refine the representations of the CSC models.\nExtensive experiments and detailed analyses on the SIGHAN benchmark datasets\ndemonstrate the effectiveness of our proposed methods.", "published": "2022-10-19 06:31:34", "link": "http://arxiv.org/abs/2210.10320v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Stability of Fine-Tuning Pretrained Language Models via\n  Component-Wise Gradient Norm Clipping", "abstract": "Fine-tuning over large pretrained language models (PLMs) has established many\nstate-of-the-art results. Despite its superior performance, such fine-tuning\ncan be unstable, resulting in significant variance in performance and potential\nrisks for practical applications. Previous works have attributed such\ninstability to the catastrophic forgetting problem in the top layers of PLMs,\nwhich indicates iteratively that fine-tuning layers in a top-down manner is a\npromising solution. In this paper, we first point out that this method does not\nalways work out due to the different convergence speeds of different\nlayers/modules. Inspired by this observation, we propose a simple\ncomponent-wise gradient norm clipping method to adjust the convergence speed\nfor different components. Experiment results demonstrate that our method\nachieves consistent improvements in terms of generalization performance,\nconvergence speed, and training stability. The codebase can be found at\nhttps://github.com/yangalan123/FineTuningStability.", "published": "2022-10-19 06:44:20", "link": "http://arxiv.org/abs/2210.10325v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MuGER$^2$: Multi-Granularity Evidence Retrieval and Reasoning for Hybrid\n  Question Answering", "abstract": "Hybrid question answering (HQA) aims to answer questions over heterogeneous\ndata, including tables and passages linked to table cells. The heterogeneous\ndata can provide different granularity evidence to HQA models, e.t., column,\nrow, cell, and link. Conventional HQA models usually retrieve coarse- or\nfine-grained evidence to reason the answer. Through comparison, we find that\ncoarse-grained evidence is easier to retrieve but contributes less to the\nreasoner, while fine-grained evidence is the opposite. To preserve the\nadvantage and eliminate the disadvantage of different granularity evidence, we\npropose MuGER$^2$, a Multi-Granularity Evidence Retrieval and Reasoning\napproach. In evidence retrieval, a unified retriever is designed to learn the\nmulti-granularity evidence from the heterogeneous data. In answer reasoning, an\nevidence selector is proposed to navigate the fine-grained evidence for the\nanswer reader based on the learned multi-granularity evidence. Experiment\nresults on the HybridQA dataset show that MuGER$^2$ significantly boosts the\nHQA performance. Further ablation analysis verifies the effectiveness of both\nthe retrieval and reasoning designs.", "published": "2022-10-19 07:36:03", "link": "http://arxiv.org/abs/2210.10350v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging a New Spanish Corpus for Multilingual and Crosslingual\n  Metaphor Detection", "abstract": "The lack of wide coverage datasets annotated with everyday metaphorical\nexpressions for languages other than English is striking. This means that most\nresearch on supervised metaphor detection has been published only for that\nlanguage. In order to address this issue, this work presents the first corpus\nannotated with naturally occurring metaphors in Spanish large enough to develop\nsystems to perform metaphor detection. The presented dataset, CoMeta, includes\ntexts from various domains, namely, news, political discourse, Wikipedia and\nreviews. In order to label CoMeta, we apply the MIPVU method, the guidelines\nmost commonly used to systematically annotate metaphor on real data. We use our\nnewly created dataset to provide competitive baselines by fine-tuning several\nmultilingual and monolingual state-of-the-art large language models.\nFurthermore, by leveraging the existing VUAM English data in addition to\nCoMeta, we present the, to the best of our knowledge, first cross-lingual\nexperiments on supervised metaphor detection. Finally, we perform a detailed\nerror analysis that explores the seemingly high transfer of everyday metaphor\nacross these two languages and datasets.", "published": "2022-10-19 07:55:36", "link": "http://arxiv.org/abs/2210.10358v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tourist Guidance Robot Based on HyperCLOVA", "abstract": "This paper describes our system submitted to Dialogue Robot Competition 2022.\nOur proposed system is a combined model of rule-based and generation-based\ndialog systems. The system utilizes HyperCLOVA, a Japanese foundation model,\nnot only to generate responses but also summarization, search information, etc.\nWe also used our original speech recognition system, which was fine-tuned for\nthis dialog task. As a result, our system ranked second in the preliminary\nround and moved on to the finals.", "published": "2022-10-19 09:11:23", "link": "http://arxiv.org/abs/2210.10400v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hybrid-Regressive Neural Machine Translation", "abstract": "In this work, we empirically confirm that non-autoregressive translation with\nan iterative refinement mechanism (IR-NAT) suffers from poor acceleration\nrobustness because it is more sensitive to decoding batch size and computing\ndevice setting than autoregressive translation (AT). Inspired by it, we attempt\nto investigate how to combine the strengths of autoregressive and\nnon-autoregressive translation paradigms better. To this end, we demonstrate\nthrough synthetic experiments that prompting a small number of AT's predictions\ncan promote one-shot non-autoregressive translation to achieve the equivalent\nperformance of IR-NAT. Following this line, we propose a new two-stage\ntranslation prototype called hybrid-regressive translation (HRT). Specifically,\nHRT first generates discontinuous sequences via autoregression (e.g., make a\nprediction every k tokens, k>1) and then fills in all previously skipped tokens\nat once in a non-autoregressive manner. We also propose a bag of techniques to\neffectively and efficiently train HRT without adding any model parameters. HRT\nachieves the state-of-the-art BLEU score of 28.49 on the WMT En-De task and is\nat least 1.5x faster than AT, regardless of batch size and device. In addition,\nanother bonus of HRT is that it successfully inherits the good characteristics\nof AT in the deep-encoder-shallow-decoder architecture. Concretely, compared to\nthe vanilla HRT with a 6-layer encoder and 6-layer decoder, the inference speed\nof HRT with a 12-layer encoder and 1-layer decoder is further doubled on both\nGPU and CPU without BLEU loss.", "published": "2022-10-19 09:26:15", "link": "http://arxiv.org/abs/2210.10416v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Linguistic Investigation of Machine Learning based Contradiction\n  Detection Models: An Empirical Analysis and Future Perspectives", "abstract": "We analyze two Natural Language Inference data sets with respect to their\nlinguistic features. The goal is to identify those syntactic and semantic\nproperties that are particularly hard to comprehend for a machine learning\nmodel. To this end, we also investigate the differences between a\ncrowd-sourced, machine-translated data set (SNLI) and a collection of text\npairs from internet sources. Our main findings are, that the model has\ndifficulty recognizing the semantic importance of prepositions and verbs,\nemphasizing the importance of linguistically aware pre-training tasks.\nFurthermore, it often does not comprehend antonyms and homonyms, especially if\nthose are depending on the context. Incomplete sentences are another problem,\nas well as longer paragraphs and rare words or phrases. The study shows that\nautomated language understanding requires a more informed approach, utilizing\nas much external knowledge as possible throughout the training process.", "published": "2022-10-19 10:06:03", "link": "http://arxiv.org/abs/2210.10434v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistic Rules-Based Corpus Generation for Native Chinese Grammatical\n  Error Correction", "abstract": "Chinese Grammatical Error Correction (CGEC) is both a challenging NLP task\nand a common application in human daily life. Recently, many data-driven\napproaches are proposed for the development of CGEC research. However, there\nare two major limitations in the CGEC field: First, the lack of high-quality\nannotated training corpora prevents the performance of existing CGEC models\nfrom being significantly improved. Second, the grammatical errors in widely\nused test sets are not made by native Chinese speakers, resulting in a\nsignificant gap between the CGEC models and the real application. In this\npaper, we propose a linguistic rules-based approach to construct large-scale\nCGEC training corpora with automatically generated grammatical errors.\nAdditionally, we present a challenging CGEC benchmark derived entirely from\nerrors made by native Chinese speakers in real-world scenarios. Extensive\nexperiments and detailed analyses not only demonstrate that the training data\nconstructed by our method effectively improves the performance of CGEC models,\nbut also reflect that our benchmark is an excellent resource for further\ndevelopment of the CGEC field.", "published": "2022-10-19 10:20:39", "link": "http://arxiv.org/abs/2210.10442v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GCDT: A Chinese RST Treebank for Multigenre and Multilingual Discourse\n  Parsing", "abstract": "A lack of large-scale human-annotated data has hampered the hierarchical\ndiscourse parsing of Chinese. In this paper, we present GCDT, the largest\nhierarchical discourse treebank for Mandarin Chinese in the framework of\nRhetorical Structure Theory (RST). GCDT covers over 60K tokens across five\ngenres of freely available text, using the same relation inventory as\ncontemporary RST treebanks for English. We also report on this dataset's\nparsing experiments, including state-of-the-art (SOTA) scores for Chinese RST\nparsing and RST parsing on the English GUM dataset, using cross-lingual\ntraining in Chinese and English with multilingual embeddings.", "published": "2022-10-19 10:27:41", "link": "http://arxiv.org/abs/2210.10449v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Does More Than Describe: On The Lack Of Figurative Speech in\n  Text-To-Image Models", "abstract": "The impressive capacity shown by recent text-to-image diffusion models to\ngenerate high-quality pictures from textual input prompts has leveraged the\ndebate about the very definition of art. Nonetheless, these models have been\ntrained using text data collected from content-based labelling protocols that\nfocus on describing the items and actions in an image but neglect any\nsubjective appraisal. Consequently, these automatic systems need rigorous\ndescriptions of the elements and the pictorial style of the image to be\ngenerated, otherwise failing to deliver. As potential indicators of the actual\nartistic capabilities of current generative models, we characterise the\nsentimentality, objectiveness and degree of abstraction of publicly available\ntext data used to train current text-to-image diffusion models. Considering the\nsharp difference observed between their language style and that typically\nemployed in artistic contexts, we suggest generative models should incorporate\nadditional sources of subjective information in their training in order to\novercome (or at least to alleviate) some of their current limitations, thus\neffectively unleashing a truly artistic and creative generation.", "published": "2022-10-19 14:20:05", "link": "http://arxiv.org/abs/2210.10578v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-supervised Graph Masking Pre-training for Graph-to-Text Generation", "abstract": "Large-scale pre-trained language models (PLMs) have advanced Graph-to-Text\n(G2T) generation by processing the linearised version of a graph. However, the\nlinearisation is known to ignore the structural information. Additionally, PLMs\nare typically pre-trained on free text which introduces domain mismatch between\npre-training and downstream G2T generation tasks. To address these\nshortcomings, we propose graph masking pre-training strategies that neither\nrequire supervision signals nor adjust the architecture of the underlying\npre-trained encoder-decoder model. When used with a pre-trained T5, our\napproach achieves new state-of-the-art results on WebNLG+2020 and\nEventNarrative G2T generation datasets. Our method also shows to be very\neffective in the low-resource setting.", "published": "2022-10-19 14:44:56", "link": "http://arxiv.org/abs/2210.10599v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Arabic Word-level Readability Visualization for Assisted Text\n  Simplification", "abstract": "This demo paper presents a Google Docs add-on for automatic Arabic word-level\nreadability visualization. The add-on includes a lemmatization component that\nis connected to a five-level readability lexicon and Arabic WordNet-based\nsubstitution suggestions. The add-on can be used for assessing the reading\ndifficulty of a text and identifying difficult words as part of the task of\nmanual text simplification. We make our add-on and its code publicly available.", "published": "2022-10-19 15:37:55", "link": "http://arxiv.org/abs/2210.10672v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Procedural Fairness: Uncovering Biases in How a Toxic Language\n  Classifier Uses Sentiment Information", "abstract": "Previous works on the fairness of toxic language classifiers compare the\noutput of models with different identity terms as input features but do not\nconsider the impact of other important concepts present in the context. Here,\nbesides identity terms, we take into account high-level latent features learned\nby the classifier and investigate the interaction between these features and\nidentity terms. For a multi-class toxic language classifier, we leverage a\nconcept-based explanation framework to calculate the sensitivity of the model\nto the concept of sentiment, which has been used before as a salient feature\nfor toxic language detection. Our results show that although for some classes,\nthe classifier has learned the sentiment information as expected, this\ninformation is outweighed by the influence of identity terms as input features.\nThis work is a step towards evaluating procedural fairness, where unfair\nprocesses lead to unfair outcomes. The produced knowledge can guide debiasing\ntechniques to ensure that important concepts besides identity terms are\nwell-represented in training datasets.", "published": "2022-10-19 16:03:25", "link": "http://arxiv.org/abs/2210.10689v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Separating Grains from the Chaff: Using Data Filtering to Improve\n  Multilingual Translation for Low-Resourced African Languages", "abstract": "We participated in the WMT 2022 Large-Scale Machine Translation Evaluation\nfor the African Languages Shared Task. This work describes our approach, which\nis based on filtering the given noisy data using a sentence-pair classifier\nthat was built by fine-tuning a pre-trained language model. To train the\nclassifier, we obtain positive samples (i.e. high-quality parallel sentences)\nfrom a gold-standard curated dataset and extract negative samples (i.e.\nlow-quality parallel sentences) from automatically aligned parallel data by\nchoosing sentences with low alignment scores. Our final machine translation\nmodel was then trained on filtered data, instead of the entire noisy dataset.\nWe empirically validate our approach by evaluating on two common datasets and\nshow that data filtering generally improves overall translation quality, in\nsome cases even significantly.", "published": "2022-10-19 16:12:27", "link": "http://arxiv.org/abs/2210.10692v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniNL: Aligning Representation Learning with Scoring Function for OOD\n  Detection via Unified Neighborhood Learning", "abstract": "Detecting out-of-domain (OOD) intents from user queries is essential for\navoiding wrong operations in task-oriented dialogue systems. The key challenge\nis how to distinguish in-domain (IND) and OOD intents. Previous methods ignore\nthe alignment between representation learning and scoring function, limiting\nthe OOD detection performance. In this paper, we propose a unified neighborhood\nlearning framework (UniNL) to detect OOD intents. Specifically, we design a\nK-nearest neighbor contrastive learning (KNCL) objective for representation\nlearning and introduce a KNN-based scoring function for OOD detection. We aim\nto align representation learning with scoring function. Experiments and\nanalysis on two benchmark datasets show the effectiveness of our method.", "published": "2022-10-19 17:06:34", "link": "http://arxiv.org/abs/2210.10722v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Hate Speech Varies by Target Identity: A Computational Analysis", "abstract": "This paper investigates how hate speech varies in systematic ways according\nto the identities it targets. Across multiple hate speech datasets annotated\nfor targeted identities, we find that classifiers trained on hate speech\ntargeting specific identity groups struggle to generalize to other targeted\nidentities. This provides empirical evidence for differences in hate speech by\ntarget identity; we then investigate which patterns structure this variation.\nWe find that the targeted demographic category (e.g. gender/sexuality or\nrace/ethnicity) appears to have a greater effect on the language of hate speech\nthan does the relative social power of the targeted identity group. We also\nfind that words associated with hate speech targeting specific identities often\nrelate to stereotypes, histories of oppression, current social movements, and\nother social contexts specific to identities. These experiments suggest the\nimportance of considering targeted identity, as well as the social contexts\nassociated with these identities, in automated hate speech classification.", "published": "2022-10-19 19:06:23", "link": "http://arxiv.org/abs/2210.10839v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Two-Turn Debate Doesn't Help Humans Answer Hard Reading Comprehension\n  Questions", "abstract": "The use of language-model-based question-answering systems to aid humans in\ncompleting difficult tasks is limited, in part, by the unreliability of the\ntext these systems generate. Using hard multiple-choice reading comprehension\nquestions as a testbed, we assess whether presenting humans with arguments for\ntwo competing answer options, where one is correct and the other is incorrect,\nallows human judges to perform more accurately, even when one of the arguments\nis unreliable and deceptive. If this is helpful, we may be able to increase our\njustified trust in language-model-based systems by asking them to produce these\narguments where needed. Previous research has shown that just a single turn of\narguments in this format is not helpful to humans. However, as debate settings\nare characterized by a back-and-forth dialogue, we follow up on previous\nresults to test whether adding a second round of counter-arguments is helpful\nto humans. We find that, regardless of whether they have access to arguments or\nnot, humans perform similarly on our task. These findings suggest that, in the\ncase of answering reading comprehension questions, debate is not a helpful\nformat.", "published": "2022-10-19 19:48:50", "link": "http://arxiv.org/abs/2210.10860v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Detoxification with Attribute-Discriminative Latent Space", "abstract": "Transformer-based Language Models (LMs) have achieved impressive results on\nnatural language understanding tasks, but they can also generate toxic text\nsuch as insults, threats, and profanity, limiting their real-world\napplications. To overcome this issue, a few text generation approaches aim to\ndetoxify toxic texts using additional LMs or perturbations. However, previous\nmethods require excessive memory, computations, and time which are serious\nbottlenecks in their real-world application. To address such limitations, we\npropose an effective yet efficient method for language detoxification using an\nattribute-discriminative latent space. Specifically, we project the latent\nspace of an original Transformer LM onto a discriminative latent space that\nwell-separates texts by their attributes using a projection block and an\nattribute discriminator. This allows the LM to control the text generation to\nbe non-toxic with minimal memory and computation overhead. We validate our\nmodel, Attribute-Discriminative Language Model (ADLM) on detoxified language\nand dialogue generation tasks, on which our method significantly outperforms\nbaselines both in performance and efficiency.", "published": "2022-10-19 06:54:42", "link": "http://arxiv.org/abs/2210.10329v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Devil in Linear Transformer", "abstract": "Linear transformers aim to reduce the quadratic space-time complexity of\nvanilla transformers. However, they usually suffer from degraded performances\non various tasks and corpus. In this paper, we examine existing kernel-based\nlinear transformers and identify two key issues that lead to such performance\ngaps: 1) unbounded gradients in the attention computation adversely impact the\nconvergence of linear transformer models; 2) attention dilution which trivially\ndistributes attention scores over long sequences while neglecting neighbouring\nstructures. To address these issues, we first identify that the scaling of\nattention matrices is the devil in unbounded gradients, which turns out\nunnecessary in linear attention as we show theoretically and empirically. To\nthis end, we propose a new linear attention that replaces the scaling operation\nwith a normalization to stabilize gradients. For the issue of attention\ndilution, we leverage a diagonal attention to confine attention to only\nneighbouring tokens in early layers. Benefiting from the stable gradients and\nimproved attention, our new linear transformer model, transNormer, demonstrates\nsuperior performance on text classification and language modeling tasks, as\nwell as on the challenging Long-Range Arena benchmark, surpassing vanilla\ntransformer and existing linear variants by a clear margin while being\nsignificantly more space-time efficient. The code is available at\nhttps://github.com/OpenNLPLab/Transnormer .", "published": "2022-10-19 07:15:35", "link": "http://arxiv.org/abs/2210.10340v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text\n  Generation and Mining", "abstract": "Pre-trained language models have attracted increasing attention in the\nbiomedical domain, inspired by their great success in the general natural\nlanguage domain. Among the two main branches of pre-trained language models in\nthe general language domain, i.e., BERT (and its variants) and GPT (and its\nvariants), the first one has been extensively studied in the biomedical domain,\nsuch as BioBERT and PubMedBERT. While they have achieved great success on a\nvariety of discriminative downstream biomedical tasks, the lack of generation\nability constrains their application scope. In this paper, we propose BioGPT, a\ndomain-specific generative Transformer language model pre-trained on large\nscale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and\ndemonstrate that our model outperforms previous models on most tasks.\nEspecially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI\nend-to-end relation extraction tasks respectively, and 78.2% accuracy on\nPubMedQA, creating a new record. Our case study on text generation further\ndemonstrates the advantage of BioGPT on biomedical literature to generate\nfluent descriptions for biomedical terms. Code is available at\nhttps://github.com/microsoft/BioGPT.", "published": "2022-10-19 07:17:39", "link": "http://arxiv.org/abs/2210.10341v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Entity-to-Text based Data Augmentation for various Named Entity\n  Recognition Tasks", "abstract": "Data augmentation techniques have been used to alleviate the problem of\nscarce labeled data in various NER tasks (flat, nested, and discontinuous NER\ntasks). Existing augmentation techniques either manipulate the words in the\noriginal text that break the semantic coherence of the text, or exploit\ngenerative models that ignore preserving entities in the original text, which\nimpedes the use of augmentation techniques on nested and discontinuous NER\ntasks. In this work, we propose a novel Entity-to-Text based data augmentation\ntechnique named EnTDA to add, delete, replace or swap entities in the entity\nlist of the original texts, and adopt these augmented entity lists to generate\nsemantically coherent and entity preserving texts for various NER tasks.\nFurthermore, we introduce a diversity beam search to increase the diversity\nduring the text generation process. Experiments on thirteen NER datasets across\nthree tasks (flat, nested, and discontinuous NER tasks) and two settings (full\ndata and low resource settings) show that EnTDA could bring more performance\nimprovements compared to the baseline augmentation techniques.", "published": "2022-10-19 07:24:40", "link": "http://arxiv.org/abs/2210.10343v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Group is better than individual: Exploiting Label Topologies and Label\n  Relations for Joint Multiple Intent Detection and Slot Filling", "abstract": "Recent joint multiple intent detection and slot filling models employ label\nembeddings to achieve the semantics-label interactions. However, they treat all\nlabels and label embeddings as uncorrelated individuals, ignoring the\ndependencies among them. Besides, they conduct the decoding for the two tasks\nindependently, without leveraging the correlations between them. Therefore, in\nthis paper, we first construct a Heterogeneous Label Graph (HLG) containing two\nkinds of topologies: (1) statistical dependencies based on labels'\nco-occurrence patterns and hierarchies in slot labels; (2) rich relations among\nthe label nodes. Then we propose a novel model termed ReLa-Net. It can capture\nbeneficial correlations among the labels from HLG. The label correlations are\nleveraged to enhance semantic-label interactions. Moreover, we also propose the\nlabel-aware inter-dependent decoding mechanism to further exploit the label\ncorrelations for decoding. Experiment results show that our ReLa-Net\nsignificantly outperforms previous models. Remarkably, ReLa-Net surpasses the\nprevious best model by over 20\\% in terms of overall accuracy on MixATIS\ndataset.", "published": "2022-10-19 08:21:43", "link": "http://arxiv.org/abs/2210.10369v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Co-guiding Net: Achieving Mutual Guidances between Multiple Intent\n  Detection and Slot Filling via Heterogeneous Semantics-Label Graphs", "abstract": "Recent graph-based models for joint multiple intent detection and slot\nfilling have obtained promising results through modeling the guidance from the\nprediction of intents to the decoding of slot filling. However, existing\nmethods (1) only model the \\textit{unidirectional guidance} from intent to\nslot; (2) adopt \\textit{homogeneous graphs} to model the interactions between\nthe slot semantics nodes and intent label nodes, which limit the performance.\nIn this paper, we propose a novel model termed Co-guiding Net, which implements\na two-stage framework achieving the \\textit{mutual guidances} between the two\ntasks. In the first stage, the initial estimated labels of both tasks are\nproduced, and then they are leveraged in the second stage to model the mutual\nguidances. Specifically, we propose two \\textit{heterogeneous graph attention\nnetworks} working on the proposed two \\textit{heterogeneous semantics-label\ngraphs}, which effectively represent the relations among the semantics nodes\nand label nodes. Experiment results show that our model outperforms existing\nmodels by a large margin, obtaining a relative improvement of 19.3\\% over the\nprevious best model on MixATIS dataset in overall accuracy.", "published": "2022-10-19 08:34:51", "link": "http://arxiv.org/abs/2210.10375v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LightEA: A Scalable, Robust, and Interpretable Entity Alignment\n  Framework via Three-view Label Propagation", "abstract": "Entity Alignment (EA) aims to find equivalent entity pairs between KGs, which\nis the core step of bridging and integrating multi-source KGs. In this paper,\nwe argue that existing GNN-based EA methods inherit the inborn defects from\ntheir neural network lineage: weak scalability and poor interpretability.\nInspired by recent studies, we reinvent the Label Propagation algorithm to\neffectively run on KGs and propose a non-neural EA framework -- LightEA,\nconsisting of three efficient components: (i) Random Orthogonal Label\nGeneration, (ii) Three-view Label Propagation, and (iii) Sparse Sinkhorn\nIteration. According to the extensive experiments on public datasets, LightEA\nhas impressive scalability, robustness, and interpretability. With a mere tenth\nof time consumption, LightEA achieves comparable results to state-of-the-art\nmethods across all datasets and even surpasses them on many.", "published": "2022-10-19 10:07:08", "link": "http://arxiv.org/abs/2210.10436v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Attribution and Obfuscation of Neural Text Authorship: A Data Mining\n  Perspective", "abstract": "Two interlocking research questions of growing interest and importance in\nprivacy research are Authorship Attribution (AA) and Authorship Obfuscation\n(AO). Given an artifact, especially a text t in question, an AA solution aims\nto accurately attribute t to its true author out of many candidate authors\nwhile an AO solution aims to modify t to hide its true authorship.\nTraditionally, the notion of authorship and its accompanying privacy concern is\nonly toward human authors. However, in recent years, due to the explosive\nadvancements in Neural Text Generation (NTG) techniques in NLP, capable of\nsynthesizing human-quality open-ended texts (so-called \"neural texts\"), one has\nto now consider authorships by humans, machines, or their combination. Due to\nthe implications and potential threats of neural texts when used maliciously,\nit has become critical to understand the limitations of traditional AA/AO\nsolutions and develop novel AA/AO solutions in dealing with neural texts. In\nthis survey, therefore, we make a comprehensive review of recent literature on\nthe attribution and obfuscation of neural text authorship from a Data Mining\nperspective, and share our view on their limitations and promising research\ndirections.", "published": "2022-10-19 11:53:13", "link": "http://arxiv.org/abs/2210.10488v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CEntRE: A paragraph-level Chinese dataset for Relation Extraction among\n  Enterprises", "abstract": "Enterprise relation extraction aims to detect pairs of enterprise entities\nand identify the business relations between them from unstructured or\nsemi-structured text data, and it is crucial for several real-world\napplications such as risk analysis, rating research and supply chain security.\nHowever, previous work mainly focuses on getting attribute information about\nenterprises like personnel and corporate business, and pays little attention to\nenterprise relation extraction. To encourage further progress in the research,\nwe introduce the CEntRE, a new dataset constructed from publicly available\nbusiness news data with careful human annotation and intelligent data\nprocessing. Extensive experiments on CEntRE with six excellent models\ndemonstrate the challenges of our proposed dataset.", "published": "2022-10-19 14:22:10", "link": "http://arxiv.org/abs/2210.10581v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "NGEP: A Graph-based Event Planning Framework for Story Generation", "abstract": "To improve the performance of long text generation, recent studies have\nleveraged automatically planned event structures (i.e. storylines) to guide\nstory generation. Such prior works mostly employ end-to-end neural generation\nmodels to predict event sequences for a story. However, such generation models\nstruggle to guarantee the narrative coherence of separate events due to the\nhallucination problem, and additionally the generated event sequences are often\nhard to control due to the end-to-end nature of the models. To address these\nchallenges, we propose NGEP, an novel event planning framework which generates\nan event sequence by performing inference on an automatically constructed event\ngraph and enhances generalisation ability through a neural event advisor. We\nconduct a range of experiments on multiple criteria, and the results\ndemonstrate that our graph-based neural framework outperforms the\nstate-of-the-art (SOTA) event planning approaches, considering both the\nperformance of event sequence generation and the effectiveness on the\ndownstream task of story generation.", "published": "2022-10-19 14:49:27", "link": "http://arxiv.org/abs/2210.10602v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DALLE-2 is Seeing Double: Flaws in Word-to-Concept Mapping in Text2Image\n  Models", "abstract": "We study the way DALLE-2 maps symbols (words) in the prompt to their\nreferences (entities or properties of entities in the generated image). We show\nthat in stark contrast to the way human process language, DALLE-2 does not\nfollow the constraint that each word has a single role in the interpretation,\nand sometimes re-use the same symbol for different purposes. We collect a set\nof stimuli that reflect the phenomenon: we show that DALLE-2 depicts both\nsenses of nouns with multiple senses at once; and that a given word can modify\nthe properties of two distinct entities in the image, or can be depicted as one\nobject and also modify the properties of another object, creating a semantic\nleakage of properties between entities. Taken together, our study highlights\nthe differences between DALLE-2 and human language processing and opens an\navenue for future study on the inductive biases of text-to-image models.", "published": "2022-10-19 14:52:40", "link": "http://arxiv.org/abs/2210.10606v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Chinese Story Generation via Awareness of Syntactic\n  Dependencies and Semantics", "abstract": "Story generation aims to generate a long narrative conditioned on a given\ninput. In spite of the success of prior works with the application of\npre-trained models, current neural models for Chinese stories still struggle to\ngenerate high-quality long text narratives. We hypothesise that this stems from\nambiguity in syntactically parsing the Chinese language, which does not have\nexplicit delimiters for word segmentation. Consequently, neural models suffer\nfrom the inefficient capturing of features in Chinese narratives. In this\npaper, we present a new generation framework that enhances the feature\ncapturing mechanism by informing the generation model of dependencies between\nwords and additionally augmenting the semantic representation learning through\nsynonym denoising training. We conduct a range of experiments, and the results\ndemonstrate that our framework outperforms the state-of-the-art Chinese\ngeneration models on all evaluation metrics, demonstrating the benefits of\nenhanced dependency and semantic representation learning.", "published": "2022-10-19 15:01:52", "link": "http://arxiv.org/abs/2210.10618v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "N-Best Hypotheses Reranking for Text-To-SQL Systems", "abstract": "Text-to-SQL task maps natural language utterances to structured queries that\ncan be issued to a database. State-of-the-art (SOTA) systems rely on finetuning\nlarge, pre-trained language models in conjunction with constrained decoding\napplying a SQL parser. On the well established Spider dataset, we begin with\nOracle studies: specifically, choosing an Oracle hypothesis from a SOTA model's\n10-best list, yields a $7.7\\%$ absolute improvement in both exact match (EM)\nand execution (EX) accuracy, showing significant potential improvements with\nreranking. Identifying coherence and correctness as reranking approaches, we\ndesign a model generating a query plan and propose a heuristic schema linking\nalgorithm. Combining both approaches, with T5-Large, we obtain a consistent\n$1\\% $ improvement in EM accuracy, and a $~2.5\\%$ improvement in EX,\nestablishing a new SOTA for this task. Our comprehensive error studies on DEV\ndata show the underlying difficulty in making progress on this task.", "published": "2022-10-19 15:35:06", "link": "http://arxiv.org/abs/2210.10668v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Models Understand Us, Poorly", "abstract": "Some claim language models understand us. Others won't hear it. To clarify, I\ninvestigate three views of human language understanding: as-mapping,\nas-reliability and as-representation. I argue that while behavioral reliability\nis necessary for understanding, internal representations are sufficient; they\nclimb the right hill. I review state-of-the-art language and multi-modal\nmodels: they are pragmatically challenged by under-specification of form. I\nquestion the Scaling Paradigm: limits on resources may prohibit scaled-up\nmodels from approaching understanding. Last, I describe how as-representation\nadvances a science of understanding. We need work which probes model internals,\nadds more of human language, and measures what models can learn.", "published": "2022-10-19 15:58:59", "link": "http://arxiv.org/abs/2210.10684v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Incorporating Relevance Feedback for Information-Seeking Retrieval using\n  Few-Shot Document Re-Ranking", "abstract": "Pairing a lexical retriever with a neural re-ranking model has set\nstate-of-the-art performance on large-scale information retrieval datasets.\nThis pipeline covers scenarios like question answering or navigational queries,\nhowever, for information-seeking scenarios, users often provide information on\nwhether a document is relevant to their query in form of clicks or explicit\nfeedback. Therefore, in this work, we explore how relevance feedback can be\ndirectly integrated into neural re-ranking models by adopting few-shot and\nparameter-efficient learning techniques. Specifically, we introduce a kNN\napproach that re-ranks documents based on their similarity with the query and\nthe documents the user considers relevant. Further, we explore Cross-Encoder\nmodels that we pre-train using meta-learning and subsequently fine-tune for\neach query, training only on the feedback documents. To evaluate our different\nintegration strategies, we transform four existing information retrieval\ndatasets into the relevance feedback scenario. Extensive experiments\ndemonstrate that integrating relevance feedback directly in neural re-ranking\nmodels improves their performance, and fusing lexical ranking with our best\nperforming neural re-ranker outperforms all other methods by 5.2 nDCG@20.", "published": "2022-10-19 16:19:37", "link": "http://arxiv.org/abs/2210.10695v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "TabLLM: Few-shot Classification of Tabular Data with Large Language\n  Models", "abstract": "We study the application of large language models to zero-shot and few-shot\nclassification of tabular data. We prompt the large language model with a\nserialization of the tabular data to a natural-language string, together with a\nshort description of the classification problem. In the few-shot setting, we\nfine-tune the large language model using some labeled examples. We evaluate\nseveral serialization methods including templates, table-to-text models, and\nlarge language models. Despite its simplicity, we find that this technique\noutperforms prior deep-learning-based tabular classification methods on several\nbenchmark datasets. In most cases, even zero-shot classification obtains\nnon-trivial performance, illustrating the method's ability to exploit prior\nknowledge encoded in large language models. Unlike many deep learning methods\nfor tabular datasets, this approach is also competitive with strong traditional\nbaselines like gradient-boosted trees, especially in the very-few-shot setting.", "published": "2022-10-19 17:08:13", "link": "http://arxiv.org/abs/2210.10723v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Continuum of Generation Tasks for Investigating Length Bias and\n  Degenerate Repetition", "abstract": "Language models suffer from various degenerate behaviors. These differ\nbetween tasks: machine translation (MT) exhibits length bias, while tasks like\nstory generation exhibit excessive repetition. Recent work has attributed the\ndifference to task constrainedness, but evidence for this claim has always\ninvolved many confounding variables. To study this question directly, we\nintroduce a new experimental framework that allows us to smoothly vary task\nconstrainedness, from MT at one end to fully open-ended generation at the\nother, while keeping all other aspects fixed. We find that: (1) repetition\ndecreases smoothly with constrainedness, explaining the difference in\nrepetition across tasks; (2) length bias surprisingly also decreases with\nconstrainedness, suggesting some other cause for the difference in length bias;\n(3) across the board, these problems affect the mode, not the whole\ndistribution; (4) the differences cannot be attributed to a change in the\nentropy of the distribution, since another method of changing the entropy,\nlabel smoothing, does not produce the same effect.", "published": "2022-10-19 18:09:51", "link": "http://arxiv.org/abs/2210.10817v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prompting through Prototype: A Prototype-based Prompt Learning on\n  Pretrained Vision-Language Models", "abstract": "Prompt learning is a new learning paradigm which reformulates downstream\ntasks as similar pretraining tasks on pretrained models by leveraging textual\nprompts. Recent works have demonstrated that prompt learning is particularly\nuseful for few-shot learning, where there is limited training data. Depending\non the granularity of prompts, those methods can be roughly divided into\ntask-level prompting and instance-level prompting. Task-level prompting methods\nlearn one universal prompt for all input samples, which is efficient but\nineffective to capture subtle differences among different classes.\nInstance-level prompting methods learn a specific prompt for each input, though\neffective but inefficient. In this work, we develop a novel prototype-based\nprompt learning method to overcome the above limitations. In particular, we\nfocus on few-shot image recognition tasks on pretrained vision-language models\n(PVLMs) and develop a method of prompting through prototype (PTP), where we\ndefine $K$ image prototypes and $K$ prompt prototypes. In PTP, the image\nprototype represents a centroid of a certain image cluster in the latent space\nand a prompt prototype is defined as a soft prompt in the continuous space. The\nsimilarity between a query image and an image prototype determines how much\nthis prediction relies on the corresponding prompt prototype. Hence, in PTP,\nsimilar images will utilize similar prompting ways. Through extensive\nexperiments on seven real-world benchmarks, we show that PTP is an effective\nmethod to leverage the latent knowledge and adaptive to various PVLMs.\nMoreover, through detailed analysis, we discuss pros and cons for prompt\nlearning and parameter-efficient fine-tuning under the context of few-shot\nlearning.", "published": "2022-10-19 19:13:07", "link": "http://arxiv.org/abs/2210.10841v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "QA Domain Adaptation using Hidden Space Augmentation and Self-Supervised\n  Contrastive Adaptation", "abstract": "Question answering (QA) has recently shown impressive results for answering\nquestions from customized domains. Yet, a common challenge is to adapt QA\nmodels to an unseen target domain. In this paper, we propose a novel\nself-supervised framework called QADA for QA domain adaptation. QADA introduces\na novel data augmentation pipeline used to augment training QA samples.\nDifferent from existing methods, we enrich the samples via hidden space\naugmentation. For questions, we introduce multi-hop synonyms and sample\naugmented token embeddings with Dirichlet distributions. For contexts, we\ndevelop an augmentation method which learns to drop context spans via a custom\nattentive sampling strategy. Additionally, contrastive learning is integrated\nin the proposed self-supervised adaptation framework QADA. Unlike existing\napproaches, we generate pseudo labels and propose to train the model via a\nnovel attention-based contrastive adaptation method. The attention weights are\nused to build informative features for discrepancy estimation that helps the QA\nmodel separate answers and generalize across source and target domains. To the\nbest of our knowledge, our work is the first to leverage hidden space\naugmentation and attention-based contrastive adaptation for self-supervised\ndomain adaptation in QA. Our evaluation shows that QADA achieves considerable\nimprovements on multiple target datasets over state-of-the-art baselines in QA\ndomain adaptation.", "published": "2022-10-19 19:52:57", "link": "http://arxiv.org/abs/2210.10861v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A baseline revisited: Pushing the limits of multi-segment models for\n  context-aware translation", "abstract": "This paper addresses the task of contextual translation using multi-segment\nmodels. Specifically we show that increasing model capacity further pushes the\nlimits of this approach and that deeper models are more suited to capture\ncontext dependencies. Furthermore, improvements observed with larger models can\nbe transferred to smaller models using knowledge distillation. Our experiments\nshow that this approach achieves competitive performance across several\nlanguages and benchmarks, without additional language-specific tuning and task\nspecific architectures.", "published": "2022-10-19 22:04:25", "link": "http://arxiv.org/abs/2210.10906v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prophet Attention: Predicting Attention with Future Attention for Image\n  Captioning", "abstract": "Recently, attention based models have been used extensively in many\nsequence-to-sequence learning systems. Especially for image captioning, the\nattention based models are expected to ground correct image regions with proper\ngenerated words. However, for each time step in the decoding process, the\nattention based models usually use the hidden state of the current input to\nattend to the image regions. Under this setting, these attention models have a\n\"deviated focus\" problem that they calculate the attention weights based on\nprevious words instead of the one to be generated, impairing the performance of\nboth grounding and captioning. In this paper, we propose the Prophet Attention,\nsimilar to the form of self-supervision. In the training stage, this module\nutilizes the future information to calculate the \"ideal\" attention weights\ntowards image regions. These calculated \"ideal\" weights are further used to\nregularize the \"deviated\" attention. In this manner, image regions are grounded\nwith the correct words. The proposed Prophet Attention can be easily\nincorporated into existing image captioning models to improve their performance\nof both grounding and captioning. The experiments on the Flickr30k Entities and\nthe MSCOCO datasets show that the proposed Prophet Attention consistently\noutperforms baselines in both automatic metrics and human evaluations. It is\nworth noticing that we set new state-of-the-arts on the two benchmark datasets\nand achieve the 1st place on the leaderboard of the online MSCOCO benchmark in\nterms of the default ranking score, i.e., CIDEr-c40.", "published": "2022-10-19 22:29:31", "link": "http://arxiv.org/abs/2210.10914v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Image Semantic Relation Generation", "abstract": "Scene graphs provide structured semantic understanding beyond images. For\ndownstream tasks, such as image retrieval, visual question answering, visual\nrelationship detection, and even autonomous vehicle technology, scene graphs\ncan not only distil complex image information but also correct the bias of\nvisual models using semantic-level relations, which has broad application\nprospects. However, the heavy labour cost of constructing graph annotations may\nhinder the application of PSG in practical scenarios. Inspired by the\nobservation that people usually identify the subject and object first and then\ndetermine the relationship between them, we proposed to decouple the scene\ngraphs generation task into two sub-tasks: 1) an image segmentation task to\npick up the qualified objects. 2) a restricted auto-regressive text generation\ntask to generate the relation between given objects. Therefore, in this work,\nwe introduce image semantic relation generation (ISRG), a simple but effective\nimage-to-text model, which achieved 31 points on the OpenPSG dataset and\noutperforms strong baselines respectively by 16 points (ResNet-50) and 5 points\n(CLIP).", "published": "2022-10-19 16:15:19", "link": "http://arxiv.org/abs/2210.11253v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "LVP-M3: Language-aware Visual Prompt for Multilingual Multimodal Machine\n  Translation", "abstract": "Multimodal Machine Translation (MMT) focuses on enhancing text-only\ntranslation with visual features, which has attracted considerable attention\nfrom both natural language processing and computer vision communities. Recent\nadvances still struggle to train a separate model for each language pair, which\nis costly and unaffordable when the number of languages increases in the real\nworld. In other words, the multilingual multimodal machine translation\n(Multilingual MMT) task has not been investigated, which aims to handle the\naforementioned issues by providing a shared semantic space for multiple\nlanguages. Besides, the image modality has no language boundaries, which is\nsuperior to bridging the semantic gap between languages. To this end, we first\npropose the Multilingual MMT task by establishing two new Multilingual MMT\nbenchmark datasets covering seven languages. Then, an effective baseline LVP-M3\nusing visual prompts is proposed to support translations between different\nlanguages, which includes three stages (token encoding, language-aware visual\nprompt generation, and language translation). Extensive experimental results on\nour constructed benchmark datasets demonstrate the effectiveness of LVP-M3\nmethod for Multilingual MMT.", "published": "2022-10-19 12:21:39", "link": "http://arxiv.org/abs/2210.15461v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Explainable Slot Type Attentions to Improve Joint Intent Detection and\n  Slot Filling", "abstract": "Joint intent detection and slot filling is a key research topic in natural\nlanguage understanding (NLU). Existing joint intent and slot filling systems\nanalyze and compute features collectively for all slot types, and importantly,\nhave no way to explain the slot filling model decisions. In this work, we\npropose a novel approach that: (i) learns to generate additional slot type\nspecific features in order to improve accuracy and (ii) provides explanations\nfor slot filling decisions for the first time in a joint NLU model. We perform\nan additional constrained supervision using a set of binary classifiers for the\nslot type specific feature learning, thus ensuring appropriate attention\nweights are learned in the process to explain slot filling decisions for\nutterances. Our model is inherently explainable and does not need any post-hoc\nprocessing. We evaluate our approach on two widely used datasets and show\naccuracy improvements. Moreover, a detailed analysis is also provided for the\nexclusive slot explainability.", "published": "2022-10-19 00:56:10", "link": "http://arxiv.org/abs/2210.10227v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Speaker- and Age-Invariant Training for Child Acoustic Modeling Using\n  Adversarial Multi-Task Learning", "abstract": "One of the major challenges in acoustic modelling of child speech is the\nrapid changes that occur in the children's articulators as they grow up, their\ndiffering growth rates and the subsequent high variability in the same age\ngroup. These high acoustic variations along with the scarcity of child speech\ncorpora have impeded the development of a reliable speech recognition system\nfor children. In this paper, a speaker- and age-invariant training approach\nbased on adversarial multi-task learning is proposed. The system consists of\none generator shared network that learns to generate speaker- and age-invariant\nfeatures connected to three discrimination networks, for phoneme, age, and\nspeaker. The generator network is trained to minimize the\nphoneme-discrimination loss and maximize the speaker- and age-discrimination\nlosses in an adversarial multi-task learning fashion. The generator network is\na Time Delay Neural Network (TDNN) architecture while the three discriminators\nare feed-forward networks. The system was applied to the OGI speech corpora and\nachieved a 13% reduction in the WER of the ASR.", "published": "2022-10-19 01:17:40", "link": "http://arxiv.org/abs/2210.10231v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Data-Driven Investigation of Noise-Adaptive Utterance Generation with\n  Linguistic Modification", "abstract": "In noisy environments, speech can be hard to understand for humans. Spoken\ndialog systems can help to enhance the intelligibility of their output, either\nby modifying the speech synthesis (e.g., imitate Lombard speech) or by\noptimizing the language generation. We here focus on the second type of\napproach, by which an intended message is realized with words that are more\nintelligible in a specific noisy environment. By conducting a speech perception\nexperiment, we created a dataset of 900 paraphrases in babble noise, perceived\nby native English speakers with normal hearing. We find that careful selection\nof paraphrases can improve intelligibility by 33% at SNR -5 dB. Our analysis of\nthe data shows that the intelligibility differences between paraphrases are\nmainly driven by noise-robust acoustic cues. Furthermore, we propose an\nintelligibility-aware paraphrase ranking model, which outperforms baseline\nmodels with a relative improvement of 31.37% at SNR -5 dB.", "published": "2022-10-19 02:20:17", "link": "http://arxiv.org/abs/2210.10252v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Language Model Decomposition: Quantifying the Dependency and Correlation\n  of Language Models", "abstract": "Pre-trained language models (LMs), such as BERT (Devlin et al., 2018) and its\nvariants, have led to significant improvements on various NLP tasks in past\nyears. However, a theoretical framework for studying their relationships is\nstill missing. In this paper, we fill this gap by investigating the linear\ndependency between pre-trained LMs. The linear dependency of LMs is defined\nanalogously to the linear dependency of vectors. We propose Language Model\nDecomposition (LMD) to represent a LM using a linear combination of other LMs\nas basis, and derive the closed-form solution. A goodness-of-fit metric for LMD\nsimilar to the coefficient of determination is defined and used to measure the\nlinear dependency of a set of LMs. In experiments, we find that BERT and eleven\n(11) BERT-like LMs are 91% linearly dependent. This observation suggests that\ncurrent state-of-the-art (SOTA) LMs are highly \"correlated\". To further advance\nSOTA we need more diverse and novel LMs that are less dependent on existing\nLMs.", "published": "2022-10-19 04:28:19", "link": "http://arxiv.org/abs/2210.10289v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 (Primary) 68T30, 68T07 (Secondary)", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Revision Transformers: Instructing Language Models to Change their\n  Values", "abstract": "Current transformer language models (LM) are large-scale models with billions\nof parameters. They have been shown to provide high performances on a variety\nof tasks but are also prone to shortcut learning and bias. Addressing such\nincorrect model behavior via parameter adjustments is very costly. This is\nparticularly problematic for updating dynamic concepts, such as moral values,\nwhich vary culturally or interpersonally. In this work, we question the current\ncommon practice of storing all information in the model parameters and propose\nthe Revision Transformer (RiT) to facilitate easy model updating. The specific\ncombination of a large-scale pre-trained LM that inherently but also diffusely\nencodes world knowledge with a clear-structured revision engine makes it\npossible to update the model's knowledge with little effort and the help of\nuser interaction. We exemplify RiT on a moral dataset and simulate user\nfeedback demonstrating strong performance in model revision even with small\ndata. This way, users can easily design a model regarding their preferences,\npaving the way for more transparent AI models.", "published": "2022-10-19 07:05:06", "link": "http://arxiv.org/abs/2210.10332v3", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "CPL: Counterfactual Prompt Learning for Vision and Language Models", "abstract": "Prompt tuning is a new few-shot transfer learning technique that only tunes\nthe learnable prompt for pre-trained vision and language models such as CLIP.\nHowever, existing prompt tuning methods tend to learn spurious or entangled\nrepresentations, which leads to poor generalization to unseen concepts. Towards\nnon-spurious and efficient prompt learning from limited examples, this paper\npresents a novel \\underline{\\textbf{C}}ounterfactual\n\\underline{\\textbf{P}}rompt \\underline{\\textbf{L}}earning (CPL) method for\nvision and language models, which simultaneously employs counterfactual\ngeneration and contrastive learning in a joint optimization framework.\nParticularly, CPL constructs counterfactual by identifying minimal non-spurious\nfeature change between semantically-similar positive and negative samples that\ncauses concept change, and learns more generalizable prompt representation from\nboth factual and counterfactual examples via contrastive learning. Extensive\nexperiments demonstrate that CPL can obtain superior few-shot performance on\ndifferent vision and language tasks than previous prompt tuning methods on\nCLIP. On image classification, we achieve 3.55\\% average relative improvement\non unseen classes across seven datasets; on image-text retrieval and visual\nquestion answering, we gain up to 4.09\\% and 25.08\\% relative improvements\nacross three few-shot scenarios on unseen test sets respectively.", "published": "2022-10-19 08:06:39", "link": "http://arxiv.org/abs/2210.10362v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Towards a neural architecture of language: Deep learning versus\n  logistics of access in neural architectures for compositional processing", "abstract": "Recently, a number of articles have argued that deep learning models such as\nGPT could also capture key aspects of language processing in the human mind and\nbrain. However, I will argue that these models are not suitable as neural\nmodels of human language. Firstly, because they fail on fundamental boundary\nconditions, such as the amount of learning they require. This would in fact\nimply that the mechanisms of GPT and brain language processing are\nfundamentally different. Secondly, because they do not possess the logistics of\naccess needed for compositional and productive human language processing.\nNeural architectures could possess logistics of access based on small-world\nlike network structures, in which processing does not consist of symbol\nmanipulation but of controlling the flow of activation. In this view, two\ncomplementary approaches would be needed to investigate the relation between\nbrain and cognition. Investigating learning methods could reveal how 'learned\ncognition' as found in deep learning could develop in the brain. However,\nneural architectures with logistics of access should also be developed to\naccount for 'productive cognition' as required for natural or artificial human\nlanguage processing. Later on, these approaches could perhaps be combined to\nsee how such architectures could develop by learning and development from a\nsimpler basis.", "published": "2022-10-19 13:31:26", "link": "http://arxiv.org/abs/2210.10543v1", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Towards Realistic Low-resource Relation Extraction: A Benchmark with\n  Empirical Baseline Study", "abstract": "This paper presents an empirical study to build relation extraction systems\nin low-resource settings. Based upon recent pre-trained language models, we\ncomprehensively investigate three schemes to evaluate the performance in\nlow-resource settings: (i) different types of prompt-based methods with\nfew-shot labeled data; (ii) diverse balancing methods to address the\nlong-tailed distribution issue; (iii) data augmentation technologies and\nself-training to generate more labeled in-domain data. We create a benchmark\nwith 8 relation extraction (RE) datasets covering different languages, domains\nand contexts and perform extensive comparisons over the proposed schemes with\ncombinations. Our experiments illustrate: (i) Though prompt-based tuning is\nbeneficial in low-resource RE, there is still much potential for improvement,\nespecially in extracting relations from cross-sentence contexts with multiple\nrelational triples; (ii) Balancing methods are not always helpful for RE with\nlong-tailed distribution; (iii) Data augmentation complements existing\nbaselines and can bring much performance gain, while self-training may not\nconsistently achieve advancement to low-resource RE. Code and datasets are in\nhttps://github.com/zjunlp/LREBench.", "published": "2022-10-19 15:46:37", "link": "http://arxiv.org/abs/2210.10678v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Why Should Adversarial Perturbations be Imperceptible? Rethink the\n  Research Paradigm in Adversarial NLP", "abstract": "Textual adversarial samples play important roles in multiple subfields of NLP\nresearch, including security, evaluation, explainability, and data\naugmentation. However, most work mixes all these roles, obscuring the problem\ndefinitions and research goals of the security role that aims to reveal the\npractical concerns of NLP models. In this paper, we rethink the research\nparadigm of textual adversarial samples in security scenarios. We discuss the\ndeficiencies in previous work and propose our suggestions that the research on\nthe Security-oriented adversarial NLP (SoadNLP) should: (1) evaluate their\nmethods on security tasks to demonstrate the real-world concerns; (2) consider\nreal-world attackers' goals, instead of developing impractical methods. To this\nend, we first collect, process, and release a security datasets collection\nAdvbench. Then, we reformalize the task and adjust the emphasis on different\ngoals in SoadNLP. Next, we propose a simple method based on heuristic rules\nthat can easily fulfill the actual adversarial goals to simulate real-world\nattack methods. We conduct experiments on both the attack and the defense sides\non Advbench. Experimental results show that our method has higher practical\nvalue, indicating that the research paradigm in SoadNLP may start from our new\nbenchmark. All the code and data of Advbench can be obtained at\n\\url{https://github.com/thunlp/Advbench}.", "published": "2022-10-19 15:53:36", "link": "http://arxiv.org/abs/2210.10683v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Robustness of Demonstration-based Learning Under Limited Data Scenario", "abstract": "Demonstration-based learning has shown great potential in stimulating\npretrained language models' ability under limited data scenario. Simply\naugmenting the input with some demonstrations can significantly improve\nperformance on few-shot NER. However, why such demonstrations are beneficial\nfor the learning process remains unclear since there is no explicit alignment\nbetween the demonstrations and the predictions. In this paper, we design\npathological demonstrations by gradually removing intuitively useful\ninformation from the standard ones to take a deep dive of the robustness of\ndemonstration-based sequence labeling and show that (1) demonstrations composed\nof random tokens still make the model a better few-shot learner; (2) the length\nof random demonstrations and the relevance of random tokens are the main\nfactors affecting the performance; (3) demonstrations increase the confidence\nof model predictions on captured superficial patterns. We have publicly\nreleased our code at https://github.com/SALT-NLP/RobustDemo.", "published": "2022-10-19 16:15:04", "link": "http://arxiv.org/abs/2210.10693v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VTC: Improving Video-Text Retrieval with User Comments", "abstract": "Multi-modal retrieval is an important problem for many applications, such as\nrecommendation and search. Current benchmarks and even datasets are often\nmanually constructed and consist of mostly clean samples where all modalities\nare well-correlated with the content. Thus, current video-text retrieval\nliterature largely focuses on video titles or audio transcripts, while ignoring\nuser comments, since users often tend to discuss topics only vaguely related to\nthe video. Despite the ubiquity of user comments online, there is currently no\nmulti-modal representation learning datasets that includes comments. In this\npaper, we a) introduce a new dataset of videos, titles and comments; b) present\nan attention-based mechanism that allows the model to learn from sometimes\nirrelevant data such as comments; c) show that by using comments, our method is\nable to learn better, more contextualised, representations for image, video and\naudio representations. Project page: https://unitaryai.github.io/vtc-paper.", "published": "2022-10-19 18:11:39", "link": "http://arxiv.org/abs/2210.10820v1", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "G-Augment: Searching for the Meta-Structure of Data Augmentation\n  Policies for ASR", "abstract": "Data augmentation is a ubiquitous technique used to provide robustness to\nautomatic speech recognition (ASR) training. However, even as so much of the\nASR training process has become automated and more \"end-to-end\", the data\naugmentation policy (what augmentation functions to use, and how to apply them)\nremains hand-crafted. We present Graph-Augment, a technique to define the\naugmentation space as directed acyclic graphs (DAGs) and search over this space\nto optimize the augmentation policy itself. We show that given the same\ncomputational budget, policies produced by G-Augment are able to perform better\nthan SpecAugment policies obtained by random search on fine-tuning tasks on\nCHiME-6 and AMI. G-Augment is also able to establish a new state-of-the-art ASR\nperformance on the CHiME-6 evaluation set (30.7% WER). We further demonstrate\nthat G-Augment policies show better transfer properties across warm-start to\ncold-start training and model size compared to random-searched SpecAugment\npolicies.", "published": "2022-10-19 20:39:40", "link": "http://arxiv.org/abs/2210.10879v2", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Weakly Supervised Learning for Analyzing Political Campaigns on Facebook", "abstract": "Social media platforms are currently the main channel for political\nmessaging, allowing politicians to target specific demographics and adapt based\non their reactions. However, making this communication transparent is\nchallenging, as the messaging is tightly coupled with its intended audience and\noften echoed by multiple stakeholders interested in advancing specific\npolicies. Our goal in this paper is to take a first step towards understanding\nthese highly decentralized settings. We propose a weakly supervised approach to\nidentify the stance and issue of political ads on Facebook and analyze how\npolitical campaigns use some kind of demographic targeting by location, gender,\nor age. Furthermore, we analyze the temporal dynamics of the political ads on\nelection polls.", "published": "2022-10-19 15:35:37", "link": "http://arxiv.org/abs/2210.10669v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Schema-aware Reference as Prompt Improves Data-Efficient Knowledge Graph\n  Construction", "abstract": "With the development of pre-trained language models, many prompt-based\napproaches to data-efficient knowledge graph construction have been proposed\nand achieved impressive performance. However, existing prompt-based learning\nmethods for knowledge graph construction are still susceptible to several\npotential limitations: (i) semantic gap between natural language and output\nstructured knowledge with pre-defined schema, which means model cannot fully\nexploit semantic knowledge with the constrained templates; (ii) representation\nlearning with locally individual instances limits the performance given the\ninsufficient features, which are unable to unleash the potential analogical\ncapability of pre-trained language models. Motivated by these observations, we\npropose a retrieval-augmented approach, which retrieves schema-aware Reference\nAs Prompt (RAP), for data-efficient knowledge graph construction. It can\ndynamically leverage schema and knowledge inherited from human-annotated and\nweak-supervised data as a prompt for each sample, which is model-agnostic and\ncan be plugged into widespread existing approaches. Experimental results\ndemonstrate that previous methods integrated with RAP can achieve impressive\nperformance gains in low-resource settings on five datasets of relational\ntriple extraction and event extraction for knowledge graph construction. Code\nis available in https://github.com/zjunlp/RAP.", "published": "2022-10-19 16:40:28", "link": "http://arxiv.org/abs/2210.10709v5", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Museformer: Transformer with Fine- and Coarse-Grained Attention for\n  Music Generation", "abstract": "Symbolic music generation aims to generate music scores automatically. A\nrecent trend is to use Transformer or its variants in music generation, which\nis, however, suboptimal, because the full attention cannot efficiently model\nthe typically long music sequences (e.g., over 10,000 tokens), and the existing\nmodels have shortcomings in generating musical repetition structures. In this\npaper, we propose Museformer, a Transformer with a novel fine- and\ncoarse-grained attention for music generation. Specifically, with the\nfine-grained attention, a token of a specific bar directly attends to all the\ntokens of the bars that are most relevant to music structures (e.g., the\nprevious 1st, 2nd, 4th and 8th bars, selected via similarity statistics); with\nthe coarse-grained attention, a token only attends to the summarization of the\nother bars rather than each token of them so as to reduce the computational\ncost. The advantages are two-fold. First, it can capture both music\nstructure-related correlations via the fine-grained attention, and other\ncontextual information via the coarse-grained attention. Second, it is\nefficient and can model over 3X longer music sequences compared to its\nfull-attention counterpart. Both objective and subjective experimental results\ndemonstrate its ability to generate long music sequences with high quality and\nbetter structures.", "published": "2022-10-19 07:31:56", "link": "http://arxiv.org/abs/2210.10349v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Learning Based Stage-wise Two-dimensional Speaker Localization with\n  Large Ad-hoc Microphone Arrays", "abstract": "While deep-learning-based speaker localization has shown advantages in\nchallenging acoustic environments, it often yields only direction-of-arrival\n(DOA) cues rather than precise two-dimensional (2D) coordinates. To address\nthis, we propose a novel deep-learning-based 2D speaker localization method\nleveraging ad-hoc microphone arrays, where an ad-hoc microphone array is\ncomposed of randomly distributed microphone nodes, each of which is equipped\nwith a traditional array. Specifically, we first employ convolutional neural\nnetworks at each node to estimate speaker directions. Then, we integrate these\nDOA estimates using triangulation and clustering techniques to get 2D speaker\nlocations. To further boost the estimation accuracy, we introduce a node\nselection algorithm that strategically filters the most reliable nodes.\nExtensive experiments on both simulated and real-world data demonstrate that\nour approach significantly outperforms conventional methods. The proposed node\nselection further refines performance. The real-world dataset in the\nexperiment, named Libri-adhoc-node10 which is a newly recorded data described\nfor the first time in this paper, is online available at\nhttps://github.com/Liu-sp/Libri-adhoc-nodes10.", "published": "2022-10-19 02:59:35", "link": "http://arxiv.org/abs/2210.10265v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Two-stage training method for Japanese electrolaryngeal speech\n  enhancement based on sequence-to-sequence voice conversion", "abstract": "Sequence-to-sequence (seq2seq) voice conversion (VC) models have greater\npotential in converting electrolaryngeal (EL) speech to normal speech (EL2SP)\ncompared to conventional VC models. However, EL2SP based on seq2seq VC requires\na sufficiently large amount of parallel data for the model training and it\nsuffers from significant performance degradation when the amount of training\ndata is insufficient. To address this issue, we suggest a novel, two-stage\nstrategy to optimize the performance on EL2SP based on seq2seq VC when a small\namount of the parallel dataset is available. In contrast to utilizing\nhigh-quality data augmentations in previous studies, we first combine a large\namount of imperfect synthetic parallel data of EL and normal speech, with the\noriginal dataset into VC training. Then, a second stage training is conducted\nwith the original parallel dataset only. The results show that the proposed\nmethod progressively improves the performance of EL2SP based on seq2seq VC.", "published": "2022-10-19 06:08:17", "link": "http://arxiv.org/abs/2210.10314v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Tampering Detection Based on Shallow and Deep Feature\n  Representation Learning", "abstract": "Digital audio tampering detection can be used to verify the authenticity of\ndigital audio. However, most current methods use standard electronic network\nfrequency (ENF) databases for visual comparison analysis of ENF continuity of\ndigital audio or perform feature extraction for classification by machine\nlearning methods. ENF databases are usually tricky to obtain, visual methods\nhave weak feature representation, and machine learning methods have more\ninformation loss in features, resulting in low detection accuracy. This paper\nproposes a fusion method of shallow and deep features to fully use ENF\ninformation by exploiting the complementary nature of features at different\nlevels to more accurately describe the changes in inconsistency produced by\ntampering operations to raw digital audio. The method achieves 97.03% accuracy\non three classic databases: Carioca 1, Carioca 2, and New Spanish. In addition,\nwe have achieved an accuracy of 88.31% on the newly constructed database\nGAUDI-DI. Experimental results show that the proposed method is superior to the\nstate-of-the-art method.", "published": "2022-10-19 12:22:19", "link": "http://arxiv.org/abs/2210.10506v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spoofed training data for speech spoofing countermeasure can be\n  efficiently created using neural vocoders", "abstract": "A good training set for speech spoofing countermeasures requires diverse TTS\nand VC spoofing attacks, but generating TTS and VC spoofed trials for a target\nspeaker may be technically demanding. Instead of using full-fledged TTS and VC\nsystems, this study uses neural-network-based vocoders to do copy-synthesis on\nbona fide utterances. The output data can be used as spoofed data. To make\nbetter use of pairs of bona fide and spoofed data, this study introduces a\ncontrastive feature loss that can be plugged into the standard training\ncriterion. On the basis of the bona fide trials from the ASVspoof 2019 logical\naccess training set, this study empirically compared a few training sets\ncreated in the proposed manner using a few neural non-autoregressive vocoders.\nResults on multiple test sets suggest good practices such as fine-tuning neural\nvocoders using bona fide data from the target domain. The results also\ndemonstrated the effectiveness of the contrastive feature loss. Combining the\nbest practices, the trained CM achieved overall competitive performance. Its\nEERs on the ASVspoof 2021 hidden subsets also outperformed the top-1 challenge\nsubmission.", "published": "2022-10-19 14:10:02", "link": "http://arxiv.org/abs/2210.10570v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-End Integration of Speech Recognition, Dereverberation,\n  Beamforming, and Self-Supervised Learning Representation", "abstract": "Self-supervised learning representation (SSLR) has demonstrated its\nsignificant effectiveness in automatic speech recognition (ASR), mainly with\nclean speech. Recent work pointed out the strength of integrating SSLR with\nsingle-channel speech enhancement for ASR in noisy environments. This paper\nfurther advances this integration by dealing with multi-channel input. We\npropose a novel end-to-end architecture by integrating dereverberation,\nbeamforming, SSLR, and ASR within a single neural network. Our system achieves\nthe best performance reported in the literature on the CHiME-4 6-channel track\nwith a word error rate (WER) of 1.77%. While the WavLM-based strong SSLR\ndemonstrates promising results by itself, the end-to-end integration with the\nweighted power minimization distortionless response beamformer, which\nsimultaneously performs dereverberation and denoising, improves WER\nsignificantly. Its effectiveness is also validated on the REVERB dataset.", "published": "2022-10-19 17:33:10", "link": "http://arxiv.org/abs/2210.10742v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Modeling Animal Vocalizations through Synthesizers", "abstract": "Modeling real-world sound is a fundamental problem in the creative use of\nmachine learning and many other fields, including human speech processing and\nbioacoustics. Transformer-based generative models and some prior work (e.g.,\nDDSP) are known to produce realistic sound, although they have limited control\nand are hard to interpret. As an alternative, we aim to use modular\nsynthesizers, i.e., compositional, parametric electronic musical instruments,\nfor modeling non-music sounds. However, inferring synthesizer parameters given\na target sound, i.e., the parameter inference task, is not trivial for general\nsounds, and past research has typically focused on musical sound. In this work,\nwe optimize a differentiable synthesizer from TorchSynth in order to model,\nemulate, and creatively generate animal vocalizations. We compare an array of\noptimization methods, from gradient-based search to genetic algorithms, for\ninferring its parameters, and then demonstrate how one can control and\ninterpret the parameters for modeling non-music sounds.", "published": "2022-10-19 19:40:22", "link": "http://arxiv.org/abs/2210.10857v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Propagating Variational Model Uncertainty for Bioacoustic Call Label\n  Smoothing", "abstract": "We focus on using the predictive uncertainty signal calculated by Bayesian\nneural networks to guide learning in the self-same task the model is being\ntrained on. Not opting for costly Monte Carlo sampling of weights, we propagate\nthe approximate hidden variance in an end-to-end manner, throughout a\nvariational Bayesian adaptation of a ResNet with attention and\nsqueeze-and-excitation blocks, in order to identify data samples that should\ncontribute less into the loss value calculation. We, thus, propose\nuncertainty-aware, data-specific label smoothing, where the smoothing\nprobability is dependent on this epistemic uncertainty. We show that, through\nthe explicit usage of the epistemic uncertainty in the loss calculation, the\nvariational model is led to improved predictive and calibration performance.\nThis core machine learning methodology is exemplified at wildlife call\ndetection, from audio recordings made via passive acoustic monitoring equipment\nin the animals' natural habitats, with the future goal of automating large\nscale annotation in a trustworthy manner.", "published": "2022-10-19 13:04:26", "link": "http://arxiv.org/abs/2210.10526v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
