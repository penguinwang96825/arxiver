{"title": "Fast BTG-Forest-Based Hierarchical Sub-sentential Alignment", "abstract": "In this paper, we propose a novel BTG-forest-based alignment method. Based on\na fast unsupervised initialization of parameters using variational IBM models,\nwe synchronously parse parallel sentences top-down and align hierarchically\nunder the constraint of BTG. Our two-step method can achieve the same run-time\nand comparable translation performance as fast_align while it yields smaller\nphrase tables. Final SMT results show that our method even outperforms in the\nexperiment of distantly related languages, e.g., English-Japanese.", "published": "2017-11-20 11:54:59", "link": "http://arxiv.org/abs/1711.07265v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-Contextual Modeling of Sarcasm using a Neural Network Benchmark", "abstract": "One of the most crucial components of natural human-robot interaction is\nartificial intuition and its influence on dialog systems. The intuitive\ncapability that humans have is undeniably extraordinary, and so remains one of\nthe greatest challenges for natural communicative dialogue between humans and\nrobots. In this paper, we introduce a novel probabilistic modeling framework of\nidentifying, classifying and learning features of sarcastic text via training a\nneural network with human-informed sarcastic benchmarks. This is necessary for\nestablishing a comprehensive sentiment analysis schema that is sensitive to the\nnuances of sarcasm-ridden text by being trained on linguistic cues. We show\nthat our model provides a good fit for this type of real-world informed data,\nwith potential to achieve as accurate, if not more, than alternatives. Though\nthe implementation and benchmarking is an extensive task, it can be extended\nvia the same method that we present to capture different forms of nuances in\ncommunication and making for much more natural and engaging dialogue systems.", "published": "2017-11-20 16:47:28", "link": "http://arxiv.org/abs/1711.07404v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speech recognition for medical conversations", "abstract": "In this work we explored building automatic speech recognition models for\ntranscribing doctor patient conversation. We collected a large scale dataset of\nclinical conversations ($14,000$ hr), designed the task to represent the real\nword scenario, and explored several alignment approaches to iteratively improve\ndata quality. We explored both CTC and LAS systems for building speech\nrecognition models. The LAS was more resilient to noisy data and CTC required\nmore data clean up. A detailed analysis is provided for understanding the\nperformance for clinical tasks. Our analysis showed the speech recognition\nmodels performed well on important medical utterances, while errors occurred in\ncausal conversations. Overall we believe the resulting models can provide\nreasonable quality in practice.", "published": "2017-11-20 12:07:22", "link": "http://arxiv.org/abs/1711.07274v2", "categories": ["cs.CL", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Vision-and-Language Navigation: Interpreting visually-grounded\n  navigation instructions in real environments", "abstract": "A robot that can carry out a natural-language instruction has been a dream\nsince before the Jetsons cartoon series imagined a life of leisure mediated by\na fleet of attentive robot helpers. It is a dream that remains stubbornly\ndistant. However, recent advances in vision and language methods have made\nincredible progress in closely related areas. This is significant because a\nrobot interpreting a natural-language navigation instruction on the basis of\nwhat it sees is carrying out a vision and language process that is similar to\nVisual Question Answering. Both tasks can be interpreted as visually grounded\nsequence-to-sequence translation problems, and many of the same methods are\napplicable. To enable and encourage the application of vision and language\nmethods to the problem of interpreting visually-grounded navigation\ninstructions, we present the Matterport3D Simulator -- a large-scale\nreinforcement learning environment based on real imagery. Using this simulator,\nwhich can in future support a range of embodied vision and language tasks, we\nprovide the first benchmark dataset for visually-grounded natural language\nnavigation in real buildings -- the Room-to-Room (R2R) dataset.", "published": "2017-11-20 12:17:47", "link": "http://arxiv.org/abs/1711.07280v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Hello Edge: Keyword Spotting on Microcontrollers", "abstract": "Keyword spotting (KWS) is a critical component for enabling speech based user\ninteractions on smart devices. It requires real-time response and high accuracy\nfor good user experience. Recently, neural networks have become an attractive\nchoice for KWS architecture because of their superior accuracy compared to\ntraditional speech processing algorithms. Due to its always-on nature, KWS\napplication has highly constrained power budget and typically runs on tiny\nmicrocontrollers with limited memory and compute capability. The design of\nneural network architecture for KWS must consider these constraints. In this\nwork, we perform neural network architecture evaluation and exploration for\nrunning KWS on resource-constrained microcontrollers. We train various neural\nnetwork architectures for keyword spotting published in literature to compare\ntheir accuracy and memory/compute requirements. We show that it is possible to\noptimize these neural network architectures to fit within the memory and\ncompute constraints of microcontrollers without sacrificing accuracy. We\nfurther explore the depthwise separable convolutional neural network (DS-CNN)\nand compare it against other neural network architectures. DS-CNN achieves an\naccuracy of 95.4%, which is ~10% higher than the DNN model with similar number\nof parameters.", "published": "2017-11-20 03:19:03", "link": "http://arxiv.org/abs/1711.07128v3", "categories": ["cs.SD", "cs.CL", "cs.LG", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
