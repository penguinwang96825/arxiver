{"title": "TransG : A Generative Mixture Model for Knowledge Graph Embedding", "abstract": "Recently, knowledge graph embedding, which projects symbolic entities and\nrelations into continuous vector space, has become a new, hot topic in\nartificial intelligence. This paper addresses a new issue of multiple relation\nsemantics that a relation may have multiple meanings revealed by the entity\npairs associated with the corresponding triples, and proposes a novel Gaussian\nmixture model for embedding, TransG. The new model can discover latent\nsemantics for a relation and leverage a mixture of relation component vectors\nfor embedding a fact triple. To the best of our knowledge, this is the first\ngenerative model for knowledge graph embedding, which is able to deal with\nmultiple relation semantics. Extensive experiments show that the proposed model\nachieves substantial improvements against the state-of-the-art baselines.", "published": "2015-09-18 02:30:17", "link": "http://arxiv.org/abs/1509.05488v7", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TransA: An Adaptive Approach for Knowledge Graph Embedding", "abstract": "Knowledge representation is a major topic in AI, and many studies attempt to\nrepresent entities and relations of knowledge base in a continuous vector\nspace. Among these attempts, translation-based methods build entity and\nrelation vectors by minimizing the translation loss from a head entity to a\ntail one. In spite of the success of these methods, translation-based methods\nalso suffer from the oversimplified loss metric, and are not competitive enough\nto model various and complex entities/relations in knowledge bases. To address\nthis issue, we propose \\textbf{TransA}, an adaptive metric approach for\nembedding, utilizing the metric learning ideas to provide a more flexible\nembedding method. Experiments are conducted on the benchmark datasets and our\nproposed method makes significant and consistent improvements over the\nstate-of-the-art baselines.", "published": "2015-09-18 02:40:07", "link": "http://arxiv.org/abs/1509.05490v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Light Sliding-Window Part-of-Speech Tagger for the Apertium\n  Free/Open-Source Machine Translation Platform", "abstract": "This paper describes a free/open-source implementation of the light\nsliding-window (LSW) part-of-speech tagger for the Apertium free/open-source\nmachine translation platform. Firstly, the mechanism and training process of\nthe tagger are reviewed, and a new method for incorporating linguistic rules is\nproposed. Secondly, experiments are conducted to compare the performances of\nthe tagger under different window settings, with or without Apertium-style\n\"forbid\" rules, with or without Constraint Grammar, and also with respect to\nthe traditional HMM tagger in Apertium.", "published": "2015-09-18 06:56:38", "link": "http://arxiv.org/abs/1509.05517v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building a Pilot Software Quality-in-Use Benchmark Dataset", "abstract": "Prepared domain specific datasets plays an important role to supervised\nlearning approaches. In this article a new sentence dataset for software\nquality-in-use is proposed. Three experts were chosen to annotate the data\nusing a proposed annotation scheme. Then the data were reconciled in a (no\nmatch eliminate) process to reduce bias. The Kappa, k statistics revealed an\nacceptable level of agreement; moderate to substantial agreement between the\nexperts. The built data can be used to evaluate software quality-in-use models\nin sentiment analysis models. Moreover, the annotation scheme can be used to\nextend the current dataset.", "published": "2015-09-18 18:19:48", "link": "http://arxiv.org/abs/1509.05736v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Word, graph and manifold embedding from Markov processes", "abstract": "Continuous vector representations of words and objects appear to carry\nsurprisingly rich semantic content. In this paper, we advance both the\nconceptual and theoretical understanding of word embeddings in three ways.\nFirst, we ground embeddings in semantic spaces studied in\ncognitive-psychometric literature and introduce new evaluation tasks. Second,\nin contrast to prior work, we take metric recovery as the key object of study,\nunify existing algorithms as consistent metric recovery methods based on\nco-occurrence counts from simple Markov random walks, and propose a new\nrecovery algorithm. Third, we generalize metric recovery to graphs and\nmanifolds, relating co-occurence counts on random walks in graphs and random\nprocesses on manifolds to the underlying metric to be recovered, thereby\nreconciling manifold estimation and embedding algorithms. We compare embedding\nalgorithms across a range of tasks, from nonlinear dimensionality reduction to\nthree semantic language tasks, including analogies, sequence completion, and\nclassification.", "published": "2015-09-18 21:50:38", "link": "http://arxiv.org/abs/1509.05808v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
