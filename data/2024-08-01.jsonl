{"title": "DeliLaw: A Chinese Legal Counselling System Based on a Large Language\n  Model", "abstract": "Traditional legal retrieval systems designed to retrieve legal documents,\nstatutes, precedents, and other legal information are unable to give\nsatisfactory answers due to lack of semantic understanding of specific\nquestions. Large Language Models (LLMs) have achieved excellent results in a\nvariety of natural language processing tasks, which inspired us that we train a\nLLM in the legal domain to help legal retrieval. However, in the Chinese legal\ndomain, due to the complexity of legal questions and the rigour of legal\narticles, there is no legal large model with satisfactory practical application\nyet. In this paper, we present DeliLaw, a Chinese legal counselling system\nbased on a large language model. DeliLaw integrates a legal retrieval module\nand a case retrieval module to overcome the model hallucination. Users can\nconsult professional legal questions, search for legal articles and relevant\njudgement cases, etc. on the DeliLaw system in a dialogue mode. In addition,\nDeliLaw supports the use of English for counseling. we provide the address of\nthe system: https://data.delilegal.com/lawQuestion.", "published": "2024-08-01 07:54:52", "link": "http://arxiv.org/abs/2408.00357v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-Context Example Selection via Similarity Search Improves Low-Resource\n  Machine Translation", "abstract": "The ability of generative large language models (LLMs) to perform in-context\nlearning has given rise to a large body of research into how best to prompt\nmodels for various natural language processing tasks. In this paper, we focus\non machine translation (MT), a task that has been shown to benefit from\nin-context translation examples. However no systematic studies have been\npublished on how best to select examples, and mixed results have been reported\non the usefulness of similarity-based selection over random selection. We\nprovide a study covering multiple LLMs and multiple in-context example\nretrieval strategies, comparing multilingual sentence embeddings. We cover\nseveral language directions, representing different levels of language\nresourcedness (English into French, German, Swahili and Wolof). Contrarily to\npreviously published results, we find that sentence embedding similarity can\nimprove MT, especially for low-resource language directions, and discuss the\nbalance between selection pool diversity and quality. We also highlight\npotential problems with the evaluation of LLM-based MT and suggest a more\nappropriate evaluation protocol, adapting the COMET metric to the evaluation of\nLLMs. Code and outputs are freely available at\nhttps://github.com/ArmelRandy/ICL-MT.", "published": "2024-08-01 09:07:32", "link": "http://arxiv.org/abs/2408.00397v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Downstream bias mitigation is all you need", "abstract": "The advent of transformer-based architectures and large language models\n(LLMs) have significantly advanced the performance of natural language\nprocessing (NLP) models. Since these LLMs are trained on huge corpuses of data\nfrom the web and other sources, there has been a major concern about harmful\nprejudices that may potentially be transferred from the data. In many\napplications, these pre-trained LLMs are fine-tuned on task specific datasets,\nwhich can further contribute to biases. This paper studies the extent of biases\nabsorbed by LLMs during pre-training as well as task-specific behaviour after\nfine-tuning. We found that controlled interventions on pre-trained LLMs, prior\nto fine-tuning, have minimal effect on lowering biases in classifiers. However,\nthe biases present in domain-specific datasets play a much bigger role, and\nhence mitigating them at this stage has a bigger impact. While pre-training\ndoes matter, but after the model has been pre-trained, even slight changes to\nco-occurrence rates in the fine-tuning dataset has a significant effect on the\nbias of the model.", "published": "2024-08-01 14:52:04", "link": "http://arxiv.org/abs/2408.00612v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Entailment Judgements in Cross-Lingual Summarisation", "abstract": "Synthetically created Cross-Lingual Summarisation (CLS) datasets are prone to\ninclude document-summary pairs where the reference summary is unfaithful to the\ncorresponding document as it contains content not supported by the document\n(i.e., hallucinated content). This low data quality misleads model learning and\nobscures evaluation results. Automatic ways to assess hallucinations and\nimprove training have been proposed for monolingual summarisation,\npredominantly in English. For CLS, we propose to use off-the-shelf\ncross-lingual Natural Language Inference (X-NLI) to evaluate faithfulness of\nreference and model generated summaries. Then, we study training approaches\nthat are aware of faithfulness issues in the training data and propose an\napproach that uses unlikelihood loss to teach a model about unfaithful summary\nsequences. Our results show that it is possible to train CLS models that yield\nmore faithful summaries while maintaining comparable or better informativess.", "published": "2024-08-01 16:18:09", "link": "http://arxiv.org/abs/2408.00675v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Variety of a Concept Space Using an Unbiased Estimate of\n  Rao's Quadratic Index", "abstract": "Past research relates design creativity to 'divergent thinking,' i.e., how\nwell the concept space is explored during the early phase of design.\nResearchers have argued that generating several concepts would increase the\nchances of producing better design solutions. 'Variety' is one of the\nparameters by which one can quantify the breadth of a concept space explored by\nthe designers. It is useful to assess variety at the conceptual design stage\nbecause, at this stage, designers have the freedom to explore different\nsolution principles so as to satisfy a design problem with substantially novel\nconcepts. This article elaborates on and critically examines the existing\nvariety metrics from the engineering design literature, discussing their\nlimitations. A new distance-based variety metric is proposed, along with a\nprescriptive framework to support the assessment process. This framework uses\nthe SAPPhIRE model of causality as a knowledge representation scheme to measure\nthe real-valued distance between two design concepts. The proposed framework is\nimplemented in a software tool called 'VariAnT.' Furthermore, the tool's\napplication is demonstrated through an illustrative example.", "published": "2024-08-01 16:25:54", "link": "http://arxiv.org/abs/2408.00684v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Text Embeddings for Smaller Language Models Using Contrastive\n  Fine-tuning", "abstract": "While Large Language Models show remarkable performance in natural language\nunderstanding, their resource-intensive nature makes them less accessible. In\ncontrast, smaller language models such as MiniCPM offer more sustainable\nscalability, but often underperform without specialized optimization. In this\npaper, we explore the enhancement of smaller language models through the\nimprovement of their text embeddings. We select three language models, MiniCPM,\nPhi-2, and Gemma, to conduct contrastive fine-tuning on the NLI dataset. Our\nresults demonstrate that this fine-tuning method enhances the quality of text\nembeddings for all three models across various benchmarks, with MiniCPM showing\nthe most significant improvements of an average 56.33% performance gain. The\ncontrastive fine-tuning code is publicly available at\nhttps://github.com/trapoom555/Language-Model-STS-CFT.", "published": "2024-08-01 16:31:35", "link": "http://arxiv.org/abs/2408.00690v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence-wise Speech Summarization: Task, Datasets, and End-to-End\n  Modeling with LM Knowledge Distillation", "abstract": "This paper introduces a novel approach called sentence-wise speech\nsummarization (Sen-SSum), which generates text summaries from a spoken document\nin a sentence-by-sentence manner. Sen-SSum combines the real-time processing of\nautomatic speech recognition (ASR) with the conciseness of speech\nsummarization. To explore this approach, we present two datasets for Sen-SSum:\nMega-SSum and CSJ-SSum. Using these datasets, our study evaluates two types of\nTransformer-based models: 1) cascade models that combine ASR and strong text\nsummarization models, and 2) end-to-end (E2E) models that directly convert\nspeech into a text summary. While E2E models are appealing to develop\ncompute-efficient models, they perform worse than cascade models. Therefore, we\npropose knowledge distillation for E2E models using pseudo-summaries generated\nby the cascade models. Our experiments show that this proposed knowledge\ndistillation effectively improves the performance of the E2E model on both\ndatasets.", "published": "2024-08-01 00:18:21", "link": "http://arxiv.org/abs/2408.00205v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Lost in Translation: Latent Concept Misalignment in Text-to-Image\n  Diffusion Models", "abstract": "Advancements in text-to-image diffusion models have broadened extensive\ndownstream practical applications, but such models often encounter misalignment\nissues between text and image. Taking the generation of a combination of two\ndisentangled concepts as an example, say given the prompt \"a tea cup of iced\ncoke\", existing models usually generate a glass cup of iced coke because the\niced coke usually co-occurs with the glass cup instead of the tea one during\nmodel training. The root of such misalignment is attributed to the confusion in\nthe latent semantic space of text-to-image diffusion models, and hence we refer\nto the \"a tea cup of iced coke\" phenomenon as Latent Concept Misalignment\n(LC-Mis). We leverage large language models (LLMs) to thoroughly investigate\nthe scope of LC-Mis, and develop an automated pipeline for aligning the latent\nsemantics of diffusion models to text prompts. Empirical assessments confirm\nthe effectiveness of our approach, substantially reducing LC-Mis errors and\nenhancing the robustness and versatility of text-to-image diffusion models. The\ncode and dataset are here: https://github.com/RossoneriZhao/iced_coke.", "published": "2024-08-01 01:54:17", "link": "http://arxiv.org/abs/2408.00230v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Enhanced Structured State Space Models via Grouped FIR Filtering and\n  Attention Sink Mechanisms", "abstract": "Structured State Space Models (SSMs) have emerged as compelling alternatives\nto Transformer architectures, offering linear-time complexity and superior\nperformance in various sequence modeling tasks. Despite their advantages, SSMs\nlike the original Mamba-2 face training difficulties due to the sensitivities\nintroduced by the extended series of recurrent matrix multiplications. In this\npaper, we propose an advanced architecture that mitigates these challenges by\ndecomposing A-multiplications into multiple groups and optimizing positional\nencoding through Grouped Finite Impulse Response (FIR) filtering. This new\nstructure, denoted as Grouped FIR-enhanced SSM (GFSSM), employs semiseparable\nmatrices for efficient computation. Furthermore, inspired by the \"attention\nsink\" phenomenon identified in streaming language models, we incorporate a\nsimilar mechanism to enhance the stability and performance of our model over\nextended sequences. Our approach further bridges the gap between SSMs and\nTransformer architectures, offering a viable path forward for scalable and\nhigh-performing sequence modeling.", "published": "2024-08-01 02:49:58", "link": "http://arxiv.org/abs/2408.00244v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "QUITO: Accelerating Long-Context Reasoning through Query-Guided Context\n  Compression", "abstract": "In-context learning (ICL) capabilities are foundational to the success of\nlarge language models (LLMs). Recently, context compression has attracted\ngrowing interest since it can largely reduce reasoning complexities and\ncomputation costs of LLMs. In this paper, we introduce a novel Query-gUIded\naTtention cOmpression (QUITO) method, which leverages attention of the question\nover the contexts to filter useless information. Specifically, we take a\ntrigger token to calculate the attention distribution of the context in\nresponse to the question. Based on the distribution, we propose three different\nfiltering methods to satisfy the budget constraints of the context length. We\nevaluate the QUITO using two widely-used datasets, namely, NaturalQuestions and\nASQA. Experimental results demonstrate that QUITO significantly outperforms\nestablished baselines across various datasets and downstream LLMs, underscoring\nits effectiveness. Our code is available at\nhttps://github.com/Wenshansilvia/attention_compressor.", "published": "2024-08-01 04:28:38", "link": "http://arxiv.org/abs/2408.00274v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Navigating Text-to-Image Generative Bias across Indic Languages", "abstract": "This research investigates biases in text-to-image (TTI) models for the Indic\nlanguages widely spoken across India. It evaluates and compares the generative\nperformance and cultural relevance of leading TTI models in these languages\nagainst their performance in English. Using the proposed IndicTTI benchmark, we\ncomprehensively assess the performance of 30 Indic languages with two\nopen-source diffusion models and two commercial generation APIs. The primary\nobjective of this benchmark is to evaluate the support for Indic languages in\nthese models and identify areas needing improvement. Given the linguistic\ndiversity of 30 languages spoken by over 1.4 billion people, this benchmark\naims to provide a detailed and insightful analysis of TTI models' effectiveness\nwithin the Indic linguistic landscape. The data and code for the IndicTTI\nbenchmark can be accessed at\nhttps://iab-rubric.org/resources/other-databases/indictti.", "published": "2024-08-01 04:56:13", "link": "http://arxiv.org/abs/2408.00283v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "The Monetisation of Toxicity: Analysing YouTube Content Creators and\n  Controversy-Driven Engagement", "abstract": "YouTube is a major social media platform that plays a significant role in\ndigital culture, with content creators at its core. These creators often engage\nin controversial behaviour to drive engagement, which can foster toxicity. This\npaper presents a quantitative analysis of controversial content on YouTube,\nfocusing on the relationship between controversy, toxicity, and monetisation.\nWe introduce a curated dataset comprising 20 controversial YouTube channels\nextracted from Reddit discussions, including 16,349 videos and more than 105\nmillion comments. We identify and categorise monetisation cues from video\ndescriptions into various models, including affiliate marketing and direct\nselling, using lists of URLs and keywords. Additionally, we train a machine\nlearning model to measure the toxicity of comments in these videos. Our\nfindings reveal that while toxic comments correlate with higher engagement,\nthey negatively impact monetisation, indicating that controversy-driven\ninteraction does not necessarily lead to financial gain. We also observed\nsignificant variation in monetisation strategies, with some creators showing\nextensive monetisation despite high toxicity levels. Our study introduces a\ncurated dataset, lists of URLs and keywords to categorise monetisation, a\nmachine learning model to measure toxicity, and is a significant step towards\nunderstanding the complex relationship between controversy, engagement, and\nmonetisation on YouTube. The lists used for detecting and categorising\nmonetisation cues are available on https://github.com/thalesbertaglia/toxmon.", "published": "2024-08-01 13:10:35", "link": "http://arxiv.org/abs/2408.00534v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs", "abstract": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.", "published": "2024-08-01 13:22:01", "link": "http://arxiv.org/abs/2408.00539v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Non Verbis, Sed Rebus: Large Language Models are Weak Solvers of Italian\n  Rebuses", "abstract": "Rebuses are puzzles requiring constrained multi-step reasoning to identify a\nhidden phrase from a set of images and letters. In this work, we introduce a\nlarge collection of verbalized rebuses for the Italian language and use it to\nassess the rebus-solving capabilities of state-of-the-art large language\nmodels. While general-purpose systems such as LLaMA-3 and GPT-4o perform poorly\non this task, ad-hoc fine-tuning seems to improve models' performance. However,\nwe find that performance gains from training are largely motivated by\nmemorization. Our results suggest that rebus solving remains a challenging test\nbed to evaluate large language models' linguistic proficiency and sequential\ninstruction-following skills.", "published": "2024-08-01 14:14:15", "link": "http://arxiv.org/abs/2408.00584v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Are Bigger Encoders Always Better in Vision Large Models?", "abstract": "In recent years, multimodal large language models (MLLMs) have shown strong\npotential in real-world applications. They are developing rapidly due to their\nremarkable ability to comprehend multimodal information and their inherent\npowerful cognitive and reasoning capabilities. Among MLLMs, vision language\nmodels (VLM) stand out for their ability to understand vision information.\nHowever, the scaling trend of VLMs under the current mainstream paradigm has\nnot been extensively studied. Whether we can achieve better performance by\ntraining even larger models is still unclear. To address this issue, we\nconducted experiments on the pretraining stage of MLLMs. We conduct our\nexperiment using different encoder sizes and large language model (LLM) sizes.\nOur findings indicate that merely increasing the size of encoders does not\nnecessarily enhance the performance of VLMs. Moreover, we analyzed the effects\nof LLM backbone parameter size and data quality on the pretraining outcomes.\nAdditionally, we explored the differences in scaling laws between LLMs and\nVLMs.", "published": "2024-08-01 15:05:42", "link": "http://arxiv.org/abs/2408.00620v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SentenceVAE: Enable Next-sentence Prediction for Large Language Models\n  with Faster Speed, Higher Accuracy and Longer Context", "abstract": "Current large language models (LLMs) primarily utilize next-token prediction\nmethod for inference, which significantly impedes their processing speed. In\nthis paper, we introduce a novel inference methodology termed next-sentence\nprediction, aiming at enhancing the inference efficiency of LLMs. We present\nSentence Variational Autoencoder (SentenceVAE), which includes a Sentence\nEncoder to compress multiple tokens in a sentence into a single token, and a\nSentence Decoder to reconstruct it. By integrating SentenceVAE into the input\nand output layers of LLMs, we develop Sentence-level LLMs (SLLMs) that employ a\nsentence-by-sentence inference method. In addition, the SentenceVAE module of\nSLLMs can maintain the integrity of the original semantic content by segmenting\nthe context into sentences, thereby improving accuracy while boosting inference\nspeed. Moreover, compared to previous LLMs, SLLMs process fewer tokens over\nequivalent context length, significantly reducing memory demands for\nself-attention computation and facilitating the handling of longer context.\nExtensive experiments on Wanjuan dataset have revealed that the proposed method\ncan accelerate inference speed by 204~365%, reduce perplexity (PPL) to 46~75%\nof its original metric, and decrease memory overhead by 86~91% for the\nequivalent context length, compared to previous token-by-token methods.", "published": "2024-08-01 15:45:19", "link": "http://arxiv.org/abs/2408.00655v5", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Aligning Multiple Knowledge Graphs in a Single Pass", "abstract": "Entity alignment (EA) is to identify equivalent entities across different\nknowledge graphs (KGs), which can help fuse these KGs into a more comprehensive\none. Previous EA methods mainly focus on aligning a pair of KGs, and to the\nbest of our knowledge, no existing EA method considers aligning multiple (more\nthan two) KGs. To fill this research gap, in this work, we study a novel\nproblem of aligning multiple KGs and propose an effective framework named\nMultiEA to solve the problem. First, we embed the entities of all the candidate\nKGs into a common feature space by a shared KG encoder. Then, we explore three\nalignment strategies to minimize the distances among pre-aligned entities. In\nparticular, we propose an innovative inference enhancement technique to improve\nthe alignment performance by incorporating high-order similarities. Finally, to\nverify the effectiveness of MultiEA, we construct two new real-world benchmark\ndatasets and conduct extensive experiments on them. The results show that our\nMultiEA can effectively and efficiently align multiple KGs in a single pass. We\nrelease the source codes of MultiEA at: https://github.com/kepsail/MultiEA.", "published": "2024-08-01 15:58:05", "link": "http://arxiv.org/abs/2408.00662v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Retrieval-Augmented Generation in Medicine with Iterative\n  Follow-up Questions", "abstract": "The emergent abilities of large language models (LLMs) have demonstrated\ngreat potential in solving medical questions. They can possess considerable\nmedical knowledge, but may still hallucinate and are inflexible in the\nknowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed\nto enhance the medical question-answering capabilities of LLMs with external\nknowledge bases, it may still fail in complex cases where multiple rounds of\ninformation-seeking are required. To address such an issue, we propose\niterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up\nqueries based on previous information-seeking attempts. In each iteration of\ni-MedRAG, the follow-up queries will be answered by a conventional RAG system\nand they will be further used to guide the query generation in the next\niteration. Our experiments show the improved performance of various LLMs\nbrought by i-MedRAG compared with conventional RAG on complex questions from\nclinical vignettes in the United States Medical Licensing Examination (USMLE),\nas well as various knowledge tests in the Massive Multitask Language\nUnderstanding (MMLU) dataset. Notably, our zero-shot i-MedRAG outperforms all\nexisting prompt engineering and fine-tuning methods on GPT-3.5, achieving an\naccuracy of 69.68% on the MedQA dataset. In addition, we characterize the\nscaling properties of i-MedRAG with different iterations of follow-up queries\nand different numbers of queries per iteration. Our case studies show that\ni-MedRAG can flexibly ask follow-up queries to form reasoning chains, providing\nan in-depth analysis of medical questions. To the best of our knowledge, this\nis the first-of-its-kind study on incorporating follow-up queries into medical\nRAG. The implementation of i-MedRAG is available at\nhttps://github.com/Teddy-XiongGZ/MedRAG.", "published": "2024-08-01 17:18:17", "link": "http://arxiv.org/abs/2408.00727v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hybrid Querying Over Relational Databases and Large Language Models", "abstract": "Database queries traditionally operate under the closed-world assumption,\nproviding no answers to questions that require information beyond the data\nstored in the database. Hybrid querying using SQL offers an alternative by\nintegrating relational databases with large language models (LLMs) to answer\nbeyond-database questions. In this paper, we present the first cross-domain\nbenchmark, SWAN, containing 120 beyond-database questions over four real-world\ndatabases. To leverage state-of-the-art language models in addressing these\ncomplex questions in SWAN, we present two solutions: one based on schema\nexpansion and the other based on user defined functions. We also discuss\noptimization opportunities and potential future directions. Our evaluation\ndemonstrates that using GPT-4 Turbo with few-shot prompts, one can achieves up\nto 40.0\\% in execution accuracy and 48.2\\% in data factuality. These results\nhighlights both the potential and challenges for hybrid querying. We believe\nthat our work will inspire further research in creating more efficient and\naccurate data systems that seamlessly integrate relational databases and large\nlanguage models to address beyond-database questions.", "published": "2024-08-01 19:29:18", "link": "http://arxiv.org/abs/2408.00884v2", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Granting GPT-4 License and Opportunity: Enhancing Accuracy and\n  Confidence Estimation for Few-Shot Event Detection", "abstract": "Large Language Models (LLMs) such as GPT-4 have shown enough promise in the\nfew-shot learning context to suggest use in the generation of \"silver\" data and\nrefinement of new ontologies through iterative application and review. Such\nworkflows become more effective with reliable confidence estimation.\nUnfortunately, confidence estimation is a documented weakness of models such as\nGPT-4, and established methods to compensate require significant additional\ncomplexity and computation. The present effort explores methods for effective\nconfidence estimation with GPT-4 with few-shot learning for event detection in\nthe BETTER ontology as a vehicle. The key innovation is expanding the prompt\nand task presented to GPT-4 to provide License to speculate when unsure and\nOpportunity to quantify and explain its uncertainty (L&O). This approach\nimproves accuracy and provides usable confidence measures (0.759 AUC) with no\nadditional machinery.", "published": "2024-08-01 21:08:07", "link": "http://arxiv.org/abs/2408.00914v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Towards Zero-Shot Annotation of the Built Environment with\n  Vision-Language Models (Vision Paper)", "abstract": "Equitable urban transportation applications require high-fidelity digital\nrepresentations of the built environment: not just streets and sidewalks, but\nbike lanes, marked and unmarked crossings, curb ramps and cuts, obstructions,\ntraffic signals, signage, street markings, potholes, and more. Direct\ninspections and manual annotations are prohibitively expensive at scale.\nConventional machine learning methods require substantial annotated training\ndata for adequate performance. In this paper, we consider vision language\nmodels as a mechanism for annotating diverse urban features from satellite\nimages, reducing the dependence on human annotation to produce large training\nsets. While these models have achieved impressive results in describing common\nobjects in images captured from a human perspective, their training sets are\nless likely to include strong signals for esoteric features in the built\nenvironment, and their performance in these settings is therefore unclear. We\ndemonstrate proof-of-concept combining a state-of-the-art vision language model\nand variants of a prompting strategy that asks the model to consider segmented\nelements independently of the original image. Experiments on two urban features\n-- stop lines and raised tables -- show that while direct zero-shot prompting\ncorrectly annotates nearly zero images, the pre-segmentation strategies can\nannotate images with near 40% intersection-over-union accuracy. We describe how\nthese results inform a new research agenda in automatic annotation of the built\nenvironment to improve equity, accessibility, and safety at broad scale and in\ndiverse environments.", "published": "2024-08-01 21:50:23", "link": "http://arxiv.org/abs/2408.00932v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Leveraging Large Language Models (LLMs) for Traffic Management at Urban\n  Intersections: The Case of Mixed Traffic Scenarios", "abstract": "Urban traffic management faces significant challenges due to the dynamic\nenvironments, and traditional algorithms fail to quickly adapt to this\nenvironment in real-time and predict possible conflicts. This study explores\nthe ability of a Large Language Model (LLM), specifically, GPT-4o-mini to\nimprove traffic management at urban intersections. We recruited GPT-4o-mini to\nanalyze, predict position, detect and resolve the conflicts at an intersection\nin real-time for various basic scenarios. The key findings of this study to\ninvestigate whether LLMs can logically reason and understand the scenarios to\nenhance the traffic efficiency and safety by providing real-time analysis. The\nstudy highlights the potential of LLMs in urban traffic management creating\nmore intelligent and more adaptive systems. Results showed the GPT-4o-mini was\neffectively able to detect and resolve conflicts in heavy traffic, congestion,\nand mixed-speed conditions. The complex scenario of multiple intersections with\nobstacles and pedestrians saw successful conflict management as well. Results\nshow that the integration of LLMs promises to improve the effectiveness of\ntraffic control for safer and more efficient urban intersection management.", "published": "2024-08-01 23:06:06", "link": "http://arxiv.org/abs/2408.00948v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Risks, Causes, and Mitigations of Widespread Deployments of Large\n  Language Models (LLMs): A Survey", "abstract": "Recent advancements in Large Language Models (LLMs), such as ChatGPT and\nLLaMA, have significantly transformed Natural Language Processing (NLP) with\ntheir outstanding abilities in text generation, summarization, and\nclassification. Nevertheless, their widespread adoption introduces numerous\nchallenges, including issues related to academic integrity, copyright,\nenvironmental impacts, and ethical considerations such as data bias, fairness,\nand privacy. The rapid evolution of LLMs also raises concerns regarding the\nreliability and generalizability of their evaluations. This paper offers a\ncomprehensive survey of the literature on these subjects, systematically\ngathered and synthesized from Google Scholar. Our study provides an in-depth\nanalysis of the risks associated with specific LLMs, identifying sub-risks,\ntheir causes, and potential solutions. Furthermore, we explore the broader\nchallenges related to LLMs, detailing their causes and proposing mitigation\nstrategies. Through this literature analysis, our survey aims to deepen the\nunderstanding of the implications and complexities surrounding these powerful\nmodels.", "published": "2024-08-01 21:21:18", "link": "http://arxiv.org/abs/2408.04643v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OmniParser for Pure Vision Based GUI Agent", "abstract": "The recent success of large vision language models shows great potential in\ndriving the agent system operating on user interfaces. However, we argue that\nthe power multimodal models like GPT-4V as a general agent on multiple\noperating systems across different applications is largely underestimated due\nto the lack of a robust screen parsing technique capable of: 1) reliably\nidentifying interactable icons within the user interface, and 2) understanding\nthe semantics of various elements in a screenshot and accurately associate the\nintended action with the corresponding region on the screen. To fill these\ngaps, we introduce \\textsc{OmniParser}, a comprehensive method for parsing user\ninterface screenshots into structured elements, which significantly enhances\nthe ability of GPT-4V to generate actions that can be accurately grounded in\nthe corresponding regions of the interface. We first curated an interactable\nicon detection dataset using popular webpages and an icon description dataset.\nThese datasets were utilized to fine-tune specialized models: a detection model\nto parse interactable regions on the screen and a caption model to extract the\nfunctional semantics of the detected elements. \\textsc{OmniParser}\nsignificantly improves GPT-4V's performance on ScreenSpot benchmark. And on\nMind2Web and AITW benchmark, \\textsc{OmniParser} with screenshot only input\noutperforms the GPT-4V baselines requiring additional information outside of\nscreenshot.", "published": "2024-08-01 00:00:43", "link": "http://arxiv.org/abs/2408.00203v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Clover-2: Accurate Inference for Regressive Lightweight Speculative\n  Decoding", "abstract": "Large Language Models (LLMs) frequently suffer from inefficiencies, largely\nattributable to the discord between the requirements of auto-regressive\ndecoding and the architecture of contemporary GPUs. Recently, regressive\nlightweight speculative decoding has garnered attention for its notable\nefficiency improvements in text generation tasks. This approach utilizes a\nlightweight regressive draft model, like a Recurrent Neural Network (RNN) or a\nsingle transformer decoder layer, leveraging sequential information to\niteratively predict potential tokens. Specifically, RNN draft models are\ncomputationally economical but tend to deliver lower accuracy, while attention\ndecoder layer models exhibit the opposite traits. This paper presents Clover-2,\nan advanced iteration of Clover, an RNN-based draft model designed to achieve\ncomparable accuracy to that of attention decoder layer models while maintaining\nminimal computational overhead. Clover-2 enhances the model architecture and\nincorporates knowledge distillation to increase Clover's accuracy and improve\noverall efficiency. We conducted experiments using the open-source Vicuna 7B\nand LLaMA3-Instruct 8B models. The results demonstrate that Clover-2 surpasses\nexisting methods across various model architectures, showcasing its efficacy\nand robustness.", "published": "2024-08-01 03:43:32", "link": "http://arxiv.org/abs/2408.00264v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bailing-TTS: Chinese Dialectal Speech Synthesis Towards Human-like\n  Spontaneous Representation", "abstract": "Large-scale text-to-speech (TTS) models have made significant progress\nrecently.However, they still fall short in the generation of Chinese dialectal\nspeech. Toaddress this, we propose Bailing-TTS, a family of large-scale TTS\nmodels capable of generating high-quality Chinese dialectal speech. Bailing-TTS\nserves as a foundation model for Chinese dialectal speech generation. First,\ncontinual semi-supervised learning is proposed to facilitate the alignment of\ntext tokens and speech tokens. Second, the Chinese dialectal representation\nlearning is developed using a specific transformer architecture and multi-stage\ntraining processes. With the proposed design of novel network architecture and\ncorresponding strategy, Bailing-TTS is able to generate Chinese dialectal\nspeech from text effectively and efficiently. Experiments demonstrate that\nBailing-TTS generates Chinese dialectal speech towards human-like spontaneous\nrepresentation. Readers are encouraged to listen to demos at\n\\url{https://c9412600.github.io/bltts_tech_report/index.html}.", "published": "2024-08-01 04:57:31", "link": "http://arxiv.org/abs/2408.00284v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ABC Align: Large Language Model Alignment for Safety & Accuracy", "abstract": "Alignment of Large Language Models (LLMs) remains an unsolved problem. Human\npreferences are highly distributed and can be captured at multiple levels of\nabstraction, from the individual to diverse populations. Organisational\npreferences, represented by standards and principles, are defined to mitigate\nreputational risk or meet legislative obligations. In this paper, we present\nABC Align, a novel alignment methodology for LLMs that enables integration of\nthe standards and preferences of a large media organisation into the LLM\nitself. We combine a set of data and methods that build on recent breakthroughs\nin synthetic data generation, preference optimisation, and post-training model\nquantisation. Our unified approach mitigates bias and improves accuracy, while\npreserving reasoning capability, as measured against standard benchmarks.", "published": "2024-08-01 06:06:25", "link": "http://arxiv.org/abs/2408.00307v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T50", "I.2.7"], "primary_category": "cs.LG"}
{"title": "GalleryGPT: Analyzing Paintings with Large Multimodal Models", "abstract": "Artwork analysis is important and fundamental skill for art appreciation,\nwhich could enrich personal aesthetic sensibility and facilitate the critical\nthinking ability. Understanding artworks is challenging due to its subjective\nnature, diverse interpretations, and complex visual elements, requiring\nexpertise in art history, cultural background, and aesthetic theory. However,\nlimited by the data collection and model ability, previous works for\nautomatically analyzing artworks mainly focus on classification, retrieval, and\nother simple tasks, which is far from the goal of AI. To facilitate the\nresearch progress, in this paper, we step further to compose comprehensive\nanalysis inspired by the remarkable perception and generation ability of large\nmultimodal models. Specifically, we first propose a task of composing paragraph\nanalysis for artworks, i.e., painting in this paper, only focusing on visual\ncharacteristics to formulate more comprehensive understanding of artworks. To\nsupport the research on formal analysis, we collect a large dataset\nPaintingForm, with about 19k painting images and 50k analysis paragraphs. We\nfurther introduce a superior large multimodal model for painting analysis\ncomposing, dubbed GalleryGPT, which is slightly modified and fine-tuned based\non LLaVA architecture leveraging our collected data. We conduct formal analysis\ngeneration and zero-shot experiments across several datasets to assess the\ncapacity of our model. The results show remarkable performance improvements\ncomparing with powerful baseline LMMs, demonstrating its superb ability of art\nanalysis and generalization. \\textcolor{blue}{The codes and model are available\nat: https://github.com/steven640pixel/GalleryGPT.", "published": "2024-08-01 11:52:56", "link": "http://arxiv.org/abs/2408.00491v1", "categories": ["cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Mitigating Multilingual Hallucination in Large Vision-Language Models", "abstract": "While Large Vision-Language Models (LVLMs) have exhibited remarkable\ncapabilities across a wide range of tasks, they suffer from hallucination\nproblems, where models generate plausible yet incorrect answers given the input\nimage-query pair. This hallucination phenomenon is even more severe when\nquerying the image in non-English languages, while existing methods for\nmitigating hallucinations in LVLMs only consider the English scenarios. In this\npaper, we make the first attempt to mitigate this important multilingual\nhallucination in LVLMs. With thorough experiment analysis, we found that\nmultilingual hallucination in LVLMs is a systemic problem that could arise from\ndeficiencies in multilingual capabilities or inadequate multimodal abilities.\nTo this end, we propose a two-stage Multilingual Hallucination Removal (MHR)\nframework for LVLMs, aiming to improve resistance to hallucination for both\nhigh-resource and low-resource languages. Instead of relying on the intricate\nmanual annotations of multilingual resources, we fully leverage the inherent\ncapabilities of the LVLM and propose a novel cross-lingual alignment method,\nwhich generates multiple responses for each image-query input and then\nidentifies the hallucination-aware pairs for each language. These data pairs\nare finally used for direct preference optimization to prompt the LVLMs to\nfavor non-hallucinating responses. Experimental results show that our MHR\nachieves a substantial reduction in hallucination generation for LVLMs.\nNotably, on our extended multilingual POPE benchmark, our framework delivers an\naverage increase of 19.0% in accuracy across 13 different languages. Our code\nand model weights are available at https://github.com/ssmisya/MHR", "published": "2024-08-01 13:34:35", "link": "http://arxiv.org/abs/2408.00550v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Alleviating Hallucination in Large Vision-Language Models with Active\n  Retrieval Augmentation", "abstract": "Despite the remarkable ability of large vision-language models (LVLMs) in\nimage comprehension, these models frequently generate plausible yet factually\nincorrect responses, a phenomenon known as hallucination.Recently, in large\nlanguage models (LLMs), augmenting LLMs by retrieving information from external\nknowledge resources has been proven as a promising solution to mitigate\nhallucinations.However, the retrieval augmentation in LVLM significantly lags\nbehind the widespread applications of LVLM. Moreover, when transferred to\naugmenting LVLMs, sometimes the hallucination degree of the model is even\nexacerbated.Motivated by the research gap and counter-intuitive phenomenon, we\nintroduce a novel framework, the Active Retrieval-Augmented large\nvision-language model (ARA), specifically designed to address hallucinations by\nincorporating three critical dimensions: (i) dissecting the retrieval targets\nbased on the inherent hierarchical structures of images. (ii) pinpointing the\nmost effective retrieval methods and filtering out the reliable retrieval\nresults. (iii) timing the retrieval process to coincide with episodes of low\ncertainty, while circumventing unnecessary retrieval during periods of high\ncertainty. To assess the capability of our proposed ARA model in reducing\nhallucination, we employ three widely used LVLM models (LLaVA-1.5, Qwen-VL, and\nmPLUG-Owl2) across four benchmarks. Our empirical observations suggest that by\nutilizing fitting retrieval mechanisms and timing the retrieval judiciously, we\ncan effectively mitigate the hallucination problem. We hope that this study can\nprovide deeper insights into how to adapt the retrieval augmentation to LVLMs\nfor reducing hallucinations with more effective retrieval and minimal retrieval\noccurrences.", "published": "2024-08-01 13:38:58", "link": "http://arxiv.org/abs/2408.00555v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SynesLM: A Unified Approach for Audio-visual Speech Recognition and\n  Translation via Language Model and Synthetic Data", "abstract": "In this work, we present SynesLM, an unified model which can perform three\nmultimodal language understanding tasks: audio-visual automatic speech\nrecognition(AV-ASR) and visual-aided speech/machine translation(VST/VMT).\nUnlike previous research that focused on lip motion as visual cues for speech\nsignals, our work explores more general visual information within entire\nframes, such as objects and actions. Additionally, we use synthetic image data\nto enhance the correlation between image and speech data. We benchmark SynesLM\nagainst the How2 dataset, demonstrating performance on par with\nstate-of-the-art (SOTA) models dedicated to AV-ASR while maintaining our\nmultitasking framework. Remarkably, for zero-shot AV-ASR, SynesLM achieved SOTA\nperformance by lowering the Word Error Rate (WER) from 43.4% to 39.4% on the\nVisSpeech Dataset. Furthermore, our results in VST and VMT outperform the\nprevious results, improving the BLEU score to 43.5 from 37.2 for VST, and to\n54.8 from 54.4 for VMT.", "published": "2024-08-01 15:09:32", "link": "http://arxiv.org/abs/2408.00624v1", "categories": ["eess.AS", "cs.CL", "cs.CV"], "primary_category": "eess.AS"}
{"title": "CERT-ED: Certifiably Robust Text Classification for Edit Distance", "abstract": "With the growing integration of AI in daily life, ensuring the robustness of\nsystems to inference-time attacks is crucial. Among the approaches for\ncertifying robustness to such adversarial examples, randomized smoothing has\nemerged as highly promising due to its nature as a wrapper around arbitrary\nblack-box models. Previous work on randomized smoothing in natural language\nprocessing has primarily focused on specific subsets of edit distance\noperations, such as synonym substitution or word insertion, without exploring\nthe certification of all edit operations. In this paper, we adapt Randomized\nDeletion (Huang et al., 2023) and propose, CERTified Edit Distance defense\n(CERT-ED) for natural language classification. Through comprehensive\nexperiments, we demonstrate that CERT-ED outperforms the existing Hamming\ndistance method RanMASK (Zeng et al., 2023) in 4 out of 5 datasets in terms of\nboth accuracy and the cardinality of the certificate. By covering various\nthreat models, including 5 direct and 5 transfer attacks, our method improves\nempirical robustness in 38 out of 50 settings.", "published": "2024-08-01 17:20:24", "link": "http://arxiv.org/abs/2408.00728v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tamper-Resistant Safeguards for Open-Weight LLMs", "abstract": "Rapid advances in the capabilities of large language models (LLMs) have\nraised widespread concerns regarding their potential for malicious use.\nOpen-weight LLMs present unique challenges, as existing safeguards lack\nrobustness to tampering attacks that modify model weights. For example, recent\nworks have demonstrated that refusal and unlearning safeguards can be trivially\nremoved with a few steps of fine-tuning. These vulnerabilities necessitate new\napproaches for enabling the safe release of open-weight LLMs. We develop a\nmethod, called TAR, for building tamper-resistant safeguards into open-weight\nLLMs such that adversaries cannot remove the safeguards even after hundreds of\nsteps of fine-tuning. In extensive evaluations and red teaming analyses, we\nfind that our method greatly improves tamper-resistance while preserving benign\ncapabilities. Our results demonstrate that progress on tamper-resistance is\npossible, opening up a promising new avenue to improve the safety and security\nof open-weight LLMs.", "published": "2024-08-01 17:59:12", "link": "http://arxiv.org/abs/2408.00761v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "AgentGen: Enhancing Planning Abilities for Large Language Model based\n  Agent via Environment and Task Generation", "abstract": "Large Language Model-based agents have garnered significant attention and are\nbecoming increasingly popular. Furthermore, planning ability is a crucial\ncomponent of an LLM-based agent, which generally entails achieving a desired\ngoal from an initial state. This paper investigates enhancing the planning\nabilities of LLMs through instruction tuning, referred to as agent training.\nRecent studies have demonstrated that utilizing expert-level trajectory for\ninstruction-tuning LLMs effectively enhances their planning capabilities.\nHowever, existing work primarily focuses on synthesizing trajectories from\nmanually designed planning tasks and environments. The labor-intensive nature\nof creating these environments and tasks impedes the generation of sufficiently\nvaried and extensive trajectories. To address this limitation, this paper\nexplores the automated synthesis of diverse environments and a gradual range of\nplanning tasks, from easy to difficult. We introduce a framework, AgentGen,\nthat leverages LLMs first to generate environments and subsequently generate\nplanning tasks conditioned on these environments. Specifically, to improve\nenvironmental diversity, we propose using an inspiration corpus composed of\nvarious domain-specific text segments as the context for synthesizing\nenvironments. Moreover, to increase the difficulty diversity of generated\nplanning tasks, we propose a bidirectional evolution method, Bi-Evol, that\nevolves planning tasks from easier and harder directions to synthesize a task\nset with a smoother difficulty curve. The evaluation results derived from\nAgentBoard show that AgentGen greatly improves LLMs' planning ability, e.g.,\nthe AgentGen instruction-tuned Llama-3.1-8B surpasses GPT-3.5 in overall\nperformance. Moreover, the AgentGen-tuned Llama-3.1-70B model achieves\nstate-of-the-art results in planning tasks. Project page:\nhttps://agent-gen.github.io/.", "published": "2024-08-01 17:59:46", "link": "http://arxiv.org/abs/2408.00764v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models\n  for Integrated Capabilities", "abstract": "MM-Vet, with open-ended vision-language questions targeting at evaluating\nintegrated capabilities, has become one of the most popular benchmarks for\nlarge multimodal model evaluation. MM-Vet assesses six core vision-language\n(VL) capabilities: recognition, knowledge, spatial awareness, language\ngeneration, OCR, and math. However, its question format is restricted to single\nimage-text pairs, lacking the interleaved image and text sequences prevalent in\nreal-world scenarios. To address this limitation, we introduce MM-Vet v2, which\nincludes a new VL capability called \"image-text sequence understanding\",\nevaluating models' ability to process VL sequences. Furthermore, we maintain\nthe high quality of evaluation samples while further expanding the evaluation\nset size. Using MM-Vet v2 to benchmark large multimodal models, we found that\nClaude 3.5 Sonnet is the best model with a score of 71.8, slightly\noutperforming GPT-4o which scored 71.0. Among open-weight models,\nInternVL2-Llama3-76B leads with a score of 68.4. The code, data, and\nleaderboard are accessible at https://github.com/yuweihao/MM-Vet.", "published": "2024-08-01 17:59:54", "link": "http://arxiv.org/abs/2408.00765v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "UniMoT: Unified Molecule-Text Language Model with Discrete Token\n  Representation", "abstract": "The remarkable success of Large Language Models (LLMs) across diverse tasks\nhas driven the research community to extend their capabilities to molecular\napplications. However, most molecular LLMs employ adapter-based architectures\nthat do not treat molecule and text modalities equally and lack a supervision\nsignal for the molecule modality. To address these issues, we introduce UniMoT,\na Unified Molecule-Text LLM adopting a tokenizer-based architecture that\nexpands the vocabulary of LLM with molecule tokens. Specifically, we introduce\na Vector Quantization-driven tokenizer that incorporates a Q-Former to bridge\nthe modality gap between molecule and text. This tokenizer transforms molecules\ninto sequences of molecule tokens with causal dependency, encapsulating\nhigh-level molecular and textual information. Equipped with this tokenizer,\nUniMoT can unify molecule and text modalities under a shared token\nrepresentation and an autoregressive training paradigm, enabling it to\ninterpret molecules as a foreign language and generate them as text. Following\na four-stage training scheme, UniMoT emerges as a multi-modal generalist\ncapable of performing both molecule-to-text and text-to-molecule tasks.\nExtensive experiments demonstrate that UniMoT achieves state-of-the-art\nperformance across a wide range of molecule comprehension and generation tasks.", "published": "2024-08-01 18:31:31", "link": "http://arxiv.org/abs/2408.00863v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic Pull Request Description Generation Using LLMs: A T5 Model\n  Approach", "abstract": "Developers create pull request (PR) descriptions to provide an overview of\ntheir changes and explain the motivations behind them. These descriptions help\nreviewers and fellow developers quickly understand the updates. Despite their\nimportance, some developers omit these descriptions. To tackle this problem, we\npropose an automated method for generating PR descriptions based on commit\nmessages and source code comments. This method frames the task as a text\nsummarization problem, for which we utilized the T5 text-to-text transfer\nmodel. We fine-tuned a pre-trained T5 model using a dataset containing 33,466\nPRs. The model's effectiveness was assessed using ROUGE metrics, which are\nrecognized for their strong alignment with human evaluations. Our findings\nreveal that the T5 model significantly outperforms LexRank, which served as our\nbaseline for comparison.", "published": "2024-08-01 21:22:16", "link": "http://arxiv.org/abs/2408.00921v1", "categories": ["cs.LG", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "Long-Term Conversation Analysis: Privacy-Utility Trade-off under Noise\n  and Reverberation", "abstract": "Recordings in everyday life require privacy preservation of the speech\ncontent and speaker identity. This contribution explores the influence of noise\nand reverberation on the trade-off between privacy and utility for low-cost\nprivacy-preserving methods feasible for edge computing. These methods\ncompromise spectral and temporal smoothing, speaker anonymization using the\nMcAdams coefficient, sampling with a very low sampling rate, and combinations.\nPrivacy is assessed by automatic speech and speaker recognition, while our\nutility considers voice activity detection and speaker diarization. Overall,\nour evaluation shows that additional noise degrades the performance of all\nmodels more than reverberation. This degradation corresponds to enhanced speech\nprivacy, while utility is less deteriorated for some methods.", "published": "2024-08-01 08:43:46", "link": "http://arxiv.org/abs/2408.00382v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Concerns for Self-Localization of Ad-Hoc Arrays Using Time Difference of\n  Arrivals", "abstract": "This document presents some insights and observations regarding the paper\nthat was published in IEEE Transactions on Signal Processing (TSP), titled\n\"Self-Localization of Ad-Hoc Arrays Using Time Difference of Arrivals\". In the\nspirit of constructive feedback, I wish to highlight two key areas of\nconsideration. The first pertains to aspects related to methodology,\nexperimental results, and statements made in the paper. The second part\naddresses specific equation/typographical errors. This work aims to initiate a\nconstructive dialogue concerning certain aspects of the paper published in IEEE\nTSP. Our intention is to provide feedback that contributes to the ongoing\nimprovement of the paper's robustness and clarity.", "published": "2024-08-01 17:23:22", "link": "http://arxiv.org/abs/2408.00732v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Iterative Prototype Refinement for Ambiguous Speech Emotion Recognition", "abstract": "Recognizing emotions from speech is a daunting task due to the subtlety and\nambiguity of expressions. Traditional speech emotion recognition (SER) systems,\nwhich typically rely on a singular, precise emotion label, struggle with this\ncomplexity. Therefore, modeling the inherent ambiguity of emotions is an urgent\nproblem. In this paper, we propose an iterative prototype refinement framework\n(IPR) for ambiguous SER. IPR comprises two interlinked components: contrastive\nlearning and class prototypes. The former provides an efficient way to obtain\nhigh-quality representations of ambiguous samples. The latter are dynamically\nupdated based on ambiguous labels -- the similarity of the ambiguous data to\nall prototypes. These refined embeddings yield precise pseudo labels, thus\nreinforcing representation quality. Experimental evaluations conducted on the\nIEMOCAP dataset validate the superior performance of IPR over state-of-the-art\nmethods, thus proving the effectiveness of our proposed method.", "published": "2024-08-01 06:52:32", "link": "http://arxiv.org/abs/2408.00325v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Interaural time difference loss for binaural target sound extraction", "abstract": "Binaural target sound extraction (TSE) aims to extract a desired sound from a\nbinaural mixture of arbitrary sounds while preserving the spatial cues of the\ndesired sound. Indeed, for many applications, the target sound signal and its\nspatial cues carry important information about the sound source. Binaural TSE\ncan be realized with a neural network trained to output only the desired sound\ngiven a binaural mixture and an embedding characterizing the desired sound\nclass as inputs. Conventional TSE systems are trained using signal-level\nlosses, which measure the difference between the extracted and reference\nsignals for the left and right channels. In this paper, we propose adding\nexplicit spatial losses to better preserve the spatial cues of the target\nsound. In particular, we explore losses aiming at preserving the interaural\nlevel (ILD), phase (IPD), and time differences (ITD). We show experimentally\nthat adding such spatial losses, particularly our newly proposed ITD loss,\nhelps preserve better spatial cues while maintaining the signal-level metrics.", "published": "2024-08-01 07:30:23", "link": "http://arxiv.org/abs/2408.00344v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Explainable and Interpretable Musical Difficulty Estimation: A\n  Parameter-efficient Approach", "abstract": "Estimating music piece difficulty is important for organizing educational\nmusic collections. This process could be partially automatized to facilitate\nthe educator's role. Nevertheless, the decisions performed by prevalent\ndeep-learning models are hardly understandable, which may impair the acceptance\nof such a technology in music education curricula. Our work employs explainable\ndescriptors for difficulty estimation in symbolic music representations.\nFurthermore, through a novel parameter-efficient white-box model, we outperform\nprevious efforts while delivering interpretable results. These comprehensible\noutcomes emulate the functionality of a rubric, a tool widely used in music\neducation. Our approach, evaluated in piano repertoire categorized in 9\nclasses, achieved 41.4% accuracy independently, with a mean squared error (MSE)\nof 1.7, showing precise difficulty estimation. Through our baseline, we\nillustrate how building on top of past research can offer alternatives for\nmusic difficulty assessment which are explainable and interpretable. With this,\nwe aim to promote a more effective communication between the Music Information\nRetrieval (MIR) community and the music education one.", "published": "2024-08-01 11:23:42", "link": "http://arxiv.org/abs/2408.00473v1", "categories": ["cs.SD", "cs.AI", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ChordSync: Conformer-Based Alignment of Chord Annotations to Music Audio", "abstract": "In the Western music tradition, chords are the main constituent components of\nharmony, a fundamental dimension of music. Despite its relevance for several\nMusic Information Retrieval (MIR) tasks, chord-annotated audio datasets are\nlimited and need more diversity. One way to improve those resources is to\nleverage the large number of chord annotations available online, but this\nrequires aligning them with music audio. However, existing audio-to-score\nalignment techniques, which typically rely on Dynamic Time Warping (DTW), fail\nto address this challenge, as they require weakly aligned data for precise\nsynchronisation. In this paper, we introduce ChordSync, a novel conformer-based\nmodel designed to seamlessly align chord annotations with audio, eliminating\nthe need for weak alignment. We also provide a pre-trained model and a\nuser-friendly library, enabling users to synchronise chord annotations with\naudio tracks effortlessly. In this way, ChordSync creates opportunities for\nharnessing crowd-sourced chord data for MIR, especially in audio chord\nestimation, thereby facilitating the generation of novel datasets.\nAdditionally, our system extends its utility to music education, enhancing\nmusic learning experiences by providing accurately aligned annotations, thus\nenabling learners to engage in synchronised musical practices.", "published": "2024-08-01 16:16:29", "link": "http://arxiv.org/abs/2408.00674v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS", "68P20", "I.2.6"], "primary_category": "cs.SD"}
{"title": "Expressive MIDI-format Piano Performance Generation", "abstract": "This work presents a generative neural network that's able to generate\nexpressive piano performance in MIDI format. The musical expressivity is\nreflected by vivid micro-timing, rich polyphonic texture, varied dynamics, and\nthe sustain pedal effects. This model is innovative from many aspects of data\nprocessing to neural network design. We claim that this symbolic music\ngeneration model overcame the common critics of symbolic music and is able to\ngenerate expressive music flows as good as, if not better than generations with\nraw audio. One drawback is that, due to the limited time for submission, the\nmodel is not fine-tuned and sufficiently trained, thus the generation may sound\nincoherent and random at certain points. Despite that, this model shows its\npowerful generative ability to generate expressive piano pieces.", "published": "2024-08-01 20:36:37", "link": "http://arxiv.org/abs/2408.00900v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
