{"title": "Improving Bidirectional Decoding with Dynamic Target Semantics in Neural\n  Machine Translation", "abstract": "Generally, Neural Machine Translation models generate target words in a\nleft-to-right (L2R) manner and fail to exploit any future (right) semantics\ninformation, which usually produces an unbalanced translation. Recent works\nattempt to utilize the right-to-left (R2L) decoder in bidirectional decoding to\nalleviate this problem. In this paper, we propose a novel \\textbf{D}ynamic\n\\textbf{I}nteraction \\textbf{M}odule (\\textbf{DIM}) to dynamically exploit\ntarget semantics from R2L translation for enhancing the L2R translation\nquality. Different from other bidirectional decoding approaches, DIM firstly\nextracts helpful target information through addressing and reading operations,\nthen updates target semantics for tracking the interactive history.\nAdditionally, we further introduce an \\textbf{agreement regularization} term\ninto the training objective to narrow the gap between L2R and R2L translations.\nExperimental results on NIST Chinese$\\Rightarrow$English and WMT'16\nEnglish$\\Rightarrow$Romanian translation tasks show that our system achieves\nsignificant improvements over baseline systems, which also reaches comparable\nresults compared to the state-of-the-art Transformer model with much fewer\nparameters of it.", "published": "2019-11-05 03:43:14", "link": "http://arxiv.org/abs/1911.01597v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowing What, How and Why: A Near Complete Solution for Aspect-based\n  Sentiment Analysis", "abstract": "Target-based sentiment analysis or aspect-based sentiment analysis (ABSA)\nrefers to addressing various sentiment analysis tasks at a fine-grained level,\nwhich includes but is not limited to aspect extraction, aspect sentiment\nclassification, and opinion extraction. There exist many solvers of the above\nindividual subtasks or a combination of two subtasks, and they can work\ntogether to tell a complete story, i.e. the discussed aspect, the sentiment on\nit, and the cause of the sentiment. However, no previous ABSA research tried to\nprovide a complete solution in one shot. In this paper, we introduce a new\nsubtask under ABSA, named aspect sentiment triplet extraction (ASTE).\nParticularly, a solver of this task needs to extract triplets (What, How, Why)\nfrom the inputs, which show WHAT the targeted aspects are, HOW their sentiment\npolarities are and WHY they have such polarities (i.e. opinion reasons). For\ninstance, one triplet from \"Waiters are very friendly and the pasta is simply\naverage\" could be ('Waiters', positive, 'friendly'). We propose a two-stage\nframework to address this task. The first stage predicts what, how and why in a\nunified model, and then the second stage pairs up the predicted what (how) and\nwhy from the first stage to output triplets. In the experiments, our framework\nhas set a benchmark performance in this novel triplet extraction task.\nMeanwhile, it outperforms a few strong baselines adapted from state-of-the-art\nrelated methods.", "published": "2019-11-05 04:36:52", "link": "http://arxiv.org/abs/1911.01616v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discrete Argument Representation Learning for Interactive Argument Pair\n  Identification", "abstract": "In this paper, we focus on extracting interactive argument pairs from two\nposts with opposite stances to a certain topic. Considering opinions are\nexchanged from different perspectives of the discussing topic, we study the\ndiscrete representations for arguments to capture varying aspects in\nargumentation languages (e.g., the debate focus and the participant behavior).\nMoreover, we utilize hierarchical structure to model post-wise information\nincorporating contextual knowledge. Experimental results on the large-scale\ndataset collected from CMV show that our proposed framework can significantly\noutperform the competitive baselines. Further analyses reveal why our model\nyields superior performance and prove the usefulness of our learned\nrepresentations.", "published": "2019-11-05 05:08:08", "link": "http://arxiv.org/abs/1911.01621v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Language Games for Advanced Natural Language Intelligence", "abstract": "We study the problem of adversarial language games, in which multiple agents\nwith conflicting goals compete with each other via natural language\ninteractions. While adversarial language games are ubiquitous in human\nactivities, little attention has been devoted to this field in natural language\nprocessing. In this work, we propose a challenging adversarial language game\ncalled Adversarial Taboo as an example, in which an attacker and a defender\ncompete around a target word. The attacker is tasked with inducing the defender\nto utter the target word invisible to the defender, while the defender is\ntasked with detecting the target word before being induced by the attacker. In\nAdversarial Taboo, a successful attacker must hide its intention and subtly\ninduce the defender, while a competitive defender must be cautious with its\nutterances and infer the intention of the attacker. Such language abilities can\nfacilitate many important downstream NLP tasks. To instantiate the game, we\ncreate a game environment and a competition platform. Comprehensive experiments\nand empirical studies on several baseline attack and defense strategies show\npromising and interesting results. Based on the analysis on the game and\nexperiments, we discuss multiple promising directions for future research.", "published": "2019-11-05 05:14:08", "link": "http://arxiv.org/abs/1911.01622v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incremental Sense Weight Training for the Interpretation of\n  Contextualized Word Embeddings", "abstract": "We present a novel online algorithm that learns the essence of each dimension\nin word embeddings by minimizing the within-group distance of contextualized\nembedding groups. Three state-of-the-art neural-based language models are used,\nFlair, ELMo, and BERT, to generate contextualized word embeddings such that\ndifferent embeddings are generated for the same word type, which are grouped by\ntheir senses manually annotated in the SemCor dataset. We hypothesize that not\nall dimensions are equally important for downstream tasks so that our algorithm\ncan detect unessential dimensions and discard them without hurting the\nperformance. To verify this hypothesis, we first mask dimensions determined\nunessential by our algorithm, apply the masked word embeddings to a word sense\ndisambiguation task (WSD), and compare its performance against the one achieved\nby the original embeddings. Several KNN approaches are experimented to\nestablish strong baselines for WSD. Our results show that the masked word\nembeddings do not hurt the performance and can improve it by 3%. Our work can\nbe used to conduct future research on the interpretability of contextualized\nembeddings.", "published": "2019-11-05 05:14:54", "link": "http://arxiv.org/abs/1911.01623v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coreference Resolution as Query-based Span Prediction", "abstract": "In this paper, we present an accurate and extensible approach for the\ncoreference resolution task. We formulate the problem as a span prediction\ntask, like in machine reading comprehension (MRC): A query is generated for\neach candidate mention using its surrounding context, and a span prediction\nmodule is employed to extract the text spans of the coreferences within the\ndocument using the generated query. This formulation comes with the following\nkey advantages: (1) The span prediction strategy provides the flexibility of\nretrieving mentions left out at the mention proposal stage; (2) In the MRC\nframework, encoding the mention and its context explicitly in a query makes it\npossible to have a deep and thorough examination of cues embedded in the\ncontext of coreferent mentions; and (3) A plethora of existing MRC datasets can\nbe used for data augmentation to improve the model's generalization capability.\nExperiments demonstrate significant performance boost over previous models,\nwith 87.5 (+2.5) F1 score on the GAP benchmark and 83.1 (+3.5) F1 score on the\nCoNLL-2012 benchmark.", "published": "2019-11-05 12:44:36", "link": "http://arxiv.org/abs/1911.01746v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deepening Hidden Representations from Pre-trained Language Models", "abstract": "Transformer-based pre-trained language models have proven to be effective for\nlearning contextualized language representation. However, current approaches\nonly take advantage of the output of the encoder's final layer when fine-tuning\nthe downstream tasks. We argue that only taking single layer's output restricts\nthe power of pre-trained representation. Thus we deepen the representation\nlearned by the model by fusing the hidden representation in terms of an\nexplicit HIdden Representation Extractor (HIRE), which automatically absorbs\nthe complementary representation with respect to the output from the final\nlayer. Utilizing RoBERTa as the backbone encoder, our proposed improvement over\nthe pre-trained models is shown effective on multiple natural language\nunderstanding tasks and help our model rival with the state-of-the-art models\non the GLUE benchmark.", "published": "2019-11-05 16:59:50", "link": "http://arxiv.org/abs/1911.01940v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Cross-lingual Representation Learning at Scale", "abstract": "This paper shows that pretraining multilingual language models at scale leads\nto significant performance gains for a wide range of cross-lingual transfer\ntasks. We train a Transformer-based masked language model on one hundred\nlanguages, using more than two terabytes of filtered CommonCrawl data. Our\nmodel, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a\nvariety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI,\n+13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs\nparticularly well on low-resource languages, improving 15.7% in XNLI accuracy\nfor Swahili and 11.4% for Urdu over previous XLM models. We also present a\ndetailed empirical analysis of the key factors that are required to achieve\nthese gains, including the trade-offs between (1) positive transfer and\ncapacity dilution and (2) the performance of high and low resource languages at\nscale. Finally, we show, for the first time, the possibility of multilingual\nmodeling without sacrificing per-language performance; XLM-R is very\ncompetitive with strong monolingual models on the GLUE and XNLI benchmarks. We\nwill make our code, data and models publicly available.", "published": "2019-11-05 22:42:00", "link": "http://arxiv.org/abs/1911.02116v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Deep Learning approach for Hindi Named Entity Recognition", "abstract": "Named Entity Recognition is one of the most important text processing\nrequirement in many NLP tasks. In this paper we use a deep architecture to\naccomplish the task of recognizing named entities in a given Hindi text\nsentence. Bidirectional Long Short Term Memory (BiLSTM) based techniques have\nbeen used for NER task in literature. In this paper, we first tune BiLSTM\nlow-resource scenario to work for Hindi NER and propose two enhancements namely\n(a) de-noising auto-encoder (DAE) LSTM and (b) conditioning LSTM which show\nimprovement in NER task compared to the BiLSTM approach. We use pre-trained\nword embedding to represent the words in the corpus, and the NER tags of the\nwords are as defined by the used annotated corpora. Experiments have been\nperformed to analyze the performance of different word embeddings and batch\nsizes which is essential for training deep models.", "published": "2019-11-05 05:47:22", "link": "http://arxiv.org/abs/1911.01421v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "embComp: Visual Interactive Comparison of Vector Embeddings", "abstract": "This paper introduces embComp, a novel approach for comparing two embeddings\nthat capture the similarity between objects, such as word and document\nembeddings. We survey scenarios where comparing these embedding spaces is\nuseful. From those scenarios, we derive common tasks, introduce visual analysis\nmethods that support these tasks, and combine them into a comprehensive system.\nOne of embComp's central features are overview visualizations that are based on\nmetrics for measuring differences in the local structure around objects.\nSummarizing these local metrics over the embeddings provides global overviews\nof similarities and differences. Detail views allow comparison of the local\nstructure around selected objects and relating this local information to the\nglobal views. Integrating and connecting all of these components, embComp\nsupports a range of analysis workflows that help understand similarities and\ndifferences between embedding spaces. We assess our approach by applying it in\nseveral use cases, including understanding corpora differences via word vector\nembeddings, and understanding algorithmic differences in generating embeddings.", "published": "2019-11-05 00:06:41", "link": "http://arxiv.org/abs/1911.01542v2", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Review-based Question Generation with Adaptive Instance Transfer and\n  Augmentation", "abstract": "Online reviews provide rich information about products and service, while it\nremains inefficient for potential consumers to exploit the reviews for\nfulfilling their specific information need. We propose to explore question\ngeneration as a new way of exploiting review information. One major challenge\nof this task is the lack of review-question pairs for training a neural\ngeneration model. We propose an iterative learning framework for handling this\nchallenge via adaptive transfer and augmentation of the training instances with\nthe help of the available user-posed question-answer data. To capture the\naspect characteristics in reviews, the augmentation and generation procedures\nincorporate related features extracted via unsupervised learning. Experiments\non data from 10 categories of a popular E-commerce site demonstrate the\neffectiveness of the framework, as well as the usefulness of the new task.", "published": "2019-11-05 01:19:50", "link": "http://arxiv.org/abs/1911.01556v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Sparse Lifting of Dense Vectors: Unifying Word and Sentence\n  Representations", "abstract": "As the first step in automated natural language processing, representing\nwords and sentences is of central importance and has attracted significant\nresearch attention. Different approaches, from the early one-hot and\nbag-of-words representation to more recent distributional dense and sparse\nrepresentations, were proposed. Despite the successful results that have been\nachieved, such vectors tend to consist of uninterpretable components and face\nnontrivial challenge in both memory and computational requirement in practical\napplications. In this paper, we designed a novel representation model that\nprojects dense word vectors into a higher dimensional space and favors a highly\nsparse and binary representation of word vectors with potentially interpretable\ncomponents, while trying to maintain pairwise inner products between original\nvectors as much as possible. Computationally, our model is relaxed as a\nsymmetric non-negative matrix factorization problem which admits a fast yet\neffective solution. In a series of empirical evaluations, the proposed model\nexhibited consistent improvement and high potential in practical applications.", "published": "2019-11-05 05:28:05", "link": "http://arxiv.org/abs/1911.01625v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Joint Model for Definition Extraction with Syntactic Connection and\n  Semantic Consistency", "abstract": "Definition Extraction (DE) is one of the well-known topics in Information\nExtraction that aims to identify terms and their corresponding definitions in\nunstructured texts. This task can be formalized either as a sentence\nclassification task (i.e., containing term-definition pairs or not) or a\nsequential labeling task (i.e., identifying the boundaries of the terms and\ndefinitions). The previous works for DE have only focused on one of the two\napproaches, failing to model the inter-dependencies between the two tasks. In\nthis work, we propose a novel model for DE that simultaneously performs the two\ntasks in a single framework to benefit from their inter-dependencies. Our model\nfeatures deep learning architectures to exploit the global structures of the\ninput sentences as well as the semantic consistencies between the terms and the\ndefinitions, thereby improving the quality of the representation vectors for\nDE. Besides the joint inference between sentence classification and sequential\nlabeling, the proposed model is fundamentally different from the prior work for\nDE in that the prior work has only employed the local structures of the input\nsentences (i.e., word-to-word relations), and not yet considered the semantic\nconsistencies between terms and definitions. In order to implement these novel\nideas, our model presents a multi-task learning framework that employs graph\nconvolutional neural networks and predicts the dependency paths between the\nterms and the definitions. We also seek to enforce the consistency between the\nrepresentations of the terms and definitions both globally (i.e., increasing\nsemantic consistency between the representations of the entire sentences and\nthe terms/definitions) and locally (i.e., promoting the similarity between the\nrepresentations of the terms and the definitions).", "published": "2019-11-05 09:23:58", "link": "http://arxiv.org/abs/1911.01678v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Slot Filling by Utilizing Contextual Information", "abstract": "Slot Filling (SF) is one of the sub-tasks of Spoken Language Understanding\n(SLU) which aims to extract semantic constituents from a given natural language\nutterance. It is formulated as a sequence labeling task. Recently, it has been\nshown that contextual information is vital for this task. However, existing\nmodels employ contextual information in a restricted manner, e.g., using\nself-attention. Such methods fail to distinguish the effects of the context on\nthe word representation and the word label. To address this issue, in this\npaper, we propose a novel method to incorporate the contextual information in\ntwo different levels, i.e., representation level and task-specific (i.e.,\nlabel) level. Our extensive experiments on three benchmark datasets on SF show\nthe effectiveness of our model leading to new state-of-the-art results on all\nthree benchmark datasets for the task of SF.", "published": "2019-11-05 09:29:07", "link": "http://arxiv.org/abs/1911.01680v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-Attention and Ingredient-Attention Based Model for Recipe Retrieval\n  from Image Queries", "abstract": "Direct computer vision based-nutrient content estimation is a demanding task,\ndue to deformation and occlusions of ingredients, as well as high intra-class\nand low inter-class variability between meal classes. In order to tackle these\nissues, we propose a system for recipe retrieval from images. The recipe\ninformation can subsequently be used to estimate the nutrient content of the\nmeal. In this study, we utilize the multi-modal Recipe1M dataset, which\ncontains over 1 million recipes accompanied by over 13 million images. The\nproposed model can operate as a first step in an automatic pipeline for the\nestimation of nutrition content by supporting hints related to ingredient and\ninstruction. Through self-attention, our model can directly process raw recipe\ntext, making the upstream instruction sentence embedding process redundant and\nthus reducing training time, while providing desirable retrieval results.\nFurthermore, we propose the use of an ingredient attention mechanism, in order\nto gain insight into which instructions, parts of instructions or single\ninstruction words are of importance for processing a single ingredient within a\ncertain recipe. Attention-based recipe text encoding contributes to solving the\nissue of high intra-class/low inter-class variability by focusing on\npreparation steps specific to the meal. The experimental results demonstrate\nthe potential of such a system for recipe retrieval from images. A comparison\nwith respect to two baseline methods is also presented.", "published": "2019-11-05 13:42:30", "link": "http://arxiv.org/abs/1911.01770v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Focus on What's Informative and Ignore What's not: Communication\n  Strategies in a Referential Game", "abstract": "Research in multi-agent cooperation has shown that artificial agents are able\nto learn to play a simple referential game while developing a shared lexicon.\nThis lexicon is not easy to analyze, as it does not show many properties of a\nnatural language. In a simple referential game with two neural network-based\nagents, we analyze the object-symbol mapping trying to understand what kind of\nstrategy was used to develop the emergent language. We see that, when the\nenvironment is uniformly distributed, the agents rely on a random subset of\nfeatures to describe the objects. When we modify the objects making one feature\nnon-uniformly distributed,the agents realize it is less informative and start\nto ignore it, and, surprisingly, they make a better use of the remaining\nfeatures. This interesting result suggests that more natural, less uniformly\ndistributed environments might aid in spurring the emergence of better-behaved\nlanguages.", "published": "2019-11-05 15:55:19", "link": "http://arxiv.org/abs/1911.01892v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Scenarios and Recommendations for Ethical Interpretive AI", "abstract": "Artificially intelligent systems, given a set of non-trivial ethical rules to\nfollow, will inevitably be faced with scenarios which call into question the\nscope of those rules. In such cases, human reasoners typically will engage in\ninterpretive reasoning, where interpretive arguments are used to support or\nattack claims that some rule should be understood a certain way. Artificially\nintelligent reasoners, however, currently lack the ability to carry out\nhuman-like interpretive reasoning, and we argue that bridging this gulf is of\ntremendous importance to human-centered AI. In order to better understand how\nfuture artificial reasoners capable of human-like interpretive reasoning must\nbe developed, we have collected a dataset of ethical rules, scenarios designed\nto invoke interpretive reasoning, and interpretations of those scenarios. We\nperform a qualitative analysis of our dataset, and summarize our findings in\nthe form of practical recommendations.", "published": "2019-11-05 16:23:01", "link": "http://arxiv.org/abs/1911.01917v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Data Diversification: A Simple Strategy For Neural Machine Translation", "abstract": "We introduce Data Diversification: a simple but effective strategy to boost\nneural machine translation (NMT) performance. It diversifies the training data\nby using the predictions of multiple forward and backward models and then\nmerging them with the original dataset on which the final NMT model is trained.\nOur method is applicable to all NMT models. It does not require extra\nmonolingual data like back-translation, nor does it add more computations and\nparameters like ensembles of models. Our method achieves state-of-the-art BLEU\nscores of 30.7 and 43.7 in the WMT'14 English-German and English-French\ntranslation tasks, respectively. It also substantially improves on 8 other\ntranslation tasks: 4 IWSLT tasks (English-German and English-French) and 4\nlow-resource translation tasks (English-Nepali and English-Sinhala). We\ndemonstrate that our method is more effective than knowledge distillation and\ndual learning, it exhibits strong correlation with ensembles of models, and it\ntrades perplexity off for better BLEU score. We have released our source code\nat https://github.com/nxphi47/data_diversification", "published": "2019-11-05 18:25:42", "link": "http://arxiv.org/abs/1911.01986v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language coverage and generalization in RNN-based continuous sentence\n  embeddings for interacting agents", "abstract": "Continuous sentence embeddings using recurrent neural networks (RNNs), where\nvariable-length sentences are encoded into fixed-dimensional vectors, are often\nthe main building blocks of architectures applied to language tasks such as\ndialogue generation. While it is known that those embeddings are able to learn\nsome structures of language (e.g. grammar) in a purely data-driven manner,\nthere is very little work on the objective evaluation of their ability to cover\nthe whole language space and to generalize to sentences outside the language\nbias of the training data. Using a manually designed context-free grammar (CFG)\nto generate a large-scale dataset of sentences related to the content of\nrealistic 3D indoor scenes, we evaluate the language coverage and\ngeneralization abilities of the most common continuous sentence embeddings\nbased on RNNs. We also propose a new embedding method based on arithmetic\ncoding, AriEL, that is not data-driven and that efficiently encodes in\ncontinuous space any sentence from the CFG. We find that RNN-based embeddings\nunderfit the training data and cover only a small subset of the language\ndefined by the CFG. They also fail to learn the underlying CFG and generalize\nto unbiased sentences from that same CFG. We found that AriEL provides an\ninsightful baseline.", "published": "2019-11-05 18:57:50", "link": "http://arxiv.org/abs/1911.02002v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Infusing Knowledge into the Textual Entailment Task Using Graph\n  Convolutional Networks", "abstract": "Textual entailment is a fundamental task in natural language processing. Most\napproaches for solving the problem use only the textual content present in\ntraining data. A few approaches have shown that information from external\nknowledge sources like knowledge graphs (KGs) can add value, in addition to the\ntextual content, by providing background knowledge that may be critical for a\ntask. However, the proposed models do not fully exploit the information in the\nusually large and noisy KGs, and it is not clear how it can be effectively\nencoded to be useful for entailment. We present an approach that complements\ntext-based entailment models with information from KGs by (1) using\nPersonalized PageR- ank to generate contextual subgraphs with reduced noise and\n(2) encoding these subgraphs using graph convolutional networks to capture KG\nstructure. Our technique extends the capability of text models exploiting\nstructural and semantic information found in KGs. We evaluate our approach on\nmultiple textual entailment datasets and show that the use of external\nknowledge helps improve prediction accuracy. This is particularly evident in\nthe challenging BreakingNLI dataset, where we see an absolute improvement of\n5-20% over multiple text-based entailment models.", "published": "2019-11-05 19:52:34", "link": "http://arxiv.org/abs/1911.02060v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Path-Based Contextualization of Knowledge Graphs for Textual Entailment", "abstract": "In this paper, we introduce the problem of knowledge graph contextualization\n-- that is, given a specific NLP task, the problem of extracting meaningful and\nrelevant sub-graphs from a given knowledge graph. The task in the case of this\npaper is the textual entailment problem, and the context is a relevant\nsub-graph for an instance of the textual entailment problem -- where given two\nsentences p and h, the entailment relationship between them has to be predicted\nautomatically. We base our methodology on finding paths in a cost-customized\nexternal knowledge graph, and building the most relevant sub-graph that\nconnects p and h. We show that our path selection mechanism to generate\nsub-graphs not only reduces noise, but also retrieves meaningful information\nfrom large knowledge graphs. Our evaluation shows that using information on\nentities as well as the relationships between them improves on the performance\nof purely text-based systems.", "published": "2019-11-05 21:06:04", "link": "http://arxiv.org/abs/1911.02085v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "LIDA: Lightweight Interactive Dialogue Annotator", "abstract": "Dialogue systems have the potential to change how people interact with\nmachines but are highly dependent on the quality of the data used to train\nthem. It is therefore important to develop good dialogue annotation tools which\ncan improve the speed and quality of dialogue data annotation. With this in\nmind, we introduce LIDA, an annotation tool designed specifically for\nconversation data. As far as we know, LIDA is the first dialogue annotation\nsystem that handles the entire dialogue annotation pipeline from raw text, as\nmay be the output of transcription services, to structured conversation data.\nFurthermore it supports the integration of arbitrary machine learning models as\nannotation recommenders and also has a dedicated interface to resolve\ninter-annotator disagreements such as after crowdsourcing annotations for a\ndataset. LIDA is fully open source, documented and publicly available [\nhttps://github.com/Wluper/lida ]", "published": "2019-11-05 03:49:20", "link": "http://arxiv.org/abs/1911.01599v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Integrating Dictionary Feature into A Deep Learning Model for Disease\n  Named Entity Recognition", "abstract": "In recent years, Deep Learning (DL) models are becoming important due to\ntheir demonstrated success at overcoming complex learning problems. DL models\nhave been applied effectively for different Natural Language Processing (NLP)\ntasks such as part-of-Speech (PoS) tagging and Machine Translation (MT).\nDisease Named Entity Recognition (Disease-NER) is a crucial task which aims at\nextracting disease Named Entities (NEs) from text. In this paper, a DL model\nfor Disease-NER using dictionary information is proposed and evaluated on\nNational Center for Biotechnology Information (NCBI) disease corpus and BC5CDR\ndataset. Word embeddings trained over general domain texts as well as\nbiomedical texts have been used to represent input to the proposed model. This\nstudy also compares two different Segment Representation (SR) schemes, namely\nIOB2 and IOBES for Disease-NER. The results illustrate that using dictionary\ninformation, pre-trained word embeddings, character embeddings and CRF with\nglobal score improves the performance of Disease-NER system.", "published": "2019-11-05 03:50:16", "link": "http://arxiv.org/abs/1911.01600v1", "categories": ["cs.CL", "cs.IR", "cs.NE"], "primary_category": "cs.CL"}
{"title": "RNN-T For Latency Controlled ASR With Improved Beam Search", "abstract": "Neural transducer-based systems such as RNN Transducers (RNN-T) for automatic\nspeech recognition (ASR) blend the individual components of a traditional\nhybrid ASR systems (acoustic model, language model, punctuation model, inverse\ntext normalization) into one single model. This greatly simplifies training and\ninference and hence makes RNN-T a desirable choice for ASR systems. In this\nwork, we investigate use of RNN-T in applications that require a tune-able\nlatency budget during inference time. We also improved the decoding speed of\nthe originally proposed RNN-T beam search algorithm. We evaluated our proposed\nsystem on English videos ASR dataset and show that neural RNN-T models can\nachieve comparable WER and better computational efficiency compared to a well\ntuned hybrid ASR baseline.", "published": "2019-11-05 05:46:52", "link": "http://arxiv.org/abs/1911.01629v2", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "DocParser: Hierarchical Structure Parsing of Document Renderings", "abstract": "Translating renderings (e. g. PDFs, scans) into hierarchical document\nstructures is extensively demanded in the daily routines of many real-world\napplications. However, a holistic, principled approach to inferring the\ncomplete hierarchical structure of documents is missing. As a remedy, we\ndeveloped \"DocParser\": an end-to-end system for parsing the complete document\nstructure - including all text elements, nested figures, tables, and table cell\nstructures. Our second contribution is to provide a dataset for evaluating\nhierarchical document structure parsing. Our third contribution is to propose a\nscalable learning framework for settings where domain-specific data are scarce,\nwhich we address by a novel approach to weak supervision that significantly\nimproves the document structure parsing performance. Our experiments confirm\nthe effectiveness of our proposed weak supervision: Compared to the baseline\nwithout weak supervision, it improves the mean average precision for detecting\ndocument entities by 39.1 % and improves the F1 score of classifying\nhierarchical relations by 35.8 %.", "published": "2019-11-05 10:42:08", "link": "http://arxiv.org/abs/1911.01702v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Training Neural Machine Translation (NMT) Models using Tensor Train\n  Decomposition on TensorFlow (T3F)", "abstract": "We implement a Tensor Train layer in the TensorFlow Neural Machine\nTranslation (NMT) model using the t3f library. We perform training runs on the\nIWSLT English-Vietnamese '15 and WMT German-English '16 datasets with learning\nrates $\\in \\{0.0004,0.0008,0.0012\\}$, maximum ranks $\\in \\{2,4,8,16\\}$ and a\nrange of core dimensions. We compare against a target BLEU test score of 24.0,\nobtained by our benchmark run. For the IWSLT English-Vietnamese training, we\nobtain BLEU test/dev scores of 24.0/21.9 and 24.2/21.9 using core dimensions\n$(2, 2, 256) \\times (2, 2, 512)$ with learning rate 0.0012 and rank\ndistributions $(1,4,4,1)$ and $(1,4,16,1)$ respectively. These runs use 113\\%\nand 397\\% of the flops of the benchmark run respectively. We find that, of the\nparameters surveyed, a higher learning rate and more `rectangular' core\ndimensions generally produce higher BLEU scores. For the WMT German-English\ndataset, we obtain BLEU scores of 24.0/23.8 using core dimensions $(4, 4, 128)\n\\times (4, 4, 256)$ with learning rate 0.0012 and rank distribution\n$(1,2,2,1)$. We discuss the potential for future optimization and application\nof Tensor Train decomposition to other NMT models.", "published": "2019-11-05 16:48:30", "link": "http://arxiv.org/abs/1911.01933v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Small-Footprint Keyword Spotting on Raw Audio Data with\n  Sinc-Convolutions", "abstract": "Keyword Spotting (KWS) enables speech-based user interaction on smart\ndevices. Always-on and battery-powered application scenarios for smart devices\nput constraints on hardware resources and power consumption, while also\ndemanding high accuracy as well as real-time capability. Previous architectures\nfirst extracted acoustic features and then applied a neural network to classify\nkeyword probabilities, optimizing towards memory footprint and execution time.\nCompared to previous publications, we took additional steps to reduce power and\nmemory consumption without reducing classification accuracy. Power-consuming\naudio preprocessing and data transfer steps are eliminated by directly\nclassifying from raw audio. For this, our end-to-end architecture extracts\nspectral features using parametrized Sinc-convolutions. Its memory footprint is\nfurther reduced by grouping depthwise separable convolutions. Our network\nachieves the competitive accuracy of 96.4% on Google's Speech Commands test set\nwith only 62k parameters.", "published": "2019-11-05 21:13:19", "link": "http://arxiv.org/abs/1911.02086v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Recurrent Instance Segmentation using Sequences of Referring Expressions", "abstract": "The goal of this work is to segment the objects in an image that are referred\nto by a sequence of linguistic descriptions (referring expressions). We propose\na deep neural network with recurrent layers that output a sequence of binary\nmasks, one for each referring expression provided by the user. The recurrent\nlayers in the architecture allow the model to condition each predicted mask on\nthe previous ones, from a spatial perspective within the same image. Our\nmultimodal approach uses off-the-shelf architectures to encode both the image\nand the referring expressions. The visual branch provides a tensor of pixel\nembeddings that are concatenated with the phrase embeddings produced by a\nlanguage encoder. Our experiments on the RefCOCO dataset for still images\nindicate how the proposed architecture successfully exploits the sequences of\nreferring expressions to solve a pixel-wise task of instance segmentation.", "published": "2019-11-05 21:49:55", "link": "http://arxiv.org/abs/1911.02103v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Contextual Grounding of Natural Language Entities in Images", "abstract": "In this paper, we introduce a contextual grounding approach that captures the\ncontext in corresponding text entities and image regions to improve the\ngrounding accuracy. Specifically, the proposed architecture accepts pre-trained\ntext token embeddings and image object features from an off-the-shelf object\ndetector as input. Additional encoding to capture the positional and spatial\ninformation can be added to enhance the feature quality. There are separate\ntext and image branches facilitating respective architectural refinements for\ndifferent modalities. The text branch is pre-trained on a large-scale masked\nlanguage modeling task while the image branch is trained from scratch. Next,\nthe model learns the contextual representations of the text tokens and image\nobjects through layers of high-order interaction respectively. The final\ngrounding head ranks the correspondence between the textual and visual\nrepresentations through cross-modal interaction. In the evaluation, we show\nthat our model achieves the state-of-the-art grounding accuracy of 71.36% over\nthe Flickr30K Entities dataset. No additional pre-training is necessary to\ndeliver competitive results compared with related work that often requires\ntask-agnostic and task-specific pre-training on cross-modal dadasets. The\nimplementation is publicly available at https://gitlab.com/necla-ml/grounding.", "published": "2019-11-05 23:23:58", "link": "http://arxiv.org/abs/1911.02133v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "MML: Maximal Multiverse Learning for Robust Fine-Tuning of Language\n  Models", "abstract": "Recent state-of-the-art language models utilize a two-phase training\nprocedure comprised of (i) unsupervised pre-training on unlabeled text, and\n(ii) fine-tuning for a specific supervised task. More recently, many studies\nhave been focused on trying to improve these models by enhancing the\npre-training phase, either via better choice of hyperparameters or by\nleveraging an improved formulation. However, the pre-training phase is\ncomputationally expensive and often done on private datasets. In this work, we\npresent a method that leverages BERT's fine-tuning phase to its fullest, by\napplying an extensive number of parallel classifier heads, which are enforced\nto be orthogonal, while adaptively eliminating the weaker heads during\ntraining. Our method allows the model to converge to an optimal number of\nparallel classifiers, depending on the given dataset at hand.\n  We conduct an extensive inter- and intra-dataset evaluations, showing that\nour method improves the robustness of BERT, sometimes leading to a +9\\% gain in\naccuracy. These results highlight the importance of a proper fine-tuning\nprocedure, especially for relatively smaller-sized datasets. Our code is\nattached as supplementary and our models will be made completely public.", "published": "2019-11-05 21:21:40", "link": "http://arxiv.org/abs/1911.06182v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Emotional speech synthesis with rich and granularized control", "abstract": "This paper proposes an effective emotion control method for an end-to-end\ntext-to-speech (TTS) system. To flexibly control the distinct characteristic of\na target emotion category, it is essential to determine embedding vectors\nrepresenting the TTS input. We introduce an inter-to-intra emotional distance\nratio algorithm to the embedding vectors that can minimize the distance to the\ntarget emotion category while maximizing its distance to the other emotion\ncategories. To further enhance the expressiveness of a target speech, we also\nintroduce an effective interpolation technique that enables the intensity of a\ntarget emotion to be gradually changed to that of neutral speech. Subjective\nevaluation results in terms of emotional expressiveness and controllability\nshow the superiority of the proposed algorithm to the conventional methods.", "published": "2019-11-05 06:14:35", "link": "http://arxiv.org/abs/1911.01635v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Enhancement via Deep Spectrum Image Translation Network", "abstract": "Quality and intelligibility of speech signals are degraded under additive\nbackground noise which is a critical problem for hearing aid and cochlear\nimplant users. Motivated to address this problem, we propose a novel speech\nenhancement approach using a deep spectrum image translation network. To this\nend, we suggest a new architecture, called VGG19-UNet, where a deep fully\nconvolutional network known as VGG19 is embedded at the encoder part of an\nimage-to-image translation network, i.e. U-Net. Moreover, we propose a\nperceptually-modified version of the spectrum image that is represented in Mel\nfrequency and power-law non-linearity amplitude domains, representing good\napproximations of human auditory perception model. By conducting experiments on\na real challenge in speech enhancement, i.e. unseen noise environments, we show\nthat the proposed approach outperforms other enhancement methods in terms of\nboth quality and intelligibility measures, represented by PESQ and ESTOI,\nrespectively.", "published": "2019-11-05 16:05:09", "link": "http://arxiv.org/abs/1911.01902v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Closing the Training/Inference Gap for Deep Attractor Networks", "abstract": "This paper improves the deep attractor network (DANet) approach by closing\nits gap between training and inference. During training, DANet relies on\nattractors, which are computed from the ground truth separations. As this\ninformation is not available at inference time, the attractors have to be\nestimated, which is typically done by k-means. This results in two mismatches:\nThe first mismatch stems from using classical k-means with Euclidean norm,\nwhereas masks are computed during training using the dot product similarity. By\nusing spherical k-means instead, we can show that we can already improve the\nperformance of DANet. Furthermore, we show that we can fully incorporate\nk-means clustering into the DANet training. This yields the benefit of having\nno training/inference gap and consequently results in an scale-invariant\nsignal-to-distortion ratio (SI-SDR) improvement of 1.1dB on the Wall Street\nJournal corpus (WSJ0).", "published": "2019-11-05 21:18:47", "link": "http://arxiv.org/abs/1911.02091v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Spatial Attention for Far-field Speech Recognition with Deep Beamforming\n  Neural Networks", "abstract": "In this paper, we introduce spatial attention for refining the information in\nmulti-direction neural beamformer for far-field automatic speech recognition.\nPrevious approaches of neural beamformers with multiple look directions, such\nas the factored complex linear projection, have shown promising results.\nHowever, the features extracted by such methods contain redundant information,\nas only the direction of the target speech is relevant. We propose using a\nspatial attention subnet to weigh the features from different directions, so\nthat the subsequent acoustic model could focus on the most relevant features\nfor the speech recognition. Our experimental results show that spatial\nattention achieves up to 9% relative word error rate improvement over methods\nwithout the attention.", "published": "2019-11-05 22:40:45", "link": "http://arxiv.org/abs/1911.02115v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ASVspoof 2019: A large-scale public database of synthesized, converted\n  and replayed speech", "abstract": "Automatic speaker verification (ASV) is one of the most natural and\nconvenient means of biometric person recognition. Unfortunately, just like all\nother biometric systems, ASV is vulnerable to spoofing, also referred to as\n\"presentation attacks.\" These vulnerabilities are generally unacceptable and\ncall for spoofing countermeasures or \"presentation attack detection\" systems.\nIn addition to impersonation, ASV systems are vulnerable to replay, speech\nsynthesis, and voice conversion attacks. The ASVspoof 2019 edition is the first\nto consider all three spoofing attack types within a single challenge. While\nthey originate from the same source database and same underlying protocol, they\nare explored in two specific use case scenarios. Spoofing attacks within a\nlogical access (LA) scenario are generated with the latest speech synthesis and\nvoice conversion technologies, including state-of-the-art neural acoustic and\nwaveform model techniques. Replay spoofing attacks within a physical access\n(PA) scenario are generated through carefully controlled simulations that\nsupport much more revealing analysis than possible previously. Also new to the\n2019 edition is the use of the tandem detection cost function metric, which\nreflects the impact of spoofing and countermeasures on the reliability of a\nfixed ASV system. This paper describes the database design, protocol, spoofing\nattack implementations, and baseline ASV and countermeasure results. It also\ndescribes a human assessment on spoofed data in logical access. It was\ndemonstrated that the spoofing data in the ASVspoof 2019 database have varied\ndegrees of perceived quality and similarity to the target speakers, including\nspoofed data that cannot be differentiated from bona-fide utterances even by\nhuman subjects.", "published": "2019-11-05 03:51:37", "link": "http://arxiv.org/abs/1911.01601v4", "categories": ["eess.AS", "cs.CR", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "OtoMechanic: Auditory Automobile Diagnostics via Query-by-Example", "abstract": "Early detection and repair of failing components in automobiles reduces the\nrisk of vehicle failure in life-threatening situations. Many automobile\ncomponents in need of repair produce characteristic sounds. For example, loose\ndrive belts emit a high-pitched squeaking sound, and bad starter motors have a\ncharacteristic whirring or clicking noise. Often drivers can tell that the\nsound of their car is not normal, but may not be able to identify the cause. To\nmitigate this knowledge gap, we have developed OtoMechanic, a web application\nto detect and diagnose vehicle component issues from their corresponding\nsounds. It compares a user's recording of a problematic sound to a database of\nannotated sounds caused by failing automobile components. OtoMechanic returns\nthe most similar sounds, and provides weblinks for more information on the\ndiagnosis associated with each sound, along with an estimate of the similarity\nof each retrieved sound. In user studies, we find that OtoMechanic\nsignificantly increases diagnostic accuracy relative to a baseline accuracy of\nconsumer performance.", "published": "2019-11-05 20:25:25", "link": "http://arxiv.org/abs/1911.02073v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
