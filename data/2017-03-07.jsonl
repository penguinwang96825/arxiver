{"title": "Building a Syllable Database to Solve the Problem of Khmer Word\n  Segmentation", "abstract": "Word segmentation is a basic problem in natural language processing. With the\nlanguages having the complex writing system like the Khmer language in Southern\nof Vietnam, this problem really very intractable, posing the significant\nchallenges. Although there are some experts in Vietnam as well as international\nhaving deeply researched this problem, there are still no reasonable results\nmeeting the demand, in particular, no treated thoroughly the ambiguous\nphenomenon, in the process of Khmer language processing so far. This paper\npresent a solution based on the syllable division into component clusters using\ntwo syllable models proposed, thereby building a Khmer syllable database, is\nstill not actually available. This method using a lexical database updated from\nthe online Khmer dictionaries and some supported dictionaries serving role of\ntraining data and complementary linguistic characteristics. Each component\ncluster is labelled and located by the first and last letter to identify\nentirety a syllable. This approach is workable and the test results achieve\nhigh accuracy, eliminate the ambiguity, contribute to solving the problem of\nword segmentation and applying efficiency in Khmer language processing.", "published": "2017-03-07 01:13:39", "link": "http://arxiv.org/abs/1703.02166v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning opacity in Stratal Maximum Entropy Grammar", "abstract": "Opaque phonological patterns are sometimes claimed to be difficult to learn;\nspecific hypotheses have been advanced about the relative difficulty of\nparticular kinds of opaque processes (Kiparsky 1971, 1973), and the kind of\ndata that will be helpful in learning an opaque pattern (Kiparsky 2000). In\nthis paper, we present a computationally implemented learning theory for one\ngrammatical theory of opacity: a Maximum Entropy version of Stratal OT\n(Berm\\'udez-Otero 1999, Kiparsky 2000), and test it on simplified versions of\nopaque French tense-lax vowel alternations and the opaque interaction of\ndiphthong raising and flapping in Canadian English. We find that the difficulty\nof opacity can be influenced by evidence for stratal affiliation: the Canadian\nEnglish case is easier if the learner encounters application of raising outside\nthe flapping context, or non-application of raising between words (i.e., <life>\nwith a raised vowel; <lie for> with a non-raised vowel).", "published": "2017-03-07 18:35:33", "link": "http://arxiv.org/abs/1703.02517v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistic Knowledge as Memory for Recurrent Neural Networks", "abstract": "Training recurrent neural networks to model long term dependencies is\ndifficult. Hence, we propose to use external linguistic knowledge as an\nexplicit signal to inform the model which memories it should utilize.\nSpecifically, external knowledge is used to augment a sequence with typed edges\nbetween arbitrarily distant elements, and the resulting graph is decomposed\ninto directed acyclic subgraphs. We introduce a model that encodes such graphs\nas explicit memory in recurrent neural networks, and use it to model\ncoreference relations in text. We apply our model to several text comprehension\ntasks and achieve new state-of-the-art results on all considered benchmarks,\nincluding CNN, bAbi, and LAMBADA. On the bAbi QA tasks, our model solves 15 out\nof the 20 tasks with only 1000 training examples per task. Analysis of the\nlearned representations further demonstrates the ability of our model to encode\nfine-grained entity information across a document.", "published": "2017-03-07 22:13:17", "link": "http://arxiv.org/abs/1703.02620v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Noising as Smoothing in Neural Network Language Models", "abstract": "Data noising is an effective technique for regularizing neural network\nmodels. While noising is widely adopted in application domains such as vision\nand speech, commonly used noising primitives have not been developed for\ndiscrete sequence-level settings such as language modeling. In this paper, we\nderive a connection between input noising in neural network language models and\nsmoothing in $n$-gram models. Using this connection, we draw upon ideas from\nsmoothing to develop effective noising schemes. We demonstrate performance\ngains when applying the proposed schemes to language modeling and machine\ntranslation. Finally, we provide empirical analysis validating the relationship\nbetween noising and smoothing.", "published": "2017-03-07 19:56:26", "link": "http://arxiv.org/abs/1703.02573v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Leveraging Large Amounts of Weakly Supervised Data for Multi-Language\n  Sentiment Classification", "abstract": "This paper presents a novel approach for multi-lingual sentiment\nclassification in short texts. This is a challenging task as the amount of\ntraining data in languages other than English is very limited. Previously\nproposed multi-lingual approaches typically require to establish a\ncorrespondence to English for which powerful classifiers are already available.\nIn contrast, our method does not require such supervision. We leverage large\namounts of weakly-supervised data in various languages to train a multi-layer\nconvolutional network and demonstrate the importance of using pre-training of\nsuch networks. We thoroughly evaluate our approach on various multi-lingual\ndatasets, including the recent SemEval-2016 sentiment prediction benchmark\n(Task 4), where we achieved state-of-the-art performance. We also compare the\nperformance of our model trained individually for each language to a variant\ntrained for all languages at once. We show that the latter model reaches\nslightly worse - but still acceptable - performance when compared to the single\nlanguage model, while benefiting from better generalization properties across\nlanguages.", "published": "2017-03-07 18:15:57", "link": "http://arxiv.org/abs/1703.02504v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram\n  Features", "abstract": "The recent tremendous success of unsupervised word embeddings in a multitude\nof applications raises the obvious question if similar methods could be derived\nto improve embeddings (i.e. semantic representations) of word sequences as\nwell. We present a simple but efficient unsupervised objective to train\ndistributed representations of sentences. Our method outperforms the\nstate-of-the-art unsupervised models on most benchmark tasks, highlighting the\nrobustness of the produced general-purpose sentence embeddings.", "published": "2017-03-07 18:19:11", "link": "http://arxiv.org/abs/1703.02507v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7"], "primary_category": "cs.CL"}
