{"title": "FewRel 2.0: Towards More Challenging Few-Shot Relation Classification", "abstract": "We present FewRel 2.0, a more challenging task to investigate two aspects of\nfew-shot relation classification models: (1) Can they adapt to a new domain\nwith only a handful of instances? (2) Can they detect none-of-the-above (NOTA)\nrelations? To construct FewRel 2.0, we build upon the FewRel dataset (Han et\nal., 2018) by adding a new test set in a quite different domain, and a NOTA\nrelation choice. With the new dataset and extensive experimental analysis, we\nfound (1) that the state-of-the-art few-shot relation classification models\nstruggle on these two aspects, and (2) that the commonly-used techniques for\ndomain adaptation and NOTA detection still cannot handle the two challenges\nwell. Our research calls for more attention and further efforts to these two\nreal-world issues. All details and resources about the dataset and baselines\nare released at https: //github.com/thunlp/fewrel.", "published": "2019-10-16 01:27:40", "link": "http://arxiv.org/abs/1910.07124v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficiency through Auto-Sizing: Notre Dame NLP's Submission to the WNGT\n  2019 Efficiency Task", "abstract": "This paper describes the Notre Dame Natural Language Processing Group's\n(NDNLP) submission to the WNGT 2019 shared task (Hayashi et al., 2019). We\ninvestigated the impact of auto-sizing (Murray and Chiang, 2015; Murray et al.,\n2019) to the Transformer network (Vaswani et al., 2017) with the goal of\nsubstantially reducing the number of parameters in the model. Our method was\nable to eliminate more than 25% of the model's parameters while suffering a\ndecrease of only 1.1 BLEU.", "published": "2019-10-16 02:22:07", "link": "http://arxiv.org/abs/1910.07134v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Question Answering for Fact-Checking", "abstract": "Recent Deep Learning (DL) models have succeeded in achieving human-level\naccuracy on various natural language tasks such as question-answering, natural\nlanguage inference (NLI), and textual entailment. These tasks not only require\nthe contextual knowledge but also the reasoning abilities to be solved\nefficiently. In this paper, we propose an unsupervised question-answering based\napproach for a similar task, fact-checking. We transform the FEVER dataset into\na Cloze-task by masking named entities provided in the claims. To predict the\nanswer token, we utilize pre-trained Bidirectional Encoder Representations from\nTransformers (BERT). The classifier computes label based on the correctly\nanswered questions and a threshold. Currently, the classifier is able to\nclassify the claims as \"SUPPORTS\" and \"MANUAL_REVIEW\". This approach achieves a\nlabel accuracy of 80.2% on the development set and 80.25% on the test set of\nthe transformed dataset.", "published": "2019-10-16 03:34:34", "link": "http://arxiv.org/abs/1910.07154v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Content Enhanced BERT-based Text-to-SQL Generation", "abstract": "We present a simple methods to leverage the table content for the BERT-based\nmodel to solve the text-to-SQL problem. Based on the observation that some of\nthe table content match some words in question string and some of the table\nheader also match some words in question string, we encode two addition feature\nvector for the deep model. Our methods also benefit the model inference in\ntesting time as the tables are almost the same in training and testing time. We\ntest our model on the WikiSQL dataset and outperform the BERT-based baseline by\n3.7% in logic form and 3.7% in execution accuracy and achieve state-of-the-art.", "published": "2019-10-16 05:50:35", "link": "http://arxiv.org/abs/1910.07179v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized\n  Model Performance", "abstract": "Pretraining deep language models has led to large performance gains in NLP.\nDespite this success, Schick and Sch\\\"utze (2020) recently showed that these\nmodels struggle to understand rare words. For static word embeddings, this\nproblem has been addressed by separately learning representations for rare\nwords. In this work, we transfer this idea to pretrained language models: We\nintroduce BERTRAM, a powerful architecture based on BERT that is capable of\ninferring high-quality embeddings for rare words that are suitable as input\nrepresentations for deep language models. This is achieved by enabling the\nsurface form and contexts of a word to interact with each other in a deep\narchitecture. Integrating BERTRAM into BERT leads to large performance\nincreases due to improved representations of rare and medium frequency words on\nboth a rare word probing task and three downstream tasks.", "published": "2019-10-16 06:14:42", "link": "http://arxiv.org/abs/1910.07181v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meemi: A Simple Method for Post-processing and Integrating Cross-lingual\n  Word Embeddings", "abstract": "Word embeddings have become a standard resource in the toolset of any Natural\nLanguage Processing practitioner. While monolingual word embeddings encode\ninformation about words in the context of a particular language, cross-lingual\nembeddings define a multilingual space where word embeddings from two or more\nlanguages are integrated together. Current state-of-the-art approaches learn\nthese embeddings by aligning two disjoint monolingual vector spaces through an\northogonal transformation which preserves the structure of the monolingual\ncounterparts. In this work, we propose to apply an additional transformation\nafter this initial alignment step, which aims to bring the vector\nrepresentations of a given word and its translations closer to their average.\nSince this additional transformation is non-orthogonal, it also affects the\nstructure of the monolingual spaces. We show that our approach both improves\nthe integration of the monolingual spaces as well as the quality of the\nmonolingual spaces themselves. Furthermore, because our transformation can be\napplied to an arbitrary number of languages, we are able to effectively obtain\na truly multilingual space. The resulting (monolingual and multilingual) spaces\nshow consistent gains over the current state-of-the-art in standard intrinsic\ntasks, namely dictionary induction and word similarity, as well as in extrinsic\ntasks such as cross-lingual hypernym discovery and cross-lingual natural\nlanguage inference.", "published": "2019-10-16 08:59:31", "link": "http://arxiv.org/abs/1910.07221v4", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Why can't memory networks read effectively?", "abstract": "Memory networks have been a popular choice among neural architectures for\nmachine reading comprehension and question answering. While recent work\nrevealed that memory networks can't truly perform multi-hop reasoning, we show\nin the present paper that vanilla memory networks are ineffective even in\nsingle-hop reading comprehension. We analyze the reasons for this on two\ncloze-style datasets, one from the medical domain and another including\nchildren's fiction. We find that the output classification layer with\nentity-specific weights, and the aggregation of passage information with\nrelatively flat attention distributions are the most important contributors to\npoor results. We propose network adaptations that can serve as simple remedies.\nWe also find that the presence of unseen answers at test time can dramatically\naffect the reported results, so we suggest controlling for this factor during\nevaluation.", "published": "2019-10-16 13:56:27", "link": "http://arxiv.org/abs/1910.07350v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Challenge Datasets for Task-Oriented Conversational Agents\n  through Self-Play", "abstract": "End-to-end neural approaches are becoming increasingly common in\nconversational scenarios due to their promising performances when provided with\nsufficient amount of data. In this paper, we present a novel methodology to\naddress the interpretability of neural approaches in such scenarios by creating\nchallenge datasets using dialogue self-play over multiple tasks/intents.\nDialogue self-play allows generating large amount of synthetic data; by taking\nadvantage of the complete control over the generation process, we show how\nneural approaches can be evaluated in terms of unseen dialogue patterns. We\npropose several out-of-pattern test cases each of which introduces a natural\nand unexpected user utterance phenomenon. As a proof of concept, we built a\nsingle and a multiple memory network, and show that these two architectures\nhave diverse performances depending on the peculiar dialogue patterns.", "published": "2019-10-16 14:07:42", "link": "http://arxiv.org/abs/1910.07357v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evolution of transfer learning in natural language processing", "abstract": "In this paper, we present a study of the recent advancements which have\nhelped bring Transfer Learning to NLP through the use of semi-supervised\ntraining. We discuss cutting-edge methods and architectures such as BERT, GPT,\nELMo, ULMFit among others. Classically, tasks in natural language processing\nhave been performed through rule-based and statistical methodologies. However,\nowing to the vast nature of natural languages these methods do not generalise\nwell and failed to learn the nuances of language. Thus machine learning\nalgorithms such as Naive Bayes and decision trees coupled with traditional\nmodels such as Bag-of-Words and N-grams were used to usurp this problem.\nEventually, with the advent of advanced recurrent neural network architectures\nsuch as the LSTM, we were able to achieve state-of-the-art performance in\nseveral natural language processing tasks such as text classification and\nmachine translation. We talk about how Transfer Learning has brought about the\nwell-known ImageNet moment for NLP. Several advanced architectures such as the\nTransformer and its variants have allowed practitioners to leverage knowledge\ngained from unrelated task to drastically fasten convergence and provide better\nperformance on the target task. This survey represents an effort at providing a\nsuccinct yet complete understanding of the recent advances in natural language\nprocessing using deep learning in with a special focus on detailing transfer\nlearning and its potential advantages.", "published": "2019-10-16 14:24:37", "link": "http://arxiv.org/abs/1910.07370v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging the Knowledge Gap: Enhancing Question Answering with World and\n  Domain Knowledge", "abstract": "In this paper we present OSCAR (Ontology-based Semantic Composition Augmented\nRegularization), a method for injecting task-agnostic knowledge from an\nOntology or knowledge graph into a neural network during pretraining. We\nevaluated the impact of including OSCAR when pretraining BERT with Wikipedia\narticles by measuring the performance when fine-tuning on two question\nanswering tasks involving world knowledge and causal reasoning and one\nrequiring domain (healthcare) knowledge and obtained 33:3%, 18:6%, and 4%\nimproved accuracy compared to pretraining BERT without OSCAR and obtaining new\nstate-of-the-art results on two of the tasks.", "published": "2019-10-16 15:44:09", "link": "http://arxiv.org/abs/1910.07429v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistic evaluation of German-English Machine Translation using a Test\n  Suite", "abstract": "We present the results of the application of a grammatical test suite for\nGerman$\\rightarrow$English MT on the systems submitted at WMT19, with a\ndetailed analysis for 107 phenomena organized in 14 categories. The systems\nstill translate wrong one out of four test items in average. Low performance is\nindicated for idioms, modals, pseudo-clefts, multi-word expressions and verb\nvalency. When compared to last year, there has been a improvement of function\nwords, non-verbal agreement and punctuation. More detailed conclusions about\nparticular systems and phenomena are also presented.", "published": "2019-10-16 16:27:26", "link": "http://arxiv.org/abs/1910.07457v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-grained evaluation of German-English Machine Translation based on a\n  Test Suite", "abstract": "We present an analysis of 16 state-of-the-art MT systems on German-English\nbased on a linguistically-motivated test suite. The test suite has been devised\nmanually by a team of language professionals in order to cover a broad variety\nof linguistic phenomena that MT often fails to translate properly. It contains\n5,000 test sentences covering 106 linguistic phenomena in 14 categories, with\nan increased focus on verb tenses, aspects and moods. The MT outputs are\nevaluated in a semi-automatic way through regular expressions that focus only\non the part of the sentence that is relevant to each phenomenon. Through our\nanalysis, we are able to compare systems based on their performance on these\ncategories. Additionally, we reveal strengths and weaknesses of particular\nsystems and we identify grammatical phenomena where the overall performance of\nMT is relatively low.", "published": "2019-10-16 16:36:07", "link": "http://arxiv.org/abs/1910.07460v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-grained evaluation of Quality Estimation for Machine translation\n  based on a linguistically-motivated Test Suite", "abstract": "We present an alternative method of evaluating Quality Estimation systems,\nwhich is based on a linguistically-motivated Test Suite. We create a test-set\nconsisting of 14 linguistic error categories and we gather for each of them a\nset of samples with both correct and erroneous translations. Then, we measure\nthe performance of 5 Quality Estimation systems by checking their ability to\ndistinguish between the correct and the erroneous translations. The detailed\nresults are much more informative about the ability of each system. The fact\nthat different Quality Estimation systems perform differently at various\nphenomena confirms the usefulness of the Test Suite.", "published": "2019-10-16 16:49:28", "link": "http://arxiv.org/abs/1910.07468v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Whole Document Context in Neural Machine Translation", "abstract": "In Machine Translation, considering the document as a whole can help to\nresolve ambiguities and inconsistencies. In this paper, we propose a simple yet\npromising approach to add contextual information in Neural Machine Translation.\nWe present a method to add source context that capture the whole document with\naccurate boundaries, taking every word into account. We provide this additional\ninformation to a Transformer model and study the impact of our method on three\nlanguage pairs. The proposed approach obtains promising results in the\nEnglish-German, English-French and French-English document-level translation\ntasks. We observe interesting cross-sentential behaviors where the model learns\nto use document-level information to improve translation coherence.", "published": "2019-10-16 17:15:35", "link": "http://arxiv.org/abs/1910.07481v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer ASR with Contextual Block Processing", "abstract": "The Transformer self-attention network has recently shown promising\nperformance as an alternative to recurrent neural networks (RNNs) in end-to-end\n(E2E) automatic speech recognition (ASR) systems. However, the Transformer has\na drawback in that the entire input sequence is required to compute\nself-attention. In this paper, we propose a new block processing method for the\nTransformer encoder by introducing a context-aware inheritance mechanism. An\nadditional context embedding vector handed over from the previously processed\nblock helps to encode not only local acoustic information but also global\nlinguistic, channel, and speaker attributes. We introduce a novel mask\ntechnique to implement the context inheritance to train the model efficiently.\nEvaluations of the Wall Street Journal (WSJ), Librispeech, VoxForge Italian,\nand AISHELL-1 Mandarin speech recognition datasets show that our proposed\ncontextual block processing method outperforms naive block processing\nconsistently. Furthermore, the attention weight tendency of each layer is\nanalyzed to clarify how the added contextual inheritance mechanism models the\nglobal information.", "published": "2019-10-16 08:04:07", "link": "http://arxiv.org/abs/1910.07204v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "A Probabilistic Framework for Learning Domain Specific Hierarchical Word\n  Embeddings", "abstract": "The meaning of a word often varies depending on its usage in different\ndomains. The standard word embedding models struggle to represent this\nvariation, as they learn a single global representation for a word. We propose\na method to learn domain-specific word embeddings, from text organized into\nhierarchical domains, such as reviews in an e-commerce website, where products\nfollow a taxonomy. Our structured probabilistic model allows vector\nrepresentations for the same word to drift away from each other for distant\ndomains in the taxonomy, to accommodate its domain-specific meanings. By\nlearning sets of domain-specific word representations jointly, our model can\nleverage domain relationships, and it scales well with the number of domains.\nUsing large real-world review datasets, we demonstrate the effectiveness of our\nmodel compared to state-of-the-art approaches, in learning domain-specific word\nembeddings that are both intuitive to humans and benefit downstream NLP tasks.", "published": "2019-10-16 13:24:54", "link": "http://arxiv.org/abs/1910.07333v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Imperial College London Submission to VATEX Video Captioning Task", "abstract": "This paper describes the Imperial College London team's submission to the\n2019' VATEX video captioning challenge, where we first explore two\nsequence-to-sequence models, namely a recurrent (GRU) model and a transformer\nmodel, which generate captions from the I3D action features. We then\ninvestigate the effect of dropping the encoder and the attention mechanism and\ninstead conditioning the GRU decoder over two different vectorial\nrepresentations: (i) a max-pooled action feature vector and (ii) the output of\na multi-label classifier trained to predict visual entities from the action\nfeatures. Our baselines achieved scores comparable to the official baseline.\nConditioning over entity predictions performed substantially better than\nconditioning on the max-pooled feature vector, and only marginally worse than\nthe GRU-based sequence-to-sequence baseline.", "published": "2019-10-16 17:22:25", "link": "http://arxiv.org/abs/1910.07482v1", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Right-wing German Hate Speech on Twitter: Analysis and Automatic\n  Detection", "abstract": "Discussion about the social network Twitter often concerns its role in\npolitical discourse, involving the question of when an expression of opinion\nbecomes offensive, immoral, and/or illegal, and how to deal with it. Given the\ngrowing amount of offensive communication on the internet, there is a demand\nfor new technology that can automatically detect hate speech, to assist content\nmoderation by humans. This comes with new challenges, such as defining exactly\nwhat is free speech and what is illegal in a specific country, and knowing\nexactly what the linguistic characteristics of hate speech are. To shed light\non the German situation, we analyzed over 50,000 right-wing German hate tweets\nposted between August 2017 and April 2018, at the time of the 2017 German\nfederal elections, using both quantitative and qualitative methods. In this\npaper, we discuss the results of the analysis and demonstrate how the insights\ncan be employed for the development of automatic detection systems.", "published": "2019-10-16 00:54:17", "link": "http://arxiv.org/abs/1910.07518v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Does Gender Matter? Towards Fairness in Dialogue Systems", "abstract": "Recently there are increasing concerns about the fairness of Artificial\nIntelligence (AI) in real-world applications such as computer vision and\nrecommendations. For example, recognition algorithms in computer vision are\nunfair to black people such as poorly detecting their faces and inappropriately\nidentifying them as \"gorillas\". As one crucial application of AI, dialogue\nsystems have been extensively applied in our society. They are usually built\nwith real human conversational data; thus they could inherit some fairness\nissues which are held in the real world. However, the fairness of dialogue\nsystems has not been well investigated. In this paper, we perform a pioneering\nstudy about the fairness issues in dialogue systems. In particular, we\nconstruct a benchmark dataset and propose quantitative measures to understand\nfairness in dialogue models. Our studies demonstrate that popular dialogue\nmodels show significant prejudice towards different genders and races. Besides,\nto mitigate the bias in dialogue systems, we propose two simple but effective\ndebiasing methods. Experiments show that our methods can reduce the bias in\ndialogue systems significantly. The dataset and the implementation are released\nto foster fairness research in dialogue systems.", "published": "2019-10-16 22:17:02", "link": "http://arxiv.org/abs/1910.10486v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automated Text Summarization for the Enhancement of Public Services", "abstract": "Natural language processing and machine learning algorithms have been shown\nto be effective in a variety of applications. In this work, we contribute to\nthe area of AI adoption in the public sector. We present an automated system\nthat was used to process textual information, generate important keywords, and\nautomatically summarize key elements of the Meadville community statements. We\nalso describe the process of collaboration with My Meadville administrators\nduring the development of our system. My Meadville, a community initiative,\nsupported by the city of Meadville conducted a large number of interviews with\nthe residents of Meadville during the community events and transcribed these\ninterviews into textual data files. Their goal was to uncover the issues of\nimportance to the Meadville residents in an attempt to enhance public services.\nOur AI system cleans and pre-processes the interview data, then using machine\nlearning algorithms it finds important keywords and key excerpts from each\ninterview. It also provides searching functionality to find excerpts from\nrelevant interviews based on specific keywords. Our automated system allowed\nthe city to save over 300 hours of human labor that would have taken to read\nall interviews and highlight important points. Our findings are being used by\nMy Meadville initiative to locate important information from the collected data\nset for ongoing community enhancement projects, to highlight relevant community\nassets, and to assist in identifying the steps to be taken based on the\nconcerns and areas of improvement identified by the community members.", "published": "2019-10-16 13:55:02", "link": "http://arxiv.org/abs/1910.10490v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HiGitClass: Keyword-Driven Hierarchical Classification of GitHub\n  Repositories", "abstract": "GitHub has become an important platform for code sharing and scientific\nexchange. With the massive number of repositories available, there is a\npressing need for topic-based search. Even though the topic label functionality\nhas been introduced, the majority of GitHub repositories do not have any\nlabels, impeding the utility of search and topic-based analysis. This work\ntargets the automatic repository classification problem as keyword-driven\nhierarchical classification. Specifically, users only need to provide a label\nhierarchy with keywords to supply as supervision. This setting is flexible,\nadaptive to the users' needs, accounts for the different granularity of topic\nlabels and requires minimal human effort. We identify three key challenges of\nthis problem, namely (1) the presence of multi-modal signals; (2) supervision\nscarcity and bias; (3) supervision format mismatch. In recognition of these\nchallenges, we propose the HiGitClass framework, comprising of three modules:\nheterogeneous information network embedding; keyword enrichment; topic modeling\nand pseudo document generation. Experimental results on two GitHub repository\ncollections confirm that HiGitClass is superior to existing weakly-supervised\nand dataless hierarchical classification methods, especially in its ability to\nintegrate both structured and unstructured data for repository classification.", "published": "2019-10-16 01:05:26", "link": "http://arxiv.org/abs/1910.07115v2", "categories": ["cs.LG", "cs.CL", "cs.SE", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Analyzing the Forgetting Problem in the Pretrain-Finetuning of Dialogue\n  Response Models", "abstract": "In this work, we study how the finetuning stage in the pretrain-finetune\nframework changes the behavior of a pretrained neural language generator. We\nfocus on the transformer encoder-decoder model for the open-domain dialogue\nresponse generation task. Our major finding is that after standard finetuning,\nthe model forgets some of the important language generation skills acquired\nduring large-scale pretraining. We demonstrate the forgetting phenomenon\nthrough a set of detailed behavior analysis from the perspectives of knowledge\ntransfer, context sensitivity, and function space projection. As a preliminary\nattempt to alleviate the forgetting problem, we propose an intuitive finetuning\nstrategy named \"mix-review\". We find that mix-review effectively regularizes\nthe finetuning process, and the forgetting problem is alleviated to some\nextent. Finally, we discuss interesting behavior of the resulting dialogue\nmodel and its implications.", "published": "2019-10-16 01:10:10", "link": "http://arxiv.org/abs/1910.07117v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Joint Learning of Word and Label Embeddings for Sequence Labelling in\n  Spoken Language Understanding", "abstract": "We propose an architecture to jointly learn word and label embeddings for\nslot filling in spoken language understanding. The proposed approach encodes\nlabels using a combination of word embeddings and straightforward word-label\nassociation from the training data. Compared to the state-of-the-art methods,\nour approach does not require label embeddings as part of the input and\ntherefore lends itself nicely to a wide range of model architectures. In\naddition, our architecture computes contextual distances between words and\nlabels to avoid adding contextual windows, thus reducing memory footprint. We\nvalidate the approach on established spoken dialogue datasets and show that it\ncan achieve state-of-the-art performance with much fewer trainable parameters.", "published": "2019-10-16 03:28:14", "link": "http://arxiv.org/abs/1910.07150v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lead2Gold: Towards exploiting the full potential of noisy transcriptions\n  for speech recognition", "abstract": "The transcriptions used to train an Automatic Speech Recognition (ASR) system\nmay contain errors. Usually, either a quality control stage discards\ntranscriptions with too many errors, or the noisy transcriptions are used as\nis. We introduce Lead2Gold, a method to train an ASR system that exploits the\nfull potential of noisy transcriptions. Based on a noise model of transcription\nerrors, Lead2Gold searches for better transcriptions of the training data with\na beam search that takes this noise model into account. The beam search is\ndifferentiable and does not require a forced alignment step, thus the whole\nsystem is trained end-to-end. Lead2Gold can be viewed as a new loss function\nthat can be used on top of any sequence-to-sequence deep neural network. We\nconduct proof-of-concept experiments on noisy transcriptions generated from\nletter corruptions with different noise levels. We show that Lead2Gold obtains\na better ASR accuracy than a competitive baseline which does not account for\nthe (artificially-introduced) transcription noise.", "published": "2019-10-16 12:55:34", "link": "http://arxiv.org/abs/1910.07323v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS", "I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Root Mean Square Layer Normalization", "abstract": "Layer normalization (LayerNorm) has been successfully applied to various deep\nneural networks to help stabilize training and boost model convergence because\nof its capability in handling re-centering and re-scaling of both inputs and\nweight matrix. However, the computational overhead introduced by LayerNorm\nmakes these improvements expensive and significantly slows the underlying\nnetwork, e.g. RNN in particular. In this paper, we hypothesize that\nre-centering invariance in LayerNorm is dispensable and propose root mean\nsquare layer normalization, or RMSNorm. RMSNorm regularizes the summed inputs\nto a neuron in one layer according to root mean square (RMS), giving the model\nre-scaling invariance property and implicit learning rate adaptation ability.\nRMSNorm is computationally simpler and thus more efficient than LayerNorm. We\nalso present partial RMSNorm, or pRMSNorm where the RMS is estimated from p% of\nthe summed inputs without breaking the above properties. Extensive experiments\non several tasks using diverse network architectures show that RMSNorm achieves\ncomparable performance against LayerNorm but reduces the running time by 7%~64%\non different models. Source code is available at\nhttps://github.com/bzhangGo/rmsnorm.", "published": "2019-10-16 16:44:22", "link": "http://arxiv.org/abs/1910.07467v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "MLQA: Evaluating Cross-lingual Extractive Question Answering", "abstract": "Question answering (QA) models have shown rapid progress enabled by the\navailability of large, high-quality benchmark datasets. Such annotated datasets\nare difficult and costly to collect, and rarely exist in languages other than\nEnglish, making training QA systems in other languages challenging. An\nalternative to building large monolingual training datasets is to develop\ncross-lingual systems which can transfer to a target language without requiring\ntraining data in that language. In order to develop such systems, it is crucial\nto invest in high quality multilingual evaluation benchmarks to measure\nprogress. We present MLQA, a multi-way aligned extractive QA evaluation\nbenchmark intended to spur research in this area. MLQA contains QA instances in\n7 languages, namely English, Arabic, German, Spanish, Hindi, Vietnamese and\nSimplified Chinese. It consists of over 12K QA instances in English and 5K in\neach other language, with each QA instance being parallel between 4 languages\non average. MLQA is built using a novel alignment context strategy on Wikipedia\narticles, and serves as a cross-lingual extension to existing extractive QA\ndatasets. We evaluate current state-of-the-art cross-lingual representations on\nMLQA, and also provide machine-translation-based baselines. In all cases,\ntransfer results are shown to be significantly behind training-language\nperformance.", "published": "2019-10-16 17:05:21", "link": "http://arxiv.org/abs/1910.07475v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Designing Style Matching Conversational Agents", "abstract": "Advances in machine intelligence have enabled conversational interfaces that\nhave the potential to radically change the way humans interact with machines.\nHowever, even with the progress in the abilities of these agents, there remain\ncritical gaps in their capacity for natural interactions. One limitation is\nthat the agents are often monotonic in behavior and do not adapt to their\npartner. We built two end-to-end conversational agents: a voice-based agent\nthat can engage in naturalistic, multi-turn dialogue and align with the\ninterlocutor's conversational style, and a 2nd, expressive, embodied\nconversational agent (ECA) that can recognize human behavior during open-ended\nconversations and automatically align its responses to the visual and\nconversational style of the other party. The embodied conversational agent\nleverages multimodal inputs to produce rich and perceptually valid vocal and\nfacial responses (e.g., lip syncing and expressions) during the conversation.\nBased on empirical results from a set of user studies, we highlight several\nsignificant challenges in building such systems and provide design guidelines\nfor multi-turn dialogue interactions using style adaptation for future\nresearch.", "published": "2019-10-16 17:58:36", "link": "http://arxiv.org/abs/1910.07514v1", "categories": ["cs.HC", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Memory-Augmented Recurrent Networks for Dialogue Coherence", "abstract": "Recent dialogue approaches operate by reading each word in a conversation\nhistory, and aggregating accrued dialogue information into a single state. This\nfixed-size vector is not expandable and must maintain a consistent format over\ntime. Other recent approaches exploit an attention mechanism to extract useful\ninformation from past conversational utterances, but this introduces an\nincreased computational complexity. In this work, we explore the use of the\nNeural Turing Machine (NTM) to provide a more permanent and flexible storage\nmechanism for maintaining dialogue coherence. Specifically, we introduce two\nseparate dialogue architectures based on this NTM design. The first design\nfeatures a sequence-to-sequence architecture with two separate NTM modules, one\nfor each participant in the conversation. The second memory architecture\nincorporates a single NTM module, which stores parallel context information for\nboth speakers. This second design also replaces the sequence-to-sequence\narchitecture with a neural language model, to allow for longer context of the\nNTM and greater understanding of the dialogue history. We report perplexity\nperformance for both models, and compare them to existing baselines.", "published": "2019-10-16 15:59:33", "link": "http://arxiv.org/abs/1910.10487v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Injecting Hierarchy with U-Net Transformers", "abstract": "The Transformer architecture has become increasingly popular over the past\ntwo years, owing to its impressive performance on a number of natural language\nprocessing (NLP) tasks. However, all Transformer computations occur at the\nlevel of word representations and therefore, it may be argued that Transformer\nmodels do not explicitly attempt to learn hierarchical structure which is\nwidely assumed to be integral to language. In the present work, we introduce\nhierarchical processing into the Transformer model, taking inspiration from the\nU-Net architecture, popular in computer vision for its hierarchical view of\nnatural images. We empirically demonstrate that the proposed architecture\noutperforms both the vanilla Transformer and some strong baselines in the\ndomain of chit-chat dialogue.", "published": "2019-10-16 15:48:46", "link": "http://arxiv.org/abs/1910.10488v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "BUT System Description to VoxCeleb Speaker Recognition Challenge 2019", "abstract": "In this report, we describe the submission of Brno University of Technology\n(BUT) team to the VoxCeleb Speaker Recognition Challenge (VoxSRC) 2019. We also\nprovide a brief analysis of different systems on VoxCeleb-1 test sets.\nSubmitted systems for both Fixed and Open conditions are a fusion of 4\nConvolutional Neural Network (CNN) topologies. The first and second networks\nhave ResNet34 topology and use two-dimensional CNNs. The last two networks are\none-dimensional CNN and are based on the x-vector extraction topology. Some of\nthe networks are fine-tuned using additive margin angular softmax. Kaldi FBanks\nand Kaldi PLPs were used as features. The difference between Fixed and Open\nsystems lies in the used training data and fusion strategy. The best systems\nfor Fixed and Open conditions achieved 1.42% and 1.26% ERR on the challenge\nevaluation set respectively.", "published": "2019-10-16 11:27:27", "link": "http://arxiv.org/abs/1910.12592v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Contextual Joint Factor Acoustic Embeddings", "abstract": "Embedding acoustic information into fixed length representations is of\ninterest for a whole range of applications in speech and audio technology. Two\nnovel unsupervised approaches to generate acoustic embeddings by modelling of\nacoustic context are proposed. The first approach is a contextual joint factor\nsynthesis encoder, where the encoder in an encoder/decoder framework is trained\nto extract joint factors from surrounding audio frames to best generate the\ntarget output. The second approach is a contextual joint factor analysis\nencoder, where the encoder is trained to analyse joint factors from the source\nsignal that correlates best with the neighbouring audio. To evaluate the\neffectiveness of our approaches compared to prior work, two tasks are conducted\n-- phone classification and speaker recognition -- and test on different TIMIT\ndata sets. Experimental results show that one of the proposed approaches\noutperforms phone classification baselines, yielding a classification accuracy\nof 74.1%. When using additional out-of-domain data for training, an additional\n3% improvements can be obtained, for both for phone classification and speaker\nrecognition tasks.", "published": "2019-10-16 20:36:41", "link": "http://arxiv.org/abs/1910.07601v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Audio-Conditioned U-Net for Position Estimation in Full Sheet Images", "abstract": "The goal of score following is to track a musical performance, usually in the\nform of audio, in a corresponding score representation. Established methods\nmainly rely on computer-readable scores in the form of MIDI or MusicXML and\nachieve robust and reliable tracking results. Recently, multimodal deep\nlearning methods have been used to follow along musical performances in raw\nsheet images. Among the current limits of these systems is that they require a\nnon trivial amount of preprocessing steps that unravel the raw sheet image into\na single long system of staves. The current work is an attempt at removing this\nparticular limitation. We propose an architecture capable of estimating\nmatching score positions directly within entire unprocessed sheet images. We\nargue that this is a necessary first step towards a fully integrated score\nfollowing system that does not rely on any preprocessing steps such as optical\nmusic recognition.", "published": "2019-10-16 09:58:27", "link": "http://arxiv.org/abs/1910.07254v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Frequency and temporal convolutional attention for text-independent\n  speaker recognition", "abstract": "Majority of the recent approaches for text-independent speaker recognition\napply attention or similar techniques for aggregation of frame-level feature\ndescriptors generated by a deep neural network (DNN) front-end. In this paper,\nwe propose methods of convolutional attention for independently modelling\ntemporal and frequency information in a convolutional neural network (CNN)\nbased front-end. Our system utilizes convolutional block attention modules\n(CBAMs) [1] appropriately modified to accommodate spectrogram inputs. The\nproposed CNN front-end fitted with the proposed convolutional attention modules\noutperform the no-attention and spatial-CBAM baselines by a significant margin\non the VoxCeleb [2, 3] speaker verification benchmark, and our best model\nachieves an equal error rate of 2:031% on the VoxCeleb1 test set, improving the\nexisting state of the art result by a significant margin. For a more thorough\nassessment of the effects of frequency and temporal attention in real-world\nconditions, we conduct ablation experiments by randomly dropping frequency bins\nand temporal frames from the input spectrograms, concluding that instead of\nmodelling either of the entities, simultaneously modelling temporal and\nfrequency attention translates to better real-world performance.", "published": "2019-10-16 14:14:53", "link": "http://arxiv.org/abs/1910.07364v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
