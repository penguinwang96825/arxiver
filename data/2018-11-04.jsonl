{"title": "Elastic CRFs for Open-ontology Slot Filling", "abstract": "Slot filling is a crucial component in task-oriented dialog systems that is\nused to parse (user) utterances into semantic concepts called slots. An\nontology is defined by the collection of slots and the values that each slot\ncan take. The most widely used practice of treating slot filling as a sequence\nlabeling task suffers from two main drawbacks. First, the ontology is usually\npre-defined and fixed and therefore is not able to detect new labels for unseen\nslots. Second, the one-hot encoding of slot labels ignores the correlations\nbetween slots with similar semantics, which makes it difficult to share\nknowledge learned across different domains. To address these problems, we\npropose a new model called elastic conditional random field (eCRF), where each\nslot is represented by the embedding of its natural language description and\nmodeled by a CRF layer. New slot values can be detected by eCRF whenever a\nlanguage description is available for the slot. In our experiment, we show that\neCRFs outperform existing models in both in-domain and cross-domain tasks,\nespecially in predicting unseen slots and values.", "published": "2018-11-04 07:38:17", "link": "http://arxiv.org/abs/1811.01331v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Confidence Network aided Gated Attention based Recurrent\n  Neural Network for Clickbait Detection", "abstract": "Clickbaits are catchy headlines that are frequently used by social media\noutlets in order to allure its viewers into clicking them and thus leading them\nto dubious content. Such venal schemes thrive on exploiting the curiosity of\nnaive social media users, directing traffic to web pages that won't be visited\notherwise. In this paper, we propose a novel, semi-supervised classification\nbased approach, that employs attentions sampled from a Gumbel-Softmax\ndistribution to distill contexts that are fairly important in clickbait\ndetection. An additional loss over the attention weights is used to encode\nprior knowledge. Furthermore, we propose a confidence network that enables\nlearning over weak labels and improves robustness to noisy labels. We show that\nwith merely 30% of strongly labeled samples we can achieve over 97% of the\naccuracy, of current state of the art methods in clickbait detection.", "published": "2018-11-04 12:00:37", "link": "http://arxiv.org/abs/1811.01355v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Zero-Shot Translation of Low-Resource Languages", "abstract": "Recent work on multilingual neural machine translation reported competitive\nperformance with respect to bilingual models and surprisingly good performance\neven on (zeroshot) translation directions not observed at training time. We\ninvestigate here a zero-shot translation in a particularly lowresource\nmultilingual setting. We propose a simple iterative training procedure that\nleverages a duality of translations directly generated by the system for the\nzero-shot directions. The translations produced by the system (sub-optimal\nsince they contain mixed language from the shared vocabulary), are then used\ntogether with the original parallel data to feed and iteratively re-train the\nmultilingual network. Over time, this allows the system to learn from its own\ngenerated and increasingly better output. Our approach shows to be effective in\nimproving the two zero-shot directions of our multilingual model. In\nparticular, we observed gains of about 9 BLEU points over a baseline\nmultilingual model and up to 2.08 BLEU over a pivoting mechanism using two\nbilingual models. Further analysis shows that there is also a slight\nimprovement in the non-zero-shot language directions.", "published": "2018-11-04 15:41:42", "link": "http://arxiv.org/abs/1811.01389v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ColNet: Embedding the Semantics of Web Tables for Column Type Prediction", "abstract": "Automatically annotating column types with knowledge base (KB) concepts is a\ncritical task to gain a basic understanding of web tables. Current methods rely\non either table metadata like column name or entity correspondences of cells in\nthe KB, and may fail to deal with growing web tables with incomplete meta\ninformation. In this paper we propose a neural network based column type\nannotation framework named ColNet which is able to integrate KB reasoning and\nlookup with machine learning and can automatically train Convolutional Neural\nNetworks for prediction. The prediction model not only considers the contextual\nsemantics within a cell using word representation, but also embeds the\nsemantics of a column by learning locality features from multiple cells. The\nmethod is evaluated with DBPedia and two different web table datasets, T2Dv2\nfrom the general Web and Limaye from Wikipedia pages, and achieves higher\nperformance than the state-of-the-art approaches.", "published": "2018-11-04 00:26:00", "link": "http://arxiv.org/abs/1811.01304v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Logic Attention Based Neighborhood Aggregation for Inductive Knowledge\n  Graph Embedding", "abstract": "Knowledge graph embedding aims at modeling entities and relations with\nlow-dimensional vectors. Most previous methods require that all entities should\nbe seen during training, which is unpractical for real-world knowledge graphs\nwith new entities emerging on a daily basis. Recent efforts on this issue\nsuggest training a neighborhood aggregator in conjunction with the conventional\nentity and relation embeddings, which may help embed new entities inductively\nvia their existing neighbors. However, their neighborhood aggregators neglect\nthe unordered and unequal natures of an entity's neighbors. To this end, we\nsummarize the desired properties that may lead to effective neighborhood\naggregators. We also introduce a novel aggregator, namely, Logic Attention\nNetwork (LAN), which addresses the properties by aggregating neighbors with\nboth rules- and network-based attention weights. By comparing with conventional\naggregators on two knowledge graph completion tasks, we experimentally validate\nLAN's superiority in terms of the desired properties.", "published": "2018-11-04 16:39:28", "link": "http://arxiv.org/abs/1811.01399v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Semantic Role Labeling for Knowledge Graph Extraction from Text", "abstract": "This paper introduces TakeFive, a new semantic role labeling method that\ntransforms a text into a frame-oriented knowledge graph. It performs dependency\nparsing, identifies the words that evoke lexical frames, locates the roles and\nfillers for each frame, runs coercion techniques, and formalises the results as\na knowledge graph. This formal representation complies with the frame semantics\nused in Framester, a factual-linguistic linked data resource. The obtained\nprecision, recall and F1 values indicate that TakeFive is competitive with\nother existing methods such as SEMAFOR, Pikes, PathLSTM and FRED. We finally\ndiscuss how to combine TakeFive and FRED, obtaining higher values of precision,\nrecall and F1.", "published": "2018-11-04 17:57:07", "link": "http://arxiv.org/abs/1811.01409v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Char2char Generation with Reranking for the E2E NLG Challenge", "abstract": "This paper describes our submission to the E2E NLG Challenge. Recently,\nneural seq2seq approaches have become mainstream in NLG, often resorting to\npre- (respectively post-) processing delexicalization (relexicalization) steps\nat the word-level to handle rare words. By contrast, we train a simple\ncharacter level seq2seq model, which requires no pre/post-processing\n(delexicalization, tokenization or even lowercasing), with surprisingly good\nresults. For further improvement, we explore two re-ranking approaches for\nscoring candidates. We also introduce a synthetic dataset creation procedure,\nwhich opens up a new way of creating artificial datasets for Natural Language\nGeneration.", "published": "2018-11-04 22:56:50", "link": "http://arxiv.org/abs/1811.05826v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adversarial Gain", "abstract": "Adversarial examples can be defined as inputs to a model which induce a\nmistake - where the model output is different than that of an oracle, perhaps\nin surprising or malicious ways. Original models of adversarial attacks are\nprimarily studied in the context of classification and computer vision tasks.\nWhile several attacks have been proposed in natural language processing (NLP)\nsettings, they often vary in defining the parameters of an attack and what a\nsuccessful attack would look like. The goal of this work is to propose a\nunifying model of adversarial examples suitable for NLP tasks in both\ngenerative and classification settings. We define the notion of adversarial\ngain: based in control theory, it is a measure of the change in the output of a\nsystem relative to the perturbation of the input (caused by the so-called\nadversary) presented to the learner. This definition, as we show, can be used\nunder different feature spaces and distance conditions to determine attack or\ndefense effectiveness across different intuitive manifolds. This notion of\nadversarial gain not only provides a useful way for evaluating adversaries and\ndefenses, but can act as a building block for future work in robustness under\nadversaries due to its rooted nature in stability and manifold theory.", "published": "2018-11-04 00:02:42", "link": "http://arxiv.org/abs/1811.01302v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Towards Unsupervised Speech-to-Text Translation", "abstract": "We present a framework for building speech-to-text translation (ST) systems\nusing only monolingual speech and text corpora, in other words, speech\nutterances from a source language and independent text from a target language.\nAs opposed to traditional cascaded systems and end-to-end architectures, our\nsystem does not require any labeled data (i.e., transcribed source audio or\nparallel source and target text corpora) during training, making it especially\napplicable to language pairs with very few or even zero bilingual resources.\nThe framework initializes the ST system with a cross-modal bilingual dictionary\ninferred from the monolingual corpora, that maps every source speech segment\ncorresponding to a spoken word to its target text translation. For unseen\nsource speech utterances, the system first performs word-by-word translation on\neach speech segment in the utterance. The translation is improved by leveraging\na language model and a sequence denoising autoencoder to provide prior\nknowledge about the target language. Experimental results show that our\nunsupervised system achieves comparable BLEU scores to supervised end-to-end\nmodels despite the lack of supervision. We also provide an ablation analysis to\nexamine the utility of each component in our system.", "published": "2018-11-04 01:23:47", "link": "http://arxiv.org/abs/1811.01307v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Neural CRF transducers for sequence labeling", "abstract": "Conditional random fields (CRFs) have been shown to be one of the most\nsuccessful approaches to sequence labeling. Various linear-chain neural CRFs\n(NCRFs) are developed to implement the non-linear node potentials in CRFs, but\nstill keeping the linear-chain hidden structure. In this paper, we propose NCRF\ntransducers, which consists of two RNNs, one extracting features from\nobservations and the other capturing (theoretically infinite) long-range\ndependencies between labels. Different sequence labeling methods are evaluated\nover POS tagging, chunking and NER (English, Dutch). Experiment results show\nthat NCRF transducers achieve consistent improvements over linear-chain NCRFs\nand RNN transducers across all the four tasks, and can improve state-of-the-art\nresults.", "published": "2018-11-04 14:45:50", "link": "http://arxiv.org/abs/1811.01382v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Investigating context features hidden in End-to-End TTS", "abstract": "Recent studies have introduced end-to-end TTS, which integrates the\nproduction of context and acoustic features in statistical parametric speech\nsynthesis. As a result, a single neural network replaced laborious feature\nengineering with automated feature learning. However, little is known about\nwhat types of context information end-to-end TTS extracts from text input\nbefore synthesizing speech, and the previous knowledge about context features\nis barely utilized. In this work, we first point out the model similarity\nbetween end-to-end TTS and parametric TTS. Based on the similarity, we evaluate\nthe quality of encoder outputs from an end-to-end TTS system against eight\ncriteria that are derived from a standard set of context information used in\nparametric TTS. We conduct experiments using an evaluation procedure that has\nbeen newly developed in the machine learning literature for quantitative\nanalysis of neural representations, while adapting it to the TTS domain.\nExperimental results show that the encoder outputs reflect both linguistic and\nphonetic contexts, such as vowel reduction at phoneme level, lexical stress at\nsyllable level, and part-of-speech at word level, possibly due to the joint\noptimization of context and acoustic features.", "published": "2018-11-04 14:22:37", "link": "http://arxiv.org/abs/1811.01376v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
