{"title": "Accurate Knowledge Distillation with n-best Reranking", "abstract": "We propose utilizing n-best reranking to enhance Sequence-Level Knowledge\nDistillation (Kim and Rush, 2016) where we extract pseudo-labels for student\nmodel's training data from top n-best hypotheses and leverage a diverse set of\nmodels with different inductive biases, objective functions or architectures,\nincluding some publicly-available large language models, to pick the\nhighest-quality hypotheses as labels. The effectiveness of our proposal is\nvalidated through experiments on the WMT'21 German-English and Chinese-English\ntranslation tasks. Our results demonstrate that utilizing pseudo-labels\ngenerated by our n-best reranker leads to a significantly more accurate student\nmodel. In fact, our best student model achieves comparable accuracy to a large\ntranslation model from (Tran et al., 2021) with 4.7 billion parameters, while\nhaving two orders of magnitude fewer parameters.", "published": "2023-05-20 01:53:03", "link": "http://arxiv.org/abs/2305.12057v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DisCo: Distilled Student Models Co-training for Semi-supervised Text\n  Mining", "abstract": "Many text mining models are constructed by fine-tuning a large deep\npre-trained language model (PLM) in downstream tasks. However, a significant\nchallenge nowadays is maintaining performance when we use a lightweight model\nwith limited labelled samples. We present DisCo, a semi-supervised learning\n(SSL) framework for fine-tuning a cohort of small student models generated from\na large PLM using knowledge distillation. Our key insight is to share\ncomplementary knowledge among distilled student cohorts to promote their SSL\neffectiveness. DisCo employs a novel co-training technique to optimize a cohort\nof multiple small student models by promoting knowledge sharing among students\nunder diversified views: model views produced by different distillation\nstrategies and data views produced by various input augmentations. We evaluate\nDisCo on both semi-supervised text classification and extractive summarization\ntasks. Experimental results show that DisCo can produce student models that are\n7.6 times smaller and 4.8 times faster in inference than the baseline PLMs\nwhile maintaining comparable performance. We also show that DisCo-generated\nstudent models outperform the similar-sized models elaborately tuned in\ndistinct tasks.", "published": "2023-05-20 03:23:16", "link": "http://arxiv.org/abs/2305.12074v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-Shot Dialogue Summarization via Skeleton-Assisted Prompt Transfer in\n  Prompt Tuning", "abstract": "In real-world scenarios, labeled samples for dialogue summarization are\nusually limited (i.e., few-shot) due to high annotation costs for high-quality\ndialogue summaries. To efficiently learn from few-shot samples, previous works\nhave utilized massive annotated data from other downstream tasks and then\nperformed prompt transfer in prompt tuning so as to enable cross-task knowledge\ntransfer. However, existing general-purpose prompt transfer techniques lack\nconsideration for dialogue-specific information. In this paper, we focus on\nimproving the prompt transfer from dialogue state tracking to dialogue\nsummarization and propose Skeleton-Assisted Prompt Transfer (SAPT), which\nleverages skeleton generation as extra supervision that functions as a medium\nconnecting the distinct source and target task and resulting in the model's\nbetter consumption of dialogue state information. To automatically extract\ndialogue skeletons as supervised training data for skeleton generation, we\ndesign a novel approach with perturbation-based probes requiring neither\nannotation effort nor domain knowledge. Training the model on such skeletons\ncan also help preserve model capability during prompt transfer. Our method\nsignificantly outperforms existing baselines. In-depth analyses demonstrate the\neffectiveness of our method in facilitating cross-task knowledge transfer in\nfew-shot dialogue summarization.", "published": "2023-05-20 03:32:48", "link": "http://arxiv.org/abs/2305.12077v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Entropy Rate Constancy in Text", "abstract": "The uniform information density (UID) hypothesis states that humans tend to\ndistribute information roughly evenly across an utterance or discourse. Early\nevidence in support of the UID hypothesis came from Genzel & Charniak (2002),\nwhich proposed an entropy rate constancy principle based on the probability of\nEnglish text under n-gram language models. We re-evaluate the claims of Genzel\n& Charniak (2002) with neural language models, failing to find clear evidence\nin support of entropy rate constancy. We conduct a range of experiments across\ndatasets, model sizes, and languages and discuss implications for the uniform\ninformation density hypothesis and linguistic theories of efficient\ncommunication more broadly.", "published": "2023-05-20 03:48:31", "link": "http://arxiv.org/abs/2305.12084v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prefix Propagation: Parameter-Efficient Tuning for Long Sequences", "abstract": "Parameter-efficient tuning aims to mitigate the large memory requirements of\nadapting pretrained language models for downstream tasks. For example, one\npopular method, prefix-tuning, prepends trainable tokens to sequences while\nfreezing the rest of the model's parameters. Although such models attain\ncomparable performance with fine-tuning when applied to sequences with short to\nmoderate lengths, we show their inferior performance when modelling long\nsequences. To bridge this gap, we propose prefix-propagation, a simple but\neffective approach that conditions prefixes on previous hidden states. We\nempirically demonstrate that prefix-propagation outperforms prefix-tuning\nacross long-document tasks, while using 50% fewer parameters. To further\ninvestigate the proposed architecture, we also show its advantage in\ncalibration, and perform additional study on its relationship with kernel\nattention. To the best of our knowledge, this work is the first to focus on\nparameter-efficient learning for long-sequence language tasks.", "published": "2023-05-20 04:07:06", "link": "http://arxiv.org/abs/2305.12086v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"What do others think?\": Task-Oriented Conversational Modeling with\n  Subjective Knowledge", "abstract": "Task-oriented Dialogue (TOD) Systems aim to build dialogue systems that\nassist users in accomplishing specific goals, such as booking a hotel or a\nrestaurant. Traditional TODs rely on domain-specific APIs/DBs or external\nfactual knowledge to generate responses, which cannot accommodate subjective\nuser requests (e.g., \"Is the WIFI reliable?\" or \"Does the restaurant have a\ngood atmosphere?\"). To address this issue, we propose a novel task of\nsubjective-knowledge-based TOD (SK-TOD). We also propose the first\ncorresponding dataset, which contains subjective knowledge-seeking dialogue\ncontexts and manually annotated responses grounded in subjective knowledge\nsources. When evaluated with existing TOD approaches, we find that this task\nposes new challenges such as aggregating diverse opinions from multiple\nknowledge snippets. We hope this task and dataset can promote further research\non TOD and subjective content understanding. The code and the dataset are\navailable at https://github.com/alexa/dstc11-track5.", "published": "2023-05-20 04:43:26", "link": "http://arxiv.org/abs/2305.12091v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ESCOXLM-R: Multilingual Taxonomy-driven Pre-training for the Job Market\n  Domain", "abstract": "The increasing number of benchmarks for Natural Language Processing (NLP)\ntasks in the computational job market domain highlights the demand for methods\nthat can handle job-related tasks such as skill extraction, skill\nclassification, job title classification, and de-identification. While some\napproaches have been developed that are specific to the job market domain,\nthere is a lack of generalized, multilingual models and benchmarks for these\ntasks. In this study, we introduce a language model called ESCOXLM-R, based on\nXLM-R, which uses domain-adaptive pre-training on the European Skills,\nCompetences, Qualifications and Occupations (ESCO) taxonomy, covering 27\nlanguages. The pre-training objectives for ESCOXLM-R include dynamic masked\nlanguage modeling and a novel additional objective for inducing multilingual\ntaxonomical ESCO relations. We comprehensively evaluate the performance of\nESCOXLM-R on 6 sequence labeling and 3 classification tasks in 4 languages and\nfind that it achieves state-of-the-art results on 6 out of 9 datasets. Our\nanalysis reveals that ESCOXLM-R performs better on short spans and outperforms\nXLM-R on entity-level and surface-level span-F1, likely due to ESCO containing\nshort skill and occupation titles, and encoding information on the\nentity-level.", "published": "2023-05-20 04:50:20", "link": "http://arxiv.org/abs/2305.12092v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can NLP Models Correctly Reason Over Contexts that Break the Common\n  Assumptions?", "abstract": "Pre-training on large corpora of text enables the language models to acquire\na vast amount of factual and commonsense knowledge which allows them to achieve\nremarkable performance on a variety of language understanding tasks. They\ntypically acquire this knowledge by learning from the pre-training text and\ncapturing certain patterns from it. However, real-world settings often present\nscenarios that do not abide by these patterns i.e. scenarios that break the\ncommon assumptions. Can state-of-the-art NLP models correctly reason over the\ncontexts of such scenarios?\n  Addressing the above question, in this paper, we investigate the ability of\nmodels to correctly reason over contexts that break the common assumptions. To\nthis end, we first systematically create evaluation data in which each data\ninstance consists of (a) a common assumption, (b) a context that follows the\nassumption, (c) a context that breaks the assumption, and (d) questions based\non the contexts. Then, through evaluations on multiple models including GPT-3\nand Flan T5, we show that while doing fairly well on contexts that follow the\ncommon assumptions, the models struggle to correctly reason over contexts that\nbreak those assumptions. Specifically, the performance gap is as high as 20%\nabsolute points. Furthermore, we thoroughly analyze these results revealing\nseveral interesting findings. We believe our work and findings will encourage\nand facilitate further research in developing more robust models that can also\nreliably reason over contexts that break the common assumptions. Data is\navailable at \\url{https://github.com/nrjvarshney/break_the_common_assumptions}.", "published": "2023-05-20 05:20:37", "link": "http://arxiv.org/abs/2305.12096v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling the Q-Diversity in a Min-max Play Game for Robust Optimization", "abstract": "Models trained with empirical risk minimization (ERM) are revealed to easily\nrely on spurious correlations, resulting in poor generalization. Group\ndistributionally robust optimization (group DRO) can alleviate this problem by\nminimizing the worst-case loss over pre-defined groups. While promising, in\npractice factors like expensive annotations and privacy preclude the\navailability of group labels. More crucially, when taking a closer look at the\nfailure modes of out-of-distribution generalization, the typical procedure of\nreweighting in group DRO loses efficiency. Hinged on the limitations, in this\nwork, we reformulate the group DRO framework by proposing Q-Diversity.\nCharacterized by an interactive training mode, Q-Diversity relaxes the group\nidentification from annotation into direct parameterization. Furthermore, a\nnovel mixing strategy across groups is presented to diversify the\nunder-represented groups. In a series of experiments on both synthetic and\nreal-world text classification tasks, results demonstrate that Q-Diversity can\nconsistently improve worst-case accuracy under different distributional shifts,\noutperforming state-of-the-art alternatives.", "published": "2023-05-20 07:02:27", "link": "http://arxiv.org/abs/2305.12123v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hedges in Bidirectional Translations of Publicity-Oriented Documents", "abstract": "Hedges are widely studied across registers and disciplines, yet research on\nthe translation of hedges in political texts is extremely limited. This\ncontrastive study is dedicated to investigating whether there is a diachronic\nchange in the frequencies of hedging devices in the target texts, to what\nextent the changing frequencies of translated hedges through years are\nattributed to the source texts, and what translation strategies are adopted to\ndeal with them. For the purposes of this research, two types of official\npolitical texts and their translations from China and the United Nations were\ncollected to form three sub-corpora. Results show that hedges tend to appear\nmore frequently in English political texts, be it original English or\ntranslated English. In addition, directionality seems to play an important role\nin influencing both the frequencies and translation strategies regarding the\nuse of hedges. A noticeable diachronic increase of hedging devices is also\nobserved in our corpus.", "published": "2023-05-20 09:19:39", "link": "http://arxiv.org/abs/2305.12146v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Automated Topic Model Evaluation with Large Language Models", "abstract": "Topic models are used to make sense of large text collections. However,\nautomatically evaluating topic model output and determining the optimal number\nof topics both have been longstanding challenges, with no effective automated\nsolutions to date. This paper proposes using large language models to evaluate\nsuch output. We find that large language models appropriately assess the\nresulting topics, correlating more strongly with human judgments than existing\nautomated metrics. We then investigate whether we can use large language models\nto automatically determine the optimal number of topics. We automatically\nassign labels to documents and choosing configurations with the most pure\nlabels returns reasonable values for the optimal number of topics.", "published": "2023-05-20 09:42:00", "link": "http://arxiv.org/abs/2305.12152v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Compose Representations of Different Encoder Layers towards\n  Improving Compositional Generalization", "abstract": "Recent studies have shown that sequence-to-sequence (seq2seq) models struggle\nwith compositional generalization (CG), i.e., the ability to systematically\ngeneralize to unseen compositions of seen components. There is mounting\nevidence that one of the reasons hindering CG is the representation of the\nencoder uppermost layer is entangled, i.e., the syntactic and semantic\nrepresentations of sequences are entangled. However, we consider that the\npreviously identified representation entanglement problem is not comprehensive\nenough. Additionally, we hypothesize that the source keys and values\nrepresentations passing into different decoder layers are also entangled.\nStarting from this intuition, we propose \\textsc{CompoSition} (\\textbf{Compo}se\n\\textbf{S}yntactic and Semant\\textbf{i}c Representa\\textbf{tion}s), an\nextension to seq2seq models which learns to compose representations of\ndifferent encoder layers dynamically for different tasks, since recent studies\nreveal that the bottom layers of the Transformer encoder contain more syntactic\ninformation and the top ones contain more semantic information. Specifically,\nwe introduce a \\textit{composed layer} between the encoder and decoder to\ncompose different encoder layers' representations to generate specific keys and\nvalues passing into different decoder layers. \\textsc{CompoSition} achieves\ncompetitive results on two comprehensive and realistic benchmarks, which\nempirically demonstrates the effectiveness of our proposal. Codes are available\nat~\\url{https://github.com/thinkaboutzero/COMPOSITION}.", "published": "2023-05-20 11:16:59", "link": "http://arxiv.org/abs/2305.12169v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Glot500: Scaling Multilingual Corpora and Language Models to 500\n  Languages", "abstract": "The NLP community has mainly focused on scaling Large Language Models (LLMs)\nvertically, i.e., making them better for about 100 languages. We instead scale\nLLMs horizontally: we create, through continued pretraining, Glot500-m, an LLM\nthat covers 511 predominantly low-resource languages. An important part of this\neffort is to collect and clean Glot500-c, a corpus that covers these 511\nlanguages and allows us to train Glot500-m. We evaluate Glot500-m on five\ndiverse tasks across these languages. We observe large improvements for both\nhigh-resource and low-resource languages compared to an XLM-R baseline. Our\nanalysis shows that no single factor explains the quality of multilingual LLM\nrepresentations. Rather, a combination of factors determines quality including\ncorpus size, script, \"help\" from related languages and the total capacity of\nthe model. Our work addresses an important goal of NLP research: we should not\nlimit NLP to a small fraction of the world's languages and instead strive to\nsupport as many languages as possible to bring the benefits of NLP technology\nto all languages and cultures. Code, data and models are available at\nhttps://github.com/cisnlp/Glot500.", "published": "2023-05-20 12:26:41", "link": "http://arxiv.org/abs/2305.12182v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pointwise Mutual Information Based Metric and Decoding Strategy for\n  Faithful Generation in Document Grounded Dialogs", "abstract": "A major concern in using deep learning based generative models for\ndocument-grounded dialogs is the potential generation of responses that are not\n\\textit{faithful} to the underlying document. Existing automated metrics used\nfor evaluating the faithfulness of response with respect to the grounding\ndocument measure the degree of similarity between the generated response and\nthe document's content. However, these automated metrics are far from being\nwell aligned with human judgments. Therefore, to improve the measurement of\nfaithfulness, we propose a new metric that utilizes (Conditional) Point-wise\nMutual Information (PMI) between the generated response and the source\ndocument, conditioned on the dialogue. PMI quantifies the extent to which the\ndocument influences the generated response -- with a higher PMI indicating a\nmore faithful response. We build upon this idea to create a new decoding\ntechnique that incorporates PMI into the response generation process to predict\nmore faithful responses. Our experiments on the BEGIN benchmark demonstrate an\nimproved correlation of our metric with human evaluation. We also show that our\ndecoding technique is effective in generating more faithful responses when\ncompared to standard decoding techniques on a set of publicly available\ndocument-grounded dialog datasets.", "published": "2023-05-20 13:34:34", "link": "http://arxiv.org/abs/2305.12191v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VNHSGE: VietNamese High School Graduation Examination Dataset for Large\n  Language Models", "abstract": "The VNHSGE (VietNamese High School Graduation Examination) dataset, developed\nexclusively for evaluating large language models (LLMs), is introduced in this\narticle. The dataset, which covers nine subjects, was generated from the\nVietnamese National High School Graduation Examination and comparable tests.\n300 literary essays have been included, and there are over 19,000\nmultiple-choice questions on a range of topics. The dataset assesses LLMs in\nmultitasking situations such as question answering, text generation, reading\ncomprehension, visual question answering, and more by including both textual\ndata and accompanying images. Using ChatGPT and BingChat, we evaluated LLMs on\nthe VNHSGE dataset and contrasted their performance with that of Vietnamese\nstudents to see how well they performed. The results show that ChatGPT and\nBingChat both perform at a human level in a number of areas, including\nliterature, English, history, geography, and civics education. They still have\nspace to grow, though, especially in the areas of mathematics, physics,\nchemistry, and biology. The VNHSGE dataset seeks to provide an adequate\nbenchmark for assessing the abilities of LLMs with its wide-ranging coverage\nand variety of activities. We intend to promote future developments in the\ncreation of LLMs by making this dataset available to the scientific community,\nespecially in resolving LLMs' limits in disciplines involving mathematics and\nthe natural sciences.", "published": "2023-05-20 14:13:08", "link": "http://arxiv.org/abs/2305.12199v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Distillation with Meta Learning for Knowledge Graph Completion", "abstract": "In this paper, we propose a selfdistillation framework with meta\nlearning(MetaSD) for knowledge graph completion with dynamic pruning, which\naims to learn compressed graph embeddings and tackle the longtail samples.\nSpecifically, we first propose a dynamic pruning technique to obtain a small\npruned model from a large source model, where the pruning mask of the pruned\nmodel could be updated adaptively per epoch after the model weights are\nupdated. The pruned model is supposed to be more sensitive to difficult to\nmemorize samples(e.g., longtail samples) than the source model. Then, we\npropose a onestep meta selfdistillation method for distilling comprehensive\nknowledge from the source model to the pruned model, where the two models\ncoevolve in a dynamic manner during training. In particular, we exploit the\nperformance of the pruned model, which is trained alongside the source model in\none iteration, to improve the source models knowledge transfer ability for the\nnext iteration via meta learning. Extensive experiments show that MetaSD\nachieves competitive performance compared to strong baselines, while being 10x\nsmaller than baselines.", "published": "2023-05-20 15:12:25", "link": "http://arxiv.org/abs/2305.12209v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompting ChatGPT in MNER: Enhanced Multimodal Named Entity Recognition\n  with Auxiliary Refined Knowledge", "abstract": "Multimodal Named Entity Recognition (MNER) on social media aims to enhance\ntextual entity prediction by incorporating image-based clues. Existing studies\nmainly focus on maximizing the utilization of pertinent image information or\nincorporating external knowledge from explicit knowledge bases. However, these\nmethods either neglect the necessity of providing the model with external\nknowledge, or encounter issues of high redundancy in the retrieved knowledge.\nIn this paper, we present PGIM -- a two-stage framework that aims to leverage\nChatGPT as an implicit knowledge base and enable it to heuristically generate\nauxiliary knowledge for more efficient entity prediction. Specifically, PGIM\ncontains a Multimodal Similar Example Awareness module that selects suitable\nexamples from a small number of predefined artificial samples. These examples\nare then integrated into a formatted prompt template tailored to the MNER and\nguide ChatGPT to generate auxiliary refined knowledge. Finally, the acquired\nknowledge is integrated with the original text and fed into a downstream model\nfor further processing. Extensive experiments show that PGIM outperforms\nstate-of-the-art methods on two classic MNER datasets and exhibits a stronger\nrobustness and generalization capability.", "published": "2023-05-20 15:24:38", "link": "http://arxiv.org/abs/2305.12212v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PromptNER: A Prompting Method for Few-shot Named Entity Recognition via\n  k Nearest Neighbor Search", "abstract": "Few-shot Named Entity Recognition (NER) is a task aiming to identify named\nentities via limited annotated samples. Recently, prototypical networks have\nshown promising performance in few-shot NER. Most of prototypical networks will\nutilize the entities from the support set to construct label prototypes and use\nthe query set to compute span-level similarities and optimize these label\nprototype representations. However, these methods are usually unsuitable for\nfine-tuning in the target domain, where only the support set is available. In\nthis paper, we propose PromptNER: a novel prompting method for few-shot NER via\nk nearest neighbor search. We use prompts that contains entity category\ninformation to construct label prototypes, which enables our model to fine-tune\nwith only the support set. Our approach achieves excellent transfer learning\nability, and extensive experiments on the Few-NERD and CrossNER datasets\ndemonstrate that our model achieves superior performance over state-of-the-art\nmethods.", "published": "2023-05-20 15:47:59", "link": "http://arxiv.org/abs/2305.12217v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Transformers Provide a False Sense of Efficiency", "abstract": "Despite much success in natural language processing (NLP), pre-trained\nlanguage models typically lead to a high computational cost during inference.\nMulti-exit is a mainstream approach to address this issue by making a trade-off\nbetween efficiency and accuracy, where the saving of computation comes from an\nearly exit. However, whether such saving from early-exiting is robust remains\nunknown. Motivated by this, we first show that directly adapting existing\nadversarial attack approaches targeting model accuracy cannot significantly\nreduce inference efficiency. To this end, we propose a simple yet effective\nattacking framework, SAME, a novel slowdown attack framework on multi-exit\nmodels, which is specially tailored to reduce the efficiency of the multi-exit\nmodels. By leveraging the multi-exit models' design characteristics, we utilize\nall internal predictions to guide the adversarial sample generation instead of\nmerely considering the final prediction. Experiments on the GLUE benchmark show\nthat SAME can effectively diminish the efficiency gain of various multi-exit\nmodels by 80% on average, convincingly validating its effectiveness and\ngeneralization ability.", "published": "2023-05-20 16:41:48", "link": "http://arxiv.org/abs/2305.12228v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Measure of Explanatory Effectiveness", "abstract": "In most conversations about explanation and AI, the recipient of the\nexplanation (the explainee) is suspiciously absent, despite the problem being\nultimately communicative in nature. We pose the problem `explaining AI systems'\nin terms of a two-player cooperative game in which each agent seeks to maximise\nour proposed measure of explanatory effectiveness. This measure serves as a\nfoundation for the automated assessment of explanations, in terms of the\neffects that any given action in the game has on the internal state of the\nexplainee.", "published": "2023-05-20 16:52:30", "link": "http://arxiv.org/abs/2305.12233v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scene Graph as Pivoting: Inference-time Image-free Unsupervised\n  Multimodal Machine Translation with Visual Scene Hallucination", "abstract": "In this work, we investigate a more realistic unsupervised multimodal machine\ntranslation (UMMT) setup, inference-time image-free UMMT, where the model is\ntrained with source-text image pairs, and tested with only source-text inputs.\nFirst, we represent the input images and texts with the visual and language\nscene graphs (SG), where such fine-grained vision-language features ensure a\nholistic understanding of the semantics. To enable pure-text input during\ninference, we devise a visual scene hallucination mechanism that dynamically\ngenerates pseudo visual SG from the given textual SG. Several SG-pivoting based\nlearning objectives are introduced for unsupervised translation training. On\nthe benchmark Multi30K data, our SG-based method outperforms the\nbest-performing baseline by significant BLEU scores on the task and setup,\nhelping yield translations with better completeness, relevance and fluency\nwithout relying on paired images. Further in-depth analyses reveal how our\nmodel advances in the task setting.", "published": "2023-05-20 18:17:20", "link": "http://arxiv.org/abs/2305.12256v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constructing Code-mixed Universal Dependency Forest for Unbiased\n  Cross-lingual Relation Extraction", "abstract": "Latest efforts on cross-lingual relation extraction (XRE) aggressively\nleverage the language-consistent structural features from the universal\ndependency (UD) resource, while they may largely suffer from biased transfer\n(e.g., either target-biased or source-biased) due to the inevitable linguistic\ndisparity between languages. In this work, we investigate an unbiased UD-based\nXRE transfer by constructing a type of code-mixed UD forest. We first translate\nthe sentence of the source language to the parallel target-side language, for\nboth of which we parse the UD tree respectively. Then, we merge the\nsource-/target-side UD structures as a unified code-mixed UD forest. With such\nforest features, the gaps of UD-based XRE between the training and predicting\nphases can be effectively closed. We conduct experiments on the ACE XRE\nbenchmark datasets, where the results demonstrate that the proposed code-mixed\nUD forests help unbiased UD-based XRE transfer, with which we achieve\nsignificant XRE performance gains.", "published": "2023-05-20 18:24:06", "link": "http://arxiv.org/abs/2305.12258v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analogy in Contact: Modeling Maltese Plural Inflection", "abstract": "Maltese is often described as having a hybrid morphological system resulting\nfrom extensive contact between Semitic and Romance language varieties. Such a\ndesignation reflects an etymological divide as much as it does a larger\ntradition in the literature to consider concatenative and non-concatenative\nmorphological patterns as distinct in the language architecture. Using a\ncombination of computational modeling and information theoretic methods, we\nquantify the extent to which the phonology and etymology of a Maltese singular\nnoun may predict the morphological process (affixal vs. templatic) as well as\nthe specific plural allomorph (affix or template) relating a singular noun to\nits associated plural form(s) in the lexicon. The results indicate phonological\npressures shape the organization of the Maltese lexicon with predictive power\nthat extends beyond that of a word's etymology, in line with analogical\ntheories of language change in contact.", "published": "2023-05-20 20:16:57", "link": "http://arxiv.org/abs/2305.12276v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextualizing Argument Quality Assessment with Relevant Knowledge", "abstract": "Automatic assessment of the quality of arguments has been recognized as a\nchallenging task with significant implications for misinformation and targeted\nspeech. While real-world arguments are tightly anchored in context, existing\ncomputational methods analyze their quality in isolation, which affects their\naccuracy and generalizability. We propose SPARK: a novel method for scoring\nargument quality based on contextualization via relevant knowledge. We devise\nfour augmentations that leverage large language models to provide feedback,\ninfer hidden assumptions, supply a similar-quality argument, or give a\ncounter-argument. SPARK uses a dual-encoder Transformer architecture to enable\nthe original argument and its augmentation to be considered jointly. Our\nexperiments in both in-domain and zero-shot setups show that SPARK consistently\noutperforms existing techniques across multiple metrics.", "published": "2023-05-20 21:04:58", "link": "http://arxiv.org/abs/2305.12280v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting the Architectures like Pointer Networks to Efficiently\n  Improve the Next Word Distribution, Summarization Factuality, and Beyond", "abstract": "Is the output softmax layer, which is adopted by most language models (LMs),\nalways the best way to compute the next word probability? Given so many\nattention layers in a modern transformer-based LM, are the pointer networks\nredundant nowadays? In this study, we discover that the answers to both\nquestions are no. This is because the softmax bottleneck sometimes prevents the\nLMs from predicting the desired distribution and the pointer networks can be\nused to break the bottleneck efficiently. Based on the finding, we propose\nseveral softmax alternatives by simplifying the pointer networks and\naccelerating the word-by-word rerankers. In GPT-2, our proposals are\nsignificantly better and more efficient than mixture of softmax, a\nstate-of-the-art softmax alternative. In summarization experiments, without\nsignificantly decreasing its training/testing speed, our best method based on\nT5-Small improves factCC score by 2 points in CNN/DM and XSUM dataset, and\nimproves MAUVE scores by 30% in BookSum paragraph-level dataset.", "published": "2023-05-20 21:52:24", "link": "http://arxiv.org/abs/2305.12289v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lifting the Curse of Capacity Gap in Distilling Language Models", "abstract": "Pretrained language models (LMs) have shown compelling performance on various\ndownstream tasks, but unfortunately they require a tremendous amount of\ninference compute. Knowledge distillation finds a path to compress LMs to small\nones with a teacher-student paradigm. However, when the capacity gap between\nthe teacher and the student is large, a curse of capacity gap appears, invoking\na deficiency in distilling LMs. While a few studies have been carried out to\nfill the gap, the curse is not yet well tackled. In this paper, we aim at\nlifting the curse of capacity gap via enlarging the capacity of the student\nwithout notably increasing the inference compute. Largely motivated by sparse\nactivation regime of mixture of experts (MoE), we propose a mixture of minimal\nexperts (MiniMoE), which imposes extra parameters to the student but introduces\nalmost no additional inference compute. Experimental results on GLUE and CoNLL\ndemonstrate the curse of capacity gap is lifted by the magic of MiniMoE to a\nlarge extent. MiniMoE also achieves the state-of-the-art performance at small\nFLOPs compared with a range of competitive baselines. With a compression rate\nas much as $\\sim$50$\\times$, MiniMoE preserves $\\sim$95\\% GLUE score of the\nteacher.", "published": "2023-05-20 07:30:55", "link": "http://arxiv.org/abs/2305.12129v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LogiCoT: Logical Chain-of-Thought Instruction-Tuning", "abstract": "Generative Pre-trained Transformer 4 (GPT-4) demonstrates impressive\nchain-of-thought reasoning ability. Recent work on self-instruction tuning,\nsuch as Alpaca, has focused on enhancing the general proficiency of models.\nThese instructions enable the model to achieve performance comparable to\nGPT-3.5 on general tasks like open-domain text generation and paraphrasing.\nHowever, they fall short of helping the model handle complex reasoning tasks.\nTo bridge the gap, this paper presents LogiCoT, a new instruction-tuning\ndataset for Logical Chain-of-Thought reasoning with GPT-4. We elaborate on the\nprocess of harvesting instructions for prompting GPT-4 to generate\nchain-of-thought rationales. LogiCoT serves as an instruction set for teaching\nmodels of logical reasoning and elicits general reasoning skills.", "published": "2023-05-20 09:23:09", "link": "http://arxiv.org/abs/2305.12147v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Paragraph-level Citation Recommendation based on Topic Sentences as\n  Queries", "abstract": "Citation recommendation (CR) models may help authors find relevant articles\nat various stages of the paper writing process. Most research has dealt with\neither global CR, which produces general recommendations suitable for the\ninitial writing stage, or local CR, which produces specific recommendations\nmore fitting for the final writing stages. We propose the task of\nparagraph-level CR as a middle ground between the two approaches, where the\nparagraph's topic sentence is taken as input and recommendations for citing\nwithin the paragraph are produced at the output. We propose a model for this\ntask, fine-tune it using the quadruplet loss on the dataset of ACL papers, and\nshow improvements over the baselines.", "published": "2023-05-20 13:28:22", "link": "http://arxiv.org/abs/2305.12190v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Brain encoding models based on multimodal transformers can transfer\n  across language and vision", "abstract": "Encoding models have been used to assess how the human brain represents\nconcepts in language and vision. While language and vision rely on similar\nconcept representations, current encoding models are typically trained and\ntested on brain responses to each modality in isolation. Recent advances in\nmultimodal pretraining have produced transformers that can extract aligned\nrepresentations of concepts in language and vision. In this work, we used\nrepresentations from multimodal transformers to train encoding models that can\ntransfer across fMRI responses to stories and movies. We found that encoding\nmodels trained on brain responses to one modality can successfully predict\nbrain responses to the other modality, particularly in cortical regions that\nrepresent conceptual meaning. Further analysis of these encoding models\nrevealed shared semantic dimensions that underlie concept representations in\nlanguage and vision. Comparing encoding models trained using representations\nfrom multimodal and unimodal transformers, we found that multimodal\ntransformers learn more aligned representations of concepts in language and\nvision. Our results demonstrate how multimodal transformers can provide\ninsights into the brain's capacity for multimodal processing.", "published": "2023-05-20 17:38:44", "link": "http://arxiv.org/abs/2305.12248v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Cross2StrA: Unpaired Cross-lingual Image Captioning with Cross-lingual\n  Cross-modal Structure-pivoted Alignment", "abstract": "Unpaired cross-lingual image captioning has long suffered from irrelevancy\nand disfluency issues, due to the inconsistencies of the semantic scene and\nsyntax attributes during transfer. In this work, we propose to address the\nabove problems by incorporating the scene graph (SG) structures and the\nsyntactic constituency (SC) trees. Our captioner contains the semantic\nstructure-guided image-to-pivot captioning and the syntactic structure-guided\npivot-to-target translation, two of which are joined via pivot language. We\nthen take the SG and SC structures as pivoting, performing cross-modal semantic\nstructure alignment and cross-lingual syntactic structure alignment learning.\nWe further introduce cross-lingual&cross-modal back-translation training to\nfully align the captioning and translation stages. Experiments on\nEnglish-Chinese transfers show that our model shows great superiority in\nimproving captioning relevancy and fluency.", "published": "2023-05-20 18:30:03", "link": "http://arxiv.org/abs/2305.12260v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Lifelong Language Pretraining with Distribution-Specialized Experts", "abstract": "Pretraining on a large-scale corpus has become a standard method to build\ngeneral language models (LMs). Adapting a model to new data distributions\ntargeting different downstream tasks poses significant challenges. Naive\nfine-tuning may incur catastrophic forgetting when the over-parameterized LMs\noverfit the new data but fail to preserve the pretrained features. Lifelong\nlearning (LLL) aims to enable information systems to learn from a continuous\ndata stream across time. However, most prior work modifies the training recipe\nassuming a static fixed network architecture. We find that additional model\ncapacity and proper regularization are key elements to achieving strong LLL\nperformance. Thus, we propose Lifelong-MoE, an extensible MoE\n(Mixture-of-Experts) architecture that dynamically adds model capacity via\nadding experts with regularized pretraining. Our results show that by only\nintroducing a limited number of extra experts while keeping the computation\ncost constant, our model can steadily adapt to data distribution shifts while\npreserving the previous knowledge. Compared to existing lifelong learning\napproaches, Lifelong-MoE achieves better few-shot performance on 19 downstream\nNLP tasks.", "published": "2023-05-20 21:15:19", "link": "http://arxiv.org/abs/2305.12281v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Logic-LM: Empowering Large Language Models with Symbolic Solvers for\n  Faithful Logical Reasoning", "abstract": "Large Language Models (LLMs) have shown human-like reasoning abilities but\nstill struggle with complex logical problems. This paper introduces a novel\nframework, Logic-LM, which integrates LLMs with symbolic solvers to improve\nlogical problem-solving. Our method first utilizes LLMs to translate a natural\nlanguage problem into a symbolic formulation. Afterward, a deterministic\nsymbolic solver performs inference on the formulated problem. We also introduce\na self-refinement module, which utilizes the symbolic solver's error messages\nto revise symbolic formalizations. We demonstrate Logic-LM's effectiveness on\nfive logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO,\nLogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant\nperformance boost of 39.2% over using LLM alone with standard prompting and\n18.4% over LLM with chain-of-thought prompting. Our findings suggest that\nLogic-LM, by combining LLMs with symbolic logic, offers a promising avenue for\nfaithful logical reasoning. Code and data are publicly available at\nhttps://github.com/teacherpeterpan/Logic-LLM.", "published": "2023-05-20 22:25:38", "link": "http://arxiv.org/abs/2305.12295v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Practical PCG Through Large Language Models", "abstract": "Large Language Models (LLMs) have proven to be useful tools in various\ndomains outside of the field of their inception, which was natural language\nprocessing. In this study, we provide practical directions on how to use LLMs\nto generate 2D-game rooms for an under-development game, named Metavoidal. Our\ntechnique can harness the power of GPT-3 by Human-in-the-loop fine-tuning which\nallows our method to create 37% Playable-Novel levels from as scarce data as\nonly 60 hand-designed rooms under a scenario of the non-trivial game, with\nrespect to (Procedural Content Generation) PCG, that has a good amount of local\nand global constraints.", "published": "2023-05-20 16:08:45", "link": "http://arxiv.org/abs/2305.18243v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UP5: Unbiased Foundation Model for Fairness-aware Recommendation", "abstract": "Recent advances in Foundation Models such as Large Language Models (LLMs)\nhave propelled them to the forefront of Recommender Systems (RS). Despite their\nutility, there is a growing concern that LLMs might inadvertently perpetuate\nsocietal stereotypes, resulting in unfair recommendations. Since fairness is\ncritical for RS as many users take it for decision-making and demand\nfulfillment, this paper focuses on user-side fairness for LLM-based\nrecommendation where the users may require a recommender system to be fair on\nspecific sensitive features such as gender or age. In this paper, we dive into\nthe extent of unfairness exhibited by LLM-based recommender models based on\nboth T5 and LLaMA backbones, and discuss appropriate methods for promoting\nequitable treatment of users in LLM-based recommendation models. We introduce a\nnovel Counterfactually-Fair-Prompt (CFP) method towards Unbiased Foundation\nmOdels (UFO) for fairness-aware LLM-based recommendation. Experiments are\nconducted on two real-world datasets, MovieLens-1M and Insurance, and compared\nwith both matching-based and sequential-based fairness-aware recommendation\nmodels. Results show that CFP achieves better recommendation performance with a\nhigh level of fairness. Data and code are open-sourced at\nhttps://github.com/agiresearch/UP5.", "published": "2023-05-20 04:32:59", "link": "http://arxiv.org/abs/2305.12090v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "EE-TTS: Emphatic Expressive TTS with Linguistic Information", "abstract": "While Current TTS systems perform well in synthesizing high-quality speech,\nproducing highly expressive speech remains a challenge. Emphasis, as a critical\nfactor in determining the expressiveness of speech, has attracted more\nattention nowadays. Previous works usually enhance the emphasis by adding\nintermediate features, but they can not guarantee the overall expressiveness of\nthe speech. To resolve this matter, we propose Emphatic Expressive TTS\n(EE-TTS), which leverages multi-level linguistic information from syntax and\nsemantics. EE-TTS contains an emphasis predictor that can identify appropriate\nemphasis positions from text and a conditioned acoustic model to synthesize\nexpressive speech with emphasis and linguistic information. Experimental\nresults indicate that EE-TTS outperforms baseline with MOS improvements of 0.49\nand 0.67 in expressiveness and naturalness. EE-TTS also shows strong\ngeneralization across different datasets according to AB test results.", "published": "2023-05-20 05:58:56", "link": "http://arxiv.org/abs/2305.12107v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Experimental results from applying GPT-4 to an unpublished formal\n  language", "abstract": "Can large language models be used to complete mathematical tasks that are\ntraditionally performed either manually or with the aid of theorem provers? To\nanswer this question, a state-of-the-art system, GPT-4, was provided with a\nconcise natural language specification for a previously unpublished formal\nsystem and asked to complete a number of tasks, from stating function and type\ndefinitions to proving simple theorems and verifying user-supplied proofs. The\nsystem completed all tasks successfully, showed extensive domain knowledge,\ninvented helpful new syntax and semantics, and exhibited generalization and\ninference abilities. So the answer seems to be: yes.", "published": "2023-05-20 14:00:08", "link": "http://arxiv.org/abs/2305.12196v1", "categories": ["cs.CL", "cs.LO", "math.LO", "03B35", "F.4.1; I.2.3; F.4.3"], "primary_category": "cs.CL"}
{"title": "Collaborative Development of NLP models", "abstract": "Despite substantial advancements, Natural Language Processing (NLP) models\noften require post-training adjustments to enforce business rules, rectify\nundesired behavior, and align with user values. These adjustments involve\noperationalizing \"concepts\"--dictating desired model responses to certain\ninputs. However, it's difficult for a single entity to enumerate and define all\npossible concepts, indicating a need for a multi-user, collaborative model\nalignment framework. Moreover, the exhaustive delineation of a concept is\nchallenging, and an improper approach can create shortcuts or interfere with\noriginal data or other concepts.\n  To address these challenges, we introduce CoDev, a framework that enables\nmulti-user interaction with the model, thereby mitigating individual\nlimitations. CoDev aids users in operationalizing their concepts using Large\nLanguage Models, and relying on the principle that NLP models exhibit simpler\nbehaviors in local regions. Our main insight is learning a \\emph{local} model\nfor each concept, and a \\emph{global} model to integrate the original data with\nall concepts. We then steer a large language model to generate instances within\nconcept boundaries where local and global disagree. Our experiments show CoDev\nis effective at helping multiple users operationalize concepts and avoid\ninterference for a variety of scenarios, tasks, and models.", "published": "2023-05-20 15:55:39", "link": "http://arxiv.org/abs/2305.12219v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SEntFiN 1.0: Entity-Aware Sentiment Analysis for Financial News", "abstract": "Fine-grained financial sentiment analysis on news headlines is a challenging\ntask requiring human-annotated datasets to achieve high performance. Limited\nstudies have tried to address the sentiment extraction task in a setting where\nmultiple entities are present in a news headline. In an effort to further\nresearch in this area, we make publicly available SEntFiN 1.0, a\nhuman-annotated dataset of 10,753 news headlines with entity-sentiment\nannotations, of which 2,847 headlines contain multiple entities, often with\nconflicting sentiments. We augment our dataset with a database of over 1,000\nfinancial entities and their various representations in news media amounting to\nover 5,000 phrases. We propose a framework that enables the extraction of\nentity-relevant sentiments using a feature-based approach rather than an\nexpression-based approach. For sentiment extraction, we utilize 12 different\nlearning schemes utilizing lexicon-based and pre-trained sentence\nrepresentations and five classification approaches. Our experiments indicate\nthat lexicon-based n-gram ensembles are above par with pre-trained word\nembedding schemes such as GloVe. Overall, RoBERTa and finBERT (domain-specific\nBERT) achieve the highest average accuracy of 94.29% and F1-score of 93.27%.\nFurther, using over 210,000 entity-sentiment predictions, we validate the\neconomic effect of sentiments on aggregate market movements over a long\nduration.", "published": "2023-05-20 18:20:39", "link": "http://arxiv.org/abs/2305.12257v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Self-supervised representations in speech-based depression detection", "abstract": "This paper proposes handling training data sparsity in speech-based automatic\ndepression detection (SDD) using foundation models pre-trained with\nself-supervised learning (SSL). An analysis of SSL representations derived from\ndifferent layers of pre-trained foundation models is first presented for SDD,\nwhich provides insight to suitable indicator for depression detection.\nKnowledge transfer is then performed from automatic speech recognition (ASR)\nand emotion recognition to SDD by fine-tuning the foundation models. Results\nshow that the uses of oracle and ASR transcriptions yield similar SDD\nperformance when the hidden representations of the ASR model is incorporated\nalong with the ASR textual information. By integrating representations from\nmultiple foundation models, state-of-the-art SDD results based on real ASR were\nachieved on the DAIC-WOZ dataset.", "published": "2023-05-20 18:41:13", "link": "http://arxiv.org/abs/2305.12263v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Patton: Language Model Pretraining on Text-Rich Networks", "abstract": "A real-world text corpus sometimes comprises not only text documents but also\nsemantic links between them (e.g., academic papers in a bibliographic network\nare linked by citations and co-authorships). Text documents and semantic\nconnections form a text-rich network, which empowers a wide range of downstream\ntasks such as classification and retrieval. However, pretraining methods for\nsuch structures are still lacking, making it difficult to build one generic\nmodel that can be adapted to various tasks on text-rich networks. Current\npretraining objectives, such as masked language modeling, purely model texts\nand do not take inter-document structure information into consideration. To\nthis end, we propose our PretrAining on TexT-Rich NetwOrk framework Patton.\nPatton includes two pretraining strategies: network-contextualized masked\nlanguage modeling and masked node prediction, to capture the inherent\ndependency between textual attributes and network structure. We conduct\nexperiments on four downstream tasks in five datasets from both academic and\ne-commerce domains, where Patton outperforms baselines significantly and\nconsistently.", "published": "2023-05-20 19:17:10", "link": "http://arxiv.org/abs/2305.12268v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Autoregressive Modeling with Lookahead Attention", "abstract": "To predict the next token, autoregressive models ordinarily examine the past.\nCould they also benefit from also examining hypothetical futures? We consider a\nnovel Transformer-based autoregressive architecture that estimates the\nnext-token distribution by extrapolating multiple continuations of the past,\naccording to some proposal distribution, and attending to these extended\nstrings. This architecture draws insights from classical AI systems such as\nboard game players: when making a local decision, a policy may benefit from\nexploring possible future trajectories and analyzing them. On multiple tasks\nincluding morphological inflection and Boolean satisfiability, our lookahead\nmodel is able to outperform the ordinary Transformer model of comparable size.\nHowever, on some tasks, it appears to be benefiting from the extra computation\nwithout actually using the lookahead information. We discuss possible variant\narchitectures as well as future speedups.", "published": "2023-05-20 19:29:47", "link": "http://arxiv.org/abs/2305.12272v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sentence Embedder Guided Utterance Encoder (SEGUE) for Spoken Language\n  Understanding", "abstract": "The pre-trained speech encoder wav2vec 2.0 performs very well on various\nspoken language understanding (SLU) tasks. However, on many tasks, it trails\nbehind text encoders with textual input. To improve the understanding\ncapability of SLU encoders, various studies have used knowledge distillation to\ntransfer knowledge from natural language understanding (NLU) encoders. We use a\nvery simple method of distilling from a textual sentence embedder directly into\nwav2vec 2.0 as pre-training, utilizing paired audio-text datasets. We observed\nthat this method is indeed capable of improving SLU task performance in\nfine-tuned settings, as well as full-data and few-shot transfer on a frozen\nencoder. However, the model performs worse on certain tasks highlighting the\nstrengths and weaknesses of our approach.", "published": "2023-05-20 23:55:55", "link": "http://arxiv.org/abs/2305.12301v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "CDJUR-BR -- A Golden Collection of Legal Document from Brazilian Justice\n  with Fine-Grained Named Entities", "abstract": "A basic task for most Legal Artificial Intelligence (Legal AI) applications\nis Named Entity Recognition (NER). However, texts produced in the context of\nlegal practice make references to entities that are not trivially recognized by\nthe currently available NERs. There is a lack of categorization of legislation,\njurisprudence, evidence, penalties, the roles of people in a legal process\n(judge, lawyer, victim, defendant, witness), types of locations (crime\nlocation, defendant's address), etc. In this sense, there is still a need for a\nrobust golden collection, annotated with fine-grained entities of the legal\ndomain, and which covers various documents of a legal process, such as\npetitions, inquiries, complaints, decisions and sentences. In this article, we\ndescribe the development of the Golden Collection of the Brazilian Judiciary\n(CDJUR-BR) contemplating a set of fine-grained named entities that have been\nannotated by experts in legal documents. The creation of CDJUR-BR followed its\nown methodology that aimed to attribute a character of comprehensiveness and\nrobustness. Together with the CDJUR-BR repository we provided a NER based on\nthe BERT model and trained with the CDJUR-BR, whose results indicated the\nprevalence of the CDJUR-BR.", "published": "2023-05-20 00:48:52", "link": "http://arxiv.org/abs/2305.18315v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Joint Generative-Contrastive Representation Learning for Anomalous Sound\n  Detection", "abstract": "In this paper, we propose a joint generative and contrastive representation\nlearning method (GeCo) for anomalous sound detection (ASD). GeCo exploits a\nPredictive AutoEncoder (PAE) equipped with self-attention as a generative model\nto perform frame-level prediction. The output of the PAE together with original\nnormal samples, are used for supervised contrastive representative learning in\na multi-task framework. Besides cross-entropy loss between classes, contrastive\nloss is used to separate PAE output and original samples within each class.\nGeCo aims to better capture context information among frames, thanks to the\nself-attention mechanism for PAE model. Furthermore, GeCo combines generative\nand contrastive learning from which we aim to yield more effective and\ninformative representations, compared to existing methods. Extensive\nexperiments have been conducted on the DCASE2020 Task2 development dataset,\nshowing that GeCo outperforms state-of-the-art generative and discriminative\nmethods.", "published": "2023-05-20 06:10:53", "link": "http://arxiv.org/abs/2305.12111v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ACA-Net: Towards Lightweight Speaker Verification using Asymmetric Cross\n  Attention", "abstract": "In this paper, we propose ACA-Net, a lightweight, global context-aware\nspeaker embedding extractor for Speaker Verification (SV) that improves upon\nexisting work by using Asymmetric Cross Attention (ACA) to replace temporal\npooling. ACA is able to distill large, variable-length sequences into small,\nfixed-sized latents by attending a small query to large key and value matrices.\nIn ACA-Net, we build a Multi-Layer Aggregation (MLA) block using ACA to\ngenerate fixed-sized identity vectors from variable-length inputs. Through\nglobal attention, ACA-Net acts as an efficient global feature extractor that\nadapts to temporal variability unlike existing SV models that apply a fixed\nfunction for pooling over the temporal dimension which may obscure information\nabout the signal's non-stationary temporal variability. Our experiments on the\nWSJ0-1talker show ACA-Net outperforms a strong baseline by 5\\% relative\nimprovement in EER using only 1/5 of the parameters.", "published": "2023-05-20 06:56:00", "link": "http://arxiv.org/abs/2305.12121v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ComedicSpeech: Text To Speech For Stand-up Comedies in Low-Resource\n  Scenarios", "abstract": "Text to Speech (TTS) models can generate natural and high-quality speech, but\nit is not expressive enough when synthesizing speech with dramatic\nexpressiveness, such as stand-up comedies. Considering comedians have diverse\npersonal speech styles, including personal prosody, rhythm, and fillers, it\nrequires real-world datasets and strong speech style modeling capabilities,\nwhich brings challenges. In this paper, we construct a new dataset and develop\nComedicSpeech, a TTS system tailored for the stand-up comedy synthesis in\nlow-resource scenarios. First, we extract prosody representation by the prosody\nencoder and condition it to the TTS model in a flexible way. Second, we enhance\nthe personal rhythm modeling by a conditional duration predictor. Third, we\nmodel the personal fillers by introducing comedian-related special tokens.\nExperiments show that ComedicSpeech achieves better expressiveness than\nbaselines with only ten-minute training data for each comedian. The audio\nsamples are available at https://xh621.github.io/stand-up-comedy-demo/", "published": "2023-05-20 14:24:45", "link": "http://arxiv.org/abs/2305.12200v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
