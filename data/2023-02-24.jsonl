{"title": "Adapting Knowledge for Few-shot Table-to-Text Generation", "abstract": "Pretrained language models (PLMs) have made remarkable progress in\ntable-to-text generation tasks. However, the lack of domain-specific knowledge\nmakes it challenging to bridge the topological gap between tabular data and\ntext, especially in real-world applications with limited resources. To mitigate\nthe limitation of insufficient labeled data, we propose a novel framework:\nAdapt-Knowledge-to-Generate (AKG). The core insight of AKG is to adapt\nunlabeled domain-specific knowledge into the model, which brings at least three\nbenefits: (1) it injects representation of normal table-related descriptions to\nbridge the topological gap between tabular data and texts; (2) it enables us to\nuse large amounts of unlabeled domain-specific knowledge fully, which can\nalleviate the PLMs' inherent shortcomings of lacking domain knowledge; (3) it\nallows us to design various tasks to employ the domain-specific knowledge.\nExtensive experiments and analyses are conducted on three open-domain, few-shot\nnatural language generation (NLG) data sets: Humans, Songs, and Books. Compared\nto previous state-of-the-art approaches, our model achieves superior\nperformance in terms of both fluency and accuracy as judged by human and\nautomatic evaluations.", "published": "2023-02-24 05:48:53", "link": "http://arxiv.org/abs/2302.12468v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Sentence Similarity Estimation for Unsupervised Extractive\n  Summarization", "abstract": "Unsupervised extractive summarization aims to extract salient sentences from\na document as the summary without labeled data. Recent literatures mostly\nresearch how to leverage sentence similarity to rank sentences in the order of\nsalience. However, sentence similarity estimation using pre-trained language\nmodels mostly takes little account of document-level information and has a weak\ncorrelation with sentence salience ranking. In this paper, we proposed two\nnovel strategies to improve sentence similarity estimation for unsupervised\nextractive summarization. We use contrastive learning to optimize a\ndocument-level objective that sentences from the same document are more similar\nthan those from different documents. Moreover, we use mutual learning to\nenhance the relationship between sentence similarity estimation and sentence\nsalience ranking, where an extra signal amplifier is used to refine the pivotal\ninformation. Experimental results demonstrate the effectiveness of our\nstrategies.", "published": "2023-02-24 07:10:33", "link": "http://arxiv.org/abs/2302.12490v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Time-aware Multiway Adaptive Fusion Network for Temporal Knowledge Graph\n  Question Answering", "abstract": "Knowledge graphs (KGs) have received increasing attention due to its wide\napplications on natural language processing. However, its use case on temporal\nquestion answering (QA) has not been well-explored. Most of existing methods\nare developed based on pre-trained language models, which might not be capable\nto learn \\emph{temporal-specific} presentations of entities in terms of\ntemporal KGQA task. To alleviate this problem, we propose a novel\n\\textbf{T}ime-aware \\textbf{M}ultiway \\textbf{A}daptive (\\textbf{TMA}) fusion\nnetwork. Inspired by the step-by-step reasoning behavior of humans. For each\ngiven question, TMA first extracts the relevant concepts from the KG, and then\nfeeds them into a multiway adaptive module to produce a\n\\emph{temporal-specific} representation of the question. This representation\ncan be incorporated with the pre-trained KG embedding to generate the final\nprediction. Empirical results verify that the proposed model achieves better\nperformance than the state-of-the-art models in the benchmark dataset. Notably,\nthe Hits@1 and Hits@10 results of TMA on the CronQuestions dataset's complex\nquestions are absolutely improved by 24\\% and 10\\% compared to the\nbest-performing baseline. Furthermore, we also show that TMA employing an\nadaptive fusion mechanism can provide interpretability by analyzing the\nproportion of information in question representations.", "published": "2023-02-24 09:29:40", "link": "http://arxiv.org/abs/2302.12529v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dual Path Modeling for Semantic Matching by Perceiving Subtle Conflicts", "abstract": "Transformer-based pre-trained models have achieved great improvements in\nsemantic matching. However, existing models still suffer from insufficient\nability to capture subtle differences. The modification, addition and deletion\nof words in sentence pairs may make it difficult for the model to predict their\nrelationship. To alleviate this problem, we propose a novel Dual Path Modeling\nFramework to enhance the model's ability to perceive subtle differences in\nsentence pairs by separately modeling affinity and difference semantics. Based\non dual-path modeling framework we design the Dual Path Modeling Network\n(DPM-Net) to recognize semantic relations. And we conduct extensive experiments\non 10 well-studied semantic matching and robustness test datasets, and the\nexperimental results show that our proposed method achieves consistent\nimprovements over baselines.", "published": "2023-02-24 09:29:55", "link": "http://arxiv.org/abs/2302.12530v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VivesDebate-Speech: A Corpus of Spoken Argumentation to Leverage Audio\n  Features for Argument Mining", "abstract": "In this paper, we describe VivesDebate-Speech, a corpus of spoken\nargumentation created to leverage audio features for argument mining tasks. The\ncreation of this corpus represents an important contribution to the\nintersection of speech processing and argument mining communities, and one of\nthe most complete publicly available resources in this topic. Moreover, we have\nperformed a set of first-of-their-kind experiments which show an improvement\nwhen integrating audio features into the argument mining pipeline. The provided\nresults can be used as a baseline for future research.", "published": "2023-02-24 11:44:24", "link": "http://arxiv.org/abs/2302.12584v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CARE: Collaborative AI-Assisted Reading Environment", "abstract": "Recent years have seen impressive progress in AI-assisted writing, yet the\ndevelopments in AI-assisted reading are lacking. We propose inline commentary\nas a natural vehicle for AI-based reading assistance, and present CARE: the\nfirst open integrated platform for the study of inline commentary and reading.\nCARE facilitates data collection for inline commentaries in a commonplace\ncollaborative reading environment, and provides a framework for enhancing\nreading with NLP-based assistance, such as text classification, generation or\nquestion answering. The extensible behavioral logging allows unique insights\ninto the reading and commenting behavior, and flexible configuration makes the\nplatform easy to deploy in new scenarios. To evaluate CARE in action, we apply\nthe platform in a user study dedicated to scholarly peer review. CARE\nfacilitates the data collection and study of inline commentary in NLP,\nextrinsic evaluation of NLP assistance, and application prototyping. We invite\nthe community to explore and build upon the open source implementation of CARE.", "published": "2023-02-24 12:55:31", "link": "http://arxiv.org/abs/2302.12611v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-Depth Look at Word Filling Societal Bias Measures", "abstract": "Many measures of societal bias in language models have been proposed in\nrecent years. A popular approach is to use a set of word filling prompts to\nevaluate the behavior of the language models. In this work, we analyze the\nvalidity of two such measures -- StereoSet and CrowS-Pairs. We show that these\nmeasures produce unexpected and illogical results when appropriate control\ngroup samples are constructed. Based on this, we believe that they are\nproblematic and using them in the future should be reconsidered. We propose a\nway forward with an improved testing protocol. Finally, we also introduce a new\ngender bias dataset for Slovak.", "published": "2023-02-24 14:08:07", "link": "http://arxiv.org/abs/2302.12640v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Spanish Built Factual Freectianary (Spanish-BFF): the first AI-generated\n  free dictionary", "abstract": "Dictionaries are one of the oldest and most used linguistic resources.\nBuilding them is a complex task that, to the best of our knowledge, has yet to\nbe explored with generative Large Language Models (LLMs). We introduce the\n\"Spanish Built Factual Freectianary\" (Spanish-BFF) as the first Spanish\nAI-generated dictionary. This first-of-its-kind free dictionary uses GPT-3. We\nalso define future steps we aim to follow to improve this initial commitment to\nthe field, such as more additional languages.", "published": "2023-02-24 16:59:54", "link": "http://arxiv.org/abs/2302.12746v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Prompt Augmentation and Selection with Chain-of-Thought from\n  Labeled Data", "abstract": "Chain-of-thought (CoT) advances the reasoning abilities of large language\nmodels (LLMs) and achieves superior performance in complex reasoning tasks.\nHowever, most CoT studies rely on carefully designed human-annotated rational\nchains to prompt LLMs, posing challenges for real-world applications where\nlabeled data is available without rational chains. This paper proposes a new\nstrategy, Automate-CoT (Automatic Prompt Augmentation and Selection with\nChain-of-Thought), that can bypass human engineering of CoT by automatically\naugmenting rational chains from a small labeled dataset, and then pruning\nlow-quality chains to construct a candidate pool of machine-generated rationale\nchains based on the labels. Finally, it selects the optimal combination of\nseveral rationale chains from the pool for CoT prompting by employing a\nvariance-reduced policy gradient strategy to estimate the significance of each\nexample. Automate-CoT enables a quick adaptation of the CoT technique to\ndifferent tasks. Experimental results demonstrate the effectiveness of our\nmethod, where competitive results are achieved on arithmetic reasoning (+2.7%),\ncommonsense reasoning (+3.4%), symbolic reasoning (+3.2%), and non-reasoning\ntasks (+2.5%). The code is available at\nhttps://github.com/SHUMKASHUN/Automate-CoT.", "published": "2023-02-24 18:58:06", "link": "http://arxiv.org/abs/2302.12822v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotion Prediction Oriented method with Multiple Supervisions for\n  Emotion-Cause Pair Extraction", "abstract": "Emotion-cause pair extraction (ECPE) task aims to extract all the pairs of\nemotions and their causes from an unannotated emotion text. The previous works\nusually extract the emotion-cause pairs from two perspectives of emotion and\ncause. However, emotion extraction is more crucial to the ECPE task than cause\nextraction. Motivated by this analysis, we propose an end-to-end emotion-cause\nextraction approach oriented toward emotion prediction (EPO-ECPE), aiming to\nfully exploit the potential of emotion prediction to enhance emotion-cause pair\nextraction. Considering the strong dependence between emotion prediction and\nemotion-cause pair extraction, we propose a synchronization mechanism to share\ntheir improvement in the training process. That is, the improvement of emotion\nprediction can facilitate the emotion-cause pair extraction, and then the\nresults of emotion-cause pair extraction can also be used to improve the\naccuracy of emotion prediction simultaneously. For the emotion-cause pair\nextraction, we divide it into genuine pair supervision and fake pair\nsupervision, where the genuine pair supervision learns from the pairs with more\npossibility to be emotion-cause pairs. In contrast, fake pair supervision\nlearns from other pairs. In this way, the emotion-cause pairs can be extracted\ndirectly from the genuine pair, thereby reducing the difficulty of extraction.\nExperimental results show that our approach outperforms the 13 compared systems\nand achieves new state-of-the-art performance.", "published": "2023-02-24 02:45:49", "link": "http://arxiv.org/abs/2302.12417v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MUX-PLMs: Data Multiplexing for High-throughput Language Models", "abstract": "The widespread adoption of large language models such as ChatGPT and Bard has\nled to unprecedented demand for these technologies. The burgeoning cost of\ninference for ever-increasing model sizes coupled with hardware shortages has\nlimited affordable access and poses a pressing need for efficiency approaches\ngeared towards high throughput and performance. Multi-input multi-output (MIMO)\nalgorithms such as data multiplexing, offer a promising solution with a\nmany-fold increase in throughput by performing inference for multiple inputs at\nthe cost of a single input. Yet these approaches are not currently performant\nenough to be deployed in modern systems. We change that by developing MUX-PLMs,\na class of high throughput pre-trained language models (PLMs) trained with data\nmultiplexing, that can be fine-tuned for any downstream task to yield\nhigh-throughput high-performance. Our novel multiplexing and demultiplexing\nmodules proficiently entangle and disentangle inputs, and enable\nhigh-performance high throughput \\muxplms{} that are competitive with vanilla\nPLMs while achieving 2x/5x inference speedup with only a $1-4\\%$ drop on a\nbroad suite of tasks.", "published": "2023-02-24 04:03:15", "link": "http://arxiv.org/abs/2302.12441v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Implicit Temporal Reasoning for Evidence-Based Fact-Checking", "abstract": "Leveraging contextual knowledge has become standard practice in automated\nclaim verification, yet the impact of temporal reasoning has been largely\noverlooked. Our study demonstrates that time positively influences the claim\nverification process of evidence-based fact-checking. The temporal aspects and\nrelations between claims and evidence are first established through grounding\non shared timelines, which are constructed using publication dates and time\nexpressions extracted from their text. Temporal information is then provided to\nRNN-based and Transformer-based classifiers before or after claim and evidence\nencoding. Our time-aware fact-checking models surpass base models by up to 9%\nMicro F1 (64.17%) and 15% Macro F1 (47.43%) on the MultiFC dataset. They also\noutperform prior methods that explicitly model temporal relations between\nevidence. Our findings show that the presence of temporal information and the\nmanner in which timelines are constructed greatly influence how fact-checking\nmodels determine the relevance and supporting or refuting character of evidence\ndocuments.", "published": "2023-02-24 10:48:03", "link": "http://arxiv.org/abs/2302.12569v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TUTORING: Instruction-Grounded Conversational Agent for Language\n  Learners", "abstract": "In this paper, we propose Tutoring bot, a generative chatbot trained on a\nlarge scale of tutor-student conversations for English-language learning. To\nmimic a human tutor's behavior in language education, the tutor bot leverages\ndiverse educational instructions and grounds to each instruction as additional\ninput context for the tutor response generation. As a single instruction\ngenerally involves multiple dialogue turns to give the student sufficient\nspeaking practice, the tutor bot is required to monitor and capture when the\ncurrent instruction should be kept or switched to the next instruction. For\nthat, the tutor bot is learned to not only generate responses but also infer\nits teaching action and progress on the current conversation simultaneously by\na multi-task learning scheme. Our Tutoring bot is deployed under a\nnon-commercial use license at https://tutoringai.com.", "published": "2023-02-24 13:36:11", "link": "http://arxiv.org/abs/2302.12623v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Cross-Lingual Transfer of Cognitive Processing Complexity", "abstract": "When humans read a text, their eye movements are influenced by the structural\ncomplexity of the input sentences. This cognitive phenomenon holds across\nlanguages and recent studies indicate that multilingual language models utilize\nstructural similarities between languages to facilitate cross-lingual transfer.\nWe use sentence-level eye-tracking patterns as a cognitive indicator for\nstructural complexity and show that the multilingual model XLM-RoBERTa can\nsuccessfully predict varied patterns for 13 typologically diverse languages,\ndespite being fine-tuned only on English data. We quantify the sensitivity of\nthe model to structural complexity and distinguish a range of complexity\ncharacteristics. Our results indicate that the model develops a meaningful bias\ntowards sentence length but also integrates cross-lingual differences. We\nconduct a control experiment with randomized word order and find that the model\nseems to additionally capture more complex structural information.", "published": "2023-02-24 15:48:23", "link": "http://arxiv.org/abs/2302.12695v2", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Check Your Facts and Try Again: Improving Large Language Models with\n  External Knowledge and Automated Feedback", "abstract": "Large language models (LLMs), such as ChatGPT, are able to generate\nhuman-like, fluent responses for many downstream tasks, e.g., task-oriented\ndialog and question answering. However, applying LLMs to real-world,\nmission-critical applications remains challenging mainly due to their tendency\nto generate hallucinations and their inability to use external knowledge. This\npaper proposes a LLM-Augmenter system, which augments a black-box LLM with a\nset of plug-and-play modules. Our system makes the LLM generate responses\ngrounded in external knowledge, e.g., stored in task-specific databases. It\nalso iteratively revises LLM prompts to improve model responses using feedback\ngenerated by utility functions, e.g., the factuality score of a LLM-generated\nresponse. The effectiveness of LLM-Augmenter is empirically validated on two\ntypes of scenarios, task-oriented dialog and open-domain question answering.\nLLM-Augmenter significantly reduces ChatGPT's hallucinations without\nsacrificing the fluency and informativeness of its responses. We make the\nsource code and models publicly available.", "published": "2023-02-24 18:48:43", "link": "http://arxiv.org/abs/2302.12813v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NoPPA: Non-Parametric Pairwise Attention Random Walk Model for Sentence\n  Representation", "abstract": "We propose a novel non-parametric/un-trainable language model, named\nNon-Parametric Pairwise Attention Random Walk Model (NoPPA), to generate\nsentence embedding only with pre-trained word embedding and pre-counted word\nfrequency. To the best we know, this study is the first successful attempt to\nbreak the constraint on bag-of-words assumption with a non-parametric attention\nmechanism. We evaluate our method on eight different downstream classification\ntasks. The experiment results show that NoPPA outperforms all kinds of\nbag-of-words-based methods in each dataset and provides a comparable or better\nperformance than the state-of-the-art non-parametric methods on average.\nFurthermore, visualization supports that NoPPA can understand contextual\ntopics, common phrases, and word causalities. Our model is available at\nhttps://github.com/JacksonWuxs/NoPPA.", "published": "2023-02-24 21:20:25", "link": "http://arxiv.org/abs/2302.12903v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adapting Pre-trained Language Models for Quantum Natural Language\n  Processing", "abstract": "The emerging classical-quantum transfer learning paradigm has brought a\ndecent performance to quantum computational models in many tasks, such as\ncomputer vision, by enabling a combination of quantum models and classical\npre-trained neural networks. However, using quantum computing with pre-trained\nmodels has yet to be explored in natural language processing (NLP). Due to the\nhigh linearity constraints of the underlying quantum computing infrastructures,\nexisting Quantum NLP models are limited in performance on real tasks. We fill\nthis gap by pre-training a sentence state with complex-valued BERT-like\narchitecture, and adapting it to the classical-quantum transfer learning scheme\nfor sentence classification. On quantum simulation experiments, the pre-trained\nrepresentation can bring 50\\% to 60\\% increases to the capacity of end-to-end\nquantum models.", "published": "2023-02-24 14:59:02", "link": "http://arxiv.org/abs/2302.13812v1", "categories": ["quant-ph", "cs.CL"], "primary_category": "quant-ph"}
{"title": "Factual Consistency Oriented Speech Recognition", "abstract": "This paper presents a novel optimization framework for automatic speech\nrecognition (ASR) with the aim of reducing hallucinations produced by an ASR\nmodel. The proposed framework optimizes the ASR model to maximize an expected\nfactual consistency score between ASR hypotheses and ground-truth\ntranscriptions, where the factual consistency score is computed by a separately\ntrained estimator. Experimental results using the AMI meeting corpus and the\nVoxPopuli corpus show that the ASR model trained with the proposed framework\ngenerates ASR hypotheses that have significantly higher consistency scores with\nground-truth transcriptions while maintaining the word error rates close to\nthose of cross entropy-trained ASR models. Furthermore, it is shown that\ntraining the ASR models with the proposed framework improves the speech\nsummarization quality as measured by the factual consistency of meeting\nconversation summaries generated by a large language model.", "published": "2023-02-24 00:01:41", "link": "http://arxiv.org/abs/2302.12369v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ProofNet: Autoformalizing and Formally Proving Undergraduate-Level\n  Mathematics", "abstract": "We introduce ProofNet, a benchmark for autoformalization and formal proving\nof undergraduate-level mathematics. The ProofNet benchmarks consists of 371\nexamples, each consisting of a formal theorem statement in Lean 3, a natural\nlanguage theorem statement, and a natural language proof. The problems are\nprimarily drawn from popular undergraduate pure mathematics textbooks and cover\ntopics such as real and complex analysis, linear algebra, abstract algebra, and\ntopology. We intend for ProofNet to be a challenging benchmark that will drive\nprogress in autoformalization and automatic theorem proving. We report baseline\nresults on statement autoformalization via in-context learning. Moreover, we\nintroduce two novel statement autoformalization methods: prompt retrieval and\ndistilled backtranslation.", "published": "2023-02-24 03:28:46", "link": "http://arxiv.org/abs/2302.12433v1", "categories": ["cs.CL", "cs.AI", "cs.LO"], "primary_category": "cs.CL"}
{"title": "SGL-PT: A Strong Graph Learner with Graph Prompt Tuning", "abstract": "Recently, much exertion has been paid to design graph self-supervised methods\nto obtain generalized pre-trained models, and adapt pre-trained models onto\ndownstream tasks through fine-tuning. However, there exists an inherent gap\nbetween pretext and downstream graph tasks, which insufficiently exerts the\nability of pre-trained models and even leads to negative transfer. Meanwhile,\nprompt tuning has seen emerging success in natural language processing by\naligning pre-training and fine-tuning with consistent training objectives. In\nthis paper, we identify the challenges for graph prompt tuning: The first is\nthe lack of a strong and universal pre-training task across sundry pre-training\nmethods in graph domain. The second challenge lies in the difficulty of\ndesigning a consistent training objective for both pre-training and downstream\ntasks. To overcome above obstacles, we propose a novel framework named SGL-PT\nwhich follows the learning strategy ``Pre-train, Prompt, and Predict''.\nSpecifically, we raise a strong and universal pre-training task coined as SGL\nthat acquires the complementary merits of generative and contrastive\nself-supervised graph learning. And aiming for graph classification task, we\nunify pre-training and fine-tuning by designing a novel verbalizer-free\nprompting function, which reformulates the downstream task in a similar format\nas pretext task. Empirical results show that our method surpasses other\nbaselines under unsupervised setting, and our prompt tuning method can greatly\nfacilitate models on biological datasets over fine-tuning methods.", "published": "2023-02-24 04:31:18", "link": "http://arxiv.org/abs/2302.12449v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Analyzing And Editing Inner Mechanisms Of Backdoored Language Models", "abstract": "Poisoning of data sets is a potential security threat to large language\nmodels that can lead to backdoored models. A description of the internal\nmechanisms of backdoored language models and how they process trigger inputs,\ne.g., when switching to toxic language, has yet to be found. In this work, we\nstudy the internal representations of transformer-based backdoored language\nmodels and determine early-layer MLP modules as most important for the backdoor\nmechanism in combination with the initial embedding projection. We use this\nknowledge to remove, insert, and modify backdoor mechanisms with engineered\nreplacements that reduce the MLP module outputs to essentials for the backdoor\nmechanism. To this end, we introduce PCP ablation, where we replace transformer\nmodules with low-rank matrices based on the principal components of their\nactivations. We demonstrate our results on backdoored toy, backdoored large,\nand non-backdoored open-source models. We show that we can improve the backdoor\nrobustness of large language models by locally constraining individual modules\nduring fine-tuning on potentially poisonous data sets.\n  Trigger warning: Offensive language.", "published": "2023-02-24 05:26:08", "link": "http://arxiv.org/abs/2302.12461v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Fairness in Language Models Beyond English: Gaps and Challenges", "abstract": "With language models becoming increasingly ubiquitous, it has become\nessential to address their inequitable treatment of diverse demographic groups\nand factors. Most research on evaluating and mitigating fairness harms has been\nconcentrated on English, while multilingual models and non-English languages\nhave received comparatively little attention. This paper presents a survey of\nfairness in multilingual and non-English contexts, highlighting the\nshortcomings of current research and the difficulties faced by methods designed\nfor English. We contend that the multitude of diverse cultures and languages\nacross the world makes it infeasible to achieve comprehensive coverage in terms\nof constructing fairness datasets. Thus, the measurement and mitigation of\nbiases must evolve beyond the current dataset-driven practices that are\nnarrowly focused on specific dimensions and types of biases and, therefore,\nimpossible to scale across languages and cultures.", "published": "2023-02-24 11:25:50", "link": "http://arxiv.org/abs/2302.12578v2", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Modelling Temporal Document Sequences for Clinical ICD Coding", "abstract": "Past studies on the ICD coding problem focus on predicting clinical codes\nprimarily based on the discharge summary. This covers only a small fraction of\nthe notes generated during each hospital stay and leaves potential for\nimproving performance by analysing all the available clinical notes. We propose\na hierarchical transformer architecture that uses text across the entire\nsequence of clinical notes in each hospital stay for ICD coding, and\nincorporates embeddings for text metadata such as their position, time, and\ntype of note. While using all clinical notes increases the quantity of data\nsubstantially, superconvergence can be used to reduce training costs. We\nevaluate the model on the MIMIC-III dataset. Our model exceeds the prior\nstate-of-the-art when using only discharge summaries as input, and achieves\nfurther performance improvements when all clinical notes are used as input.", "published": "2023-02-24 14:41:48", "link": "http://arxiv.org/abs/2302.12666v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Language Models are Few-shot Learners for Prognostic Prediction", "abstract": "Clinical prediction is an essential task in the healthcare industry. However,\nthe recent success of transformers, on which large language models are built,\nhas not been extended to this domain. In this research, we explore the use of\ntransformers and language models in prognostic prediction for immunotherapy\nusing real-world patients' clinical data and molecular profiles. This paper\ninvestigates the potential of transformers to improve clinical prediction\ncompared to conventional machine learning approaches and addresses the\nchallenge of few-shot learning in predicting rare disease areas. The study\nbenchmarks the efficacy of baselines and language models on prognostic\nprediction across multiple cancer types and investigates the impact of\ndifferent pretrained language models under few-shot regimes. The results\ndemonstrate significant improvements in accuracy and highlight the potential of\nNLP in clinical research to improve early detection and intervention for\ndifferent diseases.", "published": "2023-02-24 15:35:36", "link": "http://arxiv.org/abs/2302.12692v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.QM"], "primary_category": "cs.CL"}
{"title": "Ensemble knowledge distillation of self-supervised speech models", "abstract": "Distilled self-supervised models have shown competitive performance and\nefficiency in recent years. However, there is a lack of experience in jointly\ndistilling multiple self-supervised speech models. In our work, we performed\nEnsemble Knowledge Distillation (EKD) on various self-supervised speech models\nsuch as HuBERT, RobustHuBERT, and WavLM. We tried two different aggregation\ntechniques, layerwise-average and layerwise-concatenation, to the\nrepresentations of different teacher models and found that the former was more\neffective. On top of that, we proposed a multiple prediction head method for\nstudent models to predict different layer outputs of multiple teacher models\nsimultaneously. The experimental results show that our method improves the\nperformance of the distilled models on four downstream speech processing tasks,\nPhoneme Recognition, Speaker Identification, Emotion Recognition, and Automatic\nSpeech Recognition in the hidden-set track of the SUPERB benchmark.", "published": "2023-02-24 17:15:39", "link": "http://arxiv.org/abs/2302.12757v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "STA: Self-controlled Text Augmentation for Improving Text\n  Classifications", "abstract": "Despite recent advancements in Machine Learning, many tasks still involve\nworking in low-data regimes which can make solving natural language problems\ndifficult. Recently, a number of text augmentation techniques have emerged in\nthe field of Natural Language Processing (NLP) which can enrich the training\ndata with new examples, though they are not without their caveats. For\ninstance, simple rule-based heuristic methods are effective, but lack variation\nin semantic content and syntactic structure with respect to the original text.\nOn the other hand, more complex deep learning approaches can cause extreme\nshifts in the intrinsic meaning of the text and introduce unwanted noise into\nthe training data. To more reliably control the quality of the augmented\nexamples, we introduce a state-of-the-art approach for Self-Controlled Text\nAugmentation (STA). Our approach tightly controls the generation process by\nintroducing a self-checking procedure to ensure that generated examples retain\nthe semantic content of the original text. Experimental results on multiple\nbenchmarking datasets demonstrate that STA substantially outperforms existing\nstate-of-the-art techniques, whilst qualitative analysis reveals that the\ngenerated examples are both lexically diverse and semantically reliable.", "published": "2023-02-24 17:54:12", "link": "http://arxiv.org/abs/2302.12784v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HULAT at SemEval-2023 Task 9: Data augmentation for pre-trained\n  transformers applied to Multilingual Tweet Intimacy Analysis", "abstract": "This paper describes our participation in SemEval-2023 Task 9, Intimacy\nAnalysis of Multilingual Tweets. We fine-tune some of the most popular\ntransformer models with the training dataset and synthetic data generated by\ndifferent data augmentation techniques. During the development phase, our best\nresults were obtained by using XLM-T. Data augmentation techniques provide a\nvery slight improvement in the results. Our system ranked in the 27th position\nout of the 45 participating systems. Despite its modest results, our system\nshows promising results in languages such as Portuguese, English, and Dutch.\nAll our code is available in the repository\n\\url{https://github.com/isegura/hulat_intimacy}.", "published": "2023-02-24 18:10:37", "link": "http://arxiv.org/abs/2302.12794v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Improving Massively Multilingual ASR With Auxiliary CTC Objectives", "abstract": "Multilingual Automatic Speech Recognition (ASR) models have extended the\nusability of speech technologies to a wide variety of languages. With how many\nlanguages these models have to handle, however, a key to understanding their\nimbalanced performance across different languages is to examine if the model\nactually knows which language it should transcribe. In this paper, we introduce\nour work on improving performance on FLEURS, a 102-language open ASR benchmark,\nby conditioning the entire model on language identity (LID). We investigate\ntechniques inspired from recent Connectionist Temporal Classification (CTC)\nstudies to help the model handle the large number of languages, conditioning on\nthe LID predictions of auxiliary tasks. Our experimental results demonstrate\nthe effectiveness of our technique over standard CTC/Attention-based hybrid\nmodels. Furthermore, our state-of-the-art systems using self-supervised models\nwith the Conformer architecture improve over the results of prior work on\nFLEURS by a relative 28.4% CER. Trained models and reproducible recipes are\navailable at https://github.com/espnet/espnet/tree/master/egs2/fleurs/asr1 .", "published": "2023-02-24 18:59:51", "link": "http://arxiv.org/abs/2302.12829v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "HULAT at SemEval-2023 Task 10: Data augmentation for pre-trained\n  transformers applied to the detection of sexism in social media", "abstract": "This paper describes our participation in SemEval-2023 Task 10, whose goal is\nthe detection of sexism in social media. We explore some of the most popular\ntransformer models such as BERT, DistilBERT, RoBERTa, and XLNet. We also study\ndifferent data augmentation techniques to increase the training dataset. During\nthe development phase, our best results were obtained by using RoBERTa and data\naugmentation for tasks B and C. However, the use of synthetic data does not\nimprove the results for task C. We participated in the three subtasks. Our\napproach still has much room for improvement, especially in the two\nfine-grained classifications. All our code is available in the repository\nhttps://github.com/isegura/hulat_edos.", "published": "2023-02-24 18:17:38", "link": "http://arxiv.org/abs/2302.12840v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Pre-Finetuning for Few-Shot Emotional Speech Recognition", "abstract": "Speech models have long been known to overfit individual speakers for many\nclassification tasks. This leads to poor generalization in settings where the\nspeakers are out-of-domain or out-of-distribution, as is common in production\nenvironments. We view speaker adaptation as a few-shot learning problem and\npropose investigating transfer learning approaches inspired by recent success\nwith pre-trained models in natural language tasks. We propose pre-finetuning\nspeech models on difficult tasks to distill knowledge into few-shot downstream\nclassification objectives. We pre-finetune Wav2Vec2.0 on every permutation of\nfour multiclass emotional speech recognition corpora and evaluate our\npre-finetuned models through 33,600 few-shot fine-tuning trials on the\nEmotional Speech Dataset.", "published": "2023-02-24 22:38:54", "link": "http://arxiv.org/abs/2302.12921v3", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Robot Behavior-Tree-Based Task Generation with Large Language Models", "abstract": "Nowadays, the behavior tree is gaining popularity as a representation for\nrobot tasks due to its modularity and reusability. Designing behavior-tree\ntasks manually is time-consuming for robot end-users, thus there is a need for\ninvestigating automatic behavior-tree-based task generation. Prior\nbehavior-tree-based task generation approaches focus on fixed primitive tasks\nand lack generalizability to new task domains. To cope with this issue, we\npropose a novel behavior-tree-based task generation approach that utilizes\nstate-of-the-art large language models. We propose a Phase-Step prompt design\nthat enables a hierarchical-structured robot task generation and further\nintegrate it with behavior-tree-embedding-based search to set up the\nappropriate prompt. In this way, we enable an automatic and cross-domain\nbehavior-tree task generation. Our behavior-tree-based task generation approach\ndoes not require a set of pre-defined primitive tasks. End-users only need to\ndescribe an abstract desired task and our proposed approach can swiftly\ngenerate the corresponding behavior tree. A full-process case study is provided\nto demonstrate our proposed approach. An ablation study is conducted to\nevaluate the effectiveness of our Phase-Step prompts. Assessment on Phase-Step\nprompts and the limitation of large language models are presented and\ndiscussed.", "published": "2023-02-24 22:53:10", "link": "http://arxiv.org/abs/2302.12927v1", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Phone and speaker spatial organization in self-supervised speech\n  representations", "abstract": "Self-supervised representations of speech are currently being widely used for\na large number of applications. Recently, some efforts have been made in trying\nto analyze the type of information present in each of these representations.\nMost such work uses downstream models to test whether the representations can\nbe successfully used for a specific task. The downstream models, though,\ntypically perform nonlinear operations on the representation extracting\ninformation that may not have been readily available in the original\nrepresentation. In this work, we analyze the spatial organization of phone and\nspeaker information in several state-of-the-art speech representations using\nmethods that do not require a downstream model. We measure how different layers\nencode basic acoustic parameters such as formants and pitch using\nrepresentation similarity analysis. Further, we study the extent to which each\nrepresentation clusters the speech samples by phone or speaker classes using\nnon-parametric statistical testing. Our results indicate that models represent\nthese speech attributes differently depending on the target task used during\npretraining.", "published": "2023-02-24 19:39:42", "link": "http://arxiv.org/abs/2302.14055v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LaSER: Language-Specific Event Recommendation", "abstract": "While societal events often impact people worldwide, a significant fraction\nof events has a local focus that primarily affects specific language\ncommunities. Examples include national elections, the development of the\nCoronavirus pandemic in different countries, and local film festivals such as\nthe C\\'esar Awards in France and the Moscow International Film Festival in\nRussia. However, existing entity recommendation approaches do not sufficiently\naddress the language context of recommendation. This article introduces the\nnovel task of language-specific event recommendation, which aims to recommend\nevents relevant to the user query in the language-specific context. This task\ncan support essential information retrieval activities, including web\nnavigation and exploratory search, considering the language context of user\ninformation needs. We propose LaSER, a novel approach toward language-specific\nevent recommendation. LaSER blends the language-specific latent representations\n(embeddings) of entities and events and spatio-temporal event features in a\nlearning to rank model. This model is trained on publicly available Wikipedia\nClickstream data. The results of our user study demonstrate that LaSER\noutperforms state-of-the-art recommendation baselines by up to 33 percentage\npoints in MAP@5 concerning the language-specific relevance of recommended\nevents.", "published": "2023-02-24 22:25:27", "link": "http://arxiv.org/abs/2303.04712v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Language-Driven Representation Learning for Robotics", "abstract": "Recent work in visual representation learning for robotics demonstrates the\nviability of learning from large video datasets of humans performing everyday\ntasks. Leveraging methods such as masked autoencoding and contrastive learning,\nthese representations exhibit strong transfer to policy learning for visuomotor\ncontrol. But, robot learning encompasses a diverse set of problems beyond\ncontrol including grasp affordance prediction, language-conditioned imitation\nlearning, and intent scoring for human-robot collaboration, amongst others.\nFirst, we demonstrate that existing representations yield inconsistent results\nacross these tasks: masked autoencoding approaches pick up on low-level spatial\nfeatures at the cost of high-level semantics, while contrastive learning\napproaches capture the opposite. We then introduce Voltron, a framework for\nlanguage-driven representation learning from human videos and associated\ncaptions. Voltron trades off language-conditioned visual reconstruction to\nlearn low-level visual patterns, and visually-grounded language generation to\nencode high-level semantics. We also construct a new evaluation suite spanning\nfive distinct robot learning problems $\\unicode{x2013}$ a unified platform for\nholistically evaluating visual representations for robotics. Through\ncomprehensive, controlled experiments across all five problems, we find that\nVoltron's language-driven representations outperform the prior\nstate-of-the-art, especially on targeted problems requiring higher-level\nfeatures.", "published": "2023-02-24 17:29:31", "link": "http://arxiv.org/abs/2302.12766v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Towards multi-task learning of speech and speaker recognition", "abstract": "We study multi-task learning for two orthogonal speech technology tasks:\nspeech and speaker recognition. We use wav2vec2 as a base architecture with two\ntask-specific output heads. We experiment with different architectural\ndecisions to mix speaker and speech information in the output sequence as well\nas different optimization strategies. Our multi-task learning networks can\nproduce a shared speaker and speech embedding, which on first glance achieve a\nperformance comparable to separate single-task models. However, we show that\nthe multi-task networks have strongly degraded performance on\nout-of-distribution evaluation data compared to the single-task models. Code\nand model checkpoints are available at\nhttps://github.com/nikvaessen/disjoint-mtl", "published": "2023-02-24 17:42:25", "link": "http://arxiv.org/abs/2302.12773v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PITS: Variational Pitch Inference without Fundamental Frequency for\n  End-to-End Pitch-controllable TTS", "abstract": "Previous pitch-controllable text-to-speech (TTS) models rely on directly\nmodeling fundamental frequency, leading to low variance in synthesized speech.\nTo address this issue, we propose PITS, an end-to-end pitch-controllable TTS\nmodel that utilizes variational inference to model pitch. Based on VITS, PITS\nincorporates the Yingram encoder, the Yingram decoder, and adversarial training\nof pitch-shifted synthesis to achieve pitch-controllability. Experiments\ndemonstrate that PITS generates high-quality speech that is indistinguishable\nfrom ground truth speech and has high pitch-controllability without quality\ndegradation. Code, audio samples, and demo are available at\nhttps://github.com/anonymous-pits/pits.", "published": "2023-02-24 01:43:17", "link": "http://arxiv.org/abs/2302.12391v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Catch You and I Can: Revealing Source Voiceprint Against Voice\n  Conversion", "abstract": "Voice conversion (VC) techniques can be abused by malicious parties to\ntransform their audios to sound like a target speaker, making it hard for a\nhuman being or a speaker verification/identification system to trace the source\nspeaker. In this paper, we make the first attempt to restore the source\nvoiceprint from audios synthesized by voice conversion methods with high\ncredit. However, unveiling the features of the source speaker from a converted\naudio is challenging since the voice conversion operation intends to\ndisentangle the original features and infuse the features of the target\nspeaker. To fulfill our goal, we develop Revelio, a representation learning\nmodel, which learns to effectively extract the voiceprint of the source speaker\nfrom converted audio samples. We equip Revelio with a carefully-designed\ndifferential rectification algorithm to eliminate the influence of the target\nspeaker by removing the representation component that is parallel to the\nvoiceprint of the target speaker. We have conducted extensive experiments to\nevaluate the capability of Revelio in restoring voiceprint from audios\nconverted by VQVC, VQVC+, AGAIN, and BNE. The experiments verify that Revelio\nis able to rebuild voiceprints that can be traced to the source speaker by\nspeaker verification and identification systems. Revelio also exhibits robust\nperformance under inter-gender conversion, unseen languages, and telephony\nnetworks.", "published": "2023-02-24 03:33:13", "link": "http://arxiv.org/abs/2302.12434v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Supervised Hierarchical Clustering using Graph Neural Networks for\n  Speaker Diarization", "abstract": "Conventional methods for speaker diarization involve windowing an audio file\ninto short segments to extract speaker embeddings, followed by an unsupervised\nclustering of the embeddings. This multi-step approach generates speaker\nassignments for each segment. In this paper, we propose a novel Supervised\nHierArchical gRaph Clustering algorithm (SHARC) for speaker diarization where\nwe introduce a hierarchical structure using Graph Neural Network (GNN) to\nperform supervised clustering. The supervision allows the model to update the\nrepresentations and directly improve the clustering performance, thus enabling\na single-step approach for diarization. In the proposed work, the input segment\nembeddings are treated as nodes of a graph with the edge weights corresponding\nto the similarity scores between the nodes. We also propose an approach to\njointly update the embedding extractor and the GNN model to perform end-to-end\nspeaker diarization (E2E-SHARC). During inference, the hierarchical clustering\nis performed using node densities and edge existence probabilities to merge the\nsegments until convergence. In the diarization experiments, we illustrate that\nthe proposed E2E-SHARC approach achieves 53% and 44% relative improvements over\nthe baseline systems on benchmark datasets like AMI and Voxconverse,\nrespectively.", "published": "2023-02-24 16:16:41", "link": "http://arxiv.org/abs/2302.12716v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Deep Neural Network Based Reverse Radio Spectrogram Search Algorithm", "abstract": "Modern radio astronomy instruments generate vast amounts of data, and the\nincreasingly challenging radio frequency interference (RFI) environment\nnecessitates ever-more sophisticated RFI rejection algorithms. The \"needle in a\nhaystack\" nature of searches for transients and technosignatures requires us to\ndevelop methods that can determine whether a signal of interest has unique\nproperties, or is a part of some larger set of pernicious RFI. In the past,\nthis vetting has required onerous manual inspection of very large numbers of\nsignals. In this paper we present a fast and modular deep learning algorithm to\nsearch for lookalike signals of interest in radio spectrogram data. First, we\ntrained a B-Variational Autoencoder on signals returned by an energy detection\nalgorithm. We then adapted a positional embedding layer from classical\nTransformer architecture to a embed additional metadata, which we demonstrate\nusing a frequency-based embedding. Next we used the encoder component of the\nB-Variational Autoencoder to extract features from small (~ 715,Hz, with a\nresolution of 2.79Hz per frequency bin) windows in the radio spectrogram. We\nused our algorithm to conduct a search for a given query (encoded signal of\ninterest) on a set of signals (encoded features of searched items) to produce\nthe top candidates with similar features. We successfully demonstrate that the\nalgorithm retrieves signals with similar appearance, given only the original\nradio spectrogram data. This algorithm can be used to improve the efficiency of\nvetting signals of interest in technosignature searches, but could also be\napplied to a wider variety of searches for \"lookalike\" signals in large\nastronomical datasets.", "published": "2023-02-24 04:28:46", "link": "http://arxiv.org/abs/2302.13854v2", "categories": ["eess.SP", "astro-ph.IM", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
