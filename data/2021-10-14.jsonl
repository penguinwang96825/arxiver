{"title": "MIMICause: Representation and automatic extraction of causal relation\n  types from clinical notes", "abstract": "Understanding causal narratives communicated in clinical notes can help make\nstrides towards personalized healthcare. Extracted causal information from\nclinical notes can be combined with structured EHR data such as patients'\ndemographics, diagnoses, and medications. This will enhance healthcare\nproviders' ability to identify aspects of a patient's story communicated in the\nclinical notes and help make more informed decisions.\n  In this work, we propose annotation guidelines, develop an annotated corpus\nand provide baseline scores to identify types and direction of causal relations\nbetween a pair of biomedical concepts in clinical notes; communicated\nimplicitly or explicitly, identified either in a single sentence or across\nmultiple sentences.\n  We annotate a total of 2714 de-identified examples sampled from the 2018 n2c2\nshared task dataset and train four different language model based\narchitectures. Annotation based on our guidelines achieved a high\ninter-annotator agreement i.e. Fleiss' kappa ($\\kappa$) score of 0.72, and our\nmodel for identification of causal relations achieved a macro F1 score of 0.56\non the test data. The high inter-annotator agreement for clinical text shows\nthe quality of our annotation guidelines while the provided baseline F1 score\nsets the direction for future research towards understanding narratives in\nclinical texts.", "published": "2021-10-14 00:15:36", "link": "http://arxiv.org/abs/2110.07090v2", "categories": ["cs.CL", "I.2.7; I.5.4"], "primary_category": "cs.CL"}
{"title": "Identifying Introductions in Podcast Episodes from Automatically\n  Generated Transcripts", "abstract": "As the volume of long-form spoken-word content such as podcasts explodes,\nmany platforms desire to present short, meaningful, and logically coherent\nsegments extracted from the full content. Such segments can be consumed by\nusers to sample content before diving in, as well as used by the platform to\npromote and recommend content. However, little published work is focused on the\nsegmentation of spoken-word content, where the errors (noise) in transcripts\ngenerated by automatic speech recognition (ASR) services poses many challenges.\nHere we build a novel dataset of complete transcriptions of over 400 podcast\nepisodes, in which we label the position of introductions in each episode.\nThese introductions contain information about the episodes' topics, hosts, and\nguests, providing a valuable summary of the episode content, as it is created\nby the authors. We further augment our dataset with word substitutions to\nincrease the amount of available training data. We train three Transformer\nmodels based on the pre-trained BERT and different augmentation strategies,\nwhich achieve significantly better performance compared with a static embedding\nmodel, showing that it is possible to capture generalized, larger-scale\nstructural information from noisy, loosely-organized speech data. This is\nfurther demonstrated through an analysis of the models' inner architecture. Our\nmethods and dataset can be used to facilitate future work on the\nstructure-based segmentation of spoken-word content.", "published": "2021-10-14 00:34:51", "link": "http://arxiv.org/abs/2110.07096v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "bert2BERT: Towards Reusable Pretrained Language Models", "abstract": "In recent years, researchers tend to pre-train ever-larger language models to\nexplore the upper limit of deep models. However, large language model\npre-training costs intensive computational resources and most of the models are\ntrained from scratch without reusing the existing pre-trained models, which is\nwasteful. In this paper, we propose bert2BERT, which can effectively transfer\nthe knowledge of an existing smaller pre-trained model (e.g., BERT_BASE) to a\nlarge model (e.g., BERT_LARGE) through parameter initialization and\nsignificantly improve the pre-training efficiency of the large model.\nSpecifically, we extend the previous function-preserving on Transformer-based\nlanguage model, and further improve it by proposing advanced knowledge for\nlarge model's initialization. In addition, a two-stage pre-training method is\nproposed to further accelerate the training process. We did extensive\nexperiments on representative PLMs (e.g., BERT and GPT) and demonstrate that\n(1) our method can save a significant amount of training cost compared with\nbaselines including learning from scratch, StackBERT and MSLT; (2) our method\nis generic and applicable to different types of pre-trained models. In\nparticular, bert2BERT saves about 45% and 47% computational cost of\npre-training BERT_BASE and GPT_BASE by reusing the models of almost their half\nsizes. The source code will be publicly available upon publication.", "published": "2021-10-14 04:05:25", "link": "http://arxiv.org/abs/2110.07143v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Open-Domain Question Answering with Answer Sentence\n  Generation", "abstract": "Open-Domain Generative Question Answering has achieved impressive performance\nin English by combining document-level retrieval with answer generation. These\napproaches, which we refer to as GenQA, can generate complete sentences,\neffectively answering both factoid and non-factoid questions. In this paper, we\nextend GenQA to the multilingual and cross-lingual settings. For this purpose,\nwe first introduce GenTyDiQA, an extension of the TyDiQA dataset with\nwell-formed and complete answers for Arabic, Bengali, English, Japanese, and\nRussian. Based on GenTyDiQA, we design a cross-lingual generative model that\nproduces full-sentence answers by exploiting passages written in multiple\nlanguages, including languages different from the question. Our cross-lingual\ngenerative system outperforms answer sentence selection baselines for all 5\nlanguages and monolingual generative pipelines for three out of five languages\nstudied.", "published": "2021-10-14 04:36:29", "link": "http://arxiv.org/abs/2110.07150v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interpreting the Robustness of Neural NLP Models to Textual\n  Perturbations", "abstract": "Modern Natural Language Processing (NLP) models are known to be sensitive to\ninput perturbations and their performance can decrease when applied to\nreal-world, noisy data. However, it is still unclear why models are less robust\nto some perturbations than others. In this work, we test the hypothesis that\nthe extent to which a model is affected by an unseen textual perturbation\n(robustness) can be explained by the learnability of the perturbation (defined\nas how well the model learns to identify the perturbation with a small amount\nof evidence). We further give a causal justification for the learnability\nmetric. We conduct extensive experiments with four prominent NLP models --\nTextRNN, BERT, RoBERTa and XLNet -- over eight types of textual perturbations\non three datasets. We show that a model which is better at identifying a\nperturbation (higher learnability) becomes worse at ignoring such a\nperturbation at test time (lower robustness), providing empirical support for\nour hypothesis.", "published": "2021-10-14 05:26:08", "link": "http://arxiv.org/abs/2110.07159v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer over Pre-trained Transformer for Neural Text Segmentation\n  with Enhanced Topic Coherence", "abstract": "This paper proposes a transformer over transformer framework, called\nTransformer$^2$, to perform neural text segmentation. It consists of two\ncomponents: bottom-level sentence encoders using pre-trained transformers, and\nan upper-level transformer-based segmentation model based on the sentence\nembeddings. The bottom-level component transfers the pre-trained knowledge\nlearned from large external corpora under both single and pair-wise supervised\nNLP tasks to model the sentence embeddings for the documents. Given the\nsentence embeddings, the upper-level transformer is trained to recover the\nsegmentation boundaries as well as the topic labels of each sentence. Equipped\nwith a multi-task loss and the pre-trained knowledge, Transformer$^2$ can\nbetter capture the semantic coherence within the same segments. Our experiments\nshow that (1) Transformer$^2$ manages to surpass state-of-the-art text\nsegmentation models in terms of a commonly-used semantic coherence measure; (2)\nin most cases, both single and pair-wise pre-trained knowledge contribute to\nthe model performance; (3) bottom-level sentence encoders pre-trained on\nspecific languages yield better performance than those pre-trained on specific\ndomains.", "published": "2021-10-14 05:26:39", "link": "http://arxiv.org/abs/2110.07160v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CaPE: Contrastive Parameter Ensembling for Reducing Hallucination in\n  Abstractive Summarization", "abstract": "Hallucination is a known issue for neural abstractive summarization models.\nRecent work suggests that the degree of hallucination may depend on errors in\nthe training data. In this work, we propose a new method called Contrastive\nParameter Ensembling (CaPE) to use training data more effectively, utilizing\nvariations in noise in training samples to reduce hallucination. We first\nselect clean and noisy subsets from the training data using different automatic\nfactual metrics. Then, we fine-tune a base summarization model, which is\ntrained on all training samples, on the clean (noisy) subset to obtain an\n\\textit{expert} (\\textit{anti-expert}) model. Finally, we adjust the parameters\nof base model by the difference between parameters of the \\textit{expert} and\n\\textit{anti-expert} models, steering the base model towards the\n\\textit{expert} model and away from the \\textit{anti-expert} model.\nExperimental results show that CaPE improves performance across different\nautomatic factual metrics and human evaluation, with the maximum improvement of\n16.69\\% and 15.78\\% on summary-level dependency-arc entailment accuracy for the\nXSUM and CNN/DM datasets. The improvement in factual performance does not\ndegrade the performance on other metrics of informativeness such as ROUGE.", "published": "2021-10-14 06:02:54", "link": "http://arxiv.org/abs/2110.07166v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Symbolic Knowledge Distillation: from General Language Models to\n  Commonsense Models", "abstract": "The common practice for training commonsense models has gone\nfrom-human-to-corpus-to-machine: humans author commonsense knowledge graphs in\norder to train commonsense models. In this work, we investigate an alternative,\nfrom-machine-to-corpus-to-machine: general language models author these\ncommonsense knowledge graphs to train commonsense models. Our study leads to a\nnew framework, Symbolic Knowledge Distillation. As with prior art in Knowledge\nDistillation (Hinton et al., 2015), our approach uses larger models to teach\nsmaller models. A key difference is that we distill knowledge symbolically-as\ntext-in addition to the neural model. We also distill only one aspect-the\ncommonsense of a general language model teacher, allowing the student to be a\ndifferent type, a commonsense model. Altogether, we show that careful prompt\nengineering and a separately trained critic model allow us to selectively\ndistill high-quality causal commonsense from GPT-3, a general language model.\nEmpirical results demonstrate that, for the first time, a human-authored\ncommonsense knowledge graph is surpassed by our automatically distilled variant\nin all three criteria: quantity, quality, and diversity. In addition, it\nresults in a neural commonsense model that surpasses the teacher model's\ncommonsense capabilities despite its 100x smaller size. We apply this to the\nATOMIC resource, and share our new symbolic knowledge graph and commonsense\nmodels.", "published": "2021-10-14 06:50:19", "link": "http://arxiv.org/abs/2110.07178v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Self-Supervision Objectives for Generalizable Coherence\n  Modeling", "abstract": "Given the claims of improved text generation quality across various\npre-trained neural models, we consider the coherence evaluation of machine\ngenerated text to be one of the principal applications of coherence models that\nneeds to be investigated. Prior work in neural coherence modeling has primarily\nfocused on devising new architectures for solving the permuted document task.\nWe instead use a basic model architecture and show significant improvements\nover state of the art within the same training regime. We then design a harder\nself-supervision objective by increasing the ratio of negative samples within a\ncontrastive learning setup, and enhance the model further through automatic\nhard negative mining coupled with a large global negative queue encoded by a\nmomentum encoder. We show empirically that increasing the density of negative\nsamples improves the basic model, and using a global negative queue further\nimproves and stabilizes the model while training with hard negative samples. We\nevaluate the coherence model on task-independent test sets that resemble\nreal-world applications and show significant improvements in coherence\nevaluations of downstream tasks.", "published": "2021-10-14 07:44:14", "link": "http://arxiv.org/abs/2110.07198v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Causal Transformers Perform Below Chance on Recursive Nested\n  Constructions, Unlike Humans", "abstract": "Recursive processing is considered a hallmark of human linguistic abilities.\nA recent study evaluated recursive processing in recurrent neural language\nmodels (RNN-LMs) and showed that such models perform below chance level on\nembedded dependencies within nested constructions -- a prototypical example of\nrecursion in natural language. Here, we study if state-of-the-art Transformer\nLMs do any better. We test four different Transformer LMs on two different\ntypes of nested constructions, which differ in whether the embedded (inner)\ndependency is short or long range. We find that Transformers achieve\nnear-perfect performance on short-range embedded dependencies, significantly\nbetter than previous results reported for RNN-LMs and humans. However, on\nlong-range embedded dependencies, Transformers' performance sharply drops below\nchance level. Remarkably, the addition of only three words to the embedded\ndependency caused Transformers to fall from near-perfect to below-chance\nperformance. Taken together, our results reveal Transformers' shortcoming when\nit comes to recursive, structure-based, processing.", "published": "2021-10-14 09:22:17", "link": "http://arxiv.org/abs/2110.07240v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "P-Adapters: Robustly Extracting Factual Information from Language Models\n  with Diverse Prompts", "abstract": "Recent work (e.g. LAMA (Petroni et al., 2019)) has found that the quality of\nthe factual information extracted from Large Language Models (LLMs) depends on\nthe prompts used to query them. This inconsistency is problematic because\ndifferent users will query LLMs for the same information using different\nwording, but should receive the same, accurate responses regardless. In this\nwork we aim to address this shortcoming by introducing P-Adapters: lightweight\nmodels that sit between the embedding layer and first attention layer of LLMs.\nThey take LLM embeddings as input and output continuous prompts that are used\nto query the LLM. Additionally, we investigate Mixture of Experts (MoE) models\nthat learn a set of continuous prompts (\"experts\") and select one to query the\nLLM. They require a separate classifier trained on human-annotated data to map\nnatural language prompts to the continuous ones. P-Adapters perform comparably\nto the more complex MoE models in extracting factual information from BERT and\nRoBERTa while eliminating the need for additional annotations. P-Adapters show\nbetween 12-26% absolute improvement in precision and 36-50% absolute\nimprovement in consistency over a baseline of only using natural language\nqueries. Finally, we investigate what makes P-Adapters successful and conclude\nthat a significant factor is access to the LLM's embeddings of the original\nnatural language prompt, particularly the subject of the entity pair being\nqueried.", "published": "2021-10-14 11:32:22", "link": "http://arxiv.org/abs/2110.07280v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based\n  on Prompt Tuning of T5", "abstract": "Existing approaches to lifelong language learning rely on plenty of labeled\ndata for learning a new task, which is hard to obtain in most real scenarios.\nConsidering that humans can continually learn new tasks from a handful of\nexamples, we expect the models also to be able to generalize well on new\nfew-shot tasks without forgetting the previous ones. In this work, we define\nthis more challenging yet practical problem as Lifelong Few-shot Language\nLearning (LFLL) and propose a unified framework for it based on prompt tuning\nof T5. Our framework called LFPT5 takes full advantage of PT's strong few-shot\nlearning ability, and simultaneously trains the model as a task solver and a\ndata generator. Before learning a new domain of the same task type, LFPT5\ngenerates pseudo (labeled) samples of previously learned domains, and later\ngets trained on those samples to alleviate forgetting of previous knowledge as\nit learns the new domain. In addition, a KL divergence loss is minimized to\nachieve label consistency between the previous and the current model. While\nadapting to a new task type, LFPT5 includes and tunes additional prompt\nembeddings for the new task. With extensive experiments, we demonstrate that\nLFPT5 can be applied to various different types of tasks and significantly\noutperform previous methods in different LFLL settings.", "published": "2021-10-14 12:06:29", "link": "http://arxiv.org/abs/2110.07298v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aspect-Sentiment-Multiple-Opinion Triplet Extraction", "abstract": "Aspect Sentiment Triplet Extraction (ASTE) aims to extract aspect term\n(aspect), sentiment and opinion term (opinion) triplets from sentences and can\ntell a complete story, i.e., the discussed aspect, the sentiment toward the\naspect, and the cause of the sentiment. ASTE is a charming task, however, one\ntriplet extracted by ASTE only includes one opinion of the aspect, but an\naspect in a sentence may have multiple corresponding opinions and one opinion\nonly provides part of the reason why the aspect has this sentiment, as a\nconsequence, some triplets extracted by ASTE are hard to understand, and\nprovide erroneous information for downstream tasks. In this paper, we introduce\na new task, named Aspect Sentiment Multiple Opinions Triplet Extraction\n(ASMOTE). ASMOTE aims to extract aspect, sentiment and multiple opinions\ntriplets. Specifically, one triplet extracted by ASMOTE contains all opinions\nabout the aspect and can tell the exact reason that the aspect has the\nsentiment. We propose an Aspect-Guided Framework (AGF) to address this task.\nAGF first extracts aspects, then predicts their opinions and sentiments.\nMoreover, with the help of the proposed Sequence Labeling Attention(SLA), AGF\nimproves the performance of the sentiment classification using the extracted\nopinions. Experimental results on multiple datasets demonstrate the\neffectiveness of our approach.", "published": "2021-10-14 12:12:31", "link": "http://arxiv.org/abs/2110.07303v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Investigation of Multi-bridge Multilingual NMT models", "abstract": "In this paper, we present an extensive investigation of multi-bridge,\nmany-to-many multilingual NMT models (MB-M2M) ie., models trained on\nnon-English language pairs in addition to English-centric language pairs. In\naddition to validating previous work which shows that MB-M2M models can\novercome zeroshot translation problems, our analysis reveals the following\nresults about multibridge models: (1) it is possible to extract a reasonable\namount of parallel corpora between non-English languages for low-resource\nlanguages (2) with limited non-English centric data, MB-M2M models are\ncompetitive with or outperform pivot models, (3) MB-M2M models can outperform\nEnglish-Any models and perform at par with Any-English models, so a single\nmultilingual NMT system can serve all translation directions.", "published": "2021-10-14 12:14:22", "link": "http://arxiv.org/abs/2110.07304v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Solving Aspect Category Sentiment Analysis as a Text Generation Task", "abstract": "Aspect category sentiment analysis has attracted increasing research\nattention. The dominant methods make use of pre-trained language models by\nlearning effective aspect category-specific representations, and adding\nspecific output layers to its pre-trained representation. We consider a more\ndirect way of making use of pre-trained language models, by casting the ACSA\ntasks into natural language generation tasks, using natural language sentences\nto represent the output. Our method allows more direct use of pre-trained\nknowledge in seq2seq language models by directly following the task setting\nduring pre-training. Experiments on several benchmarks show that our method\ngives the best reported results, having large advantages in few-shot and\nzero-shot settings.", "published": "2021-10-14 12:25:21", "link": "http://arxiv.org/abs/2110.07310v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WMDecompose: A Framework for Leveraging the Interpretable Properties of\n  Word Mover's Distance in Sociocultural Analysis", "abstract": "Despite the increasing popularity of NLP in the humanities and social\nsciences, advances in model performance and complexity have been accompanied by\nconcerns about interpretability and explanatory power for sociocultural\nanalysis. One popular model that balances complexity and legibility is Word\nMover's Distance (WMD). Ostensibly adapted for its interpretability, WMD has\nnonetheless been used and further developed in ways which frequently discard\nits most interpretable aspect: namely, the word-level distances required for\ntranslating a set of words into another set of words. To address this apparent\ngap, we introduce WMDecompose: a model and Python library that 1) decomposes\ndocument-level distances into their constituent word-level distances, and 2)\nsubsequently clusters words to induce thematic elements, such that useful\nlexical information is retained and summarized for analysis. To illustrate its\npotential in a social scientific context, we apply it to a longitudinal social\nmedia corpus to explore the interrelationship between conspiracy theories and\nconservative American discourses. Finally, because of the full WMD model's high\ntime-complexity, we additionally suggest a method of sampling document pairs\nfrom large datasets in a reproducible way, with tight bounds that prevent\nextrapolation of unreliable results due to poor sampling practices.", "published": "2021-10-14 13:04:38", "link": "http://arxiv.org/abs/2110.07330v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "RocketQAv2: A Joint Training Method for Dense Passage Retrieval and\n  Passage Re-ranking", "abstract": "In various natural language processing tasks, passage retrieval and passage\nre-ranking are two key procedures in finding and ranking relevant information.\nSince both the two procedures contribute to the final performance, it is\nimportant to jointly optimize them in order to achieve mutual improvement. In\nthis paper, we propose a novel joint training approach for dense passage\nretrieval and passage re-ranking. A major contribution is that we introduce the\ndynamic listwise distillation, where we design a unified listwise training\napproach for both the retriever and the re-ranker. During the dynamic\ndistillation, the retriever and the re-ranker can be adaptively improved\naccording to each other's relevance information. We also propose a hybrid data\naugmentation strategy to construct diverse training instances for listwise\ntraining approach. Extensive experiments show the effectiveness of our approach\non both MSMARCO and Natural Questions datasets. Our code is available at\nhttps://github.com/PaddlePaddle/RocketQA.", "published": "2021-10-14 13:52:55", "link": "http://arxiv.org/abs/2110.07367v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transferring Semantic Knowledge Into Language Encoders", "abstract": "We introduce semantic form mid-tuning, an approach for transferring semantic\nknowledge from semantic meaning representations into transformer-based language\nencoders. In mid-tuning, we learn to align the text of general sentences -- not\ntied to any particular inference task -- and structured semantic\nrepresentations of those sentences. Our approach does not require gold\nannotated semantic representations. Instead, it makes use of automatically\ngenerated semantic representations, such as from off-the-shelf PropBank and\nFrameNet semantic parsers. We show that this alignment can be learned\nimplicitly via classification or directly via triplet loss. Our method yields\nlanguage encoders that demonstrate improved predictive performance across\ninference, reading comprehension, textual similarity, and other semantic tasks\ndrawn from the GLUE, SuperGLUE, and SentEval benchmarks. We evaluate our\napproach on three popular baseline models, where our experimental results and\nanalysis concludes that current pre-trained language models can further benefit\nfrom structured semantic frames with the proposed mid-tuning method, as they\ninject additional task-agnostic knowledge to the encoder, improving the\ngenerated embeddings as well as the linguistic properties of the given model,\nas evident from improvements on a popular sentence embedding toolkit and a\nvariety of probing tasks.", "published": "2021-10-14 14:11:12", "link": "http://arxiv.org/abs/2110.07382v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PARE: A Simple and Strong Baseline for Monolingual and Multilingual\n  Distantly Supervised Relation Extraction", "abstract": "Neural models for distantly supervised relation extraction (DS-RE) encode\neach sentence in an entity-pair bag separately. These are then aggregated for\nbag-level relation prediction. Since, at encoding time, these approaches do not\nallow information to flow from other sentences in the bag, we believe that they\ndo not utilize the available bag data to the fullest. In response, we explore a\nsimple baseline approach (PARE) in which all sentences of a bag are\nconcatenated into a passage of sentences, and encoded jointly using BERT. The\ncontextual embeddings of tokens are aggregated using attention with the\ncandidate relation as query -- this summary of whole passage predicts the\ncandidate relation. We find that our simple baseline solution outperforms\nexisting state-of-the-art DS-RE models in both monolingual and multilingual\nDS-RE datasets.", "published": "2021-10-14 14:45:51", "link": "http://arxiv.org/abs/2110.07415v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Model Robustness to User-generated Noisy Texts", "abstract": "Sensitivity of deep-neural models to input noise is known to be a challenging\nproblem. In NLP, model performance often deteriorates with naturally occurring\nnoise, such as spelling errors. To mitigate this issue, models may leverage\nartificially noised data. However, the amount and type of generated noise has\nso far been determined arbitrarily. We therefore propose to model the errors\nstatistically from grammatical-error-correction corpora. We present a thorough\nevaluation of several state-of-the-art NLP systems' robustness in multiple\nlanguages, with tasks including morpho-syntactic analysis, named entity\nrecognition, neural machine translation, a subset of the GLUE benchmark and\nreading comprehension. We also compare two approaches to address the\nperformance drop: a) training the NLP models with noised data generated by our\nframework; and b) reducing the input noise with external system for natural\nlanguage correction. The code is released at https://github.com/ufal/kazitext.", "published": "2021-10-14 14:54:52", "link": "http://arxiv.org/abs/2110.07428v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards More Effective and Economic Sparsely-Activated Model", "abstract": "The sparsely-activated models have achieved great success in natural language\nprocessing through large-scale parameters and relatively low computational\ncost, and gradually become a feasible technique for training and implementing\nextremely large models. Due to the limit of communication cost, activating\nmultiple experts is hardly affordable during training and inference. Therefore,\nprevious work usually activate just one expert at a time to alleviate\nadditional communication cost. Such routing mechanism limits the upper bound of\nmodel performance. In this paper, we first investigate a phenomenon that\nincreasing the number of activated experts can boost the model performance with\nhigher sparse ratio. To increase the number of activated experts without an\nincrease in computational cost, we propose SAM (Switch and Mixture) routing, an\nefficient hierarchical routing mechanism that activates multiple experts in a\nsame device (GPU). Our methods shed light on the training of extremely large\nsparse models and experiments prove that our models can achieve significant\nperformance gain with great efficiency improvement.", "published": "2021-10-14 14:58:53", "link": "http://arxiv.org/abs/2110.07431v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Designing Language Technologies for Social Good: The Road not Taken", "abstract": "Development of speech and language technology for social good (LT4SG),\nespecially those targeted at the welfare of marginalized communities and\nspeakers of low-resource and under-served languages, has been a prominent theme\nof research within NLP, Speech, and the AI communities. Researchers have mostly\nrelied on their individual expertise, experiences or ad hoc surveys for\nprioritization of language technologies that provide social good to the\nend-users. This has been criticized by several scholars who argue that work on\nLT4SG must include the target linguistic communities during the design and\ndevelopment process. However, none of the LT4SG work and their critiques\nsuggest principled techniques for prioritization of the technologies and\nmethods for inclusion of the end-user during the development cycle. Drawing\ninspiration from the fields of Economics, Ethics, Psychology, and Participatory\nDesign, here we chart out a set of methodologies for prioritizing LT4SG that\nare aligned with the end-user preferences. We then analyze several LT4SG\nefforts in light of the proposed methodologies and bring out their hidden\nassumptions and potential pitfalls. While the current study is limited to\nlanguage technologies, we believe that the principles and prioritization\ntechniques highlighted here are applicable more broadly to AI for Social Good.", "published": "2021-10-14 15:12:05", "link": "http://arxiv.org/abs/2110.07444v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MReD: A Meta-Review Dataset for Structure-Controllable Text Generation", "abstract": "When directly using existing text generation datasets for controllable\ngeneration, we are facing the problem of not having the domain knowledge and\nthus the aspects that could be controlled are limited. A typical example is\nwhen using CNN/Daily Mail dataset for controllable text summarization, there is\nno guided information on the emphasis of summary sentences. A more useful text\ngenerator should leverage both the input text and the control signal to guide\nthe generation, which can only be built with a deep understanding of the domain\nknowledge. Motivated by this vision, our paper introduces a new text generation\ndataset, named MReD. Our new dataset consists of 7,089 meta-reviews and all its\n45k meta-review sentences are manually annotated with one of the 9 carefully\ndefined categories, including abstract, strength, decision, etc. We present\nexperimental results on start-of-the-art summarization models, and propose\nmethods for structure-controlled generation with both extractive and\nabstractive models using our annotated data. By exploring various settings and\nanalyzing the model behavior with respect to the control signal, we demonstrate\nthe challenges of our proposed task and the values of our dataset MReD.\nMeanwhile, MReD also allows us to have a better understanding of the\nmeta-review domain.", "published": "2021-10-14 15:48:03", "link": "http://arxiv.org/abs/2110.07474v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RecInDial: A Unified Framework for Conversational Recommendation with\n  Pretrained Language Models", "abstract": "Conversational Recommender System (CRS), which aims to recommend high-quality\nitems to users through interactive conversations, has gained great research\ninterest recently. A CRS is usually composed of a recommendation module and a\ngeneration module. In the previous work, these two modules are loosely\nconnected in the model training and are shallowly integrated during inference,\nwhere a simple switching or copy mechanism is adopted to incorporate\nrecommended items into generated responses. Moreover, the current end-to-end\nneural models trained on small crowd-sourcing datasets (e.g., 10K dialogs in\nthe ReDial dataset) tend to overfit and have poor chit-chat ability. In this\nwork, we propose a novel unified framework that integrates recommendation into\nthe dialog (RecInDial) generation by introducing a vocabulary pointer. To\ntackle the low-resource issue in CRS, we finetune the large-scale pretrained\nlanguage models to generate fluent and diverse responses, and introduce a\nknowledge-aware bias learned from an entity-oriented knowledge graph to enhance\nthe recommendation performance. Furthermore, we propose to evaluate the CRS\nmodels in an end-to-end manner, which can reflect the overall performance of\nthe entire system rather than the performance of individual modules, compared\nto the separate evaluations of the two modules used in previous work.\nExperiments on the benchmark dataset ReDial show our RecInDial model\nsignificantly surpasses the state-of-the-art methods. More extensive analyses\nshow the effectiveness of our model.", "published": "2021-10-14 15:49:48", "link": "http://arxiv.org/abs/2110.07477v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fusing Heterogeneous Factors with Triaffine Mechanism for Nested Named\n  Entity Recognition", "abstract": "Nested entities are observed in many domains due to their compositionality,\nwhich cannot be easily recognized by the widely-used sequence labeling\nframework. A natural solution is to treat the task as a span classification\nproblem. To learn better span representation and increase classification\nperformance, it is crucial to effectively integrate heterogeneous factors\nincluding inside tokens, boundaries, labels, and related spans which could be\ncontributing to nested entities recognition. To fuse these heterogeneous\nfactors, we propose a novel triaffine mechanism including triaffine attention\nand scoring. Triaffine attention uses boundaries and labels as queries and uses\ninside tokens and related spans as keys and values for span representations.\nTriaffine scoring interacts with boundaries and span representations for\nclassification. Experiments show that our proposed method outperforms previous\nspan-based methods, achieves the state-of-the-art $F_1$ scores on nested NER\ndatasets GENIA and KBP2017, and shows comparable results on ACE2004 and\nACE2005.", "published": "2021-10-14 15:52:17", "link": "http://arxiv.org/abs/2110.07480v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Pitfalls of Analyzing Individual Neurons in Language Models", "abstract": "While many studies have shown that linguistic information is encoded in\nhidden word representations, few have studied individual neurons, to show how\nand in which neurons it is encoded. Among these, the common approach is to use\nan external probe to rank neurons according to their relevance to some\nlinguistic attribute, and to evaluate the obtained ranking using the same probe\nthat produced it. We show two pitfalls in this methodology: 1. It confounds\ndistinct factors: probe quality and ranking quality. We separate them and draw\nconclusions on each. 2. It focuses on encoded information, rather than\ninformation that is used by the model. We show that these are not the same. We\ncompare two recent ranking methods and a simple one we introduce, and evaluate\nthem with regard to both of these aspects.", "published": "2021-10-14 15:57:07", "link": "http://arxiv.org/abs/2110.07483v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Non-Autoregressive Translation with Layer-Wise Prediction and Deep\n  Supervision", "abstract": "How do we perform efficient inference while retaining high translation\nquality? Existing neural machine translation models, such as Transformer,\nachieve high performance, but they decode words one by one, which is\ninefficient. Recent non-autoregressive translation models speed up the\ninference, but their quality is still inferior. In this work, we propose DSLP,\na highly efficient and high-performance model for machine translation. The key\ninsight is to train a non-autoregressive Transformer with Deep Supervision and\nfeed additional Layer-wise Predictions. We conducted extensive experiments on\nfour translation tasks (both directions of WMT'14 EN-DE and WMT'16 EN-RO).\nResults show that our approach consistently improves the BLEU scores compared\nwith respective base models. Specifically, our best variant outperforms the\nautoregressive model on three translation tasks, while being 14.8 times more\nefficient in inference.", "published": "2021-10-14 16:36:12", "link": "http://arxiv.org/abs/2110.07515v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparative Opinion Summarization via Collaborative Decoding", "abstract": "Opinion summarization focuses on generating summaries that reflect popular\nsubjective information expressed in multiple online reviews. While generated\nsummaries offer general and concise information about a particular hotel or\nproduct, the information may be insufficient to help the user compare multiple\ndifferent choices. Thus, the user may still struggle with the question \"Which\none should I pick?\" In this paper, we propose the comparative opinion\nsummarization task, which aims at generating two contrastive summaries and one\ncommon summary from two different candidate sets of reviews. We develop a\ncomparative summarization framework CoCoSum, which consists of two base\nsummarization models that jointly generate contrastive and common summaries.\nExperimental results on a newly created benchmark CoCoTrip show that CoCoSum\ncan produce higher-quality contrastive and common summaries than\nstate-of-the-art opinion summarization models. The dataset and code are\navailable at https://github.com/megagonlabs/cocosum", "published": "2021-10-14 16:41:54", "link": "http://arxiv.org/abs/2110.07520v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence-aware Contrastive Learning for Open-Domain Passage Retrieval", "abstract": "Training dense passage representations via contrastive learning has been\nshown effective for Open-Domain Passage Retrieval (ODPR). Existing studies\nfocus on further optimizing by improving negative sampling strategy or extra\npretraining. However, these studies keep unknown in capturing passage with\ninternal representation conflicts from improper modeling granularity. This work\nthus presents a refined model on the basis of a smaller granularity, contextual\nsentences, to alleviate the concerned conflicts. In detail, we introduce an\nin-passage negative sampling strategy to encourage a diverse generation of\nsentence representations within the same passage. Experiments on three\nbenchmark datasets verify the efficacy of our method, especially on datasets\nwhere conflicts are severe. Extensive experiments further present good\ntransferability of our method across datasets.", "published": "2021-10-14 16:43:43", "link": "http://arxiv.org/abs/2110.07524v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Irrationality of Neural Rationale Models", "abstract": "Neural rationale models are popular for interpretable predictions of NLP\ntasks. In these, a selector extracts segments of the input text, called\nrationales, and passes these segments to a classifier for prediction. Since the\nrationale is the only information accessible to the classifier, it is plausibly\ndefined as the explanation. Is such a characterization unconditionally correct?\nIn this paper, we argue to the contrary, with both philosophical perspectives\nand empirical evidence suggesting that rationale models are, perhaps, less\nrational and interpretable than expected. We call for more rigorous and\ncomprehensive evaluations of these models to ensure desired properties of\ninterpretability are indeed achieved. The code can be found at\nhttps://github.com/yimingz89/Neural-Rationale-Analysis.", "published": "2021-10-14 17:22:10", "link": "http://arxiv.org/abs/2110.07550v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Composable Sparse Fine-Tuning for Cross-Lingual Transfer", "abstract": "Fine-tuning the entire set of parameters of a large pretrained model has\nbecome the mainstream approach for transfer learning. To increase its\nefficiency and prevent catastrophic forgetting and interference, techniques\nlike adapters and sparse fine-tuning have been developed. Adapters are modular,\nas they can be combined to adapt a model towards different facets of knowledge\n(e.g., dedicated language and/or task adapters). Sparse fine-tuning is\nexpressive, as it controls the behavior of all model components. In this work,\nwe introduce a new fine-tuning method with both these desirable properties. In\nparticular, we learn sparse, real-valued masks based on a simple variant of the\nLottery Ticket Hypothesis. Task-specific masks are obtained from annotated data\nin a source language, and language-specific masks from masked language modeling\nin a target language. Both these masks can then be composed with the pretrained\nmodel. Unlike adapter-based fine-tuning, this method neither increases the\nnumber of parameters at inference time nor alters the original model\narchitecture. Most importantly, it outperforms adapters in zero-shot\ncross-lingual transfer by a large margin in a series of multilingual\nbenchmarks, including Universal Dependencies, MasakhaNER, and AmericasNLI.\nBased on an in-depth analysis, we additionally find that sparsity is crucial to\nprevent both 1) interference between the fine-tunings to be composed and 2)\noverfitting. We release the code and models at\nhttps://github.com/cambridgeltl/composable-sft.", "published": "2021-10-14 17:27:29", "link": "http://arxiv.org/abs/2110.07560v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Machines Learn Morality? The Delphi Experiment", "abstract": "As AI systems become increasingly powerful and pervasive, there are growing\nconcerns about machines' morality or a lack thereof. Yet, teaching morality to\nmachines is a formidable task, as morality remains among the most intensely\ndebated questions in humanity, let alone for AI. Existing AI systems deployed\nto millions of users, however, are already making decisions loaded with moral\nimplications, which poses a seemingly impossible challenge: teaching machines\nmoral sense, while humanity continues to grapple with it.\n  To explore this challenge, we introduce Delphi, an experimental framework\nbased on deep neural networks trained directly to reason about descriptive\nethical judgments, e.g., \"helping a friend\" is generally good, while \"helping a\nfriend spread fake news\" is not. Empirical results shed novel insights on the\npromises and limits of machine ethics; Delphi demonstrates strong\ngeneralization capabilities in the face of novel ethical situations, while\noff-the-shelf neural network models exhibit markedly poor judgment including\nunjust biases, confirming the need for explicitly teaching machines moral\nsense.\n  Yet, Delphi is not perfect, exhibiting susceptibility to pervasive biases and\ninconsistencies. Despite that, we demonstrate positive use cases of imperfect\nDelphi, including using it as a component model within other imperfect AI\nsystems. Importantly, we interpret the operationalization of Delphi in light of\nprominent ethical theories, which leads us to important future research\nquestions.", "published": "2021-10-14 17:38:12", "link": "http://arxiv.org/abs/2110.07574v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Explanations Be Useful for Calibrating Black Box Models?", "abstract": "NLP practitioners often want to take existing trained models and apply them\nto data from new domains. While fine-tuning or few-shot learning can be used to\nadapt a base model, there is no single recipe for making these techniques work;\nmoreover, one may not have access to the original model weights if it is\ndeployed as a black box. We study how to improve a black box model's\nperformance on a new domain by leveraging explanations of the model's behavior.\nOur approach first extracts a set of features combining human intuition about\nthe task with model attributions generated by black box interpretation\ntechniques, then uses a simple calibrator, in the form of a classifier, to\npredict whether the base model was correct or not. We experiment with our\nmethod on two tasks, extractive question answering and natural language\ninference, covering adaptation from several pairs of domains with limited\ntarget-domain data. The experimental results across all the domain pairs show\nthat explanations are useful for calibrating these models, boosting accuracy\nwhen predictions do not have to be returned on every example. We further show\nthat the calibration model transfers to some extent between tasks.", "published": "2021-10-14 17:48:16", "link": "http://arxiv.org/abs/2110.07586v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally\n  Across Scales and Tasks", "abstract": "Prompt tuning, which only tunes continuous prompts with a frozen language\nmodel, substantially reduces per-task storage and memory usage at training.\nHowever, in the context of NLU, prior work reveals that prompt tuning does not\nperform well for normal-sized pretrained models. We also find that existing\nmethods of prompt tuning cannot handle hard sequence labeling tasks, indicating\na lack of universality. We present a novel empirical finding that properly\noptimized prompt tuning can be universally effective across a wide range of\nmodel scales and NLU tasks. It matches the performance of finetuning while\nhaving only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an\nimplementation of Deep Prompt Tuning \\cite{li2021prefix,qin2021learning}\noptimized and adapted for NLU. Given the universality and simplicity of\nP-Tuning v2, we believe it can serve as an alternative to finetuning and a\nstrong baseline for future research.Our code and data are released at\nhttps://github.com/THUDM/P-tuning-v2.", "published": "2021-10-14 17:58:47", "link": "http://arxiv.org/abs/2110.07602v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Scale Substitution-based Word Sense Induction", "abstract": "We present a word-sense induction method based on pre-trained masked language\nmodels (MLMs), which can cheaply scale to large vocabularies and large corpora.\nThe result is a corpus which is sense-tagged according to a corpus-derived\nsense inventory and where each sense is associated with indicative words.\nEvaluation on English Wikipedia that was sense-tagged using our method shows\nthat both the induced senses, and the per-instance sense assignment, are of\nhigh quality even compared to WSD methods, such as Babelfy. Furthermore, by\ntraining a static word embeddings algorithm on the sense-tagged corpus, we\nobtain high-quality static senseful embeddings. These outperform existing\nsenseful embeddings methods on the WiC dataset and on a new outlier detection\ndataset we developed. The data driven nature of the algorithm allows to induce\ncorpora-specific senses, which may not appear in standard sense inventories, as\nwe demonstrate using a case study on the scientific domain.", "published": "2021-10-14 19:40:37", "link": "http://arxiv.org/abs/2110.07681v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is Stance Detection Topic-Independent and Cross-topic Generalizable? --\n  A Reproduction Study", "abstract": "Cross-topic stance detection is the task to automatically detect stances\n(pro, against, or neutral) on unseen topics. We successfully reproduce\nstate-of-the-art cross-topic stance detection work (Reimers et. al., 2019), and\nsystematically analyze its reproducibility. Our attention then turns to the\ncross-topic aspect of this work, and the specificity of topics in terms of\nvocabulary and socio-cultural context. We ask: To what extent is stance\ndetection topic-independent and generalizable across topics? We compare the\nmodel's performance on various unseen topics, and find topic (e.g. abortion,\ncloning), class (e.g. pro, con), and their interaction affecting the model's\nperformance. We conclude that investigating performance on different topics,\nand addressing topic-specific vocabulary and context, is a future avenue for\ncross-topic stance detection.", "published": "2021-10-14 20:03:36", "link": "http://arxiv.org/abs/2110.07693v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying and Mitigating Spurious Correlations for Improving\n  Robustness in NLP Models", "abstract": "Recently, NLP models have achieved remarkable progress across a variety of\ntasks; however, they have also been criticized for being not robust. Many\nrobustness problems can be attributed to models exploiting spurious\ncorrelations, or shortcuts between the training data and the task labels. Most\nexisting work identifies a limited set of task-specific shortcuts via human\npriors or error analyses, which requires extensive expertise and efforts. In\nthis paper, we aim to automatically identify such spurious correlations in NLP\nmodels at scale. We first leverage existing interpretability methods to extract\ntokens that significantly affect model's decision process from the input text.\nWe then distinguish \"genuine\" tokens and \"spurious\" tokens by analyzing model\npredictions across multiple corpora and further verify them through\nknowledge-aware perturbations. We show that our proposed method can effectively\nand efficiently identify a scalable set of \"shortcuts\", and mitigating these\nleads to more robust models in multiple applications.", "published": "2021-10-14 21:40:03", "link": "http://arxiv.org/abs/2110.07736v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A CLIP-Enhanced Method for Video-Language Understanding", "abstract": "This technical report summarizes our method for the Video-And-Language\nUnderstanding Evaluation (VALUE) challenge\n(https://value-benchmark.github.io/challenge\\_2021.html). We propose a\nCLIP-Enhanced method to incorporate the image-text pretrained knowledge into\ndownstream video-text tasks. Combined with several other improved designs, our\nmethod outperforms the state-of-the-art by $2.4\\%$ ($57.58$ to $60.00$)\nMeta-Ave score on VALUE benchmark.", "published": "2021-10-14 03:50:23", "link": "http://arxiv.org/abs/2110.07137v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Neural Attention-Aware Hierarchical Topic Model", "abstract": "Neural topic models (NTMs) apply deep neural networks to topic modelling.\nDespite their success, NTMs generally ignore two important aspects: (1) only\ndocument-level word count information is utilized for the training, while more\nfine-grained sentence-level information is ignored, and (2) external semantic\nknowledge regarding documents, sentences and words are not exploited for the\ntraining. To address these issues, we propose a variational autoencoder (VAE)\nNTM model that jointly reconstructs the sentence and document word counts using\ncombinations of bag-of-words (BoW) topical embeddings and pre-trained semantic\nembeddings. The pre-trained embeddings are first transformed into a common\nlatent topical space to align their semantics with the BoW embeddings. Our\nmodel also features hierarchical KL divergence to leverage embeddings of each\ndocument to regularize those of their sentences, thereby paying more attention\nto semantically relevant sentences. Both quantitative and qualitative\nexperiments have shown the efficacy of our model in 1) lowering the\nreconstruction errors at both the sentence and document levels, and 2)\ndiscovering more coherent topics from real-world datasets.", "published": "2021-10-14 05:42:32", "link": "http://arxiv.org/abs/2110.07161v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantically Distributed Robust Optimization for Vision-and-Language\n  Inference", "abstract": "Analysis of vision-and-language models has revealed their brittleness under\nlinguistic phenomena such as paraphrasing, negation, textual entailment, and\nword substitutions with synonyms or antonyms. While data augmentation\ntechniques have been designed to mitigate against these failure modes, methods\nthat can integrate this knowledge into the training pipeline remain\nunder-explored. In this paper, we present \\textbf{SDRO}, a model-agnostic\nmethod that utilizes a set linguistic transformations in a distributed robust\noptimization setting, along with an ensembling technique to leverage these\ntransformations during inference. Experiments on benchmark datasets with images\n(NLVR$^2$) and video (VIOLIN) demonstrate performance improvements as well as\nrobustness to adversarial attacks. Experiments on binary VQA explore the\ngeneralizability of this method to other V\\&L tasks.", "published": "2021-10-14 06:02:46", "link": "http://arxiv.org/abs/2110.07165v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Context-gloss Augmentation for Improving Word Sense Disambiguation", "abstract": "The goal of Word Sense Disambiguation (WSD) is to identify the sense of a\npolysemous word in a specific context. Deep-learning techniques using BERT have\nachieved very promising results in the field and different methods have been\nproposed to integrate structured knowledge to enhance performance. At the same\ntime, an increasing number of data augmentation techniques have been proven to\nbe useful for NLP tasks. Building upon previous works leveraging BERT and\nWordNet knowledge, we explore different data augmentation techniques on\ncontext-gloss pairs to improve the performance of WSD. In our experiment, we\nshow that both sentence-level and word-level augmentation methods are effective\nstrategies for WSD. Also, we find out that performance can be improved by\nadding hypernyms' glosses obtained from a lexical knowledge base. We compare\nand analyze different context-gloss augmentation techniques, and the results\nshow that applying back translation on gloss performs the best.", "published": "2021-10-14 06:27:19", "link": "http://arxiv.org/abs/2110.07174v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Dual-Attention Neural Network for Pun Location and Using Pun-Gloss\n  Pairs for Interpretation", "abstract": "Pun location is to identify the punning word (usually a word or a phrase that\nmakes the text ambiguous) in a given short text, and pun interpretation is to\nfind out two different meanings of the punning word. Most previous studies\nadopt limited word senses obtained by WSD(Word Sense Disambiguation) technique\nor pronunciation information in isolation to address pun location. For the task\nof pun interpretation, related work pays attention to various WSD algorithms.\nIn this paper, a model called DANN (Dual-Attentive Neural Network) is proposed\nfor pun location, effectively integrates word senses and pronunciation with\ncontext information to address two kinds of pun at the same time. Furthermore,\nwe treat pun interpretation as a classification task and construct pungloss\npairs as processing data to solve this task. Experiments on the two benchmark\ndatasets show that our proposed methods achieve new state-of-the-art results.\nOur source code is available in the public code repository.", "published": "2021-10-14 08:15:04", "link": "http://arxiv.org/abs/2110.07209v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Building Chinese Biomedical Language Models via Multi-Level Text\n  Discrimination", "abstract": "Pre-trained language models (PLMs), such as BERT and GPT, have revolutionized\nthe field of NLP, not only in the general domain but also in the biomedical\ndomain. Most prior efforts in building biomedical PLMs have resorted simply to\ndomain adaptation and focused mainly on English. In this work we introduce\neHealth, a Chinese biomedical PLM built from scratch with a new pre-training\nframework. This new framework pre-trains eHealth as a discriminator through\nboth token- and sequence-level discrimination. The former is to detect input\ntokens corrupted by a generator and recover their original identities from\nplausible candidates, while the latter is to further distinguish corruptions of\na same original sequence from those of others. As such, eHealth can learn\nlanguage semantics at both token and sequence levels. Extensive experiments on\n11 Chinese biomedical language understanding tasks of various forms verify the\neffectiveness and superiority of our approach. We release the pre-trained model\nat \\url{https://github.com/PaddlePaddle/Research/tree/master/KG/eHealth} and\nwill also release the code later.", "published": "2021-10-14 10:43:28", "link": "http://arxiv.org/abs/2110.07244v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Plug-Tagger: A Pluggable Sequence Labeling Framework Using Language\n  Models", "abstract": "Plug-and-play functionality allows deep learning models to adapt well to\ndifferent tasks without requiring any parameters modified. Recently,\nprefix-tuning was shown to be a plug-and-play method on various text generation\ntasks by simply inserting corresponding continuous vectors into the inputs.\nHowever, sequence labeling tasks invalidate existing plug-and-play methods\nsince different label sets demand changes to the architecture of the model\nclassifier. In this work, we propose the use of label word prediction instead\nof classification to totally reuse the architecture of pre-trained models for\nsequence labeling tasks. Specifically, for each task, a label word set is first\nconstructed by selecting a high-frequency word for each class respectively, and\nthen, task-specific vectors are inserted into the inputs and optimized to\nmanipulate the model predictions towards the corresponding label words. As a\nresult, by simply switching the plugin vectors on the input, a frozen\npre-trained language model is allowed to perform different tasks. Experimental\nresults on three sequence labeling tasks show that the performance of the\nproposed method can achieve comparable performance with standard fine-tuning\nwith only 0.1\\% task-specific parameters. In addition, our method is up to 70\ntimes faster than non-plug-and-play methods while switching different tasks\nunder the resource-constrained scenario.", "published": "2021-10-14 13:05:06", "link": "http://arxiv.org/abs/2110.07331v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Neglected Sibling: Isotropic Gaussian Posterior for VAE", "abstract": "Deep generative models have been widely used in several areas of NLP, and\nvarious techniques have been proposed to augment them or address their training\nchallenges. In this paper, we propose a simple modification to Variational\nAutoencoders (VAEs) by using an Isotropic Gaussian Posterior (IGP) that allows\nfor better utilisation of their latent representation space. This model avoids\nthe sub-optimal behavior of VAEs related to inactive dimensions in the\nrepresentation space. We provide both theoretical analysis, and empirical\nevidence on various datasets and tasks that show IGP leads to consistent\nimprovement on several quantitative and qualitative grounds, from downstream\ntask performance and sample efficiency to robustness. Additionally, we give\ninsights about the representational properties encouraged by IGP and also show\nthat its gain generalises to image domain as well.", "published": "2021-10-14 14:12:52", "link": "http://arxiv.org/abs/2110.07383v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Few-shot Controllable Style Transfer for Low-Resource Multilingual\n  Settings", "abstract": "Style transfer is the task of rewriting a sentence into a target style while\napproximately preserving content. While most prior literature assumes access to\na large style-labelled corpus, recent work (Riley et al. 2021) has attempted\n\"few-shot\" style transfer using only 3-10 sentences at inference for style\nextraction. In this work we study a relevant low-resource setting: style\ntransfer for languages where no style-labelled corpora are available. We notice\nthat existing few-shot methods perform this task poorly, often copying inputs\nverbatim. We push the state-of-the-art for few-shot style transfer with a new\nmethod modeling the stylistic difference between paraphrases. When compared to\nprior work, our model achieves 2-3x better performance in formality transfer\nand code-mixing addition across seven languages. Moreover, our method is better\nat controlling the style transfer magnitude using an input scalar knob. We\nreport promising qualitative results for several attribute transfer tasks\n(sentiment transfer, simplification, gender neutralization, text anonymization)\nall without retraining the model. Finally, we find model evaluation to be\ndifficult due to the lack of datasets and metrics for many languages. To\nfacilitate future research we crowdsource formality annotations for 4000\nsentence pairs in four Indic languages, and use this data to design our\nautomatic evaluations.", "published": "2021-10-14 14:16:39", "link": "http://arxiv.org/abs/2110.07385v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Query and Extract: Refining Event Extraction as Type-oriented Binary\n  Decoding", "abstract": "Event extraction is typically modeled as a multi-class classification problem\nwhere event types and argument roles are treated as atomic symbols. These\napproaches are usually limited to a set of pre-defined types. We propose a\nnovel event extraction framework that uses event types and argument roles as\nnatural language queries to extract candidate triggers and arguments from the\ninput text. With the rich semantics in the queries, our framework benefits from\nthe attention mechanisms to better capture the semantic correlation between the\nevent types or argument roles and the input text. Furthermore, the\nquery-and-extract formulation allows our approach to leverage all available\nevent annotations from various ontologies as a unified model. Experiments on\nACE and ERE demonstrate that our approach achieves state-of-the-art performance\non each dataset and significantly outperforms existing methods on zero-shot\nevent extraction.", "published": "2021-10-14 15:49:40", "link": "http://arxiv.org/abs/2110.07476v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety\n  Failures", "abstract": "Current open-domain conversational models can easily be made to talk in\ninadequate ways. Online learning from conversational feedback given by the\nconversation partner is a promising avenue for a model to improve and adapt, so\nas to generate fewer of these safety failures. However, current\nstate-of-the-art models tend to react to feedback with defensive or oblivious\nresponses. This makes for an unpleasant experience and may discourage\nconversation partners from giving feedback in the future. This work proposes\nSaFeRDialogues, a task and dataset of graceful responses to conversational\nfeedback about safety failures. We collect a dataset of 10k dialogues\ndemonstrating safety failures, feedback signaling them, and a response\nacknowledging the feedback. We show how fine-tuning on this dataset results in\nconversations that human raters deem considerably more likely to lead to a\ncivil conversation, without sacrificing engagingness or general conversational\nability.", "published": "2021-10-14 16:41:25", "link": "http://arxiv.org/abs/2110.07518v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BI-RADS BERT & Using Section Segmentation to Understand Radiology\n  Reports", "abstract": "Radiology reports are one of the main forms of communication between\nradiologists and other clinicians and contain important information for patient\ncare. In order to use this information for research and automated patient care\nprograms, it is necessary to convert the raw text into structured data suitable\nfor analysis. State-of-the-art natural language processing (NLP)\ndomain-specific contextual word embeddings have been shown to achieve\nimpressive accuracy for these tasks in medicine, but have yet to be utilized\nfor section structure segmentation. In this work, we pre-trained a contextual\nembedding BERT model using breast radiology reports and developed a classifier\nthat incorporated the embedding with auxiliary global textual features in order\nto perform section segmentation. This model achieved a 98% accuracy at\nsegregating free text reports sentence by sentence into sections of information\noutlined in the Breast Imaging Reporting and Data System (BI-RADS) lexicon, a\nsignificant improvement over the Classic BERT model without auxiliary\ninformation. We then evaluated whether using section segmentation improved the\ndownstream extraction of clinically relevant information such as\nmodality/procedure, previous cancer, menopausal status, the purpose of the\nexam, breast density, and breast MRI background parenchymal enhancement. Using\nthe BERT model pre-trained on breast radiology reports combined with section\nsegmentation resulted in an overall accuracy of 95.9% in the field extraction\ntasks. This is a 17% improvement compared to an overall accuracy of 78.9% for\nfield extraction with models using Classic BERT embeddings and not using\nsection segmentation. Our work shows the strength of using BERT in radiology\nreport analysis and the advantages of section segmentation in identifying key\nfeatures of patient factors recorded in breast radiology reports.", "published": "2021-10-14 17:25:49", "link": "http://arxiv.org/abs/2110.07552v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in\n  Semantic Parsing", "abstract": "Semantic parsing is the task of producing a structured meaning representation\nfor natural language utterances or questions. Recent research has pointed out\nthat the commonly-used sequence-to-sequence (seq2seq) semantic parsers struggle\nto generalize systematically, i.e. to handle examples that require recombining\nknown knowledge in novel settings. In this work, we show that better systematic\ngeneralization can be achieved by producing the meaning representation (MR)\ndirectly as a graph and not as a sequence. To this end we propose LAGr, the\nLabeling Aligned Graphs algorithm that produces semantic parses by predicting\nnode and edge labels for a complete multi-layer input-aligned graph. The\nstrongly-supervised LAGr algorithm requires aligned graphs as inputs, whereas\nweakly-supervised LAGr infers alignments for originally unaligned target graphs\nusing an approximate MAP inference procedure. On the COGS and CFQ compositional\ngeneralization benchmarks the strongly- and weakly- supervised LAGr algorithms\nachieve significant improvements upon the baseline seq2seq parsers.", "published": "2021-10-14 17:37:04", "link": "http://arxiv.org/abs/2110.07572v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Retrieval-guided Counterfactual Generation for QA", "abstract": "Deep NLP models have been shown to learn spurious correlations, leaving them\nbrittle to input perturbations. Recent work has shown that counterfactual or\ncontrastive data -- i.e. minimally perturbed inputs -- can reveal these\nweaknesses, and that data augmentation using counterfactuals can help\nameliorate them. Proposed techniques for generating counterfactuals rely on\nhuman annotations, perturbations based on simple heuristics, and meaning\nrepresentation frameworks. We focus on the task of creating counterfactuals for\nquestion answering, which presents unique challenges related to world\nknowledge, semantic diversity, and answerability. To address these challenges,\nwe develop a Retrieve-Generate-Filter(RGF) technique to create counterfactual\nevaluation and training data with minimal human supervision. Using an\nopen-domain QA framework and question generation model trained on original task\ndata, we create counterfactuals that are fluent, semantically diverse, and\nautomatically labeled. Data augmentation with RGF counterfactuals improves\nperformance on out-of-domain and challenging evaluation sets over and above\nexisting methods, in both the reading comprehension and open-domain QA\nsettings. Moreover, we find that RGF data leads to significant improvements in\na model's robustness to local perturbations.", "published": "2021-10-14 17:56:37", "link": "http://arxiv.org/abs/2110.07596v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sub-word Level Lip Reading With Visual Attention", "abstract": "The goal of this paper is to learn strong lip reading models that can\nrecognise speech in silent videos. Most prior works deal with the open-set\nvisual speech recognition problem by adapting existing automatic speech\nrecognition techniques on top of trivially pooled visual features. Instead, in\nthis paper we focus on the unique challenges encountered in lip reading and\npropose tailored solutions. To this end, we make the following contributions:\n(1) we propose an attention-based pooling mechanism to aggregate visual speech\nrepresentations; (2) we use sub-word units for lip reading for the first time\nand show that this allows us to better model the ambiguities of the task; (3)\nwe propose a model for Visual Speech Detection (VSD), trained on top of the lip\nreading network. Following the above, we obtain state-of-the-art results on the\nchallenging LRS2 and LRS3 benchmarks when training on public datasets, and even\nsurpass models trained on large-scale industrial datasets by using an order of\nmagnitude less data. Our best model achieves 22.6% word error rate on the LRS2\ndataset, a performance unprecedented for lip reading models, significantly\nreducing the performance gap between lip reading and automatic speech\nrecognition. Moreover, on the AVA-ActiveSpeaker benchmark, our VSD model\nsurpasses all visual-only baselines and even outperforms several recent\naudio-visual methods.", "published": "2021-10-14 17:59:57", "link": "http://arxiv.org/abs/2110.07603v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Sparks: Inspiration for Science Writing using Language Models", "abstract": "Large-scale language models are rapidly improving, performing well on a wide\nvariety of tasks with little to no customization. In this work we investigate\nhow language models can support science writing, a challenging writing task\nthat is both open-ended and highly constrained. We present a system for\ngenerating \"sparks\", sentences related to a scientific concept intended to\ninspire writers. We find that our sparks are more coherent and diverse than a\ncompetitive language model baseline, and approach a human-created gold\nstandard. In a study with 13 PhD students writing on topics of their own\nselection, we find three main use cases of sparks: aiding with crafting\ndetailed sentences, providing interesting angles to engage readers, and\ndemonstrating common reader perspectives. We also report on the various reasons\nsparks were considered unhelpful, and discuss how we might improve language\nmodels as writing support tools.", "published": "2021-10-14 18:03:11", "link": "http://arxiv.org/abs/2110.07640v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "GlobalWoZ: Globalizing MultiWoZ to Develop Multilingual Task-Oriented\n  Dialogue Systems", "abstract": "Much recent progress in task-oriented dialogue (ToD) systems has been driven\nby available annotation data across multiple domains for training. Over the\nlast few years, there has been a move towards data curation for multilingual\nToD systems that are applicable to serve people speaking different languages.\nHowever, existing multilingual ToD datasets either have a limited coverage of\nlanguages due to the high cost of data curation, or ignore the fact that\ndialogue entities barely exist in countries speaking these languages. To tackle\nthese limitations, we introduce a novel data curation method that generates\nGlobalWoZ -- a large-scale multilingual ToD dataset globalized from an English\nToD dataset for three unexplored use cases. Our method is based on translating\ndialogue templates and filling them with local entities in the target-language\ncountries. We release our dataset as well as a set of strong baselines to\nencourage research on learning multilingual ToD systems for real use cases.", "published": "2021-10-14 19:33:04", "link": "http://arxiv.org/abs/2110.07679v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Making Document-Level Information Extraction Right for the Right Reasons", "abstract": "Document-level models for information extraction tasks like slot-filling are\nflexible: they can be applied to settings where information is not necessarily\nlocalized in a single sentence. For example, key features of a diagnosis in a\nradiology report may not be explicitly stated in one place, but nevertheless\ncan be inferred from parts of the report's text. However, these models can\neasily learn spurious correlations between labels and irrelevant information.\nThis work studies how to ensure that these models make correct inferences from\ncomplex text and make those inferences in an auditable way: beyond just being\nright, are these models \"right for the right reasons?\" We experiment with\npost-hoc evidence extraction in a predict-select-verify framework using feature\nattribution techniques. We show that regularization with small amounts of\nevidence supervision during training can substantially improve the quality of\nextracted evidence. We evaluate on two domains: a small-scale labeled dataset\nof brain MRI reports and a large-scale modified version of DocRED (Yao et al.,\n2019) and show that models' plausibility can be improved with no loss in\naccuracy.", "published": "2021-10-14 19:52:47", "link": "http://arxiv.org/abs/2110.07686v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CCQA: A New Web-Scale Question Answering Dataset for Model Pre-Training", "abstract": "With the rise of large-scale pre-trained language models, open-domain\nquestion-answering (ODQA) has become an important research topic in NLP. Based\non the popular pre-training fine-tuning approach, we posit that an additional\nin-domain pre-training stage using a large-scale, natural, and diverse\nquestion-answering (QA) dataset can be beneficial for ODQA. Consequently, we\npropose a novel QA dataset based on the Common Crawl project in this paper.\nUsing the readily available schema.org annotation, we extract around 130\nmillion multilingual question-answer pairs, including about 60 million English\ndata-points. With this previously unseen number of natural QA pairs, we\npre-train popular language models to show the potential of large-scale\nin-domain pre-training for the task of question-answering. In our experiments,\nwe find that pre-training question-answering models on our Common Crawl\nQuestion Answering dataset (CCQA) achieves promising results in zero-shot, low\nresource and fine-tuned settings across multiple tasks, models and benchmarks.", "published": "2021-10-14 21:23:01", "link": "http://arxiv.org/abs/2110.07731v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hindsight: Posterior-guided training of retrievers for improved\n  open-ended generation", "abstract": "Many text generation systems benefit from using a retriever to retrieve\npassages from a textual knowledge corpus (e.g., Wikipedia) which are then\nprovided as additional context to the generator. For open-ended generation\ntasks (like generating informative utterances in conversations) many varied\npassages may be equally relevant and we find that existing methods that jointly\ntrain the retriever and generator underperform: the retriever may not find\nrelevant passages even amongst the top-10 and hence the generator may not learn\na preference to ground its generated output in them. We propose using an\nadditional guide retriever that is allowed to use the target output and \"in\nhindsight\" retrieve relevant passages during training. We model the guide\nretriever after the posterior distribution Q of passages given the input and\nthe target output and train it jointly with the standard retriever and the\ngenerator by maximizing the evidence lower bound (ELBo) in expectation over Q.\nFor informative conversations from the Wizard of Wikipedia dataset, with\nposterior-guided training, the retriever finds passages with higher relevance\nin the top-10 (23% relative improvement), the generator's responses are more\ngrounded in the retrieved passage (19% relative improvement) and the end-to-end\nsystem produces better overall output (6.4% relative improvement).", "published": "2021-10-14 22:24:57", "link": "http://arxiv.org/abs/2110.07752v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text\n  Style Transfer", "abstract": "Adversarial attacks and backdoor attacks are two common security threats that\nhang over deep learning. Both of them harness task-irrelevant features of data\nin their implementation. Text style is a feature that is naturally irrelevant\nto most NLP tasks, and thus suitable for adversarial and backdoor attacks. In\nthis paper, we make the first attempt to conduct adversarial and backdoor\nattacks based on text style transfer, which is aimed at altering the style of a\nsentence while preserving its meaning. We design an adversarial attack method\nand a backdoor attack method, and conduct extensive experiments to evaluate\nthem. Experimental results show that popular NLP models are vulnerable to both\nadversarial and backdoor attacks based on text style transfer -- the attack\nsuccess rates can exceed 90% without much effort. It reflects the limited\nability of NLP models to handle the feature of text style that has not been\nwidely realized. In addition, the style transfer-based adversarial and backdoor\nattack methods show superiority to baselines in many aspects. All the code and\ndata of this paper can be obtained at https://github.com/thunlp/StyleAttack.", "published": "2021-10-14 03:54:16", "link": "http://arxiv.org/abs/2110.07139v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Revisiting IPA-based Cross-lingual Text-to-speech", "abstract": "International Phonetic Alphabet (IPA) has been widely used in cross-lingual\ntext-to-speech (TTS) to achieve cross-lingual voice cloning (CL VC). However,\nIPA itself has been understudied in cross-lingual TTS. In this paper, we report\nsome empirical findings of building a cross-lingual TTS model using IPA as\ninputs. Experiments show that the way to process the IPA and suprasegmental\nsequence has a negligible impact on the CL VC performance. Furthermore, we find\nthat using a dataset including one speaker per language to build an IPA-based\nTTS system would fail CL VC since the language-unique IPA and tone/stress\nsymbols could leak the speaker information. In addition, we experiment with\ndifferent combinations of speakers in the training dataset to further\ninvestigate the effect of the number of speakers on the CL VC performance.", "published": "2021-10-14 07:22:23", "link": "http://arxiv.org/abs/2110.07187v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language\n  Processing", "abstract": "Motivated by the success of T5 (Text-To-Text Transfer Transformer) in\npre-trained natural language processing models, we propose a unified-modal\nSpeechT5 framework that explores the encoder-decoder pre-training for\nself-supervised speech/text representation learning. The SpeechT5 framework\nconsists of a shared encoder-decoder network and six modal-specific\n(speech/text) pre/post-nets. After preprocessing the input speech/text through\nthe pre-nets, the shared encoder-decoder network models the\nsequence-to-sequence transformation, and then the post-nets generate the output\nin the speech/text modality based on the output of the decoder. Leveraging\nlarge-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a\nunified-modal representation, hoping to improve the modeling capability for\nboth speech and text. To align the textual and speech information into this\nunified semantic space, we propose a cross-modal vector quantization approach\nthat randomly mixes up speech/text states with latent units as the interface\nbetween encoder and decoder. Extensive evaluations show the superiority of the\nproposed SpeechT5 framework on a wide variety of spoken language processing\ntasks, including automatic speech recognition, speech synthesis, speech\ntranslation, voice conversion, speech enhancement, and speaker identification.\nWe release our code and model at https://github.com/microsoft/SpeechT5.", "published": "2021-10-14 07:59:27", "link": "http://arxiv.org/abs/2110.07205v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improve Cross-lingual Voice Cloning Using Low-quality Code-switched Data", "abstract": "Recently, sequence-to-sequence (seq-to-seq) models have been successfully\napplied in text-to-speech (TTS) to synthesize speech for single-language text.\nTo synthesize speech for multiple languages usually requires multi-lingual\nspeech from the target speaker. However, it is both laborious and expensive to\ncollect high-quality multi-lingual TTS data for the target speakers. In this\npaper, we proposed to use low-quality code-switched found data from the\nnon-target speakers to achieve cross-lingual voice cloning for the target\nspeakers. Experiments show that our proposed method can generate high-quality\ncode-switched speech in the target voices in terms of both naturalness and\nspeaker consistency. More importantly, we find that our method can achieve a\ncomparable result to the state-of-the-art (SOTA) performance in cross-lingual\nvoice cloning.", "published": "2021-10-14 08:16:06", "link": "http://arxiv.org/abs/2110.07210v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Approach to Mispronunciation Detection and Diagnosis with Acoustic,\n  Phonetic and Linguistic (APL) Embeddings", "abstract": "Many mispronunciation detection and diagnosis (MD&D) research approaches try\nto exploit both the acoustic and linguistic features as input. Yet the\nimprovement of the performance is limited, partially due to the shortage of\nlarge amount annotated training data at the phoneme level. Phonetic embeddings,\nextracted from ASR models trained with huge amount of word level annotations,\ncan serve as a good representation of the content of input speech, in a\nnoise-robust and speaker-independent manner. These embeddings, when used as\nimplicit phonetic supplementary information, can alleviate the data shortage of\nexplicit phoneme annotations. We propose to utilize Acoustic, Phonetic and\nLinguistic (APL) embedding features jointly for building a more powerful MD&D\nsystem. Experimental results obtained on the L2-ARCTIC database show the\nproposed approach outperforms the baseline by 9.93%, 10.13% and 6.17% on the\ndetection accuracy, diagnosis error rate and the F-measure, respectively.", "published": "2021-10-14 11:25:02", "link": "http://arxiv.org/abs/2110.07274v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Evaluating Off-the-Shelf Machine Listening and Natural Language Models\n  for Automated Audio Captioning", "abstract": "Automated audio captioning (AAC) is the task of automatically generating\ntextual descriptions for general audio signals. A captioning system has to\nidentify various information from the input signal and express it with natural\nlanguage. Existing works mainly focus on investigating new methods and try to\nimprove their performance measured on existing datasets. Having attracted\nattention only recently, very few works on AAC study the performance of\nexisting pre-trained audio and natural language processing resources. In this\npaper, we evaluate the performance of off-the-shelf models with a\nTransformer-based captioning approach. We utilize the freely available Clotho\ndataset to compare four different pre-trained machine listening models, four\nword embedding models, and their combinations in many different settings. Our\nevaluation suggests that YAMNet combined with BERT embeddings produces the best\ncaptions. Moreover, in general, fine-tuning pre-trained word embeddings can\nlead to better performance. Finally, we show that sequences of audio embeddings\ncan be processed using a Transformer encoder to produce higher-quality\ncaptions.", "published": "2021-10-14 14:42:38", "link": "http://arxiv.org/abs/2110.07410v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Automatic Modeling of Social Concepts Evoked by Art Images as Multimodal\n  Frames", "abstract": "Social concepts referring to non-physical objects--such as revolution,\nviolence, or friendship--are powerful tools to describe, index, and query the\ncontent of visual data, including ever-growing collections of art images from\nthe Cultural Heritage (CH) field. While much progress has been made towards\ncomplete image understanding in computer vision, automatic detection of social\nconcepts evoked by images is still a challenge. This is partly due to the\nwell-known semantic gap problem, worsened for social concepts given their lack\nof unique physical features, and reliance on more unspecific features than\nconcrete concepts. In this paper, we propose the translation of recent\ncognitive theories about social concept representation into a software approach\nto represent them as multimodal frames, by integrating multisensory data. Our\nmethod focuses on the extraction, analysis, and integration of multimodal\nfeatures from visual art material tagged with the concepts of interest. We\ndefine a conceptual model and present a novel ontology for formally\nrepresenting social concepts as multimodal frames. Taking the Tate Gallery's\ncollection as an empirical basis, we experiment our method on a corpus of art\nimages to provide a proof of concept of its potential. We discuss further\ndirections of research, and provide all software, data sources, and results.", "published": "2021-10-14 14:50:22", "link": "http://arxiv.org/abs/2110.07420v1", "categories": ["cs.CV", "cs.CL", "cs.DL", "cs.SI"], "primary_category": "cs.CV"}
{"title": "Practical Benefits of Feature Feedback Under Distribution Shift", "abstract": "In attempts to develop sample-efficient and interpretable algorithms,\nresearcher have explored myriad mechanisms for collecting and exploiting\nfeature feedback (or rationales) auxiliary annotations provided for training\n(but not test) instances that highlight salient evidence. Examples include\nbounding boxes around objects and salient spans in text. Despite its intuitive\nappeal, feature feedback has not delivered significant gains in practical\nproblems as assessed on iid holdout sets. However, recent works on\ncounterfactually augmented data suggest an alternative benefit of supplemental\nannotations, beyond interpretability: lessening sensitivity to spurious\npatterns and consequently delivering gains in out-of-domain evaluations. We\nspeculate that while existing methods for incorporating feature feedback have\ndelivered negligible in-sample performance gains, they may nevertheless provide\nout-of-domain benefits. Our experiments addressing sentiment analysis, show\nthat feature feedback methods perform significantly better on various natural\nout-of-domain datasets despite comparable in-domain evaluations. By contrast,\nperformance on natural language inference remains comparable. Finally, we\ncompare those tasks where feature feedback does (and does not) help.", "published": "2021-10-14 17:35:23", "link": "http://arxiv.org/abs/2110.07566v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Spoken ObjectNet: A Bias-Controlled Spoken Caption Dataset", "abstract": "Visually-grounded spoken language datasets can enable models to learn\ncross-modal correspondences with very weak supervision. However, modern\naudio-visual datasets contain biases that undermine the real-world performance\nof models trained on that data. We introduce Spoken ObjectNet, which is\ndesigned to remove some of these biases and provide a way to better evaluate\nhow effectively models will perform in real-world scenarios. This dataset\nexpands upon ObjectNet, which is a bias-controlled image dataset that features\nsimilar image classes to those present in ImageNet. We detail our data\ncollection pipeline, which features several methods to improve caption quality,\nincluding automated language model checks. Lastly, we show baseline results on\nimage retrieval and audio retrieval tasks. These results show that models\ntrained on other datasets and then evaluated on Spoken ObjectNet tend to\nperform poorly due to biases in other datasets that the models have learned. We\nalso show evidence that the performance decrease is due to the dataset\ncontrols, and not the transfer setting.", "published": "2021-10-14 17:38:20", "link": "http://arxiv.org/abs/2110.07575v1", "categories": ["cs.CL", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.CL"}
{"title": "UniPELT: A Unified Framework for Parameter-Efficient Language Model\n  Tuning", "abstract": "Recent parameter-efficient language model tuning (PELT) methods manage to\nmatch the performance of fine-tuning with much fewer trainable parameters and\nperform especially well when training data is limited. However, different PELT\nmethods may perform rather differently on the same task, making it nontrivial\nto select the most appropriate method for a specific task, especially\nconsidering the fast-growing number of new PELT methods and tasks. In light of\nmodel diversity and the difficulty of model selection, we propose a unified\nframework, UniPELT, which incorporates different PELT methods as submodules and\nlearns to activate the ones that best suit the current data or task setup via\ngating mechanism. On the GLUE benchmark, UniPELT consistently achieves 1~4%\ngains compared to the best individual PELT method that it incorporates and even\noutperforms fine-tuning under different setups. Moreover, UniPELT generally\nsurpasses the upper bound that takes the best performance of all its submodules\nused individually on each task, indicating that a mixture of multiple PELT\nmethods may be inherently more effective than single methods.", "published": "2021-10-14 17:40:08", "link": "http://arxiv.org/abs/2110.07577v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant\n  Representations", "abstract": "Dense retrieval (DR) methods conduct text retrieval by first encoding texts\nin the embedding space and then matching them by nearest neighbor search. This\nrequires strong locality properties from the representation space, i.e, the\nclose allocations of each small group of relevant texts, which are hard to\ngeneralize to domains without sufficient training data. In this paper, we aim\nto improve the generalization ability of DR models from source training domains\nwith rich supervision signals to target domains without any relevant labels, in\nthe zero-shot setting. To achieve that, we propose Momentum adversarial Domain\nInvariant Representation learning (MoDIR), which introduces a momentum method\nin the DR training process to train a domain classifier distinguishing source\nversus target, and then adversarially updates the DR encoder to learn domain\ninvariant representations. Our experiments show that MoDIR robustly outperforms\nits baselines on 10+ ranking datasets from the BEIR benchmark in the zero-shot\nsetup, with more than 10% relative gains on datasets with enough sensitivity\nfor DR models' evaluation. Source code of this paper will be released.", "published": "2021-10-14 17:45:06", "link": "http://arxiv.org/abs/2110.07581v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "DeToxy: A Large-Scale Multimodal Dataset for Toxicity Classification in\n  Spoken Utterances", "abstract": "Toxic speech, also known as hate speech, is regarded as one of the crucial\nissues plaguing online social media today. Most recent work on toxic speech\ndetection is constrained to the modality of text and written conversations with\nvery limited work on toxicity detection from spoken utterances or using the\nmodality of speech. In this paper, we introduce a new dataset DeToxy, the first\npublicly available toxicity annotated dataset for the English language. DeToxy\nis sourced from various openly available speech databases and consists of over\n2 million utterances. We believe that our dataset would act as a benchmark for\nthe relatively new and un-explored Spoken Language Processing task of detecting\ntoxicity from spoken utterances and boost further research in this space.\nFinally, we also provide strong unimodal baselines for our dataset and compare\ntraditional two-step and E2E approaches. Our experiments show that in the case\nof spoken utterances, text-based approaches are largely dependent on gold\nhuman-annotated transcripts for their performance and also suffer from the\nproblem of keyword bias. However, the presence of speech files in DeToxy helps\nfacilitates the development of E2E speech models which alleviate both the\nabove-stated problems by better capturing speech clues.", "published": "2021-10-14 17:51:04", "link": "http://arxiv.org/abs/2110.07592v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Compressibility of Distributed Document Representations", "abstract": "Contemporary natural language processing (NLP) revolves around learning from\nlatent document representations, generated either implicitly by neural language\nmodels or explicitly by methods such as doc2vec or similar. One of the key\nproperties of the obtained representations is their dimension. Whilst the\ncommonly adopted dimensions of 256 and 768 offer sufficient performance on many\ntasks, it is many times unclear whether the default dimension is the most\nsuitable choice for the subsequent downstream learning tasks. Furthermore,\nrepresentation dimensions are seldom subject to hyperparameter tuning due to\ncomputational constraints. The purpose of this paper is to demonstrate that a\nsurprisingly simple and efficient recursive compression procedure can be\nsufficient to both significantly compress the initial representation, but also\npotentially improve its performance when considering the task of text\nclassification. Having smaller and less noisy representations is the desired\nproperty during deployment, as orders of magnitude smaller models can\nsignificantly reduce the computational overload and with it the deployment\ncosts. We propose CoRe, a straightforward, representation learner-agnostic\nframework suitable for representation compression. The CoRe's performance is\nshowcased and studied on a collection of 17 real-life corpora from biomedical,\nnews, social media, and literary domains. We explored CoRe's behavior when\nconsidering contextual and non-contextual document representations, different\ncompression levels, and 9 different compression algorithms. Current results\nbased on more than 100,000 compression experiments indicate that recursive\nSingular Value Decomposition offers a very good trade-off between the\ncompression efficiency and performance, making CoRe useful in many existing,\nrepresentation-dependent NLP pipelines.", "published": "2021-10-14 17:56:35", "link": "http://arxiv.org/abs/2110.07595v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CORAA: a large corpus of spontaneous and prepared speech manually\n  validated for speech recognition in Brazilian Portuguese", "abstract": "Automatic Speech recognition (ASR) is a complex and challenging task. In\nrecent years, there have been significant advances in the area. In particular,\nfor the Brazilian Portuguese (BP) language, there were about 376 hours public\navailable for ASR task until the second half of 2020. With the release of new\ndatasets in early 2021, this number increased to 574 hours. The existing\nresources, however, are composed of audios containing only read and prepared\nspeech. There is a lack of datasets including spontaneous speech, which are\nessential in different ASR applications. This paper presents CORAA (Corpus of\nAnnotated Audios) v1. with 290.77 hours, a publicly available dataset for ASR\nin BP containing validated pairs (audio-transcription). CORAA also contains\nEuropean Portuguese audios (4.69 hours). We also present a public ASR model\nbased on Wav2Vec 2.0 XLSR-53 and fine-tuned over CORAA. Our model achieved a\nWord Error Rate of 24.18% on CORAA test set and 20.08% on Common Voice test\nset. When measuring the Character Error Rate, we obtained 11.02% and 6.34% for\nCORAA and Common Voice, respectively. CORAA corpora were assembled to both\nimprove ASR models in BP with phenomena from spontaneous speech and motivate\nyoung researchers to start their studies on ASR for Portuguese. All the corpora\nare publicly available at https://github.com/nilc-nlp/CORAA under the CC\nBY-NC-ND 4.0 license.", "published": "2021-10-14 13:50:52", "link": "http://arxiv.org/abs/2110.15731v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Learning Semantics: An Opportunity for Effective 6G Communications", "abstract": "Recently, semantic communications are envisioned as a key enabler of future\n6G networks. Back to Shannon's information theory, the goal of communication\nhas long been to guarantee the correct reception of transmitted messages\nirrespective of their meaning. However, in general, whenever communication\noccurs to convey a meaning, what matters is the receiver's understanding of the\ntransmitted message and not necessarily its correct reconstruction. Hence,\nsemantic communications introduce a new paradigm: transmitting only relevant\ninformation sufficient for the receiver to capture the meaning intended can\nsave significant communication bandwidth. Thus, this work explores the\nopportunity offered by semantic communications for beyond 5G networks. In\nparticular, we focus on the benefit of semantic compression. We refer to\nsemantic message as a sequence of well-formed symbols learned from the\n\"meaning\" underlying data, which have to be interpreted at the receiver. This\nrequires a reasoning unit, here artificial, on a knowledge base: a symbolic\nknowledge representation of the specific application. Therefore, we present and\ndetail a novel architecture that enables representation learning of semantic\nsymbols for effective semantic communications. We first discuss theoretical\naspects and successfully design objective functions, which help learn effective\nsemantic encoders and decoders. Eventually, we show promising numerical results\nfor the scenario of text transmission, especially when the sender and receiver\nspeak different languages.", "published": "2021-10-14 08:00:54", "link": "http://arxiv.org/abs/2110.08049v1", "categories": ["cs.IT", "cs.AI", "cs.CL", "cs.LO", "math.IT"], "primary_category": "cs.IT"}
{"title": "Auxiliary Loss of Transformer with Residual Connection for End-to-End\n  Speaker Diarization", "abstract": "End-to-end neural diarization (EEND) with self-attention directly predicts\nspeaker labels from inputs and enables the handling of overlapped speech.\nAlthough the EEND outperforms clustering-based speaker diarization (SD), it\ncannot be further improved by simply increasing the number of encoder blocks\nbecause the last encoder block is dominantly supervised compared with lower\nblocks. This paper proposes a new residual auxiliary EEND (RX-EEND) learning\narchitecture for transformers to enforce the lower encoder blocks to learn more\naccurately. The auxiliary loss is applied to the output of each encoder block,\nincluding the last encoder block. The effect of auxiliary loss on the learning\nof the encoder blocks can be further increased by adding a residual connection\nbetween the encoder blocks of the EEND. Performance evaluation and ablation\nstudy reveal that the auxiliary loss in the proposed RX-EEND provides relative\nreductions in the diarization error rate (DER) by 50.3% and 21.0% on the\nsimulated and CALLHOME (CH) datasets, respectively, compared with\nself-attentive EEND (SA-EEND). Furthermore, the residual connection used in\nRX-EEND further relatively reduces the DER by 8.1% for CH dataset.", "published": "2021-10-14 02:02:28", "link": "http://arxiv.org/abs/2110.07116v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-ACCDOA: Localizing and Detecting Overlapping Sounds from the Same\n  Class with Auxiliary Duplicating Permutation Invariant Training", "abstract": "Sound event localization and detection (SELD) involves identifying the\ndirection-of-arrival (DOA) and the event class. The SELD methods with a\nclass-wise output format make the model predict activities of all sound event\nclasses and corresponding locations. The class-wise methods can output\nactivity-coupled Cartesian DOA (ACCDOA) vectors, which enable us to solve a\nSELD task with a single target using a single network. However, there is still\na challenge in detecting the same event class from multiple locations. To\novercome this problem while maintaining the advantages of the class-wise\nformat, we extended ACCDOA to a multi one and proposed auxiliary duplicating\npermutation invariant training (ADPIT). The multi- ACCDOA format (a class- and\ntrack-wise output format) enables the model to solve the cases with overlaps\nfrom the same class. The class-wise ADPIT scheme enables each track of the\nmulti-ACCDOA format to learn with the same target as the single-ACCDOA format.\nIn evaluations with the DCASE 2021 Task 3 dataset, the model trained with the\nmulti-ACCDOA format and with the class-wise ADPIT detects overlapping events\nfrom the same class while maintaining its performance in the other cases. Also,\nthe proposed method performed comparably to state-of-the-art SELD methods with\nfewer parameters.", "published": "2021-10-14 02:35:50", "link": "http://arxiv.org/abs/2110.07124v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring Timbre Disentanglement in Non-Autoregressive Cross-Lingual\n  Text-to-Speech", "abstract": "In this paper, we study the disentanglement of speaker and language\nrepresentations in non-autoregressive cross-lingual TTS models from various\naspects. We propose a phoneme length regulator that solves the length mismatch\nproblem between IPA input sequence and monolingual alignment results. Using the\nphoneme length regulator, we present a FastPitch-based cross-lingual model with\nIPA symbols as input representations. Our experiments show that\nlanguage-independent input representations (e.g. IPA symbols), an increasing\nnumber of training speakers, and explicit modeling of speech variance\ninformation all encourage non-autoregressive cross-lingual TTS model to\ndisentangle speaker and language representations. The subjective evaluation\nshows that our proposed model can achieve decent naturalness and speaker\nsimilarity in cross-language voice cloning.", "published": "2021-10-14 07:37:04", "link": "http://arxiv.org/abs/2110.07192v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FedSpeech: Federated Text-to-Speech with Continual Learning", "abstract": "Federated learning enables collaborative training of machine learning models\nunder strict privacy restrictions and federated text-to-speech aims to\nsynthesize natural speech of multiple users with a few audio training samples\nstored in their devices locally. However, federated text-to-speech faces\nseveral challenges: very few training samples from each speaker are available,\ntraining samples are all stored in local device of each user, and global model\nis vulnerable to various attacks. In this paper, we propose a novel federated\nlearning architecture based on continual learning approaches to overcome the\ndifficulties above. Specifically, 1) we use gradual pruning masks to isolate\nparameters for preserving speakers' tones; 2) we apply selective masks for\neffectively reusing knowledge from tasks; 3) a private speaker embedding is\nintroduced to keep users' privacy. Experiments on a reduced VCTK dataset\ndemonstrate the effectiveness of FedSpeech: it nearly matches multi-task\ntraining in terms of multi-speaker speech quality; moreover, it sufficiently\nretains the speakers' tones and even outperforms the multi-task training in the\nspeaker similarity experiment.", "published": "2021-10-14 08:25:34", "link": "http://arxiv.org/abs/2110.07216v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "M2MeT: The ICASSP 2022 Multi-Channel Multi-Party Meeting Transcription\n  Challenge", "abstract": "Recent development of speech processing, such as speech recognition, speaker\ndiarization, etc., has inspired numerous applications of speech technologies.\nThe meeting scenario is one of the most valuable and, at the same time, most\nchallenging scenarios for the deployment of speech technologies. Specifically,\ntwo typical tasks, speaker diarization and multi-speaker automatic speech\nrecognition have attracted much attention recently. However, the lack of large\npublic meeting data has been a major obstacle for the advancement of the field.\nTherefore, we make available the AliMeeting corpus, which consists of 120 hours\nof recorded Mandarin meeting data, including far-field data collected by\n8-channel microphone array as well as near-field data collected by headset\nmicrophone. Each meeting session is composed of 2-4 speakers with different\nspeaker overlap ratio, recorded in rooms with different size. Along with the\ndataset, we launch the ICASSP 2022 Multi-channel Multi-party Meeting\nTranscription Challenge (M2MeT) with two tracks, namely speaker diarization and\nmulti-speaker ASR, aiming to provide a common testbed for meeting rich\ntranscription and promote reproducible research in this field. In this paper we\nprovide a detailed introduction of the AliMeeting dateset, challenge rules,\nevaluation methods and baseline systems.", "published": "2021-10-14 14:27:41", "link": "http://arxiv.org/abs/2110.07393v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Student-t Networks for Melody Estimation", "abstract": "Melody estimation or melody extraction refers to the extraction of the\nprimary or fundamental dominant frequency in a melody. This sequence of\nfrequencies obtained represents the pitch of the dominant melodic line from\nrecorded music audio signals. The music signal may be monophonic or polyphonic.\nThe melody extraction problem from audio signals gets complicated when we start\ndealing with polyphonic audio data. This is because in generalized audio\nsignals,the sounds are highly correlated over both frequency and time domains.\nThis complex overlap of many sounds, makes identification of predominant\nfrequency challenging.", "published": "2021-10-14 14:49:36", "link": "http://arxiv.org/abs/2110.07419v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SpecSinGAN: Sound Effect Variation Synthesis Using Single-Image GANs", "abstract": "Single-image generative adversarial networks learn from the internal\ndistribution of a single training example to generate variations of it,\nremoving the need of a large dataset. In this paper we introduce SpecSinGAN, an\nunconditional generative architecture that takes a single one-shot sound effect\n(e.g., a footstep; a character jump) and produces novel variations of it, as if\nthey were different takes from the same recording session. We explore the use\nof multi-channel spectrograms to train the model on the various layers that\ncomprise a single sound effect. A listening study comparing our model to real\nrecordings and to digital signal processing procedural audio models in terms of\nsound plausibility and variation revealed that SpecSinGAN is more plausible and\nvaried than the procedural audio models considered, when using multi-channel\nspectrograms. Sound examples can be found at the project website:\nhttps://www.adrianbarahonarios.com/specsingan/", "published": "2021-10-14 12:25:52", "link": "http://arxiv.org/abs/2110.07311v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Conformer-Based Self-Supervised Learning for Non-Speech Audio Tasks", "abstract": "Representation learning from unlabeled data has been of major interest in\nartificial intelligence research. While self-supervised speech representation\nlearning has been popular in the speech research community, very few works have\ncomprehensively analyzed audio representation learning for non-speech audio\ntasks. In this paper, we propose a self-supervised audio representation\nlearning method and apply it to a variety of downstream non-speech audio tasks.\nWe combine the well-known wav2vec 2.0 framework, which has shown success in\nself-supervised learning for speech tasks, with parameter-efficient conformer\narchitectures. Our self-supervised pre-training can reduce the need for labeled\ndata by two-thirds. On the AudioSet benchmark, we achieve a mean average\nprecision (mAP) score of 0.415, which is a new state-of-the-art on this dataset\nthrough audio-only self-supervised learning. Our fine-tuned conformers also\nsurpass or match the performance of previous systems pre-trained in a\nsupervised way on several downstream tasks. We further discuss the important\ndesign considerations for both pre-training and fine-tuning.", "published": "2021-10-14 12:32:40", "link": "http://arxiv.org/abs/2110.07313v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SingGAN: Generative Adversarial Network For High-Fidelity Singing Voice\n  Generation", "abstract": "Deep generative models have achieved significant progress in speech synthesis\nto date, while high-fidelity singing voice synthesis is still an open problem\nfor its long continuous pronunciation, rich high-frequency parts, and strong\nexpressiveness. Existing neural vocoders designed for text-to-speech cannot\ndirectly be applied to singing voice synthesis because they result in glitches\nand poor high-frequency reconstruction. In this work, we propose SingGAN, a\ngenerative adversarial network designed for high-fidelity singing voice\nsynthesis. Specifically, 1) to alleviate the glitch problem in the generated\nsamples, we propose source excitation with the adaptive feature learning\nfilters to expand the receptive field patterns and stabilize long continuous\nsignal generation; and 2) SingGAN introduces global and local discriminators at\ndifferent scales to enrich low-frequency details and promote high-frequency\nreconstruction; and 3) To improve the training efficiency, SingGAN includes\nauxiliary spectrogram losses and sub-band feature matching penalty loss. To the\nbest of our knowledge, SingGAN is the first work designed toward high-fidelity\nsinging voice vocoding. Our evaluation of SingGAN demonstrates the\nstate-of-the-art results with higher-quality (MOS 4.05) samples. Also, SingGAN\nenables a sample speed of 50x faster than real-time on a single NVIDIA 2080Ti\nGPU. We further show that SingGAN generalizes well to the mel-spectrogram\ninversion of unseen singers, and the end-to-end singing voice synthesis system\nSingGAN-SVS enjoys a two-stage pipeline to transform the music scores into\nexpressive singing voices. Audio samples are available at\n\\url{https://SingGAN.github.io/}", "published": "2021-10-14 15:41:09", "link": "http://arxiv.org/abs/2110.07468v4", "categories": ["eess.AS", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Toward Degradation-Robust Voice Conversion", "abstract": "Any-to-any voice conversion technologies convert the vocal timbre of an\nutterance to any speaker even unseen during training. Although there have been\nseveral state-of-the-art any-to-any voice conversion models, they were all\nbased on clean utterances to convert successfully. However, in real-world\nscenarios, it is difficult to collect clean utterances of a speaker, and they\nare usually degraded by noises or reverberations. It thus becomes highly\ndesired to understand how these degradations affect voice conversion and build\na degradation-robust model. We report in this paper the first comprehensive\nstudy on the degradation robustness of any-to-any voice conversion. We show\nthat the performance of state-of-the-art models nowadays was severely hampered\ngiven degraded utterances. To this end, we then propose speech enhancement\nconcatenation and denoising training to improve the robustness. In addition to\ncommon degradations, we also consider adversarial noises, which alter the model\noutput significantly yet are human-imperceptible. It was shown that both\nconcatenations with off-the-shelf speech enhancement models and denoising\ntraining on voice conversion models could improve the robustness, while each of\nthem had pros and cons.", "published": "2021-10-14 17:00:34", "link": "http://arxiv.org/abs/2110.07537v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "HumBugDB: A Large-scale Acoustic Mosquito Dataset", "abstract": "This paper presents the first large-scale multi-species dataset of acoustic\nrecordings of mosquitoes tracked continuously in free flight. We present 20\nhours of audio recordings that we have expertly labelled and tagged precisely\nin time. Significantly, 18 hours of recordings contain annotations from 36\ndifferent species. Mosquitoes are well-known carriers of diseases such as\nmalaria, dengue and yellow fever. Collecting this dataset is motivated by the\nneed to assist applications which utilise mosquito acoustics to conduct surveys\nto help predict outbreaks and inform intervention policy. The task of detecting\nmosquitoes from the sound of their wingbeats is challenging due to the\ndifficulty in collecting recordings from realistic scenarios. To address this,\nas part of the HumBug project, we conducted global experiments to record\nmosquitoes ranging from those bred in culture cages to mosquitoes captured in\nthe wild. Consequently, the audio recordings vary in signal-to-noise ratio and\ncontain a broad range of indoor and outdoor background environments from\nTanzania, Thailand, Kenya, the USA and the UK. In this paper we describe in\ndetail how we collected, labelled and curated the data. The data is provided\nfrom a PostgreSQL database, which contains important metadata such as the\ncapture method, age, feeding status and gender of the mosquitoes. Additionally,\nwe provide code to extract features and train Bayesian convolutional neural\nnetworks for two key tasks: the identification of mosquitoes from their\ncorresponding background environments, and the classification of detected\nmosquitoes into species. Our extensive dataset is both challenging to machine\nlearning researchers focusing on acoustic identification, and critical to\nentomologists, geo-spatial modellers and other domain experts to understand\nmosquito behaviour, model their distribution, and manage the threat they pose\nto humans.", "published": "2021-10-14 14:18:17", "link": "http://arxiv.org/abs/2110.07607v1", "categories": ["cs.SD", "cs.CV", "eess.AS", "E.0; I.2.1; J.3"], "primary_category": "cs.SD"}
{"title": "Attention-Free Keyword Spotting", "abstract": "Till now, attention-based models have been used with great success in the\nkeyword spotting problem domain. However, in light of recent advances in deep\nlearning, the question arises whether self-attention is truly irreplaceable for\nrecognizing speech keywords. We thus explore the usage of gated MLPs\n--previously shown to be alternatives to transformers in vision tasks-- for the\nkeyword spotting task. We provide a family of highly efficient MLP-based models\nfor keyword spotting, with less than 0.5 million parameters. We show that our\napproach achieves competitive performance on Google Speech Commands V2-12 and\nV2-35 benchmarks with much fewer parameters than self-attention-based methods.", "published": "2021-10-14 22:23:53", "link": "http://arxiv.org/abs/2110.07749v3", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
