{"title": "Do LLMs Work on Charts? Designing Few-Shot Prompts for Chart Question\n  Answering and Summarization", "abstract": "A number of tasks have been proposed recently to facilitate easy access to\ncharts such as chart QA and summarization. The dominant paradigm to solve these\ntasks has been to fine-tune a pretrained model on the task data. However, this\napproach is not only expensive but also not generalizable to unseen tasks. On\nthe other hand, large language models (LLMs) have shown impressive\ngeneralization capabilities to unseen tasks with zero- or few-shot prompting.\nHowever, their application to chart-related tasks is not trivial as these tasks\ntypically involve considering not only the underlying data but also the visual\nfeatures in the chart image. We propose PromptChart, a multimodal few-shot\nprompting framework with LLMs for chart-related applications. By analyzing the\ntasks carefully, we have come up with a set of prompting guidelines for each\ntask to elicit the best few-shot performance from LLMs. We further propose a\nstrategy to inject visual information into the prompts. Our experiments on\nthree different chart-related information consumption tasks show that with\nproperly designed prompts LLMs can excel on the benchmarks, achieving\nstate-of-the-art.", "published": "2023-12-17 05:13:58", "link": "http://arxiv.org/abs/2312.10610v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FedMKGC: Privacy-Preserving Federated Multilingual Knowledge Graph\n  Completion", "abstract": "Knowledge graph completion (KGC) aims to predict missing facts in knowledge\ngraphs (KGs), which is crucial as modern KGs remain largely incomplete. While\ntraining KGC models on multiple aligned KGs can improve performance, previous\nmethods that rely on transferring raw data among KGs raise privacy concerns. To\naddress this challenge, we propose a new federated learning framework that\nimplicitly aggregates knowledge from multiple KGs without demanding raw data\nexchange and entity alignment. We treat each KG as a client that trains a local\nlanguage model through textbased knowledge representation learning. A central\nserver then aggregates the model weights from clients. As natural language\nprovides a universal representation, the same knowledge thus has similar\nsemantic representations across KGs. As such, the aggregated language model can\nleverage complementary knowledge from multilingual KGs without demanding raw\nuser data sharing. Extensive experiments on a benchmark dataset demonstrate\nthat our method substantially improves KGC on multilingual KGs, achieving\ncomparable performance to state-of-the-art alignment-based models without\nrequiring any labeled alignments or raw user data sharing. Our codes will be\npublicly available.", "published": "2023-12-17 08:09:27", "link": "http://arxiv.org/abs/2312.10645v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bengali Intent Classification with Generative Adversarial BERT", "abstract": "Intent classification is a fundamental task in natural language\nunderstanding, aiming to categorize user queries or sentences into predefined\nclasses to understand user intent. The most challenging aspect of this\nparticular task lies in effectively incorporating all possible classes of\nintent into a dataset while ensuring adequate linguistic variation. Plenty of\nresearch has been conducted in the related domains in rich-resource languages\nlike English. In this study, we introduce BNIntent30, a comprehensive Bengali\nintent classification dataset containing 30 intent classes. The dataset is\nexcerpted and translated from the CLINIC150 dataset containing a diverse range\nof user intents categorized over 150 classes. Furthermore, we propose a novel\napproach for Bengali intent classification using Generative Adversarial BERT to\nevaluate the proposed dataset, which we call GAN-BnBERT. Our approach leverages\nthe power of BERT-based contextual embeddings to capture salient linguistic\nfeatures and contextual information from the text data, while the generative\nadversarial network (GAN) component complements the model's ability to learn\ndiverse representations of existing intent classes through generative modeling.\nOur experimental results demonstrate that the GAN-BnBERT model achieves\nsuperior performance on the newly introduced BNIntent30 dataset, surpassing the\nexisting Bi-LSTM and the stand-alone BERT-based classification model.", "published": "2023-12-17 10:45:50", "link": "http://arxiv.org/abs/2312.10679v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distinguishing Translations by Human, NMT, and ChatGPT: A Linguistic and\n  Statistical Approach", "abstract": "The growing popularity of neural machine translation (NMT) and LLMs\nrepresented by ChatGPT underscores the need for a deeper understanding of their\ndistinct characteristics and relationships. Such understanding is crucial for\nlanguage professionals and researchers to make informed decisions and tactful\nuse of these cutting-edge translation technology, but remains underexplored.\nThis study aims to fill this gap by investigating three key questions: (1) the\ndistinguishability of ChatGPT-generated translations from NMT and human\ntranslation (HT), (2) the linguistic characteristics of each translation type,\nand (3) the degree of resemblance between ChatGPT-produced translations and HT\nor NMT. To achieve these objectives, we employ statistical testing, machine\nlearning algorithms, and multidimensional analysis (MDA) to analyze\nSpokesperson's Remarks and their translations. After extracting a wide range of\nlinguistic features, supervised classifiers demonstrate high accuracy in\ndistinguishing the three translation types, whereas unsupervised clustering\ntechniques do not yield satisfactory results. Another major finding is that\nChatGPT-produced translations exhibit greater similarity with NMT than HT in\nmost MDA dimensions, which is further corroborated by distance computing and\nvisualization. These novel insights shed light on the interrelationships among\nthe three translation types and have implications for the future advancements\nof NMT and generative AI.", "published": "2023-12-17 15:56:05", "link": "http://arxiv.org/abs/2312.10750v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "kNN-ICL: Compositional Task-Oriented Parsing Generalization with Nearest\n  Neighbor In-Context Learning", "abstract": "Task-Oriented Parsing (TOP) enables conversational assistants to interpret\nuser commands expressed in natural language, transforming them into structured\noutputs that combine elements of both natural language and intent/slot tags.\nRecently, Large Language Models (LLMs) have achieved impressive performance in\nsynthesizing computer programs based on a natural language prompt, mitigating\nthe gap between natural language and structured programs. Our paper focuses on\nharnessing the capabilities of LLMs for semantic parsing tasks, addressing the\nfollowing three key research questions: 1) How can LLMs be effectively utilized\nfor semantic parsing tasks? 2) What defines an effective prompt? and 3) How can\nLLM overcome the length constraint and streamline prompt design by including\nall examples as prompts? We introduce k Nearest Neighbor In-Context\nLearning(kNN-ICL), which simplifies prompt engineering by allowing it to be\nbuilt on top of any design strategy while providing access to all demo\nexamples. Extensive experiments show that: 1)Simple ICL without kNN search can\nachieve a comparable performance with strong supervised models on the TOP\ntasks, and 2) kNN-ICL significantly improves the comprehension of complex\nrequests by seamlessly integrating ICL with a nearest-neighbor approach.\nNotably, this enhancement is achieved without the need for additional data or\nspecialized prompts.", "published": "2023-12-17 17:26:50", "link": "http://arxiv.org/abs/2312.10771v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DER-GCN: Dialogue and Event Relation-Aware Graph Convolutional Neural\n  Network for Multimodal Dialogue Emotion Recognition", "abstract": "With the continuous development of deep learning (DL), the task of multimodal\ndialogue emotion recognition (MDER) has recently received extensive research\nattention, which is also an essential branch of DL. The MDER aims to identify\nthe emotional information contained in different modalities, e.g., text, video,\nand audio, in different dialogue scenes. However, existing research has focused\non modeling contextual semantic information and dialogue relations between\nspeakers while ignoring the impact of event relations on emotion. To tackle the\nabove issues, we propose a novel Dialogue and Event Relation-Aware Graph\nConvolutional Neural Network for Multimodal Emotion Recognition (DER-GCN)\nmethod. It models dialogue relations between speakers and captures latent event\nrelations information. Specifically, we construct a weighted multi-relationship\ngraph to simultaneously capture the dependencies between speakers and event\nrelations in a dialogue. Moreover, we also introduce a Self-Supervised Masked\nGraph Autoencoder (SMGAE) to improve the fusion representation ability of\nfeatures and structures. Next, we design a new Multiple Information Transformer\n(MIT) to capture the correlation between different relations, which can provide\na better fuse of the multivariate information between relations. Finally, we\npropose a loss optimization strategy based on contrastive learning to enhance\nthe representation learning ability of minority class features. We conduct\nextensive experiments on the IEMOCAP and MELD benchmark datasets, which verify\nthe effectiveness of the DER-GCN model. The results demonstrate that our model\nsignificantly improves both the average accuracy and the f1 value of emotion\nrecognition.", "published": "2023-12-17 01:49:40", "link": "http://arxiv.org/abs/2312.10579v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deep dive into language traits of AI-generated Abstracts", "abstract": "Generative language models, such as ChatGPT, have garnered attention for\ntheir ability to generate human-like writing in various fields, including\nacademic research. The rapid proliferation of generated texts has bolstered the\nneed for automatic identification to uphold transparency and trust in the\ninformation. However, these generated texts closely resemble human writing and\noften have subtle differences in the grammatical structure, tones, and\npatterns, which makes systematic scrutinization challenging. In this work, we\nattempt to detect the Abstracts generated by ChatGPT, which are much shorter in\nlength and bounded. We extract the texts semantic and lexical properties and\nobserve that traditional machine learning models can confidently detect these\nAbstracts.", "published": "2023-12-17 06:03:33", "link": "http://arxiv.org/abs/2312.10617v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HyperPIE: Hyperparameter Information Extraction from Scientific\n  Publications", "abstract": "Automatic extraction of information from publications is key to making\nscientific knowledge machine readable at a large scale. The extracted\ninformation can, for example, facilitate academic search, decision making, and\nknowledge graph construction. An important type of information not covered by\nexisting approaches is hyperparameters. In this paper, we formalize and tackle\nhyperparameter information extraction (HyperPIE) as an entity recognition and\nrelation extraction task. We create a labeled data set covering publications\nfrom a variety of computer science disciplines. Using this data set, we train\nand evaluate BERT-based fine-tuned models as well as five large language\nmodels: GPT-3.5, GALACTICA, Falcon, Vicuna, and WizardLM. For fine-tuned\nmodels, we develop a relation extraction approach that achieves an improvement\nof 29% F1 over a state-of-the-art baseline. For large language models, we\ndevelop an approach leveraging YAML output for structured data extraction,\nwhich achieves an average improvement of 5.5% F1 in entity recognition over\nusing JSON. With our best performing model we extract hyperparameter\ninformation from a large number of unannotated papers, and analyze patterns\nacross disciplines. All our data and source code is publicly available at\nhttps://github.com/IllDepence/hyperpie", "published": "2023-12-17 07:39:07", "link": "http://arxiv.org/abs/2312.10638v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Explorers at #SMM4H 2023: Enhancing BERT for Health Applications through\n  Knowledge and Model Fusion", "abstract": "An increasing number of individuals are willing to post states and opinions\nin social media, which has become a valuable data resource for studying human\nhealth. Furthermore, social media has been a crucial research point for\nhealthcare now. This paper outlines the methods in our participation in the\n#SMM4H 2023 Shared Tasks, including data preprocessing, continual pre-training\nand fine-tuned optimization strategies. Especially for the Named Entity\nRecognition (NER) task, we utilize the model architecture named W2NER that\neffectively enhances the model generalization ability. Our method achieved\nfirst place in the Task 3. This paper has been peer-reviewed and accepted for\npresentation at the #SMM4H 2023 Workshop.", "published": "2023-12-17 08:52:05", "link": "http://arxiv.org/abs/2312.10652v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Silkie: Preference Distillation for Large Visual Language Models", "abstract": "This paper explores preference distillation for large vision language models\n(LVLMs), improving their ability to generate helpful and faithful responses\nanchoring the visual context. We first build a vision-language feedback\n(VLFeedback) dataset utilizing AI annotation. Specifically, responses are\ngenerated by models sampled from 12 LVLMs, conditioned on multi-modal\ninstructions sourced from various datasets. We adopt GPT-4V to assess the\ngenerated outputs regarding helpfulness, visual faithfulness, and ethical\nconsiderations. Furthermore, the preference supervision is distilled into\nQwen-VL-Chat through the direct preference optimization (DPO) method. The\nresulting model Silkie, achieves 6.9% and 9.5% relative improvement on the MME\nbenchmark regarding the perception and cognition capabilities, respectively.\nSilkie also demonstrates reduced hallucination by setting a new\nstate-of-the-art score of 3.02 on the MMHal-Bench benchmark. Further analysis\nshows that DPO with our VLFeedback dataset mainly boosts the fine-grained\nperception and complex cognition abilities of LVLMs, leading to more\ncomprehensive improvements compared to human-annotated preference datasets.", "published": "2023-12-17 09:44:27", "link": "http://arxiv.org/abs/2312.10665v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Mixed Distillation Helps Smaller Language Model Better Reasoning", "abstract": "While large language models (LLMs) have demonstrated exceptional performance\nin recent natural language processing (NLP) tasks, their deployment poses\nsubstantial challenges due to high computational and memory demands in\nreal-world applications. Recent studies have focused on enhancing smaller\nmodels through knowledge distillation from LLMs, yielding promising results.\nHowever, these models often struggle to match the performance of LLMs,\nespecially in tasks that require reasoning. In this work, we introduce Mixed\nDistillation (MD) framework, which capitalizes on the strengths of Program of\nThought (PoT) and Chain of Thought (CoT) capabilities within LLMs, combining\nmultiple prompting techniques and distilling these capabilities into smaller\nmodels. Our experimental results show that MD significantly enhances the\nsingle-path and multi-path reasoning ability of smaller models in various\ntasks. In terms of accuracy and generality of reasoning tasks, the model\ngenerated by it exceeds the comprehensive performance of two individually\ndistilled models. Notably, LLaMA2-7B and CodeLlama-7B using MD achieved\nremarkable improvements of (84.5%) and (85.5%), respectively, outperforming\nGPT-3.5-Turbo by (2.5%) and (3.5%), on the SVAMP benchmark.", "published": "2023-12-17 14:28:28", "link": "http://arxiv.org/abs/2312.10730v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Label Classification of COVID-Tweets Using Large Language Models", "abstract": "Vaccination is important to minimize the risk and spread of various diseases.\nIn recent years, vaccination has been a key step in countering the COVID-19\npandemic. However, many people are skeptical about the use of vaccines for\nvarious reasons, including the politics involved, the potential side effects of\nvaccines, etc. The goal in this task is to build an effective multi-label\nclassifier to label a social media post (particularly, a tweet) according to\nthe specific concern(s) towards vaccines as expressed by the author of the\npost. We tried three different models-(a) Supervised BERT-large-uncased, (b)\nSupervised HateXplain model, and (c) Zero-Shot GPT-3.5 Turbo model. The\nSupervised BERT-large-uncased model performed best in our case. We achieved a\nmacro-F1 score of 0.66, a Jaccard similarity score of 0.66, and received the\nsixth rank among other submissions. Code is available\nat-https://github.com/anonmous1981/AISOME", "published": "2023-12-17 15:50:05", "link": "http://arxiv.org/abs/2312.10748v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Demystifying Instruction Mixing for Fine-tuning Large Language Models", "abstract": "Instruction tuning significantly enhances the performance of large language\nmodels (LLMs) across various tasks. However, the procedure to optimizing the\nmixing of instruction datasets for LLM fine-tuning is still poorly understood.\nThis study categorizes instructions into three primary types: NLP downstream\ntasks, coding, and general chat. We explore the effects of instruction tuning\non different combinations of datasets on LLM performance, and find that certain\ninstruction types are more advantageous for specific applications but can\nnegatively impact other areas. This work provides insights into instruction\nmixtures, laying the foundations for future research.", "published": "2023-12-17 18:44:26", "link": "http://arxiv.org/abs/2312.10793v3", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Deciphering Compatibility Relationships with Textual Descriptions via\n  Extraction and Explanation", "abstract": "Understanding and accurately explaining compatibility relationships between\nfashion items is a challenging problem in the burgeoning domain of AI-driven\noutfit recommendations. Present models, while making strides in this area,\nstill occasionally fall short, offering explanations that can be elementary and\nrepetitive. This work aims to address these shortcomings by introducing the\nPair Fashion Explanation (PFE) dataset, a unique resource that has been curated\nto illuminate these compatibility relationships. Furthermore, we propose an\ninnovative two-stage pipeline model that leverages this dataset. This\nfine-tuning allows the model to generate explanations that convey the\ncompatibility relationships between items. Our experiments showcase the model's\npotential in crafting descriptions that are knowledgeable, aligned with\nground-truth matching correlations, and that produce understandable and\ninformative descriptions, as assessed by both automatic metrics and human\nevaluation. Our code and data are released at\nhttps://github.com/wangyu-ustc/PairFashionExplanation", "published": "2023-12-17 05:45:49", "link": "http://arxiv.org/abs/2312.11554v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis and Text Analysis of the Public Discourse on Twitter\n  about COVID-19 and MPox", "abstract": "Mining and analysis of the big data of Twitter conversations have been of\nsignificant interest to the scientific community in the fields of healthcare,\nepidemiology, big data, data science, computer science, and their related\nareas, as can be seen from several works in the last few years that focused on\nsentiment analysis and other forms of text analysis of tweets related to Ebola,\nE-Coli, Dengue, Human Papillomavirus, Middle East Respiratory Syndrome,\nMeasles, Zika virus, H1N1, influenza like illness, swine flu, flu, Cholera,\nListeriosis, cancer, Liver Disease, Inflammatory Bowel Disease, kidney disease,\nlupus, Parkinsons, Diphtheria, and West Nile virus. The recent outbreaks of\nCOVID-19 and MPox have served as catalysts for Twitter usage related to seeking\nand sharing information, views, opinions, and sentiments involving both of\nthese viruses. None of the prior works in this field analyzed tweets focusing\non both COVID-19 and MPox simultaneously. To address this research gap, a total\nof 61,862 tweets that focused on MPox and COVID-19 simultaneously, posted\nbetween 7 May 2022 and 3 March 2023, were studied. The findings and\ncontributions of this study are manifold. First, the results of sentiment\nanalysis using the VADER approach show that nearly half the tweets had a\nnegative sentiment. It was followed by tweets that had a positive sentiment and\ntweets that had a neutral sentiment, respectively. Second, this paper presents\nthe top 50 hashtags used in these tweets. Third, it presents the top 100 most\nfrequently used words in these tweets after performing tokenization, removal of\nstopwords, and word frequency analysis. Finally, a comprehensive comparative\nstudy that compares the contributions of this paper with 49 prior works in this\nfield is presented to further uphold the relevance and novelty of this work.", "published": "2023-12-17 01:50:27", "link": "http://arxiv.org/abs/2312.10580v1", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Decoding Concerns: Multi-label Classification of Vaccine Sentiments in\n  Social Media", "abstract": "In the realm of public health, vaccination stands as the cornerstone for\nmitigating disease risks and controlling their proliferation. The recent\nCOVID-19 pandemic has highlighted how vaccines play a crucial role in keeping\nus safe. However the situation involves a mix of perspectives, with skepticism\ntowards vaccines prevailing for various reasons such as political dynamics,\napprehensions about side effects, and more. The paper addresses the challenge\nof comprehensively understanding and categorizing these diverse concerns\nexpressed in the context of vaccination. Our focus is on developing a robust\nmulti-label classifier capable of assigning specific concern labels to tweets\nbased on the articulated apprehensions towards vaccines. To achieve this, we\ndelve into the application of a diverse set of advanced natural language\nprocessing techniques and machine learning algorithms including transformer\nmodels like BERT, state of the art GPT 3.5, Classifier Chains & traditional\nmethods like SVM, Random Forest, Naive Bayes. We see that the cutting-edge\nlarge language model outperforms all other methods in this context.", "published": "2023-12-17 06:55:04", "link": "http://arxiv.org/abs/2312.10626v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Domain Robustness of Transformer-based Keyphrase Generation", "abstract": "Modern models for text generation show state-of-the-art results in many\nnatural language processing tasks. In this work, we explore the effectiveness\nof abstractive text summarization models for keyphrase selection. A list of\nkeyphrases is an important element of a text in databases and repositories of\nelectronic documents. In our experiments, abstractive text summarization models\nfine-tuned for keyphrase generation show quite high results for a target text\ncorpus. However, in most cases, the zero-shot performance on other corpora and\ndomains is significantly lower. We investigate cross-domain limitations of\nabstractive text summarization models for keyphrase generation. We present an\nevaluation of the fine-tuned BART models for the keyphrase selection task\nacross six benchmark corpora for keyphrase extraction including scientific\ntexts from two domains and news texts. We explore the role of transfer learning\nbetween different domains to improve the BART model performance on small text\ncorpora. Our experiments show that preliminary fine-tuning on out-of-domain\ncorpora can be effective under conditions of a limited number of samples.", "published": "2023-12-17 12:27:15", "link": "http://arxiv.org/abs/2312.10700v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; I.7.m; H.3.3"], "primary_category": "cs.CL"}
{"title": "StyleSinger: Style Transfer for Out-of-Domain Singing Voice Synthesis", "abstract": "Style transfer for out-of-domain (OOD) singing voice synthesis (SVS) focuses\non generating high-quality singing voices with unseen styles (such as timbre,\nemotion, pronunciation, and articulation skills) derived from reference singing\nvoice samples. However, the endeavor to model the intricate nuances of singing\nvoice styles is an arduous task, as singing voices possess a remarkable degree\nof expressiveness. Moreover, existing SVS methods encounter a decline in the\nquality of synthesized singing voices in OOD scenarios, as they rest upon the\nassumption that the target vocal attributes are discernible during the training\nphase. To overcome these challenges, we propose StyleSinger, the first singing\nvoice synthesis model for zero-shot style transfer of out-of-domain reference\nsinging voice samples. StyleSinger incorporates two critical approaches for\nenhanced effectiveness: 1) the Residual Style Adaptor (RSA) which employs a\nresidual quantization module to capture diverse style characteristics in\nsinging voices, and 2) the Uncertainty Modeling Layer Normalization (UMLN) to\nperturb the style attributes within the content representation during the\ntraining phase and thus improve the model generalization. Our extensive\nevaluations in zero-shot style transfer undeniably establish that StyleSinger\noutperforms baseline models in both audio quality and similarity to the\nreference singing voice samples. Access to singing voice samples can be found\nat https://aaronz345.github.io/StyleSingerDemo/.", "published": "2023-12-17 15:26:16", "link": "http://arxiv.org/abs/2312.10741v4", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Knowledge Trees: Gradient Boosting Decision Trees on Knowledge Neurons\n  as Probing Classifier", "abstract": "To understand how well a large language model captures certain semantic or\nsyntactic features, researchers typically apply probing classifiers. However,\nthe accuracy of these classifiers is critical for the correct interpretation of\nthe results. If a probing classifier exhibits low accuracy, this may be due\neither to the fact that the language model does not capture the property under\ninvestigation, or to shortcomings in the classifier itself, which is unable to\nadequately capture the characteristics encoded in the internal representations\nof the model. Consequently, for more effective diagnosis, it is necessary to\nuse the most accurate classifiers possible for a particular type of task.\nLogistic regression on the output representation of the transformer neural\nnetwork layer is most often used to probing the syntactic properties of the\nlanguage model.\n  We show that using gradient boosting decision trees at the Knowledge Neuron\nlayer, i.e., at the hidden layer of the feed-forward network of the transformer\nas a probing classifier for recognizing parts of a sentence is more\nadvantageous than using logistic regression on the output representations of\nthe transformer layer. This approach is also preferable to many other methods.\nThe gain in error rate, depending on the preset, ranges from 9-54%", "published": "2023-12-17 15:37:03", "link": "http://arxiv.org/abs/2312.10746v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Identification of Knowledge Neurons in Protein Language Models", "abstract": "Neural language models have become powerful tools for learning complex\nrepresentations of entities in natural language processing tasks. However,\ntheir interpretability remains a significant challenge, particularly in domains\nlike computational biology where trust in model predictions is crucial. In this\nwork, we aim to enhance the interpretability of protein language models,\nspecifically the state-of-the-art ESM model, by identifying and characterizing\nknowledge neurons - components that express understanding of key information.\nAfter fine-tuning the ESM model for the task of enzyme sequence classification,\nwe compare two knowledge neuron selection methods that preserve a subset of\nneurons from the original model. The two methods, activation-based and\nintegrated gradient-based selection, consistently outperform a random baseline.\nIn particular, these methods show that there is a high density of knowledge\nneurons in the key vector prediction networks of self-attention modules. Given\nthat key vectors specialize in understanding different features of input\nsequences, these knowledge neurons could capture knowledge of different enzyme\nsequence motifs. In the future, the types of knowledge captured by each neuron\ncould be characterized.", "published": "2023-12-17 17:23:43", "link": "http://arxiv.org/abs/2312.10770v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "q-bio.BM"], "primary_category": "cs.LG"}
{"title": "Towards Efficient Vision-Language Tuning: More Information Density, More\n  Generalizability", "abstract": "With the advancement of large pre-trained vision-language models, effectively\ntransferring the knowledge embedded within these foundational models to\ndownstream tasks has become a pivotal topic, particularly in data-scarce\nenvironments. Recently, parameter-efficient fine-tuning approaches, especially\nprompt tuning, have garnered considerable attention. To better understand the\nnature of prompt tuning, we propose the concept of ``Information Density'' (ID)\nto indicate whether a matrix strongly belongs to certain feature spaces rather\nthan being evenly distributed across various feature spaces. We suppose a\nhigher ID with strong bias across some feature spaces naturally leads to\nexcellent robustness and stability. Our research, inspired by the observation\nthat generalizability is closely linked to the information density of the\nprompt matrix, introduces the Dense Information Prompt (DIP). DIP aims to\nenhance information density to improve generalization. Furthermore, DIP\nsignificantly reduces the number of tunable parameters and the requisite\nstorage space, making it particularly advantageous in resource-constrained\nsettings. Comprehensive experiments substantiate the superiority of DIP.\nNotably, DIP surpasses the latest state-of-the-art methods by a substantial\nmargin with an exceptionally small parameter count. Across a range of tasks\nspanning 11 datasets, DIP improves the average downstream accuracy of classic\nprompt tuning by up to 5.76% using merely 0.5K parameters.", "published": "2023-12-17 20:42:43", "link": "http://arxiv.org/abs/2312.10813v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "SeGA: Preference-Aware Self-Contrastive Learning with Prompts for\n  Anomalous User Detection on Twitter", "abstract": "In the dynamic and rapidly evolving world of social media, detecting\nanomalous users has become a crucial task to address malicious activities such\nas misinformation and cyberbullying. As the increasing number of anomalous\nusers improves the ability to mimic normal users and evade detection, existing\nmethods only focusing on bot detection are ineffective in terms of capturing\nsubtle distinctions between users. To address these challenges, we proposed\nSeGA, preference-aware self-contrastive learning for anomalous user detection,\nwhich leverages heterogeneous entities and their relations in the Twittersphere\nto detect anomalous users with different malicious strategies. SeGA utilizes\nthe knowledge of large language models to summarize user preferences via posts.\nIn addition, integrating user preferences with prompts as pseudo-labels for\npreference-aware self-contrastive learning enables the model to learn\nmultifaceted aspects for describing the behaviors of users. Extensive\nexperiments on the proposed TwBNT benchmark demonstrate that SeGA significantly\noutperforms the state-of-the-art methods (+3.5\\% ~ 27.6\\%) and empirically\nvalidate the effectiveness of the model design and pre-training strategies. Our\ncode and data are publicly available at https://github.com/ying0409/SeGA.", "published": "2023-12-17 05:35:28", "link": "http://arxiv.org/abs/2312.11553v1", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "StarVector: Generating Scalable Vector Graphics Code from Images and\n  Text", "abstract": "Scalable Vector Graphics (SVGs) are vital for modern image rendering due to\ntheir scalability and versatility. Previous SVG generation methods have focused\non curve-based vectorization, lacking semantic understanding, often producing\nartifacts, and struggling with SVG primitives beyond path curves. To address\nthese issues, we introduce StarVector, a multimodal large language model for\nSVG generation. It performs image vectorization by understanding image\nsemantics and using SVG primitives for compact, precise outputs. Unlike\ntraditional methods, StarVector works directly in the SVG code space,\nleveraging visual understanding to apply accurate SVG primitives. To train\nStarVector, we create SVG-Stack, a diverse dataset of 2M samples that enables\ngeneralization across vectorization tasks and precise use of primitives like\nellipses, polygons, and text. We address challenges in SVG evaluation, showing\nthat pixel-based metrics like MSE fail to capture the unique qualities of\nvector graphics. We introduce SVG-Bench, a benchmark across 10 datasets, and 3\ntasks: Image-to-SVG, Text-to-SVG generation, and diagram generation. Using this\nsetup, StarVector achieves state-of-the-art performance, producing more compact\nand semantically rich SVGs.", "published": "2023-12-17 08:07:32", "link": "http://arxiv.org/abs/2312.11556v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Survey of Reasoning with Foundation Models", "abstract": "Reasoning, a crucial ability for complex problem-solving, plays a pivotal\nrole in various real-world settings such as negotiation, medical diagnosis, and\ncriminal investigation. It serves as a fundamental methodology in the field of\nArtificial General Intelligence (AGI). With the ongoing development of\nfoundation models, e.g., Large Language Models (LLMs), there is a growing\ninterest in exploring their abilities in reasoning tasks. In this paper, we\nintroduce seminal foundation models proposed or adaptable for reasoning,\nhighlighting the latest advancements in various reasoning tasks, methods, and\nbenchmarks. We then delve into the potential future directions behind the\nemergence of reasoning abilities within foundation models. We also discuss the\nrelevance of multimodal learning, autonomous agents, and super alignment in the\ncontext of reasoning. By discussing these future research directions, we hope\nto inspire researchers in their exploration of this field, stimulate further\nadvancements in reasoning with foundation models, and contribute to the\ndevelopment of AGI.", "published": "2023-12-17 15:16:13", "link": "http://arxiv.org/abs/2312.11562v5", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "A review-based study on different Text-to-Speech technologies", "abstract": "This research paper presents a comprehensive review-based study on various\nText-to-Speech (TTS) technologies. TTS technology is an important aspect of\nhuman-computer interaction, enabling machines to convert written text into\naudible speech. The paper examines the different TTS technologies available,\nincluding concatenative TTS, formant synthesis TTS, and statistical parametric\nTTS. The study focuses on comparing the advantages and limitations of these\ntechnologies in terms of their naturalness of voice, the level of complexity of\nthe system, and their suitability for different applications. In addition, the\npaper explores the latest advancements in TTS technology, including neural TTS\nand hybrid TTS. The findings of this research will provide valuable insights\nfor researchers, developers, and users who want to understand the different TTS\ntechnologies and their suitability for specific applications.", "published": "2023-12-17 20:07:23", "link": "http://arxiv.org/abs/2312.11563v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Investigating salient representations and label Variance in Dimensional\n  Speech Emotion Analysis", "abstract": "Representations derived from models such as BERT (Bidirectional Encoder\nRepresentations from Transformers) and HuBERT (Hidden units BERT), have helped\nto achieve state-of-the-art performance in dimensional speech emotion\nrecognition. Despite their large dimensionality, and even though these\nrepresentations are not tailored for emotion recognition tasks, they are\nfrequently used to train large speech emotion models with high memory and\ncomputational costs. In this work, we show that there exist lower-dimensional\nsubspaces within the these pre-trained representational spaces that offer a\nreduction in downstream model complexity without sacrificing performance on\nemotion estimation. In addition, we model label uncertainty in the form of\ngrader opinion variance, and demonstrate that such information can improve the\nmodels generalization capacity and robustness. Finally, we compare the\nrobustness of the emotion models against acoustic degradations and observed\nthat the reduced dimensional representations were able to retain the\nperformance similar to the full-dimensional representations without significant\nregression in dimensional emotion performance.", "published": "2023-12-17 04:54:41", "link": "http://arxiv.org/abs/2312.16180v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SD"}
{"title": "RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language\n  Models", "abstract": "This study presents RoleCraft-GLM, an innovative framework aimed at enhancing\npersonalized role-playing with Large Language Models (LLMs). RoleCraft-GLM\naddresses the key issue of lacking personalized interactions in conversational\nAI, and offers a solution with detailed and emotionally nuanced character\nportrayals. We contribute a unique conversational dataset that shifts from\nconventional celebrity-centric characters to diverse, non-celebrity personas,\nthus enhancing the realism and complexity of language modeling interactions.\nAdditionally, our approach includes meticulous character development, ensuring\ndialogues are both realistic and emotionally resonant. The effectiveness of\nRoleCraft-GLM is validated through various case studies, highlighting its\nversatility and skill in different scenarios. Our framework excels in\ngenerating dialogues that accurately reflect characters' personality traits and\nemotions, thereby boosting user engagement. In conclusion, RoleCraft-GLM marks\na significant leap in personalized AI interactions, and paves the way for more\nauthentic and immersive AI-assisted role-playing experiences by enabling more\nnuanced and emotionally rich dialogues", "published": "2023-12-17 17:57:50", "link": "http://arxiv.org/abs/2401.09432v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Meta-AF Echo Cancellation for Improved Keyword Spotting", "abstract": "Adaptive filters (AFs) are vital for enhancing the performance of downstream\ntasks, such as speech recognition, sound event detection, and keyword spotting.\nHowever, traditional AF design prioritizes isolated signal-level objectives,\noften overlooking downstream task performance. This can lead to suboptimal\nperformance. Recent research has leveraged meta-learning to automatically learn\nAF update rules from data, alleviating the need for manual tuning when using\nsimple signal-level objectives. This paper improves the Meta-AF framework by\nexpanding it to support end-to-end training for arbitrary downstream tasks. We\nfocus on classification tasks, where we introduce a novel training methodology\nthat harnesses self-supervision and classifier feedback. We evaluate our\napproach on the combined task of acoustic echo cancellation and keyword\nspotting. Our findings demonstrate consistent performance improvements with\nboth pre-trained and joint-trained keyword spotting models across synthetic and\nreal playback. Notably, these improvements come without requiring additional\ntuning, increased inference-time complexity, or reliance on oracle signal-level\ntraining data.", "published": "2023-12-17 04:45:31", "link": "http://arxiv.org/abs/2312.10605v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MM-TTS: Multi-modal Prompt based Style Transfer for Expressive\n  Text-to-Speech Synthesis", "abstract": "The style transfer task in Text-to-Speech refers to the process of\ntransferring style information into text content to generate corresponding\nspeech with a specific style. However, most existing style transfer approaches\nare either based on fixed emotional labels or reference speech clips, which\ncannot achieve flexible style transfer. Recently, some methods have adopted\ntext descriptions to guide style transfer. In this paper, we propose a more\nflexible multi-modal and style controllable TTS framework named MM-TTS. It can\nutilize any modality as the prompt in unified multi-modal prompt space,\nincluding reference speech, emotional facial images, and text descriptions, to\ncontrol the style of the generated speech in a system. The challenges of\nmodeling such a multi-modal style controllable TTS mainly lie in two\naspects:1)aligning the multi-modal information into a unified style space to\nenable the input of arbitrary modality as the style prompt in a single system,\nand 2)efficiently transferring the unified style representation into the given\ntext content, thereby empowering the ability to generate prompt style-related\nvoice. To address these problems, we propose an aligned multi-modal prompt\nencoder that embeds different modalities into a unified style space, supporting\nstyle transfer for different modalities. Additionally, we present a new\nadaptive style transfer method named Style Adaptive Convolutions to achieve a\nbetter style representation. Furthermore, we design a Rectified Flow based\nRefiner to solve the problem of over-smoothing Mel-spectrogram and generate\naudio of higher fidelity. Since there is no public dataset for multi-modal TTS,\nwe construct a dataset named MEAD-TTS, which is related to the field of\nexpressive talking head. Our experiments on the MEAD-TTS dataset and\nout-of-domain datasets demonstrate that MM-TTS can achieve satisfactory results\nbased on multi-modal prompts.", "published": "2023-12-17 11:13:08", "link": "http://arxiv.org/abs/2312.10687v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring Sound vs Vibration for Robust Fault Detection on Rotating\n  Machinery", "abstract": "Robust and real-time detection of faults on rotating machinery has become an\nultimate objective for predictive maintenance in various industries.\nVibration-based Deep Learning (DL) methodologies have become the de facto\nstandard for bearing fault detection as they can produce state-of-the-art\ndetection performances under certain conditions. Despite such particular focus\non the vibration signal, the utilization of sound, on the other hand, has been\nneglected whilst only a few studies have been proposed during the last two\ndecades, all of which were based on a conventional ML approach. One major\nreason is the lack of a benchmark dataset providing a large volume of both\nvibration and sound data over several working conditions for different machines\nand sensor locations. In this study, we address this need by presenting the new\nbenchmark Qatar University Dual-Machine Bearing Fault Benchmark dataset\n(QU-DMBF), which encapsulates sound and vibration data from two different\nmotors operating under 1080 working conditions overall. Then we draw the focus\non the major limitations and drawbacks of vibration-based fault detection due\nto numerous installation and operational conditions. Finally, we propose the\nfirst DL approach for sound-based fault detection and perform comparative\nevaluations between the sound and vibration over the QU-DMBF dataset. A wide\nrange of experimental results shows that the sound-based fault detection method\nis significantly more robust than its vibration-based counterpart, as it is\nentirely independent of the sensor location, cost-effective (requiring no\nsensor and sensor maintenance), and can achieve the same level of the best\ndetection performance by its vibration-based counterpart. With this study, the\nQU-DMBF dataset, the optimized source codes in PyTorch, and comparative\nevaluations are now publicly shared.", "published": "2023-12-17 15:27:32", "link": "http://arxiv.org/abs/2312.10742v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Attention-Driven Multichannel Speech Enhancement in Moving Sound Source\n  Scenarios", "abstract": "Current multichannel speech enhancement algorithms typically assume a\nstationary sound source, a common mismatch with reality that limits their\nperformance in real-world scenarios. This paper focuses on attention-driven\nspatial filtering techniques designed for dynamic settings. Specifically, we\nstudy the application of linear and nonlinear attention-based methods for\nestimating time-varying spatial covariance matrices used to design the filters.\nWe also investigate the direct estimation of spatial filters by attention-based\nmethods without explicitly estimating spatial statistics. The clean speech\nclips from WSJ0 are employed for simulating speech signals of moving speakers\nin a reverberant environment. The experimental dataset is built by mixing the\nsimulated speech signals with multichannel real noise from CHiME-3. Evaluation\nresults show that the attention-driven approaches are robust and consistently\noutperform conventional spatial filtering approaches in both static and dynamic\nsound environments.", "published": "2023-12-17 16:12:35", "link": "http://arxiv.org/abs/2312.10756v1", "categories": ["eess.AS", "cs.LG", "eess.SP"], "primary_category": "eess.AS"}
