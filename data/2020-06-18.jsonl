{"title": "Multi-branch Attentive Transformer", "abstract": "While the multi-branch architecture is one of the key ingredients to the\nsuccess of computer vision tasks, it has not been well investigated in natural\nlanguage processing, especially sequence learning tasks. In this work, we\npropose a simple yet effective variant of Transformer called multi-branch\nattentive Transformer (briefly, MAT), where the attention layer is the average\nof multiple branches and each branch is an independent multi-head attention\nlayer. We leverage two training techniques to regularize the training:\ndrop-branch, which randomly drops individual branches during training, and\nproximal initialization, which uses a pre-trained Transformer model to\ninitialize multiple branches. Experiments on machine translation, code\ngeneration and natural language understanding demonstrate that such a simple\nvariant of Transformer brings significant improvements. Our code is available\nat \\url{https://github.com/HA-Transformer}.", "published": "2020-06-18 04:24:28", "link": "http://arxiv.org/abs/2006.10270v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Octet: Online Catalog Taxonomy Enrichment with Self-Supervision", "abstract": "Taxonomies have found wide applications in various domains, especially online\nfor item categorization, browsing, and search. Despite the prevalent use of\nonline catalog taxonomies, most of them in practice are maintained by humans,\nwhich is labor-intensive and difficult to scale. While taxonomy construction\nfrom scratch is considerably studied in the literature, how to effectively\nenrich existing incomplete taxonomies remains an open yet important research\nquestion. Taxonomy enrichment not only requires the robustness to deal with\nemerging terms but also the consistency between existing taxonomy structure and\nnew term attachment. In this paper, we present a self-supervised end-to-end\nframework, Octet, for Online Catalog Taxonomy EnrichmenT. Octet leverages\nheterogeneous information unique to online catalog taxonomies such as user\nqueries, items, and their relations to the taxonomy nodes while requiring no\nother supervision than the existing taxonomies. We propose to distantly train a\nsequence labeling model for term extraction and employ graph neural networks\n(GNNs) to capture the taxonomy structure as well as the query-item-taxonomy\ninteractions for term attachment. Extensive experiments in different online\ndomains demonstrate the superiority of Octet over state-of-the-art methods via\nboth automatic and human evaluations. Notably, Octet enriches an online catalog\ntaxonomy in production to 2 times larger in the open-world evaluation.", "published": "2020-06-18 04:53:07", "link": "http://arxiv.org/abs/2006.10276v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine\n  Translation", "abstract": "Much recent effort has been invested in non-autoregressive neural machine\ntranslation, which appears to be an efficient alternative to state-of-the-art\nautoregressive machine translation on modern GPUs. In contrast to the latter,\nwhere generation is sequential, the former allows generation to be parallelized\nacross target token positions. Some of the latest non-autoregressive models\nhave achieved impressive translation quality-speed tradeoffs compared to\nautoregressive baselines. In this work, we reexamine this tradeoff and argue\nthat autoregressive baselines can be substantially sped up without loss in\naccuracy. Specifically, we study autoregressive models with encoders and\ndecoders of varied depths. Our extensive experiments show that given a\nsufficiently deep encoder, a single-layer autoregressive decoder can\nsubstantially outperform strong non-autoregressive models with comparable\ninference speed. We show that the speed disadvantage for autoregressive\nbaselines compared to non-autoregressive methods has been overestimated in\nthree aspects: suboptimal layer allocation, insufficient speed measurement, and\nlack of knowledge distillation. Our results establish a new protocol for future\nresearch toward fast, accurate machine translation. Our code is available at\nhttps://github.com/jungokasai/deep-shallow.", "published": "2020-06-18 09:06:49", "link": "http://arxiv.org/abs/2006.10369v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Pretrained Language Models Symbolic Reasoners Over Knowledge?", "abstract": "How can pretrained language models (PLMs) learn factual knowledge from the\ntraining set? We investigate the two most important mechanisms: reasoning and\nmemorization. Prior work has attempted to quantify the number of facts PLMs\nlearn, but we present, using synthetic data, the first study that investigates\nthe causal relation between facts present in training and facts learned by the\nPLM. For reasoning, we show that PLMs seem to learn to apply some symbolic\nreasoning rules correctly but struggle with others, including two-hop\nreasoning. Further analysis suggests that even the application of learned\nreasoning rules is flawed. For memorization, we identify schema conformity\n(facts systematically supported by other facts) and frequency as key factors\nfor its success.", "published": "2020-06-18 10:40:37", "link": "http://arxiv.org/abs/2006.10413v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AMALGUM -- A Free, Balanced, Multilayer English Web Corpus", "abstract": "We present a freely available, genre-balanced English web corpus totaling 4M\ntokens and featuring a large number of high-quality automatic annotation\nlayers, including dependency trees, non-named entity annotations, coreference\nresolution, and discourse trees in Rhetorical Structure Theory. By tapping open\nonline data sources the corpus is meant to offer a more sizable alternative to\nsmaller manually created annotated data sets, while avoiding pitfalls such as\nimbalanced or unknown composition, licensing problems, and low-quality natural\nlanguage processing. We harness knowledge from multiple annotation layers in\norder to achieve a \"better than NLP\" benchmark and evaluate the accuracy of the\nresulting resource.", "published": "2020-06-18 17:05:45", "link": "http://arxiv.org/abs/2006.10677v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "STEAM: Self-Supervised Taxonomy Expansion with Mini-Paths", "abstract": "Taxonomies are important knowledge ontologies that underpin numerous\napplications on a daily basis, but many taxonomies used in practice suffer from\nthe low coverage issue. We study the taxonomy expansion problem, which aims to\nexpand existing taxonomies with new concept terms. We propose a self-supervised\ntaxonomy expansion model named STEAM, which leverages natural supervision in\nthe existing taxonomy for expansion. To generate natural self-supervision\nsignals, STEAM samples mini-paths from the existing taxonomy, and formulates a\nnode attachment prediction task between anchor mini-paths and query terms. To\nsolve the node attachment task, it learns feature representations for\nquery-anchor pairs from multiple views and performs multi-view co-training for\nprediction. Extensive experiments show that STEAM outperforms state-of-the-art\nmethods for taxonomy expansion by 11.6\\% in accuracy and 7.0\\% in mean\nreciprocal rank on three public benchmarks. The implementation of STEAM can be\nfound at \\url{https://github.com/yueyu1030/STEAM}.", "published": "2020-06-18 00:32:53", "link": "http://arxiv.org/abs/2006.10217v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Extraction and Evaluation of Formulaic Expressions Used in Scholarly\n  Papers", "abstract": "Formulaic expressions, such as 'in this paper we propose', are helpful for\nauthors of scholarly papers because they convey communicative functions; in the\nabove, it is showing the aim of this paper'. Thus, resources of formulaic\nexpressions, such as a dictionary, that could be looked up easily would be\nuseful. However, forms of formulaic expressions can often vary to a great\nextent. For example, 'in this paper we propose', 'in this study we propose' and\n'in this paper we propose a new method to' are all regarded as formulaic\nexpressions. Such a diversity of spans and forms causes problems in both\nextraction and evaluation of formulaic expressions. In this paper, we propose a\nnew approach that is robust to variation of spans and forms of formulaic\nexpressions. Our approach regards a sentence as consisting of a formulaic part\nand non-formulaic part. Then, instead of trying to extract formulaic\nexpressions from a whole corpus, by extracting them from each sentence,\ndifferent forms can be dealt with at once. Based on this formulation, to avoid\nthe diversity problem, we propose evaluating extraction methods by how much\nthey convey specific communicative functions rather than by comparing extracted\nexpressions to an existing lexicon. We also propose a new extraction method\nthat utilises named entities and dependency structures to remove the\nnon-formulaic part from a sentence. Experimental results show that the proposed\nextraction method achieved the best performance compared to other existing\nmethods.", "published": "2020-06-18 07:42:45", "link": "http://arxiv.org/abs/2006.10334v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Compositional Generalization by Learning Analytical Expressions", "abstract": "Compositional generalization is a basic and essential intellective capability\nof human beings, which allows us to recombine known parts readily. However,\nexisting neural network based models have been proven to be extremely deficient\nin such a capability. Inspired by work in cognition which argues\ncompositionality can be captured by variable slots with symbolic functions, we\npresent a refreshing view that connects a memory-augmented neural model with\nanalytical expressions, to achieve compositional generalization. Our model\nconsists of two cooperative neural modules, Composer and Solver, fitting well\nwith the cognitive argument while being able to be trained in an end-to-end\nmanner via a hierarchical reinforcement learning algorithm. Experiments on the\nwell-known benchmark SCAN demonstrate that our model seizes a great ability of\ncompositional generalization, solving all challenges addressed by previous\nworks with 100% accuracies.", "published": "2020-06-18 15:50:57", "link": "http://arxiv.org/abs/2006.10627v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "SEAL: Segment-wise Extractive-Abstractive Long-form Text Summarization", "abstract": "Most prior work in the sequence-to-sequence paradigm focused on datasets with\ninput sequence lengths in the hundreds of tokens due to the computational\nconstraints of common RNN and Transformer architectures. In this paper, we\nstudy long-form abstractive text summarization, a sequence-to-sequence setting\nwith input sequence lengths up to 100,000 tokens and output sequence lengths up\nto 768 tokens. We propose SEAL, a Transformer-based model, featuring a new\nencoder-decoder attention that dynamically extracts/selects input snippets to\nsparsely attend to for each output segment. Using only the original documents\nand summaries, we derive proxy labels that provide weak supervision for\nextractive layers simultaneously with regular supervision from abstractive\nsummaries. The SEAL model achieves state-of-the-art results on existing\nlong-form summarization tasks, and outperforms strong baseline models on a new\ndataset/task we introduce, Search2Wiki, with much longer input text. Since\ncontent selection is explicit in the SEAL model, a desirable side effect is\nthat the selection can be inspected for enhanced interpretability.", "published": "2020-06-18 00:13:21", "link": "http://arxiv.org/abs/2006.10213v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Parameter Allocation Search", "abstract": "Training neural networks requires increasing amounts of memory. Parameter\nsharing can reduce memory and communication costs, but existing methods assume\nnetworks have many identical layers and utilize hand-crafted sharing strategies\nthat fail to generalize. We introduce Neural Parameter Allocation Search\n(NPAS), a novel task where the goal is to train a neural network given an\narbitrary, fixed parameter budget. NPAS covers both low-budget regimes, which\nproduce compact networks, as well as a novel high-budget regime, where\nadditional capacity can be added to boost performance without increasing\ninference FLOPs. To address NPAS, we introduce Shapeshifter Networks (SSNs),\nwhich automatically learn where and how to share parameters in a network to\nsupport any parameter budget without requiring any changes to the architecture\nor loss function. NPAS and SSNs provide a complete framework for addressing\ngeneralized parameter sharing, and can also be combined with prior work for\nadditional performance gains. We demonstrate the effectiveness of our approach\nusing nine network architectures across four diverse tasks, including ImageNet\nclassification and transformers.", "published": "2020-06-18 15:01:00", "link": "http://arxiv.org/abs/2006.10598v4", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Explainable and Discourse Topic-aware Neural Language Understanding", "abstract": "Marrying topic models and language models exposes language understanding to a\nbroader source of document-level context beyond sentences via topics. While\nintroducing topical semantics in language models, existing approaches\nincorporate latent document topic proportions and ignore topical discourse in\nsentences of the document. This work extends the line of research by\nadditionally introducing an explainable topic representation in language\nunderstanding, obtained from a set of key terms correspondingly for each latent\ntopic of the proportion. Moreover, we retain sentence-topic associations along\nwith document-topic association by modeling topical discourse for every\nsentence in the document. We present a novel neural composite language model\nthat exploits both the latent and explainable topics along with topical\ndiscourse at sentence-level in a joint learning framework of topic and language\nmodels. Experiments over a range of tasks such as language modeling, word sense\ndisambiguation, document classification, retrieval and text generation\ndemonstrate ability of the proposed model in improving language understanding.", "published": "2020-06-18 15:53:58", "link": "http://arxiv.org/abs/2006.10632v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Learning with Common Sense Knowledge Graphs", "abstract": "Zero-shot learning relies on semantic class representations such as\nhand-engineered attributes or learned embeddings to predict classes without any\nlabeled examples. We propose to learn class representations by embedding nodes\nfrom common sense knowledge graphs in a vector space. Common sense knowledge\ngraphs are an untapped source of explicit high-level knowledge that requires\nlittle human effort to apply to a range of tasks. To capture the knowledge in\nthe graph, we introduce ZSL-KG, a general-purpose framework with a novel\ntransformer graph convolutional network (TrGCN) for generating class\nrepresentations. Our proposed TrGCN architecture computes non-linear\ncombinations of node neighbourhoods. Our results show that ZSL-KG improves over\nexisting WordNet-based methods on five out of six zero-shot benchmark datasets\nin language and vision.", "published": "2020-06-18 17:46:17", "link": "http://arxiv.org/abs/2006.10713v4", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Learning to Format Coq Code Using Language Models", "abstract": "Should the final right bracket in a record declaration be on a separate line?\nShould arguments to the rewrite tactic be separated by a single space? Coq code\ntends to be written in distinct manners by different people and teams. The\nexpressiveness, flexibility, and extensibility of Coq's languages and notations\nmeans that Coq projects have a wide variety of recognizable coding styles,\nsometimes explicitly documented as conventions on naming and formatting. In\nparticular, even inexperienced users can distinguish vernacular using the\nstandard library and plain Ltac from idiomatic vernacular using the\nMathematical Components (MathComp) library and SSReflect.\n  While coding conventions are important for comprehension and maintenance,\nthey are costly to document and enforce. Rule-based formatters, such as Coq's\nbeautifier, have limited flexibility and only capture small fractions of\ndesired conventions in large verification projects. We believe that application\nof language models - a class of Natural Language Processing (NLP) techniques\nfor capturing regularities in corpora - can provide a solution to this\nconundrum. More specifically, we believe that an approach based on\nautomatically learning conventions from existing Coq code, and then suggesting\nidiomatic code to users in the proper context, can be superior to manual\napproaches and static analysis tools - both in terms of effort and results.\n  As a first step, we here outline initial models to learn and suggest space\nformatting in Coq files, with a preliminary implementation for Coq 8.10, and\nevaluated on a corpus based on MathComp 1.9.0 which comprises 164k lines of Coq\ncode from four core projects.", "published": "2020-06-18 14:46:15", "link": "http://arxiv.org/abs/2006.16743v1", "categories": ["cs.HC", "cs.CL", "cs.PL", "cs.SE"], "primary_category": "cs.HC"}
{"title": "Automatic Speech Recognition Benchmark for Air-Traffic Communications", "abstract": "Advances in Automatic Speech Recognition (ASR) over the last decade opened\nnew areas of speech-based automation such as in Air-Traffic Control (ATC)\nenvironment. Currently, voice communication and data links communications are\nthe only way of contact between pilots and Air-Traffic Controllers (ATCo),\nwhere the former is the most widely used and the latter is a non-spoken method\nmandatory for oceanic messages and limited for some domestic issues. ASR\nsystems on ATCo environments inherit increasing complexity due to accents from\nnon-English speakers, cockpit noise, speaker-dependent biases, and small\nin-domain ATC databases for training. Hereby, we introduce CleanSky EC-H2020\nATCO2, a project that aims to develop an ASR-based platform to collect,\norganize and automatically pre-process ATCo speech-data from air space. This\npaper conveys an exploratory benchmark of several state-of-the-art ASR models\ntrained on more than 170 hours of ATCo speech-data. We demonstrate that the\ncross-accent flaws due to speakers' accents are minimized due to the amount of\ndata, making the system feasible for ATC environments. The developed ASR system\nachieves an averaged word error rate (WER) of 7.75% across four databases. An\nadditional 35% relative improvement in WER is achieved on one test set when\ntraining a TDNNF system with byte-pair encoding.", "published": "2020-06-18 06:49:22", "link": "http://arxiv.org/abs/2006.10304v2", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Self-supervised Learning for Speech Enhancement", "abstract": "Supervised learning for single-channel speech enhancement requires carefully\nlabeled training examples where the noisy mixture is input into the network and\nthe network is trained to produce an output close to the ideal target. To relax\nthe conditions on the training data, we consider the task of training speech\nenhancement networks in a self-supervised manner. We first use a limited\ntraining set of clean speech sounds and learn a latent representation by\nautoencoding on their magnitude spectrograms. We then autoencode on speech\nmixtures recorded in noisy environments and train the resulting autoencoder to\nshare a latent representation with the clean examples. We show that using this\ntraining schema, we can now map noisy speech to its clean version using a\nnetwork that is autonomously trainable without requiring labeled training\nexamples or human intervention.", "published": "2020-06-18 09:47:20", "link": "http://arxiv.org/abs/2006.10388v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-and-Mixed Attention Decoder with Deep Acoustic Structure for\n  Transformer-based LVCSR", "abstract": "The Transformer has shown impressive performance in automatic speech\nrecognition. It uses the encoder-decoder structure with self-attention to learn\nthe relationship between the high-level representation of the source inputs and\nembedding of the target outputs. In this paper, we propose a novel decoder\nstructure that features a self-and-mixed attention decoder (SMAD) with a deep\nacoustic structure (DAS) to improve the acoustic representation of\nTransformer-based LVCSR. Specifically, we introduce a self-attention mechanism\nto learn a multi-layer deep acoustic structure for multiple levels of acoustic\nabstraction. We also design a mixed attention mechanism that learns the\nalignment between different levels of acoustic abstraction and its\ncorresponding linguistic information simultaneously in a shared embedding\nspace. The ASR experiments on Aishell-1 shown that the proposed structure\nachieves CERs of 4.8% on the dev set and 5.1% on the test set, which are the\nbest results obtained on this task to the best of our knowledge.", "published": "2020-06-18 10:24:18", "link": "http://arxiv.org/abs/2006.10407v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-Encoder-Decoder Transformer for Code-Switching Speech Recognition", "abstract": "Code-switching (CS) occurs when a speaker alternates words of two or more\nlanguages within a single sentence or across sentences. Automatic speech\nrecognition (ASR) of CS speech has to deal with two or more languages at the\nsame time. In this study, we propose a Transformer-based architecture with two\nsymmetric language-specific encoders to capture the individual language\nattributes, that improve the acoustic representation of each language. These\nrepresentations are combined using a language-specific multi-head attention\nmechanism in the decoder module. Each encoder and its corresponding attention\nmodule in the decoder are pre-trained using a large monolingual corpus aiming\nto alleviate the impact of limited CS training data. We call such a network a\nmulti-encoder-decoder (MED) architecture. Experiments on the SEAME corpus show\nthat the proposed MED architecture achieves 10.2% and 10.8% relative error rate\nreduction on the CS evaluation sets with Mandarin and English as the matrix\nlanguage respectively.", "published": "2020-06-18 10:42:52", "link": "http://arxiv.org/abs/2006.10414v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Boosting Objective Scores of a Speech Enhancement Model by MetricGAN\n  Post-processing", "abstract": "The Transformer architecture has demonstrated a superior ability compared to\nrecurrent neural networks in many different natural language processing\napplications. Therefore, our study applies a modified Transformer in a speech\nenhancement task. Specifically, positional encoding in the Transformer may not\nbe necessary for speech enhancement, and hence, it is replaced by convolutional\nlayers. To further improve the perceptual evaluation of the speech quality\n(PESQ) scores of enhanced speech, the L_1 pre-trained Transformer is fine-tuned\nusing a MetricGAN framework. The proposed MetricGAN can be treated as a general\npost-processing module to further boost the objective scores of interest. The\nexperiments were conducted using the data sets provided by the organizer of the\nDeep Noise Suppression (DNS) challenge. Experimental results demonstrated that\nthe proposed system outperformed the challenge baseline, in both subjective and\nobjective evaluations, with a large margin.", "published": "2020-06-18 06:22:09", "link": "http://arxiv.org/abs/2006.10296v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Adversarially Trained Multi-Singer Sequence-To-Sequence Singing\n  Synthesizer", "abstract": "This paper presents a high quality singing synthesizer that is able to model\na voice with limited available recordings. Based on the sequence-to-sequence\nsinging model, we design a multi-singer framework to leverage all the existing\nsinging data of different singers. To attenuate the issue of musical score\nunbalance among singers, we incorporate an adversarial task of singer\nclassification to make encoder output less singer dependent. Furthermore, we\napply multiple random window discriminators (MRWDs) on the generated acoustic\nfeatures to make the network be a GAN. Both objective and subjective\nevaluations indicate that the proposed synthesizer can generate higher quality\nsinging voice than baseline (4.12 vs 3.53 in MOS). Especially, the articulation\nof high-pitched vowels is significantly enhanced.", "published": "2020-06-18 07:20:11", "link": "http://arxiv.org/abs/2006.10317v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Dense and Convolutional Autoencoders for Unsupervised Anomaly\n  Detection in Machine Condition Sounds", "abstract": "This technical report describes two methods that were developed for Task 2 of\nthe DCASE 2020 challenge. The challenge involves an unsupervised learning to\ndetect anomalous sounds, thus only normal machine working condition samples are\navailable during the training process. The two methods involve deep\nautoencoders, based on dense and convolutional architectures that use\nmelspectogram processed sound features. Experiments were held, using the six\nmachine type datasets of the challenge. Overall, competitive results were\nachieved by the proposed dense and convolutional AE, outperforming the baseline\nchallenge method.", "published": "2020-06-18 10:49:49", "link": "http://arxiv.org/abs/2006.10417v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
