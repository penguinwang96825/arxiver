{"title": "Semantic descriptions of 24 evaluational adjectives, for application in\n  sentiment analysis", "abstract": "We apply the Natural Semantic Metalanguage (NSM) approach (Goddard and\nWierzbicka 2014) to the lexical-semantic analysis of English evaluational\nadjectives and compare the results with the picture developed in the Appraisal\nFramework (Martin and White 2005). The analysis is corpus-assisted, with\nexamples mainly drawn from film and book reviews, and supported by\ncollocational and statistical information from WordBanks Online. We propose NSM\nexplications for 24 evaluational adjectives, arguing that they fall into five\ngroups, each of which corresponds to a distinct semantic template. The groups\ncan be sketched as follows: \"First-person thought-plus-affect\", e.g. wonderful;\n\"Experiential\", e.g. entertaining; \"Experiential with bodily reaction\", e.g.\ngripping; \"Lasting impact\", e.g. memorable; \"Cognitive evaluation\", e.g.\ncomplex, excellent. These groupings and semantic templates are compared with\nthe classifications in the Appraisal Framework's system of Appreciation. In\naddition, we are particularly interested in sentiment analysis, the automatic\nidentification of evaluation and subjectivity in text. We discuss the relevance\nof the two frameworks for sentiment analysis and other language technology\napplications.", "published": "2016-08-24 03:36:04", "link": "http://arxiv.org/abs/1608.06697v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Large-Scale Multilingual Disambiguation of Glosses", "abstract": "Linking concepts and named entities to knowledge bases has become a crucial\nNatural Language Understanding task. In this respect, recent works have shown\nthe key advantage of exploiting textual definitions in various Natural Language\nProcessing applications. However, to date there are no reliable large-scale\ncorpora of sense-annotated textual definitions available to the research\ncommunity. In this paper we present a large-scale high-quality corpus of\ndisambiguated glosses in multiple languages, comprising sense annotations of\nboth concepts and named entities from a unified sense inventory. Our approach\nfor the construction and disambiguation of the corpus builds upon the structure\nof a large multilingual semantic network and a state-of-the-art disambiguation\nsystem; first, we gather complementary information of equivalent definitions\nacross different languages to provide context for disambiguation, and then we\ncombine it with a semantic similarity-based refinement. As a result we obtain a\nmultilingual corpus of textual definitions featuring over 38 million\ndefinitions in 263 languages, and we make it freely available at\nhttp://lcl.uniroma1.it/disambiguated-glosses. Experiments on Open Information\nExtraction and Sense Clustering show how two state-of-the-art approaches\nimprove their performance by integrating our disambiguated corpus into their\npipeline.", "published": "2016-08-24 05:30:45", "link": "http://arxiv.org/abs/1608.06718v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Named Entity Recognition in Idiosyncratic Domains", "abstract": "Named entity recognition often fails in idiosyncratic domains. That causes a\nproblem for depending tasks, such as entity linking and relation extraction. We\npropose a generic and robust approach for high-recall named entity recognition.\nOur approach is easy to train and offers strong generalization over diverse\ndomain-specific language, such as news documents (e.g. Reuters) or biomedical\ntext (e.g. Medline). Our approach is based on deep contextual sequence learning\nand utilizes stacked bidirectional LSTM networks. Our model is trained with\nonly few hundred labeled sentences and does not rely on further external\nknowledge. We report from our results F1 scores in the range of 84-94% on\nstandard datasets.", "published": "2016-08-24 09:06:14", "link": "http://arxiv.org/abs/1608.06757v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Sparse Word Representations with Distributional Inference for\n  Semantic Composition", "abstract": "Distributional models are derived from co-occurrences in a corpus, where only\na small proportion of all possible plausible co-occurrences will be observed.\nThis results in a very sparse vector space, requiring a mechanism for inferring\nmissing knowledge. Most methods face this challenge in ways that render the\nresulting word representations uninterpretable, with the consequence that\nsemantic composition becomes hard to model. In this paper we explore an\nalternative which involves explicitly inferring unobserved co-occurrences using\nthe distributional neighbourhood. We show that distributional inference\nimproves sparse word representations on several word similarity benchmarks and\ndemonstrate that our model is competitive with the state-of-the-art for\nadjective-noun, noun-noun and verb-object compositions while being fully\ninterpretable.", "published": "2016-08-24 12:38:45", "link": "http://arxiv.org/abs/1608.06794v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
