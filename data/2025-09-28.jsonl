{"title": "DocPruner: A Storage-Efficient Framework for Multi-Vector Visual Document Retrieval via Adaptive Patch-Level Embedding Pruning", "abstract": "Visual Document Retrieval (VDR), the task of retrieving visually-rich\ndocument pages using queries that combine visual and textual cues, is crucial\nfor numerous real-world applications. Recent state-of-the-art methods leverage\nLarge Vision-Language Models (LVLMs) in a multi-vector paradigm, representing\neach document as patch-level embeddings to capture fine-grained details. While\nhighly effective, this approach introduces a critical challenge: prohibitive\nstorage overhead, as storing hundreds of vectors per page makes large-scale\ndeployment costly and impractical. To address this, we introduce DocPruner, the\nfirst framework to employ adaptive patch-level embedding pruning for VDR to\neffectively reduce the storage overhead. DocPruner leverages the intra-document\npatch attention distribution to dynamically identify and discard redundant\nembeddings for each document. This adaptive mechanism enables a significant\n50-60% reduction in storage for leading multi-vector VDR models with negligible\ndegradation in document retrieval performance. Extensive experiments across\nmore than ten representative datasets validate that DocPruner offers a robust,\nflexible, and effective solution for building storage-efficient, large-scale\nVDR systems.", "published": "2025-09-28 13:47:24", "link": "http://arxiv.org/abs/2509.23883v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Multi-Value-Product Retrieval-Augmented Generation for Industrial Product Attribute Value Identification", "abstract": "Identifying attribute values from product profiles is a key task for\nimproving product search, recommendation, and business analytics on e-commerce\nplatforms, which we called Product Attribute Value Identification (PAVI) .\nHowever, existing PAVI methods face critical challenges, such as cascading\nerrors, inability to handle out-of-distribution (OOD) attribute values, and\nlack of generalization capability. To address these limitations, we introduce\nMulti-Value-Product Retrieval-Augmented Generation (MVP-RAG), combining the\nstrengths of retrieval, generation, and classification paradigms. MVP-RAG\ndefines PAVI as a retrieval-generation task, where the product title\ndescription serves as the query, and products and attribute values act as the\ncorpus. It first retrieves similar products of the same category and candidate\nattribute values, and then generates the standardized attribute values. The key\nadvantages of this work are: (1) the proposal of a multi-level retrieval\nscheme, with products and attribute values as distinct hierarchical levels in\nPAVI domain (2) attribute value generation of large language model to\nsignificantly alleviate the OOD problem and (3) its successful deployment in a\nreal-world industrial environment. Extensive experimental results demonstrate\nthat MVP-RAG performs better than the state-of-the-art baselines.", "published": "2025-09-28 13:29:20", "link": "http://arxiv.org/abs/2509.23874v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "Investigating Multi-layer Representations for Dense Passage Retrieval", "abstract": "Dense retrieval models usually adopt vectors from the last hidden layer of\nthe document encoder to represent a document, which is in contrast to the fact\nthat representations in different layers of a pre-trained language model\nusually contain different kinds of linguistic knowledge, and behave differently\nduring fine-tuning. Therefore, we propose to investigate utilizing\nrepresentations from multiple encoder layers to make up the representation of a\ndocument, which we denote Multi-layer Representations (MLR). We first\ninvestigate how representations in different layers affect MLR's performance\nunder the multi-vector retrieval setting, and then propose to leverage pooling\nstrategies to reduce multi-vector models to single-vector ones to improve\nretrieval efficiency. Experiments demonstrate the effectiveness of MLR over\ndual encoder, ME-BERT and ColBERT in the single-vector retrieval setting, as\nwell as demonstrate that it works well with other advanced training techniques\nsuch as retrieval-oriented pre-training and hard negative mining.", "published": "2025-09-28 13:00:53", "link": "http://arxiv.org/abs/2509.23861v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "GSID: Generative Semantic Indexing for E-Commerce Product Understanding", "abstract": "Structured representation of product information is a major bottleneck for\nthe efficiency of e-commerce platforms, especially in second-hand ecommerce\nplatforms. Currently, most product information are organized based on manually\ncurated product categories and attributes, which often fail to adequately cover\nlong-tail products and do not align well with buyer preference. To address\nthese problems, we propose \\textbf{G}enerative \\textbf{S}emantic\n\\textbf{I}n\\textbf{D}exings (GSID), a data-driven approach to generate product\nstructured representations. GSID consists of two key components: (1)\nPre-training on unstructured product metadata to learn in-domain semantic\nembeddings, and (2) Generating more effective semantic codes tailored for\ndownstream product-centric applications. Extensive experiments are conducted to\nvalidate the effectiveness of GSID, and it has been successfully deployed on\nthe real-world e-commerce platform, achieving promising results on product\nunderstanding and other downstream tasks.", "published": "2025-09-28 12:58:05", "link": "http://arxiv.org/abs/2509.23860v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "Semantic Representation of Processes with Ontology Design Patterns", "abstract": "The representation of workflows and processes is essential in materials\nscience engineering, where experimental and computational reproducibility\ndepend on structured and semantically coherent process models. Although\nnumerous ontologies have been developed for process modeling, they are often\ncomplex and challenging to reuse. Ontology Design Patterns (ODPs) offer modular\nand reusable modeling solutions to recurring problems; however, these patterns\nare frequently neither explicitly published nor documented in a manner\naccessible to domain experts. This study surveys ontologies relevant to\nscientific workflows and engineering process modeling and identifies implicit\ndesign patterns embedded within their structures. We evaluate the capacity of\nthese ontologies to fulfill key requirements for process representation in\nmaterials science. Furthermore, we propose a baseline method for the automatic\nextraction of design patterns from existing ontologies and assess the approach\nagainst curated ground truth patterns. All resources associated with this work,\nincluding the extracted patterns and the extraction workflow, are made openly\navailable in a public GitHub repository.", "published": "2025-09-28 09:42:01", "link": "http://arxiv.org/abs/2509.23776v1", "categories": ["cs.IR", "cs.IT", "math.IT"], "primary_category": "cs.IR"}
{"title": "Constructing Opera Seria in the Iberian Courts: Metastasian Repertoire for Spain and Portugal", "abstract": "The exceptional reception of Pietro Metastasio's works during the eighteenth\ncentury, all over Europe and in the Iberian Peninsula in particular, is well\ndocumented. Due to that unparalleled success, it is possible to ascertain Spain\nand Portugal's participation in international, contemporary tastes and artistic\nwebs, applicable to both composers and performers. However, this\ninternationalisation needs to be nuanced, as some characteristics of the\nrepertoire specifically written for the Peninsula indicate that their court\naudiences may have had expectations, both social and strictly musical,\ndifferent from those of the public in opera theatres elsewhere in the\ncontinent. In this light, this article investigates in what ways the style of\nfive composers in the international scene - Perez, Galuppi, Jommelli, Conforto,\nand Corselli - varied when commissioned to write opera seria for the Iberian\ncourts. The statistical analysis of fifteen settings especially written for the\ncourt theatres in Madrid and Lisbon, in comparison to the average data\nextracted from a corpus of 2,404 arias from 126 versions of a select number of\nMetastasian librettos, allows us to evaluate some particular usages regarding\nkey, metre, tempo, and treatment of the vocal part. In this manner, through\nquantitative analysis, this article places eighteenth-century Iberian music\nproduction and consumption in the context of European opera seria, while\nultimately suggesting that its unique musical characteristics were also partly\ndependent on local musical customs, gender stereotypes, and personal\nidiosyncrasies alike.", "published": "2025-09-28 09:35:51", "link": "http://arxiv.org/abs/2509.23771v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "GBSK: Skeleton Clustering via Granular-ball Computing and Multi-Sampling for Large-Scale Data", "abstract": "To effectively handle clustering task for large-scale datasets, we propose a\nnovel scalable skeleton clustering algorithm, namely GBSK, which leverages the\ngranular-ball technique to capture the underlying structure of data. By\nmulti-sampling the dataset and constructing multi-grained granular-balls, GBSK\nprogressively uncovers a statistical \"skeleton\" -- a spatial abstraction that\napproximates the essential structure and distribution of the original data.\nThis strategy enables GBSK to dramatically reduce computational overhead while\nmaintaining high clustering accuracy. In addition, we introduce an adaptive\nversion, AGBSK, with simplified parameter settings to enhance usability and\nfacilitate deployment in real-world scenarios. Extensive experiments conducted\non standard computing hardware demonstrate that GBSK achieves high efficiency\nand strong clustering performance on large-scale datasets, including one with\nup to 100 million instances across 256 dimensions. Our implementation and\nexperimental results are available at: https://github.com/XFastDataLab/GBSK/.", "published": "2025-09-28 08:41:15", "link": "http://arxiv.org/abs/2509.23742v1", "categories": ["cs.LG", "cs.CV", "cs.IR"], "primary_category": "cs.LG"}
{"title": "From Past To Path: Masked History Learning for Next-Item Prediction in Generative Recommendation", "abstract": "Generative recommendation, which directly generates item identifiers, has\nemerged as a promising paradigm for recommendation systems. However, its\npotential is fundamentally constrained by the reliance on purely autoregressive\ntraining. This approach focuses solely on predicting the next item while\nignoring the rich internal structure of a user's interaction history, thus\nfailing to grasp the underlying intent. To address this limitation, we propose\nMasked History Learning (MHL), a novel training framework that shifts the\nobjective from simple next-step prediction to deep comprehension of history.\nMHL augments the standard autoregressive objective with an auxiliary task of\nreconstructing masked historical items, compelling the model to understand\n``why'' an item path is formed from the user's past behaviors, rather than just\n``what'' item comes next. We introduce two key contributions to enhance this\nframework: (1) an entropy-guided masking policy that intelligently targets the\nmost informative historical items for reconstruction, and (2) a curriculum\nlearning scheduler that progressively transitions from history reconstruction\nto future prediction. Experiments on three public datasets show that our method\nsignificantly outperforms state-of-the-art generative models, highlighting that\na comprehensive understanding of the past is crucial for accurately predicting\na user's future path. The code will be released to the public.", "published": "2025-09-28 05:22:19", "link": "http://arxiv.org/abs/2509.23649v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "ML-Asset Management: Curation, Discovery, and Utilization", "abstract": "Machine learning (ML) assets, such as models, datasets, and metadata, are\ncentral to modern ML workflows. Despite their explosive growth in practice,\nthese assets are often underutilized due to fragmented documentation, siloed\nstorage, inconsistent licensing, and lack of unified discovery mechanisms,\nmaking ML-asset management an urgent challenge. This tutorial offers a\ncomprehensive overview of ML-asset management activities across its lifecycle,\nincluding curation, discovery, and utilization. We provide a categorization of\nML assets, and major management issues, survey state-of-the-art techniques, and\nidentify emerging opportunities at each stage. We further highlight\nsystem-level challenges related to scalability, lineage, and unified indexing.\nThrough live demonstrations of systems, this tutorial equips both researchers\nand practitioners with actionable insights and practical tools for advancing\nML-asset management in real-world and domain-specific settings.", "published": "2025-09-28 02:14:33", "link": "http://arxiv.org/abs/2509.23577v1", "categories": ["cs.DB", "cs.AI", "cs.IR"], "primary_category": "cs.DB"}
{"title": "Edge inducibility via local directed graphs", "abstract": "In this paper we introduce the edge inducibility problem. This is a common\nrefinement of both the well known Kruskal--Katona theorem and the inducibility\nquestion introduced by Pippenger and Golumbic.\n  Our first result is a hardness result. It shows that for any graph $G$, there\nis a related graph $G'$ whose edge inducibility determines the vertex\ninducibility of $G$. Moreover, we determine the edge inducibility of every $G$\nwith at most $4$ vertices, and make some progress on the cases $G=C_5,P_6$.\nLastly, we extend our hardness result to graphs with a perfect matching that is\nthe unique fractional perfect matching. This is done by introducing locally\ndirected graphs, which are natural generalizations of directed graphs.", "published": "2025-09-28 20:37:15", "link": "http://arxiv.org/abs/2509.24064v1", "categories": ["math.CO", "cs.IT", "math.IT"], "primary_category": "math.CO"}
{"title": "Diffusion Models are Kelly Gamblers", "abstract": "We draw a connection between diffusion models and the Kelly criterion for\nmaximizing returns in betting games. We find that conditional diffusion models\nstore additional information to bind the signal $X$ with the conditioning\ninformation $Y$, equal to the mutual information between them. Classifier-free\nguidance effectively boosts the mutual information between $X$ and $Y$ at\nsampling time. This is especially helpful in image models, since the mutual\ninformation between images and their labels is low, a fact which is intimately\nconnected to the manifold hypothesis. Finally, we point out some nuances in the\npopular perspective that diffusion models are infinitely deep autoencoders. In\ndoing so, we relate the denoising loss to the Fermi Golden Rule from quantum\nmechanics.", "published": "2025-09-28 15:27:25", "link": "http://arxiv.org/abs/2509.23937v1", "categories": ["cs.LG", "cond-mat.stat-mech", "cs.AI", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "Post-disaster Max-Min Rate Optimization for Multi-UAV RSMA Network in Obstacle Environments", "abstract": "This paper proposes a rate-splitting multiple access (RSMA) transmission\nscheme to maximize the minimum achievable rate among ground users for emergency\ncommunications in post-disaster scenarios with obstacles, with which the\noptimal positioning of multiple unmanned aerial vehicle (UAV)-enabled base\nstations can be achieved timely.To address the resulting non-convex and\nintractable optimization problem, we design an alternating optimization\napproach. Specifically, we relax obstacle-related constraints using penalty\nterms. In each iteration, block coordinate descent (BCD) and successive convex\napproximation (SCA) are applied alternately to obtain locally optimal\nsolutions, and penalty multipliers are updated to ensure convergence of the\nrelaxed problem to the original one. Simulation results demonstrate that the\nproposed scheme significantly outperforms benchmark methods in terms of the\nminimum achievable rate, verifying its effectiveness and superiority.", "published": "2025-09-28 14:27:17", "link": "http://arxiv.org/abs/2509.23908v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Calibration Meets Reality: Making Machine Learning Predictions Trustworthy", "abstract": "Post-hoc calibration methods are widely used to improve the reliability of\nprobabilistic predictions from machine learning models. Despite their\nprevalence, a comprehensive theoretical understanding of these methods remains\nelusive, particularly regarding their performance across different datasets and\nmodel architectures. Input features play a crucial role in shaping model\npredictions and, consequently, their calibration. However, the interplay\nbetween feature quality and calibration performance has not been thoroughly\ninvestigated. In this work, we present a rigorous theoretical analysis of\npost-hoc calibration methods, focusing on Platt scaling and isotonic\nregression. We derive convergence guarantees, computational complexity bounds,\nand finite-sample performance metrics for these methods. Furthermore, we\nexplore the impact of feature informativeness on calibration performance\nthrough controlled synthetic experiments. Our empirical evaluation spans a\ndiverse set of real-world datasets and model architectures, demonstrating\nconsistent improvements in calibration metrics across various scenarios. By\nexamining calibration performance under varying feature conditions utilizing\nonly informative features versus complete feature spaces including noise\ndimensions, we provide fundamental insights into the robustness and reliability\nof different calibration approaches. Our findings offer practical guidelines\nfor selecting appropriate calibration methods based on dataset characteristics\nand computational constraints, bridging the gap between theoretical\nunderstanding and practical implementation in uncertainty quantification. Code\nand experimental data are available at:\nhttps://github.com/Ajwebdevs/calibration-analysis-experiments.", "published": "2025-09-28 06:04:56", "link": "http://arxiv.org/abs/2509.23665v1", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "math.PR"], "primary_category": "cs.LG"}
{"title": "CORRECT: COndensed eRror RECognition via knowledge Transfer in multi-agent systems", "abstract": "Multi-agent systems (MAS) are increasingly capable of tackling complex\nreal-world tasks, yet their reliance on inter-agent coordination, tool use, and\nlong-horizon reasoning makes error recognition particularly challenging. Minor\nerrors can propagate across agents, escalating into task failures while\nproducing long, intertwined execution trajectories that impose significant\ncosts for both human developers and automated systems to debug and analyze. Our\nkey insight is that, despite surface differences in failure trajectories (e.g.,\nlogs), MAS errors often recur with similar structural patterns. This paper\npresents CORRECT, the first lightweight, training-free framework that leverages\nan online cache of distilled error schemata to recognize and transfer knowledge\nof failure structures across new requests. This cache-based reuse allows LLMs\nto perform targeted error localization at inference time, avoiding the need for\nexpensive retraining while adapting to dynamic MAS deployments in subseconds.\nTo support rigorous study in this domain, we also introduce CORRECT-Error, a\nlarge-scale dataset of over 2,000 annotated trajectories collected through a\nnovel error-injection pipeline guided by real-world distributions, and further\nvalidated through human evaluation to ensure alignment with natural failure\npatterns. Experiments across seven diverse MAS applications show that CORRECT\nimproves step-level error localization up to 19.8% over existing advances while\nat near-zero overhead, substantially narrowing the gap between automated and\nhuman-level error recognition.", "published": "2025-09-28 21:47:20", "link": "http://arxiv.org/abs/2509.24088v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "TeraAgent: A Distributed Agent-Based Simulation Engine for Simulating Half a Trillion Agents", "abstract": "Agent-based simulation is an indispensable paradigm for studying complex\nsystems. These systems can comprise billions of agents, requiring the computing\nresources of multiple servers to simulate. Unfortunately, the state-of-the-art\nplatform, BioDynaMo, does not scale out across servers due to its\nshared-memory-based implementation.\n  To overcome this key limitation, we introduce TeraAgent, a distributed\nagent-based simulation engine. A critical challenge in distributed execution is\nthe exchange of agent information across servers, which we identify as a major\nperformance bottleneck. We propose two solutions: 1) a tailored serialization\nmechanism that allows agents to be accessed and mutated directly from the\nreceive buffer, and 2) leveraging the iterative nature of agent-based\nsimulations to reduce data transfer with delta encoding.\n  Built on our solutions, TeraAgent enables extreme-scale simulations with half\na trillion agents (an 84x improvement), reduces time-to-result with additional\ncompute nodes, improves interoperability with third-party tools, and provides\nusers with more hardware flexibility.", "published": "2025-09-28 20:35:29", "link": "http://arxiv.org/abs/2509.24063v1", "categories": ["cs.DC", "cs.CE", "cs.MA", "cs.PF", "q-bio.QM"], "primary_category": "cs.DC"}
{"title": "PartnerMAS: An LLM Hierarchical Multi-Agent Framework for Business Partner Selection on High-Dimensional Features", "abstract": "High-dimensional decision-making tasks, such as business partner selection,\ninvolve evaluating large candidate pools with heterogeneous numerical,\ncategorical, and textual features. While large language models (LLMs) offer\nstrong in-context reasoning capabilities, single-agent or debate-style systems\noften struggle with scalability and consistency in such settings. We propose\nPartnerMAS, a hierarchical multi-agent framework that decomposes evaluation\ninto three layers: a Planner Agent that designs strategies, Specialized Agents\nthat perform role-specific assessments, and a Supervisor Agent that integrates\ntheir outputs. To support systematic evaluation, we also introduce a curated\nbenchmark dataset of venture capital co-investments, featuring diverse firm\nattributes and ground-truth syndicates. Across 140 cases, PartnerMAS\nconsistently outperforms single-agent and debate-based multi-agent baselines,\nachieving up to 10--15\\% higher match rates. Analysis of agent reasoning shows\nthat planners are most responsive to domain-informed prompts, specialists\nproduce complementary feature coverage, and supervisors play an important role\nin aggregation. Our findings demonstrate that structured collaboration among\nLLM agents can generate more robust outcomes than scaling individual models,\nhighlighting PartnerMAS as a promising framework for high-dimensional\ndecision-making in data-rich domains.", "published": "2025-09-28 19:39:03", "link": "http://arxiv.org/abs/2509.24046v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "GPS-MTM: Capturing Pattern of Normalcy in GPS-Trajectories with self-supervised learning", "abstract": "Foundation models have driven remarkable progress in text, vision, and video\nunderstanding, and are now poised to unlock similar breakthroughs in trajectory\nmodeling. We introduce the GPSMasked Trajectory Transformer (GPS-MTM), a\nfoundation model for large-scale mobility data that captures patterns of\nnormalcy in human movement. Unlike prior approaches that flatten trajectories\ninto coordinate streams, GPS-MTM decomposes mobility into two complementary\nmodalities: states (point-of-interest categories) and actions (agent\ntransitions). Leveraging a bi-directional Transformer with a self-supervised\nmasked modeling objective, the model reconstructs missing segments across\nmodalities, enabling it to learn rich semantic correlations without manual\nlabels. Across benchmark datasets, including Numosim-LA, Urban Anomalies, and\nGeolife, GPS-MTM consistently outperforms on downstream tasks such as\ntrajectory infilling and next-stop prediction. Its advantages are most\npronounced in dynamic tasks (inverse and forward dynamics), where contextual\nreasoning is critical. These results establish GPS-MTM as a robust foundation\nmodel for trajectory analytics, positioning mobility data as a first-class\nmodality for large-scale representation learning. Code is released for further\nreference.", "published": "2025-09-28 19:00:50", "link": "http://arxiv.org/abs/2509.24031v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA"], "primary_category": "cs.LG"}
{"title": "FedAgentBench: Towards Automating Real-world Federated Medical Image Analysis with Server-Client LLM Agents", "abstract": "Federated learning (FL) allows collaborative model training across healthcare\nsites without sharing sensitive patient data. However, real-world FL deployment\nis often hindered by complex operational challenges that demand substantial\nhuman efforts. This includes: (a) selecting appropriate clients (hospitals),\n(b) coordinating between the central server and clients, (c) client-level data\npre-processing, (d) harmonizing non-standardized data and labels across\nclients, and (e) selecting FL algorithms based on user instructions and\ncross-client data characteristics. However, the existing FL works overlook\nthese practical orchestration challenges. These operational bottlenecks\nmotivate the need for autonomous, agent-driven FL systems, where intelligent\nagents at each hospital client and the central server agent collaboratively\nmanage FL setup and model training with minimal human intervention. To this\nend, we first introduce an agent-driven FL framework that captures key phases\nof real-world FL workflows from client selection to training completion and a\nbenchmark dubbed FedAgentBench that evaluates the ability of LLM agents to\nautonomously coordinate healthcare FL. Our framework incorporates 40 FL\nalgorithms, each tailored to address diverse task-specific requirements and\ncross-client characteristics. Furthermore, we introduce a diverse set of\ncomplex tasks across 201 carefully curated datasets, simulating 6\nmodality-specific real-world healthcare environments, viz., Dermatoscopy,\nUltrasound, Fundus, Histopathology, MRI, and X-Ray. We assess the agentic\nperformance of 14 open-source and 10 proprietary LLMs spanning small, medium,\nand large model scales. While some agent cores such as GPT-4.1 and DeepSeek V3\ncan automate various stages of the FL pipeline, our results reveal that more\ncomplex, interdependent tasks based on implicit goals remain challenging for\neven the strongest models.", "published": "2025-09-28 11:06:07", "link": "http://arxiv.org/abs/2509.23803v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Sequence Pathfinder for Multi-Agent Pickup and Delivery in the Warehouse", "abstract": "Multi-Agent Pickup and Delivery (MAPD) is a challenging extension of\nMulti-Agent Path Finding (MAPF), where agents are required to sequentially\ncomplete tasks with fixed-location pickup and delivery demands. Although\nlearning-based methods have made progress in MAPD, they often perform poorly in\nwarehouse-like environments with narrow pathways and long corridors when\nrelying only on local observations for distributed decision-making.\nCommunication learning can alleviate the lack of global information but\nintroduce high computational complexity due to point-to-point communication. To\naddress this challenge, we formulate MAPF as a sequence modeling problem and\nprove that path-finding policies under sequence modeling possess\norder-invariant optimality, ensuring its effectiveness in MAPD. Building on\nthis, we propose the Sequential Pathfinder (SePar), which leverages the\nTransformer paradigm to achieve implicit information exchange, reducing\ndecision-making complexity from exponential to linear while maintaining\nefficiency and global awareness. Experiments demonstrate that SePar\nconsistently outperforms existing learning-based methods across various MAPF\ntasks and their variants, and generalizes well to unseen environments.\nFurthermore, we highlight the necessity of integrating imitation learning in\ncomplex maps like warehouses.", "published": "2025-09-28 09:48:13", "link": "http://arxiv.org/abs/2509.23778v1", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.RO"}
{"title": "SIMPOL Model for Solving Continuous-Time Heterogeneous Agent Problems", "abstract": "This paper presents SIMPOL (Simplified Policy Iteration), a modular numerical\nframework for solving continuous-time heterogeneous agent models. The core\neconomic problem, the optimization of consumption and savings under\nidiosyncratic uncertainty, is formulated as a coupled system of partial\ndifferential equations: a Hamilton-Jacobi-Bellman (HJB) equation for the\nagent's optimal policy and a Fokker-Planck-Kolmogorov (FPK) equation for the\nstationary wealth distribution. SIMPOL addresses this system using Howard's\npolicy iteration with an *upwind* finite difference scheme that guarantees\nstability. A distinctive contribution is a novel consumption policy\npost-processing module that imposes regularity through smoothing and a\nprojection onto an economically plausible slope band, improving convergence and\nmodel behavior. The robustness and accuracy of SIMPOL are validated through a\nset of integrated diagnostics, including verification of contraction in the\nWasserstein-2 metric and comparison with the analytical solution of the Merton\nmodel in the no-volatility case. The framework is shown to be not only\ncomputationally efficient but also to produce solutions consistent with\neconomic and mathematical theory, offering a reliable tool for research in\nquantitative macroeconomics.", "published": "2025-09-28 01:37:29", "link": "http://arxiv.org/abs/2509.23557v1", "categories": ["q-fin.CP", "cs.MA", "econ.TH"], "primary_category": "q-fin.CP"}
{"title": "Dimension-dependent bounds for the SDIEP via phase optimisation and Paley-type constructions", "abstract": "We refine the cycle-walk (Fourier) template of Gnacik and the author to\nquantify when a~$\\delta$-Sule\\u{\\i}manova spectrum\n$(1,\\lambda_2,\\dots,\\lambda_n)$ (with $\\lambda_j\\le 0$) is realised by a\nsymmetric doubly stochastic matrix. For the canonical cycle basis we compute\nthe \\emph{exact} size-dependent threshold \\[ \\delta_n \\;=\\;\n1-\\frac{1}{2\\cos^2\\!\\Big(\\frac{\\pi}{4n}\\rho(n)\\Big)}, \\quad\n\\rho(n)\\in\\{0,1,2,4\\}\\ \\text{determined by } n\\bmod 8, \\] which improves $1/2$\nif and only if $8\\nmid n$; we also prove sharpness for that template. We then\nintroduce an \\emph{optimally phase-aligned} cycle basis which removes the\n`$8\\mid n$' artefact and yields better sufficient bound \\[ \\delta_n^{\\rm (ph)}\n\\;=\\; \\begin{cases} \\displaystyle 1-\\dfrac{1}{2\\cos^2(\\pi/n)}, & n\\equiv\n0\\pmod{4},\\\\[2mm] \\displaystyle 1-\\dfrac{1}{2\\cos^2(\\pi/2n)}, & n\\equiv\n2\\pmod{4},\\\\[2mm] \\displaystyle 1-\\dfrac{1}{2\\cos^2(\\pi/4n)}, & n\\ \\text{odd},\n\\end{cases} \\] so that $\\delta_n^{\\rm (ph)}<\\tfrac12$ for \\emph{every} $n\\ge3$\nand $\\delta_n^{\\rm (ph)}=\\delta_n$ unless $8\\mid n$. Next, on abelian\n$2$-groups, the Walsh--Hadamard basis has coherence $M=1$ and hence suffices\nfor \\emph{all} Sule\\u{\\i}manova lists ($\\delta=0$); the same conclusion holds\nin every Hadamard order (\\emph{e.g.}, Paley families).", "published": "2025-09-28 21:29:13", "link": "http://arxiv.org/abs/2509.24079v1", "categories": ["math.SP", "cs.NA", "math.NA", "15B51 (primary), 15A18, 15B48, 05B20, 05C50, 60J10 (secondary)"], "primary_category": "math.SP"}
{"title": "Analytical and Numerical Approaches for Finding Functional Iterates and Roots", "abstract": "We investigate solutions to the functional equation $f(f(x)) = e^x$, which\ncan be interpreted as the problem of finding a half iterate of the exponential\nmap. While no elementary solution exists, we construct and analyze\nnon-elementary solutions using methods based on the Lambert W function,\ntetration, and Abel's functional equation. We examine structural properties of\npossible solutions, including monotonicity, injectivity, and behavior across\ndifferent intervals, and provide a piecewise-defined framework that extends to\nthe entire real domain. Building on this, we introduce the super-logarithm and\nits inverse, the super-root, as analytic tools for defining fractional iterates\nof $e^x$. Using a power-series expansion near $x = 1$, we numerically\napproximate the super-logarithm and demonstrate a procedure for computing\nfractional iterates, including the half-iterate of the exponential function.\nOur approach is validated by comparisons to known constructions such as\nKneser's tetration, with an emphasis on computational feasibility and numerical\nstability. Finally, we explore the broader landscape of fractional iteration,\nshowing that similar techniques can be applied to other functions beyond $e^x$.\nThrough numerical approximations and series-based methods using genetic\nalgorithms and other optimization techniques, we confirm that fractional\niterates not only exist for many analytic functions but can be computed with\npractical accuracy, opening pathways to further applications in dynamical\nsystems and functional analysis.", "published": "2025-09-28 19:46:57", "link": "http://arxiv.org/abs/2509.24049v1", "categories": ["math.NA", "cs.NA", "39B12, 39B05, 39B22"], "primary_category": "math.NA"}
{"title": "Mixed-Derivative Total Variation", "abstract": "The formulation of norms on continuous-domain Banach spaces with exact\npixel-based discretization is advantageous for solving inverse problems (IPs).\nIn this paper, we investigate a new regularization that is a convex combination\nof a TV term and the $\\M(\\R^2)$ norm of mixed derivatives. We show that the\nextreme points of the corresponding unit ball are indicator functions of\npolygons whose edges are aligned with either the $x_1$- or $x_2$-axis. We then\napply this result to construct a new regularization for IPs, which can be\ndiscretized exactly by tensor products of first-order B-splines, or\nequivalently, pixels. Furthermore, we exactly discretize the loss of the\ndenoising problem on its canonical pixel basis and prove that it admits a\nunique solution, which is also a solution to the underlying continuous-domain\nIP.", "published": "2025-09-28 17:37:31", "link": "http://arxiv.org/abs/2509.23995v1", "categories": ["math.NA", "cs.NA", "math.OC"], "primary_category": "math.NA"}
{"title": "Equation-Free Coarse Control of Distributed Parameter Systems via Local Neural Operators", "abstract": "The control of high-dimensional distributed parameter systems (DPS) remains a\nchallenge when explicit coarse-grained equations are unavailable. Classical\nequation-free (EF) approaches rely on fine-scale simulators treated as\nblack-box timesteppers. However, repeated simulations for steady-state\ncomputation, linearization, and control design are often computationally\nprohibitive, or the microscopic timestepper may not even be available, leaving\nus with data as the only resource. We propose a data-driven alternative that\nuses local neural operators, trained on spatiotemporal microscopic/mesoscopic\ndata, to obtain efficient short-time solution operators. These surrogates are\nemployed within Krylov subspace methods to compute coarse steady and\nunsteady-states, while also providing Jacobian information in a matrix-free\nmanner. Krylov-Arnoldi iterations then approximate the dominant eigenspectrum,\nyielding reduced models that capture the open-loop slow dynamics without\nexplicit Jacobian assembly. Both discrete-time Linear Quadratic Regulator\n(dLQR) and pole-placement (PP) controllers are based on this reduced system and\nlifted back to the full nonlinear dynamics, thereby closing the feedback loop.", "published": "2025-09-28 17:01:53", "link": "http://arxiv.org/abs/2509.23975v1", "categories": ["eess.SY", "cs.LG", "cs.NA", "cs.SY", "math.NA", "math.OC", "93B52, 93C20, 47N70, 65J15, 65M32, 68T07, 68T20, 68T05,"], "primary_category": "eess.SY"}
{"title": "Best weighted approximation of some kernels on the real axis", "abstract": "We calculate the exact value and find the polynomial of the best weighted\npolynomial approximation of kernels of the form $\\frac\n{A+Bt}{(t^2+\\lambda^2)^{s+1}}$, where $A$ and $B$ are fixed complex numbers,\n$\\lambda>0$, $s\\in {\\mathbb N}$, in the mean square metric.", "published": "2025-09-28 13:52:57", "link": "http://arxiv.org/abs/2509.23890v1", "categories": ["math.NA", "cs.NA", "math.CA", "math.OC", "41A10, 30J10, 41A81", "G.1.2"], "primary_category": "math.NA"}
{"title": "Stabilizing the singularity swap quadrature for near-singular line integrals", "abstract": "Singularity swap quadrature (SSQ) is an effective method for the evaluation\nat nearby targets of potentials due to densities on curves in three dimensions.\nWhile highly accurate in most settings, it is known to suffer from catastrophic\ncancellation when the kernel exhibits both near-vanishing numerators and strong\nsingularities, as arises with scalar double layer potentials or tensorial\nkernels in Stokes flow or linear elasticity. This precision loss turns out to\nbe tied to the interpolation basis, namely monomial (for open curves) or\nFourier (closed curves). We introduce a simple yet powerful remedy:\ntarget-specific translated monomial and Fourier bases that explicitly\nincorporate the near-vanishing behavior of the kernel numerator. We combine\nthis with a stable evaluation of the constant term which now dominates the\nintegral, significantly reducing cancellation. We show that our approach\nachieves close to machine precision for prototype integrals, and up to ten\norders of magnitude lower error than standard SSQ at extremely close evaluation\ndistances, without significant additional computational cost.", "published": "2025-09-28 13:41:50", "link": "http://arxiv.org/abs/2509.23881v1", "categories": ["math.NA", "cs.NA", "65D32 (Primary) 65N80, 65D05, 76D07 (Secondary)", "G.1.4; G.1.1; G.1.9"], "primary_category": "math.NA"}
{"title": "Finite Element Complexes with Traces Structures: A unified framework for cohomology and bounded interpolation", "abstract": "This paper considers the cohomology and bounded interpolation of nonstandard\nfinite element complexes, e.g. Stokes, Hessian, Elasticity, divdiv. Compared to\nthe standard finite element exterior calculus, the main challenge is the\nexistence of extra smoothness. This paper provides a unified framework for\nfinite element complexes with extra smoothness. The trace structure is\nintroduced to derive the bubble complexes in different dimensions (vertices,\nedges, faces). It is shown that if the bubble complexes in different dimensions\nare all exact, then the finite element has the correct cohomology. Moreover,\nthe $L^2$ bounded interpolation can be constructed.", "published": "2025-09-28 10:17:14", "link": "http://arxiv.org/abs/2509.23788v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "A space-time generalized finite difference method for solving the transient Stokes/Parabolic interface problem in the moving system", "abstract": "In this paper, a space-time generalized finite difference method (ST-GFDM) is\nproposed to solve the transient Stokes/Parabolic moving interface problem which\nis a type of fluid-structure interaction problem. The ST-GFDM considers the\ntime dimension as the third space dimension, and the 2D time-dependent\nStokes/Parabolic moving interface problem can be seen as a 3D interface problem\nwhere the interface is formed by the initial interface shape and the moving\ntrajectory. The GFDM has an advantage in dealing with interface problems with\ncomplex interface shape and moving interface in the ST domain. More irregular\nmoving direction, more complex interface shape, and the translation and\ndeformation of interface are analyzed to show the advantage of the ST-GFDM. The\ninterface problem can be transformed into coupled sub-problems and locally\ndense nodes is used when the subdomain is too small to satisfy the needs of the\nnumbers of the nodes to improve the performance of the ST-GFDM. Five examples\nare provided to verify the existence of the good performance of the ST-GFDM for\nStokes/Parabolic moving interface problems, including those of the simplicity,\naccuracy, high efficiency and stability.", "published": "2025-09-28 07:25:21", "link": "http://arxiv.org/abs/2509.23702v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Sketching Low-Rank Plus Diagonal Matrices", "abstract": "Many relevant machine learning and scientific computing tasks involve\nhigh-dimensional linear operators accessible only via costly matrix-vector\nproducts. In this context, recent advances in sketched methods have enabled the\nconstruction of *either* low-rank *or* diagonal approximations from few\nmatrix-vector products. This provides great speedup and scalability, but\napproximation errors arise due to the assumed simpler structure. This work\nintroduces SKETCHLORD, a method that simultaneously estimates both low-rank\n*and* diagonal components, targeting the broader class of Low-Rank *plus*\nDiagonal (LoRD) linear operators. We demonstrate theoretically and empirically\nthat this joint estimation is superior also to any sequential variant\n(diagonal-then-low-rank or low-rank-then-diagonal). Then, we cast SKETCHLORD as\na convex optimization problem, leading to a scalable algorithm. Comprehensive\nexperiments on synthetic (approximate) LoRD matrices confirm SKETCHLORD's\nperformance in accurately recovering these structures. This positions it as a\nvaluable addition to the structured approximation toolkit, particularly when\nhigh-fidelity approximations are desired for large-scale operators, such as the\ndeep learning Hessian.", "published": "2025-09-28 02:44:16", "link": "http://arxiv.org/abs/2509.23587v1", "categories": ["cs.LG", "cs.NA", "math.NA"], "primary_category": "cs.LG"}
{"title": "Performance and Numerical Aspects of Decompositional Factorizations with FP64 Floating-Point Emulation in INT8", "abstract": "Mixing precisions for performance has been an ongoing trend as the modern\nhardware accelerators started including new, and mostly lower-precision, data\nformats. The advantage of using them is a great potential of performance gain\nand energy savings. The disadvantage are the numerical issues not present in\nthe standard-mandated floating-point formats. Split integer emulation of FP64\ntakes this to an extreme with the computation performed only by fixed-point\ntensor core units. We present the new issues the emulation faces for practical\ncases involving dense linear solver. We show extensive numerical tests\nindicating the effect of extended numerical range of matrix entries. We also\nscaled the input sizes to study the performance and numerical profiles on the\nNVIDIA Hopper GPUs.", "published": "2025-09-28 01:54:22", "link": "http://arxiv.org/abs/2509.23565v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "GeoFunFlow: Geometric Function Flow Matching for Inverse Operator Learning over Complex Geometries", "abstract": "Inverse problems governed by partial differential equations (PDEs) are\ncrucial in science and engineering. They are particularly challenging due to\nill-posedness, data sparsity, and the added complexity of irregular geometries.\nClassical PDE-constrained optimization methods are computationally expensive,\nespecially when repeated posterior sampling is required. Learning-based\napproaches improve efficiency and scalability, yet most are designed for\nregular domains or focus on forward modeling. Here, we introduce {\\em\nGeoFunFlow}, a geometric diffusion model framework for inverse problems on\ncomplex geometries. GeoFunFlow combines a novel geometric function autoencoder\n(GeoFAE) and a latent diffusion model trained via rectified flow. GeoFAE\nemploys a Perceiver module to process unstructured meshes of varying sizes and\nproduces continuous reconstructions of physical fields, while the diffusion\nmodel enables posterior sampling from sparse and noisy data. Across five\nbenchmarks, GeoFunFlow achieves state-of-the-art reconstruction accuracy over\ncomplex geometries, provides calibrated uncertainty quantification, and\ndelivers efficient inference compared to operator-learning and diffusion model\nbaselines.", "published": "2025-09-28 23:21:52", "link": "http://arxiv.org/abs/2509.24117v1", "categories": ["cs.LG", "physics.comp-ph", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Singleton-Optimized Conformal Prediction", "abstract": "Conformal prediction can be used to construct prediction sets that cover the\ntrue outcome with a desired probability, but can sometimes lead to large\nprediction sets that are costly in practice. The most useful outcome is a\nsingleton prediction-an unambiguous decision-yet existing efficiency-oriented\nmethods primarily optimize average set size. Motivated by this, we propose a\nnew nonconformity score that aims to minimize the probability of producing\nnon-singleton sets. Starting from a non-convex constrained optimization problem\nas a motivation, we provide a geometric reformulation and associated algorithm\nfor computing the nonconformity score and associated split conformal prediction\nsets in O(K) time for K-class problems. Using this score in split conformal\nprediction leads to our proposed Singleton-Optimized Conformal Prediction\n(SOCOP) method. We evaluate our method in experiments on image classification\nand LLM multiple-choice question-answering, comparing with standard\nnonconformity scores such as the (negative) label probability estimates and\ntheir cumulative distribution function; both of which are motivated by\noptimizing length. The results show that SOCOP increases singleton frequency\n(sometimes by over 20%) compared to the above scores, with minimal impact on\naverage set size.", "published": "2025-09-28 22:20:40", "link": "http://arxiv.org/abs/2509.24095v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Demographic-Agnostic Fairness without Harm", "abstract": "As machine learning (ML) algorithms are increasingly used in social domains\nto make predictions about humans, there is a growing concern that these\nalgorithms may exhibit biases against certain social groups. Numerous notions\nof fairness have been proposed in the literature to measure the unfairness of\nML. Among them, one class that receives the most attention is\n\\textit{parity-based}, i.e., achieving fairness by equalizing treatment or\noutcomes for different social groups. However, achieving parity-based fairness\noften comes at the cost of lowering model accuracy and is undesirable for many\nhigh-stakes domains like healthcare. To avoid inferior accuracy, a line of\nresearch focuses on \\textit{preference-based} fairness, under which any group\nof individuals would experience the highest accuracy and collectively prefer\nthe ML outcomes assigned to them if they were given the choice between various\nsets of outcomes. However, these works assume individual demographic\ninformation is known and fully accessible during training. In this paper, we\nrelax this requirement and propose a novel \\textit{demographic-agnostic\nfairness without harm (DAFH)} optimization algorithm, which jointly learns a\ngroup classifier that partitions the population into multiple groups and a set\nof decoupled classifiers associated with these groups. Theoretically, we\nconduct sample complexity analysis and show that our method can outperform the\nbaselines when demographic information is known and used to train decoupled\nclassifiers. Experiments on both synthetic and real data validate the proposed\nmethod.", "published": "2025-09-28 21:23:32", "link": "http://arxiv.org/abs/2509.24077v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Family of Kernelized Matrix Costs for Multiple-Output Mixture Neural Networks", "abstract": "Pairwise distance-based costs are crucial for self-supervised and contrastive\nfeature learning. Mixture Density Networks (MDNs) are a widely used approach\nfor generative models and density approximation, using neural networks to\nproduce multiple centers that define a Gaussian mixture. By combining MDNs with\ncontrastive costs, this paper proposes data density approximation using four\ntypes of kernelized matrix costs: the scalar cost, the vector-matrix cost, the\nmatrix-matrix cost (the trace of Schur complement), and the SVD cost (the\nnuclear norm), for learning multiple centers required to define a mixture\ndensity.", "published": "2025-09-28 21:23:11", "link": "http://arxiv.org/abs/2509.24076v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "On The Variability of Concept Activation Vectors", "abstract": "One of the most pressing challenges in artificial intelligence is to make\nmodels more transparent to their users. Recently, explainable artificial\nintelligence has come up with numerous method to tackle this challenge. A\npromising avenue is to use concept-based explanations, that is, high-level\nconcepts instead of plain feature importance score. Among this class of\nmethods, Concept Activation vectors (CAVs), Kim et al. (2018) stands out as one\nof the main protagonists. One interesting aspect of CAVs is that their\ncomputation requires sampling random examples in the train set. Therefore, the\nactual vectors obtained may vary from user to user depending on the randomness\nof this sampling. In this paper, we propose a fine-grained theoretical analysis\nof CAVs construction in order to quantify their variability. Our results,\nconfirmed by experiments on several real-life datasets, point out towards an\nuniversal result: the variance of CAVs decreases as $1/N$, where $N$ is the\nnumber of random examples. Based on this we give practical recommendations for\na resource-efficient application of the method.", "published": "2025-09-28 20:23:31", "link": "http://arxiv.org/abs/2509.24058v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Does Weak-to-strong Generalization Happen under Spurious Correlations?", "abstract": "We initiate a unified theoretical and algorithmic study of a key problem in\nweak-to-strong (W2S) generalization: when fine-tuning a strong pre-trained\nstudent with pseudolabels from a weaker teacher on a downstream task with\nspurious correlations, does W2S happen, and how to improve it upon failures? We\nconsider two sources of spurious correlations caused by group imbalance: (i) a\nweak teacher fine-tuned on group-imbalanced labeled data with a minority group\nof fraction $\\eta_\\ell$, and (ii) a group-imbalanced unlabeled set\npseudolabeled by the teacher with a minority group of fraction $\\eta_u$.\nTheoretically, a precise characterization of W2S gain at the proportional\nasymptotic limit shows that W2S always happens with sufficient pseudolabels\nwhen $\\eta_u = \\eta_\\ell$ but may fail when $\\eta_u \\ne \\eta_\\ell$, where W2S\ngain diminishes as $(\\eta_u - \\eta_\\ell)^2$ increases. Our theory is\ncorroborated by extensive experiments on various spurious correlation\nbenchmarks and teacher-student pairs. To boost W2S performance upon failures,\nwe further propose a simple, effective algorithmic remedy that retrains the\nstrong student on its high-confidence data subset after W2S fine-tuning. Our\nalgorithm is group-label-free and achieves consistent, substantial improvements\nover vanilla W2S fine-tuning.", "published": "2025-09-28 17:57:49", "link": "http://arxiv.org/abs/2509.24005v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm", "abstract": "Chain-of-Thought (CoT) and its variants have markedly advanced the reasoning\nabilities of Large Language Models (LLMs), yet their monolithic and\nauto-regressive architecture inherently conflates high-level strategic planning\nwith low-level step-by-step execution, leading to computational inefficiency,\nlimited exploration of reasoning paths, and reduced interpretability. To\novercome these issues, we propose the Explore-Execute Chain ($E^2C$), a\nstructured reasoning framework that decouples reasoning into two distinct\nphases: an exploratory phase that stochastically generates succinct high-level\nplans, followed by an execution phase that deterministically carries out the\nchosen plan. Our approach incorporates a two-stage training methodology, which\ncombines Supervised Fine-Tuning (SFT) - augmented by a novel data generation\nalgorithm enforcing strict plan adherence - with a subsequent Reinforcement\nLearning (RL) stage that capitalizes on the informativeness of exploration and\nreinforces the determinism of execution.This decomposition enables an efficient\ntest-time scaling strategy: on AIME'2024, $E^2C$ Test Time Scaling reaches\n58.1% accuracy using <10% of the decoding tokens required by comparable methods\n(e.g., Forest-of-Thought), sharply cutting self-consistency overhead. For\ncross-domain adaptation, our Exploration-Focused SFT (EF-SFT) fine-tunes with\nonly 3.5% of the tokens used by standard SFT yet yields up to 14.5% higher\naccuracy than standard SFT on medical benchmarks, delivering state-of-the-art\nperformance, strong generalization, and greater interpretability by separating\nplanning from execution. The code and pre-trained models for the project are\navailable at: https://github.com/yks23/Explore-Execute-Chain.git", "published": "2025-09-28 15:48:40", "link": "http://arxiv.org/abs/2509.23946v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Asymptotic Expansion for Nonlinear Filtering in the Small System Noise Regime", "abstract": "We propose a new asymptotic expansion method for nonlinear filtering, based\non a small parameter in the system noise. The conditional expectation is\nexpanded as a power series in the noise level, with each coefficient computed\nby solving a system of ordinary differential equations. This approach mitigates\nthe trade-off between computational efficiency and accuracy inherent in\nexisting methods such as Gaussian approximations and particle filters.\nMoreover, by incorporating an Edgeworth-type expansion, our method captures\ncomplex features of the conditional distribution, such as multimodality, with\nsignificantly lower computational cost than conventional filtering algorithms.", "published": "2025-09-28 14:50:45", "link": "http://arxiv.org/abs/2509.23920v1", "categories": ["eess.SP", "math.PR", "stat.ME", "stat.ML"], "primary_category": "eess.SP"}
{"title": "Differentiable Sparsity via $D$-Gating: Simple and Versatile Structured Penalization", "abstract": "Structured sparsity regularization offers a principled way to compact neural\nnetworks, but its non-differentiability breaks compatibility with conventional\nstochastic gradient descent and requires either specialized optimizers or\nadditional post-hoc pruning without formal guarantees. In this work, we propose\n$D$-Gating, a fully differentiable structured overparameterization that splits\neach group of weights into a primary weight vector and multiple scalar gating\nfactors. We prove that any local minimum under $D$-Gating is also a local\nminimum using non-smooth structured $L_{2,2/D}$ penalization, and further show\nthat the $D$-Gating objective converges at least exponentially fast to the\n$L_{2,2/D}$-regularized loss in the gradient flow limit. Together, our results\nshow that $D$-Gating is theoretically equivalent to solving the original group\nsparsity problem, yet induces distinct learning dynamics that evolve from a\nnon-sparse regime into sparse optimization. We validate our theory across\nvision, language, and tabular tasks, where $D$-Gating consistently delivers\nstrong performance-sparsity tradeoffs and outperforms both direct optimization\nof structured penalties and conventional pruning baselines.", "published": "2025-09-28 14:08:29", "link": "http://arxiv.org/abs/2509.23898v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Bayesian Mixture-of-Experts: Towards Making LLMs Know What They Don't Know", "abstract": "The Mixture-of-Experts (MoE) architecture has enabled the creation of massive\nyet efficient Large Language Models (LLMs). However, the standard deterministic\nrouting mechanism presents a significant limitation: its inherent brittleness\nis a key contributor to model miscalibration and overconfidence, resulting in\nsystems that often do not know what they don't know.\n  This thesis confronts this challenge by proposing a structured\n\\textbf{Bayesian MoE routing framework}. Instead of forcing a single,\ndeterministic expert selection, our approach models a probability distribution\nover the routing decision itself. We systematically investigate three families\nof methods that introduce this principled uncertainty at different stages of\nthe routing pipeline: in the \\textbf{weight-space}, the \\textbf{logit-space},\nand the final \\textbf{selection-space}.\n  Through a series of controlled experiments on a 3-billion parameter MoE\nmodel, we demonstrate that this framework significantly improves routing\nstability, in-distribution calibration, and out-of-distribution (OoD)\ndetection. The results show that by targeting this core architectural\ncomponent, we can create a more reliable internal uncertainty signal. This work\nprovides a practical and computationally tractable pathway towards building\nmore robust and self-aware LLMs, taking a crucial step towards making them know\nwhat they don't know.", "published": "2025-09-28 12:07:35", "link": "http://arxiv.org/abs/2509.23830v1", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "primary_category": "cs.LG"}
{"title": "Define latent spaces by example: optimisation over the outputs of generative models", "abstract": "Modern generative AI models such as diffusion and flow matching can sample\nfrom rich data distributions, but many downstream tasks -- such as experimental\ndesign or creative content generation -- require a higher level of control than\nunconstrained sampling. The challenge is to efficiently identify outputs that\nare both probable under the model and satisfy task-specific constraints. We\naddress this by introducing surrogate latent spaces: non-parametric,\nlow-dimensional Euclidean embeddings that can be extracted from any generative\nmodel without additional training. The axes in the Euclidean space can be\ndefined via examples, providing a simple and interpretable approach to define\ncustom latent spaces that both express intended features and are convenient to\nuse in downstream tasks. The representation is Euclidean and has controllable\ndimensionality, permitting direct application of standard optimisation\nalgorithms to traverse the outputs of generative models. Our approach is\narchitecture-agnostic, incurs almost no additional computational cost, and\ngeneralises across modalities, including images, audio, videos, and structured\nobjects like proteins.", "published": "2025-09-28 10:50:06", "link": "http://arxiv.org/abs/2509.23800v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "FraudTransformer: Time-Aware GPT for Transaction Fraud Detection", "abstract": "Detecting payment fraud in real-world banking streams requires models that\ncan exploit both the order of events and the irregular time gaps between them.\nWe introduce FraudTransformer, a sequence model that augments a vanilla\nGPT-style architecture with (i) a dedicated time encoder that embeds either\nabsolute timestamps or inter-event values, and (ii) a learned positional\nencoder that preserves relative order. Experiments on a large industrial\ndataset -- tens of millions of transactions and auxiliary events -- show that\nFraudTransformer surpasses four strong classical baselines (Logistic\nRegression, XGBoost and LightGBM) as well as transformer ablations that omit\neither the time or positional component. On the held-out test set it delivers\nthe highest AUROC and PRAUC.", "published": "2025-09-28 07:53:41", "link": "http://arxiv.org/abs/2509.23712v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Bridging Discrete and Continuous RL: Stable Deterministic Policy Gradient with Martingale Characterization", "abstract": "The theory of discrete-time reinforcement learning (RL) has advanced rapidly\nover the past decades. Although primarily designed for discrete environments,\nmany real-world RL applications are inherently continuous and complex. A major\nchallenge in extending discrete-time algorithms to continuous-time settings is\ntheir sensitivity to time discretization, often leading to poor stability and\nslow convergence. In this paper, we investigate deterministic policy gradient\nmethods for continuous-time RL. We derive a continuous-time policy gradient\nformula based on an analogue of the advantage function and establish its\nmartingale characterization. This theoretical foundation leads to our proposed\nalgorithm, CT-DDPG, which enables stable learning with deterministic policies\nin continuous-time environments. Numerical experiments show that the proposed\nCT-DDPG algorithm offers improved stability and faster convergence compared to\nexisting discrete-time and continuous-time methods, across a wide range of\ncontrol tasks with varying time discretizations and noise levels.", "published": "2025-09-28 07:53:33", "link": "http://arxiv.org/abs/2509.23711v1", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "End-to-End Deep Learning for Predicting Metric Space-Valued Outputs", "abstract": "Many modern applications involve predicting structured, non-Euclidean outputs\nsuch as probability distributions, networks, and symmetric positive-definite\nmatrices. These outputs are naturally modeled as elements of general metric\nspaces, where classical regression techniques that rely on vector space\nstructure no longer apply. We introduce E2M (End-to-End Metric regression), a\ndeep learning framework for predicting metric space-valued outputs. E2M\nperforms prediction via a weighted Fr\\'echet means over training outputs, where\nthe weights are learned by a neural network conditioned on the input. This\nconstruction provides a principled mechanism for geometry-aware prediction that\navoids surrogate embeddings and restrictive parametric assumptions, while fully\npreserving the intrinsic geometry of the output space. We establish theoretical\nguarantees, including a universal approximation theorem that characterizes the\nexpressive capacity of the model and a convergence analysis of the\nentropy-regularized training objective. Through extensive simulations involving\nprobability distributions, networks, and symmetric positive-definite matrices,\nwe show that E2M consistently achieves state-of-the-art performance, with its\nadvantages becoming more pronounced at larger sample sizes. Applications to\nhuman mortality distributions and New York City taxi networks further\ndemonstrate the flexibility and practical utility of the framework.", "published": "2025-09-28 00:46:12", "link": "http://arxiv.org/abs/2509.23544v1", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.ME"], "primary_category": "stat.ML"}
{"title": "Disentangling Score Content and Performance Style for Joint Piano Rendering and Transcription", "abstract": "Expressive performance rendering (EPR) and automatic piano transcription\n(APT) are fundamental yet inverse tasks in music information retrieval: EPR\ngenerates expressive performances from symbolic scores, while APT recovers\nscores from performances. Despite their dual nature, prior work has addressed\nthem independently. In this paper we propose a unified framework that jointly\nmodels EPR and APT by disentangling note-level score content and global\nperformance style representations from both paired and unpaired data. Our\nframework is built on a transformer-based sequence-to-sequence architecture and\nis trained using only sequence-aligned data, without requiring fine-grained\nnote-level alignment. To automate the rendering process while ensuring\nstylistic compatibility with the score, we introduce an independent\ndiffusion-based performance style recommendation module that generates style\nembeddings directly from score content. This modular component supports both\nstyle transfer and flexible rendering across a range of expressive styles.\nExperimental results from both objective and subjective evaluations demonstrate\nthat our framework achieves competitive performance on EPR and APT tasks, while\nenabling effective content-style disentanglement, reliable style transfer, and\nstylistically appropriate rendering. Demos are available at\nhttps://jointpianist.github.io/epr-apt/", "published": "2025-09-28 13:36:33", "link": "http://arxiv.org/abs/2509.23878v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AISHELL6-whisper: A Chinese Mandarin Audio-visual Whisper Speech Dataset with Speech Recognition Baselines", "abstract": "Whisper speech recognition is crucial not only for ensuring privacy in\nsensitive communications but also for providing a critical communication bridge\nfor patients under vocal restraint and enabling discrete interaction in\nnoise-sensitive environments. The development of Chinese mandarin audio-visual\nwhisper speech recognition is hindered by the lack of large-scale datasets. We\npresent AISHELL6-Whisper, a large-scale open-source audio-visual whisper speech\ndataset, featuring 30 hours each of whisper speech and parallel normal speech,\nwith synchronized frontal facial videos. Moreover, we propose an audio-visual\nspeech recognition (AVSR) baseline based on the Whisper-Flamingo framework,\nwhich integrates a parallel training strategy to align embeddings across speech\ntypes, and employs a projection layer to adapt to whisper speech's spectral\nproperties. The model achieves a Character Error Rate (CER) of 4.13% for\nwhisper speech and 1.11% for normal speech in the test set of our dataset, and\nestablishes new state-of-the-art results on the wTIMIT benchmark. The dataset\nand the AVSR baseline codes are open-sourced at\nhttps://zutm.github.io/AISHELL6-Whisper.", "published": "2025-09-28 12:14:06", "link": "http://arxiv.org/abs/2509.23833v1", "categories": ["eess.AS", "cs.CV", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LORT: Locally Refined Convolution and Taylor Transformer for Monaural Speech Enhancement", "abstract": "Achieving superior enhancement performance while maintaining a low parameter\ncount and computational complexity remains a challenge in the field of speech\nenhancement. In this paper, we introduce LORT, a novel architecture that\nintegrates spatial-channel enhanced Taylor Transformer and locally refined\nconvolution for efficient and robust speech enhancement. We propose a Taylor\nmulti-head self-attention (T-MSA) module enhanced with spatial-channel\nenhancement attention (SCEA), designed to facilitate inter-channel information\nexchange and alleviate the spatial attention limitations inherent in\nTaylor-based Transformers. To complement global modeling, we further present a\nlocally refined convolution (LRC) block that integrates convolutional\nfeed-forward layers, time-frequency dense local convolutions, and gated units\nto capture fine-grained local details. Built upon a U-Net-like encoder-decoder\nstructure with only 16 output channels in the encoder, LORT processes noisy\ninputs through multi-resolution T-MSA modules using alternating downsampling\nand upsampling operations. The enhanced magnitude and phase spectra are decoded\nindependently and optimized through a composite loss function that jointly\nconsiders magnitude, complex, phase, discriminator, and consistency objectives.\nExperimental results on the VCTK+DEMAND and DNS Challenge datasets demonstrate\nthat LORT achieves competitive or superior performance to state-of-the-art\n(SOTA) models with only 0.96M parameters, highlighting its effectiveness for\nreal-world speech enhancement applications with limited computational\nresources.", "published": "2025-09-28 12:12:21", "link": "http://arxiv.org/abs/2509.23832v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Wideband Integrated Sensing and Communications: Spectral Efficiency and Signaling Design", "abstract": "In integrated sensing and communications (ISAC), a distinguishing feature of\n6G wireless networks, the main challenge lies in integrating the two distinct\nfunctions of sensing and communication within the same waveform. In this paper,\nthe ISAC waveform synthesis is studied in the wideband regime, since a large\nbandwidth can simplify the analysis and is justified by the employment of\nmillimeter wave or higher frequency band. Standard orthogonal frequency\ndivision multiplexing (OFDM) signaling is assumed, and the wideband analysis of\nsensing is a counterpart of the existing studies on wideband communications. It\nis proposed that the phase over such OFDM subcarriers is for modulating\ncommunication messages while the power spectral density (PSD) is shaped for the\nsensing performance. Beyond OFDM, we further reveal a duality between the\nproposed PSD-shaping rule and the orthogonal time frequency space (OTFS)\nwaveform. Flattening the OTFS delay-axis PSD produces the same integrated\nsidelobe level (ISL) reduction effect in the delay-Doppler domain as PSD\ncontrol achieves for OFDM in the frequency domain. To balance communication and\nsensing performance over frequency-selective channels, we propose a\nlow-complexity, water-filling-like allocator with an explicit PSD-flatness\n(variance) constraint. The performance of the proposed wideband ISAC scheme is\ndemonstrated using both numerical simulations and hardware experiments.", "published": "2025-09-28 22:27:20", "link": "http://arxiv.org/abs/2509.24097v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "PEARL: Peer-Enhanced Adaptive Radio via On-Device LLM", "abstract": "We present PEARL (Peer-Enhanced Adaptive Radio via On-Device LLM), a\nframework for cooperative cross-layer optimization in device-to-device (D2D)\ncommunication. Building on our previous work on single-device on-device LLMs,\nPEARL extends the paradigm by leveraging both publisher and subscriber states\nto guide Wi-Fi Aware (WA) parameter selection. A context-aware reward, which\nnormalizes latency by application tolerances and modulates energy by device\nbattery states, provides richer supervision for KL-based finetuning. We study\ntwo lightweight variants: PEARL (Head + Low-Rank Adaptation (LoRA)) achieves\nthe best overall performance, while PEARL-Lite (Head-only) delivers sub-20 ms\ninference at near-identical objective scores. Across synthetic scenarios\ngrounded in real measurements, PEARL improves objective scores over heuristic\nand compact model baselines and reduces energy by up to 16% in cooperative\nlow-battery cases. These results demonstrate that peer-aware context,\nreward-aligned training, and head-based efficiency make LLMs practical for\nalways-on, on-device cross-layer control.", "published": "2025-09-28 21:43:17", "link": "http://arxiv.org/abs/2509.24085v1", "categories": ["cs.LG", "cs.AI", "cs.NI", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Performance Analysis of Zero-Forcing Beamforming Strategies for the Uplink of an MU-MIMO System with Multi-Antenna Users", "abstract": "We conduct a comprehensive evaluation of the performance of the uplink of\nOFDMA-based MU-MIMO systems with multi-antenna users, for three Zero-Forcing\n(ZF) Beamforming (BF) strategies: Coordinated-Transmit-Receive-1 (CTR1), where\nonly the strongest data stream is enabled per scheduled user; Block\nDiagonalization (BD), where all possible streams are enabled per scheduled\nuser; Coordinated-Transmit-Receive-Flexible (CTRF), which allows a flexible\nstream allocation per user. The Radio Resource Management (RRM) of the uplink\nof all OFDMA-based systems must be done over an entire Time-Slot (TS) due to\npower management, making it challenging. To enable this study, we propose an\nefficient heuristic based on greedy-up searches for stream-sets that provides\nfeasible solutions. It operates over the TS and considers fairness, practical\nModulation and Coding Schemes and all RRM processes. The results show that, for\nRural Macro scenarios, BD (resp. CTR1) could replace the more complex CTRF if\nthe number of users is small (resp. large), while for Urban Macro scenarios,\nCTR1 emerges as an alternative to CTRF due to its similar performance. We also\nshow that the system parameters can substantially impact the performance of the\nZF strategies and that BD performance is more impaired with a simpler power\nmanagement scheme than CTR1 and CTRF.", "published": "2025-09-28 14:51:49", "link": "http://arxiv.org/abs/2509.23921v1", "categories": ["cs.NI", "eess.SP"], "primary_category": "cs.NI"}
{"title": "Integrated Communication and Control for Energy-Efficient UAV Swarms: A Multi-Agent Reinforcement Learning Approach", "abstract": "The deployment of unmanned aerial vehicle (UAV) swarm-assisted communication\nnetworks has become an increasingly vital approach for remediating coverage\nlimitations in infrastructure-deficient environments, with especially pressing\napplications in temporary scenarios, such as emergency rescue, military and\nsecurity operations, and remote area coverage. However, complex geographic\nenvironments lead to unpredictable and highly dynamic wireless channel\nconditions, resulting in frequent interruptions of air-to-ground (A2G) links\nthat severely constrain the reliability and quality of service in UAV\nswarm-assisted mobile communications. To improve the quality of UAV\nswarm-assisted communications in complex geographic environments, we propose an\nintegrated communication and control co-design mechanism. Given the stringent\nenergy constraints inherent in UAV swarms, our proposed mechanism is designed\nto optimize energy efficiency while maintaining an equilibrium between\nequitable communication rates for mobile ground users (GUs) and UAV energy\nexpenditure. We formulate the joint resource allocation and 3D trajectory\ncontrol problem as a Markov decision process (MDP), and develop a multi-agent\nreinforcement learning (MARL) framework to enable real-time coordinated actions\nacross the UAV swarm. To optimize the action policy of UAV swarms, we propose a\nnovel multi-agent hybrid proximal policy optimization with action masking\n(MAHPPO-AM) algorithm, specifically designed to handle complex hybrid action\nspaces. The algorithm incorporates action masking to enforce hard constraints\nin high-dimensional action spaces. Experimental results demonstrate that our\napproach achieves a fairness index of 0.99 while reducing energy consumption by\nup to 25% compared to baseline methods.", "published": "2025-09-28 14:23:04", "link": "http://arxiv.org/abs/2509.23905v1", "categories": ["cs.LG", "cs.SY", "eess.SP", "eess.SY"], "primary_category": "cs.LG"}
{"title": "Online Specific Emitter Identification via Collision-Alleviated Signal Hash", "abstract": "Specific Emitter Identification (SEI) has been widely studied, aiming to\ndistinguish signals from different emitters given training samples from those\nemitters. However, real-world scenarios often require identifying signals from\nnovel emitters previously unseen. Since these novel emitters only have a few or\nno prior samples, existing models struggle to identify signals from novel\nemitters online and tend to bias toward the distribution of seen emitters. To\naddress these challenges, we propose the Online Specific Emitter Identification\n(OSEI) task, comprising both online \\revise{few-shot and generalized zero-shot}\nlearning tasks. It requires constructing models using signal samples from seen\nemitters and then identifying new samples from seen and novel emitters online\nduring inference. We propose a novel hash-based model, Collision-Alleviated\nSignal Hash (CASH), providing a unified approach for addressing the OSEI task.\nThe CASH operates in two steps: in the seen emitters identifying step, a signal\nencoder and a seen emitters identifier determine whether the signal sample is\nfrom seen emitters, mitigating the model from biasing toward seen emitters\ndistribution. In the signal hash coding step, an online signal hasher assigns a\nhash code to each signal sample, identifying its specific emitter. Experimental\nresults on real-world signal datasets (i.e., ADSB and ORACLE) demonstrate that\nour method accurately identifies signals from both seen and novel emitters\nonline. This model outperforms existing methods by a minimum of 6.08\\% and\n8.55\\% in accuracy for the few-shot and \\revise{generalized zero-shot learning\n}tasks, respectively. The code will be open-sourced at\n\\href{https://github.com/IntelliSensing/OSEI-CASH}{https://github.com/IntelliSensing/OSEI-CASH}.", "published": "2025-09-28 11:12:39", "link": "http://arxiv.org/abs/2509.23807v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Expectation Propagation-Based Signal Detection for Highly Correlated MIMO Systems", "abstract": "Large-scale multiple-input-multiple-output (MIMO) systems typically operate\nin dense array deployments with limited scattering environments, leading to\nhighly correlated and ill-conditioned channel matrices that severely degrade\nthe performance of message-passing-based detectors. To tackle this issue, this\npaper proposes an expectation propagation (EP)-based detector, termed\noverlapping block partitioning EP (OvEP). In OvEP, the large-scale measurement\nvector is partitioned into partially overlapping blocks. For each block and its\noverlapping part, a low-complexity linear minimum mean square error\n(LMMSE)-based filter is designed according to the partitioned structure. The\nresulting LMMSE outputs are then combined to generate the input to the\ndenoiser. In this combining process, subtracting the overlapping-part outputs\nfrom the block outputs effectively mitigates the adverse effects of inter-block\ncorrelation induced by high spatial correlation. The proposed algorithm is\nconsistently derived within the EP framework, and its fixed point is\ntheoretically proven to coincide with the stationary point of a relaxed\nKullback- Leibler (KL) minimization problem. The mechanisms underlying the\ntheoretically predicted performance improvement are further clarified through\nnumerical simulations. The proposed algorithm achieves performance close to\nconventional LMMSE-EP with lower computational complexity.", "published": "2025-09-28 10:23:08", "link": "http://arxiv.org/abs/2509.23792v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "A quantitative analysis of intraventricular bioimpedance in an in vivo pilot study with contextual pressure measurements", "abstract": "Hydrocephalus is a neurological condition characterized by disturbed\ncerebrospinal fluid (CSF) dynamics and is typically treated with shunt systems\nthat drain excessive CSF out of the ventricular system. Continuous monitoring\nof ventricular CSF volume, however, remains a major unmet need in the clinical\nmanagement of this condition. While intraventricular bioimpedance (BI) has been\nproposed as a potential marker of CSF volume, prior investigations have been\nlimited to simulations, in vitro phantoms, and small animal models. This work\npresents the development of a measurement system for intraventricular BI and\nits evaluation in a large animal model. The measurement system was first\nvalidated in vitro using a mechatronic test bench replicating physiological CSF\ndynamics and subsequently applied in an in vivo pilot study with concurrent CSF\nand blood pressure monitoring. Time series analysis of the recorded signals\nrevealed physiological BI waveform components linked to the cardiac and\nrespiratory cycles. In addition, changes in BI following CSF volume alterations\ninduced through intrathecal bolus infusions of artificial CSF were observed and\nfound to be correlated to changes in CSF and blood pressures. These results\nprovide the first in vivo evidence in a large animal model that BI reflects CSF\ndynamics as well as cerebral hemodynamics. Complementing intracranial pressure\nand CSF drainage measurements in smart shunt systems with BI could enable more\ncomprehensive patient monitoring and physiologically informed control of\nhydrocephalus therapy.", "published": "2025-09-28 10:09:37", "link": "http://arxiv.org/abs/2509.23785v1", "categories": ["physics.med-ph", "eess.SP"], "primary_category": "physics.med-ph"}
{"title": "Joint Hybrid Beamforming and Artificial Noise Design for Secure Multi-UAV ISAC Networks", "abstract": "Integrated sensing and communication (ISAC) emerges as a key enabler for\nnext-generation applications such as smart cities and autonomous systems. Its\nintegration with unmanned aerial vehicles (UAVs) unlocks new potentials for\nreliable communication and precise sensing in dynamic aerial environments.\nHowever, existing research predominantly treats UAVs as aerial base stations,\noverlooking their role as ISAC users, and fails to leverage large-scale antenna\narrays at terrestrial base stations to enhance security and spectral\nefficiency. This paper propose a secure and spectral efficient ISAC framework\nfor multi-UAV networks, and a two-stage optimization approach is developed to\njointly design hybrid beamforming (HBF), artificial noise (AN) injection, and\nUAV trajectories. Aiming at maximizing the sum secrecy rate, the first stage\nemploys Proximal Policy Optimization (PPO) to optimize digital beamformers and\ntrajectories, and the second stage decomposes the digital solution into analog\nand digital components via low-complexity matrix factorization. Simulation\nresults demonstrate the effectiveness of the proposed framework compared to\nbenchmark schemes.", "published": "2025-09-28 06:58:04", "link": "http://arxiv.org/abs/2509.23687v1", "categories": ["eess.SP", "cs.AI"], "primary_category": "eess.SP"}
{"title": "Learnable Kernels for FRI -- Joint Kernel Encoder Optimization and Hardware Validation", "abstract": "Finite Rate of Innovation (FRI) sampling techniques provide efficient\nframeworks for reconstructing signals with inherent sparsity at rates below\nNyquist. However, traditional FRI reconstruction methods rely heavily on\npre-defined kernels, often limiting hardware implementation and reconstruction\naccuracy under noisy conditions. In this paper, we propose a robust, flexible,\nand practically implementable framework for FRI reconstruction by introducing\nnovel learnable kernel strategies. First, we demonstrate effective\nreconstruction using known, fixed kernels such as truncated Gaussian and\nGaussian pair kernels, which mitigate the requirement that the samples should\nhave a sum-of-exponentials (SoE) form. Next, we extend this concept by jointly\noptimizing both the sampling kernel and reconstruction encoder through a\nunified learning approach, yielding adaptive kernels that significantly\noutperform traditional methods in resolution and noise robustness, with reduced\nsampling rates. Furthermore, we propose a practical hardware realization by\nrepresenting kernels as sums of two exponential decay signals with jointly\noptimized poles, facilitating compact, efficient analog implementations. Our\napproach is validated experimentally through hardware implementations using a\nunity-gain Sallen-Key analog filter, achieving accurate real-world signal\nrecovery. The developed convolutional neural network-based encoder\nsubstantially reduces computational complexity, demonstrating competitive\nperformance with fewer parameters, making our method particularly suitable for\nresource-constrained, edge-based deployments.", "published": "2025-09-28 04:56:16", "link": "http://arxiv.org/abs/2509.23644v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Foundation Model-Based Adaptive Semantic Image Transmission for Dynamic Wireless Environments", "abstract": "Foundation model-based semantic transmission has recently shown great\npotential in wireless image communication. However, existing methods exhibit\ntwo major limitations: (i) they overlook the varying importance of semantic\ncomponents for specific downstream tasks, and (ii) they insufficiently exploit\nwireless domain knowledge, resulting in limited robustness under dynamic\nchannel conditions. To overcome these challenges, this paper proposes a\nfoundation model-based adaptive semantic image transmission system for dynamic\nwireless environments, such as autonomous driving. The proposed system\ndecomposes each image into a semantic segmentation map and a compressed\nrepresentation, enabling task-aware prioritization of critical objects and\nfine-grained textures. A task-adaptive precoding mechanism then allocates radio\nresources according to the semantic importance of extracted features. To ensure\naccurate channel information for precoding, a channel estimation knowledge map\n(CEKM) is constructed using a conditional diffusion model that integrates user\nposition, velocity, and sparse channel samples to train scenario-specific\nlightweight estimators. At the receiver, a conditional diffusion model\nreconstructs high-quality images from the received semantic features, ensuring\nrobustness against channel impairments and partial data loss. Simulation\nresults on the BDD100K dataset with multi-scenario channels generated by\nQuaDRiGa demonstrate that the proposed method outperforms existing approaches\nin terms of perceptual quality (SSIM, LPIPS, FID), task-specific accuracy\n(IoU), and transmission efficiency. These results highlight the effectiveness\nof integrating task-aware semantic decomposition, scenario-adaptive channel\nestimation, and diffusion-based reconstruction for robust semantic transmission\nin dynamic wireless environments.", "published": "2025-09-28 02:47:50", "link": "http://arxiv.org/abs/2509.23590v1", "categories": ["eess.IV", "eess.SP"], "primary_category": "eess.IV"}
{"title": "Sequence Pathfinder for Multi-Agent Pickup and Delivery in the Warehouse", "abstract": "Multi-Agent Pickup and Delivery (MAPD) is a challenging extension of\nMulti-Agent Path Finding (MAPF), where agents are required to sequentially\ncomplete tasks with fixed-location pickup and delivery demands. Although\nlearning-based methods have made progress in MAPD, they often perform poorly in\nwarehouse-like environments with narrow pathways and long corridors when\nrelying only on local observations for distributed decision-making.\nCommunication learning can alleviate the lack of global information but\nintroduce high computational complexity due to point-to-point communication. To\naddress this challenge, we formulate MAPF as a sequence modeling problem and\nprove that path-finding policies under sequence modeling possess\norder-invariant optimality, ensuring its effectiveness in MAPD. Building on\nthis, we propose the Sequential Pathfinder (SePar), which leverages the\nTransformer paradigm to achieve implicit information exchange, reducing\ndecision-making complexity from exponential to linear while maintaining\nefficiency and global awareness. Experiments demonstrate that SePar\nconsistently outperforms existing learning-based methods across various MAPF\ntasks and their variants, and generalizes well to unseen environments.\nFurthermore, we highlight the necessity of integrating imitation learning in\ncomplex maps like warehouses.", "published": "2025-09-28 09:48:13", "link": "http://arxiv.org/abs/2509.23778v2", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.RO"}
{"title": "A Family of Kernelized Matrix Costs for Multiple-Output Mixture Neural Networks", "abstract": "Pairwise distance-based costs are crucial for self-supervised and contrastive\nfeature learning. Mixture Density Networks (MDNs) are a widely used approach\nfor generative models and density approximation, using neural networks to\nproduce multiple centers that define a Gaussian mixture. By combining MDNs with\ncontrastive costs, this paper proposes data density approximation using four\ntypes of kernelized matrix costs: the scalar cost, the vector-matrix cost, the\nmatrix-matrix cost (the trace of Schur complement), and the SVD cost (the\nnuclear norm), for learning multiple centers required to define a mixture\ndensity.", "published": "2025-09-28 21:23:11", "link": "http://arxiv.org/abs/2509.24076v2", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm", "abstract": "Chain-of-Thought (CoT) and its variants have markedly advanced the reasoning\nabilities of Large Language Models (LLMs), yet their monolithic and\nauto-regressive architecture inherently conflates high-level strategic planning\nwith low-level step-by-step execution, leading to computational inefficiency,\nlimited exploration of reasoning paths, and reduced interpretability. To\novercome these issues, we propose the Explore-Execute Chain ($E^2C$), a\nstructured reasoning framework that decouples reasoning into two distinct\nphases: an exploratory phase that stochastically generates succinct high-level\nplans, followed by an execution phase that deterministically carries out the\nchosen plan. Our approach incorporates a two-stage training methodology, which\ncombines Supervised Fine-Tuning (SFT) - augmented by a novel data generation\nalgorithm enforcing strict plan adherence - with a subsequent Reinforcement\nLearning (RL) stage that capitalizes on the informativeness of exploration and\nreinforces the determinism of execution. This decomposition enables an\nefficient test-time scaling strategy: on AIME'2024, $E^2C$ Test Time Scaling\nreaches 58.1% accuracy using <10% of the decoding tokens required by comparable\nmethods (e.g., Forest-of-Thought), sharply cutting self-consistency overhead.\nFor cross-domain adaptation, our Exploration-Focused SFT (EF-SFT) fine-tunes\nwith only 3.5% of the tokens used by standard SFT yet yields up to 14.5% higher\naccuracy than standard SFT on medical benchmarks, delivering state-of-the-art\nperformance, strong generalization, and greater interpretability by separating\nplanning from execution. The code and pre-trained models for the project are\navailable at: https://github.com/yks23/Explore-Execute-Chain.git", "published": "2025-09-28 15:48:40", "link": "http://arxiv.org/abs/2509.23946v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Differentiable Sparsity via $D$-Gating: Simple and Versatile Structured Penalization", "abstract": "Structured sparsity regularization offers a principled way to compact neural\nnetworks, but its non-differentiability breaks compatibility with conventional\nstochastic gradient descent and requires either specialized optimizers or\nadditional post-hoc pruning without formal guarantees. In this work, we propose\n$D$-Gating, a fully differentiable structured overparameterization that splits\neach group of weights into a primary weight vector and multiple scalar gating\nfactors. We prove that any local minimum under $D$-Gating is also a local\nminimum using non-smooth structured $L_{2,2/D}$ penalization, and further show\nthat the $D$-Gating objective converges at least exponentially fast to the\n$L_{2,2/D}$-regularized loss in the gradient flow limit. Together, our results\nshow that $D$-Gating is theoretically equivalent to solving the original group\nsparsity problem, yet induces distinct learning dynamics that evolve from a\nnon-sparse regime into sparse optimization. We validate our theory across\nvision, language, and tabular tasks, where $D$-Gating consistently delivers\nstrong performance-sparsity tradeoffs and outperforms both direct optimization\nof structured penalties and conventional pruning baselines.", "published": "2025-09-28 14:08:29", "link": "http://arxiv.org/abs/2509.23898v2", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety Evaluation via Role-Specialized Collaboration", "abstract": "Existing safety evaluation methods for large language models (LLMs) suffer\nfrom inherent limitations, including evaluator bias and detection failures\narising from model homogeneity, which collectively undermine the robustness of\nrisk evaluation processes. This paper seeks to re-examine the risk evaluation\nparadigm by introducing a theoretical framework that reconstructs the\nunderlying risk concept space. Specifically, we decompose the latent risk\nconcept space into three mutually exclusive subspaces: the explicit risk\nsubspace (encompassing direct violations of safety guidelines), the implicit\nrisk subspace (capturing potential malicious content that requires contextual\nreasoning for identification), and the non-risk subspace. Furthermore, we\npropose RADAR, a multi-agent collaborative evaluation framework that leverages\nmulti-round debate mechanisms through four specialized complementary roles and\nemploys dynamic update mechanisms to achieve self-evolution of risk concept\ndistributions. This approach enables comprehensive coverage of both explicit\nand implicit risks while mitigating evaluator bias. To validate the\neffectiveness of our framework, we construct an evaluation dataset comprising\n800 challenging cases. Extensive experiments on our challenging testset and\npublic benchmarks demonstrate that RADAR significantly outperforms baseline\nevaluation methods across multiple dimensions, including accuracy, stability,\nand self-evaluation risk sensitivity. Notably, RADAR achieves a 28.87%\nimprovement in risk identification accuracy compared to the strongest baseline\nevaluation method.", "published": "2025-09-28 09:35:32", "link": "http://arxiv.org/abs/2509.25271v1", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Heterogeneous Multi-agent Collaboration in UAV-assisted Mobile Crowdsensing Networks", "abstract": "Unmanned aerial vehicles (UAVs)-assisted mobile crowdsensing (MCS) has\nemerged as a promising paradigm for data collection. However, challenges such\nas spectrum scarcity, device heterogeneity, and user mobility hinder efficient\ncoordination of sensing, communication, and computation. To tackle these\nissues, we propose a joint optimization framework that integrates time slot\npartition for sensing, communication, and computation phases, resource\nallocation, and UAV 3D trajectory planning, aiming to maximize the amount of\nprocessed sensing data. The problem is formulated as a non-convex stochastic\noptimization and further modeled as a partially observable Markov decision\nprocess (POMDP) that can be solved by multi-agent deep reinforcement learning\n(MADRL) algorithm. To overcome the limitations of conventional multi-layer\nperceptron (MLP) networks, we design a novel MADRL algorithm with hybrid actor\nnetwork. The newly developed method is based on heterogeneous agent proximal\npolicy optimization (HAPPO), empowered by convolutional neural networks (CNN)\nfor feature extraction and Kolmogorov-Arnold networks (KAN) to capture\nstructured state-action dependencies. Extensive numerical results demonstrate\nthat our proposed method achieves significant improvements in the amount of\nprocessed sensing data when compared with other benchmarks.", "published": "2025-09-28 02:13:19", "link": "http://arxiv.org/abs/2509.25261v1", "categories": ["cs.LG", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Position-Blind Ptychography: Viability of image reconstruction via data-driven variational inference", "abstract": "In this work, we present and investigate the novel blind inverse problem of\nposition-blind ptychography, i.e., ptychographic phase retrieval without any\nknowledge of scan positions, which then must be recovered jointly with the\nimage. The motivation for this problem comes from single-particle diffractive\nX-ray imaging, where particles in random orientations are illuminated and a set\nof diffraction patterns is collected. If one uses a highly focused X-ray beam,\nthe measurements would also become sensitive to the beam positions relative to\neach particle and therefore ptychographic, but these positions are also\nunknown. We investigate the viability of image reconstruction in a simulated,\nsimplified 2-D variant of this difficult problem, using variational inference\nwith modern data-driven image priors in the form of score-based diffusion\nmodels. We find that, with the right illumination structure and a strong prior,\none can achieve reliable and successful image reconstructions even under\nmeasurement noise, in all except the most difficult evaluated imaging scenario.", "published": "2025-09-28 08:49:55", "link": "http://arxiv.org/abs/2509.25269v1", "categories": ["eess.IV", "cs.CV", "cs.LG", "cs.NA", "math.NA", "physics.optics", "94A08, 68U10, 78A46, 68T07"], "primary_category": "eess.IV"}
{"title": "AW-EL-PINNs: A Multi-Task Learning Physics-Informed Neural Network for Euler-Lagrange Systems in Optimal Control Problems", "abstract": "This paper presents adaptive weighted Euler-Lagrange theorem combined\nphysics-informed neural networks (AW-EL-PINNs) for solving Euler-Lagrange\nsystems in optimal control problems. The framework systematically converts\noptimal control frameworks into two-point boundary value problems (TPBVPs)\nwhile establishing a multi-task learning paradigm through innovative\nintegration of the Euler-Lagrange theorem with deep learning architecture. An\nadaptive loss weighting mechanism dynamically balances loss function components\nduring training, decreasing tedious manual tuning of weighting the loss\nfunctions compared to the conventional physics-informed neural networks\n(PINNs). Based on six numerical examples, it's clear that AW-EL-PINNs achieve\nenhanced solution accuracy compared to baseline methods while maintaining\nstability throughout the optimization process. These results highlight the\nframework's capability to improve precision and ensure stability in solving\nEuler-Lagrange systems in optimal control problems, offering potential\nstrategies for problems under physical applications.", "published": "2025-09-28 02:45:56", "link": "http://arxiv.org/abs/2509.25262v1", "categories": ["math.NA", "cs.NA", "cs.SY", "eess.SY"], "primary_category": "math.NA"}
{"title": "How Effective Are Time-Series Models for Rainfall Nowcasting? A Comprehensive Benchmark for Rainfall Nowcasting Incorporating PWV Data", "abstract": "Rainfall nowcasting, which aims to predict precipitation within the next 0 to\n3 hours, is critical for disaster mitigation and real-time response planning.\nHowever, most time series forecasting benchmarks in meteorology are evaluated\non variables with strong periodicity, such as temperature and humidity, which\nfail to reflect model capabilities in more complex and practically meteorology\nscenarios like rainfall nowcasting. To address this gap, we propose\nRainfallBench, a benchmark designed for rainfall nowcasting, a highly\nchallenging and practically relevant task characterized by zero inflation,\ntemporal decay, and non-stationarity, focused on predicting precipitation\nwithin the next 0 to 3 hours. The dataset is derived from five years of\nmeteorological observations, recorded at 15-minute intervals across six\nessential variables, and collected from more than 12,000 GNSS stations\nglobally. In particular, it incorporates precipitable water vapor (PWV), a\ncrucial indicator of rainfall that is absent in other datasets. We further\ndesign specialized evaluation strategies to assess model performance on key\nmeteorological challenges, such as multi-scale prediction and extreme rainfall\nevents, and evaluate over 20 state-of-the-art models across six major\narchitectures on RainfallBench. Additionally, to address the zero-inflation and\ntemporal decay issues overlooked by existing models, we introduce Bi-Focus\nPrecipitation Forecaster (BFPF), a plug-and-play module that incorporates\ndomain-specific priors to enhance rainfall time series forecasting. Statistical\nanalysis and ablation studies validate the comprehensiveness of our dataset as\nwell as the superiority of our methodology. Code and datasets are available at\nhttps://anonymous.4open.science/r/RainfallBench-A710.", "published": "2025-09-28 03:21:24", "link": "http://arxiv.org/abs/2509.25263v1", "categories": ["cs.LG", "cs.AI", "physics.ao-ph", "stat.ML"], "primary_category": "cs.LG"}
{"title": "VoiceBridge: Designing Latent Bridge Models for General Speech Restoration at Scale", "abstract": "Bridge models have recently been explored for speech enhancement tasks such\nas denoising, dereverberation, and super-resolution, while these efforts are\ntypically confined to a single task or small-scale datasets, with constrained\ngeneral speech restoration (GSR) capability at scale. In this work, we\nintroduce VoiceBridge, a GSR system rooted in latent bridge models (LBMs),\ncapable of reconstructing high-fidelity speech at full-band (\\textit{i.e.,}\n48~kHz) from various distortions. By compressing speech waveform into\ncontinuous latent representations, VoiceBridge models the~\\textit{diverse\nLQ-to-HQ tasks} (namely, low-quality to high-quality) in GSR with~\\textit{a\nsingle latent-to-latent generative process} backed by a scalable transformer\narchitecture. To better inherit the advantages of bridge models from the data\ndomain to the latent space, we present an energy-preserving variational\nautoencoder, enhancing the alignment between the waveform and latent space over\nvarying energy levels. Furthermore, to address the difficulty of HQ\nreconstruction from distinctively different LQ priors, we propose a joint\nneural prior, uniformly alleviating the reconstruction burden of LBM. At last,\nconsidering the key requirement of GSR systems, human perceptual quality, a\nperceptually aware fine-tuning stage is designed to mitigate the cascading\nmismatch in generation while improving perceptual alignment. Extensive\nvalidation across in-domain and out-of-domain tasks and datasets\n(\\textit{e.g.}, refining recent zero-shot speech and podcast generation\nresults) demonstrates the superior performance of VoiceBridge. Demo samples can\nbe visited at: https://VoiceBridge-demo.github.io/.", "published": "2025-09-28 17:12:13", "link": "http://arxiv.org/abs/2509.25275v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sketching Low-Rank Plus Diagonal Matrices", "abstract": "Many relevant machine learning and scientific computing tasks involve\nhigh-dimensional linear operators accessible only via costly matrix-vector\nproducts. In this context, recent advances in sketched methods have enabled the\nconstruction of *either* low-rank *or* diagonal approximations from few\nmatrix-vector products. This provides great speedup and scalability, but\napproximation errors arise due to the assumed simpler structure. This work\nintroduces SKETCHLORD, a method that simultaneously estimates both low-rank\n*and* diagonal components, targeting the broader class of Low-Rank *plus*\nDiagonal (LoRD) linear operators. We demonstrate theoretically and empirically\nthat this joint estimation is superior also to any sequential variant\n(diagonal-then-low-rank or low-rank-then-diagonal). Then, we cast SKETCHLORD as\na convex optimization problem, leading to a scalable algorithm. Comprehensive\nexperiments on synthetic (approximate) LoRD matrices confirm SKETCHLORD's\nperformance in accurately recovering these structures. This positions it as a\nvaluable addition to the structured approximation toolkit, particularly when\nhigh-fidelity approximations are desired for large-scale operators, such as the\ndeep learning Hessian.", "published": "2025-09-28 02:44:16", "link": "http://arxiv.org/abs/2509.23587v2", "categories": ["cs.LG", "cs.NA", "math.NA"], "primary_category": "cs.LG"}
{"title": "A Family of Kernelized Matrix Costs for Multiple-Output Mixture Neural Networks", "abstract": "Pairwise distance-based costs are crucial for self-supervised and contrastive\nfeature learning. Mixture Density Networks (MDNs) are a widely used approach\nfor generative models and density approximation, using neural networks to\nproduce multiple centers that define a Gaussian mixture. By combining MDNs with\ncontrastive costs, this paper proposes data density approximation using four\ntypes of kernelized matrix costs: the scalar cost, the vector-matrix cost, the\nmatrix-matrix cost (the trace of Schur complement), and the SVD cost (the\nnuclear norm), for learning multiple centers required to define a mixture\ndensity.", "published": "2025-09-28 21:23:11", "link": "http://arxiv.org/abs/2509.24076v3", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
