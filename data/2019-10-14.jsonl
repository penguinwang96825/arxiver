{"title": "Estimating post-editing effort: a study on human judgements, task-based\n  and reference-based metrics of MT quality", "abstract": "Devising metrics to assess translation quality has always been at the core of\nmachine translation (MT) research. Traditional automatic reference-based\nmetrics, such as BLEU, have shown correlations with human judgements of\nadequacy and fluency and have been paramount for the advancement of MT system\ndevelopment. Crowd-sourcing has popularised and enabled the scalability of\nmetrics based on human judgements, such as subjective direct assessments (DA)\nof adequacy, that are believed to be more reliable than reference-based\nautomatic metrics. Finally, task-based measurements, such as post-editing time,\nare expected to provide a more detailed evaluation of the usefulness of\ntranslations for a specific task. Therefore, while DA averages adequacy\njudgements to obtain an appraisal of (perceived) quality independently of the\ntask, and reference-based automatic metrics try to objectively estimate quality\nalso in a task-independent way, task-based metrics are measurements obtained\neither during or after performing a specific task. In this paper we argue that,\nalthough expensive, task-based measurements are the most reliable when\nestimating MT quality in a specific task; in our case, this task is\npost-editing. To that end, we report experiments on a dataset with\nnewly-collected post-editing indicators and show their usefulness when\nestimating post-editing effort. Our results show that task-based metrics\ncomparing machine-translated and post-edited versions are the best at tracking\npost-editing effort, as expected. These metrics are followed by DA, and then by\nmetrics comparing the machine-translated version and independent references. We\nsuggest that MT practitioners should be aware of these differences and\nacknowledge their implications when deciding how to evaluate MT for\npost-editing purposes.", "published": "2019-10-14 15:20:30", "link": "http://arxiv.org/abs/1910.06204v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Updating Pre-trained Word Vectors and Text Classifiers using Monolingual\n  Alignment", "abstract": "In this paper, we focus on the problem of adapting word vector-based models\nto new textual data. Given a model pre-trained on large reference data, how can\nwe adapt it to a smaller piece of data with a slightly different language\ndistribution? We frame the adaptation problem as a monolingual word vector\nalignment problem, and simply average models after alignment. We align vectors\nusing the RCSLS criterion. Our formulation results in a simple and efficient\nalgorithm that allows adapting general-purpose models to changing word\ndistributions. In our evaluation, we consider applications to word embedding\nand text classification models. We show that the proposed approach yields good\nperformance in all setups and outperforms a baseline consisting in fine-tuning\nthe model on new data.", "published": "2019-10-14 16:19:20", "link": "http://arxiv.org/abs/1910.06241v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mapping Supervised Bilingual Word Embeddings from English to\n  low-resource languages", "abstract": "It is very challenging to work with low-resource languages due to the\ninadequate availability of data. Using a dictionary to map independently\ntrained word embeddings into a shared vector space has proved to be very useful\nin learning bilingual embeddings in the past. Here we have tried to map\nindividual embeddings of words in English and their corresponding translated\nwords in low-resource languages like Estonian, Slovenian, Slovakian, and\nHungarian. We have used a supervised learning approach. We report accuracy\nscores through various retrieval strategies which show that it is possible to\napproach challenging tasks in Natural Language Processing like machine\ntranslation for such languages, provided that we have at least some amount of\nproper bilingual data. We also conclude that we can follow an unsupervised\nlearning path on monolingual text data as that is more suitable for\nlow-resource languages.", "published": "2019-10-14 20:32:41", "link": "http://arxiv.org/abs/1910.06411v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Whatcha lookin' at? DeepLIFTing BERT's Attention in Question Answering", "abstract": "There has been great success recently in tackling challenging NLP tasks by\nneural networks which have been pre-trained and fine-tuned on large amounts of\ntask data. In this paper, we investigate one such model, BERT for\nquestion-answering, with the aim to analyze why it is able to achieve\nsignificantly better results than other models. We run DeepLIFT on the model\npredictions and test the outcomes to monitor shift in the attention values for\ninput. We also cluster the results to analyze any possible patterns similar to\nhuman reasoning depending on the kind of input paragraph and question the model\nis trying to answer.", "published": "2019-10-14 21:32:38", "link": "http://arxiv.org/abs/1910.06431v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interpretable Text Classification Using CNN and Max-pooling", "abstract": "Deep neural networks have been widely used in text classification. However,\nit is hard to interpret the neural models due to the complicate mechanisms. In\nthis work, we study the interpretability of a variant of the typical text\nclassification model which is based on convolutional operation and max-pooling\nlayer. Two mechanisms: convolution attribution and n-gram feature analysis are\nproposed to analyse the process procedure for the CNN model. The\ninterpretability of the model is reflected by providing posterior\ninterpretation for neural network predictions. Besides, a multi-sentence\nstrategy is proposed to enable the model to beused in multi-sentence situation\nwithout loss of performance and interpret ability. We evaluate the performance\nof the model on several classification tasks and justify the interpretable\nperformance with some case studies.", "published": "2019-10-14 11:52:03", "link": "http://arxiv.org/abs/1910.11236v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-guided Unsupervised Rhetorical Parsing for Text Summarization", "abstract": "Automatic text summarization (ATS) has recently achieved impressive\nperformance thanks to recent advances in deep learning and the availability of\nlarge-scale corpora. To make the summarization results more faithful, this\npaper presents an unsupervised approach that combines rhetorical structure\ntheory, deep neural model and domain knowledge concern for ATS. This\narchitecture mainly contains three components: domain knowledge base\nconstruction based on representation learning, attentional encoder-decoder\nmodel for rhetorical parsing and subroutine-based model for text summarization.\nDomain knowledge can be effectively used for unsupervised rhetorical parsing\nthus rhetorical structure trees for each document can be derived. In the\nunsupervised rhetorical parsing module, the idea of translation was adopted to\nalleviate the problem of data scarcity. The subroutine-based summarization\nmodel purely depends on the derived rhetorical structure trees and can generate\ncontent-balanced results. To evaluate the summary results without golden\nstandard, we proposed an unsupervised evaluation metric, whose hyper-parameters\nwere tuned by supervised learning. Experimental results show that, on a\nlarge-scale Chinese dataset, our proposed approach can obtain comparable\nperformances compared with existing methods.", "published": "2019-10-14 04:48:16", "link": "http://arxiv.org/abs/1910.05915v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Question Generation With to the Point Context", "abstract": "Question generation (QG) is the task of generating a question from a\nreference sentence and a specified answer within the sentence. A major\nchallenge in QG is to identify answer-relevant context words to finish the\ndeclarative-to-interrogative sentence transformation. Existing\nsequence-to-sequence neural models achieve this goal by proximity-based answer\nposition encoding under the intuition that neighboring words of answers are of\nhigh possibility to be answer-relevant. However, such intuition may not apply\nto all cases especially for sentences with complex answer-relevant relations.\nConsequently, the performance of these models drops sharply when the relative\ndistance between the answer fragment and other non-stop sentence words that\nalso appear in the ground truth question increases. To address this issue, we\npropose a method to jointly model the unstructured sentence and the structured\nanswer-relevant relation (extracted from the sentence in advance) for question\ngeneration. Specifically, the structured answer-relevant relation acts as the\nto the point context and it thus naturally helps keep the generated question to\nthe point, while the unstructured sentence provides the full information.\nExtensive experiments show that to the point context helps our question\ngeneration model achieve significant improvements on several automatic\nevaluation metrics. Furthermore, our model is capable of generating diverse\nquestions for a sentence which conveys multiple relations of its answer\nfragment.", "published": "2019-10-14 11:03:55", "link": "http://arxiv.org/abs/1910.06036v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Feature-Dependent Confusion Matrices for Low-Resource NER Labeling with\n  Noisy Labels", "abstract": "In low-resource settings, the performance of supervised labeling models can\nbe improved with automatically annotated or distantly supervised data, which is\ncheap to create but often noisy. Previous works have shown that significant\nimprovements can be reached by injecting information about the confusion\nbetween clean and noisy labels in this additional training data into the\nclassifier training. However, for noise estimation, these approaches either do\nnot take the input features (in our case word embeddings) into account, or they\nneed to learn the noise modeling from scratch which can be difficult in a\nlow-resource setting. We propose to cluster the training data using the input\nfeatures and then compute different confusion matrices for each cluster. To the\nbest of our knowledge, our approach is the first to leverage feature-dependent\nnoise modeling with pre-initialized confusion matrices. We evaluate on\nlow-resource named entity recognition settings in several languages, showing\nthat our methods improve upon other confusion-matrix based methods by up to 9%.", "published": "2019-10-14 12:03:07", "link": "http://arxiv.org/abs/1910.06061v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Q8BERT: Quantized 8Bit BERT", "abstract": "Recently, pre-trained Transformer based language models such as BERT and GPT,\nhave shown great improvement in many Natural Language Processing (NLP) tasks.\nHowever, these models contain a large amount of parameters. The emergence of\neven larger and more accurate models such as GPT2 and Megatron, suggest a trend\nof large pre-trained Transformer models. However, using these large models in\nproduction environments is a complex task requiring a large amount of compute,\nmemory and power resources. In this work we show how to perform\nquantization-aware training during the fine-tuning phase of BERT in order to\ncompress BERT by $4\\times$ with minimal accuracy loss. Furthermore, the\nproduced quantized model can accelerate inference speed if it is optimized for\n8bit Integer supporting hardware.", "published": "2019-10-14 14:55:19", "link": "http://arxiv.org/abs/1910.06188v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Restoring ancient text using deep learning: a case study on Greek\n  epigraphy", "abstract": "Ancient history relies on disciplines such as epigraphy, the study of ancient\ninscribed texts, for evidence of the recorded past. However, these texts,\n\"inscriptions\", are often damaged over the centuries, and illegible parts of\nthe text must be restored by specialists, known as epigraphists. This work\npresents Pythia, the first ancient text restoration model that recovers missing\ncharacters from a damaged text input using deep neural networks. Its\narchitecture is carefully designed to handle long-term context information, and\ndeal efficiently with missing or corrupted character and word representations.\nTo train it, we wrote a non-trivial pipeline to convert PHI, the largest\ndigital corpus of ancient Greek inscriptions, to machine actionable text, which\nwe call PHI-ML. On PHI-ML, Pythia's predictions achieve a 30.1% character error\nrate, compared to the 57.3% of human epigraphists. Moreover, in 73.5% of cases\nthe ground-truth sequence was among the Top-20 hypotheses of Pythia, which\neffectively demonstrates the impact of this assistive method on the field of\ndigital epigraphy, and sets the state-of-the-art in ancient text restoration.", "published": "2019-10-14 16:43:00", "link": "http://arxiv.org/abs/1910.06262v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Training Compact Models for Low Resource Entity Tagging using\n  Pre-trained Language Models", "abstract": "Training models on low-resource named entity recognition tasks has been shown\nto be a challenge, especially in industrial applications where deploying\nupdated models is a continuous effort and crucial for business operations. In\nsuch cases there is often an abundance of unlabeled data, while labeled data is\nscarce or unavailable. Pre-trained language models trained to extract\ncontextual features from text were shown to improve many natural language\nprocessing (NLP) tasks, including scarcely labeled tasks, by leveraging\ntransfer learning. However, such models impose a heavy memory and computational\nburden, making it a challenge to train and deploy such models for inference\nuse. In this work-in-progress we combined the effectiveness of transfer\nlearning provided by pre-trained masked language models with a semi-supervised\napproach to train a fast and compact model using labeled and unlabeled\nexamples. Preliminary evaluations show that the compact models can achieve\ncompetitive accuracy with 36x compression rate when compared with a\nstate-of-the-art pre-trained language model, and run significantly faster in\ninference, allowing deployment of such models in production environments or on\nedge devices.", "published": "2019-10-14 17:22:37", "link": "http://arxiv.org/abs/1910.06294v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Structured Pruning of a BERT-based Question Answering Model", "abstract": "The recent trend in industry-setting Natural Language Processing (NLP)\nresearch has been to operate large %scale pretrained language models like BERT\nunder strict computational limits. While most model compression work has\nfocused on \"distilling\" a general-purpose language representation using\nexpensive pretraining distillation, less attention has been paid to creating\nsmaller task-specific language representations which, arguably, are more useful\nin an industry setting. In this paper, we investigate compressing BERT- and\nRoBERTa-based question answering systems by structured pruning of parameters\nfrom the underlying transformer model. We find that an inexpensive combination\nof task-specific structured pruning and task-specific distillation, without the\nexpense of pretraining distillation, yields highly-performing models across a\nrange of speed/accuracy tradeoff operating points. We start from existing\nfull-size models trained for SQuAD 2.0 or Natural Questions and introduce gates\nthat allow selected parts of transformers to be individually eliminated.\nSpecifically, we investigate (1) structured pruning to reduce the number of\nparameters in each transformer layer, (2) applicability to both BERT- and\nRoBERTa-based models, (3) applicability to both SQuAD 2.0 and Natural\nQuestions, and (4) combining structured pruning with distillation. We achieve a\nnear-doubling of inference speed with less than a 0.5 F1-point loss in short\nanswer accuracy on Natural Questions.", "published": "2019-10-14 18:12:30", "link": "http://arxiv.org/abs/1910.06360v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tell-the-difference: Fine-grained Visual Descriptor via a Discriminating\n  Referee", "abstract": "In this paper, we investigate a novel problem of telling the difference\nbetween image pairs in natural language. Compared to previous approaches for\nsingle image captioning, it is challenging to fetch linguistic representation\nfrom two independent visual information. To this end, we have proposed an\neffective encoder-decoder caption framework based on Hyper Convolution Net. In\naddition, a series of novel feature fusing techniques for pairwise visual\ninformation fusing are introduced and a discriminating referee is proposed to\nevaluate the pipeline. Because of the lack of appropriate datasets to support\nthis task, we have collected and annotated a large new dataset with Amazon\nMechanical Turk (AMT) for generating captions in a pairwise manner (with 14764\nimages and 26710 image pairs in total). The dataset is the first one on the\nrelative difference caption task that provides descriptions in free language.\nWe evaluate the effectiveness of our model on two datasets in the field and it\noutperforms the state-of-the-art approach by a large margin.", "published": "2019-10-14 21:21:03", "link": "http://arxiv.org/abs/1910.06426v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Transformers without Tears: Improving the Normalization of\n  Self-Attention", "abstract": "We evaluate three simple, normalization-centric changes to improve\nTransformer training. First, we show that pre-norm residual connections\n(PreNorm) and smaller initializations enable warmup-free, validation-based\ntraining with large learning rates. Second, we propose $\\ell_2$ normalization\nwith a single scale parameter (ScaleNorm) for faster training and better\nperformance. Finally, we reaffirm the effectiveness of normalizing word\nembeddings to a fixed length (FixNorm). On five low-resource translation pairs\nfrom TED Talks-based corpora, these changes always converge, giving an average\n+1.1 BLEU over state-of-the-art bilingual baselines and a new 32.8 BLEU on\nIWSLT'15 English-Vietnamese. We observe sharper performance curves, more\nconsistent gradient norms, and a linear relationship between activation scaling\nand decoder depth. Surprisingly, in the high-resource setting (WMT'14\nEnglish-German), ScaleNorm and FixNorm remain competitive but PreNorm degrades\nperformance.", "published": "2019-10-14 02:23:43", "link": "http://arxiv.org/abs/1910.05895v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "STANCY: Stance Classification Based on Consistency Cues", "abstract": "Controversial claims are abundant in online media and discussion forums. A\nbetter understanding of such claims requires analyzing them from different\nperspectives. Stance classification is a necessary step for inferring these\nperspectives in terms of supporting or opposing the claim. In this work, we\npresent a neural network model for stance classification leveraging BERT\nrepresentations and augmenting them with a novel consistency constraint.\nExperiments on the Perspectrum dataset, consisting of claims and users'\nperspectives from various debate websites, demonstrate the effectiveness of our\napproach over state-of-the-art baselines.", "published": "2019-10-14 11:39:45", "link": "http://arxiv.org/abs/1910.06048v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dynamic Attention Networks for Task Oriented Grounding", "abstract": "In order to successfully perform tasks specified by natural language\ninstructions, an artificial agent operating in a visual world needs to map\nwords, concepts, and actions from the instruction to visual elements in its\nenvironment. This association is termed as Task-Oriented Grounding. In this\nwork, we propose a novel Dynamic Attention Network architecture for the\nefficient multi-modal fusion of text and visual representations which can\ngenerate a robust definition of state for the policy learner. Our model assumes\nno prior knowledge from visual and textual domains and is an end to end\ntrainable. For a 3D visual world where the observation changes continuously,\nthe attention on the visual elements tends to be highly co-related from a\none-time step to the next. We term this as \"Dynamic Attention\". In this work,\nwe show that Dynamic Attention helps in achieving grounding and also aids in\nthe policy learning objective. Since most practical robotic applications take\nplace in the real world where the observation space is continuous, our\nframework can be used as a generalized multi-modal fusion unit for robotic\ncontrol through natural language. We show the effectiveness of using 1D\nconvolution over Gated Attention Hadamard product on the rate of convergence of\nthe network. We demonstrate that the cell-state of a Long Short Term Memory\n(LSTM) is a natural choice for modeling Dynamic Attention and shows through\nvisualization that the generated attention is very close to how humans tend to\nfocus on the environment.", "published": "2019-10-14 17:57:21", "link": "http://arxiv.org/abs/1910.06315v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "The Sounds of Music : Science of Musical Scales III -- Indian Classical", "abstract": "In the previous articles of this series, we have discussed the development of\nmusical scales particularly that of the heptatonic scale which forms the basis\nof Western classical music today. In this last article, we take a look at the\nbasic structure of scales used in Indian classical music and how different\n`raga's are generated through the simple process of scale shifting.", "published": "2019-10-14 18:41:42", "link": "http://arxiv.org/abs/1910.06375v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Theory behind Controllable Expressive Speech Synthesis: a\n  Cross-disciplinary Approach", "abstract": "As part of the Human-Computer Interaction field, Expressive speech synthesis\nis a very rich domain as it requires knowledge in areas such as machine\nlearning, signal processing, sociology, psychology. In this Chapter, we will\nfocus mostly on the technical side. From the recording of expressive speech to\nits modeling, the reader will have an overview of the main paradigms used in\nthis field, through some of the most prominent systems and methods. We explain\nhow speech can be represented and encoded with audio features. We present a\nhistory of the main methods of Text-to-Speech synthesis: concatenative,\nparametric and statistical parametric speech synthesis. Finally, we focus on\nthe last one, with the last techniques modeling Text-to-Speech synthesis as a\nsequence-to-sequence problem. This enables the use of Deep Learning blocks such\nas Convolutional and Recurrent Neural Networks as well as Attention Mechanism.\nThe last part of the Chapter intends to assemble the different aspects of the\ntheory and summarize the concepts.", "published": "2019-10-14 16:08:33", "link": "http://arxiv.org/abs/1910.06234v1", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dual-path RNN: efficient long sequence modeling for time-domain\n  single-channel speech separation", "abstract": "Recent studies in deep learning-based speech separation have proven the\nsuperiority of time-domain approaches to conventional time-frequency-based\nmethods. Unlike the time-frequency domain approaches, the time-domain\nseparation systems often receive input sequences consisting of a huge number of\ntime steps, which introduces challenges for modeling extremely long sequences.\nConventional recurrent neural networks (RNNs) are not effective for modeling\nsuch long sequences due to optimization difficulties, while one-dimensional\nconvolutional neural networks (1-D CNNs) cannot perform utterance-level\nsequence modeling when its receptive field is smaller than the sequence length.\nIn this paper, we propose dual-path recurrent neural network (DPRNN), a simple\nyet effective method for organizing RNN layers in a deep structure to model\nextremely long sequences. DPRNN splits the long sequential input into smaller\nchunks and applies intra- and inter-chunk operations iteratively, where the\ninput length can be made proportional to the square root of the original\nsequence length in each operation. Experiments show that by replacing 1-D CNN\nwith DPRNN and apply sample-level modeling in the time-domain audio separation\nnetwork (TasNet), a new state-of-the-art performance on WSJ0-2mix is achieved\nwith a 20 times smaller model than the previous best system.", "published": "2019-10-14 18:52:49", "link": "http://arxiv.org/abs/1910.06379v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Low Bit-Rate Speech Coding with VQ-VAE and a WaveNet Decoder", "abstract": "In order to efficiently transmit and store speech signals, speech codecs\ncreate a minimally redundant representation of the input signal which is then\ndecoded at the receiver with the best possible perceptual quality. In this work\nwe demonstrate that a neural network architecture based on VQ-VAE with a\nWaveNet decoder can be used to perform very low bit-rate speech coding with\nhigh reconstruction quality. A prosody-transparent and speaker-independent\nmodel trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits\nperceptual quality which is around halfway between the MELP codec at 2.4 kbps\nand AMR-WB codec at 23.05 kbps. In addition, when training on high-quality\nrecorded speech with the test speaker included in the training set, a model\ncoding speech at 1.6 kbps produces output of similar perceptual quality to that\ngenerated by AMR-WB at 23.05 kbps.", "published": "2019-10-14 23:54:08", "link": "http://arxiv.org/abs/1910.06464v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Acoustic Scene Classification Based on a Large-margin Factorized CNN", "abstract": "In this paper, we present an acoustic scene classification framework based on\na large-margin factorized convolutional neural network (CNN). We adopt the\nfactorized CNN to learn the patterns in the time-frequency domain by\nfactorizing the 2D kernel into two separate 1D kernels. The factorized kernel\nleads to learn the main component of two patterns: the long-term ambient and\nshort-term event sounds which are the key patterns of the audio scene\nclassification. In training our model, we consider the loss function based on\nthe triplet sampling such that the same audio scene samples from different\nenvironments are minimized, and simultaneously the different audio scene\nsamples are maximized. With this loss function, the samples from the same audio\nscene are clustered independently of the environment, and thus we can get the\nclassifier with better generalization ability in an unseen environment. We\nevaluated our audio scene classification framework using the dataset of the\nDCASE challenge 2019 task1A. Experimental results show that the proposed\nalgorithm improves the performance of the baseline network and reduces the\nnumber of parameters to one third. Furthermore, the performance gain is higher\non unseen data, and it shows that the proposed algorithm has better\ngeneralization ability.", "published": "2019-10-14 07:47:15", "link": "http://arxiv.org/abs/1910.06784v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Weakly Labeled Sound Event Detection Using Tri-training and Adversarial\n  Learning", "abstract": "This paper considers a semi-supervised learning framework for weakly labeled\npolyphonic sound event detection problems for the DCASE 2019 challenge's task4\nby combining both the tri-training and adversarial learning. The goal of the\ntask4 is to detect onsets and offsets of multiple sound events in a single\naudio clip. The entire dataset consists of the synthetic data with a strong\nlabel (sound event labels with boundaries) and real data with weakly labeled\n(sound event labels) and unlabeled dataset. Given this dataset, we apply the\ntri-training where two different classifiers are used to obtain pseudo labels\non the weakly labeled and unlabeled dataset, and the final classifier is\ntrained using the strongly labeled dataset and weakly/unlabeled dataset with\npseudo labels. Also, we apply the adversarial learning to reduce the domain gap\nbetween the real and synthetic dataset. We evaluated our learning framework\nusing the validation set of the task4 dataset, and in the experiments, our\nlearning framework shows a considerable performance improvement over the\nbaseline model.", "published": "2019-10-14 07:47:55", "link": "http://arxiv.org/abs/1910.06790v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
