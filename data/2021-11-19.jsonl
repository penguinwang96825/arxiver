{"title": "Lexicon-based Methods vs. BERT for Text Sentiment Analysis", "abstract": "The performance of sentiment analysis methods has greatly increased in recent\nyears. This is due to the use of various models based on the Transformer\narchitecture, in particular BERT. However, deep neural network models are\ndifficult to train and poorly interpretable. An alternative approach is\nrule-based methods using sentiment lexicons. They are fast, require no\ntraining, and are well interpreted. But recently, due to the widespread use of\ndeep learning, lexicon-based methods have receded into the background. The\npurpose of the article is to study the performance of the SO-CAL and\nSentiStrength lexicon-based methods, adapted for the Russian language. We have\ntested these methods, as well as the RuBERT neural network model, on 16 text\ncorpora and have analyzed their results. RuBERT outperforms both lexicon-based\nmethods on average, but SO-CAL surpasses RuBERT for four corpora out of 16.", "published": "2021-11-19 08:47:32", "link": "http://arxiv.org/abs/2111.10097v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does BERT look at sentiment lexicon?", "abstract": "The main approaches to sentiment analysis are rule-based methods and ma-chine\nlearning, in particular, deep neural network models with the Trans-former\narchitecture, including BERT. The performance of neural network models in the\ntasks of sentiment analysis is superior to the performance of rule-based\nmethods. The reasons for this situation remain unclear due to the poor\ninterpretability of deep neural network models. One of the main keys to\nunderstanding the fundamental differences between the two approaches is the\nanalysis of how sentiment lexicon is taken into account in neural network\nmodels. To this end, we study the attention weights matrices of the\nRussian-language RuBERT model. We fine-tune RuBERT on sentiment text corpora\nand compare the distributions of attention weights for sentiment and neutral\nlexicons. It turns out that, on average, 3/4 of the heads of various model\nvar-iants statistically pay more attention to the sentiment lexicon compared to\nthe neutral one.", "published": "2021-11-19 08:50:48", "link": "http://arxiv.org/abs/2111.10100v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Between welcome culture and border fence. A dataset on the European\n  refugee crisis in German newspaper reports", "abstract": "Newspaper reports provide a rich source of information on the unfolding of\npublic debate on specific policy fields that can serve as basis for inquiry in\npolitical science. Such debates are often triggered by critical events, which\nattract public attention and incite the reactions of political actors: crisis\nsparks the debate. However, due to the challenges of reliable annotation and\nmodeling, few large-scale datasets with high-quality annotation are available.\nThis paper introduces DebateNet2.0, which traces the political discourse on the\nEuropean refugee crisis in the German quality newspaper taz during the year\n2015. The core units of our annotation are political claims (requests for\nspecific actions to be taken within the policy field) and the actors who make\nthem (politicians, parties, etc.). The contribution of this paper is twofold.\nFirst, we document and release DebateNet2.0 along with its companion R package,\nmardyR, guiding the reader through the practical and conceptual issues related\nto the annotation of policy debates in newspapers. Second, we outline and apply\na Discourse Network Analysis (DNA) to DebateNet2.0, comparing two crucial\nmoments of the policy debate on the 'refugee crisis': the migration flux\nthrough the Mediterranean in April/May and the one along the Balkan route in\nSeptember/October. Besides the released resources and the case-study, our\ncontribution is also methodological: we talk the reader through the steps from\na newspaper article to a discourse network, demonstrating that there is not\njust one discourse network for the German migration debate, but multiple ones,\ndepending on the topic of interest (political actors, policy fields, time\nspans).", "published": "2021-11-19 10:34:23", "link": "http://arxiv.org/abs/2111.10142v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toxicity Detection can be Sensitive to the Conversational Context", "abstract": "User posts whose perceived toxicity depends on the conversational context are\nrare in current toxicity detection datasets. Hence, toxicity detectors trained\non existing datasets will also tend to disregard context, making the detection\nof context-sensitive toxicity harder when it does occur. We construct and\npublicly release a dataset of 10,000 posts with two kinds of toxicity labels:\n(i) annotators considered each post with the previous one as context; and (ii)\nannotators had no additional context. Based on this, we introduce a new task,\ncontext sensitivity estimation, which aims to identify posts whose perceived\ntoxicity changes if the context (previous post) is also considered. We then\nevaluate machine learning systems on this task, showing that classifiers of\npractical quality can be developed, and we show that data augmentation with\nknowledge distillation can improve the performance further. Such systems could\nbe used to enhance toxicity detection datasets with more context-dependent\nposts, or to suggest when moderators should consider the parent posts, which\noften may be unnecessary and may otherwise introduce significant additional\ncost.", "published": "2021-11-19 13:57:26", "link": "http://arxiv.org/abs/2111.10223v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The ComMA Dataset V0.2: Annotating Aggression and Bias in Multilingual\n  Social Media Discourse", "abstract": "In this paper, we discuss the development of a multilingual dataset annotated\nwith a hierarchical, fine-grained tagset marking different types of aggression\nand the \"context\" in which they occur. The context, here, is defined by the\nconversational thread in which a specific comment occurs and also the \"type\" of\ndiscursive role that the comment is performing with respect to the previous\ncomment. The initial dataset, being discussed here (and made available as part\nof the ComMA@ICON shared task), consists of a total 15,000 annotated comments\nin four languages - Meitei, Bangla, Hindi, and Indian English - collected from\nvarious social media platforms such as YouTube, Facebook, Twitter and Telegram.\nAs is usual on social media websites, a large number of these comments are\nmultilingual, mostly code-mixed with English. The paper gives a detailed\ndescription of the tagset being used for annotation and also the process of\ndeveloping a multi-label, fine-grained tagset that can be used for marking\ncomments with aggression and bias of various kinds including gender bias,\nreligious intolerance (called communal bias in the tagset), class/caste bias\nand ethnic/racial bias. We also define and discuss the tags that have been used\nfor marking different the discursive role being performed through the comments,\nsuch as attack, defend, etc. We also present a statistical analysis of the\ndataset as well as results of our baseline experiments with developing an\nautomatic aggression identification system using the dataset developed.", "published": "2021-11-19 19:03:22", "link": "http://arxiv.org/abs/2111.10390v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building a Question Answering System for the Manufacturing Domain", "abstract": "The design or simulation analysis of special equipment products must follow\nthe national standards, and hence it may be necessary to repeatedly consult the\ncontents of the standards in the design process. However, it is difficult for\nthe traditional question answering system based on keyword retrieval to give\naccurate answers to technical questions. Therefore, we use natural language\nprocessing techniques to design a question answering system for the\ndecision-making process in pressure vessel design. To solve the problem of\ninsufficient training data for the technology question answering system, we\npropose a method to generate questions according to a declarative sentence from\nseveral different dimensions so that multiple question-answer pairs can be\nobtained from a declarative sentence. In addition, we designed an interactive\nattention model based on a bidirectional long short-term memory (BiLSTM)\nnetwork to improve the performance of the similarity comparison of two question\nsentences. Finally, the performance of the question answering system was tested\non public and technical domain datasets.", "published": "2021-11-19 04:52:45", "link": "http://arxiv.org/abs/2111.10044v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "More than Words: In-the-Wild Visually-Driven Prosody for Text-to-Speech", "abstract": "In this paper we present VDTTS, a Visually-Driven Text-to-Speech model.\nMotivated by dubbing, VDTTS takes advantage of video frames as an additional\ninput alongside text, and generates speech that matches the video signal. We\ndemonstrate how this allows VDTTS to, unlike plain TTS models, generate speech\nthat not only has prosodic variations like natural pauses and pitch, but is\nalso synchronized to the input video. Experimentally, we show our model\nproduces well-synchronized outputs, approaching the video-speech\nsynchronization quality of the ground-truth, on several challenging benchmarks\nincluding \"in-the-wild\" content from VoxCeleb2. Supplementary demo videos\ndemonstrating video-speech synchronization, robustness to speaker ID swapping,\nand prosody, presented at the project page.", "published": "2021-11-19 10:23:38", "link": "http://arxiv.org/abs/2111.10139v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Small Changes Make Big Differences: Improving Multi-turn Response\n  Selection in Dialogue Systems via Fine-Grained Contrastive Learning", "abstract": "Retrieve-based dialogue response selection aims to find a proper response\nfrom a candidate set given a multi-turn context. Pre-trained language models\n(PLMs) based methods have yielded significant improvements on this task. The\nsequence representation plays a key role in the learning of matching degree\nbetween the dialogue context and the response. However, we observe that\ndifferent context-response pairs sharing the same context always have a greater\nsimilarity in the sequence representations calculated by PLMs, which makes it\nhard to distinguish positive responses from negative ones. Motivated by this,\nwe propose a novel \\textbf{F}ine-\\textbf{G}rained \\textbf{C}ontrastive (FGC)\nlearning method for the response selection task based on PLMs. This FGC\nlearning strategy helps PLMs to generate more distinguishable matching\nrepresentations of each dialogue at fine grains, and further make better\npredictions on choosing positive responses. Empirical studies on two benchmark\ndatasets demonstrate that the proposed FGC learning method can generally and\nsignificantly improve the model performance of existing PLM-based matching\nmodels.", "published": "2021-11-19 11:07:07", "link": "http://arxiv.org/abs/2111.10154v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Pointer over Attention: An Improved Bangla Text Summarization Approach\n  Using Hybrid Pointer Generator Network", "abstract": "Despite the success of the neural sequence-to-sequence model for abstractive\ntext summarization, it has a few shortcomings, such as repeating inaccurate\nfactual details and tending to repeat themselves. We propose a hybrid pointer\ngenerator network to solve the shortcomings of reproducing factual details\ninadequately and phrase repetition. We augment the attention-based\nsequence-to-sequence using a hybrid pointer generator network that can generate\nOut-of-Vocabulary words and enhance accuracy in reproducing authentic details\nand a coverage mechanism that discourages repetition. It produces a\nreasonable-sized output text that preserves the conceptual integrity and\nfactual information of the input article. For evaluation, we primarily employed\n\"BANSData\" - a highly adopted publicly available Bengali dataset. Additionally,\nwe prepared a large-scale dataset called \"BANS-133\" which consists of 133k\nBangla news articles associated with human-generated summaries. Experimenting\nwith the proposed model, we achieved ROUGE-1 and ROUGE-2 scores of 0.66, 0.41\nfor the \"BANSData\" dataset and 0.67, 0.42 for the BANS-133k\" dataset,\nrespectively. We demonstrated that the proposed system surpasses previous\nstate-of-the-art Bengali abstractive summarization techniques and its stability\non a larger dataset. \"BANS-133\" datasets and code-base will be publicly\navailable for research.", "published": "2021-11-19 15:18:12", "link": "http://arxiv.org/abs/2111.10269v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semi-supervised transfer learning for language expansion of end-to-end\n  speech recognition models to low-resource languages", "abstract": "In this paper, we propose a three-stage training methodology to improve the\nspeech recognition accuracy of low-resource languages. We explore and propose\nan effective combination of techniques such as transfer learning, encoder\nfreezing, data augmentation using Text-To-Speech (TTS), and Semi-Supervised\nLearning (SSL). To improve the accuracy of a low-resource Italian ASR, we\nleverage a well-trained English model, unlabeled text corpus, and unlabeled\naudio corpus using transfer learning, TTS augmentation, and SSL respectively.\nIn the first stage, we use transfer learning from a well-trained English model.\nThis primarily helps in learning the acoustic information from a resource-rich\nlanguage. This stage achieves around 24% relative Word Error Rate (WER)\nreduction over the baseline. In stage two, We utilize unlabeled text data via\nTTS data-augmentation to incorporate language information into the model. We\nalso explore freezing the acoustic encoder at this stage. TTS data augmentation\nhelps us further reduce the WER by ~ 21% relatively. Finally, In stage three we\nreduce the WER by another 4% relative by using SSL from unlabeled audio data.\nOverall, our two-pass speech recognition system with a Monotonic Chunkwise\nAttention (MoChA) in the first pass and a full-attention in the second pass\nachieves a WER reduction of ~ 42% relative to the baseline.", "published": "2021-11-19 05:09:16", "link": "http://arxiv.org/abs/2111.10047v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Combined Scaling for Zero-shot Transfer Learning", "abstract": "We present a combined scaling method - named BASIC - that achieves 85.7%\ntop-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from\nany labeled ImageNet example. This accuracy surpasses best published similar\nmodels - CLIP and ALIGN - by 9.3%. Our BASIC model also shows significant\nimprovements in robustness benchmarks. For instance, on 5 test sets with\nnatural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our\nmodel achieves 84.3% top-1 average accuracy, only a small drop from its\noriginal ImageNet accuracy. To achieve these results, we scale up the\ncontrastive learning framework of CLIP and ALIGN in three dimensions: data\nsize, model size, and batch size. Our dataset has 6.6B noisy image-text pairs,\nwhich is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has\n3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs than\nALIGN and CLIP. Finally, our batch size is 65536 which is 2x more than CLIP and\n4x more than ALIGN. We encountered two main challenges with the scaling rules\nof BASIC. First, the main challenge with implementing the combined scaling\nrules of BASIC is the limited memory of accelerators, such as GPUs and TPUs. To\novercome the memory limit, we propose two simple methods which make use of\ngradient checkpointing and model parallelism. Second, while increasing the\ndataset size and the model size has been the defacto method to improve the\nperformance of deep learning models like BASIC, the effect of a large\ncontrastive batch size on such contrastive-trained image-text models is not\nwell-understood. To shed light on the benefits of large contrastive batch\nsizes, we develop a theoretical framework which shows that larger contrastive\nbatch sizes lead to smaller generalization gaps for image-text models such as\nBASIC.", "published": "2021-11-19 05:25:46", "link": "http://arxiv.org/abs/2111.10050v3", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "DeepQR: Neural-based Quality Ratings for Learnersourced Multiple-Choice\n  Questions", "abstract": "Automated question quality rating (AQQR) aims to evaluate question quality\nthrough computational means, thereby addressing emerging challenges in online\nlearnersourced question repositories. Existing methods for AQQR rely solely on\nexplicitly-defined criteria such as readability and word count, while not fully\nutilising the power of state-of-the-art deep-learning techniques. We propose\nDeepQR, a novel neural-network model for AQQR that is trained using\nmultiple-choice-question (MCQ) datasets collected from PeerWise, a widely-used\nlearnersourcing platform. Along with designing DeepQR, we investigate models\nbased on explicitly-defined features, or semantic features, or both. We also\nintroduce a self-attention mechanism to capture semantic correlations between\nMCQ components, and a contrastive-learning approach to acquire question\nrepresentations using quality ratings. Extensive experiments on datasets\ncollected from eight university-level courses illustrate that DeepQR has\nsuperior performance over six comparative models.", "published": "2021-11-19 05:58:39", "link": "http://arxiv.org/abs/2111.10058v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lattention: Lattice-attention in ASR rescoring", "abstract": "Lattices form a compact representation of multiple hypotheses generated from\nan automatic speech recognition system and have been shown to improve\nperformance of downstream tasks like spoken language understanding and speech\ntranslation, compared to using one-best hypothesis. In this work, we look into\nthe effectiveness of lattice cues for rescoring n-best lists in second-pass. We\nencode lattices with a recurrent network and train an attention encoder-decoder\nmodel for n-best rescoring. The rescoring model with attention to lattices\nachieves 4-5% relative word error rate reduction over first-pass and 6-8% with\nattention to both lattices and acoustic features. We show that rescoring models\nwith attention to lattices outperform models with attention to n-best\nhypotheses. We also study different ways to incorporate lattice weights in the\nlattice encoder and demonstrate their importance for n-best rescoring.", "published": "2021-11-19 11:13:37", "link": "http://arxiv.org/abs/2111.10157v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Improved Prosodic Clustering for Multispeaker and Speaker-independent\n  Phoneme-level Prosody Control", "abstract": "This paper presents a method for phoneme-level prosody control of F0 and\nduration on a multispeaker text-to-speech setup, which is based on prosodic\nclustering. An autoregressive attention-based model is used, incorporating\nmultispeaker architecture modules in parallel to a prosody encoder. Several\nimprovements over the basic single-speaker method are proposed that increase\nthe prosodic control range and coverage. More specifically we employ data\naugmentation, F0 normalization, balanced clustering for duration, and\nspeaker-independent prosodic clustering. These modifications enable\nfine-grained phoneme-level prosody control for all speakers contained in the\ntraining set, while maintaining the speaker identity. The model is also\nfine-tuned to unseen speakers with limited amounts of data and it is shown to\nmaintain its prosody control capabilities, verifying that the\nspeaker-independent prosodic clustering is effective. Experimental results\nverify that the model maintains high output speech quality and that the\nproposed method allows efficient prosody control within each speaker's range\ndespite the variability that a multispeaker setting introduces.", "published": "2021-11-19 11:43:59", "link": "http://arxiv.org/abs/2111.10168v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Word-Level Style Control for Expressive, Non-attentive Speech Synthesis", "abstract": "This paper presents an expressive speech synthesis architecture for modeling\nand controlling the speaking style at a word level. It attempts to learn\nword-level stylistic and prosodic representations of the speech data, with the\naid of two encoders. The first one models style by finding a combination of\nstyle tokens for each word given the acoustic features, and the second outputs\na word-level sequence conditioned only on the phonetic information in order to\ndisentangle it from the style information. The two encoder outputs are aligned\nand concatenated with the phoneme encoder outputs and then decoded with a\nNon-Attentive Tacotron model. An extra prior encoder is used to predict the\nstyle tokens autoregressively, in order for the model to be able to run without\na reference utterance. We find that the resulting model gives both word-level\nand global control over the style, as well as prosody transfer capabilities.", "published": "2021-11-19 12:03:53", "link": "http://arxiv.org/abs/2111.10173v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Prosodic Clustering for Phoneme-level Prosody Control in End-to-End\n  Speech Synthesis", "abstract": "This paper presents a method for controlling the prosody at the phoneme level\nin an autoregressive attention-based text-to-speech system. Instead of learning\nlatent prosodic features with a variational framework as is commonly done, we\ndirectly extract phoneme-level F0 and duration features from the speech data in\nthe training set. Each prosodic feature is discretized using unsupervised\nclustering in order to produce a sequence of prosodic labels for each\nutterance. This sequence is used in parallel to the phoneme sequence in order\nto condition the decoder with the utilization of a prosodic encoder and a\ncorresponding attention module. Experimental results show that the proposed\nmethod retains the high quality of generated speech, while allowing\nphoneme-level control of F0 and duration. By replacing the F0 cluster centroids\nwith musical notes, the model can also provide control over the note and octave\nwithin the range of the speaker.", "published": "2021-11-19 12:10:16", "link": "http://arxiv.org/abs/2111.10177v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SLUE: New Benchmark Tasks for Spoken Language Understanding Evaluation\n  on Natural Speech", "abstract": "Progress in speech processing has been facilitated by shared datasets and\nbenchmarks. Historically these have focused on automatic speech recognition\n(ASR), speaker identification, or other lower-level tasks. Interest has been\ngrowing in higher-level spoken language understanding tasks, including using\nend-to-end models, but there are fewer annotated datasets for such tasks. At\nthe same time, recent work shows the possibility of pre-training generic\nrepresentations and then fine-tuning for several tasks using relatively little\nlabeled data. We propose to create a suite of benchmark tasks for Spoken\nLanguage Understanding Evaluation (SLUE) consisting of limited-size labeled\ntraining sets and corresponding evaluation sets. This resource would allow the\nresearch community to track progress, evaluate pre-trained representations for\nhigher-level tasks, and study open questions such as the utility of pipeline\nversus end-to-end approaches. We present the first phase of the SLUE benchmark\nsuite, consisting of named entity recognition, sentiment analysis, and ASR on\nthe corresponding datasets. We focus on naturally produced (not read or\nsynthesized) speech, and freely available datasets. We provide new\ntranscriptions and annotations on subsets of the VoxCeleb and VoxPopuli\ndatasets, evaluation metrics and results for baseline models, and an\nopen-source toolkit to reproduce the baselines and evaluate new models.", "published": "2021-11-19 18:59:23", "link": "http://arxiv.org/abs/2111.10367v3", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A comparison of streaming models and data augmentation methods for\n  robust speech recognition", "abstract": "In this paper, we present a comparative study on the robustness of two\ndifferent online streaming speech recognition models: Monotonic Chunkwise\nAttention (MoChA) and Recurrent Neural Network-Transducer (RNN-T). We explore\nthree recently proposed data augmentation techniques, namely, multi-conditioned\ntraining using an acoustic simulator, Vocal Tract Length Perturbation (VTLP)\nfor speaker variability, and SpecAugment. Experimental results show that\nunidirectional models are in general more sensitive to noisy examples in the\ntraining set. It is observed that the final performance of the model depends on\nthe proportion of training examples processed by data augmentation techniques.\nMoChA models generally perform better than RNN-T models. However, we observe\nthat training of MoChA models seems to be more sensitive to various factors\nsuch as the characteristics of training sets and the incorporation of\nadditional augmentations techniques. On the other hand, RNN-T models perform\nbetter than MoChA models in terms of latency, inference time, and the stability\nof training. Additionally, RNN-T models are generally more robust against noise\nand reverberation. All these advantages make RNN-T models a better choice for\nstreaming on-device speech recognition compared to MoChA models.", "published": "2021-11-19 04:49:49", "link": "http://arxiv.org/abs/2111.10043v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Differentiable Wavetable Synthesis", "abstract": "Differentiable Wavetable Synthesis (DWTS) is a technique for neural audio\nsynthesis which learns a dictionary of one-period waveforms i.e. wavetables,\nthrough end-to-end training. We achieve high-fidelity audio synthesis with as\nlittle as 10 to 20 wavetables and demonstrate how a data-driven dictionary of\nwaveforms opens up unprecedented one-shot learning paradigms on short audio\nclips. Notably, we show audio manipulations, such as high quality\npitch-shifting, using only a few seconds of input audio. Lastly, we investigate\nperformance gains from using learned wavetables for realtime and interactive\naudio synthesis.", "published": "2021-11-19 01:42:42", "link": "http://arxiv.org/abs/2111.10003v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Interpreting deep urban sound classification using Layer-wise Relevance\n  Propagation", "abstract": "After constructing a deep neural network for urban sound classification, this\nwork focuses on the sensitive application of assisting drivers suffering from\nhearing loss. As such, clear etiology justifying and interpreting model\npredictions comprise a strong requirement. To this end, we used two different\nrepresentations of audio signals, i.e. Mel and constant-Q spectrograms, while\nthe decisions made by the deep neural network are explained via layer-wise\nrelevance propagation. At the same time, frequency content assigned with high\nrelevance in both feature sets, indicates extremely discriminative information\ncharacterizing the present classification task. Overall, we present an\nexplainable AI framework for understanding deep urban sound classification.", "published": "2021-11-19 14:15:45", "link": "http://arxiv.org/abs/2111.10235v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
