{"title": "Transfer Learning for Sequence Tagging with Hierarchical Recurrent\n  Networks", "abstract": "Recent papers have shown that neural networks obtain state-of-the-art\nperformance on several different sequence tagging tasks. One appealing property\nof such systems is their generality, as excellent performance can be achieved\nwith a unified architecture and without task-specific feature engineering.\nHowever, it is unclear if such systems can be used for tasks without large\namounts of training data. In this paper we explore the problem of transfer\nlearning for neural sequence taggers, where a source task with plentiful\nannotations (e.g., POS tagging on Penn Treebank) is used to improve performance\non a target task with fewer available annotations (e.g., POS tagging for\nmicroblogs). We examine the effects of transfer learning for deep hierarchical\nrecurrent networks across domains, applications, and languages, and show that\nsignificant improvement can often be obtained. These improvements lead to\nimprovements over the current state-of-the-art on several well-studied tasks.", "published": "2017-03-18 20:21:44", "link": "http://arxiv.org/abs/1703.06345v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-talker Speech Separation with Utterance-level Permutation\n  Invariant Training of Deep Recurrent Neural Networks", "abstract": "In this paper we propose the utterance-level Permutation Invariant Training\n(uPIT) technique. uPIT is a practically applicable, end-to-end, deep learning\nbased solution for speaker independent multi-talker speech separation.\nSpecifically, uPIT extends the recently proposed Permutation Invariant Training\n(PIT) technique with an utterance-level cost function, hence eliminating the\nneed for solving an additional permutation problem during inference, which is\notherwise required by frame-level PIT. We achieve this using Recurrent Neural\nNetworks (RNNs) that, during training, minimize the utterance-level separation\nerror, hence forcing separated frames belonging to the same speaker to be\naligned to the same output stream. In practice, this allows RNNs, trained with\nuPIT, to separate multi-talker mixed speech without any prior knowledge of\nsignal duration, number of speakers, speaker identity or gender. We evaluated\nuPIT on the WSJ0 and Danish two- and three-talker mixed-speech separation tasks\nand found that uPIT outperforms techniques based on Non-negative Matrix\nFactorization (NMF) and Computational Auditory Scene Analysis (CASA), and\ncompares favorably with Deep Clustering (DPCL) and the Deep Attractor Network\n(DANet). Furthermore, we found that models trained with uPIT generalize well to\nunseen speakers and languages. Finally, we found that a single model, trained\nwith uPIT, can handle both two-speaker, and three-speaker speech mixtures.", "published": "2017-03-18 10:59:03", "link": "http://arxiv.org/abs/1703.06284v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
