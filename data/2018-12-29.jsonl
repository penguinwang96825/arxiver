{"title": "End-to-end neural relation extraction using deep biaffine attention", "abstract": "We propose a neural network model for joint extraction of named entities and\nrelations between them, without any hand-crafted features. The key contribution\nof our model is to extend a BiLSTM-CRF-based entity recognition model with a\ndeep biaffine attention layer to model second-order interactions between latent\nfeatures for relation classification, specifically attending to the role of an\nentity in a directional relationship. On the benchmark \"relation and entity\nrecognition\" dataset CoNLL04, experimental results show that our model\noutperforms previous models, producing new state-of-the-art performances.", "published": "2018-12-29 03:32:09", "link": "http://arxiv.org/abs/1812.11275v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Weakly-Supervised Hierarchical Text Classification", "abstract": "Hierarchical text classification, which aims to classify text documents into\na given hierarchy, is an important task in many real-world applications.\nRecently, deep neural models are gaining increasing popularity for text\nclassification due to their expressive power and minimum requirement for\nfeature engineering. However, applying deep neural networks for hierarchical\ntext classification remains challenging, because they heavily rely on a large\namount of training data and meanwhile cannot easily determine appropriate\nlevels of documents in the hierarchical setting. In this paper, we propose a\nweakly-supervised neural method for hierarchical text classification. Our\nmethod does not require a large amount of training data but requires only\neasy-to-provide weak supervision signals such as a few class-related documents\nor keywords. Our method effectively leverages such weak supervision signals to\ngenerate pseudo documents for model pre-training, and then performs\nself-training on real unlabeled data to iteratively refine the model. During\nthe training process, our model features a hierarchical neural structure, which\nmimics the given hierarchy and is capable of determining the proper levels for\ndocuments with a blocking mechanism. Experiments on three datasets from\ndifferent domains demonstrate the efficacy of our method compared with a\ncomprehensive set of baselines.", "published": "2018-12-29 03:04:26", "link": "http://arxiv.org/abs/1812.11270v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Attention-Based Capsule Networks with Dynamic Routing for Relation\n  Extraction", "abstract": "A capsule is a group of neurons, whose activity vector represents the\ninstantiation parameters of a specific type of entity. In this paper, we\nexplore the capsule networks used for relation extraction in a multi-instance\nmulti-label learning framework and propose a novel neural approach based on\ncapsule networks with attention mechanisms. We evaluate our method with\ndifferent benchmarks, and it is demonstrated that our method improves the\nprecision of the predicted relations. Particularly, we show that capsule\nnetworks improve multiple entity pairs relation extraction.", "published": "2018-12-29 09:34:23", "link": "http://arxiv.org/abs/1812.11321v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
