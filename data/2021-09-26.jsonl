{"title": "Parallel Refinements for Lexically Constrained Text Generation with BART", "abstract": "Lexically constrained text generation aims to control the generated text by\nincorporating some pre-specified keywords into the output. Previous work\ninjects lexical constraints into the output by controlling the decoding process\nor refining the candidate output iteratively, which tends to generate generic\nor ungrammatical sentences, and has high computational complexity. To address\nthese challenges, we propose Constrained BART (CBART) for lexically constrained\ntext generation. CBART leverages the pre-trained model BART and transfers part\nof the generation burden from the decoder to the encoder by decomposing this\ntask into two sub-tasks, thereby improving the sentence quality. Concretely, we\nextend BART by adding a token-level classifier over the encoder, aiming at\ninstructing the decoder where to replace and insert. Guided by the encoder, the\ndecoder refines multiple tokens of the input in one step by inserting tokens\nbefore specific positions and re-predicting tokens with low confidence. To\nfurther reduce the inference latency, the decoder predicts all tokens in\nparallel. Experiment results on One-Billion-Word and Yelp show that CBART can\ngenerate plausible text with high quality and diversity while significantly\naccelerating inference.", "published": "2021-09-26 03:56:45", "link": "http://arxiv.org/abs/2109.12487v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XLM-K: Improving Cross-Lingual Language Model Pre-training with\n  Multilingual Knowledge", "abstract": "Cross-lingual pre-training has achieved great successes using monolingual and\nbilingual plain text corpora. However, most pre-trained models neglect\nmultilingual knowledge, which is language agnostic but comprises abundant\ncross-lingual structure alignment. In this paper, we propose XLM-K, a\ncross-lingual language model incorporating multilingual knowledge in\npre-training. XLM-K augments existing multilingual pre-training with two\nknowledge tasks, namely Masked Entity Prediction Task and Object Entailment\nTask. We evaluate XLM-K on MLQA, NER and XNLI. Experimental results clearly\ndemonstrate significant improvements over existing multilingual language\nmodels. The results on MLQA and NER exhibit the superiority of XLM-K in\nknowledge related tasks. The success in XNLI shows a better cross-lingual\ntransferability obtained in XLM-K. What is more, we provide a detailed probing\nanalysis to confirm the desired knowledge captured in our pre-training regimen.\nThe code is available at\nhttps://github.com/microsoft/Unicoder/tree/master/pretraining/xlmk.", "published": "2021-09-26 11:46:20", "link": "http://arxiv.org/abs/2109.12573v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MultiDoc2Dial: Modeling Dialogues Grounded in Multiple Documents", "abstract": "We propose MultiDoc2Dial, a new task and dataset on modeling goal-oriented\ndialogues grounded in multiple documents. Most previous works treat\ndocument-grounded dialogue modeling as a machine reading comprehension task\nbased on a single given document or passage. In this work, we aim to address\nmore realistic scenarios where a goal-oriented information-seeking conversation\ninvolves multiple topics, and hence is grounded on different documents. To\nfacilitate such a task, we introduce a new dataset that contains dialogues\ngrounded in multiple documents from four different domains. We also explore\nmodeling the dialogue-based and document-based context in the dataset. We\npresent strong baseline approaches and various experimental results, aiming to\nsupport further research efforts on such a task.", "published": "2021-09-26 13:12:05", "link": "http://arxiv.org/abs/2109.12595v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DialogueCSE: Dialogue-based Contrastive Learning of Sentence Embeddings", "abstract": "Learning sentence embeddings from dialogues has drawn increasing attention\ndue to its low annotation cost and high domain adaptability. Conventional\napproaches employ the siamese-network for this task, which obtains the sentence\nembeddings through modeling the context-response semantic relevance by applying\na feed-forward network on top of the sentence encoders. However, as the\nsemantic textual similarity is commonly measured through the element-wise\ndistance metrics (e.g. cosine and L2 distance), such architecture yields a\nlarge gap between training and evaluating. In this paper, we propose\nDialogueCSE, a dialogue-based contrastive learning approach to tackle this\nissue. DialogueCSE first introduces a novel matching-guided embedding (MGE)\nmechanism, which generates a context-aware embedding for each candidate\nresponse embedding (i.e. the context-free embedding) according to the guidance\nof the multi-turn context-response matching matrices. Then it pairs each\ncontext-aware embedding with its corresponding context-free embedding and\nfinally minimizes the contrastive loss across all pairs. We evaluate our model\non three multi-turn dialogue datasets: the Microsoft Dialogue Corpus, the Jing\nDong Dialogue Corpus, and the E-commerce Dialogue Corpus. Evaluation results\nshow that our approach significantly outperforms the baselines across all three\ndatasets in terms of MAP and Spearman's correlation measures, demonstrating its\neffectiveness. Further quantitative experiments show that our approach achieves\nbetter performance when leveraging more dialogue context and remains robust\nwhen less training data is provided.", "published": "2021-09-26 13:25:41", "link": "http://arxiv.org/abs/2109.12599v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Analysis of Euclidean vs. Graph-Based Framing for Bilingual Lexicon\n  Induction from Word Embedding Spaces", "abstract": "Much recent work in bilingual lexicon induction (BLI) views word embeddings\nas vectors in Euclidean space. As such, BLI is typically solved by finding a\nlinear transformation that maps embeddings to a common space. Alternatively,\nword embeddings may be understood as nodes in a weighted graph. This framing\nallows us to examine a node's graph neighborhood without assuming a linear\ntransform, and exploits new techniques from the graph matching optimization\nliterature. These contrasting approaches have not been compared in BLI so far.\nIn this work, we study the behavior of Euclidean versus graph-based approaches\nto BLI under differing data conditions and show that they complement each other\nwhen combined. We release our code at\nhttps://github.com/kellymarchisio/euc-v-graph-bli.", "published": "2021-09-26 16:06:45", "link": "http://arxiv.org/abs/2109.12640v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QA-Align: Representing Cross-Text Content Overlap by Aligning\n  Question-Answer Propositions", "abstract": "Multi-text applications, such as multi-document summarization, are typically\nrequired to model redundancies across related texts. Current methods\nconfronting consolidation struggle to fuse overlapping information. In order to\nexplicitly represent content overlap, we propose to align predicate-argument\nrelations across texts, providing a potential scaffold for information\nconsolidation. We go beyond clustering coreferring mentions, and instead model\noverlap with respect to redundancy at a propositional level, rather than merely\ndetecting shared referents. Our setting exploits QA-SRL, utilizing\nquestion-answer pairs to capture predicate-argument relations, facilitating\nlaymen annotation of cross-text alignments. We employ crowd-workers for\nconstructing a dataset of QA-based alignments, and present a baseline QA\nalignment model trained over our dataset. Analyses show that our new task is\nsemantically challenging, capturing content overlap beyond lexical similarity\nand complements cross-document coreference with proposition-level links,\noffering potential use for downstream tasks.", "published": "2021-09-26 17:19:48", "link": "http://arxiv.org/abs/2109.12655v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Prunability of Attention Heads in Multilingual BERT", "abstract": "Large multilingual models, such as mBERT, have shown promise in crosslingual\ntransfer. In this work, we employ pruning to quantify the robustness and\ninterpret layer-wise importance of mBERT. On four GLUE tasks, the relative\ndrops in accuracy due to pruning have almost identical results on mBERT and\nBERT suggesting that the reduced attention capacity of the multilingual models\ndoes not affect robustness to pruning. For the crosslingual task XNLI, we\nreport higher drops in accuracy with pruning indicating lower robustness in\ncrosslingual transfer. Also, the importance of the encoder layers sensitively\ndepends on the language family and the pre-training corpus size. The top\nlayers, which are relatively more influenced by fine-tuning, encode important\ninformation for languages similar to English (SVO) while the bottom layers,\nwhich are relatively less influenced by fine-tuning, are particularly important\nfor agglutinative and low-resource languages.", "published": "2021-09-26 19:45:44", "link": "http://arxiv.org/abs/2109.12683v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Electoral Programs of German Parties 2021: A Computational Analysis Of\n  Their Comprehensibility and Likeability Based On SentiArt", "abstract": "The electoral programs of six German parties issued before the parliamentary\nelections of 2021 are analyzed using state-of-the-art computational tools for\nquantitative narrative, topic and sentiment analysis. We compare different\nmethods for computing the textual similarity of the programs, Jaccard Bag\nsimilarity, Latent Semantic Analysis, doc2vec, and sBERT, the representational\nand computational complexity increasing from the 1st to the 4th method. A new\nsimilarity measure for entire documents derived from the Fowlkes Mallows Score\nis applied to kmeans clustering of sBERT transformed sentences. Using novel\nindices of the readability and emotion potential of texts computed via SentiArt\n(Jacobs, 2019), our data shed light on the similarities and differences of the\nprograms regarding their length, main ideas, comprehensibility, likeability,\nand semantic complexity. Among others, they reveal that the programs of the SPD\nand CDU have the best chances to be comprehensible and likeable -all other\nthings being equal-, and they raise the important issue of which similarity\nmeasure is optimal for comparing texts such as electoral programs which\nnecessarily share a lot of words. While such analyses can not replace\nqualitative analyses or a deep reading of the texts, they offer predictions\nthat can be verified in empirical studies and may serve as a motivation for\nchanging aspects of future electoral programs potentially making them more\ncomprehensible and/or likeable.", "published": "2021-09-26 05:27:14", "link": "http://arxiv.org/abs/2109.12500v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Entity Linking Meets Deep Learning: Techniques and Solutions", "abstract": "Entity linking (EL) is the process of linking entity mentions appearing in\nweb text with their corresponding entities in a knowledge base. EL plays an\nimportant role in the fields of knowledge engineering and data mining,\nunderlying a variety of downstream applications such as knowledge base\npopulation, content analysis, relation extraction, and question answering. In\nrecent years, deep learning (DL), which has achieved tremendous success in\nvarious domains, has also been leveraged in EL methods to surpass traditional\nmachine learning based methods and yield the state-of-the-art performance. In\nthis survey, we present a comprehensive review and analysis of existing DL\nbased EL methods. First of all, we propose a new taxonomy, which organizes\nexisting DL based EL methods using three axes: embedding, feature, and\nalgorithm. Then we systematically survey the representative EL methods along\nthe three axes of the taxonomy. Later, we introduce ten commonly used EL data\nsets and give a quantitative performance analysis of DL based EL methods over\nthese data sets. Finally, we discuss the remaining limitations of existing\nmethods and highlight some promising future directions.", "published": "2021-09-26 07:57:38", "link": "http://arxiv.org/abs/2109.12520v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Paradigm Shift in Natural Language Processing", "abstract": "In the era of deep learning, modeling for most NLP tasks has converged to\nseveral mainstream paradigms. For example, we usually adopt the sequence\nlabeling paradigm to solve a bundle of tasks such as POS-tagging, NER,\nChunking, and adopt the classification paradigm to solve tasks like sentiment\nanalysis. With the rapid progress of pre-trained language models, recent years\nhave observed a rising trend of Paradigm Shift, which is solving one NLP task\nby reformulating it as another one. Paradigm shift has achieved great success\non many tasks, becoming a promising way to improve model performance. Moreover,\nsome of these paradigms have shown great potential to unify a large number of\nNLP tasks, making it possible to build a single model to handle diverse tasks.\nIn this paper, we review such phenomenon of paradigm shifts in recent years,\nhighlighting several paradigms that have the potential to solve different NLP\ntasks.", "published": "2021-09-26 11:55:23", "link": "http://arxiv.org/abs/2109.12575v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Extracting and Inferring Personal Attributes from Dialogue", "abstract": "Personal attributes represent structured information about a person, such as\ntheir hobbies, pets, family, likes and dislikes. We introduce the tasks of\nextracting and inferring personal attributes from human-human dialogue, and\nanalyze the linguistic demands of these tasks. To meet these challenges, we\nintroduce a simple and extensible model that combines an autoregressive\nlanguage model utilizing constrained attribute generation with a discriminative\nreranker. Our model outperforms strong baselines on extracting personal\nattributes as well as inferring personal attributes that are not contained\nverbatim in utterances and instead requires commonsense reasoning and lexical\ninferences, which occur frequently in everyday conversation. Finally, we\ndemonstrate the benefit of incorporating personal attributes in social\nchit-chat and task-oriented dialogue settings.", "published": "2021-09-26 20:51:00", "link": "http://arxiv.org/abs/2109.12702v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "BioCopy: A Plug-And-Play Span Copy Mechanism in Seq2Seq Models", "abstract": "Copy mechanisms explicitly obtain unchanged tokens from the source (input)\nsequence to generate the target (output) sequence under the neural seq2seq\nframework. However, most of the existing copy mechanisms only consider single\nword copying from the source sentences, which results in losing essential\ntokens while copying long spans. In this work, we propose a plug-and-play\narchitecture, namely BioCopy, to alleviate the problem aforementioned.\nSpecifically, in the training stage, we construct a BIO tag for each token and\ntrain the original model with BIO tags jointly. In the inference stage, the\nmodel will firstly predict the BIO tag at each time step, then conduct\ndifferent mask strategies based on the predicted BIO label to diminish the\nscope of the probability distributions over the vocabulary list. Experimental\nresults on two separate generative tasks show that they all outperform the\nbaseline models by adding our BioCopy to the original model structure.", "published": "2021-09-26 08:55:26", "link": "http://arxiv.org/abs/2109.12533v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine\n  Translation", "abstract": "In recent times, there has been definitive progress in the field of NLP, with\nits applications growing as the utility of our language models increases with\nadvances in their performance. However, these models require a large amount of\ncomputational power and data to train, consequently leading to large carbon\nfootprints. Therefore, it is imperative that we study the carbon efficiency and\nlook for alternatives to reduce the overall environmental impact of training\nmodels, in particular large language models. In our work, we assess the\nperformance of models for machine translation, across multiple language pairs\nto assess the difference in computational power required to train these models\nfor each of these language pairs and examine the various components of these\nmodels to analyze aspects of our pipeline that can be optimized to reduce these\ncarbon emissions.", "published": "2021-09-26 12:30:10", "link": "http://arxiv.org/abs/2109.12584v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Question Answering Performance Using Knowledge Distillation\n  and Active Learning", "abstract": "Contemporary question answering (QA) systems, including transformer-based\narchitectures, suffer from increasing computational and model complexity which\nrender them inefficient for real-world applications with limited resources.\nFurther, training or even fine-tuning such models requires a vast amount of\nlabeled data which is often not available for the task at hand. In this\nmanuscript, we conduct a comprehensive analysis of the mentioned challenges and\nintroduce suitable countermeasures. We propose a novel knowledge distillation\n(KD) approach to reduce the parameter and model complexity of a pre-trained\nBERT system and utilize multiple active learning (AL) strategies for immense\nreduction in annotation efforts. In particular, we demonstrate that our model\nachieves the performance of a 6-layer TinyBERT and DistilBERT, whilst using\nonly 2% of their total parameters. Finally, by the integration of our AL\napproaches into the BERT framework, we show that state-of-the-art results on\nthe SQuAD dataset can be achieved when we only use 20% of the training data.", "published": "2021-09-26 17:49:54", "link": "http://arxiv.org/abs/2109.12662v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exposing Paid Opinion Manipulation Trolls", "abstract": "Recently, Web forums have been invaded by opinion manipulation trolls. Some\ntrolls try to influence the other users driven by their own convictions, while\nin other cases they can be organized and paid, e.g., by a political party or a\nPR agency that gives them specific instructions what to write. Finding paid\ntrolls automatically using machine learning is a hard task, as there is no\nenough training data to train a classifier; yet some test data is possible to\nobtain, as these trolls are sometimes caught and widely exposed. In this paper,\nwe solve the training data problem by assuming that a user who is called a\ntroll by several different people is likely to be such, and one who has never\nbeen called a troll is unlikely to be such. We compare the profiles of (i) paid\ntrolls vs. (ii)\"mentioned\" trolls vs. (iii) non-trolls, and we further show\nthat a classifier trained to distinguish (ii) from (iii) does quite well also\nat telling apart (i) from (iii).", "published": "2021-09-26 11:40:14", "link": "http://arxiv.org/abs/2109.13726v1", "categories": ["cs.LG", "cs.CL", "cs.IR", "cs.SI", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.LG"}
{"title": "Feature-Rich Named Entity Recognition for Bulgarian Using Conditional\n  Random Fields", "abstract": "The paper presents a feature-rich approach to the automatic recognition and\ncategorization of named entities (persons, organizations, locations, and\nmiscellaneous) in news text for Bulgarian. We combine well-established features\nused for other languages with language-specific lexical, syntactic and\nmorphological information. In particular, we make use of the rich tagset\nannotation of the BulTreeBank (680 morpho-syntactic tags), from which we derive\nsuitable task-specific tagsets (local and nonlocal). We further add\ndomain-specific gazetteers and additional unlabeled data, achieving F1=89.4%,\nwhich is comparable to the state-of-the-art results for English.", "published": "2021-09-26 12:00:06", "link": "http://arxiv.org/abs/2109.15121v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "SUper Team at SemEval-2016 Task 3: Building a feature-rich system for\n  community question answering", "abstract": "We present the system we built for participating in SemEval-2016 Task 3 on\nCommunity Question Answering. We achieved the best results on subtask C, and\nstrong results on subtasks A and B, by combining a rich set of various types of\nfeatures: semantic, lexical, metadata, and user-related. The most important\ngroup turned out to be the metadata for the question and for the comment,\nsemantic vectors trained on QatarLiving data and similarities between the\nquestion and the comment for subtasks A and C, and between the original and the\nrelated question for Subtask B.", "published": "2021-09-26 11:48:48", "link": "http://arxiv.org/abs/2109.15120v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.SI", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Rendering Spatial Sound for Interoperable Experiences in the Audio\n  Metaverse", "abstract": "Interactive audio spatialization technology previously developed for video\ngame authoring and rendering has evolved into an essential component of\nplatforms enabling shared immersive virtual experiences for future co-presence,\nremote collaboration and entertainment applications. New wearable virtual and\naugmented reality displays employ real-time binaural audio computing engines\nrendering multiple digital objects and supporting the free navigation of\nnetworked participants or their avatars through a juxtaposition of\nenvironments, real and virtual, often referred to as the Metaverse. These\napplications require a parametric audio scene programming interface to\nfacilitate the creation and deployment of shared, dynamic and realistic virtual\n3D worlds on mobile computing platforms and remote servers.\n  We propose a practical approach for designing parametric 6-degree-of-freedom\nobject-based interactive audio engines to deliver the perceptually relevant\nbinaural cues necessary for audio/visual and virtual/real congruence in\nMetaverse experiences. We address the effects of room reverberation, acoustic\nreflectors, and obstacles in both the virtual and real environments, and\ndiscuss how such effects may be driven by combinations of pre-computed and\nreal-time acoustic propagation solvers. We envision an open scene description\nmodel distilled to facilitate the development of interoperable applications\ndistributed across multiple platforms, where each audio object represents, to\nthe user, a natural sound source having controllable distance, size,\norientation, and acoustic radiation properties.", "published": "2021-09-26 01:24:08", "link": "http://arxiv.org/abs/2109.12471v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "General Theory of Music by Icosahedron 3: Musical invariant and\n  Melakarta raga", "abstract": "Raga is a central musical concept in South Asia, especially India, and we\ninvestigate connections between Western classical music and Melakarta raga that\nis a raga in Karnatak (south Indian) classical music, through musical\nicosahedron. In our previous study, we introduced some kinds of musical\nicosahedra connecting various musical concepts in Western music:\nchromatic/whole tone musical icosahedra, Pythagorean/whole tone musical\nicosahedra, and exceptional musical icosahedra. In this paper, first, we\nintroduce kinds of musical icosahedra that connect the above musical icosahedra\nthrough two kinds of permutations of 12 tones: inter-permutations and\nintra-permutations, and we call them intermediate musical icosahedra. Next, we\ndefine a neighboring number as a number of pairs of neighboring two tones in a\ngiven scale that neighbor each other on a given musical icosahedron, and we\nalso define a musical invariant as a linear combination of the neighboring\nnumbers. We find there exists a pair of a musical invariant and scales that is\nconstant for some musical icosahedra and analyze their mathematical structure.\nLast, we define an extension of a given scale by the inter-permutations of a\ngiven musical icosahedron: the permutation-extension. Then, we show that the\npermutation-extension of the C major scale by Melakarta raga musical icosahedra\nthat are four of the intermediate musical icosahedra from the type 1\nchromatic/whole tone musical icosahedron to the type 1' Pythagorean/whole tone\nmusical icosahedron, is a set of all the scales included in Melakarta raga.\nThere exists a musical invariant that is constant for all the musical\nicosahedra corresponding to the scales of Melakarta raga, and we obtained a\ndiagram representation of those scales characterizing the musical invariant.", "published": "2021-09-26 01:55:42", "link": "http://arxiv.org/abs/2109.12475v1", "categories": ["cs.SD", "eess.AS", "00A65"], "primary_category": "cs.SD"}
{"title": "Joint magnitude estimation and phase recovery using Cycle-in-Cycle GAN\n  for non-parallel speech enhancement", "abstract": "For the lack of adequate paired noisy-clean speech corpus in many real\nscenarios, non-parallel training is a promising task for DNN-based speech\nenhancement methods. However, because of the severe mismatch between input and\ntarget speeches, many previous studies only focus on the magnitude spectrum\nestimation and remain the phase unaltered, resulting in the degraded speech\nquality under low signal-to-noise ratio conditions. To tackle this problem, we\ndecouple the difficult target w.r.t. original spectrum optimization into\nspectral magnitude and phase, and a novel Cycle-in-Cycle generative adversarial\nnetwork (dubbed CinCGAN) is proposed to jointly estimate the spectral magnitude\nand phase information stage by stage under unpaired data. In the first stage,\nwe pretrain a magnitude CycleGAN to coarsely estimate the spectral magnitude of\nclean speech. In the second stage, we incorporate the pretrained CycleGAN with\na complex-valued CycleGAN as a cycle-in-cycle structure to simultaneously\nrecover phase information and refine the overall spectrum. Experimental results\ndemonstrate that the proposed approach significantly outperforms previous\nbaselines under non-parallel training. The evaluation on training the models\nwith standard paired data also shows that CinCGAN achieves remarkable\nperformance especially in reducing background noise and speech distortion.", "published": "2021-09-26 13:02:01", "link": "http://arxiv.org/abs/2109.12591v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Soundata: A Python library for reproducible use of audio datasets", "abstract": "Soundata is a Python library for loading and working with audio datasets in a\nstandardized way, removing the need for writing custom loaders in every\nproject, and improving reproducibility by providing tools to validate data\nagainst a canonical version. It speeds up research pipelines by allowing users\nto quickly download a dataset, load it into memory in a standardized and\nreproducible way, validate that the dataset is complete and correct, and more.\nSoundata is based and inspired on mirdata and design to complement mirdata by\nworking with environmental sound, bioacoustic and speech datasets, among\nothers. Soundata was created to be easy to use, easy to contribute to, and to\nincrease reproducibility and standardize usage of sound datasets in a flexible\nway.", "published": "2021-09-26 19:59:18", "link": "http://arxiv.org/abs/2109.12690v2", "categories": ["cs.SD", "cs.DB", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
