{"title": "TC-GAT: Graph Attention Network for Temporal Causality Discovery", "abstract": "The present study explores the intricacies of causal relationship extraction,\na vital component in the pursuit of causality knowledge. Causality is\nfrequently intertwined with temporal elements, as the progression from cause to\neffect is not instantaneous but rather ensconced in a temporal dimension. Thus,\nthe extraction of temporal causality holds paramount significance in the field.\nIn light of this, we propose a method for extracting causality from the text\nthat integrates both temporal and causal relations, with a particular focus on\nthe time aspect. To this end, we first compile a dataset that encompasses\ntemporal relationships. Subsequently, we present a novel model, TC-GAT, which\nemploys a graph attention mechanism to assign weights to the temporal\nrelationships and leverages a causal knowledge graph to determine the adjacency\nmatrix. Additionally, we implement an equilibrium mechanism to regulate the\ninterplay between temporal and causal relations. Our experiments demonstrate\nthat our proposed method significantly surpasses baseline models in the task of\ncausality extraction.", "published": "2023-04-21 02:26:42", "link": "http://arxiv.org/abs/2304.10706v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Downstream Task-Oriented Neural Tokenizer Optimization with Vocabulary\n  Restriction as Post Processing", "abstract": "This paper proposes a method to optimize tokenization for the performance\nimprovement of already trained downstream models. Our method generates\ntokenization results attaining lower loss values of a given downstream model on\nthe training data for restricting vocabularies and trains a tokenizer\nreproducing the tokenization results. Therefore, our method can be applied to\nvariety of tokenization methods, while existing work cannot due to the\nsimultaneous learning of the tokenizer and the downstream model. This paper\nproposes an example of the BiLSTM-based tokenizer with vocabulary restriction,\nwhich can capture wider contextual information for the tokenization process\nthan non-neural-based tokenization methods used in existing work. Experimental\nresults on text classification in Japanese, Chinese, and English text\nclassification tasks show that the proposed method improves performance\ncompared to the existing methods for tokenization optimization.", "published": "2023-04-21 08:29:14", "link": "http://arxiv.org/abs/2304.10808v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tokenization Preference for Human and Machine Learning Model: An\n  Annotation Study", "abstract": "Is preferred tokenization for humans also preferred for machine-learning (ML)\nmodels? This study examines the relations between preferred tokenization for\nhumans (appropriateness and readability) and one for ML models (performance on\nan NLP task). The question texts of the Japanese commonsense question-answering\ndataset are tokenized with six different tokenizers, and the performances of\nhuman annotators and ML models were compared. Furthermore, we analyze relations\namong performance of answers by human and ML model, the appropriateness of\ntokenization for human, and response time to questions by human. This study\nprovides a quantitative investigation result that shows that preferred\ntokenizations for humans and ML models are not necessarily always the same. The\nresult also implies that existing methods using language models for\ntokenization could be a good compromise both for human and ML models.", "published": "2023-04-21 08:49:06", "link": "http://arxiv.org/abs/2304.10813v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Better Sign Language Translation with Monolingual Data", "abstract": "Sign language translation (SLT) systems, which are often decomposed into\nvideo-to-gloss (V2G) recognition and gloss-to-text (G2T) translation through\nthe pivot gloss, heavily relies on the availability of large-scale parallel G2T\npairs. However, the manual annotation of pivot gloss, which is a sequence of\ntranscribed written-language words in the order in which they are signed,\nfurther exacerbates the scarcity of data for SLT. To address this issue, this\npaper proposes a simple and efficient rule transformation method to transcribe\nthe large-scale target monolingual data into its pseudo glosses automatically\nfor enhancing the SLT translation. Empirical results show that the proposed\napproach can significantly improve the performance of SLT, especially achieving\nstate-of-the-art results on two SLT benchmark datasets PHEONIX-WEATHER 2014T\nand ASLG-PC12. Our code has been released at:\nhttps://github.com/pengr/Mono\\_SLT.", "published": "2023-04-21 09:39:54", "link": "http://arxiv.org/abs/2304.10844v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text2Time: Transformer-based Article Time Period Prediction", "abstract": "The task of predicting the publication period of text documents, such as news\narticles, is an important but less studied problem in the field of natural\nlanguage processing. Predicting the year of a news article can be useful in\nvarious contexts, such as historical research, sentiment analysis, and media\nmonitoring. In this work, we investigate the problem of predicting the\npublication period of a text document, specifically a news article, based on\nits textual content. In order to do so, we created our own extensive labeled\ndataset of over 350,000 news articles published by The New York Times over six\ndecades. In our approach, we use a pretrained BERT model fine-tuned for the\ntask of text classification, specifically for time period prediction.This model\nexceeds our expectations and provides some very impressive results in terms of\naccurately classifying news articles into their respective publication decades.\nThe results beat the performance of the baseline model for this relatively\nunexplored task of time prediction from text.", "published": "2023-04-21 10:05:03", "link": "http://arxiv.org/abs/2304.10859v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Information Extraction from Documents: Question Answering vs Token\n  Classification in real-world setups", "abstract": "Research in Document Intelligence and especially in Document Key Information\nExtraction (DocKIE) has been mainly solved as Token Classification problem.\nRecent breakthroughs in both natural language processing (NLP) and computer\nvision helped building document-focused pre-training methods, leveraging a\nmultimodal understanding of the document text, layout and image modalities.\nHowever, these breakthroughs also led to the emergence of a new DocKIE subtask\nof extractive document Question Answering (DocQA), as part of the Machine\nReading Comprehension (MRC) research field. In this work, we compare the\nQuestion Answering approach with the classical token classification approach\nfor document key information extraction. We designed experiments to benchmark\nfive different experimental setups : raw performances, robustness to noisy\nenvironment, capacity to extract long entities, fine-tuning speed on Few-Shot\nLearning and finally Zero-Shot Learning. Our research showed that when dealing\nwith clean and relatively short entities, it is still best to use token\nclassification-based approach, while the QA approach could be a good\nalternative for noisy environment or long entities use-cases.", "published": "2023-04-21 14:43:42", "link": "http://arxiv.org/abs/2304.10994v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emergent and Predictable Memorization in Large Language Models", "abstract": "Memorization, or the tendency of large language models (LLMs) to output\nentire sequences from their training data verbatim, is a key concern for safely\ndeploying language models. In particular, it is vital to minimize a model's\nmemorization of sensitive datapoints such as those containing personal\nidentifiable information (PII). The prevalence of such undesirable memorization\ncan pose issues for model trainers, and may even require discarding an\notherwise functional model. We therefore seek to predict which sequences will\nbe memorized before a large model's full train-time by extrapolating the\nmemorization behavior of lower-compute trial runs. We measure memorization of\nthe Pythia model suite and plot scaling laws for forecasting memorization,\nallowing us to provide equi-compute recommendations to maximize the reliability\n(recall) of such predictions. We additionally provide further novel discoveries\non the distribution of memorization scores across models and data. We release\nall code and data necessary to reproduce the results in this paper at\nhttps://github.com/EleutherAI/pythia", "published": "2023-04-21 17:58:31", "link": "http://arxiv.org/abs/2304.11158v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learn What NOT to Learn: Towards Generative Safety in Chatbots", "abstract": "Conversational models that are generative and open-domain are particularly\nsusceptible to generating unsafe content since they are trained on web-based\nsocial data. Prior approaches to mitigating this issue have drawbacks, such as\ndisrupting the flow of conversation, limited generalization to unseen toxic\ninput contexts, and sacrificing the quality of the dialogue for the sake of\nsafety. In this paper, we present a novel framework, named \"LOT\" (Learn NOT\nto), that employs a contrastive loss to enhance generalization by learning from\nboth positive and negative training signals. Our approach differs from the\nstandard contrastive learning framework in that it automatically obtains\npositive and negative signals from the safe and unsafe language distributions\nthat have been learned beforehand. The LOT framework utilizes divergence to\nsteer the generations away from the unsafe subspace and towards the safe\nsubspace while sustaining the flow of conversation. Our approach is memory and\ntime-efficient during decoding and effectively reduces toxicity while\npreserving engagingness and fluency. Empirical results indicate that LOT\nreduces toxicity by up to four-fold while achieving four to six-fold higher\nrates of engagingness and fluency compared to baseline models. Our findings are\nfurther corroborated by human evaluation.", "published": "2023-04-21 18:59:06", "link": "http://arxiv.org/abs/2304.11220v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Group-Specific Approach to NLP for Hate Speech Detection", "abstract": "Automatic hate speech detection is an important yet complex task, requiring\nknowledge of common sense, stereotypes of protected groups, and histories of\ndiscrimination, each of which may constantly evolve. In this paper, we propose\na group-specific approach to NLP for online hate speech detection. The approach\nconsists of creating and infusing historical and linguistic knowledge about a\nparticular protected group into hate speech detection models, analyzing\nhistorical data about discrimination against a protected group to better\npredict spikes in hate speech against that group, and critically evaluating\nhate speech detection models through lenses of intersectionality and ethics. We\ndemonstrate this approach through a case study on NLP for detection of\nantisemitic hate speech. The case study synthesizes the current\nEnglish-language literature on NLP for antisemitism detection, introduces a\nnovel knowledge graph of antisemitic history and language from the 20th century\nto the present, infuses information from the knowledge graph into a set of\ntweets over Logistic Regression and uncased DistilBERT baselines, and suggests\nthat incorporating context from the knowledge graph can help models pick up\nsubtle stereotypes.", "published": "2023-04-21 19:08:49", "link": "http://arxiv.org/abs/2304.11223v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UBC-DLNLP at SemEval-2023 Task 12: Impact of Transfer Learning on\n  African Sentiment Analysis", "abstract": "We describe our contribution to the SemEVAl 2023 AfriSenti-SemEval shared\ntask, where we tackle the task of sentiment analysis in 14 different African\nlanguages. We develop both monolingual and multilingual models under a full\nsupervised setting (subtasks A and B). We also develop models for the zero-shot\nsetting (subtask C). Our approach involves experimenting with transfer learning\nusing six language models, including further pertaining of some of these models\nas well as a final finetuning stage. Our best performing models achieve an\nF1-score of 70.36 on development data and an F1-score of 66.13 on test data.\nUnsurprisingly, our results demonstrate the effectiveness of transfer learning\nand fine-tuning techniques for sentiment analysis across multiple languages.\nOur approach can be applied to other sentiment analysis tasks in different\nlanguages and domains.", "published": "2023-04-21 21:25:14", "link": "http://arxiv.org/abs/2304.11256v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Role of AI in Human-AI Creative Writing for Hong Kong Secondary\n  Students", "abstract": "The recent advancement in Natural Language Processing (NLP) capability has\nled to the development of language models (e.g., ChatGPT) that is capable of\ngenerating human-like language. In this study, we explore how language models\ncan be utilized to help the ideation aspect of creative writing. Our empirical\nfindings show that language models play different roles in helping student\nwriters to be more creative, such as the role of a collaborator, a provocateur,\netc", "published": "2023-04-21 23:50:09", "link": "http://arxiv.org/abs/2304.11276v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building Multimodal AI Chatbots", "abstract": "This work aims to create a multimodal AI system that chats with humans and\nshares relevant photos. While earlier works were limited to dialogues about\nspecific objects or scenes within images, recent works have incorporated images\ninto open-domain dialogues. However, their response generators are unimodal,\naccepting text input but no image input, thus prone to generating responses\ncontradictory to the images shared in the dialogue. Therefore, this work\nproposes a complete chatbot system using two multimodal deep learning models:\nan image retriever that understands texts and a response generator that\nunderstands images. The image retriever, implemented by ViT and BERT, selects\nthe most relevant image given the dialogue history and a database of images.\nThe response generator, implemented by ViT and GPT-2/DialoGPT, generates an\nappropriate response given the dialogue history and the most recently retrieved\nimage. The two models are trained and evaluated on PhotoChat, an open-domain\ndialogue dataset in which a photo is shared in each session. In automatic\nevaluation, the proposed image retriever outperforms existing baselines VSE++\nand SCAN with Recall@1/5/10 of 0.1/0.3/0.4 and MRR of 0.2 when ranking 1,000\nimages. The proposed response generator also surpasses the baseline Divter with\nPPL of 16.9, BLEU-1/2 of 0.13/0.03, and Distinct-1/2 of 0.97/0.86, showing a\nsignificant improvement in PPL by -42.8 and BLEU-1/2 by +0.07/0.02. In human\nevaluation with a Likert scale of 1-5, the complete multimodal chatbot system\nreceives higher image-groundedness of 4.3 and engagingness of 4.3, along with\ncompetitive fluency of 4.1, coherence of 3.9, and humanness of 3.1, when\ncompared to other chatbot variants. The source code is available at:\nhttps://github.com/minniie/multimodal_chat.git.", "published": "2023-04-21 16:43:54", "link": "http://arxiv.org/abs/2305.03512v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Grounded Language Understanding in a Collaborative Environment\n  by Interacting with Agents Through Help Feedback", "abstract": "Many approaches to Natural Language Processing (NLP) tasks often treat them\nas single-step problems, where an agent receives an instruction, executes it,\nand is evaluated based on the final outcome. However, human language is\ninherently interactive, as evidenced by the back-and-forth nature of human\nconversations. In light of this, we posit that human-AI collaboration should\nalso be interactive, with humans monitoring the work of AI agents and providing\nfeedback that the agent can understand and utilize. Further, the AI agent\nshould be able to detect when it needs additional information and proactively\nask for help. Enabling this scenario would lead to more natural, efficient, and\nengaging human-AI collaborations.\n  In this work, we explore these directions using the challenging task defined\nby the IGLU competition, an interactive grounded language understanding task in\na MineCraft-like world. We explore multiple types of help players can give to\nthe AI to guide it and analyze the impact of this help in AI behavior,\nresulting in performance improvements.", "published": "2023-04-21 05:37:59", "link": "http://arxiv.org/abs/2304.10750v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GeoLayoutLM: Geometric Pre-training for Visual Information Extraction", "abstract": "Visual information extraction (VIE) plays an important role in Document\nIntelligence. Generally, it is divided into two tasks: semantic entity\nrecognition (SER) and relation extraction (RE). Recently, pre-trained models\nfor documents have achieved substantial progress in VIE, particularly in SER.\nHowever, most of the existing models learn the geometric representation in an\nimplicit way, which has been found insufficient for the RE task since geometric\ninformation is especially crucial for RE. Moreover, we reveal another factor\nthat limits the performance of RE lies in the objective gap between the\npre-training phase and the fine-tuning phase for RE. To tackle these issues, we\npropose in this paper a multi-modal framework, named GeoLayoutLM, for VIE.\nGeoLayoutLM explicitly models the geometric relations in pre-training, which we\ncall geometric pre-training. Geometric pre-training is achieved by three\nspecially designed geometry-related pre-training tasks. Additionally, novel\nrelation heads, which are pre-trained by the geometric pre-training tasks and\nfine-tuned for RE, are elaborately designed to enrich and enhance the feature\nrepresentation. According to extensive experiments on standard VIE benchmarks,\nGeoLayoutLM achieves highly competitive scores in the SER task and\nsignificantly outperforms the previous state-of-the-arts for RE (\\eg, the F1\nscore of RE on FUNSD is boosted from 80.35\\% to 89.45\\%). The code and models\nare publicly available at\nhttps://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/GeoLayoutLM", "published": "2023-04-21 06:02:14", "link": "http://arxiv.org/abs/2304.10759v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Which Factors Predict the Chat Experience of a Natural Language\n  Generation Dialogue Service?", "abstract": "In this paper, we proposed a conceptual model to predict the chat experience\nin a natural language generation dialog system. We evaluated the model with 120\nparticipants with Partial Least Squares Structural Equation Modeling (PLS-SEM)\nand obtained an R-square (R2) with 0.541. The model considers various factors,\nincluding the prompts used for generation; coherence, sentiment, and similarity\nin the conversation; and users' perceived dialog agents' favorability. We then\nfurther explore the effectiveness of the subset of our proposed model. The\nresults showed that users' favorability and coherence, sentiment, and\nsimilarity in the dialogue are positive predictors of users' chat experience.\nMoreover, we found users may prefer dialog agents with characteristics of\nExtroversion, Openness, Conscientiousness, Agreeableness, and Non-Neuroticism.\nThrough our research, an adaptive dialog system might use collected data to\ninfer factors in our model, predict the chat experience for users through these\nfactors, and optimize it by adjusting prompts.", "published": "2023-04-21 07:29:07", "link": "http://arxiv.org/abs/2304.10785v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Evaluating Transformer Language Models on Arithmetic Operations Using\n  Number Decomposition", "abstract": "In recent years, Large Language Models such as GPT-3 showed remarkable\ncapabilities in performing NLP tasks in the zero and few shot settings. On the\nother hand, the experiments highlighted the difficulty of GPT-3 in carrying out\ntasks that require a certain degree of reasoning, such as arithmetic\noperations. In this paper we evaluate the ability of Transformer Language\nModels to perform arithmetic operations following a pipeline that, before\nperforming computations, decomposes numbers in units, tens, and so on. We\ndenote the models fine-tuned with this pipeline with the name Calculon and we\ntest them in the task of performing additions, subtractions and multiplications\non the same test sets of GPT-3. Results show an increase of accuracy of 63% in\nthe five-digit addition task. Moreover, we demonstrate the importance of the\ndecomposition pipeline introduced, since fine-tuning the same Language Model\nwithout decomposing numbers results in 0% accuracy in the five-digit addition\ntask.", "published": "2023-04-21 14:21:52", "link": "http://arxiv.org/abs/2304.10977v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "BERT Based Clinical Knowledge Extraction for Biomedical Knowledge Graph\n  Construction and Analysis", "abstract": "Background : Knowledge is evolving over time, often as a result of new\ndiscoveries or changes in the adopted methods of reasoning. Also, new facts or\nevidence may become available, leading to new understandings of complex\nphenomena. This is particularly true in the biomedical field, where scientists\nand physicians are constantly striving to find new methods of diagnosis,\ntreatment and eventually cure. Knowledge Graphs (KGs) offer a real way of\norganizing and retrieving the massive and growing amount of biomedical\nknowledge.\n  Objective : We propose an end-to-end approach for knowledge extraction and\nanalysis from biomedical clinical notes using the Bidirectional Encoder\nRepresentations from Transformers (BERT) model and Conditional Random Field\n(CRF) layer.\n  Methods : The approach is based on knowledge graphs, which can effectively\nprocess abstract biomedical concepts such as relationships and interactions\nbetween medical entities. Besides offering an intuitive way to visualize these\nconcepts, KGs can solve more complex knowledge retrieval problems by\nsimplifying them into simpler representations or by transforming the problems\ninto representations from different perspectives. We created a biomedical\nKnowledge Graph using using Natural Language Processing models for named entity\nrecognition and relation extraction. The generated biomedical knowledge graphs\n(KGs) are then used for question answering.\n  Results : The proposed framework can successfully extract relevant structured\ninformation with high accuracy (90.7% for Named-entity recognition (NER), 88%\nfor relation extraction (RE)), according to experimental findings based on\nreal-world 505 patient biomedical unstructured clinical notes.\n  Conclusions : In this paper, we propose a novel end-to-end system for the\nconstruction of a biomedical knowledge graph from clinical textual using a\nvariation of BERT models.", "published": "2023-04-21 14:45:33", "link": "http://arxiv.org/abs/2304.10996v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChatABL: Abductive Learning via Natural Language Interaction with\n  ChatGPT", "abstract": "Large language models (LLMs) such as ChatGPT have recently demonstrated\nsignificant potential in mathematical abilities, providing valuable reasoning\nparadigm consistent with human natural language. However, LLMs currently have\ndifficulty in bridging perception, language understanding and reasoning\ncapabilities due to incompatibility of the underlying information flow among\nthem, making it challenging to accomplish tasks autonomously. On the other\nhand, abductive learning (ABL) frameworks for integrating the two abilities of\nperception and reasoning has seen significant success in inverse decipherment\nof incomplete facts, but it is limited by the lack of semantic understanding of\nlogical reasoning rules and the dependence on complicated domain knowledge\nrepresentation. This paper presents a novel method (ChatABL) for integrating\nLLMs into the ABL framework, aiming at unifying the three abilities in a more\nuser-friendly and understandable manner. The proposed method uses the strengths\nof LLMs' understanding and logical reasoning to correct the incomplete logical\nfacts for optimizing the performance of perceptual module, by summarizing and\nreorganizing reasoning rules represented in natural language format. Similarly,\nperceptual module provides necessary reasoning examples for LLMs in natural\nlanguage format. The variable-length handwritten equation deciphering task, an\nabstract expression of the Mayan calendar decoding, is used as a testbed to\ndemonstrate that ChatABL has reasoning ability beyond most existing\nstate-of-the-art methods, which has been well supported by comparative studies.\nTo our best knowledge, the proposed ChatABL is the first attempt to explore a\nnew pattern for further approaching human-level cognitive ability via natural\nlanguage interaction with ChatGPT.", "published": "2023-04-21 16:23:47", "link": "http://arxiv.org/abs/2304.11107v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChatGPT, Large Language Technologies, and the Bumpy Road of Benefiting\n  Humanity", "abstract": "The allure of emerging AI technologies is undoubtedly thrilling. However, the\npromise that AI technologies will benefit all of humanity is empty so long as\nwe lack a nuanced understanding of what humanity is supposed to be in the face\nof widening global inequality and pressing existential threats. Going forward,\nit is crucial to invest in rigorous and collaborative AI safety and ethics\nresearch. We also need to develop standards in a sustainable and equitable way\nthat differentiate between merely speculative and well-researched questions.\nOnly the latter enable us to co-construct and deploy the values that are\nnecessary for creating beneficial AI. Failure to do so could result in a future\nin which our AI technological advancements outstrip our ability to navigate\ntheir ethical and social implications. This path we do not want to go down.", "published": "2023-04-21 22:53:45", "link": "http://arxiv.org/abs/2304.11163v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "ReCEval: Evaluating Reasoning Chains via Correctness and Informativeness", "abstract": "Multi-step reasoning ability is fundamental to many natural language tasks,\nyet it is unclear what constitutes a good reasoning chain and how to evaluate\nthem. Most existing methods focus solely on whether the reasoning chain leads\nto the correct conclusion, but this answer-oriented view may confound reasoning\nquality with other spurious shortcuts to predict the answer. To bridge this\ngap, we evaluate reasoning chains by viewing them as informal proofs that\nderive the final answer. Specifically, we propose ReCEval (Reasoning Chain\nEvaluation), a framework that evaluates reasoning chains via two key\nproperties: (1) correctness, i.e., each step makes a valid inference based on\ninformation contained within the step, preceding steps, and input context, and\n(2) informativeness, i.e., each step provides new information that is helpful\ntowards deriving the generated answer. We evaluate these properties by\ndeveloping metrics using natural language inference models and V-Information.\nOn multiple datasets, we show that ReCEval effectively identifies various error\ntypes and yields notable improvements compared to prior methods. We analyze the\nimpact of step boundaries, and previous steps on evaluating correctness and\ndemonstrate that our informativeness metric captures the expected flow of\ninformation in high-quality reasoning chains. Finally, we show that scoring\nreasoning chains based on ReCEval improves downstream task performance. Our\ncode is publicly available at: https://github.com/archiki/ReCEval", "published": "2023-04-21 02:19:06", "link": "http://arxiv.org/abs/2304.10703v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Eyettention: An Attention-based Dual-Sequence Model for Predicting Human\n  Scanpaths during Reading", "abstract": "Eye movements during reading offer insights into both the reader's cognitive\nprocesses and the characteristics of the text that is being read. Hence, the\nanalysis of scanpaths in reading have attracted increasing attention across\nfields, ranging from cognitive science over linguistics to computer science. In\nparticular, eye-tracking-while-reading data has been argued to bear the\npotential to make machine-learning-based language models exhibit a more\nhuman-like linguistic behavior. However, one of the main challenges in modeling\nhuman scanpaths in reading is their dual-sequence nature: the words are ordered\nfollowing the grammatical rules of the language, whereas the fixations are\nchronologically ordered. As humans do not strictly read from left-to-right, but\nrather skip or refixate words and regress to previous words, the alignment of\nthe linguistic and the temporal sequence is non-trivial. In this paper, we\ndevelop Eyettention, the first dual-sequence model that simultaneously\nprocesses the sequence of words and the chronological sequence of fixations.\nThe alignment of the two sequences is achieved by a cross-sequence attention\nmechanism. We show that Eyettention outperforms state-of-the-art models in\npredicting scanpaths. We provide an extensive within- and across-data set\nevaluation on different languages. An ablation study and qualitative analysis\nsupport an in-depth understanding of the model's behavior.", "published": "2023-04-21 07:26:49", "link": "http://arxiv.org/abs/2304.10784v2", "categories": ["cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LEIA: Linguistic Embeddings for the Identification of Affect", "abstract": "The wealth of text data generated by social media has enabled new kinds of\nanalysis of emotions with language models. These models are often trained on\nsmall and costly datasets of text annotations produced by readers who guess the\nemotions expressed by others in social media posts. This affects the quality of\nemotion identification methods due to training data size limitations and noise\nin the production of labels used in model development. We present LEIA, a model\nfor emotion identification in text that has been trained on a dataset of more\nthan 6 million posts with self-annotated emotion labels for happiness,\naffection, sadness, anger, and fear. LEIA is based on a word masking method\nthat enhances the learning of emotion words during model pre-training. LEIA\nachieves macro-F1 values of approximately 73 on three in-domain test datasets,\noutperforming other supervised and unsupervised methods in a strong benchmark\nthat shows that LEIA generalizes across posts, users, and time periods. We\nfurther perform an out-of-domain evaluation on five different datasets of\nsocial media and other sources, showing LEIA's robust performance across media,\ndata collection methods, and annotation schemes. Our results show that LEIA\ngeneralizes its classification of anger, happiness, and sadness beyond the\ndomain it was trained on. LEIA can be applied in future research to provide\nbetter identification of emotions in text from the perspective of the writer.\nThe models produced for this article are publicly available at\nhttps://huggingface.co/LEIA", "published": "2023-04-21 14:17:10", "link": "http://arxiv.org/abs/2304.10973v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with\n  Self-Correction", "abstract": "There is currently a significant gap between the performance of fine-tuned\nmodels and prompting approaches using Large Language Models (LLMs) on the\nchallenging task of text-to-SQL, as evaluated on datasets such as Spider. To\nimprove the performance of LLMs in the reasoning process, we study how\ndecomposing the task into smaller sub-tasks can be effective. In particular, we\nshow that breaking down the generation problem into sub-problems and feeding\nthe solutions of those sub-problems into LLMs can be an effective approach for\nsignificantly improving their performance. Our experiments with three LLMs show\nthat this approach consistently improves their simple few-shot performance by\nroughly 10%, pushing the accuracy of LLMs towards SOTA or surpassing it. On the\nholdout test set of Spider, the SOTA, in terms of execution accuracy, was 79.9\nand the new SOTA at the time of this writing using our approach is 85.3. Our\napproach with in-context learning beats many heavily fine-tuned models by at\nleast 5%. Additionally, when evaluated on the BIRD benchmark, our approach\nachieved an execution accuracy of 55.9%, setting a new SOTA on its holdout test\nset.", "published": "2023-04-21 15:02:18", "link": "http://arxiv.org/abs/2304.11015v3", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Inducing anxiety in large language models can induce bias", "abstract": "Large language models (LLMs) are transforming research on machine learning\nwhile galvanizing public debates. Understanding not only when these models work\nwell and succeed but also why they fail and misbehave is of great societal\nrelevance. We propose to turn the lens of psychiatry, a framework used to\ndescribe and modify maladaptive behavior, to the outputs produced by these\nmodels. We focus on twelve established LLMs and subject them to a questionnaire\ncommonly used in psychiatry. Our results show that six of the latest LLMs\nrespond robustly to the anxiety questionnaire, producing comparable anxiety\nscores to humans. Moreover, the LLMs' responses can be predictably changed by\nusing anxiety-inducing prompts. Anxiety-induction not only influences LLMs'\nscores on an anxiety questionnaire but also influences their behavior in a\npreviously-established benchmark measuring biases such as racism and ageism.\nImportantly, greater anxiety-inducing text leads to stronger increases in\nbiases, suggesting that how anxiously a prompt is communicated to large\nlanguage models has a strong influence on their behavior in applied settings.\nThese results demonstrate the usefulness of methods taken from psychiatry for\nstudying the capable algorithms to which we increasingly delegate authority and\nautonomy.", "published": "2023-04-21 16:29:43", "link": "http://arxiv.org/abs/2304.11111v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Who's the Best Detective? LLMs vs. MLs in Detecting Incoherent Fourth\n  Grade Math Answers", "abstract": "Written answers to open-ended questions can have a higher long-term effect on\nlearning than multiple-choice questions. However, it is critical that teachers\nimmediately review the answers, and ask to redo those that are incoherent. This\ncan be a difficult task and can be time-consuming for teachers. A possible\nsolution is to automate the detection of incoherent answers. One option is to\nautomate the review with Large Language Models (LLM). In this paper, we analyze\nthe responses of fourth graders in mathematics using three LLMs: GPT-3, BLOOM,\nand YOU. We used them with zero, one, two, three and four shots. We compared\ntheir performance with the results of various classifiers trained with Machine\nLearning (ML). We found that LLMs perform worse than MLs in detecting\nincoherent answers. The difficulty seems to reside in recursive questions that\ncontain both questions and answers, and in responses from students with typical\nfourth-grader misspellings. Upon closer examination, we have found that the\nChatGPT model faces the same challenges.", "published": "2023-04-21 21:25:30", "link": "http://arxiv.org/abs/2304.11257v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Dark Side of ChatGPT: Legal and Ethical Challenges from Stochastic\n  Parrots and Hallucination", "abstract": "With the launch of ChatGPT, Large Language Models (LLMs) are shaking up our\nwhole society, rapidly altering the way we think, create and live. For\ninstance, the GPT integration in Bing has altered our approach to online\nsearching. While nascent LLMs have many advantages, new legal and ethical risks\nare also emerging, stemming in particular from stochastic parrots and\nhallucination. The EU is the first and foremost jurisdiction that has focused\non the regulation of AI models. However, the risks posed by the new LLMs are\nlikely to be underestimated by the emerging EU regulatory paradigm. Therefore,\nthis correspondence warns that the European AI regulatory paradigm must evolve\nfurther to mitigate such risks.", "published": "2023-04-21 16:40:54", "link": "http://arxiv.org/abs/2304.14347v1", "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Generative AI Perceptions: A Survey to Measure the Perceptions of\n  Faculty, Staff, and Students on Generative AI Tools in Academia", "abstract": "ChatGPT is a natural language processing tool that can engage in human-like\nconversations and generate coherent and contextually relevant responses to\nvarious prompts. ChatGPT is capable of understanding natural text that is input\nby a user and generating appropriate responses in various forms. This tool\nrepresents a major step in how humans are interacting with technology. This\npaper specifically focuses on how ChatGPT is revolutionizing the realm of\nengineering education and the relationship between technology, students, and\nfaculty and staff. Because this tool is quickly changing and improving with the\npotential for even greater future capability, it is a critical time to collect\npertinent data. A survey was created to measure the effects of ChatGPT on\nstudents, faculty, and staff. This survey is shared as a Texas A&M University\ntechnical report to allow other universities and entities to use this survey\nand measure the effects elsewhere.", "published": "2023-04-21 23:08:39", "link": "http://arxiv.org/abs/2304.14415v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "KitchenScale: Learning to predict ingredient quantities from recipe\n  contexts", "abstract": "Determining proper quantities for ingredients is an essential part of cooking\npractice from the perspective of enriching tastiness and promoting healthiness.\nWe introduce KitchenScale, a fine-tuned Pre-trained Language Model (PLM) that\npredicts a target ingredient's quantity and measurement unit given its recipe\ncontext. To effectively train our KitchenScale model, we formulate an\ningredient quantity prediction task that consists of three sub-tasks which are\ningredient measurement type classification, unit classification, and quantity\nregression task. Furthermore, we utilized transfer learning of cooking\nknowledge from recipe texts to PLMs. We adopted the Discrete Latent Exponent\n(DExp) method to cope with high variance of numerical scales in recipe corpora.\nExperiments with our newly constructed dataset and recommendation examples\ndemonstrate KitchenScale's understanding of various recipe contexts and\ngeneralizability in predicting ingredient quantities. We implemented a web\napplication for KitchenScale to demonstrate its functionality in recommending\ningredient quantities expressed in numerals (e.g., 2) with units (e.g., ounce).", "published": "2023-04-21 04:28:16", "link": "http://arxiv.org/abs/2304.10739v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NA", "math.NA", "H.4.m"], "primary_category": "cs.CL"}
{"title": "Non-autoregressive End-to-end Approaches for Joint Automatic Speech\n  Recognition and Spoken Language Understanding", "abstract": "This paper presents the use of non-autoregressive (NAR) approaches for joint\nautomatic speech recognition (ASR) and spoken language understanding (SLU)\ntasks. The proposed NAR systems employ a Conformer encoder that applies\nconnectionist temporal classification (CTC) to transcribe the speech utterance\ninto raw ASR hypotheses, which are further refined with a bidirectional encoder\nrepresentations from Transformers (BERT)-like decoder. In the meantime, the\nintent and slot labels of the utterance are predicted simultaneously using the\nsame decoder. Both Mask-CTC and self-conditioned CTC (SC-CTC) approaches are\nexplored for this study. Experiments conducted on the SLURP dataset show that\nthe proposed SC-Mask-CTC NAR system achieves 3.7% and 3.2% absolute gains in\nSLU metrics and a competitive level of ASR accuracy, when compared to a\nConformer-Transformer based autoregressive (AR) model. Additionally, the NAR\nsystems achieve 6x faster decoding speed than the AR baseline.", "published": "2023-04-21 10:31:44", "link": "http://arxiv.org/abs/2304.10869v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Heart Rate Extraction from Abdominal Audio Signals", "abstract": "Abdominal sounds (ABS) have been traditionally used for assessing\ngastrointestinal (GI) disorders. However, the assessment requires a trained\nmedical professional to perform multiple abdominal auscultation sessions, which\nis resource-intense and may fail to provide an accurate picture of patients'\ncontinuous GI wellbeing. This has generated a technological interest in\ndeveloping wearables for continuous capture of ABS, which enables a fuller\npicture of patient's GI status to be obtained at reduced cost. This paper seeks\nto evaluate the feasibility of extracting heart rate (HR) from such ABS\nmonitoring devices. The collection of HR directly from these devices would\nenable gathering vital signs alongside GI data without the need for additional\nwearable devices, providing further cost benefits and improving general\nusability. We utilised a dataset containing 104 hours of ABS audio, collected\nfrom the abdomen using an e-stethoscope, and electrocardiogram as ground truth.\nOur evaluation shows for the first time that we can successfully extract HR\nfrom audio collected from a wearable on the abdomen. As heart sounds collected\nfrom the abdomen suffer from significant noise from GI and respiratory tracts,\nwe leverage wavelet denoising for improved heart beat detection. The mean\nabsolute error of the algorithm for average HR is 3.4 BPM with mean directional\nerror of -1.2 BPM over the whole dataset. A comparison to\nphotoplethysmography-based wearable HR sensors shows that our approach exhibits\ncomparable accuracy to consumer wrist-worn wearables for average and\ninstantaneous heart rate.", "published": "2023-04-21 15:08:15", "link": "http://arxiv.org/abs/2304.11020v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CLaMP: Contrastive Language-Music Pre-training for Cross-Modal Symbolic\n  Music Information Retrieval", "abstract": "We introduce CLaMP: Contrastive Language-Music Pre-training, which learns\ncross-modal representations between natural language and symbolic music using a\nmusic encoder and a text encoder trained jointly with a contrastive loss. To\npre-train CLaMP, we collected a large dataset of 1.4 million music-text pairs.\nIt employed text dropout as a data augmentation technique and bar patching to\nefficiently represent music data which reduces sequence length to less than\n10\\%. In addition, we developed a masked music model pre-training objective to\nenhance the music encoder's comprehension of musical context and structure.\nCLaMP integrates textual information to enable semantic search and zero-shot\nclassification for symbolic music, surpassing the capabilities of previous\nmodels. To support the evaluation of semantic search and music classification,\nwe publicly release WikiMusicText (WikiMT), a dataset of 1010 lead sheets in\nABC notation, each accompanied by a title, artist, genre, and description. In\ncomparison to state-of-the-art models that require fine-tuning, zero-shot CLaMP\ndemonstrated comparable or superior performance on score-oriented datasets. Our\nmodels and code are available at\nhttps://github.com/microsoft/muzic/tree/main/clamp.", "published": "2023-04-21 15:23:00", "link": "http://arxiv.org/abs/2304.11029v4", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A vector quantized masked autoencoder for speech emotion recognition", "abstract": "Recent years have seen remarkable progress in speech emotion recognition\n(SER), thanks to advances in deep learning techniques. However, the limited\navailability of labeled data remains a significant challenge in the field.\nSelf-supervised learning has recently emerged as a promising solution to\naddress this challenge. In this paper, we propose the vector quantized masked\nautoencoder for speech (VQ-MAE-S), a self-supervised model that is fine-tuned\nto recognize emotions from speech signals. The VQ-MAE-S model is based on a\nmasked autoencoder (MAE) that operates in the discrete latent space of a\nvector-quantized variational autoencoder. Experimental results show that the\nproposed VQ-MAE-S model, pre-trained on the VoxCeleb2 dataset and fine-tuned on\nemotional speech data, outperforms an MAE working on the raw spectrogram\nrepresentation and other state-of-the-art methods in SER.", "published": "2023-04-21 16:37:57", "link": "http://arxiv.org/abs/2304.11117v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Small-footprint slimmable networks for keyword spotting", "abstract": "In this work, we present Slimmable Neural Networks applied to the problem of\nsmall-footprint keyword spotting. We show that slimmable neural networks allow\nus to create super-nets from Convolutioanl Neural Networks and Transformers,\nfrom which sub-networks of different sizes can be extracted. We demonstrate the\nusefulness of these models on in-house Alexa data and Google Speech Commands,\nand focus our efforts on models for the on-device use case, limiting ourselves\nto less than 250k parameters. We show that slimmable models can match (and in\nsome cases, outperform) models trained from scratch. Slimmable neural networks\nare therefore a class of models particularly useful when the same functionality\nis to be replicated at different memory and compute budgets, with different\naccuracy requirements.", "published": "2023-04-21 12:59:37", "link": "http://arxiv.org/abs/2304.12183v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
