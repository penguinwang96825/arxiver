{"title": "A Syntax-Guided Grammatical Error Correction Model with Dependency Tree\n  Correction", "abstract": "Grammatical Error Correction (GEC) is a task of detecting and correcting\ngrammatical errors in sentences. Recently, neural machine translation systems\nhave become popular approaches for this task. However, these methods lack the\nuse of syntactic knowledge which plays an important role in the correction of\ngrammatical errors. In this work, we propose a syntax-guided GEC model (SG-GEC)\nwhich adopts the graph attention mechanism to utilize the syntactic knowledge\nof dependency trees. Considering the dependency trees of the grammatically\nincorrect source sentences might provide incorrect syntactic knowledge, we\npropose a dependency tree correction task to deal with it. Combining with data\naugmentation method, our model achieves strong performances without using any\nlarge pre-trained models. We evaluate our model on public benchmarks of GEC\ntask and it achieves competitive results.", "published": "2021-11-05 07:07:48", "link": "http://arxiv.org/abs/2111.03294v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Feature Selective Likelihood Ratio Estimator for Low- and Zero-frequency\n  N-grams", "abstract": "In natural language processing (NLP), the likelihood ratios (LRs) of N-grams\nare often estimated from the frequency information. However, a corpus contains\nonly a fraction of the possible N-grams, and most of them occur infrequently.\nHence, we desire an LR estimator for low- and zero-frequency N-grams. One way\nto achieve this is to decompose the N-grams into discrete values, such as\nletters and words, and take the product of the LRs for the values. However,\nbecause this method deals with a large number of discrete values, the running\ntime and memory usage for estimation are problematic. Moreover, use of\nunnecessary discrete values causes deterioration of the estimation accuracy.\nTherefore, this paper proposes combining the aforementioned method with the\nfeature selection method used in document classification, and shows that our\nestimator provides effective and efficient estimation results for low- and\nzero-frequency N-grams.", "published": "2021-11-05 09:38:43", "link": "http://arxiv.org/abs/2111.03350v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Developing Successful Shared Tasks on Offensive Language Identification\n  for Dravidian Languages", "abstract": "With the fast growth of mobile computing and Web technologies, offensive\nlanguage has become more prevalent on social networking platforms. Since\noffensive language identification in local languages is essential to moderate\nthe social media content, in this paper we work with three Dravidian languages,\nnamely Malayalam, Tamil, and Kannada, that are under-resourced. We present an\nevaluation task at FIRE 2020- HASOC-DravidianCodeMix and DravidianLangTech at\nEACL 2021, designed to provide a framework for comparing different approaches\nto this problem. This paper describes the data creation, defines the task,\nlists the participating systems, and discusses various methods.", "published": "2021-11-05 10:31:25", "link": "http://arxiv.org/abs/2111.03375v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialogue Inspectional Summarization with Factual Inconsistency Awareness", "abstract": "Dialogue summarization has been extensively studied and applied, where the\nprior works mainly focused on exploring superior model structures to align the\ninput dialogue and the output summary. However, for professional dialogues\n(e.g., legal debate and medical diagnosis), semantic/statistical alignment can\nhardly fill the logical/factual gap between input dialogue discourse and\nsummary output with external knowledge. In this paper, we mainly investigate\nthe factual inconsistency problem for Dialogue Inspectional Summarization (DIS)\nunder non-pretraining and pretraining settings. An innovative end-to-end\ndialogue summary generation framework is proposed with two auxiliary tasks:\nExpectant Factual Aspect Regularization (EFAR) and Missing Factual Entity\nDiscrimination (MFED). Comprehensive experiments demonstrate that the proposed\nmodel can generate a more readable summary with accurate coverage of factual\naspects as well as informing the user with potential missing facts detected\nfrom the input dialogue for further human intervention.", "published": "2021-11-05 06:26:22", "link": "http://arxiv.org/abs/2111.03284v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Impact of Temporal Representations on Metaphor Detection", "abstract": "State-of-the-art approaches for metaphor detection compare their literal - or\ncore - meaning and their contextual meaning using metaphor classifiers based on\nneural networks. However, metaphorical expressions evolve over time due to\nvarious reasons, such as cultural and societal impact. Metaphorical expressions\nare known to co-evolve with language and literal word meanings, and even drive,\nto some extent, this evolution. This poses the question of whether different,\npossibly time-specific, representations of literal meanings may impact the\nmetaphor detection task. To the best of our knowledge, this is the first study\nthat examines the metaphor detection task with a detailed exploratory analysis\nwhere different temporal and static word embeddings are used to account for\ndifferent representations of literal meanings. Our experimental analysis is\nbased on three popular benchmarks used for metaphor detection and word\nembeddings extracted from different corpora and temporally aligned using\ndifferent state-of-the-art approaches. The results suggest that the usage of\ndifferent static word embedding methods does impact the metaphor detection task\nand some temporal word embeddings slightly outperform static methods. However,\nthe results also suggest that temporal word embeddings may provide\nrepresentations of the core meaning of the metaphor even too close to their\ncontextual meaning, thus confusing the classifier. Overall, the interaction\nbetween temporal language evolution and metaphor detection appears tiny in the\nbenchmark datasets used in our experiments. This suggests that future work for\nthe computational analysis of this important linguistic phenomenon should first\nstart by creating a new dataset where this interaction is better represented.", "published": "2021-11-05 08:43:21", "link": "http://arxiv.org/abs/2111.03320v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LTL under reductions with weaker conditions than stutter-invariance", "abstract": "Verification of properties expressed as-regular languages such as LTL can\nbenefit hugely from stutter-insensitivity, using a diverse set of reduction\nstrategies. However properties that are not stutter-insensitive, for instance\ndue to the use of the neXt operator of LTL or to some form of counting in the\nlogic, are not covered by these techniques in general. We propose in this paper\nto study a weaker property than stutter-insensitivity. In a stutter insensitive\nlanguage both adding and removing stutter to a word does not change its\nacceptance, any stuttering can be abstracted away; by decomposing this\nequivalence relation into two implications we obtain weaker conditions. We\ndefine a shortening insensitive language where any word that stutters less than\na word in the language must also belong to the language. A lengthening\ninsensitive language has the dual property. A semi-decision procedure is then\nintroduced to reliably prove shortening insensitive properties or deny\nlengthening insensitive properties while working with a reduction of a system.\nA reduction has the property that it can only shorten runs. Lipton's\ntransaction reductions or Petri net agglomerations are examples of eligible\nstructural reduction strategies. An implementation and experimental evidence is\nprovided showing most nonrandom properties sensitive to stutter are actually\nshortening or lengthening insensitive. Performance of experiments on a large\n(random) benchmark from the model-checking competition indicate that despite\nbeing a semi-decision procedure, the approach can still improve state of the\nart verification tools.", "published": "2021-11-05 09:29:21", "link": "http://arxiv.org/abs/2111.03342v2", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
{"title": "Monitoring geometrical properties of word embeddings for detecting the\n  emergence of new topics", "abstract": "Slow emerging topic detection is a task between event detection, where we\naggregate behaviors of different words on short period of time, and language\nevolution, where we monitor their long term evolution. In this work, we tackle\nthe problem of early detection of slowly emerging new topics. To this end, we\ngather evidence of weak signals at the word level. We propose to monitor the\nbehavior of words representation in an embedding space and use one of its\ngeometrical properties to characterize the emergence of topics. As evaluation\nis typically hard for this kind of task, we present a framework for\nquantitative evaluation. We show positive results that outperform\nstate-of-the-art methods on two public datasets of press and scientific\narticles.", "published": "2021-11-05 13:31:56", "link": "http://arxiv.org/abs/2111.03496v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sexism Identification in Tweets and Gabs using Deep Neural Networks", "abstract": "Through anonymisation and accessibility, social media platforms have\nfacilitated the proliferation of hate speech, prompting increased research in\ndeveloping automatic methods to identify these texts. This paper explores the\nclassification of sexism in text using a variety of deep neural network model\narchitectures such as Long-Short-Term Memory (LSTMs) and Convolutional Neural\nNetworks (CNNs). These networks are used in conjunction with transfer learning\nin the form of Bidirectional Encoder Representations from Transformers (BERT)\nand DistilBERT models, along with data augmentation, to perform binary and\nmulticlass sexism classification on the dataset of tweets and gabs from the\nsEXism Identification in Social neTworks (EXIST) task in IberLEF 2021. The\nmodels are seen to perform comparatively to those from the competition, with\nthe best performances seen using BERT and a multi-filter CNN model. Data\naugmentation further improves these results for the multi-class classification\ntask. This paper also explores the errors made by the models and discusses the\ndifficulty in automatically classifying sexism due to the subjectivity of the\nlabels and the complexity of natural language used in social media.", "published": "2021-11-05 16:57:08", "link": "http://arxiv.org/abs/2111.03612v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Grounded Graph Decoding Improves Compositional Generalization in\n  Question Answering", "abstract": "Question answering models struggle to generalize to novel compositions of\ntraining patterns, such to longer sequences or more complex test structures.\nCurrent end-to-end models learn a flat input embedding which can lose input\nsyntax context. Prior approaches improve generalization by learning permutation\ninvariant models, but these methods do not scale to more complex train-test\nsplits. We propose Grounded Graph Decoding, a method to improve compositional\ngeneralization of language representations by grounding structured predictions\nwith an attention mechanism. Grounding enables the model to retain syntax\ninformation from the input in thereby significantly improving generalization\nover complex inputs. By predicting a structured graph containing conjunctions\nof query clauses, we learn a group invariant representation without making\nassumptions on the target domain. Our model significantly outperforms\nstate-of-the-art baselines on the Compositional Freebase Questions (CFQ)\ndataset, a challenging benchmark for compositional generalization in question\nanswering. Moreover, we effectively solve the MCD1 split with 98% accuracy.", "published": "2021-11-05 17:50:14", "link": "http://arxiv.org/abs/2111.03642v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Curious Layperson: Fine-Grained Image Recognition without Expert\n  Labels", "abstract": "Most of us are not experts in specific fields, such as ornithology.\nNonetheless, we do have general image and language understanding capabilities\nthat we use to match what we see to expert resources. This allows us to expand\nour knowledge and perform novel tasks without ad-hoc external supervision. On\nthe contrary, machines have a much harder time consulting expert-curated\nknowledge bases unless trained specifically with that knowledge in mind. Thus,\nin this paper we consider a new problem: fine-grained image recognition without\nexpert annotations, which we address by leveraging the vast knowledge available\nin web encyclopedias. First, we learn a model to describe the visual appearance\nof objects using non-expert image descriptions. We then train a fine-grained\ntextual similarity model that matches image descriptions with documents on a\nsentence-level basis. We evaluate the method on two datasets and compare with\nseveral strong baselines and the state of the art in cross-modal retrieval.\nCode is available at: https://github.com/subhc/clever", "published": "2021-11-05 17:58:37", "link": "http://arxiv.org/abs/2111.03651v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Leveraging Sentiment Analysis Knowledge to Solve Emotion Detection Tasks", "abstract": "Identifying and understanding underlying sentiment or emotions in text is a\nkey component of multiple natural language processing applications. While\nsimple polarity sentiment analysis is a well-studied subject, fewer advances\nhave been made in identifying more complex, finer-grained emotions using only\ntextual data. In this paper, we present a Transformer-based model with a Fusion\nof Adapter layers which leverages knowledge from more simple sentiment analysis\ntasks to improve the emotion detection task on large scale dataset, such as\nCMU-MOSEI, using the textual modality only. Results show that our proposed\nmethod is competitive with other approaches. We obtained state-of-the-art\nresults for emotion recognition on CMU-MOSEI even while using only the textual\nmodality.", "published": "2021-11-05 20:06:58", "link": "http://arxiv.org/abs/2111.03715v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "IBERT: Idiom Cloze-style reading comprehension with Attention", "abstract": "Idioms are special fixed phrases usually derived from stories. They are\ncommonly used in casual conversations and literary writings. Their meanings are\nusually highly non-compositional. The idiom cloze task is a challenge problem\nin Natural Language Processing (NLP) research problem. Previous approaches to\nthis task are built on sequence-to-sequence (Seq2Seq) models and achieved\nreasonably well performance on existing datasets. However, they fall short in\nunderstanding the highly non-compositional meaning of idiomatic expressions.\nThey also do not consider both the local and global context at the same time.\nIn this paper, we proposed a BERT-based embedding Seq2Seq model that encodes\nidiomatic expressions and considers them in both global and local context. Our\nmodel uses XLNET as the encoder and RoBERTa for choosing the most probable\nidiom for a given context. Experiments on the EPIE Static Corpus dataset show\nthat our model performs better than existing state-of-the-arts.", "published": "2021-11-05 21:37:15", "link": "http://arxiv.org/abs/2112.02994v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TaskDrop: A Competitive Baseline for Continual Learning of Sentiment\n  Classification", "abstract": "In this paper, we study the multi-task sentiment classification problem in\nthe continual learning setting, i.e., a model is sequentially trained to\nclassifier the sentiment of reviews of products in a particular category. The\nuse of common sentiment words in reviews of different product categories leads\nto large cross-task similarity, which differentiates it from continual learning\nin other domains. This knowledge sharing nature renders forgetting reduction\nfocused approaches less effective for the problem under consideration. Unlike\nexisting approaches, where task-specific masks are learned with specifically\npresumed training objectives, we propose an approach called Task-aware Dropout\n(TaskDrop) to generate masks in a random way. While the standard dropout\ngenerates and applies random masks for each training instance per epoch for\neffective regularization, TaskDrop applies random masking for task-wise\ncapacity allocation and reuse. We conducted experimental studies on three\nmulti-task review datasets and made comparison to various baselines and\nstate-of-the-art approaches. Our empirical results show that regardless of\nsimplicity, TaskDrop overall achieved competitive performances for all the\nthree datasets, especially after relative long term learning. This demonstrates\nthat the proposed random capacity allocation mechanism works well for continual\nsentiment classification.", "published": "2021-11-05 15:32:23", "link": "http://arxiv.org/abs/2112.02995v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sequential Randomized Smoothing for Adversarially Robust Speech\n  Recognition", "abstract": "While Automatic Speech Recognition has been shown to be vulnerable to\nadversarial attacks, defenses against these attacks are still lagging.\nExisting, naive defenses can be partially broken with an adaptive attack. In\nclassification tasks, the Randomized Smoothing paradigm has been shown to be\neffective at defending models. However, it is difficult to apply this paradigm\nto ASR tasks, due to their complexity and the sequential nature of their\noutputs. Our paper overcomes some of these challenges by leveraging\nspeech-specific tools like enhancement and ROVER voting to design an ASR model\nthat is robust to perturbations. We apply adaptive versions of state-of-the-art\nattacks, such as the Imperceptible ASR attack, to our model, and show that our\nstrongest defense is robust to all attacks that use inaudible noise, and can\nonly be broken with very high distortion.", "published": "2021-11-05 21:51:40", "link": "http://arxiv.org/abs/2112.03000v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An overview of event extraction and its applications", "abstract": "With the rapid development of information technology, online platforms have\nproduced enormous text resources. As a particular form of Information\nExtraction (IE), Event Extraction (EE) has gained increasing popularity due to\nits ability to automatically extract events from human language. However, there\nare limited literature surveys on event extraction. Existing review works\neither spend much effort describing the details of various approaches or focus\non a particular field. This study provides a comprehensive overview of the\nstate-of-the-art event extraction methods and their applications from text,\nincluding closed-domain and open-domain event extraction. A trait of this\nsurvey is that it provides an overview in moderate complexity, avoiding\ninvolving too many details of particular approaches. This study focuses on\ndiscussing the common characters, application fields, advantages, and\ndisadvantages of representative works, ignoring the specificities of individual\napproaches. Finally, we summarize the common issues, current solutions, and\nfuture research directions. We hope this work could help researchers and\npractitioners obtain a quick overview of recent event extraction.", "published": "2021-11-05 01:37:47", "link": "http://arxiv.org/abs/2111.03212v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Context-Aware Transformer Transducer for Speech Recognition", "abstract": "End-to-end (E2E) automatic speech recognition (ASR) systems often have\ndifficulty recognizing uncommon words, that appear infrequently in the training\ndata. One promising method, to improve the recognition accuracy on such rare\nwords, is to latch onto personalized/contextual information at inference. In\nthis work, we present a novel context-aware transformer transducer (CATT)\nnetwork that improves the state-of-the-art transformer-based ASR system by\ntaking advantage of such contextual signals. Specifically, we propose a\nmulti-head attention-based context-biasing network, which is jointly trained\nwith the rest of the ASR sub-networks. We explore different techniques to\nencode contextual data and to create the final attention context vectors. We\nalso leverage both BLSTM and pretrained BERT based models to encode contextual\ndata and guide the network training. Using an in-house far-field dataset, we\nshow that CATT, using a BERT based context encoder, improves the word error\nrate of the baseline transformer transducer and outperforms an existing deep\ncontextual model by 24.2% and 19.4% respectively.", "published": "2021-11-05 04:14:35", "link": "http://arxiv.org/abs/2111.03250v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Dataset of Fake News Detection and Fact Verification: A Survey", "abstract": "The rapid increase in fake news, which causes significant damage to society,\ntriggers many fake news related studies, including the development of fake news\ndetection and fact verification techniques. The resources for these studies are\nmainly available as public datasets taken from Web data. We surveyed 118\ndatasets related to fake news research on a large scale from three\nperspectives: (1) fake news detection, (2) fact verification, and (3) other\ntasks; for example, the analysis of fake news and satire detection. We also\ndescribe in detail their utilization tasks and their characteristics. Finally,\nwe highlight the challenges in the fake news dataset construction and some\nresearch opportunities that address these challenges. Our survey facilitates\nfake news research by helping researchers find suitable datasets without\nreinventing the wheel, and thereby, improves fake news studies in depth.", "published": "2021-11-05 07:22:16", "link": "http://arxiv.org/abs/2111.03299v1", "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Effective Cross-Utterance Language Modeling for Conversational Speech\n  Recognition", "abstract": "Conversational speech normally is embodied with loose syntactic structures at\nthe utterance level but simultaneously exhibits topical coherence relations\nacross consecutive utterances. Prior work has shown that capturing longer\ncontext information with a recurrent neural network or long short-term memory\nlanguage model (LM) may suffer from the recent bias while excluding the\nlong-range context. In order to capture the long-term semantic interactions\namong words and across utterances, we put forward disparate conversation\nhistory fusion methods for language modeling in automatic speech recognition\n(ASR) of conversational speech. Furthermore, a novel audio-fusion mechanism is\nintroduced, which manages to fuse and utilize the acoustic embeddings of a\ncurrent utterance and the semantic content of its corresponding conversation\nhistory in a cooperative way. To flesh out our ideas, we frame the ASR N-best\nhypothesis rescoring task as a prediction problem, leveraging BERT, an iconic\npre-trained LM, as the ingredient vehicle to facilitate selection of the oracle\nhypothesis from a given N-best hypothesis list. Empirical experiments conducted\non the AMI benchmark dataset seem to demonstrate the feasibility and efficacy\nof our methods in relation to some current top-of-line methods. The proposed\nmethods not only achieve significant inference time reduction but also improve\nthe ASR performance for conversational speech.", "published": "2021-11-05 09:07:23", "link": "http://arxiv.org/abs/2111.03333v5", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Negative Sample is Negative in Its Own Way: Tailoring Negative Sentences\n  for Image-Text Retrieval", "abstract": "Matching model is essential for Image-Text Retrieval framework. Existing\nresearch usually train the model with a triplet loss and explore various\nstrategy to retrieve hard negative sentences in the dataset. We argue that\ncurrent retrieval-based negative sample construction approach is limited in the\nscale of the dataset thus fail to identify negative sample of high difficulty\nfor every image. We propose our TAiloring neGative Sentences with\nDiscrimination and Correction (TAGS-DC) to generate synthetic sentences\nautomatically as negative samples. TAGS-DC is composed of masking and refilling\nto generate synthetic negative sentences with higher difficulty. To keep the\ndifficulty during training, we mutually improve the retrieval and generation\nthrough parameter sharing. To further utilize fine-grained semantic of mismatch\nin the negative sentence, we propose two auxiliary tasks, namely word\ndiscrimination and word correction to improve the training. In experiments, we\nverify the effectiveness of our model on MS-COCO and Flickr30K compared with\ncurrent state-of-the-art models and demonstrates its robustness and\nfaithfulness in the further analysis. Our code is available in\nhttps://github.com/LibertFan/TAGS.", "published": "2021-11-05 09:36:41", "link": "http://arxiv.org/abs/2111.03349v1", "categories": ["cs.CV", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "Conformer-based Hybrid ASR System for Switchboard Dataset", "abstract": "The recently proposed conformer architecture has been successfully used for\nend-to-end automatic speech recognition (ASR) architectures achieving\nstate-of-the-art performance on different datasets. To our best knowledge, the\nimpact of using conformer acoustic model for hybrid ASR is not investigated. In\nthis paper, we present and evaluate a competitive conformer-based hybrid model\ntraining recipe. We study different training aspects and methods to improve\nword-error-rate as well as to increase training speed. We apply time\ndownsampling methods for efficient training and use transposed convolutions to\nupsample the output sequence again. We conduct experiments on Switchboard 300h\ndataset and our conformer-based hybrid model achieves competitive results\ncompared to other architectures. It generalizes very well on Hub5'01 test set\nand outperforms the BLSTM-based hybrid model significantly.", "published": "2021-11-05 12:03:18", "link": "http://arxiv.org/abs/2111.03442v2", "categories": ["cs.CL", "eess.AS", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Disengagement Cause-and-Effect Relationships Extraction Using an NLP\n  Pipeline", "abstract": "The advancement in machine learning and artificial intelligence is promoting\nthe testing and deployment of autonomous vehicles (AVs) on public roads. The\nCalifornia Department of Motor Vehicles (CA DMV) has launched the Autonomous\nVehicle Tester Program, which collects and releases reports related to\nAutonomous Vehicle Disengagement (AVD) from autonomous driving. Understanding\nthe causes of AVD is critical to improving the safety and stability of the AV\nsystem and provide guidance for AV testing and deployment. In this work, a\nscalable end-to-end pipeline is constructed to collect, process, model, and\nanalyze the disengagement reports released from 2014 to 2020 using natural\nlanguage processing deep transfer learning. The analysis of disengagement data\nusing taxonomy, visualization and statistical tests revealed the trends of AV\ntesting, categorized cause frequency, and significant relationships between\ncauses and effects of AVD. We found that (1) manufacturers tested AVs\nintensively during the Spring and/or Winter, (2) test drivers initiated more\nthan 80% of the disengagement while more than 75% of the disengagement were led\nby errors in perception, localization & mapping, planning and control of the AV\nsystem itself, and (3) there was a significant relationship between the\ninitiator of AVD and the cause category. This study serves as a successful\npractice of deep transfer learning using pre-trained models and generates a\nconsolidated disengagement database allowing further investigation for other\nresearchers.", "published": "2021-11-05 14:00:59", "link": "http://arxiv.org/abs/2111.03511v1", "categories": ["cs.HC", "cs.CL", "cs.RO"], "primary_category": "cs.HC"}
{"title": "POSHAN: Cardinal POS Pattern Guided Attention for News Headline\n  Incongruence", "abstract": "Automatic detection of click-bait and incongruent news headlines is crucial\nto maintaining the reliability of the Web and has raised much research\nattention. However, most existing methods perform poorly when news headlines\ncontain contextually important cardinal values, such as a quantity or an\namount. In this work, we focus on this particular case and propose a neural\nattention based solution, which uses a novel cardinal Part of Speech (POS) tag\npattern based hierarchical attention network, namely POSHAN, to learn effective\nrepresentations of sentences in a news article. In addition, we investigate a\nnovel cardinal phrase guided attention, which uses word embeddings of the\ncontextually-important cardinal value and neighbouring words. In the\nexperiments conducted on two publicly available datasets, we observe that the\nproposed methodgives appropriate significance to cardinal values and\noutperforms all the baselines. An ablation study of POSHAN shows that the\ncardinal POS-tag pattern-based hierarchical attention is very effective for the\ncases in which headlines contain cardinal values.", "published": "2021-11-05 15:09:10", "link": "http://arxiv.org/abs/2111.03547v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "LILA: Language-Informed Latent Actions", "abstract": "We introduce Language-Informed Latent Actions (LILA), a framework for\nlearning natural language interfaces in the context of human-robot\ncollaboration. LILA falls under the shared autonomy paradigm: in addition to\nproviding discrete language inputs, humans are given a low-dimensional\ncontroller $-$ e.g., a 2 degree-of-freedom (DoF) joystick that can move\nleft/right and up/down $-$ for operating the robot. LILA learns to use language\nto modulate this controller, providing users with a language-informed control\nspace: given an instruction like \"place the cereal bowl on the tray,\" LILA may\nlearn a 2-DoF space where one dimension controls the distance from the robot's\nend-effector to the bowl, and the other dimension controls the robot's\nend-effector pose relative to the grasp point on the bowl. We evaluate LILA\nwith real-world user studies, where users can provide a language instruction\nwhile operating a 7-DoF Franka Emika Panda Arm to complete a series of complex\nmanipulation tasks. We show that LILA models are not only more sample efficient\nand performant than imitation learning and end-effector control baselines, but\nthat they are also qualitatively preferred by users.", "published": "2021-11-05 00:56:00", "link": "http://arxiv.org/abs/2111.03205v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Target Speech Extraction: Independent Vector Extraction Guided by\n  Supervised Speaker Identification", "abstract": "This manuscript proposes a novel robust procedure for the extraction of a\nspeaker of interest (SOI) from a mixture of audio sources. The estimation of\nthe SOI is performed via independent vector extraction (IVE). Since the blind\nIVE cannot distinguish the target source by itself, it is guided towards the\nSOI via frame-wise speaker identification based on deep learning. Still, an\nincorrect speaker can be extracted due to guidance failings, especially when\nprocessing challenging data. To identify such cases, we propose a criterion for\nnon-intrusively assessing the estimated speaker. It utilizes the same model as\nthe speaker identification, so no additional training is required. When\nincorrect extraction is detected, we propose a ``deflation'' step in which the\nincorrect source is subtracted from the mixture and, subsequently, another\nattempt to extract the SOI is performed. The process is repeated until\nsuccessful extraction is achieved. The proposed procedure is experimentally\ntested on artificial and real-world datasets containing challenging phenomena:\nsource movements, reverberation, transient noise, or microphone failures. The\nmethod is compared with state-of-the-art blind algorithms as well as with\ncurrent fully supervised deep learning-based methods.", "published": "2021-11-05 12:58:08", "link": "http://arxiv.org/abs/2111.03482v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Hybrid Spectrogram and Waveform Source Separation", "abstract": "Source separation models either work on the spectrogram or waveform domain.\nIn this work, we show how to perform end-to-end hybrid source separation,\nletting the model decide which domain is best suited for each source, and even\ncombining both. The proposed hybrid version of the Demucs architecture won the\nMusic Demixing Challenge 2021 organized by Sony. This architecture also comes\nwith additional improvements, such as compressed residual branches, local\nattention or singular value regularization. Overall, a 1.4 dB improvement of\nthe Signal-To-Distortion (SDR) was observed across all sources as measured on\nthe MusDB HQ dataset, an improvement confirmed by human subjective evaluation,\nwith an overall quality rated at 2.83 out of 5 (2.36 for the non hybrid\nDemucs), and absence of contamination at 3.04 (against 2.37 for the non hybrid\nDemucs and 2.44 for the second ranking model submitted at the competition).", "published": "2021-11-05 16:37:45", "link": "http://arxiv.org/abs/2111.03600v3", "categories": ["eess.AS", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Objective measurement of pitch extractors' responses to frequency\n  modulated sounds and two reference pitch extraction methods for analyzing\n  voice pitch responses to auditory stimulation", "abstract": "We propose an objective measurement method for pitch extractors' responses to\nfrequency-modulated signals. The method simultaneously measures the linear and\nthe non-linear time-invariant responses and random and time-varying responses.\nIt uses extended time-stretched pulses combined by binary orthogonal sequences.\nOur recent finding of involuntary voice pitch response to auditory stimulation\nwhile voicing motivated this proposal. The involuntary voice pitch response\nprovides means to investigate voice chain subsystems individually and\nobjectively. This response analysis requires reliable and precise pitch\nextraction. We found that existing pitch extractors failed to correctly analyze\nsignals used for auditory stimulation by using the proposed method. Therefore,\nwe propose two reference pitch extractors based on the instantaneous frequency\nanalysis and multi-resolution power spectrum analysis. The proposed extractors\ncorrectly analyze the test signals. We open-sourced MATLAB codes to measure\npitch extractors and codes for conducting the voice pitch response experiment\non our GitHub repository.", "published": "2021-11-05 17:27:45", "link": "http://arxiv.org/abs/2111.03629v2", "categories": ["cs.SD", "cs.HC", "eess.AS", "eess.SP", "94A12, 93C80, 42-08"], "primary_category": "cs.SD"}
{"title": "Oracle Teacher: Leveraging Target Information for Better Knowledge\n  Distillation of CTC Models", "abstract": "Knowledge distillation (KD), best known as an effective method for model\ncompression, aims at transferring the knowledge of a bigger network (teacher)\nto a much smaller network (student). Conventional KD methods usually employ the\nteacher model trained in a supervised manner, where output labels are treated\nonly as targets. Extending this supervised scheme further, we introduce a new\ntype of teacher model for connectionist temporal classification (CTC)-based\nsequence models, namely Oracle Teacher, that leverages both the source inputs\nand the output labels as the teacher model's input. Since the Oracle Teacher\nlearns a more accurate CTC alignment by referring to the target information, it\ncan provide the student with more optimal guidance. One potential risk for the\nproposed approach is a trivial solution that the model's output directly copies\nthe target input. Based on a many-to-one mapping property of the CTC algorithm,\nwe present a training strategy that can effectively prevent the trivial\nsolution and thus enables utilizing both source and target inputs for model\ntraining. Extensive experiments are conducted on two sequence learning tasks:\nspeech recognition and scene text recognition. From the experimental results,\nwe empirically show that the proposed model improves the students across these\ntasks while achieving a considerable speed-up in the teacher model's training\ntime.", "published": "2021-11-05 14:14:05", "link": "http://arxiv.org/abs/2111.03664v4", "categories": ["cs.LG", "eess.AS", "eess.IV"], "primary_category": "cs.LG"}
