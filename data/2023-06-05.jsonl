{"title": "Jambu: A historical linguistic database for South Asian languages", "abstract": "We introduce Jambu, a cognate database of South Asian languages which unifies\ndozens of previous sources in a structured and accessible format. The database\nincludes 287k lemmata from 602 lects, grouped together in 23k sets of cognates.\nWe outline the data wrangling necessary to compile the dataset and train neural\nmodels for reflex prediction on the Indo-Aryan subset of the data. We hope that\nJambu is an invaluable resource for all historical linguists and Indologists,\nand look towards further improvement and expansion of the database.", "published": "2023-06-05 00:32:57", "link": "http://arxiv.org/abs/2306.02514v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PLANNER: Generating Diversified Paragraph via Latent Language Diffusion\n  Model", "abstract": "Autoregressive models for text sometimes generate repetitive and low-quality\noutput because errors accumulate during the steps of generation. This issue is\noften attributed to exposure bias - the difference between how a model is\ntrained, and how it is used during inference. Denoising diffusion models\nprovide an alternative approach in which a model can revisit and revise its\noutput. However, they can be computationally expensive and prior efforts on\ntext have led to models that produce less fluent output compared to\nautoregressive models, especially for longer text and paragraphs. In this\npaper, we propose PLANNER, a model that combines latent semantic diffusion with\nautoregressive generation, to generate fluent text while exercising global\ncontrol over paragraphs. The model achieves this by combining an autoregressive\n\"decoding\" module with a \"planning\" module that uses latent diffusion to\ngenerate semantic paragraph embeddings in a coarse-to-fine manner. The proposed\nmethod is evaluated on various conditional generation tasks, and results on\nsemantic generation, text completion and summarization show its effectiveness\nin generating high-quality long-form text in an efficient manner.", "published": "2023-06-05 01:36:39", "link": "http://arxiv.org/abs/2306.02531v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt to be Consistent is Better than Self-Consistent? Few-Shot and\n  Zero-Shot Fact Verification with Pre-trained Language Models", "abstract": "Few-shot or zero-shot fact verification only relies on a few or no labeled\ntraining examples. In this paper, we propose a novel method called ProToCo, to\n\\underline{Pro}mpt pre-trained language models (PLMs) \\underline{To} be\n\\underline{Co}nsistent, for improving the factuality assessment capability of\nPLMs in the few-shot and zero-shot settings. Given a claim-evidence pair,\nProToCo generates multiple variants of the claim with different relations and\nframes a simple consistency mechanism as constraints for making compatible\npredictions across these variants. We update PLMs by using parameter-efficient\nfine-tuning (PEFT), leading to more accurate predictions in few-shot and\nzero-shot fact verification tasks. Our experiments on three public verification\ndatasets show that ProToCo significantly outperforms state-of-the-art few-shot\nfact verification baselines. With a small number of unlabeled instances,\nProToCo also outperforms the strong zero-shot learner T0 on zero-shot\nverification. Compared to large PLMs using in-context learning (ICL) method,\nProToCo outperforms OPT-30B and the Self-Consistency-enabled OPT-6.7B model in\nboth few- and zero-shot settings.", "published": "2023-06-05 03:49:13", "link": "http://arxiv.org/abs/2306.02569v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Early Rumor Detection Using Neural Hawkes Process with a New Benchmark\n  Dataset", "abstract": "Little attention has been paid on \\underline{EA}rly \\underline{R}umor\n\\underline{D}etection (EARD), and EARD performance was evaluated\ninappropriately on a few datasets where the actual early-stage information is\nlargely missing. To reverse such situation, we construct BEARD, a new\n\\underline{B}enchmark dataset for \\underline{EARD}, based on claims from\nfact-checking websites by trying to gather as many early relevant posts as\npossible. We also propose HEARD, a novel model based on neural\n\\underline{H}awkes process for \\underline{EARD}, which can guide a generic\nrumor detection model to make timely, accurate and stable predictions.\nExperiments show that HEARD achieves effective EARD performance on two commonly\nused general rumor detection datasets and our BEARD dataset.", "published": "2023-06-05 05:04:28", "link": "http://arxiv.org/abs/2306.02597v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Colexifications for Bootstrapping Cross-lingual Datasets: The Case of\n  Phonology, Concreteness, and Affectiveness", "abstract": "Colexification refers to the linguistic phenomenon where a single lexical\nform is used to convey multiple meanings. By studying cross-lingual\ncolexifications, researchers have gained valuable insights into fields such as\npsycholinguistics and cognitive sciences [Jackson et al.,2019]. While several\nmultilingual colexification datasets exist, there is untapped potential in\nusing this information to bootstrap datasets across such semantic features. In\nthis paper, we aim to demonstrate how colexifications can be leveraged to\ncreate such cross-lingual datasets. We showcase curation procedures which\nresult in a dataset covering 142 languages across 21 language families across\nthe world. The dataset includes ratings of concreteness and affectiveness,\nmapped with phonemes and phonological features. We further analyze the dataset\nalong different dimensions to demonstrate potential of the proposed procedures\nin facilitating further interdisciplinary research in psychology, cognitive\nscience, and multilingual natural language processing (NLP). Based on initial\ninvestigations, we observe that i) colexifications that are closer in\nconcreteness/affectiveness are more likely to colexify; ii) certain\ninitial/last phonemes are significantly correlated with\nconcreteness/affectiveness intra language families, such as /k/ as the initial\nphoneme in both Turkic and Tai-Kadai correlated with concreteness, and /p/ in\nDravidian and Sino-Tibetan correlated with Valence; iii) the type-to-token\nratio (TTR) of phonemes are positively correlated with concreteness across\nseveral language families, while the length of phoneme segments are negatively\ncorrelated with concreteness; iv) certain phonological features are negatively\ncorrelated with concreteness across languages. The dataset is made public\nonline for further research.", "published": "2023-06-05 07:32:21", "link": "http://arxiv.org/abs/2306.02646v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Grammar-based Sequence-to-Sequence Modeling with Decomposition\n  and Constraints", "abstract": "Neural QCFG is a grammar-based sequence-tosequence (seq2seq) model with\nstrong inductive biases on hierarchical structures. It excels in\ninterpretability and generalization but suffers from expensive inference. In\nthis paper, we study two low-rank variants of Neural QCFG for faster inference\nwith different trade-offs between efficiency and expressiveness. Furthermore,\nutilizing the symbolic interface provided by the grammar, we introduce two soft\nconstraints over tree hierarchy and source coverage. We experiment with various\ndatasets and find that our models outperform vanilla Neural QCFG in most\nsettings.", "published": "2023-06-05 08:05:05", "link": "http://arxiv.org/abs/2306.02671v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CELDA: Leveraging Black-box Language Model as Enhanced Classifier\n  without Labels", "abstract": "Utilizing language models (LMs) without internal access is becoming an\nattractive paradigm in the field of NLP as many cutting-edge LMs are released\nthrough APIs and boast a massive scale. The de-facto method in this type of\nblack-box scenario is known as prompting, which has shown progressive\nperformance enhancements in situations where data labels are scarce or\nunavailable. Despite their efficacy, they still fall short in comparison to\nfully supervised counterparts and are generally brittle to slight\nmodifications. In this paper, we propose Clustering-enhanced Linear\nDiscriminative Analysis, a novel approach that improves the text classification\naccuracy with a very weak-supervision signal (i.e., name of the labels). Our\nframework draws a precise decision boundary without accessing weights or\ngradients of the LM model or data labels. The core ideas of CELDA are twofold:\n(1) extracting a refined pseudo-labeled dataset from an unlabeled dataset, and\n(2) training a lightweight and robust model on the top of LM, which learns an\naccurate decision boundary from an extracted noisy dataset. Throughout in-depth\ninvestigations on various datasets, we demonstrated that CELDA reaches new\nstate-of-the-art in weakly-supervised text classification and narrows the gap\nwith a fully-supervised model. Additionally, our proposed methodology can be\napplied universally to any LM and has the potential to scale to larger models,\nmaking it a more viable option for utilizing large LMs.", "published": "2023-06-05 08:35:31", "link": "http://arxiv.org/abs/2306.02693v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PULSAR: Pre-training with Extracted Healthcare Terms for Summarising\n  Patients' Problems and Data Augmentation with Black-box Large Language Models", "abstract": "Medical progress notes play a crucial role in documenting a patient's\nhospital journey, including his or her condition, treatment plan, and any\nupdates for healthcare providers. Automatic summarisation of a patient's\nproblems in the form of a problem list can aid stakeholders in understanding a\npatient's condition, reducing workload and cognitive bias. BioNLP 2023 Shared\nTask 1A focuses on generating a list of diagnoses and problems from the\nprovider's progress notes during hospitalisation. In this paper, we introduce\nour proposed approach to this task, which integrates two complementary\ncomponents. One component employs large language models (LLMs) for data\naugmentation; the other is an abstractive summarisation LLM with a novel\npre-training objective for generating the patients' problems summarised as a\nlist. Our approach was ranked second among all submissions to the shared task.\nThe performance of our model on the development and test datasets shows that\nour approach is more robust on unknown data, with an improvement of up to 3.1\npoints over the same size of the larger model.", "published": "2023-06-05 10:17:50", "link": "http://arxiv.org/abs/2306.02754v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Transfer with Target Language-Ready Task Adapters", "abstract": "Adapters have emerged as a modular and parameter-efficient approach to\n(zero-shot) cross-lingual transfer. The established MAD-X framework employs\nseparate language and task adapters which can be arbitrarily combined to\nperform the transfer of any task to any target language. Subsequently, BAD-X,\nan extension of the MAD-X framework, achieves improved transfer at the cost of\nMAD-X's modularity by creating \"bilingual\" adapters specific to the\nsource-target language pair. In this work, we aim to take the best of both\nworlds by (i) fine-tuning task adapters adapted to the target language(s)\n(so-called \"target language-ready\" (TLR) adapters) to maintain high transfer\nperformance, but (ii) without sacrificing the highly modular design of MAD-X.\nThe main idea of \"target language-ready\" adapters is to resolve the\ntraining-vs-inference discrepancy of MAD-X: the task adapter \"sees\" the target\nlanguage adapter for the very first time during inference, and thus might not\nbe fully compatible with it. We address this mismatch by exposing the task\nadapter to the target language adapter during training, and empirically\nvalidate several variants of the idea: in the simplest form, we alternate\nbetween using the source and target language adapters during task adapter\ntraining, which can be generalized to cycling over any set of language\nadapters. We evaluate different TLR-based transfer configurations with varying\ndegrees of generality across a suite of standard cross-lingual benchmarks, and\nfind that the most general (and thus most modular) configuration consistently\noutperforms MAD-X and BAD-X on most tasks and languages.", "published": "2023-06-05 10:46:33", "link": "http://arxiv.org/abs/2306.02767v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "German CheXpert Chest X-ray Radiology Report Labeler", "abstract": "This study aimed to develop an algorithm to automatically extract annotations\nfor chest X-ray classification models from German thoracic radiology reports.\nAn automatic label extraction model was designed based on the CheXpert\narchitecture, and a web-based annotation interface was created for iterative\nimprovements. Results showed that automated label extraction can reduce time\nspent on manual labeling and improve overall modeling performance. The model\ntrained on automatically extracted labels performed competitively to manually\nlabeled data and strongly outperformed the model trained on publicly available\ndata.", "published": "2023-06-05 11:01:58", "link": "http://arxiv.org/abs/2306.02777v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MCTS: A Multi-Reference Chinese Text Simplification Dataset", "abstract": "Text simplification aims to make the text easier to understand by applying\nrewriting transformations. There has been very little research on Chinese text\nsimplification for a long time. The lack of generic evaluation data is an\nessential reason for this phenomenon. In this paper, we introduce MCTS, a\nmulti-reference Chinese text simplification dataset. We describe the annotation\nprocess of the dataset and provide a detailed analysis. Furthermore, we\nevaluate the performance of several unsupervised methods and advanced large\nlanguage models. We additionally provide Chinese text simplification parallel\ndata that can be used for training, acquired by utilizing machine translation\nand English text simplification. We hope to build a basic understanding of\nChinese text simplification through the foundational work and provide\nreferences for future research. All of the code and data are released at\nhttps://github.com/blcuicall/mcts/.", "published": "2023-06-05 11:46:36", "link": "http://arxiv.org/abs/2306.02796v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Language Representation with Constructional Information for\n  Natural Language Understanding", "abstract": "Natural language understanding (NLU) is an essential branch of natural\nlanguage processing, which relies on representations generated by pre-trained\nlanguage models (PLMs). However, PLMs primarily focus on acquiring\nlexico-semantic information, while they may be unable to adequately handle the\nmeaning of constructions. To address this issue, we introduce construction\ngrammar (CxG), which highlights the pairings of form and meaning, to enrich\nlanguage representation. We adopt usage-based construction grammar as the basis\nof our work, which is highly compatible with statistical models such as PLMs.\nThen a HyCxG framework is proposed to enhance language representation through a\nthree-stage solution. First, all constructions are extracted from sentences via\na slot-constraints approach. As constructions can overlap with each other,\nbringing redundancy and imbalance, we formulate the conditional max coverage\nproblem for selecting the discriminative constructions. Finally, we propose a\nrelational hypergraph attention network to acquire representation from\nconstructional information by capturing high-order word interactions among\nconstructions. Extensive experiments demonstrate the superiority of the\nproposed model on a variety of NLU tasks.", "published": "2023-06-05 12:15:12", "link": "http://arxiv.org/abs/2306.02819v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UNIDECOR: A Unified Deception Corpus for Cross-Corpus Deception\n  Detection", "abstract": "Verbal deception has been studied in psychology, forensics, and computational\nlinguistics for a variety of reasons, like understanding behaviour patterns,\nidentifying false testimonies, and detecting deception in online communication.\nVarying motivations across research fields lead to differences in the domain\nchoices to study and in the conceptualization of deception, making it hard to\ncompare models and build robust deception detection systems for a given\nlanguage. With this paper, we improve this situation by surveying available\nEnglish deception datasets which include domains like social media reviews,\ncourt testimonials, opinion statements on specific topics, and deceptive\ndialogues from online strategy games. We consolidate these datasets into a\nsingle unified corpus. Based on this resource, we conduct a correlation\nanalysis of linguistic cues of deception across datasets to understand the\ndifferences and perform cross-corpus modeling experiments which show that a\ncross-domain generalization is challenging to achieve. The unified deception\ncorpus (UNIDECOR) can be obtained from\nhttps://www.ims.uni-stuttgart.de/data/unidecor.", "published": "2023-06-05 12:23:04", "link": "http://arxiv.org/abs/2306.02827v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On \"Scientific Debt\" in NLP: A Case for More Rigour in Language Model\n  Pre-Training Research", "abstract": "This evidence-based position paper critiques current research practices\nwithin the language model pre-training literature. Despite rapid recent\nprogress afforded by increasingly better pre-trained language models (PLMs),\ncurrent PLM research practices often conflate different possible sources of\nmodel improvement, without conducting proper ablation studies and principled\ncomparisons between different models under comparable conditions. These\npractices (i) leave us ill-equipped to understand which pre-training approaches\nshould be used under what circumstances; (ii) impede reproducibility and credit\nassignment; and (iii) render it difficult to understand: \"How exactly does each\nfactor contribute to the progress that we have today?\" We provide a case in\npoint by revisiting the success of BERT over its baselines, ELMo and GPT-1, and\ndemonstrate how -- under comparable conditions where the baselines are tuned to\na similar extent -- these baselines (and even-simpler variants thereof) can, in\nfact, achieve competitive or better performance than BERT. These findings\ndemonstrate how disentangling different factors of model improvements can lead\nto valuable new insights. We conclude with recommendations for how to encourage\nand incentivize this line of work, and accelerate progress towards a better and\nmore systematic understanding of what factors drive the progress of our\nfoundation models today.", "published": "2023-06-05 13:43:50", "link": "http://arxiv.org/abs/2306.02870v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text-To-KG Alignment: Comparing Current Methods on Classification Tasks", "abstract": "In contrast to large text corpora, knowledge graphs (KG) provide dense and\nstructured representations of factual information. This makes them attractive\nfor systems that supplement or ground the knowledge found in pre-trained\nlanguage models with an external knowledge source. This has especially been the\ncase for classification tasks, where recent work has focused on creating\npipeline models that retrieve information from KGs like ConceptNet as\nadditional context. Many of these models consist of multiple components, and\nalthough they differ in the number and nature of these parts, they all have in\ncommon that for some given text query, they attempt to identify and retrieve a\nrelevant subgraph from the KG. Due to the noise and idiosyncrasies often found\nin KGs, it is not known how current methods compare to a scenario where the\naligned subgraph is completely relevant to the query. In this work, we try to\nbridge this knowledge gap by reviewing current approaches to text-to-KG\nalignment and evaluating them on two datasets where manually created graphs are\navailable, providing insights into the effectiveness of current methods.", "published": "2023-06-05 13:45:45", "link": "http://arxiv.org/abs/2306.02871v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DecompX: Explaining Transformers Decisions by Propagating Token\n  Decomposition", "abstract": "An emerging solution for explaining Transformer-based models is to use\nvector-based analysis on how the representations are formed. However, providing\na faithful vector-based explanation for a multi-layer model could be\nchallenging in three aspects: (1) Incorporating all components into the\nanalysis, (2) Aggregating the layer dynamics to determine the information flow\nand mixture throughout the entire model, and (3) Identifying the connection\nbetween the vector-based analysis and the model's predictions. In this paper,\nwe present DecompX to tackle these challenges. DecompX is based on the\nconstruction of decomposed token representations and their successive\npropagation throughout the model without mixing them in between layers.\nAdditionally, our proposal provides multiple advantages over existing solutions\nfor its inclusion of all encoder components (especially nonlinear feed-forward\nnetworks) and the classification head. The former allows acquiring precise\nvectors while the latter transforms the decomposition into meaningful\nprediction-based values, eliminating the need for norm- or summation-based\nvector aggregation. According to the standard faithfulness evaluations, DecompX\nconsistently outperforms existing gradient-based and vector-based approaches on\nvarious datasets. Our code is available at\nhttps://github.com/mohsenfayyaz/DecompX.", "published": "2023-06-05 13:46:31", "link": "http://arxiv.org/abs/2306.02873v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Second Language Acquisition of Neural Language Models", "abstract": "With the success of neural language models (LMs), their language acquisition\nhas gained much attention. This work sheds light on the second language (L2)\nacquisition of LMs, while previous work has typically explored their first\nlanguage (L1) acquisition. Specifically, we trained bilingual LMs with a\nscenario similar to human L2 acquisition and analyzed their cross-lingual\ntransfer from linguistic perspectives. Our exploratory experiments demonstrated\nthat the L1 pretraining accelerated their linguistic generalization in L2, and\nlanguage transfer configurations (e.g., the L1 choice, and presence of parallel\ntexts) substantially affected their generalizations. These clarify their\n(non-)human-like L2 acquisition in particular aspects.", "published": "2023-06-05 14:32:41", "link": "http://arxiv.org/abs/2306.02920v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MidMed: Towards Mixed-Type Dialogues for Medical Consultation", "abstract": "Most medical dialogue systems assume that patients have clear goals (medicine\nquerying, surgical operation querying, etc.) before medical consultation.\nHowever, in many real scenarios, due to the lack of medical knowledge, it is\nusually difficult for patients to determine clear goals with all necessary\nslots. In this paper, we identify this challenge as how to construct medical\nconsultation dialogue systems to help patients clarify their goals. To mitigate\nthis challenge, we propose a novel task and create a human-to-human mixed-type\nmedical consultation dialogue corpus, termed MidMed, covering five dialogue\ntypes: task-oriented dialogue for diagnosis, recommendation, knowledge-grounded\ndialogue, QA, and chitchat. MidMed covers four departments\n(otorhinolaryngology, ophthalmology, skin, and digestive system), with 8,175\ndialogues. Furthermore, we build baselines on MidMed and propose an\ninstruction-guiding medical dialogue generation framework, termed InsMed, to\naddress this task. Experimental results show the effectiveness of InsMed.", "published": "2023-06-05 14:36:31", "link": "http://arxiv.org/abs/2306.02923v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PokemonChat: Auditing ChatGPT for Pok\u00e9mon Universe Knowledge", "abstract": "The recently released ChatGPT model demonstrates unprecedented capabilities\nin zero-shot question-answering. In this work, we probe ChatGPT for its\nconversational understanding and introduce a conversational framework\n(protocol) that can be adopted in future studies. The Pok\\'emon universe serves\nas an ideal testing ground for auditing ChatGPT's reasoning capabilities due to\nits closed world assumption. After bringing ChatGPT's background knowledge (on\nthe Pok\\'emon universe) to light, we test its reasoning process when using\nthese concepts in battle scenarios. We then evaluate its ability to acquire new\nknowledge and include it in its reasoning process. Our ultimate goal is to\nassess ChatGPT's ability to generalize, combine features, and to acquire and\nreason over newly introduced knowledge from human feedback. We find that\nChatGPT has prior knowledge of the Pokemon universe, which can reason upon in\nbattle scenarios to a great extent, even when new information is introduced.\nThe model performs better with collaborative feedback and if there is an\ninitial phase of information retrieval, but also hallucinates occasionally and\nis susceptible to adversarial attacks.", "published": "2023-06-05 16:44:27", "link": "http://arxiv.org/abs/2306.03024v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese\n  Medical Exam Dataset", "abstract": "Recent advancements in large language models (LLMs) have transformed the\nfield of question answering (QA). However, evaluating LLMs in the medical field\nis challenging due to the lack of standardized and comprehensive datasets. To\naddress this gap, we introduce CMExam, sourced from the Chinese National\nMedical Licensing Examination. CMExam consists of 60K+ multiple-choice\nquestions for standardized and objective evaluations, as well as solution\nexplanations for model reasoning evaluation in an open-ended manner. For\nin-depth analyses of LLMs, we invited medical professionals to label five\nadditional question-wise annotations, including disease groups, clinical\ndepartments, medical disciplines, areas of competency, and question difficulty\nlevels. Alongside the dataset, we further conducted thorough experiments with\nrepresentative LLMs and QA algorithms on CMExam. The results show that GPT-4\nhad the best accuracy of 61.6% and a weighted F1 score of 0.617. These results\nhighlight a great disparity when compared to human accuracy, which stood at\n71.6%. For explanation tasks, while LLMs could generate relevant reasoning and\ndemonstrate improved performance after finetuning, they fall short of a desired\nstandard, indicating ample room for improvement. To the best of our knowledge,\nCMExam is the first Chinese medical exam dataset to provide comprehensive\nmedical annotations. The experiments and findings of LLM evaluation also\nprovide valuable insights into the challenges and potential solutions in\ndeveloping Chinese medical QA systems and LLM evaluation pipelines. The dataset\nand relevant code are available at https://github.com/williamliujl/CMExam.", "published": "2023-06-05 16:48:41", "link": "http://arxiv.org/abs/2306.03030v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Syntactic Generalization Capacity of Pre-trained Language\n  Models on Japanese Honorific Conversion", "abstract": "Using Japanese honorifics is challenging because it requires not only\nknowledge of the grammatical rules but also contextual information, such as\nsocial relationships. It remains unclear whether pre-trained large language\nmodels (LLMs) can flexibly handle Japanese honorifics like humans. To analyze\nthis, we introduce an honorific conversion task that considers social\nrelationships among people mentioned in a conversation. We construct a Japanese\nhonorifics dataset from problem templates of various sentence structures to\ninvestigate the syntactic generalization capacity of GPT-3, one of the leading\nLLMs, on this task under two settings: fine-tuning and prompt learning. Our\nresults showed that the fine-tuned GPT-3 performed better in a context-aware\nhonorific conversion task than the prompt-based one. The fine-tuned model\ndemonstrated overall syntactic generalizability towards compound honorific\nsentences, except when tested with the data involving direct speech.", "published": "2023-06-05 17:27:48", "link": "http://arxiv.org/abs/2306.03055v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Learning and Statistical Approaches to Measuring Similarity of\n  Political Parties", "abstract": "Mapping political party systems to metric policy spaces is one of the major\nmethodological problems in political science. At present, in most political\nscience project this task is performed by domain experts relying on purely\nqualitative assessments, with all the attendant problems of subjectivity and\nlabor intensiveness. We consider how advances in natural language processing,\nincluding large transformer-based language models, can be applied to solve that\nissue. We apply a number of texts similarity measures to party political\nprograms, analyze how they correlate with each other, and -- in the absence of\na satisfactory benchmark -- evaluate them against other measures, including\nthose based on expert surveys, voting records, electoral patterns, and\ncandidate networks. Finally, we consider the prospects of relying on those\nmethods to correct, supplement, and eventually replace expert judgments.", "published": "2023-06-05 17:53:41", "link": "http://arxiv.org/abs/2306.03079v1", "categories": ["cs.CL", "91F10 (Primary) 68T50 (Secondary)", "J.4; I.2.7"], "primary_category": "cs.CL"}
{"title": "Easy-to-Read in Germany: A Survey on its Current State and Available\n  Resources", "abstract": "Easy-to-Read Language (E2R) is a controlled language variant that makes any\nwritten text more accessible through the use of clear, direct and simple\nlanguage. It is mainly aimed at people with cognitive or intellectual\ndisabilities, among other target users. Plain Language (PL), on the other hand,\nis a variant of a given language, which aims to promote the use of simple\nlanguage to communicate information. German counts with Leichte Sprache (LS),\nits version of E2R, and Einfache Sprache (ES), its version of PL. In recent\nyears, important developments have been conducted in the field of LS. This\npaper offers an updated overview of the existing Natural Language Processing\n(NLP) tools and resources for LS. Besides, it also aims to set out the\nsituation with regard to LS and ES in Germany.", "published": "2023-06-05 19:00:25", "link": "http://arxiv.org/abs/2306.03189v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "shs-nlp at RadSum23: Domain-Adaptive Pre-training of Instruction-tuned\n  LLMs for Radiology Report Impression Generation", "abstract": "Instruction-tuned generative Large language models (LLMs) like ChatGPT and\nBloomz possess excellent generalization abilities, but they face limitations in\nunderstanding radiology reports, particularly in the task of generating the\nIMPRESSIONS section from the FINDINGS section. They tend to generate either\nverbose or incomplete IMPRESSIONS, mainly due to insufficient exposure to\nmedical text data during training. We present a system which leverages\nlarge-scale medical text data for domain-adaptive pre-training of\ninstruction-tuned LLMs to enhance its medical knowledge and performance on\nspecific medical tasks. We show that this system performs better in a zero-shot\nsetting than a number of pretrain-and-finetune adaptation methods on the\nIMPRESSIONS generation task, and ranks 1st among participating systems in Task\n1B: Radiology Report Summarization at the BioNLP 2023 workshop.", "published": "2023-06-05 21:33:04", "link": "http://arxiv.org/abs/2306.03264v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoSiNES: Contrastive Siamese Network for Entity Standardization", "abstract": "Entity standardization maps noisy mentions from free-form text to standard\nentities in a knowledge base. The unique challenge of this task relative to\nother entity-related tasks is the lack of surrounding context and numerous\nvariations in the surface form of the mentions, especially when it comes to\ngeneralization across domains where labeled data is scarce. Previous research\nmostly focuses on developing models either heavily relying on context, or\ndedicated solely to a specific domain. In contrast, we propose CoSiNES, a\ngeneric and adaptable framework with Contrastive Siamese Network for Entity\nStandardization that effectively adapts a pretrained language model to capture\nthe syntax and semantics of the entities in a new domain.\n  We construct a new dataset in the technology domain, which contains 640\ntechnical stack entities and 6,412 mentions collected from industrial content\nmanagement systems. We demonstrate that CoSiNES yields higher accuracy and\nfaster runtime than baselines derived from leading methods in this domain.\nCoSiNES also achieves competitive performance in four standard datasets from\nthe chemistry, medicine, and biomedical domains, demonstrating its cross-domain\napplicability.", "published": "2023-06-05 23:58:40", "link": "http://arxiv.org/abs/2306.03316v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncertainty in Natural Language Processing: Sources, Quantification, and\n  Applications", "abstract": "As a main field of artificial intelligence, natural language processing (NLP)\nhas achieved remarkable success via deep neural networks. Plenty of NLP tasks\nhave been addressed in a unified manner, with various tasks being associated\nwith each other through sharing the same paradigm. However, neural networks are\nblack boxes and rely on probability computation. Making mistakes is inevitable.\nTherefore, estimating the reliability and trustworthiness (in other words,\nuncertainty) of neural networks becomes a key research direction, which plays a\ncrucial role in reducing models' risks and making better decisions. Therefore,\nin this survey, we provide a comprehensive review of uncertainty-relevant works\nin the NLP field. Considering the data and paradigms characteristics, we first\ncategorize the sources of uncertainty in natural language into three types,\nincluding input, system, and output. Then, we systemically review uncertainty\nquantification approaches and the main applications. Finally, we discuss the\nchallenges of uncertainty estimation in NLP and discuss potential future\ndirections, taking into account recent trends in the field. Though there have\nbeen a few surveys about uncertainty estimation, our work is the first to\nreview uncertainty from the NLP perspective.", "published": "2023-06-05 06:46:53", "link": "http://arxiv.org/abs/2306.04459v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LexGPT 0.1: pre-trained GPT-J models with Pile of Law", "abstract": "This research aims to build generative language models specialized for the\nlegal domain. The manuscript presents the development of LexGPT models based on\nGPT-J models and pre-trained with Pile of Law. The foundation model built in\nthis manuscript is the initial step for the development of future applications\nin the legal domain, such as further training with reinforcement learning from\nhuman feedback. Another objective of this manuscript is to assist legal\nprofessionals in utilizing language models through the ``No Code'' approach. By\nfine-tuning models with specialized data and without modifying any source code,\nlegal professionals can create custom language models for downstream tasks with\nminimum effort and technical knowledge. The downstream task in this manuscript\nis to turn a LexGPT model into a classifier, although the performance is\nnotably lower than the state-of-the-art result. How to enhance downstream task\nperformance without modifying the model or its source code is a research topic\nfor future exploration.", "published": "2023-06-05 08:42:59", "link": "http://arxiv.org/abs/2306.05431v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Relate to Previous Turns in Conversational Search", "abstract": "Conversational search allows a user to interact with a search system in\nmultiple turns. A query is strongly dependent on the conversation context. An\neffective way to improve retrieval effectiveness is to expand the current query\nwith historical queries. However, not all the previous queries are related to,\nand useful for expanding the current query. In this paper, we propose a new\nmethod to select relevant historical queries that are useful for the current\nquery. To cope with the lack of labeled training data, we use a pseudo-labeling\napproach to annotate useful historical queries based on their impact on the\nretrieval results. The pseudo-labeled data are used to train a selection model.\nWe further propose a multi-task learning framework to jointly train the\nselector and the retriever during fine-tuning, allowing us to mitigate the\npossible inconsistency between the pseudo labels and the changed retriever.\nExtensive experiments on four conversational search datasets demonstrate the\neffectiveness and broad applicability of our method compared with several\nstrong baselines.", "published": "2023-06-05 03:00:10", "link": "http://arxiv.org/abs/2306.02553v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Building Resilient SMEs: Harnessing Large Language Models for Cyber\n  Security in Australia", "abstract": "The escalating digitalisation of our lives and enterprises has led to a\nparallel growth in the complexity and frequency of cyber-attacks. Small and\nmedium-sized enterprises (SMEs), particularly in Australia, are experiencing\nincreased vulnerability to cyber threats, posing a significant challenge to the\nnation's cyber security landscape. Embracing transformative technologies such\nas Artificial Intelligence (AI), Machine Learning (ML) and Large Language\nModels (LLMs) can potentially strengthen cyber security policies for Australian\nSMEs. However, their practical application, advantages, and limitations remain\nunderexplored, with prior research mainly focusing on large corporations. This\nstudy aims to address this gap by providing a comprehensive understanding of\nthe potential role of LLMs in enhancing cyber security policies for Australian\nSMEs. Employing a mixed-methods study design, this research includes a\nliterature review, qualitative analysis of SME case studies, and a quantitative\nassessment of LLM performance metrics in cyber security applications. The\nfindings highlight the promising potential of LLMs across various performance\ncriteria, including relevance, accuracy, and applicability, though gaps remain\nin areas such as completeness and clarity. The study underlines the importance\nof integrating human expertise with LLM technology and refining model\ndevelopment to address these limitations. By proposing a robust conceptual\nframework guiding the effective adoption of LLMs, this research aims to\ncontribute to a safer and more resilient cyber environment for Australian SMEs,\nenabling sustainable growth and competitiveness in the digital era.", "published": "2023-06-05 06:01:00", "link": "http://arxiv.org/abs/2306.02612v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Joint Pre-training and Local Re-training: Transferable Representation\n  Learning on Multi-source Knowledge Graphs", "abstract": "In this paper, we present the ``joint pre-training and local re-training''\nframework for learning and applying multi-source knowledge graph (KG)\nembeddings. We are motivated by the fact that different KGs contain\ncomplementary information to improve KG embeddings and downstream tasks. We\npre-train a large teacher KG embedding model over linked multi-source KGs and\ndistill knowledge to train a student model for a task-specific KG. To enable\nknowledge transfer across different KGs, we use entity alignment to build a\nlinked subgraph for connecting the pre-trained KGs and the target KG. The\nlinked subgraph is re-trained for three-level knowledge distillation from the\nteacher to the student, i.e., feature knowledge distillation, network knowledge\ndistillation, and prediction knowledge distillation, to generate more\nexpressive embeddings. The teacher model can be reused for different target KGs\nand tasks without having to train from scratch. We conduct extensive\nexperiments to demonstrate the effectiveness and efficiency of our framework.", "published": "2023-06-05 08:11:59", "link": "http://arxiv.org/abs/2306.02679v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "End-to-End Word-Level Pronunciation Assessment with MASK Pre-training", "abstract": "Pronunciation assessment is a major challenge in the computer-aided\npronunciation training system, especially at the word (phoneme)-level. To\nobtain word (phoneme)-level scores, current methods usually rely on aligning\ncomponents to obtain acoustic features of each word (phoneme), which limits the\nperformance of assessment to the accuracy of alignments. Therefore, to address\nthis problem, we propose a simple yet effective method, namely\n\\underline{M}asked pre-training for \\underline{P}ronunciation\n\\underline{A}ssessment (MPA). Specifically, by incorporating a mask-predict\nstrategy, our MPA supports end-to-end training without leveraging any aligning\ncomponents and can solve misalignment issues to a large extent during\nprediction. Furthermore, we design two evaluation strategies to enable our\nmodel to conduct assessments in both unsupervised and supervised settings.\nExperimental results on SpeechOcean762 dataset demonstrate that MPA could\nachieve better performance than previous methods, without any explicit\nalignment. In spite of this, MPA still has some limitations, such as requiring\nmore inference time and reference text. They expect to be addressed in future\nwork.", "published": "2023-06-05 08:18:01", "link": "http://arxiv.org/abs/2306.02682v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Orca: Progressive Learning from Complex Explanation Traces of GPT-4", "abstract": "Recent research has focused on enhancing the capability of smaller models\nthrough imitation learning, drawing on the outputs generated by large\nfoundation models (LFMs). A number of issues impact the quality of these\nmodels, ranging from limited imitation signals from shallow LFM outputs; small\nscale homogeneous training data; and most notably a lack of rigorous evaluation\nresulting in overestimating the small model's capability as they tend to learn\nto imitate the style, but not the reasoning process of LFMs. To address these\nchallenges, we develop Orca (We are working with our legal team to publicly\nrelease a diff of the model weights in accordance with LLaMA's release policy\nto be published at https://aka.ms/orca-lm), a 13-billion parameter model that\nlearns to imitate the reasoning process of LFMs. Orca learns from rich signals\nfrom GPT-4 including explanation traces; step-by-step thought processes; and\nother complex instructions, guided by teacher assistance from ChatGPT. To\npromote this progressive learning, we tap into large-scale and diverse\nimitation data with judicious sampling and selection. Orca surpasses\nconventional state-of-the-art instruction-tuned models such as Vicuna-13B by\nmore than 100% in complex zero-shot reasoning benchmarks like Big-Bench Hard\n(BBH) and 42% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH\nbenchmark and shows competitive performance (4 pts gap with optimized system\nmessage) in professional and academic examinations like the SAT, LSAT, GRE, and\nGMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our\nresearch indicates that learning from step-by-step explanations, whether these\nare generated by humans or more advanced AI models, is a promising direction to\nimprove model capabilities and skills.", "published": "2023-06-05 08:58:39", "link": "http://arxiv.org/abs/2306.02707v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring the Relationship between Alignment and Cross-lingual Transfer\n  in Multilingual Transformers", "abstract": "Without any explicit cross-lingual training data, multilingual language\nmodels can achieve cross-lingual transfer. One common way to improve this\ntransfer is to perform realignment steps before fine-tuning, i.e., to train the\nmodel to build similar representations for pairs of words from translated\nsentences. But such realignment methods were found to not always improve\nresults across languages and tasks, which raises the question of whether\naligned representations are truly beneficial for cross-lingual transfer. We\nprovide evidence that alignment is actually significantly correlated with\ncross-lingual transfer across languages, models and random seeds. We show that\nfine-tuning can have a significant impact on alignment, depending mainly on the\ndownstream task and the model. Finally, we show that realignment can, in some\ninstances, improve cross-lingual transfer, and we identify conditions in which\nrealignment methods provide significant improvements. Namely, we find that\nrealignment works better on tasks for which alignment is correlated with\ncross-lingual transfer when generalizing to a distant language and with smaller\nmodels, as well as when using a bilingual dictionary rather than FastAlign to\nextract realignment pairs. For example, for POS-tagging, between English and\nArabic, realignment can bring a +15.8 accuracy improvement on distilmBERT, even\noutperforming XLM-R Large by 1.7. We thus advocate for further research on\nrealignment methods for smaller multilingual models as an alternative to\nscaling.", "published": "2023-06-05 11:35:40", "link": "http://arxiv.org/abs/2306.02790v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Substitute Spans towards Improving Compositional\n  Generalization", "abstract": "Despite the rising prevalence of neural sequence models, recent empirical\nevidences suggest their deficiency in compositional generalization. One of the\ncurrent de-facto solutions to this problem is compositional data augmentation,\naiming to incur additional compositional inductive bias. Nonetheless, the\nimprovement offered by existing handcrafted augmentation strategies is limited\nwhen successful systematic generalization of neural sequence models requires\nmulti-grained compositional bias (i.e., not limited to either lexical or\nstructural biases only) or differentiation of training sequences in an\nimbalanced difficulty distribution. To address the two challenges, we first\npropose a novel compositional augmentation strategy dubbed \\textbf{Span}\n\\textbf{Sub}stitution (SpanSub) that enables multi-grained composition of\nsubstantial substructures in the whole training set. Over and above that, we\nintroduce the \\textbf{L}earning \\textbf{to} \\textbf{S}ubstitute \\textbf{S}pan\n(L2S2) framework which empowers the learning of span substitution probabilities\nin SpanSub in an end-to-end manner by maximizing the loss of neural sequence\nmodels, so as to outweigh those challenging compositions with elusive concepts\nand novel surroundings. Our empirical results on three standard compositional\ngeneralization benchmarks, including SCAN, COGS and GeoQuery (with an\nimprovement of at most 66.5\\%, 10.3\\%, 1.2\\%, respectively), demonstrate the\nsuperiority of SpanSub, %the learning framework L2S2 and their combination.", "published": "2023-06-05 12:44:18", "link": "http://arxiv.org/abs/2306.02840v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Conversational Recommendation Systems via Counterfactual Data\n  Simulation", "abstract": "Conversational recommender systems (CRSs) aim to provide recommendation\nservices via natural language conversations. Although a number of approaches\nhave been proposed for developing capable CRSs, they typically rely on\nsufficient training data for training. Since it is difficult to annotate\nrecommendation-oriented dialogue datasets, existing CRS approaches often suffer\nfrom the issue of insufficient training due to the scarcity of training data.\nTo address this issue, in this paper, we propose a CounterFactual data\nsimulation approach for CRS, named CFCRS, to alleviate the issue of data\nscarcity in CRSs. Our approach is developed based on the framework of\ncounterfactual data augmentation, which gradually incorporates the rewriting to\nthe user preference from a real dialogue without interfering with the entire\nconversation flow. To develop our approach, we characterize user preference and\norganize the conversation flow by the entities involved in the dialogue, and\ndesign a multi-stage recommendation dialogue simulator based on a conversation\nflow language model. Under the guidance of the learned user preference and\ndialogue schema, the flow language model can produce reasonable, coherent\nconversation flows, which can be further realized into complete dialogues.\nBased on the simulator, we perform the intervention at the representations of\nthe interacted entities of target users, and design an adversarial training\nmethod with a curriculum schedule that can gradually optimize the data\naugmentation strategy. Extensive experiments show that our approach can\nconsistently boost the performance of several competitive CRSs, and outperform\nother data augmentation methods, especially when the training data is limited.\nOur code is publicly available at https://github.com/RUCAIBox/CFCRS.", "published": "2023-06-05 12:48:56", "link": "http://arxiv.org/abs/2306.02842v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models for Topic Classification in the Domain\n  of Public Affairs", "abstract": "The analysis of public affairs documents is crucial for citizens as it\npromotes transparency, accountability, and informed decision-making. It allows\ncitizens to understand government policies, participate in public discourse,\nand hold representatives accountable. This is crucial, and sometimes a matter\nof life or death, for companies whose operation depend on certain regulations.\nLarge Language Models (LLMs) have the potential to greatly enhance the analysis\nof public affairs documents by effectively processing and understanding the\ncomplex language used in such documents. In this work, we analyze the\nperformance of LLMs in classifying public affairs documents. As a natural\nmulti-label task, the classification of these documents presents important\nchallenges. In this work, we use a regex-powered tool to collect a database of\npublic affairs documents with more than 33K samples and 22.5M tokens. Our\nexperiments assess the performance of 4 different Spanish LLMs to classify up\nto 30 different topics in the data in different configurations. The results\nshows that LLMs can be of great use to process domain-specific documents, such\nas those in the domain of public affairs.", "published": "2023-06-05 13:35:01", "link": "http://arxiv.org/abs/2306.02864v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Gen-IR @ SIGIR 2023: The First Workshop on Generative Information\n  Retrieval", "abstract": "Generative information retrieval (IR) has experienced substantial growth\nacross multiple research communities (e.g., information retrieval, computer\nvision, natural language processing, and machine learning), and has been highly\nvisible in the popular press. Theoretical, empirical, and actual user-facing\nproducts have been released that retrieve documents (via generation) or\ndirectly generate answers given an input request. We would like to investigate\nwhether end-to-end generative models are just another trend or, as some claim,\na paradigm change for IR. This necessitates new metrics, theoretical grounding,\nevaluation methods, task definitions, models, user interfaces, etc. The goal of\nthis workshop (https://coda.io/@sigir/gen-ir) is to focus on previously\nexplored Generative IR techniques like document retrieval and direct Grounded\nAnswer Generation, while also offering a venue for the discussion and\nexploration of how Generative IR can be applied to new domains like\nrecommendation systems, summarization, etc. The format of the workshop is\ninteractive, including roundtable and keynote sessions and tends to avoid the\none-sided dialogue of a mini-conference.", "published": "2023-06-05 13:56:36", "link": "http://arxiv.org/abs/2306.02887v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "SelfEvolve: A Code Evolution Framework via Large Language Models", "abstract": "Large language models (LLMs) have already revolutionized code generation,\nafter being pretrained on publicly available code data. However, while various\nmethods have been proposed to augment LLMs with retrieved knowledge and enhance\nthe quality of code generation, the performance of these retrieval-based\nmethods is limited by the strength of the retrievers used. In addition, while\nLLMs show great emergent ability, they still struggle to produce the correct\ncode in one turn. To address these challenges, we propose a novel two-step\npipeline, called \\autoknow, that leverages LLMs as both knowledge providers and\nself-reflective programmers. Unlike retrieval-based methods, \\autoknow~obtains\nthe knowledge from input prompts and generates intermediate code based on the\ngenerated knowledge. After that, \\autoknow~asks LLM to act as an expert\nprogrammer to perform debugging for the generated code. This is achieved by\nreceiving the error message from the interpreter, without requiring special\ntest cases for correctness verification. We evaluate \\autoknow~on three code\ngeneration datasets, including DS-1000 for data science code, HumanEval for\nsoftware engineering code, and TransCoder for C++-to-Python translation. Our\nempirical experiments show that \\autoknow~outperforms strong baselines by a\nsignificant margin on all datasets. We also conduct exhaustive analytical\nexperiments to validate the effectiveness of the two stages of \\autoknow, and\nfind that both are superior to other prompting-based methods. Further\nscalability analysis demonstrates that \\autoknow~can be adapted to other more\nadvanced models, such as GPT-4, and bring consistent efficacy improvement.", "published": "2023-06-05 14:12:46", "link": "http://arxiv.org/abs/2306.02907v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "A Simple and Flexible Modeling for Mental Disorder Detection by Learning\n  from Clinical Questionnaires", "abstract": "Social media is one of the most highly sought resources for analyzing\ncharacteristics of the language by its users. In particular, many researchers\nutilized various linguistic features of mental health problems from social\nmedia. However, existing approaches to detecting mental disorders face critical\nchallenges, such as the scarcity of high-quality data or the trade-off between\naddressing the complexity of models and presenting interpretable results\ngrounded in expert domain knowledge. To address these challenges, we design a\nsimple but flexible model that preserves domain-based interpretability. We\npropose a novel approach that captures the semantic meanings directly from the\ntext and compares them to symptom-related descriptions. Experimental results\ndemonstrate that our model outperforms relevant baselines on various mental\ndisorder detection tasks. Our detailed analysis shows that the proposed model\nis effective at leveraging domain knowledge, transferable to other mental\ndisorders, and providing interpretable detection results.", "published": "2023-06-05 15:23:55", "link": "http://arxiv.org/abs/2306.02955v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Which Argumentative Aspects of Hate Speech in Social Media can be\n  reliably identified?", "abstract": "With the increasing diversity of use cases of large language models, a more\ninformative treatment of texts seems necessary. An argumentative analysis could\nfoster a more reasoned usage of chatbots, text completion mechanisms or other\napplications. However, it is unclear which aspects of argumentation can be\nreliably identified and integrated in language models. In this paper, we\npresent an empirical assessment of the reliability with which different\nargumentative aspects can be automatically identified in hate speech in social\nmedia. We have enriched the Hateval corpus (Basile et al. 2019) with a manual\nannotation of some argumentative components, adapted from Wagemans (2016)'s\nPeriodic Table of Arguments. We show that some components can be identified\nwith reasonable reliability. For those that present a high error ratio, we\nanalyze the patterns of disagreement between expert annotators and errors in\nautomatic procedures, and we propose adaptations of those categories that can\nbe more reliably reproduced.", "published": "2023-06-05 15:50:57", "link": "http://arxiv.org/abs/2306.02978v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KNOW How to Make Up Your Mind! Adversarially Detecting and Alleviating\n  Inconsistencies in Natural Language Explanations", "abstract": "While recent works have been considerably improving the quality of the\nnatural language explanations (NLEs) generated by a model to justify its\npredictions, there is very limited research in detecting and alleviating\ninconsistencies among generated NLEs. In this work, we leverage external\nknowledge bases to significantly improve on an existing adversarial attack for\ndetecting inconsistent NLEs. We apply our attack to high-performing NLE models\nand show that models with higher NLE quality do not necessarily generate fewer\ninconsistencies. Moreover, we propose an off-the-shelf mitigation method to\nalleviate inconsistencies by grounding the model into external background\nknowledge. Our method decreases the inconsistencies of previous high-performing\nNLE models as detected by our attack.", "published": "2023-06-05 15:51:58", "link": "http://arxiv.org/abs/2306.02980v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PolyVoice: Language Models for Speech to Speech Translation", "abstract": "We propose PolyVoice, a language model-based framework for speech-to-speech\ntranslation (S2ST) system. Our framework consists of two language models: a\ntranslation language model and a speech synthesis language model. We use\ndiscretized speech units, which are generated in a fully unsupervised way, and\nthus our framework can be used for unwritten languages. For the speech\nsynthesis part, we adopt the existing VALL-E X approach and build a unit-based\naudio language model. This grants our framework the ability to preserve the\nvoice characteristics and the speaking style of the original speech. We examine\nour system on Chinese $\\rightarrow$ English and English $\\rightarrow$ Spanish\npairs. Experimental results show that our system can generate speech with high\ntranslation quality and audio quality. Speech samples are available at\nhttps://speechtranslation.github.io/polyvoice.", "published": "2023-06-05 15:53:15", "link": "http://arxiv.org/abs/2306.02982v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Structured Voronoi Sampling", "abstract": "Gradient-based sampling algorithms have demonstrated their effectiveness in\ntext generation, especially in the context of controlled text generation.\nHowever, there exists a lack of theoretically grounded and principled\napproaches for this task. In this paper, we take an important step toward\nbuilding a principled approach for sampling from language models with\ngradient-based methods. We use discrete distributions given by language models\nto define densities and develop an algorithm based on Hamiltonian Monte Carlo\nto sample from them. We name our gradient-based technique Structured Voronoi\nSampling (SVS). In an experimental setup where the reference distribution is\nknown, we show that the empirical distribution of SVS samples is closer to the\nreference distribution compared to alternative sampling schemes. Furthermore,\nin a controlled generation task, SVS is able to generate fluent and diverse\nsamples while following the control targets significantly better than other\nmethods.", "published": "2023-06-05 17:32:35", "link": "http://arxiv.org/abs/2306.03061v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Interactive Editing for Text Summarization", "abstract": "Summarizing lengthy documents is a common and essential task in our daily\nlives. Although recent advancements in neural summarization models can assist\nin crafting general-purpose summaries, human writers often have specific\nrequirements that call for a more customized approach. To address this need, we\nintroduce REVISE (Refinement and Editing via Iterative Summarization\nEnhancement), an innovative framework designed to facilitate iterative editing\nand refinement of draft summaries by human writers. Within our framework,\nwriters can effortlessly modify unsatisfactory segments at any location or\nlength and provide optional starting phrases -- our system will generate\ncoherent alternatives that seamlessly integrate with the existing summary. At\nits core, REVISE incorporates a modified fill-in-the-middle model with the\nencoder-decoder architecture while developing novel evaluation metrics tailored\nfor the summarization task. In essence, our framework empowers users to create\nhigh-quality, personalized summaries by effectively harnessing both human\nexpertise and AI capabilities, ultimately transforming the summarization\nprocess into a truly collaborative and adaptive experience.", "published": "2023-06-05 17:43:53", "link": "http://arxiv.org/abs/2306.03067v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight\n  Compression", "abstract": "Recent advances in large language model (LLM) pretraining have led to\nhigh-quality LLMs with impressive abilities. By compressing such LLMs via\nquantization to 3-4 bits per parameter, they can fit into memory-limited\ndevices such as laptops and mobile phones, enabling personalized use. However,\nquantization down to 3-4 bits per parameter usually leads to moderate-to-high\naccuracy losses, especially for smaller models in the 1-10B parameter range,\nwhich are well-suited for edge deployments. To address this accuracy issue, we\nintroduce the Sparse-Quantized Representation (SpQR), a new compressed format\nand quantization technique which enables for the first time near-lossless\ncompression of LLMs across model scales, while reaching similar compression\nlevels to previous methods. SpQR works by identifying and isolating outlier\nweights, which cause particularly-large quantization errors, and storing them\nin higher precision, while compressing all other weights to 3-4 bits, and\nachieves relative accuracy losses of less than 1% in perplexity for\nhighly-accurate LLaMA and Falcon LLMs. This makes it possible to run 33B\nparameter LLM on a single 24 GB consumer GPU without any performance\ndegradation at 15% speedup thus making powerful LLMs available to consumer\nwithout any downsides. SpQR comes with efficient algorithms for both encoding\nweights into its format, as well as decoding them efficiently at runtime.\nSpecifically, we provide an efficient GPU inference algorithm for SpQR which\nyields faster inference than 16-bit baselines at similar accuracy, while\nenabling memory compression gains of more than 4x.", "published": "2023-06-05 17:53:28", "link": "http://arxiv.org/abs/2306.03078v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Is ChatGPT a Good Teacher Coach? Measuring Zero-Shot Performance For\n  Scoring and Providing Actionable Insights on Classroom Instruction", "abstract": "Coaching, which involves classroom observation and expert feedback, is a\nwidespread and fundamental part of teacher training. However, the majority of\nteachers do not have access to consistent, high quality coaching due to limited\nresources and access to expertise. We explore whether generative AI could\nbecome a cost-effective complement to expert feedback by serving as an\nautomated teacher coach. In doing so, we propose three teacher coaching tasks\nfor generative AI: (A) scoring transcript segments based on classroom\nobservation instruments, (B) identifying highlights and missed opportunities\nfor good instructional strategies, and (C) providing actionable suggestions for\neliciting more student reasoning. We recruit expert math teachers to evaluate\nthe zero-shot performance of ChatGPT on each of these tasks for elementary math\nclassroom transcripts. Our results reveal that ChatGPT generates responses that\nare relevant to improving instruction, but they are often not novel or\ninsightful. For example, 82% of the model's suggestions point to places in the\ntranscript where the teacher is already implementing that suggestion. Our work\nhighlights the challenges of producing insightful, novel and truthful feedback\nfor teachers while paving the way for future research to address these\nobstacles and improve the capacity of generative AI to coach teachers.", "published": "2023-06-05 17:59:21", "link": "http://arxiv.org/abs/2306.03090v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Dense Retrieval with Relevance-Aware Contrastive\n  Pre-Training", "abstract": "Dense retrievers have achieved impressive performance, but their demand for\nabundant training data limits their application scenarios. Contrastive\npre-training, which constructs pseudo-positive examples from unlabeled data,\nhas shown great potential to solve this problem. However, the pseudo-positive\nexamples crafted by data augmentations can be irrelevant. To this end, we\npropose relevance-aware contrastive learning. It takes the intermediate-trained\nmodel itself as an imperfect oracle to estimate the relevance of positive pairs\nand adaptively weighs the contrastive loss of different pairs according to the\nestimated relevance. Our method consistently improves the SOTA unsupervised\nContriever model on the BEIR and open-domain QA retrieval benchmarks. Further\nexploration shows that our method can not only beat BM25 after further\npre-training on the target corpus but also serves as a good few-shot learner.\nOur code is publicly available at https://github.com/Yibin-Lei/ReContriever.", "published": "2023-06-05 18:20:27", "link": "http://arxiv.org/abs/2306.03166v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Composition and Deformance: Measuring Imageability with a Text-to-Image\n  Model", "abstract": "Although psycholinguists and psychologists have long studied the tendency of\nlinguistic strings to evoke mental images in hearers or readers, most\ncomputational studies have applied this concept of imageability only to\nisolated words. Using recent developments in text-to-image generation models,\nsuch as DALLE mini, we propose computational methods that use generated images\nto measure the imageability of both single English words and connected text. We\nsample text prompts for image generation from three corpora: human-generated\nimage captions, news article sentences, and poem lines. We subject these\nprompts to different deformances to examine the model's ability to detect\nchanges in imageability caused by compositional change. We find high\ncorrelation between the proposed computational measures of imageability and\nhuman judgments of individual words. We also find the proposed measures more\nconsistently respond to changes in compositionality than baseline approaches.\nWe discuss possible effects of model training and implications for the study of\ncompositionality in text-to-image models.", "published": "2023-06-05 18:22:23", "link": "http://arxiv.org/abs/2306.03168v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "AutoScrum: Automating Project Planning Using Large Language Models", "abstract": "Recent advancements in the field of large language models have made it\npossible to use language models for advanced reasoning. In this paper we\nleverage this ability for designing complex project plans based only on knowing\nthe current state and the desired state. Two approaches are demonstrated - a\nscrum based approach and a shortcut plan approach. The scrum based approach\nexecutes an automated process of requirements gathering, user story mapping,\nfeature identification, task decomposition and finally generates questions and\nsearch terms for seeking out domain specific information to assist with task\ncompletion. The shortcut approach looks at most recent snapshot of the current\nand desired state and generates the next most reasonable task to do in order to\nget to the desired state as quickly as possible. In this paper we automate\neverything using a novel concept of \"Language Programs\". These are programs\nwritten in natural language designed to process input data through the language\nmodel. Guidance language is used for all LLM programs. All demo source code for\nthis paper is available at https://github.com/autoscrum/autoscrum", "published": "2023-06-05 19:16:37", "link": "http://arxiv.org/abs/2306.03197v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "A Static Evaluation of Code Completion by Large Language Models", "abstract": "Large language models trained on code have shown great potential to increase\nproductivity of software developers. Several execution-based benchmarks have\nbeen proposed to evaluate functional correctness of model-generated code on\nsimple programming problems. Nevertheless, it is expensive to perform the same\nevaluation on complex real-world projects considering the execution cost. On\nthe contrary, static analysis tools such as linters, which can detect errors\nwithout running the program, haven't been well explored for evaluating code\ngeneration models. In this work, we propose a static evaluation framework to\nquantify static errors in Python code completions, by leveraging Abstract\nSyntax Trees. Compared with execution-based evaluation, our method is not only\nmore efficient, but also applicable to code in the wild. For experiments, we\ncollect code context from open source repos to generate one million function\nbodies using public models. Our static analysis reveals that Undefined Name and\nUnused Variable are the most common errors among others made by language\nmodels. Through extensive studies, we also show the impact of sampling\ntemperature, model size, and context on static errors in code completions.", "published": "2023-06-05 19:23:34", "link": "http://arxiv.org/abs/2306.03203v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "NLU on Data Diets: Dynamic Data Subset Selection for NLP Classification\n  Tasks", "abstract": "Finetuning large language models inflates the costs of NLU applications and\nremains the bottleneck of development cycles. Recent works in computer vision\nuse data pruning to reduce training time. Pruned data selection with static\nmethods is based on a score calculated for each training example prior to\nfinetuning, which involves important computational overhead. Moreover, the\nscore may not necessarily be representative of sample importance throughout the\nentire training duration. We propose to address these issues with a refined\nversion of dynamic data pruning, a curriculum which periodically scores and\ndiscards unimportant examples during finetuning. Our method leverages an EL2N\nmetric that we extend to the joint intent and slot classification task, and an\ninitial finetuning phase on the full train set. Our results on the GLUE\nbenchmark and four joint NLU datasets show a better time-accuracy trade-off\ncompared to static methods. Our method preserves full accuracy while training\non 50% of the data points and reduces computational times by up to 41%. If we\ntolerate instead a minor drop of accuracy of 1%, we can prune 80% of the\ntraining examples for a reduction in finetuning time reaching 66%.", "published": "2023-06-05 19:30:41", "link": "http://arxiv.org/abs/2306.03208v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Skill over Scale: The Case for Medium, Domain-Specific Models for SE", "abstract": "Recent advancements in AI have sparked a trend in constructing large,\ngeneralist language models that handle a multitude of tasks, including many\ncode-related ones. While these models are expensive to train and are often\nclosed-source, they have enjoyed broad adoption because they tend to outperform\nsmaller, domain-specific models of code. In this work, we argue that this is\nnot a foregone conclusion. We show that modestly sized domain-specific models\ncan outperform much larger ones on code labeling tasks, provided they are\ntrained to the same standards. Concretely, we focus on StackOverflow (SO),\nwhich offers large volumes of aligned code and text data. We align established\nbest-practices for pre-training large language models with properties of SO as\na data source, especially using a large context window (2,048 tokens), coupled\nwith a powerful toolkit (Megatron-LM) to train two models: SOBertBase (125M\nparameters) and SOBertLarge (762M parameters), at a budget of just $374 and\n$1600 each. We compare the performance of our models with a prior\ndomain-specific model which did not adopt many of these practices\n(BERTOverflow), as well two general-purpose BERT models and two models in\nOpenAI's GPT series (GPT-3.5 and GPT-4). We study four labeling tasks: question\nquality prediction, closed question prediction, NER and obsoletion prediction.\nThe final task is a new benchmark we introduce, on which we additionally\ncompare SOBert with a fine-tuned CodeLlama and StackLlama (models with 10x more\nparameters than SOBertLarge). Our models consistently outperform all baselines.\nIn contrast, BertOverflow is outperformed by generalist models in most tasks.\nThese results demonstrate that pre-training both extensively and properly on\nin-domain data can yield a powerful and affordable alternative to leveraging\nclosed-source general-purpose models. Both models are released to the public on\nHugging Face.", "published": "2023-06-05 21:38:30", "link": "http://arxiv.org/abs/2306.03268v3", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "A Scalable and Adaptive System to Infer the Industry Sectors of\n  Companies: Prompt + Model Tuning of Generative Language Models", "abstract": "The Private Equity (PE) firms operate investment funds by acquiring and\nmanaging companies to achieve a high return upon selling. Many PE funds are\nthematic, meaning investment professionals aim to identify trends by covering\nas many industry sectors as possible, and picking promising companies within\nthese sectors. So, inferring sectors for companies is critical to the success\nof thematic PE funds. In this work, we standardize the sector framework and\ndiscuss the typical challenges; we then introduce our sector inference system\naddressing these challenges. Specifically, our system is built on a\nmedium-sized generative language model, finetuned with a prompt + model tuning\nprocedure. The deployed model demonstrates a superior performance than the\ncommon baselines. The system has been serving many PE professionals for over a\nyear, showing great scalability to data volume and adaptability to any change\nin sector framework and/or annotation.", "published": "2023-06-05 23:55:09", "link": "http://arxiv.org/abs/2306.03313v1", "categories": ["cs.CL", "cs.AI", "68T50, 68T05", "I.2.7; I.2.1"], "primary_category": "cs.CL"}
{"title": "Few Shot Rationale Generation using Self-Training with Dual Teachers", "abstract": "Self-rationalizing models that also generate a free-text explanation for\ntheir predicted labels are an important tool to build trustworthy AI\napplications. Since generating explanations for annotated labels is a laborious\nand costly pro cess, recent models rely on large pretrained language models\n(PLMs) as their backbone and few-shot learning. In this work we explore a\nself-training approach leveraging both labeled and unlabeled data to further\nimprove few-shot models, under the assumption that neither human written\nrationales nor annotated task labels are available at scale. We introduce a\nnovel dual-teacher learning framework, which learns two specialized teacher\nmodels for task prediction and rationalization using self-training and distills\ntheir knowledge into a multi-tasking student model that can jointly generate\nthe task label and rationale. Furthermore, we formulate a new loss function,\nMasked Label Regularization (MLR) which promotes explanations to be strongly\nconditioned on predicted labels. Evaluation on three public datasets\ndemonstrate that the proposed methods are effective in modeling task labels and\ngenerating faithful rationales.", "published": "2023-06-05 23:57:52", "link": "http://arxiv.org/abs/2306.03315v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Study of Situational Reasoning for Traffic Understanding", "abstract": "Intelligent Traffic Monitoring (ITMo) technologies hold the potential for\nimproving road safety/security and for enabling smart city infrastructure.\nUnderstanding traffic situations requires a complex fusion of perceptual\ninformation with domain-specific and causal commonsense knowledge. Whereas\nprior work has provided benchmarks and methods for traffic monitoring, it\nremains unclear whether models can effectively align these information sources\nand reason in novel scenarios. To address this assessment gap, we devise three\nnovel text-based tasks for situational reasoning in the traffic domain: i)\nBDD-QA, which evaluates the ability of Language Models (LMs) to perform\nsituational decision-making, ii) TV-QA, which assesses LMs' abilities to reason\nabout complex event causality, and iii) HDT-QA, which evaluates the ability of\nmodels to solve human driving exams. We adopt four knowledge-enhanced methods\nthat have shown generalization capability across language reasoning tasks in\nprior work, based on natural language inference, commonsense knowledge-graph\nself-supervision, multi-QA joint training, and dense retrieval of domain\ninformation. We associate each method with a relevant knowledge source,\nincluding knowledge graphs, relevant benchmarks, and driving manuals. In\nextensive experiments, we benchmark various knowledge-aware methods against the\nthree datasets, under zero-shot evaluation; we provide in-depth analyses of\nmodel performance on data partitions and examine model predictions\ncategorically, to yield useful insights on traffic understanding, given\ndifferent background knowledge and reasoning strategies.", "published": "2023-06-05 01:01:12", "link": "http://arxiv.org/abs/2306.02520v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Incorporating L2 Phonemes Using Articulatory Features for Robust Speech\n  Recognition", "abstract": "The limited availability of non-native speech datasets presents a major\nchallenge in automatic speech recognition (ASR) to narrow the performance gap\nbetween native and non-native speakers. To address this, the focus of this\nstudy is on the efficient incorporation of the L2 phonemes, which in this work\nrefer to Korean phonemes, through articulatory feature analysis. This not only\nenables accurate modeling of pronunciation variants but also allows for the\nutilization of both native Korean and English speech datasets. We employ the\nlattice-free maximum mutual information (LF-MMI) objective in an end-to-end\nmanner, to train the acoustic model to align and predict one of multiple\npronunciation candidates. Experimental results show that the proposed method\nimproves ASR accuracy for Korean L2 speech by training solely on L1 speech\ndata. Furthermore, fine-tuning on L2 speech improves recognition accuracy for\nboth L1 and L2 speech without performance trade-offs.", "published": "2023-06-05 01:55:33", "link": "http://arxiv.org/abs/2306.02534v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Evaluation of AI Chatbots for Patient-Specific EHR Questions", "abstract": "This paper investigates the use of artificial intelligence chatbots for\npatient-specific question answering (QA) from clinical notes using several\nlarge language model (LLM) based systems: ChatGPT (versions 3.5 and 4), Google\nBard, and Claude. We evaluate the accuracy, relevance, comprehensiveness, and\ncoherence of the answers generated by each model using a 5-point Likert scale\non a set of patient-specific questions.", "published": "2023-06-05 02:52:54", "link": "http://arxiv.org/abs/2306.02549v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and\n  Generative Fusion", "abstract": "We present LLM-Blender, an ensembling framework designed to attain\nconsistently superior performance by leveraging the diverse strengths of\nmultiple open-source large language models (LLMs). Our framework consists of\ntwo modules: PairRanker and GenFuser, addressing the observation that optimal\nLLMs for different examples can significantly vary. PairRanker employs a\nspecialized pairwise comparison method to distinguish subtle differences\nbetween candidate outputs. It jointly encodes the input text and a pair of\ncandidates, using cross-attention encoders to determine the superior one. Our\nresults demonstrate that PairRanker exhibits the highest correlation with\nChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates,\ngenerating an improved output by capitalizing on their strengths and mitigating\ntheir weaknesses. To facilitate large-scale evaluation, we introduce a\nbenchmark dataset, MixInstruct, which is a mixture of multiple instruction\ndatasets featuring oracle pairwise comparisons. Our LLM-Blender significantly\noutperform individual LLMs and baseline methods across various metrics,\nestablishing a substantial performance gap.", "published": "2023-06-05 03:32:26", "link": "http://arxiv.org/abs/2306.02561v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Transfer Learning for Phrase Break Prediction with\n  Multilingual Language Model", "abstract": "Phrase break prediction is a crucial task for improving the prosody\nnaturalness of a text-to-speech (TTS) system. However, most proposed phrase\nbreak prediction models are monolingual, trained exclusively on a large amount\nof labeled data. In this paper, we address this issue for low-resource\nlanguages with limited labeled data using cross-lingual transfer. We\ninvestigate the effectiveness of zero-shot and few-shot cross-lingual transfer\nfor phrase break prediction using a pre-trained multilingual language model. We\nuse manually collected datasets in four Indo-European languages: one\nhigh-resource language and three with limited resources. Our findings\ndemonstrate that cross-lingual transfer learning can be a particularly\neffective approach, especially in the few-shot setting, for improving\nperformance in low-resource languages. This suggests that cross-lingual\ntransfer can be inexpensive and effective for developing TTS front-end in\nresource-poor languages.", "published": "2023-06-05 04:10:04", "link": "http://arxiv.org/abs/2306.02579v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Graph-Aware Language Model Pre-Training on a Large Graph Corpus Can Help\n  Multiple Graph Applications", "abstract": "Model pre-training on large text corpora has been demonstrated effective for\nvarious downstream applications in the NLP domain. In the graph mining domain,\na similar analogy can be drawn for pre-training graph models on large graphs in\nthe hope of benefiting downstream graph applications, which has also been\nexplored by several recent studies. However, no existing study has ever\ninvestigated the pre-training of text plus graph models on large heterogeneous\ngraphs with abundant textual information (a.k.a. large graph corpora) and then\nfine-tuning the model on different related downstream applications with\ndifferent graph schemas. To address this problem, we propose a framework of\ngraph-aware language model pre-training (GALM) on a large graph corpus, which\nincorporates large language models and graph neural networks, and a variety of\nfine-tuning methods on downstream applications. We conduct extensive\nexperiments on Amazon's real internal datasets and large public datasets.\nComprehensive empirical results and in-depth analysis demonstrate the\neffectiveness of our proposed methods along with lessons learned.", "published": "2023-06-05 04:46:44", "link": "http://arxiv.org/abs/2306.02592v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Novel Interpretable and Generalizable Re-synchronization Model for\n  Cued Speech based on a Multi-Cuer Corpus", "abstract": "Cued Speech (CS) is a multi-modal visual coding system combining lip reading\nwith several hand cues at the phonetic level to make the spoken language\nvisible to the hearing impaired. Previous studies solved asynchronous problems\nbetween lip and hand movements by a cuer\\footnote{The people who perform Cued\nSpeech are called the cuer.}-dependent piecewise linear model for English and\nFrench CS. In this work, we innovatively propose three statistical measure on\nthe lip stream to build an interpretable and generalizable model for predicting\nhand preceding time (HPT), which achieves cuer-independent by a proper\nnormalization. Particularly, we build the first Mandarin CS corpus comprising\nannotated videos from five speakers including three normal and two hearing\nimpaired individuals. Consequently, we show that the hand preceding phenomenon\nexists in Mandarin CS production with significant differences between normal\nand hearing impaired people. Extensive experiments demonstrate that our model\noutperforms the baseline and the previous state-of-the-art methods.", "published": "2023-06-05 05:03:11", "link": "http://arxiv.org/abs/2306.02596v1", "categories": ["eess.AS", "cs.CL", "cs.CV"], "primary_category": "eess.AS"}
{"title": "What Makes Entities Similar? A Similarity Flooding Perspective for\n  Multi-sourced Knowledge Graph Embeddings", "abstract": "Joint representation learning over multi-sourced knowledge graphs (KGs)\nyields transferable and expressive embeddings that improve downstream tasks.\nEntity alignment (EA) is a critical step in this process. Despite recent\nconsiderable research progress in embedding-based EA, how it works remains to\nbe explored. In this paper, we provide a similarity flooding perspective to\nexplain existing translation-based and aggregation-based EA models. We prove\nthat the embedding learning process of these models actually seeks a fixpoint\nof pairwise similarities between entities. We also provide experimental\nevidence to support our theoretical analysis. We propose two simple but\neffective methods inspired by the fixpoint computation in similarity flooding,\nand demonstrate their effectiveness on benchmark datasets. Our work bridges the\ngap between recent embedding-based models and the conventional similarity\nflooding algorithm. It would improve our understanding of and increase our\nfaith in embedding-based EA.", "published": "2023-06-05 06:50:09", "link": "http://arxiv.org/abs/2306.02622v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Do-GOOD: Towards Distribution Shift Evaluation for Pre-Trained Visual\n  Document Understanding Models", "abstract": "Numerous pre-training techniques for visual document understanding (VDU) have\nrecently shown substantial improvements in performance across a wide range of\ndocument tasks. However, these pre-trained VDU models cannot guarantee\ncontinued success when the distribution of test data differs from the\ndistribution of training data. In this paper, to investigate how robust\nexisting pre-trained VDU models are to various distribution shifts, we first\ndevelop an out-of-distribution (OOD) benchmark termed Do-GOOD for the\nfine-Grained analysis on Document image-related tasks specifically. The Do-GOOD\nbenchmark defines the underlying mechanisms that result in different\ndistribution shifts and contains 9 OOD datasets covering 3 VDU related tasks,\ne.g., document information extraction, classification and question answering.\nWe then evaluate the robustness and perform a fine-grained analysis of 5 latest\nVDU pre-trained models and 2 typical OOD generalization algorithms on these OOD\ndatasets. Results from the experiments demonstrate that there is a significant\nperformance gap between the in-distribution (ID) and OOD settings for document\nimages, and that fine-grained analysis of distribution shifts can reveal the\nbrittle nature of existing pre-trained VDU models and OOD generalization\nalgorithms. The code and datasets for our Do-GOOD benchmark can be found at\nhttps://github.com/MAEHCM/Do-GOOD.", "published": "2023-06-05 06:50:42", "link": "http://arxiv.org/abs/2306.02623v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "BeAts: Bengali Speech Acts Recognition using Multimodal Attention Fusion", "abstract": "Spoken languages often utilise intonation, rhythm, intensity, and structure,\nto communicate intention, which can be interpreted differently depending on the\nrhythm of speech of their utterance. These speech acts provide the foundation\nof communication and are unique in expression to the language. Recent\nadvancements in attention-based models, demonstrating their ability to learn\npowerful representations from multilingual datasets, have performed well in\nspeech tasks and are ideal to model specific tasks in low resource languages.\nHere, we develop a novel multimodal approach combining two models, wav2vec2.0\nfor audio and MarianMT for text translation, by using multimodal attention\nfusion to predict speech acts in our prepared Bengali speech corpus. We also\nshow that our model BeAts ($\\underline{\\textbf{Be}}$ngali speech acts\nrecognition using Multimodal $\\underline{\\textbf{At}}$tention\nFu$\\underline{\\textbf{s}}$ion) significantly outperforms both the unimodal\nbaseline using only speech data and a simpler bimodal fusion using both speech\nand text data. Project page: https://soumitri2001.github.io/BeAts", "published": "2023-06-05 08:12:17", "link": "http://arxiv.org/abs/2306.02680v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multiple output samples per input in a single-output Gaussian process", "abstract": "The standard Gaussian Process (GP) only considers a single output sample per\ninput in the training set. Datasets for subjective tasks, such as spoken\nlanguage assessment, may be annotated with output labels from multiple human\nraters per input. This paper proposes to generalise the GP to allow for these\nmultiple output samples in the training set, and thus make use of available\noutput uncertainty information. This differs from a multi-output GP, as all\noutput samples are from the same task here. The output density function is\nformulated to be the joint likelihood of observing all output samples, and\nlatent variables are not repeated to reduce computation cost. The test set\npredictions are inferred similarly to a standard GP, with a difference being in\nthe optimised hyper-parameters. This is evaluated on speechocean762, showing\nthat it allows the GP to compute a test set output distribution that is more\nsimilar to the collection of reference outputs from the multiple human raters.", "published": "2023-06-05 09:12:34", "link": "http://arxiv.org/abs/2306.02719v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Identifying the style by a qualified reader on a short fragment of\n  generated poetry", "abstract": "Style is an important concept in today's challenges in natural language\ngenerating. After the success in the field of image style transfer, the task of\ntext style transfer became actual and attractive. Researchers are also\ninterested in the tasks of style reproducing in generation of the poetic text.\nEvaluation of style reproducing in natural poetry generation remains a problem.\nI used 3 character-based LSTM-models to work with style reproducing assessment.\nAll three models were trained on the corpus of texts by famous Russian-speaking\npoets. Samples were shown to the assessors and 4 answer options were offered,\nthe style of which poet this sample reproduces. In addition, the assessors were\nasked how well they were familiar with the work of the poet they had named.\nStudents studying history of literature were the assessors, 94 answers were\nreceived. It has appeared that accuracy of definition of style increases if the\nassessor can quote the poet by heart. Each model showed at least 0.7\nmacro-average accuracy. The experiment showed that it is better to involve a\nprofessional rather than a naive reader in the evaluation of style in the tasks\nof poetry generation, while lstm models are good at reproducing the style of\nRussian poets even on a limited training corpus.", "published": "2023-06-05 10:55:15", "link": "http://arxiv.org/abs/2306.02771v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Human-like Few-Shot Learning via Bayesian Reasoning over Natural\n  Language", "abstract": "A core tension in models of concept learning is that the model must carefully\nbalance the tractability of inference against the expressivity of the\nhypothesis class. Humans, however, can efficiently learn a broad range of\nconcepts. We introduce a model of inductive learning that seeks to be\nhuman-like in that sense. It implements a Bayesian reasoning process where a\nlanguage model first proposes candidate hypotheses expressed in natural\nlanguage, which are then re-weighed by a prior and a likelihood. By estimating\nthe prior from human data, we can predict human judgments on learning problems\ninvolving numbers and sets, spanning concepts that are generative,\ndiscriminative, propositional, and higher-order.", "published": "2023-06-05 11:46:45", "link": "http://arxiv.org/abs/2306.02797v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video\n  Understanding", "abstract": "We present Video-LLaMA a multi-modal framework that empowers Large Language\nModels (LLMs) with the capability of understanding both visual and auditory\ncontent in the video. Video-LLaMA bootstraps cross-modal training from the\nfrozen pre-trained visual and audio encoders and the frozen LLMs. Unlike\nprevious works that complement LLMs to process the visual or audio signals\nonly, Video-LLaMA enables video comprehension by tackling two challenges: (1)\ncapturing the temporal changes in visual scenes, (2) integrating audio-visual\nsignals. To counter the first challenge, we propose a Video Q-former to\nassemble a pre-trained image encoder into our video encoder and introduce a\nvideo-to-text generation task to learn video-language correspondence. For the\nsecond challenge, we leverage ImageBind, a universal embedding model aligning\nmultiple modalities, as the pre-trained audio encoder and introduce an Audio\nQ-former on top of ImageBind to learn reasonable auditory query embeddings for\nthe LLM module. To align the output of both visual and audio encoders with\nLLM's embedding space, we first train Video-LLaMA on massive\nvideo/image-caption pairs and then tune our model with visual-instruction\ndatasets of moderate amount but higher quality. We found Video-LLaMA shows the\nability to perceive and comprehend video content and generate meaningful\nresponses grounded in the visual and auditory information presented in the\nvideos.", "published": "2023-06-05 13:17:27", "link": "http://arxiv.org/abs/2306.02858v4", "categories": ["cs.CL", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "N-Shot Benchmarking of Whisper on Diverse Arabic Speech Recognition", "abstract": "Whisper, the recently developed multilingual weakly supervised model, is\nreported to perform well on multiple speech recognition benchmarks in both\nmonolingual and multilingual settings. However, it is not clear how Whisper\nwould fare under diverse conditions even on languages it was evaluated on such\nas Arabic. In this work, we address this gap by comprehensively evaluating\nWhisper on several varieties of Arabic speech for the ASR task. Our evaluation\ncovers most publicly available Arabic speech data and is performed under n-shot\n(zero-, few-, and full) finetuning. We also investigate the robustness of\nWhisper under completely novel conditions, such as in dialect-accented standard\nArabic and in unseen dialects for which we develop evaluation data. Our\nexperiments show that although Whisper zero-shot outperforms fully finetuned\nXLS-R models on all datasets, its performance deteriorates significantly in the\nzero-shot setting for five unseen dialects (i.e., Algeria, Jordan, Palestine,\nUAE, and Yemen).", "published": "2023-06-05 14:09:25", "link": "http://arxiv.org/abs/2306.02902v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Sequential Monte Carlo Steering of Large Language Models using\n  Probabilistic Programs", "abstract": "Even after fine-tuning and reinforcement learning, large language models\n(LLMs) can be difficult, if not impossible, to control reliably with prompts\nalone. We propose a new inference-time approach to enforcing syntactic and\nsemantic constraints on the outputs of LLMs, called sequential Monte Carlo\n(SMC) steering. The key idea is to specify language generation tasks as\nposterior inference problems in a class of discrete probabilistic sequence\nmodels, and replace standard decoding with sequential Monte Carlo inference.\nFor a computational cost similar to that of beam search, SMC can steer LLMs to\nsolve diverse tasks, including infilling, generation under syntactic\nconstraints, and prompt intersection. To facilitate experimentation with SMC\nsteering, we present a probabilistic programming library, LLaMPPL\n(https://github.com/probcomp/hfppl), for concisely specifying new generation\ntasks as language model probabilistic programs, and automating steering of\nLLaMA-family Transformers.", "published": "2023-06-05 17:55:05", "link": "http://arxiv.org/abs/2306.03081v2", "categories": ["cs.AI", "cs.CL", "cs.PL", "stat.CO"], "primary_category": "cs.AI"}
{"title": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems", "abstract": "Large Language Models (LLMs) have greatly advanced code auto-completion\nsystems, with a potential for substantial productivity enhancements for\ndevelopers. However, current benchmarks mainly focus on single-file tasks,\nleaving an assessment gap for more complex, real-world, multi-file programming\nscenarios. To fill this gap, we introduce RepoBench, a new benchmark\nspecifically designed for evaluating repository-level code auto-completion\nsystems. RepoBench supports both Python and Java and consists of three\ninterconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code\nCompletion), and RepoBench-P (Pipeline). Each task respectively measures the\nsystem's ability to retrieve the most relevant code snippets from other files\nas cross-file context, predict the next line of code with cross-file and\nin-file context, and handle complex tasks that require a combination of both\nretrieval and next-line prediction. RepoBench aims to facilitate a more\ncomplete comparison of performance and encouraging continuous improvement in\nauto-completion systems. RepoBench is publicly available at\nhttps://github.com/Leolty/repobench.", "published": "2023-06-05 17:59:41", "link": "http://arxiv.org/abs/2306.03091v2", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Early Weight Averaging meets High Learning Rates for LLM Pre-training", "abstract": "Training Large Language Models (LLMs) incurs significant cost; hence, any\nstrategy that accelerates model convergence is helpful. In this paper, we\ninvestigate the ability of a simple idea checkpoint averaging along the\ntrajectory of a training run to improve both convergence and generalization\nquite early on during training. Here we show that models trained with high\nlearning rates observe higher gains due to checkpoint averaging. Furthermore,\nthese gains are amplified when checkpoints are sampled with considerable\nspacing in training steps. Our training recipe outperforms conventional\ntraining and popular checkpoint averaging baselines such as exponential moving\naverage (EMA) and stochastic moving average (SWA). We evaluate our training\nrecipe by pre-training LLMs, where high learning rates are inherently preferred\ndue to extremely large batch sizes. Specifically, we pre-trained nanoGPT-2\nmodels of varying sizes, small (125M), medium (335M), and large (770M)on the\nOpenWebText dataset, comprised of 9B tokens. Additionally, we present results\nfor publicly available Pythia LLMs, ranging from 1B to 12B, which were trained\non the PILE-deduped dataset containing 207B tokens.", "published": "2023-06-05 20:51:44", "link": "http://arxiv.org/abs/2306.03241v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Semantically-Prompted Language Models Improve Visual Descriptions", "abstract": "Language-vision models like CLIP have made significant strides in vision\ntasks, such as zero-shot image classification (ZSIC). However, generating\nspecific and expressive visual descriptions remains challenging; descriptions\nproduced by current methods are often ambiguous and lacking in granularity. To\ntackle these issues, we propose V-GLOSS: Visual Glosses, a novel method built\nupon two key ideas. The first is Semantic Prompting, which conditions a\nlanguage model on structured semantic knowledge. The second is a new\ncontrastive algorithm that elicits fine-grained distinctions between similar\nconcepts. With both ideas, we demonstrate that V-GLOSS improves visual\ndescriptions and achieves strong results in the zero-shot setting on general\nand fine-grained image-classification datasets, including ImageNet, STL-10,\nFGVC Aircraft, and Flowers 102. Moreover, these descriptive capabilities\ncontribute to enhancing image-generation performance. Finally, we introduce a\nquality-tested silver dataset with descriptions generated with V-GLOSS for all\nImageNet classes.", "published": "2023-06-05 17:22:54", "link": "http://arxiv.org/abs/2306.06077v4", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "OTF: Optimal Transport based Fusion of Supervised and Self-Supervised\n  Learning Models for Automatic Speech Recognition", "abstract": "Self-Supervised Learning (SSL) Automatic Speech Recognition (ASR) models have\nshown great promise over Supervised Learning (SL) ones in low-resource\nsettings. However, the advantages of SSL are gradually weakened when the amount\nof labeled data increases in many industrial applications. To further improve\nthe ASR performance when abundant labels are available, we first explore the\npotential of combining SL and SSL ASR models via analyzing their\ncomplementarity in recognition accuracy and optimization property. Then, we\npropose a novel Optimal Transport based Fusion (OTF) method for SL and SSL\nmodels without incurring extra computation cost in inference. Specifically,\noptimal transport is adopted to softly align the layer-wise weights to unify\nthe two different networks into a single one. Experimental results on the\npublic 1k-hour English LibriSpeech dataset and our in-house 2.6k-hour Chinese\ndataset show that OTF largely outperforms the individual models with lower\nerror rates.", "published": "2023-06-05 02:24:24", "link": "http://arxiv.org/abs/2306.02541v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Divided spectro-temporal attention for sound event localization and\n  detection in real scenes for DCASE2023 challenge", "abstract": "Localizing sounds and detecting events in different room environments is a\ndifficult task, mainly due to the wide range of reflections and reverberations.\nWhen training neural network models with sounds recorded in only a few room\nenvironments, there is a tendency for the models to become overly specialized\nto those specific environments, resulting in overfitting. To address this\noverfitting issue, we propose divided spectro-temporal attention. In comparison\nto the baseline method, which utilizes a convolutional recurrent neural network\n(CRNN) followed by a temporal multi-head self-attention layer (MHSA), we\nintroduce a separate spectral attention layer that aggregates spectral features\nprior to the temporal MHSA. To achieve efficient spectral attention, we reduce\nthe frequency pooling size in the convolutional encoder of the baseline to\nobtain a 3D tensor that incorporates information about frequency, time, and\nchannel. As a result, we can implement spectral attention with channel\nembeddings, which is not possible in the baseline method dealing with only\ntemporal context in the RNN and MHSA layers. We demonstrate that the proposed\ndivided spectro-temporal attention significantly improves the performance of\nsound event detection and localization scores for real test data from the\nSTARSS23 development dataset. Additionally, we show that various data\naugmentations, such as frameshift, time masking, channel swapping, and moderate\nmix-up, along with the use of external data, contribute to the overall\nimprovement in SELD performance.", "published": "2023-06-05 04:39:34", "link": "http://arxiv.org/abs/2306.02591v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "EffCRN: An Efficient Convolutional Recurrent Network for\n  High-Performance Speech Enhancement", "abstract": "Fully convolutional recurrent neural networks (FCRNs) have shown\nstate-of-the-art performance in single-channel speech enhancement. However, the\nnumber of parameters and the FLOPs/second of the original FCRN are\nrestrictively high. A further important class of efficient networks is the\nCRUSE topology, serving as reference in our work. By applying a number of\ntopological changes at once, we propose both an efficient FCRN (FCRN15), and a\nnew family of efficient convolutional recurrent neural networks (EffCRN23,\nEffCRN23lite). We show that our FCRN15 (875K parameters) and EffCRN23lite\n(396K) outperform the already efficient CRUSE5 (85M) and CRUSE4 (7.2M)\nnetworks, respectively, w.r.t. PESQ, DNSMOS and DeltaSNR, while requiring about\n94% less parameters and about 20% less #FLOPs/frame. Thereby, according to\nthese metrics, the FCRN/EffCRN class of networks provides new best-in-class\nnetwork topologies for speech enhancement.", "published": "2023-06-05 11:03:38", "link": "http://arxiv.org/abs/2306.02778v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Vocoder drift in x-vector-based speaker anonymization", "abstract": "State-of-the-art approaches to speaker anonymization typically employ some\nform of perturbation function to conceal speaker information contained within\nan x-vector embedding, then resynthesize utterances in the voice of a new\npseudo-speaker using a vocoder. Strategies to improve the x-vector\nanonymization function have attracted considerable research effort, whereas\nvocoder impacts are generally neglected. In this paper, we show that the impact\nof the vocoder is substantial and sometimes dominant. The vocoder drift, namely\nthe difference between the x-vector vocoder input and that which can be\nextracted subsequently from the output, is learnable and can hence be reversed\nby an attacker; anonymization can be undone and the level of privacy protection\nprovided by such approaches might be weaker than previously thought. The\nfindings call into question the focus upon x-vector anonymization, prompting\nthe need for greater attention to vocoder impacts and stronger attack models\nalike.", "published": "2023-06-05 14:01:20", "link": "http://arxiv.org/abs/2306.02892v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Rethinking the visual cues in audio-visual speaker extraction", "abstract": "The Audio-Visual Speaker Extraction (AVSE) algorithm employs parallel video\nrecording to leverage two visual cues, namely speaker identity and\nsynchronization, to enhance performance compared to audio-only algorithms.\nHowever, the visual front-end in AVSE is often derived from a pre-trained model\nor end-to-end trained, making it unclear which visual cue contributes more to\nthe speaker extraction performance. This raises the question of how to better\nutilize visual cues. To address this issue, we propose two training strategies\nthat decouple the learning of the two visual cues. Our experimental results\ndemonstrate that both visual cues are useful, with the synchronization cue\nhaving a higher impact. We introduce a more explainable model, the Decoupled\nAudio-Visual Speaker Extraction (DAVSE) model, which leverages both visual\ncues.", "published": "2023-06-05 06:53:36", "link": "http://arxiv.org/abs/2306.02625v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Learning Prescription, A Neural Network Hearing Aid Core", "abstract": "The definition of a hearing aid core which is based on a prescription neural\nnetwork (such as NAL-NL2) is defined here. This hearing aid core replaces a\ntraditional compressor hearing aid core which mimics the said hearing aid\nprescription. Whilst the replacement of the compressors for a neural network\nmay seem simple, the implications are vast in terms of the \"learning\nprescription\" where the topology of the neural network may be increased to make\navailable more free parameters and allow great personalisation of the hearing\naid prescription.", "published": "2023-06-05 10:12:41", "link": "http://arxiv.org/abs/2306.02750v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Simultaneous or Sequential Training? How Speech Representations\n  Cooperate in a Multi-Task Self-Supervised Learning System", "abstract": "Speech representation learning with self-supervised algorithms has resulted\nin notable performance boosts in many downstream tasks. Recent work combined\nself-supervised learning (SSL) and visually grounded speech (VGS) processing\nmechanisms for representation learning. The joint training with SSL and VGS\nmechanisms provides the opportunity to utilize both unlabeled speech and\nspeech-related visual information based on data availability. This has shown to\nenhance the quality of learned representations, especially at encoding\nsemantic- and lexical-level knowledge. In this work, we further study the joint\noptimization of wav2vec 2.0-based SSL and transformer-based VGS as a multi-task\nlearning system. We explore a set of training scenarios to understand how\nspeech representations are shared or transferred between the two tasks, and\nwhat is the optimal training strategy for cross-modal semantic retrieval and\nphoneme discrimination performance. As a result, we find that sequential\ntraining with wav2vec 2.0 first and VGS next provides higher performance on\naudio-visual retrieval compared to simultaneous optimization of both learning\nmechanisms. However, the parallel SSL-VGS training reduces the effects of\ncatastrophic forgetting when switching between optimization criteria. Moreover,\nthe results suggest that phonemic representations learned through the VGS\nmechanism may generalize better across datasets compared to those learned with\nSSL.", "published": "2023-06-05 15:35:19", "link": "http://arxiv.org/abs/2306.02972v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "LipVoicer: Generating Speech from Silent Videos Guided by Lip Reading", "abstract": "Lip-to-speech involves generating a natural-sounding speech synchronized with\na soundless video of a person talking. Despite recent advances, current methods\nstill cannot produce high-quality speech with high levels of intelligibility\nfor challenging and realistic datasets such as LRS3. In this work, we present\nLipVoicer, a novel method that generates high-quality speech, even for\nin-the-wild and rich datasets, by incorporating the text modality. Given a\nsilent video, we first predict the spoken text using a pre-trained lip-reading\nnetwork. We then condition a diffusion model on the video and use the extracted\ntext through a classifier-guidance mechanism where a pre-trained ASR serves as\nthe classifier. LipVoicer outperforms multiple lip-to-speech baselines on LRS2\nand LRS3, which are in-the-wild datasets with hundreds of unique speakers in\ntheir test set and an unrestricted vocabulary. Moreover, our experiments show\nthat the inclusion of the text modality plays a major role in the\nintelligibility of the produced speech, readily perceptible while listening,\nand is empirically reflected in the substantial reduction of the WER metric. We\ndemonstrate the effectiveness of LipVoicer through human evaluation, which\nshows that it produces more natural and synchronized speech signals compared to\ncompeting methods. Finally, we created a demo showcasing LipVoicer's\nsuperiority in producing natural, synchronized, and intelligible speech,\nproviding additional evidence of its effectiveness. Project page and code:\nhttps://github.com/yochaiye/LipVoicer", "published": "2023-06-05 21:20:33", "link": "http://arxiv.org/abs/2306.03258v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Reef Elegy: An Auditory Display of Hawaii's 2019 Coral Bleaching Data", "abstract": "This paper describes an auditory display of Hawaii's 2019 coral bleaching\ndata via means of spatial audio and parameter mapping methods. Selected data\nfields spanning 78 days are mapped to sound surrogates of coral reefs' natural\nsoundscapes, which are progressively altered in their constituent elements as\nthe corresponding coral locations undergo bleaching. For some of these\nelements, this process outlines a trajectory from a dense to a sparser, reduced\nsoundscape, while for others it translates moving away from harmonic tones and\ntowards complex spectra. This experiment is accompanied by a short evaluation\nstudy to contextualize it in an established aesthetic perspective space and to\nprobe its potential for public engagement in the discourse around climate\nchange.", "published": "2023-06-05 23:27:39", "link": "http://arxiv.org/abs/2306.03307v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Controllable Lyrics-to-Melody Generation", "abstract": "Lyrics-to-melody generation is an interesting and challenging topic in AI\nmusic research field. Due to the difficulty of learning the correlations\nbetween lyrics and melody, previous methods suffer from low generation quality\nand lack of controllability. Controllability of generative models enables human\ninteraction with models to generate desired contents, which is especially\nimportant in music generation tasks towards human-centered AI that can\nfacilitate musicians in creative activities. To address these issues, we\npropose a controllable lyrics-to-melody generation network, ConL2M, which is\nable to generate realistic melodies from lyrics in user-desired musical style.\nOur work contains three main novelties: 1) To model the dependencies of music\nattributes cross multiple sequences, inter-branch memory fusion (Memofu) is\nproposed to enable information flow between multi-branch stacked LSTM\narchitecture; 2) Reference style embedding (RSE) is proposed to improve the\nquality of generation as well as control the musical style of generated\nmelodies; 3) Sequence-level statistical loss (SeqLoss) is proposed to help the\nmodel learn sequence-level features of melodies given lyrics. Verified by\nevaluation metrics for music quality and controllability, initial study of\ncontrollable lyrics-to-melody generation shows better generation quality and\nthe feasibility of interacting with users to generate the melodies in desired\nmusical styles when given lyrics.", "published": "2023-06-05 06:14:08", "link": "http://arxiv.org/abs/2306.02613v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On the Behavior of Intrusive and Non-intrusive Speech Enhancement\n  Metrics in Predictive and Generative Settings", "abstract": "Since its inception, the field of deep speech enhancement has been dominated\nby predictive (discriminative) approaches, such as spectral mapping or masking.\nRecently, however, novel generative approaches have been applied to speech\nenhancement, attaining good denoising performance with high subjective quality\nscores. At the same time, advances in deep learning also allowed for the\ncreation of neural network-based metrics, which have desirable traits such as\nbeing able to work without a reference (non-intrusively). Since generatively\nenhanced speech tends to exhibit radically different residual distortions, its\nevaluation using instrumental speech metrics may behave differently compared to\npredictively enhanced speech. In this paper, we evaluate the performance of the\nsame speech enhancement backbone trained under predictive and generative\nparadigms on a variety of metrics and show that intrusive and non-intrusive\nmeasures correlate differently for each paradigm. This analysis motivates the\nsearch for metrics that can together paint a complete and unbiased picture of\nspeech enhancement performance, irrespective of the model's training process.", "published": "2023-06-05 16:30:17", "link": "http://arxiv.org/abs/2306.03014v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DeepVQE: Real Time Deep Voice Quality Enhancement for Joint Acoustic\n  Echo Cancellation, Noise Suppression and Dereverberation", "abstract": "Acoustic echo cancellation (AEC), noise suppression (NS) and dereverberation\n(DR) are an integral part of modern full-duplex communication systems. As the\ndemand for teleconferencing systems increases, addressing these tasks is\nrequired for an effective and efficient online meeting experience. Most prior\nresearch proposes solutions for these tasks separately, combining them with\ndigital signal processing (DSP) based components, resulting in complex\npipelines that are often impractical to deploy in real-world applications. This\npaper proposes a real-time cross-attention deep model, named DeepVQE, based on\nresidual convolutional neural networks (CNNs) and recurrent neural networks\n(RNNs) to simultaneously address AEC, NS, and DR. We conduct several ablation\nstudies to analyze the contributions of different components of our model to\nthe overall performance. DeepVQE achieves state-of-the-art performance on\nnon-personalized tracks from the ICASSP 2023 Acoustic Echo Cancellation\nChallenge and ICASSP 2023 Deep Noise Suppression Challenge test sets, showing\nthat a single model can handle multiple tasks with excellent performance.\nMoreover, the model runs in real-time and has been successfully tested for the\nMicrosoft Teams platform.", "published": "2023-06-05 18:37:05", "link": "http://arxiv.org/abs/2306.03177v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
