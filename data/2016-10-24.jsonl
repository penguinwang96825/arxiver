{"title": "Bridging Neural Machine Translation and Bilingual Dictionaries", "abstract": "Neural Machine Translation (NMT) has become the new state-of-the-art in\nseveral language pairs. However, it remains a challenging problem how to\nintegrate NMT with a bilingual dictionary which mainly contains words rarely or\nnever seen in the bilingual training data. In this paper, we propose two\nmethods to bridge NMT and the bilingual dictionaries. The core idea behind is\nto design novel models that transform the bilingual dictionaries into adequate\nsentence pairs, so that NMT can distil latent bilingual mappings from the ample\nand repetitive phenomena. One method leverages a mixed word/character model and\nthe other attempts at synthesizing parallel sentences guaranteeing massive\noccurrence of the translation lexicon. Extensive experiments demonstrate that\nthe proposed methods can remarkably improve the translation quality, and most\nof the rare words in the test sentences can obtain correct translations if they\nare covered by the dictionary.", "published": "2016-10-24 03:39:22", "link": "http://arxiv.org/abs/1610.07272v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Statistical Machine Translation for Indian Languages: Mission Hindi", "abstract": "This paper discusses Centre for Development of Advanced Computing Mumbai's\n(CDACM) submission to the NLP Tools Contest on Statistical Machine Translation\nin Indian Languages (ILSMT) 2014 (collocated with ICON 2014). The objective of\nthe contest was to explore the effectiveness of Statistical Machine Translation\n(SMT) for Indian language to Indian language and English-Hindi machine\ntranslation. In this paper, we have proposed that suffix separation and word\nsplitting for SMT from agglutinative languages to Hindi significantly improves\nover the baseline (BL). We have also shown that the factored model with\nreordering outperforms the phrase-based SMT for English-Hindi (\\enhi). We\nreport our work on all five pairs of languages, namely Bengali-Hindi (\\bnhi),\nMarathi-Hindi (\\mrhi), Tamil-Hindi (\\tahi), Telugu-Hindi (\\tehi), and \\enhi for\nHealth, Tourism, and General domains.", "published": "2016-10-24 14:06:31", "link": "http://arxiv.org/abs/1610.07418v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reordering rules for English-Hindi SMT", "abstract": "Reordering is a preprocessing stage for Statistical Machine Translation (SMT)\nsystem where the words of the source sentence are reordered as per the syntax\nof the target language. We are proposing a rich set of rules for better\nreordering. The idea is to facilitate the training process by better alignments\nand parallel phrase extraction for a phrase-based SMT system. Reordering also\nhelps the decoding process and hence improving the machine translation quality.\nWe have observed significant improvements in the translation quality by using\nour approach over the baseline SMT. We have used BLEU, NIST, multi-reference\nword error rate, multi-reference position independent error rate for judging\nthe improvements. We have exploited open source SMT toolkit MOSES to develop\nthe system.", "published": "2016-10-24 14:08:59", "link": "http://arxiv.org/abs/1610.07420v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UTD-CRSS Systems for 2016 NIST Speaker Recognition Evaluation", "abstract": "This document briefly describes the systems submitted by the Center for\nRobust Speech Systems (CRSS) from The University of Texas at Dallas (UTD) to\nthe 2016 National Institute of Standards and Technology (NIST) Speaker\nRecognition Evaluation (SRE). We developed several UBM and DNN i-Vector based\nspeaker recognition systems with different data sets and feature\nrepresentations. Given that the emphasis of the NIST SRE 2016 is on language\nmismatch between training and enrollment/test data, so-called domain mismatch,\nin our system development we focused on: (1) using unlabeled in-domain data for\ncentralizing data to alleviate the domain mismatch problem, (2) finding the\nbest data set for training LDA/PLDA, (3) using newly proposed dimension\nreduction technique incorporating unlabeled in-domain data before PLDA\ntraining, (4) unsupervised speaker clustering of unlabeled data and using them\nalone or with previous SREs for PLDA training, (5) score calibration using only\nunlabeled data and combination of unlabeled and development (Dev) data as\nseparate experiments.", "published": "2016-10-24 21:05:05", "link": "http://arxiv.org/abs/1610.07651v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Reporting Dynamics during Breaking News for Rumour Detection in\n  Social Media", "abstract": "Breaking news leads to situations of fast-paced reporting in social media,\nproducing all kinds of updates related to news stories, albeit with the caveat\nthat some of those early updates tend to be rumours, i.e., information with an\nunverified status at the time of posting. Flagging information that is\nunverified can be helpful to avoid the spread of information that may turn out\nto be false. Detection of rumours can also feed a rumour tracking system that\nultimately determines their veracity. In this paper we introduce a novel\napproach to rumour detection that learns from the sequential dynamics of\nreporting during breaking news in social media to detect rumours in new\nstories. Using Twitter datasets collected during five breaking news stories, we\nexperiment with Conditional Random Fields as a sequential classifier that\nleverages context learnt during an event for rumour detection, which we compare\nwith the state-of-the-art rumour detection system as well as other baselines.\nIn contrast to existing work, our classifier does not need to observe tweets\nquerying a piece of information to deem it a rumour, but instead we detect\nrumours from the tweet alone by exploiting context learnt during the event. Our\nclassifier achieves competitive performance, beating the state-of-the-art\nclassifier that relies on querying tweets with improved precision and recall,\nas well as outperforming our best baseline with nearly 40% improvement in terms\nof F1 score. The scale and diversity of our experiments reinforces the\ngeneralisability of our classifier.", "published": "2016-10-24 11:25:24", "link": "http://arxiv.org/abs/1610.07363v1", "categories": ["cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Introduction: Cognitive Issues in Natural Language Processing", "abstract": "This special issue is dedicated to get a better picture of the relationships\nbetween computational linguistics and cognitive science. It specifically raises\ntwo questions: \"what is the potential contribution of computational language\nmodeling to cognitive science?\" and conversely: \"what is the influence of\ncognitive science in contemporary computational linguistics?\"", "published": "2016-10-24 11:30:22", "link": "http://arxiv.org/abs/1610.07365v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Virtual Embodiment: A Scalable Long-Term Strategy for Artificial\n  Intelligence Research", "abstract": "Meaning has been called the \"holy grail\" of a variety of scientific\ndisciplines, ranging from linguistics to philosophy, psychology and the\nneurosciences. The field of Artifical Intelligence (AI) is very much a part of\nthat list: the development of sophisticated natural language semantics is a\nsine qua non for achieving a level of intelligence comparable to humans.\nEmbodiment theories in cognitive science hold that human semantic\nrepresentation depends on sensori-motor experience; the abundant evidence that\nhuman meaning representation is grounded in the perception of physical reality\nleads to the conclusion that meaning must depend on a fusion of multiple\n(perceptual) modalities. Despite this, AI research in general, and its\nsubdisciplines such as computational linguistics and computer vision in\nparticular, have focused primarily on tasks that involve a single modality.\nHere, we propose virtual embodiment as an alternative, long-term strategy for\nAI research that is multi-modal in nature and that allows for the kind of\nscalability required to develop the field coherently and incrementally, in an\nethically responsible fashion.", "published": "2016-10-24 14:37:27", "link": "http://arxiv.org/abs/1610.07432v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "68T01", "I.2.6"], "primary_category": "cs.AI"}
{"title": "Geometry of Polysemy", "abstract": "Vector representations of words have heralded a transformational approach to\nclassical problems in NLP; the most popular example is word2vec. However, a\nsingle vector does not suffice to model the polysemous nature of many\n(frequent) words, i.e., words with multiple meanings. In this paper, we propose\na three-fold approach for unsupervised polysemy modeling: (a) context\nrepresentations, (b) sense induction and disambiguation and (c) lexeme (as a\nword and sense pair) representations. A key feature of our work is the finding\nthat a sentence containing a target word is well represented by a low rank\nsubspace, instead of a point in a vector space. We then show that the subspaces\nassociated with a particular sense of the target word tend to intersect over a\nline (one-dimensional subspace), which we use to disambiguate senses using a\nclustering algorithm that harnesses the Grassmannian geometry of the\nrepresentations. The disambiguation algorithm, which we call $K$-Grassmeans,\nleads to a procedure to label the different senses of the target word in the\ncorpus -- yielding lexeme vector representations, all in an unsupervised manner\nstarting from a large (Wikipedia) corpus in English. Apart from several\nprototypical target (word,sense) examples and a host of empirical studies to\nintuit and justify the various geometric representations, we validate our\nalgorithms on standard sense induction and disambiguation datasets and present\nnew state-of-the-art results.", "published": "2016-10-24 19:35:29", "link": "http://arxiv.org/abs/1610.07569v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Learning to Reason With Adaptive Computation", "abstract": "Multi-hop inference is necessary for machine learning systems to successfully\nsolve tasks such as Recognising Textual Entailment and Machine Reading. In this\nwork, we demonstrate the effectiveness of adaptive computation for learning the\nnumber of inference steps required for examples of different complexity and\nthat learning the correct number of inference steps is difficult. We introduce\nthe first model involving Adaptive Computation Time which provides a small\nperformance benefit on top of a similar model without an adaptive component as\nwell as enabling considerable insight into the reasoning process of the model.", "published": "2016-10-24 20:48:04", "link": "http://arxiv.org/abs/1610.07647v2", "categories": ["cs.CL", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
