{"title": "An entity-driven recursive neural network model for chinese discourse\n  coherence modeling", "abstract": "Chinese discourse coherence modeling remains a challenge taskin Natural\nLanguage Processing field.Existing approaches mostlyfocus on the need for\nfeature engineering, whichadoptthe sophisticated features to capture the logic\nor syntactic or semantic relationships acrosssentences within a text.In this\npaper, we present an entity-drivenrecursive deep modelfor the Chinese discourse\ncoherence evaluation based on current English discourse coherenceneural network\nmodel. Specifically, to overcome the shortage of identifying the entity(nouns)\noverlap across sentences in the currentmodel, Our combined modelsuccessfully\ninvestigatesthe entities information into the recursive neural network\nfreamework.Evaluation results on both sentence ordering and machine translation\ncoherence rating task show the effectiveness of the proposed model, which\nsignificantly outperforms the existing strong baseline.", "published": "2017-04-14 02:41:13", "link": "http://arxiv.org/abs/1704.04336v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Cross-Sentence Context for Neural Machine Translation", "abstract": "In translation, considering the document as a whole can help to resolve\nambiguities and inconsistencies. In this paper, we propose a cross-sentence\ncontext-aware approach and investigate the influence of historical contextual\ninformation on the performance of neural machine translation (NMT). First, this\nhistory is summarized in a hierarchical way. We then integrate the historical\nrepresentation into NMT in two strategies: 1) a warm-start of encoder and\ndecoder states, and 2) an auxiliary context source for updating decoder states.\nExperimental results on a large Chinese-English translation task show that our\napproach significantly improves upon a strong attention-based NMT system by up\nto +2.1 BLEU points.", "published": "2017-04-14 04:56:36", "link": "http://arxiv.org/abs/1704.04347v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Get To The Point: Summarization with Pointer-Generator Networks", "abstract": "Neural sequence-to-sequence models have provided a viable new approach for\nabstractive text summarization (meaning they are not restricted to simply\nselecting and rearranging passages from the original text). However, these\nmodels have two shortcomings: they are liable to reproduce factual details\ninaccurately, and they tend to repeat themselves. In this work we propose a\nnovel architecture that augments the standard sequence-to-sequence attentional\nmodel in two orthogonal ways. First, we use a hybrid pointer-generator network\nthat can copy words from the source text via pointing, which aids accurate\nreproduction of information, while retaining the ability to produce novel words\nthrough the generator. Second, we use coverage to keep track of what has been\nsummarized, which discourages repetition. We apply our model to the CNN / Daily\nMail summarization task, outperforming the current abstractive state-of-the-art\nby at least 2 ROUGE points.", "published": "2017-04-14 07:55:19", "link": "http://arxiv.org/abs/1704.04368v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Robust Are Character-Based Word Embeddings in Tagging and MT Against\n  Wrod Scramlbing or Randdm Nouse?", "abstract": "This paper investigates the robustness of NLP against perturbed word forms.\nWhile neural approaches can achieve (almost) human-like accuracy for certain\ntasks and conditions, they often are sensitive to small changes in the input\nsuch as non-canonical input (e.g., typos). Yet both stability and robustness\nare desired properties in applications involving user-generated content, and\nthe more as humans easily cope with such noisy or adversary conditions. In this\npaper, we study the impact of noisy input. We consider different noise\ndistributions (one type of noise, combination of noise types) and mismatched\nnoise distributions for training and testing. Moreover, we empirically evaluate\nthe robustness of different models (convolutional neural networks, recurrent\nneural networks, non-neural models), different basic units (characters, byte\npair encoding units), and different NLP tasks (morphological tagging, machine\ntranslation).", "published": "2017-04-14 14:43:44", "link": "http://arxiv.org/abs/1704.04441v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bringing Structure into Summaries: Crowdsourcing a Benchmark Corpus of\n  Concept Maps", "abstract": "Concept maps can be used to concisely represent important information and\nbring structure into large document collections. Therefore, we study a variant\nof multi-document summarization that produces summaries in the form of concept\nmaps. However, suitable evaluation datasets for this task are currently\nmissing. To close this gap, we present a newly created corpus of concept maps\nthat summarize heterogeneous collections of web documents on educational\ntopics. It was created using a novel crowdsourcing approach that allows us to\nefficiently determine important elements in large document collections. We\nrelease the corpus along with a baseline system and proposed evaluation\nprotocol to enable further research on this variant of summarization.", "published": "2017-04-14 15:23:45", "link": "http://arxiv.org/abs/1704.04452v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cardinal Virtues: Extracting Relation Cardinalities from Text", "abstract": "Information extraction (IE) from text has largely focused on relations\nbetween individual entities, such as who has won which award. However, some\nfacts are never fully mentioned, and no IE method has perfect recall. Thus, it\nis beneficial to also tap contents about the cardinalities of these relations,\nfor example, how many awards someone has won. We introduce this novel problem\nof extracting cardinalities and discusses the specific challenges that set it\napart from standard IE. We present a distant supervision method using\nconditional random fields. A preliminary evaluation results in precision\nbetween 3% and 55%, depending on the difficulty of relations.", "published": "2017-04-14 15:28:06", "link": "http://arxiv.org/abs/1704.04455v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation Model with a Large Vocabulary Selected by\n  Branching Entropy", "abstract": "Neural machine translation (NMT), a new approach to machine translation, has\nachieved promising results comparable to those of traditional approaches such\nas statistical machine translation (SMT). Despite its recent success, NMT\ncannot handle a larger vocabulary because the training complexity and decoding\ncomplexity proportionally increase with the number of target words. This\nproblem becomes even more serious when translating patent documents, which\ncontain many technical terms that are observed infrequently. In this paper, we\npropose to select phrases that contain out-of-vocabulary words using the\nstatistical approach of branching entropy. This allows the proposed NMT system\nto be applied to a translation task of any language pair without any\nlanguage-specific knowledge about technical term identification. The selected\nphrases are then replaced with tokens during training and post-translated by\nthe phrase translation table of SMT. Evaluation on Japanese-to-Chinese,\nChinese-to-Japanese, Japanese-to-English and English-to-Japanese patent\nsentence translation proved the effectiveness of phrases selected with\nbranching entropy, where the proposed NMT model achieves a substantial\nimprovement over a baseline NMT model without our proposed technique. Moreover,\nthe number of translation errors of under-translation by the baseline NMT model\nwithout our proposed technique reduces to around half by the proposed NMT\nmodel.", "published": "2017-04-14 19:35:14", "link": "http://arxiv.org/abs/1704.04520v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Translation of Patent Sentences with a Large Vocabulary of Technical\n  Terms Using Neural Machine Translation", "abstract": "Neural machine translation (NMT), a new approach to machine translation, has\nachieved promising results comparable to those of traditional approaches such\nas statistical machine translation (SMT). Despite its recent success, NMT\ncannot handle a larger vocabulary because training complexity and decoding\ncomplexity proportionally increase with the number of target words. This\nproblem becomes even more serious when translating patent documents, which\ncontain many technical terms that are observed infrequently. In NMTs, words\nthat are out of vocabulary are represented by a single unknown token. In this\npaper, we propose a method that enables NMT to translate patent sentences\ncomprising a large vocabulary of technical terms. We train an NMT system on\nbilingual data wherein technical terms are replaced with technical term tokens;\nthis allows it to translate most of the source sentences except technical\nterms. Further, we use it as a decoder to translate source sentences with\ntechnical term tokens and replace the tokens with technical term translations\nusing SMT. We also use it to rerank the 1,000-best SMT translations on the\nbasis of the average of the SMT score and that of the NMT rescoring of the\ntranslated sentences with technical term tokens. Our experiments on\nJapanese-Chinese patent sentences show that the proposed NMT system achieves a\nsubstantial improvement of up to 3.1 BLEU points and 2.3 RIBES points over\ntraditional SMT systems and an improvement of approximately 0.6 BLEU points and\n0.8 RIBES points over an equivalent NMT system without our proposed technique.", "published": "2017-04-14 19:36:54", "link": "http://arxiv.org/abs/1704.04521v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Extractive Summarization with Side Information", "abstract": "Most extractive summarization methods focus on the main body of the document\nfrom which sentences need to be extracted. However, the gist of the document\nmay lie in side information, such as the title and image captions which are\noften available for newswire articles. We propose to explore side information\nin the context of single-document extractive summarization. We develop a\nframework for single-document summarization composed of a hierarchical document\nencoder and an attention-based extractor with attention over side information.\nWe evaluate our model on a large scale news dataset. We show that extractive\nsummarization with side information consistently outperforms its counterpart\nthat does not use any side information, in terms of both informativeness and\nfluency.", "published": "2017-04-14 20:29:23", "link": "http://arxiv.org/abs/1704.04530v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Abstract Meaning Representation Parsing", "abstract": "Abstract Meaning Representation (AMR) annotation efforts have mostly focused\non English. In order to train parsers on other languages, we propose a method\nbased on annotation projection, which involves exploiting annotations in a\nsource language and a parallel corpus of the source language and a target\nlanguage. Using English as the source language, we show promising results for\nItalian, Spanish, German and Chinese as target languages. Besides evaluating\nthe target parsers on non-gold datasets, we further propose an evaluation\nmethod that exploits the English gold annotations and does not require access\nto gold annotations for the target languages. This is achieved by inverting the\nprojection process: a new English parser is learned from the target language\nparser and evaluated on the existing English gold standard.", "published": "2017-04-14 20:41:27", "link": "http://arxiv.org/abs/1704.04539v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distributional Modeling on a Diet: One-shot Word Learning from Text Only", "abstract": "We test whether distributional models can do one-shot learning of\ndefinitional properties from text only. Using Bayesian models, we find that\nfirst learning overarching structure in the known data, regularities in textual\ncontexts and in properties, helps one-shot learning, and that individual\ncontext items can be highly informative. Our experiments show that our model\ncan learn properties from a single exposure when given an informative\nutterance.", "published": "2017-04-14 22:29:27", "link": "http://arxiv.org/abs/1704.04550v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimizing Differentiable Relaxations of Coreference Evaluation Metrics", "abstract": "Coreference evaluation metrics are hard to optimize directly as they are\nnon-differentiable functions, not easily decomposable into elementary\ndecisions. Consequently, most approaches optimize objectives only indirectly\nrelated to the end goal, resulting in suboptimal performance. Instead, we\npropose a differentiable relaxation that lends itself to gradient-based\noptimisation, thus bypassing the need for reinforcement learning or heuristic\nmodification of cross-entropy. We show that by modifying the training objective\nof a competitive neural coreference system, we obtain a substantial gain in\nperformance. This suggests that our approach can be regarded as a viable\nalternative to using reinforcement learning or more computationally expensive\nimitation learning.", "published": "2017-04-14 15:22:51", "link": "http://arxiv.org/abs/1704.04451v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ShapeWorld - A new test methodology for multimodal language\n  understanding", "abstract": "We introduce a novel framework for evaluating multimodal deep learning models\nwith respect to their language understanding and generalization abilities. In\nthis approach, artificial data is automatically generated according to the\nexperimenter's specifications. The content of the data, both during training\nand evaluation, can be controlled in detail, which enables tasks to be created\nthat require true generalization abilities, in particular the combination of\npreviously introduced concepts in novel ways. We demonstrate the potential of\nour methodology by evaluating various visual question answering models on four\ndifferent tasks, and show how our framework gives us detailed insights into\ntheir capabilities and limitations. By open-sourcing our framework, we hope to\nstimulate progress in the field of multimodal language understanding.", "published": "2017-04-14 19:01:51", "link": "http://arxiv.org/abs/1704.04517v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
