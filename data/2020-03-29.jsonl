{"title": "Noisy Text Data: Achilles' Heel of BERT", "abstract": "Owing to the phenomenal success of BERT on various NLP tasks and benchmark\ndatasets, industry practitioners are actively experimenting with fine-tuning\nBERT to build NLP applications for solving industry use cases. For most\ndatasets that are used by practitioners to build industrial NLP applications,\nit is hard to guarantee absence of any noise in the data. While BERT has\nperformed exceedingly well for transferring the learnings from one use case to\nanother, it remains unclear how BERT performs when fine-tuned on noisy text. In\nthis work, we explore the sensitivity of BERT to noise in the data. We work\nwith most commonly occurring noise (spelling mistakes, typos) and show that\nthis results in significant degradation in the performance of BERT. We present\nexperimental results to show that BERT's performance on fundamental NLP tasks\nlike sentiment analysis and textual similarity drops significantly in the\npresence of (simulated) noise on benchmark datasets viz. IMDB Movie Review,\nSTS-B, SST-2. Further, we identify shortcomings in the existing BERT pipeline\nthat are responsible for this drop in performance. Our findings suggest that\npractitioners need to be vary of presence of noise in their datasets while\nfine-tuning BERT to solve industry use cases.", "published": "2020-03-29 02:49:11", "link": "http://arxiv.org/abs/2003.12932v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Abstractive Summarization with Combination of Pre-trained\n  Sequence-to-Sequence and Saliency Models", "abstract": "Pre-trained sequence-to-sequence (seq-to-seq) models have significantly\nimproved the accuracy of several language generation tasks, including\nabstractive summarization. Although the fluency of abstractive summarization\nhas been greatly improved by fine-tuning these models, it is not clear whether\nthey can also identify the important parts of the source text to be included in\nthe summary. In this study, we investigated the effectiveness of combining\nsaliency models that identify the important parts of the source text with the\npre-trained seq-to-seq models through extensive experiments. We also proposed a\nnew combination model consisting of a saliency model that extracts a token\nsequence from a source text and a seq-to-seq model that takes the sequence as\nan additional input text. Experimental results showed that most of the\ncombination models outperformed a simple fine-tuned seq-to-seq model on both\nthe CNN/DM and XSum datasets even if the seq-to-seq model is pre-trained on\nlarge-scale corpora. Moreover, for the CNN/DM dataset, the proposed combination\nmodel exceeded the previous best-performed model by 1.33 points on ROUGE-L.", "published": "2020-03-29 14:00:25", "link": "http://arxiv.org/abs/2003.13028v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Named Entities in Medical Case Reports: Corpus and Experiments", "abstract": "We present a new corpus comprising annotations of medical entities in case\nreports, originating from PubMed Central's open access library. In the case\nreports, we annotate cases, conditions, findings, factors and negation\nmodifiers. Moreover, where applicable, we annotate relations between these\nentities. As such, this is the first corpus of this kind made available to the\nscientific community in English. It enables the initial investigation of\nautomatic information extraction from case reports through tasks like Named\nEntity Recognition, Relation Extraction and (sentence/paragraph) relevance\ndetection. Additionally, we present four strong baseline systems for the\ndetection of medical entities made available through the annotated dataset.", "published": "2020-03-29 14:08:43", "link": "http://arxiv.org/abs/2003.13032v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta Fine-Tuning Neural Language Models for Multi-Domain Text Mining", "abstract": "Pre-trained neural language models bring significant improvement for various\nNLP tasks, by fine-tuning the models on task-specific training sets. During\nfine-tuning, the parameters are initialized from pre-trained models directly,\nwhich ignores how the learning process of similar NLP tasks in different\ndomains is correlated and mutually reinforced. In this paper, we propose an\neffective learning procedure named Meta Fine-Tuning (MFT), served as a\nmeta-learner to solve a group of similar NLP tasks for neural language models.\nInstead of simply multi-task training over all the datasets, MFT only learns\nfrom typical instances of various domains to acquire highly transferable\nknowledge. It further encourages the language model to encode domain-invariant\nrepresentations by optimizing a series of novel domain corruption loss\nfunctions. After MFT, the model can be fine-tuned for each domain with better\nparameter initializations and higher generalization ability. We implement MFT\nupon BERT to solve several multi-domain text mining tasks. Experimental results\nconfirm the effectiveness of MFT and its usefulness for few-shot learning.", "published": "2020-03-29 11:27:10", "link": "http://arxiv.org/abs/2003.13003v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Dataset of German Legal Documents for Named Entity Recognition", "abstract": "We describe a dataset developed for Named Entity Recognition in German\nfederal court decisions. It consists of approx. 67,000 sentences with over 2\nmillion tokens. The resource contains 54,000 manually annotated entities,\nmapped to 19 fine-grained semantic classes: person, judge, lawyer, country,\ncity, street, landscape, organization, company, institution, court, brand, law,\nordinance, European legal norm, regulation, contract, court decision, and legal\nliterature. The legal documents were, furthermore, automatically annotated with\nmore than 35,000 TimeML-based time expressions. The dataset, which is available\nunder a CC-BY 4.0 license in the CoNNL-2002 format, was developed for training\nan NER service for German legal documents in the EU project Lynx.", "published": "2020-03-29 13:20:43", "link": "http://arxiv.org/abs/2003.13016v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Abstractive Text Summarization based on Language Model Conditioning and\n  Locality Modeling", "abstract": "We explore to what extent knowledge about the pre-trained language model that\nis used is beneficial for the task of abstractive summarization. To this end,\nwe experiment with conditioning the encoder and decoder of a Transformer-based\nneural model on the BERT language model. In addition, we propose a new method\nof BERT-windowing, which allows chunk-wise processing of texts longer than the\nBERT window size. We also explore how locality modelling, i.e., the explicit\nrestriction of calculations to the local context, can affect the summarization\nability of the Transformer. This is done by introducing 2-dimensional\nconvolutional self-attention into the first layers of the encoder. The results\nof our models are compared to a baseline and the state-of-the-art models on the\nCNN/Daily Mail dataset. We additionally train our model on the SwissText\ndataset to demonstrate usability on German. Both models outperform the baseline\nin ROUGE scores on two datasets and show its superiority in a manual\nqualitative analysis.", "published": "2020-03-29 14:00:17", "link": "http://arxiv.org/abs/2003.13027v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Recursive Non-Autoregressive Graph-to-Graph Transformer for Dependency\n  Parsing with Iterative Refinement", "abstract": "We propose the Recursive Non-autoregressive Graph-to-Graph Transformer\narchitecture (RNGTr) for the iterative refinement of arbitrary graphs through\nthe recursive application of a non-autoregressive Graph-to-Graph Transformer\nand apply it to syntactic dependency parsing. We demonstrate the power and\neffectiveness of RNGTr on several dependency corpora, using a refinement model\npre-trained with BERT. We also introduce Syntactic Transformer (SynTr), a\nnon-recursive parser similar to our refinement model. RNGTr can improve the\naccuracy of a variety of initial parsers on 13 languages from the Universal\nDependencies Treebanks, English and Chinese Penn Treebanks, and the German\nCoNLL2009 corpus, even improving over the new state-of-the-art results achieved\nby SynTr, significantly improving the state-of-the-art for all corpora tested.", "published": "2020-03-29 19:25:32", "link": "http://arxiv.org/abs/2003.13118v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neurals Networks for Projecting Named Entities from English to Ewondo", "abstract": "Named entity recognition is an important task in natural language processing.\nIt is very well studied for rich language, but still under explored for\nlow-resource languages. The main reason is that the existing techniques\nrequired a lot of annotated data to reach good performance. Recently, a new\ndistributional representation of words has been proposed to project named\nentities from a rich language to a low-resource one. This representation has\nbeen coupled to a neural network in order to project named entities from\nEnglish to Ewondo, a Bantu language spoken in Cameroon. Although the proposed\nmethod reached appreciable results, the size of the used neural network was too\nlarge compared to the size of the dataset. Furthermore the impact of the model\nparameters has not been studied. In this paper, we show experimentally that the\nsame results can be obtained using a smaller neural network. We also emphasize\nthe parameters that are highly correlated to the network performance. This work\nis a step forward to build a reliable and robust network architecture for named\nentity projection in low resource languages.", "published": "2020-03-29 22:05:30", "link": "http://arxiv.org/abs/2004.13841v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BERT Fine-tuning For Arabic Text Summarization", "abstract": "Fine-tuning a pretrained BERT model is the state of the art method for\nextractive/abstractive text summarization, in this paper we showcase how this\nfine-tuning method can be applied to the Arabic language to both construct the\nfirst documented model for abstractive Arabic text summarization and show its\nperformance in Arabic extractive summarization. Our model works with\nmultilingual BERT (as Arabic language does not have a pretrained BERT of its\nown). We show its performance in English corpus first before applying it to\nArabic corpora in both extractive and abstractive tasks.", "published": "2020-03-29 20:23:14", "link": "http://arxiv.org/abs/2004.14135v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Clickbait Detection using Multiple Categorization Techniques", "abstract": "Clickbaits are online articles with deliberately designed misleading titles\nfor luring more and more readers to open the intended web page. Clickbaits are\nused to tempted visitors to click on a particular link either to monetize the\nlanding page or to spread the false news for sensationalization. The presence\nof clickbaits on any news aggregator portal may lead to unpleasant experience\nto readers. Automatic detection of clickbait headlines from news headlines has\nbeen a challenging issue for the machine learning community. A lot of methods\nhave been proposed for preventing clickbait articles in recent past. However,\nthe recent techniques available in detecting clickbaits are not much robust.\nThis paper proposes a hybrid categorization technique for separating clickbait\nand non-clickbait articles by integrating different features, sentence\nstructure, and clustering. During preliminary categorization, the headlines are\nseparated using eleven features. After that, the headlines are recategorized\nusing sentence formality, syntactic similarity measures. In the last phase, the\nheadlines are again recategorized by applying clustering using word vector\nsimilarity based on t-Stochastic Neighbourhood Embedding (t-SNE) approach.\nAfter categorization of these headlines, machine learning models are applied to\nthe data set to evaluate machine learning algorithms. The obtained experimental\nresults indicate the proposed hybrid model is more robust, reliable and\nefficient than any individual categorization techniques for the real-world\ndataset we used.", "published": "2020-03-29 07:16:41", "link": "http://arxiv.org/abs/2003.12961v1", "categories": ["cs.SI", "cs.CL", "cs.LG", "H.3.1; H.3.3"], "primary_category": "cs.SI"}
{"title": "A Novel Method of Extracting Topological Features from Word Embeddings", "abstract": "In recent years, topological data analysis has been utilized for a wide range\nof problems to deal with high dimensional noisy data. While text\nrepresentations are often high dimensional and noisy, there are only a few work\non the application of topological data analysis in natural language processing.\nIn this paper, we introduce a novel algorithm to extract topological features\nfrom word embedding representation of text that can be used for text\nclassification. Working on word embeddings, topological data analysis can\ninterpret the embedding high-dimensional space and discover the relations among\ndifferent embedding dimensions. We will use persistent homology, the most\ncommonly tool from topological data analysis, for our experiment. Examining our\ntopological algorithm on long textual documents, we will show our defined\ntopological features may outperform conventional text mining features.", "published": "2020-03-29 16:55:23", "link": "http://arxiv.org/abs/2003.13074v2", "categories": ["cs.LG", "cs.CL", "math.AT", "stat.ML", "68U15, 68T50", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Mechanical classification of voice quality", "abstract": "While there is no a priori definition of good singing voices, we tend to make\nconsistent evaluations of the quality of singing almost instantaneously. Such\nan instantaneous evaluation might be based on the sound spectrum that can be\nperceived in a short time. Here we devise a Bayesian algorithm that learns to\nevaluate the choral proficiency, musical scale, and gender of individual\nsingers using the sound spectra of singing voices. In particular, the\nclassification is performed on a set of sound spectral intensities, whose\nfrequencies are selected by minimizing the Bayes risk. This optimization allows\nthe algorithm to capture sound frequencies that are essential for each\ndiscrimination task, resulting in a good assessment performance. Experimental\nresults revealed that a sound duration of about 0.1 sec is sufficient for\ndetermining the choral proficiency and gender of a singer. With a program\nconstructed on this algorithm, everyone can evaluate choral voices of others\nand perform private vocal exercises.", "published": "2020-03-29 14:09:08", "link": "http://arxiv.org/abs/2003.13033v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Recursive Network with Dynamic Attention for Monaural Speech\n  Enhancement", "abstract": "A person tends to generate dynamic attention towards speech under complicated\nenvironments. Based on this phenomenon, we propose a framework combining\ndynamic attention and recursive learning together for monaural speech\nenhancement. Apart from a major noise reduction network, we design a separated\nsub-network, which adaptively generates the attention distribution to control\nthe information flow throughout the major network. To effectively decrease the\nnumber of trainable parameters, recursive learning is introduced, which means\nthat the network is reused for multiple stages, where the intermediate output\nin each stage is correlated with a memory mechanism. As a result, a more\nflexible and better estimation can be obtained. We conduct experiments on TIMIT\ncorpus. Experimental results show that the proposed architecture obtains\nconsistently better performance than recent state-of-the-art models in terms of\nboth PESQ and STOI scores.", "published": "2020-03-29 08:44:02", "link": "http://arxiv.org/abs/2003.12973v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
