{"title": "Microblog Hashtag Generation via Encoding Conversation Contexts", "abstract": "Automatic hashtag annotation plays an important role in content understanding\nfor microblog posts. To date, progress made in this field has been restricted\nto phrase selection from limited candidates, or word-level hashtag discovery\nusing topic models. Different from previous work considering hashtags to be\ninseparable, our work is the first effort to annotate hashtags with a novel\nsequence generation framework via viewing the hashtag as a short sequence of\nwords. Moreover, to address the data sparsity issue in processing short\nmicroblog posts, we propose to jointly model the target posts and the\nconversation contexts initiated by them with bidirectional attention. Extensive\nexperimental results on two large-scale datasets, newly collected from English\nTwitter and Chinese Weibo, show that our model significantly outperforms\nstate-of-the-art models based on classification. Further studies demonstrate\nour ability to effectively generate rare and even unseen hashtags, which is\nhowever not possible for most existing methods.", "published": "2019-05-18 13:11:31", "link": "http://arxiv.org/abs/1905.07584v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERTSel: Answer Selection with Pre-trained Models", "abstract": "Recently, pre-trained models have been the dominant paradigm in natural\nlanguage processing. They achieved remarkable state-of-the-art performance\nacross a wide range of related tasks, such as textual entailment, natural\nlanguage inference, question answering, etc. BERT, proposed by Devlin et.al.,\nhas achieved a better marked result in GLUE leaderboard with a deep transformer\narchitecture. Despite its soaring popularity, however, BERT has not yet been\napplied to answer selection. This task is different from others with a few\nnuances: first, modeling the relevance and correctness of candidates matters\ncompared to semantic relatedness and syntactic structure; second, the length of\nan answer may be different from other candidates and questions. In this paper.\nwe are the first to explore the performance of fine-tuning BERT for answer\nselection. We achieved STOA results across five popular datasets, demonstrating\nthe success of pre-trained models in this task.", "published": "2019-05-18 13:35:33", "link": "http://arxiv.org/abs/1905.07588v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic flow in language networks", "abstract": "In this study we propose a framework to characterize documents based on their\nsemantic flow. The proposed framework encompasses a network-based model that\nconnected sentences based on their semantic similarity. Semantic fields are\ndetected using standard community detection methods. as the story unfolds,\ntransitions between semantic fields are represent in Markov networks, which in\nturned are characterized via network motifs (subgraphs). Here we show that the\nproposed framework can be used to classify books according to their style and\npublication dates. Remarkably, even without a systematic optimization of\nparameters, philosophy and investigative books were discriminated with an\naccuracy rate of 92.5%. Because this model captures semantic features of texts,\nit could be used as an additional feature in traditional network-based models\nof texts that capture only syntactical/stylistic information, as it is the case\nof word adjacency (co-occurrence) networks.", "published": "2019-05-18 14:32:53", "link": "http://arxiv.org/abs/1905.07595v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-referencing using Fine-grained Topic Modeling", "abstract": "Cross-referencing, which links passages of text to other related passages,\ncan be a valuable study aid for facilitating comprehension of a text. However,\ncross-referencing requires first, a comprehensive thematic knowledge of the\nentire corpus, and second, a focused search through the corpus specifically to\nfind such useful connections. Due to this, cross-reference resources are\nprohibitively expensive and exist only for the most well-studied texts (e.g.\nreligious texts). We develop a topic-based system for automatically producing\ncandidate cross-references which can be easily verified by human annotators.\nOur system utilizes fine-grained topic modeling with thousands of highly\nnuanced and specific topics to identify verse pairs which are topically\nrelated. We demonstrate that our system can be cost effective compared to\nhaving annotators acquire the expertise necessary to produce cross-reference\nresources unaided.", "published": "2019-05-18 00:28:37", "link": "http://arxiv.org/abs/1905.07508v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Human-like machine thinking: Language guided imagination", "abstract": "Human thinking requires the brain to understand the meaning of language\nexpression and to properly organize the thoughts flow using the language.\nHowever, current natural language processing models are primarily limited in\nthe word probability estimation. Here, we proposed a Language guided\nimagination (LGI) network to incrementally learn the meaning and usage of\nnumerous words and syntaxes, aiming to form a human-like machine thinking\nprocess. LGI contains three subsystems: (1) vision system that contains an\nencoder to disentangle the input or imagined scenarios into abstract population\nrepresentations, and an imagination decoder to reconstruct imagined scenario\nfrom higher level representations; (2) Language system, that contains a\nbinarizer to transfer symbol texts into binary vectors, an IPS (mimicking the\nhuman IntraParietal Sulcus, implemented by an LSTM) to extract the quantity\ninformation from the input texts, and a textizer to convert binary vectors into\ntext symbols; (3) a PFC (mimicking the human PreFrontal Cortex, implemented by\nan LSTM) to combine inputs of both language and vision representations, and\npredict text symbols and manipulated images accordingly. LGI has incrementally\nlearned eight different syntaxes (or tasks), with which a machine thinking loop\nhas been formed and validated by the proper interaction between language and\nvision system. The paper provides a new architecture to let the machine learn,\nunderstand and use language in a human-like way that could ultimately enable a\nmachine to construct fictitious 'mental' scenario and possess intelligence.", "published": "2019-05-18 09:23:00", "link": "http://arxiv.org/abs/1905.07562v2", "categories": ["cs.CL", "cs.AI", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Variational Hetero-Encoder Randomized GANs for Joint Image-Text Modeling", "abstract": "For bidirectional joint image-text modeling, we develop variational\nhetero-encoder (VHE) randomized generative adversarial network (GAN), a\nversatile deep generative model that integrates a probabilistic text decoder,\nprobabilistic image encoder, and GAN into a coherent end-to-end multi-modality\nlearning framework. VHE randomized GAN (VHE-GAN) encodes an image to decode its\nassociated text, and feeds the variational posterior as the source of\nrandomness into the GAN image generator. We plug three off-the-shelf modules,\nincluding a deep topic model, a ladder-structured image encoder, and\nStackGAN++, into VHE-GAN, which already achieves competitive performance. This\nfurther motivates the development of VHE-raster-scan-GAN that generates\nphoto-realistic images in not only a multi-scale low-to-high-resolution manner,\nbut also a hierarchical-semantic coarse-to-fine fashion. By capturing and\nrelating hierarchical semantic and visual concepts with end-to-end training,\nVHE-raster-scan-GAN achieves state-of-the-art performance in a wide variety of\nimage-text multi-modality learning and generation tasks.", "published": "2019-05-18 13:58:12", "link": "http://arxiv.org/abs/1905.08622v3", "categories": ["cs.CV", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CV"}
{"title": "Automatic Evaluation of Local Topic Quality", "abstract": "Topic models are typically evaluated with respect to the global topic\ndistributions that they generate, using metrics such as coherence, but without\nregard to local (token-level) topic assignments. Token-level assignments are\nimportant for downstream tasks such as classification. Even recent models,\nwhich aim to improve the quality of these token-level topic assignments, have\nbeen evaluated only with respect to global metrics. We propose a task designed\nto elicit human judgments of token-level topic assignments. We use a variety of\ntopic model types and parameters and discover that global metrics agree poorly\nwith human assignments.\n  Since human evaluation is expensive we propose a variety of automated metrics\nto evaluate topic models at a local level. Finally, we correlate our proposed\nmetrics with human judgments from the task on several datasets. We show that an\nevaluation based on the percent of topic switches correlates most strongly with\nhuman judgment of local topic quality. We suggest that this new metric, which\nwe call consistency, be adopted alongside global metrics such as topic\ncoherence when evaluating new topic models.", "published": "2019-05-18 00:44:47", "link": "http://arxiv.org/abs/1905.13126v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "A Text Classification Framework for Simple and Effective Early\n  Depression Detection Over Social Media Streams", "abstract": "With the rise of the Internet, there is a growing need to build intelligent\nsystems that are capable of efficiently dealing with early risk detection (ERD)\nproblems on social media, such as early depression detection, early rumor\ndetection or identification of sexual predators. These systems, nowadays mostly\nbased on machine learning techniques, must be able to deal with data streams\nsince users provide their data over time. In addition, these systems must be\nable to decide when the processed data is sufficient to actually classify\nusers. Moreover, since ERD tasks involve risky decisions by which people's\nlives could be affected, such systems must also be able to justify their\ndecisions. However, most standard and state-of-the-art supervised machine\nlearning models are not well suited to deal with this scenario. This is due to\nthe fact that they either act as black boxes or do not support incremental\nclassification/learning. In this paper we introduce SS3, a novel supervised\nlearning model for text classification that naturally supports these aspects.\nSS3 was designed to be used as a general framework to deal with ERD problems.\nWe evaluated our model on the CLEF's eRisk2017 pilot task on early depression\ndetection. Most of the 30 contributions submitted to this competition used\nstate-of-the-art methods. Experimental results show that our classifier was\nable to outperform these models and standard classifiers, despite being less\ncomputationally expensive and having the ability to explain its rationale.", "published": "2019-05-18 15:46:38", "link": "http://arxiv.org/abs/1905.08772v2", "categories": ["cs.CY", "cs.CL", "cs.IR", "cs.LG", "cs.SI"], "primary_category": "cs.CY"}
