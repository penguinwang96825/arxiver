{"title": "BCWS: Bilingual Contextual Word Similarity", "abstract": "This paper introduces the first dataset for evaluating English-Chinese\nBilingual Contextual Word Similarity, namely BCWS\n(https://github.com/MiuLab/BCWS). The dataset consists of 2,091 English-Chinese\nword pairs with the corresponding sentential contexts and their similarity\nscores annotated by the human. Our annotated dataset has higher consistency\ncompared to other similar datasets. We establish several baselines for the\nbilingual embedding task to benchmark the experiments. Modeling cross-lingual\nsense representations as provided in this dataset has the potential of moving\nartificial intelligence from monolingual understanding towards multilingual\nunderstanding.", "published": "2018-10-21 13:57:17", "link": "http://arxiv.org/abs/1810.08951v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constituent Parsing as Sequence Labeling", "abstract": "We introduce a method to reduce constituent parsing to sequence labeling. For\neach word w_t, it generates a label that encodes: (1) the number of ancestors\nin the tree that the words w_t and w_{t+1} have in common, and (2) the\nnonterminal symbol at the lowest common ancestor. We first prove that the\nproposed encoding function is injective for any tree without unary branches. In\npractice, the approach is made extensible to all constituency trees by\ncollapsing unary branches. We then use the PTB and CTB treebanks as testbeds\nand propose a set of fast baselines. We achieve 90.7% F-score on the PTB test\nset, outperforming the Vinyals et al. (2015) sequence-to-sequence parser. In\naddition, sacrificing some accuracy, our approach achieves the fastest\nconstituent parsing speeds reported to date on PTB by a wide margin.", "published": "2018-10-21 17:59:52", "link": "http://arxiv.org/abs/1810.08994v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transition-based Parsing with Lighter Feed-Forward Networks", "abstract": "We explore whether it is possible to build lighter parsers, that are\nstatistically equivalent to their corresponding standard version, for a wide\nset of languages showing different structures and morphologies. As testbed, we\nuse the Universal Dependencies and transition-based dependency parsers trained\non feed-forward networks. For these, most existing research assumes de facto\nstandard embedded features and relies on pre-computation tricks to obtain\nspeed-ups. We explore how these features and their size can be reduced and\nwhether this translates into speed-ups with a negligible impact on accuracy.\nThe experiments show that grand-daughter features can be removed for the\nmajority of treebanks without a significant (negative or positive) LAS\ndifference. They also show how the size of the embeddings can be notably\nreduced.", "published": "2018-10-21 18:17:37", "link": "http://arxiv.org/abs/1810.08997v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
