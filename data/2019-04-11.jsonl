{"title": "Modeling Global Syntactic Variation in English Using Dialect\n  Classification", "abstract": "This paper evaluates global-scale dialect identification for 14 national\nvarieties of English as a means for studying syntactic variation. The paper\nmakes three main contributions: (i) introducing data-driven language mapping as\na method for selecting the inventory of national varieties to include in the\ntask; (ii) producing a large and dynamic set of syntactic features using\ngrammar induction rather than focusing on a few hand-selected features such as\nfunction words; and (iii) comparing models across both web corpora and social\nmedia corpora in order to measure the robustness of syntactic variation across\nregisters.", "published": "2019-04-11 04:38:35", "link": "http://arxiv.org/abs/1904.05527v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Frequency vs. Association for Constraint Selection in Usage-Based\n  Construction Grammar", "abstract": "A usage-based Construction Grammar (CxG) posits that slot-constraints\ngeneralize from common exemplar constructions. But what is the best model of\nconstraint generalization? This paper evaluates competing frequency-based and\nassociation-based models across eight languages using a metric derived from the\nMinimum Description Length paradigm. The experiments show that\nassociation-based models produce better generalizations across all languages by\na significant margin.", "published": "2019-04-11 04:41:46", "link": "http://arxiv.org/abs/1904.05529v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scalable Cross-Lingual Transfer of Neural Sentence Embeddings", "abstract": "We develop and investigate several cross-lingual alignment approaches for\nneural sentence embedding models, such as the supervised inference classifier,\nInferSent, and sequential encoder-decoder models. We evaluate three alignment\nframeworks applied to these models: joint modeling, representation transfer\nlearning, and sentence mapping, using parallel text to guide the alignment. Our\nresults support representation transfer as a scalable approach for modular\ncross-lingual alignment of neural sentence embeddings, where we observe better\nperformance compared to joint models in intrinsic and extrinsic evaluations,\nparticularly with smaller sets of parallel data.", "published": "2019-04-11 05:41:27", "link": "http://arxiv.org/abs/1904.05542v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Searching News Articles Using an Event Knowledge Graph Leveraged by\n  Wikidata", "abstract": "News agencies produce thousands of multimedia stories describing events\nhappening in the world that are either scheduled such as sports competitions,\npolitical summits and elections, or breaking events such as military conflicts,\nterrorist attacks, natural disasters, etc. When writing up those stories,\njournalists refer to contextual background and to compare with past similar\nevents. However, searching for precise facts described in stories is hard. In\nthis paper, we propose a general method that leverages the Wikidata knowledge\nbase to produce semantic annotations of news articles. Next, we describe a\nsemantic search engine that supports both keyword based search in news articles\nand structured data search providing filters for properties belonging to\nspecific event schemas that are automatically inferred.", "published": "2019-04-11 07:08:09", "link": "http://arxiv.org/abs/1904.05557v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A high quality and phonetic balanced speech corpus for Vietnamese", "abstract": "This paper presents a high quality Vietnamese speech corpus that can be used\nfor analyzing Vietnamese speech characteristic as well as building speech\nsynthesis models. The corpus consists of 5400 clean-speech utterances spoken by\n12 speakers including 6 males and 6 females. The corpus is designed with\nphonetic balanced in mind so that it can be used for speech synthesis,\nespecially, speech adaptation approaches. Specifically, all speakers utter a\ncommon dataset contains 250 phonetic balanced sentences. To increase the\nvariety of speech context, each speaker also utters another 200 non-shared,\nphonetic-balanced sentences. The speakers are selected to cover a wide range of\nage and come from different regions of the North of Vietnam. The audios are\nrecorded in a soundproof studio room, they are sampling at 48 kHz, 16 bits PCM,\nmono channel.", "published": "2019-04-11 07:57:51", "link": "http://arxiv.org/abs/1904.05569v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling the Complexity and Descriptive Adequacy of Construction\n  Grammars", "abstract": "This paper uses the Minimum Description Length paradigm to model the\ncomplexity of CxGs (operationalized as the encoding size of a grammar)\nalongside their descriptive adequacy (operationalized as the encoding size of a\ncorpus given a grammar). These two quantities are combined to measure the\nquality of potential CxGs against unannotated corpora, supporting\ndiscovery-device CxGs for English, Spanish, French, German, and Italian. The\nresults show (i) that these grammars provide significant generalizations as\nmeasured using compression and (ii) that more complex CxGs with access to\nmultiple levels of representation provide greater generalizations than\nsingle-representation CxGs.", "published": "2019-04-11 09:06:24", "link": "http://arxiv.org/abs/1904.05588v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "wav2vec: Unsupervised Pre-training for Speech Recognition", "abstract": "We explore unsupervised pre-training for speech recognition by learning\nrepresentations of raw audio. wav2vec is trained on large amounts of unlabeled\naudio data and the resulting representations are then used to improve acoustic\nmodel training. We pre-train a simple multi-layer convolutional neural network\noptimized via a noise contrastive binary classification task. Our experiments\non WSJ reduce WER of a strong character-based log-mel filterbank baseline by up\nto 36% when only a few hours of transcribed data is available. Our approach\nachieves 2.43% WER on the nov92 test set. This outperforms Deep Speech 2, the\nbest reported character-based system in the literature while using two orders\nof magnitude less labeled training data.", "published": "2019-04-11 17:29:30", "link": "http://arxiv.org/abs/1904.05862v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Crowdsourcing Lightweight Pyramids for Manual Summary Evaluation", "abstract": "Conducting a manual evaluation is considered an essential part of summary\nevaluation methodology. Traditionally, the Pyramid protocol, which exhaustively\ncompares system summaries to references, has been perceived as very reliable,\nproviding objective scores. Yet, due to the high cost of the Pyramid method and\nthe required expertise, researchers resorted to cheaper and less thorough\nmanual evaluation methods, such as Responsiveness and pairwise comparison,\nattainable via crowdsourcing. We revisit the Pyramid approach, proposing a\nlightweight sampling-based version that is crowdsourcable. We analyze the\nperformance of our method in comparison to original expert-based Pyramid\nevaluations, showing higher correlation relative to the common Responsiveness\nmethod. We release our crowdsourced Summary-Content-Units, along with all\ncrowdsourcing scripts, for future evaluations.", "published": "2019-04-11 19:04:57", "link": "http://arxiv.org/abs/1904.05929v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Strong Baselines for Complex Word Identification across Multiple\n  Languages", "abstract": "Complex Word Identification (CWI) is the task of identifying which words or\nphrases in a sentence are difficult to understand by a target audience. The\nlatest CWI Shared Task released data for two settings: monolingual (i.e. train\nand test in the same language) and cross-lingual (i.e. test in a language not\nseen during training). The best monolingual models relied on language-dependent\nfeatures, which do not generalise in the cross-lingual setting, while the best\ncross-lingual model used neural networks with multi-task learning. In this\npaper, we present monolingual and cross-lingual CWI models that perform as well\nas (or better than) most models submitted to the latest CWI Shared Task. We\nshow that carefully selected features and simple learning models can achieve\nstate-of-the-art performance, and result in strong baselines for future\ndevelopment in this area. Finally, we discuss how inconsistencies in the\nannotation of the data can explain some of the results obtained.", "published": "2019-04-11 20:50:18", "link": "http://arxiv.org/abs/1904.05953v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gating Mechanisms for Combining Character and Word-level Word\n  Representations: An Empirical Study", "abstract": "In this paper we study how different ways of combining character and\nword-level representations affect the quality of both final word and sentence\nrepresentations. We provide strong empirical evidence that modeling characters\nimproves the learned representations at the word and sentence levels, and that\ndoing so is particularly useful when representing less frequent words. We\nfurther show that a feature-wise sigmoid gating mechanism is a robust method\nfor creating representations that encode semantic similarity, as it performed\nreasonably well in several word similarity datasets. Finally, our findings\nsuggest that properly capturing semantic similarity at the word level does not\nconsistently yield improved performance in downstream sentence-level tasks. Our\ncode is available at https://github.com/jabalazs/gating", "published": "2019-04-11 08:56:48", "link": "http://arxiv.org/abs/1904.05584v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Political Discussions in Homogeneous and Cross-Cutting Communication\n  Spaces", "abstract": "Online platforms, such as Facebook, Twitter, and Reddit, provide users with a\nrich set of features for sharing and consuming political information,\nexpressing political opinions, and exchanging potentially contrary political\nviews. In such activities, two types of communication spaces naturally emerge:\nthose dominated by exchanges between politically homogeneous users and those\nthat allow and encourage cross-cutting exchanges in politically heterogeneous\ngroups. While research on political talk in online environments abounds, we\nknow surprisingly little about the potentially varying nature of discussions in\npolitically homogeneous spaces as compared to cross-cutting communication\nspaces. To fill this gap, we use Reddit to explore the nature of political\ndiscussions in homogeneous and cross-cutting communication spaces. In\nparticular, we develop an analytical template to study interaction and\nlinguistic patterns within and between politically homogeneous and\nheterogeneous communication spaces. Our analyses reveal different behavioral\npatterns in homogeneous and cross-cutting communications spaces. We discuss\ntheoretical and practical implications in the context of research on political\ntalk online.", "published": "2019-04-11 11:46:07", "link": "http://arxiv.org/abs/1904.05643v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Cross-topic distributional semantic representations via unsupervised\n  mappings", "abstract": "In traditional Distributional Semantic Models (DSMs) the multiple senses of a\npolysemous word are conflated into a single vector space representation. In\nthis work, we propose a DSM that learns multiple distributional representations\nof a word based on different topics. First, a separate DSM is trained for each\ntopic and then each of the topic-based DSMs is aligned to a common vector\nspace. Our unsupervised mapping approach is motivated by the hypothesis that\nwords preserving their relative distances in different topic semantic\nsub-spaces constitute robust \\textit{semantic anchors} that define the mappings\nbetween them. Aligned cross-topic representations achieve state-of-the-art\nresults for the task of contextual word similarity. Furthermore, evaluation on\nNLP downstream tasks shows that multiple topic-based embeddings outperform\nsingle-prototype models.", "published": "2019-04-11 13:09:57", "link": "http://arxiv.org/abs/1904.05674v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adapting RNN Sequence Prediction Model to Multi-label Set Prediction", "abstract": "We present an adaptation of RNN sequence models to the problem of multi-label\nclassification for text, where the target is a set of labels, not a sequence.\nPrevious such RNN models define probabilities for sequences but not for sets;\nattempts to obtain a set probability are after-thoughts of the network design,\nincluding pre-specifying the label order, or relating the sequence probability\nto the set probability in ad hoc ways.\n  Our formulation is derived from a principled notion of set probability, as\nthe sum of probabilities of corresponding permutation sequences for the set. We\nprovide a new training objective that maximizes this set probability, and a new\nprediction objective that finds the most probable set on a test document. These\nnew objectives are theoretically appealing because they give the RNN model\nfreedom to discover the best label order, which often is the natural one (but\ndifferent among documents).\n  We develop efficient procedures to tackle the computation difficulties\ninvolved in training and prediction. Experiments on benchmark datasets\ndemonstrate that we outperform state-of-the-art methods for this task.", "published": "2019-04-11 16:33:54", "link": "http://arxiv.org/abs/1904.05829v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Text2Node: a Cross-Domain System for Mapping Arbitrary Phrases to a\n  Taxonomy", "abstract": "Electronic health record (EHR) systems are used extensively throughout the\nhealthcare domain. However, data interchangeability between EHR systems is\nlimited due to the use of different coding standards across systems. Existing\nmethods of mapping coding standards based on manual human experts mapping,\ndictionary mapping, symbolic NLP and classification are unscalable and cannot\naccommodate large scale EHR datasets.\n  In this work, we present Text2Node, a cross-domain mapping system capable of\nmapping medical phrases to concepts in a large taxonomy (such as SNOMED CT).\nThe system is designed to generalize from a limited set of training samples and\nmap phrases to elements of the taxonomy that are not covered by training data.\nAs a result, our system is scalable, robust to wording variants between coding\nsystems and can output highly relevant concepts when no exact concept exists in\nthe target taxonomy. Text2Node operates in three main stages: first, the\nlexicon is mapped to word embeddings; second, the taxonomy is vectorized using\nnode embeddings; and finally, the mapping function is trained to connect the\ntwo embedding spaces. We compared multiple algorithms and architectures for\neach stage of the training, including GloVe and FastText word embeddings, CNN\nand Bi-LSTM mapping functions, and node2vec for node embeddings. We confirmed\nthe robustness and generalisation properties of Text2Node by mapping ICD-9-CM\nDiagnosis phrases to SNOMED CT and by zero-shot training at comparable\naccuracy.\n  This system is a novel methodological contribution to the task of normalizing\nand linking phrases to a taxonomy, advancing data interchangeability in\nhealthcare. When applied, the system can use electronic health records to\ngenerate an embedding that incorporates taxonomical medical knowledge to\nimprove clinical predictive models.", "published": "2019-04-11 17:31:23", "link": "http://arxiv.org/abs/1905.01958v1", "categories": ["cs.CL", "cs.LG", "stat.ML", "I.2.4; I.2.7; I.2.6; I.7.0"], "primary_category": "cs.CL"}
{"title": "Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data\n  In Your Machine Translation System?", "abstract": "Data privacy is an important issue for \"machine learning as a service\"\nproviders. We focus on the problem of membership inference attacks: given a\ndata sample and black-box access to a model's API, determine whether the sample\nexisted in the model's training data. Our contribution is an investigation of\nthis problem in the context of sequence-to-sequence models, which are important\nin applications such as machine translation and video captioning. We define the\nmembership inference problem for sequence generation, provide an open dataset\nbased on state-of-the-art machine translation models, and report initial\nresults on whether these models leak private information against several kinds\nof membership inference attacks.", "published": "2019-04-11 02:53:21", "link": "http://arxiv.org/abs/1904.05506v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "UniVSE: Robust Visual Semantic Embeddings via Structured Semantic\n  Representations", "abstract": "We propose Unified Visual-Semantic Embeddings (UniVSE) for learning a joint\nspace of visual and textual concepts. The space unifies the concepts at\ndifferent levels, including objects, attributes, relations, and full scenes. A\ncontrastive learning approach is proposed for the fine-grained alignment from\nonly image-caption pairs. Moreover, we present an effective approach for\nenforcing the coverage of semantic components that appear in the sentence. We\ndemonstrate the robustness of Unified VSE in defending text-domain adversarial\nattacks on cross-modal retrieval tasks. Such robustness also empowers the use\nof visual cues to resolve word dependencies in novel sentences.", "published": "2019-04-11 04:04:06", "link": "http://arxiv.org/abs/1904.05521v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Recurrent Event Network: Autoregressive Structure Inference over\n  Temporal Knowledge Graphs", "abstract": "Knowledge graph reasoning is a critical task in natural language processing.\nThe task becomes more challenging on temporal knowledge graphs, where each fact\nis associated with a timestamp. Most existing methods focus on reasoning at\npast timestamps and they are not able to predict facts happening in the future.\nThis paper proposes Recurrent Event Network (RE-NET), a novel autoregressive\narchitecture for predicting future interactions. The occurrence of a fact\n(event) is modeled as a probability distribution conditioned on temporal\nsequences of past knowledge graphs. Specifically, our RE-NET employs a\nrecurrent event encoder to encode past facts and uses a neighborhood aggregator\nto model the connection of facts at the same timestamp. Future facts can then\nbe inferred in a sequential manner based on the two modules. We evaluate our\nproposed method via link prediction at future times on five public datasets.\nThrough extensive experiments, we demonstrate the strength of RENET, especially\non multi-step inference over future timestamps, and achieve state-of-the-art\nperformance on all five datasets. Code and data can be found at\nhttps://github.com/INK-USC/RE-Net.", "published": "2019-04-11 04:45:42", "link": "http://arxiv.org/abs/1904.05530v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "FrameRank: A Text Processing Approach to Video Summarization", "abstract": "Video summarization has been extensively studied in the past decades.\nHowever, user-generated video summarization is much less explored since there\nlack large-scale video datasets within which human-generated video summaries\nare unambiguously defined and annotated. Toward this end, we propose a\nuser-generated video summarization dataset - UGSum52 - that consists of 52\nvideos (207 minutes). In constructing the dataset, because of the subjectivity\nof user-generated video summarization, we manually annotate 25 summaries for\neach video, which are in total 1300 summaries. To the best of our knowledge, it\nis currently the largest dataset for user-generated video summarization.\n  Based on this dataset, we present FrameRank, an unsupervised video\nsummarization method that employs a frame-to-frame level affinity graph to\nidentify coherent and informative frames to summarize a video. We use the\nKullback-Leibler(KL)-divergence-based graph to rank temporal segments according\nto the amount of semantic information contained in their frames. We illustrate\nthe effectiveness of our method by applying it to three datasets SumMe, TVSum\nand UGSum52 and show it achieves state-of-the-art results.", "published": "2019-04-11 06:16:17", "link": "http://arxiv.org/abs/1904.05544v2", "categories": ["cs.CL", "cs.CV", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Reasoning Visual Dialogs with Structural and Partial Observations", "abstract": "We propose a novel model to address the task of Visual Dialog which exhibits\ncomplex dialog structures. To obtain a reasonable answer based on the current\nquestion and the dialog history, the underlying semantic dependencies between\ndialog entities are essential. In this paper, we explicitly formalize this task\nas inference in a graphical model with partially observed nodes and unknown\ngraph structures (relations in dialog). The given dialog entities are viewed as\nthe observed nodes. The answer to a given question is represented by a node\nwith missing value. We first introduce an Expectation Maximization algorithm to\ninfer both the underlying dialog structures and the missing node values\n(desired answers). Based on this, we proceed to propose a differentiable graph\nneural network (GNN) solution that approximates this process. Experiment\nresults on the VisDial and VisDial-Q datasets show that our model outperforms\ncomparative methods. It is also observed that our method can infer the\nunderlying dialog structure for better dialog reasoning.", "published": "2019-04-11 06:46:15", "link": "http://arxiv.org/abs/1904.05548v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Multi-lingual Dialogue Act Recognition with Deep Learning Methods", "abstract": "This paper deals with multi-lingual dialogue act (DA) recognition. The\nproposed approaches are based on deep neural networks and use word2vec\nembeddings for word representation. Two multi-lingual models are proposed for\nthis task. The first approach uses one general model trained on the embeddings\nfrom all available languages. The second method trains the model on a single\npivot language and a linear transformation method is used to project other\nlanguages onto the pivot language. The popular convolutional neural network and\nLSTM architectures with different set-ups are used as classifiers. To the best\nof our knowledge this is the first attempt at multi-lingual DA recognition\nusing neural networks. The multi-lingual models are validated experimentally on\ntwo languages from the Verbmobil corpus.", "published": "2019-04-11 09:55:41", "link": "http://arxiv.org/abs/1904.05606v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of Spatial Attention Mechanisms in Deep Networks", "abstract": "Attention mechanisms have become a popular component in deep neural networks,\nyet there has been little examination of how different influencing factors and\nmethods for computing attention from these factors affect performance. Toward a\nbetter general understanding of attention mechanisms, we present an empirical\nstudy that ablates various spatial attention elements within a generalized\nattention formulation, encompassing the dominant Transformer attention as well\nas the prevalent deformable convolution and dynamic convolution modules.\nConducted on a variety of applications, the study yields significant findings\nabout spatial attention in deep networks, some of which run counter to\nconventional understanding. For example, we find that the query and key content\ncomparison in Transformer attention is negligible for self-attention, but vital\nfor encoder-decoder attention. A proper combination of deformable convolution\nwith key content only saliency achieves the best accuracy-efficiency tradeoff\nin self-attention. Our results suggest that there exists much room for\nimprovement in the design of attention mechanisms.", "published": "2019-04-11 17:58:37", "link": "http://arxiv.org/abs/1904.05873v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Factor Graph Attention", "abstract": "Dialog is an effective way to exchange information, but subtle details and\nnuances are extremely important. While significant progress has paved a path to\naddress visual dialog with algorithms, details and nuances remain a challenge.\nAttention mechanisms have demonstrated compelling results to extract details in\nvisual question answering and also provide a convincing framework for visual\ndialog due to their interpretability and effectiveness. However, the many data\nutilities that accompany visual dialog challenge existing attention techniques.\nWe address this issue and develop a general attention mechanism for visual\ndialog which operates on any number of data utilities. To this end, we design a\nfactor graph based attention mechanism which combines any number of utility\nrepresentations. We illustrate the applicability of the proposed approach on\nthe challenging and recently introduced VisDial datasets, outperforming recent\nstate-of-the-art methods by 1.1% for VisDial0.9 and by 2% for VisDial1.0 on\nMRR. Our ensemble model improved the MRR score on VisDial1.0 by more than 6%.", "published": "2019-04-11 17:59:58", "link": "http://arxiv.org/abs/1904.05880v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "STC Antispoofing Systems for the ASVspoof2019 Challenge", "abstract": "This paper describes the Speech Technology Center (STC) antispoofing systems\nsubmitted to the ASVspoof 2019 challenge. The ASVspoof2019 is the extended\nversion of the previous challenges and includes 2 evaluation conditions:\nlogical access use-case scenario with speech synthesis and voice conversion\nattack types and physical access use-case scenario with replay attacks. During\nthe challenge we developed anti-spoofing solutions for both scenarios. The\nproposed systems are implemented using deep learning approach and are based on\ndifferent types of acoustic features. We enhanced Light CNN architecture\npreviously considered by the authors for replay attacks detection and which\nperformed high spoofing detection quality during the ASVspoof2017 challenge. In\nparticular here we investigate the efficiency of angular margin based softmax\nactivation for training robust deep Light CNN classifier to solve the\nmentioned-above tasks. Submitted systems achieved EER of 1.86% in logical\naccess scenario and 0.54% in physical access scenario on the evaluation part of\nthe Challenge corpora. High performance obtained for the unknown types of\nspoofing attacks demonstrates the stability of the offered approach in both\nevaluation conditions.", "published": "2019-04-11 08:37:43", "link": "http://arxiv.org/abs/1904.05576v1", "categories": ["cs.SD", "cs.CL", "cs.CR", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "A Simple Baseline for Audio-Visual Scene-Aware Dialog", "abstract": "The recently proposed audio-visual scene-aware dialog task paves the way to a\nmore data-driven way of learning virtual assistants, smart speakers and car\nnavigation systems. However, very little is known to date about how to\neffectively extract meaningful information from a plethora of sensors that\npound the computational engine of those devices. Therefore, in this paper, we\nprovide and carefully analyze a simple baseline for audio-visual scene-aware\ndialog which is trained end-to-end. Our method differentiates in a data-driven\nmanner useful signals from distracting ones using an attention mechanism. We\nevaluate the proposed approach on the recently introduced and challenging\naudio-visual scene-aware dataset, and demonstrate the key features that permit\nto outperform the current state-of-the-art by more than 20\\% on CIDEr.", "published": "2019-04-11 17:59:51", "link": "http://arxiv.org/abs/1904.05876v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Cross-task learning for audio tagging, sound event detection spatial\n  localization: DCASE 2019 baseline systems", "abstract": "The Detection and Classification of Acoustic Scenes and Events (DCASE) 2019\nchallenge focuses on audio tagging, sound event detection and spatial\nlocalisation. DCASE 2019 consists of five tasks: 1) acoustic scene\nclassification, 2) audio tagging with noisy labels and minimal supervision, 3)\nsound event localisation and detection, 4) sound event detection in domestic\nenvironments, and 5) urban sound tagging. In this paper, we propose generic\ncross-task baseline systems based on convolutional neural networks (CNNs). The\nmotivation is to investigate the performance of a variety of models across\nseveral tasks without exploiting the specific characteristics of the tasks. We\nlooked at CNNs with 5, 9, and 13 layers, and found that the optimal\narchitecture is task-dependent. For the systems we considered, we found that\nthe 9-layer CNN with average pooling is a good model for a majority of the\nDCASE 2019 tasks.", "published": "2019-04-11 11:28:18", "link": "http://arxiv.org/abs/1904.05635v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Sound of Motions", "abstract": "Sounds originate from object motions and vibrations of surrounding air.\nInspired by the fact that humans is capable of interpreting sound sources from\nhow objects move visually, we propose a novel system that explicitly captures\nsuch motion cues for the task of sound localization and separation. Our system\nis composed of an end-to-end learnable model called Deep Dense Trajectory\n(DDT), and a curriculum learning scheme. It exploits the inherent coherence of\naudio-visual signals from a large quantities of unlabeled videos. Quantitative\nand qualitative evaluations show that comparing to previous models that rely on\nvisual appearance cues, our motion based system improves performance in\nseparating musical instrument sounds. Furthermore, it separates sound\ncomponents from duets of the same category of instruments, a challenging\nproblem that has not been addressed before.", "published": "2019-04-11 23:05:52", "link": "http://arxiv.org/abs/1904.05979v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
