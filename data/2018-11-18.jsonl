{"title": "Neural Multi-Task Learning for Citation Function and Provenance", "abstract": "Citation function and provenance are two cornerstone tasks in citation\nanalysis. Given a citation, the former task determines its rhetorical role,\nwhile the latter locates the text in the cited paper that contains the relevant\ncited information. We hypothesize that these two tasks are synergistically\nrelated, and build a model that validates this claim. For both tasks, we show\nthat a single-layer convolutional neural network (CNN) outperforms existing\nstate-of-the-art baselines. More importantly, we show that the two tasks are\nindeed synergistic: by jointly training both of the tasks in a multi-task\nlearning setup, we demonstrate additional performance gains. Altogether, our\nmodels improve the current state-of-the-arts up to 2\\%, with statistical\nsignificance for both citation function and provenance prediction tasks.", "published": "2018-11-18 16:55:25", "link": "http://arxiv.org/abs/1811.07351v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantifying Uncertainties in Natural Language Processing Tasks", "abstract": "Reliable uncertainty quantification is a first step towards building\nexplainable, transparent, and accountable artificial intelligent systems.\nRecent progress in Bayesian deep learning has made such quantification\nrealizable. In this paper, we propose novel methods to study the benefits of\ncharacterizing model and data uncertainties for natural language processing\n(NLP) tasks. With empirical experiments on sentiment analysis, named entity\nrecognition, and language modeling using convolutional and recurrent neural\nnetwork models, we show that explicitly modeling uncertainties is not only\nnecessary to measure output confidence levels, but also useful at enhancing\nmodel performances in various NLP tasks.", "published": "2018-11-18 01:36:05", "link": "http://arxiv.org/abs/1811.07253v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Harmonic Recomposition using Conditional Autoregressive Modeling", "abstract": "We demonstrate a conditional autoregressive pipeline for efficient music\nrecomposition, based on methods presented in van den Oord et al.(2017).\nRecomposition (Casal & Casey, 2010) focuses on reworking existing musical\npieces, adhering to structure at a high level while also re-imagining other\naspects of the work. This can involve reuse of pre-existing themes or parts of\nthe original piece, while also requiring the flexibility to generate new\ncontent at different levels of granularity. Applying the aforementioned\nmodeling pipeline to recomposition, we show diverse and structured generation\nconditioned on chord sequence annotations.", "published": "2018-11-18 23:40:53", "link": "http://arxiv.org/abs/1811.07426v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
