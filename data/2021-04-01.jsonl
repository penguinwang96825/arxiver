{"title": "Integrating Subgraph-aware Relation and DirectionReasoning for Question\n  Answering", "abstract": "Question Answering (QA) models over Knowledge Bases (KBs) are capable of\nproviding more precise answers by utilizing relation information among\nentities. Although effective, most of these models solely rely on fixed\nrelation representations to obtain answers for different question-related KB\nsubgraphs. Hence, the rich structured information of these subgraphs may be\noverlooked by the relation representation vectors. Meanwhile, the direction\ninformation of reasoning, which has been proven effective for the answer\nprediction on graphs, has not been fully explored in existing work. To address\nthese challenges, we propose a novel neural model, Relation-updated\nDirection-guided Answer Selector (RDAS), which converts relations in each\nsubgraph to additional nodes to learn structure information. Additionally, we\nutilize direction information to enhance the reasoning ability. Experimental\nresults show that our model yields substantial improvements on two widely used\ndatasets.", "published": "2021-04-01 03:04:36", "link": "http://arxiv.org/abs/2104.00218v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Neural Word Embeddings for Sanskrit", "abstract": "Recently, the supervised learning paradigm's surprisingly remarkable\nperformance has garnered considerable attention from Sanskrit Computational\nLinguists. As a result, the Sanskrit community has put laudable efforts to\nbuild task-specific labeled data for various downstream Natural Language\nProcessing (NLP) tasks. The primary component of these approaches comes from\nrepresentations of word embeddings. Word embedding helps to transfer knowledge\nlearned from readily available unlabelled data for improving task-specific\nperformance in low-resource setting. Last decade, there has been much\nexcitement in the field of digitization of Sanskrit. To effectively use such\nreadily available resources, it is very much essential to perform a systematic\nstudy on word embedding approaches for the Sanskrit language. In this work, we\ninvestigate the effectiveness of word embeddings. We classify word embeddings\nin broad categories to facilitate systematic experimentation and evaluate them\non four intrinsic tasks. We investigate the efficacy of embeddings approaches\n(originally proposed for languages other than Sanskrit) for Sanskrit along with\nvarious challenges posed by language.", "published": "2021-04-01 06:08:21", "link": "http://arxiv.org/abs/2104.00270v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FeTaQA: Free-form Table Question Answering", "abstract": "Existing table question answering datasets contain abundant factual questions\nthat primarily evaluate the query and schema comprehension capability of a\nsystem, but they fail to include questions that require complex reasoning and\nintegration of information due to the constraint of the associated short-form\nanswers. To address these issues and to demonstrate the full challenge of table\nquestion answering, we introduce FeTaQA, a new dataset with 10K Wikipedia-based\n{table, question, free-form answer, supporting table cells} pairs. FeTaQA\nyields a more challenging table question answering setting because it requires\ngenerating free-form text answers after retrieval, inference, and integration\nof multiple discontinuous facts from a structured knowledge source. Unlike\ndatasets of generative QA over text in which answers are prevalent with copies\nof short text spans from the source, answers in our dataset are human-generated\nexplanations involving entities and their high-level relations. We provide two\nbenchmark methods for the proposed task: a pipeline method based on\nsemantic-parsing-based QA systems and an end-to-end method based on large\npretrained text generation models, and show that FeTaQA poses a challenge for\nboth methods.", "published": "2021-04-01 09:59:40", "link": "http://arxiv.org/abs/2104.00369v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "High-dimensional distributed semantic spaces for utterances", "abstract": "High-dimensional distributed semantic spaces have proven useful and effective\nfor aggregating and processing visual, auditory, and lexical information for\nmany tasks related to human-generated data. Human language makes use of a large\nand varying number of features, lexical and constructional items as well as\ncontextual and discourse-specific data of various types, which all interact to\nrepresent various aspects of communicative information. Some of these features\nare mostly local and useful for the organisation of e.g. argument structure of\na predication; others are persistent over the course of a discourse and\nnecessary for achieving a reasonable level of understanding of the content.\nThis paper describes a model for high-dimensional representation for utterance\nand text level data including features such as constructions or contextual\ndata, based on a mathematically principled and behaviourally plausible approach\nto representing linguistic information. The implementation of the\nrepresentation is a straightforward extension of Random Indexing models\npreviously used for lexical linguistic items. The paper shows how the\nimplemented model is able to represent a broad range of linguistic features in\na common integral framework of fixed dimensionality, which is computationally\nhabitable, and which is suitable as a bridge between symbolic representations\nsuch as dependency analysis and continuous representations used e.g. in\nclassifiers or further machine-learning approaches. This is achieved with\noperations on vectors that constitute a powerful computational algebra,\naccompanied with an associative memory for the vectors. The paper provides a\ntechnical overview of the framework and a worked through implemented example of\nhow it can be applied to various types of linguistic features.", "published": "2021-04-01 12:09:47", "link": "http://arxiv.org/abs/2104.00424v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recognizing and Splitting Conditional Sentences for Automation of\n  Business Processes Management", "abstract": "Business Process Management (BPM) is the discipline which is responsible for\nmanagement of discovering, analyzing, redesigning, monitoring, and controlling\nbusiness processes. One of the most crucial tasks of BPM is discovering and\nmodelling business processes from text documents. In this paper, we present our\nsystem that resolves an end-to-end problem consisting of 1) recognizing\nconditional sentences from technical documents, 2) finding boundaries to\nextract conditional and resultant clauses from each conditional sentence, and\n3) categorizing resultant clause as Action or Consequence which later helps to\ngenerate new steps in our business process model automatically. We created a\nnew dataset and three models solve this problem. Our best model achieved very\npromising results of 83.82, 87.84, and 85.75 for Precision, Recall, and F1,\nrespectively, for extracting Condition, Action, and Consequence clauses using\nExact Match metric.", "published": "2021-04-01 17:53:16", "link": "http://arxiv.org/abs/2104.00660v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SYSML: StYlometry with Structure and Multitask Learning: Implications\n  for Darknet Forum Migrant Analysis", "abstract": "Darknet market forums are frequently used to exchange illegal goods and\nservices between parties who use encryption to conceal their identities. The\nTor network is used to host these markets, which guarantees additional\nanonymization from IP and location tracking, making it challenging to link\nacross malicious users using multiple accounts (sybils). Additionally, users\nmigrate to new forums when one is closed, making it difficult to link users\nacross multiple forums. We develop a novel stylometry-based multitask learning\napproach for natural language and interaction modeling using graph embeddings\nto construct low-dimensional representations of short episodes of user activity\nfor authorship attribution. We provide a comprehensive evaluation of our\nmethods across four different darknet forums demonstrating its efficacy over\nthe state-of-the-art, with a lift of up to 2.5X on Mean Retrieval Rank and 2X\non Recall@10.", "published": "2021-04-01 20:59:02", "link": "http://arxiv.org/abs/2104.00764v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Configurable Privacy-Preserving Automatic Speech Recognition", "abstract": "Voice assistive technologies have given rise to far-reaching privacy and\nsecurity concerns. In this paper we investigate whether modular automatic\nspeech recognition (ASR) can improve privacy in voice assistive systems by\ncombining independently trained separation, recognition, and discretization\nmodules to design configurable privacy-preserving ASR systems. We evaluate\nprivacy concerns and the effects of applying various state-of-the-art\ntechniques at each stage of the system, and report results using task-specific\nmetrics (i.e. WER, ABX, and accuracy). We show that overlapping speech inputs\nto ASR systems present further privacy concerns, and how these may be mitigated\nusing speech separation and optimization techniques. Our discretization module\nis shown to minimize paralinguistics privacy leakage from ASR acoustic models\nto levels commensurate with random guessing. We show that voice privacy can be\nconfigurable, and argue this presents new opportunities for privacy-preserving\napplications incorporating ASR.", "published": "2021-04-01 21:03:49", "link": "http://arxiv.org/abs/2104.00766v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Canonical and Surface Morphological Segmentation for Nguni Languages", "abstract": "Morphological Segmentation involves decomposing words into morphemes, the\nsmallest meaning-bearing units of language. This is an important NLP task for\nmorphologically-rich agglutinative languages such as the Southern African Nguni\nlanguage group. In this paper, we investigate supervised and unsupervised\nmodels for two variants of morphological segmentation: canonical and surface\nsegmentation. We train sequence-to-sequence models for canonical segmentation,\nwhere the underlying morphemes may not be equal to the surface form of the\nword, and Conditional Random Fields (CRF) for surface segmentation.\nTransformers outperform LSTMs with attention on canonical segmentation,\nobtaining an average F1 score of 72.5% across 4 languages. Feature-based CRFs\noutperform bidirectional LSTM-CRFs to obtain an average of 97.1% F1 on surface\nsegmentation. In the unsupervised setting, an entropy-based approach using a\ncharacter-level LSTM language model fails to outperforms a Morfessor baseline,\nwhile on some of the languages neither approach performs much better than a\nrandom baseline. We hope that the high performance of the supervised\nsegmentation models will help to facilitate the development of better NLP tools\nfor Nguni languages.", "published": "2021-04-01 21:06:51", "link": "http://arxiv.org/abs/2104.00767v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low-Resource Language Modelling of South African Languages", "abstract": "Language models are the foundation of current neural network-based models for\nnatural language understanding and generation. However, research on the\nintrinsic performance of language models on African languages has been\nextremely limited, which is made more challenging by the lack of large or\nstandardised training and evaluation sets that exist for English and other\nhigh-resource languages. In this paper, we evaluate the performance of\nopen-vocabulary language models on low-resource South African languages, using\nbyte-pair encoding to handle the rich morphology of these languages. We\nevaluate different variants of n-gram models, feedforward neural networks,\nrecurrent neural networks (RNNs), and Transformers on small-scale datasets.\nOverall, well-regularized RNNs give the best performance across two isiZulu and\none Sepedi datasets. Multilingual training further improves performance on\nthese datasets. We hope that this research will open new avenues for research\ninto multilingual and low-resource language modelling for African languages.", "published": "2021-04-01 21:27:27", "link": "http://arxiv.org/abs/2104.00772v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MultiWOZ 2.4: A Multi-Domain Task-Oriented Dialogue Dataset with\n  Essential Annotation Corrections to Improve State Tracking Evaluation", "abstract": "The MultiWOZ 2.0 dataset has greatly stimulated the research of task-oriented\ndialogue systems. However, its state annotations contain substantial noise,\nwhich hinders a proper evaluation of model performance. To address this issue,\nmassive efforts were devoted to correcting the annotations. Three improved\nversions (i.e., MultiWOZ 2.1-2.3) have then been released. Nonetheless, there\nare still plenty of incorrect and inconsistent annotations. This work\nintroduces MultiWOZ 2.4, which refines the annotations in the validation set\nand test set of MultiWOZ 2.1. The annotations in the training set remain\nunchanged (same as MultiWOZ 2.1) to elicit robust and noise-resilient model\ntraining. We benchmark eight state-of-the-art dialogue state tracking models on\nMultiWOZ 2.4. All of them demonstrate much higher performance than on MultiWOZ\n2.1.", "published": "2021-04-01 21:31:48", "link": "http://arxiv.org/abs/2104.00773v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Action-Based Conversations Dataset: A Corpus for Building More In-Depth\n  Task-Oriented Dialogue Systems", "abstract": "Existing goal-oriented dialogue datasets focus mainly on identifying slots\nand values. However, customer support interactions in reality often involve\nagents following multi-step procedures derived from explicitly-defined company\npolicies as well. To study customer service dialogue systems in more realistic\nsettings, we introduce the Action-Based Conversations Dataset (ABCD), a\nfully-labeled dataset with over 10K human-to-human dialogues containing 55\ndistinct user intents requiring unique sequences of actions constrained by\npolicies to achieve task success. We propose two additional dialog tasks,\nAction State Tracking and Cascading Dialogue Success, and establish a series of\nbaselines involving large-scale, pre-trained language models on this dataset.\nEmpirical results demonstrate that while more sophisticated networks outperform\nsimpler models, a considerable gap (50.8% absolute accuracy) still exists to\nreach human-level performance on ABCD.", "published": "2021-04-01 22:04:25", "link": "http://arxiv.org/abs/2104.00783v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do RNN States Encode Abstract Phonological Processes?", "abstract": "Sequence-to-sequence models have delivered impressive results in word\nformation tasks such as morphological inflection, often learning to model\nsubtle morphophonological details with limited training data. Despite the\nperformance, the opacity of neural models makes it difficult to determine\nwhether complex generalizations are learned, or whether a kind of separate rote\nmemorization of each morphophonological process takes place. To investigate\nwhether complex alternations are simply memorized or whether there is some\nlevel of generalization across related sound changes in a sequence-to-sequence\nmodel, we perform several experiments on Finnish consonant gradation -- a\ncomplex set of sound changes triggered in some words by certain suffixes. We\nfind that our models often -- though not always -- encode 17 different\nconsonant gradation processes in a handful of dimensions in the RNN. We also\nshow that by scaling the activations in these dimensions we can control whether\nconsonant gradation occurs and the direction of the gradation.", "published": "2021-04-01 22:24:39", "link": "http://arxiv.org/abs/2104.00789v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CURIE: An Iterative Querying Approach for Reasoning About Situations", "abstract": "Recently, models have been shown to predict the effects of unexpected\nsituations, e.g., would cloudy skies help or hinder plant growth? Given a\ncontext, the goal of such situational reasoning is to elicit the consequences\nof a new situation (st) that arises in that context. We propose a method to\niteratively build a graph of relevant consequences explicitly in a structured\nsituational graph (st-graph) using natural language queries over a finetuned\nlanguage model (M). Across multiple domains, CURIE generates st-graphs that\nhumans find relevant and meaningful in eliciting the consequences of a new\nsituation. We show that st-graphs generated by CURIE improve a situational\nreasoning end task (WIQA-QA) by 3 points on accuracy by simply augmenting their\ninput with our generated situational graphs, especially for a hard subset that\nrequires background knowledge and multi-hop reasoning.", "published": "2021-04-01 23:51:33", "link": "http://arxiv.org/abs/2104.00814v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-harm: detection and support on Twitter", "abstract": "Since the advent of online social media platforms such as Twitter and\nFacebook, useful health-related studies have been conducted using the\ninformation posted by online participants. Personal health-related issues such\nas mental health, self-harm and depression have been studied because users\noften share their stories on such platforms. Online users resort to sharing\nbecause the empathy and support from online communities are crucial in helping\nthe affected individuals. A preliminary analysis shows how contents related to\nnon-suicidal self-injury (NSSI) proliferate on Twitter. Thus, we use Twitter to\ncollect relevant data, analyse, and proffer ways of supporting users prone to\nNSSI behaviour. Our approach utilises a custom crawler to retrieve relevant\ntweets from self-reporting users and relevant organisations interested in\ncombating self-harm. Through textual analysis, we identify six major categories\nof self-harming users consisting of inflicted, anti-self-harm, support seekers,\nrecovered, pro-self-harm and at risk. The inflicted category dominates the\ncollection. From an engagement perspective, we show how online users respond to\nthe information posted by self-harm support organisations on Twitter. By noting\nthe most engaged organisations, we apply a useful technique to uncover the\norganisations' strategy. The online participants show a strong inclination\ntowards online posts associated with mental health related attributes. Our\nstudy is based on the premise that social media can be used as a tool to\nsupport proactive measures to ease the negative impact of self-harm.\nConsequently, we proffer ways to prevent potential users from engaging in\nself-harm and support affected users through a set of recommendations. To\nsupport further research, the dataset will be made available for interested\nresearchers.", "published": "2021-04-01 00:39:42", "link": "http://arxiv.org/abs/2104.00174v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Multilingual and code-switching ASR challenges for low resource Indian\n  languages", "abstract": "Recently, there is increasing interest in multilingual automatic speech\nrecognition (ASR) where a speech recognition system caters to multiple low\nresource languages by taking advantage of low amounts of labeled corpora in\nmultiple languages. With multilingualism becoming common in today's world,\nthere has been increasing interest in code-switching ASR as well. In\ncode-switching, multiple languages are freely interchanged within a single\nsentence or between sentences. The success of low-resource multilingual and\ncode-switching ASR often depends on the variety of languages in terms of their\nacoustics, linguistic characteristics as well as the amount of data available\nand how these are carefully considered in building the ASR system. In this\nchallenge, we would like to focus on building multilingual and code-switching\nASR systems through two different subtasks related to a total of seven Indian\nlanguages, namely Hindi, Marathi, Odia, Tamil, Telugu, Gujarati and Bengali.\nFor this purpose, we provide a total of ~600 hours of transcribed speech data,\ncomprising train and test sets, in these languages including two code-switched\nlanguage pairs, Hindi-English and Bengali-English. We also provide a baseline\nrecipe for both the tasks with a WER of 30.73% and 32.45% on the test sets of\nmultilingual and code-switching subtasks, respectively.", "published": "2021-04-01 03:37:01", "link": "http://arxiv.org/abs/2104.00235v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Detecting over/under-translation errors for determining adequacy in\n  human translations", "abstract": "We present a novel approach to detecting over and under translations (OT/UT)\nas part of adequacy error checks in translation evaluation. We do not restrict\nourselves to machine translation (MT) outputs and specifically target\napplications with human generated translation pipeline. The goal of our system\nis to identify OT/UT errors from human translated video subtitles with high\nerror recall. We achieve this without reference translations by learning a\nmodel on synthesized training data. We compare various classification networks\nthat we trained on embeddings from pre-trained language model with our best\nhybrid network of GRU + CNN achieving 89.3% accuracy on high-quality\nhuman-annotated evaluation data in 8 languages.", "published": "2021-04-01 06:06:36", "link": "http://arxiv.org/abs/2104.00267v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mitigating Media Bias through Neutral Article Generation", "abstract": "Media bias can lead to increased political polarization, and thus, the need\nfor automatic mitigation methods is growing. Existing mitigation work displays\narticles from multiple news outlets to provide diverse news coverage, but\nwithout neutralizing the bias inherent in each of the displayed articles.\nTherefore, we propose a new task, a single neutralized article generation out\nof multiple biased articles, to facilitate more efficient access to balanced\nand unbiased information. In this paper, we compile a new dataset NeuWS, define\nan automatic evaluation metric, and provide baselines and multiple analyses to\nserve as a solid starting point for the proposed task. Lastly, we obtain a\nhuman evaluation to demonstrate the alignment between our metric and human\njudgment.", "published": "2021-04-01 08:37:26", "link": "http://arxiv.org/abs/2104.00336v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Low-Resource Neural Machine Translation for Southern African Languages", "abstract": "Low-resource African languages have not fully benefited from the progress in\nneural machine translation because of a lack of data. Motivated by this\nchallenge we compare zero-shot learning, transfer learning and multilingual\nlearning on three Bantu languages (Shona, isiXhosa and isiZulu) and English.\nOur main target is English-to-isiZulu translation for which we have just 30,000\nsentence pairs, 28% of the average size of our other corpora. We show the\nimportance of language similarity on the performance of English-to-isiZulu\ntransfer learning based on English-to-isiXhosa and English-to-Shona parent\nmodels whose BLEU scores differ by 5.2. We then demonstrate that multilingual\nlearning surpasses both transfer learning and zero-shot learning on our\ndataset, with BLEU score improvements relative to the baseline\nEnglish-to-isiZulu model of 9.9, 6.1 and 2.0 respectively. Our best model also\nimproves the previous SOTA BLEU score by more than 10.", "published": "2021-04-01 09:48:13", "link": "http://arxiv.org/abs/2104.00366v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WakaVT: A Sequential Variational Transformer for Waka Generation", "abstract": "Poetry generation has long been a challenge for artificial intelligence. In\nthe scope of Japanese poetry generation, many researchers have paid attention\nto Haiku generation, but few have focused on Waka generation. To further\nexplore the creative potential of natural language generation systems in\nJapanese poetry creation, we propose a novel Waka generation model, WakaVT,\nwhich automatically produces Waka poems given user-specified keywords. Firstly,\nan additive mask-based approach is presented to satisfy the form constraint.\nSecondly, the structures of Transformer and variational autoencoder are\nintegrated to enhance the quality of generated content. Specifically, to obtain\nnovelty and diversity, WakaVT employs a sequence of latent variables, which\neffectively captures word-level variability in Waka data. To improve linguistic\nquality in terms of fluency, coherence, and meaningfulness, we further propose\nthe fused multilevel self-attention mechanism, which properly models the\nhierarchical linguistic structure of Waka. To the best of our knowledge, we are\nthe first to investigate Waka generation with models based on Transformer\nand/or variational autoencoder. Both objective and subjective evaluation\nresults demonstrate that our model outperforms baselines significantly.", "published": "2021-04-01 12:14:41", "link": "http://arxiv.org/abs/2104.00426v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mining Wikidata for Name Resources for African Languages", "abstract": "This work supports further development of language technology for the\nlanguages of Africa by providing a Wikidata-derived resource of name lists\ncorresponding to common entity types (person, location, and organization).\nWhile we are not the first to mine Wikidata for name lists, our approach\nemphasizes scalability and replicability and addresses data quality issues for\nlanguages that do not use Latin scripts. We produce lists containing\napproximately 1.9 million names across 28 African languages. We describe the\ndata, the process used to produce it, and its limitations, and provide the\nsoftware and data for public use. Finally, we discuss the ethical\nconsiderations of producing this resource and others of its kind.", "published": "2021-04-01 15:34:53", "link": "http://arxiv.org/abs/2104.00558v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HLE-UPC at SemEval-2021 Task 5: Multi-Depth DistilBERT for Toxic Spans\n  Detection", "abstract": "This paper presents our submission to SemEval-2021 Task 5: Toxic Spans\nDetection. The purpose of this task is to detect the spans that make a text\ntoxic, which is a complex labour for several reasons. Firstly, because of the\nintrinsic subjectivity of toxicity, and secondly, due to toxicity not always\ncoming from single words like insults or offends, but sometimes from whole\nexpressions formed by words that may not be toxic individually. Following this\nidea of focusing on both single words and multi-word expressions, we study the\nimpact of using a multi-depth DistilBERT model, which uses embeddings from\ndifferent layers to estimate the final per-token toxicity. Our quantitative\nresults show that using information from multiple depths boosts the performance\nof the model. Finally, we also analyze our best model qualitatively.", "published": "2021-04-01 17:37:38", "link": "http://arxiv.org/abs/2104.00639v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AmbiFC: Fact-Checking Ambiguous Claims with Evidence", "abstract": "Automated fact-checking systems verify claims against evidence to predict\ntheir veracity. In real-world scenarios, the retrieved evidence may not\nunambiguously support or refute the claim and yield conflicting but valid\ninterpretations. Existing fact-checking datasets assume that the models\ndeveloped with them predict a single veracity label for each claim, thus\ndiscouraging the handling of such ambiguity. To address this issue we present\nAmbiFC, a fact-checking dataset with 10k claims derived from real-world\ninformation needs. It contains fine-grained evidence annotations of 50k\npassages from 5k Wikipedia pages. We analyze the disagreements arising from\nambiguity when comparing claims against evidence in AmbiFC, observing a strong\ncorrelation of annotator disagreement with linguistic phenomena such as\nunderspecification and probabilistic reasoning. We develop models for\npredicting veracity handling this ambiguity via soft labels and find that a\npipeline that learns the label distribution for sentence-level evidence\nselection and veracity prediction yields the best performance. We compare\nmodels trained on different subsets of AmbiFC and show that models trained on\nthe ambiguous instances perform better when faced with the identified\nlinguistic phenomena.", "published": "2021-04-01 17:40:08", "link": "http://arxiv.org/abs/2104.00640v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sampling and Filtering of Neural Machine Translation Distillation Data", "abstract": "In most of neural machine translation distillation or stealing scenarios, the\ngoal is to preserve the performance of the target model (teacher). The\nhighest-scoring hypothesis of the teacher model is commonly used to train a new\nmodel (student). If reference translations are also available, then better\nhypotheses (with respect to the references) can be upsampled and poor\nhypotheses either removed or undersampled.\n  This paper explores the importance sampling method landscape (pruning,\nhypothesis upsampling and undersampling, deduplication and their combination)\nwith English to Czech and English to German MT models using standard MT\nevaluation metrics. We show that careful upsampling and combination with the\noriginal data leads to better performance when compared to training only on the\noriginal or synthesized data or their direct combination.", "published": "2021-04-01 17:54:52", "link": "http://arxiv.org/abs/2104.00664v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Many-to-English Machine Translation Tools, Data, and Pretrained Models", "abstract": "While there are more than 7000 languages in the world, most translation\nresearch efforts have targeted a few high-resource languages. Commercial\ntranslation systems support only one hundred languages or fewer, and do not\nmake these models available for transfer to low resource languages. In this\nwork, we present useful tools for machine translation research: MTData,\nNLCodec, and RTG. We demonstrate their usefulness by creating a multilingual\nneural machine translation model capable of translating from 500 source\nlanguages to English. We make this multilingual model readily downloadable and\nusable as a service, or as a parent model for transfer-learning to even\nlower-resource languages.", "published": "2021-04-01 06:55:12", "link": "http://arxiv.org/abs/2104.00290v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Is Label Smoothing Truly Incompatible with Knowledge Distillation: An\n  Empirical Study", "abstract": "This work aims to empirically clarify a recently discovered perspective that\nlabel smoothing is incompatible with knowledge distillation. We begin by\nintroducing the motivation behind on how this incompatibility is raised, i.e.,\nlabel smoothing erases relative information between teacher logits. We provide\na novel connection on how label smoothing affects distributions of semantically\nsimilar and dissimilar classes. Then we propose a metric to quantitatively\nmeasure the degree of erased information in sample's representation. After\nthat, we study its one-sidedness and imperfection of the incompatibility view\nthrough massive analyses, visualizations and comprehensive experiments on Image\nClassification, Binary Networks, and Neural Machine Translation. Finally, we\nbroadly discuss several circumstances wherein label smoothing will indeed lose\nits effectiveness. Project page:\nhttp://zhiqiangshen.com/projects/LS_and_KD/index.html.", "published": "2021-04-01 17:59:12", "link": "http://arxiv.org/abs/2104.00676v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Towards General Purpose Vision Systems", "abstract": "Computer vision systems today are primarily N-purpose systems, designed and\ntrained for a predefined set of tasks. Adapting such systems to new tasks is\nchallenging and often requires non-trivial modifications to the network\narchitecture (e.g. adding new output heads) or training process (e.g. adding\nnew losses). To reduce the time and expertise required to develop new\napplications, we would like to create general purpose vision systems that can\nlearn and perform a range of tasks without any modification to the architecture\nor learning process.\n  In this paper, we propose GPV-1, a task-agnostic vision-language architecture\nthat can learn and perform tasks that involve receiving an image and producing\ntext and/or bounding boxes, including classification, localization, visual\nquestion answering, captioning, and more. We also propose evaluations of\ngenerality of architecture, skill-concept transfer, and learning efficiency\nthat may inform future work on general purpose vision. Our experiments indicate\nGPV-1 is effective at multiple tasks, reuses some concept knowledge across\ntasks, can perform the Referring Expressions task zero-shot, and further\nimproves upon the zero-shot performance using a few training samples.", "published": "2021-04-01 19:35:21", "link": "http://arxiv.org/abs/2104.00743v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Keyword Transformer: A Self-Attention Model for Keyword Spotting", "abstract": "The Transformer architecture has been successful across many domains,\nincluding natural language processing, computer vision and speech recognition.\nIn keyword spotting, self-attention has primarily been used on top of\nconvolutional or recurrent encoders. We investigate a range of ways to adapt\nthe Transformer architecture to keyword spotting and introduce the Keyword\nTransformer (KWT), a fully self-attentional architecture that exceeds\nstate-of-the-art performance across multiple tasks without any pre-training or\nadditional data. Surprisingly, this simple architecture outperforms more\ncomplex models that mix convolutional, recurrent and attentive layers. KWT can\nbe used as a drop-in replacement for these models, setting two new benchmark\nrecords on the Google Speech Commands dataset with 98.6% and 97.7% accuracy on\nthe 12 and 35-command tasks respectively.", "published": "2021-04-01 21:15:30", "link": "http://arxiv.org/abs/2104.00769v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "\"TL;DR:\" Out-of-Context Adversarial Text Summarization and Hashtag\n  Recommendation", "abstract": "This paper presents Out-of-Context Summarizer, a tool that takes arbitrary\npublic news articles out of context by summarizing them to coherently fit\neither a liberal- or conservative-leaning agenda. The Out-of-Context Summarizer\nalso suggests hashtag keywords to bolster the polarization of the summary, in\ncase one is inclined to take it to Twitter, Parler or other platforms for\ntrolling. Out-of-Context Summarizer achieved 79% precision and 99% recall when\nsummarizing COVID-19 articles, 93% precision and 93% recall when summarizing\npolitically-centered articles, and 87% precision and 88% recall when taking\nliberally-biased articles out of context. Summarizing valid sources instead of\nsynthesizing fake text, the Out-of-Context Summarizer could fairly pass the\n\"adversarial disclosure\" test, but we didn't take this easy route in our paper.\nInstead, we used the Out-of-Context Summarizer to push the debate of potential\nmisuse of automated text generation beyond the boilerplate text of responsible\ndisclosure of adversarial language models.", "published": "2021-04-01 22:03:44", "link": "http://arxiv.org/abs/2104.00782v1", "categories": ["cs.CL", "cs.CR", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Normal vs. Adversarial: Salience-based Analysis of Adversarial Samples\n  for Relation Extraction", "abstract": "Recent neural-based relation extraction approaches, though achieving\npromising improvement on benchmark datasets, have reported their vulnerability\ntowards adversarial attacks. Thus far, efforts mostly focused on generating\nadversarial samples or defending adversarial attacks, but little is known about\nthe difference between normal and adversarial samples. In this work, we take\nthe first step to leverage the salience-based method to analyze those\nadversarial samples. We observe that salience tokens have a direct correlation\nwith adversarial perturbations. We further find the adversarial perturbations\nare either those tokens not existing in the training set or superficial cues\nassociated with relation labels. To some extent, our approach unveils the\ncharacters against adversarial samples. We release an open-source testbed,\n\"DiagnoseAdv\" in https://github.com/zjunlp/DiagnoseAdv.", "published": "2021-04-01 07:36:04", "link": "http://arxiv.org/abs/2104.00312v4", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bidirectional Multiscale Feature Aggregation for Speaker Verification", "abstract": "In this paper, we propose a novel bidirectional multiscale feature\naggregation (BMFA) network with attentional fusion modules for text-independent\nspeaker verification. The feature maps from different stages of the backbone\nnetwork are iteratively combined and refined in both a bottom-up and top-down\nmanner. Furthermore, instead of simple concatenation or element-wise addition\nof feature maps from different stages, an attentional fusion module is designed\nto compute the fusion weights. Experiments are conducted on the NIST SRE16 and\nVoxCeleb1 datasets. The experimental results demonstrate the effectiveness of\nthe bidirectional aggregation strategy and show that the proposed attentional\nfusion module can further improve the performance.", "published": "2021-04-01 03:19:10", "link": "http://arxiv.org/abs/2104.00230v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Expressive Text-to-Speech using Style Tag", "abstract": "As recent text-to-speech (TTS) systems have been rapidly improved in speech\nquality and generation speed, many researchers now focus on a more challenging\nissue: expressive TTS. To control speaking styles, existing expressive TTS\nmodels use categorical style index or reference speech as style input. In this\nwork, we propose StyleTagging-TTS (ST-TTS), a novel expressive TTS model that\nutilizes a style tag written in natural language. Using a style-tagged TTS\ndataset and a pre-trained language model, we modeled the relationship between\nlinguistic embedding and speaking style domain, which enables our model to work\neven with style tags unseen during training. As style tag is written in natural\nlanguage, it can control speaking style in a more intuitive, interpretable, and\nscalable way compared with style index or reference speech. In addition, in\nterms of model architecture, we propose an efficient non-autoregressive (NAR)\nTTS architecture with single-stage training. The experimental result shows that\nST-TTS outperforms the existing expressive TTS model, Tacotron2-GST in speech\nquality and expressiveness.", "published": "2021-04-01 12:40:07", "link": "http://arxiv.org/abs/2104.00436v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Interactive spatial speech recognition maps based on simulated speech\n  recognition experiments", "abstract": "In their everyday life, the speech recognition performance of human listeners\nis influenced by diverse factors, such as the acoustic environment, the talker\nand listener positions, possibly impaired hearing, and optional hearing\ndevices. Prediction models come closer to considering all required factors\nsimultaneously to predict the individual speech recognition performance in\ncomplex acoustic environments. While such predictions may still not be\nsufficiently accurate for serious applications, they can already be performed\nand demand an accessible representation. In this contribution, an interactive\nrepresentation of speech recognition performance is proposed, which focuses on\nthe listeners head orientation and the spatial dimensions of an acoustic scene.\nA exemplary modeling toolchain, including an acoustic rendering model, a\nhearing device model, and a listener model, was used to generate a data set for\ndemonstration purposes. Using the spatial speech recognition maps to explore\nthis data set demonstrated the suitability of the approach to observe possibly\nrelevant behavior. The proposed representation provides a suitable target to\ncompare and validate different modeling approaches in ecologically relevant\ncontexts. Eventually, it may serve as a tool to use validated prediction models\nin the design of spaces and devices which take speech communication into\naccount.", "published": "2021-04-01 05:35:46", "link": "http://arxiv.org/abs/2104.00259v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CycleDRUMS: Automatic Drum Arrangement For Bass Lines Using CycleGAN", "abstract": "The two main research threads in computer-based music generation are: the\nconstruction of autonomous music-making systems, and the design of\ncomputer-based environments to assist musicians. In the symbolic domain, the\nkey problem of automatically arranging a piece music was extensively studied,\nwhile relatively fewer systems tackled this challenge in the audio domain. In\nthis contribution, we propose CycleDRUMS, a novel method for generating drums\ngiven a bass line. After converting the waveform of the bass into a\nmel-spectrogram, we are able to automatically generate original drums that\nfollow the beat, sound credible and can be directly mixed with the input bass.\nWe formulated this task as an unpaired image-to-image translation problem, and\nwe addressed it with CycleGAN, a well-established unsupervised style transfer\nframework, originally designed for treating images. The choice to deploy raw\naudio and mel-spectrograms enabled us to better represent how humans perceive\nmusic, and to potentially draw sounds for new arrangements from the vast\ncollection of music recordings accumulated in the last century. In absence of\nan objective way of evaluating the output of both generative adversarial\nnetworks and music generative systems, we further defined a possible metric for\nthe proposed task, partially based on human (and expert) judgement. Finally, as\na comparison, we replicated our results with Pix2Pix, a paired image-to-image\ntranslation network, and we showed that our approach outperforms it.", "published": "2021-04-01 09:17:48", "link": "http://arxiv.org/abs/2104.00353v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Positive Sample Propagation along the Audio-Visual Event Line", "abstract": "Visual and audio signals often coexist in natural environments, forming\naudio-visual events (AVEs). Given a video, we aim to localize video segments\ncontaining an AVE and identify its category. In order to learn discriminative\nfeatures for a classifier, it is pivotal to identify the helpful (or positive)\naudio-visual segment pairs while filtering out the irrelevant ones, regardless\nwhether they are synchronized or not. To this end, we propose a new positive\nsample propagation (PSP) module to discover and exploit the closely related\naudio-visual pairs by evaluating the relationship within every possible pair.\nIt can be done by constructing an all-pair similarity map between each audio\nand visual segment, and only aggregating the features from the pairs with high\nsimilarity scores. To encourage the network to extract high correlated features\nfor positive samples, a new audio-visual pair similarity loss is proposed. We\nalso propose a new weighting branch to better exploit the temporal correlations\nin weakly supervised setting. We perform extensive experiments on the public\nAVE dataset and achieve new state-of-the-art accuracy in both fully and weakly\nsupervised settings, thus verifying the effectiveness of our method.", "published": "2021-04-01 03:53:57", "link": "http://arxiv.org/abs/2104.00239v2", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Unsupervised Sound Localization via Iterative Contrastive Learning", "abstract": "Sound localization aims to find the source of the audio signal in the visual\nscene. However, it is labor-intensive to annotate the correlations between the\nsignals sampled from the audio and visual modalities, thus making it difficult\nto supervise the learning of a machine for this task. In this work, we propose\nan iterative contrastive learning framework that requires no data annotations.\nAt each iteration, the proposed method takes the 1) localization results in\nimages predicted in the previous iteration, and 2) semantic relationships\ninferred from the audio signals as the pseudo-labels. We then use the\npseudo-labels to learn the correlation between the visual and audio signals\nsampled from the same video (intra-frame sampling) as well as the association\nbetween those extracted across videos (inter-frame relation). Our iterative\nstrategy gradually encourages the localization of the sounding objects and\nreduces the correlation between the non-sounding regions and the reference\naudio. Quantitative and qualitative experimental results demonstrate that the\nproposed framework performs favorably against existing unsupervised and\nweakly-supervised methods on the sound localization task.", "published": "2021-04-01 07:48:29", "link": "http://arxiv.org/abs/2104.00315v1", "categories": ["cs.CV", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Speech Resynthesis from Discrete Disentangled Self-Supervised\n  Representations", "abstract": "We propose using self-supervised discrete representations for the task of\nspeech resynthesis. To generate disentangled representation, we separately\nextract low-bitrate representations for speech content, prosodic information,\nand speaker identity. This allows to synthesize speech in a controllable\nmanner. We analyze various state-of-the-art, self-supervised representation\nlearning methods and shed light on the advantages of each method while\nconsidering reconstruction quality and disentanglement properties.\nSpecifically, we evaluate the F0 reconstruction, speaker identification\nperformance (for both resynthesis and voice conversion), recordings'\nintelligibility, and overall quality using subjective human evaluation. Lastly,\nwe demonstrate how these representations can be used for an ultra-lightweight\nspeech codec. Using the obtained representations, we can get to a rate of 365\nbits per second while providing better speech quality than the baseline\nmethods. Audio samples can be found under the following link:\nspeechbot.github.io/resynthesis.", "published": "2021-04-01 09:20:33", "link": "http://arxiv.org/abs/2104.00355v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Enriched Music Representations with Multiple Cross-modal Contrastive\n  Learning", "abstract": "Modeling various aspects that make a music piece unique is a challenging\ntask, requiring the combination of multiple sources of information. Deep\nlearning is commonly used to obtain representations using various sources of\ninformation, such as the audio, interactions between users and songs, or\nassociated genre metadata. Recently, contrastive learning has led to\nrepresentations that generalize better compared to traditional supervised\nmethods. In this paper, we present a novel approach that combines multiple\ntypes of information related to music using cross-modal contrastive learning,\nallowing us to learn an audio feature from heterogeneous data simultaneously.\nWe align the latent representations obtained from playlists-track interactions,\ngenre metadata, and the tracks' audio, by maximizing the agreement between\nthese modality representations using a contrastive loss. We evaluate our\napproach in three tasks, namely, genre classification, playlist continuation\nand automatic tagging. We compare the performances with a baseline audio-based\nCNN trained to predict these modalities. We also study the importance of\nincluding multiple sources of information when training our embedding model.\nThe results suggest that the proposed method outperforms the baseline in all\nthe three downstream tasks and achieves comparable performance to the\nstate-of-the-art.", "published": "2021-04-01 12:41:15", "link": "http://arxiv.org/abs/2104.00437v1", "categories": ["cs.SD", "cs.IR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fast DCTTS: Efficient Deep Convolutional Text-to-Speech", "abstract": "We propose an end-to-end speech synthesizer, Fast DCTTS, that synthesizes\nspeech in real time on a single CPU thread. The proposed model is composed of a\ncarefully-tuned lightweight network designed by applying multiple network\nreduction and fidelity improvement techniques. In addition, we propose a novel\ngroup highway activation that can compromise between computational efficiency\nand the regularization effect of the gating mechanism. As well, we introduce a\nnew metric called Elastic mel-cepstral distortion (EMCD) to measure the\nfidelity of the output mel-spectrogram. In experiments, we analyze the effect\nof the acceleration techniques on speed and speech quality. Compared with the\nbaseline model, the proposed model exhibits improved MOS from 2.62 to 2.74 with\nonly 1.76% computation and 2.75% parameters. The speed on a single CPU thread\nwas improved by 7.45 times, which is fast enough to produce mel-spectrogram in\nreal time without GPU.", "published": "2021-04-01 17:08:01", "link": "http://arxiv.org/abs/2104.00624v1", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Multi-rate attention architecture for fast streamable Text-to-speech\n  spectrum modeling", "abstract": "Typical high quality text-to-speech (TTS) systems today use a two-stage\narchitecture, with a spectrum model stage that generates spectral frames and a\nvocoder stage that generates the actual audio. High-quality spectrum models\nusually incorporate the encoder-decoder architecture with self-attention or\nbi-directional long short-term (BLSTM) units. While these models can produce\nhigh quality speech, they often incur O($L$) increase in both latency and\nreal-time factor (RTF) with respect to input length $L$. In other words, longer\ninputs leads to longer delay and slower synthesis speed, limiting its use in\nreal-time applications. In this paper, we propose a multi-rate attention\narchitecture that breaks the latency and RTF bottlenecks by computing a compact\nrepresentation during encoding and recurrently generating the attention vector\nin a streaming manner during decoding. The proposed architecture achieves high\naudio quality (MOS of 4.31 compared to groundtruth 4.48), low latency, and low\nRTF at the same time. Meanwhile, both latency and RTF of the proposed system\nstay constant regardless of input lengths, making it ideal for real-time\napplications.", "published": "2021-04-01 18:15:30", "link": "http://arxiv.org/abs/2104.00705v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Out of a hundred trials, how many errors does your speaker verifier\n  make?", "abstract": "Out of a hundred trials, how many errors does your speaker verifier make? For\nthe user this is an important, practical question, but researchers and vendors\ntypically sidestep it and supply instead the conditional error-rates that are\ngiven by the ROC/DET curve. We posit that the user's question is answered by\nthe Bayes error-rate. We present a tutorial to show how to compute the\nerror-rate that results when making Bayes decisions with calibrated likelihood\nratios, supplied by the verifier, and an hypothesis prior, supplied by the\nuser. For perfect calibration, the Bayes error-rate is upper bounded by\nmin(EER,P,1-P), where EER is the equal-error-rate and P, 1-P are the prior\nprobabilities of the competing hypotheses. The EER represents the accuracy of\nthe verifier, while min(P,1-P) represents the hardness of the classification\nproblem. We further show how the Bayes error-rate can be computed also for\nnon-perfect calibration and how to generalize from error-rate to expected cost.\nWe offer some criticism of decisions made by direct score thresholding.\nFinally, we demonstrate by analyzing error-rates of the recently published\nDCA-PLDA speaker verifier.", "published": "2021-04-01 19:10:48", "link": "http://arxiv.org/abs/2104.00732v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Unsupervised Speech Representation Learning for Behavior Modeling using\n  Triplet Enhanced Contextualized Networks", "abstract": "Speech encodes a wealth of information related to human behavior and has been\nused in a variety of automated behavior recognition tasks. However, extracting\nbehavioral information from speech remains challenging including due to\ninadequate training data resources stemming from the often low occurrence\nfrequencies of specific behavioral patterns. Moreover, supervised behavioral\nmodeling typically relies on domain-specific construct definitions and\ncorresponding manually-annotated data, rendering generalizing across domains\nchallenging. In this paper, we exploit the stationary properties of human\nbehavior within an interaction and present a representation learning method to\ncapture behavioral information from speech in an unsupervised way. We\nhypothesize that nearby segments of speech share the same behavioral context\nand hence map onto similar underlying behavioral representations. We present an\nencoder-decoder based Deep Contextualized Network (DCN) as well as a\nTriplet-Enhanced DCN (TE-DCN) framework to capture the behavioral context and\nderive a manifold representation, where speech frames with similar behaviors\nare closer while frames of different behaviors maintain larger distances. The\nmodels are trained on movie audio data and validated on diverse domains\nincluding on a couples therapy corpus and other publicly collected data (e.g.,\nstand-up comedy). With encouraging results, our proposed framework shows the\nfeasibility of unsupervised learning within cross-domain behavioral modeling.", "published": "2021-04-01 22:44:23", "link": "http://arxiv.org/abs/2104.03899v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Collaborative Learning to Generate Audio-Video Jointly", "abstract": "There have been a number of techniques that have demonstrated the generation\nof multimedia data for one modality at a time using GANs, such as the ability\nto generate images, videos, and audio. However, so far, the task of multi-modal\ngeneration of data, specifically for audio and videos both, has not been\nsufficiently well-explored. Towards this, we propose a method that demonstrates\nthat we are able to generate naturalistic samples of video and audio data by\nthe joint correlated generation of audio and video modalities. The proposed\nmethod uses multiple discriminators to ensure that the audio, video, and the\njoint output are also indistinguishable from real-world samples. We present a\ndataset for this task and show that we are able to generate realistic samples.\nThis method is validated using various standard metrics such as Inception\nScore, Frechet Inception Distance (FID) and through human evaluation.", "published": "2021-04-01 01:00:51", "link": "http://arxiv.org/abs/2104.02656v1", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.MM", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
