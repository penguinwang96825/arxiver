{"title": "Computational Assessment of Hyperpartisanship in News Titles", "abstract": "We first adopt a human-guided machine learning framework to develop a new\ndataset for hyperpartisan news title detection with 2,200 manually labeled and\n1.8 million machine-labeled titles that were posted from 2014 to the present by\nnine representative media organizations across three media bias groups - Left,\nCentral, and Right in an active learning manner. A fine-tuned transformer-based\nlanguage model achieves an overall accuracy of 0.84 and an F1 score of 0.78 on\nan external validation set. Next, we conduct a computational analysis to\nquantify the extent and dynamics of partisanship in news titles. While some\naspects are as expected, our study reveals new or nuanced differences between\nthe three media groups. We find that overall the Right media tends to use\nproportionally more hyperpartisan titles. Roughly around the 2016 Presidential\nElection, the proportions of hyperpartisan titles increased across all media\nbias groups, with the Left media exhibiting the most significant relative\nincrease. We identify three major topics including foreign issues, political\nsystems, and societal issues that are suggestive of hyperpartisanship in news\ntitles using logistic regression models and the Shapley values. Through an\nanalysis of the topic distribution, we find that societal issues gradually gain\nmore attention from all media groups. We further apply a lexicon-based language\nanalysis tool to the titles of each topic and quantify the linguistic distance\nbetween any pairs of the three media groups, uncovering three distinct\npatterns.", "published": "2023-01-16 05:56:58", "link": "http://arxiv.org/abs/2301.06270v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distinguish Sense from Nonsense: Out-of-Scope Detection for Virtual\n  Assistants", "abstract": "Out of Scope (OOS) detection in Conversational AI solutions enables a chatbot\nto handle a conversation gracefully when it is unable to make sense of the\nend-user query. Accurately tagging a query as out-of-domain is particularly\nhard in scenarios when the chatbot is not equipped to handle a topic which has\nsemantic overlap with an existing topic it is trained on. We propose a simple\nyet effective OOS detection method that outperforms standard OOS detection\nmethods in a real-world deployment of virtual assistants. We discuss the\nvarious design and deployment considerations for a cloud platform solution to\ntrain virtual assistants and deploy them at scale. Additionally, we propose a\ncollection of datasets that replicates real-world scenarios and show\ncomprehensive results in various settings using both offline and online\nevaluation metrics.", "published": "2023-01-16 18:10:12", "link": "http://arxiv.org/abs/2301.06544v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A separation logic for sequences in pointer programs and its\n  decidability", "abstract": "Separation logic and its variants can describe various properties on pointer\nprograms. However, when it comes to properties on sequences, one may find it\nhard to formalize. To deal with properties on variable-length sequences and\nmultilevel data structures, we propose sequence-heap separation logic which\nintegrates sequences into logical reasoning on heap-manipulated programs.\nQuantifiers over sequence variables and singleton heap storing sequence\n(sequence singleton heap) are new members in our logic. Further, we study the\nsatisfiability problem of two fragments. The propositional fragment of\nsequence-heap separation logic is decidable, and the fragment with 2\nalternations on program variables and 1 alternation on sequence variables is\nundecidable. In addition, we explore boundaries between decidable and\nundecidable fragments of the logic with prenex normal form.", "published": "2023-01-16 02:52:06", "link": "http://arxiv.org/abs/2301.06237v1", "categories": ["cs.LO", "cs.CL"], "primary_category": "cs.LO"}
{"title": "An Error-Guided Correction Model for Chinese Spelling Error Correction", "abstract": "Although existing neural network approaches have achieved great success on\nChinese spelling correction, there is still room to improve. The model is\nrequired to avoid over-correction and to distinguish a correct token from its\nphonological and visually similar ones. In this paper, we propose an\nerror-guided correction model (EGCM) to improve Chinese spelling correction. By\nborrowing the powerful ability of BERT, we propose a novel zero-shot error\ndetection method to do a preliminary detection, which guides our model to\nattend more on the probably wrong tokens in encoding and to avoid modifying the\ncorrect tokens in generating. Furthermore, we introduce a new loss function to\nintegrate the error confusion set, which enables our model to distinguish\neasily misused tokens. Moreover, our model supports highly parallel decoding to\nmeet real application requirements. Experiments are conducted on widely used\nbenchmarks. Our model achieves superior performance against state-of-the-art\napproaches by a remarkable margin, on both the correction quality and\ncomputation speed.", "published": "2023-01-16 09:27:45", "link": "http://arxiv.org/abs/2301.06323v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deep Learning Models to Study Sentence Comprehension in the Human Brain", "abstract": "Recent artificial neural networks that process natural language achieve\nunprecedented performance in tasks requiring sentence-level understanding. As\nsuch, they could be interesting models of the integration of linguistic\ninformation in the human brain. We review works that compare these artificial\nlanguage models with human brain activity and we assess the extent to which\nthis approach has improved our understanding of the neural processes involved\nin natural language comprehension. Two main results emerge. First, the neural\nrepresentation of word meaning aligns with the context-dependent, dense word\nvectors used by the artificial neural networks. Second, the processing\nhierarchy that emerges within artificial neural networks broadly matches the\nbrain, but is surprisingly inconsistent across studies. We discuss current\nchallenges in establishing artificial neural networks as process models of\nnatural language comprehension. We suggest exploiting the highly structured\nrepresentational geometry of artificial neural networks when mapping\nrepresentations to brain data.", "published": "2023-01-16 10:31:25", "link": "http://arxiv.org/abs/2301.06340v2", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "XNLI 2.0: Improving XNLI dataset and performance on Cross Lingual\n  Understanding (XLU)", "abstract": "Natural Language Processing systems are heavily dependent on the availability\nof annotated data to train practical models. Primarily, models are trained on\nEnglish datasets. In recent times, significant advances have been made in\nmultilingual understanding due to the steeply increasing necessity of working\nin different languages. One of the points that stands out is that since there\nare now so many pre-trained multilingual models, we can utilize them for\ncross-lingual understanding tasks. Using cross-lingual understanding and\nNatural Language Inference, it is possible to train models whose applications\nextend beyond the training language. We can leverage the power of machine\ntranslation to skip the tiresome part of translating datasets from one language\nto another. In this work, we focus on improving the original XNLI dataset by\nre-translating the MNLI dataset in all of the 14 different languages present in\nXNLI, including the test and dev sets of XNLI using Google Translate. We also\nperform experiments by training models in all 15 languages and analyzing their\nperformance on the task of natural language inference. We then expand our\nboundary to investigate if we could improve performance in low-resource\nlanguages such as Swahili and Urdu by training models in languages other than\nEnglish.", "published": "2023-01-16 17:24:57", "link": "http://arxiv.org/abs/2301.06527v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-institution text mining to uncover clinical associations: a case\n  study relating social factors and code status in intensive care medicine", "abstract": "Objective: Text mining of clinical notes embedded in electronic medical\nrecords is increasingly used to extract patient characteristics otherwise not\nor only partly available, to assess their association with relevant health\noutcomes. As manual data labeling needed to develop text mining models is\nresource intensive, we investigated whether off-the-shelf text mining models\ndeveloped at external institutions, together with limited within-institution\nlabeled data, could be used to reliably extract study variables to conduct\nassociation studies.\n  Materials and Methods: We developed multiple text mining models on different\ncombinations of within-institution and external-institution data to extract\nsocial factors from discharge reports of intensive care patients. Subsequently,\nwe assessed the associations between social factors and having a\ndo-not-resuscitate/intubate code. Results: Important differences were found\nbetween associations based on manually labeled data compared to text-mined\nsocial factors in three out of five cases. Adopting external-institution text\nmining models using manually labeled within-institution data resulted in models\nwith higher F1-scores, but not in meaningfully different associations.\n  Discussion: While text mining facilitated scaling analyses to larger samples\nleading to discovering a larger number of associations, the estimates may be\nunreliable. Confirmation is needed with better text mining models, ideally on a\nlarger manually labeled dataset.\n  Conclusion: The currently used text mining models were not sufficiently\naccurate to be used reliably in an association study. Model adaptation using\nwithin-institution data did not improve the estimates. Further research is\nneeded to set conditions for reliable use of text mining in medical research.", "published": "2023-01-16 19:04:59", "link": "http://arxiv.org/abs/2301.06570v1", "categories": ["cs.CL", "stat.ME", "68T50, 68U35, 62-xx, 62P10, 92C60, 92D30", "I.2.7; G.3"], "primary_category": "cs.CL"}
{"title": "TEDB System Description to a Shared Task on Euphemism Detection 2022", "abstract": "In this report, we describe our Transformers for euphemism detection baseline\n(TEDB) submissions to a shared task on euphemism detection 2022. We cast the\ntask of predicting euphemism as text classification. We considered\nTransformer-based models which are the current state-of-the-art methods for\ntext classification. We explored different training schemes, pretrained models,\nand model architectures. Our best result of 0.816 F1-score (0.818 precision and\n0.814 recall) consists of a euphemism-detection-finetuned\nTweetEval/TimeLMs-pretrained RoBERTa model as a feature extractor frontend with\na KimCNN classifier backend trained end-to-end using a cosine annealing\nscheduler. We observed pretrained models on sentiment analysis and\noffensiveness detection to correlate with more F1-score while pretraining on\nother tasks, such as sarcasm detection, produces less F1-scores. Also, putting\nmore word vector channels does not improve the performance in our experiments.", "published": "2023-01-16 20:37:56", "link": "http://arxiv.org/abs/2301.06602v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dissociating language and thought in large language models", "abstract": "Large Language Models (LLMs) have come closest among all models to date to\nmastering human language, yet opinions about their linguistic and cognitive\ncapabilities remain split. Here, we evaluate LLMs using a distinction between\nformal linguistic competence -- knowledge of linguistic rules and patterns --\nand functional linguistic competence -- understanding and using language in the\nworld. We ground this distinction in human neuroscience, which has shown that\nformal and functional competence rely on different neural mechanisms. Although\nLLMs are surprisingly good at formal competence, their performance on\nfunctional competence tasks remains spotty and often requires specialized\nfine-tuning and/or coupling with external modules. We posit that models that\nuse language in human-like ways would need to master both of these competence\ntypes, which, in turn, could require the emergence of mechanisms specialized\nfor formal linguistic competence, distinct from functional competence.", "published": "2023-01-16 22:41:19", "link": "http://arxiv.org/abs/2301.06627v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records", "abstract": "We present a new text-to-SQL dataset for electronic health records (EHRs).\nThe utterances were collected from 222 hospital staff members, including\nphysicians, nurses, and insurance review and health records teams. To construct\nthe QA dataset on structured EHR data, we conducted a poll at a university\nhospital and used the responses to create seed questions. We then manually\nlinked these questions to two open-source EHR databases, MIMIC-III and eICU,\nand included various time expressions and held-out unanswerable questions in\nthe dataset, which were also collected from the poll. Our dataset poses a\nunique set of challenges: the model needs to 1) generate SQL queries that\nreflect a wide range of needs in the hospital, including simple retrieval and\ncomplex operations such as calculating survival rate, 2) understand various\ntime expressions to answer time-sensitive questions in healthcare, and 3)\ndistinguish whether a given question is answerable or unanswerable. We believe\nour dataset, EHRSQL, can serve as a practical benchmark for developing and\nassessing QA models on structured EHR data and take a step further towards\nbridging the gap between text-to-SQL research and its real-life deployment in\nhealthcare. EHRSQL is available at https://github.com/glee4810/EHRSQL.", "published": "2023-01-16 05:10:20", "link": "http://arxiv.org/abs/2301.07695v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Opening up Minds with Argumentative Dialogues", "abstract": "Recent research on argumentative dialogues has focused on persuading people\nto take some action, changing their stance on the topic of discussion, or\nwinning debates. In this work, we focus on argumentative dialogues that aim to\nopen up (rather than change) people's minds to help them become more\nunderstanding to views that are unfamiliar or in opposition to their own\nconvictions. To this end, we present a dataset of 183 argumentative dialogues\nabout 3 controversial topics: veganism, Brexit and COVID-19 vaccination. The\ndialogues were collected using the Wizard of Oz approach, where wizards\nleverage a knowledge-base of arguments to converse with participants.\nOpen-mindedness is measured before and after engaging in the dialogue using a\nquestionnaire from the psychology literature, and success of the dialogue is\nmeasured as the change in the participant's stance towards those who hold\nopinions different to theirs. We evaluate two dialogue models: a\nWikipedia-based and an argument-based model. We show that while both models\nperform closely in terms of opening up minds, the argument-based model is\nsignificantly better on other dialogue properties such as engagement and\nclarity.", "published": "2023-01-16 12:47:16", "link": "http://arxiv.org/abs/2301.06400v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards an Automatic Consolidation of French Law", "abstract": "We present preliminary results about Legistix, a tool we are developing to\nautomatically consolidate the French and European law. Legistix is based both\non regular expressions used in several compound grammars, similar to the\nsuccessive passes of a compiler, and on a new specialized language of\nfunctional type, allowing to describe the changes applied to the texts. Instead\nof creating manually a full consolidated version of a text at each modification\ndate, Legistix generates automatically programs from legal documents written in\nnatural language to automatically create the consolidated versions.", "published": "2023-01-16 15:18:57", "link": "http://arxiv.org/abs/2301.06469v1", "categories": ["cs.CL", "cs.IR", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Using Kaldi for Automatic Speech Recognition of Conversational Austrian\n  German", "abstract": "As dialogue systems are becoming more and more interactional and social, also\nthe accurate automatic speech recognition (ASR) of conversational speech is of\nincreasing importance. This shifts the focus from short, spontaneous,\ntask-oriented dialogues to the much higher complexity of casual face-to-face\nconversations. However, the collection and annotation of such conversations is\na time-consuming process and data is sparse for this specific speaking style.\nThis paper presents ASR experiments with read and conversational Austrian\nGerman as target. In order to deal with having only limited resources available\nfor conversational German and, at the same time, with a large variation among\nspeakers with respect to pronunciation characteristics, we improve a\nKaldi-based ASR system by incorporating a (large) knowledge-based pronunciation\nlexicon, while exploring different data-based methods to restrict the number of\npronunciation variants for each lexical entry. We achieve best WER of 0.4% on\nAustrian German read speech and best average WER of 48.5% on conversational\nspeech. We find that by using our best pronunciation lexicon a similarly high\nperformance can be achieved than by increasing the size of the data used for\nthe language model by approx. 360% to 760%. Our findings indicate that for\nlow-resource scenarios -- despite the general trend in speech technology\ntowards using data-based methods only -- knowledge-based approaches are a\nsuccessful, efficient method.", "published": "2023-01-16 15:28:28", "link": "http://arxiv.org/abs/2301.06475v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "CRYPTEXT: Database and Interactive Toolkit of Human-Written Text\n  Perturbations in the Wild", "abstract": "User-generated textual contents on the Internet are often noisy, erroneous,\nand not in correct forms in grammar. In fact, some online users choose to\nexpress their opinions online through carefully perturbed texts, especially in\ncontroversial topics (e.g., politics, vaccine mandate) or abusive contexts\n(e.g., cyberbullying, hate-speech). However, to the best of our knowledge,\nthere is no framework that explores these online ``human-written\" perturbations\n(as opposed to algorithm-generated perturbations). Therefore, we introduce an\ninteractive system called CRYPTEXT. CRYPTEXT is a data-intensive application\nthat provides the users with a database and several tools to extract and\ninteract with human-written perturbations. Specifically, CRYPTEXT helps look\nup, perturb, and normalize (i.e., de-perturb) texts. CRYPTEXT also provides an\ninteractive interface to monitor and analyze text perturbations online. A short\ndemo video is available at: https://youtu.be/8WT3G8xjIoI", "published": "2023-01-16 16:04:09", "link": "http://arxiv.org/abs/2301.06494v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Ankh: Optimized Protein Language Model Unlocks General-Purpose Modelling", "abstract": "As opposed to scaling-up protein language models (PLMs), we seek improving\nperformance via protein-specific optimization. Although the proportionality\nbetween the language model size and the richness of its learned representations\nis validated, we prioritize accessibility and pursue a path of data-efficient,\ncost-reduced, and knowledge-guided optimization. Through over twenty\nexperiments ranging from masking, architecture, and pre-training data, we\nderive insights from protein-specific experimentation into building a model\nthat interprets the language of life, optimally. We present Ankh, the first\ngeneral-purpose PLM trained on Google's TPU-v4 surpassing the state-of-the-art\nperformance with fewer parameters (<10% for pre-training, <7% for inference,\nand <30% for the embedding dimension). We provide a representative range of\nstructure and function benchmarks where Ankh excels. We further provide a\nprotein variant generation analysis on High-N and One-N input data scales where\nAnkh succeeds in learning protein evolutionary conservation-mutation trends and\nintroducing functional diversity while retaining key structural-functional\ncharacteristics. We dedicate our work to promoting accessibility to research\ninnovation via attainable resources.", "published": "2023-01-16 19:04:45", "link": "http://arxiv.org/abs/2301.06568v1", "categories": ["cs.LG", "cs.CL", "cs.DC", "q-bio.QM"], "primary_category": "cs.LG"}
{"title": "PromptShots at the FinNLP-2022 ERAI Tasks: Pairwise Comparison and\n  Unsupervised Ranking", "abstract": "This report describes our PromptShots submissions to a shared task on\nEvaluating the Rationales of Amateur Investors (ERAI). We participated in both\npairwise comparison and unsupervised ranking tasks. For pairwise comparison, we\nemployed instruction-based models based on T5-small and OpenAI InstructGPT\nlanguage models. Surprisingly, we observed OpenAI InstructGPT language model\nfew-shot trained on Chinese data works best in our submissions, ranking 3rd on\nthe maximal loss (ML) pairwise accuracy. This model works better than training\non the Google translated English data by a large margin, where the English\nfew-shot trained InstructGPT model even performs worse than an\ninstruction-based T5-small model finetuned on the English data. However, all\ninstruction-based submissions do not perform well on the maximal potential\nprofit (MPP) pairwise accuracy where there are more data and learning signals.\nThe Chinese few-shot trained InstructGPT model still performs best in our\nsetting. For unsupervised ranking, we utilized many language models, including\nmany financial-specific ones, and Bayesian lexicons unsupervised-learned on\nboth Chinese and English words using a method-of-moments estimator. All our\nsubmissions rank best in the MPP ranking, from 1st to 3rd. However, they all do\nnot perform well for ML scoring. Therefore, both MPP and ML scores need\ndifferent treatments since we treated MPP and ML using the same formula. Our\nonly difference is the treatment of market sentiment lexicons.", "published": "2023-01-16 20:54:27", "link": "http://arxiv.org/abs/2301.06606v1", "categories": ["cs.CL", "cs.CY", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "ClassBases at CASE-2022 Multilingual Protest Event Detection Tasks:\n  Multilingual Protest News Detection and Automatically Replicating Manually\n  Created Event Datasets", "abstract": "In this report, we describe our ClassBases submissions to a shared task on\nmultilingual protest event detection. For the multilingual protest news\ndetection, we participated in subtask-1, subtask-2, and subtask-4, which are\ndocument classification, sentence classification, and token classification. In\nsubtask-1, we compare XLM-RoBERTa-base, mLUKE-base, and XLM-RoBERTa-large on\nfinetuning in a sequential classification setting. We always use a combination\nof the training data from every language provided to train our multilingual\nmodels. We found that larger models seem to work better and entity knowledge\nhelps but at a non-negligible cost. For subtask-2, we only submitted an\nmLUKE-base system for sentence classification. For subtask-4, we only submitted\nan XLM-RoBERTa-base for token classification system for sequence labeling. For\nautomatically replicating manually created event datasets, we participated in\nCOVID-related protest events from the New York Times news corpus. We created a\nsystem to process the crawled data into a dataset of protest events.", "published": "2023-01-16 21:41:03", "link": "http://arxiv.org/abs/2301.06617v1", "categories": ["cs.CL", "cs.CY", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "BayesSpeech: A Bayesian Transformer Network for Automatic Speech\n  Recognition", "abstract": "Recent developments using End-to-End Deep Learning models have been shown to\nhave near or better performance than state of the art Recurrent Neural Networks\n(RNNs) on Automatic Speech Recognition tasks. These models tend to be lighter\nweight and require less training time than traditional RNN-based approaches.\nHowever, these models take frequentist approach to weight training. In theory,\nnetwork weights are drawn from a latent, intractable probability distribution.\nWe introduce BayesSpeech for end-to-end Automatic Speech Recognition.\nBayesSpeech is a Bayesian Transformer Network where these intractable\nposteriors are learned through variational inference and the local\nreparameterization trick without recurrence. We show how the introduction of\nvariance in the weights leads to faster training time and near state-of-the-art\nperformance on LibriSpeech-960.", "published": "2023-01-16 16:19:04", "link": "http://arxiv.org/abs/2301.11276v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "OLKAVS: An Open Large-Scale Korean Audio-Visual Speech Dataset", "abstract": "Inspired by humans comprehending speech in a multi-modal manner, various\naudio-visual datasets have been constructed. However, most existing datasets\nfocus on English, induce dependencies with various prediction models during\ndataset preparation, and have only a small number of multi-view videos. To\nmitigate the limitations, we recently developed the Open Large-scale Korean\nAudio-Visual Speech (OLKAVS) dataset, which is the largest among publicly\navailable audio-visual speech datasets. The dataset contains 1,150 hours of\ntranscribed audio from 1,107 Korean speakers in a studio setup with nine\ndifferent viewpoints and various noise situations. We also provide the\npre-trained baseline models for two tasks, audio-visual speech recognition and\nlip reading. We conducted experiments based on the models to verify the\neffectiveness of multi-modal and multi-view training over uni-modal and\nfrontal-view-only training. We expect the OLKAVS dataset to facilitate\nmulti-modal research in broader areas such as Korean speech recognition,\nspeaker recognition, pronunciation level classification, and mouth motion\nanalysis.", "published": "2023-01-16 11:40:50", "link": "http://arxiv.org/abs/2301.06375v1", "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.SD"], "primary_category": "cs.MM"}
{"title": "Multi-resolution location-based training for multi-channel continuous\n  speech separation", "abstract": "The performance of automatic speech recognition (ASR) systems severely\ndegrades when multi-talker speech overlap occurs. In meeting environments,\nspeech separation is typically performed to improve the robustness of ASR\nsystems. Recently, location-based training (LBT) was proposed as a new training\ncriterion for multi-channel talker-independent speaker separation. Assuming\nfixed array geometry, LBT outperforms widely-used permutation-invariant\ntraining in fully overlapped utterances and matched reverberant conditions.\nThis paper extends LBT to conversational multi-channel speaker separation. We\nintroduce multi-resolution LBT to estimate the complex spectrograms from low to\nhigh time and frequency resolutions. With multi-resolution LBT, convolutional\nkernels are assigned consistently based on speaker locations in physical space.\nEvaluation results show that multi-resolution LBT consistently outperforms\nother competitive methods on the recorded LibriCSS corpus.", "published": "2023-01-16 15:02:06", "link": "http://arxiv.org/abs/2301.06458v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Target Speaker Extraction with Sparse LDA-transformed Speaker\n  Embeddings", "abstract": "As a practical alternative of speech separation, target speaker extraction\n(TSE) aims to extract the speech from the desired speaker using additional\nspeaker cue extracted from the speaker. Its main challenge lies in how to\nproperly extract and leverage the speaker cue to benefit the extracted speech\nquality. The cue extraction method adopted in majority existing TSE studies is\nto directly utilize discriminative speaker embedding, which is extracted from\nthe pre-trained models for speaker verification. Although the high speaker\ndiscriminability is a most desirable property for speaker verification task, we\nargue that it may be too sophisticated for TSE. In this study, we propose that\na simplified speaker cue with clear class separability might be preferred for\nTSE. To verify our proposal, we introduce several forms of speaker cues,\nincluding naive speaker embedding (such as, x-vector and xi-vector) and new\nspeaker embeddings produced from sparse LDA-transform. Corresponding TSE models\nare built by integrating these speaker cues with SepFormer (one SOTA speech\nseparation model). Performances of these TSE models are examined on the\nbenchmark WSJ0-2mix dataset. Experimental results validate the effectiveness\nand generalizability of our proposal, showing up to 9.9% relative improvement\nin SI-SDRi. Moreover, with SI-SDRi of 19.4 dB and PESQ of 3.78, our best TSE\nsystem significantly outperforms the current SOTA systems and offers the top\nTSE results reported till date on the WSJ0-2mix.", "published": "2023-01-16 06:30:48", "link": "http://arxiv.org/abs/2301.06277v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Msanii: High Fidelity Music Synthesis on a Shoestring Budget", "abstract": "In this paper, we present Msanii, a novel diffusion-based model for\nsynthesizing long-context, high-fidelity music efficiently. Our model combines\nthe expressiveness of mel spectrograms, the generative capabilities of\ndiffusion models, and the vocoding capabilities of neural vocoders. We\ndemonstrate the effectiveness of Msanii by synthesizing tens of seconds (190\nseconds) of stereo music at high sample rates (44.1 kHz) without the use of\nconcatenative synthesis, cascading architectures, or compression techniques. To\nthe best of our knowledge, this is the first work to successfully employ a\ndiffusion-based model for synthesizing such long music samples at high sample\nrates. Our demo can be found https://kinyugo.github.io/msanii-demo and our code\nhttps://github.com/Kinyugo/msanii .", "published": "2023-01-16 15:18:26", "link": "http://arxiv.org/abs/2301.06468v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "I.2.7"], "primary_category": "cs.SD"}
{"title": "Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with\n  Multimodal Models", "abstract": "The ability to quickly learn a new task with minimal instruction - known as\nfew-shot learning - is a central aspect of intelligent agents. Classical\nfew-shot benchmarks make use of few-shot samples from a single modality, but\nsuch samples may not be sufficient to characterize an entire concept class. In\ncontrast, humans use cross-modal information to learn new concepts efficiently.\nIn this work, we demonstrate that one can indeed build a better ${\\bf visual}$\ndog classifier by ${\\bf read}$ing about dogs and ${\\bf listen}$ing to them\nbark. To do so, we exploit the fact that recent multimodal foundation models\nsuch as CLIP learn cross-modal encoders that map different modalities to the\nsame representation space. Specifically, we propose a simple strategy for ${\\bf\ncross-modal}$ ${\\bf adaptation}$: we treat examples from different modalities\nas additional few-shot examples. For example, by simply repurposing class names\nas an additional training sample, we trivially turn any n-shot learning problem\ninto a (n+1)-shot problem. This allows us to produce SOTA results with\nembarrassingly simple linear classifiers. We show that our approach can be\ncombined with existing methods such as prefix tuning, adapters, and classifier\nensembling. Finally, to explore other modalities beyond vision and language, we\nconstruct the first (to our knowledge) audiovisual few-shot benchmark and use\ncross-modal training to improve the performance of both image and audio\nclassification.", "published": "2023-01-16 05:40:42", "link": "http://arxiv.org/abs/2301.06267v5", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
