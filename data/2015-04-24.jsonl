{"title": "On the Stability of Online Language Features: How Much Text do you Need\n  to know a Person?", "abstract": "In recent years, numerous studies have inferred personality and other traits\nfrom people's online writing. While these studies are encouraging, more\ninformation is needed in order to use these techniques with confidence. How do\nlinguistic features vary across different online media, and how much text is\nrequired to have a representative sample for a person? In this paper, we\nexamine several large sets of online, user-generated text, drawn from Twitter,\nemail, blogs, and online discussion forums. We examine and compare\npopulation-wide results for the linguistic measure LIWC, and the inferred\ntraits of Big5 Personality and Basic Human Values. We also empirically measure\nthe stability of these traits across different sized samples for each\nindividual. Our results highlight the importance of tuning models to each\nonline medium, and include guidelines for the minimum amount of text required\nfor a representative result.", "published": "2015-04-24 04:45:55", "link": "http://arxiv.org/abs/1504.06391v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Dictionaries for Named Entity Recognition using Minimal\n  Supervision", "abstract": "This paper describes an approach for automatic construction of dictionaries\nfor Named Entity Recognition (NER) using large amounts of unlabeled data and a\nfew seed examples. We use Canonical Correlation Analysis (CCA) to obtain lower\ndimensional embeddings (representations) for candidate phrases and classify\nthese phrases using a small number of labeled examples. Our method achieves\n16.5% and 11.3% F-1 score improvement over co-training on disease and virus NER\nrespectively. We also show that by adding candidate phrase embeddings as\nfeatures in a sequence tagger gives better performance compared to using word\nembeddings.", "published": "2015-04-24 21:43:55", "link": "http://arxiv.org/abs/1504.06650v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Efficient Non-parametric Estimation of Multiple Embeddings per Word in\n  Vector Space", "abstract": "There is rising interest in vector-space word embeddings and their use in\nNLP, especially given recent methods for their fast estimation at very large\nscale. Nearly all this work, however, assumes a single vector per word type\nignoring polysemy and thus jeopardizing their usefulness for downstream tasks.\nWe present an extension to the Skip-gram model that efficiently learns multiple\nembeddings per word type. It differs from recent related work by jointly\nperforming word sense discrimination and embedding learning, by\nnon-parametrically estimating the number of senses per word type, and by its\nefficiency and scalability. We present new state-of-the-art results in the word\nsimilarity in context task and demonstrate its scalability by training with one\nmachine on a corpus of nearly 1 billion tokens in less than 6 hours.", "published": "2015-04-24 22:12:14", "link": "http://arxiv.org/abs/1504.06654v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Inferring Missing Entity Type Instances for Knowledge Base Completion:\n  New Dataset and Methods", "abstract": "Most of previous work in knowledge base (KB) completion has focused on the\nproblem of relation extraction. In this work, we focus on the task of inferring\nmissing entity type instances in a KB, a fundamental task for KB competition\nyet receives little attention. Due to the novelty of this task, we construct a\nlarge-scale dataset and design an automatic evaluation methodology. Our\nknowledge base completion method uses information within the existing KB and\nexternal information from Wikipedia. We show that individual methods trained\nwith a global objective that considers unobserved cells from both the entity\nand the type side gives consistently higher quality predictions compared to\nbaseline methods. We also perform manual evaluation on a small subset of the\ndata to verify the effectiveness of our knowledge base completion methods and\nthe correctness of our proposed automatic evaluation method.", "published": "2015-04-24 22:32:40", "link": "http://arxiv.org/abs/1504.06658v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Compositional Vector Space Models for Knowledge Base Completion", "abstract": "Knowledge base (KB) completion adds new facts to a KB by making inferences\nfrom existing facts, for example by inferring with high likelihood\nnationality(X,Y) from bornIn(X,Y). Most previous methods infer simple one-hop\nrelational synonyms like this, or use as evidence a multi-hop relational path\ntreated as an atomic feature, like bornIn(X,Z) -> containedIn(Z,Y). This paper\npresents an approach that reasons about conjunctions of multi-hop relations\nnon-atomically, composing the implications of a path using a recursive neural\nnetwork (RNN) that takes as inputs vector embeddings of the binary relation in\nthe path. Not only does this allow us to generalize to paths unseen at training\ntime, but also, with a single high-capacity RNN, to predict new relation types\nnot seen when the compositional model was trained (zero-shot learning). We\nassemble a new dataset of over 52M relational triples, and show that our method\nimproves over a traditional classifier by 11%, and a method leveraging\npre-trained embeddings by 7%.", "published": "2015-04-24 23:06:10", "link": "http://arxiv.org/abs/1504.06662v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Using Syntax-Based Machine Translation to Parse English into Abstract\n  Meaning Representation", "abstract": "We present a parser for Abstract Meaning Representation (AMR). We treat\nEnglish-to-AMR conversion within the framework of string-to-tree, syntax-based\nmachine translation (SBMT). To make this work, we transform the AMR structure\ninto a form suitable for the mechanics of SBMT and useful for modeling. We\nintroduce an AMR-specific language model and add data and features drawn from\nsemantic resources. Our resulting AMR parser improves upon state-of-the-art\nresults by 7 Smatch points.", "published": "2015-04-24 23:24:10", "link": "http://arxiv.org/abs/1504.06665v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Classifying Relations by Ranking with Convolutional Neural Networks", "abstract": "Relation classification is an important semantic processing task for which\nstate-ofthe-art systems still rely on costly handcrafted features. In this work\nwe tackle the relation classification task using a convolutional neural network\nthat performs classification by ranking (CR-CNN). We propose a new pairwise\nranking loss function that makes it easy to reduce the impact of artificial\nclasses. We perform experiments using the the SemEval-2010 Task 8 dataset,\nwhich is designed for the task of classifying the relationship between two\nnominals marked in a sentence. Using CRCNN, we outperform the state-of-the-art\nfor this dataset and achieve a F1 of 84.1 without using any costly handcrafted\nfeatures. Additionally, our experimental results show that: (1) our approach is\nmore effective than CNN followed by a softmax classifier; (2) omitting the\nrepresentation of the artificial class Other improves both precision and\nrecall; and (3) using only word embeddings as input features is enough to\nachieve state-of-the-art results if we consider only the text between the two\ntarget nominals.", "published": "2015-04-24 17:50:33", "link": "http://arxiv.org/abs/1504.06580v2", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
