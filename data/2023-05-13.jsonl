{"title": "Frequency-aware Dimension Selection for Static Word Embedding by Mixed\n  Product Distance", "abstract": "Static word embedding is still useful, particularly for context-unavailable\ntasks, because in the case of no context available, pre-trained language models\noften perform worse than static word embeddings. Although dimension is a key\nfactor determining the quality of static word embeddings, automatic dimension\nselection is rarely discussed. In this paper, we investigate the impact of word\nfrequency on the dimension selection, and empirically find that word frequency\nis so vital that it needs to be taken into account during dimension selection.\nBased on such an empirical finding, this paper proposes a dimension selection\nmethod that uses a metric (Mixed Product Distance, MPD) to select a proper\ndimension for word embedding algorithms without training any word embedding.\nThrough applying a post-processing function to oracle matrices, the MPD-based\nmethod can de-emphasize the impact of word frequency. Experiments on both\ncontext-unavailable and context-available tasks demonstrate the better\nefficiency-performance trade-off of our MPD-based dimension selection method\nover baselines.", "published": "2023-05-13 02:53:37", "link": "http://arxiv.org/abs/2305.07826v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Geometry of Multilingual Language Models: An Equality Lens", "abstract": "Understanding the representations of different languages in multilingual\nlanguage models is essential for comprehending their cross-lingual properties,\npredicting their performance on downstream tasks, and identifying any biases\nacross languages. In our study, we analyze the geometry of three multilingual\nlanguage models in Euclidean space and find that all languages are represented\nby unique geometries. Using a geometric separability index we find that\nalthough languages tend to be closer according to their linguistic family, they\nare almost separable with languages from other families. We also introduce a\nCross-Lingual Similarity Index to measure the distance of languages with each\nother in the semantic space. Our findings indicate that the low-resource\nlanguages are not represented as good as high resource languages in any of the\nmodels", "published": "2023-05-13 05:19:15", "link": "http://arxiv.org/abs/2305.07839v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RC3: Regularized Contrastive Cross-lingual Cross-modal Pre-training", "abstract": "Multilingual vision-language (V&L) pre-training has achieved remarkable\nprogress in learning universal representations across different modalities and\nlanguages. In spite of recent success, there still remain challenges limiting\nfurther improvements of V&L pre-trained models in multilingual settings.\nParticularly, current V&L pre-training methods rely heavily on strictly-aligned\nmultilingual image-text pairs generated from English-centric datasets through\nmachine translation. However, the cost of collecting and translating such\nstrictly-aligned datasets is usually unbearable. In this paper, we propose\nRegularized Contrastive Cross-lingual Cross-modal (RC^3) pre-training, which\nfurther exploits more abundant weakly-aligned multilingual image-text pairs.\nSpecifically, we design a regularized cross-lingual visio-textual contrastive\nlearning objective that constrains the representation proximity of\nweakly-aligned visio-textual inputs according to textual relevance. Besides,\nexisting V&L pre-training approaches mainly deal with visual inputs by either\nregion-of-interest (ROI) features or patch embeddings. We flexibly integrate\nthe two forms of visual features into our model for pre-training and downstream\nmulti-modal tasks. Extensive experiments on 5 downstream multi-modal tasks\nacross 6 languages demonstrate the effectiveness of our proposed method over\ncompetitive contrast models with stronger zero-shot capability.", "published": "2023-05-13 14:41:05", "link": "http://arxiv.org/abs/2305.07927v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content", "abstract": "This paper presents a novel approach for detecting ChatGPT-generated vs.\nhuman-written text using language models. To this end, we first collected and\nreleased a pre-processed dataset named OpenGPTText, which consists of rephrased\ncontent generated using ChatGPT. We then designed, implemented, and trained two\ndifferent models for text classification, using Robustly Optimized BERT\nPretraining Approach (RoBERTa) and Text-to-Text Transfer Transformer (T5),\nrespectively. Our models achieved remarkable results, with an accuracy of over\n97% on the test dataset, as evaluated through various metrics. Furthermore, we\nconducted an interpretability study to showcase our model's ability to extract\nand differentiate key features between human-written and ChatGPT-generated\ntext. Our findings provide important insights into the effective use of\nlanguage models to detect generated text.", "published": "2023-05-13 17:12:11", "link": "http://arxiv.org/abs/2305.07969v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-shot Faithful Factual Error Correction", "abstract": "Faithfully correcting factual errors is critical for maintaining the\nintegrity of textual knowledge bases and preventing hallucinations in\nsequence-to-sequence models. Drawing on humans' ability to identify and correct\nfactual errors, we present a zero-shot framework that formulates questions\nabout input claims, looks for correct answers in the given evidence, and\nassesses the faithfulness of each correction based on its consistency with the\nevidence. Our zero-shot framework outperforms fully-supervised approaches, as\ndemonstrated by experiments on the FEVER and SciFact datasets, where our\noutputs are shown to be more faithful. More importantly, the decomposability\nnature of our framework inherently provides interpretability. Additionally, to\nreveal the most suitable metrics for evaluating factual error corrections, we\nanalyze the correlation between commonly used metrics with human judgments in\nterms of three different dimensions regarding intelligibility and faithfulness.", "published": "2023-05-13 18:55:20", "link": "http://arxiv.org/abs/2305.07982v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SCENE: Self-Labeled Counterfactuals for Extrapolating to Negative\n  Examples", "abstract": "Detecting negatives (such as non-entailment relationships, unanswerable\nquestions, and false claims) is an important and challenging aspect of many\nnatural language understanding tasks. Though manually collecting challenging\nnegative examples can help models detect them, it is both costly and\ndomain-specific. In this work, we propose Self-labeled Counterfactuals for\nExtrapolating to Negative Examples (SCENE), an automatic method for\nsynthesizing training data that greatly improves models' ability to detect\nchallenging negative examples. In contrast with standard data augmentation,\nwhich synthesizes new examples for existing labels, SCENE can synthesize\nnegative examples zero-shot from only positive ones. Given a positive example,\nSCENE perturbs it with a mask infilling model, then determines whether the\nresulting example is negative based on a self-training heuristic. With access\nto only answerable training examples, SCENE can close 69.6% of the performance\ngap on SQuAD 2.0, a dataset where half of the evaluation examples are\nunanswerable, compared to a model trained on SQuAD 2.0. Our method also extends\nto boolean question answering and recognizing textual entailment, and improves\ngeneralization from SQuAD to ACE-whQA, an out-of-domain extractive QA\nbenchmark.", "published": "2023-05-13 19:30:58", "link": "http://arxiv.org/abs/2305.07984v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reconstruct Before Summarize: An Efficient Two-Step Framework for\n  Condensing and Summarizing Meeting Transcripts", "abstract": "Meetings typically involve multiple participants and lengthy conversations,\nresulting in redundant and trivial content. To overcome these challenges, we\npropose a two-step framework, Reconstruct before Summarize (RbS), for effective\nand efficient meeting summarization. RbS first leverages a self-supervised\nparadigm to annotate essential contents by reconstructing the meeting\ntranscripts. Secondly, we propose a relative positional bucketing (RPB)\nalgorithm to equip (conventional) summarization models to generate the summary.\nDespite the additional reconstruction process, our proposed RPB significantly\ncompressed the input, leading to faster processing and reduced memory\nconsumption compared to traditional summarization methods. We validate the\neffectiveness and efficiency of our method through extensive evaluations and\nanalysis. On two meeting summarization datasets, AMI and ICSI, our approach\noutperforms previous state-of-the-art approaches without relying on large-scale\npre-training or expert-grade annotating tools.", "published": "2023-05-13 19:54:46", "link": "http://arxiv.org/abs/2305.07988v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Previously Fact-Checked Claim Retrieval", "abstract": "Fact-checkers are often hampered by the sheer amount of online content that\nneeds to be fact-checked. NLP can help them by retrieving already existing\nfact-checks relevant to the content being investigated. This paper introduces a\nnew multilingual dataset -- MultiClaim -- for previously fact-checked claim\nretrieval. We collected 28k posts in 27 languages from social media, 206k\nfact-checks in 39 languages written by professional fact-checkers, as well as\n31k connections between these two groups. This is the most extensive and the\nmost linguistically diverse dataset of this kind to date. We evaluated how\ndifferent unsupervised methods fare on this dataset and its various dimensions.\nWe show that evaluating such a diverse dataset has its complexities and proper\ncare needs to be taken before interpreting the results. We also evaluated a\nsupervised fine-tuning approach, improving upon the unsupervised method\nsignificantly.", "published": "2023-05-13 20:00:18", "link": "http://arxiv.org/abs/2305.07991v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ProKnow: Process Knowledge for Safety Constrained and Explainable\n  Question Generation for Mental Health Diagnostic Assistance", "abstract": "Current Virtual Mental Health Assistants (VMHAs) provide counseling and\nsuggestive care. They refrain from patient diagnostic assistance because they\nlack training in safety-constrained and specialized clinical process knowledge.\nIn this work, we define Proknow as an ordered set of information that maps to\nevidence-based guidelines or categories of conceptual understanding to experts\nin a domain. We also introduce a new dataset of diagnostic conversations guided\nby safety constraints and Proknow that healthcare professionals use. We develop\na method for natural language question generation (NLG) that collects\ndiagnostic information from the patient interactively. We demonstrate the\nlimitations of using state-of-the-art large-scale language models (LMs) on this\ndataset. Our algorithm models the process knowledge through explicitly modeling\nsafety, knowledge capture, and explainability. LMs augmented with ProKnow\nguided method generated 89% safer questions in the depression and anxiety\ndomain. The Explainability of the generated question is assessed by computing\nsimilarity with concepts in depression and anxiety knowledge bases. Overall,\nirrespective of the type of LMs augmented with our ProKnow, we achieved an\naverage 82% improvement over simple pre-trained LMs on safety, explainability,\nand process-guided question generation. We qualitatively and quantitatively\nevaluate the efficacy of the proposed ProKnow-guided methods by introducing\nthree new evaluation metrics for safety, explainability, and process knowledge\nadherence.", "published": "2023-05-13 21:31:02", "link": "http://arxiv.org/abs/2305.08010v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Simple and Plug-and-play Method for Unsupervised Sentence\n  Representation Enhancement", "abstract": "Generating proper embedding of sentences through an unsupervised way is\nbeneficial to semantic matching and retrieval problems in real-world scenarios.\nThis paper presents Representation ALchemy (RepAL), an extremely simple\npost-processing method that enhances sentence representations. The basic idea\nin RepAL is to de-emphasize redundant information of sentence embedding\ngenerated by pre-trained models. Through comprehensive experiments, we show\nthat RepAL is free of training and is a plug-and-play method that can be\ncombined with most existing unsupervised sentence learning models. We also\nconducted in-depth analysis to understand RepAL.", "published": "2023-05-13 02:43:59", "link": "http://arxiv.org/abs/2305.07824v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bridging History with AI A Comparative Evaluation of GPT 3.5, GPT4, and\n  GoogleBARD in Predictive Accuracy and Fact Checking", "abstract": "The rapid proliferation of information in the digital era underscores the\nimportance of accurate historical representation and interpretation. While\nartificial intelligence has shown promise in various fields, its potential for\nhistorical fact-checking and gap-filling remains largely untapped. This study\nevaluates the performance of three large language models LLMs GPT 3.5, GPT 4,\nand GoogleBARD in the context of predicting and verifying historical events\nbased on given data. A novel metric, Distance to Reality (DTR), is introduced\nto assess the models' outputs against established historical facts. The results\nreveal a substantial potential for AI in historical studies, with GPT 4\ndemonstrating superior performance. This paper underscores the need for further\nresearch into AI's role in enriching our understanding of the past and bridging\nhistorical knowledge gaps.", "published": "2023-05-13 08:58:37", "link": "http://arxiv.org/abs/2305.07868v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dual Use Concerns of Generative AI and Large Language Models", "abstract": "We suggest the implementation of the Dual Use Research of Concern (DURC)\nframework, originally designed for life sciences, to the domain of generative\nAI, with a specific focus on Large Language Models (LLMs). With its\ndemonstrated advantages and drawbacks in biological research, we believe the\nDURC criteria can be effectively redefined for LLMs, potentially contributing\nto improved AI governance. Acknowledging the balance that must be struck when\nemploying the DURC framework, we highlight its crucial political role in\nenhancing societal awareness of the impact of generative AI. As a final point,\nwe offer a series of specific recommendations for applying the DURC approach to\nLLM research.", "published": "2023-05-13 10:08:57", "link": "http://arxiv.org/abs/2305.07882v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "PESTS: Persian_English Cross Lingual Corpus for Semantic Textual\n  Similarity", "abstract": "One of the components of natural language processing that has received a lot\nof investigation recently is semantic textual similarity. In computational\nlinguistics and natural language processing, assessing the semantic similarity\nof words, phrases, paragraphs, and texts is crucial. Calculating the degree of\nsemantic resemblance between two textual pieces, paragraphs, or phrases\nprovided in both monolingual and cross-lingual versions is known as semantic\nsimilarity. Cross lingual semantic similarity requires corpora in which there\nare sentence pairs in both the source and target languages with a degree of\nsemantic similarity between them. Many existing cross lingual semantic\nsimilarity models use a machine translation due to the unavailability of cross\nlingual semantic similarity dataset, which the propagation of the machine\ntranslation error reduces the accuracy of the model. On the other hand, when we\nwant to use semantic similarity features for machine translation the same\nmachine translations should not be used for semantic similarity. For Persian,\nwhich is one of the low resource languages, no effort has been made in this\nregard and the need for a model that can understand the context of two\nlanguages is felt more than ever. In this article, the corpus of semantic\ntextual similarity between sentences in Persian and English languages has been\nproduced for the first time by using linguistic experts. We named this dataset\nPESTS (Persian English Semantic Textual Similarity). This corpus contains 5375\nsentence pairs. Also, different models based on transformers have been\nfine-tuned using this dataset. The results show that using the PESTS dataset,\nthe Pearson correlation of the XLM ROBERTa model increases from 85.87% to\n95.62%.", "published": "2023-05-13 11:02:50", "link": "http://arxiv.org/abs/2305.07893v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OCRBench: On the Hidden Mystery of OCR in Large Multimodal Models", "abstract": "Large models have recently played a dominant role in natural language\nprocessing and multimodal vision-language learning. However, their\neffectiveness in text-related visual tasks remains relatively unexplored. In\nthis paper, we conducted a comprehensive evaluation of Large Multimodal Models,\nsuch as GPT4V and Gemini, in various text-related visual tasks including Text\nRecognition, Scene Text-Centric Visual Question Answering (VQA),\nDocument-Oriented VQA, Key Information Extraction (KIE), and Handwritten\nMathematical Expression Recognition (HMER). To facilitate the assessment of\nOptical Character Recognition (OCR) capabilities in Large Multimodal Models, we\npropose OCRBench, a comprehensive evaluation benchmark. OCRBench contains 29\ndatasets, making it the most comprehensive OCR evaluation benchmark available.\nFurthermore, our study reveals both the strengths and weaknesses of these\nmodels, particularly in handling multilingual text, handwritten text,\nnon-semantic text, and mathematical expression recognition. Most importantly,\nthe baseline results presented in this study could provide a foundational\nframework for the conception and assessment of innovative strategies targeted\nat enhancing zero-shot multimodal techniques. The evaluation pipeline and\nbenchmark are available at https://github.com/Yuliang-Liu/MultimodalOCR.", "published": "2023-05-13 11:28:37", "link": "http://arxiv.org/abs/2305.07895v7", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Pre-trained Language Model with Prompts for Temporal Knowledge Graph\n  Completion", "abstract": "Temporal Knowledge graph completion (TKGC) is a crucial task that involves\nreasoning at known timestamps to complete the missing part of facts and has\nattracted more and more attention in recent years. Most existing methods focus\non learning representations based on graph neural networks while inaccurately\nextracting information from timestamps and insufficiently utilizing the implied\ninformation in relations. To address these problems, we propose a novel TKGC\nmodel, namely Pre-trained Language Model with Prompts for TKGC (PPT). We\nconvert a series of sampled quadruples into pre-trained language model inputs\nand convert intervals between timestamps into different prompts to make\ncoherent sentences with implicit semantic information. We train our model with\na masking strategy to convert TKGC task into a masked token prediction task,\nwhich can leverage the semantic information in pre-trained language models.\nExperiments on three benchmark datasets and extensive analysis demonstrate that\nour model has great competitiveness compared to other models with four metrics.\nOur model can effectively incorporate information from temporal knowledge\ngraphs into the language models.", "published": "2023-05-13 12:53:11", "link": "http://arxiv.org/abs/2305.07912v2", "categories": ["cs.CL", "cs.AI", "I.2.4; I.2.7"], "primary_category": "cs.CL"}
{"title": "AMTSS: An Adaptive Multi-Teacher Single-Student Knowledge Distillation\n  Framework For Multilingual Language Inference", "abstract": "Knowledge distillation is of key importance to launching multilingual\npre-trained language models for real applications. To support cost-effective\nlanguage inference in multilingual settings, we propose AMTSS, an adaptive\nmulti-teacher single-student distillation framework, which allows distilling\nknowledge from multiple teachers to a single student. We first introduce an\nadaptive learning strategy and teacher importance weight, which enables a\nstudent to effectively learn from max-margin teachers and easily adapt to new\nlanguages. Moreover, we present a shared student encoder with different\nprojection layers in support of multiple languages, which contributes to\nlargely reducing development and machine cost. Experimental results show that\nAMTSS gains competitive results on the public XNLI dataset and the realistic\nindustrial dataset AliExpress (AE) in the E-commerce scenario.", "published": "2023-05-13 14:42:30", "link": "http://arxiv.org/abs/2305.07928v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Trillion Dollar Words: A New Financial Dataset, Task & Market Analysis", "abstract": "Monetary policy pronouncements by Federal Open Market Committee (FOMC) are a\nmajor driver of financial market returns. We construct the largest tokenized\nand annotated dataset of FOMC speeches, meeting minutes, and press conference\ntranscripts in order to understand how monetary policy influences financial\nmarkets. In this study, we develop a novel task of hawkish-dovish\nclassification and benchmark various pre-trained language models on the\nproposed dataset. Using the best-performing model (RoBERTa-large), we construct\na measure of monetary policy stance for the FOMC document release days. To\nevaluate the constructed measure, we study its impact on the treasury market,\nstock market, and macroeconomic indicators. Our dataset, models, and code are\npublicly available on Huggingface and GitHub under CC BY-NC 4.0 license.", "published": "2023-05-13 17:32:39", "link": "http://arxiv.org/abs/2305.07972v1", "categories": ["cs.CL", "q-fin.CP"], "primary_category": "cs.CL"}
{"title": "CodeT5+: Open Code Large Language Models for Code Understanding and\n  Generation", "abstract": "Large language models (LLMs) pretrained on vast source code have achieved\nprominent progress in code intelligence. However, existing code LLMs have two\nmain limitations in terms of architecture and pretraining tasks. First, they\noften adopt a specific architecture (encoder-only or decoder-only) or rely on a\nunified encoder-decoder network for different downstream tasks. The former\nparadigm is limited by inflexibility in applications while in the latter, the\nmodel is treated as a single system for all tasks, leading to suboptimal\nperformance on a subset of tasks. Secondly, they often employ a limited set of\npretraining objectives which might not be relevant to some downstream tasks and\nhence result in substantial performance degrade. To address these limitations,\nwe propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which\ncomponent modules can be flexibly combined to suit a wide range of downstream\ncode tasks. Such flexibility is enabled by our proposed mixture of pretraining\nobjectives to mitigate the pretrain-finetune discrepancy. These objectives\ncover span denoising, contrastive learning, text-code matching, and causal LM\npretraining tasks, on both unimodal and bimodal multilingual code corpora.\nFurthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs\nwithout training from scratch to efficiently scale up our models, and explore\ninstruction-tuning to align with natural language instructions. We extensively\nevaluate CodeT5+ on over 20 code-related benchmarks in different settings,\nincluding zero-shot, finetuning, and instruction-tuning. We observe\nstate-of-the-art (SoTA) model performance on various code-related tasks, such\nas code generation and completion, math programming, and text-to-code retrieval\ntasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA\nresults on HumanEval code generation task against other open code LLMs.", "published": "2023-05-13 14:23:07", "link": "http://arxiv.org/abs/2305.07922v2", "categories": ["cs.CL", "cs.LG", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models in Conversational Recommender Systems", "abstract": "A Conversational Recommender System (CRS) offers increased transparency and\ncontrol to users by enabling them to engage with the system through a real-time\nmulti-turn dialogue. Recently, Large Language Models (LLMs) have exhibited an\nunprecedented ability to converse naturally and incorporate world knowledge and\ncommon-sense reasoning into language understanding, unlocking the potential of\nthis paradigm. However, effectively leveraging LLMs within a CRS introduces new\ntechnical challenges, including properly understanding and controlling a\ncomplex conversation and retrieving from external sources of information. These\nissues are exacerbated by a large, evolving item corpus and a lack of\nconversational data for training. In this paper, we provide a roadmap for\nbuilding an end-to-end large-scale CRS using LLMs. In particular, we propose\nnew implementations for user preference understanding, flexible dialogue\nmanagement and explainable recommendations as part of an integrated\narchitecture powered by LLMs. For improved personalization, we describe how an\nLLM can consume interpretable natural language user profiles and use them to\nmodulate session-level context. To overcome conversational data limitations in\nthe absence of an existing production CRS, we propose techniques for building a\ncontrollable LLM-based user simulator to generate synthetic conversations. As a\nproof of concept we introduce RecLLM, a large-scale CRS for YouTube videos\nbuilt on LaMDA, and demonstrate its fluency and diverse functionality through\nsome illustrative example conversations.", "published": "2023-05-13 16:40:07", "link": "http://arxiv.org/abs/2305.07961v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Beyond the Safeguards: Exploring the Security Risks of ChatGPT", "abstract": "The increasing popularity of large language models (LLMs) such as ChatGPT has\nled to growing concerns about their safety, security risks, and ethical\nimplications. This paper aims to provide an overview of the different types of\nsecurity risks associated with ChatGPT, including malicious text and code\ngeneration, private data disclosure, fraudulent services, information\ngathering, and producing unethical content. We present an empirical study\nexamining the effectiveness of ChatGPT's content filters and explore potential\nways to bypass these safeguards, demonstrating the ethical implications and\nsecurity risks that persist in LLMs even when protections are in place. Based\non a qualitative analysis of the security implications, we discuss potential\nstrategies to mitigate these risks and inform researchers, policymakers, and\nindustry professionals about the complex security challenges posed by LLMs like\nChatGPT. This study contributes to the ongoing discussion on the ethical and\nsecurity implications of LLMs, underscoring the need for continued research in\nthis area.", "published": "2023-05-13 21:01:14", "link": "http://arxiv.org/abs/2305.08005v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CR"}
{"title": "The Whole Is Greater than the Sum of Its Parts: Improving Music Source\n  Separation by Bridging Network", "abstract": "This paper presents the crossing scheme (X-scheme) for improving the\nperformance of deep neural network (DNN)-based music source separation (MSS)\nwith almost no increasing calculation cost. It consists of three components:\n(i) multi-domain loss (MDL), (ii) bridging operation, which couples the\nindividual instrument networks, and (iii) combination loss (CL). MDL enables\nthe taking advantage of the frequency- and time-domain representations of audio\nsignals. We modify the target network, i.e., the network architecture of the\noriginal DNN-based MSS, by adding bridging paths for each output instrument to\nshare their information. MDL is then applied to the combinations of the output\nsources as well as each independent source; hence, we called it CL. MDL and CL\ncan easily be applied to many DNN-based separation methods as they are merely\nloss functions that are only used during training and do not affect the\ninference step. Bridging operation does not increase the number of learnable\nparameters in the network. Experimental results showed that the validity of\nOpen-Unmix (UMX), densely connected dilated DenseNet (D3Net) and convolutional\ntime-domain audio separation network (Conv-TasNet) extended with our X-scheme,\nrespectively called X-UMX, X-D3Net and X-Conv-TasNet, by comparing them with\ntheir original versions. We also verified the effectiveness of X-scheme in a\nlarge-scale data regime, showing its generality with respect to data size.\nX-UMX Large (X-UMXL), which was trained on large-scale internal data and used\nin our experiments, is newly available at\nhttps://github.com/asteroid-team/asteroid/tree/master/egs/musdb18/X-UMX.", "published": "2023-05-13 07:28:03", "link": "http://arxiv.org/abs/2305.07855v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Higher-Order Frequency Modulation Synthesis", "abstract": "Frequency modulation (FM) and phase modulation (PM) are well-known synthesis\nmethods, which have been deployed widely in musical instruments. More recently,\nsome synthesisers have implemented direct forms of FM (as opposed to PM),\nallowing, at least as part of their design, for higher-order modulation\ntopologies. However, such implementations are affected by well-known\ndifficulties that arise in the modulation of frequency, which are normally\nsolved by the use of PM. In this article, we analyse these problems and using a\ndirect comparison with PM, we put forward a solution for the direct application\nof FM in higher-order modulation arrangements. We begin by reviewing the theory\nof first-order FM, contrasting it to PM. We then proceed to develop a\nformulation of second-order FM which is equivalent to the issue-free PM\nsynthesis, and present a closed-form expression for the evaluation of the\nsecond-order FM spectrum. We then extend the principle to higher-order\ntopologies, by advancing the concept of an FM operator, analogous to the one\nused in PM instrument designs. From this we demonstrate that feedback FM is\nalso a practical possibility. Finally, we complement the paper by giving a\nreference implementation in C++.", "published": "2023-05-13 12:25:01", "link": "http://arxiv.org/abs/2305.07909v1", "categories": ["cs.SD", "eess.AS", "94A12", "J.5"], "primary_category": "cs.SD"}
{"title": "APNet: An All-Frame-Level Neural Vocoder Incorporating Direct Prediction\n  of Amplitude and Phase Spectra", "abstract": "This paper presents a novel neural vocoder named APNet which reconstructs\nspeech waveforms from acoustic features by predicting amplitude and phase\nspectra directly. The APNet vocoder is composed of an amplitude spectrum\npredictor (ASP) and a phase spectrum predictor (PSP). The ASP is a residual\nconvolution network which predicts frame-level log amplitude spectra from\nacoustic features. The PSP also adopts a residual convolution network using\nacoustic features as input, then passes the output of this network through two\nparallel linear convolution layers respectively, and finally integrates into a\nphase calculation formula to estimate frame-level phase spectra. Finally, the\noutputs of ASP and PSP are combined to reconstruct speech waveforms by inverse\nshort-time Fourier transform (ISTFT). All operations of the ASP and PSP are\nperformed at the frame level. We train the ASP and PSP jointly and define\nmultilevel loss functions based on amplitude mean square error, phase\nanti-wrapping error, short-time spectral inconsistency error and time domain\nreconstruction error. Experimental results show that our proposed APNet vocoder\nachieves an approximately 8x faster inference speed than HiFi-GAN v1 on a CPU\ndue to the all-frame-level operations, while its synthesized speech quality is\ncomparable to HiFi-GAN v1. The synthesized speech quality of the APNet vocoder\nis also better than that of several equally efficient models. Ablation\nexperiments also confirm that the proposed parallel phase estimation\narchitecture is essential to phase modeling and the proposed loss functions are\nhelpful for improving the synthesized speech quality.", "published": "2023-05-13 15:51:26", "link": "http://arxiv.org/abs/2305.07952v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Vocal Style Factorization for Effective Speaker Recognition in Affective\n  Scenarios", "abstract": "The accuracy of automated speaker recognition is negatively impacted by\nchange in emotions in a person's speech. In this paper, we hypothesize that\nspeaker identity is composed of various vocal style factors that may be learned\nfrom unlabeled data and re-combined using a neural network to generate a\nholistic speaker identity representation for affective scenarios. In this\nregard, we propose the E-Vector architecture, composed of a 1-D CNN for\nlearning speaker identity features and a vocal style factorization technique\nfor determining vocal styles. Experiments conducted on the MSP-Podcast dataset\ndemonstrate that the proposed architecture improves state-of-the-art speaker\nrecognition accuracy in the affective domain over baseline ECAPA-TDNN speaker\nrecognition models. For instance, the true match rate at a false match rate of\n1% improves from 27.6% to 46.2%.", "published": "2023-05-13 20:35:49", "link": "http://arxiv.org/abs/2305.07997v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Description and Discussion on DCASE 2023 Challenge Task 2: First-Shot\n  Unsupervised Anomalous Sound Detection for Machine Condition Monitoring", "abstract": "We present the task description of the Detection and Classification of\nAcoustic Scenes and Events (DCASE) 2023 Challenge Task 2: ``First-shot\nunsupervised anomalous sound detection (ASD) for machine condition\nmonitoring''. The main goal is to enable rapid deployment of ASD systems for\nnew kinds of machines without the need for hyperparameter tuning. In the past\nASD tasks, developed methods tuned hyperparameters for each machine type, as\nthe development and evaluation datasets had the same machine types. However,\ncollecting normal and anomalous data as the development dataset can be\ninfeasible in practice. In 2023 Task 2, we focus on solving the first-shot\nproblem, which is the challenge of training a model on a completely novel\nmachine type. Specifically, (i) each machine type has only one section (a\nsubset of machine type) and (ii) machine types in the development and\nevaluation datasets are completely different. Analysis of 86 submissions from\n23 teams revealed that the keys to outperform baselines were: 1) sampling\ntechniques for dealing with class imbalances across different domains and\nattributes, 2) generation of synthetic samples for robust detection, and 3) use\nof multiple large pre-trained models to extract meaningful embeddings for the\nanomaly detector.", "published": "2023-05-13 03:09:38", "link": "http://arxiv.org/abs/2305.07828v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound-to-Vibration Transformation for Sensorless Motor Health Monitoring", "abstract": "Automatic sensor-based detection of motor failures such as bearing faults is\ncrucial for predictive maintenance in various industries. Numerous\nmethodologies have been developed over the years to detect bearing faults.\nDespite the appearance of numerous different approaches for diagnosing faults\nin motors have been proposed, vibration-based methods have become the de facto\nstandard and the most commonly used techniques. However, acquiring reliable\nvibration signals, especially from rotating machinery, can sometimes be\ninfeasibly difficult due to challenging installation and operational conditions\n(e.g., variations on accelerometer locations on the motor body), which will not\nonly alter the signal patterns significantly but may also induce severe\nartifacts. Moreover, sensors are costly and require periodic maintenance to\nsustain a reliable signal acquisition. To address these drawbacks and void the\nneed for vibration sensors, in this study, we propose a novel\nsound-to-vibration transformation method that can synthesize realistic\nvibration signals directly from the sound measurements regardless of the\nworking conditions, fault type, and fault severity. As a result, using this\ntransformation, the data acquired by a simple sound recorder, e.g., a mobile\nphone, can be transformed into the vibration signal, which can then be used for\nfault detection by a pre-trained model. The proposed method is extensively\nevaluated over the benchmark Qatar University Dual-Machine Bearing Fault\nBenchmark dataset (QU-DMBF), which encapsulates sound and vibration data from\ntwo different machines operating under various conditions. Experimental results\nshow that this novel approach can synthesize such realistic vibration signals\nthat can directly be used for reliable and highly accurate motor health\nmonitoring.", "published": "2023-05-13 16:37:18", "link": "http://arxiv.org/abs/2305.07960v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by\n  Leveraging Lightweight All-ConvNet and Transfer Learning", "abstract": "Gesture recognition using low-resolution instantaneous HD-sEMG images opens\nup new avenues for the development of more fluid and natural muscle-computer\ninterfaces. However, the data variability between inter-session and\ninter-subject scenarios presents a great challenge. The existing approaches\nemployed very large and complex deep ConvNet or 2SRNN-based domain adaptation\nmethods to approximate the distribution shift caused by these inter-session and\ninter-subject data variability. Hence, these methods also require learning over\nmillions of training parameters and a large pre-trained and target domain\ndataset in both the pre-training and adaptation stages. As a result, it makes\nhigh-end resource-bounded and computationally very expensive for deployment in\nreal-time applications. To overcome this problem, we propose a lightweight\nAll-ConvNet+TL model that leverages lightweight All-ConvNet and transfer\nlearning (TL) for the enhancement of inter-session and inter-subject gesture\nrecognition performance. The All-ConvNet+TL model consists solely of\nconvolutional layers, a simple yet efficient framework for learning invariant\nand discriminative representations to address the distribution shifts caused by\ninter-session and inter-subject data variability. Experiments on four datasets\ndemonstrate that our proposed methods outperform the most complex existing\napproaches by a large margin and achieve state-of-the-art results on\ninter-session and inter-subject scenarios and perform on par or competitively\non intra-session gesture recognition. These performance gaps increase even more\nwhen a tiny amount (e.g., a single trial) of data is available on the target\ndomain for adaptation. These outstanding experimental results provide evidence\nthat the current state-of-the-art models may be overparameterized for\nsEMG-based inter-session and inter-subject gesture recognition tasks.", "published": "2023-05-13 21:47:55", "link": "http://arxiv.org/abs/2305.08014v3", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.CV"}
