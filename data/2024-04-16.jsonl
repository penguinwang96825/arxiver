{"title": "Generative Text Steganography with Large Language Model", "abstract": "Recent advances in large language models (LLMs) have blurred the boundary of\nhigh-quality text generation between humans and machines, which is favorable\nfor generative text steganography. While, current advanced steganographic\nmapping is not suitable for LLMs since most users are restricted to accessing\nonly the black-box API or user interface of the LLMs, thereby lacking access to\nthe training vocabulary and its sampling probabilities. In this paper, we\nexplore a black-box generative text steganographic method based on the user\ninterfaces of large language models, which is called LLM-Stega. The main goal\nof LLM-Stega is that the secure covert communication between Alice (sender) and\nBob (receiver) is conducted by using the user interfaces of LLMs. Specifically,\nWe first construct a keyword set and design a new encrypted steganographic\nmapping to embed secret messages. Furthermore, to guarantee accurate extraction\nof secret messages and rich semantics of generated stego texts, an optimization\nmechanism based on reject sampling is proposed. Comprehensive experiments\ndemonstrate that the proposed LLM-Stega outperforms current state-of-the-art\nmethods.", "published": "2024-04-16 02:19:28", "link": "http://arxiv.org/abs/2404.10229v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Low-Resource Health Coaching Dialogues via Neuro-Symbolic Goal\n  Summarization and Text-Units-Text Generation", "abstract": "Health coaching helps patients achieve personalized and lifestyle-related\ngoals, effectively managing chronic conditions and alleviating mental health\nissues. It is particularly beneficial, however cost-prohibitive, for\nlow-socioeconomic status populations due to its highly personalized and\nlabor-intensive nature. In this paper, we propose a neuro-symbolic goal\nsummarizer to support health coaches in keeping track of the goals and a\ntext-units-text dialogue generation model that converses with patients and\nhelps them create and accomplish specific goals for physical activities. Our\nmodels outperform previous state-of-the-art while eliminating the need for\npredefined schema and corresponding annotation. We also propose a new health\ncoaching dataset extending previous work and a metric to measure the\nunconventionality of the patient's response based on data difficulty,\nfacilitating potential coach alerts during deployment.", "published": "2024-04-16 03:46:30", "link": "http://arxiv.org/abs/2404.10268v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Balancing Speciality and Versatility: A Coarse to Fine Framework for\n  Mitigating Catastrophic Forgetting in Large Language Models", "abstract": "Aligned Large Language Models (LLMs) showcase remarkable versatility, capable\nof handling diverse real-world tasks. Meanwhile, aligned LLMs are also expected\nto exhibit speciality, excelling in specific applications. However, fine-tuning\nwith extra data, a common practice to gain speciality, often leads to\ncatastrophic forgetting (CF) of previously acquired versatility, hindering the\nmodel's performance across diverse tasks. In response to this challenge, we\npropose CoFiTune, a coarse to fine framework in an attempt to strike the\nbalance between speciality and versatility. At the coarse-grained level, an\nempirical tree-search algorithm is utilized to pinpoint and update specific\nmodules that are crucial for speciality, while keeping other parameters frozen;\nat the fine-grained level, a soft-masking mechanism regulates the update to the\nLLMs, mitigating the CF issue without harming speciality. In an overall\nevaluation of both speciality and versatility, CoFiTune consistently\noutperforms baseline methods across diverse tasks and model scales. Compared to\nthe full-parameter SFT, CoFiTune leads to about 14% versatility improvement and\nmarginal speciality loss on a 13B model. Lastly, based on further analysis, we\nprovide a speculative insight into the information forwarding process in LLMs,\nwhich helps explain the effectiveness of the proposed method. The code is\navailable at https://github.com/rattlesnakey/CoFiTune.", "published": "2024-04-16 06:27:39", "link": "http://arxiv.org/abs/2404.10306v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Confidence Expression in Large Language Models Through\n  Learning from Past Experience", "abstract": "Large Language Models (LLMs) have exhibited remarkable performance across\nvarious downstream tasks, but they may generate inaccurate or false information\nwith a confident tone. One of the possible solutions is to empower the LLM\nconfidence expression capability, in which the confidence expressed can be\nwell-aligned with the true probability of the generated answer being correct.\nHowever, leveraging the intrinsic ability of LLMs or the signals from the\noutput logits of answers proves challenging in accurately capturing the\nresponse uncertainty in LLMs. Therefore, drawing inspiration from cognitive\ndiagnostics, we propose a method of Learning from Past experience (LePe) to\nenhance the capability for confidence expression. Specifically, we first\nidentify three key problems: (1) How to capture the inherent confidence of the\nLLM? (2) How to teach the LLM to express confidence? (3) How to evaluate the\nconfidence expression of the LLM? Then we devise three stages in LePe to deal\nwith these problems. Besides, to accurately capture the confidence of an LLM\nwhen constructing the training data, we design a complete pipeline including\nquestion preparation and answer sampling. We also conduct experiments using the\nLlama family of LLMs to verify the effectiveness of our proposed method on four\ndatasets.", "published": "2024-04-16 06:47:49", "link": "http://arxiv.org/abs/2404.10315v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Explore: Enhancing Mathematical Reasoning in Language Models with\n  Fine-grained Rewards", "abstract": "Training on large amounts of rationales (i.e., CoT Fine-tuning) is effective\nat improving the reasoning capabilities of large language models (LLMs).\nHowever, acquiring human-authored rationales or augmenting rationales from\nproprietary models is costly and not scalable. In this paper, we study the\nproblem of whether LLMs could self-improve their reasoning capabilities. To\nthis end, we propose Self-Explore, where the LLM is tasked to explore the first\nwrong step (i.e., the first pit) within the rationale and use such signals as\nfine-grained rewards for further improvement. On the GSM8K and MATH test set,\nSelf-Explore achieves 11.57% and 2.89% improvement on average across three LLMs\ncompared to supervised fine-tuning (SFT). Our code is available at\nhttps://github.com/hbin0701/Self-Explore.", "published": "2024-04-16 07:30:11", "link": "http://arxiv.org/abs/2404.10346v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversations as a Source for Teaching Scientific Concepts at Different\n  Education Levels", "abstract": "Open conversations are one of the most engaging forms of teaching. However,\ncreating those conversations in educational software is a complex endeavor,\nespecially if we want to address the needs of different audiences. While\nlanguage models hold great promise for educational applications, there are\nsubstantial challenges in training them to engage in meaningful and effective\nconversational teaching, especially when considering the diverse needs of\nvarious audiences. No official data sets exist for this task to facilitate the\ntraining of language models for conversational teaching, considering the\ndiverse needs of various audiences. This paper presents a novel source for\nfacilitating conversational teaching of scientific concepts at various\ndifficulty levels (from preschooler to expert), namely dialogues taken from\nvideo transcripts. We analyse this data source in various ways to show that it\noffers a diverse array of examples that can be used to generate contextually\nappropriate and natural responses to scientific topics for specific target\naudiences. It is a freely available valuable resource for training and\nevaluating conversation models, encompassing organically occurring dialogues.\nWhile the raw data is available online, we provide additional metadata for\nconversational analysis of dialogues at each level in all available videos.", "published": "2024-04-16 11:33:36", "link": "http://arxiv.org/abs/2404.10475v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ViTextVQA: A Large-Scale Visual Question Answering Dataset for\n  Evaluating Vietnamese Text Comprehension in Images", "abstract": "Visual Question Answering (VQA) is a complicated task that requires the\ncapability of simultaneously processing natural language and images. Initially,\nthis task was researched, focusing on methods to help machines understand\nobjects and scene contexts in images. However, some text appearing in the image\nthat carries explicit information about the full content of the image is not\nmentioned. Along with the continuous development of the AI era, there have been\nmany studies on the reading comprehension ability of VQA models in the world.\nAs a developing country, conditions are still limited, and this task is still\nopen in Vietnam. Therefore, we introduce the first large-scale dataset in\nVietnamese specializing in the ability to understand text appearing in images,\nwe call it ViTextVQA (\\textbf{Vi}etnamese \\textbf{Text}-based \\textbf{V}isual\n\\textbf{Q}uestion \\textbf{A}nswering dataset) which contains \\textbf{over\n16,000} images and \\textbf{over 50,000} questions with answers. Through\nmeticulous experiments with various state-of-the-art models, we uncover the\nsignificance of the order in which tokens in OCR text are processed and\nselected to formulate answers. This finding helped us significantly improve the\nperformance of the baseline models on the ViTextVQA dataset. Our dataset is\navailable at this\n\\href{https://github.com/minhquan6203/ViTextVQA-Dataset}{link} for research\npurposes.", "published": "2024-04-16 15:28:30", "link": "http://arxiv.org/abs/2404.10652v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Integrating knowledge bases to improve coreference and bridging\n  resolution for the chemical domain", "abstract": "Resolving coreference and bridging relations in chemical patents is important\nfor better understanding the precise chemical process, where chemical domain\nknowledge is very critical. We proposed an approach incorporating external\nknowledge into a multi-task learning model for both coreference and bridging\nresolution in the chemical domain. The results show that integrating external\nknowledge can benefit both chemical coreference and bridging resolution.", "published": "2024-04-16 16:15:19", "link": "http://arxiv.org/abs/2404.10696v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study", "abstract": "Reinforcement Learning from Human Feedback (RLHF) is currently the most\nwidely used method to align large language models (LLMs) with human\npreferences. Existing RLHF methods can be roughly categorized as either\nreward-based or reward-free. Novel applications such as ChatGPT and Claude\nleverage reward-based methods that first learn a reward model and apply\nactor-critic algorithms, such as Proximal Policy Optimization (PPO). However,\nin academic benchmarks, state-of-the-art results are often achieved via\nreward-free methods, such as Direct Preference Optimization (DPO). Is DPO truly\nsuperior to PPO? Why does PPO perform poorly on these benchmarks? In this\npaper, we first conduct both theoretical and empirical studies on the\nalgorithmic properties of DPO and show that DPO may have fundamental\nlimitations. Moreover, we also comprehensively examine PPO and reveal the key\nfactors for the best performances of PPO in fine-tuning LLMs. Finally, we\nbenchmark DPO and PPO across a collection of RLHF testbeds, ranging from\ndialogue to code generation. Experiment results demonstrate that PPO is able to\nsurpass other alignment methods in all cases and achieve state-of-the-art\nresults in challenging code competitions. Our code is publicly available at\nhttps://github.com/openpsi-project/ReaLHF.", "published": "2024-04-16 16:51:53", "link": "http://arxiv.org/abs/2404.10719v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "D3CODE: Disentangling Disagreements in Data across Cultures on\n  Offensiveness Detection and Evaluation", "abstract": "While human annotations play a crucial role in language technologies,\nannotator subjectivity has long been overlooked in data collection. Recent\nstudies that have critically examined this issue are often situated in the\nWestern context, and solely document differences across age, gender, or racial\ngroups. As a result, NLP research on subjectivity have overlooked the fact that\nindividuals within demographic groups may hold diverse values, which can\ninfluence their perceptions beyond their group norms. To effectively\nincorporate these considerations into NLP pipelines, we need datasets with\nextensive parallel annotations from various social and cultural groups. In this\npaper we introduce the \\dataset dataset: a large-scale cross-cultural dataset\nof parallel annotations for offensive language in over 4.5K sentences annotated\nby a pool of over 4k annotators, balanced across gender and age, from across 21\ncountries, representing eight geo-cultural regions. The dataset contains\nannotators' moral values captured along six moral foundations: care, equality,\nproportionality, authority, loyalty, and purity. Our analyses reveal\nsubstantial regional variations in annotators' perceptions that are shaped by\nindividual moral values, offering crucial insights for building pluralistic,\nculturally sensitive NLP models.", "published": "2024-04-16 19:12:03", "link": "http://arxiv.org/abs/2404.10857v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incubating Text Classifiers Following User Instruction with Nothing but\n  LLM", "abstract": "In this paper, we aim to generate text classification data given arbitrary\nclass definitions (i.e., user instruction), so one can train a small text\nclassifier without any human annotation or raw corpus. Compared with pioneer\nattempts, our proposed Incubator is the first framework that can handle\ncomplicated and even mutually dependent classes (e.g., \"TED Talk given by\nEducator\" and \"Other\"). Specifically, Incubator is an LLM firstly tuned on the\ninstruction-to-data mappings that we obtained from classification datasets and\ndescriptions on HuggingFace together with in-context augmentation by GPT-4. We\nthen refine Incubator by learning on the cluster centers of semantic textual\nembeddings to emphasize the uniformity and semantic diversity in generations.\nWe compare Incubator on various classification tasks with strong baselines such\nas direct LLM-based inference and training data generation by prompt\nengineering. Experiments show Incubator is able to (1) perform well on\ntraditional benchmarks, (2) take label dependency and user preference into\nconsideration, and (3) enable logical text mining by incubating multiple\nclassifiers.", "published": "2024-04-16 19:53:35", "link": "http://arxiv.org/abs/2404.10877v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grounded Language Agent for Product Search via Intelligent Web\n  Interactions", "abstract": "The development of agents powered by large language models (LLMs) to\naccomplish complex high-level user intents, has attracted significant attention\nrecently. However, employing LLMs with billions of parameters (e.g., GPT-4) may\nincur substantial costs on top of handcrafting extensive prompts. To address\nthis, we introduce a Grounded Language Agent for Intelligent Web Interactions,\nnamed GLAINTEL. GLAINTEL employs Flan-T5 as its backbone and is flexible in\ntraining in various settings: unsupervised learning, supervised learning, and\nunsupervised domain adaptation. Specifically, we tackle both the challenge of\nlearning without human demonstrations and the opportunity to leverage human\ndemonstrations effectively when those are available. Additionally, we explore\nunsupervised domain adaptation for cases where demonstrations are limited to a\nspecific domain. Experimental evaluations across diverse setups demonstrate the\neffectiveness of GLAINTEL in unsupervised settings, outperforming in-context\nlearning-based approaches that employ larger models with up to 540 billion\nparameters. Surprisingly, behavioral cloning-based methods that\nstraightforwardly use human demonstrations do not outperform unsupervised\nvariants of GLAINTEL. Additionally, we show that combining human demonstrations\nwith reinforcement learning-based training yields results comparable to methods\nutilizing GPT-4. The code is available at:\nhttps://github.com/MultifacetedNLP/WebAgents-Unsupervised.", "published": "2024-04-16 20:15:32", "link": "http://arxiv.org/abs/2404.10887v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Which questions should I answer? Salience Prediction of Inquisitive\n  Questions", "abstract": "Inquisitive questions -- open-ended, curiosity-driven questions people ask as\nthey read -- are an integral part of discourse processing (Kehler and Rohde,\n2017; Onea, 2016) and comprehension (Prince, 2004). Recent work in NLP has\ntaken advantage of question generation capabilities of LLMs to enhance a wide\nrange of applications. But the space of inquisitive questions is vast: many\nquestions can be evoked from a given context. So which of those should be\nprioritized to find answers? Linguistic theories, unfortunately, have not yet\nprovided an answer to this question. This paper presents QSALIENCE, a salience\npredictor of inquisitive questions. QSALIENCE is instruction-tuned over our\ndataset of linguist-annotated salience scores of 1,766 (context, question)\npairs. A question scores high on salience if answering it would greatly enhance\nthe understanding of the text (Van Rooy, 2003). We show that highly salient\nquestions are empirically more likely to be answered in the same article,\nbridging potential questions (Onea, 2016) with Questions Under Discussion\n(Roberts, 2012). We further validate our findings by showing that answering\nsalient questions is an indicator of summarization quality in news.", "published": "2024-04-16 21:33:05", "link": "http://arxiv.org/abs/2404.10917v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "More Room for Language: Investigating the Effect of Retrieval on\n  Language Models", "abstract": "Retrieval-augmented language models pose a promising alternative to standard\nlanguage modeling. During pretraining, these models search in a corpus of\ndocuments for contextually relevant information that could aid the language\nmodeling objective. We introduce an 'ideal retrieval' methodology to study\nthese models in a fully controllable setting. We conduct an extensive\nevaluation to examine how retrieval augmentation affects the behavior of the\nunderlying language model. Among other things, we observe that these models: i)\nsave substantially less world knowledge in their weights, ii) are better at\nunderstanding local context and inter-word dependencies, but iii) are worse at\ncomprehending global context.", "published": "2024-04-16 22:43:48", "link": "http://arxiv.org/abs/2404.10939v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ClashEval: Quantifying the tug-of-war between an LLM's internal prior\n  and external evidence", "abstract": "Retrieval augmented generation (RAG) is frequently used to mitigate\nhallucinations and provide up-to-date knowledge for large language models\n(LLMs). However, given that document retrieval is an imprecise task and\nsometimes results in erroneous or even harmful content being presented in\ncontext, this raises the question of how LLMs handle retrieved information: If\nthe provided content is incorrect, does the model know to ignore it, or does it\nrecapitulate the error? Conversely, when the model's initial response is\nincorrect, does it always know to use the retrieved information to correct\nitself, or does it insist on its wrong prior response? To answer this, we\ncurate a dataset of over 1200 questions across six domains (e.g., drug dosages,\nOlympic records, locations) along with content relevant to answering each\nquestion. We further apply precise perturbations to the answers in the content\nthat range from subtle to blatant errors. We benchmark six top-performing LLMs,\nincluding GPT-4o, on this dataset and find that LLMs are susceptible to\nadopting incorrect retrieved content, overriding their own correct prior\nknowledge over 60% of the time. However, the more unrealistic the retrieved\ncontent is (i.e. more deviated from truth), the less likely the model is to\nadopt it. Also, the less confident a model is in its initial response (via\nmeasuring token probabilities), the more likely it is to adopt the information\nin the retrieved content. We exploit this finding and demonstrate simple\nmethods for improving model accuracy where there is conflicting retrieved\ncontent. Our results highlight a difficult task and benchmark for LLMs --\nnamely, their ability to correctly discern when it is wrong in light of correct\nretrieved content and to reject cases when the provided content is incorrect.", "published": "2024-04-16 00:43:03", "link": "http://arxiv.org/abs/2404.10198v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CULTURE-GEN: Revealing Global Cultural Perception in Language Models\n  through Natural Language Prompting", "abstract": "As the utilization of large language models (LLMs) has proliferated\nworld-wide, it is crucial for them to have adequate knowledge and fair\nrepresentation for diverse global cultures. In this work, we uncover culture\nperceptions of three SOTA models on 110 countries and regions on 8\nculture-related topics through culture-conditioned generations, and extract\nsymbols from these generations that are associated to each culture by the LLM.\nWe discover that culture-conditioned generation consist of linguistic \"markers\"\nthat distinguish marginalized cultures apart from default cultures. We also\ndiscover that LLMs have an uneven degree of diversity in the culture symbols,\nand that cultures from different geographic regions have different presence in\nLLMs' culture-agnostic generation. Our findings promote further research in\nstudying the knowledge and fairness of global culture perception in LLMs. Code\nand Data can be found here: https://github.com/huihanlhh/Culture-Gen/", "published": "2024-04-16 00:50:43", "link": "http://arxiv.org/abs/2404.10199v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical\n  Vision-Language Models", "abstract": "Recent advancements in general-purpose or domain-specific multimodal large\nlanguage models (LLMs) have witnessed remarkable progress for medical\ndecision-making. However, they are designated for specific classification or\ngenerative tasks, and require model training or finetuning on large-scale\ndatasets with sizeable parameters and tremendous computing, hindering their\nclinical utility across diverse resource-constrained scenarios in practice. In\nthis paper, we propose a novel and lightweight framework Med-MoE\n(Mixture-of-Experts) that tackles both discriminative and generative multimodal\nmedical tasks. The learning of Med-MoE consists of three steps: multimodal\nmedical alignment, instruction tuning and routing, and domain-specific MoE\ntuning. After aligning multimodal medical images with LLM tokens, we then\nenable the model for different multimodal medical tasks with instruction\ntuning, together with a trainable router tailored for expert selection across\ninput modalities. Finally, the model is tuned by integrating the router with\nmultiple domain-specific experts, which are selectively activated and further\nempowered by meta expert. Comprehensive experiments on both open- and close-end\nmedical question answering (Med-VQA) and image classification tasks across\ndatasets such as VQA-RAD, SLAKE and Path-VQA demonstrate that our model can\nachieve performance superior to or on par with state-of-the-art baselines,\nwhile only requiring approximately 30\\%-50\\% of activated model parameters.\nExtensive analysis and ablations corroborate the effectiveness and practical\nutility of our method.", "published": "2024-04-16 02:35:17", "link": "http://arxiv.org/abs/2404.10237v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Future Language Modeling from Temporal Document History", "abstract": "Predicting the future is of great interest across many aspects of human\nactivity. Businesses are interested in future trends, traders are interested in\nfuture stock prices, and companies are highly interested in future\ntechnological breakthroughs. While there are many automated systems for\npredicting future numerical data, such as weather, stock prices, and demand for\nproducts, there is relatively little work in automatically predicting textual\ndata. Humans are interested in textual data predictions because it is a natural\nformat for our consumption, and experts routinely make predictions in a textual\nformat (Christensen et al., 2004; Tetlock & Gardner, 2015; Frick, 2015).\nHowever, there has been relatively little formalization of this general problem\nin the machine learning or natural language processing communities. To address\nthis gap, we introduce the task of future language modeling: probabilistic\nmodeling of texts in the future based on a temporal history of texts. To our\nknowledge, our work is the first work to formalize the task of predicting the\nfuture in this way. We show that it is indeed possible to build future language\nmodels that improve upon strong non-temporal language model baselines, opening\nthe door to working on this important, and widely applicable problem.", "published": "2024-04-16 05:45:52", "link": "http://arxiv.org/abs/2404.10297v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "MAD Speech: Measures of Acoustic Diversity of Speech", "abstract": "Generative spoken language models produce speech in a wide range of voices,\nprosody, and recording conditions, seemingly approaching the diversity of\nnatural speech. However, the extent to which generated speech is acoustically\ndiverse remains unclear due to a lack of appropriate metrics. We address this\ngap by developing lightweight metrics of acoustic diversity, which we\ncollectively refer to as MAD Speech. We focus on measuring five facets of\nacoustic diversity: voice, gender, emotion, accent, and background noise. We\nconstruct the metrics as a composition of specialized, per-facet embedding\nmodels and an aggregation function that measures diversity within the embedding\nspace. Next, we build a series of datasets with a priori known diversity\npreferences for each facet. Using these datasets, we demonstrate that our\nproposed metrics achieve a stronger agreement with the ground-truth diversity\nthan baselines. Finally, we showcase the applicability of our proposed metrics\nacross several real-life evaluation scenarios. MAD Speech is made publicly\naccessible.", "published": "2024-04-16 09:35:27", "link": "http://arxiv.org/abs/2404.10419v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Language Proficiency and F0 Entrainment: A Study of L2 English Imitation\n  in Italian, French, and Slovak Speakers", "abstract": "This study explores F0 entrainment in second language (L2) English speech\nimitation during an Alternating Reading Task (ART). Participants with Italian,\nFrench, and Slovak native languages imitated English utterances, and their F0\nentrainment was quantified using the Dynamic Time Warping (DTW) distance\nbetween the parameterized F0 contours of the imitated utterances and those of\nthe model utterances. Results indicate a nuanced relationship between L2\nEnglish proficiency and entrainment: speakers with higher proficiency generally\nexhibit less entrainment in pitch variation and declination. However, within\ndyads, the more proficient speakers demonstrate a greater ability to mimic\npitch range, leading to increased entrainment. This suggests that proficiency\ninfluences entrainment differently at individual and dyadic levels,\nhighlighting the complex interplay between language skill and prosodic\nadaptation.", "published": "2024-04-16 10:10:19", "link": "http://arxiv.org/abs/2404.10440v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "DESTEIN: Navigating Detoxification of Language Models via Universal\n  Steering Pairs and Head-wise Activation Fusion", "abstract": "Despite the remarkable achievements of language models (LMs) across a broad\nspectrum of tasks, their propensity for generating toxic outputs remains a\nprevalent concern. Current solutions involving finetuning or auxiliary models\nusually require extensive computational resources, hindering their practicality\nin large language models (LLMs). In this paper, we propose DeStein, a novel\nmethod that detoxifies LMs by applying representation engineering in activation\nspaces with lower resource and time costs. Specifically, we derive\ndetoxification vectors from self-induced, universal steering pairs through\narithmetic operations in activation spaces. During inference, detoxification is\nachieved by fusing the detoxification vectors with the original representations\nin a head-wise manner. Empirical results demonstrate that our method\nsignificantly outperforms previous state-of-the-art approaches on various\nmetrics, while also maintaining satisfactory generation quality and diversity.\nWe further validate the practicality and scalability of DeStein with a series\nof white-box LLMs. The method is open-sourced at\nhttps://github.com/LizLizLi/DeStein. Warning: Some example model outputs may\ncontain highly offensive or disturbing text.", "published": "2024-04-16 11:07:48", "link": "http://arxiv.org/abs/2404.10464v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "When Emotional Stimuli meet Prompt Designing: An Auto-Prompt Graphical\n  Paradigm", "abstract": "With the development of Large Language Models (LLM), numerous prompts have\nbeen proposed, each with a rich set of features and their own merits. This\npaper summarizes the prompt words for large language models (LLMs),\ncategorizing them into stimulating and framework types, and proposes an\nAuto-Prompt Graphical Paradigm(APGP) that combines both stimulating and\nframework prompts to enhance the problem-solving capabilities of LLMs across\nmultiple domains, then exemplifies it with a framework that adheres to this\nparadigm. The framework involves automated prompt generation and consideration\nof emotion-stimulus factors, guiding LLMs in problem abstraction, diversified\nsolutions generation, comprehensive optimization, and self-verification after\nproviding answers, ensuring solution accuracy. Compared to traditional stimuli\nand framework prompts, this framework integrates the advantages of both by\nadopting automated approaches inspired by APE work, overcoming the limitations\nof manually designed prompts. Test results on the ruozhiba and BBH datasets\ndemonstrate that this framework can effectively improve the efficiency and\naccuracy of LLMs in problem-solving, paving the way for new applications of\nLLMs.", "published": "2024-04-16 12:19:08", "link": "http://arxiv.org/abs/2404.10500v1", "categories": ["cs.CL", "cs.AI", "68T20", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Sentiment Analysis of Medical Text Based on Deep Learning", "abstract": "The field of natural language processing (NLP) has made significant progress\nwith the rapid development of deep learning technologies. One of the research\ndirections in text sentiment analysis is sentiment analysis of medical texts,\nwhich holds great potential for application in clinical diagnosis. However, the\nmedical field currently lacks sufficient text datasets, and the effectiveness\nof sentiment analysis is greatly impacted by different model design approaches,\nwhich presents challenges. Therefore, this paper focuses on the medical domain,\nusing bidirectional encoder representations from transformers (BERT) as the\nbasic pre-trained model and experimenting with modules such as convolutional\nneural network (CNN), fully connected network (FCN), and graph convolutional\nnetworks (GCN) at the output layer. Experiments and analyses were conducted on\nthe METS-CoV dataset to explore the training performance after integrating\ndifferent deep learning networks. The results indicate that CNN models\noutperform other networks when trained on smaller medical text datasets in\ncombination with pre-trained models like BERT. This study highlights the\nsignificance of model selection in achieving effective sentiment analysis in\nthe medical domain and provides a reference for future research to develop more\nefficient model architectures.", "published": "2024-04-16 12:20:49", "link": "http://arxiv.org/abs/2404.10503v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unveiling the Misuse Potential of Base Large Language Models via\n  In-Context Learning", "abstract": "The open-sourcing of large language models (LLMs) accelerates application\ndevelopment, innovation, and scientific progress. This includes both base\nmodels, which are pre-trained on extensive datasets without alignment, and\naligned models, deliberately designed to align with ethical standards and human\nvalues. Contrary to the prevalent assumption that the inherent\ninstruction-following limitations of base LLMs serve as a safeguard against\nmisuse, our investigation exposes a critical oversight in this belief. By\ndeploying carefully designed demonstrations, our research demonstrates that\nbase LLMs could effectively interpret and execute malicious instructions. To\nsystematically assess these risks, we introduce a novel set of risk evaluation\nmetrics. Empirical results reveal that the outputs from base LLMs can exhibit\nrisk levels on par with those of models fine-tuned for malicious purposes. This\nvulnerability, requiring neither specialized knowledge nor training, can be\nmanipulated by almost anyone, highlighting the substantial risk and the\ncritical need for immediate attention to the base LLMs' security protocols.", "published": "2024-04-16 13:22:54", "link": "http://arxiv.org/abs/2404.10552v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Construction of Domain-specified Japanese Large Language Model for\n  Finance through Continual Pre-training", "abstract": "Large language models (LLMs) are now widely used in various fields, including\nfinance. However, Japanese financial-specific LLMs have not been proposed yet.\nHence, this study aims to construct a Japanese financial-specific LLM through\ncontinual pre-training. Before tuning, we constructed Japanese\nfinancial-focused datasets for continual pre-training. As a base model, we\nemployed a Japanese LLM that achieved state-of-the-art performance on Japanese\nfinancial benchmarks among the 10-billion-class parameter models. After\ncontinual pre-training using the datasets and the base model, the tuned model\nperformed better than the original model on the Japanese financial benchmarks.\nMoreover, the outputs comparison results reveal that the tuned model's outputs\ntend to be better than the original model's outputs in terms of the quality and\nlength of the answers. These findings indicate that domain-specific continual\npre-training is also effective for LLMs. The tuned model is publicly available\non Hugging Face.", "published": "2024-04-16 13:26:32", "link": "http://arxiv.org/abs/2404.10555v1", "categories": ["cs.CL", "q-fin.CP"], "primary_category": "cs.CL"}
{"title": "The application of Augmented Reality (AR) in Remote Work and Education", "abstract": "With the rapid advancement of technology, Augmented Reality (AR) technology,\nknown for its ability to deeply integrate virtual information with the real\nworld, is gradually transforming traditional work modes and teaching methods.\nParticularly in the realms of remote work and online education, AR technology\ndemonstrates a broad spectrum of application prospects. This paper delves into\nthe application potential and actual effects of AR technology in remote work\nand education. Through a systematic literature review, this study outlines the\nkey features, advantages, and challenges of AR technology. Based on theoretical\nanalysis, it discusses the scientific basis and technical support that AR\ntechnology provides for enhancing remote work efficiency and promoting\ninnovation in educational teaching models. Additionally, by designing an\nempirical research plan and analyzing experimental data, this article reveals\nthe specific performance and influencing factors of AR technology in practical\napplications. Finally, based on the results of the experiments, this research\nsummarizes the application value of AR technology in remote work and education,\nlooks forward to its future development trends, and proposes forward-looking\nresearch directions and strategic suggestions, offering empirical foundation\nand theoretical guidance for further promoting the in-depth application of AR\ntechnology in related fields.", "published": "2024-04-16 14:04:46", "link": "http://arxiv.org/abs/2404.10579v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "HLAT: High-quality Large Language Model Pre-trained on AWS Trainium", "abstract": "Getting large language models (LLMs) to perform well on the downstream tasks\nrequires pre-training over trillions of tokens. This typically demands a large\nnumber of powerful computational devices in addition to a stable distributed\ntraining framework to accelerate the training. The growing number of\napplications leveraging AI/ML led to a scarcity of the expensive conventional\naccelerators (such as GPUs), which emphasizes the need for the alternative\nspecialized-accelerators that are scalable and cost-efficient. AWS Trainium is\nthe second-generation machine learning accelerator purposely built for training\nlarge deep learning models. However, training LLMs with billions of parameters\non AWS Trainium is challenging due to its relatively nascent software\necosystem. In this paper, we showcase HLAT: a family of 7B and 70B decoder-only\nLLMs pre-trained using 4096 AWS Trainium accelerators over 1.8 trillion tokens.\nThe performance of HLAT is benchmarked against popular open source models\nincluding LLaMA and OpenLLaMA, which have been trained on NVIDIA GPUs and\nGoogle TPUs, respectively. On various evaluation tasks, we show that HLAT\nachieves model quality on par with the baselines of similar model size. We also\nopen-source all the training scripts and configurations of HLAT\n(https://github.com/awslabs/HLAT) and share the best practice of using the\nNeuronX Distributed Training (NxDT), a customized distributed training library\nfor AWS Trainium. Our work demonstrates that AWS Trainium powered by NxDT is\nable to successfully pre-train state-of-the-art LLM models with high\nperformance and cost-effectiveness.", "published": "2024-04-16 15:02:46", "link": "http://arxiv.org/abs/2404.10630v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-playing Adversarial Language Game Enhances LLM Reasoning", "abstract": "We explore the potential of self-play training for large language models\n(LLMs) in a two-player adversarial language game called Adversarial Taboo. In\nthis game, an attacker and a defender communicate around a target word only\nvisible to the attacker. The attacker aims to induce the defender to speak the\ntarget word unconsciously, while the defender tries to infer the target word\nfrom the attacker's utterances. To win the game, both players must have\nsufficient knowledge about the target word and high-level reasoning ability to\ninfer and express in this information-reserved conversation. Hence, we are\ncurious about whether LLMs' reasoning ability can be further enhanced by\nSelf-Playing this Adversarial language Game (SPAG). With this goal, we select\nseveral open-source LLMs and let each act as the attacker and play with a copy\nof itself as the defender on an extensive range of target words. Through\nreinforcement learning on the game outcomes, we observe that the LLMs'\nperformances uniformly improve on a broad range of reasoning benchmarks.\nFurthermore, iteratively adopting this self-play process can continuously\npromote LLMs' reasoning abilities. The code is available at\nhttps://github.com/Linear95/SPAG.", "published": "2024-04-16 15:16:22", "link": "http://arxiv.org/abs/2404.10642v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Question Difficulty Ranking for Multiple-Choice Reading Comprehension", "abstract": "Multiple-choice (MC) tests are an efficient method to assess English\nlearners. It is useful for test creators to rank candidate MC questions by\ndifficulty during exam curation. Typically, the difficulty is determined by\nhaving human test takers trial the questions in a pretesting stage. However,\nthis is expensive and not scalable. Therefore, we explore automated approaches\nto rank MC questions by difficulty. However, there is limited data for explicit\ntraining of a system for difficulty scores. Hence, we compare task transfer and\nzero-shot approaches: task transfer adapts level classification and reading\ncomprehension systems for difficulty ranking while zero-shot prompting of\ninstruction finetuned language models contrasts absolute assessment against\ncomparative. It is found that level classification transfers better than\nreading comprehension. Additionally, zero-shot comparative assessment is more\neffective at difficulty ranking than the absolute assessment and even the task\ntransfer approaches at question difficulty ranking with a Spearman's\ncorrelation of 40.4%. Combining the systems is observed to further boost the\ncorrelation.", "published": "2024-04-16 16:23:10", "link": "http://arxiv.org/abs/2404.10704v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Autoregressive Pre-Training on Pixels and Texts", "abstract": "The integration of visual and textual information represents a promising\ndirection in the advancement of language models. In this paper, we explore the\ndual modality of language--both visual and textual--within an autoregressive\nframework, pre-trained on both document images and texts. Our method employs a\nmultimodal training strategy, utilizing visual data through next patch\nprediction with a regression head and/or textual data through next token\nprediction with a classification head. We focus on understanding the\ninteraction between these two modalities and their combined impact on model\nperformance. Our extensive evaluation across a wide range of benchmarks shows\nthat incorporating both visual and textual data significantly improves the\nperformance of pixel-based language models. Remarkably, we find that a\nunidirectional pixel-based model trained solely on visual data can achieve\ncomparable results to state-of-the-art bidirectional models on several language\nunderstanding tasks. This work uncovers the untapped potential of integrating\nvisual and textual modalities for more effective language modeling. We release\nour code, data, and model checkpoints at\n\\url{https://github.com/ernie-research/pixelgpt}.", "published": "2024-04-16 16:36:50", "link": "http://arxiv.org/abs/2404.10710v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents", "abstract": "Recognizing if LLM output can be grounded in evidence is central to many\ntasks in NLP: retrieval-augmented generation, summarization, document-grounded\ndialogue, and more. Current approaches to this kind of fact-checking are based\non verifying each piece of a model generation against potential evidence using\nan LLM. However, this process can be very computationally expensive, requiring\nmany calls to a model to check a single response. In this work, we show how to\nbuild small fact-checking models that have GPT-4-level performance but for 400x\nlower cost. We do this by constructing synthetic training data with GPT-4,\nwhich involves creating realistic yet challenging instances of factual errors\nvia a structured generation procedure. Training on this data teaches models to\ncheck each fact in the claim and recognize synthesis of information across\nsentences. For evaluation, we unify datasets from recent work on fact-checking\nand grounding LLM generations into a new benchmark, LLM-AggreFact. Our best\nsystem MiniCheck-FT5 (770M parameters) outperforms all systems of comparable\nsize and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data\nsynthesis, and models.", "published": "2024-04-16 17:59:10", "link": "http://arxiv.org/abs/2404.10774v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Forcing Diffuse Distributions out of Language Models", "abstract": "Despite being trained specifically to follow user instructions, today's\ninstructiontuned language models perform poorly when instructed to produce\nrandom outputs. For example, when prompted to pick a number uniformly between\none and ten Llama-2-13B-chat disproportionately favors the number five, and\nwhen tasked with picking a first name at random, Mistral-7B-Instruct chooses\nAvery 40 times more often than we would expect based on the U.S. population.\nWhen these language models are used for real-world tasks where diversity of\noutputs is crucial, such as language model assisted dataset construction, their\ninability to produce diffuse distributions over valid choices is a major\nhurdle. In this work, we propose a fine-tuning method that encourages language\nmodels to output distributions that are diffuse over valid outcomes. The\nmethods we introduce generalize across a variety of tasks and distributions and\nmake large language models practical for synthetic dataset generation with\nlittle human intervention.", "published": "2024-04-16 19:17:23", "link": "http://arxiv.org/abs/2404.10859v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Binder: Hierarchical Concept Representation through Order Embedding of\n  Binary Vectors", "abstract": "For natural language understanding and generation, embedding concepts using\nan order-based representation is an essential task. Unlike traditional point\nvector based representation, an order-based representation imposes geometric\nconstraints on the representation vectors for explicitly capturing various\nsemantic relationships that may exist between a pair of concepts. In existing\nliterature, several approaches on order-based embedding have been proposed,\nmostly focusing on capturing hierarchical relationships; examples include\nvectors in Euclidean space, complex, Hyperbolic, order, and Box Embedding. Box\nembedding creates region-based rich representation of concepts, but along the\nprocess it sacrifices simplicity, requiring a custom-made optimization scheme\nfor learning the representation. Hyperbolic embedding improves embedding\nquality by exploiting the ever-expanding property of Hyperbolic space, but it\nalso suffers from the same fate as box embedding as gradient descent like\noptimization is not simple in the Hyperbolic space. In this work, we propose\nBinder, a novel approach for order-based representation. Binder uses binary\nvectors for embedding, so the embedding vectors are compact with an order of\nmagnitude smaller footprint than other methods. Binder uses a simple and\nefficient optimization scheme for learning representation vectors with a linear\ntime complexity. Our comprehensive experimental results show that Binder is\nvery accurate, yielding competitive results on the representation task. But\nBinder stands out from its competitors on the transitive closure link\nprediction task as it can learn concept embeddings just from the direct edges,\nwhereas all existing order-based approaches rely on the indirect edges.", "published": "2024-04-16 21:52:55", "link": "http://arxiv.org/abs/2404.10924v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Uncertainty-Based Abstention in LLMs Improves Safety and Reduces\n  Hallucinations", "abstract": "A major barrier towards the practical deployment of large language models\n(LLMs) is their lack of reliability. Three situations where this is\nparticularly apparent are correctness, hallucinations when given unanswerable\nquestions, and safety. In all three cases, models should ideally abstain from\nresponding, much like humans, whose ability to understand uncertainty makes us\nrefrain from answering questions we don't know. Inspired by analogous\napproaches in classification, this study explores the feasibility and efficacy\nof abstaining while uncertain in the context of LLMs within the domain of\nquestion-answering. We investigate two kinds of uncertainties, statistical\nuncertainty metrics and a distinct verbalized measure, termed as In-Dialogue\nUncertainty (InDU). Using these uncertainty measures combined with models with\nand without Reinforcement Learning with Human Feedback (RLHF), we show that in\nall three situations, abstention based on the right kind of uncertainty measure\ncan boost the reliability of LLMs. By sacrificing only a few highly uncertain\nsamples we can improve correctness by 2% to 8%, avoid 50% hallucinations via\ncorrectly identifying unanswerable questions and increase safety by 70% up to\n99% with almost no additional computational overhead.", "published": "2024-04-16 23:56:38", "link": "http://arxiv.org/abs/2404.10960v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving the Capabilities of Large Language Model Based Marketing\n  Analytics Copilots With Semantic Search And Fine-Tuning", "abstract": "Artificial intelligence (AI) is widely deployed to solve problems related to\nmarketing attribution and budget optimization. However, AI models can be quite\ncomplex, and it can be difficult to understand model workings and insights\nwithout extensive implementation teams. In principle, recently developed large\nlanguage models (LLMs), like GPT-4, can be deployed to provide marketing\ninsights, reducing the time and effort required to make critical decisions. In\npractice, there are substantial challenges that need to be overcome to reliably\nuse such models. We focus on domain-specific question-answering, SQL generation\nneeded for data retrieval, and tabular analysis and show how a combination of\nsemantic search, prompt engineering, and fine-tuning can be applied to\ndramatically improve the ability of LLMs to execute these tasks accurately. We\ncompare both proprietary models, like GPT-4, and open-source models, like\nLlama-2-70b, as well as various embedding methods. These models are tested on\nsample use cases specific to marketing mix modeling and attribution.", "published": "2024-04-16 03:39:16", "link": "http://arxiv.org/abs/2404.13077v1", "categories": ["cs.CL", "cs.LG", "I.2.1; I.2.7"], "primary_category": "cs.CL"}
{"title": "Empowering Interdisciplinary Research with BERT-Based Models: An\n  Approach Through SciBERT-CNN with Topic Modeling", "abstract": "Researchers must stay current in their fields by regularly reviewing academic\nliterature, a task complicated by the daily publication of thousands of papers.\nTraditional multi-label text classification methods often ignore semantic\nrelationships and fail to address the inherent class imbalances. This paper\nintroduces a novel approach using the SciBERT model and CNNs to systematically\ncategorize academic abstracts from the Elsevier OA CC-BY corpus. We use a\nmulti-segment input strategy that processes abstracts, body text, titles, and\nkeywords obtained via BERT topic modeling through SciBERT. Here, the [CLS]\ntoken embeddings capture the contextual representation of each segment,\nconcatenated and processed through a CNN. The CNN uses convolution and pooling\nto enhance feature extraction and reduce dimensionality, optimizing the data\nfor classification. Additionally, we incorporate class weights based on label\nfrequency to address the class imbalance, significantly improving the\nclassification F1 score and enhancing text classification systems and\nliterature review efficiency.", "published": "2024-04-16 05:21:47", "link": "http://arxiv.org/abs/2404.13078v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Relational Graph Convolutional Networks for Sentiment Analysis", "abstract": "With the growth of textual data across online platforms, sentiment analysis\nhas become crucial for extracting insights from user-generated content. While\ntraditional approaches and deep learning models have shown promise, they cannot\noften capture complex relationships between entities. In this paper, we propose\nleveraging Relational Graph Convolutional Networks (RGCNs) for sentiment\nanalysis, which offer interpretability and flexibility by capturing\ndependencies between data points represented as nodes in a graph. We\ndemonstrate the effectiveness of our approach by using pre-trained language\nmodels such as BERT and RoBERTa with RGCN architecture on product reviews from\nAmazon and Digikala datasets and evaluating the results. Our experiments\nhighlight the effectiveness of RGCNs in capturing relational information for\nsentiment analysis tasks.", "published": "2024-04-16 07:27:49", "link": "http://arxiv.org/abs/2404.13079v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Social Media Posts for Depression Identification: A Study on\n  Reddit Dataset", "abstract": "Depression is one of the most common mental disorders affecting an\nindividual's personal and professional life. In this work, we investigated the\npossibility of utilizing social media posts to identify depression in\nindividuals. To achieve this goal, we conducted a preliminary study where we\nextracted and analyzed the top Reddit posts made in 2022 from\ndepression-related forums. The collected data were labeled as depressive and\nnon-depressive using UMLS Metathesaurus. Further, the pre-processed data were\nfed to classical machine learning models, where we achieved an accuracy of\n92.28\\% in predicting the depressive and non-depressive posts.", "published": "2024-04-16 06:25:51", "link": "http://arxiv.org/abs/2405.06656v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Find The Gap: Knowledge Base Reasoning For Visual Question Answering", "abstract": "We analyze knowledge-based visual question answering, for which given a\nquestion, the models need to ground it into the visual modality and retrieve\nthe relevant knowledge from a given large knowledge base (KB) to be able to\nanswer. Our analysis has two folds, one based on designing neural architectures\nand training them from scratch, and another based on large pre-trained language\nmodels (LLMs). Our research questions are: 1) Can we effectively augment models\nby explicit supervised retrieval of the relevant KB information to solve the\nKB-VQA problem? 2) How do task-specific and LLM-based models perform in the\nintegration of visual and external knowledge, and multi-hop reasoning over both\nsources of information? 3) Is the implicit knowledge of LLMs sufficient for\nKB-VQA and to what extent it can replace the explicit KB? Our results\ndemonstrate the positive impact of empowering task-specific and LLM models with\nsupervised external and visual knowledge retrieval models. Our findings show\nthat though LLMs are stronger in 1-hop reasoning, they suffer in 2-hop\nreasoning in comparison with our fine-tuned NN model even if the relevant\ninformation from both modalities is available to the model. Moreover, we\nobserved that LLM models outperform the NN model for KB-related questions which\nconfirms the effectiveness of implicit knowledge in LLMs however, they do not\nalleviate the need for external KB.", "published": "2024-04-16 02:11:46", "link": "http://arxiv.org/abs/2404.10226v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Two-Stage Stance Labeling: User-Hashtag Heuristics with Graph Neural\n  Networks", "abstract": "The high volume and rapid evolution of content on social media present major\nchallenges for studying the stance of social media users. In this work, we\ndevelop a two stage stance labeling method that utilizes the user-hashtag\nbipartite graph and the user-user interaction graph. In the first stage, a\nsimple and efficient heuristic for stance labeling uses the user-hashtag\nbipartite graph to iteratively update the stance association of user and\nhashtag nodes via a label propagation mechanism. This set of soft labels is\nthen integrated with the user-user interaction graph to train a graph neural\nnetwork (GNN) model using semi-supervised learning. We evaluate this method on\ntwo large-scale datasets containing tweets related to climate change from June\n2021 to June 2022 and gun control from January 2022 to January 2023. Our\nexperiments demonstrate that enriching text-based embeddings of users with\nnetwork information from the user interaction graph using our semi-supervised\nGNN method outperforms both classifiers trained on user textual embeddings and\nzero-shot classification using LLMs such as GPT4. We discuss the need for\nintegrating nuanced understanding from social science with the scalability of\ncomputational methods to better understand how polarization on social media\noccurs for divisive issues such as climate change and gun control.", "published": "2024-04-16 02:18:30", "link": "http://arxiv.org/abs/2404.10228v2", "categories": ["cs.LG", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Reasoning on Efficient Knowledge Paths:Knowledge Graph Guides Large\n  Language Model for Domain Question Answering", "abstract": "Large language models (LLMs), such as GPT3.5, GPT4 and LLAMA2 perform\nsurprisingly well and outperform human experts on many tasks. However, in many\ndomain-specific evaluations, these LLMs often suffer from hallucination\nproblems due to insufficient training of relevant corpus. Furthermore,\nfine-tuning large models may face problems such as the LLMs are not open source\nor the construction of high-quality domain instruction is difficult. Therefore,\nstructured knowledge databases such as knowledge graph can better provide\ndomain background knowledge for LLMs and make full use of the reasoning and\nanalysis capabilities of LLMs. In some previous works, LLM was called multiple\ntimes to determine whether the current triplet was suitable for inclusion in\nthe subgraph when retrieving subgraphs through a question. Especially for the\nquestion that require a multi-hop reasoning path, frequent calls to LLM will\nconsume a lot of computing power. Moreover, when choosing the reasoning path,\nLLM will be called once for each step, and if one of the steps is selected\nincorrectly, it will lead to the accumulation of errors in the following steps.\nIn this paper, we integrated and optimized a pipeline for selecting reasoning\npaths from KG based on LLM, which can reduce the dependency on LLM. In\naddition, we propose a simple and effective subgraph retrieval method based on\nchain of thought (CoT) and page rank which can returns the paths most likely to\ncontain the answer. We conduct experiments on three datasets: GenMedGPT-5k\n[14], WebQuestions [2], and CMCQA [21]. Finally, RoK can demonstrate that using\nfewer LLM calls can achieve the same results as previous SOTAs models.", "published": "2024-04-16 08:28:16", "link": "http://arxiv.org/abs/2404.10384v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Visual Preference Alignment", "abstract": "This paper makes the first attempt towards unsupervised preference alignment\nin Vision-Language Models (VLMs). We generate chosen and rejected responses\nwith regard to the original and augmented image pairs, and conduct preference\nalignment with direct preference optimization. It is based on a core idea:\nproperly designed augmentation to the image input will induce VLM to generate\nfalse but hard negative responses, which helps the model to learn from and\nproduce more robust and powerful answers. The whole pipeline no longer hinges\non supervision from GPT-4 or human involvement during alignment, and is highly\nefficient with few lines of code. With only 8k randomly sampled unsupervised\ndata, it achieves 90\\% relative score to GPT-4 on complex reasoning in\nLLaVA-Bench, and improves LLaVA-7B/13B by 6.7\\%/5.6\\% score on complex\nmulti-modal benchmark MM-Vet. Visualizations shows its improved ability to\nalign with user-intentions. A series of ablations are firmly conducted to\nreveal the latent mechanism of the approach, which also indicates its potential\ntowards further scaling. Code are available in\nhttps://github.com/Kevinz-code/SeVa.", "published": "2024-04-16 12:19:54", "link": "http://arxiv.org/abs/2404.10501v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "White Men Lead, Black Women Help? Benchmarking Language Agency Social\n  Biases in LLMs", "abstract": "Social biases can manifest in language agency. While several studies\napproached agency-related bias in human-written language, very limited research\nhas investigated such biases in Large Language Model (LLM)-generated content.\nIn addition, previous works often rely on string-matching techniques to\nidentify agentic and communal words within texts, which fall short of\naccurately classifying language agency. We introduce the novel Language Agency\nBias Evaluation (LABE) benchmark, which comprehensively evaluates biases in\nLLMs by analyzing agency levels attributed to different demographic groups in\nmodel generations. LABE leverages 5,400 template-based prompts, an accurate\nagency classifier, and corresponding bias metrics to test for gender, racial,\nand intersectional language agency biases in LLMs on 3 text generation tasks:\nbiographies, professor reviews, and reference letters. We also contribute the\nLanguage Agency Classification (LAC) dataset, consisting of 3,724 agentic and\ncommunal sentences. Using LABE, we unveil language agency social biases in 3\nrecent LLMs: ChatGPT, Llama3, and Mistral. We observe that: (1) LLM generations\ntend to demonstrate greater gender bias than human-written texts; (2) Models\ndemonstrate remarkably higher levels of intersectional bias than the other bias\naspects. Those who are at the intersection of gender and racial minority\ngroups--such as Black females--are consistently described by texts with lower\nlevels of agency, aligning with real-world social inequalities; (3) Among the 3\nLLMs investigated, Llama3 demonstrates the greatest overall bias; (4) Not only\ndoes prompt-based mitigation fail to resolve language agency bias in LLMs, but\nit frequently leads to the exacerbation of biases in generated texts.", "published": "2024-04-16 12:27:54", "link": "http://arxiv.org/abs/2404.10508v4", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\n  Granularity", "abstract": "State-of-the-art performance in QA tasks is currently achieved by systems\nemploying Large Language Models (LLMs), however these models tend to\nhallucinate information in their responses. One approach focuses on enhancing\nthe generation process by incorporating attribution from the given input to the\noutput. However, the challenge of identifying appropriate attributions and\nverifying their accuracy against a source is a complex task that requires\nsignificant improvements in assessing such systems. We introduce an\nattribution-oriented Chain-of-Thought reasoning method to enhance the accuracy\nof attributions. This approach focuses the reasoning process on generating an\nattribution-centric output. Evaluations on two context-enhanced\nquestion-answering datasets using GPT-4 demonstrate improved accuracy and\ncorrectness of attributions. In addition, the combination of our method with\nfinetuning enhances the response and attribution accuracy of two smaller LLMs,\nshowing their potential to outperform GPT-4 in some cases.", "published": "2024-04-16 12:37:10", "link": "http://arxiv.org/abs/2404.10513v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Language Evolution of Divergent Collective Memory Around the Arab\n  Spring", "abstract": "The Arab Spring was a historic set of protests beginning in 2011 that toppled\ngovernments and led to major conflicts. Collective memories of events like\nthese can vary significantly across social contexts in response to political,\ncultural, and linguistic factors. While Wikipedia plays an important role in\ndocumenting both historic and current events, little attention has been given\nto how Wikipedia articles, created in the aftermath of major events, continue\nto evolve over years or decades. Using the archived content of Arab\nSpring-related topics across the Arabic and English Wikipedias between 2011 and\n2024, we define and evaluate multilingual measures of event salience,\ndeliberation, contextualization, and consolidation of collective memory\nsurrounding the Arab Spring. Our findings about the temporal evolution of the\nWikipedia articles' content similarity across languages has implications for\ntheorizing about online collective memory processes and evaluating linguistic\nmodels trained on these data.", "published": "2024-04-16 16:30:27", "link": "http://arxiv.org/abs/2404.10706v1", "categories": ["cs.CY", "cs.CL", "cs.HC", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Deep Learning and LLM-based Methods Applied to Stellar Lightcurve\n  Classification", "abstract": "Light curves serve as a valuable source of information on stellar formation\nand evolution. With the rapid advancement of machine learning techniques, it\ncan be effectively processed to extract astronomical patterns and information.\nIn this study, we present a comprehensive evaluation of deep-learning and large\nlanguage model (LLM) based models for the automatic classification of variable\nstar light curves, based on large datasets from the Kepler and K2 missions.\nSpecial emphasis is placed on Cepheids, RR Lyrae, and eclipsing binaries,\nexamining the influence of observational cadence and phase distribution on\nclassification precision. Employing AutoDL optimization, we achieve striking\nperformance with the 1D-Convolution+BiLSTM architecture and the Swin\nTransformer, hitting accuracies of 94\\% and 99\\% correspondingly, with the\nlatter demonstrating a notable 83\\% accuracy in discerning the elusive Type II\nCepheids-comprising merely 0.02\\% of the total dataset.We unveil StarWhisper\nLightCurve (LC), an innovative Series comprising three LLM-based models: LLM,\nmultimodal large language model (MLLM), and Large Audio Language Model (LALM).\nEach model is fine-tuned with strategic prompt engineering and customized\ntraining methods to explore the emergent abilities of these models for\nastronomical data. Remarkably, StarWhisper LC Series exhibit high accuracies\naround 90\\%, significantly reducing the need for explicit feature engineering,\nthereby paving the way for streamlined parallel data processing and the\nprogression of multifaceted multimodal models in astronomical applications. The\nstudy furnishes two detailed catalogs illustrating the impacts of phase and\nsampling intervals on deep learning classification accuracy, showing that a\nsubstantial decrease of up to 14\\% in observation duration and 21\\% in sampling\npoints can be realized without compromising accuracy by more than 10\\%.", "published": "2024-04-16 17:35:25", "link": "http://arxiv.org/abs/2404.10757v2", "categories": ["astro-ph.IM", "astro-ph.SR", "cs.CL", "cs.LG"], "primary_category": "astro-ph.IM"}
{"title": "LaDiC: Are Diffusion Models Really Inferior to Autoregressive\n  Counterparts for Image-to-Text Generation?", "abstract": "Diffusion models have exhibited remarkable capabilities in text-to-image\ngeneration. However, their performance in image-to-text generation,\nspecifically image captioning, has lagged behind Auto-Regressive (AR) models,\ncasting doubt on their applicability for such tasks. In this work, we revisit\ndiffusion models, highlighting their capacity for holistic context modeling and\nparallel decoding. With these benefits, diffusion models can alleviate the\ninherent limitations of AR methods, including their slow inference speed, error\npropagation, and unidirectional constraints. Furthermore, we identify the prior\nunderperformance of diffusion models stemming from the absence of an effective\nlatent space for image-text alignment, and the discrepancy between continuous\ndiffusion processes and discrete textual data. In response, we introduce a\nnovel architecture, LaDiC, which utilizes a split BERT to create a dedicated\nlatent space for captions and integrates a regularization module to manage\nvarying text lengths. Our framework also includes a diffuser for semantic\nimage-to-text conversion and a Back&Refine technique to enhance token\ninteractivity during inference. LaDiC achieves state-of-the-art performance for\ndiffusion-based methods on the MS COCO dataset with 38.2 BLEU@4 and 126.2\nCIDEr, demonstrating exceptional performance without pre-training or ancillary\nmodules. This indicates strong competitiveness with AR models, revealing the\npreviously untapped potential of diffusion models in image-to-text generation.", "published": "2024-04-16 17:47:16", "link": "http://arxiv.org/abs/2404.10763v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Fewer Truncations Improve Language Modeling", "abstract": "In large language model training, input documents are typically concatenated\ntogether and then split into sequences of equal length to avoid padding tokens.\nDespite its efficiency, the concatenation approach compromises data integrity\n-- it inevitably breaks many documents into incomplete pieces, leading to\nexcessive truncations that hinder the model from learning to compose logically\ncoherent and factually consistent content that is grounded on the complete\ncontext. To address the issue, we propose Best-fit Packing, a scalable and\nefficient method that packs documents into training sequences through\nlength-aware combinatorial optimization. Our method completely eliminates\nunnecessary truncations while retaining the same training efficiency as\nconcatenation. Empirical results from both text and code pre-training show that\nour method achieves superior performance (e.g., relatively +4.7% on reading\ncomprehension; +16.8% in context following; and +9.2% on program synthesis),\nand reduces closed-domain hallucination effectively by up to 58.3%.", "published": "2024-04-16 18:08:29", "link": "http://arxiv.org/abs/2404.10830v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dynamic Self-adaptive Multiscale Distillation from Pre-trained\n  Multimodal Large Model for Efficient Cross-modal Representation Learning", "abstract": "In recent years, pre-trained multimodal large models have attracted\nwidespread attention due to their outstanding performance in various multimodal\napplications. Nonetheless, the extensive computational resources and vast\ndatasets required for their training present significant hurdles for deployment\nin environments with limited computational resources. To address this\nchallenge, we propose a novel dynamic self-adaptive multiscale distillation\nfrom pre-trained multimodal large model for efficient cross-modal\nrepresentation learning for the first time. Unlike existing distillation\nmethods, our strategy employs a multiscale perspective, enabling the extraction\nstructural knowledge across from the pre-trained multimodal large model.\nEnsuring that the student model inherits a comprehensive and nuanced\nunderstanding of the teacher knowledge. To optimize each distillation loss in a\nbalanced and efficient manner, we propose a dynamic self-adaptive distillation\nloss balancer, a novel component eliminating the need for manual loss weight\nadjustments and dynamically balances each loss item during the distillation\nprocess. Our methodology streamlines pre-trained multimodal large models using\nonly their output features and original image-level information, requiring\nminimal computational resources. This efficient approach is suited for various\napplications and allows the deployment of advanced multimodal technologies even\nin resource-limited settings. Extensive experiments has demonstrated that our\nmethod maintains high performance while significantly reducing model complexity\nand training costs. Moreover, our distilled student model utilizes only\nimage-level information to achieve state-of-the-art performance on cross-modal\nretrieval tasks, surpassing previous methods that relied on region-level\ninformation.", "published": "2024-04-16 18:22:49", "link": "http://arxiv.org/abs/2404.10838v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "A LayoutLMv3-Based Model for Enhanced Relation Extraction in\n  Visually-Rich Documents", "abstract": "Document Understanding is an evolving field in Natural Language Processing\n(NLP). In particular, visual and spatial features are essential in addition to\nthe raw text itself and hence, several multimodal models were developed in the\nfield of Visual Document Understanding (VDU). However, while research is mainly\nfocused on Key Information Extraction (KIE), Relation Extraction (RE) between\nidentified entities is still under-studied. For instance, RE is crucial to\nregroup entities or obtain a comprehensive hierarchy of data in a document. In\nthis paper, we present a model that, initialized from LayoutLMv3, can match or\noutperform the current state-of-the-art results in RE applied to Visually-Rich\nDocuments (VRD) on FUNSD and CORD datasets, without any specific pre-training\nand with fewer parameters. We also report an extensive ablation study performed\non FUNSD, highlighting the great impact of certain features and modelization\nchoices on the performances.", "published": "2024-04-16 18:50:57", "link": "http://arxiv.org/abs/2404.10848v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Teaching a Multilingual Large Language Model to Understand Multilingual\n  Speech via Multi-Instructional Training", "abstract": "Recent advancements in language modeling have led to the emergence of Large\nLanguage Models (LLMs) capable of various natural language processing tasks.\nDespite their success in text-based tasks, applying LLMs to the speech domain\nremains limited and challenging. This paper presents BLOOMZMMS, a novel model\nthat integrates a multilingual LLM with a multilingual speech encoder, aiming\nto harness the capabilities of LLMs for speech recognition and beyond.\nUtilizing a multi-instructional training approach, we demonstrate the\ntransferability of linguistic knowledge from the text to the speech modality.\nOur experiments, conducted on 1900 hours of transcribed data from 139\nlanguages, establish that a multilingual speech representation can be\neffectively learned and aligned with a multilingual LLM. While this learned\nrepresentation initially shows limitations in task generalization, we address\nthis issue by generating synthetic targets in a multi-instructional style. Our\nzero-shot evaluation results confirm the robustness of our approach across\nmultiple tasks, including speech translation and multilingual spoken language\nunderstanding, thereby opening new avenues for applying LLMs in the speech\ndomain.", "published": "2024-04-16 21:45:59", "link": "http://arxiv.org/abs/2404.10922v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LLMem: Estimating GPU Memory Usage for Fine-Tuning Pre-Trained LLMs", "abstract": "Fine-tuning pre-trained large language models (LLMs) with limited hardware\npresents challenges due to GPU memory constraints. Various distributed\nfine-tuning methods have been proposed to alleviate memory constraints on GPU.\nHowever, determining the most effective method for achieving rapid fine-tuning\nwhile preventing GPU out-of-memory issues in a given environment remains\nunclear. To address this challenge, we introduce LLMem, a solution that\nestimates the GPU memory consumption when applying distributed fine-tuning\nmethods across multiple GPUs and identifies the optimal method. We conduct GPU\nmemory usage estimation prior to fine-tuning, leveraging the fundamental\nstructure of transformer-based decoder models and the memory usage distribution\nof each method. Experimental results show that LLMem accurately estimates peak\nGPU memory usage on a single GPU, with error rates of up to 1.6%. Additionally,\nit shows an average error rate of 3.0% when applying distributed fine-tuning\nmethods to LLMs with more than a billion parameters on multi-GPU setups.", "published": "2024-04-16 22:11:35", "link": "http://arxiv.org/abs/2404.10933v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Shears: Unstructured Sparsity with Neural Low-rank Adapter Search", "abstract": "Recently, several approaches successfully demonstrated that weight-sharing\nNeural Architecture Search (NAS) can effectively explore a search space of\nelastic low-rank adapters (LoRA), allowing the parameter-efficient fine-tuning\n(PEFT) and compression of large language models. In this paper, we introduce a\nnovel approach called Shears, demonstrating how the integration of\ncost-effective sparsity and a proposed Neural Low-rank adapter Search (NLS)\nalgorithm can further improve the efficiency of PEFT approaches. Results\ndemonstrate the benefits of Shears compared to other methods, reaching high\nsparsity levels while improving or with little drop in accuracy, utilizing a\nsingle GPU for a pair of hours.", "published": "2024-04-16 22:12:36", "link": "http://arxiv.org/abs/2404.10934v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Can Language Models Solve Olympiad Programming?", "abstract": "Computing olympiads contain some of the most challenging problems for humans,\nrequiring complex algorithmic reasoning, puzzle solving, in addition to\ngenerating efficient code. However, it has been understudied as a domain to\nevaluate language models (LMs). In this paper, we introduce the USACO benchmark\nwith 307 problems from the USA Computing Olympiad, along with high-quality unit\ntests, reference code, and official analyses for each problem. These resources\nenable us to construct and test a range of LM inference methods for competitive\nprogramming for the first time. We find GPT-4 only achieves a 8.7% pass@1\naccuracy with zero-shot chain-of-thought prompting, and our best inference\nmethod improves it to 20.2% using a combination of self-reflection and\nretrieval over episodic knowledge. However, this is far from solving the\nbenchmark. To better understand the remaining challenges, we design a novel\nhuman-in-the-loop study and surprisingly find that a small number of targeted\nhints enable GPT-4 to solve 13 out of 15 problems previously unsolvable by any\nmodel and method. Our benchmark, baseline methods, quantitative results, and\nqualitative analysis serve as an initial step toward LMs with grounded,\ncreative, and algorithmic reasoning.", "published": "2024-04-16 23:27:38", "link": "http://arxiv.org/abs/2404.10952v1", "categories": ["cs.CL", "cs.AI", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Uncovering Latent Arguments in Social Media Messaging by Employing\n  LLMs-in-the-Loop Strategy", "abstract": "The widespread use of social media has led to a surge in popularity for\nautomated methods of analyzing public opinion. Supervised methods are adept at\ntext categorization, yet the dynamic nature of social media discussions poses a\ncontinual challenge for these techniques due to the constant shifting of the\nfocus. On the other hand, traditional unsupervised methods for extracting\nthemes from public discourse, such as topic modeling, often reveal overarching\npatterns that might not capture specific nuances. Consequently, a significant\nportion of research into social media discourse still depends on\nlabor-intensive manual coding techniques and a human-in-the-loop approach,\nwhich are both time-consuming and costly. In this work, we study the problem of\ndiscovering arguments associated with a specific theme. We propose a generic\nLLMs-in-the-Loop strategy that leverages the advanced capabilities of Large\nLanguage Models (LLMs) to extract latent arguments from social media messaging.\nTo demonstrate our approach, we apply our framework to contentious topics. We\nuse two publicly available datasets: (1) the climate campaigns dataset of 14k\nFacebook ads with 25 themes and (2) the COVID-19 vaccine campaigns dataset of\n9k Facebook ads with 14 themes. Additionally, we design a downstream task as\nstance prediction by leveraging talking points in climate debates. Furthermore,\nwe analyze demographic targeting and the adaptation of messaging based on\nreal-world events.", "published": "2024-04-16 03:26:43", "link": "http://arxiv.org/abs/2404.10259v4", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Social Choice Should Guide AI Alignment in Dealing with Diverse Human\n  Feedback", "abstract": "Foundation models such as GPT-4 are fine-tuned to avoid unsafe or otherwise\nproblematic behavior, such as helping to commit crimes or producing racist\ntext. One approach to fine-tuning, called reinforcement learning from human\nfeedback, learns from humans' expressed preferences over multiple outputs.\nAnother approach is constitutional AI, in which the input from humans is a list\nof high-level principles. But how do we deal with potentially diverging input\nfrom humans? How can we aggregate the input into consistent data about\n\"collective\" preferences or otherwise use it to make collective choices about\nmodel behavior? In this paper, we argue that the field of social choice is well\npositioned to address these questions, and we discuss ways forward for this\nagenda, drawing on discussions in a recent workshop on Social Choice for AI\nEthics and Safety held in Berkeley, CA, USA in December 2023.", "published": "2024-04-16 03:59:33", "link": "http://arxiv.org/abs/2404.10271v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY", "cs.GT", "68T01, 68T50, 91B14, 91B12", "I.2.0; I.2.7; K.4.2; I.2.m; J.4"], "primary_category": "cs.LG"}
{"title": "Wireless Earphone-based Real-Time Monitoring of Breathing Exercises: A\n  Deep Learning Approach", "abstract": "Several therapy routines require deep breathing exercises as a key component\nand patients undergoing such therapies must perform these exercises regularly.\nAssessing the outcome of a therapy and tailoring its course necessitates\nmonitoring a patient's compliance with the therapy. While therapy compliance\nmonitoring is routine in a clinical environment, it is challenging to do in an\nat-home setting. This is so because a home setting lacks access to specialized\nequipment and skilled professionals needed to effectively monitor the\nperformance of a therapy routine by a patient. For some types of therapies,\nthese challenges can be addressed with the use of consumer-grade hardware, such\nas earphones and smartphones, as practical solutions. To accurately monitor\nbreathing exercises using wireless earphones, this paper proposes a framework\nthat has the potential for assessing a patient's compliance with an at-home\ntherapy. The proposed system performs real-time detection of breathing phases\nand channels with high accuracy by processing a $\\mathbf{500}$ ms audio signal\nthrough two convolutional neural networks. The first network, called a channel\nclassifier, distinguishes between nasal and oral breathing, and a pause. The\nsecond network, called a phase classifier, determines whether the audio segment\nis from inhalation or exhalation. According to $k$-fold cross-validation, the\nchannel and phase classifiers achieved a maximum F1 score of $\\mathbf{97.99\\%}$\nand $\\mathbf{89.46\\%}$, respectively. The results demonstrate the potential of\nusing commodity earphones for real-time breathing channel and phase detection\nfor breathing therapy compliance monitoring.", "published": "2024-04-16 06:37:19", "link": "http://arxiv.org/abs/2404.10310v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Vivo : une approche multimodale de la synthese concatenative par corpus\n  dans le cadre d'une oeuvre audiovisuelle immersive", "abstract": "Which visual descriptors are suitable for multi-modal interaction and how to\nintegrate them via real-time video data analysis into a corpus-based\nconcatenative synthesis sound system.", "published": "2024-04-16 14:02:17", "link": "http://arxiv.org/abs/2404.10578v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Clustering and Data Augmentation to Improve Accuracy of Sleep Assessment\n  and Sleep Individuality Analysis", "abstract": "Recently, growing health awareness, novel methods allow individuals to\nmonitor sleep at home. Utilizing sleep sounds offers advantages over\nconventional methods like smartwatches, being non-intrusive, and capable of\ndetecting various physiological activities. This study aims to construct a\nmachine learning-based sleep assessment model providing evidence-based\nassessments, such as poor sleep due to frequent movement during sleep onset.\nExtracting sleep sound events, deriving latent representations using VAE,\nclustering with GMM, and training LSTM for subjective sleep assessment achieved\na high accuracy of 94.8% in distinguishing sleep satisfaction. Moreover,\nTimeSHAP revealed differences in impactful sound event types and timings for\ndifferent individuals.", "published": "2024-04-16 05:56:41", "link": "http://arxiv.org/abs/2404.10299v2", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Long-form music generation with latent diffusion", "abstract": "Audio-based generative models for music have seen great strides recently, but\nso far have not managed to produce full-length music tracks with coherent\nmusical structure from text prompts. We show that by training a generative\nmodel on long temporal contexts it is possible to produce long-form music of up\nto 4m45s. Our model consists of a diffusion-transformer operating on a highly\ndownsampled continuous latent representation (latent rate of 21.5Hz). It\nobtains state-of-the-art generations according to metrics on audio quality and\nprompt alignment, and subjective tests reveal that it produces full-length\nmusic with coherent structure.", "published": "2024-04-16 06:09:33", "link": "http://arxiv.org/abs/2404.10301v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multiple Mobile Target Detection and Tracking in Active Sonar Array\n  Using a Track-Before-Detect Approach", "abstract": "We present an algorithm for detecting and tracking underwater mobile objects\nusing active acoustic transmission of broadband chirp signals whose reflections\nare received by a hydrophone array. The method overcomes the problem of high\nfalse alarm rate by applying a track-before-detect approach to the sequence of\nreceived reflections. A 2D time-space matrix is created for the reverberations\nreceived from each transmitted probe signal by performing delay and sum\nbeamforming and pulse compression. The result is filtered by a 2D constant\nfalse alarm rate (CFAR) detector to identify reflection patterns corresponding\nto potential targets. Closely spaced signals for multiple probe transmissions\nare combined into blobs to avoid multiple detections of a single object. A\ntrack-before-detect method using a Nearly Constant Velocity (NCV) model is\nemployed to track multiple objects. The position and velocity is estimated by\nthe debiased converted measurement Kalman filter. Results are analyzed for\nsimulated scenarios and for experiments at sea, where GPS tagged gilt-head\nseabream fish were tracked. Compared to two benchmark schemes, the results show\na favorable track continuity and accuracy that is robust to the choice of\ndetection threshold.", "published": "2024-04-16 06:48:41", "link": "http://arxiv.org/abs/2404.10316v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Unsupervised Speaker Diarization in Distributed IoT Networks Using\n  Federated Learning", "abstract": "This paper presents a computationally efficient and distributed speaker\ndiarization framework for networked IoT-style audio devices. The work proposes\na Federated Learning model which can identify the participants in a\nconversation without the requirement of a large audio database for training. An\nunsupervised online update mechanism is proposed for the Federated Learning\nmodel which depends on cosine similarity of speaker embeddings. Moreover, the\nproposed diarization system solves the problem of speaker change detection via.\nunsupervised segmentation techniques using Hotelling's t-squared Statistic and\nBayesian Information Criterion. In this new approach, speaker change detection\nis biased around detected quasi-silences, which reduces the severity of the\ntrade-off between the missed detection and false detection rates. Additionally,\nthe computational overhead due to frame-by-frame identification of speakers is\nreduced via. unsupervised clustering of speech segments. The results\ndemonstrate the effectiveness of the proposed training method in the presence\nof non-IID speech data. It also shows a considerable improvement in the\nreduction of false and missed detection at the segmentation stage, while\nreducing the computational overhead. Improved accuracy and reduced\ncomputational cost makes the mechanism suitable for real-time speaker\ndiarization across a distributed IoT audio network.", "published": "2024-04-16 18:40:28", "link": "http://arxiv.org/abs/2404.10842v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
