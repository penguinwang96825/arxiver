{"title": "Social Media as an Instant Source of Feedback on Water Quality", "abstract": "This paper focuses on an important environmental challenge; namely, water\nquality by analyzing the potential of social media as an immediate source of\nfeedback. The main goal of the work is to automatically analyze and retrieve\nsocial media posts relevant to water quality with particular attention to posts\ndescribing different aspects of water quality, such as watercolor, smell,\ntaste, and related illnesses. To this aim, we propose a novel framework\nincorporating different preprocessing, data augmentation, and classification\ntechniques. In total, three different Neural Networks (NNs) architectures,\nnamely (i) Bidirectional Encoder Representations from Transformers (BERT), (ii)\nRobustly Optimized BERT Pre-training Approach (XLM-RoBERTa), and (iii) custom\nLong short-term memory (LSTM) model, are employed in a merit-based fusion\nscheme. For merit-based weight assignment to the models, several optimization\nand search techniques are compared including a Particle Swarm Optimization\n(PSO), a Genetic Algorithm (GA), Brute Force (BF), Nelder-Mead, and Powell's\noptimization methods. We also provide an evaluation of the individual models\nwhere the highest F1-score of 0.81 is obtained with the BERT model. In\nmerit-based fusion, overall better results are obtained with BF achieving an\nF1-score score of 0.852.\n  We also provide comparison against existing methods, where a significant\nimprovement for our proposed solutions is obtained. We believe such rigorous\nanalysis of this relatively new topic will provide a baseline for future\nresearch.", "published": "2022-02-09 13:47:33", "link": "http://arxiv.org/abs/2202.04462v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TamilEmo: Finegrained Emotion Detection Dataset for Tamil", "abstract": "Emotional Analysis from textual input has been considered both a challenging\nand interesting task in Natural Language Processing. However, due to the lack\nof datasets in low-resource languages (i.e. Tamil), it is difficult to conduct\nresearch of high standard in this area. Therefore we introduce this labelled\ndataset (a largest manually annotated dataset of more than 42k Tamil YouTube\ncomments, labelled for 31 emotions including neutral) for emotion recognition.\nThe goal of this dataset is to improve emotion detection in multiple downstream\ntasks in Tamil. We have also created three different groupings of our emotions\n(3-class, 7-class and 31-class) and evaluated the model's performance on each\ncategory of the grouping. Our MURIL-base model has achieved a 0.60 macro\naverage F1-score across our 3-class group dataset. With 7-class and 31-class\ngroups, the Random Forest model performed well with a macro average F1-scores\nof 0.42 and 0.29 respectively.", "published": "2022-02-09 21:05:28", "link": "http://arxiv.org/abs/2202.04725v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Training Data with Language Models: Towards Zero-Shot\n  Language Understanding", "abstract": "Pretrained language models (PLMs) have demonstrated remarkable performance in\nvarious natural language processing tasks: Unidirectional PLMs (e.g., GPT) are\nwell known for their superior text generation capabilities; bidirectional PLMs\n(e.g., BERT) have been the prominent choice for natural language understanding\n(NLU) tasks. While both types of models have achieved promising few-shot\nlearning performance, their potential for zero-shot learning has been\nunderexplored. In this paper, we present a simple approach that uses both types\nof PLMs for fully zero-shot learning of NLU tasks without requiring any\ntask-specific data: A unidirectional PLM generates class-conditioned texts\nguided by prompts, which are used as the training data for fine-tuning a\nbidirectional PLM. With quality training data selected based on the generation\nprobability and regularization techniques (label smoothing and temporal\nensembling) applied to the fine-tuning stage for better generalization and\nstability, our approach demonstrates strong performance across seven\nclassification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and\n92.8 on SST-2), significantly outperforming zero-shot prompting methods and\nachieving even comparable results to strong few-shot approaches using 32\ntraining samples per class.", "published": "2022-02-09 16:02:18", "link": "http://arxiv.org/abs/2202.04538v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Predicting Human Similarity Judgments Using Large Language Models", "abstract": "Similarity judgments provide a well-established method for accessing mental\nrepresentations, with applications in psychology, neuroscience and machine\nlearning. However, collecting similarity judgments can be prohibitively\nexpensive for naturalistic datasets as the number of comparisons grows\nquadratically in the number of stimuli. One way to tackle this problem is to\nconstruct approximation procedures that rely on more accessible proxies for\npredicting similarity. Here we leverage recent advances in language models and\nonline recruitment, proposing an efficient domain-general procedure for\npredicting human similarity judgments based on text descriptions. Intuitively,\nsimilar stimuli are likely to evoke similar descriptions, allowing us to use\ndescription similarity to predict pairwise similarity judgments. Crucially, the\nnumber of descriptions required grows only linearly with the number of stimuli,\ndrastically reducing the amount of data required. We test this procedure on six\ndatasets of naturalistic images and show that our models outperform previous\napproaches based on visual information.", "published": "2022-02-09 21:09:25", "link": "http://arxiv.org/abs/2202.04728v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Image Difference Captioning with Pre-training and Contrastive Learning", "abstract": "The Image Difference Captioning (IDC) task aims to describe the visual\ndifferences between two similar images with natural language. The major\nchallenges of this task lie in two aspects: 1) fine-grained visual differences\nthat require learning stronger vision and language association and 2) high-cost\nof manual annotations that leads to limited supervised data. To address these\nchallenges, we propose a new modeling framework following the\npre-training-finetuning paradigm. Specifically, we design three self-supervised\ntasks and contrastive learning strategies to align visual differences and text\ndescriptions at a fine-grained level. Moreover, we propose a data expansion\nstrategy to utilize extra cross-task supervision information, such as data for\nfine-grained image classification, to alleviate the limitation of available\nsupervised IDC data. Extensive experiments on two IDC benchmark datasets,\nCLEVR-Change and Birds-to-Words, demonstrate the effectiveness of the proposed\nmodeling framework. The codes and models will be released at\nhttps://github.com/yaolinli/IDC.", "published": "2022-02-09 06:14:22", "link": "http://arxiv.org/abs/2202.04298v1", "categories": ["cs.MM", "cs.CL", "cs.CV"], "primary_category": "cs.MM"}
{"title": "Can Open Domain Question Answering Systems Answer Visual Knowledge\n  Questions?", "abstract": "The task of Outside Knowledge Visual Question Answering (OKVQA) requires an\nautomatic system to answer natural language questions about pictures and images\nusing external knowledge. We observe that many visual questions, which contain\ndeictic referential phrases referring to entities in the image, can be\nrewritten as \"non-grounded\" questions and can be answered by existing\ntext-based question answering systems. This allows for the reuse of existing\ntext-based Open Domain Question Answering (QA) Systems for visual question\nanswering. In this work, we propose a potentially data-efficient approach that\nreuses existing systems for (a) image analysis, (b) question rewriting, and (c)\ntext-based question answering to answer such visual questions. Given an image\nand a question pertaining to that image (a visual question), we first extract\nthe entities present in the image using pre-trained object and scene\nclassifiers. Using these detected entities, the visual questions can be\nrewritten so as to be answerable by open domain QA systems. We explore two\nrewriting strategies: (1) an unsupervised method using BERT for masking and\nrewriting, and (2) a weakly supervised approach that combines adaptive\nrewriting and reinforcement learning techniques to use the implicit feedback\nfrom the QA system. We test our strategies on the publicly available OKVQA\ndataset and obtain a competitive performance with state-of-the-art models while\nusing only 10% of the training data.", "published": "2022-02-09 06:47:40", "link": "http://arxiv.org/abs/2202.04306v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "pNLP-Mixer: an Efficient all-MLP Architecture for Language", "abstract": "Large pre-trained language models based on transformer architecture have\ndrastically changed the natural language processing (NLP) landscape. However,\ndeploying those models for on-device applications in constrained devices such\nas smart watches is completely impractical due to their size and inference\ncost. As an alternative to transformer-based architectures, recent work on\nefficient NLP has shown that weight-efficient models can attain competitive\nperformance for simple tasks, such as slot filling and intent classification,\nwith model sizes in the order of the megabyte. This work introduces the\npNLP-Mixer architecture, an embedding-free MLP-Mixer model for on-device NLP\nthat achieves high weight-efficiency thanks to a novel projection layer. We\nevaluate a pNLP-Mixer model of only one megabyte in size on two multi-lingual\nsemantic parsing datasets, MTOP and multiATIS. Our quantized model achieves\n99.4% and 97.8% the performance of mBERT on MTOP and multi-ATIS, while using\n170x fewer parameters. Our model consistently beats the state-of-the-art of\ntiny models (pQRNN), which is twice as large, by a margin up to 7.8% on MTOP.", "published": "2022-02-09 09:01:29", "link": "http://arxiv.org/abs/2202.04350v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Topic Discovery via Latent Space Clustering of Pretrained Language Model\n  Representations", "abstract": "Topic models have been the prominent tools for automatic topic discovery from\ntext corpora. Despite their effectiveness, topic models suffer from several\nlimitations including the inability of modeling word ordering information in\ndocuments, the difficulty of incorporating external linguistic knowledge, and\nthe lack of both accurate and efficient inference methods for approximating the\nintractable posterior. Recently, pretrained language models (PLMs) have brought\nastonishing performance improvements to a wide variety of tasks due to their\nsuperior representations of text. Interestingly, there have not been standard\napproaches to deploy PLMs for topic discovery as better alternatives to topic\nmodels. In this paper, we begin by analyzing the challenges of using PLM\nrepresentations for topic discovery, and then propose a joint latent space\nlearning and clustering framework built upon PLM embeddings. In the latent\nspace, topic-word and document-topic distributions are jointly modeled so that\nthe discovered topics can be interpreted by coherent and distinctive terms and\nmeanwhile serve as meaningful summaries of the documents. Our model effectively\nleverages the strong representation power and superb linguistic features\nbrought by PLMs for topic discovery, and is conceptually simpler than topic\nmodels. On two benchmark datasets in different domains, our model generates\nsignificantly more coherent and diverse topics than strong topic models, and\noffers better topic-wise document representations, based on both automatic and\nhuman evaluations.", "published": "2022-02-09 17:26:08", "link": "http://arxiv.org/abs/2202.04582v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FedQAS: Privacy-aware machine reading comprehension with federated\n  learning", "abstract": "Machine reading comprehension (MRC) of text data is one important task in\nNatural Language Understanding. It is a complex NLP problem with a lot of\nongoing research fueled by the release of the Stanford Question Answering\nDataset (SQuAD) and Conversational Question Answering (CoQA). It is considered\nto be an effort to teach computers how to \"understand\" a text, and then to be\nable to answer questions about it using deep learning. However, until now\nlarge-scale training on private text data and knowledge sharing has been\nmissing for this NLP task. Hence, we present FedQAS, a privacy-preserving\nmachine reading system capable of leveraging large-scale private data without\nthe need to pool those datasets in a central location. The proposed approach\ncombines transformer models and federated learning technologies. The system is\ndeveloped using the FEDn framework and deployed as a proof-of-concept alliance\ninitiative. FedQAS is flexible, language-agnostic, and allows intuitive\nparticipation and execution of local model training. In addition, we present\nthe architecture and implementation of the system, as well as provide a\nreference evaluation based on the SQUAD dataset, to showcase how it overcomes\ndata privacy issues and enables knowledge sharing between alliance members in a\nFederated learning setting.", "published": "2022-02-09 22:03:35", "link": "http://arxiv.org/abs/2202.04742v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SHAS: Approaching optimal Segmentation for End-to-End Speech Translation", "abstract": "Speech translation models are unable to directly process long audios, like\nTED talks, which have to be split into shorter segments. Speech translation\ndatasets provide manual segmentations of the audios, which are not available in\nreal-world scenarios, and existing segmentation methods usually significantly\nreduce translation quality at inference time. To bridge the gap between the\nmanual segmentation of training and the automatic one at inference, we propose\nSupervised Hybrid Audio Segmentation (SHAS), a method that can effectively\nlearn the optimal segmentation from any manually segmented speech corpus.\nFirst, we train a classifier to identify the included frames in a segmentation,\nusing speech representations from a pre-trained wav2vec 2.0. The optimal\nsplitting points are then found by a probabilistic Divide-and-Conquer algorithm\nthat progressively splits at the frame of lowest probability until all segments\nare below a pre-specified length. Experiments on MuST-C and mTEDx show that the\ntranslation of the segments produced by our method approaches the quality of\nthe manual segmentation on 5 language pairs. Namely, SHAS retains 95-98% of the\nmanual segmentation's BLEU score, compared to the 87-93% of the best existing\nmethods. Our method is additionally generalizable to different domains and\nachieves high zero-shot performance in unseen languages.", "published": "2022-02-09 23:55:25", "link": "http://arxiv.org/abs/2202.04774v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CAU_KU team's submission to ADD 2022 Challenge task 1: Low-quality fake\n  audio detection through frequency feature masking", "abstract": "This technical report describes Chung-Ang University and Korea University\n(CAU_KU) team's model participating in the Audio Deep Synthesis Detection (ADD)\n2022 Challenge, track 1: Low-quality fake audio detection. For track 1, we\npropose a frequency feature masking (FFM) augmentation technique to deal with a\nlow-quality audio environment. %detection that spectrogram-based models can be\napplied. We applied FFM and mixup augmentation on five spectrogram-based deep\nneural network architectures that performed well for spoofing detection using\nmel-spectrogram and constant Q transform (CQT) features. Our best submission\nachieved 23.8% of EER ranked 3rd on track 1.", "published": "2022-02-09 08:27:41", "link": "http://arxiv.org/abs/2202.04328v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Binaural Audio Rendering in the Spherical Harmonic Domain: A Summary of\n  the Mathematics and its Pitfalls", "abstract": "The present document reviews the mathematics behind binaural rendering of\nsound fields that are available as spherical harmonic expansion coefficients.\nThis process is also known as binaural ambisonic decoding. We highlight that\nthe details entail some amount peculiarity so that one has to be well aware of\nthe precise definitions that are chosen for some of the involved quantities to\nobtain a consistent formulation. We also discuss what sets of definitions\nproduce ambisonic signals that are compatible with the most common software\ntools that are available.", "published": "2022-02-09 11:01:37", "link": "http://arxiv.org/abs/2202.04393v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Volcspeech system for the ICASSP 2022 multi-channel multi-party\n  meeting transcription challenge", "abstract": "This paper describes our submission to ICASSP 2022 Multi-channel Multi-party\nMeeting Transcription (M2MeT) Challenge. For Track 1, we propose several\napproaches to empower the clustering-based speaker diarization system to handle\noverlapped speech. Front-end dereverberation and the direction-of-arrival (DOA)\nestimation are used to improve the accuracy of speaker diarization.\nMulti-channel combination and overlap detection are applied to reduce the\nmissed speaker error. A modified DOVER-Lap is also proposed to fuse the results\nof different systems. We achieve the final DER of 5.79% on the Eval set and\n7.23% on the Test set. For Track 2, we develop our system using the Conformer\nmodel in a joint CTC-attention architecture. Serialized output training is\nadopted to multi-speaker overlapped speech recognition. We propose a neural\nfront-end module to model multi-channel audio and train the model end-to-end.\nVarious data augmentation methods are utilized to mitigate over-fitting in the\nmulti-channel multi-speaker E2E system. Transformer language model fusion is\ndeveloped to achieve better performance. The final CER is 19.2% on the Eval set\nand 20.8% on the Test set.", "published": "2022-02-09 03:38:39", "link": "http://arxiv.org/abs/2202.04261v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TinyM$^2$Net: A Flexible System Algorithm Co-designed Multimodal\n  Learning Framework for Tiny Devices", "abstract": "With the emergence of Artificial Intelligence (AI), new attention has been\ngiven to implement AI algorithms on resource constrained tiny devices to expand\nthe application domain of IoT. Multimodal Learning has recently become very\npopular with the classification task due to its impressive performance for both\nimage and audio event classification. This paper presents TinyM$^2$Net -- a\nflexible system algorithm co-designed multimodal learning framework for\nresource constrained tiny devices. The framework was designed to be evaluated\non two different case-studies: COVID-19 detection from multimodal audio\nrecordings and battle field object detection from multimodal images and audios.\nIn order to compress the model to implement on tiny devices, substantial\nnetwork architecture optimization and mixed precision quantization were\nperformed (mixed 8-bit and 4-bit). TinyM$^2$Net shows that even a tiny\nmultimodal learning model can improve the classification performance than that\nof any unimodal frameworks. The most compressed TinyM$^2$Net achieves 88.4%\nCOVID-19 detection accuracy (14.5% improvement from unimodal base model) and\n96.8% battle field object detection accuracy (3.9% improvement from unimodal\nbase model). Finally, we test our TinyM$^2$Net models on a Raspberry Pi 4 to\nsee how they perform when deployed to a resource constrained tiny device.", "published": "2022-02-09 06:28:45", "link": "http://arxiv.org/abs/2202.04303v3", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Time-Frequency Mask Aware Bi-directional LSTM: A Deep Learning Approach\n  for Underwater Acoustic Signal Separation", "abstract": "The underwater acoustic signals separation is a key technique for the\nunderwater communications. The existing methods are mostly model-based, and\ncould not accurately characterise the practical underwater acoustic\ncommunication environment. They are only suitable for binary signal separation,\nbut cannot handle multivariate signal separation. On the other hand, the\nrecurrent neural network (RNN) shows powerful capability in extracting the\nfeatures of the temporal sequences. Inspired by this, in this paper, we present\na data-driven approach for underwater acoustic signals separation using deep\nlearning technology. We use the Bi-directional Long Short-Term Memory (Bi-LSTM)\nto explore the features of Time-Frequency (T-F) mask, and propose a T-F mask\naware Bi-LSTM for signal separation. Taking advantage of the sparseness of the\nT-F image, the designed Bi-LSTM network is able to extract the discriminative\nfeatures for separation, which further improves the separation performance. In\nparticular, this method breaks through the limitations of the existing methods,\nnot only achieves good results in multivariate separation, but also effectively\nseparates signals when mixed with 40dB Gaussian noise signals. The experimental\nresults show that this method can achieve a $97\\%$ guarantee ratio (PSR), and\nthe average similarity coefficient of the multivariate signal separation is\nstable above 0.8 under high noise conditions.", "published": "2022-02-09 11:34:23", "link": "http://arxiv.org/abs/2202.04405v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Conditional Drums Generation using Compound Word Representations", "abstract": "The field of automatic music composition has seen great progress in recent\nyears, specifically with the invention of transformer-based architectures. When\nusing any deep learning model which considers music as a sequence of events\nwith multiple complex dependencies, the selection of a proper data\nrepresentation is crucial. In this paper, we tackle the task of conditional\ndrums generation using a novel data encoding scheme inspired by the Compound\nWord representation, a tokenization process of sequential data. Therefore, we\npresent a sequence-to-sequence architecture where a Bidirectional Long\nshort-term memory (BiLSTM) Encoder receives information about the conditioning\nparameters (i.e., accompanying tracks and musical attributes), while a\nTransformer-based Decoder with relative global attention produces the generated\ndrum sequences. We conducted experiments to thoroughly compare the\neffectiveness of our method to several baselines. Quantitative evaluation shows\nthat our model is able to generate drums sequences that have similar\nstatistical distributions and characteristics to the training corpus. These\nfeatures include syncopation, compression ratio, and symmetry among others. We\nalso verified, through a listening test, that generated drum sequences sound\npleasant, natural and coherent while they \"groove\" with the given\naccompaniment.", "published": "2022-02-09 13:49:27", "link": "http://arxiv.org/abs/2202.04464v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multimodal Audio-Visual Information Fusion using Canonical-Correlated\n  Graph Neural Network for Energy-Efficient Speech Enhancement", "abstract": "This paper proposes a novel multimodal self-supervised architecture for\nenergy-efficient audio-visual (AV) speech enhancement that integrates Graph\nNeural Networks with canonical correlation analysis (CCA-GNN). The proposed\napproach lays its foundations on a state-of-the-art CCA-GNN that learns\nrepresentative embeddings by maximizing the correlation between pairs of\naugmented views of the same input while decorrelating disconnected features.\nThe key idea of the conventional CCA-GNN involves discarding\naugmentation-variant information and preserving augmentation-invariant\ninformation while preventing capturing of redundant information. Our proposed\nAV CCA-GNN model deals with multimodal representation learning context.\nSpecifically, our model improves contextual AV speech processing by maximizing\ncanonical correlation from augmented views of the same channel and canonical\ncorrelation from audio and visual embeddings. In addition, it proposes a\npositional node encoding that considers a prior-frame sequence distance instead\nof a feature-space representation when computing the node's nearest neighbors,\nintroducing temporal information in the embeddings through the neighborhood's\nconnectivity. Experiments conducted on the benchmark ChiME3 dataset show that\nour proposed prior frame-based AV CCA-GNN ensures better feature learning in\nthe temporal context, leading to more energy-efficient speech reconstruction\nthan state-of-the-art CCA-GNN and multilayer perceptron.", "published": "2022-02-09 15:47:07", "link": "http://arxiv.org/abs/2202.04528v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Neural Architecture Search for Energy Efficient Always-on Audio Models", "abstract": "Mobile and edge computing devices for always-on classification tasks require\nenergy-efficient neural network architectures. In this paper we present several\nchanges to neural architecture searches (NAS) that improve the chance of\nsuccess in practical situations. Our search simultaneously optimizes for\nnetwork accuracy, energy efficiency and memory usage. We benchmark the\nperformance of our search on real hardware, but since running thousands of\ntests with real hardware is difficult we use a random forest model to roughly\npredict the energy usage of a candidate network. We present a search strategy\nthat uses both Bayesian and regularized evolutionary search with particle\nswarms, and employs early-stopping to reduce the computational burden. Our\nsearch, evaluated on a sound-event classification dataset based upon AudioSet,\nresults in an order of magnitude less energy per inference and a much smaller\nmemory footprint than our baseline MobileNetV1/V2 implementations while\nslightly improving task accuracy. We also demonstrate how combining a 2D\nspectrogram with a convolution with many filters causes a computational\nbottleneck for audio classification and that alternative approaches reduce the\ncomputational burden but sacrifice task accuracy.", "published": "2022-02-09 06:10:18", "link": "http://arxiv.org/abs/2202.05397v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "QAC: Quantum-computing Aided Composition", "abstract": "In this chapter I will discuss the role of quantum computing in computer\nmusic and how it can be integrated to better serve the creative artists. I will\nstart by considering different approaches in current computer music and quantum\ncomputing tools, as well as reviewing some previous attempts to integrate them.\nThen, I will reflect on the meaning of this integration and present what I\ncoined as QAC (Quantum-computing Aided Composition) as well as an early attempt\nat realizing it. This chapter will also introduce The QAC Toolkit Max package,\nanalyze its performance, and explore some examples of what it can offer to\nrealtime creative practice. Lastly, I will present a real case scenario of QAC\nin the creative work Disklavier Prelude #3.", "published": "2022-02-09 01:17:21", "link": "http://arxiv.org/abs/2202.04215v1", "categories": ["cs.ET", "cs.HC", "cs.SD", "eess.AS", "quant-ph", "J.5; D.2.2"], "primary_category": "cs.ET"}
