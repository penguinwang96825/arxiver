{"title": "Can market volumes reveal traders' rationality and a new risk premium?", "abstract": "An empirical analysis, suggested by optimal Merton dynamics, reveals some\nunexpected features of asset volumes. These features are connected to traders'\nbelief and risk aversion. This paper proposes a trading strategy model in the\noptimal Merton framework that is representative of the collective behavior of\nheterogeneous rational traders. This model allows for the estimation of the\naverage risk aversion of traders acting on a specific risky asset, while\nrevealing the existence of a price of risk closely related to market price of\nrisk and volume rate. The empirical analysis, conducted on real data, confirms\nthe validity of the proposed model.", "published": "2024-06-09 16:56:01", "link": "http://arxiv.org/abs/2406.05854v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "Electricity Spot Prices Forecasting Using Stochastic Volatility Models", "abstract": "There are several approaches to modeling and forecasting time series as\napplied to prices of commodities and financial assets. One of the approaches is\nto model the price as a non-stationary time series process with heteroscedastic\nvolatility (variance of price). The goal of the research is to generate\nprobabilistic forecasts of day-ahead electricity prices in a spot marker\nemploying stochastic volatility models. A typical stochastic volatility model -\nthat treats the volatility as a latent stochastic process in discrete time - is\nexplored first. Then the research focuses on enriching the baseline model by\nintroducing several exogenous regressors. A better fitting model - as compared\nto the baseline model - is derived as a result of the research. Out-of-sample\nforecasts confirm the applicability and robustness of the enriched model. This\nmodel may be used in financial derivative instruments for hedging the risk\nassociated with electricity trading. Keywords: Electricity spot prices\nforecasting, Stochastic volatility, Exogenous regressors, Autoregression,\nBayesian inference, Stan", "published": "2024-06-09 11:33:44", "link": "http://arxiv.org/abs/2406.19405v1", "categories": ["q-fin.ST", "q-fin.TR"], "primary_category": "q-fin.ST"}
{"title": "Macroscopic Market Making Games", "abstract": "In continuation of the macroscopic market making \\`a la Avellaneda-Stoikov as\na control problem, this paper explores its stochastic game. Concerning the\nprice competition, each agent is compared with the best quote from the others.\nWe start with the linear case. While constructing the solution directly, the\nordering property and the dimension reduction in the equilibrium are revealed.\nFor the non-linear case, extending the decoupling approach, we introduce a\nmultidimensional characteristic equation to study the well-posedness of\nforward-backward stochastic differential equations. Properties of coefficients\nin the characteristic equation are obtained via non-smooth analysis. In\naddition to novel well-posedness results, the linear price impact arises and\nthe impact function can be further decomposed into two parts in some examples.", "published": "2024-06-09 06:37:09", "link": "http://arxiv.org/abs/2406.05662v1", "categories": ["q-fin.TR", "math.PR", "q-fin.MF"], "primary_category": "q-fin.TR"}
{"title": "GrowOVER: How Can LLMs Adapt to Growing Real-World Knowledge?", "abstract": "In the real world, knowledge is constantly evolving, which can render\nexisting knowledge-based datasets outdated. This unreliability highlights the\ncritical need for continuous updates to ensure both accuracy and relevance in\nknowledge-intensive tasks. To address this, we propose GrowOVER-QA and\nGrowOVER-Dialogue, dynamic open-domain QA and dialogue benchmarks that undergo\na continuous cycle of updates, keeping pace with the rapid evolution of\nknowledge. Our research indicates that retrieval-augmented language models\n(RaLMs) struggle with knowledge that has not been trained on or recently\nupdated. Consequently, we introduce a novel retrieval-interactive language\nmodel framework, where the language model evaluates and reflects on its answers\nfor further re-retrieval. Our exhaustive experiments demonstrate that our\ntraining-free framework significantly improves upon existing methods,\nperforming comparably to or even surpassing continuously trained language\nmodels.", "published": "2024-06-09 01:16:04", "link": "http://arxiv.org/abs/2406.05606v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Video-Language Understanding: A Survey from Model Architecture, Model\n  Training, and Data Perspectives", "abstract": "Humans use multiple senses to comprehend the environment. Vision and language\nare two of the most vital senses since they allow us to easily communicate our\nthoughts and perceive the world around us. There has been a lot of interest in\ncreating video-language understanding systems with human-like senses since a\nvideo-language pair can mimic both our linguistic medium and visual environment\nwith temporal dynamics. In this survey, we review the key tasks of these\nsystems and highlight the associated challenges. Based on the challenges, we\nsummarize their methods from model architecture, model training, and data\nperspectives. We also conduct performance comparison among the methods, and\ndiscuss promising directions for future research.", "published": "2024-06-09 02:36:28", "link": "http://arxiv.org/abs/2406.05615v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ATLAS: Improving Lay Summarisation with Attribute-based Control", "abstract": "Lay summarisation aims to produce summaries of scientific articles that are\ncomprehensible to non-expert audiences. However, previous work assumes a\none-size-fits-all approach, where the content and style of the produced summary\nare entirely dependent on the data used to train the model. In practice,\naudiences with different levels of expertise will have specific needs,\nimpacting what content should appear in a lay summary and how it should be\npresented. Aiming to address this, we propose ATLAS, a novel abstractive\nsummarisation approach that can control various properties that contribute to\nthe overall \"layness\" of the generated summary using targeted control\nattributes. We evaluate ATLAS on a combination of biomedical lay summarisation\ndatasets, where it outperforms state-of-the-art baselines using mainstream\nsummarisation metrics. Additional analyses provided on the discriminatory power\nand emergent influence of our selected controllable attributes further attest\nto the effectiveness of our approach.", "published": "2024-06-09 03:22:55", "link": "http://arxiv.org/abs/2406.05625v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked\n  Language Modelling methods for learning Speech Representations", "abstract": "In recent years, self-supervised pre-training methods have gained significant\ntraction in learning high-level information from raw speech. Among these\nmethods, HuBERT has demonstrated SOTA performance in automatic speech\nrecognition (ASR). However, HuBERT's performance lags behind data2vec due to\ndisparities in pre-training strategies. In this paper, we propose (i) a Swap\nmethod to address pre-training and inference mismatch observed in HuBERT and\n(ii) incorporates Multicluster masked prediction loss for more effective\nutilization of the models capacity. The resulting method is, MS-HuBERT, an\nend-to-end self-supervised pre-training method for learning robust speech\nrepresentations. It beats vanilla HuBERT on the ASR Librispeech benchmark on\naverage by a 5% margin when evaluated on different finetuning splits.\nAdditionally, we demonstrate that the learned embeddings obtained during\npre-training encode essential information for improving performance of content\nbased tasks such as ASR.", "published": "2024-06-09 06:30:28", "link": "http://arxiv.org/abs/2406.05661v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context\n  Large Language Models", "abstract": "Extending the functionality of the Transformer model to accommodate longer\nsequence lengths has become a critical challenge. This extension is crucial not\nonly for improving tasks such as language translation and long-context\nprocessing but also for enabling novel applications like chatbots, code\ngeneration, and multimedia content creation. The primary obstacle is the\nself-attention mechanism, which scales quadratically with sequence length in\nterms of computation time and memory requirements. LongLoRA proposed shifted\nsparse attention (S\\(^2\\)-Attn), effectively enabling context extension and\nleading to non-trivial computation savings with similar performance to\nfine-tuning with vanilla attention. However, LongLoRA is still not as efficient\nas vanilla attention, reaching only 39\\% of the perplexity improvement compared\nto full attention. This inefficiency is due to the cyclic shift applied within\ndifferent attention head patterns, causing either chaos in the attention head\nstructure or unnecessary information exchange between token groups. To address\nthese issues, We propose \\textbf{SinkLoRA}, which features better work\npartitioning. Specifically, (1) we developed SF-Attn with a segmentation and\nreassembly algorithm to proportionally return cyclically shifted groups of\nattention heads to their un-shifted state together with global attention of\n\"sink attention tokens\", achieving 92\\% of the perplexity improvement compared\nto full attention after fine tuning, and (2) applied a SOTA KV cache\ncompression algorithm H$_2$O to accelerate inference. Furthermore, We conducted\nsupervised fine-tuning with SinkLoRA using a self collected LongAlpaca-plus\ndataset. All our code, models, datasets, and demos are available at\n\\url{https://github.com/Dexter-GT-86/SinkLoRA}.", "published": "2024-06-09 07:23:34", "link": "http://arxiv.org/abs/2406.05678v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MoPS: Modular Story Premise Synthesis for Open-Ended Automatic Story\n  Generation", "abstract": "A story premise succinctly defines a story's main idea, foundation, and\ntrajectory. It serves as the initial trigger in automatic story generation.\nExisting sources of story premises are limited by a lack of diversity, uneven\nquality, and high costs that make them difficult to scale. In response, we\nintroduce Modular Story Premise Synthesis (MoPS) which breaks down story\npremises into modules like background and persona for automated design and\ngeneration. MoPS consists of three phases: (1) Precollect a consistent set of\ncandidates for each module to form a nested dictionary. (2) Extract a key path\nfrom the nested dictionary as the premise design. (3) Instruct an LLM to\nintegrate the design into a coherent premise sentence. Thorough evaluations\ndemonstrate that our synthesized premises excel in diversity, fascination,\ncompleteness, and originality compared to those induced from large language\nmodels and captured from public story datasets. Similarly, the extended novels\nand scripts generated from our premises also exhibit higher quality. In\nsupplementary materials, we provide the MoPS code suite, along with 7.6k\ngenerated premises and 1k extended stories. Code:\nhttps://github.com/GAIR-NLP/MoPS.", "published": "2024-06-09 08:31:14", "link": "http://arxiv.org/abs/2406.05690v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MrRank: Improving Question Answering Retrieval System through\n  Multi-Result Ranking Model", "abstract": "Large Language Models (LLMs) often struggle with hallucinations and outdated\ninformation. To address this, Information Retrieval (IR) systems can be\nemployed to augment LLMs with up-to-date knowledge. However, existing IR\ntechniques contain deficiencies, posing a performance bottleneck. Given the\nextensive array of IR systems, combining diverse approaches presents a viable\nstrategy. Nevertheless, prior attempts have yielded restricted efficacy. In\nthis work, we propose an approach that leverages learning-to-rank techniques to\ncombine heterogeneous IR systems. We demonstrate the method on two Retrieval\nQuestion Answering (ReQA) tasks. Our empirical findings exhibit a significant\nperformance enhancement, outperforming previous approaches and achieving\nstate-of-the-art results on ReQA SQuAD.", "published": "2024-06-09 11:00:01", "link": "http://arxiv.org/abs/2406.05733v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Arabic Diacritics in the Wild: Exploiting Opportunities for Improved\n  Diacritization", "abstract": "The widespread absence of diacritical marks in Arabic text poses a\nsignificant challenge for Arabic natural language processing (NLP). This paper\nexplores instances of naturally occurring diacritics, referred to as\n\"diacritics in the wild,\" to unveil patterns and latent information across six\ndiverse genres: news articles, novels, children's books, poetry, political\ndocuments, and ChatGPT outputs. We present a new annotated dataset that maps\nreal-world partially diacritized words to their maximal full diacritization in\ncontext. Additionally, we propose extensions to the analyze-and-disambiguate\napproach in Arabic NLP to leverage these diacritics, resulting in notable\nimprovements. Our contributions encompass a thorough analysis, valuable\ndatasets, and an extended diacritization algorithm. We release our code and\ndatasets as open source.", "published": "2024-06-09 12:29:55", "link": "http://arxiv.org/abs/2406.05760v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The BiGGen Bench: A Principled Benchmark for Fine-grained Evaluation of\n  Language Models with Language Models", "abstract": "As language models (LMs) become capable of handling a wide range of tasks,\ntheir evaluation is becoming as challenging as their development. Most\ngeneration benchmarks currently assess LMs using abstract evaluation criteria\nlike helpfulness and harmlessness, which often lack the flexibility and\ngranularity of human assessment. Additionally, these benchmarks tend to focus\ndisproportionately on specific capabilities such as instruction following,\nleading to coverage bias. To overcome these limitations, we introduce the\nBiGGen Bench, a principled generation benchmark designed to thoroughly evaluate\nnine distinct capabilities of LMs across 77 diverse tasks. A key feature of the\nBiGGen Bench is its use of instance-specific evaluation criteria, closely\nmirroring the nuanced discernment of human evaluation. We apply this benchmark\nto assess 103 frontier LMs using five evaluator LMs. Our code, data, and\nevaluation results are all publicly available at\nhttps://github.com/prometheus-eval/prometheus-eval/tree/main/BiGGen-Bench.", "published": "2024-06-09 12:30:30", "link": "http://arxiv.org/abs/2406.05761v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MedREQAL: Examining Medical Knowledge Recall of Large Language Models\n  via Question Answering", "abstract": "In recent years, Large Language Models (LLMs) have demonstrated an impressive\nability to encode knowledge during pre-training on large text corpora. They can\nleverage this knowledge for downstream tasks like question answering (QA), even\nin complex areas involving health topics. Considering their high potential for\nfacilitating clinical work in the future, understanding the quality of encoded\nmedical knowledge and its recall in LLMs is an important step forward. In this\nstudy, we examine the capability of LLMs to exhibit medical knowledge recall by\nconstructing a novel dataset derived from systematic reviews -- studies\nsynthesizing evidence-based answers for specific medical questions. Through\nexperiments on the new MedREQAL dataset, comprising question-answer pairs\nextracted from rigorous systematic reviews, we assess six LLMs, such as GPT and\nMixtral, analyzing their classification and generation performance. Our\nexperimental insights into LLM performance on the novel biomedical QA dataset\nreveal the still challenging nature of this task.", "published": "2024-06-09 16:33:28", "link": "http://arxiv.org/abs/2406.05845v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Large Language Models Actually Good at Text Style Transfer?", "abstract": "We analyze the performance of large language models (LLMs) on Text Style\nTransfer (TST), specifically focusing on sentiment transfer and text\ndetoxification across three languages: English, Hindi, and Bengali. Text Style\nTransfer involves modifying the linguistic style of a text while preserving its\ncore content. We evaluate the capabilities of pre-trained LLMs using zero-shot\nand few-shot prompting as well as parameter-efficient finetuning on publicly\navailable datasets. Our evaluation using automatic metrics, GPT-4 and human\nevaluations reveals that while some prompted LLMs perform well in English,\ntheir performance in on other languages (Hindi, Bengali) remains average.\nHowever, finetuning significantly improves results compared to zero-shot and\nfew-shot prompting, making them comparable to previous state-of-the-art. This\nunderscores the necessity of dedicated datasets and specialized models for\neffective TST.", "published": "2024-06-09 18:45:41", "link": "http://arxiv.org/abs/2406.05885v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Feriji: A French-Zarma Parallel Corpus, Glossary & Translator", "abstract": "Machine translation (MT) is a rapidly expanding field that has experienced\nsignificant advancements in recent years with the development of models capable\nof translating multiple languages with remarkable accuracy. However, the\nrepresentation of African languages in this field still needs to improve due to\nlinguistic complexities and limited resources. This applies to the Zarma\nlanguage, a dialect of Songhay (of the Nilo-Saharan language family) spoken by\nover 5 million people across Niger and neighboring countries\n\\cite{lewis2016ethnologue}. This paper introduces Feriji, the first robust\nFrench-Zarma parallel corpus and glossary designed for MT. The corpus,\ncontaining 61,085 sentences in Zarma and 42,789 in French, and a glossary of\n4,062 words represent a significant step in addressing the need for more\nresources for Zarma. We fine-tune three large language models on our dataset,\nobtaining a BLEU score of 30.06 on the best-performing model. We further\nevaluate the models on human judgments of fluency, comprehension, and\nreadability and the importance and impact of the corpus and models. Our\ncontributions help to bridge a significant language gap and promote an\nessential and overlooked indigenous African language.", "published": "2024-06-09 19:08:33", "link": "http://arxiv.org/abs/2406.05888v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semisupervised Neural Proto-Language Reconstruction", "abstract": "Existing work implementing comparative reconstruction of ancestral languages\n(proto-languages) has usually required full supervision. However, historical\nreconstruction models are only of practical value if they can be trained with a\nlimited amount of labeled data. We propose a semisupervised historical\nreconstruction task in which the model is trained on only a small amount of\nlabeled data (cognate sets with proto-forms) and a large amount of unlabeled\ndata (cognate sets without proto-forms). We propose a neural architecture for\ncomparative reconstruction (DPD-BiReconstructor) incorporating an essential\ninsight from linguists' comparative method: that reconstructed words should not\nonly be reconstructable from their daughter words, but also deterministically\ntransformable back into their daughter words. We show that this architecture is\nable to leverage unlabeled cognate sets to outperform strong semisupervised\nbaselines on this novel task.", "published": "2024-06-09 22:46:41", "link": "http://arxiv.org/abs/2406.05930v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Prompt Modifiers Control Bias? A Comparative Analysis of\n  Text-to-Image Generative Models", "abstract": "It has been shown that many generative models inherit and amplify societal\nbiases. To date, there is no uniform/systematic agreed standard to\ncontrol/adjust for these biases. This study examines the presence and\nmanipulation of societal biases in leading text-to-image models: Stable\nDiffusion, DALL-E 3, and Adobe Firefly. Through a comprehensive analysis\ncombining base prompts with modifiers and their sequencing, we uncover the\nnuanced ways these AI technologies encode biases across gender, race,\ngeography, and region/culture. Our findings reveal the challenges and potential\nof prompt engineering in controlling biases, highlighting the critical need for\nethical AI development promoting diversity and inclusivity.\n  This work advances AI ethics by not only revealing the nuanced dynamics of\nbias in text-to-image generation models but also by offering a novel framework\nfor future research in controlling bias. Our contributions-panning comparative\nanalyses, the strategic use of prompt modifiers, the exploration of prompt\nsequencing effects, and the introduction of a bias sensitivity taxonomy-lay the\ngroundwork for the development of common metrics and standard analyses for\nevaluating whether and how future AI models exhibit and respond to requests to\nadjust for inherent biases.", "published": "2024-06-09 00:54:57", "link": "http://arxiv.org/abs/2406.05602v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "DomainRAG: A Chinese Benchmark for Evaluating Domain-specific\n  Retrieval-Augmented Generation", "abstract": "Retrieval-Augmented Generation (RAG) offers a promising solution to address\nvarious limitations of Large Language Models (LLMs), such as hallucination and\ndifficulties in keeping up with real-time updates. This approach is\nparticularly critical in expert and domain-specific applications where LLMs\nstruggle to cover expert knowledge. Therefore, evaluating RAG models in such\nscenarios is crucial, yet current studies often rely on general knowledge\nsources like Wikipedia to assess the models' abilities in solving common-sense\nproblems. In this paper, we evaluated LLMs by RAG settings in a domain-specific\ncontext, college enrollment. We identified six required abilities for RAG\nmodels, including the ability in conversational RAG, analyzing structural\ninformation, faithfulness to external knowledge, denoising, solving\ntime-sensitive problems, and understanding multi-document interactions. Each\nability has an associated dataset with shared corpora to evaluate the RAG\nmodels' performance. We evaluated popular LLMs such as Llama, Baichuan,\nChatGLM, and GPT models. Experimental results indicate that existing\nclosed-book LLMs struggle with domain-specific questions, highlighting the need\nfor RAG models to solve expert problems. Moreover, there is room for RAG models\nto improve their abilities in comprehending conversational history, analyzing\nstructural information, denoising, processing multi-document interactions, and\nfaithfulness in expert knowledge. We expect future studies could solve these\nproblems better.", "published": "2024-06-09 05:33:51", "link": "http://arxiv.org/abs/2406.05654v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Do LLMs Exhibit Human-Like Reasoning? Evaluating Theory of Mind in LLMs\n  for Open-Ended Responses", "abstract": "Theory of Mind (ToM) reasoning entails recognizing that other individuals\npossess their own intentions, emotions, and thoughts, which is vital for\nguiding one's own thought processes. Although large language models (LLMs)\nexcel in tasks such as summarization, question answering, and translation, they\nstill face challenges with ToM reasoning, especially in open-ended questions.\nDespite advancements, the extent to which LLMs truly understand ToM reasoning\nand how closely it aligns with human ToM reasoning remains inadequately\nexplored in open-ended scenarios. Motivated by this gap, we assess the\nabilities of LLMs to perceive and integrate human intentions and emotions into\ntheir ToM reasoning processes within open-ended questions. Our study utilizes\nposts from Reddit's ChangeMyView platform, which demands nuanced social\nreasoning to craft persuasive responses. Our analysis, comparing semantic\nsimilarity and lexical overlap metrics between responses generated by humans\nand LLMs, reveals clear disparities in ToM reasoning capabilities in open-ended\nquestions, with even the most advanced models showing notable limitations. To\nenhance LLM capabilities, we implement a prompt tuning method that incorporates\nhuman intentions and emotions, resulting in improvements in ToM reasoning\nperformance. However, despite these improvements, the enhancement still falls\nshort of fully achieving human-like reasoning. This research highlights the\ndeficiencies in LLMs' social reasoning and demonstrates how integrating human\nintentions and emotions can boost their effectiveness.", "published": "2024-06-09 05:57:59", "link": "http://arxiv.org/abs/2406.05659v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Flow of Reasoning:Training LLMs for Divergent Problem Solving with\n  Minimal Examples", "abstract": "The ability to generate diverse solutions to a given problem is a hallmark of\nhuman creativity. This divergent reasoning is also crucial for machines,\nenhancing their robustness and enabling them to assist humans in many\napplications such as scientific discovery. However, existing approaches to\nmulti-step reasoning with large language models (LLMs) have mostly focused only\non reasoning accuracy, without further discovering more diverse valid\nsolutions. For example, supervised fine-tuning can improve LLM reasoning\nquality, but requires extensive supervised data to capture the full range of\npossible solutions. Reward-maximization reinforcement learning aims to find\nlimited highest-reward solutions while neglecting the solution diversity. To\nfill this gap, we propose Flow of Reasoning (FoR), an efficient\ndiversity-seeking LLM finetuning method aimed at improving reasoning quality\nand diversity with minimal data. FoR formulates multi-step LLM reasoning as a\nMarkovian flow on a DAG-structured reasoning graph. This formulation allows us\nto incorporate and adapt principled GFlowNet approaches, for finetuning LLMs to\nsample divergent paths with probabilities proportional to the (unnormalized)\nreward of target problems. Extensive experiments show that, with limited\ntraining examples (e.g., 15 examples), FoR enables the discovery of diverse,\ncreative, high-quality solutions, greatly outperforming a wide range of\nexisting inference and training methods across six challenging reasoning tasks,\nincluding BlocksWorld (embodied reasoning), Game24 (math puzzle solving),\nRubik's Cube (spatial reasoning), 1D-ARC (abstraction reasoning), GSM8k (math\nreasoning), and ProntoQA (logical reasoning). Code is available at\nhttps://github.com/Yu-Fangxu/FoR.", "published": "2024-06-09 07:06:58", "link": "http://arxiv.org/abs/2406.05673v5", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "QGEval: Benchmarking Multi-dimensional Evaluation for Question\n  Generation", "abstract": "Automatically generated questions often suffer from problems such as unclear\nexpression or factual inaccuracies, requiring a reliable and comprehensive\nevaluation of their quality. Human evaluation is widely used in the field of\nquestion generation (QG) and serves as the gold standard for automatic metrics.\nHowever, there is a lack of unified human evaluation criteria, which hampers\nconsistent and reliable evaluations of both QG models and automatic metrics. To\naddress this, we propose QGEval, a multi-dimensional Evaluation benchmark for\nQuestion Generation, which evaluates both generated questions and existing\nautomatic metrics across 7 dimensions: fluency, clarity, conciseness,\nrelevance, consistency, answerability, and answer consistency. We demonstrate\nthe appropriateness of these dimensions by examining their correlations and\ndistinctions. Through consistent evaluations of QG models and automatic metrics\nwith QGEval, we find that 1) most QG models perform unsatisfactorily in terms\nof answerability and answer consistency, and 2) existing metrics fail to align\nwell with human judgments when evaluating generated questions across the 7\ndimensions. We expect this work to foster the development of both QG\ntechnologies and their evaluation.", "published": "2024-06-09 09:51:55", "link": "http://arxiv.org/abs/2406.05707v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RE-RAG: Improving Open-Domain QA Performance and Interpretability with\n  Relevance Estimator in Retrieval-Augmented Generation", "abstract": "The Retrieval Augmented Generation (RAG) framework utilizes a combination of\nparametric knowledge and external knowledge to demonstrate state-of-the-art\nperformance on open-domain question answering tasks. However, the RAG framework\nsuffers from performance degradation when the query is accompanied by\nirrelevant contexts. In this work, we propose the RE-RAG framework, which\nintroduces a relevance estimator (RE) that not only provides relative relevance\nbetween contexts as previous rerankers did, but also provides confidence, which\ncan be used to classify whether given context is useful for answering the given\nquestion. We propose a weakly supervised method for training the RE simply\nutilizing question-answer data without any labels for correct contexts. We show\nthat RE trained with a small generator (sLM) can not only improve the sLM\nfine-tuned together with RE but also improve previously unreferenced large\nlanguage models (LLMs). Furthermore, we investigate new decoding strategies\nthat utilize the proposed confidence measured by RE such as choosing to let the\nuser know that it is \"unanswerable\" to answer the question given the retrieved\ncontexts or choosing to rely on LLM's parametric knowledge rather than\nunrelated contexts.", "published": "2024-06-09 14:11:19", "link": "http://arxiv.org/abs/2406.05794v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Seventeenth-Century Spanish American Notary Records for Fine-Tuning\n  Spanish Large Language Models", "abstract": "Large language models have gained tremendous popularity in domains such as\ne-commerce, finance, healthcare, and education. Fine-tuning is a common\napproach to customize an LLM on a domain-specific dataset for a desired\ndownstream task. In this paper, we present a valuable resource for fine-tuning\nLLMs developed for the Spanish language to perform a variety of tasks such as\nclassification, masked language modeling, clustering, and others. Our resource\nis a collection of handwritten notary records from the seventeenth century\nobtained from the National Archives of Argentina. This collection contains a\ncombination of original images and transcribed text (and metadata) of 160+\npages that were handwritten by two notaries, namely, Estenban Agreda de Vergara\nand Nicolas de Valdivia y Brisuela nearly 400 years ago. Through empirical\nevaluation, we demonstrate that our collection can be used to fine-tune Spanish\nLLMs for tasks such as classification and masked language modeling, and can\noutperform pre-trained Spanish models and ChatGPT-3.5/ChatGPT-4o. Our resource\nwill be an invaluable resource for historical text analysis and is publicly\navailable on GitHub.", "published": "2024-06-09 14:54:22", "link": "http://arxiv.org/abs/2406.05812v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TTM-RE: Memory-Augmented Document-Level Relation Extraction", "abstract": "Document-level relation extraction aims to categorize the association between\nany two entities within a document. We find that previous methods for\ndocument-level relation extraction are ineffective in exploiting the full\npotential of large amounts of training data with varied noise levels. For\nexample, in the ReDocRED benchmark dataset, state-of-the-art methods trained on\nthe large-scale, lower-quality, distantly supervised training data generally do\nnot perform better than those trained solely on the smaller, high-quality,\nhuman-annotated training data. To unlock the full potential of large-scale\nnoisy training data for document-level relation extraction, we propose TTM-RE,\na novel approach that integrates a trainable memory module, known as the Token\nTuring Machine, with a noisy-robust loss function that accounts for the\npositive-unlabeled setting. Extensive experiments on ReDocRED, a benchmark\ndataset for document-level relation extraction, reveal that TTM-RE achieves\nstate-of-the-art performance (with an absolute F1 score improvement of over\n3%). Ablation studies further illustrate the superiority of TTM-RE in other\ndomains (the ChemDisGene dataset in the biomedical domain) and under highly\nunlabeled settings.", "published": "2024-06-09 20:18:58", "link": "http://arxiv.org/abs/2406.05906v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hello Again! LLM-powered Personalized Agent for Long-term Dialogue", "abstract": "Open-domain dialogue systems have seen remarkable advancements with the\ndevelopment of large language models (LLMs). Nonetheless, most existing\ndialogue systems predominantly focus on brief single-session interactions,\nneglecting the real-world demands for long-term companionship and personalized\ninteractions with chatbots. Crucial to addressing this real-world need are\nevent summary and persona management, which enable reasoning for appropriate\nlong-term dialogue responses. Recent progress in the human-like cognitive and\nreasoning capabilities of LLMs suggests that LLM-based agents could\nsignificantly enhance automated perception, decision-making, and\nproblem-solving. In response to this potential, we introduce a model-agnostic\nframework, the Long-term Dialogue Agent (LD-Agent), which incorporates three\nindependently tunable modules dedicated to event perception, persona\nextraction, and response generation. For the event memory module, long and\nshort-term memory banks are employed to separately focus on historical and\nongoing sessions, while a topic-based retrieval mechanism is introduced to\nenhance the accuracy of memory retrieval. Furthermore, the persona module\nconducts dynamic persona modeling for both users and agents. The integration of\nretrieved memories and extracted personas is subsequently fed into the\ngenerator to induce appropriate responses. The effectiveness, generality, and\ncross-domain capabilities of LD-Agent are empirically demonstrated across\nvarious illustrative benchmarks, models, and tasks. The code is released at\nhttps://github.com/leolee99/LD-Agent.", "published": "2024-06-09 21:58:32", "link": "http://arxiv.org/abs/2406.05925v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM Questionnaire Completion for Automatic Psychiatric Assessment", "abstract": "We employ a Large Language Model (LLM) to convert unstructured psychological\ninterviews into structured questionnaires spanning various psychiatric and\npersonality domains. The LLM is prompted to answer these questionnaires by\nimpersonating the interviewee. The obtained answers are coded as features,\nwhich are used to predict standardized psychiatric measures of depression\n(PHQ-8) and PTSD (PCL-C), using a Random Forest regressor. Our approach is\nshown to enhance diagnostic accuracy compared to multiple baselines. It thus\nestablishes a novel framework for interpreting unstructured psychological\ninterviews, bridging the gap between narrative-driven and data-driven\napproaches for mental health assessment.", "published": "2024-06-09 09:03:11", "link": "http://arxiv.org/abs/2406.06636v1", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Investigation of the Impact of Economic and Social Factors on Energy\n  Demand through Natural Language Processing", "abstract": "The relationship between energy demand and variables such as economic\nactivity and weather is well established. However, this paper aims to explore\nthe connection between energy demand and other social aspects, which receive\nlittle attention. Through the use of natural language processing on a large\nnews corpus, we shed light on this important link. This study was carried out\nin five regions of the UK and Ireland and considers multiple horizons from 1 to\n30 days. It also considers economic variables such as GDP, unemployment and\ninflation. We found that: 1) News about military conflicts, transportation, the\nglobal pandemic, regional economics, and the international energy market are\nrelated to electricity demand. 2) Economic indicators are more important in the\nEast Midlands and Northern Ireland, while social indicators are more useful in\nthe West Midlands and the South West of England. 3) The use of these indices\nimproved forecasting performance by up to 9%.", "published": "2024-06-09 16:25:14", "link": "http://arxiv.org/abs/2406.06641v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Reality check of the benefits of LLM in business", "abstract": "Large language models (LLMs) have achieved remarkable performance in language\nunderstanding and generation tasks by leveraging vast amounts of online texts.\nUnlike conventional models, LLMs can adapt to new domains through prompt\nengineering without the need for retraining, making them suitable for various\nbusiness functions, such as strategic planning, project implementation, and\ndata-driven decision-making. However, their limitations in terms of bias,\ncontextual understanding, and sensitivity to prompts raise concerns about their\nreadiness for real-world applications. This paper thoroughly examines the\nusefulness and readiness of LLMs for business processes. The limitations and\ncapacities of LLMs are evaluated through experiments conducted on four\naccessible LLMs using real-world data. The findings have significant\nimplications for organizations seeking to leverage generative AI and provide\nvaluable insights into future research directions. To the best of our\nknowledge, this represents the first quantified study of LLMs applied to core\nbusiness operations and challenges.", "published": "2024-06-09 02:36:00", "link": "http://arxiv.org/abs/2406.10249v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "How Alignment and Jailbreak Work: Explain LLM Safety through\n  Intermediate Hidden States", "abstract": "Large language models (LLMs) rely on safety alignment to avoid responding to\nmalicious user inputs. Unfortunately, jailbreak can circumvent safety\nguardrails, resulting in LLMs generating harmful content and raising concerns\nabout LLM safety. Due to language models with intensive parameters often\nregarded as black boxes, the mechanisms of alignment and jailbreak are\nchallenging to elucidate. In this paper, we employ weak classifiers to explain\nLLM safety through the intermediate hidden states. We first confirm that LLMs\nlearn ethical concepts during pre-training rather than alignment and can\nidentify malicious and normal inputs in the early layers. Alignment actually\nassociates the early concepts with emotion guesses in the middle layers and\nthen refines them to the specific reject tokens for safe generations. Jailbreak\ndisturbs the transformation of early unethical classification into negative\nemotions. We conduct experiments on models from 7B to 70B across various model\nfamilies to prove our conclusion. Overall, our paper indicates the intrinsical\nmechanism of LLM safety and how jailbreaks circumvent safety guardrails,\noffering a new perspective on LLM safety and reducing concerns. Our code is\navailable at https://github.com/ydyjya/LLM-IHS-Explanation.", "published": "2024-06-09 05:04:37", "link": "http://arxiv.org/abs/2406.05644v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.CY"], "primary_category": "cs.CL"}
{"title": "A Superalignment Framework in Autonomous Driving with Large Language\n  Models", "abstract": "Over the last year, significant advancements have been made in the realms of\nlarge language models (LLMs) and multi-modal large language models (MLLMs),\nparticularly in their application to autonomous driving. These models have\nshowcased remarkable abilities in processing and interacting with complex\ninformation. In autonomous driving, LLMs and MLLMs are extensively used,\nrequiring access to sensitive vehicle data such as precise locations, images,\nand road conditions. These data are transmitted to an LLM-based inference cloud\nfor advanced analysis. However, concerns arise regarding data security, as the\nprotection against data and privacy breaches primarily depends on the LLM's\ninherent security measures, without additional scrutiny or evaluation of the\nLLM's inference outputs. Despite its importance, the security aspect of LLMs in\nautonomous driving remains underexplored. Addressing this gap, our research\nintroduces a novel security framework for autonomous vehicles, utilizing a\nmulti-agent LLM approach. This framework is designed to safeguard sensitive\ninformation associated with autonomous vehicles from potential leaks, while\nalso ensuring that LLM outputs adhere to driving regulations and align with\nhuman values. It includes mechanisms to filter out irrelevant queries and\nverify the safety and reliability of LLM outputs. Utilizing this framework, we\nevaluated the security, privacy, and cost aspects of eleven large language\nmodel-driven autonomous driving cues. Additionally, we performed QA tests on\nthese driving prompts, which successfully demonstrated the framework's\nefficacy.", "published": "2024-06-09 05:26:38", "link": "http://arxiv.org/abs/2406.05651v1", "categories": ["cs.RO", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based\n  Interactions", "abstract": "Large Language Models (LLMs) have demonstrated wide-ranging applications\nacross various fields and have shown significant potential in the academic\npeer-review process. However, existing applications are primarily limited to\nstatic review generation based on submitted papers, which fail to capture the\ndynamic and iterative nature of real-world peer reviews. In this paper, we\nreformulate the peer-review process as a multi-turn, long-context dialogue,\nincorporating distinct roles for authors, reviewers, and decision makers. We\nconstruct a comprehensive dataset containing over 26,841 papers with 92,017\nreviews collected from multiple sources, including the top-tier conference and\nprestigious journal. This dataset is meticulously designed to facilitate the\napplications of LLMs for multi-turn dialogues, effectively simulating the\ncomplete peer-review process. Furthermore, we propose a series of metrics to\nevaluate the performance of LLMs for each role under this reformulated\npeer-review setting, ensuring fair and comprehensive evaluations. We believe\nthis work provides a promising perspective on enhancing the LLM-driven\npeer-review process by incorporating dynamic, role-based interactions. It\naligns closely with the iterative and interactive nature of real-world academic\npeer review, offering a robust foundation for future research and development\nin this area. We open-source the dataset at\nhttps://github.com/chengtan9907/ReviewMT.", "published": "2024-06-09 08:24:17", "link": "http://arxiv.org/abs/2406.05688v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks\n  with Large Vision-Language Models", "abstract": "The recent rapid development of Large Vision-Language Models (LVLMs) has\nindicated their potential for embodied tasks.However, the critical skill of\nspatial understanding in embodied environments has not been thoroughly\nevaluated, leaving the gap between current LVLMs and qualified embodied\nintelligence unknown. Therefore, we construct EmbSpatial-Bench, a benchmark for\nevaluating embodied spatial understanding of LVLMs.The benchmark is\nautomatically derived from embodied scenes and covers 6 spatial relationships\nfrom an egocentric perspective.Experiments expose the insufficient capacity of\ncurrent LVLMs (even GPT-4V). We further present EmbSpatial-SFT, an\ninstruction-tuning dataset designed to improve LVLMs' embodied spatial\nunderstanding.", "published": "2024-06-09 12:23:14", "link": "http://arxiv.org/abs/2406.05756v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.AI"}
{"title": "Set-CLIP: Exploring Aligned Semantic From Low-Alignment Multimodal Data\n  Through A Distribution View", "abstract": "Multimodal fusion breaks through the boundaries between diverse modalities\nand has already achieved notable performances. However, in many specialized\nfields, it is struggling to obtain sufficient alignment data for training,\nwhich seriously limits the use of previously effective models. Therefore,\nsemi-supervised learning approaches are attempted to facilitate multimodal\nalignment by learning from low-alignment data with fewer matched pairs, but\ntraditional techniques like pseudo-labeling may run into troubles in the\nlabel-deficient scenarios. To tackle these challenges, we reframe\nsemi-supervised multimodal alignment as a manifold matching issue and propose a\nnew methodology based on CLIP, termed Set-CLIP. Specifically, by designing a\nnovel semantic density distribution loss, we constrain the latent\nrepresentation distribution with fine granularity and extract implicit semantic\nalignment from unpaired multimodal data, thereby reducing the reliance on\nnumerous strictly matched pairs. Furthermore, we apply coarse-grained modality\nadaptation and unimodal self-supervised guidance to narrow the gaps between\nmodality spaces and improve the stability of representation distributions.\nExtensive experiments conducted on a range of tasks in various fields,\nincluding protein analysis, remote sensing, and the general vision-language\nfield, validate the efficacy of our proposed Set-CLIP method. Especially with\nno paired data for supervised training, Set-CLIP is still outstanding, which\nbrings an improvement of 144.83% over CLIP.", "published": "2024-06-09 12:41:14", "link": "http://arxiv.org/abs/2406.05766v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Hidden Holes: topological aspects of language models", "abstract": "We explore the topology of representation manifolds arising in autoregressive\nneural language models trained on raw text data. In order to study their\nproperties, we introduce tools from computational algebraic topology, which we\nuse as a basis for a measure of topological complexity, that we call\nperforation.\n  Using this measure, we study the evolution of topological structure in GPT\nbased large language models across depth and time during training. We then\ncompare these to gated recurrent models, and show that the latter exhibit more\ntopological complexity, with a distinct pattern of changes common to all\nnatural languages but absent from synthetically generated data. The paper\npresents a detailed analysis of the representation manifolds derived by these\nmodels based on studying the shapes of vector clouds induced by them as they\nare conditioned on sentences from corpora of natural language text.\n  The methods developed in this paper are novel in the field and based on\nmathematical apparatus that might be unfamiliar to the target audience. To help\nwith that we introduce the minimum necessary theory, and provide additional\nvisualizations in the appendices.\n  The main contribution of the paper is a striking observation about the\ntopological structure of the transformer as compared to LSTM based neural\narchitectures. It suggests that further research into mathematical properties\nof these neural networks is necessary to understand the operation of large\ntransformer language models. We hope this work inspires further explorations in\nthis direction within the NLP community.", "published": "2024-06-09 14:25:09", "link": "http://arxiv.org/abs/2406.05798v1", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
{"title": "A Review of Prominent Paradigms for LLM-Based Agents: Tool Use\n  (Including RAG), Planning, and Feedback Learning", "abstract": "Tool use, planning, and feedback learning are currently three prominent\nparadigms for developing Large Language Model (LLM)-based agents across various\ntasks. Although numerous frameworks have been devised for each paradigm, their\nintricate workflows and inconsistent taxonomy create challenges in\nunderstanding and reviewing the frameworks across different paradigms. This\nsurvey introduces a unified taxonomy to systematically review and discuss these\nframeworks. Specifically, 1) the taxonomy defines environments/tasks, common\nLLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models),\nand universally applicable workflows found in prior work, and 2) it enables a\ncomparison of key perspectives on the implementations of LMPRs and workflow\ndesigns across different agent paradigms and frameworks. 3) Finally, we\nidentify three limitations in existing workflow designs and systematically\ndiscuss the future work. Resources have been made publicly available at in our\nGitHub repository https://github.com/xinzhel/LLM-Agent-Survey.", "published": "2024-06-09 14:42:55", "link": "http://arxiv.org/abs/2406.05804v6", "categories": ["cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.AI"}
{"title": "Do Prompts Really Prompt? Exploring the Prompt Understanding Capability\n  of Whisper", "abstract": "This research explores how the information of prompts interacts with the\nhigh-performing speech recognition model, Whisper. We compare its performances\nwhen prompted by prompts with correct information and those corrupted with\nincorrect information. Our results unexpectedly show that Whisper may not\nunderstand the textual prompts in a human-expected way. Additionally, we find\nthat performance improvement is not guaranteed even with stronger adherence to\nthe topic information in textual prompts. It is also noted that English prompts\ngenerally outperform Mandarin ones on datasets of both languages, likely due to\ndifferences in training data distributions for these languages despite the\nmismatch with pre-training scenarios. Conversely, we discover that Whisper\nexhibits awareness of misleading information in language tokens by ignoring\nincorrect language tokens and focusing on the correct ones. In sum, We raise\ninsightful questions about Whisper's prompt understanding and reveal its\ncounter-intuitive behaviors. We encourage further studies.", "published": "2024-06-09 14:44:59", "link": "http://arxiv.org/abs/2406.05806v4", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "II-Bench: An Image Implication Understanding Benchmark for Multimodal\n  Large Language Models", "abstract": "The rapid advancements in the development of multimodal large language models\n(MLLMs) have consistently led to new breakthroughs on various benchmarks. In\nresponse, numerous challenging and comprehensive benchmarks have been proposed\nto more accurately assess the capabilities of MLLMs. However, there is a dearth\nof exploration of the higher-order perceptual capabilities of MLLMs. To fill\nthis gap, we propose the Image Implication understanding Benchmark, II-Bench,\nwhich aims to evaluate the model's higher-order perception of images. Through\nextensive experiments on II-Bench across multiple MLLMs, we have made\nsignificant findings. Initially, a substantial gap is observed between the\nperformance of MLLMs and humans on II-Bench. The pinnacle accuracy of MLLMs\nattains 74.8%, whereas human accuracy averages 90%, peaking at an impressive\n98%. Subsequently, MLLMs perform worse on abstract and complex images,\nsuggesting limitations in their ability to understand high-level semantics and\ncapture image details. Finally, it is observed that most models exhibit\nenhanced accuracy when image sentiment polarity hints are incorporated into the\nprompts. This observation underscores a notable deficiency in their inherent\nunderstanding of image sentiment. We believe that II-Bench will inspire the\ncommunity to develop the next generation of MLLMs, advancing the journey\ntowards expert artificial general intelligence (AGI). II-Bench is publicly\navailable at https://huggingface.co/datasets/m-a-p/II-Bench.", "published": "2024-06-09 17:25:47", "link": "http://arxiv.org/abs/2406.05862v3", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Machine Against the RAG: Jamming Retrieval-Augmented Generation with\n  Blocker Documents", "abstract": "Retrieval-augmented generation (RAG) systems respond to queries by retrieving\nrelevant documents from a knowledge database and applying an LLM to the\nretrieved documents. We demonstrate that RAG systems that operate on databases\nwith untrusted content are vulnerable to denial-of-service attacks we call\njamming. An adversary can add a single ``blocker'' document to the database\nthat will be retrieved in response to a specific query and result in the RAG\nsystem not answering this query, ostensibly because it lacks relevant\ninformation or because the answer is unsafe.\n  We describe and measure the efficacy of several methods for generating\nblocker documents, including a new method based on black-box optimization. Our\nmethod (1) does not rely on instruction injection, (2) does not require the\nadversary to know the embedding or LLM used by the target RAG system, and (3)\ndoes not employ an auxiliary LLM.\n  We evaluate jamming attacks on several embeddings and LLMs and demonstrate\nthat the existing safety metrics for LLMs do not capture their vulnerability to\njamming. We then discuss defenses against blocker documents.", "published": "2024-06-09 17:55:55", "link": "http://arxiv.org/abs/2406.05870v4", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "STARLING: Self-supervised Training of Text-based Reinforcement Learning\n  Agent with Large Language Models", "abstract": "Interactive fiction games have emerged as an important application to improve\nthe generalization capabilities of language-based reinforcement learning (RL)\nagents. Existing environments for interactive fiction games are domain-specific\nor time-consuming to generate and do not train the RL agents to master a\nspecific set of skills. In this work, we introduce an interactive environment\nfor self-supervised RL, STARLING, for text-based games that bootstraps the\ntext-based RL agents with automatically generated games (based on the seed set\nof game ideas) to boost the performance and generalization capabilities to\nreach a goal of the target environment. These games let the agent hone their\nskills on a predefined set of tasks. We create and test an environment with 100\ngames, generated using this automated framework that uses large language models\n(GPT-3) and an interactive fiction game engine (based on Inform7) to provide\nthe user with the ability to generate more games under minimal human\nsupervision. Experimental results based on both the human participants and\nbaseline text-based RL agents reveal that current state-of-the-art text-based\nRL agents cannot use previously learned skills in new situations at the level\nhumans can. These results enforce STARLING's potential to serve as a sandbox\nenvironment for further research in self-supervised text-based RL.", "published": "2024-06-09 18:07:47", "link": "http://arxiv.org/abs/2406.05872v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Zero-Shot End-To-End Spoken Question Answering In Medical Domain", "abstract": "In the rapidly evolving landscape of spoken question-answering (SQA), the\nintegration of large language models (LLMs) has emerged as a transformative\ndevelopment. Conventional approaches often entail the use of separate models\nfor question audio transcription and answer selection, resulting in significant\nresource utilization and error accumulation. To tackle these challenges, we\nexplore the effectiveness of end-to-end (E2E) methodologies for SQA in the\nmedical domain. Our study introduces a novel zero-shot SQA approach, compared\nto traditional cascade systems. Through a comprehensive evaluation conducted on\na new open benchmark of 8 medical tasks and 48 hours of synthetic audio, we\ndemonstrate that our approach requires up to 14.7 times fewer resources than a\ncombined 1.3B parameters LLM with a 1.55B parameters ASR model while improving\naverage accuracy by 0.5\\%. These findings underscore the potential of E2E\nmethodologies for SQA in resource-constrained contexts.", "published": "2024-06-09 18:13:36", "link": "http://arxiv.org/abs/2406.05876v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical\n  Reinforcement Learning", "abstract": "Developing interactive systems that utilize natural language instructions to\nsolve complex robotic control tasks has long been a goal of the robotics\ncommunity. While Large Language Models (LLMs) excel at logical reasoning,\nin-context learning, and code generation, translating high-level instructions\ninto low-level robotic actions still remains challenging. Furthermore, solving\nsuch tasks often requires acquiring policies to execute diverse subtasks and\nintegrating them to achieve the final objective. Hierarchical Reinforcement\nLearning (HRL) offers a promising solution for solving such tasks by enabling\ntemporal abstraction and improved exploration. However, HRL suffers from\nnon-stationarity caused by the changing lower-level behaviour, which hinders\neffective policy learning. We propose LGR2, a novel HRL framework that\nmitigates non-stationarity in HRL by using language-guided higher-level rewards\nthat remain unaffected by the changing lower-level policy behaviour. To analyze\nthe efficacy of our approach, we perform empirical analysis to demonstrate that\nLGR2 effectively mitigates non-stationarity in HRL and attains success rates\nexceeding 70% in challenging, sparsely-rewarded robotic navigation and\nmanipulation environments, where other baselines typically fail to show\nsignificant progress. Finally, we perform real-world robotic experiments on\ncomplex tasks and demonstrate that LGR2 consistently outperforms the baselines.", "published": "2024-06-09 18:40:24", "link": "http://arxiv.org/abs/2406.05881v3", "categories": ["cs.LG", "cs.CL", "cs.RO"], "primary_category": "cs.LG"}
{"title": "Whose Preferences? Differences in Fairness Preferences and Their Impact\n  on the Fairness of AI Utilizing Human Feedback", "abstract": "There is a growing body of work on learning from human feedback to align\nvarious aspects of machine learning systems with human values and preferences.\nWe consider the setting of fairness in content moderation, in which human\nfeedback is used to determine how two comments -- referencing different\nsensitive attribute groups -- should be treated in comparison to one another.\nWith a novel dataset collected from Prolific and MTurk, we find significant\ngaps in fairness preferences depending on the race, age, political stance,\neducational level, and LGBTQ+ identity of annotators. We also demonstrate that\ndemographics mentioned in text have a strong influence on how users perceive\nindividual fairness in moderation. Further, we find that differences also exist\nin downstream classifiers trained to predict human preferences. Finally, we\nobserve that an ensemble, giving equal weight to classifiers trained on\nannotations from different demographics, performs better for different\ndemographic intersections; compared to a single classifier that gives equal\nweight to each annotation.", "published": "2024-06-09 19:42:25", "link": "http://arxiv.org/abs/2406.05902v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Why Don't Prompt-Based Fairness Metrics Correlate?", "abstract": "The widespread use of large language models has brought up essential\nquestions about the potential biases these models might learn. This led to the\ndevelopment of several metrics aimed at evaluating and mitigating these biases.\nIn this paper, we first demonstrate that prompt-based fairness metrics exhibit\npoor agreement, as measured by correlation, raising important questions about\nthe reliability of fairness assessment using prompts. Then, we outline six\nrelevant reasons why such a low correlation is observed across existing\nmetrics. Based on these insights, we propose a method called Correlated\nFairness Output (CAIRO) to enhance the correlation between fairness metrics.\nCAIRO augments the original prompts of a given fairness metric by using several\npre-trained language models and then selects the combination of the augmented\nprompts that achieves the highest correlation across metrics. We show a\nsignificant improvement in Pearson correlation from 0.3 and 0.18 to 0.90 and\n0.98 across metrics for gender and religion biases, respectively. Our code is\navailable at https://github.com/chandar-lab/CAIRO.", "published": "2024-06-09 21:12:15", "link": "http://arxiv.org/abs/2406.05918v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "3D-MolT5: Leveraging Discrete Structural Information for Molecule-Text\n  Modeling", "abstract": "The integration of molecular and natural language representations has emerged\nas a focal point in molecular science, with recent advancements in Language\nModels (LMs) demonstrating significant potential for comprehensive modeling of\nboth domains. However, existing approaches face notable limitations,\nparticularly in their neglect of three-dimensional (3D) information, which is\ncrucial for understanding molecular structures and functions. While some\nefforts have been made to incorporate 3D molecular information into LMs using\nexternal structure encoding modules, significant difficulties remain, such as\ninsufficient interaction across modalities in pre-training and challenges in\nmodality alignment. To address the limitations, we propose \\textbf{3D-MolT5}, a\nunified framework designed to model molecule in both sequence and 3D structure\nspaces. The key innovation of our approach lies in mapping fine-grained 3D\nsubstructure representations into a specialized 3D token vocabulary. This\nmethodology facilitates the seamless integration of sequence and structure\nrepresentations in a tokenized format, enabling 3D-MolT5 to encode molecular\nsequences, molecular structures, and text sequences within a unified\narchitecture. Leveraging this tokenized input strategy, we build a foundation\nmodel that unifies the sequence and structure data formats. We then conduct\njoint pre-training with multi-task objectives to enhance the model's\ncomprehension of these diverse modalities within a shared representation space.\nThus, our approach significantly improves cross-modal interaction and\nalignment, addressing key challenges in previous work. Further instruction\ntuning demonstrated that our 3D-MolT5 has strong generalization ability and\nsurpasses existing methods with superior performance in multiple downstream\ntasks. Our code is available at https://github.com/QizhiPei/3D-MolT5.", "published": "2024-06-09 14:20:55", "link": "http://arxiv.org/abs/2406.05797v2", "categories": ["q-bio.BM", "cs.AI", "cs.CE", "cs.CL", "cs.LG"], "primary_category": "q-bio.BM"}
{"title": "TIGeR: Unifying Text-to-Image Generation and Retrieval with Large\n  Multimodal Models", "abstract": "How humans can effectively and efficiently acquire images has always been a\nperennial question. A classic solution is text-to-image retrieval from an\nexisting database; however, the limited database typically lacks creativity. By\ncontrast, recent breakthroughs in text-to-image generation have made it\npossible to produce attractive and counterfactual visual content, but it faces\nchallenges in synthesizing knowledge-intensive images. In this work, we rethink\nthe relationship between text-to-image generation and retrieval, proposing a\nunified framework for both tasks with one single Large Multimodal Model (LMM).\nSpecifically, we first explore the intrinsic discriminative abilities of LMMs\nand introduce an efficient generative retrieval method for text-to-image\nretrieval in a training-free manner. Subsequently, we unify generation and\nretrieval autoregressively and propose an autonomous decision mechanism to\nchoose the best-matched one between generated and retrieved images as the\nresponse to the text prompt. To standardize the evaluation of unified\ntext-to-image generation and retrieval, we construct TIGeR-Bench, a benchmark\nspanning both creative and knowledge-intensive domains. Extensive experiments\non TIGeR-Bench and two retrieval benchmarks, i.e., Flickr30K and MS-COCO,\ndemonstrate the superiority of our proposed framework.", "published": "2024-06-09 15:00:28", "link": "http://arxiv.org/abs/2406.05814v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Separating the \"Chirp\" from the \"Chat\": Self-supervised Visual Grounding\n  of Sound and Language", "abstract": "We present DenseAV, a novel dual encoder grounding architecture that learns\nhigh-resolution, semantically meaningful, and audio-visually aligned features\nsolely through watching videos. We show that DenseAV can discover the\n``meaning'' of words and the ``location'' of sounds without explicit\nlocalization supervision. Furthermore, it automatically discovers and\ndistinguishes between these two types of associations without supervision. We\nshow that DenseAV's localization abilities arise from a new multi-head feature\naggregation operator that directly compares dense image and audio\nrepresentations for contrastive learning. In contrast, many other systems that\nlearn ``global'' audio and video representations cannot localize words and\nsound. Finally, we contribute two new datasets to improve the evaluation of AV\nrepresentations through speech and sound prompted semantic segmentation. On\nthese and other datasets we show DenseAV dramatically outperforms the prior art\non speech and sound prompted semantic segmentation. DenseAV outperforms the\nprevious state-of-the-art, ImageBind, on cross-modal retrieval using fewer than\nhalf of the parameters. Project Page:\n\\href{https://aka.ms/denseav}{https://aka.ms/denseav}", "published": "2024-06-09 03:38:21", "link": "http://arxiv.org/abs/2406.05629v1", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Text-aware and Context-aware Expressive Audiobook Speech Synthesis", "abstract": "Recent advances in text-to-speech have significantly improved the\nexpressiveness of synthetic speech. However, a major challenge remains in\ngenerating speech that captures the diverse styles exhibited by professional\nnarrators in audiobooks without relying on manually labeled data or reference\nspeech. To address this problem, we propose a text-aware and\ncontext-aware(TACA) style modeling approach for expressive audiobook speech\nsynthesis. We first establish a text-aware style space to cover diverse styles\nvia contrastive learning with the supervision of the speech style. Meanwhile,\nwe adopt a context encoder to incorporate cross-sentence information and the\nstyle embedding obtained from text. Finally, we introduce the context encoder\nto two typical TTS models, VITS-based TTS and language model-based TTS.\nExperimental results demonstrate that our proposed approach can effectively\ncapture diverse styles and coherent prosody, and consequently improves\nnaturalness and expressiveness in audiobook speech synthesis.", "published": "2024-06-09 07:04:03", "link": "http://arxiv.org/abs/2406.05672v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "WenetSpeech4TTS: A 12,800-hour Mandarin TTS Corpus for Large Speech\n  Generation Model Benchmark", "abstract": "With the development of large text-to-speech (TTS) models and scale-up of the\ntraining data, state-of-the-art TTS systems have achieved impressive\nperformance. In this paper, we present WenetSpeech4TTS, a multi-domain Mandarin\ncorpus derived from the open-sourced WenetSpeech dataset. Tailored for the\ntext-to-speech tasks, we refined WenetSpeech by adjusting segment boundaries,\nenhancing the audio quality, and eliminating speaker mixing within each\nsegment. Following a more accurate transcription process and quality-based data\nfiltering process, the obtained WenetSpeech4TTS corpus contains $12,800$ hours\nof paired audio-text data. Furthermore, we have created subsets of varying\nsizes, categorized by segment quality scores to allow for TTS model training\nand fine-tuning. VALL-E and NaturalSpeech 2 systems are trained and fine-tuned\non these subsets to validate the usability of WenetSpeech4TTS, establishing\nbaselines on benchmark for fair comparison of TTS systems. The corpus and\ncorresponding benchmarks are publicly available on huggingface.", "published": "2024-06-09 12:32:42", "link": "http://arxiv.org/abs/2406.05763v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Towards Expressive Zero-Shot Speech Synthesis with Hierarchical Prosody\n  Modeling", "abstract": "Recent research in zero-shot speech synthesis has made significant progress\nin speaker similarity. However, current efforts focus on timbre generalization\nrather than prosody modeling, which results in limited naturalness and\nexpressiveness. To address this, we introduce a novel speech synthesis model\ntrained on large-scale datasets, including both timbre and hierarchical prosody\nmodeling. As timbre is a global attribute closely linked to expressiveness, we\nadopt a global vector to model speaker timbre while guiding prosody modeling.\nBesides, given that prosody contains both global consistency and local\nvariations, we introduce a diffusion model as the pitch predictor and employ a\nprosody adaptor to model prosody hierarchically, further enhancing the prosody\nquality of the synthesized speech. Experimental results show that our model not\nonly maintains comparable timbre quality to the baseline but also exhibits\nbetter naturalness and expressiveness.", "published": "2024-06-09 07:33:57", "link": "http://arxiv.org/abs/2406.05681v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MaLa-ASR: Multimedia-Assisted LLM-Based ASR", "abstract": "As more and more information-rich data like video become available, utilizing\nmulti-modal auxiliary information to enhance audio tasks has sparked widespread\nresearch interest. The recent surge in research on LLM-based audio models\nprovides fresh perspectives for tackling audio tasks. Given that LLM can\nflexibly ingest multiple inputs, we propose MaLa-ASR, an LLM-based ASR model\nthat can integrate textual keywords extracted from presentation slides to\nimprove recognition of conference content. MaLa-ASR yields average WERs of 9.4%\nand 11.7% on the L95 and S95 subsets of the SlideSpeech corpus, representing a\nsignificant relative WER drop of 27.9% and 44.7% over the baseline model\nreported in SlideSpeech. MaLa-ASR underscores LLM's strong performance in\nspeech tasks and the capability to integrate auxiliary information\nconveniently. By adding keywords to the input prompt, the biased word error\nrate (B-WER) reduces relatively by 46.0% and 44.2%, establishing a new SOTA on\nthis dataset.", "published": "2024-06-09 16:00:00", "link": "http://arxiv.org/abs/2406.05839v2", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Heart Sound Segmentation Using Deep Learning Techniques", "abstract": "Heart disease remains a leading cause of mortality worldwide. Auscultation,\nthe process of listening to heart sounds, can be enhanced through\ncomputer-aided analysis using Phonocardiogram (PCG) signals. This paper\npresents a novel approach for heart sound segmentation and classification into\nS1 (LUB) and S2 (DUB) sounds. We employ FFT-based filtering, dynamic\nprogramming for event detection, and a Siamese network for robust\nclassification. Our method demonstrates superior performance on the PASCAL\nheart sound dataset compared to existing approaches.", "published": "2024-06-09 05:30:05", "link": "http://arxiv.org/abs/2406.05653v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SPA-SVC: Self-supervised Pitch Augmentation for Singing Voice Conversion", "abstract": "Diffusion-based singing voice conversion (SVC) models have shown better\nsynthesis quality compared to traditional methods. However, in cross-domain SVC\nscenarios, where there is a significant disparity in pitch between the source\nand target voice domains, the models tend to generate audios with hoarseness,\nposing challenges in achieving high-quality vocal outputs. Therefore, in this\npaper, we propose a Self-supervised Pitch Augmentation method for Singing Voice\nConversion (SPA-SVC), which can enhance the voice quality in SVC tasks without\nrequiring additional data or increasing model parameters. We innovatively\nintroduce a cycle pitch shifting training strategy and Structural Similarity\nIndex (SSIM) loss into our SVC model, effectively enhancing its performance.\nExperimental results on the public singing datasets M4Singer indicate that our\nproposed method significantly improves model performance in both general SVC\nscenarios and particularly in cross-domain SVC scenarios.", "published": "2024-06-09 08:34:01", "link": "http://arxiv.org/abs/2406.05692v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Investigation of Noise Robustness for Flow-Matching-Based Zero-Shot\n  TTS", "abstract": "Recently, zero-shot text-to-speech (TTS) systems, capable of synthesizing any\nspeaker's voice from a short audio prompt, have made rapid advancements.\nHowever, the quality of the generated speech significantly deteriorates when\nthe audio prompt contains noise, and limited research has been conducted to\naddress this issue. In this paper, we explored various strategies to enhance\nthe quality of audio generated from noisy audio prompts within the context of\nflow-matching-based zero-shot TTS. Our investigation includes comprehensive\ntraining strategies: unsupervised pre-training with masked speech denoising,\nmulti-speaker detection and DNSMOS-based data filtering on the pre-training\ndata, and fine-tuning with random noise mixing. The results of our experiments\ndemonstrate significant improvements in intelligibility, speaker similarity,\nand overall audio quality compared to the approach of applying speech\nenhancement to the audio prompt.", "published": "2024-06-09 08:51:50", "link": "http://arxiv.org/abs/2406.05699v1", "categories": ["eess.AS", "cs.AI", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Optimizing Multi-Stuttered Speech Classification: Leveraging Whisper's\n  Encoder for Efficient Parameter Reduction in Automated Assessment", "abstract": "The automated classification of stuttered speech has significant implications\nfor timely assessments providing assistance to speech language pathologists.\nDespite notable advancements in the field, the cases in which multiple\ndisfluencies occur in speech require attention. We have taken a progressive\napproach to fill this gap by classifying multi-stuttered speech more\nefficiently. The problem has been addressed by firstly curating a dataset of\nmulti-stuttered disfluencies from open source dataset SEP-28k audio clips.\nSecondly, employing Whisper, a state-of-the-art speech recognition model has\nbeen leveraged by using its encoder and taking the problem as multi label\nclassification. Thirdly, using a 6 encoder layer Whisper and experimenting with\nvarious layer freezing strategies, a computationally efficient configuration of\nthe model was identified. The proposed configuration achieved micro, macro, and\nweighted F1-scores of 0.88, 0.85, and 0.87, correspondingly on an external test\ndataset i.e. Fluency-Bank. In addition, through layer freezing strategies, we\nwere able to achieve the aforementioned results by fine-tuning a single encoder\nlayer, consequently, reducing the model's trainable parameters from 20.27\nmillion to 3.29 million. This research study unveils the contribution of the\nlast encoder layer in the identification of disfluencies in stuttered speech.\nConsequently, it has led to a computationally efficient approach, 83.7% less\nparameters to train, making the proposed approach more adaptable for various\ndialects and languages.", "published": "2024-06-09 13:42:51", "link": "http://arxiv.org/abs/2406.05784v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Source -Free Domain Adaptation for Speaker Verification in Data-Scarce\n  Languages and Noisy Channels", "abstract": "Domain adaptation is often hampered by exceedingly small target datasets and\ninaccessible source data. These conditions are prevalent in speech\nverification, where privacy policies and/or languages with scarce speech\nresources limit the availability of sufficient data. This paper explored\ntechniques of sourcefree domain adaptation unto a limited target speech dataset\nfor speaker verificationin data-scarce languages. Both language and channel\nmis-match between source and target were investigated. Fine-tuning methods were\nevaluated and compared across different sizes of labeled target data. A novel\niterative cluster-learn algorithm was studied for unlabeled target datasets.", "published": "2024-06-09 17:27:20", "link": "http://arxiv.org/abs/2406.05863v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Soundscape Captioning using Sound Affective Quality Network and Large\n  Language Model", "abstract": "We live in a rich and varied acoustic world, which is experienced by\nindividuals or communities as a soundscape. Computational auditory scene\nanalysis, disentangling acoustic scenes by detecting and classifying events,\nfocuses on objective attributes of sounds, such as their category and temporal\ncharacteristics, ignoring their effects on people, such as the emotions they\nevoke within a context. To fill this gap, we propose the soundscape captioning\ntask, which enables automated soundscape analysis, thus avoiding\nlabour-intensive subjective ratings and surveys in conventional methods. With\nsoundscape captioning, context-aware descriptions are generated for soundscape\nby capturing the acoustic scene, event information, and the corresponding human\naffective qualities (AQs). To this end, we propose an automatic soundscape\ncaptioner (SoundSCaper) system composed of an acoustic model, i.e. SoundAQnet,\nand a large language model (LLM). SoundAQnet simultaneously models multi-scale\ninformation about acoustic scenes, events, and perceived AQs, while the LLM\ndescribes the soundscape with captions by parsing the information captured with\nSoundAQnet. The soundscape caption's quality is assessed by a jury of 16\naudio/soundscape experts. The average score (out of 5) of SoundSCaper-generated\ncaptions is lower than the score of captions generated by two soundscape\nexperts by 0.21 and 0.25, respectively, on the evaluation set and the\nmodel-unknown mixed external dataset with varying lengths and acoustic\nproperties, but the differences are not statistically significant. Overall, the\nproposed SoundSCaper shows promising performance, with captions generated being\ncomparable to those annotated by soundscape experts. The code of models, LLM\nscripts, human assessment data and instructions, and expert evaluation\nstatistics are all publicly available.", "published": "2024-06-09 20:56:38", "link": "http://arxiv.org/abs/2406.05914v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Contrastive Learning from Synthetic Audio Doppelg\u00e4ngers", "abstract": "Learning robust audio representations currently demands extensive datasets of\nreal-world sound recordings. By applying artificial transformations to these\nrecordings, models can learn to recognize similarities despite subtle\nvariations through techniques like contrastive learning. However, these\ntransformations are only approximations of the true diversity found in\nreal-world sounds, which are generated by complex interactions of physical\nprocesses, from vocal cord vibrations to the resonance of musical instruments.\nWe propose a solution to both the data scale and transformation limitations,\nleveraging synthetic audio. By randomly perturbing the parameters of a sound\nsynthesizer, we generate audio doppelg\\\"angers-synthetic positive pairs with\ncausally manipulated variations in timbre, pitch, and temporal envelopes. These\nvariations, difficult to achieve through augmentations of existing audio,\nprovide a rich source of contrastive information. Despite the shift to randomly\ngenerated synthetic data, our method produces strong representations,\noutperforming real data on several standard audio classification tasks.\nNotably, our approach is lightweight, requires no data storage, and has only a\nsingle hyperparameter, which we extensively analyze. We offer this method as a\ncomplement to existing strategies for contrastive learning in audio, using\nsynthesized sounds to reduce the data burden on practitioners.", "published": "2024-06-09 21:44:06", "link": "http://arxiv.org/abs/2406.05923v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sparse Binarization for Fast Keyword Spotting", "abstract": "With the increasing prevalence of voice-activated devices and applications,\nkeyword spotting (KWS) models enable users to interact with technology\nhands-free, enhancing convenience and accessibility in various contexts.\nDeploying KWS models on edge devices, such as smartphones and embedded systems,\noffers significant benefits for real-time applications, privacy, and bandwidth\nefficiency. However, these devices often possess limited computational power\nand memory. This necessitates optimizing neural network models for efficiency\nwithout significantly compromising their accuracy. To address these challenges,\nwe propose a novel keyword-spotting model based on sparse input representation\nfollowed by a linear classifier. The model is four times faster than the\nprevious state-of-the-art edge device-compatible model with better accuracy. We\nshow that our method is also more robust in noisy environments while being\nfast. Our code is available at: https://github.com/jsvir/sparknet.", "published": "2024-06-09 08:03:48", "link": "http://arxiv.org/abs/2406.06634v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
