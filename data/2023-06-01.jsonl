{"title": "Focused Prefix Tuning for Controllable Text Generation", "abstract": "In a controllable text generation dataset, there exist unannotated attributes\nthat could provide irrelevant learning signals to models that use it for\ntraining and thus degrade their performance. We propose focused prefix\ntuning(FPT) to mitigate the problem and to enable the control to focus on the\ndesired attribute. Experimental results show that FPT can achieve better\ncontrol accuracy and text fluency than baseline models in single-attribute\ncontrol tasks. In multi-attribute control tasks, FPT achieves comparable\ncontrol accuracy with the state-of-the-art approach while keeping the\nflexibility to control new attributes without retraining existing models.", "published": "2023-06-01 06:00:43", "link": "http://arxiv.org/abs/2306.00369v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Preference-grounded Token-level Guidance for Language Model Fine-tuning", "abstract": "Aligning language models (LMs) with preferences is an important problem in\nnatural language generation. A key challenge is that preferences are typically\nprovided at the sequence level while LM training and generation both occur at\nthe token level. There is, therefore, a granularity mismatch between the\npreference and the LM training losses, which may complicate the learning\nproblem. In this paper, we address this issue by developing an alternate\ntraining process, where we iterate between grounding the sequence-level\npreference into token-level training guidance, and improving the LM with the\nlearned guidance. For guidance learning, we design a framework that extends the\npairwise-preference learning in imitation learning to both variable-length LM\ngeneration and the utilization of the preference among multiple generations.\nFor LM training, based on the amount of supervised data, we present two\nminimalist learning objectives that utilize the learned guidance. In\nexperiments, our method performs competitively on two distinct representative\nLM tasks -- discrete-prompt generation and text summarization.", "published": "2023-06-01 07:00:07", "link": "http://arxiv.org/abs/2306.00398v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BiSync: A Bilingual Editor for Synchronized Monolingual Texts", "abstract": "In our globalized world, a growing number of situations arise where people\nare required to communicate in one or several foreign languages. In the case of\nwritten communication, users with a good command of a foreign language may find\nassistance from computer-aided translation (CAT) technologies. These\ntechnologies often allow users to access external resources, such as\ndictionaries, terminologies or bilingual concordancers, thereby interrupting\nand considerably hindering the writing process. In addition, CAT systems assume\nthat the source sentence is fixed and also restrict the possible changes on the\ntarget side. In order to make the writing process smoother, we present BiSync,\na bilingual writing assistant that allows users to freely compose text in two\nlanguages, while maintaining the two monolingual texts synchronized. We also\ninclude additional functionalities, such as the display of alternative prefix\ntranslations and paraphrases, which are intended to facilitate the authoring of\ntexts. We detail the model architecture used for synchronization and evaluate\nthe resulting tool, showing that high accuracy can be attained with limited\ncomputational resources. The interface and models are publicly available at\nhttps://github.com/jmcrego/BiSync and a demonstration video can be watched on\nYouTube at https://youtu.be/_l-ugDHfNgU .", "published": "2023-06-01 07:03:47", "link": "http://arxiv.org/abs/2306.00400v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Divide, Conquer, and Combine: Mixture of Semantic-Independent Experts\n  for Zero-Shot Dialogue State Tracking", "abstract": "Zero-shot transfer learning for Dialogue State Tracking (DST) helps to handle\na variety of task-oriented dialogue domains without the cost of collecting\nin-domain data. Existing works mainly study common data- or model-level\naugmentation methods to enhance the generalization but fail to effectively\ndecouple the semantics of samples, limiting the zero-shot performance of DST.\nIn this paper, we present a simple and effective \"divide, conquer and combine\"\nsolution, which explicitly disentangles the semantics of seen data, and\nleverages the performance and robustness with the mixture-of-experts mechanism.\nSpecifically, we divide the seen data into semantically independent subsets and\ntrain corresponding experts, the newly unseen samples are mapped and inferred\nwith mixture-of-experts with our designed ensemble inference. Extensive\nexperiments on MultiWOZ2.1 upon the T5-Adapter show our schema significantly\nand consistently improves the zero-shot performance, achieving the SOTA on\nsettings without external knowledge, with only 10M trainable parameters1.", "published": "2023-06-01 08:21:20", "link": "http://arxiv.org/abs/2306.00434v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Many Answers Should I Give? An Empirical Study of Multi-Answer\n  Reading Comprehension", "abstract": "The multi-answer phenomenon, where a question may have multiple answers\nscattered in the document, can be well handled by humans but is challenging\nenough for machine reading comprehension (MRC) systems. Despite recent progress\nin multi-answer MRC, there lacks a systematic analysis of how this phenomenon\narises and how to better address it. In this work, we design a taxonomy to\ncategorize commonly-seen multi-answer MRC instances, with which we inspect\nthree multi-answer datasets and analyze where the multi-answer challenge comes\nfrom. We further analyze how well different paradigms of current multi-answer\nMRC models deal with different types of multi-answer instances. We find that\nsome paradigms capture well the key information in the questions while others\nbetter model the relationship between questions and contexts. We thus explore\nstrategies to make the best of the strengths of different paradigms.\nExperiments show that generation models can be a promising platform to\nincorporate different paradigms. Our annotations and code are released for\nfurther research.", "published": "2023-06-01 08:22:21", "link": "http://arxiv.org/abs/2306.00435v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Responsibility Perspective Transfer for Italian Femicide News", "abstract": "Different ways of linguistically expressing the same real-world event can\nlead to different perceptions of what happened. Previous work has shown that\ndifferent descriptions of gender-based violence (GBV) influence the reader's\nperception of who is to blame for the violence, possibly reinforcing\nstereotypes which see the victim as partly responsible, too. As a contribution\nto raise awareness on perspective-based writing, and to facilitate access to\nalternative perspectives, we introduce the novel task of automatically\nrewriting GBV descriptions as a means to alter the perceived level of\nresponsibility on the perpetrator. We present a quasi-parallel dataset of\nsentences with low and high perceived responsibility levels for the\nperpetrator, and experiment with unsupervised (mBART-based), zero-shot and\nfew-shot (GPT3-based) methods for rewriting sentences. We evaluate our models\nusing a questionnaire study and a suite of automatic metrics.", "published": "2023-06-01 08:27:00", "link": "http://arxiv.org/abs/2306.00437v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Anisotropy and Outliers in Multilingual Language Models for\n  Cross-Lingual Semantic Sentence Similarity", "abstract": "Previous work has shown that the representations output by contextual\nlanguage models are more anisotropic than static type embeddings, and typically\ndisplay outlier dimensions. This seems to be true for both monolingual and\nmultilingual models, although much less work has been done on the multilingual\ncontext. Why these outliers occur and how they affect the representations is\nstill an active area of research. We investigate outlier dimensions and their\nrelationship to anisotropy in multiple pre-trained multilingual language\nmodels. We focus on cross-lingual semantic similarity tasks, as these are\nnatural tasks for evaluating multilingual representations. Specifically, we\nexamine sentence representations. Sentence transformers which are fine-tuned on\nparallel resources (that are not always available) perform better on this task,\nand we show that their representations are more isotropic. However, we aim to\nimprove multilingual representations in general. We investigate how much of the\nperformance difference can be made up by only transforming the embedding space\nwithout fine-tuning, and visualise the resulting spaces. We test different\noperations: Removing individual outlier dimensions, cluster-based isotropy\nenhancement, and ZCA whitening. We publish our code for reproducibility.", "published": "2023-06-01 09:01:48", "link": "http://arxiv.org/abs/2306.00458v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual Distortion Reveals Constituency: Masked Language Models are\n  Implicit Parsers", "abstract": "Recent advancements in pre-trained language models (PLMs) have demonstrated\nthat these models possess some degree of syntactic awareness. To leverage this\nknowledge, we propose a novel chart-based method for extracting parse trees\nfrom masked language models (LMs) without the need to train separate parsers.\nOur method computes a score for each span based on the distortion of contextual\nrepresentations resulting from linguistic perturbations. We design a set of\nperturbations motivated by the linguistic concept of constituency tests, and\nuse these to score each span by aggregating the distortion scores. To produce a\nparse tree, we use chart parsing to find the tree with the minimum score. Our\nmethod consistently outperforms previous state-of-the-art methods on English\nwith masked LMs, and also demonstrates superior performance in a multilingual\nsetting, outperforming the state of the art in 6 out of 8 languages. Notably,\nalthough our method does not involve parameter updates or extensive\nhyperparameter search, its performance can even surpass some unsupervised\nparsing methods that require fine-tuning. Our analysis highlights that the\ndistortion of contextual representation resulting from syntactic perturbation\ncan serve as an effective indicator of constituency across languages.", "published": "2023-06-01 13:10:48", "link": "http://arxiv.org/abs/2306.00645v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Polish to English Neural Machine Translation with Transfer\n  Learning: Effects of Data Volume and Language Similarity", "abstract": "This paper investigates the impact of data volume and the use of similar\nlanguages on transfer learning in a machine translation task. We find out that\nhaving more data generally leads to better performance, as it allows the model\nto learn more patterns and generalizations from the data. However, related\nlanguages can also be particularly effective when there is limited data\navailable for a specific language pair, as the model can leverage the\nsimilarities between the languages to improve performance. To demonstrate, we\nfine-tune mBART model for a Polish-English translation task using the OPUS-100\ndataset. We evaluate the performance of the model under various transfer\nlearning configurations, including different transfer source languages and\ndifferent shot levels for Polish, and report the results. Our experiments show\nthat a combination of related languages and larger amounts of data outperforms\nthe model trained on related languages or larger amounts of data alone.\nAdditionally, we show the importance of related languages in zero-shot and\nfew-shot configurations.", "published": "2023-06-01 13:34:21", "link": "http://arxiv.org/abs/2306.00660v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Glossary of Clinical Terminology: a Large-Scale Dictionary of\n  Biomedical Definitions Generated from Ontological Knowledge", "abstract": "Background: More than 400,000 biomedical concepts and some of their\nrelationships are contained in SnomedCT, a comprehensive biomedical ontology.\nHowever, their concept names are not always readily interpretable by\nnon-experts, or patients looking at their own electronic health records (EHR).\nClear definitions or descriptions in understandable language are often not\navailable. Therefore, generating human-readable definitions for biomedical\nconcepts might help make the information they encode more accessible and\nunderstandable to a wider public.\n  Objective: In this article, we introduce the Automatic Glossary of Clinical\nTerminology (AGCT), a large-scale biomedical dictionary of clinical concepts\ngenerated using high-quality information extracted from the biomedical\nknowledge contained in SnomedCT.\n  Methods: We generate a novel definition for every SnomedCT concept, after\nprompting the OpenAI Turbo model, a variant of GPT 3.5, using a high-quality\nverbalization of the SnomedCT relationships of the to-be-defined concept. A\nsignificant subset of the generated definitions was subsequently judged by NLP\nresearchers with biomedical expertise on 5-point scales along the following\nthree axes: factuality, insight, and fluency.\n  Results: AGCT contains 422,070 computer-generated definitions for SnomedCT\nconcepts, covering various domains such as diseases, procedures, drugs, and\nanatomy. The average length of the definitions is 49 words. The definitions\nwere assigned average scores of over 4.5 out of 5 on all three axes, indicating\na majority of factual, insightful, and fluent definitions.\n  Conclusion: AGCT is a novel and valuable resource for biomedical tasks that\nrequire human-readable definitions for SnomedCT concepts. It can also serve as\na base for developing robust biomedical retrieval models or other applications\nthat leverage natural language understanding of biomedical knowledge.", "published": "2023-06-01 13:37:55", "link": "http://arxiv.org/abs/2306.00665v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Argument-Aware Abstractive Summarization of Long Legal Opinions\n  with Summary Reranking", "abstract": "We propose a simple approach for the abstractive summarization of long legal\nopinions that considers the argument structure of the document. Legal opinions\noften contain complex and nuanced argumentation, making it challenging to\ngenerate a concise summary that accurately captures the main points of the\nlegal opinion. Our approach involves using argument role information to\ngenerate multiple candidate summaries, then reranking these candidates based on\nalignment with the document's argument structure. We demonstrate the\neffectiveness of our approach on a dataset of long legal opinions and show that\nit outperforms several strong baselines.", "published": "2023-06-01 13:44:45", "link": "http://arxiv.org/abs/2306.00672v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Boosting the Performance of Transformer Architectures for Semantic\n  Textual Similarity", "abstract": "Semantic textual similarity is the task of estimating the similarity between\nthe meaning of two texts. In this paper, we fine-tune transformer architectures\nfor semantic textual similarity on the Semantic Textual Similarity Benchmark by\ntuning the model partially and then end-to-end. We experiment with BERT,\nRoBERTa, and DeBERTaV3 cross-encoders by approaching the problem as a binary\nclassification task or a regression task. We combine the outputs of the\ntransformer models and use handmade features as inputs for boosting algorithms.\nDue to worse test set results coupled with improvements on the validation set,\nwe experiment with different dataset splits to further investigate this\noccurrence. We also provide an error analysis, focused on the edges of the\nprediction range.", "published": "2023-06-01 14:16:53", "link": "http://arxiv.org/abs/2306.00708v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Column Type Annotation using ChatGPT", "abstract": "Column type annotation is the task of annotating the columns of a relational\ntable with the semantic type of the values contained in each column. Column\ntype annotation is an important pre-processing step for data search and data\nintegration in the context of data lakes. State-of-the-art column type\nannotation methods either rely on matching table columns to properties of a\nknowledge graph or fine-tune pre-trained language models such as BERT for\ncolumn type annotation. In this work, we take a different approach and explore\nusing ChatGPT for column type annotation. We evaluate different prompt designs\nin zero- and few-shot settings and experiment with providing task definitions\nand detailed instructions to the model. We further implement a two-step table\nannotation pipeline which first determines the class of the entities described\nin the table and depending on this class asks ChatGPT to annotate columns using\nonly the relevant subset of the overall vocabulary. Using instructions as well\nas the two-step pipeline, ChatGPT reaches F1 scores of over 85% in zero- and\none-shot setups. To reach a similar F1 score a RoBERTa model needs to be\nfine-tuned with 356 examples. This comparison shows that ChatGPT is able\ndeliver competitive results for the column type annotation task given no or\nonly a minimal amount of task-specific demonstrations.", "published": "2023-06-01 14:40:52", "link": "http://arxiv.org/abs/2306.00745v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero and Few-shot Semantic Parsing with Ambiguous Inputs", "abstract": "Despite the frequent challenges posed by ambiguity when representing meaning\nvia natural language, it is often ignored or deliberately removed in tasks\nmapping language to formally-designed representations, which generally assume a\none-to-one mapping between linguistic and formal representations. We attempt to\naddress this shortcoming by introducing AmP, a framework, dataset, and\nchallenge for translating ambiguous natural language to formal representations\nlike logic and code. We define templates and generate data for five\nwell-documented linguistic ambiguities. Using AmP, we investigate how several\nfew-shot text-to-code systems handle ambiguity, introducing three new metrics.\nWe find that large pre-trained models perform poorly at capturing the\ndistribution of possible meanings without deliberate instruction. However,\nmodels are able to capture the distribution well when ambiguity is attested in\ntheir inputs. These results motivate a call for including ambiguity explicitly\nin datasets and promote considering the distribution of possible outputs when\nevaluating systems. Data and code: https://github.com/esteng/ambiguous_parsing", "published": "2023-06-01 15:46:36", "link": "http://arxiv.org/abs/2306.00824v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial learning of neural user simulators for dialogue policy\n  optimisation", "abstract": "Reinforcement learning based dialogue policies are typically trained in\ninteraction with a user simulator. To obtain an effective and robust policy,\nthis simulator should generate user behaviour that is both realistic and\nvaried. Current data-driven simulators are trained to accurately model the user\nbehaviour in a dialogue corpus. We propose an alternative method using\nadversarial learning, with the aim to simulate realistic user behaviour with\nmore variation. We train and evaluate several simulators on a corpus of\nrestaurant search dialogues, and then use them to train dialogue system\npolicies. In policy cross-evaluation experiments we demonstrate that an\nadversarially trained simulator produces policies with 8.3% higher success rate\nthan those trained with a maximum likelihood simulator. Subjective results from\na crowd-sourced dialogue system user evaluation confirm the effectiveness of\nadversarially training user simulators.", "published": "2023-06-01 16:17:16", "link": "http://arxiv.org/abs/2306.00858v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OpenPI-C: A Better Benchmark and Stronger Baseline for Open-Vocabulary\n  State Tracking", "abstract": "Open-vocabulary state tracking is a more practical version of state tracking\nthat aims to track state changes of entities throughout a process without\nrestricting the state space and entity space. OpenPI is to date the only\ndataset annotated for open-vocabulary state tracking. However, we identify\nissues with the dataset quality and evaluation metric. For the dataset, we\ncategorize 3 types of problems on the procedure level, step level and state\nchange level respectively, and build a clean dataset OpenPI-C using multiple\nrounds of human judgment. For the evaluation metric, we propose a cluster-based\nmetric to fix the original metric's preference for repetition.\n  Model-wise, we enhance the seq2seq generation baseline by reinstating two key\nproperties for state tracking: temporal dependency and entity awareness. The\nstate of the world after an action is inherently dependent on the previous\nstate. We model this dependency through a dynamic memory bank and allow the\nmodel to attend to the memory slots during decoding. On the other hand, the\nstate of the world is naturally a union of the states of involved entities.\nSince the entities are unknown in the open-vocabulary setting, we propose a\ntwo-stage model that refines the state change prediction conditioned on\nentities predicted from the first stage. Empirical results show the\neffectiveness of our proposed model especially on the cluster-based metric. The\ncode and data are released at https://github.com/shirley-wu/openpi-c", "published": "2023-06-01 16:48:20", "link": "http://arxiv.org/abs/2306.00887v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TopEx: Topic-based Explanations for Model Comparison", "abstract": "Meaningfully comparing language models is challenging with current\nexplanation methods. Current explanations are overwhelming for humans due to\nlarge vocabularies or incomparable across models. We present TopEx, an\nexplanation method that enables a level playing field for comparing language\nmodels via model-agnostic topics. We demonstrate how TopEx can identify\nsimilarities and differences between DistilRoBERTa and GPT-2 on a variety of\nNLP tasks.", "published": "2023-06-01 17:59:10", "link": "http://arxiv.org/abs/2306.00976v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AWQ: Activation-aware Weight Quantization for LLM Compression and\n  Acceleration", "abstract": "Large language models (LLMs) have transformed numerous AI applications.\nOn-device LLM is becoming increasingly important: running LLMs locally on edge\ndevices can reduce the cloud computing cost and protect users' privacy.\nHowever, the astronomical model size and the limited hardware resource pose\nsignificant deployment challenges. We propose Activation-aware Weight\nQuantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only\nquantization. AWQ finds that not all weights in an LLM are equally important.\nProtecting only 1% salient weights can greatly reduce quantization error. To\nidentify salient weight channels, we should refer to the activation\ndistribution, not weights. To avoid the hardware-inefficient mix-precision\nquantization, we mathematically derive that scaling up the salient channels can\nreduce the quantization error. AWQ employs an equivalent transformation to\nscale the salient weight channels to protect them. The scale is determined by\ncollecting the activation statistics offline. AWQ does not rely on any\nbackpropagation or reconstruction, so it generalizes to different domains and\nmodalities without overfitting the calibration set. AWQ outperforms existing\nwork on various language modeling and domain-specific benchmarks (coding and\nmath). Thanks to better generalization, it achieves excellent quantization\nperformance for instruction-tuned LMs and, for the first time, multi-modal LMs.\nAlongside AWQ, we implement TinyChat, an efficient and flexible inference\nframework tailored for 4-bit on-device LLM/VLMs. With kernel fusion and\nplatform-aware weight packing, TinyChat offers more than 3x speedup over the\nHuggingface FP16 implementation on both desktop and mobile GPUs. It also\ndemocratizes the deployment of the 70B Llama-2 model on mobile GPUs.", "published": "2023-06-01 17:59:10", "link": "http://arxiv.org/abs/2306.00978v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Layout-Infused Language Models Robust to Layout Distribution Shifts?\n  A Case Study with Scientific Documents", "abstract": "Recent work has shown that infusing layout features into language models\n(LMs) improves processing of visually-rich documents such as scientific papers.\nLayout-infused LMs are often evaluated on documents with familiar layout\nfeatures (e.g., papers from the same publisher), but in practice models\nencounter documents with unfamiliar distributions of layout features, such as\nnew combinations of text sizes and styles, or new spatial configurations of\ntextual elements. In this work we test whether layout-infused LMs are robust to\nlayout distribution shifts. As a case study we use the task of scientific\ndocument structure recovery, segmenting a scientific paper into its structural\ncategories (e.g., \"title\", \"caption\", \"reference\"). To emulate distribution\nshifts that occur in practice we re-partition the GROTOAP2 dataset. We find\nthat under layout distribution shifts model performance degrades by up to 20\nF1. Simple training strategies, such as increasing training diversity, can\nreduce this degradation by over 35% relative F1; however, models fail to reach\nin-distribution performance in any tested out-of-distribution conditions. This\nwork highlights the need to consider layout distribution shifts during model\nevaluation, and presents a methodology for conducting such evaluations.", "published": "2023-06-01 18:01:33", "link": "http://arxiv.org/abs/2306.01058v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving the Robustness of Summarization Systems with Dual Augmentation", "abstract": "A robust summarization system should be able to capture the gist of the\ndocument, regardless of the specific word choices or noise in the input. In\nthis work, we first explore the summarization models' robustness against\nperturbations including word-level synonym substitution and noise. To create\nsemantic-consistent substitutes, we propose a SummAttacker, which is an\nefficient approach to generating adversarial samples based on language models.\nExperimental results show that state-of-the-art summarization models have a\nsignificant decrease in performance on adversarial and noisy test sets. Next,\nwe analyze the vulnerability of the summarization systems and explore improving\nthe robustness by data augmentation. Specifically, the first brittleness factor\nwe found is the poor understanding of infrequent words in the input.\nCorrespondingly, we feed the encoder with more diverse cases created by\nSummAttacker in the input space. The other factor is in the latent space, where\nthe attacked inputs bring more variations to the hidden states. Hence, we\nconstruct adversarial decoder input and devise manifold softmixing operation in\nhidden space to introduce more diversity. Experimental results on Gigaword and\nCNN/DM datasets demonstrate that our approach achieves significant improvements\nover strong baselines and exhibits higher robustness on noisy, attacked, and\nclean datasets.", "published": "2023-06-01 19:04:17", "link": "http://arxiv.org/abs/2306.01090v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Hate Speech Benchmarks: From Data Curation to System\n  Deployment", "abstract": "Social media is awash with hateful content, much of which is often veiled\nwith linguistic and topical diversity. The benchmark datasets used for hate\nspeech detection do not account for such divagation as they are predominantly\ncompiled using hate lexicons. However, capturing hate signals becomes\nchallenging in neutrally-seeded malicious content. Thus, designing models and\ndatasets that mimic the real-world variability of hate warrants further\ninvestigation.\n  To this end, we present GOTHate, a large-scale code-mixed crowdsourced\ndataset of around 51k posts for hate speech detection from Twitter. GOTHate is\nneutrally seeded, encompassing different languages and topics. We conduct\ndetailed comparisons of GOTHate with the existing hate speech datasets,\nhighlighting its novelty. We benchmark it with 10 recent baselines. Our\nextensive empirical and benchmarking experiments suggest that GOTHate is hard\nto classify in a text-only setup. Thus, we investigate how adding endogenous\nsignals enhances the hate speech detection task. We augment GOTHate with the\nuser's timeline information and ego network, bringing the overall data source\ncloser to the real-world setup for understanding hateful content. Our proposed\nsolution HEN-mBERT is a modular, multilingual, mixture-of-experts model that\nenriches the linguistic subspace with latent endogenous signals from history,\ntopology, and exemplars. HEN-mBERT transcends the best baseline by 2.5% and 5%\nin overall macro-F1 and hate class F1, respectively. Inspired by our\nexperiments, in partnership with Wipro AI, we are developing a semi-automated\npipeline to detect hateful content as a part of their mission to tackle online\nharm.", "published": "2023-06-01 19:36:52", "link": "http://arxiv.org/abs/2306.01105v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Examining the Causal Effect of First Names on Language Models: The Case\n  of Social Commonsense Reasoning", "abstract": "As language models continue to be integrated into applications of personal\nand societal relevance, ensuring these models' trustworthiness is crucial,\nparticularly with respect to producing consistent outputs regardless of\nsensitive attributes. Given that first names may serve as proxies for\n(intersectional) socio-demographic representations, it is imperative to examine\nthe impact of first names on commonsense reasoning capabilities. In this paper,\nwe study whether a model's reasoning given a specific input differs based on\nthe first names provided. Our underlying assumption is that the reasoning about\nAlice should not differ from the reasoning about James. We propose and\nimplement a controlled experimental framework to measure the causal effect of\nfirst names on commonsense reasoning, enabling us to distinguish between model\npredictions due to chance and caused by actual factors of interest. Our results\nindicate that the frequency of first names has a direct effect on model\nprediction, with less frequent names yielding divergent predictions compared to\nmore frequent names. To gain insights into the internal mechanisms of models\nthat are contributing to these behaviors, we also conduct an in-depth\nexplainable analysis. Overall, our findings suggest that to ensure model\nrobustness, it is essential to augment datasets with more diverse first names\nduring the configuration stage.", "published": "2023-06-01 20:05:05", "link": "http://arxiv.org/abs/2306.01117v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diverse and Faithful Knowledge-Grounded Dialogue Generation via\n  Sequential Posterior Inference", "abstract": "The capability to generate responses with diversity and faithfulness using\nfactual knowledge is paramount for creating a human-like, trustworthy dialogue\nsystem. Common strategies either adopt a two-step paradigm, which optimizes\nknowledge selection and response generation separately, and may overlook the\ninherent correlation between these two tasks, or leverage conditional\nvariational method to jointly optimize knowledge selection and response\ngeneration by employing an inference network. In this paper, we present an\nend-to-end learning framework, termed Sequential Posterior Inference (SPI),\ncapable of selecting knowledge and generating dialogues by approximately\nsampling from the posterior distribution. Unlike other methods, SPI does not\nrequire the inference network or assume a simple geometry of the posterior\ndistribution. This straightforward and intuitive inference procedure of SPI\ndirectly queries the response generation model, allowing for accurate knowledge\nselection and generation of faithful responses. In addition to modeling\ncontributions, our experimental results on two common dialogue datasets (Wizard\nof Wikipedia and Holl-E) demonstrate that SPI outperforms previous strong\nbaselines according to both automatic and human evaluation metrics.", "published": "2023-06-01 21:23:13", "link": "http://arxiv.org/abs/2306.01153v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hybrid Long Document Summarization using C2F-FAR and ChatGPT: A\n  Practical Study", "abstract": "Text summarization is a downstream natural language processing (NLP) task\nthat challenges the understanding and generation capabilities of language\nmodels. Considerable progress has been made in automatically summarizing short\ntexts, such as news articles, often leading to satisfactory results. However,\nsummarizing long documents remains a major challenge. This is due to the\ncomplex contextual information in the text and the lack of open-source\nbenchmarking datasets and evaluation frameworks that can be used to develop and\ntest model performance. In this work, we use ChatGPT, the latest breakthrough\nin the field of large language models (LLMs), together with the extractive\nsummarization model C2F-FAR (Coarse-to-Fine Facet-Aware Ranking) to propose a\nhybrid extraction and summarization pipeline for long documents such as\nbusiness articles and books. We work with the world-renowned company\ngetAbstract AG and leverage their expertise and experience in professional book\nsummarization. A practical study has shown that machine-generated summaries can\nperform at least as well as human-written summaries when evaluated using\ncurrent automated evaluation metrics. However, a closer examination of the\ntexts generated by ChatGPT through human evaluations has shown that there are\nstill critical issues in terms of text coherence, faithfulness, and style.\nOverall, our results show that the use of ChatGPT is a very promising but not\nyet mature approach for summarizing long documents and can at best serve as an\ninspiration for human editors. We anticipate that our work will inform NLP\nresearchers about the extent to which ChatGPT's capabilities for summarizing\nlong documents overlap with practitioners' needs. Further work is needed to\ntest the proposed hybrid summarization pipeline, in particular involving GPT-4,\nand to propose a new evaluation framework tailored to the task of summarizing\nlong documents.", "published": "2023-06-01 21:58:33", "link": "http://arxiv.org/abs/2306.01169v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation", "abstract": "Very large language models (LLMs) perform extremely well on a spectrum of NLP\ntasks in a zero-shot setting. However, little is known about their performance\non human-level NLP problems which rely on understanding psychological concepts,\nsuch as assessing personality traits. In this work, we investigate the\nzero-shot ability of GPT-3 to estimate the Big 5 personality traits from users'\nsocial media posts. Through a set of systematic experiments, we find that\nzero-shot GPT-3 performance is somewhat close to an existing pre-trained SotA\nfor broad classification upon injecting knowledge about the trait in the\nprompts. However, when prompted to provide fine-grained classification, its\nperformance drops to close to a simple most frequent class (MFC) baseline. We\nfurther analyze where GPT-3 performs better, as well as worse, than a\npretrained lexical model, illustrating systematic errors that suggest ways to\nimprove LLMs on human-level NLP tasks.", "published": "2023-06-01 22:43:37", "link": "http://arxiv.org/abs/2306.01183v1", "categories": ["cs.CL", "68T50", "J.4; I.2; I.7"], "primary_category": "cs.CL"}
{"title": "Multi-Dimensional Evaluation of Text Summarization with In-Context\n  Learning", "abstract": "Evaluation of natural language generation (NLG) is complex and\nmulti-dimensional. Generated text can be evaluated for fluency, coherence,\nfactuality, or any other dimensions of interest. Most frameworks that perform\nsuch multi-dimensional evaluation require training on large manually or\nsynthetically generated datasets. In this paper, we study the efficacy of large\nlanguage models as multi-dimensional evaluators using in-context learning,\nobviating the need for large training datasets. Our experiments show that\nin-context learning-based evaluators are competitive with learned evaluation\nframeworks for the task of text summarization, establishing state-of-the-art on\ndimensions such as relevance and factual consistency. We then analyze the\neffects of factors such as the selection and number of in-context examples on\nperformance. Finally, we study the efficacy of in-context learning based\nevaluators in evaluating zero-shot summaries written by large language models\nsuch as GPT-3.", "published": "2023-06-01 23:27:49", "link": "http://arxiv.org/abs/2306.01200v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AfriNames: Most ASR models \"butcher\" African Names", "abstract": "Useful conversational agents must accurately capture named entities to\nminimize error for downstream tasks, for example, asking a voice assistant to\nplay a track from a certain artist, initiating navigation to a specific\nlocation, or documenting a laboratory result for a patient. However, where\nnamed entities such as ``Ukachukwu`` (Igbo), ``Lakicia`` (Swahili), or\n``Ingabire`` (Rwandan) are spoken, automatic speech recognition (ASR) models'\nperformance degrades significantly, propagating errors to downstream systems.\nWe model this problem as a distribution shift and demonstrate that such model\nbias can be mitigated through multilingual pre-training, intelligent data\naugmentation strategies to increase the representation of African-named\nentities, and fine-tuning multilingual ASR models on multiple African accents.\nThe resulting fine-tuned models show an 81.5\\% relative WER improvement\ncompared with the baseline on samples with African-named entities.", "published": "2023-06-01 00:17:52", "link": "http://arxiv.org/abs/2306.00253v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "CapText: Large Language Model-based Caption Generation From Image\n  Context and Description", "abstract": "While deep-learning models have been shown to perform well on image-to-text\ndatasets, it is difficult to use them in practice for captioning images. This\nis because captions traditionally tend to be context-dependent and offer\ncomplementary information about an image, while models tend to produce\ndescriptions that describe the visual features of the image. Prior research in\ncaption generation has explored the use of models that generate captions when\nprovided with the images alongside their respective descriptions or contexts.\nWe propose and evaluate a new approach, which leverages existing large language\nmodels to generate captions from textual descriptions and context alone,\nwithout ever processing the image directly. We demonstrate that after\nfine-tuning, our approach outperforms current state-of-the-art image-text\nalignment models like OSCAR-VinVL on this task on the CIDEr metric.", "published": "2023-06-01 02:40:44", "link": "http://arxiv.org/abs/2306.00301v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CAISA at SemEval-2023 Task 8: Counterfactual Data Augmentation for\n  Mitigating Class Imbalance in Causal Claim Identification", "abstract": "The class imbalance problem can cause machine learning models to produce an\nundesirable performance on the minority class as well as the whole dataset.\nUsing data augmentation techniques to increase the number of samples is one way\nto tackle this problem. We introduce a novel counterfactual data augmentation\nby verb replacement for the identification of medical claims. In addition, we\ninvestigate the impact of this method and compare it with 3 other data\naugmentation techniques, showing that the proposed method can result in a\nsignificant (relative) improvement in the minority class.", "published": "2023-06-01 04:55:43", "link": "http://arxiv.org/abs/2306.00346v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CFL: Causally Fair Language Models Through Token-level Attribute\n  Controlled Generation", "abstract": "We propose a method to control the attributes of Language Models (LMs) for\nthe text generation task using Causal Average Treatment Effect (ATE) scores and\ncounterfactual augmentation. We explore this method, in the context of LM\ndetoxification, and propose the Causally Fair Language (CFL) architecture for\ndetoxifying pre-trained LMs in a plug-and-play manner. Our architecture is\nbased on a Structural Causal Model (SCM) that is mathematically transparent and\ncomputationally efficient as compared with many existing detoxification\ntechniques. We also propose several new metrics that aim to better understand\nthe behaviour of LMs in the context of toxic text generation. Further, we\nachieve state of the art performance for toxic degeneration, which are computed\nusing \\RTP (RTP) benchmark. Our experiments show that CFL achieves such a\ndetoxification without much impact on the model perplexity. We also show that\nCFL mitigates the unintended bias problem through experiments on the BOLD\ndataset.", "published": "2023-06-01 06:13:51", "link": "http://arxiv.org/abs/2306.00374v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Uncertainty-Aware Unlikelihood Learning Improves Generative Aspect\n  Sentiment Quad Prediction", "abstract": "Recently, aspect sentiment quad prediction has received widespread attention\nin the field of aspect-based sentiment analysis. Existing studies extract\nquadruplets via pre-trained generative language models to paraphrase the\noriginal sentence into a templated target sequence. However, previous works\nonly focus on what to generate but ignore what not to generate. We argue that\nconsidering the negative samples also leads to potential benefits. In this\nwork, we propose a template-agnostic method to control the token-level\ngeneration, which boosts original learning and reduces mistakes simultaneously.\nSpecifically, we introduce Monte Carlo dropout to understand the built-in\nuncertainty of pre-trained language models, acquiring the noises and errors. We\nfurther propose marginalized unlikelihood learning to suppress the\nuncertainty-aware mistake tokens. Finally, we introduce minimization entropy to\nbalance the effects of marginalized unlikelihood learning. Extensive\nexperiments on four public datasets demonstrate the effectiveness of our\napproach on various generation templates.", "published": "2023-06-01 07:49:06", "link": "http://arxiv.org/abs/2306.00418v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A big data approach towards sarcasm detection in Russian", "abstract": "We present a set of deterministic algorithms for Russian inflection and\nautomated text synthesis. These algorithms are implemented in a publicly\navailable web-service www.passare.ru. This service provides functions for\ninflection of single words, word matching and synthesis of grammatically\ncorrect Russian text. Selected code and datasets are available at\nhttps://github.com/passare-ru/PassareFunctions/ Performance of the inflectional\nfunctions has been tested against the annotated corpus of Russian language\nOpenCorpora, compared with that of other solutions, and used for estimating the\nmorphological variability and complexity of different parts of speech in\nRussian.", "published": "2023-06-01 08:34:26", "link": "http://arxiv.org/abs/2306.00445v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Parallel Neurosymbolic Integration with Concordia", "abstract": "Parallel neurosymbolic architectures have been applied effectively in NLP by\ndistilling knowledge from a logic theory into a deep model.However, prior art\nfaces several limitations including supporting restricted forms of logic\ntheories and relying on the assumption of independence between the logic and\nthe deep network. We present Concordia, a framework overcoming the limitations\nof prior art. Concordia is agnostic both to the deep network and the logic\ntheory offering support for a wide range of probabilistic theories. Our\nframework can support supervised training of both components and unsupervised\ntraining of the neural component. Concordia has been successfully applied to\ntasks beyond NLP and data classification, improving the accuracy of\nstate-of-the-art on collective activity detection, entity linking and\nrecommendation tasks.", "published": "2023-06-01 09:30:24", "link": "http://arxiv.org/abs/2306.00480v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Revisiting Event Argument Extraction: Can EAE Models Learn Better When\n  Being Aware of Event Co-occurrences?", "abstract": "Event co-occurrences have been proved effective for event extraction (EE) in\nprevious studies, but have not been considered for event argument extraction\n(EAE) recently. In this paper, we try to fill this gap between EE research and\nEAE research, by highlighting the question that ``Can EAE models learn better\nwhen being aware of event co-occurrences?''. To answer this question, we\nreformulate EAE as a problem of table generation and extend a SOTA prompt-based\nEAE model into a non-autoregressive generation framework, called TabEAE, which\nis able to extract the arguments of multiple events in parallel. Under this\nframework, we experiment with 3 different training-inference schemes on 4\ndatasets (ACE05, RAMS, WikiEvents and MLEE) and discover that via training the\nmodel to extract all events in parallel, it can better distinguish the semantic\nboundary of each event and its ability to extract single event gets\nsubstantially improved. Experimental results show that our method achieves new\nstate-of-the-art performance on the 4 datasets. Our code is avilable at\nhttps://github.com/Stardust-hyx/TabEAE.", "published": "2023-06-01 09:53:53", "link": "http://arxiv.org/abs/2306.00502v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Effects of Input Type and Pronunciation Dictionary Usage in Transfer\n  Learning for Low-Resource Text-to-Speech", "abstract": "We compare phone labels and articulatory features as input for cross-lingual\ntransfer learning in text-to-speech (TTS) for low-resource languages (LRLs).\nExperiments with FastSpeech 2 and the LRL West Frisian show that using\narticulatory features outperformed using phone labels in both intelligibility\nand naturalness. For LRLs without pronunciation dictionaries, we propose two\nnovel approaches: a) using a massively multilingual model to convert\ngrapheme-to-phone (G2P) in both training and synthesizing, and b) using a\nuniversal phone recognizer to create a makeshift dictionary. Results show that\nthe G2P approach performs largely on par with using a ground-truth dictionary\nand the phone recognition approach, while performing generally worse, remains a\nviable option for LRLs less suitable for the G2P approach. Within each\napproach, using articulatory features as input outperforms using phone labels.", "published": "2023-06-01 10:42:56", "link": "http://arxiv.org/abs/2306.00535v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Call for Standardization and Validation of Text Style Transfer\n  Evaluation", "abstract": "Text Style Transfer (TST) evaluation is, in practice, inconsistent.\nTherefore, we conduct a meta-analysis on human and automated TST evaluation and\nexperimentation that thoroughly examines existing literature in the field. The\nmeta-analysis reveals a substantial standardization gap in human and automated\nevaluation. In addition, we also find a validation gap: only few automated\nmetrics have been validated using human experiments. To this end, we thoroughly\nscrutinize both the standardization and validation gap and reveal the resulting\npitfalls. This work also paves the way to close the standardization and\nvalidation gap in TST evaluation by calling out requirements to be met by\nfuture research.", "published": "2023-06-01 10:46:08", "link": "http://arxiv.org/abs/2306.00539v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Enhancing Programming eTextbooks with ChatGPT Generated\n  Counterfactual-Thinking-Inspired Questions", "abstract": "Digital textbooks have become an integral part of everyday learning tasks. In\nthis work, we consider the use of digital textbooks for programming classes.\nGenerally, students struggle with utilizing textbooks on programming to the\nmaximum, with a possible reason being that the example programs provided as\nillustration of concepts in these textbooks don't offer sufficient\ninteractivity for students, and thereby not sufficiently motivating to explore\nor understand these programming examples better. In our work, we explore the\nidea of enhancing the navigability of intelligent textbooks with the use of\n``counterfactual'' questions, to make students think critically about these\nprograms and enhance possible program comprehension. Inspired from previous\nworks on nudging students on counter factual thinking, we present the\npossibility to enhance digital textbooks with questions generated using GPT.", "published": "2023-06-01 11:14:15", "link": "http://arxiv.org/abs/2306.00551v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Being Right for Whose Right Reasons?", "abstract": "Explainability methods are used to benchmark the extent to which model\npredictions align with human rationales i.e., are 'right for the right\nreasons'. Previous work has failed to acknowledge, however, that what counts as\na rationale is sometimes subjective. This paper presents what we think is a\nfirst of its kind, a collection of human rationale annotations augmented with\nthe annotators demographic information. We cover three datasets spanning\nsentiment analysis and common-sense reasoning, and six demographic groups\n(balanced across age and ethnicity). Such data enables us to ask both what\ndemographics our predictions align with and whose reasoning patterns our\nmodels' rationales align with. We find systematic inter-group annotator\ndisagreement and show how 16 Transformer-based models align better with\nrationales provided by certain demographic groups: We find that models are\nbiased towards aligning best with older and/or white annotators. We zoom in on\nthe effects of model size and model distillation, finding -- contrary to our\nexpectations -- negative correlations between model size and rationale\nagreement as well as no evidence that either model size or model distillation\nimproves fairness.", "published": "2023-06-01 13:06:43", "link": "http://arxiv.org/abs/2306.00639v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Explanation Graph Generation via Generative Pre-training over Synthetic\n  Graphs", "abstract": "The generation of explanation graphs is a significant task that aims to\nproduce explanation graphs in response to user input, revealing the internal\nreasoning process. This task is challenging due to the significant discrepancy\nbetween unstructured user queries and structured explanation graphs. Current\nresearch commonly fine-tunes a text-based pre-trained language model on a small\ndownstream dataset that is annotated with labeled graphs. However, due to the\nlimited scale of available datasets, this approach may prove to be insufficient\nin bridging the gap between natural language text and structured graphs. In\nthis paper, to alleviate the above limitations, we propose a novel pre-trained\nframework EG3P(for Explanation Graph Generation via Generative Pre-training\nover synthetic graphs) for the explanation graph generation task. Specifically,\nwe first propose a text-to-graph generative task to pre-train the model with\nthe goal of bridging the text-graph gap. Additionally, we propose an automatic\ncorpus synthesis strategy for synthesizing a large scale of high-quality\ncorpus, reducing the reliance on costly manual annotation methods. Experimental\nresults on ExplaGraphs show the effectiveness of EG3P that our model surpasses\nall baseline systems with remarkable margins. Besides, further analysis\ndemonstrates that EG3P is able to generate better explanation graphs on actual\nreasoning tasks such as CommonsenseQA and OpenbookQA.", "published": "2023-06-01 13:20:22", "link": "http://arxiv.org/abs/2306.00652v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Predicting the Quality of Revisions in Argumentative Writing", "abstract": "The ability to revise in response to feedback is critical to students'\nwriting success. In the case of argument writing in specific, identifying\nwhether an argument revision (AR) is successful or not is a complex problem\nbecause AR quality is dependent on the overall content of an argument. For\nexample, adding the same evidence sentence could strengthen or weaken existing\nclaims in different argument contexts (ACs). To address this issue we developed\nChain-of-Thought prompts to facilitate ChatGPT-generated ACs for AR quality\npredictions. The experiments on two corpora, our annotated elementary essays\nand existing college essays benchmark, demonstrate the superiority of the\nproposed ACs over baselines.", "published": "2023-06-01 13:39:33", "link": "http://arxiv.org/abs/2306.00667v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ReFACT: Updating Text-to-Image Models by Editing the Text Encoder", "abstract": "Our world is marked by unprecedented technological, global, and\nsocio-political transformations, posing a significant challenge to\ntext-to-image generative models. These models encode factual associations\nwithin their parameters that can quickly become outdated, diminishing their\nutility for end-users. To that end, we introduce ReFACT, a novel approach for\nediting factual associations in text-to-image models without relaying on\nexplicit input from end-users or costly re-training. ReFACT updates the weights\nof a specific layer in the text encoder, modifying only a tiny portion of the\nmodel's parameters and leaving the rest of the model unaffected. We empirically\nevaluate ReFACT on an existing benchmark, alongside a newly curated dataset.\nCompared to other methods, ReFACT achieves superior performance in both\ngeneralization to related concepts and preservation of unrelated concepts.\nFurthermore, ReFACT maintains image generation quality, making it a practical\ntool for updating and correcting factual information in text-to-image models.", "published": "2023-06-01 14:32:34", "link": "http://arxiv.org/abs/2306.00738v2", "categories": ["cs.CL", "cs.CV", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Differentiable Tree Operations Promote Compositional Generalization", "abstract": "In the context of structure-to-structure transformation tasks, learning\nsequences of discrete symbolic operations poses significant challenges due to\ntheir non-differentiability. To facilitate the learning of these symbolic\nsequences, we introduce a differentiable tree interpreter that compiles\nhigh-level symbolic tree operations into subsymbolic matrix operations on\ntensors. We present a novel Differentiable Tree Machine (DTM) architecture that\nintegrates our interpreter with an external memory and an agent that learns to\nsequentially select tree operations to execute the target transformation in an\nend-to-end manner. With respect to out-of-distribution compositional\ngeneralization on synthetic semantic parsing and language generation tasks, DTM\nachieves 100% while existing baselines such as Transformer, Tree Transformer,\nLSTM, and Tree2Tree LSTM achieve less than 30%. DTM remains highly\ninterpretable in addition to its perfect performance.", "published": "2023-06-01 14:46:34", "link": "http://arxiv.org/abs/2306.00751v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "In-Context Learning User Simulators for Task-Oriented Dialog Systems", "abstract": "This paper presents a novel application of large language models in user\nsimulation for task-oriented dialog systems, specifically focusing on an\nin-context learning approach. By harnessing the power of these models, the\nproposed approach generates diverse utterances based on user goals and limited\ndialog examples. Unlike traditional simulators, this method eliminates the need\nfor labor-intensive rule definition or extensive annotated data, making it more\nefficient and accessible. Additionally, an error analysis of the interaction\nbetween the user simulator and dialog system uncovers common mistakes,\nproviding valuable insights into areas that require improvement. Our\nimplementation is available at\nhttps://github.com/telepathylabsai/prompt-based-user-simulator.", "published": "2023-06-01 15:06:11", "link": "http://arxiv.org/abs/2306.00774v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interpretable Math Word Problem Solution Generation Via Step-by-step\n  Planning", "abstract": "Solutions to math word problems (MWPs) with step-by-step explanations are\nvaluable, especially in education, to help students better comprehend\nproblem-solving strategies. Most existing approaches only focus on obtaining\nthe final correct answer. A few recent approaches leverage intermediate\nsolution steps to improve final answer correctness but often cannot generate\ncoherent steps with a clear solution strategy. Contrary to existing work, we\nfocus on improving the correctness and coherence of the intermediate solutions\nsteps. We propose a step-by-step planning approach for intermediate solution\ngeneration, which strategically plans the generation of the next solution step\nbased on the MWP and the previous solution steps. Our approach first plans the\nnext step by predicting the necessary math operation needed to proceed, given\nhistory steps, then generates the next step, token-by-token, by prompting a\nlanguage model with the predicted math operation. Experiments on the GSM8K\ndataset demonstrate that our approach improves the accuracy and\ninterpretability of the solution on both automatic metrics and human\nevaluation.", "published": "2023-06-01 15:16:18", "link": "http://arxiv.org/abs/2306.00784v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Modeling and Analyzing Scorer Preferences in Short-Answer Math Questions", "abstract": "Automated scoring of student responses to open-ended questions, including\nshort-answer questions, has great potential to scale to a large number of\nresponses. Recent approaches for automated scoring rely on supervised learning,\ni.e., training classifiers or fine-tuning language models on a small number of\nresponses with human-provided score labels. However, since scoring is a\nsubjective process, these human scores are noisy and can be highly variable,\ndepending on the scorer. In this paper, we investigate a collection of models\nthat account for the individual preferences and tendencies of each human scorer\nin the automated scoring task. We apply these models to a short-answer math\nresponse dataset where each response is scored (often differently) by multiple\ndifferent human scorers. We conduct quantitative experiments to show that our\nscorer models lead to improved automated scoring accuracy. We also conduct\nquantitative experiments and case studies to analyze the individual preferences\nand tendencies of scorers. We found that scorers can be grouped into several\nobvious clusters, with each cluster having distinct features, and analyzed them\nin detail.", "published": "2023-06-01 15:22:05", "link": "http://arxiv.org/abs/2306.00791v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLaVA-Med: Training a Large Language-and-Vision Assistant for\n  Biomedicine in One Day", "abstract": "Conversational generative AI has demonstrated remarkable promise for\nempowering biomedical practitioners, but current investigations focus on\nunimodal text. Multimodal conversational AI has seen rapid progress by\nleveraging billions of image-text pairs from the public web, but such\ngeneral-domain vision-language models still lack sophistication in\nunderstanding and conversing about biomedical images. In this paper, we propose\na cost-efficient approach for training a vision-language conversational\nassistant that can answer open-ended research questions of biomedical images.\nThe key idea is to leverage a large-scale, broad-coverage biomedical\nfigure-caption dataset extracted from PubMed Central, use GPT-4 to\nself-instruct open-ended instruction-following data from the captions, and then\nfine-tune a large general-domain vision-language model using a novel curriculum\nlearning method. Specifically, the model first learns to align biomedical\nvocabulary using the figure-caption pairs as is, then learns to master\nopen-ended conversational semantics using GPT-4 generated instruction-following\ndata, broadly mimicking how a layperson gradually acquires biomedical\nknowledge. This enables us to train a Large Language and Vision Assistant for\nBioMedicine (LLaVA-Med) in less than 15 hours (with eight A100s). LLaVA-Med\nexhibits excellent multimodal conversational capability and can follow\nopen-ended instruction to assist with inquiries about a biomedical image. On\nthree standard biomedical visual question answering datasets, LLaVA-Med\noutperforms previous supervised state-of-the-art on certain metrics. To\nfacilitate biomedical multimodal research, we will release our\ninstruction-following data and the LLaVA-Med model.", "published": "2023-06-01 16:50:07", "link": "http://arxiv.org/abs/2306.00890v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "\"Let's not Quote out of Context\": Unified Vision-Language Pretraining\n  for Context Assisted Image Captioning", "abstract": "Well-formed context aware image captions and tags in enterprise content such\nas marketing material are critical to ensure their brand presence and content\nrecall. Manual creation and updates to ensure the same is non trivial given the\nscale and the tedium towards this task. We propose a new unified\nVision-Language (VL) model based on the One For All (OFA) model, with a focus\non context-assisted image captioning where the caption is generated based on\nboth the image and its context. Our approach aims to overcome the\ncontext-independent (image and text are treated independently) nature of the\nexisting approaches. We exploit context by pretraining our model with datasets\nof three tasks: news image captioning where the news article is the context,\ncontextual visual entailment, and keyword extraction from the context. The\nsecond pretraining task is a new VL task, and we construct and release two\ndatasets for the task with 1.1M and 2.2K data instances. Our system achieves\nstate-of-the-art results with an improvement of up to 8.34 CIDEr score on the\nbenchmark news image captioning datasets. To the best of our knowledge, ours is\nthe first effort at incorporating contextual information in pretraining the\nmodels for the VL tasks.", "published": "2023-06-01 17:34:25", "link": "http://arxiv.org/abs/2306.00931v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "AMR4NLI: Interpretable and robust NLI measures from semantic graphs", "abstract": "The task of natural language inference (NLI) asks whether a given premise\n(expressed in NL) entails a given NL hypothesis. NLI benchmarks contain human\nratings of entailment, but the meaning relationships driving these ratings are\nnot formalized. Can the underlying sentence pair relationships be made more\nexplicit in an interpretable yet robust fashion? We compare semantic structures\nto represent premise and hypothesis, including sets of contextualized\nembeddings and semantic graphs (Abstract Meaning Representations), and measure\nwhether the hypothesis is a semantic substructure of the premise, utilizing\ninterpretable metrics. Our evaluation on three English benchmarks finds value\nin both contextualized embeddings and semantic graphs; moreover, they provide\ncomplementary signals, and can be leveraged together in a hybrid model.", "published": "2023-06-01 17:39:40", "link": "http://arxiv.org/abs/2306.00936v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Exposing Attention Glitches with Flip-Flop Language Modeling", "abstract": "Why do large language models sometimes output factual inaccuracies and\nexhibit erroneous reasoning? The brittleness of these models, particularly when\nexecuting long chains of reasoning, currently seems to be an inevitable price\nto pay for their advanced capabilities of coherently synthesizing knowledge,\npragmatics, and abstract thought. Towards making sense of this fundamentally\nunsolved problem, this work identifies and analyzes the phenomenon of attention\nglitches, in which the Transformer architecture's inductive biases\nintermittently fail to capture robust reasoning. To isolate the issue, we\nintroduce flip-flop language modeling (FFLM), a parametric family of synthetic\nbenchmarks designed to probe the extrapolative behavior of neural language\nmodels. This simple generative task requires a model to copy binary symbols\nover long-range dependencies, ignoring the tokens in between. We find that\nTransformer FFLMs suffer from a long tail of sporadic reasoning errors, some of\nwhich we can eliminate using various regularization techniques. Our preliminary\nmechanistic analyses show why the remaining errors may be very difficult to\ndiagnose and resolve. We hypothesize that attention glitches account for (some\nof) the closed-domain hallucinations in natural LLMs.", "published": "2023-06-01 17:44:35", "link": "http://arxiv.org/abs/2306.00946v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Reimagining Retrieval Augmented Language Models for Answering Queries", "abstract": "We present a reality check on large language models and inspect the promise\nof retrieval augmented language models in comparison. Such language models are\nsemi-parametric, where models integrate model parameters and knowledge from\nexternal data sources to make their predictions, as opposed to the parametric\nnature of vanilla large language models. We give initial experimental findings\nthat semi-parametric architectures can be enhanced with views, a query\nanalyzer/planner, and provenance to make a significantly more powerful system\nfor question answering in terms of accuracy and efficiency, and potentially for\nother NLP tasks", "published": "2023-06-01 18:08:51", "link": "http://arxiv.org/abs/2306.01061v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Quantization-Aware and Tensor-Compressed Training of Transformers for\n  Natural Language Understanding", "abstract": "Fine-tuned transformer models have shown superior performances in many\nnatural language tasks. However, the large model size prohibits deploying\nhigh-performance transformer models on resource-constrained devices. This paper\nproposes a quantization-aware tensor-compressed training approach to reduce the\nmodel size, arithmetic operations, and ultimately runtime latency of\ntransformer-based models. We compress the embedding and linear layers of\ntransformers into small low-rank tensor cores, which significantly reduces\nmodel parameters. A quantization-aware training with learnable scale factors is\nused to further obtain low-precision representations of the tensor-compressed\nmodels. The developed approach can be used for both end-to-end training and\ndistillation-based training. To improve the convergence, a layer-by-layer\ndistillation is applied to distill a quantized and tensor-compressed student\nmodel from a pre-trained transformer. The performance is demonstrated in two\nnatural language understanding tasks, showing up to $63\\times$ compression\nratio, little accuracy loss and remarkable inference and training speedup.", "published": "2023-06-01 18:32:08", "link": "http://arxiv.org/abs/2306.01076v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UCAS-IIE-NLP at SemEval-2023 Task 12: Enhancing Generalization of\n  Multilingual BERT for Low-resource Sentiment Analysis", "abstract": "This paper describes our system designed for SemEval-2023 Task 12: Sentiment\nanalysis for African languages. The challenge faced by this task is the\nscarcity of labeled data and linguistic resources in low-resource settings. To\nalleviate these, we propose a generalized multilingual system SACL-XLMR for\nsentiment analysis on low-resource languages. Specifically, we design a\nlexicon-based multilingual BERT to facilitate language adaptation and\nsentiment-aware representation learning. Besides, we apply a supervised\nadversarial contrastive learning technique to learn sentiment-spread structured\nrepresentations and enhance model generalization. Our system achieved\ncompetitive results, largely outperforming baselines on both multilingual and\nzero-shot sentiment classification subtasks. Notably, the system obtained the\n1st rank on the zero-shot classification subtask in the official ranking.\nExtensive experiments demonstrate the effectiveness of our system.", "published": "2023-06-01 19:10:09", "link": "http://arxiv.org/abs/2306.01093v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora\n  with Web Data, and Web Data Only", "abstract": "Large language models are commonly trained on a mixture of filtered web data\nand curated high-quality corpora, such as social media conversations, books, or\ntechnical papers. This curation process is believed to be necessary to produce\nperformant models with broad zero-shot generalization abilities. However, as\nlarger models requiring pretraining on trillions of tokens are considered, it\nis unclear how scalable is curation and whether we will run out of unique\nhigh-quality data soon. At variance with previous beliefs, we show that\nproperly filtered and deduplicated web data alone can lead to powerful models;\neven significantly outperforming models from the state-of-the-art trained on\nThe Pile. Despite extensive filtering, the high-quality data we extract from\nthe web is still plentiful, and we are able to obtain five trillion tokens from\nCommonCrawl. We publicly release an extract of 600 billion tokens from our\nRefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.", "published": "2023-06-01 20:03:56", "link": "http://arxiv.org/abs/2306.01116v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Transformer Programs", "abstract": "Recent research in mechanistic interpretability has attempted to\nreverse-engineer Transformer models by carefully inspecting network weights and\nactivations. However, these approaches require considerable manual effort and\nstill fall short of providing complete, faithful descriptions of the underlying\nalgorithms. In this work, we introduce a procedure for training Transformers\nthat are mechanistically interpretable by design. We build on RASP [Weiss et\nal., 2021], a programming language that can be compiled into Transformer\nweights. Instead of compiling human-written programs into Transformers, we\ndesign a modified Transformer that can be trained using gradient-based\noptimization and then automatically converted into a discrete, human-readable\nprogram. We refer to these models as Transformer Programs. To validate our\napproach, we learn Transformer Programs for a variety of problems, including an\nin-context learning task, a suite of algorithmic problems (e.g. sorting,\nrecognizing Dyck languages), and NLP tasks including named entity recognition\nand text classification. The Transformer Programs can automatically find\nreasonable solutions, performing on par with standard Transformers of\ncomparable size; and, more importantly, they are easy to interpret. To\ndemonstrate these advantages, we convert Transformers into Python programs and\nuse off-the-shelf code analysis tools to debug model errors and identify the\n\"circuits\" used to solve different sub-problems. We hope that Transformer\nPrograms open a new path toward the goal of intrinsically interpretable machine\nlearning.", "published": "2023-06-01 20:27:01", "link": "http://arxiv.org/abs/2306.01128v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Evaluating the Capabilities of Multi-modal Reasoning Models with\n  Synthetic Task Data", "abstract": "The impressive advances and applications of large language and joint\nlanguage-and-visual understanding models has led to an increased need for\nmethods of probing their potential reasoning capabilities. However, the\ndifficulty of gather naturally-occurring data for complex multi-modal reasoning\ntasks bottlenecks the evaluation of AI methods on tasks which are not already\ncovered by an academic dataset. In this work, we leverage recent advances in\nhigh resolution text-to-image generation to develop a framework for generating\nevaluation data for multi-modal reasoning tasks. We apply this framework to\ngenerate context-dependent anomaly data, creating a synthetic dataset on a\nchallenging task which is not well covered by existing datasets. We benchmark\nthe performance of a state-of-the-art visual question answering (VQA) model\nagainst data generated with this method, and demonstrate that while the task is\ntractable, the model performs significantly worse on the context-dependent\nanomaly detection task than on standard VQA tasks.", "published": "2023-06-01 20:56:34", "link": "http://arxiv.org/abs/2306.01144v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Did You Read the Instructions? Rethinking the Effectiveness of Task\n  Definitions in Instruction Learning", "abstract": "Large language models (LLMs) have shown impressive performance in following\nnatural language instructions to solve unseen tasks. However, it remains\nunclear whether models truly understand task definitions and whether the\nhuman-written definitions are optimal. In this paper, we systematically study\nthe role of task definitions in instruction learning. We first conduct an\nablation analysis informed by human annotations to understand which parts of a\ntask definition are most important, and find that model performance only drops\nsubstantially when removing contents describing the task output, in particular\nlabel information. Next, we propose an automatic algorithm to compress task\ndefinitions to a minimal supporting set of tokens, and find that 60\\% of tokens\ncan be removed while maintaining or even improving model performance. Based on\nthese results, we propose two strategies to help models better leverage task\ninstructions: (1) providing only key information for tasks in a common\nstructured format, and (2) adding a meta-tuning stage to help the model better\nunderstand the definitions. With these two strategies, we achieve a 4.2 Rouge-L\nimprovement over 119 unseen test tasks.", "published": "2023-06-01 21:11:24", "link": "http://arxiv.org/abs/2306.01150v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging Natural Language Processing For Public Health Screening On\n  YouTube: A COVID-19 Case Study", "abstract": "Background: Social media platforms have become a viable source of medical\ninformation, with patients and healthcare professionals using them to share\nhealth-related information and track diseases. Similarly, YouTube, the largest\nvideo-sharing platform in the world contains vlogs where individuals talk about\ntheir illnesses. The aim of our study was to investigate the use of Natural\nLanguage Processing (NLP) to identify the spoken content of YouTube vlogs\nrelated to the diagnosis of Coronavirus disease of 2019 (COVID-19) for public\nhealth screening. Methods: COVID-19 videos on YouTube were searched using\nrelevant keywords. A total of 1000 videos being spoken in English were\ndownloaded out of which 791 were classified as vlogs, 192 were non-vlogs, and\n17 were deleted by the channel. The videos were converted into a textual format\nusing Microsoft Streams. The textual data was preprocessed using basic and\nadvanced preprocessing methods. A lexicon of 200 words was created which\ncontained words related to COVID-19. The data was analyzed using topic\nmodeling, word clouds, and lexicon matching. Results: The word cloud results\nrevealed discussions about COVID-19 symptoms like \"fever\", along with generic\nterms such as \"mask\" and \"isolation\". Lexical analysis demonstrated that in\n96.46% of videos, patients discussed generic terms, and in 95.45% of videos,\npeople talked about COVID-19 symptoms. LDA Topic Modeling results also\ngenerated topics that successfully captured key themes and content related to\nour investigation of COVID-19 diagnoses in YouTube vlogs. Conclusion: By\nleveraging NLP techniques on YouTube vlogs public health practitioners can\nenhance their ability to mitigate the effects of pandemics and effectively\nrespond to public health challenges.", "published": "2023-06-01 21:40:48", "link": "http://arxiv.org/abs/2306.01164v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Training-free Neural Architecture Search for RNNs and Transformers", "abstract": "Neural architecture search (NAS) has allowed for the automatic creation of\nnew and effective neural network architectures, offering an alternative to the\nlaborious process of manually designing complex architectures. However,\ntraditional NAS algorithms are slow and require immense amounts of computing\npower. Recent research has investigated training-free NAS metrics for image\nclassification architectures, drastically speeding up search algorithms. In\nthis paper, we investigate training-free NAS metrics for recurrent neural\nnetwork (RNN) and BERT-based transformer architectures, targeted towards\nlanguage modeling tasks. First, we develop a new training-free metric, named\nhidden covariance, that predicts the trained performance of an RNN architecture\nand significantly outperforms existing training-free metrics. We experimentally\nevaluate the effectiveness of the hidden covariance metric on the NAS-Bench-NLP\nbenchmark. Second, we find that the current search space paradigm for\ntransformer architectures is not optimized for training-free neural\narchitecture search. Instead, a simple qualitative analysis can effectively\nshrink the search space to the best performing architectures. This conclusion\nis based on our investigation of existing training-free metrics and new metrics\ndeveloped from recent transformer pruning literature, evaluated on our own\nbenchmark of trained BERT architectures. Ultimately, our analysis shows that\nthe architecture search space and the training-free metric must be developed\ntogether in order to achieve effective results.", "published": "2023-06-01 02:06:13", "link": "http://arxiv.org/abs/2306.00288v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Developing and Building Ontologies in Cyber Security", "abstract": "Cyber Security is one of the most arising disciplines in our modern society.\nWe work on Cybersecurity domain and in this the topic we chose is Cyber\nSecurity Ontologies. In this we gather all latest and previous ontologies and\ncompare them on the basis of different analyzing factors to get best of them.\nReason to select this topic is to assemble different ontologies from different\nera of time. Because, researches that included in this SLR is mostly studied\nsingle ontology. If any researcher wants to study ontologies, he has to study\nevery single ontology and select which one is best for his research. So, we\nassemble different types of ontology and compare them against each other to get\nbest of them. A total 24 papers between years 2010-2020 are carefully selected\nthrough systematic process and classified accordingly. Lastly, this SLR have\nbeen presented to provide the researchers promising future directions in the\ndomain of cybersecurity ontologies.", "published": "2023-06-01 06:19:01", "link": "http://arxiv.org/abs/2306.00377v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Towards hate speech detection in low-resource languages: Comparing ASR\n  to acoustic word embeddings on Wolof and Swahili", "abstract": "We consider hate speech detection through keyword spotting on radio\nbroadcasts. One approach is to build an automatic speech recognition (ASR)\nsystem for the target low-resource language. We compare this to using acoustic\nword embedding (AWE) models that map speech segments to a space where matching\nwords have similar vectors. We specifically use a multilingual AWE model\ntrained on labelled data from well-resourced languages to spot keywords in data\nin the unseen target language. In contrast to ASR, the AWE approach only\nrequires a few keyword exemplars. In controlled experiments on Wolof and\nSwahili where training and test data are from the same domain, an ASR model\ntrained on just five minutes of data outperforms the AWE approach. But in an\nin-the-wild test on Swahili radio broadcasts with actual hate speech keywords,\nthe AWE model (using one minute of template data) is more robust, giving\nsimilar performance to an ASR system trained on 30 hours of labelled data.", "published": "2023-06-01 07:25:10", "link": "http://arxiv.org/abs/2306.00410v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "End-to-end Knowledge Retrieval with Multi-modal Queries", "abstract": "We investigate knowledge retrieval with multi-modal queries, i.e. queries\ncontaining information split across image and text inputs, a challenging task\nthat differs from previous work on cross-modal retrieval. We curate a new\ndataset called ReMuQ for benchmarking progress on this task. ReMuQ requires a\nsystem to retrieve knowledge from a large corpus by integrating contents from\nboth text and image queries. We introduce a retriever model ``ReViz'' that can\ndirectly process input text and images to retrieve relevant knowledge in an\nend-to-end fashion without being dependent on intermediate modules such as\nobject detectors or caption generators. We introduce a new pretraining task\nthat is effective for learning knowledge retrieval with multimodal queries and\nalso improves performance on downstream tasks. We demonstrate superior\nperformance in retrieval on two datasets (ReMuQ and OK-VQA) under zero-shot\nsettings as well as further improvements when finetuned on these datasets.", "published": "2023-06-01 08:04:12", "link": "http://arxiv.org/abs/2306.00424v1", "categories": ["cs.CL", "cs.CV", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Make Pre-trained Model Reversible: From Parameter to Memory Efficient\n  Fine-Tuning", "abstract": "Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs)\nhas emerged as a highly successful approach, with training only a small number\nof parameters without sacrificing performance and becoming the de-facto\nlearning paradigm with the increasing size of PLMs. However, existing PEFT\nmethods are not memory-efficient, because they still require caching most of\nthe intermediate activations for the gradient calculation, akin to fine-tuning.\nOne effective way to reduce the activation memory is to apply a reversible\nmodel, so the intermediate activations are not necessary to be cached and can\nbe recomputed. Nevertheless, modifying a PLM to its reversible variant is not\nstraightforward, since the reversible model has a distinct architecture from\nthe currently released PLMs. In this paper, we first investigate what is a key\nfactor for the success of existing PEFT methods, and realize that it's\nessential to preserve the PLM's starting point when initializing a PEFT method.\nWith this finding, we propose memory-efficient fine-tuning (MEFT) that inserts\nadapters into a PLM, preserving the PLM's starting point and making it\nreversible without additional pre-training. We evaluate MEFT on the GLUE\nbenchmark and five question-answering tasks with various backbones, BERT,\nRoBERTa, BART and OPT. MEFT significantly reduces the activation memory up to\n84% of full fine-tuning with a negligible amount of trainable parameters.\nMoreover, MEFT achieves the same score on GLUE and a comparable score on the\nquestion-answering tasks as full fine-tuning. A similar finding is also\nobserved for the image classification task.", "published": "2023-06-01 09:26:17", "link": "http://arxiv.org/abs/2306.00477v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MEWL: Few-shot multimodal word learning with referential uncertainty", "abstract": "Without explicit feedback, humans can rapidly learn the meaning of words.\nChildren can acquire a new word after just a few passive exposures, a process\nknown as fast mapping. This word learning capability is believed to be the most\nfundamental building block of multimodal understanding and reasoning. Despite\nrecent advancements in multimodal learning, a systematic and rigorous\nevaluation is still missing for human-like word learning in machines. To fill\nin this gap, we introduce the MachinE Word Learning (MEWL) benchmark to assess\nhow machines learn word meaning in grounded visual scenes. MEWL covers human's\ncore cognitive toolkits in word learning: cross-situational reasoning,\nbootstrapping, and pragmatic learning. Specifically, MEWL is a few-shot\nbenchmark suite consisting of nine tasks for probing various word learning\ncapabilities. These tasks are carefully designed to be aligned with the\nchildren's core abilities in word learning and echo the theories in the\ndevelopmental literature. By evaluating multimodal and unimodal agents'\nperformance with a comparative analysis of human performance, we notice a sharp\ndivergence in human and machine word learning. We further discuss these\ndifferences between humans and machines and call for human-like few-shot word\nlearning in machines.", "published": "2023-06-01 09:54:31", "link": "http://arxiv.org/abs/2306.00503v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Layout and Task Aware Instruction Prompt for Zero-shot Document Image\n  Question Answering", "abstract": "Layout-aware pre-trained models has achieved significant progress on document\nimage question answering. They introduce extra learnable modules into existing\nlanguage models to capture layout information within document images from text\nbounding box coordinates obtained by OCR tools. However, extra modules\nnecessitate pre-training on extensive document images. This prevents these\nmethods from directly utilizing off-the-shelf instruction-tuning language\nfoundation models, which have recently shown promising potential in zero-shot\nlearning. Instead, in this paper, we find that instruction-tuning language\nmodels like Claude and ChatGPT can understand layout by spaces and line breaks.\nBased on this observation, we propose the LAyout and Task aware Instruction\nPrompt (LATIN-Prompt), which consists of layout-aware document content and\ntask-aware instruction. Specifically, the former uses appropriate spaces and\nline breaks to recover the layout information among text segments obtained by\nOCR tools, and the latter ensures that generated answers adhere to formatting\nrequirements. Moreover, we propose the LAyout and Task aware Instruction Tuning\n(LATIN-Tuning) to improve the performance of small instruction-tuning models\nlike Alpaca. Experimental results show that LATIN-Prompt enables zero-shot\nperformance of Claude and ChatGPT to be comparable to the fine-tuning\nperformance of SOTAs on document image question answering, and LATIN-Tuning\nenhances the zero-shot performance of Alpaca significantly. For example,\nLATIN-Prompt improves the performance of Claude and ChatGPT on DocVQA by 263%\nand 20% respectively. LATIN-Tuning improves the performance of Alpaca on DocVQA\nby 87.7%. Quantitative and qualitative analyses demonstrate the effectiveness\nof LATIN-Prompt and LATIN-Tuning. We provide the code in supplementary and will\nrelease it to facilitate future research.", "published": "2023-06-01 10:28:12", "link": "http://arxiv.org/abs/2306.00526v4", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Chain-Of-Thought Prompting Under Streaming Batch: A Case Study", "abstract": "Recently, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities. Chain-of-Thought (CoT) has been proposed as a way of assisting\nLLMs in performing complex reasoning. However, developing effective prompts can\nbe a challenging and labor-intensive task. Many studies come out of some way to\nautomatically construct CoT from test data. Most of them assume that all test\ndata is visible before testing and only select a small subset to generate\nrationales, which is an unrealistic assumption. In this paper, we present a\ncase study on how to construct and optimize chain-of-thought prompting using\nbatch data in streaming settings.", "published": "2023-06-01 11:11:39", "link": "http://arxiv.org/abs/2306.00550v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Effective Structured Prompting by Meta-Learning and Representative\n  Verbalizer", "abstract": "Prompt tuning for pre-trained masked language models (MLM) has shown\npromising performance in natural language processing tasks with few labeled\nexamples. It tunes a prompt for the downstream task, and a verbalizer is used\nto bridge the predicted token and label prediction. Due to the limited training\ndata, prompt initialization is crucial for prompt tuning. Recently,\nMetaPrompting (Hou et al., 2022) uses meta-learning to learn a shared\ninitialization for all task-specific prompts. However, a single initialization\nis insufficient to obtain good prompts for all tasks and samples when the tasks\nare complex. Moreover, MetaPrompting requires tuning the whole MLM, causing a\nheavy burden on computation and memory as the MLM is usually large. To address\nthese issues, we use a prompt pool to extract more task knowledge and construct\ninstance-dependent prompts via attention. We further propose a novel soft\nverbalizer (RepVerb) which constructs label embedding from feature embeddings\ndirectly. Combining meta-learning the prompt pool and RepVerb, we propose\nMetaPrompter for effective structured prompting. MetaPrompter is\nparameter-efficient as only the pool is required to be tuned. Experimental\nresults demonstrate that MetaPrompter performs better than the recent\nstate-of-the-arts and RepVerb outperforms existing soft verbalizers.", "published": "2023-06-01 12:44:33", "link": "http://arxiv.org/abs/2306.00618v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ReviewerGPT? An Exploratory Study on Using Large Language Models for\n  Paper Reviewing", "abstract": "Given the rapid ascent of large language models (LLMs), we study the\nquestion: (How) can large language models help in reviewing of scientific\npapers or proposals? We first conduct some pilot studies where we find that (i)\nGPT-4 outperforms other LLMs (Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly,\nOpenAssistant, StableLM), and (ii) prompting with a specific question (e.g., to\nidentify errors) outperforms prompting to simply write a review. With these\ninsights, we study the use of LLMs (specifically, GPT-4) for three tasks:\n  1. Identifying errors: We construct 13 short computer science papers each\nwith a deliberately inserted error, and ask the LLM to check for the\ncorrectness of these papers. We observe that the LLM finds errors in 7 of them,\nspanning both mathematical and conceptual errors.\n  2. Verifying checklists: We task the LLM to verify 16 closed-ended checklist\nquestions in the respective sections of 15 NeurIPS 2022 papers. We find that\nacross 119 {checklist question, paper} pairs, the LLM had an 86.6% accuracy.\n  3. Choosing the \"better\" paper: We generate 10 pairs of abstracts,\ndeliberately designing each pair in such a way that one abstract was clearly\nsuperior than the other. The LLM, however, struggled to discern these\nrelatively straightforward distinctions accurately, committing errors in its\nevaluations for 6 out of the 10 pairs.\n  Based on these experiments, we think that LLMs have a promising use as\nreviewing assistants for specific reviewing tasks, but not (yet) for complete\nevaluations of papers or proposals.", "published": "2023-06-01 12:45:53", "link": "http://arxiv.org/abs/2306.00622v1", "categories": ["cs.CL", "cs.AI", "cs.DL"], "primary_category": "cs.CL"}
{"title": "How Generative Spoken Language Modeling Encodes Noisy Speech:\n  Investigation from Phonetics to Syntactics", "abstract": "We examine the speech modeling potential of generative spoken language\nmodeling (GSLM), which involves using learned symbols derived from data rather\nthan phonemes for speech analysis and synthesis. Since GSLM facilitates\ntextless spoken language processing, exploring its effectiveness is critical\nfor paving the way for novel paradigms in spoken-language processing. This\npaper presents the findings of GSLM's encoding and decoding effectiveness at\nthe spoken-language and speech levels. Through speech resynthesis experiments,\nwe revealed that resynthesis errors occur at the levels ranging from phonology\nto syntactics and GSLM frequently resynthesizes natural but content-altered\nspeech.", "published": "2023-06-01 14:07:19", "link": "http://arxiv.org/abs/2306.00697v1", "categories": ["cs.CL", "cs.AI", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Enhancing the Unified Streaming and Non-streaming Model with Contrastive\n  Learning", "abstract": "The unified streaming and non-streaming speech recognition model has achieved\ngreat success due to its comprehensive capabilities. In this paper, we propose\nto improve the accuracy of the unified model by bridging the inherent\nrepresentation gap between the streaming and non-streaming modes with a\ncontrastive objective. Specifically, the top-layer hidden representation at the\nsame frame of the streaming and non-streaming modes are regarded as a positive\npair, encouraging the representation of the streaming mode close to its\nnon-streaming counterpart. The multiple negative samples are randomly selected\nfrom the rest frames of the same sample under the non-streaming mode.\nExperimental results demonstrate that the proposed method achieves consistent\nimprovements toward the unified model in both streaming and non-streaming\nmodes. Our method achieves CER of 4.66% in the streaming mode and CER of 4.31%\nin the non-streaming mode, which sets a new state-of-the-art on the AISHELL-1\nbenchmark.", "published": "2023-06-01 14:50:39", "link": "http://arxiv.org/abs/2306.00755v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Improved Cross-Lingual Transfer Learning For Automatic Speech\n  Translation", "abstract": "Research in multilingual speech-to-text translation is topical. Having a\nsingle model that supports multiple translation tasks is desirable. The goal of\nthis work it to improve cross-lingual transfer learning in multilingual\nspeech-to-text translation via semantic knowledge distillation. We show that by\ninitializing the encoder of the encoder-decoder sequence-to-sequence\ntranslation model with SAMU-XLS-R, a multilingual speech transformer encoder\ntrained using multi-modal (speech-text) semantic knowledge distillation, we\nachieve significantly better cross-lingual task knowledge transfer than the\nbaseline XLS-R, a multilingual speech transformer encoder trained via\nself-supervised learning. We demonstrate the effectiveness of our approach on\ntwo popular datasets, namely, CoVoST-2 and Europarl. On the 21 translation\ntasks of the CoVoST-2 benchmark, we achieve an average improvement of 12.8 BLEU\npoints over the baselines. In the zero-shot translation scenario, we achieve an\naverage gain of 18.8 and 11.9 average BLEU points on unseen medium and\nlow-resource languages. We make similar observations on Europarl speech\ntranslation benchmark.", "published": "2023-06-01 15:19:06", "link": "http://arxiv.org/abs/2306.00789v4", "categories": ["cs.CL", "cs.AI", "eess.AS", "eess.SP"], "primary_category": "cs.CL"}
{"title": "Birth of a Transformer: A Memory Viewpoint", "abstract": "Large language models based on transformers have achieved great empirical\nsuccesses. However, as they are deployed more widely, there is a growing need\nto better understand their internal mechanisms in order to make them more\nreliable. These models appear to store vast amounts of knowledge from their\ntraining data, and to adapt quickly to new information provided in their\ncontext or prompt. We study how transformers balance these two types of\nknowledge by considering a synthetic setup where tokens are generated from\neither global or context-specific bigram distributions. By a careful empirical\nanalysis of the training process on a simplified two-layer transformer, we\nillustrate the fast learning of global bigrams and the slower development of an\n\"induction head\" mechanism for the in-context bigrams. We highlight the role of\nweight matrices as associative memories, provide theoretical insights on how\ngradients enable their learning during training, and study the role of\ndata-distributional properties.", "published": "2023-06-01 15:30:33", "link": "http://arxiv.org/abs/2306.00802v2", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Adaptive Contextual Biasing for Transducer Based Streaming Speech\n  Recognition", "abstract": "By incorporating additional contextual information, deep biasing methods have\nemerged as a promising solution for speech recognition of personalized words.\nHowever, for real-world voice assistants, always biasing on such personalized\nwords with high prediction scores can significantly degrade the performance of\nrecognizing common words. To address this issue, we propose an adaptive\ncontextual biasing method based on Context-Aware Transformer Transducer (CATT)\nthat utilizes the biased encoder and predictor embeddings to perform streaming\nprediction of contextual phrase occurrences. Such prediction is then used to\ndynamically switch the bias list on and off, enabling the model to adapt to\nboth personalized and common scenarios. Experiments on Librispeech and internal\nvoice assistant datasets show that our approach can achieve up to 6.7% and\n20.7% relative reduction in WER and CER compared to the baseline respectively,\nmitigating up to 96.7% and 84.9% of the relative WER and CER increase for\ncommon cases. Furthermore, our approach has a minimal performance impact in\npersonalized scenarios while maintaining a streaming inference pipeline with\nnegligible RTF increase.", "published": "2023-06-01 15:33:30", "link": "http://arxiv.org/abs/2306.00804v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Transformer-based representation-learning model with unified\n  processing of multimodal input for clinical diagnostics", "abstract": "During the diagnostic process, clinicians leverage multimodal information,\nsuch as chief complaints, medical images, and laboratory-test results.\nDeep-learning models for aiding diagnosis have yet to meet this requirement.\nHere we report a Transformer-based representation-learning model as a clinical\ndiagnostic aid that processes multimodal input in a unified manner. Rather than\nlearning modality-specific features, the model uses embedding layers to convert\nimages and unstructured and structured text into visual tokens and text tokens,\nand bidirectional blocks with intramodal and intermodal attention to learn a\nholistic representation of radiographs, the unstructured chief complaint and\nclinical history, structured clinical information such as laboratory-test\nresults and patient demographic information. The unified model outperformed an\nimage-only model and non-unified multimodal diagnosis models in the\nidentification of pulmonary diseases (by 12% and 9%, respectively) and in the\nprediction of adverse clinical outcomes in patients with COVID-19 (by 29% and\n7%, respectively). Leveraging unified multimodal Transformer-based models may\nhelp streamline triage of patients and facilitate the clinical decision\nprocess.", "published": "2023-06-01 16:23:47", "link": "http://arxiv.org/abs/2306.00864v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "T2IAT: Measuring Valence and Stereotypical Biases in Text-to-Image\n  Generation", "abstract": "Warning: This paper contains several contents that may be toxic, harmful, or\noffensive.\n  In the last few years, text-to-image generative models have gained remarkable\nsuccess in generating images with unprecedented quality accompanied by a\nbreakthrough of inference speed. Despite their rapid progress, human biases\nthat manifest in the training examples, particularly with regard to common\nstereotypical biases, like gender and skin tone, still have been found in these\ngenerative models. In this work, we seek to measure more complex human biases\nexist in the task of text-to-image generations. Inspired by the well-known\nImplicit Association Test (IAT) from social psychology, we propose a novel\nText-to-Image Association Test (T2IAT) framework that quantifies the implicit\nstereotypes between concepts and valence, and those in the images. We replicate\nthe previously documented bias tests on generative models, including morally\nneutral tests on flowers and insects as well as demographic stereotypical tests\non diverse social attributes. The results of these experiments demonstrate the\npresence of complex stereotypical behaviors in image generations.", "published": "2023-06-01 17:02:51", "link": "http://arxiv.org/abs/2306.00905v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "I.2.6"], "primary_category": "cs.CL"}
{"title": "Minding Language Models' (Lack of) Theory of Mind: A Plug-and-Play\n  Multi-Character Belief Tracker", "abstract": "Theory of Mind (ToM)$\\unicode{x2014}$the ability to reason about the mental\nstates of other people$\\unicode{x2014}$is a key element of our social\nintelligence. Yet, despite their ever more impressive performance, large-scale\nneural language models still lack basic theory of mind capabilities\nout-of-the-box. We posit that simply scaling up models will not imbue them with\ntheory of mind due to the inherently symbolic and implicit nature of the\nphenomenon, and instead investigate an alternative: can we design a\ndecoding-time algorithm that enhances theory of mind of off-the-shelf neural\nlanguage models without explicit supervision? We present SymbolicToM, a\nplug-and-play approach to reason about the belief states of multiple characters\nin reading comprehension tasks via explicit symbolic representation. More\nconcretely, our approach tracks each entity's beliefs, their estimation of\nother entities' beliefs, and higher-order levels of reasoning, all through\ngraphical representations, allowing for more precise and interpretable\nreasoning than previous approaches. Empirical results on the well-known ToMi\nbenchmark (Le et al., 2019) demonstrate that SymbolicToM dramatically enhances\noff-the-shelf neural networks' theory of mind in a zero-shot setting while\nshowing robust out-of-distribution performance compared to supervised\nbaselines. Our work also reveals spurious patterns in existing theory of mind\nbenchmarks, emphasizing the importance of out-of-distribution evaluation and\nmethods that do not overfit a particular dataset.", "published": "2023-06-01 17:24:35", "link": "http://arxiv.org/abs/2306.00924v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ACLM: A Selective-Denoising based Generative Data Augmentation Approach\n  for Low-Resource Complex NER", "abstract": "Complex Named Entity Recognition (NER) is the task of detecting\nlinguistically complex named entities in low-context text. In this paper, we\npresent ACLM Attention-map aware keyword selection for Conditional Language\nModel fine-tuning), a novel data augmentation approach based on conditional\ngeneration to address the data scarcity problem in low-resource complex NER.\nACLM alleviates the context-entity mismatch issue, a problem existing NER data\naugmentation techniques suffer from and often generates incoherent\naugmentations by placing complex named entities in the wrong context. ACLM\nbuilds on BART and is optimized on a novel text reconstruction or denoising\ntask - we use selective masking (aided by attention maps) to retain the named\nentities and certain keywords in the input sentence that provide contextually\nrelevant additional knowledge or hints about the named entities. Compared with\nother data augmentation strategies, ACLM can generate more diverse and coherent\naugmentations preserving the true word sense of complex entities in the\nsentence. We demonstrate the effectiveness of ACLM both qualitatively and\nquantitatively on monolingual, cross-lingual, and multilingual complex NER\nacross various low-resource settings. ACLM outperforms all our neural baselines\nby a significant margin (1%-36%). In addition, we demonstrate the application\nof ACLM to other domains that suffer from data scarcity (e.g., biomedical). In\npractice, ACLM generates more effective and factual augmentations for these\ndomains than prior methods. Code: https://github.com/Sreyan88/ACLM", "published": "2023-06-01 17:33:04", "link": "http://arxiv.org/abs/2306.00928v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "EEL: Efficiently Encoding Lattices for Reranking", "abstract": "Standard decoding approaches for conditional text generation tasks typically\nsearch for an output hypothesis with high model probability, but this may not\nyield the best hypothesis according to human judgments of quality. Reranking to\noptimize for \"downstream\" metrics can better optimize for quality, but many\nmetrics of interest are computed with pre-trained language models, which are\nslow to apply to large numbers of hypotheses. We explore an approach for\nreranking hypotheses by using Transformers to efficiently encode lattices of\ngenerated outputs, a method we call EEL. With a single Transformer pass over\nthe entire lattice, we can approximately compute a contextualized\nrepresentation of each token as if it were only part of a single hypothesis in\nisolation. We combine this approach with a new class of token-factored\nrerankers (TFRs) that allow for efficient extraction of high reranker-scoring\nhypotheses from the lattice. Empirically, our approach incurs minimal\ndegradation error compared to the exponentially slower approach of encoding\neach hypothesis individually. When applying EEL with TFRs across three text\ngeneration tasks, our results show both substantial speedup compared to naive\nreranking and often better performance on downstream metrics than comparable\napproaches.", "published": "2023-06-01 17:45:32", "link": "http://arxiv.org/abs/2306.00947v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How to Estimate Model Transferability of Pre-Trained Speech Models?", "abstract": "In this work, we introduce a \"score-based assessment\" framework for\nestimating the transferability of pre-trained speech models (PSMs) for\nfine-tuning target tasks. We leverage upon two representation theories,\nBayesian likelihood estimation and optimal transport, to generate rank scores\nfor the PSM candidates using the extracted representations. Our framework\nefficiently computes transferability scores without actual fine-tuning of\ncandidate models or layers by making a temporal independent hypothesis. We\nevaluate some popular supervised speech models (e.g., Conformer RNN-Transducer)\nand self-supervised speech models (e.g., HuBERT) in cross-layer and cross-model\nsettings using public data. Experimental results show a high Spearman's rank\ncorrelation and low $p$-value between our estimation framework and fine-tuning\nground truth. Our proposed transferability framework requires less\ncomputational time and resources, making it a resource-saving and\ntime-efficient approach for tuning speech foundation models.", "published": "2023-06-01 04:52:26", "link": "http://arxiv.org/abs/2306.01015v3", "categories": ["cs.CL", "cs.NE", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Bypass Temporal Classification: Weakly Supervised Automatic Speech\n  Recognition with Imperfect Transcripts", "abstract": "This paper presents a novel algorithm for building an automatic speech\nrecognition (ASR) model with imperfect training data. Imperfectly transcribed\nspeech is a prevalent issue in human-annotated speech corpora, which degrades\nthe performance of ASR models. To address this problem, we propose Bypass\nTemporal Classification (BTC) as an expansion of the Connectionist Temporal\nClassification (CTC) criterion. BTC explicitly encodes the uncertainties\nassociated with transcripts during training. This is accomplished by enhancing\nthe flexibility of the training graph, which is implemented as a weighted\nfinite-state transducer (WFST) composition. The proposed algorithm improves the\nrobustness and accuracy of ASR systems, particularly when working with\nimprecisely transcribed speech corpora. Our implementation will be\nopen-sourced.", "published": "2023-06-01 14:56:19", "link": "http://arxiv.org/abs/2306.01031v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "TimelineQA: A Benchmark for Question Answering over Timelines", "abstract": "Lifelogs are descriptions of experiences that a person had during their life.\nLifelogs are created by fusing data from the multitude of digital services,\nsuch as online photos, maps, shopping and content streaming services. Question\nanswering over lifelogs can offer personal assistants a critical resource when\nthey try to provide advice in context. However, obtaining answers to questions\nover lifelogs is beyond the current state of the art of question answering\ntechniques for a variety of reasons, the most pronounced of which is that\nlifelogs combine free text with some degree of structure such as temporal and\ngeographical information.\n  We create and publicly release TimelineQA1, a benchmark for accelerating\nprogress on querying lifelogs. TimelineQA generates lifelogs of imaginary\npeople. The episodes in the lifelog range from major life episodes such as high\nschool graduation to those that occur on a daily basis such as going for a run.\nWe describe a set of experiments on TimelineQA with several state-of-the-art QA\nmodels. Our experiments reveal that for atomic queries, an extractive QA system\nsignificantly out-performs a state-of-the-art retrieval-augmented QA system.\nFor multi-hop queries involving aggregates, we show that the best result is\nobtained with a state-of-the-art table QA technique, assuming the ground truth\nset of episodes for deriving the answer is available.", "published": "2023-06-01 18:17:13", "link": "http://arxiv.org/abs/2306.01069v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "LLMatic: Neural Architecture Search via Large Language Models and\n  Quality Diversity Optimization", "abstract": "Large Language Models (LLMs) have emerged as powerful tools capable of\naccomplishing a broad spectrum of tasks. Their abilities span numerous areas,\nand one area where they have made a significant impact is in the domain of code\ngeneration. Here, we propose using the coding abilities of LLMs to introduce\nmeaningful variations to code defining neural networks. Meanwhile,\nQuality-Diversity (QD) algorithms are known to discover diverse and robust\nsolutions. By merging the code-generating abilities of LLMs with the diversity\nand robustness of QD solutions, we introduce \\texttt{LLMatic}, a Neural\nArchitecture Search (NAS) algorithm. While LLMs struggle to conduct NAS\ndirectly through prompts, \\texttt{LLMatic} uses a procedural approach,\nleveraging QD for prompts and network architecture to create diverse and\nhigh-performing networks. We test \\texttt{LLMatic} on the CIFAR-10 and\nNAS-bench-201 benchmarks, demonstrating that it can produce competitive\nnetworks while evaluating just $2,000$ candidates, even without prior knowledge\nof the benchmark domain or exposure to any previous top-performing models for\nthe benchmark. The open-sourced code is available in\n\\url{https://github.com/umair-nasir14/LLMatic}.", "published": "2023-06-01 19:33:21", "link": "http://arxiv.org/abs/2306.01102v8", "categories": ["cs.NE", "cs.AI", "cs.CL"], "primary_category": "cs.NE"}
{"title": "Faster Causal Attention Over Large Sequences Through Sparse Flash\n  Attention", "abstract": "Transformer-based language models have found many diverse applications\nrequiring them to process sequences of increasing length. For these\napplications, the causal self-attention -- which is the only component scaling\nquadratically w.r.t. the sequence length -- becomes a central concern. While\nmany works have proposed schemes to sparsify the attention patterns and reduce\nthe computational overhead of self-attention, those are often limited by\nimplementations concerns and end up imposing a simple and static structure over\nthe attention matrix. Conversely, implementing more dynamic sparse attentions\noften results in runtimes significantly slower than computing the full\nattention using the Flash implementation from Dao et al. (2022). We extend\nFlashAttention to accommodate a large class of attention sparsity patterns\nthat, in particular, encompass key/query dropping and hashing-based attention.\nThis leads to implementations with no computational complexity overhead and a\nmulti-fold runtime speedup on top of FlashAttention. Even with relatively low\ndegrees of sparsity, our method improves visibly upon FlashAttention as the\nsequence length increases. Without sacrificing perplexity, we increase the\ntraining speed of a transformer language model by $2.0\\times$ and $3.3\\times$\nfor sequences of respectively $8k$ and $16k$ tokens.", "published": "2023-06-01 21:33:59", "link": "http://arxiv.org/abs/2306.01160v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Learning When to Speak: Latency and Quality Trade-offs for Simultaneous\n  Speech-to-Speech Translation with Offline Models", "abstract": "Recent work in speech-to-speech translation (S2ST) has focused primarily on\noffline settings, where the full input utterance is available before any output\nis given. This, however, is not reasonable in many real-world scenarios. In\nlatency-sensitive applications, rather than waiting for the full utterance,\ntranslations should be spoken as soon as the information in the input is\npresent. In this work, we introduce a system for simultaneous S2ST targeting\nreal-world use cases. Our system supports translation from 57 languages to\nEnglish with tunable parameters for dynamically adjusting the latency of the\noutput -- including four policies for determining when to speak an output\nsequence. We show that these policies achieve offline-level accuracy with\nminimal increases in latency over a Greedy (wait-$k$) baseline. We open-source\nour evaluation code and interactive test script to aid future SimulS2ST\nresearch and application development.", "published": "2023-06-01 23:29:23", "link": "http://arxiv.org/abs/2306.01201v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Estimating Semantic Similarity between In-Domain and Out-of-Domain\n  Samples", "abstract": "Prior work typically describes out-of-domain (OOD) or out-of-distribution\n(OODist) samples as those that originate from dataset(s) or source(s) different\nfrom the training set but for the same task. When compared to in-domain (ID)\nsamples, the models have been known to usually perform poorer on OOD samples,\nalthough this observation is not consistent. Another thread of research has\nfocused on OOD detection, albeit mostly using supervised approaches. In this\nwork, we first consolidate and present a systematic analysis of multiple\ndefinitions of OOD and OODist as discussed in prior literature. Then, we\nanalyze the performance of a model under ID and OOD/OODist settings in a\nprincipled way. Finally, we seek to identify an unsupervised method for\nreliably identifying OOD/OODist samples without using a trained model. The\nresults of our extensive evaluation using 12 datasets from 4 different tasks\nsuggest the promising potential of unsupervised metrics in this task.", "published": "2023-06-01 23:39:07", "link": "http://arxiv.org/abs/2306.01206v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adapting an Unadaptable ASR System", "abstract": "As speech recognition model sizes and training data requirements grow, it is\nincreasingly common for systems to only be available via APIs from online\nservice providers rather than having direct access to models themselves. In\nthis scenario it is challenging to adapt systems to a specific target domain.\nTo address this problem we consider the recently released OpenAI Whisper ASR as\nan example of a large-scale ASR system to assess adaptation methods. An error\ncorrection based approach is adopted, as this does not require access to the\nmodel, but can be trained from either 1-best or N-best outputs that are\nnormally available via the ASR API. LibriSpeech is used as the primary target\ndomain for adaptation. The generalization ability of the system in two distinct\ndimensions are then evaluated. First, whether the form of correction model is\nportable to other speech recognition domains, and secondly whether it can be\nused for ASR models having a different architecture.", "published": "2023-06-01 23:54:11", "link": "http://arxiv.org/abs/2306.01208v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cook-Gen: Robust Generative Modeling of Cooking Actions from Recipes", "abstract": "As people become more aware of their food choices, food computation models\nhave become increasingly popular in assisting people in maintaining healthy\neating habits. For example, food recommendation systems analyze recipe\ninstructions to assess nutritional contents and provide recipe recommendations.\nThe recent and remarkable successes of generative AI methods, such as\nauto-regressive large language models, can lead to robust methods for a more\ncomprehensive understanding of recipes for healthy food recommendations beyond\nsurface-level nutrition content assessments. In this study, we explore the use\nof generative AI methods to extend current food computation models, primarily\ninvolving the analysis of nutrition and ingredients, to also incorporate\ncooking actions (e.g., add salt, fry the meat, boil the vegetables, etc.).\nCooking actions are notoriously hard to model using statistical learning\nmethods due to irregular data patterns - significantly varying natural language\ndescriptions for the same action (e.g., marinate the meat vs. marinate the meat\nand leave overnight) and infrequently occurring patterns (e.g., add salt occurs\nfar more frequently than marinating the meat). The prototypical approach to\nhandling irregular data patterns is to increase the volume of data that the\nmodel ingests by orders of magnitude. Unfortunately, in the cooking domain,\nthese problems are further compounded with larger data volumes presenting a\nunique challenge that is not easily handled by simply scaling up. In this work,\nwe propose novel aggregation-based generative AI methods, Cook-Gen, that\nreliably generate cooking actions from recipes, despite difficulties with\nirregular data patterns, while also outperforming Large Language Models and\nother strong baselines.", "published": "2023-06-01 18:49:47", "link": "http://arxiv.org/abs/2306.01805v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Some voices are too common: Building fair speech recognition systems\n  using the Common Voice dataset", "abstract": "Automatic speech recognition (ASR) systems become increasingly efficient\nthanks to new advances in neural network training like self-supervised\nlearning. However, they are known to be unfair toward certain groups, for\ninstance, people speaking with an accent. In this work, we use the French\nCommon Voice dataset to quantify the biases of a pre-trained wav2vec~2.0 model\ntoward several demographic groups. By fine-tuning the pre-trained model on a\nvariety of fixed-size, carefully crafted training sets, we demonstrate the\nimportance of speaker diversity. We also run an in-depth analysis of the Common\nVoice corpus and identify important shortcomings that should be taken into\naccount by users of this dataset.", "published": "2023-06-01 11:42:34", "link": "http://arxiv.org/abs/2306.03773v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On the Robustness of Arabic Speech Dialect Identification", "abstract": "Arabic dialect identification (ADI) tools are an important part of the\nlarge-scale data collection pipelines necessary for training speech recognition\nmodels. As these pipelines require application of ADI tools to potentially\nout-of-domain data, we aim to investigate how vulnerable the tools may be to\nthis domain shift. With self-supervised learning (SSL) models as a starting\npoint, we evaluate transfer learning and direct classification from SSL\nfeatures. We undertake our evaluation under rich conditions, with a goal to\ndevelop ADI systems from pretrained models and ultimately evaluate performance\non newly collected data. In order to understand what factors contribute to\nmodel decisions, we carry out a careful human study of a subset of our data.\nOur analysis confirms that domain shift is a major challenge for ADI models. We\nalso find that while self-training does alleviate this challenges, it may be\ninsufficient for realistic conditions.", "published": "2023-06-01 21:31:00", "link": "http://arxiv.org/abs/2306.03789v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Inspecting Spoken Language Understanding from Kids for Basic Math\n  Learning at Home", "abstract": "Enriching the quality of early childhood education with interactive math\nlearning at home systems, empowered by recent advances in conversational AI\ntechnologies, is slowly becoming a reality. With this motivation, we implement\na multimodal dialogue system to support play-based learning experiences at\nhome, guiding kids to master basic math concepts. This work explores Spoken\nLanguage Understanding (SLU) pipeline within a task-oriented dialogue system\ndeveloped for Kid Space, with cascading Automatic Speech Recognition (ASR) and\nNatural Language Understanding (NLU) components evaluated on our home\ndeployment data with kids going through gamified math learning activities. We\nvalidate the advantages of a multi-task architecture for NLU and experiment\nwith a diverse set of pretrained language representations for Intent\nRecognition and Entity Extraction tasks in the math learning domain. To\nrecognize kids' speech in realistic home environments, we investigate several\nASR systems, including the commercial Google Cloud and the latest open-source\nWhisper solutions with varying model sizes. We evaluate the SLU pipeline by\ntesting our best-performing NLU models on noisy ASR output to inspect the\nchallenges of understanding children for math learning in authentic homes.", "published": "2023-06-01 09:31:57", "link": "http://arxiv.org/abs/2306.00482v1", "categories": ["cs.CY", "cs.CL", "cs.SD", "eess.AS", "math.HO"], "primary_category": "cs.CY"}
{"title": "Topic-Guided Sampling For Data-Efficient Multi-Domain Stance Detection", "abstract": "Stance Detection is concerned with identifying the attitudes expressed by an\nauthor towards a target of interest. This task spans a variety of domains\nranging from social media opinion identification to detecting the stance for a\nlegal claim. However, the framing of the task varies within these domains, in\nterms of the data collection protocol, the label dictionary and the number of\navailable annotations. Furthermore, these stance annotations are significantly\nimbalanced on a per-topic and inter-topic basis. These make multi-domain stance\ndetection a challenging task, requiring standardization and domain adaptation.\nTo overcome this challenge, we propose $\\textbf{T}$opic $\\textbf{E}$fficient\n$\\textbf{St}$anc$\\textbf{E}$ $\\textbf{D}$etection (TESTED), consisting of a\ntopic-guided diversity sampling technique and a contrastive objective that is\nused for fine-tuning a stance classifier. We evaluate the method on an existing\nbenchmark of $16$ datasets with in-domain, i.e. all topics seen and\nout-of-domain, i.e. unseen topics, experiments. The results show that our\nmethod outperforms the state-of-the-art with an average of $3.5$ F1 points\nincrease in-domain, and is more generalizable with an averaged increase of\n$10.2$ F1 on out-of-domain evaluation while using $\\leq10\\%$ of the training\ndata. We show that our sampling technique mitigates both inter- and per-topic\nclass imbalances. Finally, our analysis demonstrates that the contrastive\nlearning objective allows the model a more pronounced segmentation of samples\nwith varying labels.", "published": "2023-06-01 15:00:39", "link": "http://arxiv.org/abs/2306.00765v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "stat.CO", "stat.ML"], "primary_category": "cs.CL"}
{"title": "PV2TEA: Patching Visual Modality to Textual-Established Information\n  Extraction", "abstract": "Information extraction, e.g., attribute value extraction, has been\nextensively studied and formulated based only on text. However, many attributes\ncan benefit from image-based extraction, like color, shape, pattern, among\nothers. The visual modality has long been underutilized, mainly due to\nmultimodal annotation difficulty. In this paper, we aim to patch the visual\nmodality to the textual-established attribute information extractor. The\ncross-modality integration faces several unique challenges: (C1) images and\ntextual descriptions are loosely paired intra-sample and inter-samples; (C2)\nimages usually contain rich backgrounds that can mislead the prediction; (C3)\nweakly supervised labels from textual-established extractors are biased for\nmultimodal training. We present PV2TEA, an encoder-decoder architecture\nequipped with three bias reduction schemes: (S1) Augmented label-smoothed\ncontrast to improve the cross-modality alignment for loosely-paired image and\ntext; (S2) Attention-pruning that adaptively distinguishes the visual\nforeground; (S3) Two-level neighborhood regularization that mitigates the label\ntextual bias via reliability estimation. Empirical results on real-world\ne-Commerce datasets demonstrate up to 11.74% absolute (20.97% relatively) F1\nincrease over unimodal baselines.", "published": "2023-06-01 05:39:45", "link": "http://arxiv.org/abs/2306.01016v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Frame-wise and overlap-robust speaker embeddings for meeting diarization", "abstract": "Using a Teacher-Student training approach we developed a speaker embedding\nextraction system that outputs embeddings at frame rate. Given this high\ntemporal resolution and the fact that the student produces sensible speaker\nembeddings even for segments with speech overlap, the frame-wise embeddings\nserve as an appropriate representation of the input speech signal for an\nend-to-end neural meeting diarization (EEND) system. We show in experiments\nthat this representation helps mitigate a well-known problem of EEND systems:\nwhen increasing the number of speakers the diarization performance drop is\nsignificantly reduced. We also introduce block-wise processing to be able to\ndiarize arbitrarily long meetings.", "published": "2023-06-01 12:47:47", "link": "http://arxiv.org/abs/2306.00625v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Speaker verification using attentive multi-scale convolutional recurrent\n  network", "abstract": "In this paper, we propose a speaker verification method by an Attentive\nMulti-scale Convolutional Recurrent Network (AMCRN). The proposed AMCRN can\nacquire both local spatial information and global sequential information from\nthe input speech recordings. In the proposed method, logarithm Mel spectrum is\nextracted from each speech recording and then fed to the proposed AMCRN for\nlearning speaker embedding. Afterwards, the learned speaker embedding is fed to\nthe back-end classifier (such as cosine similarity metric) for scoring in the\ntesting stage. The proposed method is compared with state-of-the-art methods\nfor speaker verification. Experimental data are three public datasets that are\nselected from two large-scale speech corpora (VoxCeleb1 and VoxCeleb2).\nExperimental results show that our method exceeds baseline methods in terms of\nequal error rate and minimal detection cost function, and has advantages over\nmost of baseline methods in terms of computational complexity and memory\nrequirement. In addition, our method generalizes well across truncated speech\nsegments with different durations, and the speaker embedding learned by the\nproposed AMCRN has stronger generalization ability across two back-end\nclassifiers.", "published": "2023-06-01 08:05:33", "link": "http://arxiv.org/abs/2306.00426v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Self-Supervised Representation Benchmarking: Are We Doing it\n  Right?", "abstract": "Self-supervised learning (SSL) has recently allowed leveraging large datasets\nof unlabeled speech signals to reach impressive performance on speech tasks\nusing only small amounts of annotated data. The high number of proposed\napproaches fostered the need and rise of extended benchmarks that evaluate\ntheir performance on a set of downstream tasks exploring various aspects of the\nspeech signal. However, and while the number of considered tasks has been\ngrowing, most rely upon a single decoding architecture that maps the frozen SSL\nrepresentations to the downstream labels. This work investigates the robustness\nof such benchmarking results to changes in the decoder architecture.\nInterestingly, it appears that varying the architecture of the downstream\ndecoder leads to significant variations in the leaderboards of most tasks.\nConcerningly, our study reveals that benchmarking using limited decoders may\ncause a counterproductive increase in the sizes of the developed SSL models.", "published": "2023-06-01 08:51:18", "link": "http://arxiv.org/abs/2306.00452v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Automatic Data Augmentation for Domain Adapted Fine-Tuning of\n  Self-Supervised Speech Representations", "abstract": "Self-Supervised Learning (SSL) has allowed leveraging large amounts of\nunlabeled speech data to improve the performance of speech recognition models\neven with small annotated datasets. Despite this, speech SSL representations\nmay fail while facing an acoustic mismatch between the pretraining and target\ndatasets. To address this issue, we propose a novel supervised domain\nadaptation method, designed for cases exhibiting such a mismatch in acoustic\ndomains. It consists in applying properly calibrated data augmentations on a\nlarge clean dataset, bringing it closer to the target domain, and using it as\npart of an initial fine-tuning stage. Augmentations are automatically selected\nthrough the minimization of a conditional-dependence estimator, based on the\ntarget dataset. The approach is validated during an oracle experiment with\ncontrolled distortions and on two amateur-collected low-resource domains,\nreaching better performances compared to the baselines in both cases.", "published": "2023-06-01 09:30:49", "link": "http://arxiv.org/abs/2306.00481v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "A Teacher-Student approach for extracting informative speaker embeddings\n  from speech mixtures", "abstract": "We introduce a monaural neural speaker embeddings extractor that computes an\nembedding for each speaker present in a speech mixture. To allow for supervised\ntraining, a teacher-student approach is employed: the teacher computes the\ntarget embeddings from each speaker's utterance before the utterances are added\nto form the mixture, and the student embedding extractor is then tasked to\nreproduce those embeddings from the speech mixture at its input. The system\nmuch more reliably verifies the presence or absence of a given speaker in a\nmixture than a conventional speaker embedding extractor, and even exhibits\ncomparable performance to a multi-channel approach that exploits spatial\ninformation for embedding extraction. Further, it is shown that a speaker\nembedding computed from a mixture can be used to check for the presence of that\nspeaker in another mixture.", "published": "2023-06-01 12:59:04", "link": "http://arxiv.org/abs/2306.00634v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "EmoMix: Emotion Mixing via Diffusion Models for Emotional Speech\n  Synthesis", "abstract": "There has been significant progress in emotional Text-To-Speech (TTS)\nsynthesis technology in recent years. However, existing methods primarily focus\non the synthesis of a limited number of emotion types and have achieved\nunsatisfactory performance in intensity control. To address these limitations,\nwe propose EmoMix, which can generate emotional speech with specified intensity\nor a mixture of emotions. Specifically, EmoMix is a controllable emotional TTS\nmodel based on a diffusion probabilistic model and a pre-trained speech emotion\nrecognition (SER) model used to extract emotion embedding. Mixed emotion\nsynthesis is achieved by combining the noises predicted by diffusion model\nconditioned on different emotions during only one sampling process at the\nrun-time. We further apply the Neutral and specific primary emotion mixed in\nvarying degrees to control intensity. Experimental results validate the\neffectiveness of EmoMix for synthesizing mixed emotion and intensity control.", "published": "2023-06-01 13:14:56", "link": "http://arxiv.org/abs/2306.00648v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spoken Language Identification System for English-Mandarin\n  Code-Switching Child-Directed Speech", "abstract": "This work focuses on improving the Spoken Language Identification (LangId)\nsystem for a challenge that focuses on developing robust language\nidentification systems that are reliable for non-standard, accented\n(Singaporean accent), spontaneous code-switched, and child-directed speech\ncollected via Zoom. We propose a two-stage Encoder-Decoder-based E2E model. The\nencoder module consists of 1D depth-wise separable convolutions with\nSqueeze-and-Excitation (SE) layers with a global context. The decoder module\nuses an attentive temporal pooling mechanism to get fixed length\ntime-independent feature representation. The total number of parameters in the\nmodel is around 22.1 M, which is relatively light compared to using some\nlarge-scale pre-trained speech models. We achieved an EER of 15.6% in the\nclosed track and 11.1% in the open track (baseline system 22.1%). We also\ncurated additional LangId data from YouTube videos (having Singaporean\nspeakers), which will be released for public use.", "published": "2023-06-01 14:30:28", "link": "http://arxiv.org/abs/2306.00736v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Harmonic enhancement using learnable comb filter for light-weight\n  full-band speech enhancement model", "abstract": "With fewer feature dimensions, filter banks are often used in light-weight\nfull-band speech enhancement models. In order to further enhance the coarse\nspeech in the sub-band domain, it is necessary to apply a post-filtering for\nharmonic retrieval. The signal processing-based comb filters used in RNNoise\nand PercepNet have limited performance and may cause speech quality degradation\ndue to inaccurate fundamental frequency estimation. To tackle this problem, we\npropose a learnable comb filter to enhance harmonics. Based on the sub-band\nmodel, we design a DNN-based fundamental frequency estimator to estimate the\ndiscrete fundamental frequencies and a comb filter for harmonic enhancement,\nwhich are trained via an end-to-end pattern. The experiments show the\nadvantages of our proposed method over PecepNet and DeepFilterNet.", "published": "2023-06-01 15:39:20", "link": "http://arxiv.org/abs/2306.00812v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Adapting a ConvNeXt model to audio classification on AudioSet", "abstract": "In computer vision, convolutional neural networks (CNN) such as ConvNeXt,\nhave been able to surpass state-of-the-art transformers, partly thanks to\ndepthwise separable convolutions (DSC). DSC, as an approximation of the regular\nconvolution, has made CNNs more efficient in time and memory complexity without\ndeteriorating their accuracy, and sometimes even improving it. In this paper,\nwe first implement DSC into the Pretrained Audio Neural Networks (PANN) family\nfor audio classification on AudioSet, to show its benefits in terms of\naccuracy/model size trade-off. Second, we adapt the now famous ConvNeXt model\nto the same task. It rapidly overfits, so we report on techniques that improve\nthe learning process. Our best ConvNeXt model reached 0.471 mean-average\nprecision on AudioSet, which is better than or equivalent to recent large audio\ntransformers, while using three times less parameters. We also achieved\npositive results in audio captioning and audio retrieval with this model. Our\nPyTorch source code and checkpoint models are available at\nhttps://github.com/topel/audioset-convnext-inf.", "published": "2023-06-01 15:52:11", "link": "http://arxiv.org/abs/2306.00830v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Differentiable Allpass Filters for Phase Response Estimation and\n  Automatic Signal Alignment", "abstract": "Virtual analog (VA) audio effects are increasingly based on neural networks\nand deep learning frameworks. Due to the underlying black-box methodology, a\nsuccessful model will learn to approximate the data it is presented, including\npotential errors such as latency and audio dropouts as well as non-linear\ncharacteristics and frequency-dependent phase shifts produced by the hardware.\nThe latter is of particular interest as the learned phase-response might cause\nunwanted audible artifacts when the effect is used for creative processing\ntechniques such as dry-wet mixing or parallel compression. To overcome these\nartifacts we propose differentiable signal processing tools and deep\noptimization structures for automatically tuning all-pass filters to predict\nthe phase response of different VA simulations, and align processed signals\nthat are out of phase. The approaches are assessed using objective metrics\nwhile listening tests evaluate their ability to enhance the quality of parallel\npath processing techniques. Ultimately, an over-parameterized, BiasNet-based,\nall-pass model is proposed for the optimization problem under consideration,\nresulting in models that can estimate all-pass filter coefficients to align a\ndry signal with its affected, wet, equivalent.", "published": "2023-06-01 16:19:18", "link": "http://arxiv.org/abs/2306.00860v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploration on HuBERT with Multiple Resolutions", "abstract": "Hidden-unit BERT (HuBERT) is a widely-used self-supervised learning (SSL)\nmodel in speech processing. However, we argue that its fixed 20ms resolution\nfor hidden representations would not be optimal for various speech-processing\ntasks since their attributes (e.g., speaker characteristics and semantics) are\nbased on different time scales. To address this limitation, we propose\nutilizing HuBERT representations at multiple resolutions for downstream tasks.\nWe explore two approaches, namely the parallel and hierarchical approaches, for\nintegrating HuBERT features with different resolutions. Through experiments, we\ndemonstrate that HuBERT with multiple resolutions outperforms the original\nmodel. This highlights the potential of utilizing multiple resolutions in SSL\nmodels like HuBERT to capture diverse information from speech signals.", "published": "2023-06-01 18:51:34", "link": "http://arxiv.org/abs/2306.01084v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Transfer Learning for Underrepresented Music Generation", "abstract": "This paper investigates a combinational creativity approach to transfer\nlearning to improve the performance of deep neural network-based models for\nmusic generation on out-of-distribution (OOD) genres. We identify Iranian folk\nmusic as an example of such an OOD genre for MusicVAE, a large generative music\nmodel. We find that a combinational creativity transfer learning approach can\nefficiently adapt MusicVAE to an Iranian folk music dataset, indicating\npotential for generating underrepresented music genres in the future.", "published": "2023-06-01 01:53:10", "link": "http://arxiv.org/abs/2306.00281v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Speech inpainting: Context-based speech synthesis guided by video", "abstract": "Audio and visual modalities are inherently connected in speech signals: lip\nmovements and facial expressions are correlated with speech sounds. This\nmotivates studies that incorporate the visual modality to enhance an acoustic\nspeech signal or even restore missing audio information. Specifically, this\npaper focuses on the problem of audio-visual speech inpainting, which is the\ntask of synthesizing the speech in a corrupted audio segment in a way that it\nis consistent with the corresponding visual content and the uncorrupted audio\ncontext. We present an audio-visual transformer-based deep learning model that\nleverages visual cues that provide information about the content of the\ncorrupted audio. It outperforms the previous state-of-the-art audio-visual\nmodel and audio-only baselines. We also show how visual features extracted with\nAV-HuBERT, a large audio-visual transformer for speech recognition, are\nsuitable for synthesizing speech.", "published": "2023-06-01 09:40:47", "link": "http://arxiv.org/abs/2306.00489v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Masked Autoencoders with Multi-Window Local-Global Attention Are Better\n  Audio Learners", "abstract": "In this work, we propose a Multi-Window Masked Autoencoder (MW-MAE) fitted\nwith a novel Multi-Window Multi-Head Attention (MW-MHA) module that facilitates\nthe modelling of local-global interactions in every decoder transformer block\nthrough attention heads of several distinct local and global windows. Empirical\nresults on ten downstream audio tasks show that MW-MAEs consistently outperform\nstandard MAEs in overall performance and learn better general-purpose audio\nrepresentations, along with demonstrating considerably better scaling\ncharacteristics. Investigating attention distances and entropies reveals that\nMW-MAE encoders learn heads with broader local and global attention. Analyzing\nattention head feature representations through Projection Weighted Canonical\nCorrelation Analysis (PWCCA) shows that attention heads with the same window\nsizes across the decoder layers of the MW-MAE learn correlated feature\nrepresentations which enables each block to independently capture local and\nglobal information, leading to a decoupled decoder feature hierarchy. Code for\nfeature extraction and downstream experiments along with pre-trained models\nwill be released publically.", "published": "2023-06-01 11:20:59", "link": "http://arxiv.org/abs/2306.00561v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Encoder-decoder multimodal speaker change detection", "abstract": "The task of speaker change detection (SCD), which detects points where\nspeakers change in an input, is essential for several applications. Several\nstudies solved the SCD task using audio inputs only and have shown limited\nperformance. Recently, multimodal SCD (MMSCD) models, which utilise text\nmodality in addition to audio, have shown improved performance. In this study,\nthe proposed model are built upon two main proposals, a novel mechanism for\nmodality fusion and the adoption of a encoder-decoder architecture. Different\nto previous MMSCD works that extract speaker embeddings from extremely short\naudio segments, aligned to a single word, we use a speaker embedding extracted\nfrom 1.5s. A transformer decoder layer further improves the performance of an\nencoder-only MMSCD model. The proposed model achieves state-of-the-art results\namong studies that report SCD performance and is also on par with recent work\nthat combines SCD with automatic speech recognition via human transcription.", "published": "2023-06-01 13:55:23", "link": "http://arxiv.org/abs/2306.00680v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Stuttering Detection Using Speaker Representations and Self-supervised\n  Contextual Embeddings", "abstract": "The adoption of advanced deep learning architectures in stuttering detection\n(SD) tasks is challenging due to the limited size of the available datasets. To\nthis end, this work introduces the application of speech embeddings extracted\nfrom pre-trained deep learning models trained on large audio datasets for\ndifferent tasks. In particular, we explore audio representations obtained using\nemphasized channel attention, propagation, and aggregation time delay neural\nnetwork (ECAPA-TDNN) and Wav2Vec2.0 models trained on VoxCeleb and LibriSpeech\ndatasets respectively. After extracting the embeddings, we benchmark with\nseveral traditional classifiers, such as the K-nearest neighbour (KNN),\nGaussian naive Bayes, and neural network, for the SD tasks. In comparison to\nthe standard SD systems trained only on the limited SEP-28k dataset, we obtain\na relative improvement of 12.08%, 28.71%, 37.9% in terms of unweighted average\nrecall (UAR) over the baselines. Finally, we have shown that combining two\nembeddings and concatenating multiple layers of Wav2Vec2.0 can further improve\nthe UAR by up to 2.60% and 6.32% respectively.", "published": "2023-06-01 14:00:47", "link": "http://arxiv.org/abs/2306.00689v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "UnDiff: Unsupervised Voice Restoration with Unconditional Diffusion\n  Model", "abstract": "This paper introduces UnDiff, a diffusion probabilistic model capable of\nsolving various speech inverse tasks. Being once trained for speech waveform\ngeneration in an unconditional manner, it can be adapted to different tasks\nincluding degradation inversion, neural vocoding, and source separation. In\nthis paper, we, first, tackle the challenging problem of unconditional waveform\ngeneration by comparing different neural architectures and preconditioning\ndomains. After that, we demonstrate how the trained unconditional diffusion\ncould be adapted to different tasks of speech processing by the means of recent\ndevelopments in post-training conditioning of diffusion models. Finally, we\ndemonstrate the performance of the proposed technique on the tasks of bandwidth\nextension, declipping, vocoding, and speech source separation and compare it to\nthe baselines. The codes are publicly available.", "published": "2023-06-01 14:22:55", "link": "http://arxiv.org/abs/2306.00721v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SlothSpeech: Denial-of-service Attack Against Speech Recognition Models", "abstract": "Deep Learning (DL) models have been popular nowadays to execute different\nspeech-related tasks, including automatic speech recognition (ASR). As ASR is\nbeing used in different real-time scenarios, it is important that the ASR model\nremains efficient against minor perturbations to the input. Hence, evaluating\nefficiency robustness of the ASR model is the need of the hour. We show that\npopular ASR models like Speech2Text model and Whisper model have dynamic\ncomputation based on different inputs, causing dynamic efficiency. In this\nwork, we propose SlothSpeech, a denial-of-service attack against ASR models,\nwhich exploits the dynamic behaviour of the model. SlothSpeech uses the\nprobability distribution of the output text tokens to generate perturbations to\nthe audio such that efficiency of the ASR model is decreased. We find that\nSlothSpeech generated inputs can increase the latency up to 40X times the\nlatency induced by benign input.", "published": "2023-06-01 15:25:14", "link": "http://arxiv.org/abs/2306.00794v1", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Vocos: Closing the gap between time-domain and Fourier-based neural\n  vocoders for high-quality audio synthesis", "abstract": "Recent advancements in neural vocoding are predominantly driven by Generative\nAdversarial Networks (GANs) operating in the time-domain. While effective, this\napproach neglects the inductive bias offered by time-frequency representations,\nresulting in reduntant and computionally-intensive upsampling operations.\nFourier-based time-frequency representation is an appealing alternative,\naligning more accurately with human auditory perception, and benefitting from\nwell-established fast algorithms for its computation. Nevertheless, direct\nreconstruction of complex-valued spectrograms has been historically\nproblematic, primarily due to phase recovery issues. This study seeks to close\nthis gap by presenting Vocos, a new model that directly generates Fourier\nspectral coefficients. Vocos not only matches the state-of-the-art in audio\nquality, as demonstrated in our evaluations, but it also substantially improves\ncomputational efficiency, achieving an order of magnitude increase in speed\ncompared to prevailing time-domain neural vocoding approaches. The source code\nand model weights have been open-sourced at https://github.com/gemelo-ai/vocos.", "published": "2023-06-01 15:40:32", "link": "http://arxiv.org/abs/2306.00814v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Meta-Learning Framework for End-to-End Imposter Identification in Unseen\n  Speaker Recognition", "abstract": "Speaker identification systems are deployed in diverse environments, often\ndifferent from the lab conditions on which they are trained and tested. In this\npaper, first, we show the problem of generalization using fixed thresholds\n(computed using EER metric) for imposter identification in unseen speaker\nrecognition and then introduce a robust speaker-specific thresholding technique\nfor better performance. Secondly, inspired by the recent use of meta-learning\ntechniques in speaker verification, we propose an end-to-end meta-learning\nframework for imposter detection which decouples the problem of imposter\ndetection from unseen speaker identification. Thus, unlike most prior works\nthat use some heuristics to detect imposters, the proposed network learns to\ndetect imposters by leveraging the utterances of the enrolled speakers.\nFurthermore, we show the efficacy of the proposed techniques on VoxCeleb1, VCTK\nand the FFSVC 2022 datasets, beating the baselines by up to 10%.", "published": "2023-06-01 17:49:58", "link": "http://arxiv.org/abs/2306.00952v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ALO-VC: Any-to-any Low-latency One-shot Voice Conversion", "abstract": "This paper presents ALO-VC, a non-parallel low-latency one-shot phonetic\nposteriorgrams (PPGs) based voice conversion method. ALO-VC enables any-to-any\nvoice conversion using only one utterance from the target speaker, with only\n47.5 ms future look-ahead. The proposed hybrid signal processing and machine\nlearning pipeline combines a pre-trained speaker encoder, a pitch predictor to\npredict the converted speech's prosody, and positional encoding to convey the\nphoneme's location information. We introduce two system versions: ALO-VC-R,\nwhich uses a pre-trained d-vector speaker encoder, and ALO-VC-E, which improves\nperformance using the ECAPA-TDNN speaker encoder. The experimental results\ndemonstrate both ALO-VC-R and ALO-VC-E can achieve comparable performance to\nnon-causal baseline systems on the VCTK dataset and two out-of-domain datasets.\nFurthermore, both proposed systems can be deployed on a single CPU core with 55\nms latency and 0.78 real-time factor. Our demo is available online.", "published": "2023-06-01 19:23:38", "link": "http://arxiv.org/abs/2306.01100v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Adaptation and Optimization of Automatic Speech Recognition (ASR) for\n  the Maritime Domain in the Field of VHF Communication", "abstract": "This paper introduces a multilingual automatic speech recognizer (ASR) for\nmaritime radio communi-cation that automatically converts received VHF radio\nsignals into text. The challenges of maritime radio communication are described\nat first, and the deep learning architecture of marFM consisting of audio\nprocessing techniques and machine learning algorithms is presented.\nSubsequently, maritime radio data of interest is analyzed and then used to\nevaluate the transcription performance of our ASR model for various maritime\nradio data.", "published": "2023-06-01 12:38:11", "link": "http://arxiv.org/abs/2306.00614v1", "categories": ["cs.SD", "cs.AI", "cs.HC", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Multi-dimensional Deep Structured State Space Approach to Speech\n  Enhancement Using Small-footprint Models", "abstract": "We propose a multi-dimensional structured state space (S4) approach to speech\nenhancement. To better capture the spectral dependencies across the frequency\naxis, we focus on modifying the multi-dimensional S4 layer with whitening\ntransformation to build new small-footprint models that also achieve good\nperformance. We explore several S4-based deep architectures in time (T) and\ntime-frequency (TF) domains. The 2-D S4 layer can be considered a particular\nconvolutional layer with an infinite receptive field although it utilizes fewer\nparameters than a conventional convolutional layer. Evaluated on the\nVoiceBank-DEMAND data set, when compared with the conventional U-net model\nbased on convolutional layers, the proposed TF-domain S4-based model is 78.6%\nsmaller in size, yet it still achieves competitive results with a PESQ score of\n3.15 with data augmentation. By increasing the model size, we can even reach a\nPESQ score of 3.18.", "published": "2023-06-01 04:19:57", "link": "http://arxiv.org/abs/2306.00331v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "cs.SY", "eess.SP", "eess.SY"], "primary_category": "eess.AS"}
