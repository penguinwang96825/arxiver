{"title": "Understanding the Use of Quantifiers in Mandarin", "abstract": "We introduce a corpus of short texts in Mandarin, in which quantified\nexpressions figure prominently. We illustrate the significance of the corpus by\nexamining the hypothesis (known as Huang's \"coolness\" hypothesis) that speakers\nof East Asian Languages tend to speak more briefly but less informatively than,\nfor example, speakers of West-European languages. The corpus results from an\nelicitation experiment in which participants were asked to describe abstract\nvisual scenes. We compare the resulting corpus, called MQTUNA, with an English\ncorpus that was collected using the same experimental paradigm. The comparison\nreveals that some, though not all, aspects of quantifier use support the\nabove-mentioned hypothesis. Implications of these findings for the generation\nof quantified noun phrases are discussed.", "published": "2022-09-24 10:43:07", "link": "http://arxiv.org/abs/2209.11977v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dead or Murdered? Predicting Responsibility Perception in Femicide News\n  Reports", "abstract": "Different linguistic expressions can conceptualize the same event from\ndifferent viewpoints by emphasizing certain participants over others. Here, we\ninvestigate a case where this has social consequences: how do linguistic\nexpressions of gender-based violence (GBV) influence who we perceive as\nresponsible? We build on previous psycholinguistic research in this area and\nconduct a large-scale perception survey of GBV descriptions automatically\nextracted from a corpus of Italian newspapers. We then train regression models\nthat predict the salience of GBV participants with respect to different\ndimensions of perceived responsibility. Our best model (fine-tuned BERT) shows\nsolid overall performance, with large differences between dimensions and\nparticipants: salient _focus_ is more predictable than salient _blame_, and\nperpetrators' salience is more predictable than victims' salience. Experiments\nwith ridge regression models using different representations show that features\nbased on linguistic theory similarly to word-based features. Overall, we show\nthat different linguistic choices do trigger different perceptions of\nresponsibility, and that such perceptions can be modelled automatically. This\nwork can be a core instrument to raise awareness of the consequences of\ndifferent perspectivizations in the general public and in news producers alike.", "published": "2022-09-24 15:14:03", "link": "http://arxiv.org/abs/2209.12030v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Moral Mimicry: Large Language Models Produce Moral Rationalizations\n  Tailored to Political Identity", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ngenerating fluent text, as well as tendencies to reproduce undesirable social\nbiases. This study investigates whether LLMs reproduce the moral biases\nassociated with political groups in the United States, an instance of a broader\ncapability herein termed moral mimicry. This hypothesis is explored in the\nGPT-3/3.5 and OPT families of Transformer-based LLMs. Using tools from Moral\nFoundations Theory, it is shown that these LLMs are indeed moral mimics. When\nprompted with a liberal or conservative political identity, the models generate\ntext reflecting corresponding moral biases. This study also explores the\nrelationship between moral mimicry and model size, and similarity between human\nand LLM moral word use.", "published": "2022-09-24 23:55:53", "link": "http://arxiv.org/abs/2209.12106v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Focused Study on Sequence Length for Dialogue Summarization", "abstract": "Output length is critical to dialogue summarization systems. The dialogue\nsummary length is determined by multiple factors, including dialogue\ncomplexity, summary objective, and personal preferences. In this work, we\napproach dialogue summary length from three perspectives. First, we analyze the\nlength differences between existing models' outputs and the corresponding human\nreferences and find that summarization models tend to produce more verbose\nsummaries due to their pretraining objectives. Second, we identify salient\nfeatures for summary length prediction by comparing different model settings.\nThird, we experiment with a length-aware summarizer and show notable\nimprovement on existing models if summary length can be well incorporated.\nAnalysis and experiments are conducted on popular DialogSum and SAMSum datasets\nto validate our findings.", "published": "2022-09-24 02:49:48", "link": "http://arxiv.org/abs/2209.11910v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "A Deep Investigation of RNN and Self-attention for the\n  Cyrillic-Traditional Mongolian Bidirectional Conversion", "abstract": "Cyrillic and Traditional Mongolian are the two main members of the Mongolian\nwriting system. The Cyrillic-Traditional Mongolian Bidirectional Conversion\n(CTMBC) task includes two conversion processes, including Cyrillic Mongolian to\nTraditional Mongolian (C2T) and Traditional Mongolian to Cyrillic Mongolian\nconversions (T2C). Previous researchers adopted the traditional joint sequence\nmodel, since the CTMBC task is a natural Sequence-to-Sequence (Seq2Seq)\nmodeling problem. Recent studies have shown that Recurrent Neural Network (RNN)\nand Self-attention (or Transformer) based encoder-decoder models have shown\nsignificant improvement in machine translation tasks between some major\nlanguages, such as Mandarin, English, French, etc. However, an open problem\nremains as to whether the CTMBC quality can be improved by utilizing the RNN\nand Transformer models. To answer this question, this paper investigates the\nutility of these two powerful techniques for CTMBC task combined with\nagglutinative characteristics of Mongolian language. We build the\nencoder-decoder based CTMBC model based on RNN and Transformer respectively and\ncompare the different network configurations deeply. The experimental results\nshow that both RNN and Transformer models outperform the traditional joint\nsequence model, where the Transformer achieves the best performance. Compared\nwith the joint sequence baseline, the word error rate (WER) of the Transformer\nfor C2T and T2C decreased by 5.72\\% and 5.06\\% respectively.", "published": "2022-09-24 08:55:22", "link": "http://arxiv.org/abs/2209.11963v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can Transformer Models Effectively Detect Software Aspects in\n  StackOverflow Discussion?", "abstract": "Dozens of new tools and technologies are being incorporated to help\ndevelopers, which is becoming a source of consternation as they struggle to\nchoose one over the others. For example, there are at least ten frameworks\navailable to developers for developing web applications, posing a conundrum in\nselecting the best one that meets their needs. As a result, developers are\ncontinuously searching for all of the benefits and drawbacks of each API,\nframework, tool, and so on. One of the typical approaches is to examine all of\nthe features through official documentation and discussion. This approach is\ntime-consuming, often makes it difficult to determine which aspects are the\nmost important to a particular developer and whether a particular aspect is\nimportant to the community at large. In this paper, we have used a benchmark\nAPI aspects dataset (Opiner) collected from StackOverflow posts and observed\nhow Transformer models (BERT, RoBERTa, DistilBERT, and XLNet) perform in\ndetecting software aspects in textual developer discussion with respect to the\nbaseline Support Vector Machine (SVM) model. Through extensive experimentation,\nwe have found that transformer models improve the performance of baseline SVM\nfor most of the aspects, i.e., `Performance', `Security', `Usability',\n`Documentation', `Bug', `Legal', `OnlySentiment', and `Others'. However, the\nmodels fail to apprehend some of the aspects (e.g., `Community' and\n`Potability') and their performance varies depending on the aspects. Also,\nlarger architectures like XLNet are ineffective in interpreting software\naspects compared to smaller architectures like DistilBERT.", "published": "2022-09-24 18:28:14", "link": "http://arxiv.org/abs/2209.12065v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Controllable Text Generation for Open-Domain Creativity and Fairness", "abstract": "Recent advances in large pre-trained language models have demonstrated strong\nresults in generating natural languages and significantly improved performances\nfor many natural language generation (NLG) applications such as machine\ntranslation and text summarization. However, when the generation tasks are more\nopen-ended and the content is under-specified, existing techniques struggle to\ngenerate long-term coherent and creative content. Moreover, the models exhibit\nand even amplify social biases that are learned from the training corpora. This\nhappens because the generation models are trained to capture the surface\npatterns (i.e. sequences of words), instead of capturing underlying semantics\nand discourse structures, as well as background knowledge including social\nnorms. In this paper, I introduce our recent works on controllable text\ngeneration to enhance the creativity and fairness of language generation\nmodels. We explore hierarchical generation and constrained decoding, with\napplications to creative language generation including story, poetry, and\nfigurative languages, and bias mitigation for generation models.", "published": "2022-09-24 22:40:01", "link": "http://arxiv.org/abs/2209.12099v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Chess With Language Models and Transformers", "abstract": "Representing a board game and its positions by text-based notation enables\nthe possibility of NLP applications. Language models, can help gain insight\ninto a variety of interesting problems such as unsupervised learning rules of a\ngame, detecting player behavior patterns, player attribution, and ultimately\nlearning the game to beat state of the art. In this study, we applied BERT\nmodels, first to the simple Nim game to analyze its performance in the presence\nof noise in a setup of a few-shot learning architecture. We analyzed the model\nperformance via three virtual players, namely Nim Guru, Random player, and\nQ-learner. In the second part, we applied the game learning language model to\nthe chess game, and a large set of grandmaster games with exhaustive\nencyclopedia openings. Finally, we have shown that model practically learns the\nrules of the chess game and can survive games against Stockfish at a category-A\nrating level.", "published": "2022-09-24 01:22:59", "link": "http://arxiv.org/abs/2209.11902v1", "categories": ["cs.AI", "cs.CL", "cs.GT", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Hybrid Multimodal Fusion for Humor Detection", "abstract": "In this paper, we present our solution to the MuSe-Humor sub-challenge of the\nMultimodal Emotional Challenge (MuSe) 2022. The goal of the MuSe-Humor\nsub-challenge is to detect humor and calculate AUC from audiovisual recordings\nof German football Bundesliga press conferences. It is annotated for humor\ndisplayed by the coaches. For this sub-challenge, we first build a discriminant\nmodel using the transformer module and BiLSTM module, and then propose a hybrid\nfusion strategy to use the prediction results of each modality to improve the\nperformance of the model. Our experiments demonstrate the effectiveness of our\nproposed model and hybrid fusion strategy on multimodal fusion, and the AUC of\nour proposed model on the test set is 0.8972.", "published": "2022-09-24 07:45:04", "link": "http://arxiv.org/abs/2209.11949v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MM", "68T10 (Primary) 68T50", "F.5.4; I.2.7; I.4.9"], "primary_category": "cs.LG"}
{"title": "TransPOS: Transformers for Consolidating Different POS Tagset Datasets", "abstract": "In hope of expanding training data, researchers often want to merge two or\nmore datasets that are created using different labeling schemes. This paper\nconsiders two datasets that label part-of-speech (POS) tags under different\ntagging schemes and leverage the supervised labels of one dataset to help\ngenerate labels for the other dataset. This paper further discusses the\ntheoretical difficulties of this approach and proposes a novel supervised\narchitecture employing Transformers to tackle the problem of consolidating two\ncompletely disjoint datasets. The results diverge from initial expectations and\ndiscourage exploration into the use of disjoint labels to consolidate datasets\nwith different labels.", "published": "2022-09-24 08:43:53", "link": "http://arxiv.org/abs/2209.11959v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speech Enhancement with Perceptually-motivated Optimization and Dual\n  Transformations", "abstract": "To address the monaural speech enhancement problem, numerous research studies\nhave been conducted to enhance speech via operations either in time-domain on\nthe inner-domain learned from the speech mixture or in time--frequency domain\non the fixed full-band short time Fourier transform (STFT) spectrograms. Very\nrecently, a few studies on sub-band based speech enhancement have been\nproposed. By enhancing speech via operations on sub-band spectrograms, those\nstudies demonstrated competitive performances on the benchmark dataset of\nDNS2020. Despite attractive, this new research direction has not been fully\nexplored and there is still room for improvement. As such, in this study, we\ndelve into the latest research direction and propose a sub-band based speech\nenhancement system with perceptually-motivated optimization and dual\ntransformations, called PT-FSE. Specially, our proposed PT-FSE model improves\nits backbone, a full-band and sub-band fusion model, by three efforts. First,\nwe design a frequency transformation module that aims to strengthen the global\nfrequency correlation. Then a temporal transformation is introduced to capture\nlong range temporal contexts. Lastly, a novel loss, with leverage of properties\nof human auditory perception, is proposed to facilitate the model to focus on\nlow frequency enhancement. To validate the effectiveness of our proposed model,\nextensive experiments are conducted on the DNS2020 dataset. Experimental\nresults show that our PT-FSE system achieves substantial improvements over its\nbackbone, but also outperforms the current state-of-the-art while being 27\\%\nsmaller than the SOTA. With average NB-PESQ of 3.57 on the benchmark dataset,\nour system offers the best speech enhancement results reported till date.", "published": "2022-09-24 02:33:40", "link": "http://arxiv.org/abs/2209.11905v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Joint Speech Activity and Overlap Detection with Multi-Exit Architecture", "abstract": "Overlapped speech detection (OSD) is critical for speech applications in\nscenario of multi-party conversion. Despite numerous research efforts and\nprogresses, comparing with speech activity detection (VAD), OSD remains an open\nchallenge and its overall performance is far from satisfactory. The majority of\nprior research typically formulates the OSD problem as a standard\nclassification problem, to identify speech with binary (OSD) or three-class\nlabel (joint VAD and OSD) at frame level. In contrast to the mainstream, this\nstudy investigates the joint VAD and OSD task from a new perspective. In\nparticular, we propose to extend traditional classification network with\nmulti-exit architecture. Such an architecture empowers our system with unique\ncapability to identify class using either low-level features from early exits\nor high-level features from last exit. In addition, two training schemes,\nknowledge distillation and dense connection, are adopted to further boost our\nsystem performance. Experimental results on benchmark datasets (AMI and\nDIHARD-III) validated the effectiveness and generality of our proposed system.\nOur ablations further reveal the complementary contribution of proposed\nschemes. With $F_1$ score of 0.792 on AMI and 0.625 on DIHARD-III, our proposed\nsystem outperforms several top performing models on these datasets, but also\nsurpasses the current state-of-the-art by large margins across both datasets.\nBesides the performance benefit, our proposed system offers another appealing\npotential for quality-complexity trade-offs, which is highly preferred for\nefficient OSD deployment.", "published": "2022-09-24 02:34:11", "link": "http://arxiv.org/abs/2209.11906v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NWPU-ASLP System for the VoicePrivacy 2022 Challenge", "abstract": "This paper presents the NWPU-ASLP speaker anonymization system for\nVoicePrivacy 2022 Challenge. Our submission does not involve additional\nAutomatic Speaker Verification (ASV) model or x-vector pool. Our system\nconsists of four modules, including feature extractor, acoustic model,\nanonymization module, and neural vocoder. First, the feature extractor extracts\nthe Phonetic Posteriorgram (PPG) and pitch from the input speech signal. Then,\nwe reserve a pseudo speaker ID from a speaker look-up table (LUT), which is\nsubsequently fed into a speaker encoder to generate the pseudo speaker\nembedding that is not corresponding to any real speaker. To ensure the pseudo\nspeaker is distinguishable, we further average the randomly selected speaker\nembedding and weighted concatenate it with the pseudo speaker embedding to\ngenerate the anonymized speaker embedding. Finally, the acoustic model outputs\nthe anonymized mel-spectrogram from the anonymized speaker embedding and a\nmodified version of HifiGAN transforms the mel-spectrogram into the anonymized\nspeech waveform. Experimental results demonstrate the effectiveness of our\nproposed anonymization system.", "published": "2022-09-24 09:36:43", "link": "http://arxiv.org/abs/2209.11969v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Spatial-aware Speaker Diarization for Multi-channel Multi-party Meeting", "abstract": "This paper describes a spatial-aware speaker diarization system for the\nmulti-channel multi-party meeting. The diarization system obtains direction\ninformation of speaker by microphone array. Speaker spatial embedding is\ngenerated by xvector and s-vector derived from superdirective beamforming (SDB)\nwhich makes the embedding more robust. Specifically, we propose a novel\nmulti-channel sequence-to-sequence neural network architecture named\ndiscriminative multi-stream neural network (DMSNet) which consists of attention\nsuperdirective beamforming (ASDB) block and Conformer encoder. The proposed\nASDB is a self-adapted channel-wise block that extracts the latent spatial\nfeatures of array audios by modeling interdependencies between channels. We\nexplore DMSNet to address overlapped speech problem on multi-channel audio and\nachieve 93.53% accuracy on evaluation set. By performing DMSNet based\noverlapped speech detection (OSD) module, the diarization error rate (DER) of\ncluster-based diarization system decrease significantly from 13.45% to 7.64%.", "published": "2022-09-24 13:04:52", "link": "http://arxiv.org/abs/2209.12002v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised active speaker detection in media content using cross-modal\n  information", "abstract": "We present a cross-modal unsupervised framework for active speaker detection\nin media content such as TV shows and movies. Machine learning advances have\nenabled impressive performance in identifying individuals from speech and\nfacial images. We leverage speaker identity information from speech and faces,\nand formulate active speaker detection as a speech-face assignment task such\nthat the active speaker's face and the underlying speech identify the same\nperson (character). We express the speech segments in terms of their associated\nspeaker identity distances, from all other speech segments, to capture a\nrelative identity structure for the video. Then we assign an active speaker's\nface to each speech segment from the concurrently appearing faces such that the\nobtained set of active speaker faces displays a similar relative identity\nstructure. Furthermore, we propose a simple and effective approach to address\nspeech segments where speakers are present off-screen. We evaluate the proposed\nsystem on three benchmark datasets -- Visual Person Clustering dataset,\nAVA-active speaker dataset, and Columbia dataset -- consisting of videos from\nentertainment and broadcast media, and show competitive performance to\nstate-of-the-art fully supervised methods.", "published": "2022-09-24 00:51:38", "link": "http://arxiv.org/abs/2209.11896v1", "categories": ["eess.IV", "cs.CV", "eess.AS"], "primary_category": "eess.IV"}
{"title": "Unsupervised domain adaptation for speech recognition with unsupervised\n  error correction", "abstract": "The transcription quality of automatic speech recognition (ASR) systems\ndegrades significantly when transcribing audios coming from unseen domains. We\npropose an unsupervised error correction method for unsupervised ASR domain\nadaption, aiming to recover transcription errors caused by domain mismatch.\nUnlike existing correction methods that rely on transcribed audios for\ntraining, our approach requires only unlabeled data of the target domains in\nwhich a pseudo-labeling technique is applied to generate correction training\nsamples. To reduce over-fitting to the pseudo data, we also propose an\nencoder-decoder correction model that can take into account additional\ninformation such as dialogue context and acoustic features. Experiment results\nshow that our method obtains a significant word error rate (WER) reduction over\nnon-adapted ASR systems. The correction model can also be applied on top of\nother adaptation approaches to bring an additional improvement of 10%\nrelatively.", "published": "2022-09-24 16:05:23", "link": "http://arxiv.org/abs/2209.12043v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Song Emotion Recognition: a Performance Comparison Between Audio\n  Features and Artificial Neural Networks", "abstract": "When songs are composed or performed, there is often an intent by the\nsinger/songwriter of expressing feelings or emotions through it. For humans,\nmatching the emotiveness in a musical composition or performance with the\nsubjective perception of an audience can be quite challenging. Fortunately, the\nmachine learning approach for this problem is simpler. Usually, it takes a\ndata-set, from which audio features are extracted to present this information\nto a data-driven model, that will, in turn, train to predict what is the\nprobability that a given song matches a target emotion. In this paper, we\nstudied the most common features and models used in recent publications to\ntackle this problem, revealing which ones are best suited for recognizing\nemotion in a cappella songs.", "published": "2022-09-24 16:13:25", "link": "http://arxiv.org/abs/2209.12045v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "I.2"], "primary_category": "cs.SD"}
