{"title": "A systematic review of Hate Speech automatic detection using Natural\n  Language Processing", "abstract": "With the multiplication of social media platforms, which offer anonymity,\neasy access and online community formation, and online debate, the issue of\nhate speech detection and tracking becomes a growing challenge to society,\nindividual, policy-makers and researchers. Despite efforts for leveraging\nautomatic techniques for automatic detection and monitoring, their performances\nare still far from satisfactory, which constantly calls for future research on\nthe issue. This paper provides a systematic review of literature in this field,\nwith a focus on natural language processing and deep learning technologies,\nhighlighting the terminology, processing pipeline, core methods employed, with\na focal point on deep learning architecture. From a methodological perspective,\nwe adopt PRISMA guideline of systematic review of the last 10 years literature\nfrom ACM Digital Library and Google Scholar. In the sequel, existing surveys,\nlimitations, and future research directions are extensively discussed.", "published": "2021-05-22 21:48:14", "link": "http://arxiv.org/abs/2106.00742v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COVID-19 Detection Using Recorded Coughs in the 2021 DiCOVA Challenge", "abstract": "COVID-19 has resulted in over 100 million infections and caused worldwide\nlock downs due to its high transmission rate and limited testing options.\nCurrent diagnostic tests can be expensive, limited in availability,\ntime-intensive and require risky in-person appointments. It has been\nestablished that symptomatic COVID-19 seriously impairs normal functioning of\nthe respiratory system, thus affecting the coughing acoustics. The 2021 DiCOVA\nChallenge @ INTERSPEECH was designed to find scientific and engineering\ninsights to the question by enabling participants to analyze an acoustic\ndataset gathered from COVID-19 positive and non-COVID-19 individuals. In this\nreport we describe our participation in the Challenge (Track 1). We achieved\n82.37% AUC ROC on the blind test outperforming the Challenge's baseline of\n69.85%.", "published": "2021-05-22 02:04:42", "link": "http://arxiv.org/abs/2105.10619v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Quanta in sound, the sound of quanta: a voice-informed quantum\n  theoretical perspective on sound", "abstract": "Humans have a privileged, embodied way to explore the world of sounds,\nthrough vocal imitation. The Quantum Vocal Theory of Sounds (QVTS) starts from\nthe assumption that any sound can be expressed and described as the evolution\nof a superposition of vocal states, i.e., phonation, turbulence, and\nsupraglottal myoelastic vibrations. The postulates of quantum mechanics, with\nthe notions of observable, measurement, and time evolution of state, provide a\nmodel that can be used for sound processing, in both directions of analysis and\nsynthesis. QVTS can give a quantum-theoretic explanation to some auditory\nstreaming phenomena, eventually leading to practical solutions of relevant\nsound-processing problems, or it can be creatively exploited to manipulate\nsuperpositions of sonic elements. Perhaps more importantly, QVTS may be a\nfertile ground to host a dialogue between physicists, computer scientists,\nmusicians, and sound designers, possibly giving us unheard manifestations of\nhuman creativity.", "published": "2021-05-22 18:06:47", "link": "http://arxiv.org/abs/2105.10781v2", "categories": ["cs.SD", "eess.AS", "quant-ph"], "primary_category": "cs.SD"}
