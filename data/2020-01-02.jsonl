{"title": "Chemical-induced Disease Relation Extraction with Dependency Information\n  and Prior Knowledge", "abstract": "Chemical-disease relation (CDR) extraction is significantly important to\nvarious areas of biomedical research and health care. Nowadays, many\nlarge-scale biomedical knowledge bases (KBs) containing triples about entity\npairs and their relations have been built. KBs are important resources for\nbiomedical relation extraction. However, previous research pays little\nattention to prior knowledge. In addition, the dependency tree contains\nimportant syntactic and semantic information, which helps to improve relation\nextraction. So how to effectively use it is also worth studying. In this paper,\nwe propose a novel convolutional attention network (CAN) for CDR extraction.\nFirstly, we extract the shortest dependency path (SDP) between chemical and\ndisease pairs in a sentence, which includes a sequence of words, dependency\ndirections, and dependency relation tags. Then the convolution operations are\nperformed on the SDP to produce deep semantic dependency features. After that,\nan attention mechanism is employed to learn the importance/weight of each\nsemantic dependency vector related to knowledge representations learned from\nKBs. Finally, in order to combine dependency information and prior knowledge,\nthe concatenation of weighted semantic dependency representations and knowledge\nrepresentations is fed to the softmax layer for classification. Experiments on\nthe BioCreative V CDR dataset show that our method achieves comparable\nperformance with the state-of-the-art systems, and both dependency information\nand prior knowledge play important roles in CDR extraction task.", "published": "2020-01-02 02:24:53", "link": "http://arxiv.org/abs/2001.00295v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Morphological Word Segmentation on Agglutinative Languages for Neural\n  Machine Translation", "abstract": "Neural machine translation (NMT) has achieved impressive performance on\nmachine translation task in recent years. However, in consideration of\nefficiency, a limited-size vocabulary that only contains the top-N highest\nfrequency words are employed for model training, which leads to many rare and\nunknown words. It is rather difficult when translating from the low-resource\nand morphologically-rich agglutinative languages, which have complex morphology\nand large vocabulary. In this paper, we propose a morphological word\nsegmentation method on the source-side for NMT that incorporates morphology\nknowledge to preserve the linguistic and semantic information in the word\nstructure while reducing the vocabulary size at training time. It can be\nutilized as a preprocessing tool to segment the words in agglutinative\nlanguages for other natural language processing (NLP) tasks. Experimental\nresults show that our morphologically motivated word segmentation method is\nbetter suitable for the NMT model, which achieves significant improvements on\nTurkish-English and Uyghur-Chinese machine translation tasks on account of\nreducing data sparseness and language complexity.", "published": "2020-01-02 10:05:02", "link": "http://arxiv.org/abs/2001.01589v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mining Disinformation and Fake News: Concepts, Methods, and Recent\n  Advancements", "abstract": "In recent years, disinformation including fake news, has became a global\nphenomenon due to its explosive growth, particularly on social media. The wide\nspread of disinformation and fake news can cause detrimental societal effects.\nDespite the recent progress in detecting disinformation and fake news, it is\nstill non-trivial due to its complexity, diversity, multi-modality, and costs\nof fact-checking or annotation. The goal of this chapter is to pave the way for\nappreciating the challenges and advancements via: (1) introducing the types of\ninformation disorder on social media and examine their differences and\nconnections; (2) describing important and emerging tasks to combat\ndisinformation for characterization, detection and attribution; and (3)\ndiscussing a weak supervision approach to detect disinformation with limited\nlabeled data. We then provide an overview of the chapters in this book that\nrepresent the recent advancements in three related parts: (1) user engagements\nin the dissemination of information disorder; (2) techniques on detecting and\nmitigating disinformation; and (3) trending issues such as ethics, blockchain,\nclickbaits, etc. We hope this book to be a convenient entry point for\nresearchers, practitioners, and students to understand the problems and\nchallenges, learn state-of-the-art solutions for their specific needs, and\nquickly identify new research problems in their domains.", "published": "2020-01-02 21:01:02", "link": "http://arxiv.org/abs/2001.00623v1", "categories": ["cs.SI", "cs.CL", "H.2.8", "H.2.8"], "primary_category": "cs.SI"}
{"title": "Why Moli\u00e8re most likely did write his plays", "abstract": "As for Shakespeare, a hard-fought debate has emerged about Moli\\`ere, a\nsupposedly uneducated actor who, according to some, could not have written the\nmasterpieces attributed to him. In the past decades, the century-old thesis\naccording to which Pierre Corneille would be their actual author has become\npopular, mostly because of new works in computational linguistics. These\nresults are reassessed here through state-of-the-art attribution methods. We\nstudy a corpus of comedies in verse by major authors of Moli\\`ere and\nCorneille's time. Analysis of lexicon, rhymes, word forms, affixes,\nmorphosyntactic sequences, and function words do not give any clue that another\nauthor among the major playwrights of the time would have written the plays\nsigned under the name Moli\\`ere.", "published": "2020-01-02 15:23:11", "link": "http://arxiv.org/abs/2001.01595v1", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Phase-based Information for Voice Pathology Detection", "abstract": "In most current approaches of speech processing, information is extracted\nfrom the magnitude spectrum. However recent perceptual studies have underlined\nthe importance of the phase component. The goal of this paper is to investigate\nthe potential of using phase-based features for automatically detecting voice\ndisorders. It is shown that group delay functions are appropriate for\ncharacterizing irregularities in the phonation. Besides the respect of the\nmixed-phase model of speech is discussed. The proposed phase-based features are\nevaluated and compared to other parameters derived from the magnitude spectrum.\nBoth streams are shown to be interestingly complementary. Furthermore\nphase-based features turn out to convey a great amount of relevant information,\nleading to high discrimination performance.", "published": "2020-01-02 09:51:51", "link": "http://arxiv.org/abs/2001.00372v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Comparative Evaluation of Pitch Modification Techniques", "abstract": "This paper addresses the problem of pitch modification, as an important\nmodule for an efficient voice transformation system. The Deterministic plus\nStochastic Model of the residual signal we proposed in a previous work is\ncompared to TDPSOLA, HNM and STRAIGHT. The four methods are compared through an\nimportant subjective test. The influence of the speaker gender and of the pitch\nmodification ratio is analyzed. Despite its higher compression level, the DSM\ntechnique is shown to give similar or better results than other methods,\nespecially for male speakers and important ratios of modification. The DSM\nturns out to be only outperformed by STRAIGHT for female voices.", "published": "2020-01-02 09:25:30", "link": "http://arxiv.org/abs/2001.00579v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Eigenresiduals for improved Parametric Speech Synthesis", "abstract": "Statistical parametric speech synthesizers have recently shown their ability\nto produce natural-sounding and flexible voices. Unfortunately the delivered\nquality suffers from a typical buzziness due to the fact that speech is\nvocoded. This paper proposes a new excitation model in order to reduce this\nundesirable effect. This model is based on the decomposition of\npitch-synchronous residual frames on an orthonormal basis obtained by Principal\nComponent Analysis. This basis contains a limited number of eigenresiduals and\nis computed on a relatively small speech database. A stream of PCA-based\ncoefficients is added to our HMM-based synthesizer and allows to generate the\nvoiced excitation during the synthesis. An improvement compared to the\ntraditional excitation is reported while the synthesis engine footprint remains\nunder about 1Mb.", "published": "2020-01-02 09:39:07", "link": "http://arxiv.org/abs/2001.00581v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Excitation-based Voice Quality Analysis and Modification", "abstract": "This paper investigates the differences occuring in the excitation for\ndifferent voice qualities. Its goal is two-fold. First a large corpus\ncontaining three voice qualities (modal, soft and loud) uttered by the same\nspeaker is analyzed and significant differences in characteristics extracted\nfrom the excitation are observed. Secondly rules of modification derived from\nthe analysis are used to build a voice quality transformation system applied as\na post-process to HMM-based speech synthesis. The system is shown to\neffectively achieve the transformations while maintaining the delivered\nquality.", "published": "2020-01-02 09:44:52", "link": "http://arxiv.org/abs/2001.00582v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On the Mutual Information between Source and Filter Contributions for\n  Voice Pathology Detection", "abstract": "This paper addresses the problem of automatic detection of voice pathologies\ndirectly from the speech signal. For this, we investigate the use of the\nglottal source estimation as a means to detect voice disorders. Three sets of\nfeatures are proposed, depending on whether they are related to the speech or\nthe glottal signal, or to prosody. The relevancy of these features is assessed\nthrough mutual information-based measures. This allows an intuitive\ninterpretation in terms of discrimation power and redundancy between the\nfeatures, independently of any subsequent classifier. It is discussed which\ncharacteristics are interestingly informative or complementary for detecting\nvoice pathologies.", "published": "2020-01-02 10:04:37", "link": "http://arxiv.org/abs/2001.00583v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speaker-aware speech-transformer", "abstract": "Recently, end-to-end (E2E) models become a competitive alternative to the\nconventional hybrid automatic speech recognition (ASR) systems. However, they\nstill suffer from speaker mismatch in training and testing condition. In this\npaper, we use Speech-Transformer (ST) as the study platform to investigate\nspeaker aware training of E2E models. We propose a model called Speaker-Aware\nSpeech-Transformer (SAST), which is a standard ST equipped with a speaker\nattention module (SAM). The SAM has a static speaker knowledge block (SKB) that\nis made of i-vectors. At each time step, the encoder output attends to the\ni-vectors in the block, and generates a weighted combined speaker embedding\nvector, which helps the model to normalize the speaker variations. The SAST\nmodel trained in this way becomes independent of specific training speakers and\nthus generalizes better to unseen testing speakers. We investigate different\nfactors of SAM. Experimental results on the AISHELL-1 task show that SAST\nachieves a relative 6.5% CER reduction (CERR) over the speaker-independent (SI)\nbaseline. Moreover, we demonstrate that SAST still works quite well even if the\ni-vectors in SKB all come from a different data source other than the acoustic\ntraining set.", "published": "2020-01-02 15:04:08", "link": "http://arxiv.org/abs/2001.01557v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Deep Representation Learning in Speech Processing: Challenges, Recent\n  Advances, and Future Trends", "abstract": "Research on speech processing has traditionally considered the task of\ndesigning hand-engineered acoustic features (feature engineering) as a separate\ndistinct problem from the task of designing efficient machine learning (ML)\nmodels to make prediction and classification decisions. There are two main\ndrawbacks to this approach: firstly, the feature engineering being manual is\ncumbersome and requires human knowledge; and secondly, the designed features\nmight not be best for the objective at hand. This has motivated the adoption of\na recent trend in speech community towards utilisation of representation\nlearning techniques, which can learn an intermediate representation of the\ninput signal automatically that better suits the task at hand and hence lead to\nimproved performance. The significance of representation learning has increased\nwith advances in deep learning (DL), where the representations are more useful\nand less dependent on human knowledge, making it very conducive for tasks like\nclassification, prediction, etc. The main contribution of this paper is to\npresent an up-to-date and comprehensive survey on different techniques of\nspeech representation learning by bringing together the scattered research\nacross three distinct research areas including Automatic Speech Recognition\n(ASR), Speaker Recognition (SR), and Speaker Emotion Recognition (SER). Recent\nreviews in speech have been conducted for ASR, SR, and SER, however, none of\nthese has focused on the representation learning from speech -- a gap that our\nsurvey aims to bridge.", "published": "2020-01-02 10:12:23", "link": "http://arxiv.org/abs/2001.00378v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Temporal-Spatial Neural Filter: Direction Informed End-to-End\n  Multi-channel Target Speech Separation", "abstract": "Target speech separation refers to extracting the target speaker's speech\nfrom mixed signals. Despite the recent advances in deep learning based\nclose-talk speech separation, the applications to real-world are still an open\nissue. Two main challenges are the complex acoustic environment and the\nreal-time processing requirement. To address these challenges, we propose a\ntemporal-spatial neural filter, which directly estimates the target speech\nwaveform from multi-speaker mixture in reverberant environments, assisted with\ndirectional information of the speaker(s). Firstly, against variations brought\nby complex environment, the key idea is to increase the acoustic representation\ncompleteness through the jointly modeling of temporal, spectral and spatial\ndiscriminability between the target and interference source. Specifically,\ntemporal, spectral, spatial along with the designed directional features are\nintegrated to create a joint acoustic representation. Secondly, to reduce the\nlatency, we design a fully-convolutional autoencoder framework, which is purely\nend-to-end and single-pass. All the feature computation is implemented by the\nnetwork layers and operations to speed up the separation procedure. Evaluation\nis conducted on simulated reverberant dataset WSJ0-2mix and WSJ0-3mix under\nspeaker-independent scenario. Experimental results demonstrate that the\nproposed method outperforms state-of-the-art deep learning based multi-channel\napproaches with fewer parameters and faster processing speed. Furthermore, the\nproposed temporal-spatial neural filter can handle mixtures with varying and\nunknown number of speakers and exhibits persistent performance even when\nexisting a direction estimation error. Codes and models will be released soon.", "published": "2020-01-02 11:12:50", "link": "http://arxiv.org/abs/2001.00391v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DAWSON: A Domain Adaptive Few Shot Generation Framework", "abstract": "Training a Generative Adversarial Networks (GAN) for a new domain from\nscratch requires an enormous amount of training data and days of training time.\nTo this end, we propose DAWSON, a Domain Adaptive FewShot Generation\nFrameworkFor GANs based on meta-learning. A major challenge of applying\nmeta-learning GANs is to obtain gradients for the generator from evaluating it\non development sets due to the likelihood-free nature of GANs. To address this\nchallenge, we propose an alternative GAN training procedure that naturally\ncombines the two-step training procedure of GANs and the two-step training\nprocedure of meta-learning algorithms. DAWSON is a plug-and-play framework that\nsupports a broad family of meta-learning algorithms and various GANs with\narchitectural-variants. Based on DAWSON, We also propose MUSIC MATINEE, which\nis the first few-shot music generation model. Our experiments show that MUSIC\nMATINEE could quickly adapt to new domains with only tens of songs from the\ntarget domains. We also show that DAWSON can learn to generate new digits with\nonly four samples in the MNIST dataset. We release source codes implementation\nof DAWSON in both PyTorch and Tensorflow, generated music samples on two genres\nand the lightning video.", "published": "2020-01-02 00:59:10", "link": "http://arxiv.org/abs/2001.00576v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Attention based on-device streaming speech recognition with large speech\n  corpus", "abstract": "In this paper, we present a new on-device automatic speech recognition (ASR)\nsystem based on monotonic chunk-wise attention (MoChA) models trained with\nlarge (> 10K hours) corpus. We attained around 90% of a word recognition rate\nfor general domain mainly by using joint training of connectionist temporal\nclassifier (CTC) and cross entropy (CE) losses, minimum word error rate (MWER)\ntraining, layer-wise pre-training and data augmentation methods. In addition,\nwe compressed our models by more than 3.4 times smaller using an iterative\nhyper low-rank approximation (LRA) method while minimizing the degradation in\nrecognition accuracy. The memory footprint was further reduced with 8-bit\nquantization to bring down the final model size to lower than 39 MB. For\non-demand adaptation, we fused the MoChA models with statistical n-gram models,\nand we could achieve a relatively 36% improvement on average in word error rate\n(WER) for target domains including the general domain.", "published": "2020-01-02 04:24:44", "link": "http://arxiv.org/abs/2001.00577v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Assessment of Audio Features for Automatic Cough Detection", "abstract": "This paper addresses the issue of cough detection using only audio\nrecordings, with the ultimate goal of quantifying and qualifying the degree of\npathology for patients suffering from respiratory diseases, notably\nmucoviscidosis. A large set of audio features describing various aspects of the\naudio signal is proposed. These features are assessed in two steps. First,\ntheir intrisic potential and redundancy are evaluated using mutual\ninformation-based measures. Secondly, their efficiency is confirmed relying on\nthree classifiers: Artificial Neural Network, Gaussian Mixture Model and\nSupport Vector Machine. The influence of both the feature dimension and the\nclassifier complexity are also investigated.", "published": "2020-01-02 09:30:23", "link": "http://arxiv.org/abs/2001.00580v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
