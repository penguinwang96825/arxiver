{"title": "Transforming Unstructured Text into Data with Context Rule Assisted\n  Machine Learning (CRAML)", "abstract": "We describe a method and new no-code software tools enabling domain experts\nto build custom structured, labeled datasets from the unstructured text of\ndocuments and build niche machine learning text classification models traceable\nto expert-written rules. The Context Rule Assisted Machine Learning (CRAML)\nmethod allows accurate and reproducible labeling of massive volumes of\nunstructured text. CRAML enables domain experts to access uncommon constructs\nburied within a document corpus, and avoids limitations of current\ncomputational approaches that often lack context, transparency, and\ninterpetability. In this research methods paper, we present three use cases for\nCRAML: we analyze recent management literature that draws from text data,\ndescribe and release new machine learning models from an analysis of\nproprietary job advertisement text, and present findings of social and economic\ninterest from a public corpus of franchise documents. CRAML produces\ndocument-level coded tabular datasets that can be used for quantitative\nacademic research, and allows qualitative researchers to scale niche\nclassification schemes over massive text data. CRAML is a low-resource,\nflexible, and scalable methodology for building training data for supervised\nML. We make available as open-source resources: the software, job advertisement\ntext classifiers, a novel corpus of franchise documents, and a fully replicable\nstart-to-finish trained example in the context of no poach clauses.", "published": "2023-01-20 13:12:35", "link": "http://arxiv.org/abs/2301.08549v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Augmentation for Modeling Human Personality: The Dexter Machine", "abstract": "Modeling human personality is important for several AI challenges, from the\nengineering of artificial psychotherapists to the design of persona bots.\nHowever, the field of computational personality analysis heavily relies on\nlabeled data, which may be expensive, difficult or impossible to get. This\nproblem is amplified when dealing with rare personality types or disorders\n(e.g., the anti-social psychopathic personality disorder). In this context, we\ndeveloped a text-based data augmentation approach for human personality\n(PEDANT). PEDANT doesn't rely on the common type of labeled data but on the\ngenerative pre-trained model (GPT) combined with domain expertise. Testing the\nmethodology on three different datasets, provides results that support the\nquality of the generated data.", "published": "2023-01-20 14:36:22", "link": "http://arxiv.org/abs/2301.08606v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Peanuts Fall in Love with Distributional Semantics?", "abstract": "Context changes expectations about upcoming words - following a story\ninvolving an anthropomorphic peanut, comprehenders expect the sentence the\npeanut was in love more than the peanut was salted, as indexed by N400\namplitude (Nieuwland & van Berkum, 2006). This updating of expectations has\nbeen explained using Situation Models - mental representations of a described\nevent. However, recent work showing that N400 amplitude is predictable from\ndistributional information alone raises the question whether situation models\nare necessary for these contextual effects. We model the results of Nieuwland\nand van Berkum (2006) using six computational language models and three sets of\nword vectors, none of which have explicit situation models or semantic\ngrounding. We find that a subset of these can fully model the effect found by\nNieuwland and van Berkum (2006). Thus, at least some processing effects\nnormally explained through situation models may not in fact require explicit\nsituation models.", "published": "2023-01-20 18:50:38", "link": "http://arxiv.org/abs/2301.08731v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine", "abstract": "This report provides a preliminary evaluation of ChatGPT for machine\ntranslation, including translation prompt, multilingual translation, and\ntranslation robustness. We adopt the prompts advised by ChatGPT to trigger its\ntranslation ability and find that the candidate prompts generally work well\nwith minor performance differences. By evaluating on a number of benchmark test\nsets, we find that ChatGPT performs competitively with commercial translation\nproducts (e.g., Google Translate) on high-resource European languages but lags\nbehind significantly on low-resource or distant languages. As for the\ntranslation robustness, ChatGPT does not perform as well as the commercial\nsystems on biomedical abstracts or Reddit comments but exhibits good results on\nspoken language. Further, we explore an interesting strategy named\n$\\mathbf{pivot~prompting}$ for distant languages, which asks ChatGPT to\ntranslate the source sentence into a high-resource pivot language before into\nthe target language, improving the translation performance noticeably. With the\nlaunch of the GPT-4 engine, the translation performance of ChatGPT is\nsignificantly boosted, becoming comparable to commercial translation products,\neven for distant languages. Human analysis on Google Translate and ChatGPT\nsuggests that ChatGPT with GPT-3.5 tends to generate more hallucinations and\nmis-translation errors while that with GPT-4 makes the least errors. In other\nwords, ChatGPT has already become a good translator. Please refer to our Github\nproject for more details:\nhttps://github.com/wxjiao/Is-ChatGPT-A-Good-Translator", "published": "2023-01-20 08:51:36", "link": "http://arxiv.org/abs/2301.08745v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Document Summarization with Text Segmentation", "abstract": "In this paper, we exploit the innate document segment structure for improving\nthe extractive summarization task. We build two text segmentation models and\nfind the most optimal strategy to introduce their output predictions in an\nextractive summarization model. Experimental results on a corpus of scientific\narticles show that extractive summarization benefits from using a highly\naccurate segmentation method. In particular, most of the improvement is in\ndocuments where the most relevant information is not at the beginning thus, we\nconclude that segmentation helps in reducing the lead bias problem.", "published": "2023-01-20 22:24:22", "link": "http://arxiv.org/abs/2301.08817v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Review of the Trends and Challenges in Adopting Natural Language\n  Processing Methods for Education Feedback Analysis", "abstract": "Artificial Intelligence (AI) is a fast-growing area of study that stretching\nits presence to many business and research domains. Machine learning, deep\nlearning, and natural language processing (NLP) are subsets of AI to tackle\ndifferent areas of data processing and modelling. This review article presents\nan overview of AI impact on education outlining with current opportunities. In\nthe education domain, student feedback data is crucial to uncover the merits\nand demerits of existing services provided to students. AI can assist in\nidentifying the areas of improvement in educational infrastructure, learning\nmanagement systems, teaching practices and study environment. NLP techniques\nplay a vital role in analyzing student feedback in textual format. This\nresearch focuses on existing NLP methodologies and applications that could be\nadapted to educational domain applications like sentiment annotations, entity\nannotations, text summarization, and topic modelling. Trends and challenges in\nadopting NLP in education were reviewed and explored. Contextbased challenges\nin NLP like sarcasm, domain-specific language, ambiguity, and aspect-based\nsentiment analysis are explained with existing methodologies to overcome them.\nResearch community approaches to extract the semantic meaning of emoticons and\nspecial characters in feedback which conveys user opinion and challenges in\nadopting NLP in education are explored.", "published": "2023-01-20 23:38:58", "link": "http://arxiv.org/abs/2301.08826v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Translation for Accessible Multi-Language Text Analysis", "abstract": "English is the international standard of social research, but scholars are\nincreasingly conscious of their responsibility to meet the need for scholarly\ninsight into communication processes globally. This tension is as true in\ncomputational methods as any other area, with revolutionary advances in the\ntools for English language texts leaving most other languages far behind. In\nthis paper, we aim to leverage those very advances to demonstrate that\nmulti-language analysis is currently accessible to all computational scholars.\nWe show that English-trained measures computed after translation to English\nhave adequate-to-excellent accuracy compared to source-language measures\ncomputed on original texts. We show this for three major analytics -- sentiment\nanalysis, topic analysis, and word embeddings -- over 16 languages, including\nSpanish, Chinese, Hindi, and Arabic. We validate this claim by comparing\npredictions on original language tweets and their backtranslations: double\ntranslations from their source language to English and back to the source\nlanguage. Overall, our results suggest that Google Translate, a simple and\nwidely accessible tool, is effective in preserving semantic content across\nlanguages and methods. Modern machine translation can thus help computational\nscholars make more inclusive and general claims about human communication.", "published": "2023-01-20 04:11:38", "link": "http://arxiv.org/abs/2301.08416v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Language Agnostic Data-Driven Inverse Text Normalization", "abstract": "With the emergence of automatic speech recognition (ASR) models, converting\nthe spoken form text (from ASR) to the written form is in urgent need. This\ninverse text normalization (ITN) problem attracts the attention of researchers\nfrom various fields. Recently, several works show that data-driven ITN methods\ncan output high-quality written form text. Due to the scarcity of labeled\nspoken-written datasets, the studies on non-English data-driven ITN are quite\nlimited. In this work, we propose a language-agnostic data-driven ITN framework\nto fill this gap. Specifically, we leverage the data augmentation in\nconjunction with neural machine translated data for low resource languages.\nMoreover, we design an evaluation method for language agnostic ITN model when\nonly English data is available. Our empirical evaluation shows this language\nagnostic modeling approach is effective for low resource languages while\npreserving the performance for high resource languages.", "published": "2023-01-20 10:33:03", "link": "http://arxiv.org/abs/2301.08506v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Matching Exemplar as Next Sentence Prediction (MeNSP): Zero-shot Prompt\n  Learning for Automatic Scoring in Science Education", "abstract": "Developing models to automatically score students' written responses to\nscience problems is critical for science education. However, collecting and\nlabeling sufficient student responses for training models is time and\ncost-consuming. Recent studies suggest that pre-trained language models can be\nadapted to downstream tasks without fine-tuning with prompts. However, no\nresearch has employed such a prompt approach in science education. As student\nresponses are presented with natural language, aligning the scoring procedure\nas the next sentence prediction task using prompts can skip the costly\nfine-tuning stage. In this study, we developed a zero-shot approach to\nautomatically score student responses via Matching Exemplars as Next Sentence\nPrediction (MeNSP). This approach employs no training samples. We first apply\nMeNSP in scoring three assessment tasks of scientific argumentation and found\nmachine-human scoring agreements, Cohen's Kappa ranges from 0.30 to 0.57, and\nF1 score ranges from 0.54 to 0.81. To improve the performance, we extend our\nresearch to the few-shots setting, either randomly selecting labeled student\nresponses or manually constructing responses to fine-tune the models. We find\nthat one task's performance is improved with more samples, Cohen's Kappa from\n0.30 to 0.38, and F1 score from 0.54 to 0.59; for the two others, scoring\nperformance is not improved. We also find that randomly selected few-shots\nperform better than the human expert-crafted approach. This study suggests that\nMeNSP can yield referable automatic scoring for student responses while\nsignificantly reducing the cost of model training. This method can benefit\nlow-stakes classroom assessment practices in science education. Future research\nshould further explore the applicability of the MeNSP in different types of\nassessment tasks in science education and improve the model performance.", "published": "2023-01-20 19:13:09", "link": "http://arxiv.org/abs/2301.08771v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Visual Semantic Relatedness Dataset for Image Captioning", "abstract": "Modern image captioning system relies heavily on extracting knowledge from\nimages to capture the concept of a static story. In this paper, we propose a\ntextual visual context dataset for captioning, in which the publicly available\ndataset COCO Captions (Lin et al., 2014) has been extended with information\nabout the scene (such as objects in the image). Since this information has a\ntextual form, it can be used to leverage any NLP task, such as text similarity\nor semantic relation methods, into captioning systems, either as an end-to-end\ntraining strategy or a post-processing based approach.", "published": "2023-01-20 20:04:35", "link": "http://arxiv.org/abs/2301.08784v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Which Features are Learned by CodeBert: An Empirical Study of the\n  BERT-based Source Code Representation Learning", "abstract": "The Bidirectional Encoder Representations from Transformers (BERT) were\nproposed in the natural language process (NLP) and shows promising results.\nRecently researchers applied the BERT to source-code representation learning\nand reported some good news on several downstream tasks. However, in this\npaper, we illustrated that current methods cannot effectively understand the\nlogic of source codes. The representation of source code heavily relies on the\nprogrammer-defined variable and function names. We design and implement a set\nof experiments to demonstrate our conjecture and provide some insights for\nfuture works.", "published": "2023-01-20 05:39:26", "link": "http://arxiv.org/abs/2301.08427v2", "categories": ["cs.CL", "cs.CR", "cs.LG", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Visual Writing Prompts: Character-Grounded Story Generation with Curated\n  Image Sequences", "abstract": "Current work on image-based story generation suffers from the fact that the\nexisting image sequence collections do not have coherent plots behind them. We\nimprove visual story generation by producing a new image-grounded dataset,\nVisual Writing Prompts (VWP). VWP contains almost 2K selected sequences of\nmovie shots, each including 5-10 images. The image sequences are aligned with a\ntotal of 12K stories which were collected via crowdsourcing given the image\nsequences and a set of grounded characters from the corresponding image\nsequence. Our new image sequence collection and filtering process has allowed\nus to obtain stories that are more coherent and have more narrativity compared\nto previous work. We also propose a character-based story generation model\ndriven by coherence as a strong baseline. Evaluations show that our generated\nstories are more coherent, visually grounded, and have more narrativity than\nstories generated with the current state-of-the-art model.", "published": "2023-01-20 13:38:24", "link": "http://arxiv.org/abs/2301.08571v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Phoneme-Level BERT for Enhanced Prosody of Text-to-Speech with Grapheme\n  Predictions", "abstract": "Large-scale pre-trained language models have been shown to be helpful in\nimproving the naturalness of text-to-speech (TTS) models by enabling them to\nproduce more naturalistic prosodic patterns. However, these models are usually\nword-level or sup-phoneme-level and jointly trained with phonemes, making them\ninefficient for the downstream TTS task where only phonemes are needed. In this\nwork, we propose a phoneme-level BERT (PL-BERT) with a pretext task of\npredicting the corresponding graphemes along with the regular masked phoneme\npredictions. Subjective evaluations show that our phoneme-level BERT encoder\nhas significantly improved the mean opinion scores (MOS) of rated naturalness\nof synthesized speech compared with the state-of-the-art (SOTA) StyleTTS\nbaseline on out-of-distribution (OOD) texts.", "published": "2023-01-20 21:36:16", "link": "http://arxiv.org/abs/2301.08810v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Same Words, Different Meanings: Semantic Polarization in Broadcast Media\n  Language Forecasts Polarization on Social Media Discourse", "abstract": "With the growth of online news over the past decade, empirical studies on\npolitical discourse and news consumption have focused on the phenomenon of\nfilter bubbles and echo chambers. Yet recently, scholars have revealed limited\nevidence around the impact of such phenomenon, leading some to argue that\npartisan segregation across news audiences cannot be fully explained by online\nnews consumption alone and that the role of traditional legacy media may be as\nsalient in polarizing public discourse around current events. In this work, we\nexpand the scope of analysis to include both online and more traditional media\nby investigating the relationship between broadcast news media language and\nsocial media discourse. By analyzing a decade's worth of closed captions (2\nmillion speaker turns) from CNN and Fox News along with topically corresponding\ndiscourse from Twitter, we provide a novel framework for measuring semantic\npolarization between America's two major broadcast networks to demonstrate how\nsemantic polarization between these outlets has evolved (Study 1), peaked\n(Study 2) and influenced partisan discussions on Twitter (Study 3) across the\nlast decade. Our results demonstrate a sharp increase in polarization in how\ntopically important keywords are discussed between the two channels, especially\nafter 2016, with overall highest peaks occurring in 2020. The two stations\ndiscuss identical topics in drastically distinct contexts in 2020, to the\nextent that there is barely any linguistic overlap in how identical keywords\nare contextually discussed. Further, we demonstrate at scale, how such partisan\ndivision in broadcast media language significantly shapes semantic polarity\ntrends on Twitter (and vice-versa), empirically linking for the first time, how\nonline discussions are influenced by televised media.", "published": "2023-01-20 23:59:26", "link": "http://arxiv.org/abs/2301.08832v2", "categories": ["cs.CL", "cs.HC", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Contextualizing Emerging Trends in Financial News Articles", "abstract": "Identifying and exploring emerging trends in the news is becoming more\nessential than ever with many changes occurring worldwide due to the global\nhealth crises. However, most of the recent research has focused mainly on\ndetecting trends in social media, thus, benefiting from social features (e.g.\nlikes and retweets on Twitter) which helped the task as they can be used to\nmeasure the engagement and diffusion rate of content. Yet, formal text data,\nunlike short social media posts, comes with a longer, less restricted writing\nformat, and thus, more challenging. In this paper, we focus our study on\nemerging trends detection in financial news articles about Microsoft, collected\nbefore and during the start of the COVID-19 pandemic (July 2019 to July 2020).\nWe make the dataset accessible and propose a strong baseline (Contextual\nLeap2Trend) for exploring the dynamics of similarities between pairs of\nkeywords based on topic modelling and term frequency. Finally, we evaluate\nagainst a gold standard (Google Trends) and present noteworthy real-world\nscenarios regarding the influence of the pandemic on Microsoft.", "published": "2023-01-20 12:56:52", "link": "http://arxiv.org/abs/2301.11318v1", "categories": ["cs.CL", "cs.SI", "q-fin.GN"], "primary_category": "cs.CL"}
{"title": "Integrated Planning of Multi-energy Grids: Concepts and Challenges", "abstract": "In order to meet ever-stricter climate targets and achieve the eventual\ndecarbonization of the energy supply of German industrial metropolises, the\nfocus is on gradually phasing out nuclear power, then coal and gas combined\nwith the increased use of renewable energy sources and employing hydrogen as a\nclean energy carrier. While complete electrification of the energy supply of\nhouseholds and the transportation sector may be the ultimate goal, a\ntransitional phase is necessary as such massive as well as rapid expansion of\nthe electrical distribution grid is infeasible. Additionally, German industries\nhave expressed their plans to use hydrogen as their primary strategy in meeting\ncarbon targets. This poses challenges to the existing electrical, gas, and\nheating distribution grids. It becomes necessary to integrate the planning and\ndeveloping procedures for these grids to maximize efficiencies and guarantee\nsecurity of supply during the transition. The aim of this paper is thus to\nhighlight those challenges and present novel concepts for the integrated\nplanning of the three grids as one multi-energy grid.", "published": "2023-01-20 07:36:18", "link": "http://arxiv.org/abs/2301.08454v1", "categories": ["eess.SY", "cs.SY", "eess.AS"], "primary_category": "eess.SY"}
{"title": "Adjoint-Based Identification of Sound Sources for Sound Reinforcement\n  and Source Localization", "abstract": "The identification of sound sources is a common problem in acoustics.\nDifferent parameters are sought, among these are signal and position of the\nsources. We present an adjoint-based approach for sound source identification,\nwhich employs computational aeroacoustic techniques. Two different applications\nare presented as a proof-of-concept: optimization of a sound reinforcement\nsetup and the localization of (moving) sound sources.", "published": "2023-01-20 15:01:46", "link": "http://arxiv.org/abs/2301.08620v1", "categories": ["cs.SD", "eess.AS", "physics.flu-dyn", "76Q05"], "primary_category": "cs.SD"}
{"title": "Novel-View Acoustic Synthesis", "abstract": "We introduce the novel-view acoustic synthesis (NVAS) task: given the sight\nand sound observed at a source viewpoint, can we synthesize the sound of that\nscene from an unseen target viewpoint? We propose a neural rendering approach:\nVisually-Guided Acoustic Synthesis (ViGAS) network that learns to synthesize\nthe sound of an arbitrary point in space by analyzing the input audio-visual\ncues. To benchmark this task, we collect two first-of-their-kind large-scale\nmulti-view audio-visual datasets, one synthetic and one real. We show that our\nmodel successfully reasons about the spatial cues and synthesizes faithful\naudio on both datasets. To our knowledge, this work represents the very first\nformulation, dataset, and approach to solve the novel-view acoustic synthesis\ntask, which has exciting potential applications ranging from AR/VR to art and\ndesign. Unlocked by this work, we believe that the future of novel-view\nsynthesis is in multi-modal learning from videos.", "published": "2023-01-20 18:49:58", "link": "http://arxiv.org/abs/2301.08730v3", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
