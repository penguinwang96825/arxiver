{"title": "Large Language Models for Data Annotation and Synthesis: A Survey", "abstract": "Data annotation and synthesis generally refers to the labeling or generating\nof raw data with relevant information, which could be used for improving the\nefficacy of machine learning models. The process, however, is labor-intensive\nand costly. The emergence of advanced Large Language Models (LLMs), exemplified\nby GPT-4, presents an unprecedented opportunity to automate the complicated\nprocess of data annotation and synthesis. While existing surveys have\nextensively covered LLM architecture, training, and general applications, we\nuniquely focus on their specific utility for data annotation. This survey\ncontributes to three core aspects: LLM-Based Annotation Generation,\nLLM-Generated Annotations Assessment, and LLM-Generated Annotations\nUtilization. Furthermore, this survey includes an in-depth taxonomy of data\ntypes that LLMs can annotate, a comprehensive review of learning strategies for\nmodels utilizing LLM-generated annotations, and a detailed discussion of the\nprimary challenges and limitations associated with using LLMs for data\nannotation and synthesis. Serving as a key guide, this survey aims to assist\nresearchers and practitioners in exploring the potential of the latest LLMs for\ndata annotation, thereby fostering future advancements in this critical field.", "published": "2024-02-21 00:44:04", "link": "http://arxiv.org/abs/2402.13446v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CAMELoT: Towards Large Language Models with Training-Free Consolidated\n  Associative Memory", "abstract": "Large Language Models (LLMs) struggle to handle long input sequences due to\nhigh memory and runtime costs. Memory-augmented models have emerged as a\npromising solution to this problem, but current methods are hindered by limited\nmemory capacity and require costly re-training to integrate with a new LLM. In\nthis work, we introduce an associative memory module which can be coupled to\nany pre-trained (frozen) attention-based LLM without re-training, enabling it\nto handle arbitrarily long input sequences. Unlike previous methods, our\nassociative memory module consolidates representations of individual tokens\ninto a non-parametric distribution model, dynamically managed by properly\nbalancing the novelty and recency of the incoming data. By retrieving\ninformation from this consolidated associative memory, the base LLM can achieve\nsignificant (up to 29.7% on Arxiv) perplexity reduction in long-context\nmodeling compared to other baselines evaluated on standard benchmarks. This\narchitecture, which we call CAMELoT (Consolidated Associative Memory Enhanced\nLong Transformer), demonstrates superior performance even with a tiny context\nwindow of 128 tokens, and also enables improved in-context learning with a much\nlarger set of demonstrations.", "published": "2024-02-21 01:00:17", "link": "http://arxiv.org/abs/2402.13449v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Important is Domain Specificity in Language Models and Instruction\n  Finetuning for Biomedical Relation Extraction?", "abstract": "Cutting edge techniques developed in the general NLP domain are often\nsubsequently applied to the high-value, data-rich biomedical domain. The past\nfew years have seen generative language models (LMs), instruction finetuning,\nand few-shot learning become foci of NLP research. As such, generative LMs\npretrained on biomedical corpora have proliferated and biomedical instruction\nfinetuning has been attempted as well, all with the hope that domain\nspecificity improves performance on downstream tasks. Given the nontrivial\neffort in training such models, we investigate what, if any, benefits they have\nin the key biomedical NLP task of relation extraction. Specifically, we address\ntwo questions: (1) Do LMs trained on biomedical corpora outperform those\ntrained on general domain corpora? (2) Do models instruction finetuned on\nbiomedical datasets outperform those finetuned on assorted datasets or those\nsimply pretrained? We tackle these questions using existing LMs, testing across\nfour datasets. In a surprising result, general-domain models typically\noutperformed biomedical-domain models. However, biomedical instruction\nfinetuning improved performance to a similar degree as general instruction\nfinetuning, despite having orders of magnitude fewer instructions. Our findings\nsuggest it may be more fruitful to focus research effort on larger-scale\nbiomedical instruction finetuning of general LMs over building domain-specific\nbiomedical LMs", "published": "2024-02-21 01:57:58", "link": "http://arxiv.org/abs/2402.13470v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval\n  Augmentation to Language Models", "abstract": "While large language models (LMs) demonstrate remarkable performance, they\nencounter challenges in providing accurate responses when queried for\ninformation beyond their pre-trained memorization. Although augmenting them\nwith relevant external information can mitigate these issues, failure to\nconsider the necessity of retrieval may adversely affect overall performance.\nPrevious research has primarily focused on examining how entities influence\nretrieval models and knowledge recall in LMs, leaving other aspects relatively\nunexplored. In this work, our goal is to offer a more detailed, fact-centric\nanalysis by exploring the effects of combinations of entities and relations. To\nfacilitate this, we construct a new question answering (QA) dataset called\nWiTQA (Wikipedia Triple Question Answers). This dataset includes questions\nabout entities and relations of various popularity levels, each accompanied by\na supporting passage. Our extensive experiments with diverse LMs and retrievers\nreveal when retrieval does not consistently enhance LMs from the viewpoints of\nfact-centric popularity. Confirming earlier findings, we observe that larger\nLMs excel in recalling popular facts. However, they notably encounter\ndifficulty with infrequent entity-relation pairs compared to retrievers.\nInterestingly, they can effectively retain popular relations of less common\nentities. We demonstrate the efficacy of our finer-grained metric and insights\nthrough an adaptive retrieval system that selectively employs retrieval and\nrecall based on the frequencies of entities and relations in the question.", "published": "2024-02-21 03:05:50", "link": "http://arxiv.org/abs/2402.13492v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Lay Person's Guide to Biomedicine: Orchestrating Large Language\n  Models", "abstract": "Automated lay summarisation (LS) aims to simplify complex technical documents\ninto a more accessible format to non-experts. Existing approaches using\npre-trained language models, possibly augmented with external background\nknowledge, tend to struggle with effective simplification and explanation.\nMoreover, automated methods that can effectively assess the `layness' of\ngenerated summaries are lacking. Recently, large language models (LLMs) have\ndemonstrated a remarkable capacity for text simplification, background\ninformation generation, and text evaluation. This has motivated our systematic\nexploration into using LLMs to generate and evaluate lay summaries of\nbiomedical articles. We propose a novel \\textit{Explain-then-Summarise} LS\nframework, which leverages LLMs to generate high-quality background knowledge\nto improve supervised LS. We also evaluate the performance of LLMs for\nzero-shot LS and propose two novel LLM-based LS evaluation metrics, which\nassess layness from multiple perspectives. Finally, we conduct a human\nassessment of generated lay summaries. Our experiments reveal that\nLLM-generated background information can support improved supervised LS.\nFurthermore, our novel zero-shot LS evaluation metric demonstrates a high\ndegree of alignment with human preferences. We conclude that LLMs have an\nimportant part to play in improving both the performance and evaluation of LS\nmethods.", "published": "2024-02-21 03:21:14", "link": "http://arxiv.org/abs/2402.13498v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RecMind: Japanese Movie Recommendation Dialogue with Seeker's Internal\n  State", "abstract": "Humans pay careful attention to the interlocutor's internal state in\ndialogues. For example, in recommendation dialogues, we make recommendations\nwhile estimating the seeker's internal state, such as his/her level of\nknowledge and interest. Since there are no existing annotated resources for the\nanalysis, we constructed RecMind, a Japanese movie recommendation dialogue\ndataset with annotations of the seeker's internal state at the entity level.\nEach entity has a subjective label annotated by the seeker and an objective\nlabel annotated by the recommender. RecMind also features engaging dialogues\nwith long seeker's utterances, enabling a detailed analysis of the seeker's\ninternal state. Our analysis based on RecMind reveals that entities that the\nseeker has no knowledge about but has an interest in contribute to\nrecommendation success. We also propose a response generation framework that\nexplicitly considers the seeker's internal state, utilizing the\nchain-of-thought prompting. The human evaluation results show that our proposed\nmethod outperforms the baseline method in both consistency and the success of\nrecommendations.", "published": "2024-02-21 04:15:22", "link": "http://arxiv.org/abs/2402.13522v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large\n  Language Models", "abstract": "Modern large language models (LLMs) should generally benefit individuals from\nvarious cultural backgrounds around the world. However, most recent advanced\ngenerative evaluation benchmarks tailed for LLMs mainly focus on English. To\nthis end, we introduce OMGEval, the first Open-source Multilingual Generative\ntest set that can assess the capability of LLMs in different languages. For\neach language, OMGEval provides 804 open-ended questions, covering a wide range\nof important capabilities of LLMs, such as general knowledge, logical\nreasoning, and so on. Each question is rigorously verified by human annotators.\nNotably, to sufficiently reflect the compatibility of LLMs in different\ncultural backgrounds, we perform localization for each non-English language.\nSpecifically, the current version of OMGEval includes 5 languages (i.e., Zh,\nRu, Fr, Es, Ar). Following AlpacaEval, we employ GPT-4 as the adjudicator to\nautomatically score different model outputs, which is shown closely related to\nhuman evaluation. We evaluate several representative multilingual LLMs on the\nproposed OMGEval, which we believe will provide a valuable reference for the\ncommunity to further understand and improve the multilingual capability of\nLLMs. OMGEval is available at https://github.com/blcuicall/OMGEval.", "published": "2024-02-21 04:42:41", "link": "http://arxiv.org/abs/2402.13524v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Whispers in Grammars: Injecting Covert Backdoors to Compromise Dense\n  Retrieval Systems", "abstract": "Dense retrieval systems have been widely used in various NLP applications.\nHowever, their vulnerabilities to potential attacks have been underexplored.\nThis paper investigates a novel attack scenario where the attackers aim to\nmislead the retrieval system into retrieving the attacker-specified contents.\nThose contents, injected into the retrieval corpus by attackers, can include\nharmful text like hate speech or spam. Unlike prior methods that rely on model\nweights and generate conspicuous, unnatural outputs, we propose a covert\nbackdoor attack triggered by grammar errors. Our approach ensures that the\nattacked models can function normally for standard queries while covertly\ntriggering the retrieval of the attacker's contents in response to minor\nlinguistic mistakes. Specifically, dense retrievers are trained with\ncontrastive loss and hard negative sampling. Surprisingly, our findings\ndemonstrate that contrastive loss is notably sensitive to grammatical errors,\nand hard negative sampling can exacerbate susceptibility to backdoor attacks.\nOur proposed method achieves a high attack success rate with a minimal corpus\npoisoning rate of only 0.048%, while preserving normal retrieval performance.\nThis indicates that the method has negligible impact on user experience for\nerror-free queries. Furthermore, evaluations across three real-world defense\nstrategies reveal that the malicious passages embedded within the corpus remain\nhighly resistant to detection and filtering, underscoring the robustness and\nsubtlety of the proposed attack.", "published": "2024-02-21 05:03:07", "link": "http://arxiv.org/abs/2402.13532v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ActiveRAG: Autonomously Knowledge Assimilation and Accommodation through\n  Retrieval-Augmented Agents", "abstract": "Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to\nleverage external knowledge, enhancing their performance on knowledge-intensive\ntasks. However, existing RAG models often treat LLMs as passive recipients of\ninformation, which can lead to interference from noisy retrieved content. In\nthis paper, we introduce ActiveRAG, a multi-agent framework that mimics human\nlearning behavior to help LLMs actively engage with and learn from retrieved\nevidence. ActiveRAG designs a knowledge assimilation agent to form the\nknowledge understanding by associating external knowledge with the parametric\nmemory of LLMs. Then our model employs the thought accommodation agent to\ncalibrate the internal thought of LLMs for response refinement. Our experiments\nshow that ActiveRAG achieves a 10\\% improvement over vanilla RAG on various\nquestion-answering benchmarks. Further analysis reveals that ActiveRAG\nmitigates the impact of noisy retrievals, alleviates conflicts between external\nknowledge and parametric memory and improves the self-consistency of LLMs in\nanswering the question. All data and codes are available at\nhttps://github.com/OpenMatch/ActiveRAG.", "published": "2024-02-21 06:04:53", "link": "http://arxiv.org/abs/2402.13547v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysis of Multi-Source Language Training in Cross-Lingual Transfer", "abstract": "The successful adaptation of multilingual language models (LMs) to a specific\nlanguage-task pair critically depends on the availability of data tailored for\nthat condition. While cross-lingual transfer (XLT) methods have contributed to\naddressing this data scarcity problem, there still exists ongoing debate about\nthe mechanisms behind their effectiveness. In this work, we focus on one of\npromising assumptions about inner workings of XLT, that it encourages\nmultilingual LMs to place greater emphasis on language-agnostic or\ntask-specific features. We test this hypothesis by examining how the patterns\nof XLT change with a varying number of source languages involved in the\nprocess. Our experimental findings show that the use of multiple source\nlanguages in XLT-a technique we term Multi-Source Language Training\n(MSLT)-leads to increased mingling of embedding spaces for different languages,\nsupporting the claim that XLT benefits from making use of language-independent\ninformation. On the other hand, we discover that using an arbitrary combination\nof source languages does not always guarantee better performance. We suggest\nsimple heuristics for identifying effective language combinations for MSLT and\nempirically prove its effectiveness.", "published": "2024-02-21 06:37:07", "link": "http://arxiv.org/abs/2402.13562v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BBA: Bi-Modal Behavioral Alignment for Reasoning with Large\n  Vision-Language Models", "abstract": "Multimodal reasoning stands as a pivotal capability for large vision-language\nmodels (LVLMs). The integration with Domain-Specific Languages (DSL), offering\nprecise visual representations, equips these models with the opportunity to\nexecute more accurate reasoning in complex and professional domains. However,\nthe vanilla Chain-of-Thought (CoT) prompting method faces challenges in\neffectively leveraging the unique strengths of visual and DSL representations,\nprimarily due to their differing reasoning mechanisms. Additionally, it often\nfalls short in addressing critical steps in multi-step reasoning tasks. To\nmitigate these challenges, we introduce the \\underline{B}i-Modal\n\\underline{B}ehavioral \\underline{A}lignment (BBA) prompting method, designed\nto maximize the potential of DSL in augmenting complex multi-modal reasoning\ntasks. This method initiates by guiding LVLMs to create separate reasoning\nchains for visual and DSL representations. Subsequently, it aligns these chains\nby addressing any inconsistencies, thus achieving a cohesive integration of\nbehaviors from different modalities. Our experiments demonstrate that BBA\nsubstantially improves the performance of GPT-4V(ision) on geometry problem\nsolving ($28.34\\% \\to 34.22\\%$), chess positional advantage prediction\n($42.08\\% \\to 46.99\\%$) and molecular property prediction ($77.47\\% \\to\n83.52\\%$).", "published": "2024-02-21 07:16:29", "link": "http://arxiv.org/abs/2402.13577v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LongWanjuan: Towards Systematic Measurement for Long Text Quality", "abstract": "The quality of training data are crucial for enhancing the long-text\ncapabilities of foundation models. Despite existing efforts to refine data\nquality through heuristic rules and evaluations based on data diversity and\ndifficulty, there's a lack of systematic approaches specifically tailored for\nassessing long texts. Addressing this gap, our work systematically measures the\nquality of long texts by evaluating three fundamental linguistic dimensions:\ncoherence, cohesion, and complexity. Drawing inspiration from the\naforementioned three dimensions, we introduce a suite of metrics designed to\nevaluate the quality of long texts, encompassing both statistical and\npre-trained language model-based ones. Leveraging these metrics, we present\nLongWanjuan, a bilingual dataset specifically tailored to enhance the training\nof language models for long-text tasks with over 160B tokens. In LongWanjuan,\nwe categorize long texts into holistic, aggregated, and chaotic types, enabling\na detailed analysis of long-text quality. Furthermore, we devise a data mixture\nrecipe that strategically balances different types of long texts within\nLongWanjuan, leading to significant improvements in model performance on\nlong-text tasks. The code and dataset are available at\nhttps://github.com/OpenLMLab/LongWanjuan.", "published": "2024-02-21 07:27:18", "link": "http://arxiv.org/abs/2402.13583v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WinoViz: Probing Visual Properties of Objects Under Different States", "abstract": "Humans perceive and comprehend different visual properties of an object based\non specific contexts. For instance, we know that a banana turns brown ``when it\nbecomes rotten,'' whereas it appears green ``when it is unripe.'' Previous\nstudies on probing visual commonsense knowledge have primarily focused on\nexamining language models' understanding of typical properties (e.g., colors\nand shapes) of objects. We present WinoViz, a text-only evaluation dataset,\nconsisting of 1,380 examples that probe the reasoning abilities of language\nmodels regarding variant visual properties of objects under different contexts\nor states. Our task is challenging since it requires pragmatic reasoning\n(finding intended meanings) and visual knowledge reasoning. We also present\nmulti-hop data, a more challenging version of our data, which requires\nmulti-step reasoning chains to solve our task. In our experimental analysis,\nour findings are: a) Large language models such as GPT-4 demonstrate effective\nperformance, but when it comes to multi-hop data, their performance is\nsignificantly degraded. b) Large models perform well on pragmatic reasoning,\nbut visual knowledge reasoning is a bottleneck in our task. c) Vision-language\nmodels outperform their language-model counterparts. d) A model with\nmachine-generated images performs poorly in our task. This is due to the poor\nquality of the generated images.", "published": "2024-02-21 07:31:47", "link": "http://arxiv.org/abs/2402.13584v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Enhanced Large Language Model Editing", "abstract": "Large language models (LLMs) are pivotal in advancing natural language\nprocessing (NLP) tasks, yet their efficacy is hampered by inaccuracies and\noutdated knowledge. Model editing emerges as a promising solution to address\nthese challenges. However, existing editing methods struggle to track and\nincorporate changes in knowledge associated with edits, which limits the\ngeneralization ability of postedit LLMs in processing edited knowledge. To\ntackle these problems, we propose a novel model editing method that leverages\nknowledge graphs for enhancing LLM editing, namely GLAME. Specifically, we\nfirst utilize a knowledge graph augmentation module to uncover associated\nknowledge that has changed due to editing, obtaining its internal\nrepresentations within LLMs. This approach allows knowledge alterations within\nLLMs to be reflected through an external graph structure. Subsequently, we\ndesign a graph-based knowledge edit module to integrate structured knowledge\ninto the model editing. This ensures that the updated parameters reflect not\nonly the modifications of the edited knowledge but also the changes in other\nassociated knowledge resulting from the editing process. Comprehensive\nexperiments conducted on GPT-J and GPT-2 XL demonstrate that GLAME\nsignificantly improves the generalization capabilities of post-edit LLMs in\nemploying edited knowledge.", "published": "2024-02-21 07:52:26", "link": "http://arxiv.org/abs/2402.13593v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KorNAT: LLM Alignment Benchmark for Korean Social Values and Common\n  Knowledge", "abstract": "For Large Language Models (LLMs) to be effectively deployed in a specific\ncountry, they must possess an understanding of the nation's culture and basic\nknowledge. To this end, we introduce National Alignment, which measures an\nalignment between an LLM and a targeted country from two aspects: social value\nalignment and common knowledge alignment. Social value alignment evaluates how\nwell the model understands nation-specific social values, while common\nknowledge alignment examines how well the model captures basic knowledge\nrelated to the nation. We constructed KorNAT, the first benchmark that measures\nnational alignment with South Korea. For the social value dataset, we obtained\nground truth labels from a large-scale survey involving 6,174 unique Korean\nparticipants. For the common knowledge dataset, we constructed samples based on\nKorean textbooks and GED reference materials. KorNAT contains 4K and 6K\nmultiple-choice questions for social value and common knowledge, respectively.\nOur dataset creation process is meticulously designed and based on statistical\nsampling theory and was refined through multiple rounds of human review. The\nexperiment results of seven LLMs reveal that only a few models met our\nreference score, indicating a potential for further enhancement. KorNAT has\nreceived government approval after passing an assessment conducted by a\ngovernment-affiliated organization dedicated to evaluating dataset quality.\nSamples and detailed evaluation protocols of our dataset can be found in\nhttps://huggingface.co/datasets/jiyounglee0523/KorNAT .", "published": "2024-02-21 08:12:26", "link": "http://arxiv.org/abs/2402.13605v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Study of Multilingual Confidence Estimation on Large\n  Language Models", "abstract": "The tendency of Large Language Models (LLMs) to generate hallucinations\nraises concerns regarding their reliability. Therefore, confidence estimations\nindicating the extent of trustworthiness of the generations become essential.\nHowever, current LLM confidence estimations in languages other than English\nremain underexplored. This paper addresses this gap by introducing a\ncomprehensive investigation of Multilingual Confidence estimation (MlingConf)\non LLMs, focusing on both language-agnostic (LA) and language-specific (LS)\ntasks to explore the performance and language dominance effects of multilingual\nconfidence estimations on different tasks. The benchmark comprises four\nmeticulously checked and human-evaluate high-quality multilingual datasets for\nLA tasks and one for the LS task tailored to specific social, cultural, and\ngeographical contexts of a language. Our experiments reveal that on LA tasks\nEnglish exhibits notable linguistic dominance in confidence estimations than\nother languages, while on LS tasks, using question-related language to prompt\nLLMs demonstrates better linguistic dominance in multilingual confidence\nestimations. The phenomena inspire a simple yet effective native-tone prompting\nstrategy by employing language-specific prompts for LS tasks, effectively\nimproving LLMs' reliability and accuracy on LS tasks.", "published": "2024-02-21 08:20:06", "link": "http://arxiv.org/abs/2402.13606v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning", "abstract": "Since commonsense information has been recorded significantly less frequently\nthan its existence, language models pre-trained by text generation have\ndifficulty to learn sufficient commonsense knowledge. Several studies have\nleveraged text retrieval to augment the models' commonsense ability. Unlike\ntext, images capture commonsense information inherently but little effort has\nbeen paid to effectively utilize them. In this work, we propose a novel\nMulti-mOdal REtrieval (MORE) augmentation framework, to leverage both text and\nimages to enhance the commonsense ability of language models. Extensive\nexperiments on the Common-Gen task have demonstrated the efficacy of MORE based\non the pre-trained models of both single and multiple modalities.", "published": "2024-02-21 08:54:47", "link": "http://arxiv.org/abs/2402.13625v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GCOF: Self-iterative Text Generation for Copywriting Using Large\n  Language Model", "abstract": "Large language models(LLM) such as ChatGPT have substantially simplified the\ngeneration of marketing copy, yet producing content satisfying domain specific\nrequirements, such as effectively engaging customers, remains a significant\nchallenge. In this work, we introduce the Genetic Copy Optimization Framework\n(GCOF) designed to enhance both efficiency and engagememnt of marketing copy\ncreation. We conduct explicit feature engineering within the prompts of LLM.\nAdditionally, we modify the crossover operator in Genetic Algorithm (GA),\nintegrating it into the GCOF to enable automatic feature engineering. This\nintegration facilitates a self-iterative refinement of the marketing copy.\nCompared to human curated copy, Online results indicate that copy produced by\nour framework achieves an average increase in click-through rate (CTR) of over\n$50\\%$.", "published": "2024-02-21 09:59:20", "link": "http://arxiv.org/abs/2402.13667v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning", "abstract": "The surge in Large Language Models (LLMs) has revolutionized natural language\nprocessing, but fine-tuning them for specific tasks often encounters challenges\nin balancing performance and preserving general instruction-following\nabilities. In this paper, we posit that the distribution gap between task\ndatasets and the LLMs serves as the primary underlying cause. To address the\nproblem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach\nthat bridges the distribution gap by guiding fine-tuning with a distilled\ndataset generated by the model itself to match its original distribution.\nExperimental results on the Llama-2-chat model across various benchmarks\ndemonstrate that SDFT effectively mitigates catastrophic forgetting while\nachieving comparable or superior performance on downstream tasks compared to\nthe vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain\nthe helpfulness and safety alignment of LLMs. Our code is available at\nhttps://github.com/sail-sg/sdft.", "published": "2024-02-21 10:06:08", "link": "http://arxiv.org/abs/2402.13669v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CMNER: A Chinese Multimodal NER Dataset based on Social Media", "abstract": "Multimodal Named Entity Recognition (MNER) is a pivotal task designed to\nextract named entities from text with the support of pertinent images.\nNonetheless, a notable paucity of data for Chinese MNER has considerably\nimpeded the progress of this natural language processing task within the\nChinese domain. Consequently, in this study, we compile a Chinese Multimodal\nNER dataset (CMNER) utilizing data sourced from Weibo, China's largest social\nmedia platform. Our dataset encompasses 5,000 Weibo posts paired with 18,326\ncorresponding images. The entities are classified into four distinct\ncategories: person, location, organization, and miscellaneous. We perform\nbaseline experiments on CMNER, and the outcomes underscore the effectiveness of\nincorporating images for NER. Furthermore, we conduct cross-lingual experiments\non the publicly available English MNER dataset (Twitter2015), and the results\nsubstantiate our hypothesis that Chinese and English multimodal NER data can\nmutually enhance the performance of the NER model.", "published": "2024-02-21 10:53:45", "link": "http://arxiv.org/abs/2402.13693v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand\n  for Multilingual Instructions?", "abstract": "The adaption of multilingual pre-trained LLMs into eloquent and helpful\nassistants is essential to facilitate their use across different language\nregions. In that spirit, we are the first to conduct an extensive study of the\nperformance of multilingual models instruction-tuned on different language\ncompositions on parallel instruction-tuning benchmarks across a selection of\nthe most spoken Indo-European languages. We systematically examine the effects\nof language and instruction dataset size on a mid-sized and a large,\nmultilingual LLMs by instruction-tuning them on parallel instruction-tuning\ndatasets. Our results demonstrate that instruction-tuning on parallel instead\nof monolingual corpora benefits cross-lingual instruction following\ncapabilities by up to 9.9%. Furthermore, we show that the Superficial Alignment\nHypothesis does not hold in general, as the investigated multilingual 7B\nparameter model presents a counter-example requiring large-scale\ninstruction-tuning datasets. Finally, we conduct a human annotation study to\nunderstand the alignment between human-based and GPT-4-based evaluation within\nmultilingual chat scenarios.", "published": "2024-02-21 11:07:07", "link": "http://arxiv.org/abs/2402.13703v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character\n  Role-Playing Agent", "abstract": "Large Language Models (LLMs) have revolutionized open-domain dialogue agents\nbut encounter challenges in multi-character role-playing (MCRP) scenarios. To\naddress the issue, we present Neeko, an innovative framework designed for\nefficient multiple characters imitation. Unlike existing methods, Neeko employs\na dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to\ndiverse characters. Our framework breaks down the role-playing process into\nagent pre-training, multiple characters playing, and character incremental\nlearning, effectively handling both seen and unseen roles. This dynamic\napproach, coupled with distinct LoRA blocks for each character, enhances\nNeeko's adaptability to unique attributes, personalities, and speaking\npatterns. As a result, Neeko demonstrates superior performance in MCRP over\nmost existing methods, offering more engaging and versatile user interaction\nexperiences. Code and data are available at\nhttps://github.com/weiyifan1023/Neeko.", "published": "2024-02-21 11:30:20", "link": "http://arxiv.org/abs/2402.13717v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "$\\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens", "abstract": "Processing and reasoning over long contexts is crucial for many practical\napplications of Large Language Models (LLMs), such as document comprehension\nand agent construction. Despite recent strides in making LLMs process contexts\nwith more than 100K tokens, there is currently a lack of a standardized\nbenchmark to evaluate this long-context capability. Existing public benchmarks\ntypically focus on contexts around 10K tokens, limiting the assessment and\ncomparison of LLMs in processing longer contexts. In this paper, we propose\n$\\infty$Bench, the first LLM benchmark featuring an average data length\nsurpassing 100K tokens. $\\infty$Bench comprises synthetic and realistic tasks\nspanning diverse domains, presented in both English and Chinese. The tasks in\n$\\infty$Bench are designed to require well understanding of long dependencies\nin contexts, and make simply retrieving a limited number of passages from\ncontexts not sufficient for these tasks. In our experiments, based on\n$\\infty$Bench, we evaluate the state-of-the-art proprietary and open-source\nLLMs tailored for processing long contexts. The results indicate that existing\nlong context LLMs still require significant advancements to effectively process\n100K+ context. We further present three intriguing analyses regarding the\nbehavior of LLMs processing long context.", "published": "2024-02-21 11:30:29", "link": "http://arxiv.org/abs/2402.13718v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster\n  Speculative Decoding", "abstract": "Speculative decoding is a widely used method that accelerates the generation\nprocess of large language models (LLMs) with no compromise in model\nperformance. It achieves this goal by using an existing smaller model for\ndrafting and then employing the target LLM to verify the draft in a low-cost\nparallel manner. Under such a drafting-verification framework, drafting\nefficiency has become a bottleneck in the final speedup of speculative\ndecoding. Therefore, generating longer drafts at less cost can lead to better\ndecoding speedup. To achieve this, we introduce Ouroboros, which can generate\ndraft phrases to parallelize the drafting process and meanwhile lengthen drafts\nin a training-free manner. The experimental results on various typical text\ngeneration tasks show that Ouroboros can achieve speedups of up to $2.8\\times$\nover speculative decoding and $3.9\\times$ over vanilla decoding, without\nfine-tuning draft and target models. The source code of Ouroboros is available\nat https://github.com/thunlp/Ouroboros.", "published": "2024-02-21 11:31:28", "link": "http://arxiv.org/abs/2402.13720v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Adaptive Contextual Masking for Aspect-Based Sentiment\n  Analysis", "abstract": "Aspect-Based Sentiment Analysis (ABSA) is a fine-grained linguistics problem\nthat entails the extraction of multifaceted aspects, opinions, and sentiments\nfrom the given text. Both standalone and compound ABSA tasks have been\nextensively used in the literature to examine the nuanced information present\nin online reviews and social media posts. Current ABSA methods often rely on\nstatic hyperparameters for attention-masking mechanisms, which can struggle\nwith context adaptation and may overlook the unique relevance of words in\nvaried situations. This leads to challenges in accurately analyzing complex\nsentences containing multiple aspects with differing sentiments. In this work,\nwe present adaptive masking methods that remove irrelevant tokens based on\ncontext to assist in Aspect Term Extraction and Aspect Sentiment Classification\nsubtasks of ABSA. We show with our experiments that the proposed methods\noutperform the baseline methods in terms of accuracy and F1 scores on four\nbenchmark online review datasets. Further, we show that the proposed methods\ncan be extended with multiple adaptations and demonstrate a qualitative\nanalysis of the proposed approach using sample text for aspect term extraction.", "published": "2024-02-21 11:33:09", "link": "http://arxiv.org/abs/2402.13722v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Text to CQL: Bridging Natural Language and Corpus Search Engine", "abstract": "Natural Language Processing (NLP) technologies have revolutionized the way we\ninteract with information systems, with a significant focus on converting\nnatural language queries into formal query languages such as SQL. However, less\nemphasis has been placed on the Corpus Query Language (CQL), a critical tool\nfor linguistic research and detailed analysis within text corpora. The manual\nconstruction of CQL queries is a complex and time-intensive task that requires\na great deal of expertise, which presents a notable challenge for both\nresearchers and practitioners. This paper presents the first text-to-CQL task\nthat aims to automate the translation of natural language into CQL. We present\na comprehensive framework for this task, including a specifically curated\nlarge-scale dataset and methodologies leveraging large language models (LLMs)\nfor effective text-to-CQL task. In addition, we established advanced evaluation\nmetrics to assess the syntactic and semantic accuracy of the generated queries.\nWe created innovative LLM-based conversion approaches and detailed experiments.\nThe results demonstrate the efficacy of our methods and provide insights into\nthe complexities of text-to-CQL task.", "published": "2024-02-21 12:11:28", "link": "http://arxiv.org/abs/2402.13740v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens", "abstract": "Large context window is a desirable feature in large language models (LLMs).\nHowever, due to high fine-tuning costs, scarcity of long texts, and\ncatastrophic values introduced by new token positions, current extended context\nwindows are limited to around 128k tokens. This paper introduces LongRoPE that,\nfor the first time, extends the context window of pre-trained LLMs to an\nimpressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k\ntraining lengths, while maintaining performance at the original short context\nwindow. This is achieved by three key innovations: (i) we identify and exploit\ntwo forms of non-uniformities in positional interpolation through an efficient\nsearch, providing a better initialization for fine-tuning and enabling an 8x\nextension in non-fine-tuning scenarios; (ii) we introduce a progressive\nextension strategy that first fine-tunes a 256k length LLM and then conducts a\nsecond positional interpolation on the fine-tuned extended LLM to achieve a\n2048k context window; (iii) we readjust LongRoPE on 8k length to recover the\nshort context window performance. Extensive experiments on LLaMA2 and Mistral\nacross various tasks demonstrate the effectiveness of our method. Models\nextended via LongRoPE retain the original architecture with minor modifications\nto the positional embedding, and can reuse most pre-existing optimizations.", "published": "2024-02-21 12:30:33", "link": "http://arxiv.org/abs/2402.13753v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Factual consistency evaluation of summarization in the Era of large\n  language models", "abstract": "Factual inconsistency with source documents in automatically generated\nsummaries can lead to misinformation or pose risks. Existing factual\nconsistency (FC) metrics are constrained by their performance, efficiency, and\nexplainability. Recent advances in Large language models (LLMs) have\ndemonstrated remarkable potential in text evaluation but their effectiveness in\nassessing FC in summarization remains underexplored. Prior research has mostly\nfocused on proprietary LLMs, leaving essential factors that affect their\nassessment capabilities unexplored. Additionally, current FC evaluation\nbenchmarks are restricted to news articles, casting doubt on the generality of\nthe FC methods tested on them. In this paper, we first address the gap by\nintroducing TreatFact a dataset of LLM-generated summaries of clinical texts,\nannotated for FC by domain experts. Moreover, we benchmark 11 LLMs for FC\nevaluation across news and clinical domains and analyse the impact of model\nsize, prompts, pre-training and fine-tuning data. Our findings reveal that\ndespite proprietary models prevailing on the task, open-source LLMs lag behind.\nNevertheless, there is potential for enhancing the performance of open-source\nLLMs through increasing model size, expanding pre-training data, and developing\nwell-curated fine-tuning data. Experiments on TreatFact suggest that both\nprevious methods and LLM-based evaluators are unable to capture factual\ninconsistencies in clinical summaries, posing a new challenge for FC\nevaluation.", "published": "2024-02-21 12:35:19", "link": "http://arxiv.org/abs/2402.13758v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Hate Speech: NLP's Challenges and Opportunities in Uncovering\n  Dehumanizing Language", "abstract": "Dehumanization, characterized as a subtle yet harmful manifestation of hate\nspeech, involves denying individuals of their human qualities and often results\nin violence against marginalized groups. Despite significant progress in\nNatural Language Processing across various domains, its application in\ndetecting dehumanizing language is limited, largely due to the scarcity of\npublicly available annotated data for this domain. This paper evaluates the\nperformance of cutting-edge NLP models, including GPT-4, GPT-3.5, and LLAMA-2,\nin identifying dehumanizing language. Our findings reveal that while these\nmodels demonstrate potential, achieving a 70\\% accuracy rate in distinguishing\ndehumanizing language from broader hate speech, they also display biases. They\nare over-sensitive in classifying other forms of hate speech as dehumanization\nfor a specific subset of target groups, while more frequently failing to\nidentify clear cases of dehumanization for other target groups. Moreover,\nleveraging one of the best-performing models, we automatically annotated a\nlarger dataset for training more accessible models. However, our findings\nindicate that these models currently do not meet the high-quality data\ngeneration threshold necessary for this task.", "published": "2024-02-21 13:57:36", "link": "http://arxiv.org/abs/2402.13818v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "$Se^2$: Sequential Example Selection for In-Context Learning", "abstract": "The remarkable capability of large language models (LLMs) for in-context\nlearning (ICL) needs to be activated by demonstration examples. Prior work has\nextensively explored the selection of examples for ICL, predominantly following\nthe \"select then organize\" paradigm, such approaches often neglect the internal\nrelationships between examples and exist an inconsistency between the training\nand inference. In this paper, we formulate the problem as a $Se$quential\n$Se$lection problem and introduce $Se^2$, a sequential-aware method that\nleverages the LLM's feedback on varying context, aiding in capturing\ninter-relationships and sequential information among examples, significantly\nenriching the contextuality and relevance of ICL prompts. Meanwhile, we utilize\nbeam search to seek and construct example sequences, enhancing both quality and\ndiversity. Extensive experiments across 23 NLP tasks from 8 distinct categories\nillustrate that $Se^2$ markedly surpasses competitive baselines and achieves\n42\\% relative improvement over random selection. Further in-depth analysis\nshows the effectiveness of proposed strategies, highlighting $Se^2$'s\nexceptional stability and adaptability across various scenarios. Code available\nat https://github.com/microsoft/LMOps.", "published": "2024-02-21 15:35:04", "link": "http://arxiv.org/abs/2402.13874v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Probabilities: Unveiling the Misalignment in Evaluating Large\n  Language Models", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious applications, fundamentally reshaping the landscape of natural language\nprocessing (NLP) research. However, recent evaluation frameworks often rely on\nthe output probabilities of LLMs for predictions, primarily due to\ncomputational constraints, diverging from real-world LLM usage scenarios. While\nwidely employed, the efficacy of these probability-based evaluation strategies\nremains an open research question. This study aims to scrutinize the validity\nof such probability-based evaluation methods within the context of using LLMs\nfor Multiple Choice Questions (MCQs), highlighting their inherent limitations.\nOur empirical investigation reveals that the prevalent probability-based\nevaluation method inadequately aligns with generation-based prediction.\nFurthermore, current evaluation frameworks typically assess LLMs through\npredictive tasks based on output probabilities rather than directly generating\nresponses, owing to computational limitations. We illustrate that these\nprobability-based approaches do not effectively correspond with generative\npredictions. The outcomes of our study can enhance the understanding of LLM\nevaluation methodologies and provide insights for future research in this\ndomain.", "published": "2024-02-21 15:58:37", "link": "http://arxiv.org/abs/2402.13887v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Calibrating Large Language Models with Sample Consistency", "abstract": "Accurately gauging the confidence level of Large Language Models' (LLMs)\npredictions is pivotal for their reliable application. However, LLMs are often\nuncalibrated inherently and elude conventional calibration techniques due to\ntheir proprietary nature and massive scale. In this work, we explore the\npotential of deriving confidence from the distribution of multiple randomly\nsampled model generations, via three measures of consistency. We perform an\nextensive evaluation across various open and closed-source models on nine\nreasoning datasets. Results show that consistency-based calibration methods\noutperform existing post-hoc approaches. Meanwhile, we find that factors such\nas intermediate explanations, model scaling, and larger sample sizes enhance\ncalibration, while instruction-tuning makes calibration more difficult.\nMoreover, confidence scores obtained from consistency have the potential to\nenhance model performance. Finally, we offer practical guidance on choosing\nsuitable consistency metrics for calibration, tailored to the characteristics\nof various LMs.", "published": "2024-02-21 16:15:20", "link": "http://arxiv.org/abs/2402.13904v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Collection-Wide Similarities for Unsupervised Document\n  Structure Extraction", "abstract": "Document collections of various domains, e.g., legal, medical, or financial,\noften share some underlying collection-wide structure, which captures\ninformation that can aid both human users and structure-aware models. We\npropose to identify the typical structure of document within a collection,\nwhich requires to capture recurring topics across the collection, while\nabstracting over arbitrary header paraphrases, and ground each topic to\nrespective document locations. These requirements pose several challenges:\nheaders that mark recurring topics frequently differ in phrasing, certain\nsection headers are unique to individual documents and do not reflect the\ntypical structure, and the order of topics can vary between documents.\nSubsequently, we develop an unsupervised graph-based method which leverages\nboth inter- and intra-document similarities, to extract the underlying\ncollection-wide structure. Our evaluations on three diverse domains in both\nEnglish and Hebrew indicate that our method extracts meaningful collection-wide\nstructure, and we hope that future work will leverage our method for\nmulti-document applications and structure-aware models.", "published": "2024-02-21 16:22:21", "link": "http://arxiv.org/abs/2402.13906v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Making Reasoning Matter: Measuring and Improving Faithfulness of\n  Chain-of-Thought Reasoning", "abstract": "Large language models (LLMs) have been shown to perform better when asked to\nreason step-by-step before answering a question. However, it is unclear to what\ndegree the model's final answer is faithful to the stated reasoning steps. In\nthis paper, we perform a causal mediation analysis on twelve LLMs to examine\nhow intermediate reasoning steps generated by the LLM influence the final\noutcome and find that LLMs do not reliably use their intermediate reasoning\nsteps when generating an answer. To address this issue, we introduce FRODO, a\nframework to tailor small-sized LMs to generate correct reasoning steps and\nrobustly reason over these steps. FRODO consists of an inference module that\nlearns to generate correct reasoning steps using an implicit causal reward\nfunction and a reasoning module that learns to faithfully reason over these\nintermediate inferences using a counterfactual and causal preference objective.\nOur experiments show that FRODO significantly outperforms four competitive\nbaselines. Furthermore, FRODO improves the robustness and generalization\nability of the reasoning LM, yielding higher performance on out-of-distribution\ntest sets. Finally, we find that FRODO's rationales are more faithful to its\nfinal answer predictions than standard supervised fine-tuning.", "published": "2024-02-21 17:23:59", "link": "http://arxiv.org/abs/2402.13950v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Social Biases in Masked Language Models by Proxy of Prediction\n  Quality", "abstract": "Transformer language models have achieved state-of-the-art performance for a\nvariety of natural language tasks but have been shown to encode unwanted\nbiases. We evaluate the social biases encoded by transformers trained with the\nmasked language modeling objective using proposed proxy functions within an\niterative masking experiment to measure the quality of transformer models'\npredictions and assess the preference of MLMs towards disadvantaged and\nadvantaged groups. We find all models encode concerning social biases. We\ncompare bias estimations with those produced by other evaluation methods using\nbenchmark datasets and assess their alignment with human annotated biases. We\nextend previous work by evaluating social biases introduced after retraining an\nMLM under the masked language modeling objective and find proposed measures\nproduce more accurate and sensitive estimations of biases introduced by\nretraining MLMs based on relative preference for biased sentences between\nmodels, while other methods tend to underestimate biases after retraining on\nsentences biased towards disadvantaged groups.", "published": "2024-02-21 17:33:13", "link": "http://arxiv.org/abs/2402.13954v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can You Learn Semantics Through Next-Word Prediction? The Case of\n  Entailment", "abstract": "Do LMs infer the semantics of text from co-occurrence patterns in their\ntraining data? Merrill et al. (2022) argue that, in theory, sentence\nco-occurrence probabilities predicted by an optimal LM should reflect the\nentailment relationship of the constituent sentences, but it is unclear whether\nprobabilities predicted by neural LMs encode entailment in this way because of\nstrong assumptions made by Merrill et al. (namely, that humans always avoid\nredundancy). In this work, we investigate whether their theory can be used to\ndecode entailment relations from neural LMs. We find that a test similar to\ntheirs can decode entailment relations between natural sentences, well above\nrandom chance, though not perfectly, across many datasets and LMs. This\nsuggests LMs implicitly model aspects of semantics to predict semantic effects\non sentence co-occurrence patterns. However, we find the test that predicts\nentailment in practice works in the opposite direction to the theoretical test.\nWe thus revisit the assumptions underlying the original test, finding its\nderivation did not adequately account for redundancy in human-written text. We\nargue that better accounting for redundancy related to explanations might\nderive the observed flipped test and, more generally, improve computational\nmodels of speakers in linguistics.", "published": "2024-02-21 17:36:07", "link": "http://arxiv.org/abs/2402.13956v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Building Multilingual Language Model for Medicine", "abstract": "The development of open-source, multilingual medical language models can\nbenefit a wide, linguistically diverse audience from different regions. To\npromote this domain, we present contributions from the following: First, we\nconstruct a multilingual medical corpus, containing approximately 25.5B tokens\nencompassing 6 main languages, termed as MMedC, enabling auto-regressive domain\nadaptation for general LLMs; Second, to monitor the development of multilingual\nmedical LLMs, we propose a multilingual medical multi-choice question-answering\nbenchmark with rationale, termed as MMedBench; Third, we have assessed a number\nof open-source large language models (LLMs) on our benchmark, along with those\nfurther auto-regressive trained on MMedC. Our final model, MMed-Llama 3, with\nonly 8B parameters, achieves superior performance compared to all other\nopen-source models on both MMedBench and English benchmarks, even rivaling\nGPT-4. In conclusion, in this work, we present a large-scale corpus, a\nbenchmark and a series of models to support the development of multilingual\nmedical LLMs.", "published": "2024-02-21 17:47:20", "link": "http://arxiv.org/abs/2402.13963v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysing The Impact of Sequence Composition on Language Model\n  Pre-Training", "abstract": "Most language model pre-training frameworks concatenate multiple documents\ninto fixed-length sequences and use causal masking to compute the likelihood of\neach token given its context; this strategy is widely adopted due to its\nsimplicity and efficiency. However, to this day, the influence of the\npre-training sequence composition strategy on the generalisation properties of\nthe model remains under-explored. In this work, we find that applying causal\nmasking can lead to the inclusion of distracting information from previous\ndocuments during pre-training, which negatively impacts the performance of the\nmodels on language modelling and downstream tasks. In intra-document causal\nmasking, the likelihood of each token is only conditioned on the previous\ntokens in the same document, eliminating potential distracting information from\nprevious documents and significantly improving performance. Furthermore, we\nfind that concatenating related documents can reduce some potential\ndistractions during pre-training, and our proposed efficient retrieval-based\nsequence construction method, BM25Chunk, can improve in-context learning\n(+11.6\\%), knowledge memorisation (+9.8\\%), and context utilisation (+7.2\\%)\nabilities of language models without sacrificing efficiency.", "published": "2024-02-21 18:23:16", "link": "http://arxiv.org/abs/2402.13991v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hallucinations or Attention Misdirection? The Path to Strategic Value\n  Extraction in Business Using Large Language Models", "abstract": "Large Language Models with transformer architecture have revolutionized the\ndomain of text generation, setting unprecedented benchmarks. Despite their\nimpressive capabilities, LLMs have been criticized for generating outcomes that\ndeviate from factual accuracy or display logical inconsistencies, phenomena\ncommonly referred to as hallucinations. This term, however, has often been\nmisapplied to any results deviating from the instructor's expectations, which\nthis paper defines as attention misdirection rather than true hallucinations.\nUnderstanding the distinction between hallucinations and attention misdirection\nbecomes increasingly relevant in business contexts, where the ramifications of\nsuch errors can significantly impact the value extraction from these inherently\npre-trained models. This paper highlights the best practices of the PGI,\nPersona, Grouping, and Intelligence, method, a strategic framework that\nachieved a remarkable error rate of only 3,15 percent across 4,000 responses\ngenerated by GPT in response to a real business challenge. It emphasizes that\nby equipping experimentation with knowledge, businesses can unlock\nopportunities for innovation through the use of these natively pre-trained\nmodels. This reinforces the notion that strategic application grounded in a\nskilled team can maximize the benefits of emergent technologies such as the\nLLMs.", "published": "2024-02-21 18:40:24", "link": "http://arxiv.org/abs/2402.14002v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with\n  Olympiad-Level Bilingual Multimodal Scientific Problems", "abstract": "Recent advancements have seen Large Language Models (LLMs) and Large\nMultimodal Models (LMMs) surpassing general human capabilities in various\ntasks, approaching the proficiency level of human experts across multiple\ndomains. With traditional benchmarks becoming less challenging for these\nmodels, new rigorous challenges are essential to gauge their advanced\nabilities. In this work, we present OlympiadBench, an Olympiad-level bilingual\nmultimodal scientific benchmark, featuring 8,476 problems from Olympiad-level\nmathematics and physics competitions, including the Chinese college entrance\nexam. Each problem is detailed with expert-level annotations for step-by-step\nreasoning. Evaluating top-tier models on OlympiadBench, we implement a\ncomprehensive assessment methodology to accurately evaluate model responses.\nNotably, the best-performing model, GPT-4V, attains an average score of 17.97%\non OlympiadBench, with a mere 10.74% in physics, highlighting the benchmark\nrigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V\npoints out prevalent issues with hallucinations, knowledge omissions, and\nlogical fallacies. We hope that our challenging benchmark can serve as a\nvaluable resource for helping future AGI research endeavors. The data and\nevaluation code are available at \\url{https://github.com/OpenBMB/OlympiadBench}", "published": "2024-02-21 18:49:26", "link": "http://arxiv.org/abs/2402.14008v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on\n  Zero-shot LLM Assessment", "abstract": "Large Language Models (LLMs) are powerful zero-shot assessors used in\nreal-world situations such as assessing written exams and benchmarking systems.\nDespite these critical applications, no existing work has analyzed the\nvulnerability of judge-LLMs to adversarial manipulation. This work presents the\nfirst study on the adversarial robustness of assessment LLMs, where we\ndemonstrate that short universal adversarial phrases can be concatenated to\ndeceive judge LLMs to predict inflated scores. Since adversaries may not know\nor have access to the judge-LLMs, we propose a simple surrogate attack where a\nsurrogate model is first attacked, and the learned attack phrase then\ntransferred to unknown judge-LLMs. We propose a practical algorithm to\ndetermine the short universal attack phrases and demonstrate that when\ntransferred to unseen models, scores can be drastically inflated such that\nirrespective of the assessed text, maximum scores are predicted. It is found\nthat judge-LLMs are significantly more susceptible to these adversarial attacks\nwhen used for absolute scoring, as opposed to comparative assessment. Our\nfindings raise concerns on the reliability of LLM-as-a-judge methods, and\nemphasize the importance of addressing vulnerabilities in LLM assessment\nmethods before deployment in high-stakes real-world scenarios.", "published": "2024-02-21 18:55:20", "link": "http://arxiv.org/abs/2402.14016v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Leveraging Encoder-only Pre-trained Language Models for Effective\n  Keyphrase Generation", "abstract": "This study addresses the application of encoder-only Pre-trained Language\nModels (PLMs) in keyphrase generation (KPG) amidst the broader availability of\ndomain-tailored encoder-only models compared to encoder-decoder models. We\ninvestigate three core inquiries: (1) the efficacy of encoder-only PLMs in KPG,\n(2) optimal architectural decisions for employing encoder-only PLMs in KPG, and\n(3) a performance comparison between in-domain encoder-only and encoder-decoder\nPLMs across varied resource settings. Our findings, derived from extensive\nexperimentation in two domains reveal that with encoder-only PLMs, although KPE\nwith Conditional Random Fields slightly excels in identifying present\nkeyphrases, the KPG formulation renders a broader spectrum of keyphrase\npredictions. Additionally, prefix-LM fine-tuning of encoder-only PLMs emerges\nas a strong and data-efficient strategy for KPG, outperforming general-domain\nseq2seq PLMs. We also identify a favorable parameter allocation towards model\ndepth rather than width when employing encoder-decoder architectures\ninitialized with encoder-only PLMs. The study sheds light on the potential of\nutilizing encoder-only PLMs for advancing KPG systems and provides a groundwork\nfor future KPG methods. Our code and pre-trained checkpoints are released at\nhttps://github.com/uclanlp/DeepKPG.", "published": "2024-02-21 18:57:54", "link": "http://arxiv.org/abs/2402.14052v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot\n  Annotator Adaptation", "abstract": "In subjective NLP tasks, where a single ground truth does not exist, the\ninclusion of diverse annotators becomes crucial as their unique perspectives\nsignificantly influence the annotations. In realistic scenarios, the annotation\nbudget often becomes the main determinant of the number of perspectives (i.e.,\nannotators) included in the data and subsequent modeling. We introduce a novel\nframework for annotation collection and modeling in subjective tasks that aims\nto minimize the annotation budget while maximizing the predictive performance\nfor each annotator. Our framework has a two-stage design: first, we rely on a\nsmall set of annotators to build a multitask model, and second, we augment the\nmodel for a new perspective by strategically annotating a few samples per\nannotator. To test our framework at scale, we introduce and release a unique\ndataset, Moral Foundations Subjective Corpus, of 2000 Reddit posts annotated by\n24 annotators for moral sentiment. We demonstrate that our framework surpasses\nthe previous SOTA in capturing the annotators' individual perspectives with as\nlittle as 25% of the original annotation budget on two datasets. Furthermore,\nour framework results in more equitable models, reducing the performance\ndisparity among annotators.", "published": "2024-02-21 19:53:36", "link": "http://arxiv.org/abs/2402.14101v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation", "abstract": "Textual style expresses a diverse set of information, including interpersonal\ndynamics (e.g., formality) and the author's emotions or attitudes (e.g.,\ndisgust). An open question is how language models can be explicitly controlled\nso that they weave together target styles when generating text: for example, to\nproduce text that is both negative and non-toxic. One approach to such\ncontrolled generation is multi-objective reinforcement learning (RL), but how\nbest to combine multiple objectives in a reward function is an open question.\nIn this paper, we investigate various formulations of multi-style rewards,\nincluding calibrated outputs from discriminators and dynamic weighting by\ndiscriminator gradient magnitudes. We find that our proposed dynamic weighting\noutperforms static weighting approaches with respect to style control while\nmaintaining linguistic quality, and we explore its effectiveness in 2- and\n3-style control.", "published": "2024-02-21 22:02:37", "link": "http://arxiv.org/abs/2402.14146v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TOOLVERIFIER: Generalization to New Tools via Self-Verification", "abstract": "Teaching language models to use tools is an important milestone towards\nbuilding general assistants, but remains an open problem. While there has been\nsignificant progress on learning to use specific tools via fine-tuning,\nlanguage models still struggle with learning how to robustly use new tools from\nonly a few demonstrations. In this work we introduce a self-verification method\nwhich distinguishes between close candidates by self-asking contrastive\nquestions during (1) tool selection; and (2) parameter generation. We construct\nsynthetic, high-quality, self-generated data for this goal using Llama-2 70B,\nwhich we intend to release publicly. Extensive experiments on 4 tasks from the\nToolBench benchmark, consisting of 17 unseen tools, demonstrate an average\nimprovement of 22% over few-shot baselines, even in scenarios where the\ndistinctions between candidate tools are finely nuanced.", "published": "2024-02-21 22:41:38", "link": "http://arxiv.org/abs/2402.14158v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of a semi-autonomous attentive listening system with takeover\n  prompting", "abstract": "The handling of communication breakdowns and loss of engagement is an\nimportant aspect of spoken dialogue systems, particularly for chatting systems\nsuch as attentive listening, where the user is mostly speaking. We presume that\na human is best equipped to handle this task and rescue the flow of\nconversation. To this end, we propose a semi-autonomous system, where a remote\noperator can take control of an autonomous attentive listening system in\nreal-time. In order to make human intervention easy and consistent, we\nintroduce automatic detection of low interest and engagement to provide\nexplicit takeover prompts to the remote operator. We implement this\nsemi-autonomous system which detects takeover points for the operator and\ncompare it to fully tele-operated and fully autonomous attentive listening\nsystems. We find that the semi-autonomous system is generally perceived more\npositively than the autonomous system. The results suggest that identifying\npoints of conversation when the user starts to lose interest may help us\nimprove a fully autonomous dialogue system.", "published": "2024-02-21 03:43:57", "link": "http://arxiv.org/abs/2402.14863v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structured Tree Alignment for Evaluation of (Speech) Constituency\n  Parsing", "abstract": "We present the structured average intersection-over-union ratio (STRUCT-IOU),\na similarity metric between constituency parse trees motivated by the problem\nof evaluating speech parsers. STRUCT-IOU enables comparison between a\nconstituency parse tree (over automatically recognized spoken word boundaries)\nwith the ground-truth parse (over written words). To compute the metric, we\nproject the ground-truth parse tree to the speech domain by forced alignment,\nalign the projected ground-truth constituents with the predicted ones under\ncertain structured constraints, and calculate the average IOU score across all\naligned constituent pairs. STRUCT-IOU takes word boundaries into account and\novercomes the challenge that the predicted words and ground truth may not have\nperfect one-to-one correspondence. Extending to the evaluation of text\nconstituency parsing, we demonstrate that STRUCT-IOU can address token-mismatch\nissues, and shows higher tolerance to syntactically plausible parses than\nPARSEVAL (Black et al., 1991).", "published": "2024-02-21 00:01:17", "link": "http://arxiv.org/abs/2402.13433v2", "categories": ["cs.CL", "cs.DS"], "primary_category": "cs.CL"}
{"title": "Potential and Challenges of Model Editing for Social Debiasing", "abstract": "Large language models (LLMs) trained on vast corpora suffer from inevitable\nstereotype biases. Mitigating these biases with fine-tuning could be both\ncostly and data-hungry. Model editing methods, which focus on modifying LLMs in\na post-hoc manner, are of great potential to address debiasing. However, it\nlacks a comprehensive study that facilitates both internal and external model\nediting methods, supports various bias types, as well as understands the pros\nand cons of applying editing methods to stereotypical debiasing. To mitigate\nthis gap, we carefully formulate social debiasing into an editing problem and\nbenchmark seven existing model editing algorithms on stereotypical debiasing,\ni.e., debias editing. Our findings in three scenarios reveal both the potential\nand challenges of debias editing: (1) Existing model editing methods can\neffectively preserve knowledge and mitigate biases, while the generalization of\ndebias effect from edited sentences to semantically equivalent sentences is\nlimited.(2) Sequential editing highlights the robustness of SERAC (Mitchell et\nal. 2022b), while internal editing methods degenerate with the number of edits.\n(3) Model editing algorithms achieve generalization towards unseen biases both\nwithin the same type and from different types. In light of these findings, we\nfurther propose two simple but effective methods to improve debias editing, and\nexperimentally show the effectiveness of the proposed methods.", "published": "2024-02-21 01:35:26", "link": "http://arxiv.org/abs/2402.13462v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RefuteBench: Evaluating Refuting Instruction-Following for Large\n  Language Models", "abstract": "The application scope of large language models (LLMs) is increasingly\nexpanding. In practical use, users might provide feedback based on the model's\noutput, hoping for a responsive model that can complete responses according to\ntheir feedback. Whether the model can appropriately respond to users' refuting\nfeedback and consistently follow through with execution has not been thoroughly\nanalyzed. In light of this, this paper proposes a comprehensive benchmark,\nRefuteBench, covering tasks such as question answering, machine translation,\nand email writing. The evaluation aims to assess whether models can positively\naccept feedback in form of refuting instructions and whether they can\nconsistently adhere to user demands throughout the conversation. We conduct\nevaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit\ninclination to their internal knowledge, often failing to comply with user\nfeedback. Additionally, as the length of the conversation increases, models\ngradually forget the user's stated feedback and roll back to their own\nresponses. We further propose a recall-and-repeat prompts as a simple and\neffective way to enhance the model's responsiveness to feedback.", "published": "2024-02-21 01:39:56", "link": "http://arxiv.org/abs/2402.13463v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "STENCIL: Submodular Mutual Information Based Weak Supervision for\n  Cold-Start Active Learning", "abstract": "As supervised fine-tuning of pre-trained models within NLP applications\nincreases in popularity, larger corpora of annotated data are required,\nespecially with increasing parameter counts in large language models. Active\nlearning, which attempts to mine and annotate unlabeled instances to improve\nmodel performance maximally fast, is a common choice for reducing the\nannotation cost; however, most methods typically ignore class imbalance and\neither assume access to initial annotated data or require multiple rounds of\nactive learning selection before improving rare classes. We present STENCIL,\nwhich utilizes a set of text exemplars and the recently proposed submodular\nmutual information to select a set of weakly labeled rare-class instances that\nare then strongly labeled by an annotator. We show that STENCIL improves\noverall accuracy by $10\\%-18\\%$ and rare-class F-1 score by $17\\%-40\\%$ on\nmultiple text classification datasets over common active learning methods\nwithin the class-imbalanced cold-start setting.", "published": "2024-02-21 01:54:58", "link": "http://arxiv.org/abs/2402.13468v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel\n  Decoding", "abstract": "Recent advancements in generative large language models (LLMs) have\nsignificantly boosted the performance in natural language processing tasks.\nHowever, their efficiency is hampered by the inherent limitations in\nautoregressive token generation. While parallel decoding with token tree\nverification, e.g., Medusa, has been proposed to improve decoding parallelism\nand efficiency, it often struggles with maintaining contextual relationships\ndue to its independent token prediction approach and incurs significant\nverification overhead, especially with large tree sizes and batch processing.\nIn this paper, we propose ProPD, an efficient LLM parallel decoding framework\nbased on dynamic token tree pruning and generation. ProPD features an advanced\nearly pruning mechanism to efficiently eliminate unpromising token sequences to\nimprove verification efficiency. Additionally, it introduces a dynamic token\ntree generation algorithm to balance the computation and parallelism of the\nverification phase in real-time and maximize the overall efficiency across\ndifferent batch sizes, sequence lengths, and tasks, etc. We verify ProPD across\na diverse set of datasets, LLMs, and batch sizes and demonstrate ProPD\nconsistently outperforms existing decoding algorithms by 1.1-3.2x.", "published": "2024-02-21 02:51:07", "link": "http://arxiv.org/abs/2402.13485v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical\n  Gradient Analysis", "abstract": "Large Language Models (LLMs) face threats from jailbreak prompts. Existing\nmethods for detecting jailbreak prompts are primarily online moderation APIs or\nfinetuned LLMs. These strategies, however, often require extensive and\nresource-intensive data collection and training processes. In this study, we\npropose GradSafe, which effectively detects jailbreak prompts by scrutinizing\nthe gradients of safety-critical parameters in LLMs. Our method is grounded in\na pivotal observation: the gradients of an LLM's loss for jailbreak prompts\npaired with compliance response exhibit similar patterns on certain\nsafety-critical parameters. In contrast, safe prompts lead to different\ngradient patterns. Building on this observation, GradSafe analyzes the\ngradients from prompts (paired with compliance responses) to accurately detect\njailbreak prompts. We show that GradSafe, applied to Llama-2 without further\ntraining, outperforms Llama Guard, despite its extensive finetuning with a\nlarge dataset, in detecting jailbreak prompts. This superior performance is\nconsistent across both zero-shot and adaptation scenarios, as evidenced by our\nevaluations on ToxicChat and XSTest. The source code is available at\nhttps://github.com/xyq7/GradSafe.", "published": "2024-02-21 03:09:21", "link": "http://arxiv.org/abs/2402.13494v2", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Leveraging Translation For Optimal Recall: Tailoring LLM Personalization\n  With User Profiles", "abstract": "This paper explores a novel technique for improving recall in cross-language\ninformation retrieval (CLIR) systems using iterative query refinement grounded\nin the user's lexical-semantic space. The proposed methodology combines\nmulti-level translation, semantic embedding-based expansion, and user\nprofile-centered augmentation to address the challenge of matching variance\nbetween user queries and relevant documents. Through an initial BM25 retrieval,\ntranslation into intermediate languages, embedding lookup of similar terms, and\niterative re-ranking, the technique aims to expand the scope of potentially\nrelevant results personalized to the individual user. Comparative experiments\non news and Twitter datasets demonstrate superior performance over baseline\nBM25 ranking for the proposed approach across ROUGE metrics. The translation\nmethodology also showed maintained semantic accuracy through the multi-step\nprocess. This personalized CLIR framework paves the path for improved\ncontext-aware retrieval attentive to the nuances of user language.", "published": "2024-02-21 03:25:14", "link": "http://arxiv.org/abs/2402.13500v1", "categories": ["cs.IR", "cs.CL", "F.2.2; I.2.7"], "primary_category": "cs.IR"}
{"title": "Self-DC: When to Reason and When to Act? Self Divide-and-Conquer for\n  Compositional Unknown Questions", "abstract": "Previous research has typically concentrated on leveraging the internal\nknowledge of Large Language Models (LLMs) to answer known questions (i.e.,\n\\textit{internal reasoning such as generate-then-read}). In contrast, for\nquestions that fall outside their known scope, these models rely on external\nknowledge retrieval to provide accurate responses (i.e., \\textit{external\nacting such as retrieve-then-read}). However, few previous works consider the\n\\textit{compositional questions}, which consist of several known and unknown\nsub-questions, necessitating the dynamic combination of previous two methods\n(i.e., \\textit{internal reasoning and external acting}) to achieve a better\ntrade-off between effectiveness and efficiency. To this end, we introduce a\n\\textbf{Self} \\textbf{D}ivide-and-\\textbf{C}onquer (\\textit{\\texttt{Self-DC}})\nframework, accompanying with the first \\textbf{C}ompositional \\textbf{u}nknown\n\\textbf{Q}uestion-\\textbf{A}nswering dataset (CuQA). This framework enables\nLLMs to adaptively choose between using internal knowledge and retrieving\nexternal knowledge as needed, resulting in a better trade-off between\neffectiveness and efficiency. Experimental results on two datasets demonstrate\nthat \\textit{\\texttt{Self-DC}} can achieve comparable or even better\nperformance with much fewer external calls compared with several strong\nbaselines.", "published": "2024-02-21 03:55:02", "link": "http://arxiv.org/abs/2402.13514v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Round Trip Translation Defence against Large Language Model Jailbreaking\n  Attacks", "abstract": "Large language models (LLMs) are susceptible to social-engineered attacks\nthat are human-interpretable but require a high level of comprehension for LLMs\nto counteract. Existing defensive measures can only mitigate less than half of\nthese attacks at most. To address this issue, we propose the Round Trip\nTranslation (RTT) method, the first algorithm specifically designed to defend\nagainst social-engineered attacks on LLMs. RTT paraphrases the adversarial\nprompt and generalizes the idea conveyed, making it easier for LLMs to detect\ninduced harmful behavior. This method is versatile, lightweight, and\ntransferrable to different LLMs. Our defense successfully mitigated over 70% of\nPrompt Automatic Iterative Refinement (PAIR) attacks, which is currently the\nmost effective defense to the best of our knowledge. We are also the first to\nattempt mitigating the MathsAttack and reduced its attack success rate by\nalmost 40%. Our code is publicly available at\nhttps://github.com/Cancanxxx/Round_Trip_Translation_Defence", "published": "2024-02-21 03:59:52", "link": "http://arxiv.org/abs/2402.13517v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RITFIS: Robust input testing framework for LLMs-based intelligent\n  software", "abstract": "The dependence of Natural Language Processing (NLP) intelligent software on\nLarge Language Models (LLMs) is increasingly prominent, underscoring the\nnecessity for robustness testing. Current testing methods focus solely on the\nrobustness of LLM-based software to prompts. Given the complexity and diversity\nof real-world inputs, studying the robustness of LLMbased software in handling\ncomprehensive inputs (including prompts and examples) is crucial for a thorough\nunderstanding of its performance.\n  To this end, this paper introduces RITFIS, a Robust Input Testing Framework\nfor LLM-based Intelligent Software. To our knowledge, RITFIS is the first\nframework designed to assess the robustness of LLM-based intelligent software\nagainst natural language inputs. This framework, based on given threat models\nand prompts, primarily defines the testing process as a combinatorial\noptimization problem. Successful test cases are determined by a goal function,\ncreating a transformation space for the original examples through perturbation\nmeans, and employing a series of search methods to filter cases that meet both\nthe testing objectives and language constraints. RITFIS, with its modular\ndesign, offers a comprehensive method for evaluating the robustness of LLMbased\nintelligent software.\n  RITFIS adapts 17 automated testing methods, originally designed for Deep\nNeural Network (DNN)-based intelligent software, to the LLM-based software\ntesting scenario. It demonstrates the effectiveness of RITFIS in evaluating\nLLM-based intelligent software through empirical validation. However, existing\nmethods generally have limitations, especially when dealing with lengthy texts\nand structurally complex threat models. Therefore, we conducted a comprehensive\nanalysis based on five metrics and provided insightful testing method\noptimization strategies, benefiting both researchers and everyday users.", "published": "2024-02-21 04:00:54", "link": "http://arxiv.org/abs/2402.13518v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "An Effective Incorporating Heterogeneous Knowledge Curriculum Learning\n  for Sequence Labeling", "abstract": "Sequence labeling models often benefit from incorporating external knowledge.\nHowever, this practice introduces data heterogeneity and complicates the model\nwith additional modules, leading to increased expenses for training a\nhigh-performing model. To address this challenge, we propose a two-stage\ncurriculum learning (TCL) framework specifically designed for sequence labeling\ntasks. The TCL framework enhances training by gradually introducing data\ninstances from easy to hard, aiming to improve both performance and training\nspeed. Furthermore, we explore different metrics for assessing the difficulty\nlevels of sequence labeling tasks. Through extensive experimentation on six\nChinese word segmentation (CWS) and Part-of-speech tagging (POS) datasets, we\ndemonstrate the effectiveness of our model in enhancing the performance of\nsequence labeling models. Additionally, our analysis indicates that TCL\naccelerates training and alleviates the slow training problem associated with\ncomplex models.", "published": "2024-02-21 05:04:29", "link": "http://arxiv.org/abs/2402.13534v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMs Meet Long Video: Advancing Long Video Question Answering with An\n  Interactive Visual Adapter in LLMs", "abstract": "Long video understanding is a significant and ongoing challenge in the\nintersection of multimedia and artificial intelligence. Employing large\nlanguage models (LLMs) for comprehending video becomes an emerging and\npromising method. However, this approach incurs high computational costs due to\nthe extensive array of video tokens, experiences reduced visual clarity as a\nconsequence of token aggregation, and confronts challenges arising from\nirrelevant visual tokens while answering video-related questions. To alleviate\nthese issues, we present an Interactive Visual Adapter (IVA) within LLMs,\ndesigned to enhance interaction with fine-grained visual elements.\nSpecifically, we first transform long videos into temporal video tokens via\nleveraging a visual encoder alongside a pretrained causal transformer, then\nfeed them into LLMs with the video instructions. Subsequently, we integrated\nIVA, which contains a lightweight temporal frame selector and a spatial feature\ninteractor, within the internal blocks of LLMs to capture instruction-aware and\nfine-grained visual signals. Consequently, the proposed video-LLM facilitates a\ncomprehensive understanding of long video content through appropriate long\nvideo modeling and precise visual interactions. We conducted extensive\nexperiments on nine video understanding benchmarks and experimental results\nshow that our interactive visual adapter significantly improves the performance\nof video LLMs on long video QA tasks. Ablation studies further verify the\neffectiveness of IVA in understanding long and short video.", "published": "2024-02-21 05:56:52", "link": "http://arxiv.org/abs/2402.13546v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Are LLMs Effective Negotiators? Systematic Evaluation of the\n  Multifaceted Capabilities of LLMs in Negotiation Dialogues", "abstract": "A successful negotiation requires a range of capabilities, including\ncomprehension of the conversation context, Theory-of-Mind (ToM) skills to infer\nthe partner's motives, strategic reasoning, and effective communication, making\nit challenging for automated systems. Despite the remarkable performance of\nLLMs in various NLP tasks, there is no systematic evaluation of their\ncapabilities in negotiation. Such an evaluation is critical for advancing AI\nnegotiation agents and negotiation research, ranging from designing dialogue\nsystems to providing pedagogical feedback and scaling up data collection\npractices. This work aims to systematically analyze the multifaceted\ncapabilities of LLMs across diverse dialogue scenarios throughout the stages of\na typical negotiation interaction. Our analysis highlights GPT-4's superior\nperformance in many tasks while identifying specific challenges, such as making\nsubjective assessments and generating contextually appropriate, strategically\nadvantageous responses.", "published": "2024-02-21 06:11:03", "link": "http://arxiv.org/abs/2402.13550v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fine-Grained Modeling of Narrative Context: A Coherence Perspective via\n  Retrospective Questions", "abstract": "This work introduces an original and practical paradigm for narrative\ncomprehension, stemming from the characteristics that individual passages\nwithin narratives tend to be more cohesively related than isolated.\nComplementary to the common end-to-end paradigm, we propose a fine-grained\nmodeling of narrative context, by formulating a graph dubbed NarCo, which\nexplicitly depicts task-agnostic coherence dependencies that are ready to be\nconsumed by various downstream tasks. In particular, edges in NarCo encompass\nfree-form retrospective questions between context snippets, inspired by human\ncognitive perception that constantly reinstates relevant events from prior\ncontext. Importantly, our graph formalism is practically instantiated by LLMs\nwithout human annotations, through our designed two-stage prompting scheme. To\nexamine the graph properties and its utility, we conduct three studies in\nnarratives, each from a unique angle: edge relation efficacy, local context\nenrichment, and broader application in QA. All tasks could benefit from the\nexplicit coherence captured by NarCo.", "published": "2024-02-21 06:14:04", "link": "http://arxiv.org/abs/2402.13551v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension\n  with Enhanced Visual Knowledge Alignment", "abstract": "Evaluating and Rethinking the current landscape of Large Multimodal Models\n(LMMs), we observe that widely-used visual-language projection approaches\n(e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet\nignore the visual knowledge-dimension alignment, i.e., connecting visuals to\ntheir relevant knowledge. Visual knowledge plays a significant role in\nanalyzing, inferring, and interpreting information from visuals, helping\nimprove the accuracy of answers to knowledge-based visual questions. In this\npaper, we mainly explore improving LMMs with visual-language knowledge\nalignment, especially aimed at challenging knowledge-based visual question\nanswering (VQA). To this end, we present a Cognitive Visual-Language Mapper\n(CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a\nFine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning\nstage. Specifically, we design the VKA based on the interaction between a small\nlanguage model and a visual encoder, training it on collected image-knowledge\npairs to achieve visual knowledge acquisition and projection. FKA is employed\nto distill the fine-grained visual knowledge of an image and inject it into\nLarge Language Models (LLMs). We conduct extensive experiments on\nknowledge-based VQA benchmarks and experimental results show that CVLM\nsignificantly improves the performance of LMMs on knowledge-based VQA (average\ngain by 5.0%). Ablation studies also verify the effectiveness of VKA and FKA,\nrespectively. The codes are available at\nhttps://github.com/HITsz-TMG/Cognitive-Visual-Language-Mapper", "published": "2024-02-21 06:34:46", "link": "http://arxiv.org/abs/2402.13561v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Multilingual Coreference Resolution in Low-resource South Asian\n  Languages", "abstract": "Coreference resolution involves the task of identifying text spans within a\ndiscourse that pertain to the same real-world entity. While this task has been\nextensively explored in the English language, there has been a notable scarcity\nof publicly accessible resources and models for coreference resolution in South\nAsian languages. We introduce a Translated dataset for Multilingual Coreference\nResolution (TransMuCoRes) in 31 South Asian languages using off-the-shelf tools\nfor translation and word-alignment. Nearly all of the predicted translations\nsuccessfully pass a sanity check, and 75% of English references align with\ntheir predicted translations. Using multilingual encoders, two off-the-shelf\ncoreference resolution models were trained on a concatenation of TransMuCoRes\nand a Hindi coreference resolution dataset with manual annotations. The best\nperforming model achieved a score of 64 and 68 for LEA F1 and CoNLL F1,\nrespectively, on our test-split of Hindi golden set. This study is the first to\nevaluate an end-to-end coreference resolution model on a Hindi golden set.\nFurthermore, this work underscores the limitations of current coreference\nevaluation metrics when applied to datasets with split antecedents, advocating\nfor the development of more suitable evaluation metrics.", "published": "2024-02-21 07:05:51", "link": "http://arxiv.org/abs/2402.13571v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Multimodal In-Context Tuning Approach for E-Commerce Product\n  Description Generation", "abstract": "In this paper, we propose a new setting for generating product descriptions\nfrom images, augmented by marketing keywords. It leverages the combined power\nof visual and textual information to create descriptions that are more tailored\nto the unique features of products. For this setting, previous methods utilize\nvisual and textual encoders to encode the image and keywords and employ a\nlanguage model-based decoder to generate the product description. However, the\ngenerated description is often inaccurate and generic since same-category\nproducts have similar copy-writings, and optimizing the overall framework on\nlarge-scale samples makes models concentrate on common words yet ignore the\nproduct features. To alleviate the issue, we present a simple and effective\nMultimodal In-Context Tuning approach, named ModICT, which introduces a similar\nproduct sample as the reference and utilizes the in-context learning capability\nof language models to produce the description. During training, we keep the\nvisual encoder and language model frozen, focusing on optimizing the modules\nresponsible for creating multimodal in-context references and dynamic prompts.\nThis approach preserves the language generation prowess of large language\nmodels (LLMs), facilitating a substantial increase in description diversity. To\nassess the effectiveness of ModICT across various language model scales and\ntypes, we collect data from three distinct product categories within the\nE-commerce domain. Extensive experiments demonstrate that ModICT significantly\nimproves the accuracy (by up to 3.3% on Rouge-L) and diversity (by up to 9.4%\non D-5) of generated results compared to conventional methods. Our findings\nunderscore the potential of ModICT as a valuable tool for enhancing automatic\ngeneration of product descriptions in a wide range of applications. Code is at:\nhttps://github.com/HITsz-TMG/Multimodal-In-Context-Tuning", "published": "2024-02-21 07:38:29", "link": "http://arxiv.org/abs/2402.13587v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Breaking the HISCO Barrier: Automatic Occupational Standardization with\n  OccCANINE", "abstract": "This paper introduces a new tool, OccCANINE, to automatically transform\noccupational descriptions into the HISCO classification system. The manual work\ninvolved in processing and classifying occupational descriptions is\nerror-prone, tedious, and time-consuming. We finetune a preexisting language\nmodel (CANINE) to do this automatically, thereby performing in seconds and\nminutes what previously took days and weeks. The model is trained on 14 million\npairs of occupational descriptions and HISCO codes in 13 different languages\ncontributed by 22 different sources. Our approach is shown to have accuracy,\nrecall, and precision above 90 percent. Our tool breaks the metaphorical HISCO\nbarrier and makes this data readily available for analysis of occupational\nstructures with broad applicability in economics, economic history, and various\nrelated disciplines.", "published": "2024-02-21 08:10:43", "link": "http://arxiv.org/abs/2402.13604v2", "categories": ["cs.CL", "econ.EM", "I.2.7; I.7.0"], "primary_category": "cs.CL"}
{"title": "CODIS: Benchmarking Context-Dependent Visual Comprehension for\n  Multimodal Large Language Models", "abstract": "Multimodal large language models (MLLMs) have demonstrated promising results\nin a variety of tasks that combine vision and language. As these models become\nmore integral to research and applications, conducting comprehensive\nevaluations of their capabilities has grown increasingly important. However,\nmost existing benchmarks fail to consider that, in certain situations, images\nneed to be interpreted within a broader context. In this work, we introduce a\nnew benchmark, named as CODIS, designed to assess the ability of models to use\ncontext provided in free-form text to enhance visual comprehension. Our\nfindings indicate that MLLMs consistently fall short of human performance on\nthis benchmark. Further analysis confirms that these models struggle to\neffectively extract and utilize contextual information to improve their\nunderstanding of images. This underscores the pressing need to enhance the\nability of MLLMs to comprehend visuals in a context-dependent manner. View our\nproject website at https://thunlp-mt.github.io/CODIS.", "published": "2024-02-21 08:21:12", "link": "http://arxiv.org/abs/2402.13607v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Overview of the VLSP 2023 -- ComOM Shared Task: A Data Challenge for\n  Comparative Opinion Mining from Vietnamese Product Reviews", "abstract": "This paper presents a comprehensive overview of the Comparative Opinion\nMining from Vietnamese Product Reviews shared task (ComOM), held as part of the\n10$^{th}$ International Workshop on Vietnamese Language and Speech Processing\n(VLSP 2023). The primary objective of this shared task is to advance the field\nof natural language processing by developing techniques that proficiently\nextract comparative opinions from Vietnamese product reviews. Participants are\nchallenged to propose models that adeptly extract a comparative \"quintuple\"\nfrom a comparative sentence, encompassing Subject, Object, Aspect, Predicate,\nand Comparison Type Label. We construct a human-annotated dataset comprising\n$120$ documents, encompassing $7427$ non-comparative sentences and $2468$\ncomparisons within $1798$ sentences. Participating models undergo evaluation\nand ranking based on the Exact match macro-averaged quintuple F1 score.", "published": "2024-02-21 08:29:26", "link": "http://arxiv.org/abs/2402.13613v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FLAME: Self-Supervised Low-Resource Taxonomy Expansion using Large\n  Language Models", "abstract": "Taxonomies represent an arborescence hierarchical structure that establishes\nrelationships among entities to convey knowledge within a specific domain. Each\nedge in the taxonomy signifies a hypernym-hyponym relationship. Taxonomies find\nutility in various real-world applications, such as e-commerce search engines\nand recommendation systems. Consequently, there arises a necessity to enhance\nthese taxonomies over time. However, manually curating taxonomies with neoteric\ndata presents challenges due to limitations in available human resources and\nthe exponential growth of data. Therefore, it becomes imperative to develop\nautomatic taxonomy expansion methods. Traditional supervised taxonomy expansion\napproaches encounter difficulties stemming from limited resources, primarily\ndue to the small size of existing taxonomies. This scarcity of training data\noften leads to overfitting. In this paper, we propose FLAME, a novel approach\nfor taxonomy expansion in low-resource environments by harnessing the\ncapabilities of large language models that are trained on extensive real-world\nknowledge. LLMs help compensate for the scarcity of domain-specific knowledge.\nSpecifically, FLAME leverages prompting in few-shot settings to extract the\ninherent knowledge within the LLMs, ascertaining the hypernym entities within\nthe taxonomy. Furthermore, it employs reinforcement learning to fine-tune the\nlarge language models, resulting in more accurate predictions. Experiments on\nthree real-world benchmark datasets demonstrate the effectiveness of FLAME in\nreal-world scenarios, achieving a remarkable improvement of 18.5% in accuracy\nand 12.3% in Wu & Palmer metric over eight baselines. Furthermore, we elucidate\nthe strengths and weaknesses of FLAME through an extensive case study, error\nanalysis and ablation studies on the benchmarks.", "published": "2024-02-21 08:50:40", "link": "http://arxiv.org/abs/2402.13623v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Text Style Transfer via LLMs and Attention Masking with\n  Multi-way Interactions", "abstract": "Unsupervised Text Style Transfer (UTST) has emerged as a critical task within\nthe domain of Natural Language Processing (NLP), aiming to transfer one\nstylistic aspect of a sentence into another style without changing its\nsemantics, syntax, or other attributes. This task is especially challenging\ngiven the intrinsic lack of parallel text pairings. Among existing methods for\nUTST tasks, attention masking approach and Large Language Models (LLMs) are\ndeemed as two pioneering methods. However, they have shortcomings in generating\nunsmooth sentences and changing the original contents, respectively. In this\npaper, we investigate if we can combine these two methods effectively. We\npropose four ways of interactions, that are pipeline framework with tuned\norders; knowledge distillation from LLMs to attention masking model; in-context\nlearning with constructed parallel examples. We empirically show these\nmulti-way interactions can improve the baselines in certain perspective of\nstyle strength, content preservation and text fluency. Experiments also\ndemonstrate that simply conducting prompting followed by attention\nmasking-based revision can consistently surpass the other systems, including\nsupervised text style transfer systems. On Yelp-clean and Amazon-clean\ndatasets, it improves the previously best mean metric by 0.5 and 3.0 absolute\npercentages respectively, and achieves new SOTA results.", "published": "2024-02-21 09:28:02", "link": "http://arxiv.org/abs/2402.13647v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Privacy-Preserving Instructions for Aligning Large Language Models", "abstract": "Service providers of large language model (LLM) applications collect user\ninstructions in the wild and use them in further aligning LLMs with users'\nintentions. These instructions, which potentially contain sensitive\ninformation, are annotated by human workers in the process. This poses a new\nprivacy risk not addressed by the typical private optimization. To this end, we\npropose using synthetic instructions to replace real instructions in data\nannotation and model fine-tuning. Formal differential privacy is guaranteed by\ngenerating those synthetic instructions using privately fine-tuned generators.\nCrucial in achieving the desired utility is our novel filtering algorithm that\nmatches the distribution of the synthetic instructions to that of the real\nones. In both supervised fine-tuning and reinforcement learning from human\nfeedback, our extensive experiments demonstrate the high utility of the final\nset of synthetic instructions by showing comparable results to real\ninstructions. In supervised fine-tuning, models trained with private synthetic\ninstructions outperform leading open-source models such as Vicuna.", "published": "2024-02-21 09:45:08", "link": "http://arxiv.org/abs/2402.13659v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual\n  Machine-Generated Text Detection", "abstract": "SemEval-2024 Task 8 is focused on multigenerator, multidomain, and\nmultilingual black-box machine-generated text detection. Such a detection is\nimportant for preventing a potential misuse of large language models (LLMs),\nthe newest of which are very capable in generating multilingual human-like\ntexts. We have coped with this task in multiple ways, utilizing language\nidentification and parameter-efficient fine-tuning of smaller LLMs for text\nclassification. We have further used the per-language classification-threshold\ncalibration to uniquely combine fine-tuned models predictions with statistical\ndetection metrics to improve generalization of the system detection\nperformance. Our submitted method achieved competitive results, ranking at the\nfourth place, just under 1 percentage point behind the winner.", "published": "2024-02-21 10:09:56", "link": "http://arxiv.org/abs/2402.13671v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SaGE: Evaluating Moral Consistency in Large Language Models", "abstract": "Despite recent advancements showcasing the impressive capabilities of Large\nLanguage Models (LLMs) in conversational systems, we show that even\nstate-of-the-art LLMs are morally inconsistent in their generations,\nquestioning their reliability (and trustworthiness in general). Prior works in\nLLM evaluation focus on developing ground-truth data to measure accuracy on\nspecific tasks. However, for moral scenarios that often lack universally\nagreed-upon answers, consistency in model responses becomes crucial for their\nreliability. To address this issue, we propose an information-theoretic measure\ncalled Semantic Graph Entropy (SaGE), grounded in the concept of \"Rules of\nThumb\" (RoTs) to measure a model's moral consistency. RoTs are abstract\nprinciples learned by a model and can help explain their decision-making\nstrategies effectively. To this extent, we construct the Moral Consistency\nCorpus (MCC), containing 50K moral questions, responses to them by LLMs, and\nthe RoTs that these models followed. Furthermore, to illustrate the\ngeneralizability of SaGE, we use it to investigate LLM consistency on two\npopular datasets -- TruthfulQA and HellaSwag. Our results reveal that\ntask-accuracy and consistency are independent problems, and there is a dire\nneed to investigate these issues further.", "published": "2024-02-21 11:23:21", "link": "http://arxiv.org/abs/2402.13709v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cracking Factual Knowledge: A Comprehensive Analysis of Degenerate\n  Knowledge Neurons in Large Language Models", "abstract": "Large language models (LLMs) store extensive factual knowledge, but the\nunderlying mechanisms remain unclear. Previous research suggests that factual\nknowledge is stored within multi-layer perceptron weights, and some storage\nunits exhibit degeneracy, referred to as Degenerate Knowledge Neurons (DKNs).\nDespite the novelty and unique properties of this concept, it has not been\nrigorously defined or systematically studied. We first consider the connection\nweight patterns of MLP neurons and define DKNs from both structural and\nfunctional aspects. Based on this, we introduce the Neurological Topology\nClustering method, which allows the formation of DKNs in any numbers and\nstructures, leading to a more accurate DKN acquisition. Furthermore, inspired\nby cognitive science, we explore the relationship between DKNs and the\nrobustness, evolvability, and complexity of LLMs. Our execution of 34\nexperiments under 6 settings demonstrates the connection between DKNs and these\nthree properties. The code will be available soon.", "published": "2024-02-21 11:50:32", "link": "http://arxiv.org/abs/2402.13731v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unlocking Instructive In-Context Learning with Tabular Prompting for\n  Relational Triple Extraction", "abstract": "The in-context learning (ICL) for relational triple extraction (RTE) has\nachieved promising performance, but still encounters two key challenges: (1)\nhow to design effective prompts and (2) how to select proper demonstrations.\nExisting methods, however, fail to address these challenges appropriately. On\nthe one hand, they usually recast RTE task to text-to-text prompting formats,\nwhich is unnatural and results in a mismatch between the output format at the\npre-training time and the inference time for large language models (LLMs). On\nthe other hand, they only utilize surface natural language features and lack\nconsideration of triple semantics in sample selection. These issues are\nblocking improved performance in ICL for RTE, thus we aim to tackle prompt\ndesigning and sample selection challenges simultaneously. To this end, we\ndevise a tabular prompting for RTE (\\textsc{TableIE}) which frames RTE task\ninto a table generation task to incorporate explicit structured information\ninto ICL, facilitating conversion of outputs to RTE structures. Then we propose\ninstructive in-context learning (I$^2$CL) which only selects and annotates a\nfew samples considering internal triple semantics in massive unlabeled samples.", "published": "2024-02-21 12:12:16", "link": "http://arxiv.org/abs/2402.13741v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CriticEval: Evaluating Large Language Model as Critic", "abstract": "Critique ability, i.e., the capability of Large Language Models (LLMs) to\nidentify and rectify flaws in responses, is crucial for their applications in\nself-improvement and scalable oversight. While numerous studies have been\nproposed to evaluate critique ability of LLMs, their comprehensiveness and\nreliability are still limited. To overcome this problem, we introduce\nCriticEval, a novel benchmark designed to comprehensively and reliably evaluate\ncritique ability of LLMs. Specifically, to ensure the comprehensiveness,\nCriticEval evaluates critique ability from four dimensions across nine diverse\ntask scenarios. It evaluates both scalar-valued and textual critiques,\ntargeting responses of varying quality. To ensure the reliability, a large\nnumber of critiques are annotated to serve as references, enabling GPT-4 to\nevaluate textual critiques reliably. Extensive evaluations of open-source and\nclosed-source LLMs first validate the reliability of evaluation in CriticEval.\nThen, experimental results demonstrate the promising potential of open-source\nLLMs, the effectiveness of critique datasets and several intriguing\nrelationships between the critique ability and some critical factors, including\ntask types, response qualities and critique dimensions.", "published": "2024-02-21 12:38:59", "link": "http://arxiv.org/abs/2402.13764v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Geography of Information Diffusion in Online Discourse on Europe and\n  Migration", "abstract": "The online diffusion of information related to Europe and migration has been\nlittle investigated from an external point of view. However, this is a very\nrelevant topic, especially if users have had no direct contact with Europe and\nits perception depends solely on information retrieved online. In this work we\nanalyse the information circulating online about Europe and migration after\nretrieving a large amount of data from social media (Twitter), to gain new\ninsights into topics, magnitude, and dynamics of their diffusion. We combine\nretweets and hashtags network analysis with geolocation of users, linking thus\ndata to geography and allowing analysis from an \"outside Europe\" perspective,\nwith a special focus on Africa. We also introduce a novel approach based on\ncross-lingual quotes, i.e. when content in a language is commented and\nretweeted in another language, assuming these interactions are a proxy for\nconnections between very distant communities. Results show how the majority of\nonline discussions occurs at a national level, especially when discussing\nmigration. Language (English) is pivotal for information to become\ntransnational and reach far. Transnational information flow is strongly\nunbalanced, with content mainly produced in Europe and amplified outside.\nConversely Europe-based accounts tend to be self-referential when they discuss\nmigration-related topics. Football is the most exported topic from Europe\nworldwide. Moreover, important nodes in the communities discussing\nmigration-related topics include accounts of official institutions and\ninternational agencies, together with journalists, news, commentators and\nactivists.", "published": "2024-02-21 13:30:34", "link": "http://arxiv.org/abs/2402.13800v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Kuaiji: the First Chinese Accounting Large Language Model", "abstract": "Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated\nimpressive proficiency in comprehending and generating natural language.\nHowever, they encounter difficulties when tasked with adapting to specialized\ndomains such as accounting. To address this challenge, we introduce Kuaiji, a\ntailored Accounting Large Language Model. Kuaiji is meticulously fine-tuned\nusing the Baichuan framework, which encompasses continuous pre-training and\nsupervised fine-tuning processes. Supported by CAtAcctQA, a dataset containing\nlarge genuine accountant-client dialogues, Kuaiji exhibits exceptional accuracy\nand response speed. Our contributions encompass the creation of the first\nChinese accounting dataset, the establishment of Kuaiji as a leading\nopen-source Chinese accounting LLM, and the validation of its efficacy through\nreal-world accounting scenarios.", "published": "2024-02-21 15:14:20", "link": "http://arxiv.org/abs/2402.13866v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Could We Have Had Better Multilingual LLMs If English Was Not the\n  Central Language?", "abstract": "Large Language Models (LLMs) demonstrate strong machine translation\ncapabilities on languages they are trained on. However, the impact of factors\nbeyond training data size on translation performance remains a topic of debate,\nespecially concerning languages not directly encountered during training. Our\nstudy delves into Llama2's translation capabilities. By modeling a linear\nrelationship between linguistic feature distances and machine translation\nscores, we ask ourselves if there are potentially better central languages for\nLLMs other than English. Our experiments show that the 7B Llama2 model yields\nabove 10 BLEU when translating into all languages it has seen, which rarely\nhappens for languages it has not seen. Most translation improvements into\nunseen languages come from scaling up the model size rather than instruction\ntuning or increasing shot count. Furthermore, our correlation analysis reveals\nthat syntactic similarity is not the only linguistic factor that strongly\ncorrelates with machine translation scores. Interestingly, we discovered that\nunder specific circumstances, some languages (e.g. Swedish, Catalan), despite\nhaving significantly less training data, exhibit comparable correlation levels\nto English. These insights challenge the prevailing landscape of LLMs,\nsuggesting that models centered around languages other than English could\nprovide a more efficient foundation for multilingual applications.", "published": "2024-02-21 16:32:38", "link": "http://arxiv.org/abs/2402.13917v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in\n  Clinical Summarization", "abstract": "Large Language Models (LLMs) such as GPT & Llama have demonstrated\nsignificant achievements in summarization tasks but struggle with factual\ninaccuracies, a critical issue in clinical NLP applications where errors could\nlead to serious consequences. To counter the high costs and limited\navailability of expert-annotated data for factual alignment, this study\nintroduces an innovative pipeline that utilizes >100B parameter GPT variants\nlike GPT-3.5 & GPT-4 to act as synthetic experts to generate high-quality\nsynthetics feedback aimed at enhancing factual consistency in clinical note\nsummarization. Our research primarily focuses on edit feedback generated by\nthese synthetic feedback experts without additional human annotations,\nmirroring and optimizing the practical scenario in which medical professionals\nrefine AI system outputs. Although such 100B+ parameter GPT variants have\nproven to demonstrate expertise in various clinical NLP tasks, such as the\nMedical Licensing Examination, there is scant research on their capacity to act\nas synthetic feedback experts and deliver expert-level edit feedback for\nimproving the generation quality of weaker (<10B parameter) LLMs like GPT-2\n(1.5B) & Llama 2 (7B) in clinical domain. So in this work, we leverage 100B+\nGPT variants to act as synthetic feedback experts offering expert-level edit\nfeedback, that is used to reduce hallucinations and align weaker (<10B\nparameter) LLMs with medical facts using two distinct alignment algorithms (DPO\n& SALT), endeavoring to narrow the divide between AI-generated content and\nfactual accuracy. This highlights the substantial potential of LLM-based\nsynthetic edits in enhancing the alignment of clinical factuality.", "published": "2024-02-21 16:33:22", "link": "http://arxiv.org/abs/2402.13919v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Vulnerable to Bait-and-Switch Attacks for\n  Generating Harmful Content", "abstract": "The risks derived from large language models (LLMs) generating deceptive and\ndamaging content have been the subject of considerable research, but even safe\ngenerations can lead to problematic downstream impacts. In our study, we shift\nthe focus to how even safe text coming from LLMs can be easily turned into\npotentially dangerous content through Bait-and-Switch attacks. In such attacks,\nthe user first prompts LLMs with safe questions and then employs a simple\nfind-and-replace post-hoc technique to manipulate the outputs into harmful\nnarratives. The alarming efficacy of this approach in generating toxic content\nhighlights a significant challenge in developing reliable safety guardrails for\nLLMs. In particular, we stress that focusing on the safety of the verbatim LLM\noutputs is insufficient and that we also need to consider post-hoc\ntransformations.", "published": "2024-02-21 16:46:36", "link": "http://arxiv.org/abs/2402.13926v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP\n  Guided Reinforcement Learning", "abstract": "Training image captioning models using teacher forcing results in very\ngeneric samples, whereas more distinctive captions can be very useful in\nretrieval applications or to produce alternative texts describing images for\naccessibility. Reinforcement Learning (RL) allows to use cross-modal retrieval\nsimilarity score between the generated caption and the input image as reward to\nguide the training, leading to more distinctive captions. Recent studies show\nthat pre-trained cross-modal retrieval models can be used to provide this\nreward, completely eliminating the need for reference captions. However, we\nargue in this paper that Ground Truth (GT) captions can still be useful in this\nRL framework. We propose a new image captioning model training strategy that\nmakes use of GT captions in different ways. Firstly, they can be used to train\na simple MLP discriminator that serves as a regularization to prevent reward\nhacking and ensures the fluency of generated captions, resulting in a textual\nGAN setup extended for multimodal inputs. Secondly, they can serve as\nadditional trajectories in the RL strategy, resulting in a teacher forcing loss\nweighted by the similarity of the GT to the image. This objective acts as an\nadditional learning signal grounded to the distribution of the GT captions.\nThirdly, they can serve as strong baselines when added to the pool of captions\nused to compute the proposed contrastive reward to reduce the variance of\ngradient estimate. Experiments on MS-COCO demonstrate the interest of the\nproposed training strategy to produce highly distinctive captions while\nmaintaining high writing quality.", "published": "2024-02-21 17:05:06", "link": "http://arxiv.org/abs/2402.13936v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Can Watermarks Survive Translation? On the Cross-lingual Consistency of\n  Text Watermark for Large Language Models", "abstract": "Text watermarking technology aims to tag and identify content produced by\nlarge language models (LLMs) to prevent misuse. In this study, we introduce the\nconcept of cross-lingual consistency in text watermarking, which assesses the\nability of text watermarks to maintain their effectiveness after being\ntranslated into other languages. Preliminary empirical results from two LLMs\nand three watermarking methods reveal that current text watermarking\ntechnologies lack consistency when texts are translated into various languages.\nBased on this observation, we propose a Cross-lingual Watermark Removal Attack\n(CWRA) to bypass watermarking by first obtaining a response from an LLM in a\npivot language, which is then translated into the target language. CWRA can\neffectively remove watermarks, decreasing the AUCs to a random-guessing level\nwithout performance loss. Furthermore, we analyze two key factors that\ncontribute to the cross-lingual consistency in text watermarking and propose\nX-SIR as a defense method against CWRA. Code: https://github.com/zwhe99/X-SIR.", "published": "2024-02-21 18:48:38", "link": "http://arxiv.org/abs/2402.14007v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FanOutQA: A Multi-Hop, Multi-Document Question Answering Benchmark for\n  Large Language Models", "abstract": "One type of question that is commonly found in day-to-day scenarios is\n``fan-out'' questions, complex multi-hop, multi-document reasoning questions\nthat require finding information about a large number of entities. However,\nthere exist few resources to evaluate this type of question-answering\ncapability among large language models. To evaluate complex reasoning in LLMs\nmore fully, we present FanOutQA, a high-quality dataset of fan-out\nquestion-answer pairs and human-annotated decompositions with English Wikipedia\nas the knowledge base. We formulate three benchmark settings across our dataset\nand benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B,\nfinding that contemporary models still have room to improve reasoning over\ninter-document dependencies in a long context. We provide our dataset and\nopen-source tools to run models to encourage evaluation at https://fanoutqa.com", "published": "2024-02-21 20:30:45", "link": "http://arxiv.org/abs/2402.14116v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Combining Language and Graph Models for Semi-structured Information\n  Extraction on the Web", "abstract": "Relation extraction is an efficient way of mining the extraordinary wealth of\nhuman knowledge on the Web. Existing methods rely on domain-specific training\ndata or produce noisy outputs. We focus here on extracting targeted relations\nfrom semi-structured web pages given only a short description of the relation.\nWe present GraphScholarBERT, an open-domain information extraction method based\non a joint graph and language model structure. GraphScholarBERT can generalize\nto previously unseen domains without additional data or training and produces\nonly clean extraction results matched to the search keyword. Experiments show\nthat GraphScholarBERT can improve extraction F1 scores by as much as 34.8\\%\ncompared to previous work in a zero-shot domain and zero-shot website setting.", "published": "2024-02-21 20:53:29", "link": "http://arxiv.org/abs/2402.14129v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Can Similarity-Based Domain-Ordering Reduce Catastrophic Forgetting for\n  Intent Recognition?", "abstract": "Task-oriented dialogue systems are expected to handle a constantly expanding\nset of intents and domains even after they have been deployed to support more\nand more functionalities. To live up to this expectation, it becomes critical\nto mitigate the catastrophic forgetting problem (CF) that occurs in continual\nlearning (CL) settings for a task such as intent recognition. While existing\ndialogue systems research has explored replay-based and regularization-based\nmethods to this end, the effect of domain ordering on the CL performance of\nintent recognition models remains unexplored. If understood well, domain\nordering has the potential to be an orthogonal technique that can be leveraged\nalongside existing techniques such as experience replay. Our work fills this\ngap by comparing the impact of three domain-ordering strategies (min-sum path,\nmax-sum path, random) on the CL performance of a generative intent recognition\nmodel. Our findings reveal that the min-sum path strategy outperforms the\nothers in reducing catastrophic forgetting when training on the 220M T5-Base\nmodel. However, this advantage diminishes with the larger 770M T5-Large model.\nThese results underscores the potential of domain ordering as a complementary\nstrategy for mitigating catastrophic forgetting in continually learning intent\nrecognition models, particularly in resource-constrained scenarios.", "published": "2024-02-21 22:30:57", "link": "http://arxiv.org/abs/2402.14155v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bangla AI: A Framework for Machine Translation Utilizing Large Language\n  Models for Ethnic Media", "abstract": "Ethnic media, which caters to diaspora communities in host nations, serves as\na vital platform for these communities to both produce content and access\ninformation. Rather than utilizing the language of the host nation, ethnic\nmedia delivers news in the language of the immigrant community. For instance,\nin the USA, Bangla ethnic media presents news in Bangla rather than English.\nThis research delves into the prospective integration of large language models\n(LLM) and multi-lingual machine translations (MMT) within the ethnic media\nindustry. It centers on the transformative potential of using LLM in MMT in\nvarious facets of news translation, searching, and categorization. The paper\noutlines a theoretical framework elucidating the integration of LLM and MMT\ninto the news searching and translation processes for ethnic media.\nAdditionally, it briefly addresses the potential ethical challenges associated\nwith the incorporation of LLM and MMT in news translation procedures.", "published": "2024-02-21 23:43:04", "link": "http://arxiv.org/abs/2402.14179v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Technical Report on the Pangram AI-Generated Text Classifier", "abstract": "We present Pangram Text, a transformer-based neural network trained to\ndistinguish text written by large language models from text written by humans.\nPangram Text outperforms zero-shot methods such as DetectGPT as well as leading\ncommercial AI detection tools with over 38 times lower error rates on a\ncomprehensive benchmark comprised of 10 text domains (student writing, creative\nwriting, scientific writing, books, encyclopedias, news, email, scientific\npapers, short-form Q&A) and 8 open- and closed-source large language models. We\npropose a training algorithm, hard negative mining with synthetic mirrors, that\nenables our classifier to achieve orders of magnitude lower false positive\nrates on high-data domains such as reviews. Finally, we show that Pangram Text\nis not biased against nonnative English speakers and generalizes to domains and\nmodels unseen during training.", "published": "2024-02-21 17:13:41", "link": "http://arxiv.org/abs/2402.14873v3", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Driving Generative Agents With Their Personality", "abstract": "This research explores the potential of Large Language Models (LLMs) to\nutilize psychometric values, specifically personality information, within the\ncontext of video game character development. Affective Computing (AC) systems\nquantify a Non-Player character's (NPC) psyche, and an LLM can take advantage\nof the system's information by using the values for prompt generation. The\nresearch shows an LLM can consistently represent a given personality profile,\nthereby enhancing the human-like characteristics of game characters.\nRepurposing a human examination, the International Personality Item Pool (IPIP)\nquestionnaire, to evaluate an LLM shows that the model can accurately generate\ncontent concerning the personality provided. Results show that the improvement\nof LLM, such as the latest GPT-4 model, can consistently utilize and interpret\na personality to represent behavior.", "published": "2024-02-21 21:29:57", "link": "http://arxiv.org/abs/2402.14879v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing", "abstract": "Automated essay scoring (AES) is a useful tool in English as a Foreign\nLanguage (EFL) writing education, offering real-time essay scores for students\nand instructors. However, previous AES models were trained on essays and scores\nirrelevant to the practical scenarios of EFL writing education and usually\nprovided a single holistic score due to the lack of appropriate datasets. In\nthis paper, we release DREsS, a large-scale, standard dataset for rubric-based\nautomated essay scoring. DREsS comprises three sub-datasets: DREsS_New,\nDREsS_Std., and DREsS_CASE. We collect DREsS_New, a real-classroom dataset with\n2.3K essays authored by EFL undergraduate students and scored by English\neducation experts. We also standardize existing rubric-based essay scoring\ndatasets as DREsS_Std. We suggest CASE, a corruption-based augmentation\nstrategy for essays, which generates 40.1K synthetic samples of DREsS_CASE and\nimproves the baseline results by 45.44%. DREsS will enable further research to\nprovide a more accurate and practical AES system for EFL writing education.", "published": "2024-02-21 09:12:16", "link": "http://arxiv.org/abs/2402.16733v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ED-Copilot: Reduce Emergency Department Wait Time with Language Model\n  Diagnostic Assistance", "abstract": "In the emergency department (ED), patients undergo triage and multiple\nlaboratory tests before diagnosis. This time-consuming process causes ED\ncrowding which impacts patient mortality, medical errors, staff burnout, etc.\nThis work proposes (time) cost-effective diagnostic assistance that leverages\nartificial intelligence systems to help ED clinicians make efficient and\naccurate diagnoses. In collaboration with ED clinicians, we use public patient\ndata to curate MIMIC-ED-Assist, a benchmark for AI systems to suggest\nlaboratory tests that minimize wait time while accurately predicting critical\noutcomes such as death. With MIMIC-ED-Assist, we develop ED-Copilot which\nsequentially suggests patient-specific laboratory tests and makes diagnostic\npredictions. ED-Copilot employs a pre-trained bio-medical language model to\nencode patient information and uses reinforcement learning to minimize ED wait\ntime and maximize prediction accuracy. On MIMIC-ED-Assist, ED-Copilot improves\nprediction accuracy over baselines while halving average wait time from four\nhours to two hours. ED-Copilot can also effectively personalize treatment\nrecommendations based on patient severity, further highlighting its potential\nas a diagnostic assistant. Since MIMIC-ED-Assist is a retrospective benchmark,\nED-Copilot is restricted to recommend only observed tests. We show ED-Copilot\nachieves competitive performance without this restriction as the maximum\nallowed time increases. Our code is available at\nhttps://github.com/cxcscmu/ED-Copilot.", "published": "2024-02-21 00:49:42", "link": "http://arxiv.org/abs/2402.13448v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based\n  on Twitter Data", "abstract": "Prior research on Twitter (now X) data has provided positive evidence of its\nutility in developing supplementary health surveillance systems. In this study,\nwe present a new framework to surveil public health, focusing on mental health\n(MH) outcomes. We hypothesize that locally posted tweets are indicative of\nlocal MH outcomes and collect tweets posted from 765 neighborhoods (census\nblock groups) in the USA. We pair these tweets from each neighborhood with the\ncorresponding MH outcome reported by the Center for Disease Control (CDC) to\ncreate a benchmark dataset, LocalTweets. With LocalTweets, we present the first\npopulation-level evaluation task for Twitter-based MH surveillance systems. We\nthen develop an efficient and effective method, LocalHealth, for predicting MH\noutcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the\nhighest F1-score and accuracy of 0.7429 and 79.78\\%, respectively, a 59\\%\nimprovement in F1-score over the GPT3.5 in zero-shot setting. We also utilize\nLocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods,\nachieving an F1-score of 0.7291. Our work suggests that Twitter data can be\neffectively leveraged to simulate neighborhood-level MH outcomes.", "published": "2024-02-21 01:11:28", "link": "http://arxiv.org/abs/2402.13452v2", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Learning to Poison Large Language Models During Instruction Tuning", "abstract": "The advent of Large Language Models (LLMs) has marked significant\nachievements in language processing and reasoning capabilities. Despite their\nadvancements, LLMs face vulnerabilities to data poisoning attacks, where\nadversaries insert backdoor triggers into training data to manipulate outputs\nfor malicious purposes. This work further identifies additional security risks\nin LLMs by designing a new data poisoning attack tailored to exploit the\ninstruction tuning process. We propose a novel gradient-guided backdoor trigger\nlearning (GBTL) algorithm to identify adversarial triggers efficiently,\nensuring an evasion of detection by conventional defenses while maintaining\ncontent integrity. Through experimental validation across various tasks,\nincluding sentiment analysis, domain generation, and question answering, our\npoisoning strategy demonstrates a high success rate in compromising various\nLLMs' outputs. We further propose two defense strategies against data poisoning\nattacks, including in-context learning (ICL) and continuous learning (CL),\nwhich effectively rectify the behavior of LLMs and significantly reduce the\ndecline in performance. Our work highlights the significant security risks\npresent during the instruction tuning of LLMs and emphasizes the necessity of\nsafeguarding LLMs against data poisoning attacks.", "published": "2024-02-21 01:30:03", "link": "http://arxiv.org/abs/2402.13459v2", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks", "abstract": "Despite large successes of recent language models on diverse tasks, they\nsuffer from severe performance degeneration in low-resource settings with\nlimited training data available. Many existing works tackle this problem by\ngenerating synthetic data from the training data and then training models on\nthem, recently using Large Language Models (LLMs). However, in low-resource\nsettings, the amount of seed data samples to use for data augmentation is very\nsmall, which makes generated samples suboptimal and less diverse. To tackle\nthis challenge, we propose a novel method that augments training data by\nincorporating a wealth of examples from other datasets, along with the given\ntraining data. Specifically, we first retrieve the relevant instances from\nother datasets, such as their input-output pairs or contexts, based on their\nsimilarities with the given seed data, and then prompt LLMs to generate new\nsamples with the contextual information within and across the original and\nretrieved samples. This approach can ensure that the generated data is not only\nrelevant but also more diverse than what could be achieved using the limited\nseed data alone. We validate our proposed Retrieval-Augmented Data Augmentation\n(RADA) framework on multiple datasets under low-resource settings of training\nand test-time data augmentation scenarios, on which it outperforms existing\nLLM-powered data augmentation baselines.", "published": "2024-02-21 02:45:46", "link": "http://arxiv.org/abs/2402.13482v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "From Self-Attention to Markov Models: Unveiling the Dynamics of\n  Generative Transformers", "abstract": "Modern language models rely on the transformer architecture and attention\nmechanism to perform language understanding and text generation. In this work,\nwe study learning a 1-layer self-attention model from a set of prompts and\nassociated output data sampled from the model. We first establish a precise\nmapping between the self-attention mechanism and Markov models: Inputting a\nprompt to the model samples the output token according to a context-conditioned\nMarkov chain (CCMC) which weights the transition matrix of a base Markov chain.\nAdditionally, incorporating positional encoding results in position-dependent\nscaling of the transition probabilities. Building on this formalism, we develop\nidentifiability/coverage conditions for the prompt distribution that guarantee\nconsistent estimation and establish sample complexity guarantees under IID\nsamples. Finally, we study the problem of learning from a single output\ntrajectory generated from an initial prompt. We characterize an intriguing\nwinner-takes-all phenomenon where the generative process implemented by\nself-attention collapses into sampling a limited subset of tokens due to its\nnon-mixing nature. This provides a mathematical explanation to the tendency of\nmodern LLMs to generate repetitive text. In summary, the equivalence to CCMC\nprovides a simple but powerful framework to study self-attention and its\nproperties.", "published": "2024-02-21 03:51:34", "link": "http://arxiv.org/abs/2402.13512v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity\n  within Large Language Models", "abstract": "Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, activation sparsity has been\nproven a promising paradigm to boost model inference efficiency. Nevertheless,\nmost large language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces a simple and effective sparsification method named \"ProSparse\"\nto push LLMs for higher activation sparsity while maintaining comparable\nperformance. Specifically, after substituting the activation function of LLMs\nwith ReLU, ProSparse adopts progressive sparsity regularization with a factor\nsmoothly increasing along the multi-stage sine curves. This can enhance\nactivation sparsity and mitigate performance degradation by avoiding radical\nshifts in activation distributions. With ProSparse, we obtain high sparsity of\n89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size\nMiniCPM-1B, respectively, achieving comparable performance to their original\nSwish-activated versions. These present the most sparsely activated models\namong open-source LLaMA versions and competitive end-size models, considerably\nsurpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference\nacceleration experiments further demonstrate the significant practical\nacceleration potential of LLMs with higher activation sparsity, obtaining up to\n4.52$\\times$ inference speedup.", "published": "2024-02-21 03:58:49", "link": "http://arxiv.org/abs/2402.13516v7", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Infrastructure Ombudsman: Mining Future Failure Concerns from Structural\n  Disaster Response", "abstract": "Current research concentrates on studying discussions on social media related\nto structural failures to improve disaster response strategies. However,\ndetecting social web posts discussing concerns about anticipatory failures is\nunder-explored. If such concerns are channeled to the appropriate authorities,\nit can aid in the prevention and mitigation of potential infrastructural\nfailures. In this paper, we develop an infrastructure ombudsman -- that\nautomatically detects specific infrastructure concerns. Our work considers\nseveral recent structural failures in the US. We present a first-of-its-kind\ndataset of 2,662 social web instances for this novel task mined from Reddit and\nYouTube.", "published": "2024-02-21 04:55:03", "link": "http://arxiv.org/abs/2402.13528v2", "categories": ["cs.CY", "cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CY"}
{"title": "FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models\n  for Financial Applications with High-Performance Computing", "abstract": "Large language models (LLMs) are computationally intensive. The computation\nworkload and the memory footprint grow quadratically with the dimension (layer\nwidth). Most of LLMs' parameters come from the linear layers of the transformer\nstructure and are highly redundant. These linear layers contribute more than\n80% of the computation workload and 99% of the model size. To pretrain and\nfinetune LLMs efficiently, there are three major challenges to address: 1)\nreducing redundancy of the linear layers; 2) reducing GPU memory footprint; 3)\nimproving GPU utilization when using distributed training. Prior methods, such\nas LoRA and QLoRA, utilized low-rank matrices and quantization to reduce the\nnumber of trainable parameters and model size, respectively. However, the\nresulting model still consumes a large amount of GPU memory. In this paper, we\npresent high-performance GPU-based methods that exploit low-rank structures to\npretrain and finetune LLMs for financial applications. We replace one\nconventional linear layer of the transformer structure with two narrower linear\nlayers, which allows us to reduce the number of parameters by several orders of\nmagnitude. By quantizing the parameters into low precision (8-bit and 4-bit),\nthe memory consumption of the resulting model is further reduced. Compared with\nexisting LLMs, our methods achieve a speedup of 1.3X and a model compression\nratio of 2.64X for pretaining without accuracy drop. For finetuning, our\nmethods achieve an average accuracy increase of 6.3% and 24.0% in general tasks\nand financial tasks, respectively, and GPU memory consumption ratio of 6.3X.\nThe sizes of our models are smaller than 0.59 GB, allowing inference on a\nsmartphone.", "published": "2024-02-21 05:03:17", "link": "http://arxiv.org/abs/2402.13533v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "ARL2: Aligning Retrievers for Black-box Large Language Models via\n  Self-guided Adaptive Relevance Labeling", "abstract": "Retrieval-augmented generation enhances large language models (LLMs) by\nincorporating relevant information from external knowledge sources. This\nenables LLMs to adapt to specific domains and mitigate hallucinations in\nknowledge-intensive tasks. However, existing retrievers are often misaligned\nwith LLMs due to their separate training processes and the black-box nature of\nLLMs. To address this challenge, we propose ARL2, a retriever learning\ntechnique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and\nscore relevant evidence, enabling learning the retriever from robust LLM\nsupervision. Furthermore, ARL2 uses an adaptive self-training strategy for\ncurating high-quality and diverse relevance data, which can effectively reduce\nthe annotation cost. Extensive experiments demonstrate the effectiveness of\nARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared\nto the state-of-the-art methods. Additionally, ARL2 exhibits robust transfer\nlearning capabilities and strong zero-shot generalization abilities. Our code\nwill be published at \\url{https://github.com/zhanglingxi-cs/ARL2}.", "published": "2024-02-21 05:41:34", "link": "http://arxiv.org/abs/2402.13542v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "User-LLM: Efficient LLM Contextualization with User Embeddings", "abstract": "Large language models (LLMs) have achieved remarkable success across various\ndomains, but effectively incorporating complex and potentially noisy user\ntimeline data into LLMs remains a challenge. Current approaches often involve\ntranslating user timelines into text descriptions before feeding them to LLMs,\nwhich can be inefficient and may not fully capture the nuances of user\nbehavior. Inspired by how LLMs are effectively integrated with images through\ndirect embeddings, we propose User-LLM, a novel framework that leverages user\nembeddings to directly contextualize LLMs with user history interactions. These\nembeddings, generated by a user encoder pretrained using self-supervised\nlearning on diverse user interactions, capture latent user behaviors and\ninterests as well as their evolution over time. We integrate these user\nembeddings with LLMs through cross-attention, enabling LLMs to dynamically\nadapt their responses based on the context of a user's past actions and\npreferences.\n  Our approach achieves significant efficiency gains by representing user\ntimelines directly as embeddings, leading to substantial inference speedups of\nup to 78.1X. Comprehensive experiments on MovieLens, Amazon Review, and Google\nLocal Review datasets demonstrate that User-LLM outperforms text-prompt-based\ncontextualization on tasks requiring deep user understanding, with improvements\nof up to 16.33%, particularly excelling on long sequences that capture subtle\nshifts in user behavior. Furthermore, the incorporation of Perceiver layers\nstreamlines the integration between user encoders and LLMs, yielding additional\ncomputational savings.", "published": "2024-02-21 08:03:27", "link": "http://arxiv.org/abs/2402.13598v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Data-driven Discovery with Large Generative Models", "abstract": "With the accumulation of data at an unprecedented rate, its potential to fuel\nscientific discovery is growing exponentially. This position paper urges the\nMachine Learning (ML) community to exploit the capabilities of large generative\nmodels (LGMs) to develop automated systems for end-to-end data-driven discovery\n-- a paradigm encompassing the search and verification of hypotheses purely\nfrom a set of provided datasets, without the need for additional data\ncollection or physical experiments. We first outline several desiderata for an\nideal data-driven discovery system. Then, through DATAVOYAGER, a\nproof-of-concept utilizing GPT-4, we demonstrate how LGMs fulfill several of\nthese desiderata -- a feat previously unattainable -- while also highlighting\nimportant limitations in the current system that open up opportunities for\nnovel ML research. We contend that achieving accurate, reliable, and robust\nend-to-end discovery systems solely through the current capabilities of LGMs is\nchallenging. We instead advocate for fail-proof tool integration, along with\nactive user moderation through feedback mechanisms, to foster data-driven\nscientific discoveries with efficiency and reproducibility.", "published": "2024-02-21 08:26:43", "link": "http://arxiv.org/abs/2402.13610v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Unified Framework and Dataset for Assessing Societal Bias in\n  Vision-Language Models", "abstract": "Vision-language models (VLMs) have gained widespread adoption in both\nindustry and academia. In this study, we propose a unified framework for\nsystematically evaluating gender, race, and age biases in VLMs with respect to\nprofessions. Our evaluation encompasses all supported inference modes of the\nrecent VLMs, including image-to-text, text-to-text, text-to-image, and\nimage-to-image. Additionally, we propose an automated pipeline to generate\nhigh-quality synthetic datasets that intentionally conceal gender, race, and\nage information across different professional domains, both in generated text\nand images. The dataset includes action-based descriptions of each profession\nand serves as a benchmark for evaluating societal biases in vision-language\nmodels (VLMs). In our comparative analysis of widely used VLMs, we have\nidentified that varying input-output modalities lead to discernible differences\nin bias magnitudes and directions. Additionally, we find that VLM models\nexhibit distinct biases across different bias attributes we investigated. We\nhope our work will help guide future progress in improving VLMs to learn\nsocially unbiased representations. We will release our data and code.", "published": "2024-02-21 09:17:51", "link": "http://arxiv.org/abs/2402.13636v2", "categories": ["cs.CV", "cs.CL", "cs.CY"], "primary_category": "cs.CV"}
{"title": "Breaking the Barrier: Utilizing Large Language Models for Industrial\n  Recommendation Systems through an Inferential Knowledge Graph", "abstract": "Recommendation systems are widely used in e-commerce websites and online\nplatforms to address information overload. However, existing systems primarily\nrely on historical data and user feedback, making it difficult to capture user\nintent transitions. Recently, Knowledge Base (KB)-based models are proposed to\nincorporate expert knowledge, but it struggle to adapt to new items and the\nevolving e-commerce environment. To address these challenges, we propose a\nnovel Large Language Model based Complementary Knowledge Enhanced\nRecommendation System (LLM-KERec). It introduces an entity extractor that\nextracts unified concept terms from item and user information. To provide\ncost-effective and reliable prior knowledge, entity pairs are generated based\non entity popularity and specific strategies. The large language model\ndetermines complementary relationships in each entity pair, constructing a\ncomplementary knowledge graph. Furthermore, a new complementary recall module\nand an Entity-Entity-Item (E-E-I) weight decision model refine the scoring of\nthe ranking model using real complementary exposure-click samples. Extensive\nexperiments conducted on three industry datasets demonstrate the significant\nperformance improvement of our model compared to existing approaches.\nAdditionally, detailed analysis shows that LLM-KERec enhances users' enthusiasm\nfor consumption by recommending complementary items. In summary, LLM-KERec\naddresses the limitations of traditional recommendation systems by\nincorporating complementary knowledge and utilizing a large language model to\ncapture user intent transitions, adapt to new items, and enhance recommendation\nefficiency in the evolving e-commerce landscape.", "published": "2024-02-21 12:22:01", "link": "http://arxiv.org/abs/2402.13750v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Large Language Models are Advanced Anonymizers", "abstract": "Recent privacy research on large language models (LLMs) has shown that they\nachieve near-human-level performance at inferring personal data from online\ntexts. With ever-increasing model capabilities, existing text anonymization\nmethods are currently lacking behind regulatory requirements and adversarial\nthreats. In this work, we take two steps to bridge this gap: First, we present\na new setting for evaluating anonymization in the face of adversarial LLM\ninferences, allowing for a natural measurement of anonymization performance\nwhile remedying some of the shortcomings of previous metrics. Then, within this\nsetting, we develop a novel LLM-based adversarial anonymization framework\nleveraging the strong inferential capabilities of LLMs to inform our\nanonymization procedure. We conduct a comprehensive experimental evaluation of\nadversarial anonymization across 13 LLMs on real-world and synthetic online\ntexts, comparing it against multiple baselines and industry-grade anonymizers.\nOur evaluation shows that adversarial anonymization outperforms current\ncommercial anonymizers both in terms of the resulting utility and privacy. We\nsupport our findings with a human study (n=50) highlighting a strong and\nconsistent human preference for LLM-anonymized texts.", "published": "2024-02-21 14:44:00", "link": "http://arxiv.org/abs/2402.13846v2", "categories": ["cs.AI", "cs.CL", "cs.CR", "I.2.7"], "primary_category": "cs.AI"}
{"title": "Science Checker Reloaded: A Bidirectional Paradigm for Transparency and\n  Logical Reasoning", "abstract": "Information retrieval is a rapidly evolving field. However it still faces\nsignificant limitations in the scientific and industrial vast amounts of\ninformation, such as semantic divergence and vocabulary gaps in sparse\nretrieval, low precision and lack of interpretability in semantic search, or\nhallucination and outdated information in generative models. In this paper, we\nintroduce a two-block approach to tackle these hurdles for long documents. The\nfirst block enhances language understanding in sparse retrieval by query\nexpansion to retrieve relevant documents. The second block deepens the result\nby providing comprehensive and informative answers to the complex question\nusing only the information spread in the long document, enabling bidirectional\nengagement. At various stages of the pipeline, intermediate results are\npresented to users to facilitate understanding of the system's reasoning. We\nbelieve this bidirectional approach brings significant advancements in terms of\ntransparency, logical thinking, and comprehensive understanding in the field of\nscientific information retrieval.", "published": "2024-02-21 16:09:25", "link": "http://arxiv.org/abs/2402.13897v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "H.3.1; H.3.3; I.7; K.4"], "primary_category": "cs.IR"}
{"title": "Do Efficient Transformers Really Save Computation?", "abstract": "As transformer-based language models are trained on increasingly large\ndatasets and with vast numbers of parameters, finding more efficient\nalternatives to the standard Transformer has become very valuable. While many\nefficient Transformers and Transformer alternatives have been proposed, none\nprovide theoretical guarantees that they are a suitable replacement for the\nstandard Transformer. This makes it challenging to identify when to use a\nspecific model and what directions to prioritize for further investigation. In\nthis paper, we aim to understand the capabilities and limitations of efficient\nTransformers, specifically the Sparse Transformer and the Linear Transformer.\nWe focus on their reasoning capability as exhibited by Chain-of-Thought (CoT)\nprompts and follow previous works to model them as Dynamic Programming (DP)\nproblems. Our results show that while these models are expressive enough to\nsolve general DP tasks, contrary to expectations, they require a model size\nthat scales with the problem size. Nonetheless, we identify a class of DP\nproblems for which these models can be more efficient than the standard\nTransformer. We confirm our theoretical results through experiments on\nrepresentative DP tasks, adding to the understanding of efficient Transformers'\npractical strengths and weaknesses.", "published": "2024-02-21 17:00:56", "link": "http://arxiv.org/abs/2402.13934v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Coercing LLMs to do and reveal (almost) anything", "abstract": "It has recently been shown that adversarial attacks on large language models\n(LLMs) can \"jailbreak\" the model into making harmful statements. In this work,\nwe argue that the spectrum of adversarial attacks on LLMs is much larger than\nmerely jailbreaking. We provide a broad overview of possible attack surfaces\nand attack goals. Based on a series of concrete examples, we discuss,\ncategorize and systematize attacks that coerce varied unintended behaviors,\nsuch as misdirection, model control, denial-of-service, or data extraction.\n  We analyze these attacks in controlled experiments, and find that many of\nthem stem from the practice of pre-training LLMs with coding capabilities, as\nwell as the continued existence of strange \"glitch\" tokens in common LLM\nvocabularies that should be removed for security reasons.", "published": "2024-02-21 18:59:13", "link": "http://arxiv.org/abs/2402.14020v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Improving Language Understanding from Screenshots", "abstract": "An emerging family of language models (LMs), capable of processing both text\nand images within a single visual view, has the promise to unlock complex tasks\nsuch as chart understanding and UI navigation. We refer to these models as\nscreenshot language models. Despite their appeal, existing screenshot LMs\nsubstantially lag behind text-only models on language understanding tasks. To\nclose this gap, we adopt a simplified setting where the model inputs are\nplain-text-rendered screenshots, and we focus on improving the text ability of\nscreenshot LMs. We propose a novel Patch-and-Text Prediction (PTP) objective,\nwhich masks and recovers both image patches of screenshots and text within\nscreenshots. We also conduct extensive ablation studies on masking rates and\npatch sizes, as well as designs for improving training stability. Our\npre-trained model, while solely taking visual inputs, achieves comparable\nperformance with BERT on 6 out of 8 GLUE tasks (within 2%) and improves up to\n8% over prior work. Additionally, we extend PTP to train autoregressive\nscreenshot LMs and demonstrate its effectiveness--our models can significantly\nreduce perplexity by utilizing the screenshot context. Together, we hope our\nfindings can inspire future research on developing powerful screenshot LMs and\nextending their reach to broader applications.", "published": "2024-02-21 19:01:03", "link": "http://arxiv.org/abs/2402.14073v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LexC-Gen: Generating Data for Extremely Low-Resource Languages with\n  Large Language Models and Bilingual Lexicons", "abstract": "Data scarcity in low-resource languages can be addressed with word-to-word\ntranslations from labeled task data in high-resource languages using bilingual\nlexicons. However, bilingual lexicons often have limited lexical overlap with\ntask data, which results in poor translation coverage and lexicon utilization.\nWe propose lexicon-conditioned data generation LexC-Gen, a method that\ngenerates low-resource-language classification task data at scale.\nSpecifically, LexC-Gen first uses high-resource-language words from bilingual\nlexicons to generate lexicon-compatible task data, and then it translates them\ninto low-resource languages with bilingual lexicons via word translation.\nAcross 17 extremely low-resource languages, LexC-Gen generated data is\ncompetitive with expert-translated gold data, and yields on average 5.6 and 8.9\npoints improvement over existing lexicon-based word translation methods on\nsentiment analysis and topic classification tasks respectively. Through\nablation study, we show that conditioning on bilingual lexicons is the key\ncomponent of LexC-Gen. LexC-Gen serves as a potential solution to close the\nperformance gap between open-source multilingual models, such as BLOOMZ and\nAya-101, and state-of-the-art commercial models like GPT-4o on\nlow-resource-language tasks.", "published": "2024-02-21 19:20:06", "link": "http://arxiv.org/abs/2402.14086v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BIRCO: A Benchmark of Information Retrieval Tasks with Complex\n  Objectives", "abstract": "We present the Benchmark of Information Retrieval (IR) tasks with Complex\nObjectives (BIRCO). BIRCO evaluates the ability of IR systems to retrieve\ndocuments given multi-faceted user objectives. The benchmark's complexity and\ncompact size make it suitable for evaluating large language model (LLM)-based\ninformation retrieval systems. We present a modular framework for investigating\nfactors that may influence LLM performance on retrieval tasks, and identify a\nsimple baseline model which matches or outperforms existing approaches and more\ncomplex alternatives. No approach achieves satisfactory performance on all\nbenchmark tasks, suggesting that stronger models and new retrieval protocols\nare necessary to address complex user needs.", "published": "2024-02-21 22:22:30", "link": "http://arxiv.org/abs/2402.14151v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "MM-Soc: Benchmarking Multimodal Large Language Models in Social Media\n  Platforms", "abstract": "Social media platforms are hubs for multimodal information exchange,\nencompassing text, images, and videos, making it challenging for machines to\ncomprehend the information or emotions associated with interactions in online\nspaces. Multimodal Large Language Models (MLLMs) have emerged as a promising\nsolution to these challenges, yet they struggle to accurately interpret human\nemotions and complex content such as misinformation. This paper introduces\nMM-Soc, a comprehensive benchmark designed to evaluate MLLMs' understanding of\nmultimodal social media content. MM-Soc compiles prominent multimodal datasets\nand incorporates a novel large-scale YouTube tagging dataset, targeting a range\nof tasks from misinformation detection, hate speech detection, and social\ncontext generation. Through our exhaustive evaluation on ten size-variants of\nfour open-source MLLMs, we have identified significant performance disparities,\nhighlighting the need for advancements in models' social understanding\ncapabilities. Our analysis reveals that, in a zero-shot setting, various types\nof MLLMs generally exhibit difficulties in handling social media tasks.\nHowever, MLLMs demonstrate performance improvements post fine-tuning,\nsuggesting potential pathways for improvement. Our code and data are available\nat https://github.com/claws-lab/MMSoc.git.", "published": "2024-02-21 22:27:40", "link": "http://arxiv.org/abs/2402.14154v3", "categories": ["cs.CL", "cs.CV", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Ranking Large Language Models without Ground Truth", "abstract": "Evaluation and ranking of large language models (LLMs) has become an\nimportant problem with the proliferation of these models and their impact.\nEvaluation methods either require human responses which are expensive to\nacquire or use pairs of LLMs to evaluate each other which can be unreliable. In\nthis paper, we provide a novel perspective where, given a dataset of prompts\n(viz. questions, instructions, etc.) and a set of LLMs, we rank them without\naccess to any ground truth or reference responses. Inspired by real life where\nboth an expert and a knowledgeable person can identify a novice our main idea\nis to consider triplets of models, where each one of them evaluates the other\ntwo, correctly identifying the worst model in the triplet with high\nprobability. We also analyze our idea and provide sufficient conditions for it\nto succeed. Applying this idea repeatedly, we propose two methods to rank LLMs.\nIn experiments on different generative tasks (summarization, multiple-choice,\nand dialog), our methods reliably recover close to true rankings without\nreference data. This points to a viable low-resource mechanism for practical\nuse.", "published": "2024-02-21 00:49:43", "link": "http://arxiv.org/abs/2402.14860v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dynamic Evaluation of Large Language Models by Meta Probing Agents", "abstract": "Evaluation of large language models (LLMs) has raised great concerns in the\ncommunity due to the issue of data contamination. Existing work designed\nevaluation protocols using well-defined algorithms for specific tasks, which\ncannot be easily extended to diverse scenarios. Moreover, current evaluation\nbenchmarks can only provide the overall benchmark results and cannot support a\nfine-grained and multifaceted analysis of LLMs' abilities. In this paper, we\npropose meta probing agents (MPA), a general dynamic evaluation protocol\ninspired by psychometrics to evaluate LLMs. MPA is the key component of DyVal\n2, which naturally extends the previous DyVal~\\citep{zhu2023dyval}. MPA designs\nthe probing and judging agents to automatically transform an original\nevaluation problem into a new one following psychometric theory on three basic\ncognitive abilities: language understanding, problem solving, and domain\nknowledge. These basic abilities are also dynamically configurable, allowing\nmultifaceted analysis. We conducted extensive evaluations using MPA and found\nthat most LLMs achieve poorer performance, indicating room for improvement. Our\nmultifaceted analysis demonstrated the strong correlation between the basic\nabilities and an implicit Matthew effect on model size, i.e., larger models\npossess stronger correlations of the abilities. MPA can also be used as a data\naugmentation approach to enhance LLMs. Code is available at:\nhttps://github.com/microsoft/promptbench.", "published": "2024-02-21 06:46:34", "link": "http://arxiv.org/abs/2402.14865v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "APTQ: Attention-aware Post-Training Mixed-Precision Quantization for\n  Large Language Models", "abstract": "Large Language Models (LLMs) have greatly advanced the natural language\nprocessing paradigm. However, the high computational load and huge model sizes\npose a grand challenge for deployment on edge devices. To this end, we propose\nAPTQ (Attention-aware Post-Training Mixed-Precision Quantization) for LLMs,\nwhich considers not only the second-order information of each layer's weights,\nbut also, for the first time, the nonlinear effect of attention outputs on the\nentire model. We leverage the Hessian trace as a sensitivity metric for\nmixed-precision quantization, ensuring an informed precision reduction that\nretains model performance. Experiments show APTQ surpasses previous\nquantization methods, achieving an average of 4 bit width a 5.22 perplexity\nnearly equivalent to full precision in the C4 dataset. In addition, APTQ\nattains state-of-the-art zero-shot accuracy of 68.24\\% and 70.48\\% at an\naverage bitwidth of 3.8 in LLaMa-7B and LLaMa-13B, respectively, demonstrating\nits effectiveness to produce high-quality quantized LLMs.", "published": "2024-02-21 07:45:22", "link": "http://arxiv.org/abs/2402.14866v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Effects of term weighting approach with and without stop words removing\n  on Arabic text classification", "abstract": "Classifying text is a method for categorizing documents into pre-established\ngroups. Text documents must be prepared and represented in a way that is\nappropriate for the algorithms used for data mining prior to classification. As\na result, a number of term weighting strategies have been created in the\nliterature to enhance text categorization algorithms' functionality. This study\ncompares the effects of Binary and Term frequency weighting feature\nmethodologies on the text's classification method when stop words are\neliminated once and when they are not. In recognition of assessing the effects\nof prior weighting of features approaches on classification results in terms of\naccuracy, recall, precision, and F-measure values, we used an Arabic data set\nmade up of 322 documents divided into six main topics (agriculture, economy,\nhealth, politics, science, and sport), each of which contains 50 documents,\nwith the exception of the health category, which contains 61 documents. The\nresults demonstrate that for all metrics, the term frequency feature weighting\napproach with stop word removal outperforms the binary approach, while for\naccuracy, recall, and F-Measure, the binary approach outperforms the TF\napproach without stop word removal. However, for precision, the two approaches\nproduce results that are very similar. Additionally, it is clear from the data\nthat, using the same phrase weighting approach, stop word removing increases\nclassification accuracy.", "published": "2024-02-21 11:31:04", "link": "http://arxiv.org/abs/2402.14867v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM Based Multi-Agent Generation of Semi-structured Documents from\n  Semantic Templates in the Public Administration Domain", "abstract": "In the last years' digitalization process, the creation and management of\ndocuments in various domains, particularly in Public Administration (PA), have\nbecome increasingly complex and diverse. This complexity arises from the need\nto handle a wide range of document types, often characterized by\nsemi-structured forms. Semi-structured documents present a fixed set of data\nwithout a fixed format. As a consequence, a template-based solution cannot be\nused, as understanding a document requires the extraction of the data\nstructure. The recent introduction of Large Language Models (LLMs) has enabled\nthe creation of customized text output satisfying user requests. In this work,\nwe propose a novel approach that combines the LLMs with prompt engineering and\nmulti-agent systems for generating new documents compliant with a desired\nstructure. The main contribution of this work concerns replacing the commonly\nused manual prompting with a task description generated by semantic retrieval\nfrom an LLM. The potential of this approach is demonstrated through a series of\nexperiments and case studies, showcasing its effectiveness in real-world PA\nscenarios.", "published": "2024-02-21 13:54:53", "link": "http://arxiv.org/abs/2402.14871v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts\n  Against Open-source LLMs", "abstract": "Large Language Models (LLMs), used in creative writing, code generation, and\ntranslation, generate text based on input sequences but are vulnerable to\njailbreak attacks, where crafted prompts induce harmful outputs. Most jailbreak\nprompt methods use a combination of jailbreak templates followed by questions\nto ask to create jailbreak prompts. However, existing jailbreak prompt designs\ngenerally suffer from excessive semantic differences, resulting in an inability\nto resist defenses that use simple semantic metrics as thresholds. Jailbreak\nprompts are semantically more varied than the original questions used for\nqueries. In this paper, we introduce a Semantic Mirror Jailbreak (SMJ) approach\nthat bypasses LLMs by generating jailbreak prompts that are semantically\nsimilar to the original question. We model the search for jailbreak prompts\nthat satisfy both semantic similarity and jailbreak validity as a\nmulti-objective optimization problem and employ a standardized set of genetic\nalgorithms for generating eligible prompts. Compared to the baseline\nAutoDAN-GA, SMJ achieves attack success rates (ASR) that are at most 35.4%\nhigher without ONION defense and 85.2% higher with ONION defense. SMJ's better\nperformance in all three semantic meaningfulness metrics of Jailbreak Prompt,\nSimilarity, and Outlier, also means that SMJ is resistant to defenses that use\nthose metrics as thresholds.", "published": "2024-02-21 15:13:50", "link": "http://arxiv.org/abs/2402.14872v2", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Distillation Contrastive Decoding: Improving LLMs Reasoning with\n  Contrastive Decoding and Distillation", "abstract": "We propose a straightforward approach called Distillation Contrastive\nDecoding (DCD) to enhance the reasoning capabilities of Large Language Models\n(LLMs) during inference. In contrast to previous approaches that relied on\nsmaller amateur models or analysis of hidden state differences, DCD employs\nContrastive Chain-of-thought Prompting and advanced distillation techniques,\nincluding Dropout and Quantization. This approach effectively addresses the\nlimitations of Contrastive Decoding (CD), which typically requires both an\nexpert and an amateur model, thus increasing computational resource demands. By\nintegrating contrastive prompts with distillation, DCD obviates the need for an\namateur model and reduces memory usage. Our evaluations demonstrate that DCD\nsignificantly enhances LLM performance across a range of reasoning benchmarks,\nsurpassing both CD and existing methods in the GSM8K and StrategyQA datasets.", "published": "2024-02-21 17:20:38", "link": "http://arxiv.org/abs/2402.14874v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What's in a Name? Auditing Large Language Models for Race and Gender\n  Bias", "abstract": "We employ an audit design to investigate biases in state-of-the-art large\nlanguage models, including GPT-4. In our study, we prompt the models for advice\ninvolving a named individual across a variety of scenarios, such as during car\npurchase negotiations or election outcome predictions. We find that the advice\nsystematically disadvantages names that are commonly associated with racial\nminorities and women. Names associated with Black women receive the least\nadvantageous outcomes. The biases are consistent across 42 prompt templates and\nseveral models, indicating a systemic issue rather than isolated incidents.\nWhile providing numerical, decision-relevant anchors in the prompt can\nsuccessfully counteract the biases, qualitative details have inconsistent\neffects and may even increase disparities. Our findings underscore the\nimportance of conducting audits at the point of LLM deployment and\nimplementation to mitigate their potential for harm against marginalized\ncommunities.", "published": "2024-02-21 18:25:25", "link": "http://arxiv.org/abs/2402.14875v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic Histograms: Leveraging Language Models for Text Dataset\n  Exploration", "abstract": "Making sense of unstructured text datasets is perennially difficult, yet\nincreasingly relevant with Large Language Models. Data workers often rely on\ndataset summaries, especially distributions of various derived features. Some\nfeatures, like toxicity or topics, are relevant to many datasets, but many\ninteresting features are domain specific: instruments and genres for a music\ndataset, or diseases and symptoms for a medical dataset. Accordingly, data\nworkers often run custom analyses for each dataset, which is cumbersome and\ndifficult. We present AutoHistograms, a visualization tool leveragingLLMs.\nAutoHistograms automatically identifies relevant features, visualizes them with\nhistograms, and allows the user to interactively query the dataset for\ncategories of entities and create new histograms. In a user study with 10 data\nworkers (n=10), we observe that participants can quickly identify insights and\nexplore the data using AutoHistograms, and conceptualize a broad range of\napplicable use cases. Together, this tool and user study contributeto the\ngrowing field of LLM-assisted sensemaking tools.", "published": "2024-02-21 22:29:16", "link": "http://arxiv.org/abs/2402.14880v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "A Study on the Vulnerability of Test Questions against ChatGPT-based\n  Cheating", "abstract": "ChatGPT is a chatbot that can answer text prompts fairly accurately, even\nperforming very well on postgraduate-level questions. Many educators have found\nthat their take-home or remote tests and exams are vulnerable to ChatGPT-based\ncheating because students may directly use answers provided by tools like\nChatGPT. In this paper, we try to provide an answer to an important question:\nhow well ChatGPT can answer test questions and how we can detect whether the\nquestions of a test can be answered correctly by ChatGPT. We generated\nChatGPT's responses to the MedMCQA dataset, which contains over 10,000 medical\nschool entrance exam questions. We analyzed the responses and uncovered certain\ntypes of questions ChatGPT answers more inaccurately than others. In addition,\nwe have created a basic natural language processing model to single out the\nmost vulnerable questions to ChatGPT in a collection of questions or a sample\nexam. Our tool can be used by test-makers to avoid ChatGPT-vulnerable test\nquestions.", "published": "2024-02-21 23:51:06", "link": "http://arxiv.org/abs/2402.14881v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7"], "primary_category": "cs.CL"}
{"title": "PCA-Bench: Evaluating Multimodal Large Language Models in\n  Perception-Cognition-Action Chain", "abstract": "We present PCA-Bench, a multimodal decision-making benchmark for evaluating\nthe integrated capabilities of Multimodal Large Language Models (MLLMs).\nDeparting from previous benchmarks focusing on simplistic tasks and individual\nmodel capability, PCA-Bench introduces three complex scenarios: autonomous\ndriving, domestic robotics, and open-world games. Given task instructions and\ndiverse contexts, the model is required to seamlessly integrate multiple\ncapabilities of Perception, Cognition, and Action in a reasoning chain to make\naccurate decisions. Moreover, PCA-Bench features error localization\ncapabilities, scrutinizing model inaccuracies in areas such as perception,\nknowledge, or reasoning. This enhances the reliability of deploying MLLMs. To\nbalance accuracy and efficiency in evaluation, we propose PCA-Eval, an\nautomatic evaluation protocol, and assess 10 prevalent MLLMs. The results\nreveal significant performance disparities between open-source models and\npowerful proprietary models like GPT-4 Vision. To address this, we introduce\nEmbodied-Instruction-Evolution (EIE), an automatic framework for synthesizing\ninstruction tuning examples in multimodal embodied environments. EIE generates\n7,510 training examples in PCA-Bench and enhances the performance of\nopen-source MLLMs, occasionally surpassing GPT-4 Vision (+3\\% in decision\naccuracy), thereby validating the effectiveness of EIE. Our findings suggest\nthat robust MLLMs like GPT4-Vision show promise for decision-making in embodied\nagents, opening new avenues for MLLM research.", "published": "2024-02-21 07:09:58", "link": "http://arxiv.org/abs/2402.15527v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Understanding the Dataset Practitioners Behind Large Language Model\n  Development", "abstract": "As large language models (LLMs) become more advanced and impactful, it is\nincreasingly important to scrutinize the data that they rely upon and produce.\nWhat is it to be a dataset practitioner doing this work? We approach this in\ntwo parts: first, we define the role of \"dataset practitioners\" by performing a\nretrospective analysis on the responsibilities of teams contributing to LLM\ndevelopment at a technology company, Google. Then, we conduct semi-structured\ninterviews with a cross-section of these practitioners (N=10). We find that\nalthough data quality is a top priority, there is little consensus around what\ndata quality is and how to evaluate it. Consequently, practitioners either rely\non their own intuition or write custom code to evaluate their data. We discuss\npotential reasons for this phenomenon and opportunities for alignment.", "published": "2024-02-21 23:50:37", "link": "http://arxiv.org/abs/2402.16611v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Exploring ChatGPT and its Impact on Society", "abstract": "Artificial intelligence has been around for a while, but suddenly it has\nreceived more attention than ever before. Thanks to innovations from companies\nlike Google, Microsoft, Meta, and other major brands in technology. OpenAI,\nthough, has triggered the button with its ground-breaking invention ChatGPT.\nChatGPT is a Large Language Model (LLM) based on Transformer architecture that\nhas the ability to generate human-like responses in a conversational context.\nIt uses deep learning algorithms to generate natural language responses to\ninput text. Its large number of parameters, contextual generation, and\nopen-domain training make it a versatile and effective tool for a wide range of\napplications, from chatbots to customer service to language translation. It has\nthe potential to revolutionize various industries and transform the way we\ninteract with technology. However, the use of ChatGPT has also raised several\nconcerns, including ethical, social, and employment challenges, which must be\ncarefully considered to ensure the responsible use of this technology. The\narticle provides an overview of ChatGPT, delving into its architecture and\ntraining process. It highlights the potential impacts of ChatGPT on the\nsociety. In this paper, we suggest some approaches involving technology,\nregulation, education, and ethics in an effort to maximize ChatGPT's benefits\nwhile minimizing its negative impacts. This study is expected to contribute to\na greater understanding of ChatGPT and aid in predicting the potential changes\nit may bring about.", "published": "2024-02-21 16:44:35", "link": "http://arxiv.org/abs/2403.14643v2", "categories": ["cs.CY", "cs.AI", "cs.CL", "68Txx"], "primary_category": "cs.CY"}
{"title": "Mel-FullSubNet: Mel-Spectrogram Enhancement for Improving Both Speech\n  Quality and ASR", "abstract": "In this work, we propose Mel-FullSubNet, a single-channel Mel-spectrogram\ndenoising and dereverberation network for improving both speech quality and\nautomatic speech recognition (ASR) performance. Mel-FullSubNet takes as input\nthe noisy and reverberant Mel-spectrogram and predicts the corresponding clean\nMel-spectrogram. The enhanced Mel-spectrogram can be either transformed to\nspeech waveform with a neural vocoder or directly used for ASR. Mel-FullSubNet\nencapsulates interleaved full-band and sub-band networks, for learning the\nfull-band spectral pattern of signals and the sub-band/narrow-band properties\nof signals, respectively. Compared to linear-frequency domain or time-domain\nspeech enhancement, the major advantage of Mel-spectrogram enhancement is that\nMel-frequency presents speech in a more compact way and thus is easier to\nlearn, which will benefit both speech quality and ASR. Experimental results\ndemonstrate a significant improvement in both speech quality and ASR\nperformance achieved by the proposed model.", "published": "2024-02-21 03:48:53", "link": "http://arxiv.org/abs/2402.13511v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Music Style Transfer with Time-Varying Inversion of Diffusion Models", "abstract": "With the development of diffusion models, text-guided image style transfer\nhas demonstrated high-quality controllable synthesis results. However, the\nutilization of text for diverse music style transfer poses significant\nchallenges, primarily due to the limited availability of matched audio-text\ndatasets. Music, being an abstract and complex art form, exhibits variations\nand intricacies even within the same genre, thereby making accurate textual\ndescriptions challenging. This paper presents a music style transfer approach\nthat effectively captures musical attributes using minimal data. We introduce a\nnovel time-varying textual inversion module to precisely capture\nmel-spectrogram features at different levels. During inference, we propose a\nbias-reduced stylization technique to obtain stable results. Experimental\nresults demonstrate that our method can transfer the style of specific\ninstruments, as well as incorporate natural sounds to compose melodies. Samples\nand source code are available at https://lsfhuihuiff.github.io/MusicTI/.", "published": "2024-02-21 12:38:48", "link": "http://arxiv.org/abs/2402.13763v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "HOMULA-RIR: A Room Impulse Response Dataset for Teleconferencing and\n  Spatial Audio Applications Acquired Through Higher-Order Microphones and\n  Uniform Linear Microphone Arrays", "abstract": "In this paper, we present HOMULA-RIR, a dataset of room impulse responses\n(RIRs) acquired using both higher-order microphones (HOMs) and a uniform linear\narray (ULA), in order to model a remote attendance teleconferencing scenario.\nSpecifically, measurements were performed in a seminar room, where a\n64-microphone ULA was used as a multichannel audio acquisition system in the\nproximity of the speakers, while HOMs were used to model 25 attendees actually\npresent in the seminar room. The HOMs cover a wide area of the room, making the\ndataset suitable also for applications of virtual acoustics. Through the\nmeasurement of the reverberation time and clarity index, and sample\napplications such as source localization and separation, we demonstrate the\neffectiveness of the HOMULA-RIR dataset.", "published": "2024-02-21 16:09:17", "link": "http://arxiv.org/abs/2402.13896v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "The Effect of Batch Size on Contrastive Self-Supervised Speech\n  Representation Learning", "abstract": "Foundation models in speech are often trained using many GPUs, which\nimplicitly leads to large effective batch sizes. In this paper we study the\neffect of batch size on pre-training, both in terms of statistics that can be\nmonitored during training, and in the effect on the performance of a downstream\nfine-tuning task. By using batch sizes varying from 87.5 seconds to 80 minutes\nof speech we show that, for a fixed amount of iterations, larger batch sizes\nresult in better pre-trained models. However, there is lower limit for\nstability, and an upper limit for effectiveness. We then show that the quality\nof the pre-trained model depends mainly on the amount of speech data seen\nduring training, i.e., on the product of batch size and number of iterations.\nAll results are produced with an independent implementation of the wav2vec 2.0\narchitecture, which to a large extent reproduces the results of the original\nwork (arXiv:2006.11477). Our extensions can help researchers choose effective\noperating conditions when studying self-supervised learning in speech, and\nhints towards benchmarking self-supervision with a fixed amount of seen data.\nCode and model checkpoints are available at\nhttps://github.com/nikvaessen/w2v2-batch-size.", "published": "2024-02-21 11:35:19", "link": "http://arxiv.org/abs/2402.13723v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Voice-Driven Mortality Prediction in Hospitalized Heart Failure\n  Patients: A Machine Learning Approach Enhanced with Diagnostic Biomarkers", "abstract": "Addressing heart failure (HF) as a prevalent global health concern poses\ndifficulties in implementing innovative approaches for enhanced patient care.\nPredicting mortality rates in HF patients, in particular, is difficult yet\ncritical, necessitating individualized care, proactive management, and enabling\neducated decision-making to enhance outcomes. Recently, the significance of\nvoice biomarkers coupled with Machine Learning (ML) has surged, demonstrating\nremarkable efficacy, particularly in predicting heart failure. The synergy of\nvoice analysis and ML algorithms provides a non-invasive and easily accessible\nmeans to evaluate patients' health. However, there is a lack of voice\nbiomarkers for predicting mortality rates among heart failure patients with\nstandardized speech protocols. Here, we demonstrate a powerful and effective ML\nmodel for predicting mortality rates in hospitalized HF patients through the\nutilization of voice biomarkers. By seamlessly integrating voice biomarkers\ninto routine patient monitoring, this strategy has the potential to improve\npatient outcomes, optimize resource allocation, and advance patient-centered HF\nmanagement. In this study, a Machine Learning system, specifically a logistic\nregression model, is trained to predict patients' 5-year mortality rates using\ntheir speech as input. The model performs admirably and consistently, as\ndemonstrated by cross-validation and statistical approaches (p-value < 0.001).\nFurthermore, integrating NT-proBNP, a diagnostic biomarker in HF, improves the\nmodel's predictive accuracy substantially.", "published": "2024-02-21 13:50:46", "link": "http://arxiv.org/abs/2402.13812v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Advancing Audio Fingerprinting Accuracy Addressing Background Noise and\n  Distortion Challenges", "abstract": "Audio fingerprinting, exemplified by pioneers like Shazam, has transformed\ndigital audio recognition. However, existing systems struggle with accuracy in\nchallenging conditions, limiting broad applicability. This research proposes an\nAI and ML integrated audio fingerprinting algorithm to enhance accuracy. Built\non the Dejavu Project's foundations, the study emphasizes real-world scenario\nsimulations with diverse background noises and distortions. Signal processing,\ncentral to Dejavu's model, includes the Fast Fourier Transform, spectrograms,\nand peak extraction. The \"constellation\" concept and fingerprint hashing enable\nunique song identification. Performance evaluation attests to 100% accuracy\nwithin a 5-second audio input, with a system showcasing predictable matching\nspeed for efficiency. Storage analysis highlights the critical space-speed\ntrade-off for practical implementation. This research advances audio\nfingerprinting's adaptability, addressing challenges in varied environments and\napplications.", "published": "2024-02-21 17:37:30", "link": "http://arxiv.org/abs/2402.13957v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
