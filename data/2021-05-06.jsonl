{"title": "XeroAlign: Zero-Shot Cross-lingual Transformer Alignment", "abstract": "The introduction of pretrained cross-lingual language models brought decisive\nimprovements to multilingual NLP tasks. However, the lack of labelled task data\nnecessitates a variety of methods aiming to close the gap to high-resource\nlanguages. Zero-shot methods in particular, often use translated task data as a\ntraining signal to bridge the performance gap between the source and target\nlanguage(s). We introduce XeroAlign, a simple method for task-specific\nalignment of cross-lingual pretrained transformers such as XLM-R. XeroAlign\nuses translated task data to encourage the model to generate similar sentence\nembeddings for different languages. The XeroAligned XLM-R, called XLM-RA, shows\nstrong improvements over the baseline models to achieve state-of-the-art\nzero-shot results on three multilingual natural language understanding tasks.\nXLM-RA's text classification accuracy exceeds that of XLM-R trained with\nlabelled data and performs on par with state-of-the-art models on a\ncross-lingual adversarial paraphrasing task.", "published": "2021-05-06 07:10:00", "link": "http://arxiv.org/abs/2105.02472v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantitative Evaluation of Alternative Translations in a Corpus of\n  Highly Dissimilar Finnish Paraphrases", "abstract": "In this paper, we present a quantitative evaluation of differences between\nalternative translations in a large recently released Finnish paraphrase corpus\nfocusing in particular on non-trivial variation in translation. We combine a\nseries of automatic steps detecting systematic variation with manual analysis\nto reveal regularities and identify categories of translation differences. We\nfind the paraphrase corpus to contain highly non-trivial translation variants\ndifficult to recognize through automatic approaches.", "published": "2021-05-06 07:22:16", "link": "http://arxiv.org/abs/2105.02477v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Unified Pre-training Framework for Conversational AI", "abstract": "In this work, we explore the application of PLATO-2 on various dialogue\nsystems, including open-domain conversation, knowledge grounded dialogue, and\ntask-oriented conversation. PLATO-2 is initially designed as an open-domain\nchatbot, trained via two-stage curriculum learning. In the first stage, a\ncoarse-grained response generation model is learned to fit the simplified\none-to-one mapping relationship. This model is applied to the task-oriented\nconversation, given that the semantic mappings tend to be deterministic in task\ncompletion. In the second stage, another fine-grained generation model and an\nevaluation model are further learned for diverse response generation and\ncoherence estimation, respectively. With superior capability on capturing\none-to-many mapping, such models are suitable for the open-domain conversation\nand knowledge grounded dialogue. For the comprehensive evaluation of PLATO-2,\nwe have participated in multiple tasks of DSTC9, including interactive\nevaluation of open-domain conversation (Track3-task2), static evaluation of\nknowledge grounded dialogue (Track3-task1), and end-to-end task-oriented\nconversation (Track2-task1). PLATO-2 has obtained the 1st place in all three\ntasks, verifying its effectiveness as a unified framework for various dialogue\nsystems.", "published": "2021-05-06 07:27:11", "link": "http://arxiv.org/abs/2105.02482v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards General Natural Language Understanding with Probabilistic\n  Worldbuilding", "abstract": "We introduce the Probabilistic Worldbuilding Model (PWM), a new\nfully-symbolic Bayesian model of semantic parsing and reasoning, as a first\nstep in a research program toward more domain- and task-general NLU and AI.\nHumans create internal mental models of their observations which greatly aid in\ntheir ability to understand and reason about a large variety of problems. In\nPWM, the meanings of sentences, acquired facts about the world, and\nintermediate steps in reasoning are all expressed in a human-readable formal\nlanguage, with the design goal of interpretability. PWM is Bayesian, designed\nspecifically to be able to generalize to new domains and new tasks. We derive\nand implement an inference algorithm that reads sentences by parsing and\nabducing updates to its latent world model that capture the semantics of those\nsentences, and evaluate it on two out-of-domain question-answering datasets:\n(1) ProofWriter and (2) a new dataset we call FictionalGeoQA, designed to be\nmore representative of real language but still simple enough to focus on\nevaluating reasoning ability, while being robust against heuristics. Our method\noutperforms baselines on both, thereby demonstrating its value as a\nproof-of-concept.", "published": "2021-05-06 07:38:43", "link": "http://arxiv.org/abs/2105.02486v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learn from Syntax: Improving Pair-wise Aspect and Opinion Terms\n  Extractionwith Rich Syntactic Knowledge", "abstract": "In this paper, we propose to enhance the pair-wise aspect and opinion terms\nextraction (PAOTE) task by incorporating rich syntactic knowledge. We first\nbuild a syntax fusion encoder for encoding syntactic features, including a\nlabel-aware graph convolutional network (LAGCN) for modeling the dependency\nedges and labels, as well as the POS tags unifiedly, and a local-attention\nmodule encoding POS tags for better term boundary detection. During pairing, we\nthen adopt Biaffine and Triaffine scoring for high-order aspect-opinion term\npairing, in the meantime re-harnessing the syntax-enriched representations in\nLAGCN for syntactic-aware scoring. Experimental results on four benchmark\ndatasets demonstrate that our model outperforms current state-of-the-art\nbaselines, meanwhile yielding explainable predictions with syntactic knowledge.", "published": "2021-05-06 08:45:40", "link": "http://arxiv.org/abs/2105.02520v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing Dialogue Systems with Distribution Distances", "abstract": "An important aspect of developing dialogue systems is how to evaluate and\ncompare the performance of different systems. Existing automatic evaluation\nmetrics are based on turn-level quality evaluation and use average scores for\nsystem-level comparison. In this paper, we propose to measure the performance\nof a dialogue system by computing the distribution-wise distance between its\ngenerated conversations and real-world conversations. Specifically, two\ndistribution-wise metrics, FBD and PRD, are developed and evaluated.\nExperiments on several dialogue corpora show that our proposed metrics\ncorrelate better with human judgments than existing metrics.", "published": "2021-05-06 10:30:13", "link": "http://arxiv.org/abs/2105.02573v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TABBIE: Pretrained Representations of Tabular Data", "abstract": "Existing work on tabular representation learning jointly models tables and\nassociated text using self-supervised objective functions derived from\npretrained language models such as BERT. While this joint pretraining improves\ntasks involving paired tables and text (e.g., answering questions about\ntables), we show that it underperforms on tasks that operate over tables\nwithout any associated text (e.g., populating missing cells). We devise a\nsimple pretraining objective (corrupt cell detection) that learns exclusively\nfrom tabular data and reaches the state-of-the-art on a suite of table based\nprediction tasks. Unlike competing approaches, our model (TABBIE) provides\nembeddings of all table substructures (cells, rows, and columns), and it also\nrequires far less compute to train. A qualitative analysis of our model's\nlearned cell, column, and row representations shows that it understands complex\ntable semantics and numerical trends.", "published": "2021-05-06 11:15:16", "link": "http://arxiv.org/abs/2105.02584v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bird's Eye: Probing for Linguistic Graph Structures with a Simple\n  Information-Theoretic Approach", "abstract": "NLP has a rich history of representing our prior understanding of language in\nthe form of graphs. Recent work on analyzing contextualized text\nrepresentations has focused on hand-designed probe models to understand how and\nto what extent do these representations encode a particular linguistic\nphenomenon. However, due to the inter-dependence of various phenomena and\nrandomness of training probe models, detecting how these representations encode\nthe rich information in these linguistic graphs remains a challenging problem.\nIn this paper, we propose a new information-theoretic probe, Bird's Eye, which\nis a fairly simple probe method for detecting if and how these representations\nencode the information in these linguistic graphs. Instead of using classifier\nperformance, our probe takes an information-theoretic view of probing and\nestimates the mutual information between the linguistic graph embedded in a\ncontinuous space and the contextualized word representations. Furthermore, we\nalso propose an approach to use our probe to investigate localized linguistic\ninformation in the linguistic graphs using perturbation analysis. We call this\nprobing setup Worm's Eye. Using these probes, we analyze BERT models on their\nability to encode a syntactic and a semantic graph structure, and find that\nthese models encode to some degree both syntactic as well as semantic\ninformation; albeit syntactic information to a greater extent.", "published": "2021-05-06 13:01:57", "link": "http://arxiv.org/abs/2105.02629v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving the Faithfulness of Attention-based Explanations with\n  Task-specific Information for Text Classification", "abstract": "Neural network architectures in natural language processing often use\nattention mechanisms to produce probability distributions over input token\nrepresentations. Attention has empirically been demonstrated to improve\nperformance in various tasks, while its weights have been extensively used as\nexplanations for model predictions. Recent studies (Jain and Wallace, 2019;\nSerrano and Smith, 2019; Wiegreffe and Pinter, 2019) have showed that it cannot\ngenerally be considered as a faithful explanation (Jacovi and Goldberg, 2020)\nacross encoders and tasks. In this paper, we seek to improve the faithfulness\nof attention-based explanations for text classification. We achieve this by\nproposing a new family of Task-Scaling (TaSc) mechanisms that learn\ntask-specific non-contextualised information to scale the original attention\nweights. Evaluation tests for explanation faithfulness, show that the three\nproposed variants of TaSc improve attention-based explanations across two\nattention mechanisms, five encoders and five text classification datasets\nwithout sacrificing predictive performance. Finally, we demonstrate that TaSc\nconsistently provides more faithful attention-based explanations compared to\nthree widely-used interpretability techniques.", "published": "2021-05-06 13:35:03", "link": "http://arxiv.org/abs/2105.02657v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Perturb Word Embeddings for Out-of-distribution QA", "abstract": "QA models based on pretrained language mod-els have achieved remarkable\nperformance on various benchmark datasets.However, QA models do not generalize\nwell to unseen data that falls outside the training distribution, due to\ndistributional shifts.Data augmentation (DA) techniques which drop/replace\nwords have shown to be effective in regularizing the model from overfitting to\nthe training data.Yet, they may adversely affect the QA tasks since they incur\nsemantic changes that may lead to wrong answers for the QA task. To tackle this\nproblem, we propose a simple yet effective DA method based on a stochastic\nnoise generator, which learns to perturb the word embedding of the input\nquestions and context without changing their semantics. We validate the\nperformance of the QA models trained with our word embedding perturbation on a\nsingle source dataset, on five different target domains.The results show that\nour method significantly outperforms the baselineDA methods. Notably, the model\ntrained with ours outperforms the model trained with more than 240K\nartificially generated QA pairs.", "published": "2021-05-06 14:12:26", "link": "http://arxiv.org/abs/2105.02692v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What's in the Box? A Preliminary Analysis of Undesirable Content in the\n  Common Crawl Corpus", "abstract": "Whereas much of the success of the current generation of neural language\nmodels has been driven by increasingly large training corpora, relatively\nlittle research has been dedicated to analyzing these massive sources of\ntextual data. In this exploratory analysis, we delve deeper into the Common\nCrawl, a colossal web corpus that is extensively used for training language\nmodels. We find that it contains a significant amount of undesirable content,\nincluding hate speech and sexually explicit content, even after filtering\nprocedures. We discuss the potential impacts of this content on language models\nand conclude with future research directions and a more mindful approach to\ncorpus collection and analysis.", "published": "2021-05-06 14:49:43", "link": "http://arxiv.org/abs/2105.02732v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Authors Matter: Understanding and Mitigating Implicit Bias in Deep\n  Text Classification", "abstract": "It is evident that deep text classification models trained on human data\ncould be biased. In particular, they produce biased outcomes for texts that\nexplicitly include identity terms of certain demographic groups. We refer to\nthis type of bias as explicit bias, which has been extensively studied.\nHowever, deep text classification models can also produce biased outcomes for\ntexts written by authors of certain demographic groups. We refer to such bias\nas implicit bias of which we still have a rather limited understanding. In this\npaper, we first demonstrate that implicit bias exists in different text\nclassification tasks for different demographic groups. Then, we build a\nlearning-based interpretation method to deepen our knowledge of implicit bias.\nSpecifically, we verify that classifiers learn to make predictions based on\nlanguage features that are related to the demographic attributes of the\nauthors. Next, we propose a framework Debiased-TC to train deep text\nclassifiers to make predictions on the right features and consequently mitigate\nimplicit bias. We conduct extensive experiments on three real-world datasets.\nThe results show that the text classification models trained under our proposed\nframework outperform traditional models significantly in terms of fairness, and\nalso slightly in terms of classification performance.", "published": "2021-05-06 16:17:38", "link": "http://arxiv.org/abs/2105.02778v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stylistic Analysis of the French Presidential Speeches: Is Macron really\n  different?", "abstract": "Presidential speeches indicate the government's intentions and justifications\nsupported by a dedicated style and rhetoric oscillating between explanation and\ncontroversy. Over a period of sixty years, can we observe stylistic variations\nby the different French presidents of the Fifth Republic (1958-2018)? Based on\nofficial transcripts of all their allocution, this paper illustrates the\nstylistic evolution and presents the underlying main trends. This study shows\nthat de Gaulle's rhetoric is not mainly dedicated to his own person, or that\nthe two terms of J. Chirac are not fully similar. According to several overall\nstylistic indicators, Macron's style does not appear as complex compared to his\npredecessors (F. Hollande or N. Sarkozy) but a more careful analysis clearly\ndemonstrates his noticeable new style. Compared to the recent US presidents,\nthe French ones present some similarities (e.g., similar mean sentence length)\nand dissimilarities (more I-words, less we-words). In this comparative\nanalysis, Macron's style is also clearly distinctive from both the US and\nformer French presidents. Opting for a more abstract discourse, less anchored\nin space, using less numbers, E. Macron tends to use long sentences. These\nvarious stylistic and rhetorical features could explain his being misunderstood\nby the French people and his recurrent low approval ratings.", "published": "2021-05-06 17:35:31", "link": "http://arxiv.org/abs/2105.02844v1", "categories": ["cs.CL", "J.5"], "primary_category": "cs.CL"}
{"title": "Adapting Monolingual Models: Data can be Scarce when Language Similarity\n  is High", "abstract": "For many (minority) languages, the resources needed to train large models are\nnot available. We investigate the performance of zero-shot transfer learning\nwith as little data as possible, and the influence of language similarity in\nthis process. We retrain the lexical layers of four BERT-based models using\ndata from two low-resource target language varieties, while the Transformer\nlayers are independently fine-tuned on a POS-tagging task in the model's source\nlanguage. By combining the new lexical layers and fine-tuned Transformer\nlayers, we achieve high task performance for both target languages. With high\nlanguage similarity, 10MB of data appears sufficient to achieve substantial\nmonolingual transfer performance. Monolingual BERT-based models generally\nachieve higher downstream task performance after retraining the lexical layer\nthan multilingual BERT, even when the target language is included in the\nmultilingual model.", "published": "2021-05-06 17:43:40", "link": "http://arxiv.org/abs/2105.02855v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph-based Multilingual Product Retrieval in E-commerce Search", "abstract": "Nowadays, with many e-commerce platforms conducting global business,\ne-commerce search systems are required to handle product retrieval under\nmultilingual scenarios. Moreover, comparing with maintaining per-country\nspecific e-commerce search systems, having a universal system across countries\ncan further reduce the operational and computational costs, and facilitate\nbusiness expansion to new countries. In this paper, we introduce a universal\nend-to-end multilingual retrieval system, and discuss our learnings and\ntechnical details when training and deploying the system to serve billion-scale\nproduct retrieval for e-commerce search. In particular, we propose a\nmultilingual graph attention based retrieval network by leveraging recent\nadvances in transformer-based multilingual language models and graph neural\nnetwork architectures to capture the interactions between search queries and\nitems in e-commerce search. Offline experiments on five countries data show\nthat our algorithm outperforms the state-of-the-art baselines by 35% recall and\n25% mAP on average. Moreover, the proposed model shows significant increase of\nconversion/revenue in online A/B experiments and has been deployed in\nproduction for multiple countries.", "published": "2021-05-06 21:49:10", "link": "http://arxiv.org/abs/2105.02978v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do language models learn typicality judgments from text?", "abstract": "Building on research arguing for the possibility of conceptual and\ncategorical knowledge acquisition through statistics contained in language, we\nevaluate predictive language models (LMs) -- informed solely by textual input\n-- on a prevalent phenomenon in cognitive science: typicality. Inspired by\nexperiments that involve language processing and show robust typicality effects\nin humans, we propose two tests for LMs. Our first test targets whether\ntypicality modulates LM probabilities in assigning taxonomic category\nmemberships to items. The second test investigates sensitivities to typicality\nin LMs' probabilities when extending new information about items to their\ncategories. Both tests show modest -- but not completely absent --\ncorrespondence between LMs and humans, suggesting that text-based exposure\nalone is insufficient to acquire typicality knowledge.", "published": "2021-05-06 21:56:40", "link": "http://arxiv.org/abs/2105.02987v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SGG: Learning to Select, Guide, and Generate for Keyphrase Generation", "abstract": "Keyphrases, that concisely summarize the high-level topics discussed in a\ndocument, can be categorized into present keyphrase which explicitly appears in\nthe source text, and absent keyphrase which does not match any contiguous\nsubsequence but is highly semantically related to the source. Most existing\nkeyphrase generation approaches synchronously generate present and absent\nkeyphrases without explicitly distinguishing these two categories. In this\npaper, a Select-Guide-Generate (SGG) approach is proposed to deal with present\nand absent keyphrase generation separately with different mechanisms.\nSpecifically, SGG is a hierarchical neural network which consists of a\npointing-based selector at low layer concentrated on present keyphrase\ngeneration, a selection-guided generator at high layer dedicated to absent\nkeyphrase generation, and a guider in the middle to transfer information from\nselector to generator. Experimental results on four keyphrase generation\nbenchmarks demonstrate the effectiveness of our model, which significantly\noutperforms the strong baselines for both present and absent keyphrases\ngeneration. Furthermore, we extend SGG to a title generation task which\nindicates its extensibility in natural language generation tasks.", "published": "2021-05-06 09:43:33", "link": "http://arxiv.org/abs/2105.02544v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Introducing Information Retrieval for Biomedical Informatics Students", "abstract": "Introducing biomedical informatics (BMI) students to natural language\nprocessing (NLP) requires balancing technical depth with practical know-how to\naddress application-focused needs. We developed a set of three activities\nintroducing introductory BMI students to information retrieval with NLP,\ncovering document representation strategies and language models from TF-IDF to\nBERT. These activities provide students with hands-on experience targeted\ntowards common use cases, and introduce fundamental components of NLP workflows\nfor a wide variety of applications.", "published": "2021-05-06 15:15:54", "link": "http://arxiv.org/abs/2105.02746v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "On the Ethical Limits of Natural Language Processing on Legal Text", "abstract": "Natural language processing (NLP) methods for analyzing legal text offer\nlegal scholars and practitioners a range of tools allowing to empirically\nanalyze law on a large scale. However, researchers seem to struggle when it\ncomes to identifying ethical limits to using NLP systems for acquiring genuine\ninsights both about the law and the systems' predictive capacity. In this paper\nwe set out a number of ways in which to think systematically about such issues.\nWe place emphasis on three crucial normative parameters which have, to the best\nof our knowledge, been underestimated by current debates: (a) the importance of\nacademic freedom, (b) the existence of a wide diversity of legal and ethical\nnorms domestically but even more so internationally and (c) the threat of\nmoralism in research related to computational law. For each of these three\nparameters we provide specific recommendations for the legal NLP community. Our\ndiscussion is structured around the study of a real-life scenario that has\nprompted recent debate in the legal NLP research community.", "published": "2021-05-06 15:22:24", "link": "http://arxiv.org/abs/2105.02751v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hone as You Read: A Practical Type of Interactive Summarization", "abstract": "We present HARE, a new task where reader feedback is used to optimize\ndocument summaries for personal interest during the normal flow of reading.\nThis task is related to interactive summarization, where personalized summaries\nare produced following a long feedback stage where users may read the same\nsentences many times. However, this process severely interrupts the flow of\nreading, making it impractical for leisurely reading. We propose to gather\nminimally-invasive feedback during the reading process to adapt to user\ninterests and augment the document in real-time. Building off of recent\nadvances in unsupervised summarization evaluation, we propose a suitable metric\nfor this task and use it to evaluate a variety of approaches. Our approaches\nrange from simple heuristics to preference-learning and their analysis provides\ninsight into this important task. Human evaluation additionally supports the\npracticality of HARE. The code to reproduce this work is available at\nhttps://github.com/tannerbohn/HoneAsYouRead.", "published": "2021-05-06 19:36:40", "link": "http://arxiv.org/abs/2105.02923v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "On the logistical difficulties and findings of Jopara Sentiment Analysis", "abstract": "This paper addresses the problem of sentiment analysis for Jopara, a\ncode-switching language between Guarani and Spanish. We first collect a corpus\nof Guarani-dominant tweets and discuss on the difficulties of finding quality\ndata for even relatively easy-to-annotate tasks, such as sentiment analysis.\nThen, we train a set of neural models, including pre-trained language models,\nand explore whether they perform better than traditional machine learning ones\nin this low-resource setup. Transformer architectures obtain the best results,\ndespite not considering Guarani during pre-training, but traditional machine\nlearning models perform close due to the low-resource nature of the problem.", "published": "2021-05-06 20:52:29", "link": "http://arxiv.org/abs/2105.02947v2", "categories": ["cs.CL", "cs.LG", "68-02 68T50 68T07 91D30"], "primary_category": "cs.CL"}
{"title": "Capturing the diversity of multilingual societies", "abstract": "Cultural diversity encoded within languages of the world is at risk, as many\nlanguages have become endangered in the last decades in a context of growing\nglobalization. To preserve this diversity, it is first necessary to understand\nwhat drives language extinction, and which mechanisms might enable coexistence.\nHere, we study language shift mechanisms using theoretical and data-driven\nperspectives. A large-scale empirical analysis of multilingual societies using\nTwitter and census data yields a wide diversity of spatial patterns of language\ncoexistence. It ranges from a mixing of language speakers to segregation with\nmultilinguals on the boundaries of disjoint linguistic domains. To understand\nhow these different states can emerge and, especially, become stable, we\npropose a model in which language coexistence is reached when learning the\nother language is facilitated and when bilinguals favor the use of the\nendangered language. Simulations carried out in a metapopulation framework\nhighlight the importance of spatial interactions arising from people mobility\nto explain the stability of a mixed state or the presence of a boundary between\ntwo linguistic regions. Further, we find that the history of languages is\ncritical to understand their present state.", "published": "2021-05-06 10:27:43", "link": "http://arxiv.org/abs/2105.02570v4", "categories": ["physics.soc-ph", "cs.CL", "cs.SI"], "primary_category": "physics.soc-ph"}
{"title": "GraphFormers: GNN-nested Transformers for Representation Learning on\n  Textual Graph", "abstract": "The representation learning on textual graph is to generate low-dimensional\nembeddings for the nodes based on the individual textual features and the\nneighbourhood information. Recent breakthroughs on pretrained language models\nand graph neural networks push forward the development of corresponding\ntechniques. The existing works mainly rely on the cascaded model architecture:\nthe textual features of nodes are independently encoded by language models at\nfirst; the textual embeddings are aggregated by graph neural networks\nafterwards. However, the above architecture is limited due to the independent\nmodeling of textual features. In this work, we propose GraphFormers, where\nlayerwise GNN components are nested alongside the transformer blocks of\nlanguage models. With the proposed architecture, the text encoding and the\ngraph aggregation are fused into an iterative workflow, {making} each node's\nsemantic accurately comprehended from the global perspective. In addition, a\n{progressive} learning strategy is introduced, where the model is successively\ntrained on manipulated data and original data to reinforce its capability of\nintegrating information on graph. Extensive evaluations are conducted on three\nlarge-scale benchmark datasets, where GraphFormers outperform the SOTA\nbaselines with comparable running efficiency.", "published": "2021-05-06 12:20:41", "link": "http://arxiv.org/abs/2105.02605v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Text similarity analysis for evaluation of descriptive answers", "abstract": "Keeping in mind the necessity of intelligent system in educational sector,\nthis paper proposes a text analysis based automated approach for automatic\nevaluation of the descriptive answers in an examination. In particular, the\nresearch focuses on the use of intelligent concepts of Natural Language\nProcessing and Data Mining for computer aided examination evaluation system.\nThe paper present an architecture for fair evaluation of answer sheet. In this\narchitecture, the examiner creates a sample answer sheet for given sets of\nquestion. By using the concept of text summarization, text semantics and\nkeywords summarization, the final score for each answer is calculated. The text\nsimilarity model is based on Siamese Manhattan LSTM (MaLSTM). The results of\nthis research were compared to manually graded assignments and other existing\nsystem. This approach was found to be very efficient in order to be implemented\nin an institution or in an university.", "published": "2021-05-06 20:19:58", "link": "http://arxiv.org/abs/2105.02935v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Reducing Streaming ASR Model Delay with Self Alignment", "abstract": "Reducing prediction delay for streaming end-to-end ASR models with minimal\nperformance regression is a challenging problem. Constrained alignment is a\nwell-known existing approach that penalizes predicted word boundaries using\nexternal low-latency acoustic models. On the contrary, recently proposed\nFastEmit is a sequence-level delay regularization scheme encouraging vocabulary\ntokens over blanks without any reference alignments. Although all these schemes\nare successful in reducing delay, ASR word error rate (WER) often severely\ndegrades after applying these delay constraining schemes. In this paper, we\npropose a novel delay constraining method, named self alignment. Self alignment\ndoes not require external alignment models. Instead, it utilizes Viterbi\nforced-alignments from the trained model to find the lower latency alignment\ndirection. From LibriSpeech evaluation, self alignment outperformed existing\nschemes: 25% and 56% less delay compared to FastEmit and constrained alignment\nat the similar word error rate. For Voice Search evaluation,12% and 25% delay\nreductions were achieved compared to FastEmit and constrained alignment with\nmore than 2% WER improvements.", "published": "2021-05-06 18:00:11", "link": "http://arxiv.org/abs/2105.05005v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Reliability Testing for Natural Language Processing Systems", "abstract": "Questions of fairness, robustness, and transparency are paramount to address\nbefore deploying NLP systems. Central to these concerns is the question of\nreliability: Can NLP systems reliably treat different demographics fairly and\nfunction correctly in diverse and noisy environments? To address this, we argue\nfor the need for reliability testing and contextualize it among existing work\non improving accountability. We show how adversarial attacks can be reframed\nfor this goal, via a framework for developing reliability tests. We argue that\nreliability testing -- with an emphasis on interdisciplinary collaboration --\nwill enable rigorous and targeted testing, and aid in the enactment and\nenforcement of industry standards.", "published": "2021-05-06 11:24:58", "link": "http://arxiv.org/abs/2105.02590v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY", "cs.NE"], "primary_category": "cs.LG"}
{"title": "DBNet: A Dual-branch Network Architecture Processing on Spectrum and\n  Waveform for Single-channel Speech Enhancement", "abstract": "In real acoustic environment, speech enhancement is an arduous task to\nimprove the quality and intelligibility of speech interfered by background\nnoise and reverberation. Over the past years, deep learning has shown great\npotential on speech enhancement. In this paper, we propose a novel real-time\nframework called DBNet which is a dual-branch structure with alternate\ninterconnection. Each branch incorporates an encoder-decoder architecture with\nskip connections. The two branches are responsible for spectrum and waveform\nmodeling, respectively. A bridge layer is adopted to exchange information\nbetween the two branches. Systematic evaluation and comparison show that the\nproposed system substantially outperforms related algorithms under very\nchallenging environments. And in INTERSPEECH 2021 Deep Noise Suppression (DNS)\nchallenge, the proposed system ranks the top 8 in real-time track 1 in terms of\nthe Mean Opinion Score (MOS) of the ITU-T P.835 framework.", "published": "2021-05-06 04:31:42", "link": "http://arxiv.org/abs/2105.02436v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deficient Basis Estimation of Noise Spatial Covariance Matrix for\n  Rank-Constrained Spatial Covariance Matrix Estimation Method in Blind Speech\n  Extraction", "abstract": "Rank-constrained spatial covariance matrix estimation (RCSCME) is a\nstate-of-the-art blind speech extraction method applied to cases where one\ndirectional target speech and diffuse noise are mixed. In this paper, we\nproposed a new algorithmic extension of RCSCME. RCSCME complements a deficient\none rank of the diffuse noise spatial covariance matrix, which cannot be\nestimated via preprocessing such as independent low-rank matrix analysis, and\nestimates the source model parameters simultaneously. In the conventional\nRCSCME, a direction of the deficient basis is fixed in advance and only the\nscale is estimated; however, the candidate of this deficient basis is not\nunique in general. In the proposed RCSCME model, the deficient basis itself can\nbe accurately estimated as a vector variable by solving a vector optimization\nproblem. Also, we derive new update rules based on the EM algorithm. We confirm\nthat the proposed method outperforms conventional methods under several noise\nconditions.", "published": "2021-05-06 07:44:43", "link": "http://arxiv.org/abs/2105.02491v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "USM-SED - A Dataset for Polyphonic Sound Event Detection in Urban Sound\n  Monitoring Scenarios", "abstract": "This paper introduces a novel dataset for polyphonic sound event detection in\nurban sound monitoring use-cases. Based on isolated sounds taken from the\nFSD50k dataset, 20,000 polyphonic soundscapes are synthesized with sounds being\nrandomly positioned in the stereo panorama using different loudness levels. The\npaper gives a detailed discussion of possible application scenarios, explains\nthe dataset generation process in detail, and discusses current limitations of\nthe proposed USM-SED dataset.", "published": "2021-05-06 11:45:47", "link": "http://arxiv.org/abs/2105.02592v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Weakly Supervised Source-Specific Sound Level Estimation in Noisy\n  Soundscapes", "abstract": "While the estimation of what sound sources are, when they occur, and from\nwhere they originate has been well-studied, the estimation of how loud these\nsound sources are has been often overlooked. Current solutions to this task,\nwhich we refer to as source-specific sound level estimation (SSSLE), suffer\nfrom challenges due to the impracticality of acquiring realistic data and a\nlack of robustness to realistic recording conditions. Recently proposed weakly\nsupervised source separation offer a means of leveraging clip-level source\nannotations to train source separation models, which we augment with modified\nloss functions to bridge the gap between source separation and SSSLE and to\naddress the presence of background. We show that our approach improves SSSLE\nperformance compared to baseline source separation models and provide an\nablation analysis to explore our method's design choices, showing that SSSLE in\npractical recording and annotation scenarios is possible.", "published": "2021-05-06 18:34:12", "link": "http://arxiv.org/abs/2105.02911v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism", "abstract": "Singing voice synthesis (SVS) systems are built to synthesize high-quality\nand expressive singing voice, in which the acoustic model generates the\nacoustic features (e.g., mel-spectrogram) given a music score. Previous singing\nacoustic models adopt a simple loss (e.g., L1 and L2) or generative adversarial\nnetwork (GAN) to reconstruct the acoustic features, while they suffer from\nover-smoothing and unstable training issues respectively, which hinder the\nnaturalness of synthesized singing. In this work, we propose DiffSinger, an\nacoustic model for SVS based on the diffusion probabilistic model. DiffSinger\nis a parameterized Markov chain that iteratively converts the noise into\nmel-spectrogram conditioned on the music score. By implicitly optimizing\nvariational bound, DiffSinger can be stably trained and generate realistic\noutputs. To further improve the voice quality and speed up inference, we\nintroduce a shallow diffusion mechanism to make better use of the prior\nknowledge learned by the simple loss. Specifically, DiffSinger starts\ngeneration at a shallow step smaller than the total number of diffusion steps,\naccording to the intersection of the diffusion trajectories of the ground-truth\nmel-spectrogram and the one predicted by a simple mel-spectrogram decoder.\nBesides, we propose boundary prediction methods to locate the intersection and\ndetermine the shallow step adaptively. The evaluations conducted on a Chinese\nsinging dataset demonstrate that DiffSinger outperforms state-of-the-art SVS\nwork. Extensional experiments also prove the generalization of our methods on\ntext-to-speech task (DiffSpeech). Audio samples: https://diffsinger.github.io.\nCodes: https://github.com/MoonInTheRiver/DiffSinger. The old title of this\nwork: \"Diffsinger: Diffusion acoustic model for singing voice synthesis\".", "published": "2021-05-06 05:21:42", "link": "http://arxiv.org/abs/2105.02446v6", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Point Cloud Audio Processing", "abstract": "Most audio processing pipelines involve transformations that act on\nfixed-dimensional input representations of audio. For example, when using the\nShort Time Fourier Transform (STFT) the DFT size specifies a fixed dimension\nfor the input representation. As a consequence, most audio machine learning\nmodels are designed to process fixed-size vector inputs which often prohibits\nthe repurposing of learned models on audio with different sampling rates or\nalternative representations. We note, however, that the intrinsic spectral\ninformation in the audio signal is invariant to the choice of the input\nrepresentation or the sampling rate. Motivated by this, we introduce a novel\nway of processing audio signals by treating them as a collection of points in\nfeature space, and we use point cloud machine learning models that give us\ninvariance to the choice of representation parameters, such as DFT size or the\nsampling rate. Additionally, we observe that these methods result in smaller\nmodels, and allow us to significantly subsample the input representation with\nminimal effects to a trained model performance.", "published": "2021-05-06 07:04:59", "link": "http://arxiv.org/abs/2105.02469v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Enhancement using Separable Polling Attention and Global Layer\n  Normalization followed with PReLU", "abstract": "Single channel speech enhancement is a challenging task in speech community.\nRecently, various neural networks based methods have been applied to speech\nenhancement. Among these models, PHASEN and T-GSA achieve state-of-the-art\nperformances on the publicly opened VoiceBank+DEMAND corpus. Both of the models\nreach the COVL score of 3.62. PHASEN achieves the highest CSIG score of 4.21\nwhile T-GSA gets the highest PESQ score of 3.06. However, both of these two\nmodels are very large. The contradiction between the model performance and the\nmodel size is hard to reconcile. In this paper, we introduce three kinds of\ntechniques to shrink the PHASEN model and improve the performance. Firstly,\nseperable polling attention is proposed to replace the frequency transformation\nblocks in PHASEN. Secondly, global layer normalization followed with PReLU is\nused to replace batch normalization followed with ReLU. Finally, BLSTM in\nPHASEN is replaced with Conv2d operation and the phase stream is simplified.\nWith all these modifications, the size of the PHASEN model is shrunk from 33M\nparameters to 5M parameters, while the performance on VoiceBank+DEMAND is\nimproved to the CSIG score of 4.30, the PESQ score of 3.07 and the COVL score\nof 3.73.", "published": "2021-05-06 08:18:02", "link": "http://arxiv.org/abs/2105.02509v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MIMII DUE: Sound Dataset for Malfunctioning Industrial Machine\n  Investigation and Inspection with Domain Shifts due to Changes in Operational\n  and Environmental Conditions", "abstract": "In this paper, we introduce MIMII DUE, a new dataset for malfunctioning\nindustrial machine investigation and inspection with domain shifts due to\nchanges in operational and environmental conditions. Conventional methods for\nanomalous sound detection face practical challenges because the distribution of\nfeatures changes between the training and operational phases (called domain\nshift) due to various real-world factors. To check the robustness against\ndomain shifts, we need a dataset that actually includes domain shifts, but such\na dataset does not exist so far. The new dataset we created consists of the\nnormal and abnormal operating sounds of five different types of industrial\nmachines under two different operational/environmental conditions (source\ndomain and target domain) independent of normal/abnormal, with domain shifts\noccurring between the two domains. Experimental results showed significant\nperformance differences between the source and target domains, indicating that\nthe dataset contains the domain shifts. These findings demonstrate that the\ndataset will be helpful for checking the robustness against domain shifts. The\ndataset is a subset of the dataset for DCASE 2021 Challenge Task 2 and freely\navailable for download at https://zenodo.org/record/4740355", "published": "2021-05-06 14:18:24", "link": "http://arxiv.org/abs/2105.02702v3", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Signal Analysis via the Stochastic Geometry of Spectrogram Level Sets", "abstract": "Spectrograms are fundamental tools in time-frequency analysis, being the\nsquared magnitude of the so-called short time Fourier transform (STFT). Signal\nanalysis via spectrograms has traditionally explored their peaks, i.e. their\nmaxima. This is complemented by a recent interest in their zeros or minima,\nfollowing seminal work by Flandrin and others, which exploits connections with\nGaussian analytic functions (GAFs). However, the zero sets (or extrema) of GAFs\nhave a complicated stochastic structure, complicating any direct theoretical\nanalysis. Standard techniques largely rely on statistical observables from the\nanalysis of spatial data, whose distributional properties for spectrograms are\nmostly understood only at an empirical level. In this work, we investigate\nspectrogram analysis via an examination of the stochastic geometric properties\nof their level sets. We obtain rigorous theorems demonstrating the efficacy of\na spectrogram level sets based approach to the detection and estimation of\nsignals, framed in a concrete inferential set-up. Exploiting these ideas as\ntheoretical underpinnings, we propose a level sets based algorithm for signal\nanalysis that is intrinsic to given spectrogram data, and substantiate its\neffectiveness via extensive empirical studies. Our results also have\ntheoretical implications for spectrogram zero based approaches to signal\nanalysis. To our knowledge, these results are arguably among the first to\nprovide a rigorous statistical understanding of signal detection and\nreconstruction in this set up, complemented with provable guarantees on\ndetection thresholds and rates of convergence.", "published": "2021-05-06 07:06:34", "link": "http://arxiv.org/abs/2105.02471v2", "categories": ["eess.SP", "cs.SD", "eess.AS", "math.PR", "math.ST", "stat.TH"], "primary_category": "eess.SP"}
