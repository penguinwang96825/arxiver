{"title": "Implementing a Chatbot Solution for Learning Management System", "abstract": "Innovation is a key component in trying new solutions for the students to\nlearn efficiently and in ways that correspond to their own experience, where\nchatbots are one of these new solutions. One of the main problem that chatbots\nface today is to mimic human language, where they try to find the best answer\nto an input, which is not how a human conversation usually works, rather taking\ninto account the previous messages and building onto them. Extreme programming\nmethodology was chosen to use integrate ChatterBot, Pyside2, web scraping and\nTampermonkey into Blackboard as a test case. Problems occurred with the bot and\nmore training was needed for the bot to work perfectly, but the integration and\nweb scraping worked, giving us a chatbot that was able to talk with. We showed\nthe plausibility of integrating an AI bot in an educational setting.", "published": "2022-06-27 11:04:42", "link": "http://arxiv.org/abs/2206.13187v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-Shot Stance Detection via Target-Aware Prompt Distillation", "abstract": "Stance detection aims to identify whether the author of a text is in favor\nof, against, or neutral to a given target. The main challenge of this task\ncomes two-fold: few-shot learning resulting from the varying targets and the\nlack of contextual information of the targets. Existing works mainly focus on\nsolving the second issue by designing attention-based models or introducing\nnoisy external knowledge, while the first issue remains under-explored. In this\npaper, inspired by the potential capability of pre-trained language models\n(PLMs) serving as knowledge bases and few-shot learners, we propose to\nintroduce prompt-based fine-tuning for stance detection. PLMs can provide\nessential contextual information for the targets and enable few-shot learning\nvia prompts. Considering the crucial role of the target in stance detection\ntask, we design target-aware prompts and propose a novel verbalizer. Instead of\nmapping each label to a concrete word, our verbalizer maps each label to a\nvector and picks the label that best captures the correlation between the\nstance and the target. Moreover, to alleviate the possible defect of dealing\nwith varying targets with a single hand-crafted prompt, we propose to distill\nthe information learned from multiple prompts. Experimental results show the\nsuperior performance of our proposed model in both full-data and few-shot\nscenarios.", "published": "2022-06-27 12:04:14", "link": "http://arxiv.org/abs/2206.13214v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Center-Embedding and Constituency in the Brain and a New\n  Characterization of Context-Free Languages", "abstract": "A computational system implemented exclusively through the spiking of neurons\nwas recently shown capable of syntax, that is, of carrying out the dependency\nparsing of simple English sentences. We address two of the most important\nquestions left open by that work: constituency (the identification of key parts\nof the sentence such as the verb phrase) and the processing of dependent\nsentences, especially center-embedded ones. We show that these two aspects of\nlanguage can also be implemented by neurons and synapses in a way that is\ncompatible with what is known, or widely believed, about the structure and\nfunction of the language organ. Surprisingly, the way we implement center\nembedding points to a new characterization of context-free languages.", "published": "2022-06-27 12:11:03", "link": "http://arxiv.org/abs/2206.13217v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Which one is more toxic? Findings from Jigsaw Rate Severity of Toxic\n  Comments", "abstract": "The proliferation of online hate speech has necessitated the creation of\nalgorithms which can detect toxicity. Most of the past research focuses on this\ndetection as a classification task, but assigning an absolute toxicity label is\noften tricky. Hence, few of the past works transform the same task into a\nregression. This paper shows the comparative evaluation of different\ntransformers and traditional machine learning models on a recently released\ntoxicity severity measurement dataset by Jigsaw. We further demonstrate the\nissues with the model predictions using explainability analysis.", "published": "2022-06-27 13:20:24", "link": "http://arxiv.org/abs/2206.13284v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discovering Salient Neurons in Deep NLP Models", "abstract": "While a lot of work has been done in understanding representations learned\nwithin deep NLP models and what knowledge they capture, little attention has\nbeen paid towards individual neurons. We present a technique called as\nLinguistic Correlation Analysis to extract salient neurons in the model, with\nrespect to any extrinsic property - with the goal of understanding how such a\nknowledge is preserved within neurons. We carry out a fine-grained analysis to\nanswer the following questions: (i) can we identify subsets of neurons in the\nnetwork that capture specific linguistic properties? (ii) how localized or\ndistributed neurons are across the network? iii) how redundantly is the\ninformation preserved? iv) how fine-tuning pre-trained models towards\ndownstream NLP tasks, impacts the learned linguistic knowledge? iv) how do\narchitectures vary in learning different linguistic properties? Our\ndata-driven, quantitative analysis illuminates interesting findings: (i) we\nfound small subsets of neurons that can predict different linguistic tasks, ii)\nwith neurons capturing basic lexical information (such as suffixation)\nlocalized in lower most layers, iii) while those learning complex concepts\n(such as syntactic role) predominantly in middle and higher layers, iii) that\nsalient linguistic neurons are relocated from higher to lower layers during\ntransfer learning, as the network preserve the higher layers for task specific\ninformation, iv) we found interesting differences across pre-trained models,\nwith respect to how linguistic information is preserved within, and v) we found\nthat concept exhibit similar neuron distribution across different languages in\nthe multilingual transformer models. Our code is publicly available as part of\nthe NeuroX toolkit.", "published": "2022-06-27 13:31:49", "link": "http://arxiv.org/abs/2206.13288v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simplifying Semantic Annotations of SMCalFlow", "abstract": "SMCalFlow is a large corpus of semantically detailed annotations of\ntask-oriented natural dialogues. The annotations use a dataflow approach, in\nwhich the annotations are programs which represent user requests. Despite the\navailability, size and richness of this annotated corpus, it has seen only very\nlimited use in dialogue systems research work, at least in part due to the\ndifficulty in understanding and using the annotations. To address these\ndifficulties, this paper suggests a simplification of the SMCalFlow\nannotations, as well as releases code needed to inspect the execution of the\nannotated dataflow programs, which should allow researchers of dialogue systems\nan easy entry point to experiment with various dataflow based implementations\nand annotations.", "published": "2022-06-27 16:22:16", "link": "http://arxiv.org/abs/2206.13425v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Wav2Vec-Aug: Improved self-supervised training with limited data", "abstract": "Self-supervised learning (SSL) of speech representations has received much\nattention over the last few years but most work has focused on languages and\ndomains with an abundance of unlabeled data. However, for many languages there\nis a shortage even in the unlabeled data which limits the effectiveness of SSL.\nIn this work, we focus on the problem of applying SSL to domains with limited\navailable data by leveraging data augmentation for Wav2Vec 2.0 pretraining.\nFurther, we propose improvements to each component of the model which result in\na combined relative word error rate (WER) improvement of up to 13% compared to\nWav2Vec 2.0 on Librispeech test-clean / other.", "published": "2022-06-27 22:31:22", "link": "http://arxiv.org/abs/2206.13654v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Endowing Language Models with Multimodal Knowledge Graph Representations", "abstract": "We propose a method to make natural language understanding models more\nparameter efficient by storing knowledge in an external knowledge graph (KG)\nand retrieving from this KG using a dense index. Given (possibly multilingual)\ndownstream task data, e.g., sentences in German, we retrieve entities from the\nKG and use their multimodal representations to improve downstream task\nperformance. We use the recently released VisualSem KG as our external\nknowledge repository, which covers a subset of Wikipedia and WordNet entities,\nand compare a mix of tuple-based and graph-based algorithms to learn entity and\nrelation representations that are grounded on the KG multimodal information. We\ndemonstrate the usefulness of the learned entity representations on two\ndownstream tasks, and show improved performance on the multilingual named\nentity recognition task by $0.3\\%$--$0.7\\%$ F1, while we achieve up to $2.5\\%$\nimprovement in accuracy on the visual sense disambiguation task. All our code\nand data are available in: \\url{https://github.com/iacercalixto/visualsem-kg}.", "published": "2022-06-27 10:10:42", "link": "http://arxiv.org/abs/2206.13163v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; I.2.10; I.2.4"], "primary_category": "cs.CL"}
{"title": "Analyzing Encoded Concepts in Transformer Language Models", "abstract": "We propose a novel framework ConceptX, to analyze how latent concepts are\nencoded in representations learned within pre-trained language models. It uses\nclustering to discover the encoded concepts and explains them by aligning with\na large set of human-defined concepts. Our analysis on seven transformer\nlanguage models reveal interesting insights: i) the latent space within the\nlearned representations overlap with different linguistic concepts to a varying\ndegree, ii) the lower layers in the model are dominated by lexical concepts\n(e.g., affixation), whereas the core-linguistic concepts (e.g., morphological\nor syntactic relations) are better represented in the middle and higher layers,\niii) some encoded concepts are multi-faceted and cannot be adequately explained\nusing the existing human-defined concepts.", "published": "2022-06-27 13:32:10", "link": "http://arxiv.org/abs/2206.13289v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improved Text Classification via Test-Time Augmentation", "abstract": "Test-time augmentation -- the aggregation of predictions across transformed\nexamples of test inputs -- is an established technique to improve the\nperformance of image classification models. Importantly, TTA can be used to\nimprove model performance post-hoc, without additional training. Although\ntest-time augmentation (TTA) can be applied to any data modality, it has seen\nlimited adoption in NLP due in part to the difficulty of identifying\nlabel-preserving transformations. In this paper, we present augmentation\npolicies that yield significant accuracy improvements with language models. A\nkey finding is that augmentation policy design -- for instance, the number of\nsamples generated from a single, non-deterministic augmentation -- has a\nconsiderable impact on the benefit of TTA. Experiments across a binary\nclassification task and dataset show that test-time augmentation can deliver\nconsistent improvements over current state-of-the-art approaches.", "published": "2022-06-27 19:57:27", "link": "http://arxiv.org/abs/2206.13607v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Long Range Language Modeling via Gated State Spaces", "abstract": "State space models have shown to be effective at modeling long range\ndependencies, specially on sequence classification tasks. In this work we focus\non autoregressive sequence modeling over English books, Github source code and\nArXiv mathematics articles. Based on recent developments around the\neffectiveness of gated activation functions, we propose a new layer named Gated\nState Space (GSS) and show that it trains significantly faster than the\ndiagonal version of S4 (i.e. DSS) on TPUs, is fairly competitive with several\nwell-tuned Transformer-based baselines and exhibits zero-shot generalization to\nlonger inputs while being straightforward to implement. Finally, we show that\nleveraging self-attention to model local dependencies improves the performance\nof GSS even further.", "published": "2022-06-27 01:50:18", "link": "http://arxiv.org/abs/2206.13947v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Extracting Weighted Finite Automata from Recurrent Neural Networks for\n  Natural Languages", "abstract": "Recurrent Neural Networks (RNNs) have achieved tremendous success in\nsequential data processing. However, it is quite challenging to interpret and\nverify RNNs' behaviors directly. To this end, many efforts have been made to\nextract finite automata from RNNs. Existing approaches such as exact learning\nare effective in extracting finite-state models to characterize the state\ndynamics of RNNs for formal languages, but are limited in the scalability to\nprocess natural languages. Compositional approaches that are scablable to\nnatural languages fall short in extraction precision. In this paper, we\nidentify the transition sparsity problem that heavily impacts the extraction\nprecision. To address this problem, we propose a transition rule extraction\napproach, which is scalable to natural language processing models and effective\nin improving extraction precision. Specifically, we propose an empirical method\nto complement the missing rules in the transition diagram. In addition, we\nfurther adjust the transition matrices to enhance the context-aware ability of\nthe extracted weighted finite automaton (WFA). Finally, we propose two data\naugmentation tactics to track more dynamic behaviors of the target RNN.\nExperiments on two popular natural language datasets show that our method can\nextract WFA from RNN for natural language processing with better precision than\nexisting approaches. Our code is available at\nhttps://github.com/weizeming/Extract_WFA_from_RNN_for_NL.", "published": "2022-06-27 09:30:13", "link": "http://arxiv.org/abs/2206.14621v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Zero-Shot Classification Approach for a Word-Guessing Challenge", "abstract": "The Taboo Challenge competition, a task based on the well-known Taboo game,\nhas been proposed to stimulate research in the AI field. The challenge requires\nbuilding systems able to comprehend the implied inferences between the\nexchanged messages of guesser and describer agents. A describer sends\npre-determined hints to guessers indirectly describing cities, and guessers are\nrequired to return the matching cities implied by the hints. Climbing up the\nscoring ledger requires the resolving of the highest amount of cities with the\nsmallest amount of hints in a specified time frame. Here, we present TabooLM, a\nlanguage-model approach that tackles the challenge based on a zero-shot\nsetting. We start by presenting and comparing the results of this approach with\nthree studies from the literature. The results show that our method achieves\nSOTA results on the Taboo challenge, suggesting that TabooLM can guess the\nimplied cities faster and more accurately than existing approaches.", "published": "2022-06-27 08:06:38", "link": "http://arxiv.org/abs/2206.13099v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.0; I.5.1"], "primary_category": "cs.LG"}
{"title": "TALCS: An Open-Source Mandarin-English Code-Switching Corpus and a\n  Speech Recognition Baseline", "abstract": "This paper introduces a new corpus of Mandarin-English code-switching speech\nrecognition--TALCS corpus, suitable for training and evaluating code-switching\nspeech recognition systems. TALCS corpus is derived from real online one-to-one\nEnglish teaching scenes in TAL education group, which contains roughly 587\nhours of speech sampled at 16 kHz. To our best knowledge, TALCS corpus is the\nlargest well labeled Mandarin-English code-switching open source automatic\nspeech recognition (ASR) dataset in the world. In this paper, we will introduce\nthe recording procedure in detail, including audio capturing devices and corpus\nenvironments. And the TALCS corpus is freely available for download under the\npermissive license1. Using TALCS corpus, we conduct ASR experiments in two\npopular speech recognition toolkits to make a baseline system, including ESPnet\nand Wenet. The Mixture Error Rate (MER) performance in the two speech\nrecognition toolkits is compared in TALCS corpus. The experimental results\nimplies that the quality of audio recordings and transcriptions are promising\nand the baseline system is workable.", "published": "2022-06-27 09:30:25", "link": "http://arxiv.org/abs/2206.13135v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Bi-VLDoc: Bidirectional Vision-Language Modeling for Visually-Rich\n  Document Understanding", "abstract": "Multi-modal document pre-trained models have proven to be very effective in a\nvariety of visually-rich document understanding (VrDU) tasks. Though existing\ndocument pre-trained models have achieved excellent performance on standard\nbenchmarks for VrDU, the way they model and exploit the interactions between\nvision and language on documents has hindered them from better generalization\nability and higher accuracy. In this work, we investigate the problem of\nvision-language joint representation learning for VrDU mainly from the\nperspective of supervisory signals. Specifically, a pre-training paradigm\ncalled Bi-VLDoc is proposed, in which a bidirectional vision-language\nsupervision strategy and a vision-language hybrid-attention mechanism are\ndevised to fully explore and utilize the interactions between these two\nmodalities, to learn stronger cross-modal document representations with richer\nsemantics. Benefiting from the learned informative cross-modal document\nrepresentations, Bi-VLDoc significantly advances the state-of-the-art\nperformance on three widely-used document understanding benchmarks, including\nForm Understanding (from 85.14% to 93.44%), Receipt Information Extraction\n(from 96.01% to 97.84%), and Document Classification (from 96.08% to 97.12%).\nOn Document Visual QA, Bi-VLDoc achieves the state-of-the-art performance\ncompared to previous single model methods.", "published": "2022-06-27 09:58:34", "link": "http://arxiv.org/abs/2206.13155v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Is the Language Familiarity Effect gradual? A computational modelling\n  approach", "abstract": "According to the Language Familiarity Effect (LFE), people are better at\ndiscriminating between speakers of their native language. Although this\ncognitive effect was largely studied in the literature, experiments have only\nbeen conducted on a limited number of language pairs and their results only\nshow the presence of the effect without yielding a gradual measure that may\nvary across language pairs. In this work, we show that the computational model\nof LFE introduced by Thorburn, Feldmand and Schatz (2019) can address these two\nlimitations. In a first experiment, we attest to this model's capacity to\nobtain a gradual measure of the LFE by replicating behavioural findings on\nnative and accented speech. In a second experiment, we evaluate LFE on a large\nnumber of language pairs, including many which have never been tested on\nhumans. We show that the effect is replicated across a wide array of languages,\nproviding further evidence of its universality. Building on the gradual measure\nof LFE, we also show that languages belonging to the same family yield smaller\nscores, supporting the idea of an effect of language distance on LFE.", "published": "2022-06-27 16:08:42", "link": "http://arxiv.org/abs/2206.13415v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Multilingual Dataset of COVID-19 Vaccination Attitudes on Twitter", "abstract": "Vaccine hesitancy is considered as one main cause of the stagnant uptake\nratio of COVID-19 vaccines in Europe and the US where vaccines are sufficiently\nsupplied. Fast and accurate grasp of public attitudes toward vaccination is\ncritical to address vaccine hesitancy, and social media platforms have proved\nto be an effective source of public opinions. In this paper, we describe the\ncollection and release of a dataset of tweets related to COVID-19 vaccines.\nThis dataset consists of the IDs of 2,198,090 tweets collected from Western\nEurope, 17,934 of which are annotated with the originators' vaccination\nstances. Our annotation will facilitate using and developing data-driven models\nto extract vaccination attitudes from social media posts and thus further\nconfirm the power of social media in public health surveillance. To lay the\ngroundwork for future research, we not only perform statistical analysis and\nvisualisation of our dataset, but also evaluate and compare the performance of\nestablished text-based benchmarks in vaccination stance extraction. We\ndemonstrate one potential use of our data in practice in tracking the temporal\nchanges of public COVID-19 vaccination attitudes.", "published": "2022-06-27 13:44:48", "link": "http://arxiv.org/abs/2206.14619v1", "categories": ["cs.CL", "cs.CY", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Instance Discriminative Learning for Depression Detection\n  from Speech Signals", "abstract": "Major Depressive Disorder (MDD) is a severe illness that affects millions of\npeople, and it is critical to diagnose this disorder as early as possible.\nDetecting depression from voice signals can be of great help to physicians and\ncan be done without any invasive procedure. Since relevant labelled data are\nscarce, we propose a modified Instance Discriminative Learning (IDL) method, an\nunsupervised pre-training technique, to extract augment-invariant and\ninstance-spread-out embeddings. In terms of learning augment-invariant\nembeddings, various data augmentation methods for speech are investigated, and\ntime-masking yields the best performance. To learn instance-spread-out\nembeddings, we explore methods for sampling instances for a training batch\n(distinct speaker-based and random sampling). It is found that the distinct\nspeaker-based sampling provides better performance than the random one, and we\nhypothesize that this result is because relevant speaker information is\npreserved in the embedding. Additionally, we propose a novel sampling strategy,\nPseudo Instance-based Sampling (PIS), based on clustering algorithms, to\nenhance spread-out characteristics of the embeddings. Experiments are conducted\nwith DepAudioNet on DAIC-WOZ (English) and CONVERGE (Mandarin) datasets, and\nstatistically significant improvements, with p-value 0.0015 and 0.05,\nrespectively, are observed using PIS in the detection of MDD relative to the\nbaseline without pre-training.", "published": "2022-06-27 02:05:48", "link": "http://arxiv.org/abs/2206.13016v1", "categories": ["eess.AS", "q-bio.QM"], "primary_category": "eess.AS"}
{"title": "Speak Like a Professional: Increasing Speech Intelligibility by\n  Mimicking Professional Announcer Voice with Voice Conversion", "abstract": "In most of practical scenarios, the announcement system must deliver speech\nmessages in a noisy environment, in which the background noise cannot be\ncancelled out. The local noise reduces speech intelligibility and increases\nlistening effort of the listener, hence hamper the effectiveness of\nannouncement system. There has been reported that voices of professional\nannouncers are clearer and more comprehensive than that of non-expert speakers\nin noisy environment. This finding suggests that the speech intelligibility\nmight be related to the speaking style of professional announcer, which can be\nadapted using voice conversion method. Motivated by this idea, this paper\nproposes a speech intelligibility enhancement in noisy environment by applying\nvoice conversion method on non-professional voice. We discovered that the\nprofessional announcers and non-professional speakers are clusterized into\ndifferent clusters on the speaker embedding plane. This implies that the speech\nintelligibility can be controlled as an independent feature of speaker\nindividuality. To examine the advantage of converted voice in noisy\nenvironment, we experimented using test words masked in pink noise at different\nSNR levels. The results of objective and subjective evaluations confirm that\nthe speech intelligibility of converted voice is higher than that of original\nvoice in low SNR conditions.", "published": "2022-06-27 02:39:04", "link": "http://arxiv.org/abs/2206.13021v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Extended U-Net for Speaker Verification in Noisy Environments", "abstract": "Background noise is a well-known factor that deteriorates the accuracy and\nreliability of speaker verification (SV) systems by blurring speech\nintelligibility. Various studies have used separate pretrained enhancement\nmodels as the front-end module of the SV system in noisy environments, and\nthese methods effectively remove noises. However, the denoising process of\nindependent enhancement models not tailored to the SV task can also distort the\nspeaker information included in utterances. We argue that the enhancement\nnetwork and speaker embedding extractor should be fully jointly trained for SV\ntasks under noisy conditions to alleviate this issue. Therefore, we proposed a\nU-Net-based integrated framework that simultaneously optimizes speaker\nidentification and feature enhancement losses. Moreover, we analyzed the\nstructural limitations of using U-Net directly for noise SV tasks and further\nproposed Extended U-Net to reduce these drawbacks. We evaluated the models on\nthe noise-synthesized VoxCeleb1 test set and VOiCES development set recorded in\nvarious noisy scenarios. The experimental results demonstrate that the\nU-Net-based fully joint training framework is more effective than the baseline,\nand the extended U-Net exhibited state-of-the-art performance versus the\nrecently proposed compensation systems.", "published": "2022-06-27 05:01:09", "link": "http://arxiv.org/abs/2206.13044v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Detection of Doctored Speech: Towards an End-to-End Parametric\n  Learn-able Filter Approach", "abstract": "The Automatic Speaker Verification systems have potential in biometrics\napplications for logical control access and authentication. A lot of things\nhappen to be at stake if the ASV system is compromised. The preliminary work\npresents a comparative analysis of the wavelet and MFCC-based state-of-the-art\nspoof detection techniques developed in these papers, respectively (Novoselov\net al., 2016) (Alam et al., 2016a). The results on ASVspoof 2015 justify our\ninclination towards wavelet-based features instead of MFCC features. The\nexperiments on the ASVspoof 2019 database show the lack of credibility of the\ntraditional handcrafted features and give us more reason to progress towards\nusing end-to-end deep neural networks and more recent techniques. We use\nSincnet architecture as our baseline. We get E2E deep learning models, which we\ncall WSTnet and CWTnet, respectively, by replacing the Sinc layer with the\nWavelet Scattering and Continuous wavelet transform layers. The fusion model\nachieved 62% and 17% relative improvement over traditional handcrafted models\nand our Sincnet baseline when evaluated on the modern spoofing attacks in\nASVspoof 2019.\n  The final scale distribution and the number of scales used in CWTnet are far\nfrom optimal for the task at hand. So to solve this problem, we replaced the\nCWT layer with a Wavelet Deconvolution(WD) (Khan and Yener, 2018) layer in our\nCWTnet architecture. This layer calculates the Discrete-Continuous Wavelet\nTransform similar to the CWTnet but also optimizes the scale parameter using\nback-propagation. The WDnet model achieved 26% and 7% relative improvement over\nCWTnet and Sincnet models respectively when evaluated over ASVspoof 2019\ndataset. This shows that more generalized features are extracted as compared to\nthe features extracted by CWTnet as only the most important and relevant\nfrequency regions are focused upon.", "published": "2022-06-27 06:28:46", "link": "http://arxiv.org/abs/2206.13066v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sequence-level Speaker Change Detection with Difference-based Continuous\n  Integrate-and-fire", "abstract": "Speaker change detection is an important task in multi-party interactions\nsuch as meetings and conversations. In this paper, we address the speaker\nchange detection task from the perspective of sequence transduction.\nSpecifically, we propose a novel encoder-decoder framework that directly\nconverts the input feature sequence to the speaker identity sequence. The\ndifference-based continuous integrate-and-fire mechanism is designed to support\nthis framework. It detects speaker changes by integrating the speaker\ndifference between the encoder outputs frame-by-frame and transfers encoder\noutputs to segment-level speaker embeddings according to the detected speaker\nchanges. The whole framework is supervised by the speaker identity sequence, a\nweaker label than the precise speaker change points. The experiments on the AMI\nand DIHARD-I corpora show that our sequence-level method consistently\noutperforms a strong frame-level baseline that uses the precise speaker change\nlabels.", "published": "2022-06-27 08:29:41", "link": "http://arxiv.org/abs/2206.13110v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A two-stage full-band speech enhancement model with effective spectral\n  compression mapping", "abstract": "The direct expansion of deep neural network (DNN) based wide-band speech\nenhancement (SE) to full-band processing faces the challenge of low frequency\nresolution in low frequency range, which would highly likely lead to\ndeteriorated performance of the model. In this paper, we propose a learnable\nspectral compression mapping (SCM) to effectively compress the high frequency\ncomponents so that they can be processed in a more efficient manner. By doing\nso, the model can pay more attention to low and middle frequency range, where\nmost of the speech power is concentrated. Instead of suppressing noise in a\nsingle network structure, we first estimate a spectral magnitude mask,\nconverting the speech to a high signal-to-ratio (SNR) state, and then utilize a\nsubsequent model to further optimize the real and imaginary mask of the\npre-enhanced signal. We conduct comprehensive experiments to validate the\nefficacy of the proposed method.", "published": "2022-06-27 09:30:53", "link": "http://arxiv.org/abs/2206.13136v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Similarity is Unreliable as a Proxy for Audio Quality", "abstract": "Many audio processing tasks require perceptual assessment. However, the time\nand expense of obtaining ``gold standard'' human judgments limit the\navailability of such data. Most applications incorporate full reference or\nother similarity-based metrics (e.g. PESQ) that depend on a clean reference.\nResearchers have relied on such metrics to evaluate and compare various\nproposed methods, often concluding that small, measured differences imply one\nis more effective than another. This paper demonstrates several practical\nscenarios where similarity metrics fail to agree with human perception, because\nthey: (1) vary with clean references; (2) rely on attributes that humans factor\nout when considering quality, and (3) are sensitive to imperceptible signal\nlevel differences. In those scenarios, we show that no-reference metrics do not\nsuffer from such shortcomings and correlate better with human perception. We\nconclude therefore that similarity serves as an unreliable proxy for audio\nquality.", "published": "2022-06-27 16:02:24", "link": "http://arxiv.org/abs/2206.13411v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Voice Activity Detection by Modeling Source and System\n  Information using Zero Frequency Filtering", "abstract": "Voice activity detection (VAD) is an important pre-processing step for speech\ntechnology applications. The task consists of deriving segment boundaries of\naudio signals which contain voicing information. In recent years, it has been\nshown that voice source and vocal tract system information can be extracted\nusing zero-frequency filtering (ZFF) without making any explicit model\nassumptions about the speech signal. This paper investigates the potential of\nzero-frequency filtering for jointly modeling voice source and vocal tract\nsystem information, and proposes two approaches for VAD. The first approach\ndemarcates voiced regions using a composite signal composed of different\nzero-frequency filtered signals. The second approach feeds the composite signal\nas input to the rVAD algorithm. These approaches are compared with other\nsupervised and unsupervised VAD methods in the literature, and are evaluated on\nthe Aurora-2 database, across a range of SNRs (20 to -5 dB). Our studies show\nthat the proposed ZFF-based methods perform comparable to state-of-art VAD\nmethods and are more invariant to added degradation and different channel\ncharacteristics.", "published": "2022-06-27 16:15:30", "link": "http://arxiv.org/abs/2206.13420v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "CopyCat2: A Single Model for Multi-Speaker TTS and Many-to-Many\n  Fine-Grained Prosody Transfer", "abstract": "In this paper, we present CopyCat2 (CC2), a novel model capable of: a)\nsynthesizing speech with different speaker identities, b) generating speech\nwith expressive and contextually appropriate prosody, and c) transferring\nprosody at fine-grained level between any pair of seen speakers. We do this by\nactivating distinct parts of the network for different tasks. We train our\nmodel using a novel approach to two-stage training. In Stage I, the model\nlearns speaker-independent word-level prosody representations from speech which\nit uses for many-to-many fine-grained prosody transfer. In Stage II, we learn\nto predict these prosody representations using the contextual information\navailable in text, thereby, enabling multi-speaker TTS with contextually\nappropriate prosody. We compare CC2 to two strong baselines, one in TTS with\ncontextually appropriate prosody, and one in fine-grained prosody transfer. CC2\nreduces the gap in naturalness between our baseline and copy-synthesised speech\nby $22.79\\%$. In fine-grained prosody transfer evaluations, it obtains a\nrelative improvement of $33.15\\%$ in target speaker similarity.", "published": "2022-06-27 16:48:23", "link": "http://arxiv.org/abs/2206.13443v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ClearBuds: Wireless Binaural Earbuds for Learning-Based Speech\n  Enhancement", "abstract": "We present ClearBuds, the first hardware and software system that utilizes a\nneural network to enhance speech streamed from two wireless earbuds. Real-time\nspeech enhancement for wireless earbuds requires high-quality sound separation\nand background cancellation, operating in real-time and on a mobile phone.\nClear-Buds bridges state-of-the-art deep learning for blind audio source\nseparation and in-ear mobile systems by making two key technical contributions:\n1) a new wireless earbud design capable of operating as a synchronized,\nbinaural microphone array, and 2) a lightweight dual-channel speech enhancement\nneural network that runs on a mobile device. Our neural network has a novel\ncascaded architecture that combines a time-domain conventional neural network\nwith a spectrogram-based frequency masking neural network to reduce the\nartifacts in the audio output. Results show that our wireless earbuds achieve a\nsynchronization error less than 64 microseconds and our network has a runtime\nof 21.4 milliseconds on an accompanying mobile phone. In-the-wild evaluation\nwith eight users in previously unseen indoor and outdoor multipath scenarios\ndemonstrates that our neural network generalizes to learn both spatial and\nacoustic cues to perform noise suppression and background speech removal. In a\nuser-study with 37 participants who spent over 15.4 hours rating 1041 audio\nsamples collected in-the-wild, our system achieves improved mean opinion score\nand background noise suppression.\n  Project page with demos: https://clearbuds.cs.washington.edu", "published": "2022-06-27 20:09:25", "link": "http://arxiv.org/abs/2206.13611v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Challenges and Opportunities in Multi-device Speech Processing", "abstract": "We review current solutions and technical challenges for automatic speech\nrecognition, keyword spotting, device arbitration, speech enhancement, and\nsource localization in multidevice home environments to provide context for the\nINTERSPEECH 2022 special session, \"Challenges and opportunities for signal\nprocessing and machine learning for multiple smart devices\". We also identify\nthe datasets needed to support these research areas. Based on the review and\nour research experience in the multi-device domain, we conclude with an outlook\non the future evolution", "published": "2022-06-27 19:18:43", "link": "http://arxiv.org/abs/2206.15432v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Joint Optimization of Sampling Rate Offsets Based on Entire Signal\n  Relationship Among Distributed Microphones", "abstract": "In this paper, we propose to simultaneously estimate all the sampling rate\noffsets (SROs) of multiple devices. In a distributed microphone array, the SRO\nis inevitable, which deteriorates the performance of array signal processing.\nMost of the existing SRO estimation methods focused on synchronizing two\nmicrophones. When synchronizing more than two microphones, we select one\nreference microphone and estimate the SRO of each non-reference microphone\nindependently. Hence, the relationship among signals observed by non-reference\nmicrophones is not considered. To address this problem, the proposed method\njointly optimizes all SROs based on a probabilistic model of a multichannel\nsignal. The SROs and model parameters are alternately updated to increase the\nlog-likelihood based on an auxiliary function. The effectiveness of the\nproposed method is validated on mixtures of various numbers of speakers.", "published": "2022-06-27 01:57:04", "link": "http://arxiv.org/abs/2206.13014v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Uncertainty Calibration for Deep Audio Classifiers", "abstract": "Although deep Neural Networks (DNNs) have achieved tremendous success in\naudio classification tasks, their uncertainty calibration are still\nunder-explored. A well-calibrated model should be accurate when it is certain\nabout its prediction and indicate high uncertainty when it is likely to be\ninaccurate. In this work, we investigate the uncertainty calibration for deep\naudio classifiers. In particular, we empirically study the performance of\npopular calibration methods: (i) Monte Carlo Dropout, (ii) ensemble, (iii)\nfocal loss, and (iv) spectral-normalized Gaussian process (SNGP), on audio\nclassification datasets. To this end, we evaluate (i-iv) for the tasks of\nenvironment sound and music genre classification. Results indicate that\nuncalibrated deep audio classifiers may be over-confident, and SNGP performs\nthe best and is very efficient on the two datasets of this paper.", "published": "2022-06-27 06:33:01", "link": "http://arxiv.org/abs/2206.13071v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound Model Factory: An Integrated System Architecture for Generative\n  Audio Modelling", "abstract": "We introduce a new system for data-driven audio sound model design built\naround two different neural network architectures, a Generative Adversarial\nNetwork(GAN) and a Recurrent Neural Network (RNN), that takes advantage of the\nunique characteristics of each to achieve the system objectives that neither is\ncapable of addressing alone. The objective of the system is to generate\ninteractively controllable sound models given (a) a range of sounds the model\nshould be able to synthesize, and (b) a specification of the parametric\ncontrols for navigating that space of sounds. The range of sounds is defined by\na dataset provided by the designer, while the means of navigation is defined by\na combination of data labels and the selection of a sub-manifold from the\nlatent space learned by the GAN. Our proposed system takes advantage of the\nrich latent space of a GAN that consists of sounds that fill out the spaces\n''between\" real data-like sounds. This augmented data from the GAN is then used\nto train an RNN for its ability to respond immediately and continuously to\nparameter changes and to generate audio over unlimited periods of time.\nFurthermore, we develop a self-organizing map technique for ``smoothing\" the\nlatent space of GAN that results in perceptually smooth interpolation between\naudio timbres. We validate this process through user studies. The system\ncontributes advances to the state of the art for generative sound model design\nthat include system configuration and components for improving interpolation\nand the expansion of audio modeling capabilities beyond musical pitch and\npercussive instrument sounds into the more complex space of audio textures.", "published": "2022-06-27 07:10:22", "link": "http://arxiv.org/abs/2206.13085v1", "categories": ["cs.SD", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SpeechEQ: Speech Emotion Recognition based on Multi-scale Unified\n  Datasets and Multitask Learning", "abstract": "Speech emotion recognition (SER) has many challenges, but one of the main\nchallenges is that each framework does not have a unified standard. In this\npaper, we propose SpeechEQ, a framework for unifying SER tasks based on a\nmulti-scale unified metric. This metric can be trained by Multitask Learning\n(MTL), which includes two emotion recognition tasks of Emotion States Category\n(EIS) and Emotion Intensity Scale (EIS), and two auxiliary tasks of phoneme\nrecognition and gender recognition. For this framework, we build a Mandarin SER\ndataset - SpeechEQ Dataset (SEQD). We conducted experiments on the public CASIA\nand ESD datasets in Mandarin, which exhibit that our method outperforms\nbaseline methods by a relatively large margin, yielding 8.0% and 6.5%\nimprovement in accuracy respectively. Additional experiments on IEMOCAP with\nfour emotion categories (i.e., angry, happy, sad, and neutral) also show the\nproposed method achieves a state-of-the-art of both weighted accuracy (WA) of\n78.16% and unweighted accuracy (UA) of 77.47%.", "published": "2022-06-27 08:11:54", "link": "http://arxiv.org/abs/2206.13101v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Wideband Audio Waveform Evaluation Networks: Efficient, Accurate\n  Estimation of Speech Qualities", "abstract": "Wideband Audio Waveform Evaluation Networks (WAWEnets) are convolutional\nneural networks that operate directly on wideband audio waveforms in order to\nproduce evaluations of those waveforms. In the present work these evaluations\ngive qualities of telecommunications speech (e.g., noisiness, intelligibility,\noverall speech quality). WAWEnets are no-reference networks because they do not\nrequire ``reference'' (original or undistorted) versions of the waveforms they\nevaluate. Our initial WAWEnet publication introduced four WAWEnets and each\nemulated the output of an established full-reference speech quality or\nintelligibility estimation algorithm. We have updated the WAWEnet architecture\nto be more efficient and effective. Here we present a single WAWEnet that\nclosely tracks seven different quality and intelligibility values. We create a\nsecond network that additionally tracks four subjective speech quality\ndimensions. We offer a third network that focuses on just subjective quality\nscores and achieves very high levels of agreement. This work has leveraged 334\nhours of speech in 13 languages, over two million full-reference target values\nand over 93,000 subjective mean opinion scores. We also interpret the operation\nof WAWEnets and identify the key to their operation using the language of\nsignal processing: ReLUs strategically move spectral information from non-DC\ncomponents into the DC component. The DC values of 96 output signals define a\nvector in a 96-D latent space and this vector is then mapped to a quality or\nintelligibility value for the input waveform.", "published": "2022-06-27 13:08:49", "link": "http://arxiv.org/abs/2206.13272v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Insights Into Deep Non-linear Filters for Improved Multi-channel Speech\n  Enhancement", "abstract": "The key advantage of using multiple microphones for speech enhancement is\nthat spatial filtering can be used to complement the tempo-spectral processing.\nIn a traditional setting, linear spatial filtering (beamforming) and\nsingle-channel post-filtering are commonly performed separately. In contrast,\nthere is a trend towards employing deep neural networks (DNNs) to learn a joint\nspatial and tempo-spectral non-linear filter, which means that the restriction\nof a linear processing model and that of a separate processing of spatial and\ntempo-spectral information can potentially be overcome. However, the internal\nmechanisms that lead to good performance of such data-driven filters for\nmulti-channel speech enhancement are not well understood. Therefore, in this\nwork, we analyse the properties of a non-linear spatial filter realized by a\nDNN as well as its interdependency with temporal and spectral processing by\ncarefully controlling the information sources (spatial, spectral, and temporal)\navailable to the network. We confirm the superiority of a non-linear spatial\nprocessing model, which outperforms an oracle linear spatial filter in a\nchallenging speaker extraction scenario for a low number of microphones by 0.24\nPOLQA score. Our analyses reveal that in particular spectral information should\nbe processed jointly with spatial information as this increases the spatial\nselectivity of the filter. Our systematic evaluation then leads to a simple\nnetwork architecture, that outperforms state-of-the-art network architectures\non a speaker extraction task by 0.22 POLQA score and by 0.32 POLQA score on the\nCHiME3 data.", "published": "2022-06-27 13:54:14", "link": "http://arxiv.org/abs/2206.13310v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Interpretable Acoustic Representation Learning on Breathing and Speech\n  Signals for COVID-19 Detection", "abstract": "In this paper, we describe an approach for representation learning of audio\nsignals for the task of COVID-19 detection. The raw audio samples are processed\nwith a bank of 1-D convolutional filters that are parameterized as cosine\nmodulated Gaussian functions. The choice of these kernels allows the\ninterpretation of the filterbanks as smooth band-pass filters. The filtered\noutputs are pooled, log-compressed and used in a self-attention based relevance\nweighting mechanism. The relevance weighting emphasizes the key regions of the\ntime-frequency decomposition that are important for the downstream task. The\nsubsequent layers of the model consist of a recurrent architecture and the\nmodels are trained for a COVID-19 detection task. In our experiments on the\nCoswara data set, we show that the proposed model achieves significant\nperformance improvements over the baseline system as well as other\nrepresentation learning approaches. Further, the approach proposed is shown to\nbe uniformly applicable for speech and breathing signals and for transfer\nlearning from a larger data set.", "published": "2022-06-27 15:20:51", "link": "http://arxiv.org/abs/2206.13365v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Avocodo: Generative Adversarial Network for Artifact-free Vocoder", "abstract": "Neural vocoders based on the generative adversarial neural network (GAN) have\nbeen widely used due to their fast inference speed and lightweight networks\nwhile generating high-quality speech waveforms. Since the perceptually\nimportant speech components are primarily concentrated in the low-frequency\nbands, most GAN-based vocoders perform multi-scale analysis that evaluates\ndownsampled speech waveforms. This multi-scale analysis helps the generator\nimprove speech intelligibility. However, in preliminary experiments, we\ndiscovered that the multi-scale analysis which focuses on the low-frequency\nbands causes unintended artifacts, e.g., aliasing and imaging artifacts, which\ndegrade the synthesized speech waveform quality. Therefore, in this paper, we\ninvestigate the relationship between these artifacts and GAN-based vocoders and\npropose a GAN-based vocoder, called Avocodo, that allows the synthesis of\nhigh-fidelity speech with reduced artifacts. We introduce two kinds of\ndiscriminators to evaluate speech waveforms in various perspectives: a\ncollaborative multi-band discriminator and a sub-band discriminator. We also\nutilize a pseudo quadrature mirror filter bank to obtain downsampled multi-band\nspeech waveforms while avoiding aliasing. According to experimental results,\nAvocodo outperforms baseline GAN-based vocoders, both objectively and\nsubjectively, while reproducing speech with fewer artifacts.", "published": "2022-06-27 15:54:41", "link": "http://arxiv.org/abs/2206.13404v3", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Few-Shot Cross-Lingual TTS Using Transferable Phoneme Embedding", "abstract": "This paper studies a transferable phoneme embedding framework that aims to\ndeal with the cross-lingual text-to-speech (TTS) problem under the few-shot\nsetting. Transfer learning is a common approach when it comes to few-shot\nlearning since training from scratch on few-shot training data is bound to\noverfit. Still, we find that the naive transfer learning approach fails to\nadapt to unseen languages under extremely few-shot settings, where less than 8\nminutes of data is provided. We deal with the problem by proposing a framework\nthat consists of a phoneme-based TTS model and a codebook module to project\nphonemes from different languages into a learned latent space. Furthermore, by\nutilizing phoneme-level averaged self-supervised learned features, we\neffectively improve the quality of synthesized speeches. Experiments show that\nusing 4 utterances, which is about 30 seconds of data, is enough to synthesize\nintelligible speech when adapting to an unseen language using our framework.", "published": "2022-06-27 11:24:40", "link": "http://arxiv.org/abs/2206.15427v2", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Impact of Acoustic Event Tagging on Scene Classification in a Multi-Task\n  Learning Framework", "abstract": "Acoustic events are sounds with well-defined spectro-temporal characteristics\nwhich can be associated with the physical objects generating them. Acoustic\nscenes are collections of such acoustic events in no specific temporal order.\nGiven this natural linkage between events and scenes, a common belief is that\nthe ability to classify events must help in the classification of scenes. This\nhas led to several efforts attempting to do well on Acoustic Event Tagging\n(AET) and Acoustic Scene Classification (ASC) using a multi-task network.\nHowever, in these efforts, improvement in one task does not guarantee an\nimprovement in the other, suggesting a tension between ASC and AET. It is\nunclear if improvements in AET translates to improvements in ASC. We explore\nthis conundrum through an extensive empirical study and show that under certain\nconditions, using AET as an auxiliary task in the multi-task network\nconsistently improves ASC performance. Additionally, ASC performance further\nimproves with the AET data-set size and is not sensitive to the choice of\nevents or the number of events in the AET data-set. We conclude that this\nimprovement in ASC performance comes from the regularization effect of using\nAET and not from the network's improved ability to discern between acoustic\nevents.", "published": "2022-06-27 17:38:25", "link": "http://arxiv.org/abs/2206.13476v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
