{"title": "The Complexity of Blocking All Solutions", "abstract": "We consider the general problem of blocking all solutions of some given\ncombinatorial problem with only few elements. For example, the problem of\ndestroying all maximum cliques of a given graph by forbidding only few\nvertices. Problems of this kind are so fundamental that they have been studied\nunder many different names in many different disjoint research communities\nalready since the 90s. Depending on the context, they have been called the\ninterdiction, most vital vertex, most vital edge, blocker, or vertex deletion\nproblem. Despite their apparent popularity, surprisingly little is known about\nthe computational complexity of interdiction problems in the case where the\noriginal problem is already NP-complete. In this paper, we fill that gap of\nknowledge by showing that a large amount of interdiction problems are even\nharder than NP-hard. Namely, they are complete for the second stage of\nStockmeyer's polynomial hierarchy, the complexity class $\\Sigma^p_2$. Such\ncomplexity insights are important because they imply that all these problems\ncan not be modelled by a compact integer program (unless the unlikely\nconjecture NP $= \\Sigma_2^p$ holds). Concretely, we prove\n$\\Sigma^p_2$-completeness of the following interdiction problems:\nsatisfiability, 3satisfiability, dominating set, set cover, hitting set,\nfeedback vertex set, feedback arc set, uncapacitated facility location,\n$p$-center, $p$-median, independent set, clique, subset sum, knapsack,\nHamiltonian path/cycle (directed/undirected), TSP, $k$ directed vertex disjoint\npath ($k \\geq 2$), Steiner tree. We show that all of these problems share an\nabstract property which implies that their interdiction counterpart is\n$\\Sigma_2^p$-complete. Thus, all of these problems are $\\Sigma_2^p$-complete\n\\enquote{for the same reason}. Our result extends a recent framework by Gr\\\"une\nand Wulf.", "published": "2025-02-07 21:38:17", "link": "http://arxiv.org/abs/2502.05348v1", "categories": ["cs.CC", "cs.DM", "math.OC", "F.2.2"], "primary_category": "cs.CC"}
{"title": "Induced Disjoint Paths Without an Induced Minor", "abstract": "We exhibit a new obstacle to the nascent algorithmic theory for classes\nexcluding an induced minor. We indeed show that on the class of string graphs\n-- which avoids the 1-subdivision of, say, $K_5$ as an induced minor -- Induced\n2-Disjoint Paths is NP-complete. So, while $k$-Disjoint Paths, for a fixed $k$,\nis polynomial-time solvable in general graphs, the absence of a graph as an\ninduced minor does not make its induced variant tractable, even for $k=2$. This\nanswers a question of Korhonen and Lokshtanov [SODA '24], and complements a\npolynomial-time algorithm for Induced $k$-Disjoint Paths in classes of bounded\ngenus by Kobayashi and Kawarabayashi [SODA '09]. In addition to being string\ngraphs, our produced hard instances are subgraphs of a constant power of\nbounded-degree planar graphs, hence have bounded twin-width and bounded maximum\ndegree.\n  We also leverage our new result to show that there is a fixed subcubic graph\n$H$ such that deciding if an input graph contains $H$ as an induced subdivision\nis NP-complete. Until now, all the graphs $H$ for which such a statement was\nknown had a vertex of degree at least 4. This answers a question by Chudnovsky,\nSeymour, and the fourth author [JCTB '13], and by Le [JGT '19]. Finally we\nresolve another question of Korhonen and Lokshtanov by exhibiting a subcubic\ngraph $H$ without two adjacent degree-3 vertices and such that deciding if an\ninput $n$-vertex graph contains $H$ as an induced minor is NP-complete, and\nunless the Exponential-Time Hypothesis fails, requires time $2^{\\Omega(\\sqrt\nn)}$. This complements an algorithm running in subexponential time\n$2^{O(n^{2/3} \\log n)}$ by these authors [SODA '24] under the same technical\ncondition.", "published": "2025-02-07 19:45:26", "link": "http://arxiv.org/abs/2502.05289v1", "categories": ["cs.CC", "cs.DM", "math.CO", "68Q25", "F.2.2"], "primary_category": "cs.CC"}
{"title": "A Randomised Approach to Distributed Sorting", "abstract": "We introduce and analyse a new, extremely simple, randomised sorting\nalgorithm:\n  - choose a pair of indices $\\{i, j\\}$ according to some distribution $q$;\n  - sort the elements in positions $i$ and $j$ of the array in ascending order.\n  Choosing $q_{\\{i,j\\}} \\propto 1/|j - i|$ yields an order-$n (\\log n)^2$\nsorting time. We call it the harmonic sorter.\n  The sorter trivially parallelises in the asynchronous setting, yielding a\nlinear speed-up. We also exhibit a low-communication, synchronous version with\na linear speed-up.\n  We compare and contrast this algorithm with other sorters, and discuss some\nof its benefits, particularly its robustness and amenability to parallelisation\nand distributed computing.", "published": "2025-02-07 16:57:48", "link": "http://arxiv.org/abs/2502.05082v1", "categories": ["cs.DS", "cs.DC", "cs.DM", "math.PR"], "primary_category": "cs.DS"}
{"title": "Robust valuation and optimal harvesting of forestry resources in the presence of catastrophe risk and parameter uncertainty", "abstract": "We determine forest lease value and optimal harvesting strategies under model\nparameter uncertainty within stochastic bio-economic models that account for\ncatastrophe risk. Catastrophic events are modeled as a Poisson point process,\nwith a two-factor stochastic convenience yield model capturing the lumber spot\nprice dynamics. Using lumber futures and US wildfire data, we estimate model\nparameters through a Kalman filter and maximum likelihood estimation and define\nthe model parameter uncertainty set as the 95% confidence region. We\nnumerically determine the forest lease value under catastrophe risk and\nparameter uncertainty using reflected backward stochastic differential\nequations (RBSDEs) and establish conservative and optimistic bounds for lease\nvalues and optimal stopping boundaries for harvesting, facilitating Monte Carlo\nsimulations. Numerical experiments further explore how parameter uncertainty,\ncatastrophe intensity, and carbon sequestration impact the lease valuation and\nharvesting decision. In particular, we explore the costs arising from this form\nof uncertainty in the form of a reduction of the lease value. These are\nimplicit costs that can be attributed to climate risk and will be emphasized\nthrough the importance of forestry resources in the energy transition process.\nWe conclude that in the presence of parameter uncertainty, it is better to lean\ntoward a conservative strategy reflecting, to some extent, the worst case than\nbeing overly optimistic. Our results also highlight the critical role of\nconvenience yield in determining optimal harvesting strategies.", "published": "2025-02-07 21:24:27", "link": "http://arxiv.org/abs/2502.05340v1", "categories": ["q-fin.MF", "econ.GN", "q-fin.EC"], "primary_category": "q-fin.MF"}
{"title": "Phonetic Reconstruction of the Consonant System of Middle Chinese via\n  Mixed Integer Optimization", "abstract": "This paper is concerned with phonetic reconstruction of the consonant system\nof Middle Chinese. We propose to cast the problem as a Mixed Integer\nProgramming problem, which is able to automatically explore homophonic\ninformation from ancient rhyme dictionaries and phonetic information from\nmodern Chinese dialects, the descendants of Middle Chinese. Numerical\nevaluation on a wide range of synthetic and real data demonstrates the\neffectiveness and robustness of the new method. We apply the method to\ninformation from Guangyun and 20 modern Chinese dialects to obtain a new\nphonetic reconstruction result. A linguistically-motivated discussion of this\nresult is also provided.", "published": "2025-02-07 02:51:43", "link": "http://arxiv.org/abs/2502.04625v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Before It's Too Late: A State Space Model for the Early Prediction of\n  Misinformation and Disinformation Engagement", "abstract": "In today's digital age, conspiracies and information campaigns can emerge\nrapidly and erode social and democratic cohesion. While recent deep learning\napproaches have made progress in modeling engagement through language and\npropagation models, they struggle with irregularly sampled data and early\ntrajectory assessment. We present IC-Mamba, a novel state space model that\nforecasts social media engagement by modeling interval-censored data with\nintegrated temporal embeddings. Our model excels at predicting engagement\npatterns within the crucial first 15-30 minutes of posting (RMSE 0.118-0.143),\nenabling rapid assessment of content reach. By incorporating interval-censored\nmodeling into the state space framework, IC-Mamba captures fine-grained\ntemporal dynamics of engagement growth, achieving a 4.72% improvement over\nstate-of-the-art across multiple engagement metrics (likes, shares, comments,\nand emojis). Our experiments demonstrate IC-Mamba's effectiveness in\nforecasting both post-level dynamics and broader narrative patterns (F1\n0.508-0.751 for narrative-level predictions). The model maintains strong\npredictive performance across extended time horizons, successfully forecasting\nopinion-level engagement up to 28 days ahead using observation windows of 3-10\ndays. These capabilities enable earlier identification of potentially\nproblematic content, providing crucial lead time for designing and implementing\ncountermeasures. Code is available at: https://github.com/ltian678/ic-mamba. An\ninteractive dashboard demonstrating our results is available at:\nhttps://ic-mamba.behavioral-ds.science.", "published": "2025-02-07 04:43:11", "link": "http://arxiv.org/abs/2502.04655v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Text Style Transfer Evaluation: Are There Any Reliable\n  Metrics?", "abstract": "Text Style Transfer (TST) is the task of transforming a text to reflect a\nparticular style while preserving its original content. Evaluating TST outputs\nis a multidimensional challenge, requiring the assessment of style transfer\naccuracy, content preservation, and naturalness. Using human evaluation is\nideal but costly, same as in other natural language processing (NLP) tasks,\nhowever, automatic metrics for TST have not received as much attention as\nmetrics for, e.g., machine translation or summarization. In this paper, we\nexamine both set of existing and novel metrics from broader NLP tasks for TST\nevaluation, focusing on two popular subtasks-sentiment transfer and\ndetoxification-in a multilingual context comprising English, Hindi, and\nBengali. By conducting meta-evaluation through correlation with human\njudgments, we demonstrate the effectiveness of these metrics when used\nindividually and in ensembles. Additionally, we investigate the potential of\nLarge Language Models (LLMs) as tools for TST evaluation. Our findings\nhighlight that certain advanced NLP metrics and experimental-hybrid-techniques,\nprovide better insights than existing TST metrics for delivering more accurate,\nconsistent, and reproducible TST evaluations.", "published": "2025-02-07 07:39:17", "link": "http://arxiv.org/abs/2502.04718v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SeDi-Instruct: Enhancing Alignment of Language Models through\n  Self-Directed Instruction Generation", "abstract": "The rapid evolution of Large Language Models (LLMs) has enabled the industry\nto develop various AI-based services. Instruction tuning is considered\nessential in adapting foundation models for target domains to provide\nhigh-quality services to customers. A key challenge in instruction tuning is\nobtaining high-quality instruction data. Self-Instruct, which automatically\ngenerates instruction data using ChatGPT APIs, alleviates the data scarcity\nproblem. To improve the quality of instruction data, Self-Instruct discards\nmany of the instructions generated from ChatGPT, even though it is inefficient\nin terms of cost owing to many useless API calls. To generate high-quality\ninstruction data at a low cost, we propose a novel data generation framework,\nSelf-Direct Instruction generation (SeDi-Instruct), which employs\ndiversity-based filtering and iterative feedback task generation.\nDiversity-based filtering maintains model accuracy without excessively\ndiscarding low-quality generated instructions by enhancing the diversity of\ninstructions in a batch. This reduces the cost of synthesizing instruction\ndata. The iterative feedback task generation integrates instruction generation\nand training tasks and utilizes information obtained during the training to\ncreate high-quality instruction sets. Our results show that SeDi-Instruct\nenhances the accuracy of AI models by 5.2%, compared with traditional methods,\nwhile reducing data generation costs by 36%.", "published": "2025-02-07 09:20:11", "link": "http://arxiv.org/abs/2502.04774v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing Internal Representations of Multi-Word Verbs in Large Language\n  Models", "abstract": "This study investigates the internal representations of verb-particle\ncombinations, called multi-word verbs, within transformer-based large language\nmodels (LLMs), specifically examining how these models capture lexical and\nsyntactic properties at different neural network layers. Using the BERT\narchitecture, we analyze the representations of its layers for two different\nverb-particle constructions: phrasal verbs like 'give up' and prepositional\nverbs like 'look at'. Our methodology includes training probing classifiers on\nthe internal representations to classify these categories at both word and\nsentence levels. The results indicate that the model's middle layers achieve\nthe highest classification accuracies. To further analyze the nature of these\ndistinctions, we conduct a data separability test using the Generalized\nDiscrimination Value (GDV). While GDV results show weak linear separability\nbetween the two verb types, probing classifiers still achieve high accuracy,\nsuggesting that representations of these linguistic categories may be\nnon-linearly separable. This aligns with previous research indicating that\nlinguistic distinctions in neural networks are not always encoded in a linearly\nseparable manner. These findings computationally support usage-based claims on\nthe representation of verb-particle constructions and highlight the complex\ninteraction between neural network architectures and linguistic structures.", "published": "2025-02-07 09:49:13", "link": "http://arxiv.org/abs/2502.04789v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Developmentally-plausible Working Memory Shapes a Critical Period for\n  Language Acquisition", "abstract": "Large language models possess general linguistic abilities but acquire\nlanguage less efficiently than humans. This study proposes a method for\nintegrating the developmental characteristics of working memory during the\ncritical period, a stage when human language acquisition is particularly\nefficient, into the training process of language models. The proposed method\nintroduces a mechanism that initially constrains working memory during the\nearly stages of training and gradually relaxes this constraint in an\nexponential manner as learning progresses. Targeted syntactic evaluation shows\nthat the proposed method outperforms conventional methods without memory\nconstraints or with static memory constraints. These findings not only provide\nnew directions for designing data-efficient language models but also offer\nindirect evidence supporting the role of the developmental characteristics of\nworking memory as the underlying mechanism of the critical period in language\nacquisition.", "published": "2025-02-07 09:58:58", "link": "http://arxiv.org/abs/2502.04795v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Rationalization in the Wild: A Large Scale Out-of-Distribution\n  Evaluation on NLI-related tasks", "abstract": "Free-text explanations are expressive and easy to understand, but many\ndatasets lack annotated explanation data, making it challenging to train models\nfor explainable predictions. To address this, we investigate how to use\nexisting explanation datasets for self-rationalization and evaluate models'\nout-of-distribution (OOD) performance. We fine-tune T5-Large and OLMo-7B models\nand assess the impact of fine-tuning data quality, the number of fine-tuning\nsamples, and few-shot selection methods. The models are evaluated on 19 diverse\nOOD datasets across three tasks: natural language inference (NLI),\nfact-checking, and hallucination detection in abstractive summarization. For\nthe generated explanation evaluation, we conduct a human study on 13 selected\nmodels and study its correlation with the Acceptability score (T5-11B) and\nthree other LLM-based reference-free metrics. Human evaluation shows that the\nAcceptability score correlates most strongly with human judgments,\ndemonstrating its effectiveness in evaluating free-text explanations. Our\nfindings reveal: 1) few annotated examples effectively adapt models for OOD\nexplanation generation; 2) compared to sample selection strategies, fine-tuning\ndata source has a larger impact on OOD performance; and 3) models with higher\nlabel prediction accuracy tend to produce better explanations, as reflected by\nhigher Acceptability scores.", "published": "2025-02-07 10:01:32", "link": "http://arxiv.org/abs/2502.04797v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Disinformation Detection with Explainable AI and Named Entity\n  Replacement", "abstract": "The automatic detection of disinformation presents a significant challenge in\nthe field of natural language processing. This task addresses a multifaceted\nsocietal and communication issue, which needs approaches that extend beyond the\nidentification of general linguistic patterns through data-driven algorithms.\nIn this research work, we hypothesise that text classification methods are not\nable to capture the nuances of disinformation and they often ground their\ndecision in superfluous features. Hence, we apply a post-hoc explainability\nmethod (SHAP, SHapley Additive exPlanations) to identify spurious elements with\nhigh impact on the classification models. Our findings show that\nnon-informative elements (e.g., URLs and emoticons) should be removed and named\nentities (e.g., Rwanda) should be pseudo-anonymized before training to avoid\nmodels' bias and increase their generalization capabilities. We evaluate this\nmethodology with internal dataset and external dataset before and after\napplying extended data preprocessing and named entity replacement. The results\nshow that our proposal enhances on average the performance of a disinformation\nclassification method with external test data in 65.78% without a significant\ndecrease of the internal test performance.", "published": "2025-02-07 12:01:26", "link": "http://arxiv.org/abs/2502.04863v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "pytopicgram: A library for data extraction and topic modeling from\n  Telegram channels", "abstract": "Telegram is a popular platform for public communication, generating large\namounts of messages through its channels. pytopicgram is a Python library that\nhelps researchers collect, organize, and analyze these Telegram messages. The\nlibrary offers key features such as easy message retrieval, detailed channel\ninformation, engagement metrics, and topic identification using advanced\nmodeling techniques. By simplifying data extraction and analysis, pytopicgram\nallows users to understand how content spreads and how audiences interact on\nTelegram. This paper describes the design, main features, and practical uses of\n\\pytopicgram, showcasing its effectiveness for studying public conversations on\nTelegram.", "published": "2025-02-07 12:41:47", "link": "http://arxiv.org/abs/2502.04882v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Claim Extraction for Fact-Checking: Data, Models, and Automated Metrics", "abstract": "In this paper, we explore the problem of Claim Extraction using one-to-many\ntext generation methods, comparing LLMs, small summarization models finetuned\nfor the task, and a previous NER-centric baseline QACG. As the current\npublications on Claim Extraction, Fact Extraction, Claim Generation and\nCheck-worthy Claim Detection are quite scattered in their means and\nterminology, we compile their common objectives, releasing the FEVERFact\ndataset, with 17K atomic factual claims extracted from 4K contextualised\nWikipedia sentences, adapted from the original FEVER. We compile the known\nobjectives into an Evaluation framework of: Atomicity, Fluency,\nDecontextualization, Faithfulness checked for each generated claim separately,\nand Focus and Coverage measured against the full set of predicted claims for a\nsingle input. For each metric, we implement a scale using a reduction to an\nalready-explored NLP task. We validate our metrics against human grading of\ngeneric claims, to see that the model ranking on $F_{fact}$, our hardest\nmetric, did not change and the evaluation framework approximates human grading\nvery closely in terms of $F_1$ and RMSE.", "published": "2025-02-07 14:20:45", "link": "http://arxiv.org/abs/2502.04955v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SSMLoRA: Enhancing Low-Rank Adaptation with State Space Model", "abstract": "Fine-tuning is a key approach for adapting language models to specific\ndownstream tasks, but updating all model parameters becomes impractical as\nmodel sizes increase. Parameter-Efficient Fine-Tuning (PEFT) methods, such as\nLow-Rank Adaptation (LoRA), address this challenge by introducing additional\nadaptation parameters into pre-trained weight matrices. However, LoRA's\nperformance varies across different insertion points within the model,\nhighlighting potential parameter inefficiency due to unnecessary insertions. To\nthis end, we propose SSMLoRA (State Space Model Low-Rank Adaptation), an\nextension of LoRA that incorporates a State Space Model (SSM) to interconnect\nlow-rank matrices. SSMLoRA ensures that performance is maintained even with\nsparser insertions. SSMLoRA allows the model to not only map inputs to a\nlow-rank space for better feature extraction but also leverage the computations\nfrom the previous low-rank space. Our method achieves comparable performance to\nLoRA on the General Language Understanding Evaluation (GLUE) benchmark while\nusing only half the parameters. Additionally, due to its structure, SSMLoRA\nshows promise in handling tasks with longer input sequences. .You can find our\ncode here:https://github.com/yuhkalhic/SSMLoRA.", "published": "2025-02-07 14:22:35", "link": "http://arxiv.org/abs/2502.04958v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Commonality and Individuality! Integrating Humor Commonality with\n  Speaker Individuality for Humor Recognition", "abstract": "Humor recognition aims to identify whether a specific speaker's text is\nhumorous. Current methods for humor recognition mainly suffer from two\nlimitations: (1) they solely focus on one aspect of humor commonalities,\nignoring the multifaceted nature of humor; and (2) they typically overlook the\ncritical role of speaker individuality, which is essential for a comprehensive\nunderstanding of humor expressions. To bridge these gaps, we introduce the\nCommonality and Individuality Incorporated Network for Humor Recognition\n(CIHR), a novel model designed to enhance humor recognition by integrating\nmultifaceted humor commonalities with the distinctive individuality of\nspeakers. The CIHR features a Humor Commonality Analysis module that explores\nvarious perspectives of multifaceted humor commonality within user texts, and a\nSpeaker Individuality Extraction module that captures both static and dynamic\naspects of a speaker's profile to accurately model their distinctive\nindividuality. Additionally, Static and Dynamic Fusion modules are introduced\nto effectively incorporate the humor commonality with speaker's individuality\nin the humor recognition process. Extensive experiments demonstrate the\neffectiveness of CIHR, underscoring the importance of concurrently addressing\nboth multifaceted humor commonality and distinctive speaker individuality in\nhumor recognition.", "published": "2025-02-07 14:23:49", "link": "http://arxiv.org/abs/2502.04960v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoCoA: A Generalized Approach to Uncertainty Quantification by\n  Integrating Confidence and Consistency of LLM Outputs", "abstract": "Uncertainty quantification (UQ) methods for Large Language Models (LLMs)\nencompasses a variety of approaches, with two major types being particularly\nprominent: information-based, which focus on model confidence expressed as\ntoken probabilities, and consistency-based, which assess the semantic\nrelationship between multiple outputs generated using repeated sampling.\nSeveral recent methods have combined these two approaches and shown impressive\nperformance in various applications. However, they sometimes fail to outperform\nmuch simpler baseline methods. Our investigation reveals distinctive\ncharacteristics of LLMs as probabilistic models, which help to explain why\nthese UQ methods underperform in certain tasks. Based on these findings, we\npropose a new way of synthesizing model confidence and output consistency that\nleads to a family of efficient and robust UQ methods. We evaluate our approach\nacross a variety of tasks such as question answering, abstractive\nsummarization, and machine translation, demonstrating sizable improvements over\nstate-of-the-art UQ approaches.", "published": "2025-02-07 14:30:12", "link": "http://arxiv.org/abs/2502.04964v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "nvAgent: Automated Data Visualization from Natural Language via\n  Collaborative Agent Workflow", "abstract": "Natural Language to Visualization (NL2Vis) seeks to convert natural-language\ndescriptions into visual representations of given tables, empowering users to\nderive insights from large-scale data. Recent advancements in Large Language\nModels (LLMs) show promise in automating code generation to transform tabular\ndata into accessible visualizations. However, they often struggle with complex\nqueries that require reasoning across multiple tables. To address this\nlimitation, we propose a collaborative agent workflow, termed nvAgent, for\nNL2Vis. Specifically, nvAgent comprises three agents: a processor agent for\ndatabase processing and context filtering, a composer agent for planning\nvisualization generation, and a validator agent for code translation and output\nverification. Comprehensive evaluations on the new VisEval benchmark\ndemonstrate that nvAgent consistently surpasses state-of-the-art baselines,\nachieving a 7.88% improvement in single-table and a 9.23% improvement in\nmulti-table scenarios. Qualitative analyses further highlight that nvAgent\nmaintains nearly a 20% performance margin over previous models, underscoring\nits capacity to produce high-quality visual representations from complex,\nheterogeneous data sources.", "published": "2025-02-07 16:03:08", "link": "http://arxiv.org/abs/2502.05036v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GiesKaNe: Bridging Past and Present in Grammatical Theory and Practical\n  Application", "abstract": "This article explores the requirements for corpus compilation within the\nGiesKaNe project (University of Giessen and Kassel, Syntactic Basic Structures\nof New High German). The project is defined by three central characteristics:\nit is a reference corpus, a historical corpus, and a syntactically deeply\nannotated treebank. As a historical corpus, GiesKaNe aims to establish\nconnections with both historical and contemporary corpora, ensuring its\nrelevance across temporal and linguistic contexts. The compilation process\nstrikes the balance between innovation and adherence to standards, addressing\nboth internal project goals and the broader interests of the research\ncommunity. The methodological complexity of such a project is managed through a\ncomplementary interplay of human expertise and machine-assisted processes. The\narticle discusses foundational topics such as tokenization, normalization,\nsentence definition, tagging, parsing, and inter-annotator agreement, alongside\nadvanced considerations. These include comparisons between grammatical models,\nannotation schemas, and established de facto annotation standards as well as\nthe integration of human and machine collaboration. Notably, a novel method for\nmachine-assisted classification of texts along the continuum of conceptual\norality and literacy is proposed, offering new perspectives on text selection.\nFurthermore, the article introduces an approach to deriving de facto standard\nannotations from existing ones, mediating between standardization and\ninnovation. In the course of describing the workflow the article demonstrates\nthat even ambitious projects like GiesKaNe can be effectively implemented using\nexisting research infrastructure, requiring no specialized annotation tools.\nInstead, it is shown that the workflow can be based on the strategic use of a\nsimple spreadsheet and integrates the capabilities of the existing\ninfrastructure.", "published": "2025-02-07 17:35:33", "link": "http://arxiv.org/abs/2502.05113v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CodeSCM: Causal Analysis for Multi-Modal Code Generation", "abstract": "In this paper, we propose CodeSCM, a Structural Causal Model (SCM) for\nanalyzing multi-modal code generation using large language models (LLMs). By\napplying interventions to CodeSCM, we measure the causal effects of different\nprompt modalities, such as natural language, code, and input-output examples,\non the model. CodeSCM introduces latent mediator variables to separate the code\nand natural language semantics of a multi-modal code generation prompt. Using\nthe principles of Causal Mediation Analysis on these mediators we quantify\ndirect effects representing the model's spurious leanings. We find that, in\naddition to natural language instructions, input-output examples significantly\ninfluence code generation.", "published": "2025-02-07 18:26:15", "link": "http://arxiv.org/abs/2502.05150v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NoLiMa: Long-Context Evaluation Beyond Literal Matching", "abstract": "Recent large language models (LLMs) support long contexts ranging from 128K\nto 1M tokens. A popular method for evaluating these capabilities is the\nneedle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\"\n(relevant information) from a \"haystack\" (long irrelevant context). Extensions\nof this approach include increasing distractors, fact chaining, and in-context\nreasoning. However, in these benchmarks, models can exploit existing literal\nmatches between the needle and haystack to simplify the task. To address this,\nwe introduce NoLiMa, a benchmark extending NIAH with a carefully designed\nneedle set, where questions and needles have minimal lexical overlap, requiring\nmodels to infer latent associations to locate the needle within the haystack.\nWe evaluate 12 popular LLMs that claim to support contexts of at least 128K\ntokens. While they perform well in short contexts (<1K), performance degrades\nsignificantly as context length increases. At 32K, for instance, 10 models drop\nbelow 50% of their strong short-length baselines. Even GPT-4o, one of the\ntop-performing exceptions, experiences a reduction from an almost-perfect\nbaseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the\nincreased difficulty the attention mechanism faces in longer contexts when\nliteral matches are absent, making it harder to retrieve relevant information.\nWe publicly release the dataset and evaluation code at\nhttps://github.com/adobe-research/NoLiMa.", "published": "2025-02-07 18:49:46", "link": "http://arxiv.org/abs/2502.05167v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can LLMs Rank the Harmfulness of Smaller LLMs? We are Not There Yet", "abstract": "Large language models (LLMs) have become ubiquitous, thus it is important to\nunderstand their risks and limitations. Smaller LLMs can be deployed where\ncompute resources are constrained, such as edge devices, but with different\npropensity to generate harmful output. Mitigation of LLM harm typically depends\non annotating the harmfulness of LLM output, which is expensive to collect from\nhumans. This work studies two questions: How do smaller LLMs rank regarding\ngeneration of harmful content? How well can larger LLMs annotate harmfulness?\nWe prompt three small LLMs to elicit harmful content of various types, such as\ndiscriminatory language, offensive content, privacy invasion, or negative\ninfluence, and collect human rankings of their outputs. Then, we evaluate three\nstate-of-the-art large LLMs on their ability to annotate the harmfulness of\nthese responses. We find that the smaller models differ with respect to\nharmfulness. We also find that large LLMs show low to moderate agreement with\nhumans. These findings underline the need for further work on harm mitigation\nin LLMs.", "published": "2025-02-07 19:50:02", "link": "http://arxiv.org/abs/2502.05291v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Tuned LLMs are \"Time Capsules\" for Tracking Societal Bias Through\n  Books", "abstract": "Books, while often rich in cultural insights, can also mirror societal biases\nof their eras - biases that Large Language Models (LLMs) may learn and\nperpetuate during training. We introduce a novel method to trace and quantify\nthese biases using fine-tuned LLMs. We develop BookPAGE, a corpus comprising\n593 fictional books across seven decades (1950-2019), to track bias evolution.\nBy fine-tuning LLMs on books from each decade and using targeted prompts, we\nexamine shifts in biases related to gender, sexual orientation, race, and\nreligion. Our findings indicate that LLMs trained on decade-specific books\nmanifest biases reflective of their times, with both gradual trends and notable\nshifts. For example, model responses showed a progressive increase in the\nportrayal of women in leadership roles (from 8% to 22%) from the 1950s to\n2010s, with a significant uptick in the 1990s (from 4% to 12%), possibly\naligning with third-wave feminism. Same-sex relationship references increased\nmarkedly from the 1980s to 2000s (from 0% to 10%), mirroring growing LGBTQ+\nvisibility. Concerningly, negative portrayals of Islam rose sharply in the\n2000s (26% to 38%), likely reflecting post-9/11 sentiments. Importantly, we\ndemonstrate that these biases stem mainly from the books' content and not the\nmodels' architecture or initial training. Our study offers a new perspective on\nsocietal bias trends by bridging AI, literary studies, and social science\nresearch.", "published": "2025-02-07 21:13:27", "link": "http://arxiv.org/abs/2502.05331v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probabilistic Subspace Manifolds for Contextual Inference in Large\n  Language Models", "abstract": "Representing token embeddings as probability distributions over learned\nmanifolds allows for more flexible contextual inference, reducing\nrepresentational rigidity while enhancing semantic granularity. Comparative\nevaluations demonstrate that probabilistic embeddings improve neighborhood\nconsistency and decrease redundancy, ensuring that token relationships remain\nmore structurally coherent across fine-tuning iterations. The integration of\nprobabilistic subspaces within attention mechanisms facilitates more adaptive\ncontextual weighting, enabling models to capture latent dependencies that would\notherwise be obscured in conventional embeddings. Experimental results\nhighlight increased robustness against adversarial modifications, with\nprobabilistic embeddings preserving contextual integrity even under\nperturbation-based evaluation scenarios. Performance assessments indicate that\nprobabilistic representations achieve greater adaptability in domain-specific\napplications, mitigating the need for extensive retraining when shifting across\nlinguistic domains. Computational trade-offs remain within operationally\nfeasible limits, with marginal increases in inference latency balanced against\nthe benefits of enhanced representation stability and contextual\nexpressiveness. The capacity to encode structured uncertainty provides\nadvantages in generative modeling tasks, particularly where maintaining\ncoherence across extended sequences requires a representation framework capable\nof handling ambiguous or context-dependent linguistic constructs.", "published": "2025-02-07 21:32:32", "link": "http://arxiv.org/abs/2502.05346v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Regulation and Requesting Interventions", "abstract": "Human intelligence involves metacognitive abilities like self-regulation,\nrecognizing limitations, and seeking assistance only when needed. While LLM\nAgents excel in many domains, they often lack this awareness. Overconfident\nagents risk catastrophic failures, while those that seek help excessively\nhinder efficiency. A key challenge is enabling agents with a limited\nintervention budget $C$ is to decide when to request assistance. In this paper,\nwe propose an offline framework that trains a \"helper\" policy to request\ninterventions, such as more powerful models or test-time compute, by combining\nLLM-based process reward models (PRMs) with tabular reinforcement learning.\nUsing state transitions collected offline, we score optimal intervention timing\nwith PRMs and train the helper model on these labeled trajectories. This\noffline approach significantly reduces costly intervention calls during\ntraining. Furthermore, the integration of PRMs with tabular RL enhances\nrobustness to off-policy data while avoiding the inefficiencies of deep RL. We\nempirically find that our method delivers optimal helper behavior.", "published": "2025-02-07 00:06:17", "link": "http://arxiv.org/abs/2502.04576v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Position-aware Automatic Circuit Discovery", "abstract": "A widely used strategy to discover and understand language model mechanisms\nis circuit analysis. A circuit is a minimal subgraph of a model's computation\ngraph that executes a specific task. We identify a gap in existing circuit\ndiscovery methods: they assume circuits are position-invariant, treating model\ncomponents as equally relevant across input positions. This limits their\nability to capture cross-positional interactions or mechanisms that vary across\npositions. To address this gap, we propose two improvements to incorporate\npositionality into circuits, even on tasks containing variable-length examples.\nFirst, we extend edge attribution patching, a gradient-based method for circuit\ndiscovery, to differentiate between token positions. Second, we introduce the\nconcept of a dataset schema, which defines token spans with similar semantics\nacross examples, enabling position-aware circuit discovery in datasets with\nvariable length examples. We additionally develop an automated pipeline for\nschema generation and application using large language models. Our approach\nenables fully automated discovery of position-sensitive circuits, yielding\nbetter trade-offs between circuit size and faithfulness compared to prior work.", "published": "2025-02-07 00:18:20", "link": "http://arxiv.org/abs/2502.04577v1", "categories": ["cs.LG", "cs.CL", "68T50", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Extracting and Understanding the Superficial Knowledge in Alignment", "abstract": "Alignment of large language models (LLMs) with human values and preferences,\noften achieved through fine-tuning based on human feedback, is essential for\nensuring safe and responsible AI behaviors. However, the process typically\nrequires substantial data and computation resources. Recent studies have\nrevealed that alignment might be attainable at lower costs through simpler\nmethods, such as in-context learning. This leads to the question: Is alignment\npredominantly superficial? In this paper, we delve into this question and\nprovide a quantitative analysis. We formalize the concept of superficial\nknowledge, defining it as knowledge that can be acquired through easily token\nrestyling, without affecting the model's ability to capture underlying causal\nrelationships between tokens. We propose a method to extract and isolate\nsuperficial knowledge from aligned models, focusing on the shallow\nmodifications to the final token selection process. By comparing models\naugmented only with superficial knowledge to fully aligned models, we quantify\nthe superficial portion of alignment. Our findings reveal that while\nsuperficial knowledge constitutes a significant portion of alignment,\nparticularly in safety and detoxification tasks, it is not the whole story.\nTasks requiring reasoning and contextual understanding still rely on deeper\nknowledge. Additionally, we demonstrate two practical advantages of isolated\nsuperficial knowledge: (1) it can be transferred between models, enabling\nefficient offsite alignment of larger models using extracted superficial\nknowledge from smaller models, and (2) it is recoverable, allowing for the\nrestoration of alignment in compromised models without sacrificing performance.", "published": "2025-02-07 01:32:19", "link": "http://arxiv.org/abs/2502.04602v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Agentic Reasoning: Reasoning LLMs with Tools for the Deep Research", "abstract": "We introduce Agentic Reasoning, a framework that enhances large language\nmodel (LLM) reasoning by integrating external tool-using agents. Unlike\nconventional LLM-based reasoning approaches, which rely solely on internal\ninference, Agentic Reasoning dynamically engages web search, code execution,\nand structured reasoning-context memory to solve complex problems requiring\ndeep research and multi-step logical deduction. Our framework introduces the\nMind Map agent, which constructs a structured knowledge graph to track logical\nrelationships, improving deductive reasoning. Additionally, the integration of\nweb-search and coding agents enables real-time retrieval and computational\nanalysis, enhancing reasoning accuracy and decision-making. Evaluations on\nPhD-level scientific reasoning (GPQA) and domain-specific deep research tasks\ndemonstrate that our approach significantly outperforms existing models,\nincluding leading retrieval-augmented generation (RAG) systems and\nclosed-source LLMs. Moreover, our results indicate that agentic reasoning\nimproves expert-level knowledge synthesis, test-time scalability, and\nstructured problem-solving. The code is at:\nhttps://github.com/theworldofagents/Agentic-Reasoning.", "published": "2025-02-07 04:08:46", "link": "http://arxiv.org/abs/2502.04644v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "AdParaphrase: Paraphrase Dataset for Analyzing Linguistic Features\n  toward Generating Attractive Ad Texts", "abstract": "Effective linguistic choices that attract potential customers play crucial\nroles in advertising success. This study aims to explore the linguistic\nfeatures of ad texts that influence human preferences. Although the creation of\nattractive ad texts is an active area of research, progress in understanding\nthe specific linguistic features that affect attractiveness is hindered by\nseveral obstacles. First, human preferences are complex and influenced by\nmultiple factors, including their content, such as brand names, and their\nlinguistic styles, making analysis challenging. Second, publicly available ad\ntext datasets that include human preferences are lacking, such as ad\nperformance metrics and human feedback, which reflect people's interests. To\naddress these problems, we present AdParaphrase, a paraphrase dataset that\ncontains human preferences for pairs of ad texts that are semantically\nequivalent but differ in terms of wording and style. This dataset allows for\npreference analysis that focuses on the differences in linguistic features. Our\nanalysis revealed that ad texts preferred by human judges have higher fluency,\nlonger length, more nouns, and use of bracket symbols. Furthermore, we\ndemonstrate that an ad text-generation model that considers these findings\nsignificantly improves the attractiveness of a given text. The dataset is\npublicly available at: https://github.com/CyberAgentAILab/AdParaphrase.", "published": "2025-02-07 05:39:55", "link": "http://arxiv.org/abs/2502.04674v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "M-IFEval: Multilingual Instruction-Following Evaluation", "abstract": "Instruction following is a core capability of modern Large language models\n(LLMs), making evaluating this capability essential to understanding these\nmodels. The Instruction Following Evaluation (IFEval) benchmark from the\nliterature does this using objective criteria, offering a measure of LLM\nperformance without subjective AI or human judgement. However, it only includes\nEnglish instructions, limiting its ability to assess LLMs in other languages.\n  We propose the Multilingual Instruction Following Evaluation (M-IFEval)\nbenchmark, expanding the evaluation to French, Japanese, and Spanish, with both\ngeneral and language-specific instructions. Applying this benchmark to 8\nstate-of-the-art LLMs, we find that benchmark performance across languages and\ninstruction types can vary widely, underscoring the importance of a\nmultilingual benchmark for evaluating LLMs in a diverse cultural context.", "published": "2025-02-07 06:27:04", "link": "http://arxiv.org/abs/2502.04688v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Impression Change Prediction in Speed Dating Simulations Based\n  on Speakers' Personalities", "abstract": "This paper focuses on simulating text dialogues in which impressions between\nspeakers improve during speed dating. This simulation involves selecting an\nutterance from multiple candidates generated by a text generation model that\nreplicates a specific speaker's utterances, aiming to improve the impression of\nthe speaker. Accurately selecting an utterance that improves the impression is\ncrucial for the simulation. We believe that whether an utterance improves a\ndialogue partner's impression of the speaker may depend on the personalities of\nboth parties. However, recent methods for utterance selection do not consider\nthe impression per utterance or the personalities. To address this, we propose\na method that predicts whether an utterance improves a partner's impression of\nthe speaker, considering the personalities. The evaluation results showed that\npersonalities are useful in predicting impression changes per utterance.\nFurthermore, we conducted a human evaluation of simulated dialogues using our\nmethod. The results showed that it could simulate dialogues more favorably\nreceived than those selected without considering personalities.", "published": "2025-02-07 07:18:32", "link": "http://arxiv.org/abs/2502.04706v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "The \"negative end\" of change in grammar: terminology, concepts and\n  causes", "abstract": "The topic of \"negative end\" of change is, contrary to the fields of\ninnovation and emergence, largely under-researched. Yet, it has lately started\nto gain an increasing attention from language scholars worldwide. The main\nfocus of this article is threefold, namely to discuss the i) terminology; ii)\nconcepts and iii) causes associated with the \"negative end\" of change in\ngrammar. The article starts with an overview of research conducted on the\ntopic. It then moves to situating phenomena referred to as loss, decline or\nobsolescence among processes of language change, before elaborating on the\nterminology and concepts behind it. The last part looks at possible causes for\nconstructions to display a (gradual or rapid, but very consistent) decrease in\nthe frequency of use over time, which continues until the construction\ndisappears or there are only residual or fossilised forms left. Keywords: loss,\nobsolescence, decline, competition, higher", "published": "2025-02-07 07:54:08", "link": "http://arxiv.org/abs/2502.04729v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Holistically Guided Monte Carlo Tree Search for Intricate Information\n  Seeking", "abstract": "In the era of vast digital information, the sheer volume and heterogeneity of\navailable information present significant challenges for intricate information\nseeking. Users frequently face multistep web search tasks that involve\nnavigating vast and varied data sources. This complexity demands every step\nremains comprehensive, accurate, and relevant. However, traditional search\nmethods often struggle to balance the need for localized precision with the\nbroader context required for holistic understanding, leaving critical facets of\nintricate queries underexplored. In this paper, we introduce an LLM-based\nsearch assistant that adopts a new information seeking paradigm with\nholistically guided Monte Carlo tree search (HG-MCTS). We reformulate the task\nas a progressive information collection process with a knowledge memory and\nunite an adaptive checklist with multi-perspective reward modeling in MCTS. The\nadaptive checklist provides explicit sub-goals to guide the MCTS process toward\ncomprehensive coverage of complex user queries. Simultaneously, our\nmulti-perspective reward modeling offers both exploration and retrieval\nrewards, along with progress feedback that tracks completed and remaining\nsub-goals, refining the checklist as the tree search progresses. By striking a\nbalance between localized tree expansion and global guidance, HG-MCTS reduces\nredundancy in search paths and ensures that all crucial aspects of an intricate\nquery are properly addressed. Extensive experiments on real-world intricate\ninformation seeking tasks demonstrate that HG-MCTS acquires thorough knowledge\ncollections and delivers more accurate final responses compared with existing\nbaselines.", "published": "2025-02-07 08:36:39", "link": "http://arxiv.org/abs/2502.04751v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "ELITE: Enhanced Language-Image Toxicity Evaluation for Safety", "abstract": "Current Vision Language Models (VLMs) remain vulnerable to malicious prompts\nthat induce harmful outputs. Existing safety benchmarks for VLMs primarily rely\non automated evaluation methods, but these methods struggle to detect implicit\nharmful content or produce inaccurate evaluations. Therefore, we found that\nexisting benchmarks have low levels of harmfulness, ambiguous data, and limited\ndiversity in image-text pair combinations. To address these issues, we propose\nthe ELITE benchmark, a high-quality safety evaluation benchmark for VLMs,\nunderpinned by our enhanced evaluation method, the ELITE evaluator. The ELITE\nevaluator explicitly incorporates a toxicity score to accurately assess\nharmfulness in multimodal contexts, where VLMs often provide specific,\nconvincing, but unharmful descriptions of images. We filter out ambiguous and\nlow-quality image-text pairs from existing benchmarks using the ELITE evaluator\nand generate diverse combinations of safe and unsafe image-text pairs. Our\nexperiments demonstrate that the ELITE evaluator achieves superior alignment\nwith human evaluations compared to prior automated methods, and the ELITE\nbenchmark offers enhanced benchmark quality and diversity. By introducing\nELITE, we pave the way for safer, more robust VLMs, contributing essential\ntools for evaluating and mitigating safety risks in real-world applications.", "published": "2025-02-07 08:43:15", "link": "http://arxiv.org/abs/2502.04757v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "S$^2$-MAD: Breaking the Token Barrier to Enhance Multi-Agent Debate\n  Efficiency", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious natural language processing (NLP) scenarios, but they still face\nchallenges when handling complex arithmetic and logical reasoning tasks. While\nChain-Of-Thought (CoT) reasoning, self-consistency (SC) and self-correction\nstrategies have attempted to guide models in sequential, multi-step reasoning,\nMulti-agent Debate (MAD) has emerged as a viable approach for enhancing the\nreasoning capabilities of LLMs. By increasing both the number of agents and the\nfrequency of debates, the performance of LLMs improves significantly. However,\nthis strategy results in a significant increase in token costs, presenting a\nbarrier to scalability. To address this challenge, we introduce a novel\nsparsification strategy designed to reduce token costs within MAD. This\napproach minimizes ineffective exchanges of information and unproductive\ndiscussions among agents, thereby enhancing the overall efficiency of the\ndebate process. We conduct comparative experiments on multiple datasets across\nvarious models, demonstrating that our approach significantly reduces the token\ncosts in MAD to a considerable extent. Specifically, compared to MAD, our\napproach achieves an impressive reduction of up to 94.5\\% in token costs while\nmaintaining performance degradation below 2.0\\%.", "published": "2025-02-07 09:49:56", "link": "http://arxiv.org/abs/2502.04790v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Paying Attention to Facts: Quantifying the Knowledge Capacity of\n  Attention Layers", "abstract": "In this paper, we investigate the ability of single-layer attention-only\ntransformers (i.e. attention layers) to memorize facts contained in databases\nfrom a linear-algebraic perspective. We associate with each database a\n3-tensor, propose the rank of this tensor as a measure of the size of the\ndatabase, and provide bounds on the rank in terms of properties of the\ndatabase. We also define a 3-tensor corresponding to an attention layer, and\nempirically demonstrate the relationship between its rank and database rank on\na dataset of toy models and random databases. By highlighting the roles played\nby the value-output and query-key weights, and the effects of argmax and\nsoftmax on rank, our results shed light on the `additive motif' of factual\nrecall in transformers, while also suggesting a way of increasing layer\ncapacity without increasing the number of parameters.", "published": "2025-02-07 16:50:27", "link": "http://arxiv.org/abs/2502.05076v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Adaptive Graph of Thoughts: Test-Time Adaptive Reasoning Unifying Chain,\n  Tree, and Graph Structures", "abstract": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities, yet their performance is highly dependent on the prompting\nstrategy and model scale. While reinforcement learning and fine-tuning have\nbeen deployed to boost reasoning, these approaches incur substantial\ncomputational and data overhead. In this work, we introduce Adaptive Graph of\nThoughts (AGoT), a dynamic, graph-based inference framework that enhances LLM\nreasoning solely at test time. Rather than relying on fixed-step methods like\nChain of Thought (CoT) or Tree of Thoughts (ToT), AGoT recursively decomposes\ncomplex queries into structured subproblems, forming an dynamic directed\nacyclic graph (DAG) of interdependent reasoning steps. By selectively expanding\nonly those subproblems that require further analysis, AGoT unifies the\nstrengths of chain, tree, and graph paradigms into a cohesive framework that\nallocates computation where it is most needed. We validate our approach on\ndiverse benchmarks spanning multi-hop retrieval, scientific reasoning, and\nmathematical problem-solving, achieving up to 46.2% improvement on scientific\nreasoning tasks (GPQA) - comparable to gains achieved through computationally\nintensive reinforcement learning approaches and outperforming state-of-the-art\niterative approaches. These results suggest that dynamic decomposition and\nstructured recursion offer a scalable, cost-effective alternative to\npost-training modifications, paving the way for more robust, general-purpose\nreasoning in LLMs.", "published": "2025-02-07 16:54:19", "link": "http://arxiv.org/abs/2502.05078v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "ChallengeMe: An Adversarial Learning-enabled Text Summarization\n  Framework", "abstract": "The astonishing performance of large language models (LLMs) and their\nremarkable achievements in production and daily life have led to their\nwidespread application in collaborative tasks. However, current large models\nface challenges such as hallucination and lack of specificity in content\ngeneration in vertical domain tasks. Inspired by the contrast and\nclassification mechanisms in human cognitive processes, this paper constructs\nan adversarial learning-based prompt framework named ChallengeMe, which\nincludes three cascaded solutions: generation prompts, evaluation prompts, and\nfeedback optimization. In this process, we designed seven core optimization\ndimensions and set the threshold for adversarial learning. The results of mixed\ncase studies on the text summarization task show that the proposed framework\ncan generate more accurate and fluent text summaries compared to the current\nadvanced mainstream LLMs.", "published": "2025-02-07 16:59:34", "link": "http://arxiv.org/abs/2502.05084v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Flexible and Efficient Grammar-Constrained Decoding", "abstract": "Large Language Models (LLMs) are often asked to generate structured outputs\nthat obey precise syntactic rules, such as code snippets or formatted data.\nGrammar-constrained decoding (GCD) can guarantee that LLM outputs matches such\nrules by masking out tokens that will provably lead to outputs that do not\nbelong to a specified context-free grammar (CFG). To guarantee soundness, GCD\nalgorithms have to compute how a given LLM subword tokenizer can align with the\ntokens used\n  by a given context-free grammar and compute token masks based on this\ninformation. Doing so efficiently is challenging and existing GCD algorithms\nrequire tens of minutes to preprocess common grammars. We present a new GCD\nalgorithm together with an implementation that offers 17.71x faster offline\npreprocessing than existing approaches while preserving state-of-the-art\nefficiency in online mask computation.", "published": "2025-02-07 17:35:17", "link": "http://arxiv.org/abs/2502.05111v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Annotated Reading of 'The Singer of Tales' in the LLM Era", "abstract": "The Parry-Lord oral-formulaic theory was a breakthrough in understanding how\noral narrative poetry is learned, composed, and transmitted by illiterate\nbards. In this paper, we provide an annotated reading of the mechanism\nunderlying this theory from the lens of large language models (LLMs) and\ngenerative artificial intelligence (AI). We point out the the similarities and\ndifferences between oral composition and LLM generation, and comment on the\nimplications to society and AI policy.", "published": "2025-02-07 18:26:01", "link": "http://arxiv.org/abs/2502.05148v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "A Lightweight Method to Disrupt Memorized Sequences in LLM", "abstract": "Large language models (LLMs) demonstrate impressive capabilities across many\ntasks yet risk reproducing copyrighted content verbatim, raising legal and\nethical concerns. Although methods like differential privacy or neuron editing\ncan reduce memorization, they typically require costly retraining or direct\naccess to model weights and may degrade performance. To address these\nchallenges, we propose TokenSwap, a lightweight, post-hoc approach that\nreplaces the probabilities of grammar-related tokens with those from a small\nauxiliary model (e.g., DistilGPT-2). We run extensive experiments on commercial\ngrade models such as Pythia-6.9b and LLaMA-3-8b and demonstrate that our method\neffectively reduces well-known cases of memorized generation by upto 10x with\nlittle to no impact on downstream tasks. Our approach offers a uniquely\naccessible and effective solution to users of real-world systems.", "published": "2025-02-07 18:41:21", "link": "http://arxiv.org/abs/2502.05159v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM\n  Guardrails", "abstract": "The rapid advancement of large language models (LLMs) has increased the need\nfor guardrail models to ensure responsible use, particularly in detecting\nunsafe and illegal content. While substantial safety data exist in English,\nmultilingual guardrail modeling remains underexplored due to the scarcity of\nopen-source safety data in other languages. To address this gap, we propose a\nnovel two-player Reinforcement Learning (RL) framework, where a generator and a\nguardrail model co-evolve adversarially to produce high-quality synthetic data\nfor multilingual guardrail training. We theoretically formalize this\ninteraction as a two-player game, proving convergence to a Nash equilibrium.\nEmpirical evaluations show that our model \\ours outperforms state-of-the-art\nmodels, achieving nearly 10% improvement over LlamaGuard3 (8B) on English\nbenchmarks while being 4.5x faster at inference with a significantly smaller\nmodel (0.5B). We achieve substantial advancements in multilingual safety tasks,\nparticularly in addressing the imbalance for lower-resource languages in a\ncollected real dataset. Ablation studies emphasize the critical role of\nsynthetic data generation in bridging the imbalance in open-source data between\nEnglish and other languages. These findings establish a scalable and efficient\napproach to synthetic data generation, paving the way for improved multilingual\nguardrail models to enhance LLM safety. Code, model, and data will be\nopen-sourced at https://github.com/yihedeng9/DuoGuard.", "published": "2025-02-07 18:45:03", "link": "http://arxiv.org/abs/2502.05163v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth\n  Approach", "abstract": "We study a novel language model architecture that is capable of scaling\ntest-time computation by implicitly reasoning in latent space. Our model works\nby iterating a recurrent block, thereby unrolling to arbitrary depth at\ntest-time. This stands in contrast to mainstream reasoning models that scale up\ncompute by producing more tokens. Unlike approaches based on chain-of-thought,\nour approach does not require any specialized training data, can work with\nsmall context windows, and can capture types of reasoning that are not easily\nrepresented in words. We scale a proof-of-concept model to 3.5 billion\nparameters and 800 billion tokens. We show that the resulting model can improve\nits performance on reasoning benchmarks, sometimes dramatically, up to a\ncomputation load equivalent to 50 billion parameters.", "published": "2025-02-07 18:55:02", "link": "http://arxiv.org/abs/2502.05171v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Efficient Knowledge Feeding to Language Models: A Novel Integrated\n  Encoder-Decoder Architecture", "abstract": "This paper introduces a novel approach to efficiently feeding knowledge to\nlanguage models (LLMs) during prediction by integrating retrieval and\ngeneration processes within a unified framework. While the Retrieval-Augmented\nGeneration (RAG) model addresses gaps in LLMs' training data and knowledge\nlimits, it is hindered by token limit restrictions and dependency on the\nretrieval system's accuracy. Our proposed architecture incorporates in-context\nvectors (ICV) to overcome these challenges. ICV recasts in-context learning by\nusing latent embeddings of LLMs to create a vector that captures essential task\ninformation. This vector is then used to shift the latent states of the LLM,\nenhancing the generation process without adding demonstration examples to the\nprompt. ICV directly integrates information into the model, enabling it to\nprocess this information more effectively. Our extensive experimental\nevaluation demonstrates that ICV outperforms standard in-context learning and\nfine-tuning across question-answering, information retrieval, and other tasks.\nThis approach mitigates the limitations of current RAG models and offers a more\nrobust solution for handling extensive and diverse datasets. Despite leveraging\na fraction of the parameters, our ICV-enhanced model achieves competitive\nperformance against models like LLaMA-3, Gemma, and Phi-3, significantly\nreducing computational costs and memory requirements. ICV reduces prompt\nlength, is easy to control, surpasses token limitations, and is computationally\nefficient compared to fine-tuning.", "published": "2025-02-07 04:24:07", "link": "http://arxiv.org/abs/2502.05233v1", "categories": ["cs.CL", "cs.IR", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Enhancing Knowledge Graph Construction: Evaluating with Emphasis on\n  Hallucination, Omission, and Graph Similarity Metrics", "abstract": "Recent advancements in large language models have demonstrated significant\npotential in the automated construction of knowledge graphs from unstructured\ntext. This paper builds upon our previous work [16], which evaluated various\nmodels using metrics like precision, recall, F1 score, triple matching, and\ngraph matching, and introduces a refined approach to address the critical\nissues of hallucination and omission. We propose an enhanced evaluation\nframework incorporating BERTScore for graph similarity, setting a practical\nthreshold of 95% for graph matching. Our experiments focus on the Mistral\nmodel, comparing its original and fine-tuned versions in zero-shot and few-shot\nsettings. We further extend our experiments using examples from the KELM-sub\ntraining dataset, illustrating that the fine-tuned model significantly improves\nknowledge graph construction accuracy while reducing the exact hallucination\nand omission. However, our findings also reveal that the fine-tuned models\nperform worse in generalization tasks on the KELM-sub dataset. This study\nunderscores the importance of comprehensive evaluation metrics in advancing the\nstate-of-the-art in knowledge graph construction from textual data.", "published": "2025-02-07 11:19:01", "link": "http://arxiv.org/abs/2502.05239v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GSM-Infinite: How Do Your LLMs Behave over Infinitely Increasing Context\n  Length and Reasoning Complexity?", "abstract": "Long-context large language models (LLMs) have recently shown strong\nperformance in information retrieval and long-document QA. However, to tackle\nthe most challenging intellectual problems, LLMs must reason effectively in\nlong and complex contexts (e.g., frontier mathematical research). Studying how\nLLMs handle increasing reasoning complexity and context length is essential,\nyet existing benchmarks lack a solid basis for quantitative evaluation.\nInspired by the abstraction of GSM-8K problems as computational graphs, and the\nability to introduce noise by adding unnecessary nodes and edges, we develop a\ngrade school math problem generator capable of producing arithmetic problems\nwith infinite difficulty and context length under fine-grained control. Using\nour newly synthesized GSM-Infinite benchmark, we comprehensively evaluate\nexisting LLMs. We find a consistent sigmoid decline in reasoning performance as\ncomplexity increases, along with a systematic inference scaling trend:\nexponentially increasing inference computation yields only linear performance\ngains. These findings underscore the fundamental limitations of current\nlong-context LLMs and the key challenges in scaling reasoning capabilities. Our\nGSM-Infinite benchmark provides a scalable and controllable testbed for\nsystematically studying and advancing LLM reasoning in long and complex\ncontexts.", "published": "2025-02-07 17:05:25", "link": "http://arxiv.org/abs/2502.05252v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMs Can Teach Themselves to Better Predict the Future", "abstract": "We present an outcome-driven fine-tuning framework that enhances the\nforecasting capabilities of large language models (LLMs) without relying on\nhuman-curated reasoning samples. Our method leverages model self-play to\ngenerate pairs of diverse reasoning trajectories and probabilistic forecasts\nfor a set of diverse questions that resolve after the models' knowledge cutoff\ndate. We then rank pairs of these reasoning traces by their distance to the\nactual outcomes before fine-tuning the model via Direct Preference Optimization\n(DPO). On a separate test set, our approach increases prediction accuracy of\nPhi-4 14B and DeepSeek-R1 14B by between 7--10\\% over a base model and a DPO\nfine-tuned control model with randomized labels, bringing them on par with\nforecasting capabilities of much larger frontier models like GPT-4o.", "published": "2025-02-07 17:21:16", "link": "http://arxiv.org/abs/2502.05253v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards the Development of Balanced Synthetic Data for Correcting\n  Grammatical Errors in Arabic: An Approach Based on Error Tagging Model and\n  Synthetic Data Generating Model", "abstract": "Synthetic data generation is widely recognized as a way to enhance the\nquality of neural grammatical error correction (GEC) systems. However, current\napproaches often lack diversity or are too simplistic to generate the wide\nrange of grammatical errors made by humans, especially for low-resource\nlanguages such as Arabic. In this paper, we will develop the error tagging\nmodel and the synthetic data generation model to create a large synthetic\ndataset in Arabic for grammatical error correction. In the error tagging model,\nthe correct sentence is categorized into multiple error types by using the\nDeBERTav3 model. Arabic Error Type Annotation tool (ARETA) is used to guide\nmulti-label classification tasks in an error tagging model in which each\nsentence is classified into 26 error tags. The synthetic data generation model\nis a back-translation-based model that generates incorrect sentences by\nappending error tags before the correct sentence that was generated from the\nerror tagging model using the ARAT5 model. In the QALB-14 and QALB-15 Test\nsets, the error tagging model achieved 94.42% F1, which is state-of-the-art in\nidentifying error tags in clean sentences. As a result of our syntactic data\ntraining in grammatical error correction, we achieved a new state-of-the-art\nresult of F1-Score: 79.36% in the QALB-14 Test set. We generate 30,219,310\nsynthetic sentence pairs by using a synthetic data generation model.", "published": "2025-02-07 20:28:37", "link": "http://arxiv.org/abs/2502.05312v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards LLM Unlearning Resilient to Relearning Attacks: A\n  Sharpness-Aware Minimization Perspective and Beyond", "abstract": "The LLM unlearning technique has recently been introduced to comply with data\nregulations and address the safety and ethical concerns of LLMs by removing the\nundesired data-model influence. However, state-of-the-art unlearning methods\nface a critical vulnerability: they are susceptible to ``relearning'' the\nremoved information from a small number of forget data points, known as\nrelearning attacks. In this paper, we systematically investigate how to make\nunlearned models robust against such attacks. For the first time, we establish\na connection between robust unlearning and sharpness-aware minimization (SAM)\nthrough a unified robust optimization framework, in an analogy to adversarial\ntraining designed to defend against adversarial attacks. Our analysis for SAM\nreveals that smoothness optimization plays a pivotal role in mitigating\nrelearning attacks. Thus, we further explore diverse smoothing strategies to\nenhance unlearning robustness. Extensive experiments on benchmark datasets,\nincluding WMDP and MUSE, demonstrate that SAM and other smoothness optimization\napproaches consistently improve the resistance of LLM unlearning to relearning\nattacks. Notably, smoothness-enhanced unlearning also helps defend against\n(input-level) jailbreaking attacks, broadening our proposal's impact in\nrobustifying LLM unlearning. Codes are available at\nhttps://github.com/OPTML-Group/Unlearn-Smooth.", "published": "2025-02-07 23:03:55", "link": "http://arxiv.org/abs/2502.05374v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LLM-Supported Natural Language to Bash Translation", "abstract": "The Bourne-Again Shell (Bash) command-line interface for Linux systems has\ncomplex syntax and requires extensive specialized knowledge. Using the natural\nlanguage to Bash command (NL2SH) translation capabilities of large language\nmodels (LLMs) for command composition circumvents these issues. However, the\nNL2SH performance of LLMs is difficult to assess due to inaccurate test data\nand unreliable heuristics for determining the functional equivalence of Bash\ncommands. We present a manually verified test dataset of 600\ninstruction-command pairs and a training dataset of 40,939 pairs, increasing\nthe size of previous datasets by 441% and 135%, respectively. Further, we\npresent a novel functional equivalence heuristic that combines command\nexecution with LLM evaluation of command outputs. Our heuristic can determine\nthe functional equivalence of two Bash commands with 95% confidence, a 16%\nincrease over previous heuristics. Evaluation of popular LLMs using our test\ndataset and heuristic demonstrates that parsing, in-context learning, in-weight\nlearning, and constrained decoding can improve NL2SH accuracy by up to 32%. Our\nfindings emphasize the importance of dataset quality, execution-based\nevaluation and translation method for advancing NL2SH translation. Our code is\navailable at https://github.com/westenfelder/NL2SH", "published": "2025-02-07 19:35:55", "link": "http://arxiv.org/abs/2502.06858v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AceWGS: An LLM-Aided Framework to Accelerate Catalyst Design for\n  Water-Gas Shift Reactions", "abstract": "While the Water-Gas Shift (WGS) reaction plays a crucial role in hydrogen\nproduction for fuel cells, finding suitable catalysts to achieve high yields\nfor low-temperature WGS reactions remains a persistent challenge. Artificial\nIntelligence (AI) has shown promise in accelerating catalyst design by\nexploring vast candidate spaces, however, two key gaps limit its effectiveness.\nFirst, AI models primarily train on numerical data, which fail to capture\nessential text-based information, such as catalyst synthesis methods. Second,\nthe cross-disciplinary nature of catalyst design requires seamless\ncollaboration between AI, theory, experiments, and numerical simulations, often\nleading to communication barriers. To address these gaps, we present AceWGS, a\nLarge Language Models (LLMs)-aided framework to streamline WGS catalyst design.\nAceWGS interacts with researchers through natural language, answering queries\nbased on four features: (i) answering general queries, (ii) extracting\ninformation about the database comprising WGS-related journal articles, (iii)\ncomprehending the context described in these articles, and (iv) identifying\ncatalyst candidates using our proposed AI inverse model. We presented a\npractical case study demonstrating how AceWGS can accelerate the catalyst\ndesign process. AceWGS, built with open-source tools, offers an adjustable\nframework that researchers can readily adapt for a range of AI-accelerated\ncatalyst design applications, supporting seamless integration across\ncross-disciplinary studies.", "published": "2025-02-07 02:36:47", "link": "http://arxiv.org/abs/2503.05607v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Confidence Elicitation: A New Attack Vector for Large Language Models", "abstract": "A fundamental issue in deep learning has been adversarial robustness. As\nthese systems have scaled, such issues have persisted. Currently, large\nlanguage models (LLMs) with billions of parameters suffer from adversarial\nattacks just like their earlier, smaller counterparts. However, the threat\nmodels have changed. Previously, having gray-box access, where input embeddings\nor output logits/probabilities were visible to the user, might have been\nreasonable. However, with the introduction of closed-source models, no\ninformation about the model is available apart from the generated output. This\nmeans that current black-box attacks can only utilize the final prediction to\ndetect if an attack is successful. In this work, we investigate and demonstrate\nthe potential of attack guidance, akin to using output probabilities, while\nhaving only black-box access in a classification setting. This is achieved\nthrough the ability to elicit confidence from the model. We empirically show\nthat the elicited confidence is calibrated and not hallucinated for current\nLLMs. By minimizing the elicited confidence, we can therefore increase the\nlikelihood of misclassification. Our new proposed paradigm demonstrates\npromising state-of-the-art results on three datasets across two models\n(LLaMA-3-8B-Instruct and Mistral-7B-Instruct-V0.3) when comparing our technique\nto existing hard-label black-box attack methods that introduce word-level\nsubstitutions.", "published": "2025-02-07 04:07:36", "link": "http://arxiv.org/abs/2502.04643v2", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Unveiling the Mechanisms of Explicit CoT Training: How Chain-of-Thought\n  Enhances Reasoning Generalization", "abstract": "Training large language models (LLMs) with high-quality Chain-of-Thought\n(CoT) annotations has become a widely adopted strategy due to its significant\nenhancement of reasoning capabilities. To fully comprehend this approach, two\nquestions naturally arise: (Q1) What advantages does training with CoT offer\ncompared to training without CoT? (Q2) If there are advantages, what are the\nunderlying mechanisms of explicit CoT training? Analyzing the advantages and\nmechanisms of CoT training is challenging due to the many factors involved. To\naddress this, we conduct a detailed analysis using clear and controllable data\ndistributions and, for the first time, reveal that CoT training offers the\nfollowing advantages: (1) Training with CoT markedly improves reasoning\ngeneralization, extending it from in-distribution (ID) to both ID and\nout-of-distribution (OOD) scenarios, while also speeding up convergence; (2)\nEven when training with CoT includes a certain range of erroneous reasoning\nsteps, it still enables the model to learn reasoning patterns, leading to\nsystematic generalization. We further explore the underlying mechanisms from a\ncircuit perspective: (1) The data distribution (e.g., ratio $\\lambda$ and\npattern) plays a crucial role in influencing the model's systematic\ngeneralization; (2) CoT training (with two-hop facts) internalizes reasoning\ninto a two-stage generalizing circuit, where the number of stages corresponds\nto the explicit reasoning steps during training. Our findings elucidate the\nmechanisms underlying explicit CoT training and offer critical insights into\ntuning strategies for LLMs to achieve robust generalization.", "published": "2025-02-07 05:21:13", "link": "http://arxiv.org/abs/2502.04667v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Scalable Oversight for Superhuman AI via Recursive Self-Critiquing", "abstract": "As AI capabilities increasingly surpass human proficiency in complex tasks,\ncurrent alignment techniques including SFT and RLHF face fundamental challenges\nin ensuring reliable oversight. These methods rely on direct human assessment\nand become untenable when AI outputs exceed human cognitive thresholds. In\nresponse to this challenge, we explore two hypotheses: (1) critique of critique\ncan be easier than critique itself, extending the widely-accepted observation\nthat verification is easier than generation to the critique domain, as critique\nitself is a specialized form of generation; (2) this difficulty relationship is\nrecursively held, suggesting that when direct evaluation is infeasible,\nperforming high-order critiques (e.g., critique of critique of critique) offers\na more tractable supervision pathway. To examine these hypotheses, we perform\nHuman-Human, Human-AI, and AI-AI experiments across multiple tasks. Our results\ndemonstrate encouraging evidence supporting these hypotheses and suggest that\nrecursive self-critiquing is a promising direction for scalable oversight.", "published": "2025-02-07 05:41:23", "link": "http://arxiv.org/abs/2502.04675v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "ARR: Question Answering with Large Language Models via Analyzing,\n  Retrieving, and Reasoning", "abstract": "Large language models (LLMs) achieve remarkable performance on challenging\nbenchmarks that are often structured as multiple-choice question-answering (QA)\ntasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs\nbut provides only vague and generic guidance (\"think step by step\"). This paper\nintroduces ARR, an intuitive and effective zero-shot prompting method that\nexplicitly incorporates three key steps in QA solving: analyzing the intent of\nthe question, retrieving relevant information, and reasoning step by step.\nComprehensive experiments across diverse and challenging QA tasks demonstrate\nthat ARR consistently improves the Baseline (without ARR prompting) and\noutperforms CoT. Ablation and case studies further validate the positive\ncontributions of each component: analyzing, retrieving, and reasoning. Notably,\nintent analysis plays a vital role in ARR. Additionally, extensive evaluations\nacross various model sizes, LLM series, and generation settings solidify the\neffectiveness, robustness, and generalizability of ARR.", "published": "2025-02-07 06:30:33", "link": "http://arxiv.org/abs/2502.04689v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Concept Navigation and Classification via Open-Source Large Language\n  Model Processing", "abstract": "This paper presents a novel methodological framework for detecting and\nclassifying latent constructs, including frames, narratives, and topics, from\ntextual data using Open-Source Large Language Models (LLMs). The proposed\nhybrid approach combines automated summarization with human-in-the-loop\nvalidation to enhance the accuracy and interpretability of construct\nidentification. By employing iterative sampling coupled with expert refinement,\nthe framework guarantees methodological robustness and ensures conceptual\nprecision. Applied to diverse data sets, including AI policy debates, newspaper\narticles on encryption, and the 20 Newsgroups data set, this approach\ndemonstrates its versatility in systematically analyzing complex political\ndiscourses, media framing, and topic classification tasks.", "published": "2025-02-07 08:42:34", "link": "http://arxiv.org/abs/2502.04756v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Lightweight Operations for Visual Speech Recognition", "abstract": "Visual speech recognition (VSR), which decodes spoken words from video data,\noffers significant benefits, particularly when audio is unavailable. However,\nthe high dimensionality of video data leads to prohibitive computational costs\nthat demand powerful hardware, limiting VSR deployment on resource-constrained\ndevices. This work addresses this limitation by developing lightweight VSR\narchitectures. Leveraging efficient operation design paradigms, we create\ncompact yet powerful models with reduced resource requirements and minimal\naccuracy loss. We train and evaluate our models on a large-scale public dataset\nfor recognition of words from video sequences, demonstrating their\neffectiveness for practical applications. We also conduct an extensive array of\nablative experiments to thoroughly analyze the size and complexity of each\nmodel. Code and trained models will be made publicly available.", "published": "2025-02-07 11:08:32", "link": "http://arxiv.org/abs/2502.04834v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Evaluating Standard and Dialectal Frisian ASR: Multilingual Fine-tuning\n  and Language Identification for Improved Low-resource Performance", "abstract": "Automatic Speech Recognition (ASR) performance for low-resource languages is\nstill far behind that of higher-resource languages such as English, due to a\nlack of sufficient labeled data. State-of-the-art methods deploy\nself-supervised transfer learning where a model pre-trained on large amounts of\ndata is fine-tuned using little labeled data in a target low-resource language.\nIn this paper, we present and examine a method for fine-tuning an SSL-based\nmodel in order to improve the performance for Frisian and its regional dialects\n(Clay Frisian, Wood Frisian, and South Frisian). We show that Frisian ASR\nperformance can be improved by using multilingual (Frisian, Dutch, English and\nGerman) fine-tuning data and an auxiliary language identification task. In\naddition, our findings show that performance on dialectal speech suffers\nsubstantially, and, importantly, that this effect is moderated by the\nelicitation approach used to collect the dialectal data. Our findings also\nparticularly suggest that relying solely on standard language data for ASR\nevaluation may underestimate real-world performance, particularly in languages\nwith substantial dialectal variation.", "published": "2025-02-07 12:42:46", "link": "http://arxiv.org/abs/2502.04883v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Aligning Black-box Language Models with Human Judgments", "abstract": "Large language models (LLMs) are increasingly used as automated judges to\nevaluate recommendation systems, search engines, and other subjective tasks,\nwhere relying on human evaluators can be costly, time-consuming, and\nunscalable. LLMs offer an efficient solution for continuous, automated\nevaluation. However, since the systems that are built and improved with these\njudgments are ultimately designed for human use, it is crucial that LLM\njudgments align closely with human evaluators to ensure such systems remain\nhuman-centered. On the other hand, aligning LLM judgments with human evaluators\nis challenging due to individual variability and biases in human judgments. We\npropose a simple yet effective framework to align LLM judgments with individual\nhuman evaluators or their aggregated judgments, without retraining or\nfine-tuning the LLM. Our approach learns a linear mapping between the LLM's\noutputs and human judgments, achieving over 142% average improvement in\nagreement across 29 tasks with only a small number of calibration examples used\nfor training. Notably, our method works in zero-shot and few-shot settings,\nexceeds inter-human agreement on four out of six tasks, and enables smaller\nLLMs to achieve performance comparable to that of larger models.", "published": "2025-02-07 15:19:40", "link": "http://arxiv.org/abs/2502.04997v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Mitigating Unintended Memorization with LoRA in Federated Learning for\n  LLMs", "abstract": "Federated learning (FL) is a popular paradigm for collaborative training\nwhich avoids direct data exposure between clients. However, data privacy issues\nstill remain: FL-trained large language models are capable of memorizing and\ncompleting phrases and sentences contained in training data when given with\ntheir prefixes. Thus, it is possible for adversarial and honest-but-curious\nclients to recover training data of other participants simply through targeted\nprompting. In this work, we demonstrate that a popular and simple fine-tuning\nstrategy, low-rank adaptation (LoRA), reduces memorization during FL up to a\nfactor of 10. We study this effect by performing a medical question-answering\nfine-tuning task and injecting multiple replicas of out-of-distribution\nsensitive sequences drawn from an external clinical dataset. We observe a\nreduction in memorization for a wide variety of Llama 2 and 3 models, and find\nthat LoRA can reduce memorization in centralized learning as well. Furthermore,\nwe show that LoRA can be combined with other privacy-preserving techniques such\nas gradient clipping and Gaussian noising, secure aggregation, and Goldfish\nloss to further improve record-level privacy while maintaining performance.", "published": "2025-02-07 17:04:39", "link": "http://arxiv.org/abs/2502.05087v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Lost in Time: Clock and Calendar Understanding Challenges in Multimodal\n  LLMs", "abstract": "Understanding time from visual representations is a fundamental cognitive\nskill, yet it remains a challenge for multimodal large language models (MLLMs).\nIn this work, we investigate the capabilities of MLLMs in interpreting time and\ndate through analogue clocks and yearly calendars. To facilitate this, we\ncurated a structured dataset comprising two subsets: 1) $\\textit{ClockQA}$,\nwhich comprises various types of clock styles$-$standard, black-dial,\nno-second-hand, Roman numeral, and arrow-hand clocks$-$paired with time related\nquestions; and 2) $\\textit{CalendarQA}$, which consists of yearly calendar\nimages with questions ranging from commonly known dates (e.g., Christmas, New\nYear's Day) to computationally derived ones (e.g., the 100th or 153rd day of\nthe year). We aim to analyse how MLLMs can perform visual recognition,\nnumerical reasoning, and temporal inference when presented with time-related\nvisual data. Our evaluations show that despite recent advancements, reliably\nunderstanding time remains a significant challenge for MLLMs.", "published": "2025-02-07 17:11:23", "link": "http://arxiv.org/abs/2502.05092v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Transforming Science with Large Language Models: A Survey on AI-assisted\n  Scientific Discovery, Experimentation, Content Generation, and Evaluation", "abstract": "With the advent of large multimodal language models, science is now at a\nthreshold of an AI-based technological transformation. Recently, a plethora of\nnew AI models and tools has been proposed, promising to empower researchers and\nacademics worldwide to conduct their research more effectively and efficiently.\nThis includes all aspects of the research cycle, especially (1) searching for\nrelevant literature; (2) generating research ideas and conducting\nexperimentation; generating (3) text-based and (4) multimodal content (e.g.,\nscientific figures and diagrams); and (5) AI-based automatic peer review. In\nthis survey, we provide an in-depth overview over these exciting recent\ndevelopments, which promise to fundamentally alter the scientific research\nprocess for good. Our survey covers the five aspects outlined above, indicating\nrelevant datasets, methods and results (including evaluation) as well as\nlimitations and scope for future research. Ethical concerns regarding\nshortcomings of these tools and potential for misuse (fake science, plagiarism,\nharms to research integrity) take a particularly prominent place in our\ndiscussion. We hope that our survey will not only become a reference guide for\nnewcomers to the field but also a catalyst for new AI-based initiatives in the\narea of \"AI4Science\".", "published": "2025-02-07 18:26:45", "link": "http://arxiv.org/abs/2502.05151v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Joint MoE Scaling Laws: Mixture of Experts Can Be Memory Efficient", "abstract": "Mixture of Experts (MoE) architectures have significantly increased\ncomputational efficiency in both research and real-world applications of\nlarge-scale machine learning models. However, their scalability and efficiency\nunder memory constraints remain relatively underexplored. In this work, we\npresent joint scaling laws for dense and MoE models, incorporating key factors\nsuch as the number of active parameters, dataset size, and the number of\nexperts. Our findings provide a principled framework for selecting the optimal\nMoE configuration under fixed memory and compute budgets. Surprisingly, we show\nthat MoE models can be more memory-efficient than dense models, contradicting\nconventional wisdom. To derive and validate the theoretical predictions of our\nscaling laws, we conduct over 280 experiments with up to 2.7B active parameters\nand up to 5B total parameters. These results offer actionable insights for\ndesigning and deploying MoE models in practical large-scale training scenarios.", "published": "2025-02-07 18:55:38", "link": "http://arxiv.org/abs/2502.05172v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Optimizing Temperature for Language Models with Multi-Sample Inference", "abstract": "Multi-sample aggregation strategies, such as majority voting and best-of-N\nsampling, are widely used in contemporary large language models (LLMs) to\nenhance predictive accuracy across various tasks. A key challenge in this\nprocess is temperature selection, which significantly impacts model\nperformance. Existing approaches either rely on a fixed default temperature or\nrequire labeled validation data for tuning, which are often scarce and\ndifficult to obtain. This paper addresses the challenge of automatically\nidentifying the (near)-optimal temperature for different LLMs using\nmulti-sample aggregation strategies, without relying on task-specific\nvalidation data. We provide a comprehensive analysis of temperature's role in\nperformance optimization, considering variations in model architectures,\ndatasets, task types, model sizes, and predictive accuracy. Furthermore, we\npropose a novel entropy-based metric for automated temperature optimization,\nwhich consistently outperforms fixed-temperature baselines. Additionally, we\nincorporate a stochastic process model to enhance interpretability, offering\ndeeper insights into the relationship between temperature and model\nperformance.", "published": "2025-02-07 19:35:25", "link": "http://arxiv.org/abs/2502.05234v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SEER: Self-Explainability Enhancement of Large Language Models'\n  Representations", "abstract": "Explaining the hidden representations of Large Language Models (LLMs) is a\nperspective to understand LLMs' underlying inference logic and improve their\nreliability in application scenarios. However, previous methods introduce\nexternal ''black-box'' modules to explain ''black-box'' LLMs, increasing the\npotential uncertainty and failing to provide faithful explanations. In this\npaper, we propose a self-explaining method SEER, enhancing LLMs' explainability\nby aggregating the same concept and disentangling the different concepts in the\nrepresentation space. In this way, SEER provides faithful explanations carried\nby representations synchronously with the LLMs' output. Additionally, we\nshowcase the applications of SEER on trustworthiness-related tasks (e.g., the\nsafety risks classification and detoxification tasks), where self-explained\nLLMs achieve consistent improvement in explainability and performance. More\ncrucially, we theoretically analyze the improvement of SEER on LLMs'\ngeneralization ability through optimal transport theory.", "published": "2025-02-07 13:25:33", "link": "http://arxiv.org/abs/2502.05242v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Personality Traits in Large Language Models: Insights from\n  Psychological Questionnaires", "abstract": "Psychological assessment tools have long helped humans understand behavioural\npatterns. While Large Language Models (LLMs) can generate content comparable to\nthat of humans, we explore whether they exhibit personality traits. To this\nend, this work applies psychological tools to LLMs in diverse scenarios to\ngenerate personality profiles. Using established trait-based questionnaires\nsuch as the Big Five Inventory and by addressing the possibility of training\ndata contamination, we examine the dimensional variability and dominance of\nLLMs across five core personality dimensions: Openness, Conscientiousness,\nExtraversion, Agreeableness, and Neuroticism. Our findings reveal that LLMs\nexhibit unique dominant traits, varying characteristics, and distinct\npersonality profiles even within the same family of models.", "published": "2025-02-07 16:12:52", "link": "http://arxiv.org/abs/2502.05248v1", "categories": ["cs.CL", "cs.AI", "cs.MA", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Survey on Vision-Language-Action Models", "abstract": "This paper presents an AI-generated review of Vision-Language-Action (VLA)\nmodels, summarizing key methodologies, findings, and future directions. The\ncontent is produced using large language models (LLMs) and is intended only for\ndemonstration purposes. This work does not represent original research, but\nhighlights how AI can help automate literature reviews. As AI-generated content\nbecomes more prevalent, ensuring accuracy, reliability, and proper synthesis\nremains a challenge. Future research will focus on developing a structured\nframework for AI-assisted literature reviews, exploring techniques to enhance\ncitation accuracy, source credibility, and contextual understanding. By\nexamining the potential and limitations of LLM in academic writing, this study\naims to contribute to the broader discussion of integrating AI into research\nworkflows. This work serves as a preliminary step toward establishing\nsystematic approaches for leveraging AI in literature review generation, making\nacademic knowledge synthesis more efficient and scalable.", "published": "2025-02-07 11:56:46", "link": "http://arxiv.org/abs/2502.06851v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Understand Intermediate Representations?", "abstract": "Intermediate Representations (IRs) are essential in compiler design and\nprogram analysis, yet their comprehension by Large Language Models (LLMs)\nremains underexplored. This paper presents a pioneering empirical study to\ninvestigate the capabilities of LLMs, including GPT-4, GPT-3, Gemma 2, LLaMA\n3.1, and Code Llama, in understanding IRs. We analyze their performance across\nfour tasks: Control Flow Graph (CFG) reconstruction, decompilation, code\nsummarization, and execution reasoning. Our results indicate that while LLMs\ndemonstrate competence in parsing IR syntax and recognizing high-level\nstructures, they struggle with control flow reasoning, execution semantics, and\nloop handling. Specifically, they often misinterpret branching instructions,\nomit critical IR operations, and rely on heuristic-based reasoning, leading to\nerrors in CFG reconstruction, IR decompilation, and execution reasoning. The\nstudy underscores the necessity for IR-specific enhancements in LLMs,\nrecommending fine-tuning on structured IR datasets and integration of explicit\ncontrol flow models to augment their comprehension and handling of IR-related\ntasks.", "published": "2025-02-07 17:23:48", "link": "http://arxiv.org/abs/2502.06854v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Self-Supervised Prompt Optimization", "abstract": "Well-designed prompts are crucial for enhancing Large language models' (LLMs)\nreasoning capabilities while aligning their outputs with task requirements\nacross diverse domains. However, manually designed prompts require expertise\nand iterative experimentation. While existing prompt optimization methods aim\nto automate this process, they rely heavily on external references such as\nground truth or by humans, limiting their applicability in real-world scenarios\nwhere such data is unavailable or costly to obtain. To address this, we propose\nSelf-Supervised Prompt Optimization (SPO), a cost-efficient framework that\ndiscovers effective prompts for both closed and open-ended tasks without\nrequiring external reference. Motivated by the observations that prompt quality\nmanifests directly in LLM outputs and LLMs can effectively assess adherence to\ntask requirements, we derive evaluation and optimization signals purely from\noutput comparisons. Specifically, SPO selects superior prompts through pairwise\noutput comparisons evaluated by an LLM evaluator, followed by an LLM optimizer\nthat aligns outputs with task requirements. Extensive experiments demonstrate\nthat SPO outperforms state-of-the-art prompt optimization methods, achieving\ncomparable or superior results with significantly lower costs (e.g., 1.1% to\n5.6% of existing methods) and fewer samples (e.g., three samples). The code is\navailable at https://github.com/geekan/MetaGPT/blob/main/examples/spo", "published": "2025-02-07 17:45:16", "link": "http://arxiv.org/abs/2502.06855v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analyzing the Resource Utilization of Lambda Functions on Mobile\n  Devices: Case Studies on Kotlin and Swift", "abstract": "With billions of smartphones in use globally, the daily time spent on these\ndevices contributes significantly to overall electricity consumption. Given\nthis scale, even minor reductions in smartphone power use could result in\nsubstantial energy savings. This study explores the impact of Lambda functions\non resource consumption in mobile programming. While Lambda functions are known\nfor enhancing code readability and conciseness, their use does not add to the\nfunctional capabilities of a programming language. Our research investigates\nthe implications of using Lambda functions in terms of battery utilization,\nmemory usage, and execution time compared to equivalent code structures without\nLambda functions. Our findings reveal that Lambda functions impose a\nconsiderable resource overhead on mobile devices without offering additional\nfunctionalities.", "published": "2025-02-07 20:26:07", "link": "http://arxiv.org/abs/2502.07809v1", "categories": ["cs.SE", "cs.CL", "cs.PF", "D.1.0"], "primary_category": "cs.SE"}
{"title": "Detection of LLM-Generated Java Code Using Discretized Nested Bigrams", "abstract": "Large Language Models (LLMs) are currently used extensively to generate code\nby professionals and students, motivating the development of tools to detect\nLLM-generated code for applications such as academic integrity and\ncybersecurity. We address this authorship attribution problem as a binary\nclassification task along with feature identification and extraction. We\npropose new Discretized Nested Bigram Frequency features on source code groups\nof various sizes. Compared to prior work, improvements are obtained by\nrepresenting sparse information in dense membership bins. Experimental\nevaluation demonstrated that our approach significantly outperformed a commonly\nused GPT code-detection API and baseline features, with accuracy exceeding 96%\ncompared to 72% and 79% respectively in detecting GPT-rewritten Java code\nfragments for 976 files with GPT 3.5 and GPT4 using 12 features. We also\noutperformed three prior works on code author identification in a 40-author\ndataset. Our approach scales well to larger data sets, and we achieved 99%\naccuracy and 0.999 AUC for 76,089 files and over 1,000 authors with GPT 4o\nusing 227 features.", "published": "2025-02-07 14:32:20", "link": "http://arxiv.org/abs/2502.15740v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG", "68T50, 62H30", "I.2.7; K.6.5; D.2.8"], "primary_category": "cs.SE"}
{"title": "Dynamic Frequency-Adaptive Knowledge Distillation for Speech Enhancement", "abstract": "Deep learning-based speech enhancement (SE) models have recently outperformed\ntraditional techniques, yet their deployment on resource-constrained devices\nremains challenging due to high computational and memory demands. This paper\nintroduces a novel dynamic frequency-adaptive knowledge distillation (DFKD)\napproach to effectively compress SE models. Our method dynamically assesses the\nmodel's output, distinguishing between high and low-frequency components, and\nadapts the learning objectives to meet the unique requirements of different\nfrequency bands, capitalizing on the SE task's inherent characteristics. To\nevaluate the DFKD's efficacy, we conducted experiments on three\nstate-of-the-art models: DCCRN, ConTasNet, and DPTNet. The results demonstrate\nthat our method not only significantly enhances the performance of the\ncompressed model (student model) but also surpasses other logit-based knowledge\ndistillation methods specifically for SE tasks.", "published": "2025-02-07 07:25:59", "link": "http://arxiv.org/abs/2502.04711v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Efficient Evaluation of Quantization-Effects in Neural Codecs", "abstract": "Neural codecs, comprising an encoder, quantizer, and decoder, enable signal\ntransmission at exceptionally low bitrates. Training these systems requires\ntechniques like the straight-through estimator, soft-to-hard annealing, or\nstatistical quantizer emulation to allow a non-zero gradient across the\nquantizer. Evaluating the effect of quantization in neural codecs, like the\ninfluence of gradient passing techniques on the whole system, is often costly\nand time-consuming due to training demands and the lack of affordable and\nreliable metrics. This paper proposes an efficient evaluation framework for\nneural codecs using simulated data with a defined number of bits and\nlow-complexity neural encoders/decoders to emulate the non-linear behavior in\nlarger networks. Our system is highly efficient in terms of training time and\ncomputational and hardware requirements, allowing us to uncover distinct\nbehaviors in neural codecs. We propose a modification to stabilize training\nwith the straight-through estimator based on our findings. We validate our\nfindings against an internal neural audio codec and against the\nstate-of-the-art descript-audio-codec.", "published": "2025-02-07 09:11:19", "link": "http://arxiv.org/abs/2502.04770v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Distillation and Pruning for Scalable Self-Supervised\n  Representation-Based Speech Quality Assessment", "abstract": "In this paper, we investigate distillation and pruning methods to reduce\nmodel size for non-intrusive speech quality assessment based on self-supervised\nrepresentations. Our experiments build on XLS-R-SQA, a speech quality\nassessment model using wav2vec 2.0 XLS-R embeddings. We retrain this model on a\nlarge compilation of mean opinion score datasets, encompassing over 100,000\nlabeled clips. For distillation, using this model as a teacher, we generate\npseudo-labels on unlabeled degraded speech signals and train student models of\nvarying sizes. For pruning, we use a data-driven strategy. While data-driven\npruning performs better at larger model sizes, distillation on unlabeled data\nis more effective for smaller model sizes. Distillation can halve the gap\nbetween the baseline's correlation with ground-truth MOS labels and that of the\nXLS-R-based teacher model, while reducing model size by two orders of magnitude\ncompared to the teacher model.", "published": "2025-02-07 22:08:12", "link": "http://arxiv.org/abs/2502.05356v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Singing Voice Conversion with Accompaniment Using Self-Supervised\n  Representation-Based Melody Features", "abstract": "Melody preservation is crucial in singing voice conversion (SVC). However, in\nmany scenarios, audio is often accompanied with background music (BGM), which\ncan cause audio distortion and interfere with the extraction of melody and\nother key features, significantly degrading SVC performance. Previous methods\nhave attempted to address this by using more robust neural network-based melody\nextractors, but their performance drops sharply in the presence of complex\naccompaniment. Other approaches involve performing source separation before\nconversion, but this often introduces noticeable artifacts, leading to a\nsignificant drop in conversion quality and increasing the user's operational\ncosts. To address these issues, we introduce a novel SVC method that uses\nself-supervised representation-based melody features to improve melody modeling\naccuracy in the presence of BGM. In our experiments, we compare the\neffectiveness of different self-supervised learning (SSL) models for melody\nextraction and explore for the first time how SSL benefits the task of melody\nextraction. The experimental results demonstrate that our proposed SVC model\nsignificantly outperforms existing baseline methods in terms of melody accuracy\nand shows higher similarity and naturalness in both subjective and objective\nevaluations across noisy and clean audio environments.", "published": "2025-02-07 07:46:19", "link": "http://arxiv.org/abs/2502.04722v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Meta Audiobox Aesthetics: Unified Automatic Quality Assessment for\n  Speech, Music, and Sound", "abstract": "The quantification of audio aesthetics remains a complex challenge in audio\nprocessing, primarily due to its subjective nature, which is influenced by\nhuman perception and cultural context. Traditional methods often depend on\nhuman listeners for evaluation, leading to inconsistencies and high resource\ndemands. This paper addresses the growing need for automated systems capable of\npredicting audio aesthetics without human intervention. Such systems are\ncrucial for applications like data filtering, pseudo-labeling large datasets,\nand evaluating generative audio models, especially as these models become more\nsophisticated. In this work, we introduce a novel approach to audio aesthetic\nevaluation by proposing new annotation guidelines that decompose human\nlistening perspectives into four distinct axes. We develop and train\nno-reference, per-item prediction models that offer a more nuanced assessment\nof audio quality. Our models are evaluated against human mean opinion scores\n(MOS) and existing methods, demonstrating comparable or superior performance.\nThis research not only advances the field of audio aesthetics but also provides\nopen-source models and datasets to facilitate future work and benchmarking. We\nrelease our code and pre-trained model at:\nhttps://github.com/facebookresearch/audiobox-aesthetics", "published": "2025-02-07 18:15:57", "link": "http://arxiv.org/abs/2502.05139v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Koel-TTS: Enhancing LLM based Speech Generation with Preference\n  Alignment and Classifier Free Guidance", "abstract": "While autoregressive speech token generation models produce speech with\nremarkable variety and naturalness, their inherent lack of controllability\noften results in issues such as hallucinations and undesired vocalizations that\ndo not conform to conditioning inputs. We introduce Koel-TTS, a suite of\nenhanced encoder-decoder Transformer TTS models that address these challenges\nby incorporating preference alignment techniques guided by automatic speech\nrecognition and speaker verification models. Additionally, we incorporate\nclassifier-free guidance to further improve synthesis adherence to the\ntranscript and reference speaker audio. Our experiments demonstrate that these\noptimizations significantly enhance target speaker similarity, intelligibility,\nand naturalness of synthesized speech. Notably, Koel-TTS directly maps text and\ncontext audio to acoustic tokens, and on the aforementioned metrics,\noutperforms state-of-the-art TTS models, despite being trained on a\nsignificantly smaller dataset. Audio samples and demos are available on our\nwebsite.", "published": "2025-02-07 06:47:11", "link": "http://arxiv.org/abs/2502.05236v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Latent Swap Joint Diffusion for 2D Long-Form Latent Generation", "abstract": "This paper introduces Swap Forward (SaFa), a modality-agnostic and efficient\nmethod to generate seamless and coherence long spectrum and panorama through\nlatent swap joint diffusion across multi-views. We first investigate the\nspectrum aliasing problem in spectrum-based audio generation caused by existing\njoint diffusion methods. Through a comparative analysis of the VAE latent\nrepresentation of Mel-spectra and RGB images, we identify that the failure\narises from excessive suppression of high-frequency components during the\nspectrum denoising process due to the averaging operator. To address this\nissue, we propose Self-Loop Latent Swap, a frame-level bidirectional swap\napplied to the overlapping region of adjacent views. Leveraging stepwise\ndifferentiated trajectories of adjacent subviews, this swap operator adaptively\nenhances high-frequency components and avoid spectrum distortion. Furthermore,\nto improve global cross-view consistency in non-overlapping regions, we\nintroduce Reference-Guided Latent Swap, a unidirectional latent swap operator\nthat provides a centralized reference trajectory to synchronize subview\ndiffusions. By refining swap timing and intervals, we can achieve a cross-view\nsimilarity-diversity balance in a forward-only manner. Quantitative and\nqualitative experiments demonstrate that SaFa significantly outperforms\nexisting joint diffusion methods and even training-based methods in audio\ngeneration using both U-Net and DiT models, along with effective longer length\nadaptation. It also adapts well to panorama generation, achieving comparable\nperformance with 2 $\\sim$ 20 $\\times$ faster speed and greater model\ngeneralizability. More generation demos are available at\nhttps://swapforward.github.io/", "published": "2025-02-07 18:02:47", "link": "http://arxiv.org/abs/2502.05130v2", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
