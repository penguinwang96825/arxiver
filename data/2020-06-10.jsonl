{"title": "Adversarial Training Based Multi-Source Unsupervised Domain Adaptation\n  for Sentiment Analysis", "abstract": "Multi-source unsupervised domain adaptation (MS-UDA) for sentiment analysis\n(SA) aims to leverage useful information in multiple source domains to help do\nSA in an unlabeled target domain that has no supervised information. Existing\nalgorithms of MS-UDA either only exploit the shared features, i.e., the\ndomain-invariant information, or based on some weak assumption in NLP, e.g.,\nsmoothness assumption. To avoid these problems, we propose two transfer\nlearning frameworks based on the multi-source domain adaptation methodology for\nSA by combining the source hypotheses to derive a good target hypothesis. The\nkey feature of the first framework is a novel Weighting Scheme based\nUnsupervised Domain Adaptation framework (WS-UDA), which combine the source\nclassifiers to acquire pseudo labels for target instances directly. While the\nsecond framework is a Two-Stage Training based Unsupervised Domain Adaptation\nframework (2ST-UDA), which further exploits these pseudo labels to train a\ntarget private extractor. Importantly, the weights assigned to each source\nclassifier are based on the relations between target instances and source\ndomains, which measured by a discriminator through the adversarial training.\nFurthermore, through the same discriminator, we also fulfill the separation of\nshared features and private features. Experimental results on two SA datasets\ndemonstrate the promising performance of our frameworks, which outperforms\nunsupervised state-of-the-art competitors.", "published": "2020-06-10 01:41:00", "link": "http://arxiv.org/abs/2006.05602v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Points of Correspondence between Sentences for Abstractive\n  Summarization", "abstract": "Fusing sentences containing disparate content is a remarkable human ability\nthat helps create informative and succinct summaries. Such a simple task for\nhumans has remained challenging for modern abstractive summarizers,\nsubstantially restricting their applicability in real-world scenarios. In this\npaper, we present an investigation into fusing sentences drawn from a document\nby introducing the notion of points of correspondence, which are cohesive\ndevices that tie any two sentences together into a coherent text. The types of\npoints of correspondence are delineated by text cohesion theory, covering\npronominal and nominal referencing, repetition and beyond. We create a dataset\ncontaining the documents, source and fusion sentences, and human annotations of\npoints of correspondence between sentences. Our dataset bridges the gap between\ncoreference resolution and summarization. It is publicly shared to serve as a\nbasis for future work to measure the success of sentence fusion systems.\n(https://github.com/ucfnlp/points-of-correspondence)", "published": "2020-06-10 02:42:38", "link": "http://arxiv.org/abs/2006.05621v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Unified Dialogue System Evaluation: A Comprehensive Analysis of\n  Current Evaluation Protocols", "abstract": "As conversational AI-based dialogue management has increasingly become a\ntrending topic, the need for a standardized and reliable evaluation procedure\ngrows even more pressing. The current state of affairs suggests various\nevaluation protocols to assess chat-oriented dialogue management systems,\nrendering it difficult to conduct fair comparative studies across different\napproaches and gain an insightful understanding of their values. To foster this\nresearch, a more robust evaluation protocol must be set in place. This paper\npresents a comprehensive synthesis of both automated and human evaluation\nmethods on dialogue systems, identifying their shortcomings while accumulating\nevidence towards the most effective evaluation dimensions. A total of 20 papers\nfrom the last two years are surveyed to analyze three types of evaluation\nprotocols: automated, static, and interactive. Finally, the evaluation\ndimensions used in these papers are compared against our expert evaluation on\nthe system-user dialogue data collected from the Alexa Prize 2020.", "published": "2020-06-10 23:29:05", "link": "http://arxiv.org/abs/2006.06110v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-shot Slot Tagging with Collapsed Dependency Transfer and\n  Label-enhanced Task-adaptive Projection Network", "abstract": "In this paper, we explore the slot tagging with only a few labeled support\nsentences (a.k.a. few-shot). Few-shot slot tagging faces a unique challenge\ncompared to the other few-shot classification problems as it calls for modeling\nthe dependencies between labels. But it is hard to apply previously learned\nlabel dependencies to an unseen domain, due to the discrepancy of label sets.\nTo tackle this, we introduce a collapsed dependency transfer mechanism into the\nconditional random field (CRF) to transfer abstract label dependency patterns\nas transition scores. In the few-shot setting, the emission score of CRF can be\ncalculated as a word's similarity to the representation of each label. To\ncalculate such similarity, we propose a Label-enhanced Task-Adaptive Projection\nNetwork (L-TapNet) based on the state-of-the-art few-shot classification model\n-- TapNet, by leveraging label name semantics in representing labels.\nExperimental results show that our model significantly outperforms the\nstrongest few-shot learning baseline by 14.64 F1 scores in the one-shot\nsetting.", "published": "2020-06-10 07:50:44", "link": "http://arxiv.org/abs/2006.05702v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Estimating semantic structure for the VQA answer space", "abstract": "Since its appearance, Visual Question Answering (VQA, i.e. answering a\nquestion posed over an image), has always been treated as a classification\nproblem over a set of predefined answers. Despite its convenience, this\nclassification approach poorly reflects the semantics of the problem limiting\nthe answering to a choice between independent proposals, without taking into\naccount the similarity between them (e.g. equally penalizing for answering cat\nor German shepherd instead of dog). We address this issue by proposing (1) two\nmeasures of proximity between VQA classes, and (2) a corresponding loss which\ntakes into account the estimated proximity. This significantly improves the\ngeneralization of VQA models by reducing their language bias. In particular, we\nshow that our approach is completely model-agnostic since it allows consistent\nimprovements with three different VQA models. Finally, by combining our method\nwith a language bias reduction approach, we report SOTA-level performance on\nthe challenging VQAv2-CP dataset.", "published": "2020-06-10 08:32:56", "link": "http://arxiv.org/abs/2006.05726v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MC-BERT: Efficient Language Pre-Training via a Meta Controller", "abstract": "Pre-trained contextual representations (e.g., BERT) have become the\nfoundation to achieve state-of-the-art results on many NLP tasks. However,\nlarge-scale pre-training is computationally expensive. ELECTRA, an early\nattempt to accelerate pre-training, trains a discriminative model that predicts\nwhether each input token was replaced by a generator. Our studies reveal that\nELECTRA's success is mainly due to its reduced complexity of the pre-training\ntask: the binary classification (replaced token detection) is more efficient to\nlearn than the generation task (masked language modeling). However, such a\nsimplified task is less semantically informative. To achieve better efficiency\nand effectiveness, we propose a novel meta-learning framework, MC-BERT. The\npre-training task is a multi-choice cloze test with a reject option, where a\nmeta controller network provides training input and candidates. Results over\nGLUE natural language understanding benchmark demonstrate that our proposed\nmethod is both efficient and effective: it outperforms baselines on GLUE\nsemantic tasks given the same computational budget.", "published": "2020-06-10 09:22:19", "link": "http://arxiv.org/abs/2006.05744v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Embed2Detect: Temporally Clustered Embedded Words for Event Detection in\n  Social Media", "abstract": "Social media is becoming a primary medium to discuss what is happening around\nthe world. Therefore, the data generated by social media platforms contain rich\ninformation which describes the ongoing events. Further, the timeliness\nassociated with these data is capable of facilitating immediate insights.\nHowever, considering the dynamic nature and high volume of data production in\nsocial media data streams, it is impractical to filter the events manually and\ntherefore, automated event detection mechanisms are invaluable to the\ncommunity. Apart from a few notable exceptions, most previous research on\nautomated event detection have focused only on statistical and syntactical\nfeatures in data and lacked the involvement of underlying semantics which are\nimportant for effective information retrieval from text since they represent\nthe connections between words and their meanings. In this paper, we propose a\nnovel method termed Embed2Detect for event detection in social media by\ncombining the characteristics in word embeddings and hierarchical agglomerative\nclustering. The adoption of word embeddings gives Embed2Detect the capability\nto incorporate powerful semantical features into event detection and overcome a\nmajor limitation inherent in previous approaches. We experimented our method on\ntwo recent real social media data sets which represent the sports and political\ndomain and also compared the results to several state-of-the-art methods. The\nobtained results show that Embed2Detect is capable of effective and efficient\nevent detection and it outperforms the recent event detection methods. For the\nsports data set, Embed2Detect achieved 27% higher F-measure than the\nbest-performed baseline and for the political data set, it was an increase of\n29%.", "published": "2020-06-10 15:52:52", "link": "http://arxiv.org/abs/2006.05908v4", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Revisiting Few-sample BERT Fine-tuning", "abstract": "This paper is a study of fine-tuning of BERT contextual representations, with\nfocus on commonly observed instabilities in few-sample scenarios. We identify\nseveral factors that cause this instability: the common use of a non-standard\noptimization method with biased gradient estimation; the limited applicability\nof significant parts of the BERT network for down-stream tasks; and the\nprevalent practice of using a pre-determined, and small number of training\niterations. We empirically test the impact of these factors, and identify\nalternative practices that resolve the commonly observed instability of the\nprocess. In light of these observations, we re-visit recently proposed methods\nto improve few-sample fine-tuning with BERT and re-evaluate their\neffectiveness. Generally, we observe the impact of these methods diminishes\nsignificantly with our modified process.", "published": "2020-06-10 17:57:03", "link": "http://arxiv.org/abs/2006.05987v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Report from the NSF Future Directions Workshop, Toward User-Oriented\n  Agents: Research Directions and Challenges", "abstract": "This USER Workshop was convened with the goal of defining future research\ndirections for the burgeoning intelligent agent research community and to\ncommunicate them to the National Science Foundation. It took place in\nPittsburgh Pennsylvania on October 24 and 25, 2019 and was sponsored by\nNational Science Foundation Grant Number IIS-1934222. Any opinions, findings\nand conclusions or future directions expressed in this document are those of\nthe authors and do not necessarily reflect the views of the National Science\nFoundation. The 27 participants presented their individual research interests\nand their personal research goals. In the breakout sessions that followed, the\nparticipants defined the main research areas within the domain of intelligent\nagents and they discussed the major future directions that the research in each\narea of this domain should take", "published": "2020-06-10 18:32:35", "link": "http://arxiv.org/abs/2006.06026v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Consolidating Commonsense Knowledge", "abstract": "Commonsense reasoning is an important aspect of building robust AI systems\nand is receiving significant attention in the natural language understanding,\ncomputer vision, and knowledge graphs communities. At present, a number of\nvaluable commonsense knowledge sources exist, with different foci, strengths,\nand weaknesses. In this paper, we list representative sources and their\nproperties. Based on this survey, we propose principles and a representation\nmodel in order to consolidate them into a Common Sense Knowledge Graph (CSKG).\nWe apply this approach to consolidate seven separate sources into a first\nintegrated CSKG. We present statistics of CSKG, present initial investigations\nof its utility on four QA datasets, and list learned lessons.", "published": "2020-06-10 23:40:11", "link": "http://arxiv.org/abs/2006.06114v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Data Augmentation for Training Dialog Models Robust to Speech\n  Recognition Errors", "abstract": "Speech-based virtual assistants, such as Amazon Alexa, Google assistant, and\nApple Siri, typically convert users' audio signals to text data through\nautomatic speech recognition (ASR) and feed the text to downstream dialog\nmodels for natural language understanding and response generation. The ASR\noutput is error-prone; however, the downstream dialog models are often trained\non error-free text data, making them sensitive to ASR errors during inference\ntime. To bridge the gap and make dialog models more robust to ASR errors, we\nleverage an ASR error simulator to inject noise into the error-free text data,\nand subsequently train the dialog models with the augmented data. Compared to\nother approaches for handling ASR errors, such as using ASR lattice or\nend-to-end methods, our data augmentation approach does not require any\nmodification to the ASR or downstream dialog models; our approach also does not\nintroduce any additional latency during inference time. We perform extensive\nexperiments on benchmark data and show that our approach improves the\nperformance of downstream dialog models in the presence of ASR errors, and it\nis particularly effective in the low-resource situations where there are\nconstraints on model size or the training data is scarce.", "published": "2020-06-10 03:18:15", "link": "http://arxiv.org/abs/2006.05635v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gender in Danger? Evaluating Speech Translation Technology on the\n  MuST-SHE Corpus", "abstract": "Translating from languages without productive grammatical gender like English\ninto gender-marked languages is a well-known difficulty for machines. This\ndifficulty is also due to the fact that the training data on which models are\nbuilt typically reflect the asymmetries of natural languages, gender bias\nincluded. Exclusively fed with textual data, machine translation is\nintrinsically constrained by the fact that the input sentence does not always\ncontain clues about the gender identity of the referred human entities. But\nwhat happens with speech translation, where the input is an audio signal? Can\naudio provide additional information to reduce gender bias? We present the\nfirst thorough investigation of gender bias in speech translation, contributing\nwith: i) the release of a benchmark useful for future studies, and ii) the\ncomparison of different technologies (cascade and end-to-end) on two language\ndirections (English-Italian/French).", "published": "2020-06-10 09:55:38", "link": "http://arxiv.org/abs/2006.05754v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ClarQ: A large-scale and diverse dataset for Clarification Question\n  Generation", "abstract": "Question answering and conversational systems are often baffled and need help\nclarifying certain ambiguities. However, limitations of existing datasets\nhinder the development of large-scale models capable of generating and\nutilising clarification questions. In order to overcome these limitations, we\ndevise a novel bootstrapping framework (based on self-supervision) that assists\nin the creation of a diverse, large-scale dataset of clarification questions\nbased on post-comment tuples extracted from stackexchange. The framework\nutilises a neural network based architecture for classifying clarification\nquestions. It is a two-step method where the first aims to increase the\nprecision of the classifier and second aims to increase its recall. We\nquantitatively demonstrate the utility of the newly created dataset by applying\nit to the downstream task of question-answering. The final dataset, ClarQ,\nconsists of ~2M examples distributed across 173 domains of stackexchange. We\nrelease this dataset in order to foster research into the field of\nclarification question generation with the larger goal of enhancing dialog and\nquestion answering systems.", "published": "2020-06-10 17:56:50", "link": "http://arxiv.org/abs/2006.05986v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PeopleMap: Visualization Tool for Mapping Out Researchers using Natural\n  Language Processing", "abstract": "Discovering research expertise at institutions can be a difficult task.\nManually curated university directories easily become out of date and they\noften lack the information necessary for understanding a researcher's interests\nand past work, making it harder to explore the diversity of research at an\ninstitution and identify research talents. This results in lost opportunities\nfor both internal and external entities to discover new connections and nurture\nresearch collaboration. To solve this problem, we have developed PeopleMap, the\nfirst interactive, open-source, web-based tool that visually \"maps out\"\nresearchers based on their research interests and publications by leveraging\nembeddings generated by natural language processing (NLP) techniques. PeopleMap\nprovides a new engaging way for institutions to summarize their research\ntalents and for people to discover new connections. The platform is developed\nwith ease-of-use and sustainability in mind. Using only researchers' Google\nScholar profiles as input, PeopleMap can be readily adopted by any institution\nusing its publicly-accessible repository and detailed documentation.", "published": "2020-06-10 23:06:25", "link": "http://arxiv.org/abs/2006.06105v1", "categories": ["cs.DL", "cs.CL", "cs.HC"], "primary_category": "cs.DL"}
{"title": "A novel sentence embedding based topic detection method for micro-blog", "abstract": "Topic detection is a challenging task, especially without knowing the exact\nnumber of topics. In this paper, we present a novel approach based on neural\nnetwork to detect topics in the micro-blogging dataset. We use an unsupervised\nneural sentence embedding model to map the blogs to an embedding space. Our\nmodel is a weighted power mean word embedding model, and the weights are\ncalculated by attention mechanism. Experimental result shows our embedding\nmethod performs better than baselines in sentence clustering. In addition, we\npropose an improved clustering algorithm referred as relationship-aware DBSCAN\n(RADBSCAN). It can discover topics from a micro-blogging dataset, and the topic\nnumber depends on dataset character itself. Moreover, in order to solve the\nproblem of parameters sensitive, we take blog forwarding relationship as a\nbridge of two independent clusters. Finally, we validate our approach on a\ndataset from sina micro-blog. The result shows that we can detect all the\ntopics successfully and extract keywords in each topic.", "published": "2020-06-10 09:58:57", "link": "http://arxiv.org/abs/2006.09977v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Speaker Diarization: Using Recurrent Neural Networks", "abstract": "Speaker Diarization is the problem of separating speakers in an audio. There\ncould be any number of speakers and final result should state when speaker\nstarts and ends. In this project, we analyze given audio file with 2 channels\nand 2 speakers (on separate channel). We train Neural Network for learning when\na person is speaking. We use different type of Neural Networks specifically,\nSingle Layer Perceptron (SLP), Multi Layer Perceptron (MLP), Recurrent Neural\nNetwork (RNN) and Convolution Neural Network (CNN) we achieve $\\sim$92\\% of\naccuracy with RNN. The code for this project is available at\nhttps://github.com/vishalshar/SpeakerDiarization_RNN_CNN_LSTM", "published": "2020-06-10 01:19:18", "link": "http://arxiv.org/abs/2006.05596v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Uniphore's submission to Fearless Steps Challenge Phase-2", "abstract": "We propose supervised systems for speech activity detection (SAD) and speaker\nidentification (SID) tasks in Fearless Steps Challenge Phase-2. The proposed\nsystems for both the tasks share a common convolutional neural network (CNN)\narchitecture. Mel spectrogram is used as features. For speech activity\ndetection, the spectrogram is divided into smaller overlapping chunks. The\nnetwork is trained to recognize the chunks. The network architecture and the\ntraining steps used for the SID task are similar to that of the SAD task,\nexcept that longer spectrogram chunks are used. We propose a two-level\nidentification method for SID task. First, for each chunk, a set of speakers is\nhypothesized based on the neural network posterior probabilities. Finally, the\nspeaker identity of the utterance is identified using the chunk-level\nhypotheses by applying a voting rule. On SAD task, a detection cost function\nscore of 5.96%, and 5.33% are obtained on dev and eval sets, respectively. A\ntop 5 retrieval accuracy of 82.07% and 82.42% are obtained on the dev and eval\nsets for SID task. A brief analysis is made on the results to provide insights\ninto the miss-classified cases in both the tasks.", "published": "2020-06-10 09:34:46", "link": "http://arxiv.org/abs/2006.05747v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring Quality and Generalizability in Parameterized Neural Audio\n  Effects", "abstract": "Deep neural networks have shown promise for music audio signal processing\napplications, often surpassing prior approaches, particularly as end-to-end\nmodels in the waveform domain. Yet results to date have tended to be\nconstrained by low sample rates, noise, narrow domains of signal types, and/or\nlack of parameterized controls (i.e. \"knobs\"), making their suitability for\nprofessional audio engineering workflows still lacking. This work expands on\nprior research published on modeling nonlinear time-dependent signal processing\neffects associated with music production by means of a deep neural network, one\nwhich includes the ability to emulate the parameterized settings you would see\non an analog piece of equipment, with the goal of eventually producing\ncommercially viable, high quality audio, i.e. 44.1 kHz sampling rate at 16-bit\nresolution. The results in this paper highlight progress in modeling these\neffects through architecture and optimization changes, towards increasing\ncomputational efficiency, lowering signal-to-noise ratio, and extending to a\nlarger variety of nonlinear audio effects. Toward these ends, the strategies\nemployed involved a three-pronged approach: model speed, model accuracy, and\nmodel generalizability. Most of the presented methods provide marginal or no\nincrease in output accuracy over the original model, with the exception of\ndataset manipulation. We found that limiting the audio content of the dataset,\nfor example using datasets of just a single instrument, provided a significant\nimprovement in model accuracy over models trained on more general datasets.", "published": "2020-06-10 00:52:08", "link": "http://arxiv.org/abs/2006.05584v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "I.2.6"], "primary_category": "eess.AS"}
{"title": "Integrated Replay Spoofing-aware Text-independent Speaker Verification", "abstract": "A number of studies have successfully developed speaker verification or\npresentation attack detection systems. However, studies integrating the two\ntasks remain in the preliminary stages. In this paper, we propose two\napproaches for building an integrated system of speaker verification and\npresentation attack detection: an end-to-end monolithic approach and a back-end\nmodular approach. The first approach simultaneously trains speaker\nidentification, presentation attack detection, and the integrated system using\nmulti-task learning using a common feature. However, through experiments, we\nhypothesize that the information required for performing speaker verification\nand presentation attack detection might differ because speaker verification\nsystems try to remove device-specific information from speaker embeddings,\nwhile presentation attack detection systems exploit such information.\nTherefore, we propose a back-end modular approach using a separate deep neural\nnetwork (DNN) for speaker verification and presentation attack detection. This\napproach has thee input components: two speaker embeddings (for enrollment and\ntest each) and prediction of presentation attacks. Experiments are conducted\nusing the ASVspoof 2017-v2 dataset, which includes official trials on the\nintegration of speaker verification and presentation attack detection. The\nproposed back-end approach demonstrates a relative improvement of 21.77% in\nterms of the equal error rate for integrated trials compared to a conventional\nspeaker verification system.", "published": "2020-06-10 01:24:55", "link": "http://arxiv.org/abs/2006.05599v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "HiFi-GAN: High-Fidelity Denoising and Dereverberation Based on Speech\n  Deep Features in Adversarial Networks", "abstract": "Real-world audio recordings are often degraded by factors such as noise,\nreverberation, and equalization distortion. This paper introduces HiFi-GAN, a\ndeep learning method to transform recorded speech to sound as though it had\nbeen recorded in a studio. We use an end-to-end feed-forward WaveNet\narchitecture, trained with multi-scale adversarial discriminators in both the\ntime domain and the time-frequency domain. It relies on the deep feature\nmatching losses of the discriminators to improve the perceptual quality of\nenhanced speech. The proposed model generalizes well to new speakers, new\nspeech content, and new environments. It significantly outperforms\nstate-of-the-art baseline methods in both objective and subjective experiments.", "published": "2020-06-10 07:24:39", "link": "http://arxiv.org/abs/2006.05694v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Listen to What You Want: Neural Network-based Universal Sound Selector", "abstract": "Being able to control the acoustic events (AEs) to which we want to listen\nwould allow the development of more controllable hearable devices. This paper\naddresses the AE sound selection (or removal) problems, that we define as the\nextraction (or suppression) of all the sounds that belong to one or multiple\ndesired AE classes. Although this problem could be addressed with a combination\nof source separation followed by AE classification, this is a sub-optimal way\nof solving the problem. Moreover, source separation usually requires knowing\nthe maximum number of sources, which may not be practical when dealing with\nAEs. In this paper, we propose instead a universal sound selection neural\nnetwork that enables to directly select AE sounds from a mixture given\nuser-specified target AE classes. The proposed framework can be explicitly\noptimized to simultaneously select sounds from multiple desired AE classes,\nindependently of the number of sources in the mixture. We experimentally show\nthat the proposed method achieves promising AE sound selection performance and\ncould be generalized to mixtures with a number of sources that are unseen\nduring training.", "published": "2020-06-10 08:06:02", "link": "http://arxiv.org/abs/2006.05712v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Description and Discussion on DCASE2020 Challenge Task2: Unsupervised\n  Anomalous Sound Detection for Machine Condition Monitoring", "abstract": "In this paper, we present the task description and discuss the results of the\nDCASE 2020 Challenge Task 2: Unsupervised Detection of Anomalous Sounds for\nMachine Condition Monitoring. The goal of anomalous sound detection (ASD) is to\nidentify whether the sound emitted from a target machine is normal or\nanomalous. The main challenge of this task is to detect unknown anomalous\nsounds under the condition that only normal sound samples have been provided as\ntraining data. We have designed this challenge as the first benchmark of ASD\nresearch, which includes a large-scale dataset, evaluation metrics, and a\nsimple baseline system. We received 117 submissions from 40 teams, and several\nnovel approaches have been developed as a result of this challenge. On the\nbasis of the analysis of the evaluation results, we discuss two new approaches\nand their problems.", "published": "2020-06-10 13:17:36", "link": "http://arxiv.org/abs/2006.05822v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Exploring Automatic Diagnosis of COVID-19 from Crowdsourced Respiratory\n  Sound Data", "abstract": "Audio signals generated by the human body (e.g., sighs, breathing, heart,\ndigestion, vibration sounds) have routinely been used by clinicians as\nindicators to diagnose disease or assess disease progression. Until recently,\nsuch signals were usually collected through manual auscultation at scheduled\nvisits. Research has now started to use digital technology to gather bodily\nsounds (e.g., from digital stethoscopes) for cardiovascular or respiratory\nexamination, which could then be used for automatic analysis. Some initial work\nshows promise in detecting diagnostic signals of COVID-19 from voice and\ncoughs. In this paper we describe our data analysis over a large-scale\ncrowdsourced dataset of respiratory sounds collected to aid diagnosis of\nCOVID-19. We use coughs and breathing to understand how discernible COVID-19\nsounds are from those in asthma or healthy controls. Our results show that even\na simple binary machine learning classifier is able to classify correctly\nhealthy and COVID-19 sounds. We also show how we distinguish a user who tested\npositive for COVID-19 and has a cough from a healthy user with a cough, and\nusers who tested positive for COVID-19 and have a cough from users with asthma\nand a cough. Our models achieve an AUC of above 80% across all tasks. These\nresults are preliminary and only scratch the surface of the potential of this\ntype of data and audio-based machine learning. This work opens the door to\nfurther investigation of how automatically analysed respiratory patterns could\nbe used as pre-screening signals to aid COVID-19 diagnosis.", "published": "2020-06-10 16:13:06", "link": "http://arxiv.org/abs/2006.05919v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep generative models for musical audio synthesis", "abstract": "Sound modelling is the process of developing algorithms that generate sound\nunder parametric control. There are a few distinct approaches that have been\ndeveloped historically including modelling the physics of sound production and\npropagation, assembling signal generating and processing elements to capture\nacoustic features, and manipulating collections of recorded audio samples.\nWhile each of these approaches has been able to achieve high-quality synthesis\nand interaction for specific applications, they are all labour-intensive and\neach comes with its own challenges for designing arbitrary control strategies.\nRecent generative deep learning systems for audio synthesis are able to learn\nmodels that can traverse arbitrary spaces of sound defined by the data they\ntrain on. Furthermore, machine learning systems are providing new techniques\nfor designing control and navigation strategies for these models. This paper is\na review of developments in deep learning that are changing the practice of\nsound modelling.", "published": "2020-06-10 04:02:42", "link": "http://arxiv.org/abs/2006.06426v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML", "I.2.6; J.5"], "primary_category": "eess.AS"}
