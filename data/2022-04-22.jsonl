{"title": "Towards Multi-Turn Empathetic Dialogs with Positive Emotion Elicitation", "abstract": "Emotional support is a crucial skill for many real-world scenarios, including\ncaring for the elderly, mental health support, and customer service chats. This\npaper presents a novel task of empathetic dialog generation with positive\nemotion elicitation to promote users' positive emotions, similar to that of\nemotional support between humans. In this task, the agent conducts empathetic\nresponses along with the target of eliciting the user's positive emotions in\nthe multi-turn dialog. To facilitate the study of this task, we collect a\nlarge-scale emotional dialog dataset with positive emotion elicitation, called\nPosEmoDial (about 820k dialogs, 3M utterances). In these dialogs, the agent\ntries to guide the user from any possible initial emotional state, e.g.,\nsadness, to a positive emotional state. Then we present a\npositive-emotion-guided dialog generation model with a novel loss function\ndesign. This loss function encourages the dialog model to not only elicit\npositive emotions from users but also ensure smooth emotional transitions along\nwith the whole dialog. Finally, we establish benchmark results on PosEmoDial,\nand we will release this dataset and related source code to facilitate future\nstudies.", "published": "2022-04-22 05:32:08", "link": "http://arxiv.org/abs/2204.10509v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero and Few-shot Learning for Author Profiling", "abstract": "Author profiling classifies author characteristics by analyzing how language\nis shared among people. In this work, we study that task from a low-resource\nviewpoint: using little or no training data. We explore different zero and\nfew-shot models based on entailment and evaluate our systems on several\nprofiling tasks in Spanish and English. In addition, we study the effect of\nboth the entailment hypothesis and the size of the few-shot training sample. We\nfind that entailment-based models out-perform supervised text classifiers based\non roberta-XLM and that we can reach 80% of the accuracy of previous approaches\nusing less than 50\\% of the training data on average.", "published": "2022-04-22 07:22:37", "link": "http://arxiv.org/abs/2204.10543v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KALA: Knowledge-Augmented Language Model Adaptation", "abstract": "Pre-trained language models (PLMs) have achieved remarkable success on\nvarious natural language understanding tasks. Simple fine-tuning of PLMs, on\nthe other hand, might be suboptimal for domain-specific tasks because they\ncannot possibly cover knowledge from all domains. While adaptive pre-training\nof PLMs can help them obtain domain-specific knowledge, it requires a large\ntraining cost. Moreover, adaptive pre-training can harm the PLM's performance\non the downstream task by causing catastrophic forgetting of its general\nknowledge. To overcome such limitations of adaptive pre-training for PLM\nadaption, we propose a novel domain adaption framework for PLMs coined as\nKnowledge-Augmented Language model Adaptation (KALA), which modulates the\nintermediate hidden representations of PLMs with domain knowledge, consisting\nof entities and their relational facts. We validate the performance of our KALA\non question answering and named entity recognition tasks on multiple datasets\nacross various domains. The results show that, despite being computationally\nefficient, our KALA largely outperforms adaptive pre-training. Code is\navailable at: https://github.com/Nardien/KALA/.", "published": "2022-04-22 08:11:59", "link": "http://arxiv.org/abs/2204.10555v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Out-of-Domain Evaluation of Finnish Dependency Parsing", "abstract": "The prevailing practice in the academia is to evaluate the model performance\non in-domain evaluation data typically set aside from the training corpus.\nHowever, in many real world applications the data on which the model is applied\nmay very substantially differ from the characteristics of the training data. In\nthis paper, we focus on Finnish out-of-domain parsing by introducing a novel UD\nFinnish-OOD out-of-domain treebank including five very distinct data sources\n(web documents, clinical, online discussions, tweets, and poetry), and a total\nof 19,382 syntactic words in 2,122 sentences released under the Universal\nDependencies framework. Together with the new treebank, we present extensive\nout-of-domain parsing evaluation utilizing the available section-level\ninformation from three different Finnish UD treebanks (TDT, PUD, OOD). Compared\nto the previously existing treebanks, the new Finnish-OOD is shown include\nsections more challenging for the general parser, creating an interesting\nevaluation setting and yielding valuable information for those applying the\nparser outside of its training domain.", "published": "2022-04-22 10:34:19", "link": "http://arxiv.org/abs/2204.10621v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Persona-Guided Planning for Controlling the Protagonist's Persona in\n  Story Generation", "abstract": "Endowing the protagonist with a specific personality is essential for writing\nan engaging story. In this paper, we aim to control the protagonist's persona\nin story generation, i.e., generating a story from a leading context and a\npersona description, where the protagonist should exhibit the specified\npersonality through a coherent event sequence. Considering that personas are\nusually embodied implicitly and sparsely in stories, we propose a\nplanning-based generation model named CONPER to explicitly model the\nrelationship between personas and events. CONPER first plans events of the\nprotagonist's behavior which are motivated by the specified persona through\npredicting one target sentence, then plans the plot as a sequence of keywords\nwith the guidance of the predicted persona-related events and commonsense\nknowledge, and finally generates the whole story. Both automatic and manual\nevaluation results demonstrate that CONPER outperforms state-of-the-art\nbaselines for generating more coherent and persona-controllable stories.", "published": "2022-04-22 13:45:02", "link": "http://arxiv.org/abs/2204.10703v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Chinese Opinion Expressions with Extremely-Noisy\n  Crowdsourcing Annotations", "abstract": "Recent works of opinion expression identification (OEI) rely heavily on the\nquality and scale of the manually-constructed training corpus, which could be\nextremely difficult to satisfy. Crowdsourcing is one practical solution for\nthis problem, aiming to create a large-scale but quality-unguaranteed corpus.\nIn this work, we investigate Chinese OEI with extremely-noisy crowdsourcing\nannotations, constructing a dataset at a very low cost. Following zhang et al.\n(2021), we train the annotator-adapter model by regarding all annotations as\ngold-standard in terms of crowd annotators, and test the model by using a\nsynthetic expert, which is a mixture of all annotators. As this\nannotator-mixture for testing is never modeled explicitly in the training\nphase, we propose to generate synthetic training samples by a pertinent mixup\nstrategy to make the training and testing highly consistent. The simulation\nexperiments on our constructed dataset show that crowdsourcing is highly\npromising for OEI, and our proposed annotator-mixup can further enhance the\ncrowdsourcing modeling.", "published": "2022-04-22 14:08:38", "link": "http://arxiv.org/abs/2204.10714v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Summary of the ALQAC 2021 Competition", "abstract": "We summarize the evaluation of the first Automated Legal Question Answering\nCompetition (ALQAC 2021). The competition this year contains three tasks, which\naims at processing the statute law document, which are Legal Text Information\nRetrieval (Task 1), Legal Text Entailment Prediction (Task 2), and Legal Text\nQuestion Answering (Task 3). The final goal of these tasks is to build a system\nthat can automatically determine whether a particular statement is lawful.\nThere is no limit to the approaches of the participating teams. This year,\nthere are 5 teams participating in Task 1, 6 teams participating in Task 2, and\n5 teams participating in Task 3. There are in total 36 runs submitted to the\norganizer. In this paper, we summarize each team's approaches, official\nresults, and some discussion about the competition. Only results of the teams\nwho successfully submit their approach description paper are reported in this\npaper.", "published": "2022-04-22 14:13:38", "link": "http://arxiv.org/abs/2204.10717v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FaithDial: A Faithful Benchmark for Information-Seeking Dialogue", "abstract": "The goal of information-seeking dialogue is to respond to seeker queries with\nnatural language utterances that are grounded on knowledge sources. However,\ndialogue systems often produce unsupported utterances, a phenomenon known as\nhallucination. To mitigate this behavior, we adopt a data-centric solution and\ncreate FaithDial, a new benchmark for hallucination-free dialogues, by editing\nhallucinated responses in the Wizard of Wikipedia (WoW) benchmark. We observe\nthat FaithDial is more faithful than WoW while also maintaining engaging\nconversations. We show that FaithDial can serve as training signal for: i) a\nhallucination critic, which discriminates whether an utterance is faithful or\nnot, and boosts the performance by 12.8 F1 score on the BEGIN benchmark\ncompared to existing datasets for dialogue coherence; ii) high-quality dialogue\ngeneration. We benchmark a series of state-of-the-art models and propose an\nauxiliary contrastive objective that achieves the highest level of faithfulness\nand abstractiveness based on several automated metrics. Further, we find that\nthe benefits of FaithDial generalize to zero-shot transfer on other datasets,\nsuch as CMU-Dog and TopicalChat. Finally, human evaluation reveals that\nresponses generated by models trained on FaithDial are perceived as more\ninterpretable, cooperative, and engaging.", "published": "2022-04-22 15:25:12", "link": "http://arxiv.org/abs/2204.10757v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revise and Resubmit: An Intertextual Model of Text-based Collaboration\n  in Peer Review", "abstract": "Peer review is a key component of the publishing process in most fields of\nscience. The increasing submission rates put a strain on reviewing quality and\nefficiency, motivating the development of applications to support the reviewing\nand editorial work. While existing NLP studies focus on the analysis of\nindividual texts, editorial assistance often requires modeling interactions\nbetween pairs of texts -- yet general frameworks and datasets to support this\nscenario are missing. Relationships between texts are the core object of the\nintertextuality theory -- a family of approaches in literary studies not yet\noperationalized in NLP. Inspired by prior theoretical work, we propose the\nfirst intertextual model of text-based collaboration, which encompasses three\nmajor phenomena that make up a full iteration of the review-revise-and-resubmit\ncycle: pragmatic tagging, linking and long-document version alignment. While\npeer review is used across the fields of science and publication formats,\nexisting datasets solely focus on conference-style review in computer science.\nAddressing this, we instantiate our proposed model in the first annotated\nmulti-domain corpus in journal-style post-publication open peer review, and\nprovide detailed insights into the practical aspects of intertextual\nannotation. Our resource is a major step towards multi-domain, fine-grained\napplications of NLP in editorial support for peer review, and our intertextual\nframework paves the path for general-purpose modeling of text-based\ncollaboration. Our corpus and accompanying code are publicly available.", "published": "2022-04-22 16:39:38", "link": "http://arxiv.org/abs/2204.10805v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meet Your Favorite Character: Open-domain Chatbot Mimicking Fictional\n  Characters with only a Few Utterances", "abstract": "In this paper, we consider mimicking fictional characters as a promising\ndirection for building engaging conversation models. To this end, we present a\nnew practical task where only a few utterances of each fictional character are\navailable to generate responses mimicking them. Furthermore, we propose a new\nmethod named Pseudo Dialog Prompting (PDP) that generates responses by\nleveraging the power of large-scale language models with prompts containing the\ntarget character's utterances. To better reflect the style of the character,\nPDP builds the prompts in the form of dialog that includes the character's\nutterances as dialog history. Since only utterances of the characters are\navailable in the proposed task, PDP matches each utterance with an appropriate\npseudo-context from a predefined set of context candidates using a retrieval\nmodel. Through human and automatic evaluation, we show that PDP generates\nresponses that better reflect the style of fictional characters than baseline\nmethods.", "published": "2022-04-22 17:11:17", "link": "http://arxiv.org/abs/2204.10825v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Metric Learning and Adaptive Boundary for Out-of-Domain Detection", "abstract": "Conversational agents are usually designed for closed-world environments.\nUnfortunately, users can behave unexpectedly. Based on the open-world\nenvironment, we often encounter the situation that the training and test data\nare sampled from different distributions. Then, data from different\ndistributions are called out-of-domain (OOD). A robust conversational agent\nneeds to react to these OOD utterances adequately. Thus, the importance of\nrobust OOD detection is emphasized. Unfortunately, collecting OOD data is a\nchallenging task. We have designed an OOD detection algorithm independent of\nOOD data that outperforms a wide range of current state-of-the-art algorithms\non publicly available datasets. Our algorithm is based on a simple but\nefficient approach of combining metric learning with adaptive decision\nboundary. Furthermore, compared to other algorithms, we have found that our\nproposed algorithm has significantly improved OOD performance in a scenario\nwith a lower number of classes while preserving the accuracy for in-domain\n(IND) classes.", "published": "2022-04-22 17:54:55", "link": "http://arxiv.org/abs/2204.10849v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChapterBreak: A Challenge Dataset for Long-Range Language Models", "abstract": "While numerous architectures for long-range language models (LRLMs) have\nrecently been proposed, a meaningful evaluation of their discourse-level\nlanguage understanding capabilities has not yet followed. To this end, we\nintroduce ChapterBreak, a challenge dataset that provides an LRLM with a long\nsegment from a narrative that ends at a chapter boundary and asks it to\ndistinguish the beginning of the ground-truth next chapter from a set of\nnegative segments sampled from the same narrative. A fine-grained human\nannotation reveals that our dataset contains many complex types of chapter\ntransitions (e.g., parallel narratives, cliffhanger endings) that require\nprocessing global context to comprehend. Experiments on ChapterBreak show that\nexisting LRLMs fail to effectively leverage long-range context, substantially\nunderperforming a segment-level model trained directly for this task. We\npublicly release our ChapterBreak dataset to spur more principled future\nresearch into LRLMs.", "published": "2022-04-22 18:20:23", "link": "http://arxiv.org/abs/2204.10878v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MCSE: Multimodal Contrastive Learning of Sentence Embeddings", "abstract": "Learning semantically meaningful sentence embeddings is an open problem in\nnatural language processing. In this work, we propose a sentence embedding\nlearning approach that exploits both visual and textual information via a\nmultimodal contrastive objective. Through experiments on a variety of semantic\ntextual similarity tasks, we demonstrate that our approach consistently\nimproves the performance across various datasets and pre-trained encoders. In\nparticular, combining a small amount of multimodal data with a large text-only\ncorpus, we improve the state-of-the-art average Spearman's correlation by 1.7%.\nBy analyzing the properties of the textual embedding space, we show that our\nmodel excels in aligning semantically similar sentences, providing an\nexplanation for its improved performance.", "published": "2022-04-22 21:19:24", "link": "http://arxiv.org/abs/2204.10931v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Contrastive Clustering: Fully Unsupervised Bias Reduction for\n  Sentiment Classification", "abstract": "Background: Neural networks produce biased classification results due to\ncorrelation bias (they learn correlations between their inputs and outputs to\nclassify samples, even when those correlations do not represent\ncause-and-effect relationships).\n  Objective: This study introduces a fully unsupervised method of mitigating\ncorrelation bias, demonstrated with sentiment classification on COVID-19 social\nmedia data.\n  Methods: Correlation bias in sentiment classification often arises in\nconversations about controversial topics. Therefore, this study uses\nadversarial learning to contrast clusters based on sentiment classification\nlabels, with clusters produced by unsupervised topic modeling. This discourages\nthe neural network from learning topic-related features that produce biased\nclassification results.\n  Results: Compared to a baseline classifier, neural contrastive clustering\napproximately doubles accuracy on bias-prone sentences for human-labeled\nCOVID-19 social media data, without adversely affecting the classifier's\noverall F1 score. Despite being a fully unsupervised approach, neural\ncontrastive clustering achieves a larger improvement in accuracy on bias-prone\nsentences than a supervised masking approach.\n  Conclusions: Neural contrastive clustering reduces correlation bias in\nsentiment text classification. Further research is needed to explore\ngeneralizing this technique to other neural network architectures and\napplication domains.", "published": "2022-04-22 02:34:41", "link": "http://arxiv.org/abs/2204.10467v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NLP Based Anomaly Detection for Categorical Time Series", "abstract": "Identifying anomalies in large multi-dimensional time series is a crucial and\ndifficult task across multiple domains. Few methods exist in the literature\nthat address this task when some of the variables are categorical in nature. We\nformalize an analogy between categorical time series and classical Natural\nLanguage Processing and demonstrate the strength of this analogy for anomaly\ndetection and root cause investigation by implementing and testing three\ndifferent machine learning anomaly detection and root cause investigation\nmodels based upon it.", "published": "2022-04-22 03:53:51", "link": "http://arxiv.org/abs/2204.10483v1", "categories": ["cs.LG", "cs.CL", "68T07 (Primary) 68T50 (Secondary)", "I.2.6; I.2.7"], "primary_category": "cs.LG"}
{"title": "Taygete at SemEval-2022 Task 4: RoBERTa based models for detecting\n  Patronising and Condescending Language", "abstract": "This work describes the development of different models to detect patronising\nand condescending language within extracts of news articles as part of the\nSemEval 2022 competition (Task-4). This work explores different models based on\nthe pre-trained RoBERTa language model coupled with LSTM and CNN layers. The\nbest models achieved 15$^{th}$ rank with an F1-score of 0.5924 for subtask-A\nand 12$^{th}$ in subtask-B with a macro-F1 score of 0.3763.", "published": "2022-04-22 06:11:47", "link": "http://arxiv.org/abs/2204.10519v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SalesBot: Transitioning from Chit-Chat to Task-Oriented Dialogues", "abstract": "Dialogue systems are usually categorized into two types, open-domain and\ntask-oriented. The first one focuses on chatting with users and making them\nengage in the conversations, where selecting a proper topic to fit the dialogue\ncontext is essential for a successful dialogue. The other one focuses on a\nspecific task instead of casual talks, e.g., finding a movie on Friday night,\nor playing a song. These two directions have been studied separately due to\ntheir different purposes. However, how smoothly transitioning from social\nchatting to task-oriented dialogues is important for triggering business\nopportunities, and there is no public data focusing on such scenarios. Hence,\nthis paper focuses on investigating the conversations starting from open-domain\nsocial chatting and then gradually transitioning to task-oriented purposes, and\nreleases a large-scale dataset with detailed annotations for encouraging this\nresearch direction. To achieve this goal, this paper proposes a framework to\nautomatically generate many dialogues without human involvement, in which any\npowerful open-domain dialogue generation model can be easily leveraged. The\nhuman evaluation shows that our generated dialogue data has a natural flow at a\nreasonable quality, showing that our released data has a great potential of\nguiding future research directions and commercial activities. Furthermore, the\nreleased models allow researchers to automatically generate unlimited dialogues\nin the target scenarios, which can greatly benefit semi-supervised and\nunsupervised approaches.", "published": "2022-04-22 09:31:13", "link": "http://arxiv.org/abs/2204.10591v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generalized Quantifiers as a Source of Error in Multilingual NLU\n  Benchmarks", "abstract": "Logical approaches to representing language have developed and evaluated\ncomputational models of quantifier words since the 19th century, but today's\nNLU models still struggle to capture their semantics. We rely on Generalized\nQuantifier Theory for language-independent representations of the semantics of\nquantifier words, to quantify their contribution to the errors of NLU models.\nWe find that quantifiers are pervasive in NLU benchmarks, and their occurrence\nat test time is associated with performance drops. Multilingual models also\nexhibit unsatisfying quantifier reasoning abilities, but not necessarily worse\nfor non-English languages. To facilitate directly-targeted probing, we present\nan adversarial generalized quantifier NLI task (GQNLI) and show that\npre-trained language models have a clear lack of robustness in generalized\nquantifier reasoning.", "published": "2022-04-22 10:21:46", "link": "http://arxiv.org/abs/2204.10615v2", "categories": ["cs.CL", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Learning Functional Distributional Semantics with Visual Data", "abstract": "Functional Distributional Semantics is a recently proposed framework for\nlearning distributional semantics that provides linguistic interpretability. It\nmodels the meaning of a word as a binary classifier rather than a numerical\nvector. In this work, we propose a method to train a Functional Distributional\nSemantics model with grounded visual data. We train it on the Visual Genome\ndataset, which is closer to the kind of data encountered in human language\nacquisition than a large text corpus. On four external evaluation datasets, our\nmodel outperforms previous work on learning semantics from Visual Genome.", "published": "2022-04-22 10:41:39", "link": "http://arxiv.org/abs/2204.10624v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Autoregressive Search Engines: Generating Substrings as Document\n  Identifiers", "abstract": "Knowledge-intensive language tasks require NLP systems to both provide the\ncorrect answer and retrieve supporting evidence for it in a given corpus.\nAutoregressive language models are emerging as the de-facto standard for\ngenerating answers, with newer and more powerful systems emerging at an\nastonishing pace. In this paper we argue that all this (and future) progress\ncan be directly applied to the retrieval problem with minimal intervention to\nthe models' architecture. Previous work has explored ways to partition the\nsearch space into hierarchical structures and retrieve documents by\nautoregressively generating their unique identifier. In this work we propose an\nalternative that doesn't force any structure in the search space: using all\nngrams in a passage as its possible identifiers. This setup allows us to use an\nautoregressive model to generate and score distinctive ngrams, that are then\nmapped to full passages through an efficient data structure. Empirically, we\nshow this not only outperforms prior autoregressive approaches but also leads\nto an average improvement of at least 10 points over more established retrieval\nsolutions for passage-level retrieval on the KILT benchmark, establishing new\nstate-of-the-art downstream performance on some datasets, while using a\nconsiderably lighter memory footprint than competing systems. Code and\npre-trained models at https://github.com/facebookresearch/SEAL.", "published": "2022-04-22 10:45:01", "link": "http://arxiv.org/abs/2204.10628v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Hierarchical Label-wise Attention Transformer Model for Explainable ICD\n  Coding", "abstract": "International Classification of Diseases (ICD) coding plays an important role\nin systematically classifying morbidity and mortality data. In this study, we\npropose a hierarchical label-wise attention Transformer model (HiLAT) for the\nexplainable prediction of ICD codes from clinical documents. HiLAT firstly\nfine-tunes a pretrained Transformer model to represent the tokens of clinical\ndocuments. We subsequently employ a two-level hierarchical label-wise attention\nmechanism that creates label-specific document representations. These\nrepresentations are in turn used by a feed-forward neural network to predict\nwhether a specific ICD code is assigned to the input clinical document of\ninterest. We evaluate HiLAT using hospital discharge summaries and their\ncorresponding ICD-9 codes from the MIMIC-III database. To investigate the\nperformance of different types of Transformer models, we develop\nClinicalplusXLNet, which conducts continual pretraining from XLNet-Base using\nall the MIMIC-III clinical notes. The experiment results show that the F1\nscores of the HiLAT+ClinicalplusXLNet outperform the previous state-of-the-art\nmodels for the top-50 most frequent ICD-9 codes from MIMIC-III. Visualisations\nof attention weights present a potential explainability tool for checking the\nface validity of ICD code predictions.", "published": "2022-04-22 14:12:22", "link": "http://arxiv.org/abs/2204.10716v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task\n  Learning", "abstract": "Subword tokenization is a commonly used input pre-processing step in most\nrecent NLP models. However, it limits the models' ability to leverage\nend-to-end task learning. Its frequency-based vocabulary creation compromises\ntokenization in low-resource languages, leading models to produce suboptimal\nrepresentations. Additionally, the dependency on a fixed vocabulary limits the\nsubword models' adaptability across languages and domains. In this work, we\npropose a vocabulary-free neural tokenizer by distilling segmentation\ninformation from heuristic-based subword tokenization. We pre-train our\ncharacter-based tokenizer by processing unique words from multilingual corpus,\nthereby extensively increasing word diversity across languages. Unlike the\npredefined and fixed vocabularies in subword methods, our tokenizer allows\nend-to-end task learning, resulting in optimal task-specific tokenization. The\nexperimental results show that replacing the subword tokenizer with our neural\ntokenizer consistently improves performance on multilingual (NLI) and\ncode-switching (sentiment analysis) tasks, with larger gains in low-resource\nlanguages. Additionally, our neural tokenizer exhibits a robust performance on\ndownstream tasks when adversarial noise is present (typos and misspelling),\nfurther increasing the initial improvements over statistical subword\ntokenizers.", "published": "2022-04-22 16:50:49", "link": "http://arxiv.org/abs/2204.10815v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Detecting early signs of depression in the conversational domain: The\n  role of transfer learning in low-resource scenarios", "abstract": "The high prevalence of depression in society has given rise to the need for\nnew digital tools to assist in its early detection. To this end, existing\nresearch has mainly focused on detecting depression in the domain of social\nmedia, where there is a sufficient amount of data. However, with the rise of\nconversational agents like Siri or Alexa, the conversational domain is becoming\nmore critical. Unfortunately, there is a lack of data in the conversational\ndomain. We perform a study focusing on domain adaptation from social media to\nthe conversational domain. Our approach mainly exploits the linguistic\ninformation preserved in the vector representation of text. We describe\ntransfer learning techniques to classify users who suffer from early signs of\ndepression with high recall. We achieve state-of-the-art results on a commonly\nused conversational dataset, and we highlight how the method can easily be used\nin conversational agents. We publicly release all source code.", "published": "2022-04-22 17:35:37", "link": "http://arxiv.org/abs/2204.10841v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Locally Aggregated Feature Attribution on Natural Language Model\n  Understanding", "abstract": "With the growing popularity of deep-learning models, model understanding\nbecomes more important. Much effort has been devoted to demystify deep neural\nnetworks for better interpretability. Some feature attribution methods have\nshown promising results in computer vision, especially the gradient-based\nmethods where effectively smoothing the gradients with reference data is key to\na robust and faithful result. However, direct application of these\ngradient-based methods to NLP tasks is not trivial due to the fact that the\ninput consists of discrete tokens and the \"reference\" tokens are not explicitly\ndefined. In this work, we propose Locally Aggregated Feature Attribution\n(LAFA), a novel gradient-based feature attribution method for NLP models.\nInstead of relying on obscure reference tokens, it smooths gradients by\naggregating similar reference texts derived from language model embeddings. For\nevaluation purpose, we also design experiments on different NLP tasks including\nEntity Recognition and Sentiment Analysis on public datasets as well as key\nfeature detection on a constructed Amazon catalogue dataset. The superior\nperformance of the proposed method is demonstrated through experiments.", "published": "2022-04-22 18:59:27", "link": "http://arxiv.org/abs/2204.10893v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unified Pretraining Framework for Document Understanding", "abstract": "Document intelligence automates the extraction of information from documents\nand supports many business applications. Recent self-supervised learning\nmethods on large-scale unlabeled document datasets have opened up promising\ndirections towards reducing annotation efforts by training models with\nself-supervised objectives. However, most of the existing document pretraining\nmethods are still language-dominated. We present UDoc, a new unified\npretraining framework for document understanding. UDoc is designed to support\nmost document understanding tasks, extending the Transformer to take multimodal\nembeddings as input. Each input element is composed of words and visual\nfeatures from a semantic region of the input document image. An important\nfeature of UDoc is that it learns a generic representation by making use of\nthree self-supervised losses, encouraging the representation to model\nsentences, learn similarities, and align modalities. Extensive empirical\nanalysis demonstrates that the pretraining procedure learns better joint\nrepresentations and leads to improvements in downstream tasks.", "published": "2022-04-22 21:47:04", "link": "http://arxiv.org/abs/2204.10939v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Hypergraph Transformer: Weakly-supervised Multi-hop Reasoning for\n  Knowledge-based Visual Question Answering", "abstract": "Knowledge-based visual question answering (QA) aims to answer a question\nwhich requires visually-grounded external knowledge beyond image content\nitself. Answering complex questions that require multi-hop reasoning under weak\nsupervision is considered as a challenging problem since i) no supervision is\ngiven to the reasoning process and ii) high-order semantics of multi-hop\nknowledge facts need to be captured. In this paper, we introduce a concept of\nhypergraph to encode high-level semantics of a question and a knowledge base,\nand to learn high-order associations between them. The proposed model,\nHypergraph Transformer, constructs a question hypergraph and a query-aware\nknowledge hypergraph, and infers an answer by encoding inter-associations\nbetween two hypergraphs and intra-associations in both hypergraph itself.\nExtensive experiments on two knowledge-based visual QA and two knowledge-based\ntextual QA demonstrate the effectiveness of our method, especially for\nmulti-hop reasoning problem. Our source code is available at\nhttps://github.com/yujungheo/kbvqa-public.", "published": "2022-04-22 00:49:50", "link": "http://arxiv.org/abs/2204.10448v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "WaBERT: A Low-resource End-to-end Model for Spoken Language\n  Understanding and Speech-to-BERT Alignment", "abstract": "Historically lower-level tasks such as automatic speech recognition (ASR) and\nspeaker identification are the main focus in the speech field. Interest has\nbeen growing in higher-level spoken language understanding (SLU) tasks\nrecently, like sentiment analysis (SA). However, improving performances on SLU\ntasks remains a big challenge. Basically, there are two main methods for SLU\ntasks: (1) Two-stage method, which uses a speech model to transfer speech to\ntext, then uses a language model to get the results of downstream tasks; (2)\nOne-stage method, which just fine-tunes a pre-trained speech model to fit in\nthe downstream tasks. The first method loses emotional cues such as intonation,\nand causes recognition errors during ASR process, and the second one lacks\nnecessary language knowledge. In this paper, we propose the Wave BERT (WaBERT),\na novel end-to-end model combining the speech model and the language model for\nSLU tasks. WaBERT is based on the pre-trained speech and language model, hence\ntraining from scratch is not needed. We also set most parameters of WaBERT\nfrozen during training. By introducing WaBERT, audio-specific information and\nlanguage knowledge are integrated in the short-time and low-resource training\nprocess to improve results on the dev dataset of SLUE SA tasks by 1.15% of\nrecall score and 0.82% of F1 score. Additionally, we modify the serial\nContinuous Integrate-and-Fire (CIF) mechanism to achieve the monotonic\nalignment between the speech and text modalities.", "published": "2022-04-22 02:14:40", "link": "http://arxiv.org/abs/2204.10461v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Rethinking Offensive Text Detection as a Multi-Hop Reasoning Problem", "abstract": "We introduce the task of implicit offensive text detection in dialogues,\nwhere a statement may have either an offensive or non-offensive interpretation,\ndepending on the listener and context. We argue that reasoning is crucial for\nunderstanding this broader class of offensive utterances and release SLIGHT, a\ndataset to support research on this task. Experiments using the data show that\nstate-of-the-art methods of offense detection perform poorly when asked to\ndetect implicitly offensive statements, achieving only ${\\sim} 11\\%$ accuracy.\n  In contrast to existing offensive text detection datasets, SLIGHT features\nhuman-annotated chains of reasoning which describe the mental process by which\nan offensive interpretation can be reached from each ambiguous statement. We\nexplore the potential for a multi-hop reasoning approach by utilizing existing\nentailment models to score the probability of these chains and show that even\nnaive reasoning models can yield improved performance in most situations.\nFurthermore, analysis of the chains provides insight into the human\ninterpretation process and emphasizes the importance of incorporating\nadditional commonsense knowledge.", "published": "2022-04-22 06:20:15", "link": "http://arxiv.org/abs/2204.10521v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Sparse and Dense Approaches for the Full-rank Retrieval of Responses for\n  Dialogues", "abstract": "Ranking responses for a given dialogue context is a popular benchmark in\nwhich the setup is to re-rank the ground-truth response over a limited set of\n$n$ responses, where $n$ is typically 10. The predominance of this setup in\nconversation response ranking has lead to a great deal of attention to building\nneural re-rankers, while the first-stage retrieval step has been overlooked.\nSince the correct answer is always available in the candidate list of $n$\nresponses, this artificial evaluation setup assumes that there is a first-stage\nretrieval step which is always able to rank the correct response in its top-$n$\nlist. In this paper we focus on the more realistic task of full-rank retrieval\nof responses, where $n$ can be up to millions of responses. We investigate both\ndialogue context and response expansion techniques for sparse retrieval, as\nwell as zero-shot and fine-tuned dense retrieval approaches. Our findings based\non three different information-seeking dialogue datasets reveal that a learned\nresponse expansion technique is a solid baseline for sparse retrieval. We find\nthe best performing method overall to be dense retrieval with intermediate\ntraining, i.e. a step after the language model pre-training where sentence\nrepresentations are learned, followed by fine-tuning on the target\nconversational data. We also investigate the intriguing phenomena that harder\nnegatives sampling techniques lead to worse results for the fine-tuned dense\nretrieval models. The code and datasets are available at\nhttps://github.com/Guzpenha/transformer_rankers/tree/full_rank_retrieval_dialogues.", "published": "2022-04-22 08:15:15", "link": "http://arxiv.org/abs/2204.10558v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Efficient Training of Neural Transducer for Speech Recognition", "abstract": "As one of the most popular sequence-to-sequence modeling approaches for\nspeech recognition, the RNN-Transducer has achieved evolving performance with\nmore and more sophisticated neural network models of growing size and\nincreasing training epochs. While strong computation resources seem to be the\nprerequisite of training superior models, we try to overcome it by carefully\ndesigning a more efficient training pipeline. In this work, we propose an\nefficient 3-stage progressive training pipeline to build highly-performing\nneural transducer models from scratch with very limited computation resources\nin a reasonable short time period. The effectiveness of each stage is\nexperimentally verified on both Librispeech and Switchboard corpora. The\nproposed pipeline is able to train transducer models approaching\nstate-of-the-art performance with a single GPU in just 2-3 weeks. Our best\nconformer transducer achieves 4.1% WER on Librispeech test-other with only 35\nepochs of training.", "published": "2022-04-22 09:16:51", "link": "http://arxiv.org/abs/2204.10586v4", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Emergent Communication for Understanding Human Language Evolution:\n  What's Missing?", "abstract": "Emergent communication protocols among humans and artificial neural network\nagents do not yet share the same properties and show some critical mismatches\nin results. We describe three important phenomena with respect to the emergence\nand benefits of compositionality: ease-of-learning, generalization, and group\nsize effects (i.e., larger groups create more systematic languages). The latter\ntwo are not fully replicated with neural agents, which hinders the use of\nneural emergent communication for language evolution research. We argue that\none possible reason for these mismatches is that key cognitive and\ncommunicative constraints of humans are not yet integrated. Specifically, in\nhumans, memory constraints and the alternation between the roles of speaker and\nlistener underlie the emergence of linguistic structure, yet these constraints\nare typically absent in neural simulations. We suggest that introducing such\ncommunicative and cognitive constraints would promote more linguistically\nplausible behaviors with neural agents.", "published": "2022-04-22 09:21:53", "link": "http://arxiv.org/abs/2204.10590v1", "categories": ["cs.CL", "cs.LG", "cs.NE", "I.2.7"], "primary_category": "cs.CL"}
{"title": "LibriS2S: A German-English Speech-to-Speech Translation Corpus", "abstract": "Recently, we have seen an increasing interest in the area of speech-to-text\ntranslation. This has led to astonishing improvements in this area. In\ncontrast, the activities in the area of speech-to-speech translation is still\nlimited, although it is essential to overcome the language barrier. We believe\nthat one of the limiting factors is the availability of appropriate training\ndata. We address this issue by creating LibriS2S, to our knowledge the first\npublicly available speech-to-speech training corpus between German and English.\nFor this corpus, we used independently created audio for German and English\nleading to an unbiased pronunciation of the text in both languages. This allows\nthe creation of a new text-to-speech and speech-to-speech translation model\nthat directly learns to generate the speech signal based on the pronunciation\nof the source language. Using this created corpus, we propose Text-to-Speech\nmodels based on the example of the recently proposed FastSpeech 2 model that\nintegrates source language information. We do this by adapting the model to\ntake information such as the pitch, energy or transcript from the source speech\nas additional input.", "published": "2022-04-22 09:33:31", "link": "http://arxiv.org/abs/2204.10593v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MEKER: Memory Efficient Knowledge Embedding Representation for Link\n  Prediction and Question Answering", "abstract": "Knowledge Graphs (KGs) are symbolically structured storages of facts. The KG\nembedding contains concise data used in NLP tasks requiring implicit\ninformation about the real world. Furthermore, the size of KGs that may be\nuseful in actual NLP assignments is enormous, and creating embedding over it\nhas memory cost issues. We represent KG as a 3rd-order binary tensor and move\nbeyond the standard CP decomposition by using a data-specific generalized\nversion of it. The generalization of the standard CP-ALS algorithm allows\nobtaining optimization gradients without a backpropagation mechanism. It\nreduces the memory needed in training while providing computational benefits.\nWe propose a MEKER, a memory-efficient KG embedding model, which yields\nSOTA-comparable performance on link prediction tasks and KG-based Question\nAnswering.", "published": "2022-04-22 10:47:03", "link": "http://arxiv.org/abs/2204.10629v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pathways through Conspiracy: The Evolution of Conspiracy Radicalization\n  through Engagement in Online Conspiracy Discussions", "abstract": "The disruptive offline mobilization of participants in online conspiracy\ntheory (CT) discussions has highlighted the importance of understanding how\nonline users may form radicalized conspiracy beliefs. While prior work\nresearched the factors leading up to joining online CT discussions and provided\ntheories of how conspiracy beliefs form, we have little understanding of how\nconspiracy radicalization evolves after users join CT discussion communities.\nIn this paper, we provide the empirical modeling of various radicalization\nphases in online CT discussion participants. To unpack how conspiracy\nengagement is related to radicalization, we first characterize the users'\njourney through CT discussions via conspiracy engagement pathways.\nSpecifically, by studying 36K Reddit users through their 169M contributions, we\nuncover four distinct pathways of conspiracy engagement: steady high,\nincreasing, decreasing, and steady low. We further model three successive\nstages of radicalization guided by prior theoretical works. Specific\nsub-populations of users, namely those on steady high and increasing conspiracy\nengagement pathways, progress successively through various radicalization\nstages. In contrast, users on the decreasing engagement pathway show distinct\nbehavior: they limit their CT discussions to specialized topics, participate in\ndiverse discussion groups, and show reduced conformity with conspiracy\nsubreddits. By examining users who disengage from online CT discussions, this\npaper provides promising insights about conspiracy recovery process.", "published": "2022-04-22 14:31:53", "link": "http://arxiv.org/abs/2204.10729v1", "categories": ["cs.CY", "cs.CL", "cs.SI"], "primary_category": "cs.CY"}
{"title": "E2E Segmenter: Joint Segmenting and Decoding for Long-Form ASR", "abstract": "Improving the performance of end-to-end ASR models on long utterances ranging\nfrom minutes to hours in length is an ongoing challenge in speech recognition.\nA common solution is to segment the audio in advance using a separate voice\nactivity detector (VAD) that decides segment boundary locations based purely on\nacoustic speech/non-speech information. VAD segmenters, however, may be\nsub-optimal for real-world speech where, e.g., a complete sentence that should\nbe taken as a whole may contain hesitations in the middle (\"set an alarm for...\n5 o'clock\").\n  We propose to replace the VAD with an end-to-end ASR model capable of\npredicting segment boundaries in a streaming fashion, allowing the segmentation\ndecision to be conditioned not only on better acoustic features but also on\nsemantic features from the decoded text with negligible extra computation. In\nexperiments on real world long-form audio (YouTube) with lengths of up to 30\nminutes, we demonstrate 8.5% relative WER improvement and 250 ms reduction in\nmedian end-of-segment latency compared to the VAD segmenter baseline on a\nstate-of-the-art Conformer RNN-T model.", "published": "2022-04-22 15:13:12", "link": "http://arxiv.org/abs/2204.10749v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning to Scaffold: Optimizing Model Explanations for Teaching", "abstract": "Modern machine learning models are opaque, and as a result there is a\nburgeoning academic subfield on methods that explain these models' behavior.\nHowever, what is the precise goal of providing such explanations, and how can\nwe demonstrate that explanations achieve this goal? Some research argues that\nexplanations should help teach a student (either human or machine) to simulate\nthe model being explained, and that the quality of explanations can be measured\nby the simulation accuracy of students on unexplained examples. In this work,\nleveraging meta-learning techniques, we extend this idea to improve the quality\nof the explanations themselves, specifically by optimizing explanations such\nthat student models more effectively learn to simulate the original model. We\ntrain models on three natural language processing and computer vision tasks,\nand find that students trained with explanations extracted with our framework\nare able to simulate the teacher significantly more effectively than ones\nproduced with previous methods. Through human annotations and a user study, we\nfurther find that these learned explanations more closely align with how humans\nwould explain the required decisions in these tasks. Our code is available at\nhttps://github.com/coderpat/learning-scaffold", "published": "2022-04-22 16:43:39", "link": "http://arxiv.org/abs/2204.10810v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "A Multi-level Alignment Training Scheme for Video-and-Language Grounding", "abstract": "To solve video-and-language grounding tasks, the key is for the network to\nunderstand the connection between the two modalities. For a pair of video and\nlanguage description, their semantic relation is reflected by their encodings'\nsimilarity. A good multi-modality encoder should be able to well capture both\ninputs' semantics and encode them in the shared feature space where embedding\ndistance gets properly translated into their semantic similarity. In this work,\nwe focused on this semantic connection between video and language, and\ndeveloped a multi-level alignment training scheme to directly shape the\nencoding process. Global and segment levels of video-language alignment pairs\nwere designed, based on the information similarity ranging from high-level\ncontext to fine-grained semantics. The contrastive loss was used to contrast\nthe encodings' similarities between the positive and negative alignment pairs,\nand to ensure the network is trained in such a way that similar information is\nencoded closely in the shared feature space while information of different\nsemantics is kept apart. Our multi-level alignment training can be applied to\nvarious video-and-language grounding tasks. Together with the task-specific\ntraining loss, our framework achieved comparable performance to previous\nstate-of-the-arts on multiple video QA and retrieval datasets.", "published": "2022-04-22 21:46:52", "link": "http://arxiv.org/abs/2204.10938v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Data Distributional Properties Drive Emergent In-Context Learning in\n  Transformers", "abstract": "Large transformer-based models are able to perform in-context few-shot\nlearning, without being explicitly trained for it. This observation raises the\nquestion: what aspects of the training regime lead to this emergent behavior?\nHere, we show that this behavior is driven by the distributions of the training\ndata itself. In-context learning emerges when the training data exhibits\nparticular distributional properties such as burstiness (items appear in\nclusters rather than being uniformly distributed over time) and having large\nnumbers of rarely occurring classes. In-context learning also emerges more\nstrongly when item meanings or interpretations are dynamic rather than fixed.\nThese properties are exemplified by natural language, but are also inherent to\nnaturalistic data in a wide range of other domains. They also depart\nsignificantly from the uniform, i.i.d. training distributions typically used\nfor standard supervised learning. In our initial experiments, we found that\nin-context learning traded off against more conventional weight-based learning,\nand models were unable to achieve both simultaneously. However, our later\nexperiments uncovered that the two modes of learning could co-exist in a single\nmodel when it was trained on data following a skewed Zipfian distribution --\nanother common property of naturalistic data, including language. In further\nexperiments, we found that naturalistic data distributions were only able to\nelicit in-context learning in transformers, and not in recurrent models. In\nsum, our findings indicate how the transformer architecture works together with\nparticular properties of the training data to drive the intriguing emergent\nin-context learning behaviour of large language models, and how future work\nmight encourage both in-context and in-weights learning in domains beyond\nlanguage.", "published": "2022-04-22 16:10:50", "link": "http://arxiv.org/abs/2205.05055v6", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multimodal Adaptive Distillation for Leveraging Unimodal Encoders for\n  Vision-Language Tasks", "abstract": "Cross-modal encoders for vision-language (VL) tasks are often pretrained with\ncarefully curated vision-language datasets. While these datasets reach an order\nof 10 million samples, the labor cost is prohibitive to scale further.\nConversely, unimodal encoders are pretrained with simpler annotations that are\nless cost-prohibitive, achieving scales of hundreds of millions to billions. As\na result, unimodal encoders have achieved state-of-art (SOTA) on many\ndownstream tasks. However, challenges remain when applying to VL tasks. The\npretraining data is not optimal for cross-modal architectures and requires\nheavy computational resources. In addition, unimodal architectures lack\ncross-modal interactions that have demonstrated significant benefits for VL\ntasks. Therefore, how to best leverage pretrained unimodal encoders for VL\ntasks is still an area of active research. In this work, we propose a method to\nleverage unimodal vision and text encoders for VL tasks that augment existing\nVL approaches while conserving computational complexity. Specifically, we\npropose Multimodal Adaptive Distillation (MAD), which adaptively distills\nuseful knowledge from pretrained encoders to cross-modal VL encoders. Second,\nto better capture nuanced impacts on VL task performance, we introduce an\nevaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual\nEntailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of\ndata constraints and conditions of domain shift. Experiments demonstrate that\nMAD leads to consistent gains in the low-shot, domain-shifted, and\nfully-supervised conditions on VCR, SNLI-VE, and VQA, achieving SOTA\nperformance on VCR compared to other single models pretrained with image-text\ndata. Finally, MAD outperforms concurrent works utilizing pretrained vision\nencoder from CLIP. Code will be made available.", "published": "2022-04-22 04:41:04", "link": "http://arxiv.org/abs/2204.10496v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Unifying Cosine and PLDA Back-ends for Speaker Verification", "abstract": "State-of-art speaker verification (SV) systems use a back-end model to score\nthe similarity of speaker embeddings extracted from a neural network model. The\ncommonly used back-end models are the cosine scoring and the probabilistic\nlinear discriminant analysis (PLDA) scoring. With the recently developed neural\nembeddings, the theoretically more appealing PLDA approach is found to have no\nadvantage against or even be inferior the simple cosine scoring in terms of SV\nsystem performance. This paper presents an investigation on the relation\nbetween the two scoring approaches, aiming to explain the above\ncounter-intuitive observation. It is shown that the cosine scoring is\nessentially a special case of PLDA scoring. In other words, by properly setting\nthe parameters of PLDA, the two back-ends become equivalent. As a consequence,\nthe cosine scoring not only inherits the basic assumptions for the PLDA but\nalso introduces additional assumptions on the properties of input embeddings.\nExperiments show that the dimensional independence assumption required by the\ncosine scoring contributes most to the performance gap between the two methods\nunder the domain-matched condition. When there is severe domain mismatch and\nthe dimensional independence assumption does not hold, the PLDA would perform\nbetter than the cosine for domain adaptation.", "published": "2022-04-22 06:25:44", "link": "http://arxiv.org/abs/2204.10523v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speaking-Rate-Controllable HiFi-GAN Using Feature Interpolation", "abstract": "This paper presents a speaking-rate-controllable HiFi-GAN neural vocoder.\nOriginal HiFi-GAN is a high-fidelity, computationally efficient, and\ntiny-footprint neural vocoder. We attempt to incorporate a speaking rate\ncontrol function into HiFi-GAN for improving the accessibility of synthetic\nspeech. The proposed method inserts a differentiable interpolation layer into\nthe HiFi-GAN architecture. A signal resampling method and an image scaling\nmethod are implemented in the proposed method to warp the mel-spectrograms or\nhidden features of the neural vocoder. We also design and open-source a\nJapanese speech corpus containing three kinds of speaking rates to evaluate the\nproposed speaking rate control method. Experimental results of comprehensive\nobjective and subjective evaluations demonstrate that 1) the proposed method\noutperforms a baseline time-scale modification algorithm in speech naturalness,\n2) warping mel-spectrograms by image scaling obtained the best performance\namong all proposed methods, and 3) the proposed speaking rate control method\ncan be incorporated into HiFi-GAN without losing computational efficiency.", "published": "2022-04-22 08:18:08", "link": "http://arxiv.org/abs/2204.10561v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fused Audio Instance and Representation for Respiratory Disease\n  Detection", "abstract": "Audio-based classification techniques on body sounds have long been studied\nto aid in the diagnosis of respiratory diseases. While most research is\ncentered on the use of cough as the main biomarker, other body sounds also have\nthe potential to detect respiratory diseases. Recent studies on COVID-19 have\nshown that breath and speech sounds, in addition to cough, correlate with the\ndisease. Our study proposes Fused Audio Instance and Representation (FAIR) as a\nmethod for respiratory disease detection. FAIR relies on constructing a joint\nfeature vector from various body sounds represented in waveform and spectrogram\nform. We conducted experiments on the use case of COVID-19 detection by\ncombining waveform and spectrogram representation of body sounds. Our findings\nshow that the use of self-attention to combine extracted features from cough,\nbreath, and speech sounds leads to the best performance with an Area Under the\nReceiver Operating Characteristic Curve (AUC) score of 0.8658, a sensitivity of\n0.8057, and a specificity of 0.7958. Compared to models trained solely on\nspectrograms or waveforms, the use of both representations results in an\nimproved AUC score, demonstrating that combining spectrogram and waveform\nrepresentation helps to enrich the extracted features and outperforms the\nmodels that use only one representation.", "published": "2022-04-22 09:01:29", "link": "http://arxiv.org/abs/2204.10581v4", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
