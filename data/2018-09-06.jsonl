{"title": "Top-down Tree Structured Decoding with Syntactic Connections for Neural\n  Machine Translation and Parsing", "abstract": "The addition of syntax-aware decoding in Neural Machine Translation (NMT)\nsystems requires an effective tree-structured neural network, a syntax-aware\nattention model and a language generation model that is sensitive to sentence\nstructure. We exploit a top-down tree-structured model called DRNN\n(Doubly-Recurrent Neural Networks) first proposed by Alvarez-Melis and Jaakola\n(2017) to create an NMT model called Seq2DRNN that combines a sequential\nencoder with tree-structured decoding augmented with a syntax-aware attention\nmodel. Unlike previous approaches to syntax-based NMT which use dependency\nparsing models our method uses constituency parsing which we argue provides\nuseful information for translation. In addition, we use the syntactic structure\nof the sentence to add new connections to the tree-structured decoder neural\nnetwork (Seq2DRNN+SynC). We compare our NMT model with sequential and state of\nthe art syntax-based NMT models and show that our model produces more fluent\ntranslations with better reordering. Since our model is capable of doing\ntranslation and constituency parsing at the same time we also compare our\nparsing accuracy against other neural parsing models.", "published": "2018-09-06 07:33:48", "link": "http://arxiv.org/abs/1809.01854v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Millions of Personalized Dialogue Agents", "abstract": "Current dialogue systems are not very engaging for users, especially when\ntrained end-to-end without relying on proactive reengaging scripted strategies.\nZhang et al. (2018) showed that the engagement level of end-to-end dialogue\nmodels increases when conditioning them on text personas providing some\npersonalized back-story to the model. However, the dataset used in Zhang et al.\n(2018) is synthetic and of limited size as it contains around 1k different\npersonas. In this paper we introduce a new dataset providing 5 million personas\nand 700 million persona-based dialogues. Our experiments show that, at this\nscale, training using personas still improves the performance of end-to-end\nsystems. In addition, we show that other tasks benefit from the wide coverage\nof our dataset by fine-tuning our model on the data from Zhang et al. (2018)\nand achieving state-of-the-art results.", "published": "2018-09-06 13:36:40", "link": "http://arxiv.org/abs/1809.01984v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Syntactic Properties of Seq2seq Output with a Broad Coverage\n  HPSG: A Case Study on Machine Translation", "abstract": "Sequence to sequence (seq2seq) models are often employed in settings where\nthe target output is natural language. However, the syntactic properties of the\nlanguage generated from these models are not well understood. We explore\nwhether such output belongs to a formal and realistic grammar, by employing the\nEnglish Resource Grammar (ERG), a broad coverage, linguistically precise\nHPSG-based grammar of English. From a French to English parallel corpus, we\nanalyze the parseability and grammatical constructions occurring in output from\na seq2seq translation model. Over 93\\% of the model translations are parseable,\nsuggesting that it learns to generate conforming to a grammar. The model has\ntrouble learning the distribution of rarer syntactic rules, and we pinpoint\nseveral constructions that differentiate translations between the references\nand our model.", "published": "2018-09-06 15:09:46", "link": "http://arxiv.org/abs/1809.02035v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Upcycle Your OCR: Reusing OCRs for Post-OCR Text Correction in Romanised\n  Sanskrit", "abstract": "We propose a post-OCR text correction approach for digitising texts in\nRomanised Sanskrit. Owing to the lack of resources our approach uses OCR models\ntrained for other languages written in Roman. Currently, there exists no\ndataset available for Romanised Sanskrit OCR. So, we bootstrap a dataset of 430\nimages, scanned in two different settings and their corresponding ground truth.\nFor training, we synthetically generate training images for both the settings.\nWe find that the use of copying mechanism (Gu et al., 2016) yields a percentage\nincrease of 7.69 in Character Recognition Rate (CRR) than the current state of\nthe art model in solving monotone sequence-to-sequence tasks (Schnober et al.,\n2016). We find that our system is robust in combating OCR-prone errors, as it\nobtains a CRR of 87.01% from an OCR output with CRR of 35.76% for one of the\ndataset settings. A human judgment survey performed on the models shows that\nour proposed model results in predictions which are faster to comprehend and\nfaster to improve for a human than the other systems.", "published": "2018-09-06 18:02:55", "link": "http://arxiv.org/abs/1809.02147v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Character-Aware Decoder for Translation into Morphologically Rich\n  Languages", "abstract": "Neural machine translation (NMT) systems operate primarily on words (or\nsub-words), ignoring lower-level patterns of morphology. We present a\ncharacter-aware decoder designed to capture such patterns when translating into\nmorphologically rich languages. We achieve character-awareness by augmenting\nboth the softmax and embedding layers of an attention-based encoder-decoder\nmodel with convolutional neural networks that operate on the spelling of a\nword. To investigate performance on a wide variety of morphological phenomena,\nwe translate English into 14 typologically diverse target languages using the\nTED multi-target dataset. In this low-resource setting, the character-aware\ndecoder provides consistent improvements with BLEU score gains of up to\n$+3.05$. In addition, we analyze the relationship between the gains obtained\nand properties of the target language and find evidence that our model does\nindeed exploit morphological patterns.", "published": "2018-09-06 21:26:31", "link": "http://arxiv.org/abs/1809.02223v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "82 Treebanks, 34 Models: Universal Dependency Parsing with\n  Multi-Treebank Models", "abstract": "We present the Uppsala system for the CoNLL 2018 Shared Task on universal\ndependency parsing. Our system is a pipeline consisting of three components:\nthe first performs joint word and sentence segmentation; the second predicts\npart-of- speech tags and morphological features; the third predicts dependency\ntrees from words and tags. Instead of training a single parsing model for each\ntreebank, we trained models with multiple treebanks for one language or closely\nrelated languages, greatly reducing the number of models. On the official test\nrun, we ranked 7th of 27 teams for the LAS and MLAS metrics. Our system\nobtained the best scores overall for word segmentation, universal POS tagging,\nand morphological features.", "published": "2018-09-06 22:10:38", "link": "http://arxiv.org/abs/1809.02237v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Describing a Knowledge Base", "abstract": "We aim to automatically generate natural language descriptions about an input\nstructured knowledge base (KB). We build our generation framework based on a\npointer network which can copy facts from the input KB, and add two attention\nmechanisms: (i) slot-aware attention to capture the association between a slot\ntype and its corresponding slot value; and (ii) a new \\emph{table position\nself-attention} to capture the inter-dependencies among related slots. For\nevaluation, besides standard metrics including BLEU, METEOR, and ROUGE, we\npropose a KB reconstruction based metric by extracting a KB from the generation\noutput and comparing it with the input KB. We also create a new data set which\nincludes 106,216 pairs of structured KBs and their corresponding natural\nlanguage descriptions for two distinct entity types. Experiments show that our\napproach significantly outperforms state-of-the-art methods. The reconstructed\nKB achieves 68.8% - 72.6% F-score.", "published": "2018-09-06 02:56:58", "link": "http://arxiv.org/abs/1809.01797v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Why are Sequence-to-Sequence Models So Dull? Understanding the\n  Low-Diversity Problem of Chatbots", "abstract": "Diversity is a long-studied topic in information retrieval that usually\nrefers to the requirement that retrieved results should be non-repetitive and\ncover different aspects. In a conversational setting, an additional dimension\nof diversity matters: an engaging response generation system should be able to\noutput responses that are diverse and interesting. Sequence-to-sequence\n(Seq2Seq) models have been shown to be very effective for response generation.\nHowever, dialogue responses generated by Seq2Seq models tend to have low\ndiversity. In this paper, we review known sources and existing approaches to\nthis low-diversity problem. We also identify a source of low diversity that has\nbeen little studied so far, namely model over-confidence. We sketch several\ndirections for tackling model over-confidence and, hence, the low-diversity\nproblem, including confidence penalties and label smoothing.", "published": "2018-09-06 12:24:04", "link": "http://arxiv.org/abs/1809.01941v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Code-switched Language Models Using Dual RNNs and Same-Source\n  Pretraining", "abstract": "This work focuses on building language models (LMs) for code-switched text.\nWe propose two techniques that significantly improve these LMs: 1) A novel\nrecurrent neural network unit with dual components that focus on each language\nin the code-switched text separately 2) Pretraining the LM using synthetic text\nfrom a generative model estimated using the training data. We demonstrate the\neffectiveness of our proposed techniques by reporting perplexities on a\nMandarin-English task and derive significant reductions in perplexity.", "published": "2018-09-06 13:12:27", "link": "http://arxiv.org/abs/1809.01962v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Graph-structured Passage Representation for Multi-hop Reading\n  Comprehension with Graph Neural Networks", "abstract": "Multi-hop reading comprehension focuses on one type of factoid question,\nwhere a system needs to properly integrate multiple pieces of evidence to\ncorrectly answer a question. Previous work approximates global evidence with\nlocal coreference information, encoding coreference chains with DAG-styled GRU\nlayers within a gated-attention reader. However, coreference is limited in\nproviding information for rich inference. We introduce a new method for better\nconnecting global evidence, which forms more complex graphs compared to DAGs.\nTo perform evidence integration on our graphs, we investigate two recent graph\nneural networks, namely graph convolutional network (GCN) and graph recurrent\nnetwork (GRN). Experiments on two standard datasets show that richer global\ninformation leads to better answers. Our method performs better than all\npublished results on these datasets.", "published": "2018-09-06 15:18:14", "link": "http://arxiv.org/abs/1809.02040v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adversarial Over-Sensitivity and Over-Stability Strategies for Dialogue\n  Models", "abstract": "We present two categories of model-agnostic adversarial strategies that\nreveal the weaknesses of several generative, task-oriented dialogue models:\nShould-Not-Change strategies that evaluate over-sensitivity to small and\nsemantics-preserving edits, as well as Should-Change strategies that test if a\nmodel is over-stable against subtle yet semantics-changing modifications. We\nnext perform adversarial training with each strategy, employing a max-margin\napproach for negative generative examples. This not only makes the target\ndialogue model more robust to the adversarial inputs, but also helps it perform\nsignificantly better on the original inputs. Moreover, training on all\nstrategies combined achieves further improvements, achieving a new\nstate-of-the-art performance on the original task (also verified via human\nevaluation). In addition to adversarial training, we also address the\nrobustness task at the model-level, by feeding it subword units as both inputs\nand outputs, and show that the resulting model is equally competitive, requires\nonly 1/4 of the original vocabulary size, and is robust to one of the\nadversarial strategies (to which the original model is vulnerable) even without\nadversarial training.", "published": "2018-09-06 16:27:32", "link": "http://arxiv.org/abs/1809.02079v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Object Hallucination in Image Captioning", "abstract": "Despite continuously improving performance, contemporary image captioning\nmodels are prone to \"hallucinating\" objects that are not actually in a scene.\nOne problem is that standard metrics only measure similarity to ground truth\ncaptions and may not fully capture image relevance. In this work, we propose a\nnew image relevance metric to evaluate current models with veridical visual\nlabels and assess their rate of object hallucination. We analyze how captioning\nmodel architectures and learning objectives contribute to object hallucination,\nexplore when hallucination is likely due to image misclassification or language\npriors, and assess how well current sentence metrics capture object\nhallucination. We investigate these questions on the standard image captioning\nbenchmark, MSCOCO, using a diverse set of models. Our analysis yields several\ninteresting findings, including that models which score best on standard\nsentence metrics do not always have lower hallucination and that models which\nhallucinate more tend to make errors driven by language priors.", "published": "2018-09-06 18:25:18", "link": "http://arxiv.org/abs/1809.02156v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Assessing Gender Bias in Machine Translation -- A Case Study with Google\n  Translate", "abstract": "Recently there has been a growing concern about machine bias, where trained\nstatistical models grow to reflect controversial societal asymmetries, such as\ngender or racial bias. A significant number of AI tools have recently been\nsuggested to be harmfully biased towards some minority, with reports of racist\ncriminal behavior predictors, Iphone X failing to differentiate between two\nAsian people and Google photos' mistakenly classifying black people as\ngorillas. Although a systematic study of such biases can be difficult, we\nbelieve that automated translation tools can be exploited through gender\nneutral languages to yield a window into the phenomenon of gender bias in AI.\n  In this paper, we start with a comprehensive list of job positions from the\nU.S. Bureau of Labor Statistics (BLS) and used it to build sentences in\nconstructions like \"He/She is an Engineer\" in 12 different gender neutral\nlanguages such as Hungarian, Chinese, Yoruba, and several others. We translate\nthese sentences into English using the Google Translate API, and collect\nstatistics about the frequency of female, male and gender-neutral pronouns in\nthe translated output. We show that GT exhibits a strong tendency towards male\ndefaults, in particular for fields linked to unbalanced gender distribution\nsuch as STEM jobs. We ran these statistics against BLS' data for the frequency\nof female participation in each job position, showing that GT fails to\nreproduce a real-world distribution of female workers. We provide experimental\nevidence that even if one does not expect in principle a 50:50 pronominal\ngender distribution, GT yields male defaults much more frequently than what\nwould be expected from demographic data alone.\n  We are hopeful that this work will ignite a debate about the need to augment\ncurrent statistical translation tools with debiasing techniques which can\nalready be found in the scientific literature.", "published": "2018-09-06 20:39:23", "link": "http://arxiv.org/abs/1809.02208v4", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "An Analysis of Hierarchical Text Classification Using Word Embeddings", "abstract": "Efficient distributed numerical word representation models (word embeddings)\ncombined with modern machine learning algorithms have recently yielded\nconsiderable improvement on automatic document classification tasks. However,\nthe effectiveness of such techniques has not been assessed for the hierarchical\ntext classification (HTC) yet. This study investigates the application of those\nmodels and algorithms on this specific problem by means of experimentation and\nanalysis. We trained classification models with prominent machine learning\nalgorithm implementations---fastText, XGBoost, SVM, and Keras' CNN---and\nnoticeable word embeddings generation methods---GloVe, word2vec, and\nfastText---with publicly available data and evaluated them with measures\nspecifically appropriate for the hierarchical context. FastText achieved an\n${}_{LCA}F_1$ of 0.893 on a single-labeled version of the RCV1 dataset. An\nanalysis indicates that using word embeddings and its flavors is a very\npromising approach for HTC.", "published": "2018-09-06 00:31:51", "link": "http://arxiv.org/abs/1809.01771v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Noise Contrastive Estimation and Negative Sampling for Conditional\n  Models: Consistency and Statistical Efficiency", "abstract": "Noise Contrastive Estimation (NCE) is a powerful parameter estimation method\nfor log-linear models, which avoids calculation of the partition function or\nits derivatives at each training step, a computationally demanding step in many\ncases. It is closely related to negative sampling methods, now widely used in\nNLP. This paper considers NCE-based estimation of conditional models.\nConditional models are frequently encountered in practice; however there has\nnot been a rigorous theoretical analysis of NCE in this setting, and we will\nargue there are subtle but important questions when generalizing NCE to the\nconditional case. In particular, we analyze two variants of NCE for conditional\nmodels: one based on a classification objective, the other based on a ranking\nobjective. We show that the ranking-based variant of NCE gives consistent\nparameter estimates under weaker assumptions than the classification-based\nmethod; we analyze the statistical efficiency of the ranking-based and\nclassification-based variants of NCE; finally we describe experiments on\nsynthetic data and language modeling showing the effectiveness and trade-offs\nof both methods.", "published": "2018-09-06 04:11:46", "link": "http://arxiv.org/abs/1809.01812v1", "categories": ["cs.CL", "cs.LG", "stat.ME"], "primary_category": "cs.CL"}
{"title": "Visual Coreference Resolution in Visual Dialog using Neural Module\n  Networks", "abstract": "Visual dialog entails answering a series of questions grounded in an image,\nusing dialog history as context. In addition to the challenges found in visual\nquestion answering (VQA), which can be seen as one-round dialog, visual dialog\nencompasses several more. We focus on one such problem called visual\ncoreference resolution that involves determining which words, typically noun\nphrases and pronouns, co-refer to the same entity/object instance in an image.\nThis is crucial, especially for pronouns (e.g., `it'), as the dialog agent must\nfirst link it to a previous coreference (e.g., `boat'), and only then can rely\non the visual grounding of the coreference `boat' to reason about the pronoun\n`it'. Prior work (in visual dialog) models visual coreference resolution either\n(a) implicitly via a memory network over history, or (b) at a coarse level for\nthe entire question; and not explicitly at a phrase level of granularity. In\nthis work, we propose a neural module network architecture for visual dialog by\nintroducing two novel modules - Refer and Exclude - that perform explicit,\ngrounded, coreference resolution at a finer word level. We demonstrate the\neffectiveness of our model on MNIST Dialog, a visually simple yet\ncoreference-wise complex dataset, by achieving near perfect accuracy, and on\nVisDial, a large and challenging visual dialog dataset on real images, where\nour model outperforms other approaches, and is more interpretable, grounded,\nand consistent qualitatively.", "published": "2018-09-06 04:36:22", "link": "http://arxiv.org/abs/1809.01816v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Cascaded Mutual Modulation for Visual Reasoning", "abstract": "Visual reasoning is a special visual question answering problem that is\nmulti-step and compositional by nature, and also requires intensive text-vision\ninteractions. We propose CMM: Cascaded Mutual Modulation as a novel end-to-end\nvisual reasoning model. CMM includes a multi-step comprehension process for\nboth question and image. In each step, we use a Feature-wise Linear Modulation\n(FiLM) technique to enable textual/visual pipeline to mutually control each\nother. Experiments show that CMM significantly outperforms most related models,\nand reach state-of-the-arts on two visual reasoning benchmarks: CLEVR and NLVR,\ncollected from both synthetic and natural languages. Ablation studies confirm\nthat both our multistep framework and our visual-guided language modulation are\ncritical to the task. Our code is available at\nhttps://github.com/FlamingHorizon/CMM-VR.", "published": "2018-09-06 12:26:24", "link": "http://arxiv.org/abs/1809.01943v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.IR"}
{"title": "Dual Ask-Answer Network for Machine Reading Comprehension", "abstract": "There are three modalities in the reading comprehension setting: question,\nanswer and context. The task of question answering or question generation aims\nto infer an answer or a question when given the counterpart based on context.\nWe present a novel two-way neural sequence transduction model that connects\nthree modalities, allowing it to learn two tasks simultaneously and mutually\nbenefit one another. During training, the model receives\nquestion-context-answer triplets as input and captures the cross-modal\ninteraction via a hierarchical attention process. Unlike previous joint\nlearning paradigms that leverage the duality of question generation and\nquestion answering at data level, we solve such dual tasks at the architecture\nlevel by mirroring the network structure and partially sharing components at\ndifferent layers. This enables the knowledge to be transferred from one task to\nanother, helping the model to find a general representation for each modality.\nThe evaluation on four public datasets shows that our dual-learning model\noutperforms the mono-learning counterpart as well as the state-of-the-art joint\nmodels on both question answering and question generation tasks.", "published": "2018-09-06 13:57:03", "link": "http://arxiv.org/abs/1809.01997v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Uncovering divergent linguistic information in word embeddings with\n  lessons for intrinsic and extrinsic evaluation", "abstract": "Following the recent success of word embeddings, it has been argued that\nthere is no such thing as an ideal representation for words, as different\nmodels tend to capture divergent and often mutually incompatible aspects like\nsemantics/syntax and similarity/relatedness. In this paper, we show that each\nembedding model captures more information than directly apparent. A linear\ntransformation that adjusts the similarity order of the model without any\nexternal resource can tailor it to achieve better results in those aspects,\nproviding a new perspective on how embeddings encode divergent linguistic\ninformation. In addition, we explore the relation between intrinsic and\nextrinsic evaluation, as the effect of our transformations in downstream tasks\nis higher for unsupervised systems than for supervised ones.", "published": "2018-09-06 17:08:21", "link": "http://arxiv.org/abs/1809.02094v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adversarial Feature-Mapping for Speech Enhancement", "abstract": "Feature-mapping with deep neural networks is commonly used for single-channel\nspeech enhancement, in which a feature-mapping network directly transforms the\nnoisy features to the corresponding enhanced ones and is trained to minimize\nthe mean square errors between the enhanced and clean features. In this paper,\nwe propose an adversarial feature-mapping (AFM) method for speech enhancement\nwhich advances the feature-mapping approach with adversarial learning. An\nadditional discriminator network is introduced to distinguish the enhanced\nfeatures from the real clean ones. The two networks are jointly optimized to\nminimize the feature-mapping loss and simultaneously mini-maximize the\ndiscrimination loss. The distribution of the enhanced features is further\npushed towards that of the clean features through this adversarial multi-task\ntraining. To achieve better performance on ASR task, senone-aware (SA) AFM is\nfurther proposed in which an acoustic model network is jointly trained with the\nfeature-mapping and discriminator networks to optimize the senone\nclassification loss in addition to the AFM losses. Evaluated on the CHiME-3\ndataset, the proposed AFM achieves 16.95% and 5.27% relative word error rate\n(WER) improvements over the real noisy data and the feature-mapping baseline\nrespectively and the SA-AFM achieves 9.85% relative WER improvement over the\nmulti-conditional acoustic model.", "published": "2018-09-06 23:42:21", "link": "http://arxiv.org/abs/1809.02251v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cycle-Consistent Speech Enhancement", "abstract": "Feature mapping using deep neural networks is an effective approach for\nsingle-channel speech enhancement. Noisy features are transformed to the\nenhanced ones through a mapping network and the mean square errors between the\nenhanced and clean features are minimized. In this paper, we propose a\ncycle-consistent speech enhancement (CSE) in which an additional inverse\nmapping network is introduced to reconstruct the noisy features from the\nenhanced ones. A cycle-consistent constraint is enforced to minimize the\nreconstruction loss. Similarly, a backward cycle of mappings is performed in\nthe opposite direction with the same networks and losses. With\ncycle-consistency, the speech structure is well preserved in the enhanced\nfeatures while noise is effectively reduced such that the feature-mapping\nnetwork generalizes better to unseen data. In cases where only unparalleled\nnoisy and clean data is available for training, two discriminator networks are\nused to distinguish the enhanced and noised features from the clean and noisy\nones. The discrimination losses are jointly optimized with reconstruction\nlosses through adversarial multi-task learning. Evaluated on the CHiME-3\ndataset, the proposed CSE achieves 19.60% and 6.69% relative word error rate\nimprovements respectively when using or without using parallel clean and noisy\nspeech data.", "published": "2018-09-06 23:55:49", "link": "http://arxiv.org/abs/1809.02253v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
