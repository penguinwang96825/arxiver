{"title": "Massive Exploration of Neural Machine Translation Architectures", "abstract": "Neural Machine Translation (NMT) has shown remarkable progress over the past\nfew years with production systems now being deployed to end-users. One major\ndrawback of current architectures is that they are expensive to train,\ntypically requiring days to weeks of GPU time to converge. This makes\nexhaustive hyperparameter search, as is commonly done with other neural network\narchitectures, prohibitively expensive. In this work, we present the first\nlarge-scale analysis of NMT architecture hyperparameters. We report empirical\nresults and variance numbers for several hundred experimental runs,\ncorresponding to over 250,000 GPU hours on the standard WMT English to German\ntranslation task. Our experiments lead to novel insights and practical advice\nfor building and extending NMT architectures. As part of this contribution, we\nrelease an open-source NMT framework that enables researchers to easily\nexperiment with novel techniques and reproduce state of the art results.", "published": "2017-03-11 04:17:46", "link": "http://arxiv.org/abs/1703.03906v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Hate Speech Detection and the Problem of Offensive Language", "abstract": "A key challenge for automatic hate-speech detection on social media is the\nseparation of hate speech from other instances of offensive language. Lexical\ndetection methods tend to have low precision because they classify all messages\ncontaining particular terms as hate speech and previous work using supervised\nlearning has failed to distinguish between the two categories. We used a\ncrowd-sourced hate speech lexicon to collect tweets containing hate speech\nkeywords. We use crowd-sourcing to label a sample of these tweets into three\ncategories: those containing hate speech, only offensive language, and those\nwith neither. We train a multi-class classifier to distinguish between these\ndifferent categories. Close analysis of the predictions and the errors shows\nwhen we can reliably separate hate speech from other offensive language and\nwhen this differentiation is more difficult. We find that racist and homophobic\ntweets are more likely to be classified as hate speech but that sexist tweets\nare generally classified as offensive. Tweets without explicit hate keywords\nare also more difficult to classify.", "published": "2017-03-11 18:20:13", "link": "http://arxiv.org/abs/1703.04009v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extending Automatic Discourse Segmentation for Texts in Spanish to\n  Catalan", "abstract": "At present, automatic discourse analysis is a relevant research topic in the\nfield of NLP. However, discourse is one of the phenomena most difficult to\nprocess. Although discourse parsers have been already developed for several\nlanguages, this tool does not exist for Catalan. In order to implement this\nkind of parser, the first step is to develop a discourse segmenter. In this\narticle we present the first discourse segmenter for texts in Catalan. This\nsegmenter is based on Rhetorical Structure Theory (RST) for Spanish, and uses\nlexical and syntactic information to translate rules valid for Spanish into\nrules for Catalan. We have evaluated the system by using a gold standard corpus\nincluding manually segmented texts and results are promising.", "published": "2017-03-11 07:37:37", "link": "http://arxiv.org/abs/1703.04718v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A German Corpus for Text Similarity Detection Tasks", "abstract": "Text similarity detection aims at measuring the degree of similarity between\na pair of texts. Corpora available for text similarity detection are designed\nto evaluate the algorithms to assess the paraphrase level among documents. In\nthis paper we present a textual German corpus for similarity detection. The\npurpose of this corpus is to automatically assess the similarity between a pair\nof texts and to evaluate different similarity measures, both for whole\ndocuments or for individual sentences. Therefore we have calculated several\nsimple measures on our corpus based on a library of similarity functions.", "published": "2017-03-11 07:35:28", "link": "http://arxiv.org/abs/1703.03923v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Language Use Matters: Analysis of the Linguistic Structure of Question\n  Texts Can Characterize Answerability in Quora", "abstract": "Quora is one of the most popular community Q&A sites of recent times.\nHowever, many question posts on this Q&A site often do not get answered. In\nthis paper, we quantify various linguistic activities that discriminates an\nanswered question from an unanswered one. Our central finding is that the way\nusers use language while writing the question text can be a very effective\nmeans to characterize answerability. This characterization helps us to predict\nearly if a question remaining unanswered for a specific time period t will\neventually be answered or not and achieve an accuracy of 76.26% (t = 1 month)\nand 68.33% (t = 3 months). Notably, features representing the language use\npatterns of the users are most discriminative and alone account for an accuracy\nof 74.18%. We also compare our method with some of the similar works (Dror et\nal., Yang et al.) achieving a maximum improvement of ~39% in terms of accuracy.", "published": "2017-03-11 17:14:55", "link": "http://arxiv.org/abs/1703.04001v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Ask Me Even More: Dynamic Memory Tensor Networks (Extended Model)", "abstract": "We examine Memory Networks for the task of question answering (QA), under\ncommon real world scenario where training examples are scarce and under weakly\nsupervised scenario, that is only extrinsic labels are available for training.\nWe propose extensions for the Dynamic Memory Network (DMN), specifically within\nthe attention mechanism, we call the resulting Neural Architecture as Dynamic\nMemory Tensor Network (DMTN). Ultimately, we see that our proposed extensions\nresults in over 80% improvement in the number of task passed against the\nbaselined standard DMN and 20% more task passed compared to state-of-the-art\nEnd-to-End Memory Network for Facebook's single task weakly trained 1K bAbi\ndataset.", "published": "2017-03-11 10:05:19", "link": "http://arxiv.org/abs/1703.03939v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
