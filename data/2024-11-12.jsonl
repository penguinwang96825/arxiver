{"title": "Automated Market Making: the case of Pegged Assets", "abstract": "In this paper, we introduce a novel framework to model the exchange rate\ndynamics between two intrinsically linked cryptoassets, such as stablecoins\npegged to the same fiat currency or a liquid staking token and its associated\nnative token. Our approach employs multi-level nested Ornstein-Uhlenbeck (OU)\nprocesses, for which we derive key properties and develop calibration and\nfiltering techniques. Then, we design an automated market maker (AMM) model\nspecifically tailored for the swapping of closely related cryptoassets.\nDistinct from existing models, our AMM leverages the unique exchange rate\ndynamics provided by the multi-level nested OU processes, enabling more precise\nrisk management and enhanced liquidity provision. We validate the model through\nnumerical simulations using real-world data for the USDC/USDT and wstETH/WETH\npairs, demonstrating that it consistently yields efficient quotes. This\napproach offers significant potential to improve liquidity in markets for\npegged assets.", "published": "2024-11-12 19:47:47", "link": "http://arxiv.org/abs/2411.08145v1", "categories": ["q-fin.TR", "q-fin.CP"], "primary_category": "q-fin.TR"}
{"title": "Implementing Dynamic Pricing Across Multiple Pricing Groups in Real Estate", "abstract": "This article presents a mathematical model of dynamic pricing for real estate\n(RE) that incorporates multiple pricing groups, thereby expanding the\ncapabilities of existing models. The developed model solves the problem of\nmaximizing aggregate cumulative revenue at the end of the sales period while\nmeeting the revenue and sales goals. A method is proposed for distributing\naggregate cumulative revenue goals across different RE pricing groups. The\nmodel is further modified to account for the time value of money and the real\nestate value increase as construction progresses. The algorithm for\nconstructing a pricing policy for multiple pricing groups is described, and\nnumerical simulations are performed to demonstrate how the algorithm operates.", "published": "2024-11-12 11:45:17", "link": "http://arxiv.org/abs/2411.07732v1", "categories": ["q-fin.MF", "econ.TH", "q-fin.CP", "q-fin.TR"], "primary_category": "q-fin.MF"}
{"title": "Reinforcement Learning Framework for Quantitative Trading", "abstract": "The inherent volatility and dynamic fluctuations within the financial stock\nmarket underscore the necessity for investors to employ a comprehensive and\nreliable approach that integrates risk management strategies, market trends,\nand the movement trends of individual securities. By evaluating specific data,\ninvestors can make more informed decisions. However, the current body of\nliterature lacks substantial evidence supporting the practical efficacy of\nreinforcement learning (RL) agents, as many models have only demonstrated\nsuccess in back testing using historical data. This highlights the urgent need\nfor a more advanced methodology capable of addressing these challenges. There\nis a significant disconnect in the effective utilization of financial\nindicators to better understand the potential market trends of individual\nsecurities. The disclosure of successful trading strategies is often restricted\nwithin financial markets, resulting in a scarcity of widely documented and\npublished strategies leveraging RL. Furthermore, current research frequently\noverlooks the identification of financial indicators correlated with various\nmarket trends and their potential advantages.\n  This research endeavors to address these complexities by enhancing the\nability of RL agents to effectively differentiate between positive and negative\nbuy/sell actions using financial indicators. While we do not address all\nconcerns, this paper provides deeper insights and commentary on the utilization\nof technical indicators and their benefits within reinforcement learning. This\nwork establishes a foundational framework for further exploration and\ninvestigation of more complex scenarios.", "published": "2024-11-12 06:44:28", "link": "http://arxiv.org/abs/2411.07585v1", "categories": ["q-fin.TR", "cs.AI", "q-fin.CP"], "primary_category": "q-fin.TR"}
{"title": "New approaches of the DCC-GARCH residual: Application to foreign exchange rates", "abstract": "Two formulations are proposed to filter out correlations in the residuals of\nthe multivariate GARCH model. The first approach is to estimate the correlation\nmatrix as a parameter and transform any joint distribution to have an arbitrary\ncorrelation matrix. The second approach transforms time series data into an\nuncorrelated residual based on the eigenvalue decomposition of a correlation\nmatrix. The empirical performance of these methods is examined through a\nprediction task for foreign exchange rates and compared with other\nmethodologies in terms of the out-of-sample likelihood. By using these\napproaches, the DCC-GARCH residual can be almost independent.", "published": "2024-11-12 23:50:13", "link": "http://arxiv.org/abs/2411.08246v1", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
{"title": "Efficient and Accurate Prompt Optimization: the Benefit of Memory in\n  Exemplar-Guided Reflection", "abstract": "Automatic prompt engineering aims to enhance the generation quality of large\nlanguage models (LLMs). Recent works utilize feedbacks generated from erroneous\ncases to guide the prompt optimization. During inference, they may further\nretrieve several semantically-related exemplars and concatenate them to the\noptimized prompts to improve the performance. However, those works only utilize\nthe feedback at the current step, ignoring historical and unseleccted feedbacks\nwhich are potentially beneficial. Moreover, the selection of exemplars only\nconsiders the general semantic relationship and may not be optimal in terms of\ntask performance and matching with the optimized prompt. In this work, we\npropose an Exemplar-Guided Reflection with Memory mechanism (ERM) to realize\nmore efficient and accurate prompt optimization. Specifically, we design an\nexemplar-guided reflection mechanism where the feedback generation is\nadditionally guided by the generated exemplars. We further build two kinds of\nmemory to fully utilize the historical feedback information and support more\neffective exemplar retrieval. Empirical evaluations show our method surpasses\nprevious state-of-the-arts with less optimization steps, i.e., improving F1\nscore by 10.1 on LIAR dataset, and reducing half of the optimization steps on\nProTeGi.", "published": "2024-11-12 00:07:29", "link": "http://arxiv.org/abs/2411.07446v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DecoPrompt : Decoding Prompts Reduces Hallucinations when Large Language\n  Models Meet False Premises", "abstract": "While large language models (LLMs) have demonstrated increasing power, they\nhave also called upon studies on their hallucinated outputs that deviate from\nfactually correct statements. In this paper, we focus on one important scenario\nof false premises, where LLMs are distracted by misaligned claims although the\nmodel possesses the required factual knowledge to answer original questions\naccurately. Inspired by the observation that entropy of the false-premise\nprompt is closely related to its likelihood to elicit hallucination generation,\nwe propose a new prompting algorithm, named DecoPrompt, to mitigate\nhallucination. DecoPrompt leverages LLMs to \"decode\" the false-premise prompts\nwithout really eliciting hallucination output from LLMs. We perform experiments\non two datasets, demonstrating that DecoPrompt can reduce hallucinations\neffectively on outputs from different LLMs. Moreover, DecoPrompt exhibits\ncross-model transferability, which facilitates its applications to scenarios\nsuch as LLMs of large sizes or unavailable model logits.", "published": "2024-11-12 00:48:01", "link": "http://arxiv.org/abs/2411.07457v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controlled Evaluation of Syntactic Knowledge in Multilingual Language\n  Models", "abstract": "Language models (LMs) are capable of acquiring elements of human-like\nsyntactic knowledge. Targeted syntactic evaluation tests have been employed to\nmeasure how well they form generalizations about syntactic phenomena in\nhigh-resource languages such as English. However, we still lack a thorough\nunderstanding of LMs' capacity for syntactic generalizations in low-resource\nlanguages, which are responsible for much of the diversity of syntactic\npatterns worldwide. In this study, we develop targeted syntactic evaluation\ntests for three low-resource languages (Basque, Hindi, and Swahili) and use\nthem to evaluate five families of open-access multilingual Transformer LMs. We\nfind that some syntactic tasks prove relatively easy for LMs while others\n(agreement in sentences containing indirect objects in Basque, agreement across\na prepositional phrase in Swahili) are challenging. We additionally uncover\nissues with publicly available Transformers, including a bias toward the\nhabitual aspect in Hindi in multilingual BERT and underperformance compared to\nsimilar-sized models in XGLM-4.5B.", "published": "2024-11-12 01:26:41", "link": "http://arxiv.org/abs/2411.07474v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rapid Response: Mitigating LLM Jailbreaks with a Few Examples", "abstract": "As large language models (LLMs) grow more powerful, ensuring their safety\nagainst misuse becomes crucial. While researchers have focused on developing\nrobust defenses, no method has yet achieved complete invulnerability to\nattacks. We propose an alternative approach: instead of seeking perfect\nadversarial robustness, we develop rapid response techniques to look to block\nwhole classes of jailbreaks after observing only a handful of attacks. To study\nthis setting, we develop RapidResponseBench, a benchmark that measures a\ndefense's robustness against various jailbreak strategies after adapting to a\nfew observed examples. We evaluate five rapid response methods, all of which\nuse jailbreak proliferation, where we automatically generate additional\njailbreaks similar to the examples observed. Our strongest method, which\nfine-tunes an input classifier to block proliferated jailbreaks, reduces attack\nsuccess rate by a factor greater than 240 on an in-distribution set of\njailbreaks and a factor greater than 15 on an out-of-distribution set, having\nobserved just one example of each jailbreaking strategy. Moreover, further\nstudies suggest that the quality of proliferation model and number of\nproliferated examples play an key role in the effectiveness of this defense.\nOverall, our results highlight the potential of responding rapidly to novel\njailbreaks to limit LLM misuse.", "published": "2024-11-12 02:44:49", "link": "http://arxiv.org/abs/2411.07494v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt-enhanced Network for Hateful Meme Classification", "abstract": "The dynamic expansion of social media has led to an inundation of hateful\nmemes on media platforms, accentuating the growing need for efficient\nidentification and removal. Acknowledging the constraints of conventional\nmultimodal hateful meme classification, which heavily depends on external\nknowledge and poses the risk of including irrelevant or redundant content, we\ndeveloped Pen -- a prompt-enhanced network framework based on the prompt\nlearning approach. Specifically, after constructing the sequence through the\nprompt method and encoding it with a language model, we performed region\ninformation global extraction on the encoded sequence for multi-view\nperception. By capturing global information about inference instances and\ndemonstrations, Pen facilitates category selection by fully leveraging sequence\ninformation. This approach significantly improves model classification\naccuracy. Additionally, to bolster the model's reasoning capabilities in the\nfeature space, we introduced prompt-aware contrastive learning into the\nframework to improve the quality of sample feature distributions. Through\nextensive ablation experiments on two public datasets, we evaluate the\neffectiveness of the Pen framework, concurrently comparing it with\nstate-of-the-art model baselines. Our research findings highlight that Pen\nsurpasses manual prompt methods, showcasing superior generalization and\nclassification accuracy in hateful meme classification tasks. Our code is\navailable at https://github.com/juszzi/Pen.", "published": "2024-11-12 03:55:27", "link": "http://arxiv.org/abs/2411.07527v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models as Neurolinguistic Subjects: Discrepancy in\n  Performance and Competence for Form and Meaning", "abstract": "This study investigates the linguistic understanding of Large Language Models\n(LLMs) regarding signifier (form) and signified (meaning) by distinguishing two\nLLM assessment paradigms: psycholinguistic and neurolinguistic. Traditional\npsycholinguistic evaluations often reflect statistical rules that may not\naccurately represent LLMs' true linguistic competence. We introduce a\nneurolinguistic approach, utilizing a novel method that combines minimal pair\nand diagnostic probing to analyze activation patterns across model layers. This\nmethod allows for a detailed examination of how LLMs represent form and\nmeaning, and whether these representations are consistent across languages. We\nfound: (1) Psycholinguistic and neurolinguistic methods reveal that language\nperformance and competence are distinct; (2) Direct probability measurement may\nnot accurately assess linguistic competence; (3) Instruction tuning won't\nchange much competence but improve performance; (4) LLMs exhibit higher\ncompetence and performance in form compared to meaning. Additionally, we\nintroduce new conceptual minimal pair datasets for Chinese (COMPS-ZH) and\nGerman (COMPS-DE), complementing existing English datasets.", "published": "2024-11-12 04:16:44", "link": "http://arxiv.org/abs/2411.07533v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Annotating Constructions with UD: the experience of the Italian\n  Constructicon", "abstract": "The paper descirbes a first attempt of linking the Italian constructicon to\nUD resources", "published": "2024-11-12 08:10:54", "link": "http://arxiv.org/abs/2411.07623v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Verbosity $\\neq$ Veracity: Demystify Verbosity Compensation Behavior of\n  Large Language Models", "abstract": "Although Large Language Models (LLMs) have demonstrated their strong\ncapabilities in various tasks, recent work has revealed LLMs also exhibit\nundesirable behaviors, such as hallucination and toxicity, limiting their\nreliability and broader adoption. In this paper, we discover an understudied\ntype of undesirable behavior of LLMs, which we term Verbosity Compensation\n(VC), similar to the hesitation behavior of humans under uncertainty, where\nthey respond with excessive words such as repeating questions, introducing\nambiguity, or providing excessive enumeration. We present the first work that\ndefines and analyzes Verbosity Compensation, explores its causes, and proposes\na simple mitigating approach. Our experiments, conducted on five datasets of\nknowledge and reasoning-based QA tasks with 14 newly developed LLMs, reveal\nthree conclusions. 1) We reveal a pervasive presence of VC across all models\nand all datasets. Notably, GPT-4 exhibits a VC frequency of 50.40%. 2) We\nreveal the large performance gap between verbose and concise responses, with a\nnotable difference of 27.61% on the Qasper dataset. We also demonstrate that\nthis difference does not naturally diminish as LLM capability increases. Both\n1) and 2) highlight the urgent need to mitigate the frequency of VC behavior\nand disentangle verbosity with veracity. We propose a simple yet effective\ncascade algorithm that replaces the verbose responses with the other\nmodel-generated responses. The results show that our approach effectively\nalleviates the VC of the Mistral model from 63.81% to 16.16% on the Qasper\ndataset. 3) We also find that verbose responses exhibit higher uncertainty\nacross all five datasets, suggesting a strong connection between verbosity and\nmodel uncertainty. Our dataset and code are available at\nhttps://github.com/psunlpgroup/VerbosityLLM.", "published": "2024-11-12 15:15:20", "link": "http://arxiv.org/abs/2411.07858v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CryptoLLM: Unleashing the Power of Prompted LLMs for SmartQnA and\n  Classification of Crypto Posts", "abstract": "The rapid growth of social media has resulted in an large volume of\nuser-generated content, particularly in niche domains such as cryptocurrency.\nThis task focuses on developing robust classification models to accurately\ncategorize cryptocurrency-related social media posts into predefined classes,\nincluding but not limited to objective, positive, negative, etc. Additionally,\nthe task requires participants to identify the most relevant answers from a set\nof posts in response to specific questions. By leveraging advanced LLMs, this\nresearch aims to enhance the understanding and filtering of cryptocurrency\ndiscourse, thereby facilitating more informed decision-making in this volatile\nsector. We have used a prompt-based technique to solve the classification task\nfor reddit posts and twitter posts. Also, we have used 64-shot technique along\nwith prompts on GPT-4-Turbo model to determine whether a answer is relevant to\na question or not.", "published": "2024-11-12 16:49:51", "link": "http://arxiv.org/abs/2411.07917v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SHARP: Unlocking Interactive Hallucination via Stance Transfer in\n  Role-Playing Agents", "abstract": "The advanced role-playing capabilities of Large Language Models (LLMs) have\npaved the way for developing Role-Playing Agents (RPAs). However, existing\nbenchmarks in social interaction such as HPD and SocialBench have not\ninvestigated hallucination and face limitations like poor generalizability and\nimplicit judgments for character fidelity. To address these issues, we propose\na generalizable, explicit and effective paradigm to unlock the interactive\npatterns in diverse worldviews. Specifically, we define the interactive\nhallucination based on stance transfer and construct a benchmark, SHARP, by\nextracting relations from a general commonsense knowledge graph and leveraging\nthe inherent hallucination properties of RPAs to simulate interactions across\nroles. Extensive experiments validate the effectiveness and stability of our\nparadigm. Our findings further explore the factors influencing these metrics\nand discuss the trade-off between blind loyalty to roles and adherence to facts\nin RPAs.", "published": "2024-11-12 17:41:16", "link": "http://arxiv.org/abs/2411.07965v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SparrowVQE: Visual Question Explanation for Course Content Understanding", "abstract": "Visual Question Answering (VQA) research seeks to create AI systems to answer\nnatural language questions in images, yet VQA methods often yield overly\nsimplistic and short answers. This paper aims to advance the field by\nintroducing Visual Question Explanation (VQE), which enhances the ability of\nVQA to provide detailed explanations rather than brief responses and address\nthe need for more complex interaction with visual content. We first created an\nMLVQE dataset from a 14-week streamed video machine learning course, including\n885 slide images, 110,407 words of transcripts, and 9,416 designed\nquestion-answer (QA) pairs. Next, we proposed a novel SparrowVQE, a small 3\nbillion parameters multimodal model. We trained our model with a three-stage\ntraining mechanism consisting of multimodal pre-training (slide images and\ntranscripts feature alignment), instruction tuning (tuning the pre-trained\nmodel with transcripts and QA pairs), and domain fine-tuning (fine-tuning slide\nimage and QA pairs). Eventually, our SparrowVQE can understand and connect\nvisual information using the SigLIP model with transcripts using the Phi-2\nlanguage model with an MLP adapter. Experimental results demonstrate that our\nSparrowVQE achieves better performance in our developed MLVQE dataset and\noutperforms state-of-the-art methods in the other five benchmark VQA datasets.\nThe source code is available at\n\\url{https://github.com/YoushanZhang/SparrowVQE}.", "published": "2024-11-12 03:25:33", "link": "http://arxiv.org/abs/2411.07516v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Fair Summarization: Bridging Quality and Diversity in Extractive\n  Summaries", "abstract": "Fairness in multi-document summarization of user-generated content remains a\ncritical challenge in natural language processing (NLP). Existing summarization\nmethods often fail to ensure equitable representation across different social\ngroups, leading to biased outputs. In this paper, we introduce two novel\nmethods for fair extractive summarization: FairExtract, a clustering-based\napproach, and FairGPT, which leverages GPT-3.5-turbo with fairness constraints.\nWe evaluate these methods using Divsumm summarization dataset of White-aligned,\nHispanic, and African-American dialect tweets and compare them against relevant\nbaselines. The results obtained using a comprehensive set of summarization\nquality metrics such as SUPERT, BLANC, SummaQA, BARTScore, and UniEval, as well\nas a fairness metric F, demonstrate that FairExtract and FairGPT achieve\nsuperior fairness while maintaining competitive summarization quality.\nAdditionally, we introduce composite metrics (e.g., SUPERT+F, BLANC+F) that\nintegrate quality and fairness into a single evaluation framework, offering a\nmore nuanced understanding of the trade-offs between these objectives. Our code\nis available online.", "published": "2024-11-12 03:37:53", "link": "http://arxiv.org/abs/2411.07521v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Problem-Oriented Segmentation and Retrieval: Case Study on Tutoring\n  Conversations", "abstract": "Many open-ended conversations (e.g., tutoring lessons or business meetings)\nrevolve around pre-defined reference materials, like worksheets or meeting\nbullets. To provide a framework for studying such conversation structure, we\nintroduce Problem-Oriented Segmentation & Retrieval (POSR), the task of jointly\nbreaking down conversations into segments and linking each segment to the\nrelevant reference item. As a case study, we apply POSR to education where\neffectively structuring lessons around problems is critical yet difficult. We\npresent LessonLink, the first dataset of real-world tutoring lessons, featuring\n3,500 segments, spanning 24,300 minutes of instruction and linked to 116 SAT\nmath problems. We define and evaluate several joint and independent approaches\nfor POSR, including segmentation (e.g., TextTiling), retrieval (e.g., ColBERT),\nand large language models (LLMs) methods. Our results highlight that modeling\nPOSR as one joint task is essential: POSR methods outperform independent\nsegmentation and retrieval pipelines by up to +76% on joint metrics and surpass\ntraditional segmentation methods by up to +78% on segmentation metrics. We\ndemonstrate POSR's practical impact on downstream education applications,\nderiving new insights on the language and time use in real-world lesson\nstructures.", "published": "2024-11-12 07:16:51", "link": "http://arxiv.org/abs/2411.07598v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multimodal Clinical Reasoning through Knowledge-augmented Rationale\n  Generation", "abstract": "Clinical rationales play a pivotal role in accurate disease diagnosis;\nhowever, many models predominantly use discriminative methods and overlook the\nimportance of generating supportive rationales. Rationale distillation is a\nprocess that transfers knowledge from large language models (LLMs) to smaller\nlanguage models (SLMs), thereby enhancing the latter's ability to break down\ncomplex tasks. Despite its benefits, rationale distillation alone is inadequate\nfor addressing domain knowledge limitations in tasks requiring specialized\nexpertise, such as disease diagnosis. Effectively embedding domain knowledge in\nSLMs poses a significant challenge. While current LLMs are primarily geared\ntoward processing textual data, multimodal LLMs that incorporate time series\ndata, especially electronic health records (EHRs), are still evolving. To\ntackle these limitations, we introduce ClinRaGen, an SLM optimized for\nmultimodal rationale generation in disease diagnosis. ClinRaGen incorporates a\nunique knowledge-augmented attention mechanism to merge domain knowledge with\ntime series EHR data, utilizing a stepwise rationale distillation strategy to\nproduce both textual and time series-based clinical rationales. Our evaluations\nshow that ClinRaGen markedly improves the SLM's capability to interpret\nmultimodal EHR data and generate accurate clinical rationales, supporting more\nreliable disease diagnosis, advancing LLM applications in healthcare, and\nnarrowing the performance divide between LLMs and SLMs.", "published": "2024-11-12 07:34:56", "link": "http://arxiv.org/abs/2411.07611v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Direct Preference Optimization Using Sparse Feature-Level Constraints", "abstract": "The alignment of large language models (LLMs) with human preferences remains\na key challenge. While post-training techniques like Reinforcement Learning\nfrom Human Feedback (RLHF) and Direct Preference Optimization (DPO) have\nachieved notable success, they often introduce computational inefficiencies and\ntraining instability. In this paper, we propose Feature-level constrained\nPreference Optimization (FPO), a novel method designed to simplify the\nalignment process while ensuring stability. FPO leverages pre-trained Sparse\nAutoencoders (SAEs) and introduces feature-level constraints, allowing for\nefficient, sparsity-enforced alignment. Our approach enjoys efficiency by using\nsparse features activated in a well-trained sparse autoencoder and the quality\nof sequential KL divergence by using the feature-level offline reference.\nExperimental results on benchmark datasets demonstrate that FPO achieves a\n5.08% absolute improvement in win rate with much lower computational cost\ncompared to state-of-the-art baselines, making it a promising solution for\nefficient and controllable LLM alignments.", "published": "2024-11-12 07:54:13", "link": "http://arxiv.org/abs/2411.07618v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Mitigating Bias in Queer Representation within Large Language Models: A\n  Collaborative Agent Approach", "abstract": "Large Language Models (LLMs) often perpetuate biases in pronoun usage,\nleading to misrepresentation or exclusion of queer individuals. This paper\naddresses the specific problem of biased pronoun usage in LLM outputs,\nparticularly the inappropriate use of traditionally gendered pronouns (\"he,\"\n\"she\") when inclusive language is needed to accurately represent all\nidentities. We introduce a collaborative agent pipeline designed to mitigate\nthese biases by analyzing and optimizing pronoun usage for inclusivity. Our\nmulti-agent framework includes specialized agents for both bias detection and\ncorrection. Experimental evaluations using the Tango dataset-a benchmark\nfocused on gender pronoun usage-demonstrate that our approach significantly\nimproves inclusive pronoun classification, achieving a 32.6 percentage point\nincrease over GPT-4o in correctly disagreeing with inappropriate traditionally\ngendered pronouns $(\\chi^2 = 38.57, p < 0.0001)$. These results accentuate the\npotential of agent-driven frameworks in enhancing fairness and inclusivity in\nAI-generated content, demonstrating their efficacy in reducing biases and\npromoting socially responsible AI.", "published": "2024-11-12 09:14:16", "link": "http://arxiv.org/abs/2411.07656v2", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Query Optimization for Parametric Knowledge Refinement in\n  Retrieval-Augmented Large Language Models", "abstract": "We introduce the Extract-Refine-Retrieve-Read (ERRR) framework, a novel\napproach designed to bridge the pre-retrieval information gap in\nRetrieval-Augmented Generation (RAG) systems through query optimization\ntailored to meet the specific knowledge requirements of Large Language Models\n(LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR\nframework begins by extracting parametric knowledge from LLMs, followed by\nusing a specialized query optimizer for refining these queries. This process\nensures the retrieval of only the most pertinent information essential for\ngenerating accurate responses. Moreover, to enhance flexibility and reduce\ncomputational costs, we propose a trainable scheme for our pipeline that\nutilizes a smaller, tunable model as the query optimizer, which is refined\nthrough knowledge distillation from a larger teacher model. Our evaluations on\nvarious question-answering (QA) datasets and with different retrieval systems\nshow that ERRR consistently outperforms existing baselines, proving to be a\nversatile and cost-effective module for improving the utility and accuracy of\nRAG systems.", "published": "2024-11-12 14:12:45", "link": "http://arxiv.org/abs/2411.07820v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Chain Association-based Attacking and Shielding Natural Language\n  Processing Systems", "abstract": "Association as a gift enables people do not have to mention something in\ncompletely straightforward words and allows others to understand what they\nintend to refer to. In this paper, we propose a chain association-based\nadversarial attack against natural language processing systems, utilizing the\ncomprehension gap between humans and machines. We first generate a chain\nassociation graph for Chinese characters based on the association paradigm for\nbuilding search space of potential adversarial examples. Then, we introduce an\ndiscrete particle swarm optimization algorithm to search for the optimal\nadversarial examples. We conduct comprehensive experiments and show that\nadvanced natural language processing models and applications, including large\nlanguage models, are vulnerable to our attack, while humans appear good at\nunderstanding the perturbed text. We also explore two methods, including\nadversarial training and associative graph-based recovery, to shield systems\nfrom chain association-based attack. Since a few examples that use some\nderogatory terms, this paper contains materials that may be offensive or\nupsetting to some people.", "published": "2024-11-12 14:51:41", "link": "http://arxiv.org/abs/2411.07843v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IAE: Irony-based Adversarial Examples for Sentiment Analysis Systems", "abstract": "Adversarial examples, which are inputs deliberately perturbed with\nimperceptible changes to induce model errors, have raised serious concerns for\nthe reliability and security of deep neural networks (DNNs). While adversarial\nattacks have been extensively studied in continuous data domains such as\nimages, the discrete nature of text presents unique challenges. In this paper,\nwe propose Irony-based Adversarial Examples (IAE), a method that transforms\nstraightforward sentences into ironic ones to create adversarial text. This\napproach exploits the rhetorical device of irony, where the intended meaning is\nopposite to the literal interpretation, requiring a deeper understanding of\ncontext to detect. The IAE method is particularly challenging due to the need\nto accurately locate evaluation words, substitute them with appropriate\ncollocations, and expand the text with suitable ironic elements while\nmaintaining semantic coherence. Our research makes the following key\ncontributions: (1) We introduce IAE, a strategy for generating textual\nadversarial examples using irony. This method does not rely on pre-existing\nirony corpora, making it a versatile tool for creating adversarial text in\nvarious NLP tasks. (2) We demonstrate that the performance of several\nstate-of-the-art deep learning models on sentiment analysis tasks significantly\ndeteriorates when subjected to IAE attacks. This finding underscores the\nsusceptibility of current NLP systems to adversarial manipulation through\nirony. (3) We compare the impact of IAE on human judgment versus NLP systems,\nrevealing that humans are less susceptible to the effects of irony in text.", "published": "2024-11-12 15:01:47", "link": "http://arxiv.org/abs/2411.07850v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Trustful LLMs: Customizing and Grounding Text Generation with Knowledge\n  Bases and Dual Decoders", "abstract": "Although people are impressed by the content generation skills of large\nlanguage models, the use of LLMs, such as ChatGPT, is limited by the domain\ngrounding of the content. The correctness and groundedness of the generated\ncontent need to be based on a verified context, such as results from\nRetrieval-Augmented Generation (RAG). One important issue when adapting LLMs to\na customized domain is that the generated responses are often incomplete, or\nthe additions are not verified and may even be hallucinated. Prior studies on\nhallucination detection have focused on evaluation metrics, which are not\neasily adaptable to dynamic domains and can be vulnerable to attacks like\njail-breaking. In this work, we propose 1) a post-processing algorithm that\nleverages knowledge triplets in RAG context to correct hallucinations and 2) a\ndual-decoder model that fuses RAG context to guide the generation process.", "published": "2024-11-12 15:26:17", "link": "http://arxiv.org/abs/2411.07870v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mapping the Podcast Ecosystem with the Structured Podcast Research\n  Corpus", "abstract": "Podcasts provide highly diverse content to a massive listener base through a\nunique on-demand modality. However, limited data has prevented large-scale\ncomputational analysis of the podcast ecosystem. To fill this gap, we introduce\na massive dataset of over 1.1M podcast transcripts that is largely\ncomprehensive of all English language podcasts available through public RSS\nfeeds from May and June of 2020. This data is not limited to text, but rather\nincludes audio features and speaker turns for a subset of 370K episodes, and\nspeaker role inferences and other metadata for all 1.1M episodes. Using this\ndata, we also conduct a foundational investigation into the content, structure,\nand responsiveness of this ecosystem. Together, our data and analyses open the\ndoor to continued computational research of this popular and impactful medium.", "published": "2024-11-12 15:56:48", "link": "http://arxiv.org/abs/2411.07892v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "ExpressivityArena: Can LLMs Express Information Implicitly?", "abstract": "While Large Language Models (LLMs) have demonstrated remarkable performance\nin certain dimensions, their ability to express implicit language cues that\nhuman use for effective communication remains unclear. This paper presents\nExpressivityArena, a Python library for measuring the implicit communication\nabilities of LLMs. We provide a comprehensive framework to evaluate\nexpressivity of arbitrary LLMs and explore its practical implications. To this\nend, we refine the definition and measurements of ``expressivity,'' and use our\nframework in a set of small experiments. These experiments test LLMs in\ncreative and logical tasks such as poetry, coding, and emotion-based responses.\nThey are then evaluated by an automated grader, through ExpressivityArena,\nwhich we verify to be the most pragmatic for testing expressivity. Building on\nthese experiments, we deepen our understanding of the expressivity of LLMs by\nassessing their ability to remain expressive in conversations. Our findings\nindicate that LLMs are capable of generating and understanding expressive\ncontent, however, with some limitations. These insights will inform the future\ndevelopment and deployment of expressive LLMs. We provide the code for\nExpressivityArena alongside our paper.", "published": "2024-11-12 18:35:28", "link": "http://arxiv.org/abs/2411.08010v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Large Language Models Can Self-Improve in Long-context Reasoning", "abstract": "Large language models (LLMs) have achieved substantial progress in processing\nlong contexts but still struggle with long-context reasoning. Existing\napproaches typically involve fine-tuning LLMs with synthetic data, which\ndepends on annotations from human experts or advanced models like GPT-4, thus\nrestricting further advancements. To address this issue, we investigate the\npotential for LLMs to self-improve in long-context reasoning and propose \\ours,\nan approach specifically designed for this purpose. This approach is\nstraightforward: we sample multiple outputs for each question, score them with\nMinimum Bayes Risk, and then apply supervised fine-tuning or preference\noptimization based on these outputs. Extensive experiments on several leading\nLLMs demonstrate the effectiveness of \\ours, with an absolute improvement of\n$4.2$ points for Llama-3.1-8B-Instruct. Furthermore, \\ours achieves superior\nperformance compared to prior approaches that depend on data produced by human\nexperts or advanced models. We anticipate that this work will open new avenues\nfor self-improvement techniques in long-context scenarios, which are essential\nfor the continual advancement of LLMs.", "published": "2024-11-12 19:53:00", "link": "http://arxiv.org/abs/2411.08147v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for\n  Knowledge Graph Completion", "abstract": "The Knowledge Graph Completion~(KGC) task aims to infer the missing entity\nfrom an incomplete triple. Existing embedding-based methods rely solely on\ntriples in the KG, which is vulnerable to specious relation patterns and\nlong-tail entities. On the other hand, text-based methods struggle with the\nsemantic gap between KG triples and natural language. Apart from triples,\nentity contexts (e.g., labels, descriptions, aliases) also play a significant\nrole in augmenting KGs. To address these limitations, we propose KGR3, a\ncontext-enriched framework for KGC. KGR3 is composed of three modules. Firstly,\nthe Retrieval module gathers supporting triples from the KG, collects plausible\ncandidate answers from a base embedding model, and retrieves context for each\nrelated entity. Then, the Reasoning module employs a large language model to\ngenerate potential answers for each query triple. Finally, the Re-ranking\nmodule combines candidate answers from the two modules mentioned above, and\nfine-tunes an LLM to provide the best answer. Extensive experiments on widely\nused datasets demonstrate that KGR3 consistently improves various KGC methods.\nSpecifically, the best variant of KGR3 achieves absolute Hits@1 improvements of\n12.3% and 5.6% on the FB15k237 and WN18RR datasets.", "published": "2024-11-12 20:15:58", "link": "http://arxiv.org/abs/2411.08165v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset", "abstract": "In an effort to mitigate the harms of large language models (LLMs), learning\nfrom human feedback (LHF) has been used to steer LLMs towards outputs that are\nintended to be both less harmful and more helpful. Despite the widespread\nadoption of LHF in practice, the quality of this feedback and its effectiveness\nas a safety mitigation technique remain unclear. This study addresses these\nissues by auditing the widely-used Helpful and Harmless (HH) dataset by\nAnthropic. Our work includes: (1) a thorough investigation of the dataset's\ncontent through both manual and automated evaluation; (2) experiments\ndemonstrating the dataset's impact on models' safety; and (3) an analysis of\nthe 100 most influential papers citing this dataset. Through our audit, we\nshowcase how conceptualization failures and quality issues identified in the HH\ndataset can create additional harms by leading to disparate safety behaviors\nacross demographic groups. Our findings highlight the need for more nuanced,\ncontext-sensitive approaches to safety mitigation in LLMs.", "published": "2024-11-12 23:43:20", "link": "http://arxiv.org/abs/2411.08243v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "BudgetMLAgent: A Cost-Effective LLM Multi-Agent system for Automating\n  Machine Learning Tasks", "abstract": "Large Language Models (LLMs) excel in diverse applications including\ngeneration of code snippets, but often struggle with generating code for\ncomplex Machine Learning (ML) tasks. Although existing LLM single-agent based\nsystems give varying performance depending on the task complexity, they purely\nrely on larger and expensive models such as GPT-4. Our investigation reveals\nthat no-cost and low-cost models such as Gemini-Pro, Mixtral and CodeLlama\nperform far worse than GPT-4 in a single-agent setting. With the motivation of\ndeveloping a cost-efficient LLM based solution for solving ML tasks, we propose\nan LLM Multi-Agent based system which leverages combination of experts using\nprofiling, efficient retrieval of past observations, LLM cascades, and\nask-the-expert calls. Through empirical analysis on ML engineering tasks in the\nMLAgentBench benchmark, we demonstrate the effectiveness of our system, using\nno-cost models, namely Gemini as the base LLM, paired with GPT-4 in cascade and\nexpert to serve occasional ask-the-expert calls for planning. With 94.2\\%\nreduction in the cost (from \\$0.931 per run cost averaged over all tasks for\nGPT-4 single agent system to \\$0.054), our system is able to yield better\naverage success rate of 32.95\\% as compared to GPT-4 single-agent system\nyielding 22.72\\% success rate averaged over all the tasks of MLAgentBench.", "published": "2024-11-12 00:57:30", "link": "http://arxiv.org/abs/2411.07464v2", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG", "68T42", "I.2.1; I.2.2; I.2.5; I.2.7; I.2.8"], "primary_category": "cs.MA"}
{"title": "IdentifyMe: A Challenging Long-Context Mention Resolution Benchmark", "abstract": "Recent evaluations of LLMs on coreference resolution have revealed that\ntraditional output formats and evaluation metrics do not fully capture the\nmodels' referential understanding. To address this, we introduce IdentifyMe, a\nnew benchmark for mention resolution presented in a multiple-choice question\n(MCQ) format, commonly used for evaluating LLMs. IdentifyMe features long\nnarratives and employs heuristics to exclude easily identifiable mentions,\ncreating a more challenging task. The benchmark also consists of a curated\nmixture of different mention types and corresponding entities, allowing for a\nfine-grained analysis of model performance. We evaluate both closed- and open\nsource LLMs on IdentifyMe and observe a significant performance gap (20-30%)\nbetween the state-of-the-art sub-10B open models vs. closed ones. We observe\nthat pronominal mentions, which have limited surface information, are typically\nmuch harder for models to resolve than nominal mentions. Additionally, we find\nthat LLMs often confuse entities when their mentions overlap in nested\nstructures. The highest-scoring model, GPT-4o, achieves 81.9% accuracy,\nhighlighting the strong referential capabilities of state-of-the-art LLMs while\nalso indicating room for further improvement.", "published": "2024-11-12 01:05:55", "link": "http://arxiv.org/abs/2411.07466v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "SecEncoder: Logs are All You Need in Security", "abstract": "Large and Small Language Models (LMs) are typically pretrained using\nextensive volumes of text, which are sourced from publicly accessible platforms\nsuch as Wikipedia, Book Corpus, or through web scraping. These models, due to\ntheir exposure to a wide range of language data, exhibit impressive\ngeneralization capabilities and can perform a multitude of tasks\nsimultaneously. However, they often fall short when it comes to domain-specific\ntasks due to their broad training data. This paper introduces SecEncoder, a\nspecialized small language model that is pretrained using security logs.\nSecEncoder is designed to address the domain-specific limitations of general\nLMs by focusing on the unique language and patterns found in security logs.\nExperimental results indicate that SecEncoder outperforms other LMs, such as\nBERTlarge, DeBERTa-v3-large and OpenAI's Embedding (textembedding-ada-002)\nmodels, which are pretrained mainly on natural language, across various tasks.\nFurthermore, although SecEncoder is primarily pretrained on log data, it\noutperforms models pretrained on natural language for a range of tasks beyond\nlog analysis, such as incident prioritization and threat intelligence document\nretrieval. This suggests that domain specific pretraining with logs can\nsignificantly enhance the performance of LMs in security. These findings pave\nthe way for future research into security-specific LMs and their potential\napplications.", "published": "2024-11-12 03:56:07", "link": "http://arxiv.org/abs/2411.07528v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Contrastive Language Prompting to Ease False Positives in Medical\n  Anomaly Detection", "abstract": "A pre-trained visual-language model, contrastive language-image pre-training\n(CLIP), successfully accomplishes various downstream tasks with text prompts,\nsuch as finding images or localizing regions within the image. Despite CLIP's\nstrong multi-modal data capabilities, it remains limited in specialized\nenvironments, such as medical applications. For this purpose, many CLIP\nvariants-i.e., BioMedCLIP, and MedCLIP-SAMv2-have emerged, but false positives\nrelated to normal regions persist. Thus, we aim to present a simple yet\nimportant goal of reducing false positives in medical anomaly detection. We\nintroduce a Contrastive LAnguage Prompting (CLAP) method that leverages both\npositive and negative text prompts. This straightforward approach identifies\npotential lesion regions by visual attention to the positive prompts in the\ngiven image. To reduce false positives, we attenuate attention on normal\nregions using negative prompts. Extensive experiments with the BMAD dataset,\nincluding six biomedical benchmarks, demonstrate that CLAP method enhances\nanomaly detection performance. Our future plans include developing an automated\nfine prompting method for more practical usage.", "published": "2024-11-12 04:50:10", "link": "http://arxiv.org/abs/2411.07546v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Entropy Controllable Direct Preference Optimization", "abstract": "In the post-training of large language models (LLMs), Reinforcement Learning\nfrom Human Feedback (RLHF) is an effective approach to achieve generation\naligned with human preferences. Direct Preference Optimization (DPO) allows for\npolicy training with a simple binary cross-entropy loss without a reward model.\nThe objective of DPO is regularized by reverse KL divergence that encourages\nmode-seeking fitting to the reference policy. Nonetheless, we indicate that\nminimizing reverse KL divergence could fail to capture a mode of the reference\ndistribution, which may hurt the policy's performance. Based on this\nobservation, we propose a simple modification to DPO, H-DPO, which allows for\ncontrol over the entropy of the resulting policy, enhancing the distribution's\nsharpness and thereby enabling mode-seeking fitting more effectively. In our\nexperiments, we show that H-DPO outperformed DPO across various tasks,\ndemonstrating superior results in pass@$k$ evaluations for mathematical tasks.\nMoreover, H-DPO is simple to implement, requiring only minor modifications to\nthe loss calculation of DPO, which makes it highly practical and promising for\nwide-ranging applications in the training of LLMs.", "published": "2024-11-12 07:09:44", "link": "http://arxiv.org/abs/2411.07595v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Circuit Complexity Bounds for RoPE-based Transformer Architecture", "abstract": "Characterizing the express power of the Transformer architecture is critical\nto understanding its capacity limits and scaling law. Recent works provide the\ncircuit complexity bounds to Transformer-like architecture. On the other hand,\nRotary Position Embedding ($\\mathsf{RoPE}$) has emerged as a crucial technique\nin modern large language models, offering superior performance in capturing\npositional information compared to traditional position embeddings, which shows\ngreat potential in application prospects, particularly for the long context\nscenario. Empirical evidence also suggests that $\\mathsf{RoPE}$-based\nTransformer architectures demonstrate greater generalization capabilities\ncompared to conventional Transformer models. In this work, we establish a\ncircuit complexity bound for Transformers with $\\mathsf{RoPE}$ attention. Our\nkey contribution is that we show that unless $\\mathsf{TC}^0 = \\mathsf{NC}^1$, a\n$\\mathsf{RoPE}$-based Transformer with $\\mathrm{poly}(n)$-precision, $O(1)$\nlayers, hidden dimension $d \\leq O(n)$ cannot solve the Arithmetic formula\nevaluation problem or the Boolean formula value problem. This result\nsignificantly demonstrates the fundamental limitation of the expressivity of\nthe $\\mathsf{RoPE}$-based Transformer architecture, although it achieves giant\nempirical success. Our theoretical result not only establishes the complexity\nbound but also may instruct further work on the $\\mathsf{RoPE}$-based\nTransformer.", "published": "2024-11-12 07:24:41", "link": "http://arxiv.org/abs/2411.07602v2", "categories": ["cs.LG", "cs.AI", "cs.CC", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Spider 2.0: Evaluating Language Models on Real-World Enterprise\n  Text-to-SQL Workflows", "abstract": "Real-world enterprise text-to-SQL workflows often involve complex cloud or\nlocal data across various database systems, multiple SQL queries in various\ndialects, and diverse operations from data transformation to analytics. We\nintroduce Spider 2.0, an evaluation framework comprising 632 real-world\ntext-to-SQL workflow problems derived from enterprise-level database use cases.\nThe databases in Spider 2.0 are sourced from real data applications, often\ncontaining over 1,000 columns and stored in local or cloud database systems\nsuch as BigQuery and Snowflake. We show that solving problems in Spider 2.0\nfrequently requires understanding and searching through database metadata,\ndialect documentation, and even project-level codebases. This challenge calls\nfor models to interact with complex SQL workflow environments, process\nextremely long contexts, perform intricate reasoning, and generate multiple SQL\nqueries with diverse operations, often exceeding 100 lines, which goes far\nbeyond traditional text-to-SQL challenges. Our evaluations indicate that based\non o1-preview, our code agent framework successfully solves only 21.3% of the\ntasks, compared with 91.2% on Spider 1.0 and 73.0% on BIRD. Our results on\nSpider 2.0 show that while language models have demonstrated remarkable\nperformance in code generation -- especially in prior text-to-SQL benchmarks --\nthey require significant improvement in order to achieve adequate performance\nfor real-world enterprise usage. Progress on Spider 2.0 represents crucial\nsteps towards developing intelligent, autonomous, code agents for real-world\nenterprise settings. Our code, baseline models, and data are available at\nhttps://spider2-sql.github.io", "published": "2024-11-12 12:52:17", "link": "http://arxiv.org/abs/2411.07763v2", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Pointwise Mutual Information as a Performance Gauge for\n  Retrieval-Augmented Generation", "abstract": "Recent work suggests that large language models enhanced with\nretrieval-augmented generation are easily influenced by the order, in which the\nretrieved documents are presented to the model when solving tasks such as\nquestion answering (QA). However, there is no method to date that exploits this\nphenomenon to improve generation. We fill this gap. In this study, we show that\nthe pointwise mutual information between a context and a question is an\neffective gauge for language model performance. Importantly, this gauge does\nnot depend on knowing the answer to the question a priori. Through experiments\non two question-answering datasets and a variety of large language models, we\nfind evidence for an empirical correlation between answer accuracy and\npointwise mutual information. Additionally, we propose two methods that use the\npointwise mutual information between a document and a question as a gauge for\nselecting and constructing prompts that lead to better performance, whose\neffectiveness we demonstrate through experimentation.", "published": "2024-11-12 13:14:09", "link": "http://arxiv.org/abs/2411.07773v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Ethical Concern Identification in NLP: A Corpus of ACL Anthology Ethics\n  Statements", "abstract": "What ethical concerns, if any, do LLM researchers have? We introduce EthiCon,\na corpus of 1,580 ethical concern statements extracted from scientific papers\npublished in the ACL Anthology. We extract ethical concern keywords from the\nstatements and show promising results in automating the concern identification\nprocess. Through a survey, we compare the ethical concerns of the corpus to the\nconcerns listed by the general public and professionals in the field. Finally,\nwe compare our retrieved ethical concerns with existing taxonomies pointing to\ngaps and future research directions.", "published": "2024-11-12 14:53:12", "link": "http://arxiv.org/abs/2411.07845v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Tucano: Advancing Neural Text Generation for Portuguese", "abstract": "Significant advances have been made in natural language processing in recent\nyears. However, our current deep learning approach to language modeling\nrequires substantial resources in terms of data and computation. One of the\nside effects of this data-hungry paradigm is the current schism between\nlanguages, separating those considered high-resource, where most of the\ndevelopment happens and resources are available, and the low-resource ones,\nwhich struggle to attain the same level of performance and autonomy. This study\naims to introduce a new set of resources to stimulate the future development of\nneural text generation in Portuguese. In this work, we document the development\nof GigaVerbo, a concatenation of deduplicated Portuguese text corpora amounting\nto 200 billion tokens. Via this corpus, we trained a series of\ndecoder-transformers named Tucano. Our models perform equal or superior to\nother Portuguese and multilingual language models of similar size in several\nPortuguese benchmarks. The evaluation of our models also reveals that model\nperformance on many currently available benchmarks used by the Portuguese NLP\ncommunity has little to no correlation with the scaling of token ingestion\nduring training, highlighting the limitations of such evaluations when it comes\nto the assessment of Portuguese generative language models. All derivatives of\nour study are openly released on GitHub and Hugging Face. See\nhttps://nkluge-correa.github.io/Tucano/", "published": "2024-11-12 15:06:06", "link": "http://arxiv.org/abs/2411.07854v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified\n  Multimodal Understanding and Generation", "abstract": "We present JanusFlow, a powerful framework that unifies image understanding\nand generation in a single model. JanusFlow introduces a minimalist\narchitecture that integrates autoregressive language models with rectified\nflow, a state-of-the-art method in generative modeling. Our key finding\ndemonstrates that rectified flow can be straightforwardly trained within the\nlarge language model framework, eliminating the need for complex architectural\nmodifications. To further improve the performance of our unified model, we\nadopt two key strategies: (i) decoupling the understanding and generation\nencoders, and (ii) aligning their representations during unified training.\nExtensive experiments show that JanusFlow achieves comparable or superior\nperformance to specialized models in their respective domains, while\nsignificantly outperforming existing unified approaches across standard\nbenchmarks. This work represents a step toward more efficient and versatile\nvision-language models.", "published": "2024-11-12 17:55:10", "link": "http://arxiv.org/abs/2411.07975v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Derivational Morphology Reveals Analogical Generalization in Large\n  Language Models", "abstract": "What mechanisms underlie linguistic generalization in large language models\n(LLMs)? This question has attracted considerable attention, with most studies\nanalyzing the extent to which the language skills of LLMs resemble rules. As of\nyet, it is not known whether linguistic generalization in LLMs could equally\nwell be explained as the result of analogical processes, which can be\nformalized as similarity operations on stored exemplars. A key shortcoming of\nprior research is its focus on linguistic phenomena with a high degree of\nregularity, for which rule-based and analogical approaches make the same\npredictions. Here, we instead examine derivational morphology, specifically\nEnglish adjective nominalization, which displays notable variability. We\nintroduce a new method for investigating linguistic generalization in LLMs:\nfocusing on GPT-J, we fit cognitive models that instantiate rule-based and\nanalogical learning to the LLM training data and compare their predictions on a\nset of nonce adjectives with those of the LLM, allowing us to draw direct\nconclusions regarding underlying mechanisms. As expected, rule-based and\nanalogical models explain the predictions of GPT-J equally well for adjectives\nwith regular nominalization patterns. However, for adjectives with variable\nnominalization patterns, the analogical model provides a much better match.\nFurthermore, GPT-J's behavior is sensitive to the individual word frequencies,\neven for regular forms, a behavior that is consistent with an analogical\naccount of regular forms but not a rule-based one. These findings refute the\nhypothesis that GPT-J's linguistic generalization on adjective nominalization\ninvolves rules, suggesting similarity operations on stored exemplars as the\nunderlying mechanism. Overall, our study suggests that analogical processes\nplay a bigger role in the linguistic generalization of LLMs than previously\nthought.", "published": "2024-11-12 18:15:19", "link": "http://arxiv.org/abs/2411.07990v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can adversarial attacks by large language models be attributed?", "abstract": "Attributing outputs from Large Language Models (LLMs) in adversarial\nsettings-such as cyberattacks and disinformation-presents significant\nchallenges that are likely to grow in importance. We investigate this\nattribution problem using formal language theory, specifically language\nidentification in the limit as introduced by Gold and extended by Angluin. By\nmodeling LLM outputs as formal languages, we analyze whether finite text\nsamples can uniquely pinpoint the originating model. Our results show that due\nto the non-identifiability of certain language classes, under some mild\nassumptions about overlapping outputs from fine-tuned models it is\ntheoretically impossible to attribute outputs to specific LLMs with certainty.\nThis holds also when accounting for expressivity limitations of Transformer\narchitectures. Even with direct model access or comprehensive monitoring,\nsignificant computational hurdles impede attribution efforts. These findings\nhighlight an urgent need for proactive measures to mitigate risks posed by\nadversarial LLM use as their influence continues to expand.", "published": "2024-11-12 18:28:57", "link": "http://arxiv.org/abs/2411.08003v1", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.FL"], "primary_category": "cs.AI"}
{"title": "Deceiving Question-Answering Models: A Hybrid Word-Level Adversarial\n  Approach", "abstract": "Deep learning underpins most of the currently advanced natural language\nprocessing (NLP) tasks such as textual classification, neural machine\ntranslation (NMT), abstractive summarization and question-answering (QA).\nHowever, the robustness of the models, particularly QA models, against\nadversarial attacks is a critical concern that remains insufficiently explored.\nThis paper introduces QA-Attack (Question Answering Attack), a novel word-level\nadversarial strategy that fools QA models. Our attention-based attack exploits\nthe customized attention mechanism and deletion ranking strategy to identify\nand target specific words within contextual passages. It creates deceptive\ninputs by carefully choosing and substituting synonyms, preserving grammatical\nintegrity while misleading the model to produce incorrect responses. Our\napproach demonstrates versatility across various question types, particularly\nwhen dealing with extensive long textual inputs. Extensive experiments on\nmultiple benchmark datasets demonstrate that QA-Attack successfully deceives\nbaseline QA models and surpasses existing adversarial techniques regarding\nsuccess rate, semantics changes, BLEU score, fluency and grammar error rate.", "published": "2024-11-12 23:54:58", "link": "http://arxiv.org/abs/2411.08248v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Role of Speech Data in Reducing Toxicity Detection Bias", "abstract": "Text toxicity detection systems exhibit significant biases, producing\ndisproportionate rates of false positives on samples mentioning demographic\ngroups. But what about toxicity detection in speech? To investigate the extent\nto which text-based biases are mitigated by speech-based systems, we produce a\nset of high-quality group annotations for the multilingual MuTox dataset, and\nthen leverage these annotations to systematically compare speech- and\ntext-based toxicity classifiers. Our findings indicate that access to speech\ndata during inference supports reduced bias against group mentions,\nparticularly for ambiguous and disagreement-inducing samples. Our results also\nsuggest that improving classifiers, rather than transcription pipelines, is\nmore helpful for reducing group bias. We publicly release our annotations and\nprovide recommendations for future toxicity dataset construction.", "published": "2024-11-12 19:26:43", "link": "http://arxiv.org/abs/2411.08135v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Automatic Album Sequencing", "abstract": "Album sequencing is a critical part of the album production process.\nRecently, a data-driven approach was proposed that sequences general\ncollections of independent media by extracting the narrative essence of the\nitems in the collections. While this approach implies an album sequencing\ntechnique, it is not widely accessible to a less technical audience, requiring\nadvanced knowledge of machine learning techniques to use. To address this, we\nintroduce a new user-friendly web-based tool that allows a less technical\naudience to upload music tracks, execute this technique in one click, and\nsubsequently presents the result in a clean visualization to the user. To both\nincrease the number of templates available to the user and address shortcomings\nof previous work, we also introduce a new direct transformer-based album\nsequencing method. We find that our more direct method outperforms a random\nbaseline but does not reach the same performance as the narrative essence\napproach. Both methods are included in our web-based user interface, and this\n-- alongside a full copy of our implementation -- is publicly available at\nhttps://github.com/dylanashley/automatic-album-sequencing", "published": "2024-11-12 13:13:20", "link": "http://arxiv.org/abs/2411.07772v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MM", "cs.SD", "eess.AS", "68T07", "H.5.5; I.2.6; I.5.1; J.5"], "primary_category": "cs.LG"}
{"title": "Language Models as Causal Effect Generators", "abstract": "We present a framework for large language model (LLM) based data generation\nwith controllable causal structure. In particular, we define a procedure for\nturning any language model and any directed acyclic graph (DAG) into a\nsequence-driven structural causal model (SD-SCM). Broadly speaking, an SD-SCM\nis a causal model with user-defined structure and LLM-defined structural\nequations. We characterize how an SD-SCM allows sampling from observational,\ninterventional, and counterfactual distributions according to the desired\ncausal structure. We then leverage this procedure to propose a new type of\nbenchmark for causal inference methods, generating individual-level\ncounterfactual data without needing to manually specify functional\nrelationships between variables. We create an example benchmark consisting of\nthousands of datasets, and test a suite of popular estimation methods on these\ndatasets for average, conditional average, and individual treatment effect\nestimation, both with and without hidden confounding. Apart from generating\ndata, the same procedure also allows us to test for the presence of a causal\neffect that might be encoded in an LLM. This procedure can underpin auditing\nLLMs for misinformation, discrimination, or otherwise undesirable behavior. We\nbelieve SD-SCMs can serve as a useful tool in any application that would\nbenefit from sequential data with controllable causal structure.", "published": "2024-11-12 18:50:35", "link": "http://arxiv.org/abs/2411.08019v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.AP", "stat.ME", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Study on Inter and Intra Speaker Variability in Speaker Recognition", "abstract": "Optimization of a trade-off between the number of speakers and their temporal\nvariability (or session diversity) is crucial for the development of a speaker\nrecognition system together with making the data collection process feasible\nfrom a time perspective. In this article, we provide the analysis of dependency\nbetween inter and intra speaker variability in training data for the modern\nneural network-based speaker recognition system using the VoxTube dataset for\ntext-independent speaker recognition task. Besides, an auxiliary contribution\nof this work is a release of upload date metadata per utterance in a VoxTube\ndataset. We want this article to contribute to guidelines and best practices\nfor collecting and filtering data from media hosting platforms to facilitate\nthe efforts of researchers in developing speaker recognition systems.", "published": "2024-11-12 12:25:17", "link": "http://arxiv.org/abs/2411.07754v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Generalist Audio Foundation Model for Comprehensive Body Sound\n  Auscultation", "abstract": "Accurate and efficient auscultation-based diagnostics are vital for early\ndisease detection, especially in resource-limited settings where specialized\nclinical expertise is scarce. Traditional auscultation, which heavily depends\non clinician experience, suffers from significant inter-observer variability,\nwhile existing AI models often falter due to the limitations of\nnon-representative training data. In this study, we introduce AuscultaBase, a\nnovel AI-driven diagnostic framework that harnesses self-supervised and\ncontrastive learning techniques alongside large-scale, multi-source data\nintegration to advance body sound analysis. By generating robust feature\nrepresentations, AuscultaBase markedly enhances performance in abnormality\ndetection, disease classification, and activity recognition tasks.\nComprehensive evaluations on our newly established benchmark, AuscultaBench,\ndemonstrate that AuscultaBase consistently outperforms state-of-the-art methods\nacross key performance metrics, underscoring its potential as a scalable and\ncost-effective tool for clinical screening and early disease intervention. The\ncode and model checkpoint has been released in\nhttps://github.com/applewpj/AuscultaBase.", "published": "2024-11-12 04:50:33", "link": "http://arxiv.org/abs/2411.07547v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CJST: CTC Compressor based Joint Speech and Text Training for\n  Decoder-Only ASR", "abstract": "CTC compressor can be an effective approach to integrate audio encoders to\ndecoder-only models, which has gained growing interest for different speech\napplications. In this work, we propose a novel CTC compressor based joint\nspeech and text training (CJST) framework for decoder-only ASR. CJST matches\nspeech and text modalities from both directions by exploring a simple modality\nadaptor and several features of the CTC compressor, including sequence\ncompression, on-the-fly forced peaky alignment and CTC class embeddings.\nExperimental results on the Librispeech and TED-LIUM2 corpora show that the\nproposed CJST achieves an effective text injection without the need of duration\nhandling, leading to the best performance for both in-domain and cross-domain\nscenarios. We also provide a comprehensive study on CTC compressor, covering\nvarious compression modes, edge case handling and behavior under both clean and\nnoisy data conditions, which reveals the most robust setting to use CTC\ncompressor for decoder-only models.", "published": "2024-11-12 07:30:29", "link": "http://arxiv.org/abs/2411.07607v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Investigating the Effectiveness of Explainability Methods in Parkinson's\n  Detection from Speech", "abstract": "Speech impairments in Parkinson's disease (PD) provide significant early\nindicators for diagnosis. While models for speech-based PD detection have shown\nstrong performance, their interpretability remains underexplored. This study\nsystematically evaluates several explainability methods to identify PD-specific\nspeech features, aiming to support the development of accurate, interpretable\nmodels for clinical decision-making in PD diagnosis and monitoring. Our\nmethodology involves (i) obtaining attributions and saliency maps using\nmainstream interpretability techniques, (ii) quantitatively evaluating the\nfaithfulness of these maps and their combinations obtained via union and\nintersection through a range of established metrics, and (iii) assessing the\ninformation conveyed by the saliency maps for PD detection from an auxiliary\nclassifier. Our results reveal that, while explanations are aligned with the\nclassifier, they often fail to provide valuable information for domain experts.", "published": "2024-11-12 18:43:27", "link": "http://arxiv.org/abs/2411.08013v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SoundSil-DS: Deep Denoising and Segmentation of Sound-field Images with\n  Silhouettes", "abstract": "Development of optical technology has enabled imaging of two-dimensional (2D)\nsound fields. This acousto-optic sensing enables understanding of the\ninteraction between sound and objects such as reflection and diffraction.\nMoreover, it is expected to be used an advanced measurement technology for\nsonars in self-driving vehicles and assistive robots. However, the low\nsound-pressure sensitivity of the acousto-optic sensing results in high\nintensity of noise on images. Therefore, denoising is an essential task to\nvisualize and analyze the sound fields. In addition to denoising, segmentation\nof sound and object silhouette is also required to analyze interactions between\nthem. In this paper, we propose sound-field-images-with-object-silhouette\ndenoising and segmentation (SoundSil-DS) that jointly perform denoising and\nsegmentation for sound fields and object silhouettes on a visualized image. We\ndeveloped a new model based on the current state-of-the-art denoising network.\nWe also created a dataset to train and evaluate the proposed method through\nacoustic simulation. The proposed method was evaluated using both simulated and\nmeasured data. We confirmed that our method can applied to experimentally\nmeasured data. These results suggest that the proposed method may improve the\npost-processing for sound fields, such as physical model-based\nthree-dimensional reconstruction since it can remove unwanted noise and\nseparate sound fields and other object silhouettes. Our code is available at\nhttps://github.com/nttcslab/soundsil-ds.", "published": "2024-11-12 03:29:06", "link": "http://arxiv.org/abs/2411.07517v1", "categories": ["eess.SP", "cs.SD", "eess.AS", "eess.IV", "physics.optics"], "primary_category": "eess.SP"}
{"title": "SAV-SE: Scene-aware Audio-Visual Speech Enhancement with Selective State\n  Space Model", "abstract": "Speech enhancement plays an essential role in various applications, and the\nintegration of visual information has been demonstrated to bring substantial\nadvantages. However, the majority of current research concentrates on the\nexamination of facial and lip movements, which can be compromised or entirely\ninaccessible in scenarios where occlusions occur or when the camera view is\ndistant. Whereas contextual visual cues from the surrounding environment have\nbeen overlooked: for example, when we see a dog bark, our brain has the innate\nability to discern and filter out the barking noise. To this end, in this\npaper, we introduce a novel task, i.e. SAV-SE. To our best knowledge, this is\nthe first proposal to use rich contextual information from synchronized video\nas auxiliary cues to indicate the type of noise, which eventually improves the\nspeech enhancement performance. Specifically, we propose the VC-S$^2$E method,\nwhich incorporates the Conformer and Mamba modules for their complementary\nstrengths. Extensive experiments are conducted on public MUSIC, AVSpeech and\nAudioSet datasets, where the results demonstrate the superiority of VC-S$^2$E\nover other competitive methods. We will make the source code publicly\navailable. Project demo page: https://AVSEPage.github.io/", "published": "2024-11-12 12:23:41", "link": "http://arxiv.org/abs/2411.07751v2", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
