{"title": "Later-stage Minimum Bayes-Risk Decoding for Neural Machine Translation", "abstract": "For extended periods of time, sequence generation models rely on beam search\nalgorithm to generate output sequence. However, the correctness of beam search\ndegrades when the a model is over-confident about a suboptimal prediction. In\nthis paper, we propose to perform minimum Bayes-risk (MBR) decoding for some\nextra steps at a later stage. In order to speed up MBR decoding, we compute the\nBayes risks on GPU in batch mode. In our experiments, we found that MBR\nreranking works with a large beam size. Later-stage MBR decoding is shown to\noutperform simple MBR reranking in machine translation tasks.", "published": "2017-04-11 06:48:45", "link": "http://arxiv.org/abs/1704.03169v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Keyword Extraction for Text Summarization: A Survey", "abstract": "In recent times, data is growing rapidly in every domain such as news, social\nmedia, banking, education, etc. Due to the excessiveness of data, there is a\nneed of automatic summarizer which will be capable to summarize the data\nespecially textual data in original document without losing any critical\npurposes. Text summarization is emerged as an important research area in recent\npast. In this regard, review of existing work on text summarization process is\nuseful for carrying out further research. In this paper, recent literature on\nautomatic keyword extraction and text summarization are presented since text\nsummarization process is highly depend on keyword extraction. This literature\nincludes the discussion about different methodology used for keyword extraction\nand text summarization. It also discusses about different databases used for\ntext summarization in several domains along with evaluation matrices. Finally,\nit discusses briefly about issues and research challenges faced by researchers\nalong with future direction.", "published": "2017-04-11 11:20:19", "link": "http://arxiv.org/abs/1704.03242v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unfolding and Shrinking Neural Machine Translation Ensembles", "abstract": "Ensembling is a well-known technique in neural machine translation (NMT) to\nimprove system performance. Instead of a single neural net, multiple neural\nnets with the same topology are trained separately, and the decoder generates\npredictions by averaging over the individual models. Ensembling often improves\nthe quality of the generated translations drastically. However, it is not\nsuitable for production systems because it is cumbersome and slow. This work\naims to reduce the runtime to be on par with a single system without\ncompromising the translation quality. First, we show that the ensemble can be\nunfolded into a single large neural network which imitates the output of the\nensemble system. We show that unfolding can already improve the runtime in\npractice since more work can be done on the GPU. We proceed by describing a set\nof techniques to shrink the unfolded network by reducing the dimensionality of\nlayers. On Japanese-English we report that the resulting network has the size\nand decoding speed of a single NMT network but performs on the level of a\n3-ensemble system.", "published": "2017-04-11 13:27:00", "link": "http://arxiv.org/abs/1704.03279v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What do Neural Machine Translation Models Learn about Morphology?", "abstract": "Neural machine translation (MT) models obtain state-of-the-art performance\nwhile maintaining a simple, end-to-end architecture. However, little is known\nabout what these models learn about source and target languages during the\ntraining process. In this work, we analyze the representations learned by\nneural MT models at various levels of granularity and empirically evaluate the\nquality of the representations for learning morphology through extrinsic\npart-of-speech and morphological tagging tasks. We conduct a thorough\ninvestigation along several parameters: word-based vs. character-based\nrepresentations, depth of the encoding layer, the identity of the target\nlanguage, and encoder vs. decoder representations. Our data-driven,\nquantitative evaluation sheds light on important aspects in the neural MT\nsystem and its ability to capture word structure.", "published": "2017-04-11 18:01:07", "link": "http://arxiv.org/abs/1704.03471v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "ConceptNet at SemEval-2017 Task 2: Extending Word Embeddings with\n  Multilingual Relational Knowledge", "abstract": "This paper describes Luminoso's participation in SemEval 2017 Task 2,\n\"Multilingual and Cross-lingual Semantic Word Similarity\", with a system based\non ConceptNet. ConceptNet is an open, multilingual knowledge graph that focuses\non general knowledge that relates the meanings of words and phrases. Our\nsubmission to SemEval was an update of previous work that builds high-quality,\nmultilingual word embeddings from a combination of ConceptNet and\ndistributional semantics. Our system took first place in both subtasks. It\nranked first in 4 out of 5 of the separate languages, and also ranked first in\nall 10 of the cross-lingual language pairs.", "published": "2017-04-11 22:44:35", "link": "http://arxiv.org/abs/1704.03560v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "What we really want to find by Sentiment Analysis: The Relationship\n  between Computational Models and Psychological State", "abstract": "As the first step to model emotional state of a person, we build sentiment\nanalysis models with existing deep neural network algorithms and compare the\nmodels with psychological measurements to enlighten the relationship. In the\nexperiments, we first examined psychological state of 64 participants and asked\nthem to summarize the story of a book, Chronicle of a Death Foretold (Marquez,\n1981). Secondly, we trained models using crawled 365,802 movie review data;\nthen we evaluated participants' summaries using the pretrained model as a\nconcept of transfer learning. With the background that emotion affects on\nmemories, we investigated the relationship between the evaluation score of the\nsummaries from computational models and the examined psychological\nmeasurements. The result shows that although CNN performed the best among other\ndeep neural network algorithms (LSTM, GRU), its results are not related to the\npsychological state. Rather, GRU shows more explainable results depending on\nthe psychological state. The contribution of this paper can be summarized as\nfollows: (1) we enlighten the relationship between computational models and\npsychological measurements. (2) we suggest this framework as objective methods\nto evaluate the emotion; the real sentiment analysis of a person.", "published": "2017-04-11 16:42:55", "link": "http://arxiv.org/abs/1704.03407v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Persian Wordnet Construction using Supervised Learning", "abstract": "This paper presents an automated supervised method for Persian wordnet\nconstruction. Using a Persian corpus and a bi-lingual dictionary, the initial\nlinks between Persian words and Princeton WordNet synsets have been generated.\nThese links will be discriminated later as correct or incorrect by employing\nseven features in a trained classification system. The whole method is just a\nclassification system, which has been trained on a train set containing FarsNet\nas a set of correct instances. State of the art results on the automatically\nderived Persian wordnet is achieved. The resulted wordnet with a precision of\n91.18% includes more than 16,000 words and 22,000 synsets.", "published": "2017-04-11 09:47:28", "link": "http://arxiv.org/abs/1704.03223v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Unsupervised Event Abstraction using Pattern Abstraction and Local\n  Process Models", "abstract": "Process mining analyzes business processes based on events stored in event\nlogs. However, some recorded events may correspond to activities on a very low\nlevel of abstraction. When events are recorded on a too low level of\ngranularity, process discovery methods tend to generate overgeneralizing\nprocess models. Grouping low-level events to higher level activities, i.e.,\nevent abstraction, can be used to discover better process models. Existing\nevent abstraction methods are mainly based on common sub-sequences and\nclustering techniques. In this paper, we propose to first discover local\nprocess models and then use those models to lift the event log to a higher\nlevel of abstraction. Our conjecture is that process models discovered on the\nobtained high-level event log return process models of higher quality: their\nfitness and precision scores are more balanced. We show this with preliminary\nresults on several real-life event logs.", "published": "2017-04-11 20:08:14", "link": "http://arxiv.org/abs/1704.03520v2", "categories": ["cs.DB", "cs.AI", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Leveraging Term Banks for Answering Complex Questions: A Case for Sparse\n  Vectors", "abstract": "While open-domain question answering (QA) systems have proven effective for\nanswering simple questions, they struggle with more complex questions. Our goal\nis to answer more complex questions reliably, without incurring a significant\ncost in knowledge resource construction to support the QA. One readily\navailable knowledge resource is a term bank, enumerating the key concepts in a\ndomain. We have developed an unsupervised learning approach that leverages a\nterm bank to guide a QA system, by representing the terminological knowledge\nwith thousands of specialized vector spaces. In experiments with complex\nscience questions, we show that this approach significantly outperforms several\nstate-of-the-art QA systems, demonstrating that significant leverage can be\ngained from continuous vector representations of domain terminology.", "published": "2017-04-11 21:21:39", "link": "http://arxiv.org/abs/1704.03543v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "H.3.1; I.2.6; I.2.7"], "primary_category": "cs.IR"}
