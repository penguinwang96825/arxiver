{"title": "Okay, Let's Do This! Modeling Event Coreference with Generated\n  Rationales and Knowledge Distillation", "abstract": "In NLP, Event Coreference Resolution (ECR) is the task of connecting event\nclusters that refer to the same underlying real-life event, usually via neural\nsystems. In this work, we investigate using abductive free-text rationales\n(FTRs) generated by modern autoregressive LLMs as distant supervision of\nsmaller student models for cross-document coreference (CDCR) of events. We\nimplement novel rationale-oriented event clustering and knowledge distillation\nmethods for event coreference scoring that leverage enriched information from\nthe FTRs for improved CDCR without additional annotation or expensive document\nclustering. Our model using coreference specific knowledge distillation\nachieves SOTA B3 F1 on the ECB+ and GVC corpora and we establish a new baseline\non the AIDA Phase 1 corpus. Our code can be found at\nhttps://github.com/csu-signal/llama_cdcr", "published": "2024-04-04 04:49:46", "link": "http://arxiv.org/abs/2404.03196v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Document Simplification: On the Importance of Separately\n  Assessing Simplicity and Meaning Preservation", "abstract": "Text simplification intends to make a text easier to read while preserving\nits core meaning. Intuitively and as shown in previous works, these two\ndimensions (simplification and meaning preservation) are often-times inversely\ncorrelated. An overly conservative text will fail to simplify sufficiently,\nwhereas extreme simplification will degrade meaning preservation. Yet, popular\nevaluation metrics either aggregate meaning preservation and simplification\ninto a single score (SARI, LENS), or target meaning preservation alone\n(BERTScore, QuestEval). Moreover, these metrics usually require a set of\nreferences and most previous work has only focused on sentence-level\nsimplification. In this paper, we focus on the evaluation of document-level\ntext simplification and compare existing models using distinct metrics for\nmeaning preservation and simplification. We leverage existing metrics from\nsimilar tasks and introduce a reference-less metric variant for simplicity,\nshowing that models are mostly biased towards either simplification or meaning\npreservation, seldom performing well on both dimensions. Making use of the fact\nthat the metrics we use are all reference-less, we also investigate the\nperformance of existing models when applied to unseen data (where reference\nsimplifications are unavailable).", "published": "2024-04-04 08:04:24", "link": "http://arxiv.org/abs/2404.03278v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing Large Language Models for Scalar Adjective Lexical Semantics and\n  Scalar Diversity Pragmatics", "abstract": "Scalar adjectives pertain to various domain scales and vary in intensity\nwithin each scale (e.g. certain is more intense than likely on the likelihood\nscale). Scalar implicatures arise from the consideration of alternative\nstatements which could have been made. They can be triggered by scalar\nadjectives and require listeners to reason pragmatically about them. Some\nscalar adjectives are more likely to trigger scalar implicatures than others.\nThis phenomenon is referred to as scalar diversity. In this study, we probe\ndifferent families of Large Language Models such as GPT-4 for their knowledge\nof the lexical semantics of scalar adjectives and one specific aspect of their\npragmatics, namely scalar diversity. We find that they encode rich\nlexical-semantic information about scalar adjectives. However, the rich\nlexical-semantic knowledge does not entail a good understanding of scalar\ndiversity. We also compare current models of different sizes and complexities\nand find that larger models are not always better. Finally, we explain our\nprobing results by leveraging linguistic intuitions and model training\nobjectives.", "published": "2024-04-04 08:52:25", "link": "http://arxiv.org/abs/2404.03301v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Easily do Irrelevant Inputs Skew the Responses of Large Language\n  Models?", "abstract": "By leveraging the retrieval of information from external knowledge databases,\nLarge Language Models (LLMs) exhibit enhanced capabilities for accomplishing\nmany knowledge-intensive tasks. However, due to the inherent flaws of current\nretrieval systems, there might exist irrelevant information within those\nretrieving top-ranked passages. In this work, we present a comprehensive\ninvestigation into the robustness of LLMs to different types of irrelevant\ninformation under various conditions. We initially introduce a framework to\nconstruct high-quality irrelevant information that ranges from semantically\nunrelated, partially related, and related to questions. Furthermore, our\nanalysis demonstrates that the constructed irrelevant information not only\nscores highly on similarity metrics, being highly retrieved by existing\nsystems, but also bears semantic connections to the context. Our investigation\nreveals that current LLMs still face challenges in discriminating highly\nsemantically related information and can be easily distracted by these\nirrelevant yet misleading content. Besides, we also find that current solutions\nfor handling irrelevant information have limitations in improving the\nrobustness of LLMs to such distractions. All the resources are available on\nGitHub at https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information.", "published": "2024-04-04 08:52:30", "link": "http://arxiv.org/abs/2404.03302v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comparative Analysis of Word-Level Metric Differential Privacy:\n  Benchmarking The Privacy-Utility Trade-off", "abstract": "The application of Differential Privacy to Natural Language Processing\ntechniques has emerged in relevance in recent years, with an increasing number\nof studies published in established NLP outlets. In particular, the adaptation\nof Differential Privacy for use in NLP tasks has first focused on the\n$\\textit{word-level}$, where calibrated noise is added to word embedding\nvectors to achieve \"noisy\" representations. To this end, several\nimplementations have appeared in the literature, each presenting an alternative\nmethod of achieving word-level Differential Privacy. Although each of these\nincludes its own evaluation, no comparative analysis has been performed to\ninvestigate the performance of such methods relative to each other. In this\nwork, we conduct such an analysis, comparing seven different algorithms on two\nNLP tasks with varying hyperparameters, including the $\\textit{epsilon\n($\\varepsilon$)}$ parameter, or privacy budget. In addition, we provide an\nin-depth analysis of the results with a focus on the privacy-utility trade-off,\nas well as open-source our implementation code for further reproduction. As a\nresult of our analysis, we give insight into the benefits and challenges of\nword-level Differential Privacy, and accordingly, we suggest concrete steps\nforward for the research field.", "published": "2024-04-04 09:48:14", "link": "http://arxiv.org/abs/2404.03324v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Schroedinger's Threshold: When the AUC doesn't predict Accuracy", "abstract": "The Area Under Curve measure (AUC) seems apt to evaluate and compare diverse\nmodels, possibly without calibration. An important example of AUC application\nis the evaluation and benchmarking of models that predict faithfulness of\ngenerated text. But we show that the AUC yields an academic and optimistic\nnotion of accuracy that can misalign with the actual accuracy observed in\napplication, yielding significant changes in benchmark rankings. To paint a\nmore realistic picture of downstream model performance (and prepare a model for\nactual application), we explore different calibration modes, testing\ncalibration data and method.", "published": "2024-04-04 10:18:03", "link": "http://arxiv.org/abs/2404.03344v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Pareto Optimal Throughput in Small Language Model Serving", "abstract": "Large language models (LLMs) have revolutionized the state-of-the-art of many\ndifferent natural language processing tasks. Although serving LLMs is\ncomputationally and memory demanding, the rise of Small Language Models (SLMs)\noffers new opportunities for resource-constrained users, who now are able to\nserve small models with cutting-edge performance. In this paper, we present a\nset of experiments designed to benchmark SLM inference at performance and\nenergy levels. Our analysis provides a new perspective in serving, highlighting\nthat the small memory footprint of SLMs allows for reaching the Pareto-optimal\nthroughput within the resource capacity of a single accelerator. In this\nregard, we present an initial set of findings demonstrating how model\nreplication can effectively improve resource utilization for serving SLMs.", "published": "2024-04-04 10:45:07", "link": "http://arxiv.org/abs/2404.03353v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "nicolay-r at SemEval-2024 Task 3: Using Flan-T5 for Reasoning Emotion\n  Cause in Conversations with Chain-of-Thought on Emotion States", "abstract": "Emotion expression is one of the essential traits of conversations. It may be\nself-related or caused by another speaker. The variety of reasons may serve as\na source of the further emotion causes: conversation history, speaker's\nemotional state, etc. Inspired by the most recent advances in Chain-of-Thought,\nin this work, we exploit the existing three-hop reasoning approach (THOR) to\nperform large language model instruction-tuning for answering: emotion states\n(THOR-state), and emotion caused by one speaker to the other (THOR-cause). We\nequip THOR-cause with the reasoning revision (rr) for devising a reasoning path\nin fine-tuning. In particular, we rely on the annotated speaker emotion states\nto revise reasoning path. Our final submission, based on Flan-T5-base (250M)\nand the rule-based span correction technique, preliminary tuned with THOR-state\nand fine-tuned with THOR-cause-rr on competition training data, results in 3rd\nand 4th places (F1-proportional) and 5th place (F1-strict) among 15\nparticipating teams. Our THOR implementation fork is publicly available:\nhttps://github.com/nicolay-r/THOR-ECAC", "published": "2024-04-04 11:03:33", "link": "http://arxiv.org/abs/2404.03361v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Plan and Generate Text with Citations", "abstract": "The increasing demand for the deployment of LLMs in information-seeking\nscenarios has spurred efforts in creating verifiable systems, which generate\nresponses to queries along with supporting evidence. In this paper, we explore\nthe attribution capabilities of plan-based models which have been recently\nshown to improve the faithfulness, grounding, and controllability of generated\ntext. We conceptualize plans as a sequence of questions which serve as\nblueprints of the generated content and its organization. We propose two\nattribution models that utilize different variants of blueprints, an\nabstractive model where questions are generated from scratch, and an extractive\nmodel where questions are copied from the input. Experiments on long-form\nquestion-answering show that planning consistently improves attribution\nquality. Moreover, the citations generated by blueprint models are more\naccurate compared to those obtained from LLM-based pipelines lacking a planning\ncomponent.", "published": "2024-04-04 11:27:54", "link": "http://arxiv.org/abs/2404.03381v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Edisum: Summarizing and Explaining Wikipedia Edits at Scale", "abstract": "An edit summary is a succinct comment written by a Wikipedia editor\nexplaining the nature of, and reasons for, an edit to a Wikipedia page. Edit\nsummaries are crucial for maintaining the encyclopedia: they are the first\nthing seen by content moderators and they help them decide whether to accept or\nreject an edit. Additionally, edit summaries constitute a valuable data source\nfor researchers. Unfortunately, as we show, for many edits, summaries are\neither missing or incomplete. To overcome this problem and help editors write\nuseful edit summaries, we propose a model for recommending edit summaries\ngenerated by a language model trained to produce good edit summaries given the\nrepresentation of an edit diff. To overcome the challenges of mixed-quality\ntraining data and efficiency requirements imposed by the scale of Wikipedia, we\nfine-tune a small generative language model on a curated mix of human and\nsynthetic data. Our model performs on par with human editors. Commercial large\nlanguage models are able to solve this task better than human editors, but are\nnot well suited for Wikipedia, while open-source ones fail on this task. More\nbroadly, we showcase how language modeling technology can be used to support\nhumans in maintaining one of the largest and most visible projects on the Web.", "published": "2024-04-04 13:15:28", "link": "http://arxiv.org/abs/2404.03428v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scaffolding Language Learning via Multi-modal Tutoring Systems with\n  Pedagogical Instructions", "abstract": "Intelligent tutoring systems (ITSs) that imitate human tutors and aim to\nprovide immediate and customized instructions or feedback to learners have\nshown their effectiveness in education. With the emergence of generative\nartificial intelligence, large language models (LLMs) further entitle the\nsystems to complex and coherent conversational interactions. These systems\nwould be of great help in language education as it involves developing skills\nin communication, which, however, drew relatively less attention. Additionally,\ndue to the complicated cognitive development at younger ages, more endeavors\nare needed for practical uses. Scaffolding refers to a teaching technique where\nteachers provide support and guidance to students for learning and developing\nnew concepts or skills. It is an effective way to support diverse learning\nneeds, goals, processes, and outcomes. In this work, we investigate how\npedagogical instructions facilitate the scaffolding in ITSs, by conducting a\ncase study on guiding children to describe images for language learning. We\nconstruct different types of scaffolding tutoring systems grounded in four\nfundamental learning theories: knowledge construction, inquiry-based learning,\ndialogic teaching, and zone of proximal development. For qualitative and\nquantitative analyses, we build and refine a seven-dimension rubric to evaluate\nthe scaffolding process. In our experiment on GPT-4V, we observe that LLMs\ndemonstrate strong potential to follow pedagogical instructions and achieve\nself-paced learning in different student groups. Moreover, we extend our\nevaluation framework from a manual to an automated approach, paving the way to\nbenchmark various conversational tutoring systems.", "published": "2024-04-04 13:22:28", "link": "http://arxiv.org/abs/2404.03429v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generative AI and Teachers -- For Us or Against Us? A Case Study", "abstract": "We present insightful results of a survey on the adoption of generative\nartificial intelligence (GenAI) by university teachers in their teaching\nactivities. The transformation of education by GenAI, particularly large\nlanguage models (LLMs), has been presenting both opportunities and challenges,\nincluding cheating by students. We prepared the online survey according to best\npractices and the questions were created by the authors, who have pedagogy\nexperience. The survey contained 12 questions and a pilot study was first\nconducted. The survey was then sent to all teachers in multiple departments\nacross different campuses of the university of interest in Sweden: Lule{\\aa}\nUniversity of Technology. The survey was available in both Swedish and English.\nThe results show that 35 teachers (more than half) use GenAI out of 67\nrespondents. Preparation is the teaching activity with the most frequency that\nGenAI is used for and ChatGPT is the most commonly used GenAI. 59% say it has\nimpacted their teaching, however, 55% say there should be legislation around\nthe use of GenAI, especially as inaccuracies and cheating are the biggest\nconcerns.", "published": "2024-04-04 14:40:07", "link": "http://arxiv.org/abs/2404.03486v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Generative Language Models in Information Extraction as\n  Subjective Question Correction", "abstract": "Modern Large Language Models (LLMs) have showcased remarkable prowess in\nvarious tasks necessitating sophisticated cognitive behaviors. Nevertheless, a\nparadoxical performance discrepancy is observed, where these models\nunderperform in seemingly elementary tasks like relation extraction and event\nextraction due to two issues in conventional evaluation. (1) The imprecision of\nexisting evaluation metrics that struggle to effectively gauge semantic\nconsistency between model outputs and ground truth, and (2) The inherent\nincompleteness of evaluation benchmarks, primarily due to restrictive human\nannotation schemas, resulting in underestimated LLM performances. Inspired by\nthe principles in subjective question correction, we propose a new evaluation\nmethod, SQC-Score. This method innovatively utilizes LLMs, fine-tuned through\nsubjective question correction data, to refine matching between model outputs\nand golden labels. Additionally, by incorporating a Natural Language Inference\n(NLI) model, SQC-Score enriches golden labels, addressing benchmark\nincompleteness by acknowledging correct yet previously omitted answers. Results\non three information extraction tasks show that SQC-Score is more preferred by\nhuman annotators than the baseline metrics. Utilizing SQC-Score, we conduct a\ncomprehensive evaluation of the state-of-the-art LLMs and provide insights for\nfuture research for information extraction. Dataset and associated codes can be\naccessed at https://github.com/THU-KEG/SQC-Score.", "published": "2024-04-04 15:36:53", "link": "http://arxiv.org/abs/2404.03532v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From News to Summaries: Building a Hungarian Corpus for Extractive and\n  Abstractive Summarization", "abstract": "Training summarization models requires substantial amounts of training data.\nHowever for less resourceful languages like Hungarian, openly available models\nand datasets are notably scarce. To address this gap our paper introduces\nHunSum-2 an open-source Hungarian corpus suitable for training abstractive and\nextractive summarization models. The dataset is assembled from segments of the\nCommon Crawl corpus undergoing thorough cleaning, preprocessing and\ndeduplication. In addition to abstractive summarization we generate\nsentence-level labels for extractive summarization using sentence similarity.\nWe train baseline models for both extractive and abstractive summarization\nusing the collected dataset. To demonstrate the effectiveness of the trained\nmodels, we perform both quantitative and qualitative evaluation. Our dataset,\nmodels and code are publicly available, encouraging replication, further\nresearch, and real-world applications across various domains.", "published": "2024-04-04 16:07:06", "link": "http://arxiv.org/abs/2404.03555v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Select and Summarize: Scene Saliency for Movie Script Summarization", "abstract": "Abstractive summarization for long-form narrative texts such as movie scripts\nis challenging due to the computational and memory constraints of current\nlanguage models. A movie script typically comprises a large number of scenes;\nhowever, only a fraction of these scenes are salient, i.e., important for\nunderstanding the overall narrative. The salience of a scene can be\noperationalized by considering it as salient if it is mentioned in the summary.\nAutomatically identifying salient scenes is difficult due to the lack of\nsuitable datasets. In this work, we introduce a scene saliency dataset that\nconsists of human-annotated salient scenes for 100 movies. We propose a\ntwo-stage abstractive summarization approach which first identifies the salient\nscenes in script and then generates a summary using only those scenes. Using\nQA-based evaluation, we show that our model outperforms previous\nstate-of-the-art summarization methods and reflects the information content of\na movie more accurately than a model that takes the whole movie script as\ninput.", "published": "2024-04-04 16:16:53", "link": "http://arxiv.org/abs/2404.03561v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EASSE-DE: Easier Automatic Sentence Simplification Evaluation for German", "abstract": "In this work, we propose EASSE-multi, a framework for easier automatic\nsentence evaluation for languages other than English. Compared to the original\nEASSE framework, EASSE-multi does not focus only on English. It contains\ntokenizers and versions of text simplification evaluation metrics which are\nsuitable for multiple languages. In this paper, we exemplify the usage of\nEASSE-multi for German TS, resulting in EASSE-DE. Further, we compare text\nsimplification results when evaluating with different language or tokenization\nsettings of the metrics. Based on this, we formulate recommendations on how to\nmake the evaluation of (German) TS models more transparent and better\ncomparable. The code of EASSE-multi and its German specialisation (EASSE-DE)\ncan be found at https://github.com/rstodden/easse-de.", "published": "2024-04-04 16:18:37", "link": "http://arxiv.org/abs/2404.03563v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Personalized LLM Response Generation with Parameterized Memory Injection", "abstract": "Large Language Models (LLMs) have exhibited remarkable proficiency in\ncomprehending and generating natural language. On the other hand, personalized\nLLM response generation holds the potential to offer substantial benefits for\nindividuals in critical areas such as medical. Existing research has explored\nmemory-augmented methods to prompt the LLM with pre-stored user-specific\nknowledge for personalized response generation in terms of new queries. We\ncontend that such paradigm is unable to perceive fine-granularity information.\nIn this study, we propose a novel \\textbf{M}emory-\\textbf{i}njected approach\nusing parameter-efficient fine-tuning (PEFT) and along with a Bayesian\nOptimisation searching strategy to achieve \\textbf{L}LM\n\\textbf{P}ersonalization(\\textbf{MiLP}).", "published": "2024-04-04 16:20:34", "link": "http://arxiv.org/abs/2404.03565v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning\n  Skills in Large Language Models", "abstract": "Providing knowledge documents for large language models (LLMs) has emerged as\na promising solution to update the static knowledge inherent in their\nparameters. However, knowledge in the document may conflict with the memory of\nLLMs due to outdated or incorrect knowledge in the LLMs' parameters. This leads\nto the necessity of examining the capability of LLMs to assimilate supplemental\nexternal knowledge that conflicts with their memory. While previous studies\nhave explained to what extent LLMs extract conflicting knowledge from the\nprovided text, they neglect the necessity to reason with conflicting knowledge.\nFurthermore, there lack a detailed analysis on strategies to enable LLMs to\nresolve conflicting knowledge via prompting, decoding strategy, and supervised\nfine-tuning. To address these limitations, we construct a new dataset, dubbed\nKNOT, for knowledge conflict resolution examination in the form of question\nanswering. KNOT facilitates in-depth analysis by dividing reasoning with\nconflicting knowledge into three levels: (1) Direct Extraction, which directly\nextracts conflicting knowledge to answer questions. (2) Explicit Reasoning,\nwhich reasons with conflicting knowledge when the reasoning path is explicitly\nprovided in the question. (3) Implicit Reasoning, where reasoning with\nconflicting knowledge requires LLMs to infer the reasoning path independently\nto answer questions. We also conduct extensive experiments on KNOT to establish\nempirical guidelines for LLMs to utilize conflicting knowledge in complex\ncircumstances. Dataset and associated codes can be accessed at\nhttps://github.com/THU-KEG/KNOT .", "published": "2024-04-04 16:40:11", "link": "http://arxiv.org/abs/2404.03577v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intent Detection and Entity Extraction from BioMedical Literature", "abstract": "Biomedical queries have become increasingly prevalent in web searches,\nreflecting the growing interest in accessing biomedical literature. Despite\nrecent research on large-language models (LLMs) motivated by endeavours to\nattain generalized intelligence, their efficacy in replacing task and\ndomain-specific natural language understanding approaches remains questionable.\nIn this paper, we address this question by conducting a comprehensive empirical\nevaluation of intent detection and named entity recognition (NER) tasks from\nbiomedical text. We show that Supervised Fine Tuned approaches are still\nrelevant and more effective than general-purpose LLMs. Biomedical transformer\nmodels such as PubMedBERT can surpass ChatGPT on NER task with only 5\nsupervised examples.", "published": "2024-04-04 17:09:52", "link": "http://arxiv.org/abs/2404.03598v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating LLMs at Detecting Errors in LLM Responses", "abstract": "With Large Language Models (LLMs) being widely used across various tasks,\ndetecting errors in their responses is increasingly crucial. However, little\nresearch has been conducted on error detection of LLM responses. Collecting\nerror annotations on LLM responses is challenging due to the subjective nature\nof many NLP tasks, and thus previous research focuses on tasks of little\npractical value (e.g., word sorting) or limited error types (e.g., faithfulness\nin summarization). This work introduces ReaLMistake, the first error detection\nbenchmark consisting of objective, realistic, and diverse errors made by LLMs.\nReaLMistake contains three challenging and meaningful tasks that introduce\nobjectively assessable errors in four categories (reasoning correctness,\ninstruction-following, context-faithfulness, and parameterized knowledge),\neliciting naturally observed and diverse errors in responses of GPT-4 and Llama\n2 70B annotated by experts. We use ReaLMistake to evaluate error detectors\nbased on 12 LLMs. Our findings show: 1) Top LLMs like GPT-4 and Claude 3 detect\nerrors made by LLMs at very low recall, and all LLM-based error detectors\nperform much worse than humans. 2) Explanations by LLM-based error detectors\nlack reliability. 3) LLMs-based error detection is sensitive to small changes\nin prompts but remains challenging to improve. 4) Popular approaches to\nimproving LLMs, including self-consistency and majority vote, do not improve\nthe error detection performance. Our benchmark and code are provided at\nhttps://github.com/psunlpgroup/ReaLMistake.", "published": "2024-04-04 17:19:47", "link": "http://arxiv.org/abs/2404.03602v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning\n  in Large Language Models", "abstract": "Large language models (LLMs) have exhibited impressive performance in\nlanguage comprehension and various reasoning tasks. However, their abilities in\nspatial reasoning, a crucial aspect of human cognition, remain relatively\nunexplored. Human possess a remarkable ability to create mental images of\nunseen objects and actions through a process known as the Mind's Eye, enabling\nthe imagination of the unseen world. Inspired by this cognitive capacity, we\npropose Visualization-of-Thought (VoT) prompting. VoT aims to elicit spatial\nreasoning of LLMs by visualizing their reasoning traces, thereby guiding\nsubsequent reasoning steps. We employed VoT for multi-hop spatial reasoning\ntasks, including natural language navigation, visual navigation, and visual\ntiling in 2D grid worlds. Experimental results demonstrated that VoT\nsignificantly enhances the spatial reasoning abilities of LLMs. Notably, VoT\noutperformed existing multimodal large language models (MLLMs) in these tasks.\nWhile VoT works surprisingly well on LLMs, the ability to generate mental\nimages to facilitate spatial reasoning resembles the mind's eye process,\nsuggesting its potential viability in MLLMs. Please find the dataset and codes\nat https://microsoft.github.io/visualization-of-thought", "published": "2024-04-04 17:45:08", "link": "http://arxiv.org/abs/2404.03622v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Locating and Editing Factual Associations in Mamba", "abstract": "We investigate the mechanisms of factual recall in the Mamba state space\nmodel. Our work is inspired by previous findings in autoregressive transformer\nlanguage models suggesting that their knowledge recall is localized to\nparticular modules at specific token locations; we therefore ask whether\nfactual recall in Mamba can be similarly localized. To investigate this, we\nconduct four lines of experiments on Mamba. First, we apply causal tracing or\ninterchange interventions to localize key components inside Mamba that are\nresponsible for recalling facts, revealing that specific components within\nmiddle layers show strong causal effects at the last token of the subject,\nwhile the causal effect of intervening on later layers is most pronounced at\nthe last token of the prompt, matching previous findings on autoregressive\ntransformers. Second, we show that rank-one model editing methods can\nsuccessfully insert facts at specific locations, again resembling findings on\ntransformer LMs. Third, we examine the linearity of Mamba's representations of\nfactual relations. Finally we adapt attention-knockout techniques to Mamba in\norder to dissect information flow during factual recall. We compare Mamba\ndirectly to a similar-sized autoregressive transformer LM and conclude that\ndespite significant differences in architectural approach, when it comes to\nfactual recall, the two architectures share many similarities.", "published": "2024-04-04 17:58:31", "link": "http://arxiv.org/abs/2404.03646v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AutoWebGLM: A Large Language Model-based Web Navigating Agent", "abstract": "Large language models (LLMs) have fueled many intelligent web agents, but\nmost existing ones perform far from satisfying in real-world web navigation\ntasks due to three factors: (1) the complexity of HTML text data (2)\nversatility of actions on webpages, and (3) task difficulty due to the\nopen-domain nature of the web. In light of these challenges, we develop the\nopen AutoWebGLM based on ChatGLM3-6B. AutoWebGLM can serve as a powerful\nautomated web navigation agent that outperform GPT-4. Inspired by human\nbrowsing patterns, we first design an HTML simplification algorithm to\nrepresent webpages with vital information preserved succinctly. We then employ\na hybrid human-AI method to build web browsing data for curriculum training.\nFinally, we bootstrap the model by reinforcement learning and rejection\nsampling to further facilitate webpage comprehension, browser operations, and\nefficient task decomposition by itself. For comprehensive evaluation, we\nestablish a bilingual benchmark -- AutoWebBench -- for real-world web\nnavigation tasks. We evaluate AutoWebGLM across diverse web navigation\nbenchmarks, demonstrating its potential to tackle challenging tasks in real\nenvironments. Related code, model, and data are released at\n\\url{https://github.com/THUDM/AutoWebGLM}.", "published": "2024-04-04 17:58:40", "link": "http://arxiv.org/abs/2404.03648v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PRobELM: Plausibility Ranking Evaluation for Language Models", "abstract": "This paper introduces PRobELM (Plausibility Ranking Evaluation for Language\nModels), a benchmark designed to assess language models' ability to discern\nmore plausible from less plausible scenarios through their parametric\nknowledge. While benchmarks such as TruthfulQA emphasise factual accuracy or\ntruthfulness, and others such as COPA explore plausible scenarios without\nexplicitly incorporating world knowledge, PRobELM seeks to bridge this gap by\nevaluating models' capabilities to prioritise plausible scenarios that leverage\nworld knowledge over less plausible alternatives. This design allows us to\nassess the potential of language models for downstream use cases such as\nliterature-based discovery where the focus is on identifying information that\nis likely but not yet known. Our benchmark is constructed from a dataset\ncurated from Wikidata edit histories, tailored to align the temporal bounds of\nthe training data for the evaluated models. PRobELM facilitates the evaluation\nof language models across multiple prompting types, including statement, text\ncompletion, and question-answering. Experiments with 10 models of various sizes\nand architectures on the relationship between model scales, training recency,\nand plausibility performance, reveal that factual accuracy does not directly\ncorrelate with plausibility performance and that up-to-date training data\nenhances plausibility assessment across different model architectures.", "published": "2024-04-04 21:57:11", "link": "http://arxiv.org/abs/2404.03818v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CantTalkAboutThis: Aligning Language Models to Stay on Topic in\n  Dialogues", "abstract": "Recent advancements in instruction-tuning datasets have predominantly focused\non specific tasks like mathematical or logical reasoning. There has been a\nnotable gap in data designed for aligning language models to maintain topic\nrelevance in conversations - a critical aspect for deploying chatbots to\nproduction. We introduce the CantTalkAboutThis dataset to help language models\nremain focused on the subject at hand during task-oriented interactions. It\nconsists of synthetic dialogues on a wide range of conversation topics from\ndifferent domains. These dialogues are interspersed with distractor turns that\nintentionally divert the chatbot from the predefined topic. Fine-tuning\nlanguage models on this dataset helps make them resilient to deviating from the\nrole assigned and improves their ability to maintain topical coherence compared\nto general-purpose instruction-tuned LLMs like GPT-4-turbo and\nMixtral-Instruct. Additionally, preliminary observations suggest that training\nmodels on this dataset also enhance their performance on fine-grained\ninstruction following tasks, including safety alignment.", "published": "2024-04-04 22:31:58", "link": "http://arxiv.org/abs/2404.03820v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Pronoun Fidelity with English LLMs: Are they Reasoning,\n  Repeating, or Just Biased?", "abstract": "Robust, faithful and harm-free pronoun use for individuals is an important\ngoal for language model development as their use increases, but prior work\ntends to study only one or two of these characteristics at a time. To measure\nprogress towards the combined goal, we introduce the task of pronoun fidelity:\ngiven a context introducing a co-referring entity and pronoun, the task is to\nreuse the correct pronoun later. We present RUFF, a carefully-designed dataset\nof over 5 million instances to measure robust pronoun fidelity in English, and\nwe evaluate 37 model variants from nine popular families, across architectures\n(encoder-only, decoder-only and encoder-decoder) and scales (11M-70B\nparameters). When an individual is introduced with a pronoun, models can mostly\nfaithfully reuse this pronoun in the next sentence, but they are significantly\nworse with she/her/her, singular they and neopronouns. Moreover, models are\neasily distracted by non-adversarial sentences discussing other people; even\none sentence with a distractor pronoun causes accuracy to drop on average by 34\npercentage points. Our results show that pronoun fidelity is not robust, in a\nsimple, naturalistic setting where humans achieve nearly 100% accuracy. We\nencourage researchers to bridge the gaps we find and to carefully evaluate\nreasoning in settings where superficial repetition might inflate perceptions of\nmodel performance.", "published": "2024-04-04 01:07:14", "link": "http://arxiv.org/abs/2404.03134v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "NLP at UC Santa Cruz at SemEval-2024 Task 5: Legal Answer Validation\n  using Few-Shot Multi-Choice QA", "abstract": "This paper presents our submission to the SemEval 2024 Task 5: The Legal\nArgument Reasoning Task in Civil Procedure. We present two approaches to\nsolving the task of legal answer validation, given an introduction to the case,\na question and an answer candidate. Firstly, we fine-tuned pre-trained\nBERT-based models and found that models trained on domain knowledge perform\nbetter. Secondly, we performed few-shot prompting on GPT models and found that\nreformulating the answer validation task to be a multiple-choice QA task\nremarkably improves the performance of the model. Our best submission is a\nBERT-based model that achieved the 7th place out of 20.", "published": "2024-04-04 01:50:20", "link": "http://arxiv.org/abs/2404.03150v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Death of Feature Engineering? BERT with Linguistic Features on SQuAD\n  2.0", "abstract": "Machine reading comprehension is an essential natural language processing\ntask, which takes into a pair of context and query and predicts the\ncorresponding answer to query. In this project, we developed an end-to-end\nquestion answering model incorporating BERT and additional linguistic features.\nWe conclude that the BERT base model will be improved by incorporating the\nfeatures. The EM score and F1 score are improved 2.17 and 2.14 compared with\nBERT(base). Our best single model reaches EM score 76.55 and F1 score 79.97 in\nthe hidden test set. Our error analysis also shows that the linguistic\narchitecture can help model understand the context better in that it can locate\nanswers that BERT only model predicted \"No Answer\" wrongly.", "published": "2024-04-04 03:50:34", "link": "http://arxiv.org/abs/2404.03184v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Probabilities Also Matter: A More Faithful Metric for Faithfulness\n  of Free-Text Explanations in Large Language Models", "abstract": "In order to oversee advanced AI systems, it is important to understand their\nunderlying decision-making process. When prompted, large language models (LLMs)\ncan provide natural language explanations or reasoning traces that sound\nplausible and receive high ratings from human annotators. However, it is\nunclear to what extent these explanations are faithful, i.e., truly capture the\nfactors responsible for the model's predictions. In this work, we introduce\nCorrelational Explanatory Faithfulness (CEF), a metric that can be used in\nfaithfulness tests based on input interventions. Previous metrics used in such\ntests take into account only binary changes in the predictions. Our metric\naccounts for the total shift in the model's predicted label distribution, more\naccurately reflecting the explanations' faithfulness. We then introduce the\nCorrelational Counterfactual Test (CCT) by instantiating CEF on the\nCounterfactual Test (CT) from Atanasova et al. (2023). We evaluate the\nfaithfulness of free-text explanations generated by few-shot-prompted LLMs from\nthe Llama2 family on three NLP tasks. We find that our metric measures aspects\nof faithfulness which the CT misses.", "published": "2024-04-04 04:20:04", "link": "http://arxiv.org/abs/2404.03189v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do Large Language Models Rank Fairly? An Empirical Study on the Fairness\n  of LLMs as Rankers", "abstract": "The integration of Large Language Models (LLMs) in information retrieval has\nraised a critical reevaluation of fairness in the text-ranking models. LLMs,\nsuch as GPT models and Llama2, have shown effectiveness in natural language\nunderstanding tasks, and prior works (e.g., RankGPT) have also demonstrated\nthat the LLMs exhibit better performance than the traditional ranking models in\nthe ranking task. However, their fairness remains largely unexplored. This\npaper presents an empirical study evaluating these LLMs using the TREC Fair\nRanking dataset, focusing on the representation of binary protected attributes\nsuch as gender and geographic location, which are historically underrepresented\nin search outcomes. Our analysis delves into how these LLMs handle queries and\ndocuments related to these attributes, aiming to uncover biases in their\nranking algorithms. We assess fairness from both user and content perspectives,\ncontributing an empirical benchmark for evaluating LLMs as the fair ranker.", "published": "2024-04-04 04:23:19", "link": "http://arxiv.org/abs/2404.03192v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Advancing Aspect-Based Sentiment Analysis through Deep Learning Models", "abstract": "Aspect-based sentiment analysis predicts sentiment polarity with fine\ngranularity. While graph convolutional networks (GCNs) are widely utilized for\nsentimental feature extraction, their naive application for syntactic feature\nextraction can compromise information preservation. This study introduces an\ninnovative edge-enhanced GCN, named SentiSys, to navigate the syntactic graph\nwhile preserving intact feature information, leading to enhanced performance.\nSpecifically,we first integrate a bidirectional long short-term memory\n(Bi-LSTM) network and a self-attention-based transformer. This combination\nfacilitates effective text encoding, preventing the loss of information and\npredicting long dependency text. A bidirectional GCN (Bi-GCN) with message\npassing is then employed to encode relationships between entities.\nAdditionally, unnecessary information is filtered out using an aspect-specific\nmasking technique. To validate the effectiveness of our proposed model, we\nconduct extensive evaluation experiments on four benchmark datasets. The\nexperimental results demonstrate enhanced performance in aspect-based sentiment\nanalysis with the use of SentiSys.", "published": "2024-04-04 07:31:56", "link": "http://arxiv.org/abs/2404.03259v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Concept -- An Evaluation Protocol on Conversational Recommender Systems\n  with System-centric and User-centric Factors", "abstract": "The conversational recommendation system (CRS) has been criticized regarding\nits user experience in real-world scenarios, despite recent significant\nprogress achieved in academia. Existing evaluation protocols for CRS may\nprioritize system-centric factors such as effectiveness and fluency in\nconversation while neglecting user-centric aspects. Thus, we propose a new and\ninclusive evaluation protocol, Concept, which integrates both system- and\nuser-centric factors. We conceptualise three key characteristics in\nrepresenting such factors and further divide them into six primary abilities.\nTo implement Concept, we adopt a LLM-based user simulator and evaluator with\nscoring rubrics that are tailored for each primary ability. Our protocol,\nConcept, serves a dual purpose. First, it provides an overview of the pros and\ncons in current CRS models. Second, it pinpoints the problem of low usability\nin the \"omnipotent\" ChatGPT and offers a comprehensive reference guide for\nevaluating CRS, thereby setting the foundation for CRS improvement.", "published": "2024-04-04 08:56:48", "link": "http://arxiv.org/abs/2404.03304v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can Small Language Models Help Large Language Models Reason Better?:\n  LM-Guided Chain-of-Thought", "abstract": "We introduce a novel framework, LM-Guided CoT, that leverages a lightweight\n(i.e., <1B) language model (LM) for guiding a black-box large (i.e., >10B) LM\nin reasoning tasks. Specifically, the lightweight LM first generates a\nrationale for each input instance. The Frozen large LM is then prompted to\npredict a task output based on the rationale generated by the lightweight LM.\nOur approach is resource-efficient in the sense that it only requires training\nthe lightweight LM. We optimize the model through 1) knowledge distillation and\n2) reinforcement learning from rationale-oriented and task-oriented reward\nsignals. We assess our method with multi-hop extractive question answering (QA)\nbenchmarks, HotpotQA, and 2WikiMultiHopQA. Experimental results show that our\napproach outperforms all baselines regarding answer prediction accuracy. We\nalso find that reinforcement learning helps the model to produce higher-quality\nrationales with improved QA performance.", "published": "2024-04-04 12:46:37", "link": "http://arxiv.org/abs/2404.03414v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Representation for Political Information Sources", "abstract": "With the rise of computational social science, many scholars utilize data\nanalysis and natural language processing tools to analyze social media, news\narticles, and other accessible data sources for examining political and social\ndiscourse. Particularly, the study of the emergence of echo-chambers due to the\ndissemination of specific information has become a topic of interest in mixed\nmethods research areas. In this paper, we analyze data collected from two news\nportals, Breitbart News (BN) and New York Times (NYT) to prove the hypothesis\nthat the formation of echo-chambers can be partially explained on the level of\nan individual information consumption rather than a collective topology of\nindividuals' social networks. Our research findings are presented through\nknowledge graphs, utilizing a dataset spanning 11.5 years gathered from BN and\nNYT media portals. We demonstrate that the application of knowledge\nrepresentation techniques to the aforementioned news streams highlights,\ncontrary to common assumptions, shows relative \"internal\" neutrality of both\nsources and polarizing attitude towards a small fraction of entities.\nAdditionally, we argue that such characteristics in information sources lead to\nfundamental disparities in audience worldviews, potentially acting as a\ncatalyst for the formation of echo-chambers.", "published": "2024-04-04 13:36:01", "link": "http://arxiv.org/abs/2404.03437v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded\n  Dialogue Generation", "abstract": "Empowered by the large-scale pretrained language models, existing dialogue\nsystems have demonstrated impressive performance conducting fluent and\nnatural-sounding conversations. However, they are still plagued by the\nhallucination problem, causing unpredictable factual errors in the generated\nresponses. Recently, knowledge-grounded dialogue generation models, that\nintentionally invoke external knowledge resources to more informative\nresponses, are also proven to be effective in reducing hallucination. Following\nthe idea of getting high-quality knowledge, a few efforts have achieved pretty\ngood performance on this issue. As some inevitable knowledge noises may also\nlead to hallucinations, it is emergent to investigate the reason and future\ndirections for building noise-tolerant methods in KGD tasks. In this paper, we\nanalyze the causal story behind this problem with counterfactual reasoning\nmethods. Based on the causal effect analysis, we propose a possible solution\nfor alleviating the hallucination in KGD by exploiting the dialogue-knowledge\ninteraction. Experimental results of our example implementation show that this\nmethod can reduce hallucination without disrupting other dialogue performance,\nwhile keeping adaptive to different generation models. We hope our efforts can\nsupport and call for more attention to developing lightweight techniques\ntowards robust and trusty dialogue systems.", "published": "2024-04-04 14:45:26", "link": "http://arxiv.org/abs/2404.03491v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Embedding-Informed Adaptive Retrieval-Augmented Generation of Large\n  Language Models", "abstract": "Retrieval-augmented large language models (LLMs) have been remarkably\ncompetent in various NLP tasks. However, it was observed by previous works that\nretrieval is not always helpful, especially when the LLM is already\nknowledgeable on the query to answer. Motivated by this, Adaptive\nRetrieval-Augmented Generation (ARAG) studies retrieving only when the\nknowledge asked by the query is absent in the LLM. Previous works of ARAG\neither require accessing the pre-training corpus or prompting with additional\nmodel inferences. Aiming to avoid such drawbacks, we propose to determine\nwhether the model is knowledgeable on a query via inspecting the\n(contextualized) pre-trained token embeddings of LLMs. We hypothesize that such\nembeddings capture rich information on the model's intrinsic knowledge base,\nwhich enables an efficient way of judging the necessity to retrieve from an\nexternal corpus. Extensive experiments demonstrate our ARAG approach's superior\nperformance across various benchmarks.", "published": "2024-04-04 15:21:22", "link": "http://arxiv.org/abs/2404.03514v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How does Multi-Task Training Affect Transformer In-Context Capabilities?\n  Investigations with Function Classes", "abstract": "Large language models (LLM) have recently shown the extraordinary ability to\nperform unseen tasks based on few-shot examples provided as text, also known as\nin-context learning (ICL). While recent works have attempted to understand the\nmechanisms driving ICL, few have explored training strategies that incentivize\nthese models to generalize to multiple tasks. Multi-task learning (MTL) for\ngeneralist models is a promising direction that offers transfer learning\npotential, enabling large parameterized models to be trained from simpler,\nrelated tasks. In this work, we investigate the combination of MTL with ICL to\nbuild models that efficiently learn tasks while being robust to\nout-of-distribution examples. We propose several effective curriculum learning\nstrategies that allow ICL models to achieve higher data efficiency and more\nstable convergence. Our experiments reveal that ICL models can effectively\nlearn difficult tasks by training on progressively harder tasks while mixing in\nprior tasks, denoted as mixed curriculum in this work. Our code and models are\navailable at https://github.com/harmonbhasin/curriculum_learning_icl .", "published": "2024-04-04 16:15:23", "link": "http://arxiv.org/abs/2404.03558v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mitigating the Impact of Outlier Channels for Language Model\n  Quantization with Activation Regularization", "abstract": "We consider the problem of accurate quantization for language models, where\nboth the weights and activations are uniformly quantized to 4 bits per\nparameter, the lowest bitwidth format natively supported by GPU hardware. In\nthis context, the key challenge is activation quantization: it is known that\nlanguage models contain outlier channels whose values on average are orders of\nmagnitude higher than than other channels, which prevents accurate low-bitwidth\nquantization with known techniques. We systematically study this phenomena and\nfind that these outlier channels emerge early in training, and that they occur\nmore frequently in layers with residual streams. We then propose a simple\nstrategy which regularizes a layer's inputs via quantization-aware training\n(QAT) and its outputs via activation kurtosis regularization. We show that\nregularizing both the inputs and outputs is crucial for preventing a model's\n\"migrating\" the difficulty in input quantization to the weights, which makes\npost-training quantization (PTQ) of weights more difficult. When combined with\nweight PTQ, we show that our approach can obtain a W4A4 model that performs\ncompetitively to the standard-precision W16A16 baseline.", "published": "2024-04-04 17:25:30", "link": "http://arxiv.org/abs/2404.03605v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Sailor: Open Language Models for South-East Asia", "abstract": "We present Sailor, a family of open language models ranging from 0.5B to 7B\nparameters, tailored for South-East Asian (SEA) languages. These models are\ncontinually pre-trained from Qwen1.5, a great language model for multilingual\nuse cases. From Qwen1.5, Sailor models accept 200B to 400B tokens, primarily\ncovering the languages of English, Chinese, Vietnamese, Thai, Indonesian,\nMalay, and Lao. The training leverages several techniques, including BPE\ndropout for improving the model robustness, aggressive data cleaning and\ndeduplication, and small proxy models to optimize data mixture. Experimental\nresults on four typical tasks indicate that Sailor models demonstrate strong\nperformance across different benchmarks, including commonsense reasoning,\nquestion answering, reading comprehension and examination. Embracing the\nopen-source spirit, we share our insights through this report to spark a wider\ninterest in developing large language models for multilingual use cases.", "published": "2024-04-04 17:31:32", "link": "http://arxiv.org/abs/2404.03608v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Training LLMs over Neurally Compressed Text", "abstract": "In this paper, we explore the idea of training large language models (LLMs)\nover highly compressed text. While standard subword tokenizers compress text by\na small factor, neural text compressors can achieve much higher rates of\ncompression. If it were possible to train LLMs directly over neurally\ncompressed text, this would confer advantages in training and serving\nefficiency, as well as easier handling of long text spans. The main obstacle to\nthis goal is that strong compression tends to produce opaque outputs that are\nnot well-suited for learning. In particular, we find that text na\\\"ively\ncompressed via Arithmetic Coding is not readily learnable by LLMs. To overcome\nthis, we propose Equal-Info Windows, a novel compression technique whereby text\nis segmented into blocks that each compress to the same bit length. Using this\nmethod, we demonstrate effective learning over neurally compressed text that\nimproves with scale, and outperforms byte-level baselines by a wide margin on\nperplexity and inference speed benchmarks. While our method delivers worse\nperplexity than subword tokenizers for models trained with the same parameter\ncount, it has the benefit of shorter sequence lengths. Shorter sequence lengths\nrequire fewer autoregressive generation steps, and reduce latency. Finally, we\nprovide extensive analysis of the properties that contribute to learnability,\nand offer concrete suggestions for how to further improve the performance of\nhigh-compression tokenizers.", "published": "2024-04-04 17:48:28", "link": "http://arxiv.org/abs/2404.03626v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SHROOM-INDElab at SemEval-2024 Task 6: Zero- and Few-Shot LLM-Based\n  Classification for Hallucination Detection", "abstract": "We describe the University of Amsterdam Intelligent Data Engineering Lab\nteam's entry for the SemEval-2024 Task 6 competition. The SHROOM-INDElab system\nbuilds on previous work on using prompt programming and in-context learning\nwith large language models (LLMs) to build classifiers for hallucination\ndetection, and extends that work through the incorporation of context-specific\ndefinition of task, role, and target concept, and automated generation of\nexamples for use in a few-shot prompting approach. The resulting system\nachieved fourth-best and sixth-best performance in the model-agnostic track and\nmodel-aware tracks for Task 6, respectively, and evaluation using the\nvalidation sets showed that the system's classification decisions were\nconsistent with those of the crowd-sourced human labellers. We further found\nthat a zero-shot approach provided better accuracy than a few-shot approach\nusing automatically generated examples. Code for the system described in this\npaper is available on Github.", "published": "2024-04-04 18:01:21", "link": "http://arxiv.org/abs/2404.03732v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CONFLARE: CONFormal LArge language model REtrieval", "abstract": "Retrieval-augmented generation (RAG) frameworks enable large language models\n(LLMs) to retrieve relevant information from a knowledge base and incorporate\nit into the context for generating responses. This mitigates hallucinations and\nallows for the updating of knowledge without retraining the LLM. However, RAG\ndoes not guarantee valid responses if retrieval fails to identify the necessary\ninformation as the context for response generation. Also, if there is\ncontradictory content, the RAG response will likely reflect only one of the two\npossible responses. Therefore, quantifying uncertainty in the retrieval process\nis crucial for ensuring RAG trustworthiness. In this report, we introduce a\nfour-step framework for applying conformal prediction to quantify retrieval\nuncertainty in RAG frameworks. First, a calibration set of questions answerable\nfrom the knowledge base is constructed. Each question's embedding is compared\nagainst document embeddings to identify the most relevant document chunks\ncontaining the answer and record their similarity scores. Given a\nuser-specified error rate ({\\alpha}), these similarity scores are then analyzed\nto determine a similarity score cutoff threshold. During inference, all chunks\nwith similarity exceeding this threshold are retrieved to provide context to\nthe LLM, ensuring the true answer is captured in the context with a\n(1-{\\alpha}) confidence level. We provide a Python package that enables users\nto implement the entire workflow proposed in our work, only using LLMs and\nwithout human intervention.", "published": "2024-04-04 02:58:21", "link": "http://arxiv.org/abs/2404.04287v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Conversational Disease Diagnosis via External Planner-Controlled Large\n  Language Models", "abstract": "The development of large language models (LLMs) has brought unprecedented\npossibilities for artificial intelligence (AI) based medical diagnosis.\nHowever, the application perspective of LLMs in real diagnostic scenarios is\nstill unclear because they are not adept at collecting patient data\nproactively. This study presents a LLM-based diagnostic system that enhances\nplanning capabilities by emulating doctors. Our system involves two external\nplanners to handle planning tasks. The first planner employs a reinforcement\nlearning approach to formulate disease screening questions and conduct initial\ndiagnoses. The second planner uses LLMs to parse medical guidelines and conduct\ndifferential diagnoses. By utilizing real patient electronic medical record\ndata, we constructed simulated dialogues between virtual patients and doctors\nand evaluated the diagnostic abilities of our system. We demonstrated that our\nsystem obtained impressive performance in both disease screening and\ndifferential diagnoses tasks. This research represents a step towards more\nseamlessly integrating AI into clinical settings, potentially enhancing the\naccuracy and accessibility of medical diagnostics.", "published": "2024-04-04 06:16:35", "link": "http://arxiv.org/abs/2404.04292v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reason from Fallacy: Enhancing Large Language Models' Logical Reasoning\n  through Logical Fallacy Understanding", "abstract": "Large Language Models (LLMs) have demonstrated good performance in many\nreasoning tasks, but they still struggle with some complicated reasoning tasks\nincluding logical reasoning. One non-negligible reason for LLMs' suboptimal\nperformance on logical reasoning is their overlooking of understanding logical\nfallacies correctly. To evaluate LLMs' capability of logical fallacy\nunderstanding (LFU), we propose five concrete tasks from three cognitive\ndimensions of WHAT, WHY, and HOW in this paper. Towards these LFU tasks, we\nhave successfully constructed a new dataset LFUD based on GPT-4 accompanied by\na little human effort. Our extensive experiments justify that our LFUD can be\nused not only to evaluate LLMs' LFU capability, but also to fine-tune LLMs to\nobtain significantly enhanced performance on logical reasoning.", "published": "2024-04-04 08:38:03", "link": "http://arxiv.org/abs/2404.04293v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CBR-RAG: Case-Based Reasoning for Retrieval Augmented Generation in LLMs\n  for Legal Question Answering", "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Model (LLM)\noutput by providing prior knowledge as context to input. This is beneficial for\nknowledge-intensive and expert reliant tasks, including legal\nquestion-answering, which require evidence to validate generated text outputs.\nWe highlight that Case-Based Reasoning (CBR) presents key opportunities to\nstructure retrieval as part of the RAG process in an LLM. We introduce CBR-RAG,\nwhere CBR cycle's initial retrieval stage, its indexing vocabulary, and\nsimilarity knowledge containers are used to enhance LLM queries with\ncontextually relevant cases. This integration augments the original LLM query,\nproviding a richer prompt. We present an evaluation of CBR-RAG, and examine\ndifferent representations (i.e. general and domain-specific embeddings) and\nmethods of comparison (i.e. inter, intra and hybrid similarity) on the task of\nlegal question-answering. Our results indicate that the context provided by\nCBR's case reuse enforces similarity between relevant components of the\nquestions and the evidence base leading to significant improvements in the\nquality of generated answers.", "published": "2024-04-04 21:47:43", "link": "http://arxiv.org/abs/2404.04302v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BioVL-QR: Egocentric Biochemical Vision-and-Language Dataset Using Micro\n  QR Codes", "abstract": "This paper introduces BioVL-QR, a biochemical vision-and-language dataset\ncomprising 23 egocentric experiment videos, corresponding protocols, and\nvision-and-language alignments. A major challenge in understanding biochemical\nvideos is detecting equipment, reagents, and containers because of the\ncluttered environment and indistinguishable objects. Previous studies assumed\nmanual object annotation, which is costly and time-consuming. To address the\nissue, we focus on Micro QR Codes. However, detecting objects using only Micro\nQR Codes is still difficult due to blur and occlusion caused by object\nmanipulation. To overcome this, we propose an object labeling method combining\na Micro QR Code detector with an off-the-shelf hand object detector. As an\napplication of the method and BioVL-QR, we tackled the task of localizing the\nprocedural steps in an instructional video. The experimental results show that\nusing Micro QR Codes and our method improves biochemical video understanding.\nData and code are available through https://nishi10mo.github.io/BioVL-QR/", "published": "2024-04-04 02:22:37", "link": "http://arxiv.org/abs/2404.03161v2", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Uncertainty in Language Models: Assessment through Rank-Calibration", "abstract": "Language Models (LMs) have shown promising performance in natural language\ngeneration. However, as LMs often generate incorrect or hallucinated responses,\nit is crucial to correctly quantify their uncertainty in responding to given\ninputs. In addition to verbalized confidence elicited via prompting, many\nuncertainty measures ($e.g.$, semantic entropy and affinity-graph-based\nmeasures) have been proposed. However, these measures can differ greatly, and\nit is unclear how to compare them, partly because they take values over\ndifferent ranges ($e.g.$, $[0,\\infty)$ or $[0,1]$). In this work, we address\nthis issue by developing a novel and practical framework, termed\n$Rank$-$Calibration$, to assess uncertainty and confidence measures for LMs.\nOur key tenet is that higher uncertainty (or lower confidence) should imply\nlower generation quality, on average. Rank-calibration quantifies deviations\nfrom this ideal relationship in a principled manner, without requiring ad hoc\nbinary thresholding of the correctness score ($e.g.$, ROUGE or METEOR). The\nbroad applicability and the granular interpretability of our methods are\ndemonstrated empirically.", "published": "2024-04-04 02:31:05", "link": "http://arxiv.org/abs/2404.03163v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "M3TCM: Multi-modal Multi-task Context Model for Utterance Classification\n  in Motivational Interviews", "abstract": "Accurate utterance classification in motivational interviews is crucial to\nautomatically understand the quality and dynamics of client-therapist\ninteraction, and it can serve as a key input for systems mediating such\ninteractions. Motivational interviews exhibit three important characteristics.\nFirst, there are two distinct roles, namely client and therapist. Second, they\nare often highly emotionally charged, which can be expressed both in text and\nin prosody. Finally, context is of central importance to classify any given\nutterance. Previous works did not adequately incorporate all of these\ncharacteristics into utterance classification approaches for mental health\ndialogues. In contrast, we present M3TCM, a Multi-modal, Multi-task Context\nModel for utterance classification. Our approach for the first time employs\nmulti-task learning to effectively model both joint and individual components\nof therapist and client behaviour. Furthermore, M3TCM integrates information\nfrom the text and speech modality as well as the conversation context. With our\nnovel approach, we outperform the state of the art for utterance classification\non the recently introduced AnnoMI dataset with a relative improvement of 20%\nfor the client- and by 15% for therapist utterance classification. In extensive\nablation studies, we quantify the improvement resulting from each contribution.", "published": "2024-04-04 09:17:22", "link": "http://arxiv.org/abs/2404.03312v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak\n  Attacks?", "abstract": "Various jailbreak attacks have been proposed to red-team Large Language\nModels (LLMs) and revealed the vulnerable safeguards of LLMs. Besides, some\nmethods are not limited to the textual modality and extend the jailbreak attack\nto Multimodal Large Language Models (MLLMs) by perturbing the visual input.\nHowever, the absence of a universal evaluation benchmark complicates the\nperformance reproduction and fair comparison. Besides, there is a lack of\ncomprehensive evaluation of closed-source state-of-the-art (SOTA) models,\nespecially MLLMs, such as GPT-4V. To address these issues, this work first\nbuilds a comprehensive jailbreak evaluation dataset with 1445 harmful questions\ncovering 11 different safety policies. Based on this dataset, extensive\nred-teaming experiments are conducted on 11 different LLMs and MLLMs, including\nboth SOTA proprietary models and open-source models. We then conduct a deep\nanalysis of the evaluated results and find that (1) GPT4 and GPT-4V demonstrate\nbetter robustness against jailbreak attacks compared to open-source LLMs and\nMLLMs. (2) Llama2 and Qwen-VL-Chat are more robust compared to other\nopen-source models. (3) The transferability of visual jailbreak methods is\nrelatively limited compared to textual jailbreak methods. The dataset and code\ncan be found https://github.com/chenxshuo/RedTeamingGPT4V", "published": "2024-04-04 12:38:14", "link": "http://arxiv.org/abs/2404.03411v2", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Benchmarking ChatGPT on Algorithmic Reasoning", "abstract": "We evaluate ChatGPT's ability to solve algorithm problems from the CLRS\nbenchmark suite that is designed for GNNs. The benchmark requires the use of a\nspecified classical algorithm to solve a given problem. We find that ChatGPT\noutperforms specialist GNN models, using Python to successfully solve these\nproblems. This raises new points in the discussion about learning algorithms\nwith neural networks and how we think about what out of distribution testing\nlooks like with web scale training data.", "published": "2024-04-04 13:39:06", "link": "http://arxiv.org/abs/2404.03441v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "The Impact of Unstated Norms in Bias Analysis of Language Models", "abstract": "Bias in large language models (LLMs) has many forms, from overt\ndiscrimination to implicit stereotypes. Counterfactual bias evaluation is a\nwidely used approach to quantifying bias and often relies on template-based\nprobes that explicitly state group membership. It measures whether the outcome\nof a task performed by an LLM is invariant to a change in group membership. In\nthis work, we find that template-based probes can lead to unrealistic bias\nmeasurements. For example, LLMs appear to mistakenly cast text associated with\nWhite race as negative at higher rates than other groups. We hypothesize that\nthis arises artificially via a mismatch between commonly unstated norms, in the\nform of markedness, in the pretraining text of LLMs (e.g., Black president vs.\npresident) and templates used for bias measurement (e.g., Black president vs.\nWhite president). The findings highlight the potential misleading impact of\nvarying group membership through explicit mention in counterfactual bias\nquantification.", "published": "2024-04-04 14:24:06", "link": "http://arxiv.org/abs/2404.03471v4", "categories": ["cs.CL", "cs.CY", "cs.LG", "68T50"], "primary_category": "cs.CL"}
{"title": "CodeEditorBench: Evaluating Code Editing Capability of Large Language\n  Models", "abstract": "Large Language Models (LLMs) for code are rapidly evolving, with code editing\nemerging as a critical capability. We introduce CodeEditorBench, an evaluation\nframework designed to rigorously assess the performance of LLMs in code editing\ntasks, including debugging, translating, polishing, and requirement switching.\nUnlike existing benchmarks focusing solely on code generation, CodeEditorBench\nemphasizes real-world scenarios and practical aspects of software development.\nWe curate diverse coding challenges and scenarios from five sources, covering\nvarious programming languages, complexity levels, and editing tasks. Evaluation\nof 19 LLMs reveals that closed-source models (particularly Gemini-Ultra and\nGPT-4), outperform open-source models in CodeEditorBench, highlighting\ndifferences in model performance based on problem types and prompt\nsensitivities. CodeEditorBench aims to catalyze advancements in LLMs by\nproviding a robust platform for assessing code editing capabilities. We will\nrelease all prompts and datasets to enable the community to expand the dataset\nand benchmark emerging LLMs. By introducing CodeEditorBench, we contribute to\nthe advancement of LLMs in code editing and provide a valuable resource for\nresearchers and practitioners.", "published": "2024-04-04 15:49:49", "link": "http://arxiv.org/abs/2404.03543v3", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "ReFT: Representation Finetuning for Language Models", "abstract": "Parameter-efficient finetuning (PEFT) methods seek to adapt large neural\nmodels via updates to a small number of weights. However, much prior\ninterpretability work has shown that representations encode rich semantic\ninformation, suggesting that editing representations might be a more powerful\nalternative. We pursue this hypothesis by developing a family of Representation\nFinetuning (ReFT) methods. ReFT methods operate on a frozen base model and\nlearn task-specific interventions on hidden representations. We define a strong\ninstance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT), and we\nidentify an ablation of this method that trades some performance for increased\nefficiency. Both are drop-in replacements for existing PEFTs and learn\ninterventions that are 15x--65x more parameter-efficient than LoRA. We showcase\nLoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks,\ninstruction-tuning, and GLUE. In all these evaluations, our ReFTs deliver the\nbest balance of efficiency and performance, and almost always outperform\nstate-of-the-art PEFTs. We release a generic ReFT training library publicly at\nhttps://github.com/stanfordnlp/pyreft.", "published": "2024-04-04 17:00:37", "link": "http://arxiv.org/abs/2404.03592v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unveiling LLMs: The Evolution of Latent Representations in a Dynamic\n  Knowledge Graph", "abstract": "Large Language Models (LLMs) demonstrate an impressive capacity to recall a\nvast range of factual knowledge. However, understanding their underlying\nreasoning and internal mechanisms in exploiting this knowledge remains a key\nresearch area. This work unveils the factual information an LLM represents\ninternally for sentence-level claim verification. We propose an end-to-end\nframework to decode factual knowledge embedded in token representations from a\nvector space to a set of ground predicates, showing its layer-wise evolution\nusing a dynamic knowledge graph. Our framework employs activation patching, a\nvector-level technique that alters a token representation during inference, to\nextract encoded knowledge. Accordingly, we neither rely on training nor\nexternal models. Using factual and common-sense claims from two claim\nverification datasets, we showcase interpretability analyses at local and\nglobal levels. The local analysis highlights entity centrality in LLM\nreasoning, from claim-related information and multi-hop reasoning to\nrepresentation errors causing erroneous evaluation. On the other hand, the\nglobal reveals trends in the underlying evolution, such as word-based knowledge\nevolving into claim-related facts. By interpreting semantics from LLM latent\nrepresentations and enabling graph-related analyses, this work enhances the\nunderstanding of the factual knowledge resolution process.", "published": "2024-04-04 17:45:59", "link": "http://arxiv.org/abs/2404.03623v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept\n  Matching", "abstract": "Diffusion models have demonstrated great success in the field of\ntext-to-image generation. However, alleviating the misalignment between the\ntext prompts and images is still challenging. The root reason behind the\nmisalignment has not been extensively investigated. We observe that the\nmisalignment is caused by inadequate token attention activation. We further\nattribute this phenomenon to the diffusion model's insufficient condition\nutilization, which is caused by its training paradigm. To address the issue, we\npropose CoMat, an end-to-end diffusion model fine-tuning strategy with an\nimage-to-text concept matching mechanism. We leverage an image captioning model\nto measure image-to-text alignment and guide the diffusion model to revisit\nignored tokens. A novel attribute concentration module is also proposed to\naddress the attribute binding problem. Without any image or human preference\ndata, we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL.\nExtensive experiments show that CoMat-SDXL significantly outperforms the\nbaseline model SDXL in two text-to-image alignment benchmarks and achieves\nstart-of-the-art performance.", "published": "2024-04-04 17:59:46", "link": "http://arxiv.org/abs/2404.03653v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Direct Nash Optimization: Teaching Language Models to Self-Improve with\n  General Preferences", "abstract": "This paper studies post-training large language models (LLMs) using\npreference feedback from a powerful oracle to help a model iteratively improve\nover itself. The typical approach for post-training LLMs involves Reinforcement\nLearning from Human Feedback (RLHF), which traditionally separates reward\nlearning and subsequent policy optimization. However, such a reward\nmaximization approach is limited by the nature of \"point-wise\" rewards (such as\nBradley-Terry model), which fails to express complex intransitive or cyclic\npreference relations. While advances on RLHF show reward learning and policy\noptimization can be merged into a single contrastive objective for stability,\nthey yet still remain tethered to the reward maximization framework. Recently,\na new wave of research sidesteps the reward maximization presumptions in favor\nof directly optimizing over \"pair-wise\" or general preferences. In this paper,\nwe introduce Direct Nash Optimization (DNO), a provable and scalable algorithm\nthat marries the simplicity and stability of contrastive learning with\ntheoretical generality from optimizing general preferences. Because DNO is a\nbatched on-policy algorithm using a regression-based objective, its\nimplementation is straightforward and efficient. Moreover, DNO enjoys monotonic\nimprovement across iterations that help it improve even over a strong teacher\n(such as GPT-4). In our experiments, a resulting 7B parameter Orca-2.5 model\naligned by DNO achieves the state-of-the-art win-rate against GPT-4-Turbo of\n33% on AlpacaEval 2.0 (even after controlling for response length), an absolute\ngain of 26% (7% to 33%) over the initializing model. It outperforms models with\nfar more parameters, including Mistral Large, Self-Rewarding LM (70B\nparameters), and older versions of GPT-4.", "published": "2024-04-04 17:56:41", "link": "http://arxiv.org/abs/2404.03715v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Fakes of Varying Shades: How Warning Affects Human Perception and\n  Engagement Regarding LLM Hallucinations", "abstract": "The widespread adoption and transformative effects of large language models\n(LLMs) have sparked concerns regarding their capacity to produce inaccurate and\nfictitious content, referred to as `hallucinations'. Given the potential risks\nassociated with hallucinations, humans should be able to identify them. This\nresearch aims to understand the human perception of LLM hallucinations by\nsystematically varying the degree of hallucination (genuine, minor\nhallucination, major hallucination) and examining its interaction with warning\n(i.e., a warning of potential inaccuracies: absent vs. present). Participants\n(N=419) from Prolific rated the perceived accuracy and engaged with content\n(e.g., like, dislike, share) in a Q/A format. Participants ranked content as\ntruthful in the order of genuine, minor hallucination, and major hallucination,\nand user engagement behaviors mirrored this pattern. More importantly, we\nobserved that warning improved the detection of hallucination without\nsignificantly affecting the perceived truthfulness of genuine content. We\nconclude by offering insights for future tools to aid human detection of\nhallucinations. All survey materials, demographic questions, and post-session\nquestions are available at:\nhttps://github.com/MahjabinNahar/fakes-of-varying-shades-survey-materials", "published": "2024-04-04 18:34:32", "link": "http://arxiv.org/abs/2404.03745v3", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "GenQREnsemble: Zero-Shot LLM Ensemble Prompting for Generative Query\n  Reformulation", "abstract": "Query Reformulation(QR) is a set of techniques used to transform a user's\noriginal search query to a text that better aligns with the user's intent and\nimproves their search experience. Recently, zero-shot QR has been shown to be a\npromising approach due to its ability to exploit knowledge inherent in large\nlanguage models. By taking inspiration from the success of ensemble prompting\nstrategies which have benefited many tasks, we investigate if they can help\nimprove query reformulation. In this context, we propose an ensemble based\nprompting technique, GenQREnsemble which leverages paraphrases of a zero-shot\ninstruction to generate multiple sets of keywords ultimately improving\nretrieval performance. We further introduce its post-retrieval variant,\nGenQREnsembleRF to incorporate pseudo relevant feedback. On evaluations over\nfour IR benchmarks, we find that GenQREnsemble generates better reformulations\nwith relative nDCG@10 improvements up to 18% and MAP improvements upto 24% over\nthe previous zero-shot state-of-art. On the MSMarco Passage Ranking task,\nGenQREnsembleRF shows relative gains of 5% MRR using pseudo-relevance feedback,\nand 9% nDCG@10 using relevant feedback documents.", "published": "2024-04-04 18:35:25", "link": "http://arxiv.org/abs/2404.03746v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "An Investigation into Misuse of Java Security APIs by Large Language\n  Models", "abstract": "The increasing trend of using Large Language Models (LLMs) for code\ngeneration raises the question of their capability to generate trustworthy\ncode. While many researchers are exploring the utility of code generation for\nuncovering software vulnerabilities, one crucial but often overlooked aspect is\nthe security Application Programming Interfaces (APIs). APIs play an integral\nrole in upholding software security, yet effectively integrating security APIs\npresents substantial challenges. This leads to inadvertent misuse by\ndevelopers, thereby exposing software to vulnerabilities. To overcome these\nchallenges, developers may seek assistance from LLMs. In this paper, we\nsystematically assess ChatGPT's trustworthiness in code generation for security\nAPI use cases in Java. To conduct a thorough evaluation, we compile an\nextensive collection of 48 programming tasks for 5 widely used security APIs.\nWe employ both automated and manual approaches to effectively detect security\nAPI misuse in the code generated by ChatGPT for these tasks. Our findings are\nconcerning: around 70% of the code instances across 30 attempts per task\ncontain security API misuse, with 20 distinct misuse types identified.\nMoreover, for roughly half of the tasks, this rate reaches 100%, indicating\nthat there is a long way to go before developers can rely on ChatGPT to\nsecurely implement security API code.", "published": "2024-04-04 22:52:41", "link": "http://arxiv.org/abs/2404.03823v1", "categories": ["cs.CR", "cs.CL", "cs.CY"], "primary_category": "cs.CR"}
{"title": "No \"Zero-Shot\" Without Exponential Data: Pretraining Concept Frequency\n  Determines Multimodal Model Performance", "abstract": "Web-crawled pretraining datasets underlie the impressive \"zero-shot\"\nevaluation performance of multimodal models, such as CLIP for\nclassification/retrieval and Stable-Diffusion for image generation. However, it\nis unclear how meaningful the notion of \"zero-shot\" generalization is for such\nmultimodal models, as it is not known to what extent their pretraining datasets\nencompass the downstream concepts targeted for during \"zero-shot\" evaluation.\nIn this work, we ask: How is the performance of multimodal models on downstream\nconcepts influenced by the frequency of these concepts in their pretraining\ndatasets? We comprehensively investigate this question across 34 models and\nfive standard pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M,\nLAION-Aesthetics), generating over 300GB of data artifacts. We consistently\nfind that, far from exhibiting \"zero-shot\" generalization, multimodal models\nrequire exponentially more data to achieve linear improvements in downstream\n\"zero-shot\" performance, following a sample inefficient log-linear scaling\ntrend. This trend persists even when controlling for sample-level similarity\nbetween pretraining and downstream datasets, and testing on purely synthetic\ndata distributions. Furthermore, upon benchmarking models on long-tailed data\nsampled based on our analysis, we demonstrate that multimodal models across the\nboard perform poorly. We contribute this long-tail test set as the \"Let it\nWag!\" benchmark to further research in this direction. Taken together, our\nstudy reveals an exponential need for training data which implies that the key\nto \"zero-shot\" generalization capabilities under large-scale training paradigms\nremains to be found.", "published": "2024-04-04 17:58:02", "link": "http://arxiv.org/abs/2404.04125v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Bias Amplification in Language Model Evolution: An Iterated Learning\n  Perspective", "abstract": "With the widespread adoption of Large Language Models (LLMs), the prevalence\nof iterative interactions among these models is anticipated to increase.\nNotably, recent advancements in multi-round self-improving methods allow LLMs\nto generate new examples for training subsequent models. At the same time,\nmulti-agent LLM systems, involving automated interactions among agents, are\nalso increasing in prominence. Thus, in both short and long terms, LLMs may\nactively engage in an evolutionary process. We draw parallels between the\nbehavior of LLMs and the evolution of human culture, as the latter has been\nextensively studied by cognitive scientists for decades. Our approach involves\nleveraging Iterated Learning (IL), a Bayesian framework that elucidates how\nsubtle biases are magnified during human cultural evolution, to explain some\nbehaviors of LLMs. This paper outlines key characteristics of agents' behavior\nin the Bayesian-IL framework, including predictions that are supported by\nexperimental verification with various LLMs. This theoretical framework could\nhelp to more effectively predict and guide the evolution of LLMs in desired\ndirections.", "published": "2024-04-04 02:01:25", "link": "http://arxiv.org/abs/2404.04286v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transducers with Pronunciation-aware Embeddings for Automatic Speech\n  Recognition", "abstract": "This paper proposes Transducers with Pronunciation-aware Embeddings (PET).\nUnlike conventional Transducers where the decoder embeddings for different\ntokens are trained independently, the PET model's decoder embedding\nincorporates shared components for text tokens with the same or similar\npronunciations. With experiments conducted in multiple datasets in Mandarin\nChinese and Korean, we show that PET models consistently improve speech\nrecognition accuracy compared to conventional Transducers. Our investigation\nalso uncovers a phenomenon that we call error chain reactions. Instead of\nrecognition errors being evenly spread throughout an utterance, they tend to\ngroup together, with subsequent errors often following earlier ones. Our\nanalysis shows that PET models effectively mitigate this issue by substantially\nreducing the likelihood of the model generating additional errors following a\nprior one. Our implementation will be open-sourced with the NeMo toolkit.", "published": "2024-04-04 17:03:47", "link": "http://arxiv.org/abs/2404.04295v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SELF-[IN]CORRECT: LLMs Struggle with Discriminating Self-Generated\n  Responses", "abstract": "Can LLMs consistently improve their previous outputs for better results? For\nthis to be true, LLMs would need to be better at discriminating among\npreviously-generated alternatives, than generating initial responses. We\nexplore the validity of this hypothesis in practice. We first formulate a\nunified framework that allows us to compare the generative and discriminative\ncapability of any model on any task. In our resulting experimental analysis of\nseveral open-source and industrial LLMs, we observe that models are not\nreliably better at discriminating among previously-generated alternatives than\ngenerating initial responses. This finding challenges the notion that LLMs may\nbe able to enhance their performance only through their own judgment.", "published": "2024-04-04 20:27:37", "link": "http://arxiv.org/abs/2404.04298v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Using Large Language Models to Enrich the Documentation of Datasets for\n  Machine Learning", "abstract": "Recent regulatory initiatives like the European AI Act and relevant voices in\nthe Machine Learning (ML) community stress the need to describe datasets along\nseveral key dimensions for trustworthy AI, such as the provenance processes and\nsocial concerns. However, this information is typically presented as\nunstructured text in accompanying documentation, hampering their automated\nanalysis and processing. In this work, we explore using large language models\n(LLM) and a set of prompting strategies to automatically extract these\ndimensions from documents and enrich the dataset description with them. Our\napproach could aid data publishers and practitioners in creating\nmachine-readable documentation to improve the discoverability of their\ndatasets, assess their compliance with current AI regulations, and improve the\noverall quality of ML models trained on them.\n  In this paper, we evaluate the approach on 12 scientific dataset papers\npublished in two scientific journals (Nature's Scientific Data and Elsevier's\nData in Brief) using two different LLMs (GPT3.5 and Flan-UL2). Results show\ngood accuracy with our prompt extraction strategies. Concrete results vary\ndepending on the dimensions, but overall, GPT3.5 shows slightly better accuracy\n(81,21%) than FLAN-UL2 (69,13%) although it is more prone to hallucinations. We\nhave released an open-source tool implementing our approach and a replication\npackage, including the experiments' code and results, in an open-source\nrepository.", "published": "2024-04-04 10:09:28", "link": "http://arxiv.org/abs/2404.15320v2", "categories": ["cs.DL", "cs.AI", "cs.CL", "H.4.4"], "primary_category": "cs.DL"}
{"title": "Mitigating LLM Hallucinations via Conformal Abstention", "abstract": "We develop a principled procedure for determining when a large language model\n(LLM) should abstain from responding (e.g., by saying \"I don't know\") in a\ngeneral domain, instead of resorting to possibly \"hallucinating\" a non-sensical\nor incorrect answer. Building on earlier approaches that use self-consistency\nas a more reliable measure of model confidence, we propose using the LLM itself\nto self-evaluate the similarity between each of its sampled responses for a\ngiven query. We then further leverage conformal prediction techniques to\ndevelop an abstention procedure that benefits from rigorous theoretical\nguarantees on the hallucination rate (error rate). Experimentally, our\nresulting conformal abstention method reliably bounds the hallucination rate on\nvarious closed-book, open-domain generative question answering datasets, while\nalso maintaining a significantly less conservative abstention rate on a dataset\nwith long responses (Temporal Sequences) compared to baselines using\nlog-probability scores to quantify uncertainty, while achieveing comparable\nperformance on a dataset with short answers (TriviaQA). To evaluate the\nexperiments automatically, one needs to determine if two responses are\nequivalent given a question. Following standard practice, we use a thresholded\nsimilarity function to determine if two responses match, but also provide a\nmethod for calibrating the threshold based on conformal prediction, with\ntheoretical guarantees on the accuracy of the match prediction, which might be\nof independent interest.", "published": "2024-04-04 11:32:03", "link": "http://arxiv.org/abs/2405.01563v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting\n  for Text-to-Speech Synthesis", "abstract": "We present RALL-E, a robust language modeling method for text-to-speech (TTS)\nsynthesis. While previous work based on large language models (LLMs) shows\nimpressive performance on zero-shot TTS, such methods often suffer from poor\nrobustness, such as unstable prosody (weird pitch and rhythm/duration) and a\nhigh word error rate (WER), due to the autoregressive prediction style of\nlanguage models. The core idea behind RALL-E is chain-of-thought (CoT)\nprompting, which decomposes the task into simpler steps to enhance the\nrobustness of LLM-based TTS. To accomplish this idea, RALL-E first predicts\nprosody features (pitch and duration) of the input text and uses them as\nintermediate conditions to predict speech tokens in a CoT style. Second, RALL-E\nutilizes the predicted duration prompt to guide the computing of self-attention\nweights in Transformer to enforce the model to focus on the corresponding\nphonemes and prosody features when predicting speech tokens. Results of\ncomprehensive objective and subjective evaluations demonstrate that, compared\nto a powerful baseline method VALL-E, RALL-E significantly improves the WER of\nzero-shot TTS from $5.6\\%$ (without reranking) and $1.7\\%$ (with reranking) to\n$2.5\\%$ and $1.0\\%$, respectively. Furthermore, we demonstrate that RALL-E\ncorrectly synthesizes sentences that are hard for VALL-E and reduces the error\nrate from $68\\%$ to $4\\%$.", "published": "2024-04-04 05:15:07", "link": "http://arxiv.org/abs/2404.03204v3", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with\n  Semantic Neural Graph Filtering", "abstract": "Knowledge Graphs (KGs) have proven essential in information processing and\nreasoning applications because they link related entities and give context-rich\ninformation, supporting efficient information retrieval and knowledge\ndiscovery; presenting information flow in a very effective manner. Despite\nbeing widely used globally, Bangla is relatively underrepresented in KGs due to\na lack of comprehensive datasets, encoders, NER (named entity recognition)\nmodels, POS (part-of-speech) taggers, and lemmatizers, hindering efficient\ninformation processing and reasoning applications in the language. Addressing\nthe KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework\nthat is able to automatically construct Bengali KGs from any Bangla text. We\nutilize multilingual LLMs to understand various languages and correlate\nentities and relations universally. By employing a translation dictionary to\nidentify English equivalents and extracting word features from pre-trained BERT\nmodels, we construct the foundational KG. To reduce noise and align word\nembeddings with our goal, we employ graph-based polynomial filters. Lastly, we\nimplement a GNN-based semantic filter, which elevates contextual understanding\nand trims unnecessary edges, culminating in the formation of the definitive KG.\nEmpirical findings and case studies demonstrate the universal effectiveness of\nour model, capable of autonomously constructing semantically enriched KGs from\nany text.", "published": "2024-04-04 15:31:21", "link": "http://arxiv.org/abs/2404.03528v3", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.NE", "cs.SI"], "primary_category": "cs.CL"}
{"title": "WorDepth: Variational Language Prior for Monocular Depth Estimation", "abstract": "Three-dimensional (3D) reconstruction from a single image is an ill-posed\nproblem with inherent ambiguities, i.e. scale. Predicting a 3D scene from text\ndescription(s) is similarly ill-posed, i.e. spatial arrangements of objects\ndescribed. We investigate the question of whether two inherently ambiguous\nmodalities can be used in conjunction to produce metric-scaled reconstructions.\nTo test this, we focus on monocular depth estimation, the problem of predicting\na dense depth map from a single image, but with an additional text caption\ndescribing the scene. To this end, we begin by encoding the text caption as a\nmean and standard deviation; using a variational framework, we learn the\ndistribution of the plausible metric reconstructions of 3D scenes corresponding\nto the text captions as a prior. To \"select\" a specific reconstruction or depth\nmap, we encode the given image through a conditional sampler that samples from\nthe latent space of the variational text encoder, which is then decoded to the\noutput depth map. Our approach is trained alternatingly between the text and\nimage branches: in one optimization step, we predict the mean and standard\ndeviation from the text description and sample from a standard Gaussian, and in\nthe other, we sample using a (image) conditional sampler. Once trained, we\ndirectly predict depth from the encoded text using the conditional sampler. We\ndemonstrate our approach on indoor (NYUv2) and outdoor (KITTI) scenarios, where\nwe show that language can consistently improve performance in both.", "published": "2024-04-04 17:54:33", "link": "http://arxiv.org/abs/2404.03635v4", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Interpreting End-to-End Deep Learning Models for Speech Source\n  Localization Using Layer-wise Relevance Propagation", "abstract": "Deep learning models are widely applied in the signal processing community,\nyet their inner working procedure is often treated as a black box. In this\npaper, we investigate the use of eXplainable Artificial Intelligence (XAI)\ntechniques to learning-based end-to-end speech source localization models. We\nconsider the Layer-wise Relevance Propagation (LRP) technique, which aims to\ndetermine which parts of the input are more important for the output\nprediction. Using LRP we analyze two state-of-the-art models, of differing\narchitectural complexity that map audio signals acquired by the microphones to\nthe cartesian coordinates of the source. Specifically, we inspect the relevance\nassociated with the input features of the two models and discover that both\nnetworks denoise and de-reverberate the microphone signals to compute more\naccurate statistical correlations between them and consequently localize the\nsources. To further demonstrate this fact, we estimate the Time-Difference of\nArrivals (TDoAs) via the Generalized Cross Correlation with Phase Transform\n(GCC-PHAT) using both microphone signals and relevance signals extracted from\nthe two networks and show that through the latter we obtain more accurate\ntime-delay estimation results.", "published": "2024-04-04 13:29:30", "link": "http://arxiv.org/abs/2404.03436v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "UniAV: Unified Audio-Visual Perception for Multi-Task Video Event\n  Localization", "abstract": "Video localization tasks aim to temporally locate specific instances in\nvideos, including temporal action localization (TAL), sound event detection\n(SED) and audio-visual event localization (AVEL). Existing methods\nover-specialize on each task, overlooking the fact that these instances often\noccur in the same video to form the complete video content. In this work, we\npresent UniAV, a Unified Audio-Visual perception network, to achieve joint\nlearning of TAL, SED and AVEL tasks for the first time. UniAV can leverage\ndiverse data available in task-specific datasets, allowing the model to learn\nand share mutually beneficial knowledge across tasks and modalities. To tackle\nthe challenges posed by substantial variations in datasets\n(size/domain/duration) and distinct task characteristics, we propose to\nuniformly encode visual and audio modalities of all videos to derive generic\nrepresentations, while also designing task-specific experts to capture unique\nknowledge for each task. Besides, we develop a unified language-aware\nclassifier by utilizing a pre-trained text encoder, enabling the model to\nflexibly detect various types of instances and previously unseen ones by simply\nchanging prompts during inference. UniAV outperforms its single-task\ncounterparts by a large margin with fewer parameters, achieving on-par or\nsuperior performances compared to state-of-the-art task-specific methods across\nActivityNet 1.3, DESED and UnAV-100 benchmarks.", "published": "2024-04-04 03:28:57", "link": "http://arxiv.org/abs/2404.03179v2", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Correlation and Spectral Density Functions in Mode-Stirred Reverberation\n  -- II. Spectral Moments, Sampling, Noise, EMI and Understirring", "abstract": "In part I, spectral moments and kurtosis were established as parameters in\nanalytic models of correlation and spectral density functions for dynamic\nreverberation fields. In this part II, several practical limitations affecting\nthe accuracy of estimating these parameters from measured stir sweep data are\ninvestigated. For sampled fields, the contributions of finite differencing and\naliasing are evaluated. Finite differencing results in a negative bias that\ndepends, to leading order, quadratically on the product of the sampling time\ninterval and the stir bandwidth. Numerical estimates of moments extracted\ndirectly from sampled stir sweeps show good agreement with values obtained by\nan autocovariance method. The effects of data decimation and noise-to-stir\nratios of RMS amplitudes are determined and experimentally verified. In\naddition, the dependencies on the noise-to-stir-bandwidth ratio, EMI, and\nunstirred energy are characterized.", "published": "2024-04-04 15:24:03", "link": "http://arxiv.org/abs/2404.03520v1", "categories": ["physics.class-ph", "cs.SD", "eess.AS"], "primary_category": "physics.class-ph"}
{"title": "Analyzing Musical Characteristics of National Anthems in Relation to\n  Global Indices", "abstract": "Music plays a huge part in shaping peoples' psychology and behavioral\npatterns. This paper investigates the connection between national anthems and\ndifferent global indices with computational music analysis and statistical\ncorrelation analysis. We analyze national anthem musical data to determine\nwhether certain musical characteristics are associated with peace, happiness,\nsuicide rate, crime rate, etc. To achieve this, we collect national anthems\nfrom 169 countries and use computational music analysis techniques to extract\npitch, tempo, beat, and other pertinent audio features. We then compare these\nmusical characteristics with data on different global indices to ascertain\nwhether a significant correlation exists. Our findings indicate that there may\nbe a correlation between the musical characteristics of national anthems and\nthe indices we investigated. The implications of our findings for music\npsychology and policymakers interested in promoting social well-being are\ndiscussed. This paper emphasizes the potential of musical data analysis in\nsocial research and offers a novel perspective on the relationship between\nmusic and social indices. The source code and data are made open-access for\nreproducibility and future research endeavors. It can be accessed at\nhttp://bit.ly/na_code.", "published": "2024-04-04 17:25:31", "link": "http://arxiv.org/abs/2404.03606v1", "categories": ["cs.SD", "cs.AI", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
