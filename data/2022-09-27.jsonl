{"title": "WikiDes: A Wikipedia-Based Dataset for Generating Short Descriptions\n  from Paragraphs", "abstract": "As free online encyclopedias with massive volumes of content, Wikipedia and\nWikidata are key to many Natural Language Processing (NLP) tasks, such as\ninformation retrieval, knowledge base building, machine translation, text\nclassification, and text summarization. In this paper, we introduce WikiDes, a\nnovel dataset to generate short descriptions of Wikipedia articles for the\nproblem of text summarization. The dataset consists of over 80k English samples\non 6987 topics. We set up a two-phase summarization method - description\ngeneration (Phase I) and candidate ranking (Phase II) - as a strong approach\nthat relies on transfer and contrastive learning. For description generation,\nT5 and BART show their superiority compared to other small-scale pre-trained\nmodels. By applying contrastive learning with the diverse input from beam\nsearch, the metric fusion-based ranking models outperform the direct\ndescription generation models significantly up to 22 ROUGE in topic-exclusive\nsplit and topic-independent split. Furthermore, the outcome descriptions in\nPhase II are supported by human evaluation in over 45.33% chosen compared to\n23.66% in Phase I against the gold descriptions. In the aspect of sentiment\nanalysis, the generated descriptions cannot effectively capture all sentiment\npolarities from paragraphs while doing this task better from the gold\ndescriptions. The automatic generation of new descriptions reduces the human\nefforts in creating them and enriches Wikidata-based knowledge graphs. Our\npaper shows a practical impact on Wikipedia and Wikidata since there are\nthousands of missing descriptions. Finally, we expect WikiDes to be a useful\ndataset for related works in capturing salient information from short\nparagraphs. The curated dataset is publicly available at:\nhttps://github.com/declare-lab/WikiDes.", "published": "2022-09-27 01:28:02", "link": "http://arxiv.org/abs/2209.13101v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Style Matters! Investigating Linguistic Style in Online Communities", "abstract": "Content has historically been the primary lens used to study language in\nonline communities. This paper instead focuses on the linguistic style of\ncommunities. While we know that individuals have distinguishable styles, here\nwe ask whether communities have distinguishable styles. Additionally, while\nprior work has relied on a narrow definition of style, we employ a broad\ndefinition involving 262 features to analyze the linguistic style of 9 online\ncommunities from 3 social media platforms discussing politics, television and\ntravel. We find that communities indeed have distinct styles. Also, style is an\nexcellent predictor of group membership (F-score 0.952 and Accuracy 96.09%).\nWhile on average it is statistically equivalent to predictions using content\nalone, it is more resilient to reductions in training data.", "published": "2022-09-27 02:08:34", "link": "http://arxiv.org/abs/2209.13114v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Direct Speech Translation for Automatic Subtitling", "abstract": "Automatic subtitling is the task of automatically translating the speech of\naudiovisual content into short pieces of timed text, i.e. subtitles and their\ncorresponding timestamps. The generated subtitles need to conform to space and\ntime requirements, while being synchronised with the speech and segmented in a\nway that facilitates comprehension. Given its considerable complexity, the task\nhas so far been addressed through a pipeline of components that separately deal\nwith transcribing, translating, and segmenting text into subtitles, as well as\npredicting timestamps. In this paper, we propose the first direct ST model for\nautomatic subtitling that generates subtitles in the target language along with\ntheir timestamps with a single model. Our experiments on 7 language pairs show\nthat our approach outperforms a cascade system in the same data condition, also\nbeing competitive with production tools on both in-domain and newly-released\nout-domain benchmarks covering new scenarios.", "published": "2022-09-27 06:47:42", "link": "http://arxiv.org/abs/2209.13192v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual analysis of intelligibility classification using English,\n  Korean, and Tamil dysarthric speech datasets", "abstract": "This paper analyzes dysarthric speech datasets from three languages with\ndifferent prosodic systems: English, Korean, and Tamil. We inspect 39 acoustic\nmeasurements which reflect three speech dimensions including voice quality,\npronunciation, and prosody. As multilingual analysis, examination on the mean\nvalues of acoustic measurements by intelligibility levels is conducted.\nFurther, automatic intelligibility classification is performed to scrutinize\nthe optimal feature set by languages. Analyses suggest pronunciation features,\nsuch as Percentage of Correct Consonants, Percentage of Correct Vowels, and\nPercentage of Correct Phonemes to be language-independent measurements. Voice\nquality and prosody features, however, generally present different aspects by\nlanguages. Experimental results additionally show that different speech\ndimension play a greater role for different languages: prosody for English,\npronunciation for Korean, both prosody and pronunciation for Tamil. This paper\ncontributes to speech pathology in that it differentiates between\nlanguage-independent and language-dependent measurements in intelligibility\nclassification for English, Korean, and Tamil dysarthric speech.", "published": "2022-09-27 09:00:41", "link": "http://arxiv.org/abs/2209.13260v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Multilingual Neural Machine Translation System for Indic\n  Languages", "abstract": "Machine Translation System (MTS) serves as an effective tool for\ncommunication by translating text or speech from one language to another\nlanguage. The need of an efficient translation system becomes obvious in a\nlarge multilingual environment like India, where English and a set of Indian\nLanguages (ILs) are officially used. In contrast with English, ILs are still\nentreated as low-resource languages due to unavailability of corpora. In order\nto address such asymmetric nature, multilingual neural machine translation\n(MNMT) system evolves as an ideal approach in this direction. In this paper, we\npropose a MNMT system to address the issues related to low-resource language\ntranslation. Our model comprises of two MNMT systems i.e. for English-Indic\n(one-to-many) and the other for Indic-English (many-to-one) with a shared\nencoder-decoder containing 15 language pairs (30 translation directions). Since\nmost of IL pairs have scanty amount of parallel corpora, not sufficient for\ntraining any machine translation model. We explore various augmentation\nstrategies to improve overall translation quality through the proposed model. A\nstate-of-the-art transformer architecture is used to realize the proposed\nmodel. Trials over a good amount of data reveal its superiority over the\nconventional models. In addition, the paper addresses the use of language\nrelationships (in terms of dialect, script, etc.), particularly about the role\nof high-resource languages of the same family in boosting the performance of\nlow-resource languages. Moreover, the experimental results also show the\nadvantage of backtranslation and domain adaptation for ILs to enhance the\ntranslation quality of both source and target languages. Using all these key\napproaches, our proposed model emerges to be more efficient than the baseline\nmodel in terms of evaluation metrics i.e BLEU (BiLingual Evaluation Understudy)\nscore for a set of ILs.", "published": "2022-09-27 09:51:56", "link": "http://arxiv.org/abs/2209.13279v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment is all you need to win US Presidential elections", "abstract": "Election speeches play an integral role in communicating the vision and\nmission of the candidates. From lofty promises to mud-slinging, the electoral\ncandidate accounts for all. However, there remains an open question about what\nexactly wins over the voters. In this work, we used state-of-the-art natural\nlanguage processing methods to study the speeches and sentiments of the\nRepublican candidate, Donald Trump, and Democratic candidate, Joe Biden,\nfighting for the 2020 US Presidential election. Comparing the racial dichotomy\nof the United States, we analyze what led to the victory and defeat of the\ndifferent candidates. We believe this work will inform the election campaigning\nstrategy and provide a basis for communicating to diverse crowds.", "published": "2022-09-27 16:06:53", "link": "http://arxiv.org/abs/2209.13487v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing Digital Language Support on a Global Scale", "abstract": "The users of endangered languages struggle to thrive in a digitally-mediated\nworld. We have developed an automated method for assessing how well every\nlanguage recognized by ISO 639 is faring in terms of digital language support.\nThe assessment is based on scraping the names of supported languages from the\nwebsites of 143 digital tools selected to represent a full range of ways that\ndigital technology can support languages. The method uses Mokken scale analysis\nto produce an explainable model for quantifying digital language support and\nmonitoring it on a global scale.", "published": "2022-09-27 16:35:49", "link": "http://arxiv.org/abs/2209.13515v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Embarrassingly Easy Document-Level MT Metrics: How to Convert Any\n  Pretrained Metric Into a Document-Level Metric", "abstract": "We hypothesize that existing sentence-level machine translation (MT) metrics\nbecome less effective when the human reference contains ambiguities. To verify\nthis hypothesis, we present a very simple method for extending pretrained\nmetrics to incorporate context at the document level. We apply our method to\nthree popular metrics, BERTScore, Prism, and COMET, and to the reference free\nmetric COMET-QE. We evaluate the extended metrics on the WMT 2021 metrics\nshared task using the provided MQM annotations. Our results show that the\nextended metrics outperform their sentence-level counterparts in about 85% of\nthe tested conditions, when excluding results on low-quality human references.\nAdditionally, we show that our document-level extension of COMET-QE\ndramatically improves its accuracy on discourse phenomena tasks, outperforming\na dedicated baseline by up to 6.1%. Our experimental results support our\ninitial hypothesis and show that a simple extension of the metrics permits them\nto take advantage of context to resolve ambiguities in the reference.", "published": "2022-09-27 19:42:22", "link": "http://arxiv.org/abs/2209.13654v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Effective, Performant Named Entity Recognition System for Noisy\n  Business Telephone Conversation Transcripts", "abstract": "We present a simple yet effective method to train a named entity recognition\n(NER) model that operates on business telephone conversation transcripts that\ncontain noise due to the nature of spoken conversation and artifacts of\nautomatic speech recognition. We first fine-tune LUKE, a state-of-the-art Named\nEntity Recognition (NER) model, on a limited amount of transcripts, then use it\nas the teacher model to teach a smaller DistilBERT-based student model using a\nlarge amount of weakly labeled data and a small amount of human-annotated data.\nThe model achieves high accuracy while also satisfying the practical\nconstraints for inclusion in a commercial telephony product: realtime\nperformance when deployed on cost-effective CPUs rather than GPUs.", "published": "2022-09-27 23:11:42", "link": "http://arxiv.org/abs/2209.13736v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extractive Question Answering on Queries in Hindi and Tamil", "abstract": "Indic languages like Hindi and Tamil are underrepresented in the natural\nlanguage processing (NLP) field compared to languages like English. Due to this\nunderrepresentation, performance on NLP tasks (such as search algorithms) in\nIndic languages are inferior to their English counterparts. This difference\ndisproportionately affects those who come from lower socioeconomic statuses\nbecause they consume the most Internet content in local languages. The goal of\nthis project is to build an NLP model that performs better than pre-existing\nmodels for the task of extractive question-answering (QA) on a public dataset\nin Hindi and Tamil. Extractive QA is an NLP task where answers to questions are\nextracted from a corresponding body of text. To build the best solution, we\nused three different models. The first model is an unmodified cross-lingual\nversion of the NLP model RoBERTa, known as XLM-RoBERTa, that is pretrained on\n100 languages. The second model is based on the pretrained RoBERTa model with\nan extra classification head for the question answering, but we used a custom\nIndic tokenizer, then optimized hyperparameters and fine tuned on the Indic\ndataset. The third model is based on XLM-RoBERTa, but with extra finetuning and\ntraining on the Indic dataset. We hypothesize the third model will perform best\nbecause of the variety of languages the XLM-RoBERTa model has been pretrained\non and the additional finetuning on the Indic dataset. This hypothesis was\nproven wrong because the paired RoBERTa models performed the best as the\ntraining data used was most specific to the task performed as opposed to the\nXLM-RoBERTa models which had much data that was not in either Hindi or Tamil.", "published": "2022-09-27 00:40:21", "link": "http://arxiv.org/abs/2210.06356v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "IdeaReader: A Machine Reading System for Understanding the Idea Flow of\n  Scientific Publications", "abstract": "Understanding the origin and influence of the publication's idea is critical\nto conducting scientific research. However, the proliferation of scientific\npublications makes it difficult for researchers to sort out the evolution of\nall relevant literature. To this end, we present IdeaReader, a machine reading\nsystem that finds out which papers are most likely to inspire or be influenced\nby the target publication and summarizes the ideas of these papers in natural\nlanguage. Specifically, IdeaReader first clusters the references and citations\n(first-order or higher-order) of the target publication, and the obtained\nclusters are regarded as the topics that inspire or are influenced by the\ntarget publication. It then picks out the important papers from each cluster to\nextract the skeleton of the idea flow. Finally, IdeaReader automatically\ngenerates a literature review of the important papers in each topic. Our system\ncan help researchers gain insight into how scientific ideas flow from the\ntarget publication's references to citations by the automatically generated\nsurvey and the visualization of idea flow.", "published": "2022-09-27 08:28:42", "link": "http://arxiv.org/abs/2209.13243v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "EditEval: An Instruction-Based Benchmark for Text Improvements", "abstract": "Evaluation of text generation to date has primarily focused on content\ncreated sequentially, rather than improvements on a piece of text. Writing,\nhowever, is naturally an iterative and incremental process that requires\nexpertise in different modular skills such as fixing outdated information or\nmaking the style more consistent. Even so, comprehensive evaluation of a\nmodel's capacity to perform these skills and the ability to edit remains\nsparse. This work presents EditEval: An instruction-based, benchmark and\nevaluation suite that leverages high-quality existing and new datasets for\nautomatic evaluation of editing capabilities such as making text more cohesive\nand paraphrasing. We evaluate several pre-trained models, which shows that\nInstructGPT and PEER perform the best, but that most baselines fall below the\nsupervised SOTA, particularly when neutralizing and updating information. Our\nanalysis also shows that commonly used metrics for editing tasks do not always\ncorrelate well, and that optimization for prompts with the highest performance\ndoes not necessarily entail the strongest robustness to different models.\nThrough the release of this benchmark and a publicly available leaderboard\nchallenge, we hope to unlock future research in developing models capable of\niterative and more controllable editing.", "published": "2022-09-27 12:26:05", "link": "http://arxiv.org/abs/2209.13331v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PROD: Progressive Distillation for Dense Retrieval", "abstract": "Knowledge distillation is an effective way to transfer knowledge from a\nstrong teacher to an efficient student model. Ideally, we expect the better the\nteacher is, the better the student. However, this expectation does not always\ncome true. It is common that a better teacher model results in a bad student\nvia distillation due to the nonnegligible gap between teacher and student. To\nbridge the gap, we propose PROD, a PROgressive Distillation method, for dense\nretrieval. PROD consists of a teacher progressive distillation and a data\nprogressive distillation to gradually improve the student. We conduct extensive\nexperiments on five widely-used benchmarks, MS MARCO Passage, TREC Passage 19,\nTREC Document 19, MS MARCO Document and Natural Questions, where PROD achieves\nthe state-of-the-art within the distillation methods for dense retrieval. The\ncode and models will be released.", "published": "2022-09-27 12:40:29", "link": "http://arxiv.org/abs/2209.13335v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "BanglaSarc: A Dataset for Sarcasm Detection", "abstract": "Being one of the most widely spoken language in the world, the use of Bangla\nhas been increasing in the world of social media as well. Sarcasm is a positive\nstatement or remark with an underlying negative motivation that is extensively\nemployed in today's social media platforms. There has been a significant\nimprovement in sarcasm detection in English over the previous many years,\nhowever the situation regarding Bangla sarcasm detection remains unchanged. As\na result, it is still difficult to identify sarcasm in bangla, and a lack of\nhigh-quality data is a major contributing factor. This article proposes\nBanglaSarc, a dataset constructed specifically for bangla textual data sarcasm\ndetection. This dataset contains of 5112 comments/status and contents collected\nfrom various online social platforms such as Facebook, YouTube, along with a\nfew online blogs. Due to the limited amount of data collection of categorized\ncomments in Bengali, this dataset will aid in the of study identifying sarcasm,\nrecognizing people's emotion, detecting various types of Bengali expressions,\nand other domains. The dataset is publicly available at\nhttps://www.kaggle.com/datasets/sakibapon/banglasarc.", "published": "2022-09-27 15:28:21", "link": "http://arxiv.org/abs/2209.13461v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Information Extraction and Human-Robot Dialogue towards Real-life Tasks:\n  A Baseline Study with the MobileCS Dataset", "abstract": "Recently, there have merged a class of task-oriented dialogue (TOD) datasets\ncollected through Wizard-of-Oz simulated games. However, the Wizard-of-Oz data\nare in fact simulated data and thus are fundamentally different from real-life\nconversations, which are more noisy and casual. Recently, the SereTOD challenge\nis organized and releases the MobileCS dataset, which consists of real-world\ndialog transcripts between real users and customer-service staffs from China\nMobile. Based on the MobileCS dataset, the SereTOD challenge has two tasks, not\nonly evaluating the construction of the dialogue system itself, but also\nexamining information extraction from dialog transcripts, which is crucial for\nbuilding the knowledge base for TOD. This paper mainly presents a baseline\nstudy of the two tasks with the MobileCS dataset. We introduce how the two\nbaselines are constructed, the problems encountered, and the results. We\nanticipate that the baselines can facilitate exciting future research to build\nhuman-robot dialogue systems for real-life tasks.", "published": "2022-09-27 15:30:43", "link": "http://arxiv.org/abs/2209.13464v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Interactivism in Spoken Dialogue Systems", "abstract": "The interactivism model introduces a dynamic approach to language,\ncommunication and cognition. In this work, we explore this fundamental theory\nin the context of dialogue modelling for spoken dialogue systems (SDS). To\nextend such a theoretical framework, we present a set of design principles\nwhich adhere to central psycholinguistic and communication theories to achieve\ninteractivism in SDS. From these, key ideas are linked to constitute the basis\nof our proposed design principles.", "published": "2022-09-27 16:56:58", "link": "http://arxiv.org/abs/2209.13547v2", "categories": ["cs.CL", "cs.CY", "H.1.2, H.5.2, I.2.11, J.4"], "primary_category": "cs.CL"}
{"title": "What Are You Anxious About? Examining Subjects of Anxiety during the\n  COVID-19 Pandemic", "abstract": "COVID-19 poses disproportionate mental health consequences to the public\nduring different phases of the pandemic. We use a computational approach to\ncapture the specific aspects that trigger an online community's anxiety about\nthe pandemic and investigate how these aspects change over time. First, we\nidentified nine subjects of anxiety (SOAs) in a sample of Reddit posts ($N$=86)\nfrom r/COVID19\\_support using thematic analysis. Then, we quantified Reddit\nusers' anxiety by training algorithms on a manually annotated sample ($N$=793)\nto automatically label the SOAs in a larger chronological sample ($N$=6,535).\nThe nine SOAs align with items in various recently developed pandemic anxiety\nmeasurement scales. We observed that Reddit users' concerns about health risks\nremained high in the first eight months of the pandemic. These concerns\ndiminished dramatically despite the surge of cases occurring later. In general,\nusers' language disclosing the SOAs became less intense as the pandemic\nprogressed. However, worries about mental health and the future increased\nsteadily throughout the period covered in this study. People also tended to use\nmore intense language to describe mental health concerns than health risks or\ndeath concerns. Our results suggest that this online group's mental health\ncondition does not necessarily improve despite COVID-19 gradually weakening as\na health threat due to appropriate countermeasures. Our system lays the\ngroundwork for population health and epidemiology scholars to examine aspects\nthat provoke pandemic anxiety in a timely fashion.", "published": "2022-09-27 05:22:38", "link": "http://arxiv.org/abs/2209.13595v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "mRobust04: A Multilingual Version of the TREC Robust 2004 Benchmark", "abstract": "Robust 2004 is an information retrieval benchmark whose large number of\njudgments per query make it a reliable evaluation dataset. In this paper, we\npresent mRobust04, a multilingual version of Robust04 that was translated to 8\nlanguages using Google Translate. We also provide results of three different\nmultilingual retrievers on this dataset. The dataset is available at\nhttps://huggingface.co/datasets/unicamp-dl/mrobust", "published": "2022-09-27 23:14:37", "link": "http://arxiv.org/abs/2209.13738v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A general-purpose material property data extraction pipeline from large\n  polymer corpora using Natural Language Processing", "abstract": "The ever-increasing number of materials science articles makes it hard to\ninfer chemistry-structure-property relations from published literature. We used\nnatural language processing (NLP) methods to automatically extract material\nproperty data from the abstracts of polymer literature. As a component of our\npipeline, we trained MaterialsBERT, a language model, using 2.4 million\nmaterials science abstracts, which outperforms other baseline models in three\nout of five named entity recognition datasets when used as the encoder for\ntext. Using this pipeline, we obtained ~300,000 material property records from\n~130,000 abstracts in 60 hours. The extracted data was analyzed for a diverse\nrange of applications such as fuel cells, supercapacitors, and polymer solar\ncells to recover non-trivial insights. The data extracted through our pipeline\nis made available through a web platform at https://polymerscholar.org which\ncan be used to locate material property data recorded in abstracts\nconveniently. This work demonstrates the feasibility of an automatic pipeline\nthat starts from published literature and ends with a complete set of extracted\nmaterial property information.", "published": "2022-09-27 03:47:03", "link": "http://arxiv.org/abs/2209.13136v1", "categories": ["cs.CL", "cond-mat.mtrl-sci", "cond-mat.soft"], "primary_category": "cs.CL"}
{"title": "DAMO-NLP at NLPCC-2022 Task 2: Knowledge Enhanced Robust NER for Speech\n  Entity Linking", "abstract": "Speech Entity Linking aims to recognize and disambiguate named entities in\nspoken languages. Conventional methods suffer gravely from the unfettered\nspeech styles and the noisy transcripts generated by ASR systems. In this\npaper, we propose a novel approach called Knowledge Enhanced Named Entity\nRecognition (KENER), which focuses on improving robustness through painlessly\nincorporating proper knowledge in the entity recognition stage and thus\nimproving the overall performance of entity linking. KENER first retrieves\ncandidate entities for a sentence without mentions, and then utilizes the\nentity descriptions as extra information to help recognize mentions. The\ncandidate entities retrieved by a dense retrieval module are especially useful\nwhen the input is short or noisy. Moreover, we investigate various data\nsampling strategies and design effective loss functions, in order to improve\nthe quality of retrieved entities in both recognition and disambiguation\nstages. Lastly, a linking with filtering module is applied as the final\nsafeguard, making it possible to filter out wrongly-recognized mentions. Our\nsystem achieves 1st place in Track 1 and 2nd place in Track 2 of NLPCC-2022\nShared Task 2.", "published": "2022-09-27 06:43:56", "link": "http://arxiv.org/abs/2209.13187v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Text-Adaptive Multiple Visual Prototype Matching for Video-Text\n  Retrieval", "abstract": "Cross-modal retrieval between videos and texts has gained increasing research\ninterest due to the rapid emergence of videos on the web. Generally, a video\ncontains rich instance and event information and the query text only describes\na part of the information. Thus, a video can correspond to multiple different\ntext descriptions and queries. We call this phenomenon the ``Video-Text\nCorrespondence Ambiguity'' problem. Current techniques mostly concentrate on\nmining local or multi-level alignment between contents of a video and text\n(\\textit{e.g.}, object to entity and action to verb). It is difficult for these\nmethods to alleviate the video-text correspondence ambiguity by describing a\nvideo using only one single feature, which is required to be matched with\nmultiple different text features at the same time. To address this problem, we\npropose a Text-Adaptive Multiple Visual Prototype Matching model, which\nautomatically captures multiple prototypes to describe a video by adaptive\naggregation of video token features. Given a query text, the similarity is\ndetermined by the most similar prototype to find correspondence in the video,\nwhich is termed text-adaptive matching. To learn diverse prototypes for\nrepresenting the rich information in videos, we propose a variance loss to\nencourage different prototypes to attend to different contents of the video.\nOur method outperforms state-of-the-art methods on four public video retrieval\ndatasets.", "published": "2022-09-27 11:13:48", "link": "http://arxiv.org/abs/2209.13307v1", "categories": ["cs.CV", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "How GPT-3 responds to different publics on climate change and Black\n  Lives Matter: A critical appraisal of equity in conversational AI", "abstract": "Autoregressive language models, which use deep learning to produce human-like\ntexts, have become increasingly widespread. Such models are powering popular\nvirtual assistants in areas like smart health, finance, and autonomous driving.\nWhile the parameters of these large language models are improving, concerns\npersist that these models might not work equally for all subgroups in society.\nDespite growing discussions of AI fairness across disciplines, there lacks\nsystemic metrics to assess what equity means in dialogue systems and how to\nengage different populations in the assessment loop. Grounded in theories of\ndeliberative democracy and science and technology studies, this paper proposes\nan analytical framework for unpacking the meaning of equity in human-AI\ndialogues. Using this framework, we conducted an auditing study to examine how\nGPT-3 responded to different sub-populations on crucial science and social\ntopics: climate change and the Black Lives Matter (BLM) movement. Our corpus\nconsists of over 20,000 rounds of dialogues between GPT-3 and 3290 individuals\nwho vary in gender, race and ethnicity, education level, English as a first\nlanguage, and opinions toward the issues. We found a substantively worse user\nexperience with GPT-3 among the opinion and the education minority\nsubpopulations; however, these two groups achieved the largest knowledge gain,\nchanging attitudes toward supporting BLM and climate change efforts after the\nchat. We traced these user experience divides to conversational differences and\nfound that GPT-3 used more negative expressions when it responded to the\neducation and opinion minority groups, compared to its responses to the\nmajority groups. We discuss the implications of our findings for a deliberative\nconversational AI system that centralizes diversity, equity, and inclusion.", "published": "2022-09-27 18:44:41", "link": "http://arxiv.org/abs/2209.13627v2", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Local Grammar-Based Coding Revisited", "abstract": "We revisit the problem of minimal local grammar-based coding. In this\nsetting, the local grammar encoder encodes grammars symbol by symbol, whereas\nthe minimal grammar transform minimizes the grammar length in a preset class of\ngrammars as given by the length of local grammar encoding. It has been known\nthat such minimal codes are strongly universal for a strictly positive entropy\nrate, whereas the number of rules in the minimal grammar constitutes an upper\nbound for the mutual information of the source. Whereas the fully minimal code\nis likely intractable, the constrained minimal block code can be efficiently\ncomputed. In this article, we present a new, simpler, and more general proof of\nstrong universality of the minimal block code, regardless of the entropy rate.\nThe proof is based on a simple Zipfian bound for ranked probabilities. By the\nway, we also show empirically that the number of rules in the minimal block\ncode cannot clearly discriminate between long-memory and memoryless sources,\nsuch as a text in English and a random permutation of its characters. This\ncontradicts our previous expectations.", "published": "2022-09-27 19:05:22", "link": "http://arxiv.org/abs/2209.13636v2", "categories": ["cs.IT", "cs.CL", "math.IT", "94A29"], "primary_category": "cs.IT"}
{"title": "Improving Radiology Report Generation Systems by Removing Hallucinated\n  References to Non-existent Priors", "abstract": "Current deep learning models trained to generate radiology reports from chest\nradiographs are capable of producing clinically accurate, clear, and actionable\ntext that can advance patient care. However, such systems all succumb to the\nsame problem: making hallucinated references to non-existent prior reports.\nSuch hallucinations occur because these models are trained on datasets of\nreal-world patient reports that inherently refer to priors. To this end, we\npropose two methods to remove references to priors in radiology reports: (1) a\nGPT-3-based few-shot approach to rewrite medical reports without references to\npriors; and (2) a BioBERT-based token classification approach to directly\nremove words referring to priors. We use the aforementioned approaches to\nmodify MIMIC-CXR, a publicly available dataset of chest X-rays and their\nassociated free-text radiology reports; we then retrain CXR-RePaiR, a radiology\nreport generation system, on the adapted MIMIC-CXR dataset. We find that our\nre-trained model--which we call CXR-ReDonE--outperforms previous report\ngeneration methods on clinical metrics, achieving an average BERTScore of\n0.2351 (2.57% absolute improvement). We expect our approach to be broadly\nvaluable in enabling current radiology report generation systems to be more\ndirectly integrated into clinical pipelines.", "published": "2022-09-27 00:44:41", "link": "http://arxiv.org/abs/2210.06340v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Predicting Affective Vocal Bursts with Finetuned wav2vec 2.0", "abstract": "The studies of predicting affective states from human voices have relied\nheavily on speech. This study, indeed, explores the recognition of humans'\naffective state from their vocal burst, a short non-verbal vocalization.\nBorrowing the idea from the recent success of wav2vec 2.0, we evaluated\nfinetuned wav2vec 2.0 models from different datasets to predict the affective\nstate of the speaker from their vocal burst. The finetuned wav2vec 2.0 models\nare then trained on the vocal burst data. The results show that the finetuned\nwav2vec 2.0 models, particularly on an affective speech dataset, outperform the\nbaseline model, which is handcrafted acoustic features. However, there is no\nlarge gap between the model finetuned on non-affective speech dataset and\naffective speech dataset.", "published": "2022-09-27 04:30:33", "link": "http://arxiv.org/abs/2209.13146v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Hyperbolic Timbre Embedding for Musical Instrument Sound Synthesis Based\n  on Variational Autoencoders", "abstract": "In this paper, we propose a musical instrument sound synthesis (MISS) method\nbased on a variational autoencoder (VAE) that has a hierarchy-inducing latent\nspace for timbre. VAE-based MISS methods embed an input signal into a\nlow-dimensional latent representation that captures the characteristics of the\ninput. Adequately manipulating this representation leads to sound morphing and\ntimbre replacement. Although most VAE-based MISS methods seek a disentangled\nrepresentation of pitch and timbre, how to capture an underlying structure in\ntimbre remains an open problem. To address this problem, we focus on the fact\nthat musical instruments can be hierarchically classified on the basis of their\nphysical mechanisms. Motivated by this hierarchy, we propose a VAE-based MISS\nmethod by introducing a hyperbolic space for timbre. The hyperbolic space can\nrepresent treelike data more efficiently than the Euclidean space owing to its\nexponential growth property. Results of experiments show that the proposed\nmethod provides an efficient latent representation of timbre compared with the\nmethod using the Euclidean space.", "published": "2022-09-27 07:28:28", "link": "http://arxiv.org/abs/2209.13211v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Automated Sex Classification of Children's Voices and Changes in\n  Differentiating Factors with Age", "abstract": "Sex classification of children's voices allows for an investigation of the\ndevelopment of secondary sex characteristics which has been a key interest in\nthe field of speech analysis. This research investigated a broad range of\nacoustic features from scripted and spontaneous speech and applied a\nhierarchical clustering-based machine learning model to distinguish the sex of\nchildren aged between 5 and 15 years. We proposed an optimal feature set and\nour modelling achieved an average F1 score (the harmonic mean of the precision\nand recall) of 0.84 across all ages. Our results suggest that the sex\nclassification is generally more accurate when a model is developed for each\nyear group rather than for children in 4-year age bands, with classification\naccuracy being better for older age groups. We found that spontaneous speech\ncould provide more helpful cues in sex classification than scripted speech,\nespecially for children younger than 7 years. For younger age groups, a broad\nrange of acoustic factors contributed evenly to sex classification, while for\nolder age groups, F0-related acoustic factors were found to be the most\ncritical predictors generally. Other important acoustic factors for older age\ngroups include vocal tract length estimators, spectral flux, loudness and\nunvoiced features.", "published": "2022-09-27 02:01:01", "link": "http://arxiv.org/abs/2209.13112v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Beyond Heart Murmur Detection: Automatic Murmur Grading from\n  Phonocardiogram", "abstract": "Objective: Murmurs are abnormal heart sounds, identified by experts through\ncardiac auscultation. The murmur grade, a quantitative measure of the murmur\nintensity, is strongly correlated with the patient's clinical condition. This\nwork aims to estimate each patient's murmur grade (i.e., absent, soft, loud)\nfrom multiple auscultation location phonocardiograms (PCGs) of a large\npopulation of pediatric patients from a low-resource rural area. Methods: The\nMel spectrogram representation of each PCG recording is given to an ensemble of\n15 convolutional residual neural networks with channel-wise attention\nmechanisms to classify each PCG recording. The final murmur grade for each\npatient is derived based on the proposed decision rule and considering all\nestimated labels for available recordings. The proposed method is\ncross-validated on a dataset consisting of 3456 PCG recordings from 1007\npatients using a stratified ten-fold cross-validation. Additionally, the method\nwas tested on a hidden test set comprised of 1538 PCG recordings from 442\npatients. Results: The overall cross-validation performances for patient-level\nmurmur gradings are 86.3% and 81.6% in terms of the unweighted average of\nsensitivities and F1-scores, respectively. The sensitivities (and F1-scores)\nfor absent, soft, and loud murmurs are 90.7% (93.6%), 75.8% (66.8%), and 92.3%\n(84.2%), respectively. On the test set, the algorithm achieves an unweighted\naverage of sensitivities of 80.4% and an F1-score of 75.8%. Conclusions: This\nstudy provides a potential approach for algorithmic pre-screening in\nlow-resource settings with relatively high expert screening costs.\nSignificance: The proposed method represents a significant step beyond\ndetection of murmurs, providing characterization of intensity which may provide\na enhanced classification of clinical outcomes.", "published": "2022-09-27 13:42:22", "link": "http://arxiv.org/abs/2209.13385v2", "categories": ["q-bio.QM", "cs.SD", "eess.AS"], "primary_category": "q-bio.QM"}
{"title": "Computing Melodic Templates in Oral Music Traditions", "abstract": "The term melodic template or skeleton refers to a basic melody which is\nsubject to variation during a music performance. In many oral music tradition,\nthese templates are implicitly passed throughout generations without ever being\nformalized in a score. In this work, we introduce a new geometric optimization\nproblem, the spanning tube problem, to approximate a melodic template for a set\nof labeled performance transcriptions corresponding to an specific style in\noral music traditions. Given a set of $n$ piecewise linear functions, we solve\nthe problem of finding a continuous function, $f^*$, and a minimum value,\n$\\varepsilon^*$, such that, the vertical segment of length $2\\varepsilon^*$\ncentered at $(x,f^*(x))$ intersects at least $p$ functions ($p\\leq n$). The\nmethod explored here also provide a novel tool for quantitatively assess the\namount of melodic variation which occurs across performances.", "published": "2022-09-27 08:40:55", "link": "http://arxiv.org/abs/2209.13598v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
