{"title": "A Multimodal Dialogue System for Conversational Image Editing", "abstract": "In this paper, we present a multimodal dialogue system for Conversational\nImage Editing. We formulate our multimodal dialogue system as a Partially\nObserved Markov Decision Process (POMDP) and trained it with Deep Q-Network\n(DQN) and a user simulator. Our evaluation shows that the DQN policy\noutperforms a rule-based baseline policy, achieving 90\\% success rate under\nhigh error rates. We also conducted a real user study and analyzed real user\nbehavior.", "published": "2020-02-16 01:09:41", "link": "http://arxiv.org/abs/2002.06484v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation with Joint Representation", "abstract": "Though early successes of Statistical Machine Translation (SMT) systems are\nattributed in part to the explicit modelling of the interaction between any two\nsource and target units, e.g., alignment, the recent Neural Machine Translation\n(NMT) systems resort to the attention which partially encodes the interaction\nfor efficiency. In this paper, we employ Joint Representation that fully\naccounts for each possible interaction. We sidestep the inefficiency issue by\nrefining representations with the proposed efficient attention operation. The\nresulting Reformer models offer a new Sequence-to- Sequence modelling paradigm\nbesides the Encoder-Decoder framework and outperform the Transformer baseline\nin either the small scale IWSLT14 German-English, English-German and IWSLT15\nVietnamese-English or the large scale NIST12 Chinese-English translation tasks\nby about 1 BLEU point.We also propose a systematic model scaling approach,\nallowing the Reformer model to beat the state-of-the-art Transformer in IWSLT14\nGerman-English and NIST12 Chinese-English with about 50% fewer parameters. The\ncode is publicly available at https://github.com/lyy1994/reformer.", "published": "2020-02-16 09:42:01", "link": "http://arxiv.org/abs/2002.06546v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Detection of Subjective Bias using Contextualized Word\n  Embeddings", "abstract": "Subjective bias detection is critical for applications like propaganda\ndetection, content recommendation, sentiment analysis, and bias neutralization.\nThis bias is introduced in natural language via inflammatory words and phrases,\ncasting doubt over facts, and presupposing the truth. In this work, we perform\ncomprehensive experiments for detecting subjective bias using BERT-based models\non the Wiki Neutrality Corpus(WNC). The dataset consists of $360k$ labeled\ninstances, from Wikipedia edits that remove various instances of the bias. We\nfurther propose BERT-based ensembles that outperform state-of-the-art methods\nlike $BERT_{large}$ by a margin of $5.6$ F1 score.", "published": "2020-02-16 18:39:16", "link": "http://arxiv.org/abs/2002.06644v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-layer Representation Fusion for Neural Machine Translation", "abstract": "Neural machine translation systems require a number of stacked layers for\ndeep models. But the prediction depends on the sentence representation of the\ntop-most layer with no access to low-level representations. This makes it more\ndifficult to train the model and poses a risk of information loss to\nprediction. In this paper, we propose a multi-layer representation fusion\n(MLRF) approach to fusing stacked layers. In particular, we design three fusion\nfunctions to learn a better representation from the stack. Experimental results\nshow that our approach yields improvements of 0.92 and 0.56 BLEU points over\nthe strong Transformer baseline on IWSLT German-English and NIST\nChinese-English MT tasks respectively. The result is new state-of-the-art in\nGerman-English translation.", "published": "2020-02-16 23:53:07", "link": "http://arxiv.org/abs/2002.06714v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Generate Multiple Style Transfer Outputs for an Input\n  Sentence", "abstract": "Text style transfer refers to the task of rephrasing a given text in a\ndifferent style. While various methods have been proposed to advance the state\nof the art, they often assume the transfer output follows a delta distribution,\nand thus their models cannot generate different style transfer results for a\ngiven input text. To address the limitation, we propose a one-to-many text\nstyle transfer framework. In contrast to prior works that learn a one-to-one\nmapping that converts an input sentence to one output sentence, our approach\nlearns a one-to-many mapping that can convert an input sentence to multiple\ndifferent output sentences, while preserving the input content. This is\nachieved by applying adversarial training with a latent decomposition scheme.\nSpecifically, we decompose the latent representation of the input sentence to a\nstyle code that captures the language style variation and a content code that\nencodes the language style-independent content. We then combine the content\ncode with the style code for generating a style transfer output. By combining\nthe same content code with a different style code, we generate a different\nstyle transfer output. Extensive experimental results with comparisons to\nseveral text style transfer approaches on multiple public datasets using a\ndiverse set of performance metrics validate effectiveness of the proposed\napproach.", "published": "2020-02-16 07:10:45", "link": "http://arxiv.org/abs/2002.06525v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Neural Models for Parsing Natural Language into First-Order\n  Logic", "abstract": "Semantic parsing is the task of obtaining machine-interpretable\nrepresentations from natural language text. We consider one such formal\nrepresentation - First-Order Logic (FOL) and explore the capability of neural\nmodels in parsing English sentences to FOL. We model FOL parsing as a sequence\nto sequence mapping task where given a natural language sentence, it is encoded\ninto an intermediate representation using an LSTM followed by a decoder which\nsequentially generates the predicates in the corresponding FOL formula. We\nimprove the standard encoder-decoder model by introducing a variable alignment\nmechanism that enables it to align variables across predicates in the predicted\nFOL. We further show the effectiveness of predicting the category of FOL entity\n- Unary, Binary, Variables and Scoped Entities, at each decoder step as an\nauxiliary task on improving the consistency of generated FOL. We perform\nrigorous evaluations and extensive ablations. We also aim to release our code\nas well as large scale FOL dataset along with models to aid further research in\nlogic-based parsing and inference in NLP.", "published": "2020-02-16 09:22:32", "link": "http://arxiv.org/abs/2002.06544v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Utility of General Domain Transfer Learning for Medical Language\n  Tasks", "abstract": "The purpose of this study is to analyze the efficacy of transfer learning\ntechniques and transformer-based models as applied to medical natural language\nprocessing (NLP) tasks, specifically radiological text classification. We used\n1,977 labeled head CT reports, from a corpus of 96,303 total reports, to\nevaluate the efficacy of pretraining using general domain corpora and a\ncombined general and medical domain corpus with a bidirectional representations\nfrom transformers (BERT) model for the purpose of radiological text\nclassification. Model performance was benchmarked to a logistic regression\nusing bag-of-words vectorization and a long short-term memory (LSTM)\nmulti-label multi-class classification model, and compared to the published\nliterature in medical text classification. The BERT models using either set of\npretrained checkpoints outperformed the logistic regression model, achieving\nsample-weighted average F1-scores of 0.87 and 0.87 for the general domain model\nand the combined general and biomedical-domain model. General text transfer\nlearning may be a viable technique to generate state-of-the-art results within\nmedical NLP tasks on radiological corpora, outperforming other deep models such\nas LSTMs. The efficacy of pretraining and transformer-based models could serve\nto facilitate the creation of groundbreaking NLP models in the uniquely\nchallenging data environment of medical text.", "published": "2020-02-16 20:20:38", "link": "http://arxiv.org/abs/2002.06670v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Text-based Question Answering from Information Retrieval and Deep Neural\n  Network Perspectives: A Survey", "abstract": "Text-based Question Answering (QA) is a challenging task which aims at\nfinding short concrete answers for users' questions. This line of research has\nbeen widely studied with information retrieval techniques and has received\nincreasing attention in recent years by considering deep neural network\napproaches. Deep learning approaches, which are the main focus of this paper,\nprovide a powerful technique to learn multiple layers of representations and\ninteraction between questions and texts. In this paper, we provide a\ncomprehensive overview of different models proposed for the QA task, including\nboth traditional information retrieval perspective, and more recent deep neural\nnetwork perspective. We also introduce well-known datasets for the task and\npresent available results from the literature to have a comparison between\ndifferent techniques.", "published": "2020-02-16 16:24:39", "link": "http://arxiv.org/abs/2002.06612v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "SBERT-WK: A Sentence Embedding Method by Dissecting BERT-based Word\n  Models", "abstract": "Sentence embedding is an important research topic in natural language\nprocessing (NLP) since it can transfer knowledge to downstream tasks.\nMeanwhile, a contextualized word representation, called BERT, achieves the\nstate-of-the-art performance in quite a few NLP tasks. Yet, it is an open\nproblem to generate a high quality sentence representation from BERT-based word\nmodels. It was shown in previous study that different layers of BERT capture\ndifferent linguistic properties. This allows us to fusion information across\nlayers to find better sentence representation. In this work, we study the\nlayer-wise pattern of the word representation of deep contextualized models.\nThen, we propose a new sentence embedding method by dissecting BERT-based word\nmodels through geometric analysis of the space spanned by the word\nrepresentation. It is called the SBERT-WK method. No further training is\nrequired in SBERT-WK. We evaluate SBERT-WK on semantic textual similarity and\ndownstream supervised tasks. Furthermore, ten sentence-level probing tasks are\npresented for detailed linguistic analysis. Experiments show that SBERT-WK\nachieves the state-of-the-art performance. Our codes are publicly available.", "published": "2020-02-16 19:02:52", "link": "http://arxiv.org/abs/2002.06652v2", "categories": ["cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Speech Corpus of Ainu Folklore and End-to-end Speech Recognition for\n  Ainu Language", "abstract": "Ainu is an unwritten language that has been spoken by Ainu people who are one\nof the ethnic groups in Japan. It is recognized as critically endangered by\nUNESCO and archiving and documentation of its language heritage is of paramount\nimportance. Although a considerable amount of voice recordings of Ainu folklore\nhas been produced and accumulated to save their culture, only a quite limited\nparts of them are transcribed so far. Thus, we started a project of automatic\nspeech recognition (ASR) for the Ainu language in order to contribute to the\ndevelopment of annotated language archives. In this paper, we report speech\ncorpus development and the structure and performance of end-to-end ASR for\nAinu. We investigated four modeling units (phone, syllable, word piece, and\nword) and found that the syllable-based model performed best in terms of both\nword and phone recognition accuracy, which were about 60% and over 85%\nrespectively in speaker-open condition. Furthermore, word and phone accuracy of\n80% and 90% has been achieved in a speaker-closed setting. We also found out\nthat a multilingual ASR training with additional speech corpora of English and\nJapanese further improves the speaker-open test accuracy.", "published": "2020-02-16 20:44:11", "link": "http://arxiv.org/abs/2002.06675v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Gaussian Smoothen Semantic Features (GSSF) -- Exploring the Linguistic\n  Aspects of Visual Captioning in Indian Languages (Bengali) Using MSCOCO\n  Framework", "abstract": "In this work, we have introduced Gaussian Smoothen Semantic Features (GSSF)\nfor Better Semantic Selection for Indian regional language-based image\ncaptioning and introduced a procedure where we used the existing translation\nand English crowd-sourced sentences for training. We have shown that this\narchitecture is a promising alternative source, where there is a crunch in\nresources. Our main contribution of this work is the development of deep\nlearning architectures for the Bengali language (is the fifth widely spoken\nlanguage in the world) with a completely different grammar and language\nattributes. We have shown that these are working well for complex applications\nlike language generation from image contexts and can diversify the\nrepresentation through introducing constraints, more extensive features, and\nunique feature spaces. We also established that we could achieve absolute\nprecision and diversity when we use smoothened semantic tensor with the\ntraditional LSTM and feature decomposition networks. With better learning\narchitecture, we succeeded in establishing an automated algorithm and\nassessment procedure that can help in the evaluation of competent applications\nwithout the requirement for expertise and human intervention.", "published": "2020-02-16 23:03:32", "link": "http://arxiv.org/abs/2002.06701v1", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Real-time binaural speech separation with preserved spatial cues", "abstract": "Deep learning speech separation algorithms have achieved great success in\nimproving the quality and intelligibility of separated speech from mixed audio.\nMost previous methods focused on generating a single-channel output for each of\nthe target speakers, hence discarding the spatial cues needed for the\nlocalization of sound sources in space. However, preserving the spatial\ninformation is important in many applications that aim to accurately render the\nacoustic scene such as in hearing aids and augmented reality (AR). Here, we\npropose a speech separation algorithm that preserves the interaural cues of\nseparated sound sources and can be implemented with low latency and high\nfidelity, therefore enabling a real-time modification of the acoustic scene.\nBased on the time-domain audio separation network (TasNet), a single-channel\ntime-domain speech separation system that can be implemented in real-time, we\npropose a multi-input-multi-output (MIMO) end-to-end extension of TasNet that\ntakes binaural mixed audio as input and simultaneously separates target\nspeakers in both channels. Experimental results show that the proposed\nend-to-end MIMO system is able to significantly improve the separation\nperformance and keep the perceived location of the modified sources intact in\nvarious acoustic scenes.", "published": "2020-02-16 18:18:19", "link": "http://arxiv.org/abs/2002.06637v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech-to-Singing Conversion in an Encoder-Decoder Framework", "abstract": "In this paper our goal is to convert a set of spoken lines into sung ones.\nUnlike previous signal processing based methods, we take a learning based\napproach to the problem. This allows us to automatically model various aspects\nof this transformation, thus overcoming dependence on specific inputs such as\nhigh quality singing templates or phoneme-score synchronization information.\nSpecifically, we propose an encoder--decoder framework for our task. Given\ntime-frequency representations of speech and a target melody contour, we learn\nencodings that enable us to synthesize singing that preserves the linguistic\ncontent and timbre of the speaker while adhering to the target melody. We also\npropose a multi-task learning based objective to improve lyric intelligibility.\nWe present a quantitative and qualitative analysis of our framework.", "published": "2020-02-16 15:33:41", "link": "http://arxiv.org/abs/2002.06595v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-Task Siamese Neural Network for Improving Replay Attack Detection", "abstract": "Automatic speaker verification systems are vulnerable to audio replay attacks\nwhich bypass security by replaying recordings of authorized speakers. Replay\nattack detection (RA) detection systems built upon Residual Neural Networks\n(ResNet)s have yielded astonishing results on the public benchmark ASVspoof\n2019 Physical Access challenge. With most teams using fine-tuned feature\nextraction pipelines and model architectures, the generalizability of such\nsystems remains questionable though. In this work, we analyse the effect of\ndiscriminative feature learning in a multi-task learning (MTL) setting can have\non the generalizability and discriminability of RA detection systems. We use a\npopular ResNet architecture optimized by the cross-entropy criterion as our\nbaseline and compare it to the same architecture optimized by MTL using Siamese\nNeural Networks (SNN). It can be shown that SNN outperform the baseline by\nrelative 26.8 % Equal Error Rate (EER). We further enhance the model's\narchitecture and demonstrate that SNN with additional reconstruction loss yield\nanother significant improvement of relative 13.8 % EER.", "published": "2020-02-16 00:21:16", "link": "http://arxiv.org/abs/2002.07629v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
