{"title": "Creating Reverse Bilingual Dictionaries", "abstract": "Bilingual dictionaries are expensive resources and not many are available\nwhen one of the languages is resource-poor. In this paper, we propose\nalgorithms for creation of new reverse bilingual dictionaries from existing\nbilingual dictionaries in which English is one of the two languages. Our\nalgorithms exploit the similarity between word-concept pairs using the English\nWordnet to produce reverse dictionary entries. Since our algorithms rely on\navailable bilingual dictionaries, they are applicable to any bilingual\ndictionary as long as one of the two languages has Wordnet type lexical\nontology.", "published": "2022-08-08 01:41:55", "link": "http://arxiv.org/abs/2208.03863v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatically constructing Wordnet synsets", "abstract": "Manually constructing a Wordnet is a difficult task, needing years of\nexperts' time. As a first step to automatically construct full Wordnets, we\npropose approaches to generate Wordnet synsets for languages both resource-rich\nand resource-poor, using publicly available Wordnets, a machine translator\nand/or a single bilingual dictionary. Our algorithms translate synsets of\nexisting Wordnets to a target language T, then apply a ranking method on the\ntranslation candidates to find best translations in T. Our approaches are\napplicable to any language which has at least one existing bilingual dictionary\ntranslating from English to it.", "published": "2022-08-08 02:02:18", "link": "http://arxiv.org/abs/2208.03870v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Creating Lexical Resources for Endangered Languages", "abstract": "This paper examines approaches to generate lexical resources for endangered\nlanguages. Our algorithms construct bilingual dictionaries and multilingual\nthesauruses using public Wordnets and a machine translator (MT). Since our work\nrelies on only one bilingual dictionary between an endangered language and an\n\"intermediate helper\" language, it is applicable to languages that lack many\nexisting resources.", "published": "2022-08-08 02:31:28", "link": "http://arxiv.org/abs/2208.03876v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DialogSum Challenge: Results of the Dialogue Summarization Shared Task", "abstract": "We report the results of DialogSum Challenge, the shared task on summarizing\nreal-life scenario dialogues at INLG 2022. Four teams participate in this\nshared task and three submit their system reports, exploring different methods\nto improve the performance of dialogue summarization. Although there is a great\nimprovement over the baseline models regarding automatic evaluation metrics,\nsuch as Rouge scores, we find that there is a salient gap between model\ngenerated outputs and human annotated summaries by human evaluation from\nmultiple aspects. These findings demonstrate the difficulty of dialogue\nsummarization and suggest that more fine-grained evaluatuion metrics are in\nneed.", "published": "2022-08-08 03:39:42", "link": "http://arxiv.org/abs/2208.03898v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Enhanced Text-to-SQL Parsing via Iteratively Learning Schema\n  Linking Graph", "abstract": "The generalizability to new databases is of vital importance to Text-to-SQL\nsystems which aim to parse human utterances into SQL statements. Existing works\nachieve this goal by leveraging the exact matching method to identify the\nlexical matching between the question words and the schema items. However,\nthese methods fail in other challenging scenarios, such as the synonym\nsubstitution in which the surface form differs between the corresponding\nquestion words and schema items. In this paper, we propose a framework named\nISESL-SQL to iteratively build a semantic enhanced schema-linking graph between\nquestion tokens and database schemas. First, we extract a schema linking graph\nfrom PLMs through a probing procedure in an unsupervised manner. Then the\nschema linking graph is further optimized during the training process through a\ndeep graph learning method. Meanwhile, we also design an auxiliary task called\ngraph regularization to improve the schema information mentioned in the\nschema-linking graph. Extensive experiments on three benchmarks demonstrate\nthat ISESL-SQL could consistently outperform the baselines and further\ninvestigations show its generalizability and robustness.", "published": "2022-08-08 03:59:33", "link": "http://arxiv.org/abs/2208.03903v1", "categories": ["cs.CL", "03B65", "I.7.0"], "primary_category": "cs.CL"}
{"title": "Generating Coherent Narratives by Learning Dynamic and Discrete Entity\n  States with a Contrastive Framework", "abstract": "Despite advances in generating fluent texts, existing pretraining models tend\nto attach incoherent event sequences to involved entities when generating\nnarratives such as stories and news. We conjecture that such issues result from\nrepresenting entities as static embeddings of superficial words, while\nneglecting to model their ever-changing states, i.e., the information they\ncarry, as the text unfolds. Therefore, we extend the Transformer model to\ndynamically conduct entity state updates and sentence realization for narrative\ngeneration. We propose a contrastive framework to learn the state\nrepresentations in a discrete space, and insert additional attention layers\ninto the decoder to better exploit these states. Experiments on two narrative\ndatasets show that our model can generate more coherent and diverse narratives\nthan strong baselines with the guidance of meaningful entity states.", "published": "2022-08-08 09:02:19", "link": "http://arxiv.org/abs/2208.03985v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Information Extraction from Scanned Invoice Images using Text Analysis\n  and Layout Features", "abstract": "While storing invoice content as metadata to avoid paper document processing\nmay be the future trend, almost all of daily issued invoices are still printed\non paper or generated in digital formats such as PDFs. In this paper, we\nintroduce the OCRMiner system for information extraction from scanned document\nimages which is based on text analysis techniques in combination with layout\nfeatures to extract indexing metadata of (semi-)structured documents. The\nsystem is designed to process the document in a similar way a human reader\nuses, i.e. to employ different layout and text attributes in a coordinated\ndecision. The system consists of a set of interconnected modules that start\nwith (possibly erroneous) character-based output from a standard OCR system and\nallow to apply different techniques and to expand the extracted knowledge at\neach step. Using an open source OCR, the system is able to recover the invoice\ndata in 90% for English and in 88% for the Czech set.", "published": "2022-08-08 09:46:33", "link": "http://arxiv.org/abs/2208.04011v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Abstractive Meeting Summarization: A Survey", "abstract": "A system that could reliably identify and sum up the most important points of\na conversation would be valuable in a wide variety of real-world contexts, from\nbusiness meetings to medical consultations to customer service calls. Recent\nadvances in deep learning, and especially the invention of encoder-decoder\narchitectures, has significantly improved language generation systems, opening\nthe door to improved forms of abstractive summarization, a form of\nsummarization particularly well-suited for multi-party conversation. In this\npaper, we provide an overview of the challenges raised by the task of\nabstractive meeting summarization and of the data sets, models and evaluation\nmetrics that have been used to tackle the problems.", "published": "2022-08-08 14:04:38", "link": "http://arxiv.org/abs/2208.04163v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A High-Quality and Large-Scale Dataset for English-Vietnamese Speech\n  Translation", "abstract": "In this paper, we introduce a high-quality and large-scale benchmark dataset\nfor English-Vietnamese speech translation with 508 audio hours, consisting of\n331K triplets of (sentence-lengthed audio, English source transcript sentence,\nVietnamese target subtitle sentence). We also conduct empirical experiments\nusing strong baselines and find that the traditional \"Cascaded\" approach still\noutperforms the modern \"End-to-End\" approach. To the best of our knowledge,\nthis is the first large-scale English-Vietnamese speech translation study. We\nhope both our publicly available dataset and study can serve as a starting\npoint for future research and applications on English-Vietnamese speech\ntranslation. Our dataset is available at https://github.com/VinAIResearch/PhoST", "published": "2022-08-08 16:11:26", "link": "http://arxiv.org/abs/2208.04243v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Efficiently Extending Transformers for Long Input\n  Summarization", "abstract": "While large pretrained Transformer models have proven highly capable at\ntackling natural language tasks, handling long sequence inputs continues to be\na significant challenge. One such task is long input summarization, where\ninputs are longer than the maximum input context of most pretrained models.\nThrough an extensive set of experiments, we investigate what model\narchitectural changes and pretraining paradigms can most efficiently adapt a\npretrained Transformer for long input summarization. We find that a staggered,\nblock-local Transformer with global encoder tokens strikes a good balance of\nperformance and efficiency, and that an additional pretraining phase on long\nsequences meaningfully improves downstream summarization performance. Based on\nour findings, we introduce PEGASUS-X, an extension of the PEGASUS model with\nadditional long input pretraining to handle inputs of up to 16K tokens.\nPEGASUS-X achieves strong performance on long input summarization tasks\ncomparable with much larger models while adding few additional parameters and\nnot requiring model parallelism to train.", "published": "2022-08-08 18:10:58", "link": "http://arxiv.org/abs/2208.04347v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Systematic Evaluation of Response Selection for Open Domain Dialogue", "abstract": "Recent progress on neural approaches for language processing has triggered a\nresurgence of interest on building intelligent open-domain chatbots. However,\neven the state-of-the-art neural chatbots cannot produce satisfying responses\nfor every turn in a dialog. A practical solution is to generate multiple\nresponse candidates for the same context, and then perform response\nranking/selection to determine which candidate is the best. Previous work in\nresponse selection typically trains response rankers using synthetic data that\nis formed from existing dialogs by using a ground truth response as the single\nappropriate response and constructing inappropriate responses via random\nselection or using adversarial methods. In this work, we curated a dataset\nwhere responses from multiple response generators produced for the same dialog\ncontext are manually annotated as appropriate (positive) and inappropriate\n(negative). We argue that such training data better matches the actual use case\nexamples, enabling the models to learn to rank responses effectively. With this\nnew dataset, we conduct a systematic evaluation of state-of-the-art methods for\nresponse selection, and demonstrate that both strategies of using multiple\npositive candidates and using manually verified hard negative candidates can\nbring in significant performance improvement in comparison to using the\nadversarial training data, e.g., increase of 3% and 13% in Recall@1 score,\nrespectively.", "published": "2022-08-08 19:33:30", "link": "http://arxiv.org/abs/2208.04379v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Entity Linking Features for Emerging Entities", "abstract": "Entity linking (EL) is the process of linking entity mentions appearing in\ntext with their corresponding entities in a knowledge base. EL features of\nentities (e.g., prior probability, relatedness score, and entity embedding) are\nusually estimated based on Wikipedia. However, for newly emerging entities\n(EEs) which have just been discovered in news, they may still not be included\nin Wikipedia yet. As a consequence, it is unable to obtain required EL features\nfor those EEs from Wikipedia and EL models will always fail to link ambiguous\nmentions with those EEs correctly as the absence of their EL features. To deal\nwith this problem, in this paper we focus on a new task of learning EL features\nfor emerging entities in a general way. We propose a novel approach called\nSTAMO to learn high-quality EL features for EEs automatically, which needs just\na small number of labeled documents for each EE collected from the Web, as it\ncould further leverage the knowledge hidden in the unlabeled data. STAMO is\nmainly based on self-training, which makes it flexibly integrated with any EL\nfeature or EL model, but also makes it easily suffer from the error\nreinforcement problem caused by the mislabeled data. Instead of some common\nself-training strategies that try to throw the mislabeled data away explicitly,\nwe regard self-training as a multiple optimization process with respect to the\nEL features of EEs, and propose both intra-slot and inter-slot optimizations to\nalleviate the error reinforcement problem implicitly. We construct two EL\ndatasets involving selected EEs to evaluate the quality of obtained EL features\nfor EEs, and the experimental results show that our approach significantly\noutperforms other baseline methods of learning EL features.", "published": "2022-08-08 02:32:47", "link": "http://arxiv.org/abs/2208.03877v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What can we know about that which we cannot even imagine?", "abstract": "In this essay I will consider a sequence of questions. The first questions\nconcern the biological function of intelligence in general, and cognitive\nprostheses of human intelligence in particular. These will lead into questions\nconcerning human language, perhaps the most important cognitive prosthesis\nhumanity has ever developed. While it is traditional to rhapsodize about the\ncognitive power encapsulated in human language, I will emphasize how horribly\nlimited human language is - and therefore how limited our cognitive abilities\nare, despite their being augmented with language. This will lead to questions\nof whether human mathematics, being ultimately formulated in terms of human\nlanguage, is also deeply limited. I will then combine these questions to pose a\npartial, sort-of, sideways answer to the guiding concern of this essay: what we\ncan ever discern about that we cannot even conceive?", "published": "2022-08-08 03:07:02", "link": "http://arxiv.org/abs/2208.03886v5", "categories": ["physics.hist-ph", "cs.CL"], "primary_category": "physics.hist-ph"}
{"title": "Template-based Abstractive Microblog Opinion Summarisation", "abstract": "We introduce the task of microblog opinion summarisation (MOS) and share a\ndataset of 3100 gold-standard opinion summaries to facilitate research in this\ndomain. The dataset contains summaries of tweets spanning a 2-year period and\ncovers more topics than any other public Twitter summarisation dataset.\nSummaries are abstractive in nature and have been created by journalists\nskilled in summarising news articles following a template separating factual\ninformation (main story) from author opinions. Our method differs from previous\nwork on generating gold-standard summaries from social media, which usually\ninvolves selecting representative posts and thus favours extractive\nsummarisation models. To showcase the dataset's utility and challenges, we\nbenchmark a range of abstractive and extractive state-of-the-art summarisation\nmodels and achieve good performance, with the former outperforming the latter.\nWe also show that fine-tuning is necessary to improve performance and\ninvestigate the benefits of using different sample sizes.", "published": "2022-08-08 12:16:01", "link": "http://arxiv.org/abs/2208.04083v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Diverse Document Representations with Deep Query Interactions\n  for Dense Retrieval", "abstract": "In this paper, we propose a new dense retrieval model which learns diverse\ndocument representations with deep query interactions. Our model encodes each\ndocument with a set of generated pseudo-queries to get query-informed,\nmulti-view document representations. It not only enjoys high inference\nefficiency like the vanilla dual-encoder models, but also enables deep\nquery-document interactions in document encoding and provides multi-faceted\nrepresentations to better match different queries. Experiments on several\nbenchmarks demonstrate the effectiveness of the proposed method, out-performing\nstrong dual encoder baselines.The code is available at\n\\url{https://github.com/jordane95/dual-cross-encoder", "published": "2022-08-08 16:00:55", "link": "http://arxiv.org/abs/2208.04232v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Deep Learning Driven Natural Languages Text to SQL Query Conversion: A\n  Survey", "abstract": "With the future striving toward data-centric decision-making, seamless access\nto databases is of utmost importance. There is extensive research on creating\nan efficient text-to-sql (TEXT2SQL) model to access data from the database.\nUsing a Natural language is one of the best interfaces that can bridge the gap\nbetween the data and results by accessing the database efficiently, especially\nfor non-technical users. It will open the doors and create tremendous interest\namong users who are well versed in technical skills or not very skilled in\nquery languages. Even if numerous deep learning-based algorithms are proposed\nor studied, there still is very challenging to have a generic model to solve\nthe data query issues using natural language in a real-work scenario. The\nreason is the use of different datasets in different studies, which comes with\nits limitations and assumptions. At the same time, we do lack a thorough\nunderstanding of these proposed models and their limitations with the specific\ndataset it is trained on. In this paper, we try to present a holistic overview\nof 24 recent neural network models studied in the last couple of years,\nincluding their architectures involving convolutional neural networks,\nrecurrent neural networks, pointer networks, reinforcement learning, generative\nmodels, etc. We also give an overview of the 11 datasets that are widely used\nto train the models for TEXT2SQL technologies. We also discuss the future\napplication possibilities of TEXT2SQL technologies for seamless data queries.", "published": "2022-08-08 20:54:34", "link": "http://arxiv.org/abs/2208.04415v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Debiased Large Language Models Still Associate Muslims with Uniquely\n  Violent Acts", "abstract": "Recent work demonstrates a bias in the GPT-3 model towards generating violent\ntext completions when prompted about Muslims, compared with Christians and\nHindus. Two pre-registered replication attempts, one exact and one approximate,\nfound only the weakest bias in the more recent Instruct Series version of\nGPT-3, fine-tuned to eliminate biased and toxic outputs. Few violent\ncompletions were observed. Additional pre-registered experiments, however,\nshowed that using common names associated with the religions in prompts yields\na highly significant increase in violent completions, also revealing a stronger\nsecond-order bias against Muslims. Names of Muslim celebrities from non-violent\ndomains resulted in relatively fewer violent completions, suggesting that\naccess to individualized information can steer the model away from using\nstereotypes. Nonetheless, content analysis revealed religion-specific violent\nthemes containing highly offensive ideas regardless of prompt format. Our\nresults show the need for additional debiasing of large language models to\naddress higher-order schemas and associations.", "published": "2022-08-08 20:59:16", "link": "http://arxiv.org/abs/2208.04417v2", "categories": ["cs.CL", "cs.AI", "68T50, 91F20", "I.2.7"], "primary_category": "cs.CL"}
{"title": "INSPIRED2: An Improved Dataset for Sociable Conversational\n  Recommendation", "abstract": "Conversational recommender systems (CRS) that are able to interact with users\nin natural language often utilize recommendation dialogs which were previously\ncollected with the help of paired humans, where one plays the role of a seeker\nand the other as a recommender. These recommendation dialogs include items and\nentities that indicate the users' preferences. In order to precisely model the\nseekers' preferences and respond consistently, CRS typically rely on item and\nentity annotations. A recent example of such a dataset is INSPIRED, which\nconsists of recommendation dialogs for sociable conversational recommendation,\nwhere items and entities were annotated using automatic keyword or pattern\nmatching techniques. An analysis of this dataset unfortunately revealed that\nthere is a substantial number of cases where items and entities were either\nwrongly annotated or annotations were missing at all. This leads to the\nquestion to what extent automatic techniques for annotations are effective.\nMoreover, it is important to study impact of annotation quality on the overall\neffectiveness of a CRS in terms of the quality of the system's responses. To\nstudy these aspects, we manually fixed the annotations in INSPIRED. We then\nevaluated the performance of several benchmark CRS using both versions of the\ndataset. Our analyses suggest that the improved version of the dataset, i.e.,\nINSPIRED2, helped increase the performance of several benchmark CRS,\nemphasizing the importance of data quality both for end-to-end learning and\nretrieval-based approaches to conversational recommendation. We release our\nimproved dataset (INSPIRED2) publicly at\nhttps://github.com/ahtsham58/INSPIRED2.", "published": "2022-08-08 12:51:30", "link": "http://arxiv.org/abs/2208.04104v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "H.4.3"], "primary_category": "cs.CL"}
{"title": "Analog Bits: Generating Discrete Data using Diffusion Models with\n  Self-Conditioning", "abstract": "We present Bit Diffusion: a simple and generic approach for generating\ndiscrete data with continuous state and continuous time diffusion models. The\nmain idea behind our approach is to first represent the discrete data as binary\nbits, and then train a continuous diffusion model to model these bits as real\nnumbers which we call analog bits. To generate samples, the model first\ngenerates the analog bits, which are then thresholded to obtain the bits that\nrepresent the discrete variables. We further propose two simple techniques,\nnamely Self-Conditioning and Asymmetric Time Intervals, which lead to a\nsignificant improvement in sample quality. Despite its simplicity, the proposed\napproach can achieve strong performance in both discrete image generation and\nimage captioning tasks. For discrete image generation, we significantly improve\nprevious state-of-the-art on both CIFAR-10 (which has 3K discrete 8-bit tokens)\nand ImageNet-64x64 (which has 12K discrete 8-bit tokens), outperforming the\nbest autoregressive model in both sample quality (measured by FID) and\nefficiency. For image captioning on MS-COCO dataset, our approach achieves\ncompetitive results compared to autoregressive models.", "published": "2022-08-08 15:08:40", "link": "http://arxiv.org/abs/2208.04202v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "PerD: Perturbation Sensitivity-based Neural Trojan Detection Framework\n  on NLP Applications", "abstract": "Deep Neural Networks (DNNs) have been shown to be susceptible to Trojan\nattacks. Neural Trojan is a type of targeted poisoning attack that embeds the\nbackdoor into the victim and is activated by the trigger in the input space.\nThe increasing deployment of DNNs in critical systems and the surge of\noutsourcing DNN training (which makes Trojan attack easier) makes the detection\nof Trojan attacks necessary. While Neural Trojan detection has been studied in\nthe image domain, there is a lack of solutions in the NLP domain. In this\npaper, we propose a model-level Trojan detection framework by analyzing the\ndeviation of the model output when we introduce a specially crafted\nperturbation to the input. Particularly, we extract the model's responses to\nperturbed inputs as the `signature' of the model and train a meta-classifier to\ndetermine if a model is Trojaned based on its signature. We demonstrate the\neffectiveness of our proposed method on both a dataset of NLP models we create\nand a public dataset of Trojaned NLP models from TrojAI. Furthermore, we\npropose a lightweight variant of our detection method that reduces the\ndetection time while preserving the detection rates.", "published": "2022-08-08 22:50:03", "link": "http://arxiv.org/abs/2208.04943v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "TGAVC: Improving Autoencoder Voice Conversion with Text-Guided and\n  Adversarial Training", "abstract": "Non-parallel many-to-many voice conversion remains an interesting but\nchallenging speech processing task. Recently, AutoVC, a conditional autoencoder\nbased method, achieved excellent conversion results by disentangling the\nspeaker identity and the speech content using information-constraining\nbottlenecks. However, due to the pure autoencoder training method, it is\ndifficult to evaluate the separation effect of content and speaker identity. In\nthis paper, a novel voice conversion framework, named $\\boldsymbol T$ext\n$\\boldsymbol G$uided $\\boldsymbol A$utoVC(TGAVC), is proposed to more\neffectively separate content and timbre from speech, where an expected content\nembedding produced based on the text transcriptions is designed to guide the\nextraction of voice content. In addition, the adversarial training is applied\nto eliminate the speaker identity information in the estimated content\nembedding extracted from speech. Under the guidance of the expected content\nembedding and the adversarial training, the content encoder is trained to\nextract speaker-independent content embedding from speech. Experiments on\nAIShell-3 dataset show that the proposed model outperforms AutoVC in terms of\nnaturalness and similarity of converted speech.", "published": "2022-08-08 10:33:36", "link": "http://arxiv.org/abs/2208.04035v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FRA-RIR: Fast Random Approximation of the Image-source Method", "abstract": "The training of modern speech processing systems often requires a large\namount of simulated room impulse response (RIR) data in order to allow the\nsystems to generalize well in real-world, reverberant environments. However,\nsimulating realistic RIR data typically requires accurate physical modeling,\nand the acceleration of such simulation process typically requires certain\ncomputational platforms such as a graphics processing unit (GPU). In this\npaper, we propose FRA-RIR, a fast random approximation method of the\nwidely-used image-source method (ISM), to efficiently generate realistic RIR\ndata without specific computational devices. FRA-RIR replaces the physical\nsimulation in the standard ISM by a series of random approximations, which\nsignificantly speeds up the simulation process and enables its application in\non-the-fly data generation pipelines. Experiments show that FRA-RIR can not\nonly be significantly faster than other existing ISM-based RIR simulation tools\non standard computational platforms, but also improves the performance of\nspeech denoising systems evaluated on real-world RIR when trained with\nsimulated RIR. A Python implementation of FRA-RIR is available\nonline\\footnote{\\url{https://github.com/yluo42/FRA-RIR}}.", "published": "2022-08-08 12:46:30", "link": "http://arxiv.org/abs/2208.04101v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Denoising Induction Motor Sounds Using an Autoencoder", "abstract": "Denoising is the process of removing noise from sound signals while improving\nthe quality and adequacy of the sound signals. Denoising sound has many\napplications in speech processing, sound events classification, and machine\nfailure detection systems. This paper describes a method for creating an\nautoencoder to map noisy machine sounds to clean sounds for denoising purposes.\nThere are several types of noise in sounds, for example, environmental noise\nand generated frequency-dependent noise from signal processing methods. Noise\ngenerated by environmental activities is environmental noise. In the factory,\nenvironmental noise can be created by vehicles, drilling, people working or\ntalking in the survey area, wind, and flowing water. Those noises appear as\nspikes in the sound record. In the scope of this paper, we demonstrate the\nremoval of generated noise with Gaussian distribution and the environmental\nnoise with a specific example of the water sink faucet noise from the induction\nmotor sounds. The proposed method was trained and verified on 49 normal\nfunction sounds and 197 horizontal misalignment fault sounds from the Machinery\nFault Database (MAFAULDA). The mean square error (MSE) was used as the\nassessment criteria to evaluate the similarity between denoised sounds using\nthe proposed autoencoder and the original sounds in the test set. The MSE is\nbelow or equal to 0.14 when denoise both types of noises on 15 testing sounds\nof the normal function category. The MSE is below or equal to 0.15 when\ndenoising 60 testing sounds on the horizontal misalignment fault category. The\nlow MSE shows that both the generated Gaussian noise and the environmental\nnoise were almost removed from the original sounds with the proposed trained\nautoencoder.", "published": "2022-08-08 23:14:51", "link": "http://arxiv.org/abs/2208.04462v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
