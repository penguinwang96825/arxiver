{"title": "Quick and (not so) Dirty: Unsupervised Selection of Justification\n  Sentences for Multi-hop Question Answering", "abstract": "We propose an unsupervised strategy for the selection of justification\nsentences for multi-hop question answering (QA) that (a) maximizes the\nrelevance of the selected sentences, (b) minimizes the overlap between the\nselected facts, and (c) maximizes the coverage of both question and answer.\nThis unsupervised sentence selection method can be coupled with any supervised\nQA approach. We show that the sentences selected by our method improve the\nperformance of a state-of-the-art supervised QA model on two multi-hop QA\ndatasets: AI2's Reasoning Challenge (ARC) and Multi-Sentence Reading\nComprehension (MultiRC). We obtain new state-of-the-art performance on both\ndatasets among approaches that do not use external resources for training the\nQA system: 56.82% F1 on ARC (41.24% on Challenge and 64.49% on Easy) and 26.1%\nEM0 on MultiRC. Our justification sentences have higher quality than the\njustifications selected by a strong information retrieval baseline, e.g., by\n5.4% F1 in MultiRC. We also show that our unsupervised selection of\njustification sentences is more stable across domains than a state-of-the-art\nsupervised sentence selection method.", "published": "2019-11-17 07:51:42", "link": "http://arxiv.org/abs/1911.07176v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Zone Unit for Recurrent Neural Networks", "abstract": "Recurrent neural networks (RNNs) have been widely used to deal with sequence\nlearning problems. The input-dependent transition function, which folds new\nobservations into hidden states to sequentially construct fixed-length\nrepresentations of arbitrary-length sequences, plays a critical role in RNNs.\nBased on single space composition, transition functions in existing RNNs often\nhave difficulty in capturing complicated long-range dependencies. In this\npaper, we introduce a new Multi-zone Unit (MZU) for RNNs. The key idea is to\ndesign a transition function that is capable of modeling multiple space\ncomposition. The MZU consists of three components: zone generation, zone\ncomposition, and zone aggregation. Experimental results on multiple datasets of\nthe character-level language modeling task and the aspect-based sentiment\nanalysis task demonstrate the superiority of the MZU.", "published": "2019-11-17 08:27:26", "link": "http://arxiv.org/abs/1911.07184v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mining Unfollow Behavior in Large-Scale Online Social Networks via\n  Spatial-Temporal Interaction", "abstract": "Online Social Networks (OSNs) evolve through two pervasive behaviors: follow\nand unfollow, which respectively signify relationship creation and relationship\ndissolution. Researches on social network evolution mainly focus on the follow\nbehavior, while the unfollow behavior has largely been ignored. Mining unfollow\nbehavior is challenging because user's decision on unfollow is not only\naffected by the simple combination of user's attributes like informativeness\nand reciprocity, but also affected by the complex interaction among them.\nMeanwhile, prior datasets seldom contain sufficient records for inferring such\ncomplex interaction. To address these issues, we first construct a large-scale\nreal-world Weibo dataset, which records detailed post content and relationship\ndynamics of 1.8 million Chinese users. Next, we define user's attributes as two\ncategories: spatial attributes (e.g., social role of user) and temporal\nattributes (e.g., post content of user). Leveraging the constructed dataset, we\nsystematically study how the interaction effects between user's spatial and\ntemporal attributes contribute to the unfollow behavior. Afterwards, we propose\na novel unified model with heterogeneous information (UMHI) for unfollow\nprediction. Specifically, our UMHI model: 1) captures user's spatial attributes\nthrough social network structure; 2) infers user's temporal attributes through\nuser-posted content and unfollow history; and 3) models the interaction between\nspatial and temporal attributes by the nonlinear MLP layers. Comprehensive\nevaluations on the constructed dataset demonstrate that the proposed UMHI model\noutperforms baseline methods by 16.44% on average in terms of precision. In\naddition, factor analyses verify that both spatial attributes and temporal\nattributes are essential for mining unfollow behavior.", "published": "2019-11-17 05:40:17", "link": "http://arxiv.org/abs/1911.07156v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "DualVD: An Adaptive Dual Encoding Model for Deep Visual Understanding in\n  Visual Dialogue", "abstract": "Different from Visual Question Answering task that requires to answer only\none question about an image, Visual Dialogue involves multiple questions which\ncover a broad range of visual content that could be related to any objects,\nrelationships or semantics. The key challenge in Visual Dialogue task is thus\nto learn a more comprehensive and semantic-rich image representation which may\nhave adaptive attentions on the image for variant questions. In this research,\nwe propose a novel model to depict an image from both visual and semantic\nperspectives. Specifically, the visual view helps capture the appearance-level\ninformation, including objects and their relationships, while the semantic view\nenables the agent to understand high-level visual semantics from the whole\nimage to the local regions. Futhermore, on top of such multi-view image\nfeatures, we propose a feature selection framework which is able to adaptively\ncapture question-relevant information hierarchically in fine-grained level. The\nproposed method achieved state-of-the-art results on benchmark Visual Dialogue\ndatasets. More importantly, we can tell which modality (visual or semantic) has\nmore contribution in answering the current question by visualizing the gate\nvalues. It gives us insights in understanding of human cognition in Visual\nDialogue.", "published": "2019-11-17 14:58:17", "link": "http://arxiv.org/abs/1911.07251v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning", "abstract": "In sequence to sequence learning, the self-attention mechanism proves to be\nhighly effective, and achieves significant improvements in many tasks. However,\nthe self-attention mechanism is not without its own flaws. Although\nself-attention can model extremely long dependencies, the attention in deep\nlayers tends to overconcentrate on a single token, leading to insufficient use\nof local information and difficultly in representing long sequences. In this\nwork, we explore parallel multi-scale representation learning on sequence data,\nstriving to capture both long-range and short-range language structures. To\nthis end, we propose the Parallel MUlti-Scale attEntion (MUSE) and MUSE-simple.\nMUSE-simple contains the basic idea of parallel multi-scale sequence\nrepresentation learning, and it encodes the sequence in parallel, in terms of\ndifferent scales with the help from self-attention, and pointwise\ntransformation. MUSE builds on MUSE-simple and explores combining convolution\nand self-attention for learning sequence representations from more different\nscales. We focus on machine translation and the proposed approach achieves\nsubstantial performance improvements over Transformer, especially on long\nsequences. More importantly, we find that although conceptually simple, its\nsuccess in practice requires intricate considerations, and the multi-scale\nattention must build on unified semantic space. Under common setting, the\nproposed model achieves substantial performance and outperforms all previous\nmodels on three main machine translation tasks. In addition, MUSE has potential\nfor accelerating inference due to its parallelism. Code will be available at\nhttps://github.com/lancopku/MUSE", "published": "2019-11-17 09:36:07", "link": "http://arxiv.org/abs/1911.09483v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Working Memory Graphs", "abstract": "Transformers have increasingly outperformed gated RNNs in obtaining new\nstate-of-the-art results on supervised tasks involving text sequences. Inspired\nby this trend, we study the question of how Transformer-based models can\nimprove the performance of sequential decision-making agents. We present the\nWorking Memory Graph (WMG), an agent that employs multi-head self-attention to\nreason over a dynamic set of vectors representing observed and recurrent state.\nWe evaluate WMG in three environments featuring factored observation spaces: a\nPathfinding environment that requires complex reasoning over past observations,\nBabyAI gridworld levels that involve variable goals, and Sokoban which\nemphasizes future planning. We find that the combination of WMG's\nTransformer-based architecture with factored observation spaces leads to\nsignificant gains in learning efficiency compared to baseline architectures\nacross all tasks. WMG demonstrates how Transformer-based models can\ndramatically boost sample efficiency in RL environments for which observations\ncan be factored.", "published": "2019-11-17 03:14:02", "link": "http://arxiv.org/abs/1911.07141v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Rumor Detection on Social Media: Datasets, Methods and Opportunities", "abstract": "Social media platforms have been used for information and news gathering, and\nthey are very valuable in many applications. However, they also lead to the\nspreading of rumors and fake news. Many efforts have been taken to detect and\ndebunk rumors on social media by analyzing their content and social context\nusing machine learning techniques. This paper gives an overview of the recent\nstudies in the rumor detection field. It provides a comprehensive list of\ndatasets used for rumor detection, and reviews the important studies based on\nwhat types of information they exploit and the approaches they take. And more\nimportantly, we also present several new directions for future research.", "published": "2019-11-17 09:40:24", "link": "http://arxiv.org/abs/1911.07199v1", "categories": ["cs.IR", "cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.IR"}
{"title": "Deep Learning versus Traditional Classifiers on Vietnamese Students'\n  Feedback Corpus", "abstract": "Student's feedback is an important source of collecting students' opinions to\nimprove the quality of training activities. Implementing sentiment analysis\ninto student feedback data, we can determine sentiments polarities which\nexpress all problems in the institution since changes necessary will be applied\nto improve the quality of teaching and learning. This study focused on machine\nlearning and natural language processing techniques (NaiveBayes, Maximum\nEntropy, Long Short-Term Memory, Bi-Directional Long Short-Term Memory) on the\nVietnameseStudents' Feedback Corpus collected from a university. The final\nresults were compared and evaluated to find the most effective model based on\ndifferent evaluation criteria. The experimental results show that the\nBi-Directional LongShort-Term Memory algorithm outperformed than three other\nalgorithms in terms of the F1-score measurement with 92.0% on the sentiment\nclassification task and 89.6% on the topic classification task. In addition, we\ndeveloped a sentiment analysis application analyzing student feedback. The\napplication will help the institution to recognize students' opinions about a\nproblem and identify shortcomings that still exist. With the use of this\napplication, the institution can propose an appropriate method to improve the\nquality of training activities in the future.", "published": "2019-11-17 12:32:50", "link": "http://arxiv.org/abs/1911.07223v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Error Analysis for Vietnamese Named Entity Recognition on Deep Neural\n  Network Models", "abstract": "In recent years, Vietnamese Named Entity Recognition (NER) systems have had a\ngreat breakthrough when using Deep Neural Network methods. This paper describes\nthe primary errors of the state-of-the-art NER systems on Vietnamese language.\nAfter conducting experiments on BLSTM-CNN-CRF and BLSTM-CRF models with\ndifferent word embeddings on the Vietnamese NER dataset. This dataset is\nprovided by VLSP in 2016 and used to evaluate most of the current Vietnamese\nNER systems. We noticed that BLSTM-CNN-CRF gives better results, therefore, we\nanalyze the errors on this model in detail. Our error-analysis results provide\nus thorough insights in order to increase the performance of NER for the\nVietnamese language and improve the quality of the corpus in the future works.", "published": "2019-11-17 13:03:07", "link": "http://arxiv.org/abs/1911.07228v2", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Using Error Decay Prediction to Overcome Practical Issues of Deep Active\n  Learning for Named Entity Recognition", "abstract": "Existing deep active learning algorithms achieve impressive sampling\nefficiency on natural language processing tasks. However, they exhibit several\nweaknesses in practice, including (a) inability to use uncertainty sampling\nwith black-box models, (b) lack of robustness to labeling noise, and (c) lack\nof transparency. In response, we propose a transparent batch active sampling\nframework by estimating the error decay curves of multiple feature-defined\nsubsets of the data. Experiments on four named entity recognition (NER) tasks\ndemonstrate that the proposed methods significantly outperform\ndiversification-based methods for black-box NER taggers, and can make the\nsampling process more robust to labeling noise when combined with\nuncertainty-based methods. Furthermore, the analysis of experimental results\nsheds light on the weaknesses of different active sampling strategies, and when\ntraditional uncertainty-based or diversification-based methods can be expected\nto work well.", "published": "2019-11-17 20:41:32", "link": "http://arxiv.org/abs/1911.07335v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
