{"title": "How Context Affects Language Models' Factual Predictions", "abstract": "When pre-trained on large unsupervised textual corpora, language models are\nable to store and retrieve factual knowledge to some extent, making it possible\nto use them directly for zero-shot cloze-style question answering. However,\nstoring factual knowledge in a fixed number of weights of a language model\nclearly has limitations. Previous approaches have successfully provided access\nto information outside the model weights using supervised architectures that\ncombine an information retrieval system with a machine reading component. In\nthis paper, we go a step further and integrate information from a retrieval\nsystem with a pre-trained language model in a purely unsupervised way. We\nreport that augmenting pre-trained language models in this way dramatically\nimproves performance and that the resulting system, despite being unsupervised,\nis competitive with a supervised machine reading baseline. Furthermore,\nprocessing query and context with different segment tokens allows BERT to\nutilize its Next Sentence Prediction pre-trained classifier to determine\nwhether the context is relevant or not, substantially improving BERT's\nzero-shot cloze-style question-answering performance and making its predictions\nrobust to noisy contexts.", "published": "2020-05-10 09:28:12", "link": "http://arxiv.org/abs/2005.04611v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A SentiWordNet Strategy for Curriculum Learning in Sentiment Analysis", "abstract": "Curriculum Learning (CL) is the idea that learning on a training set\nsequenced or ordered in a manner where samples range from easy to difficult,\nresults in an increment in performance over otherwise random ordering. The idea\nparallels cognitive science's theory of how human brains learn, and that\nlearning a difficult task can be made easier by phrasing it as a sequence of\neasy to difficult tasks. This idea has gained a lot of traction in machine\nlearning and image processing for a while and recently in Natural Language\nProcessing (NLP). In this paper, we apply the ideas of curriculum learning,\ndriven by SentiWordNet in a sentiment analysis setting. In this setting, given\na text segment, our aim is to extract its sentiment or polarity. SentiWordNet\nis a lexical resource with sentiment polarity annotations. By comparing\nperformance with other curriculum strategies and with no curriculum, the\neffectiveness of the proposed strategy is presented. Convolutional, Recurrence,\nand Attention-based architectures are employed to assess this improvement. The\nmodels are evaluated on a standard sentiment dataset, Stanford Sentiment\nTreebank.", "published": "2020-05-10 18:50:40", "link": "http://arxiv.org/abs/2005.04749v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer Based Language Models for Similar Text Retrieval and Ranking", "abstract": "Most approaches for similar text retrieval and ranking with long natural\nlanguage queries rely at some level on queries and responses having words in\ncommon with each other. Recent applications of transformer-based neural\nlanguage models to text retrieval and ranking problems have been very\npromising, but still involve a two-step process in which result candidates are\nfirst obtained through bag-of-words-based approaches, and then reranked by a\nneural transformer. In this paper, we introduce novel approaches for\neffectively applying neural transformer models to similar text retrieval and\nranking without an initial bag-of-words-based step. By eliminating the\nbag-of-words-based step, our approach is able to accurately retrieve and rank\nresults even when they have no non-stopwords in common with the query. We\naccomplish this by using bidirectional encoder representations from\ntransformers (BERT) to create vectorized representations of sentence-length\ntexts, along with a vector nearest neighbor search index. We demonstrate both\nsupervised and unsupervised means of using BERT to accomplish this task.", "published": "2020-05-10 06:12:53", "link": "http://arxiv.org/abs/2005.04588v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "From Standard Summarization to New Tasks and Beyond: Summarization with\n  Manifold Information", "abstract": "Text summarization is the research area aiming at creating a short and\ncondensed version of the original document, which conveys the main idea of the\ndocument in a few words. This research topic has started to attract the\nattention of a large community of researchers, and it is nowadays counted as\none of the most promising research areas. In general, text summarization\nalgorithms aim at using a plain text document as input and then output a\nsummary. However, in real-world applications, most of the data is not in a\nplain text format. Instead, there is much manifold information to be\nsummarized, such as the summary for a web page based on a query in the search\nengine, extreme long document (e.g., academic paper), dialog history and so on.\nIn this paper, we focus on the survey of these new summarization tasks and\napproaches in the real-world application.", "published": "2020-05-10 14:59:36", "link": "http://arxiv.org/abs/2005.04684v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CTC-synchronous Training for Monotonic Attention Model", "abstract": "Monotonic chunkwise attention (MoChA) has been studied for the online\nstreaming automatic speech recognition (ASR) based on a sequence-to-sequence\nframework. In contrast to connectionist temporal classification (CTC), backward\nprobabilities cannot be leveraged in the alignment marginalization process\nduring training due to left-to-right dependency in the decoder. This results in\nthe error propagation of alignments to subsequent token generation. To address\nthis problem, we propose CTC-synchronous training (CTC-ST), in which MoChA uses\nCTC alignments to learn optimal monotonic alignments. Reference CTC alignments\nare extracted from a CTC branch sharing the same encoder with the decoder. The\nentire model is jointly optimized so that the expected boundaries from MoChA\nare synchronized with the alignments. Experimental evaluations of the TEDLIUM\nrelease-2 and Librispeech corpora show that the proposed method significantly\nimproves recognition, especially for long utterances. We also show that CTC-ST\ncan bring out the full potential of SpecAugment for MoChA.", "published": "2020-05-10 16:48:23", "link": "http://arxiv.org/abs/2005.04712v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Robustifying NLI Models Against Lexical Dataset Biases", "abstract": "While deep learning models are making fast progress on the task of Natural\nLanguage Inference, recent studies have also shown that these models achieve\nhigh accuracy by exploiting several dataset biases, and without deep\nunderstanding of the language semantics. Using contradiction-word bias and\nword-overlapping bias as our two bias examples, this paper explores both\ndata-level and model-level debiasing methods to robustify models against\nlexical dataset biases. First, we debias the dataset through data augmentation\nand enhancement, but show that the model bias cannot be fully removed via this\nmethod. Next, we also compare two ways of directly debiasing the model without\nknowing what the dataset biases are in advance. The first approach aims to\nremove the label bias at the embedding level. The second approach employs a\nbag-of-words sub-model to capture the features that are likely to exploit the\nbias and prevents the original model from learning these biased features by\nforcing orthogonality between these two sub-models. We performed evaluations on\nnew balanced datasets extracted from the original MNLI dataset as well as the\nNLI stress tests, and show that the orthogonality approach is better at\ndebiasing the model while maintaining competitive overall accuracy. Our code\nand data are available at: https://github.com/owenzx/LexicalDebias-ACL2020", "published": "2020-05-10 17:56:10", "link": "http://arxiv.org/abs/2005.04732v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Article citation study: Context enhanced citation sentiment detection", "abstract": "Citation sentimet analysis is one of the little studied tasks for\nscientometric analysis. For citation analysis, we developed eight datasets\ncomprising citation sentences, which are manually annotated by us into three\nsentiment polarities viz. positive, negative, and neutral. Among eight\ndatasets, three were developed by considering the whole context of citations.\nFurthermore, we proposed an ensembled feature engineering method comprising\nword embeddings obtained for texts, parts-of-speech tags, and dependency\nrelationships together. Ensembled features were considered as input to deep\nlearning based approaches for citation sentiment classification, which is in\nturn compared with Bag-of-Words approach. Experimental results demonstrate that\ndeep learning is useful for higher number of samples, whereas support vector\nmachine is the winner for smaller number of samples. Moreover, context-based\nsamples are proved to be more effective than context-less samples for citation\nsentiment analysis.", "published": "2020-05-10 00:27:19", "link": "http://arxiv.org/abs/2005.04534v1", "categories": ["cs.CL", "cs.DL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Posterior Control of Blackbox Generation", "abstract": "Text generation often requires high-precision output that obeys task-specific\nrules. This fine-grained control is difficult to enforce with off-the-shelf\ndeep learning models. In this work, we consider augmenting neural generation\nmodels with discrete control states learned through a structured\nlatent-variable approach. Under this formulation, task-specific knowledge can\nbe encoded through a range of rich, posterior constraints that are effectively\ntrained into the model. This approach allows users to ground internal model\ndecisions based on prior knowledge, without sacrificing the representational\npower of neural generative models. Experiments consider applications of this\napproach for text generation. We find that this method improves over standard\nbenchmarks, while also providing fine-grained control.", "published": "2020-05-10 03:22:45", "link": "http://arxiv.org/abs/2005.04560v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby\n  Steps", "abstract": "Learning to follow instructions is of fundamental importance to autonomous\nagents for vision-and-language navigation (VLN). In this paper, we study how an\nagent can navigate long paths when learning from a corpus that consists of\nshorter ones. We show that existing state-of-the-art agents do not generalize\nwell. To this end, we propose BabyWalk, a new VLN agent that is learned to\nnavigate by decomposing long instructions into shorter ones (BabySteps) and\ncompleting them sequentially. A special design memory buffer is used by the\nagent to turn its past experiences into contexts for future steps. The learning\nprocess is composed of two phases. In the first phase, the agent uses imitation\nlearning from demonstration to accomplish BabySteps. In the second phase, the\nagent uses curriculum-based reinforcement learning to maximize rewards on\nnavigation tasks with increasingly longer instructions. We create two new\nbenchmark datasets (of long navigation tasks) and use them in conjunction with\nexisting ones to examine BabyWalk's generalization ability. Empirical results\nshow that BabyWalk achieves state-of-the-art results on several metrics, in\nparticular, is able to follow long instructions better. The codes and the\ndatasets are released on our project page https://github.com/Sha-Lab/babywalk.", "published": "2020-05-10 10:46:41", "link": "http://arxiv.org/abs/2005.04625v2", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Non-Autoregressive Image Captioning with Counterfactuals-Critical\n  Multi-Agent Learning", "abstract": "Most image captioning models are autoregressive, i.e. they generate each word\nby conditioning on previously generated words, which leads to heavy latency\nduring inference. Recently, non-autoregressive decoding has been proposed in\nmachine translation to speed up the inference time by generating all words in\nparallel. Typically, these models use the word-level cross-entropy loss to\noptimize each word independently. However, such a learning process fails to\nconsider the sentence-level consistency, thus resulting in inferior generation\nquality of these non-autoregressive models. In this paper, we propose a\nNon-Autoregressive Image Captioning (NAIC) model with a novel training\nparadigm: Counterfactuals-critical Multi-Agent Learning (CMAL). CMAL formulates\nNAIC as a multi-agent reinforcement learning system where positions in the\ntarget sequence are viewed as agents that learn to cooperatively maximize a\nsentence-level reward. Besides, we propose to utilize massive unlabeled images\nto boost captioning performance. Extensive experiments on MSCOCO image\ncaptioning benchmark show that our NAIC model achieves a performance comparable\nto state-of-the-art autoregressive models, while brings 13.9x decoding speedup.", "published": "2020-05-10 15:09:44", "link": "http://arxiv.org/abs/2005.04690v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Chirp Complex Cepstrum-based Decomposition for Asynchronous Glottal\n  Analysis", "abstract": "It was recently shown that complex cepstrum can be effectively used for\nglottal flow estimation by separating the causal and anticausal components of\nspeech. In order to guarantee a correct estimation, some constraints on the\nwindow have been derived. Among these, the window has to be synchronized on a\nGlottal Closure Instant. This paper proposes an extension of the complex\ncepstrum-based decomposition by incorporating a chirp analysis. The resulting\nmethod is shown to give a reliable estimation of the glottal flow wherever the\nwindow is located. This technique is then suited for its integration in usual\nspeech processing systems, which generally operate in an asynchronous way.\nBesides its potential for automatic voice quality analysis is highlighted.", "published": "2020-05-10 17:33:48", "link": "http://arxiv.org/abs/2005.04724v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Knowledge Graph semantic enhancement of input data for improving AI", "abstract": "Intelligent systems designed using machine learning algorithms require a\nlarge number of labeled data. Background knowledge provides complementary, real\nworld factual information that can augment the limited labeled data to train a\nmachine learning algorithm. The term Knowledge Graph (KG) is in vogue as for\nmany practical applications, it is convenient and useful to organize this\nbackground knowledge in the form of a graph. Recent academic research and\nimplemented industrial intelligent systems have shown promising performance for\nmachine learning algorithms that combine training data with a knowledge graph.\nIn this article, we discuss the use of relevant KGs to enhance input data for\ntwo applications that use machine learning -- recommendation and community\ndetection. The KG improves both accuracy and explainability.", "published": "2020-05-10 17:37:38", "link": "http://arxiv.org/abs/2005.04726v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.1"], "primary_category": "cs.AI"}
{"title": "The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes", "abstract": "This work proposes a new challenge set for multimodal classification,\nfocusing on detecting hate speech in multimodal memes. It is constructed such\nthat unimodal models struggle and only multimodal models can succeed: difficult\nexamples (\"benign confounders\") are added to the dataset to make it hard to\nrely on unimodal signals. The task requires subtle reasoning, yet is\nstraightforward to evaluate as a binary classification problem. We provide\nbaseline performance numbers for unimodal models, as well as for multimodal\nmodels with various degrees of sophistication. We find that state-of-the-art\nmethods perform poorly compared to humans (64.73% vs. 84.7% accuracy),\nillustrating the difficulty of the task and highlighting the challenge that\nthis important problem poses to the community.", "published": "2020-05-10 21:31:00", "link": "http://arxiv.org/abs/2005.04790v3", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "SpEx+: A Complete Time Domain Speaker Extraction Network", "abstract": "Speaker extraction aims to extract the target speech signal from a\nmulti-talker environment given a target speaker's reference speech. We recently\nproposed a time-domain solution, SpEx, that avoids the phase estimation in\nfrequency-domain approaches. Unfortunately, SpEx is not fully a time-domain\nsolution since it performs time-domain speech encoding for speaker extraction,\nwhile taking frequency-domain speaker embedding as the reference. The size of\nthe analysis window for time-domain and the size for frequency-domain input are\nalso different. Such mismatch has an adverse effect on the system performance.\nTo eliminate such mismatch, we propose a complete time-domain speaker\nextraction solution, that is called SpEx+. Specifically, we tie the weights of\ntwo identical speech encoder networks, one for the encoder-extractor-decoder\npipeline, another as part of the speaker encoder. Experiments show that the\nSpEx+ achieves 0.8dB and 2.1dB SDR improvement over the state-of-the-art SpEx\nbaseline, under different and same gender conditions on WSJ0-2mix-extr database\nrespectively.", "published": "2020-05-10 15:00:07", "link": "http://arxiv.org/abs/2005.04686v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio and Contact Microphones for Cough Detection", "abstract": "In the framework of assessing the pathology severity in chronic cough\ndiseases, medical literature underlines the lack of tools for allowing the\nautomatic, objective and reliable detection of cough events. This paper\ndescribes a system based on two microphones which we developed for this\npurpose. The proposed approach relies on a large variety of audio descriptors,\nan efficient algorithm of feature selection based on their mutual information\nand the use of artificial neural networks. First, the possible use of a contact\nmicrophone (placed on the patient's thorax or trachea) in complement to the\naudio signal is investigated. This study underlines that this contact\nmicrophone suffers from reliability issues, and conveys little new relevant\ninformation compared to the audio modality. Secondly, the proposed audio-only\napproach is compared to a commercially available system using four sensors on a\ndatabase with different sound categories often misdetected as coughs, and\nproduced in various conditions. With average sensitivity and specificity of\n94.7% and 95% respectively, the proposed method achieves better cough detection\nperformance than the commercial system.", "published": "2020-05-10 17:58:21", "link": "http://arxiv.org/abs/2005.05313v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "From Speaker Verification to Multispeaker Speech Synthesis, Deep\n  Transfer with Feedback Constraint", "abstract": "High-fidelity speech can be synthesized by end-to-end text-to-speech models\nin recent years. However, accessing and controlling speech attributes such as\nspeaker identity, prosody, and emotion in a text-to-speech system remains a\nchallenge. This paper presents a system involving feedback constraint for\nmultispeaker speech synthesis. We manage to enhance the knowledge transfer from\nthe speaker verification to the speech synthesis by engaging the speaker\nverification network. The constraint is taken by an added loss related to the\nspeaker identity, which is centralized to improve the speaker similarity\nbetween the synthesized speech and its natural reference audio. The model is\ntrained and evaluated on publicly available datasets. Experimental results,\nincluding visualization on speaker embedding space, show significant\nimprovement in terms of speaker identity cloning in the spectrogram level.\nSynthesized samples are available online for listening.\n(https://caizexin.github.io/mlspk-syn-samples/index.html)", "published": "2020-05-10 06:11:37", "link": "http://arxiv.org/abs/2005.04587v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cognitive-driven convolutional beamforming using EEG-based auditory\n  attention decoding", "abstract": "The performance of speech enhancement algorithms in a multi-speaker scenario\ndepends on correctly identifying the target speaker to be enhanced. Auditory\nattention decoding (AAD) methods allow to identify the target speaker which the\nlistener is attending to from single-trial EEG recordings. Aiming at enhancing\nthe target speaker and suppressing interfering speakers, reverberation and\nambient noise, in this paper we propose a cognitive-driven multi-microphone\nspeech enhancement system, which combines a neural-network-based mask\nestimator, weighted minimum power distortionless response convolutional\nbeamformers and AAD. To control the suppression of the interfering speaker, we\nalso propose an extension incorporating an interference suppression constraint.\nThe experimental results show that the proposed system outperforms the\nstate-of-the-art cognitive-driven speech enhancement systems in challenging\nreverberant and noisy conditions.", "published": "2020-05-10 13:56:31", "link": "http://arxiv.org/abs/2005.04669v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
