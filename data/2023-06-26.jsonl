{"title": "Constraint-aware and Ranking-distilled Token Pruning for Efficient\n  Transformer Inference", "abstract": "Deploying pre-trained transformer models like BERT on downstream tasks in\nresource-constrained scenarios is challenging due to their high inference cost,\nwhich grows rapidly with input sequence length. In this work, we propose a\nconstraint-aware and ranking-distilled token pruning method ToP, which\nselectively removes unnecessary tokens as input sequence passes through layers,\nallowing the model to improve online inference speed while preserving accuracy.\nToP overcomes the limitation of inaccurate token importance ranking in the\nconventional self-attention mechanism through a ranking-distilled token\ndistillation technique, which distills effective token rankings from the final\nlayer of unpruned models to early layers of pruned models. Then, ToP introduces\na coarse-to-fine pruning approach that automatically selects the optimal subset\nof transformer layers and optimizes token pruning decisions within these layers\nthrough improved $L_0$ regularization. Extensive experiments on GLUE benchmark\nand SQuAD tasks demonstrate that ToP outperforms state-of-the-art token pruning\nand model compression methods with improved accuracy and speedups. ToP reduces\nthe average FLOPs of BERT by 8.1x while achieving competitive accuracy on GLUE,\nand provides a real latency speedup of up to 7.4x on an Intel CPU.", "published": "2023-06-26 03:06:57", "link": "http://arxiv.org/abs/2306.14393v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fauno: The Italian Large Language Model that will leave you senza\n  parole!", "abstract": "This paper presents Fauno, the first and largest open-source Italian\nconversational Large Language Model (LLM). Our goal with Fauno is to\ndemocratize the study of LLMs in Italian, demonstrating that obtaining a\nfine-tuned conversational bot with a single GPU is possible. In addition, we\nrelease a collection of datasets for conversational AI in Italian. The datasets\non which we fine-tuned Fauno include various topics such as general question\nanswering, computer science, and medical questions. We release our code and\ndatasets on \\url{https://github.com/RSTLess-research/Fauno-Italian-LLM}", "published": "2023-06-26 07:00:38", "link": "http://arxiv.org/abs/2306.14457v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JSEEGraph: Joint Structured Event Extraction as Graph Parsing", "abstract": "We propose a graph-based event extraction framework JSEEGraph that approaches\nthe task of event extraction as general graph parsing in the tradition of\nMeaning Representation Parsing. It explicitly encodes entities and events in a\nsingle semantic graph, and further has the flexibility to encode a wider range\nof additional IE relations and jointly infer individual tasks. JSEEGraph\nperforms in an end-to-end manner via general graph parsing: (1) instead of flat\nsequence labelling, nested structures between entities/triggers are efficiently\nencoded as separate nodes in the graph, allowing for nested and overlapping\nentities and triggers; (2) both entities, relations, and events can be encoded\nin the same graph, where entities and event triggers are represented as nodes\nand entity relations and event arguments are constructed via edges; (3) joint\ninference avoids error propagation and enhances the interpolation of different\nIE tasks. We experiment on two benchmark datasets of varying structural\ncomplexities; ACE05 and Rich ERE, covering three languages: English, Chinese,\nand Spanish. Experimental results show that JSEEGraph can handle nested event\nstructures, that it is beneficial to solve different IE tasks jointly, and that\nevent argument extraction in particular benefits from entity extraction. Our\ncode and models are released as open-source.", "published": "2023-06-26 12:12:54", "link": "http://arxiv.org/abs/2306.14633v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ontology Enrichment from Texts: A Biomedical Dataset for Concept\n  Discovery and Placement", "abstract": "Mentions of new concepts appear regularly in texts and require automated\napproaches to harvest and place them into Knowledge Bases (KB), e.g.,\nontologies and taxonomies. Existing datasets suffer from three issues, (i)\nmostly assuming that a new concept is pre-discovered and cannot support\nout-of-KB mention discovery; (ii) only using the concept label as the input\nalong with the KB and thus lacking the contexts of a concept label; and (iii)\nmostly focusing on concept placement w.r.t a taxonomy of atomic concepts,\ninstead of complex concepts, i.e., with logical operators. To address these\nissues, we propose a new benchmark, adapting MedMentions dataset (PubMed\nabstracts) with SNOMED CT versions in 2014 and 2017 under the Diseases\nsub-category and the broader categories of Clinical finding, Procedure, and\nPharmaceutical / biologic product. We provide usage on the evaluation with the\ndataset for out-of-KB mention discovery and concept placement, adapting recent\nLarge Language Model based methods.", "published": "2023-06-26 13:54:47", "link": "http://arxiv.org/abs/2306.14704v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Repr\u00e9sentation graphique de la langue des signes fran\u00e7aise et\n  \u00e9dition logicielle", "abstract": "Cet article propose une m\\'ethode pour d\\'efinir une forme graphique\n\\'editable standardis\\'ee pour les langues des signes, ainsi qu'une proposition\n\"AZVD\" et un \\'editeur logiciel associ\\'e. Inspir\\'ee d'une part par les\nr\\'egularit\\'es observ\\'ees dans les pratiques spontan\\'ees de locuteurs\npratiquant la sch\\'ematisation, la d\\'emarche tente garantir un syst\\`eme\nqualifi\\'e d'adoptable. Li\\'ee d'autre part au mod\\`ele formel de\nrepr\\'esentation AZee, elle vise \\'egalement \\`a sp\\'ecifier un syst\\`eme dont\ntoutes les productions ont une lecture d\\'etermin\\'ee au point o\\`u elles sont\nautomatiquement synth\\'etisables par un avatar.\n  --\n  This paper proposes a definition method for an editable standard graphical\nform of Sign Language discourse representation. It also puts forward a\ntentative system \"AZVD\", and presents an associated software editor. The system\nis inspired by the regularities observed in spontaneous diagrams produced by\nsome language users, in order to make it as adoptable as possible. Moreover, it\nis built upon the formal representation model AZee, so that any graphical\ninstance produced by the system determines its own read-out form, to the point\nthat they can be automatically synthesised by an avatar.", "published": "2023-06-26 15:09:51", "link": "http://arxiv.org/abs/2306.14754v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncovering Political Hate Speech During Indian Election Campaign: A New\n  Low-Resource Dataset and Baselines", "abstract": "The detection of hate speech in political discourse is a critical issue, and\nthis becomes even more challenging in low-resource languages. To address this\nissue, we introduce a new dataset named IEHate, which contains 11,457 manually\nannotated Hindi tweets related to the Indian Assembly Election Campaign from\nNovember 1, 2021, to March 9, 2022. We performed a detailed analysis of the\ndataset, focusing on the prevalence of hate speech in political communication\nand the different forms of hateful language used. Additionally, we benchmark\nthe dataset using a range of machine learning, deep learning, and\ntransformer-based algorithms. Our experiments reveal that the performance of\nthese models can be further improved, highlighting the need for more advanced\ntechniques for hate speech detection in low-resource languages. In particular,\nthe relatively higher score of human evaluation over algorithms emphasizes the\nimportance of utilizing both human and automated approaches for effective hate\nspeech moderation. Our IEHate dataset can serve as a valuable resource for\nresearchers and practitioners working on developing and evaluating hate speech\ndetection techniques in low-resource languages. Overall, our work underscores\nthe importance of addressing the challenges of identifying and mitigating hate\nspeech in political discourse, particularly in the context of low-resource\nlanguages. The dataset and resources for this work are made available at\nhttps://github.com/Farhan-jafri/Indian-Election.", "published": "2023-06-26 15:17:54", "link": "http://arxiv.org/abs/2306.14764v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enriching the NArabizi Treebank: A Multifaceted Approach to Supporting\n  an Under-Resourced Language", "abstract": "In this paper we address the scarcity of annotated data for NArabizi, a\nRomanized form of North African Arabic used mostly on social media, which poses\nchallenges for Natural Language Processing (NLP). We introduce an enriched\nversion of NArabizi Treebank (Seddah et al., 2020) with three main\ncontributions: the addition of two novel annotation layers (named entity\nrecognition and offensive language detection) and a re-annotation of the\ntokenization, morpho-syntactic and syntactic layers that ensure annotation\nconsistency. Our experimental results, using different tokenization schemes,\nshowcase the value of our contributions and highlight the impact of working\nwith non-gold tokenization for NER and dependency parsing. To facilitate future\nresearch, we make these annotations publicly available. Our enhanced NArabizi\nTreebank paves the way for creating sophisticated language models and NLP tools\nfor this under-represented language.", "published": "2023-06-26 17:27:31", "link": "http://arxiv.org/abs/2306.14866v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Composing Parameter-Efficient Modules with Arithmetic Operations", "abstract": "As an efficient alternative to conventional full finetuning,\nparameter-efficient finetuning (PEFT) is becoming the prevailing method to\nadapt pretrained language models. In PEFT, a lightweight module is learned on\neach dataset while the underlying pretrained language model remains unchanged,\nresulting in multiple compact modules representing diverse skills when applied\nto various domains and tasks. In this paper, we propose to compose these\nparameter-efficient modules through linear arithmetic operations in the weight\nspace, thereby integrating different module capabilities. Specifically, we\nfirst define addition and negation operators for the module, and then further\ncompose these two basic operators to perform flexible arithmetic. Our approach\nrequires \\emph{no additional training} and enables highly flexible module\ncomposition. We apply different arithmetic operations to compose the\nparameter-efficient modules for (1) distribution generalization, (2)\nmulti-tasking, (3) unlearning, and (4) domain transfer. Additionally, we extend\nour approach to detoxify Alpaca-LoRA, the latest instruction-tuned large\nlanguage model based on LLaMA. Empirical results demonstrate that our approach\nproduces new and effective parameter-efficient modules that significantly\noutperform existing ones across all settings.", "published": "2023-06-26 17:33:21", "link": "http://arxiv.org/abs/2306.14870v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding In-Context Learning via Supportive Pretraining Data", "abstract": "In-context learning (ICL) improves language models' performance on a variety\nof NLP tasks by simply demonstrating a handful of examples at inference time.\nIt is not well understood why ICL ability emerges, as the model has never been\nspecifically trained on such demonstrations. Unlike prior work that explores\nimplicit mechanisms behind ICL, we study ICL via investigating the pretraining\ndata. Specifically, we first adapt an iterative, gradient-based approach to\nfind a small subset of pretraining data that supports ICL. We observe that a\ncontinued pretraining on this small subset significantly improves the model's\nICL ability, by up to 18%. We then compare the supportive subset constrastively\nwith random subsets of pretraining data and discover: (1) The supportive\npretraining data to ICL do not have a higher domain relevance to downstream\ntasks. (2) The supportive pretraining data have a higher mass of rarely\noccurring, long-tail tokens. (3) The supportive pretraining data are\nchallenging examples where the information gain from long-range context is\nbelow average, indicating learning to incorporate difficult long-range context\nencourages ICL. Our work takes a first step towards understanding ICL via\nanalyzing instance-level pretraining data. Our insights have a potential to\nenhance the ICL ability of language models by actively guiding the construction\nof pretraining data in the future.", "published": "2023-06-26 22:14:04", "link": "http://arxiv.org/abs/2306.15091v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structured Dialogue Discourse Parsing", "abstract": "Dialogue discourse parsing aims to uncover the internal structure of a\nmulti-participant conversation by finding all the discourse~\\emph{links} and\ncorresponding~\\emph{relations}. Previous work either treats this task as a\nseries of independent multiple-choice problems, in which the link existence and\nrelations are decoded separately, or the encoding is restricted to only local\ninteraction, ignoring the holistic structural information. In contrast, we\npropose a principled method that improves upon previous work from two\nperspectives: encoding and decoding. From the encoding side, we perform\nstructured encoding on the adjacency matrix followed by the matrix-tree\nlearning algorithm, where all discourse links and relations in the dialogue are\njointly optimized based on latent tree-level distribution. From the decoding\nside, we perform structured inference using the modified Chiu-Liu-Edmonds\nalgorithm, which explicitly generates the labeled multi-root non-projective\nspanning tree that best captures the discourse structure. In addition, unlike\nin previous work, we do not rely on hand-crafted features; this improves the\nmodel's robustness. Experiments show that our method achieves new\nstate-of-the-art, surpassing the previous model by 2.3 on STAC and 1.5 on\nMolweni (F1 scores). \\footnote{Code released\nat~\\url{https://github.com/chijames/structured_dialogue_discourse_parsing}.}", "published": "2023-06-26 22:51:01", "link": "http://arxiv.org/abs/2306.15103v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FeedbackMap: a tool for making sense of open-ended survey responses", "abstract": "Analyzing open-ended survey responses is a crucial yet challenging task for\nsocial scientists, non-profit organizations, and educational institutions, as\nthey often face the trade-off between obtaining rich data and the burden of\nreading and coding textual responses. This demo introduces FeedbackMap, a\nweb-based tool that uses natural language processing techniques to facilitate\nthe analysis of open-ended survey responses. FeedbackMap lets researchers\ngenerate summaries at multiple levels, identify interesting response examples,\nand visualize the response space through embeddings. We discuss the importance\nof examining survey results from multiple perspectives and the potential biases\nintroduced by summarization methods, emphasizing the need for critical\nevaluation of the representation and omission of respondent voices.", "published": "2023-06-26 23:38:24", "link": "http://arxiv.org/abs/2306.15112v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inter-Annotator Agreement in the Wild: Uncovering Its Emerging Roles and\n  Considerations in Real-World Scenarios", "abstract": "Inter-Annotator Agreement (IAA) is commonly used as a measure of label\nconsistency in natural language processing tasks. However, in real-world\nscenarios, IAA has various roles and implications beyond its traditional usage.\nIn this paper, we not only consider IAA as a measure of consistency but also as\na versatile tool that can be effectively utilized in practical applications.\nMoreover, we discuss various considerations and potential concerns when\napplying IAA and suggest strategies for effectively navigating these\nchallenges.", "published": "2023-06-26 01:28:58", "link": "http://arxiv.org/abs/2306.14373v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transcending Traditional Boundaries: Leveraging Inter-Annotator\n  Agreement (IAA) for Enhancing Data Management Operations (DMOps)", "abstract": "This paper presents a novel approach of leveraging Inter-Annotator Agreement\n(IAA), traditionally used for assessing labeling consistency, to optimize Data\nManagement Operations (DMOps). We advocate for the use of IAA in predicting the\nlabeling quality of individual annotators, leading to cost and time efficiency\nin data production. Additionally, our work highlights the potential of IAA in\nforecasting document difficulty, thereby boosting the data construction\nprocess's overall efficiency. This research underscores IAA's broader\napplication potential in data-driven research optimization and holds\nsignificant implications for large-scale data projects prioritizing efficiency,\ncost reduction, and high-quality data.", "published": "2023-06-26 01:33:58", "link": "http://arxiv.org/abs/2306.14374v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Synthetic Alone: Exploring the Dark Side of Synthetic Data for\n  Grammatical Error Correction", "abstract": "Data-centric AI approach aims to enhance the model performance without\nmodifying the model and has been shown to impact model performance positively.\nWhile recent attention has been given to data-centric AI based on synthetic\ndata, due to its potential for performance improvement, data-centric AI has\nlong been exclusively validated using real-world data and publicly available\nbenchmark datasets. In respect of this, data-centric AI still highly depends on\nreal-world data, and the verification of models using synthetic data has not\nyet been thoroughly carried out. Given the challenges above, we ask the\nquestion: Does data quality control (noise injection and balanced data), a\ndata-centric AI methodology acclaimed to have a positive impact, exhibit the\nsame positive impact in models trained solely with synthetic data? To address\nthis question, we conducted comparative analyses between models trained on\nsynthetic and real-world data based on grammatical error correction (GEC) task.\nOur experimental results reveal that the data quality control method has a\npositive impact on models trained with real-world data, as previously reported\nin existing studies, while a negative impact is observed in models trained\nsolely on synthetic data.", "published": "2023-06-26 01:40:28", "link": "http://arxiv.org/abs/2306.14377v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph-Augmented Korean Generative Commonsense Reasoning", "abstract": "Generative commonsense reasoning refers to the task of generating acceptable\nand logical assumptions about everyday situations based on commonsense\nunderstanding. By utilizing an existing dataset such as Korean CommonGen,\nlanguage generation models can learn commonsense reasoning specific to the\nKorean language. However, language models often fail to consider the\nrelationships between concepts and the deep knowledge inherent to concepts. To\naddress these limitations, we propose a method to utilize the Korean knowledge\ngraph data for text generation. Our experimental result shows that the proposed\nmethod can enhance the efficiency of Korean commonsense inference, thereby\nunderlining the significance of employing supplementary data.", "published": "2023-06-26 07:23:47", "link": "http://arxiv.org/abs/2306.14470v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data-Driven Approach for Formality-Sensitive Machine Translation:\n  Language-Specific Handling and Synthetic Data Generation", "abstract": "In this paper, we introduce a data-driven approach for Formality-Sensitive\nMachine Translation (FSMT) that caters to the unique linguistic properties of\nfour target languages. Our methodology centers on two core strategies: 1)\nlanguage-specific data handling, and 2) synthetic data generation using\nlarge-scale language models and empirical prompt engineering. This approach\ndemonstrates a considerable improvement over the baseline, highlighting the\neffectiveness of data-centric techniques. Our prompt engineering strategy\nfurther improves performance by producing superior synthetic translation\nexamples.", "published": "2023-06-26 08:45:47", "link": "http://arxiv.org/abs/2306.14514v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TransERR: Translation-based Knowledge Graph Embedding via Efficient\n  Relation Rotation", "abstract": "This paper presents a translation-based knowledge geraph embedding method via\nefficient relation rotation (TransERR), a straightforward yet effective\nalternative to traditional translation-based knowledge graph embedding models.\nDifferent from the previous translation-based models, TransERR encodes\nknowledge graphs in the hypercomplex-valued space, thus enabling it to possess\na higher degree of translation freedom in mining latent information between the\nhead and tail entities. To further minimize the translation distance, TransERR\nadaptively rotates the head entity and the tail entity with their corresponding\nunit quaternions, which are learnable in model training. We also provide\nmathematical proofs to demonstrate the ability of TransERR in modeling various\nrelation patterns, including symmetry, antisymmetry, inversion, composition,\nand subrelation patterns. The experiments on 10 benchmark datasets validate the\neffectiveness and the generalization of TransERR. The results also indicate\nthat TransERR can better encode large-scale datasets with fewer parameters than\nthe previous translation-based models. Our code and datasets are available\nat~\\url{https://github.com/dellixx/TransERR}.", "published": "2023-06-26 10:45:16", "link": "http://arxiv.org/abs/2306.14580v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transfer Learning across Several Centuries: Machine and Historian\n  Integrated Method to Decipher Royal Secretary's Diary", "abstract": "A named entity recognition and classification plays the first and foremost\nimportant role in capturing semantics in data and anchoring in translation as\nwell as downstream study for history. However, NER in historical text has faced\nchallenges such as scarcity of annotated corpus, multilanguage variety, various\nnoise, and different convention far different from the contemporary language\nmodel. This paper introduces Korean historical corpus (Diary of Royal secretary\nwhich is named SeungJeongWon) recorded over several centuries and recently\nadded with named entity information as well as phrase markers which historians\ncarefully annotated. We fined-tuned the language model on history corpus,\nconducted extensive comparative experiments using our language model and\npretrained muti-language models. We set up the hypothesis of combination of\ntime and annotation information and tested it based on statistical t test. Our\nfinding shows that phrase markers clearly improve the performance of NER model\nin predicting unseen entity in documents written far different time period. It\nalso shows that each of phrase marker and corpus-specific trained model does\nnot improve the performance. We discuss the future research directions and\npractical strategies to decipher the history document.", "published": "2023-06-26 11:00:35", "link": "http://arxiv.org/abs/2306.14592v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Factorised Speaker-environment Adaptive Training of Conformer Speech\n  Recognition Systems", "abstract": "Rich sources of variability in natural speech present significant challenges\nto current data intensive speech recognition technologies. To model both\nspeaker and environment level diversity, this paper proposes a novel Bayesian\nfactorised speaker-environment adaptive training and test time adaptation\napproach for Conformer ASR models. Speaker and environment level\ncharacteristics are separately modeled using compact hidden output transforms,\nwhich are then linearly or hierarchically combined to represent any\nspeaker-environment combination. Bayesian learning is further utilized to model\nthe adaptation parameter uncertainty. Experiments on the 300-hr WHAM noise\ncorrupted Switchboard data suggest that factorised adaptation consistently\noutperforms the baseline and speaker label only adapted Conformers by up to\n3.1% absolute (10.4% relative) word error rate reductions. Further analysis\nshows the proposed method offers potential for rapid adaption to unseen\nspeaker-environment conditions.", "published": "2023-06-26 11:32:05", "link": "http://arxiv.org/abs/2306.14608v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "How About Kind of Generating Hedges using End-to-End Neural Models?", "abstract": "Hedging is a strategy for softening the impact of a statement in\nconversation. In reducing the strength of an expression, it may help to avoid\nembarrassment (more technically, ``face threat'') to one's listener. For this\nreason, it is often found in contexts of instruction, such as tutoring. In this\nwork, we develop a model of hedge generation based on i) fine-tuning\nstate-of-the-art language models trained on human-human tutoring data, followed\nby ii) reranking to select the candidate that best matches the expected hedging\nstrategy within a candidate pool using a hedge classifier. We apply this method\nto a natural peer-tutoring corpus containing a significant number of\ndisfluencies, repetitions, and repairs. The results show that generation in\nthis noisy environment is feasible with reranking. By conducting an error\nanalysis for both approaches, we reveal the challenges faced by systems\nattempting to accomplish both social and task-oriented goals in conversation.", "published": "2023-06-26 13:43:06", "link": "http://arxiv.org/abs/2306.14696v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Automatic Assessment of Divergent Thinking in Chinese Language with\n  TransDis: A Transformer-Based Language Model Approach", "abstract": "Language models have been increasingly popular for automatic creativity\nassessment, generating semantic distances to objectively measure the quality of\ncreative ideas. However, there is currently a lack of an automatic assessment\nsystem for evaluating creative ideas in the Chinese language. To address this\ngap, we developed TransDis, a scoring system using transformer-based language\nmodels, capable of providing valid originality (quality) and flexibility\n(variety) scores for Alternative Uses Task (AUT) responses in Chinese. Study 1\ndemonstrated that the latent model-rated originality factor, comprised of three\ntransformer-based models, strongly predicted human originality ratings, and the\nmodel-rated flexibility strongly correlated with human flexibility ratings as\nwell. Criterion validity analyses indicated that model-rated originality and\nflexibility positively correlated to other creativity measures, demonstrating\nsimilar validity to human ratings. Study 2 & 3 showed that TransDis effectively\ndistinguished participants instructed to provide creative vs. common uses\n(Study 2) and participants instructed to generate ideas in a flexible vs.\npersistent way (Study 3). Our findings suggest that TransDis can be a reliable\nand low-cost tool for measuring idea originality and flexibility in Chinese\nlanguage, potentially paving the way for automatic creativity assessment in\nother languages. We offer an open platform to compute originality and\nflexibility for AUT responses in Chinese and over 50 other languages\n(https://osf.io/59jv2/).", "published": "2023-06-26 15:48:05", "link": "http://arxiv.org/abs/2306.14790v3", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "A Positive-Unlabeled Metric Learning Framework for Document-Level\n  Relation Extraction with Incomplete Labeling", "abstract": "The goal of document-level relation extraction (RE) is to identify relations\nbetween entities that span multiple sentences. Recently, incomplete labeling in\ndocument-level RE has received increasing attention, and some studies have used\nmethods such as positive-unlabeled learning to tackle this issue, but there is\nstill a lot of room for improvement. Motivated by this, we propose a\npositive-augmentation and positive-mixup positive-unlabeled metric learning\nframework (P3M). Specifically, we formulate document-level RE as a metric\nlearning problem. We aim to pull the distance closer between entity pair\nembedding and their corresponding relation embedding, while pushing it farther\naway from the none-class relation embedding. Additionally, we adapt the\npositive-unlabeled learning to this loss objective. In order to improve the\ngeneralizability of the model, we use dropout to augment positive samples and\npropose a positive-none-class mixup method. Extensive experiments show that P3M\nimproves the F1 score by approximately 4-10 points in document-level RE with\nincomplete labeling, and achieves state-of-the-art results in fully labeled\nscenarios. Furthermore, P3M has also demonstrated robustness to prior\nestimation bias in incomplete labeled scenarios.", "published": "2023-06-26 16:05:59", "link": "http://arxiv.org/abs/2306.14806v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Label-Aware Hyperbolic Embeddings for Fine-grained Emotion\n  Classification", "abstract": "Fine-grained emotion classification (FEC) is a challenging task.\nSpecifically, FEC needs to handle subtle nuance between labels, which can be\ncomplex and confusing. Most existing models only address text classification\nproblem in the euclidean space, which we believe may not be the optimal\nsolution as labels of close semantic (e.g., afraid and terrified) may not be\ndifferentiated in such space, which harms the performance. In this paper, we\npropose HypEmo, a novel framework that can integrate hyperbolic embeddings to\nimprove the FEC task. First, we learn label embeddings in the hyperbolic space\nto better capture their hierarchical structure, and then our model projects\ncontextualized representations to the hyperbolic space to compute the distance\nbetween samples and labels. Experimental results show that incorporating such\ndistance to weight cross entropy loss substantially improves the performance\nwith significantly higher efficiency. We evaluate our proposed model on two\nbenchmark datasets and found 4.8% relative improvement compared to the previous\nstate of the art with 43.2% fewer parameters and 76.9% less training time. Code\nis available at https: //github.com/dinobby/HypEmo.", "published": "2023-06-26 16:28:33", "link": "http://arxiv.org/abs/2306.14822v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Kosmos-2: Grounding Multimodal Large Language Models to the World", "abstract": "We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new\ncapabilities of perceiving object descriptions (e.g., bounding boxes) and\ngrounding text to the visual world. Specifically, we represent refer\nexpressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where\nobject descriptions are sequences of location tokens. Together with multimodal\ncorpora, we construct large-scale data of grounded image-text pairs (called\nGrIT) to train the model. In addition to the existing capabilities of MLLMs\n(e.g., perceiving general modalities, following instructions, and performing\nin-context learning), Kosmos-2 integrates the grounding capability into\ndownstream applications. We evaluate Kosmos-2 on a wide range of tasks,\nincluding (i) multimodal grounding, such as referring expression comprehension,\nand phrase grounding, (ii) multimodal referring, such as referring expression\ngeneration, (iii) perception-language tasks, and (iv) language understanding\nand generation. This work lays out the foundation for the development of\nEmbodiment AI and sheds light on the big convergence of language, multimodal\nperception, action, and world modeling, which is a key step toward artificial\ngeneral intelligence. Code and pretrained models are available at\nhttps://aka.ms/kosmos-2.", "published": "2023-06-26 16:32:47", "link": "http://arxiv.org/abs/2306.14824v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Vietnamese multi-document summary using subgraph selection approach --\n  VLSP 2022 AbMuSu Shared Task", "abstract": "Document summarization is a task to generate afluent, condensed summary for a\ndocument, andkeep important information. A cluster of documents serves as the\ninput for multi-document summarizing (MDS), while the cluster summary serves as\nthe output. In this paper, we focus on transforming the extractive MDS problem\ninto subgraph selection. Approaching the problem in the form of graphs helps to\ncapture simultaneously the relationship between sentences in the same document\nand between sentences in the same cluster based on exploiting the overall graph\nstructure and selected subgraphs. Experiments have been implemented on the\nVietnamese dataset published in VLSP Evaluation Campaign 2022. This model\ncurrently results in the top 10 participating teams reported on the ROUGH-2\n$F\\_1$ measure on the public test set.", "published": "2023-06-26 16:34:02", "link": "http://arxiv.org/abs/2306.14827v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HonestBait: Forward References for Attractive but Faithful Headline\n  Generation", "abstract": "Current methods for generating attractive headlines often learn directly from\ndata, which bases attractiveness on the number of user clicks and views.\nAlthough clicks or views do reflect user interest, they can fail to reveal how\nmuch interest is raised by the writing style and how much is due to the event\nor topic itself. Also, such approaches can lead to harmful inventions by\nover-exaggerating the content, aggravating the spread of false information. In\nthis work, we propose HonestBait, a novel framework for solving these issues\nfrom another aspect: generating headlines using forward references (FRs), a\nwriting technique often used for clickbait. A self-verification process is\nincluded during training to avoid spurious inventions. We begin with a\npreliminary user study to understand how FRs affect user interest, after which\nwe present PANCO1, an innovative dataset containing pairs of fake news with\nverified news for attractive but faithful news headline generation. Automatic\nmetrics and human evaluations show that our framework yields more attractive\nresults (+11.25% compared to human-written verified news headlines) while\nmaintaining high veracity, which helps promote real information to fight\nagainst fake news.", "published": "2023-06-26 16:34:37", "link": "http://arxiv.org/abs/2306.14828v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Art of Embedding Fusion: Optimizing Hate Speech Detection", "abstract": "Hate speech detection is a challenging natural language processing task that\nrequires capturing linguistic and contextual nuances. Pre-trained language\nmodels (PLMs) offer rich semantic representations of text that can improve this\ntask. However there is still limited knowledge about ways to effectively\ncombine representations across PLMs and leverage their complementary strengths.\nIn this work, we shed light on various combination techniques for several PLMs\nand comprehensively analyze their effectiveness. Our findings show that\ncombining embeddings leads to slight improvements but at a high computational\ncost and the choice of combination has marginal effect on the final outcome. We\nalso make our codebase public at\nhttps://github.com/aflah02/The-Art-of-Embedding-Fusion-Optimizing-Hate-Speech-Detection .", "published": "2023-06-26 17:30:35", "link": "http://arxiv.org/abs/2306.14939v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in\n  Large Language Models", "abstract": "We present WinoQueer: a benchmark specifically designed to measure whether\nlarge language models (LLMs) encode biases that are harmful to the LGBTQ+\ncommunity. The benchmark is community-sourced, via application of a novel\nmethod that generates a bias benchmark from a community survey. We apply our\nbenchmark to several popular LLMs and find that off-the-shelf models generally\ndo exhibit considerable anti-queer bias. Finally, we show that LLM bias against\na marginalized community can be somewhat mitigated by finetuning on data\nwritten about or by members of that community, and that social media text\nwritten by community members is more effective than news text written about the\ncommunity by non-members. Our method for community-in-the-loop benchmark\ndevelopment provides a blueprint for future researchers to develop\ncommunity-driven, harms-grounded LLM benchmarks for other marginalized\ncommunities.\n  Note: This version corrects a bug found in evaluation code after publication.\nGeneral findings have not changed, but tables 5 and 6 and figure 1 have been\ncorrected.", "published": "2023-06-26 22:07:33", "link": "http://arxiv.org/abs/2306.15087v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "The Singing Voice Conversion Challenge 2023", "abstract": "We present the latest iteration of the voice conversion challenge (VCC)\nseries, a bi-annual scientific event aiming to compare and understand different\nvoice conversion (VC) systems based on a common dataset. This year we shifted\nour focus to singing voice conversion (SVC), thus named the challenge the\nSinging Voice Conversion Challenge (SVCC). A new database was constructed for\ntwo tasks, namely in-domain and cross-domain SVC. The challenge was run for two\nmonths, and in total we received 26 submissions, including 2 baselines. Through\na large-scale crowd-sourced listening test, we observed that for both tasks,\nalthough human-level naturalness was achieved by the top system, no team was\nable to obtain a similarity score as high as the target speakers. Also, as\nexpected, cross-domain SVC is harder than in-domain SVC, especially in the\nsimilarity aspect. We also investigated whether existing objective measurements\nwere able to predict perceptual performance, and found that only few of them\ncould reach a significant correlation.", "published": "2023-06-26 05:04:58", "link": "http://arxiv.org/abs/2306.14422v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cross-Lingual Cross-Age Group Adaptation for Low-Resource Elderly Speech\n  Emotion Recognition", "abstract": "Speech emotion recognition plays a crucial role in human-computer\ninteractions. However, most speech emotion recognition research is biased\ntoward English-speaking adults, which hinders its applicability to other\ndemographic groups in different languages and age groups. In this work, we\nanalyze the transferability of emotion recognition across three different\nlanguages--English, Mandarin Chinese, and Cantonese; and 2 different age\ngroups--adults and the elderly. To conduct the experiment, we develop an\nEnglish-Mandarin speech emotion benchmark for adults and the elderly, BiMotion,\nand a Cantonese speech emotion dataset, YueMotion. This study concludes that\ndifferent language and age groups require specific speech features, thus making\ncross-lingual inference an unsuitable method. However, cross-group data\naugmentation is still beneficial to regularize the model, with linguistic\ndistance being a significant influence on cross-lingual transferability. We\nrelease publicly release our code at https://github.com/HLTCHKUST/elderly_ser.", "published": "2023-06-26 08:48:08", "link": "http://arxiv.org/abs/2306.14517v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Exploring the Robustness of Large Language Models for Solving\n  Programming Problems", "abstract": "Using large language models (LLMs) for source code has recently gained\nattention. LLMs, such as Transformer-based models like Codex and ChatGPT, have\nbeen shown to be highly capable of solving a wide range of programming\nproblems. However, the extent to which LLMs understand problem descriptions and\ngenerate programs accordingly or just retrieve source code from the most\nrelevant problem in training data based on superficial cues has not been\ndiscovered yet. To explore this research question, we conduct experiments to\nunderstand the robustness of several popular LLMs, CodeGen and GPT-3.5 series\nmodels, capable of tackling code generation tasks in introductory programming\nproblems. Our experimental results show that CodeGen and Codex are sensitive to\nthe superficial modifications of problem descriptions and significantly impact\ncode generation performance. Furthermore, we observe that Codex relies on\nvariable names, as randomized variables decrease the solved rate significantly.\nHowever, the state-of-the-art (SOTA) models, such as InstructGPT and ChatGPT,\nshow higher robustness to superficial modifications and have an outstanding\ncapability for solving programming problems. This highlights the fact that\nslight modifications to the prompts given to the LLMs can greatly affect code\ngeneration performance, and careful formatting of prompts is essential for\nhigh-quality code generation, while the SOTA models are becoming more robust to\nperturbations.", "published": "2023-06-26 10:48:50", "link": "http://arxiv.org/abs/2306.14583v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "SugarCrepe: Fixing Hackable Benchmarks for Vision-Language\n  Compositionality", "abstract": "In the last year alone, a surge of new benchmarks to measure compositional\nunderstanding of vision-language models have permeated the machine learning\necosystem. Given an image, these benchmarks probe a model's ability to identify\nits associated caption amongst a set of compositional distractors.\nSurprisingly, we find significant biases in all these benchmarks rendering them\nhackable. This hackability is so dire that blind models with no access to the\nimage outperform state-of-the-art vision-language models. To remedy this\nrampant vulnerability, we introduce SugarCrepe, a new benchmark for\nvision-language compositionality evaluation. We employ large language models,\ninstead of rule-based templates used in previous benchmarks, to generate fluent\nand sensical hard negatives, and utilize an adversarial refinement mechanism to\nmaximally reduce biases. We re-evaluate state-of-the-art models and recently\nproposed compositionality inducing strategies, and find that their improvements\nwere hugely overestimated, suggesting that more innovation is needed in this\nimportant direction. We release SugarCrepe and the code for evaluation at:\nhttps://github.com/RAIVNLab/sugar-crepe.", "published": "2023-06-26 11:35:22", "link": "http://arxiv.org/abs/2306.14610v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Learn over Past, Evolve for Future: Forecasting Temporal Trends for Fake\n  News Detection", "abstract": "Fake news detection has been a critical task for maintaining the health of\nthe online news ecosystem. However, very few existing works consider the\ntemporal shift issue caused by the rapidly-evolving nature of news data in\npractice, resulting in significant performance degradation when training on\npast data and testing on future data. In this paper, we observe that the\nappearances of news events on the same topic may display discernible patterns\nover time, and posit that such patterns can assist in selecting training\ninstances that could make the model adapt better to future data. Specifically,\nwe design an effective framework FTT (Forecasting Temporal Trends), which could\nforecast the temporal distribution patterns of news data and then guide the\ndetector to fast adapt to future distribution. Experiments on the real-world\ntemporally split dataset demonstrate the superiority of our proposed framework.\nThe code is available at https://github.com/ICTMCG/FTT-ACL23.", "published": "2023-06-26 14:29:05", "link": "http://arxiv.org/abs/2306.14728v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "MotionGPT: Human Motion as a Foreign Language", "abstract": "Though the advancement of pre-trained large language models unfolds, the\nexploration of building a unified model for language and other multi-modal\ndata, such as motion, remains challenging and untouched so far. Fortunately,\nhuman motion displays a semantic coupling akin to human language, often\nperceived as a form of body language. By fusing language data with large-scale\nmotion models, motion-language pre-training that can enhance the performance of\nmotion-related tasks becomes feasible. Driven by this insight, we propose\nMotionGPT, a unified, versatile, and user-friendly motion-language model to\nhandle multiple motion-relevant tasks. Specifically, we employ the discrete\nvector quantization for human motion and transfer 3D motion into motion tokens,\nsimilar to the generation process of word tokens. Building upon this \"motion\nvocabulary\", we perform language modeling on both motion and text in a unified\nmanner, treating human motion as a specific language. Moreover, inspired by\nprompt learning, we pre-train MotionGPT with a mixture of motion-language data\nand fine-tune it on prompt-based question-and-answer tasks. Extensive\nexperiments demonstrate that MotionGPT achieves state-of-the-art performances\non multiple motion tasks including text-driven motion generation, motion\ncaptioning, motion prediction, and motion in-between.", "published": "2023-06-26 15:53:02", "link": "http://arxiv.org/abs/2306.14795v2", "categories": ["cs.CV", "cs.CL", "cs.GR"], "primary_category": "cs.CV"}
{"title": "LongCoder: A Long-Range Pre-trained Language Model for Code Completion", "abstract": "In this paper, we introduce a new task for code completion that focuses on\nhandling long code input and propose a sparse Transformer model, called\nLongCoder, to address this task. LongCoder employs a sliding window mechanism\nfor self-attention and introduces two types of globally accessible tokens -\nbridge tokens and memory tokens - to improve performance and efficiency. Bridge\ntokens are inserted throughout the input sequence to aggregate local\ninformation and facilitate global interaction, while memory tokens are included\nto highlight important statements that may be invoked later and need to be\nmemorized, such as package imports and definitions of classes, functions, or\nstructures. We conduct experiments on a newly constructed dataset that contains\nlonger code context and the publicly available CodeXGLUE benchmark.\nExperimental results demonstrate that LongCoder achieves superior performance\non code completion tasks compared to previous models while maintaining\ncomparable efficiency in terms of computational resources during inference. All\nthe codes and data are available at https://github.com/microsoft/CodeBERT.", "published": "2023-06-26 17:59:24", "link": "http://arxiv.org/abs/2306.14893v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "InterCode: Standardizing and Benchmarking Interactive Coding with\n  Execution Feedback", "abstract": "Humans write code in a fundamentally interactive manner and rely on constant\nexecution feedback to correct errors, resolve ambiguities, and decompose tasks.\nWhile LLMs have recently exhibited promising coding capabilities, current\ncoding benchmarks mostly consider a static instruction-to-code sequence\ntransduction process, which has the potential for error propagation and a\ndisconnect between the generated code and its final execution environment. To\naddress this gap, we introduce InterCode, a lightweight, flexible, and\neasy-to-use framework of interactive coding as a standard reinforcement\nlearning (RL) environment, with code as actions and execution feedback as\nobservations. Our framework is language and platform agnostic, uses\nself-contained Docker environments to provide safe and reproducible execution,\nand is compatible out-of-the-box with traditional seq2seq coding methods, while\nenabling the development of new methods for interactive code generation. We use\nInterCode to create three interactive code environments with Bash, SQL, and\nPython as action spaces, leveraging data from the static NL2Bash, Spider, and\nMBPP datasets. We demonstrate InterCode's viability as a testbed by evaluating\nmultiple state-of-the-art LLMs configured with different prompting strategies\nsuch as ReAct and Plan & Solve. Our results showcase the benefits of\ninteractive code generation and demonstrate that InterCode can serve as a\nchallenging benchmark for advancing code understanding and generation\ncapabilities. InterCode is designed to be easily extensible and can even be\nused to create new tasks such as Capture the Flag, a popular coding puzzle that\nis inherently multi-step and involves multiple programming languages. Project\nsite with code and data: https://intercode-benchmark.github.io", "published": "2023-06-26 17:59:50", "link": "http://arxiv.org/abs/2306.14898v3", "categories": ["cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "FunQA: Towards Surprising Video Comprehension", "abstract": "Surprising videos, such as funny clips, creative performances, or visual\nillusions, attract significant attention. Enjoyment of these videos is not\nsimply a response to visual stimuli; rather, it hinges on the human capacity to\nunderstand (and appreciate) commonsense violations depicted in these videos. We\nintroduce FunQA, a challenging video question-answering (QA) dataset\nspecifically designed to evaluate and enhance the depth of video reasoning\nbased on counter-intuitive and fun videos. Unlike most video QA benchmarks\nwhich focus on less surprising contexts, e.g., cooking or instructional videos,\nFunQA covers three previously unexplored types of surprising videos: 1)\nHumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous\nQA tasks designed to assess the model's capability in counter-intuitive\ntimestamp localization, detailed video description, and reasoning around\ncounter-intuitiveness. We also pose higher-level tasks, such as attributing a\nfitting and vivid title to the video and scoring the video creativity. In\ntotal, the FunQA benchmark consists of 312K free-text QA pairs derived from\n4.3K video clips, spanning a total of 24 video hours. Moreover, we propose\nFunMentor, an agent designed for Vision-Language Models (VLMs) that uses\nmulti-turn dialogues to enhance models' understanding of counter-intuitiveness.\nExtensive experiments with existing VLMs demonstrate the effectiveness of\nFunMentor and reveal significant performance gaps for the FunQA videos across\nspatial-temporal reasoning, visual-centered reasoning, and free-text\ngeneration.", "published": "2023-06-26 17:59:55", "link": "http://arxiv.org/abs/2306.14899v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Integrating Bidirectional Long Short-Term Memory with Subword Embedding\n  for Authorship Attribution", "abstract": "The problem of unveiling the author of a given text document from multiple\ncandidate authors is called authorship attribution. Manifold word-based\nstylistic markers have been successfully used in deep learning methods to deal\nwith the intrinsic problem of authorship attribution. Unfortunately, the\nperformance of word-based authorship attribution systems is limited by the\nvocabulary of the training corpus. Literature has recommended character-based\nstylistic markers as an alternative to overcome the hidden word problem.\nHowever, character-based methods often fail to capture the sequential\nrelationship of words in texts which is a chasm for further improvement. The\nquestion addressed in this paper is whether it is possible to address the\nambiguity of hidden words in text documents while preserving the sequential\ncontext of words. Consequently, a method based on bidirectional long short-term\nmemory (BLSTM) with a 2-dimensional convolutional neural network (CNN) is\nproposed to capture sequential writing styles for authorship attribution. The\nBLSTM was used to obtain the sequential relationship among characteristics\nusing subword information. The 2-dimensional CNN was applied to understand the\nlocal syntactical position of the style from unlabeled input text. The proposed\nmethod was experimentally evaluated against numerous state-of-the-art methods\nacross the public corporal of CCAT50, IMDb62, Blog50, and Twitter50.\nExperimental results indicate accuracy improvement of 1.07\\%, and 0.96\\% on\nCCAT50 and Twitter, respectively, and produce comparable results on the\nremaining datasets.", "published": "2023-06-26 11:35:47", "link": "http://arxiv.org/abs/2306.14933v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species\n  Genome", "abstract": "Decoding the linguistic intricacies of the genome is a crucial problem in\nbiology, and pre-trained foundational models such as DNABERT and Nucleotide\nTransformer have made significant strides in this area. Existing works have\nlargely hinged on k-mer, fixed-length permutations of A, T, C, and G, as the\ntoken of the genome language due to its simplicity. However, we argue that the\ncomputation and sample inefficiencies introduced by k-mer tokenization are\nprimary obstacles in developing large genome foundational models. We provide\nconceptual and empirical insights into genome tokenization, building on which\nwe propose to replace k-mer tokenization with Byte Pair Encoding (BPE), a\nstatistics-based data compression algorithm that constructs tokens by\niteratively merging the most frequent co-occurring genome segment in the\ncorpus. We demonstrate that BPE not only overcomes the limitations of k-mer\ntokenization but also benefits from the computational efficiency of\nnon-overlapping tokenization. Based on these insights, we introduce DNABERT-2,\na refined genome foundation model that adapts an efficient tokenizer and\nemploys multiple strategies to overcome input length constraints, reduce time\nand memory expenditure, and enhance model capability. Furthermore, we identify\nthe absence of a comprehensive and standardized benchmark for genome\nunderstanding as another significant impediment to fair comparative analysis.\nIn response, we propose the Genome Understanding Evaluation (GUE), a\ncomprehensive multi-species genome classification dataset that amalgamates $36$\ndistinct datasets across $9$ tasks, with input lengths ranging from $70$ to\n$10000$. Through comprehensive experiments on the GUE benchmark, we demonstrate\nthat DNABERT-2 achieves comparable performance to the state-of-the-art model\nwith $21 \\times$ fewer parameters and approximately $92 \\times$ less GPU time\nin pre-training.", "published": "2023-06-26 18:43:46", "link": "http://arxiv.org/abs/2306.15006v2", "categories": ["q-bio.GN", "cs.AI", "cs.CE", "cs.CL"], "primary_category": "q-bio.GN"}
{"title": "Pretraining task diversity and the emergence of non-Bayesian in-context\n  learning for regression", "abstract": "Pretrained transformers exhibit the remarkable ability of in-context learning\n(ICL): they can learn tasks from just a few examples provided in the prompt\nwithout updating any weights. This raises a foundational question: can ICL\nsolve fundamentally $\\textit{new}$ tasks that are very different from those\nseen during pretraining? To probe this question, we examine ICL's performance\non linear regression while varying the diversity of tasks in the pretraining\ndataset. We empirically demonstrate a $\\textit{task diversity threshold}$ for\nthe emergence of ICL. Below this threshold, the pretrained transformer cannot\nsolve unseen regression tasks, instead behaving like a Bayesian estimator with\nthe $\\textit{non-diverse pretraining task distribution}$ as the prior. Beyond\nthis threshold, the transformer significantly outperforms this estimator; its\nbehavior aligns with that of ridge regression, corresponding to a Gaussian\nprior over $\\textit{all tasks}$, including those not seen during pretraining.\nThus, when pretrained on data with task diversity greater than the threshold,\ntransformers $\\textit{can}$ optimally solve fundamentally new tasks in-context.\nImportantly, this capability hinges on it deviating from the Bayes optimal\nestimator with the pretraining distribution as the prior. This study also\nexplores the effect of regularization, model capacity and task structure and\nunderscores, in a concrete example, the critical role of task diversity,\nalongside data and model scale, in the emergence of ICL. Code is available at\nhttps://github.com/mansheej/icl-task-diversity.", "published": "2023-06-26 21:05:20", "link": "http://arxiv.org/abs/2306.15063v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Are aligned neural networks adversarially aligned?", "abstract": "Large language models are now tuned to align with the goals of their\ncreators, namely to be \"helpful and harmless.\" These models should respond\nhelpfully to user questions, but refuse to answer requests that could cause\nharm. However, adversarial users can construct inputs which circumvent attempts\nat alignment. In this work, we study adversarial alignment, and ask to what\nextent these models remain aligned when interacting with an adversarial user\nwho constructs worst-case inputs (adversarial examples). These inputs are\ndesigned to cause the model to emit harmful content that would otherwise be\nprohibited. We show that existing NLP-based optimization attacks are\ninsufficiently powerful to reliably attack aligned text models: even when\ncurrent NLP-based attacks fail, we can find adversarial inputs with brute\nforce. As a result, the failure of current attacks should not be seen as proof\nthat aligned text models remain aligned under adversarial inputs.\n  However the recent trend in large-scale ML models is multimodal models that\nallow users to provide images that influence the text that is generated. We\nshow these models can be easily attacked, i.e., induced to perform arbitrary\nun-aligned behavior through adversarial perturbation of the input image. We\nconjecture that improved NLP attacks may demonstrate this same level of\nadversarial control over text-only models.", "published": "2023-06-26 17:18:44", "link": "http://arxiv.org/abs/2306.15447v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mitigating Hallucination in Large Multi-Modal Models via Robust\n  Instruction Tuning", "abstract": "Despite the promising progress in multi-modal tasks, current large\nmulti-modal models (LMMs) are prone to hallucinating inconsistent descriptions\nwith respect to the associated image and human instructions. This paper\naddresses this issue by introducing the first large and diverse visual\ninstruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction.\nOur dataset comprises 400k visual instructions generated by GPT4, covering 16\nvision-and-language tasks with open-ended instructions and answers. Unlike\nexisting studies that primarily focus on positive instruction samples, we\ndesign LRV-Instruction to include both positive and negative instructions for\nmore robust visual instruction tuning. Our negative instructions are designed\nat three semantic levels: (i) Nonexistent Object Manipulation, (ii) Existent\nObject Manipulation and (iii) Knowledge Manipulation. To efficiently measure\nthe hallucination generated by LMMs, we propose GPT4-Assisted Visual\nInstruction Evaluation (GAVIE), a stable approach to evaluate visual\ninstruction tuning like human experts. GAVIE does not require human-annotated\ngroundtruth answers and can adapt to diverse instruction formats. We conduct\ncomprehensive experiments to investigate the hallucination of LMMs. Our results\ndemonstrate existing LMMs exhibit significant hallucinations when presented\nwith our negative instructions, particularly Existent Object and Knowledge\nManipulation instructions. Moreover, we successfully mitigate hallucination by\nfinetuning MiniGPT4 and mPLUG-Owl on LRV-Instruction while improving\nperformance on several public datasets compared to state-of-the-art methods.\nAdditionally, we observed that a balanced ratio of positive and negative\ninstances in the training data leads to a more robust model. Code and data are\navailable at https://github.com/FuxiaoLiu/LRV-Instruction.", "published": "2023-06-26 10:26:33", "link": "http://arxiv.org/abs/2306.14565v4", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Community Detection Graph Convolutional Network for Overlap-Aware\n  Speaker Diarization", "abstract": "The clustering algorithm plays a crucial role in speaker diarization systems.\nHowever, traditional clustering algorithms suffer from the complex distribution\nof speaker embeddings and lack of digging potential relationships between\nspeakers in a session. We propose a novel graph-based clustering approach\ncalled Community Detection Graph Convolutional Network (CDGCN) to improve the\nperformance of the speaker diarization system. The CDGCN-based clustering\nmethod consists of graph generation, sub-graph detection, and Graph-based\nOverlapped Speech Detection (Graph-OSD). Firstly, the graph generation refines\nthe local linkages among speech segments. Secondly the sub-graph detection\nfinds the optimal global partition of the speaker graph. Finally, we view\nspeaker clustering for overlap-aware speaker diarization as an overlapped\ncommunity detection task and design a Graph-OSD component to output\noverlap-aware labels. By capturing local and global information, the speaker\ndiarization system with CDGCN clustering outperforms the traditional\nClustering-based Speaker Diarization (CSD) systems on the DIHARD III corpus.", "published": "2023-06-26 09:08:19", "link": "http://arxiv.org/abs/2306.14530v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Mono-to-stereo through parametric stereo generation", "abstract": "Generating a stereophonic presentation from a monophonic audio signal is a\nchallenging open task, especially if the goal is to obtain a realistic spatial\nimaging with a specific panning of sound elements. In this work, we propose to\nconvert mono to stereo by means of predicting parametric stereo (PS) parameters\nusing both nearest neighbor and deep network approaches. In combination with\nPS, we also propose to model the task with generative approaches, allowing to\nsynthesize multiple and equally-plausible stereo renditions from the same mono\nsignal. To achieve this, we consider both autoregressive and masked token\nmodelling approaches. We provide evidence that the proposed PS-based models\noutperform a competitive classical decorrelation baseline and that, within a PS\nprediction framework, modern generative models outshine equivalent\nnon-generative counterparts. Overall, our work positions both PS and generative\nmodelling as strong and appealing methodologies for mono-to-stereo upmixing. A\ndiscussion of the limitations of these approaches is also provided.", "published": "2023-06-26 12:33:29", "link": "http://arxiv.org/abs/2306.14647v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
