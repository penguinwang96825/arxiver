{"title": "Syntax-guided Controlled Generation of Paraphrases", "abstract": "Given a sentence (e.g., \"I like mangoes\") and a constraint (e.g., sentiment\nflip), the goal of controlled text generation is to produce a sentence that\nadapts the input sentence to meet the requirements of the constraint (e.g., \"I\nhate mangoes\"). Going beyond such simple constraints, recent works have started\nexploring the incorporation of complex syntactic-guidance as constraints in the\ntask of controlled paraphrase generation. In these methods, syntactic-guidance\nis sourced from a separate exemplar sentence. However, these prior works have\nonly utilized limited syntactic information available in the parse tree of the\nexemplar sentence. We address this limitation in the paper and propose Syntax\nGuided Controlled Paraphraser (SGCP), an end-to-end framework for syntactic\nparaphrase generation. We find that SGCP can generate syntax conforming\nsentences while not compromising on relevance. We perform extensive automated\nand human evaluations over multiple real-world English language datasets to\ndemonstrate the efficacy of SGCP over state-of-the-art baselines. To drive\nfuture research, we have made SGCP's source code available", "published": "2020-05-18 01:31:28", "link": "http://arxiv.org/abs/2005.08417v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Classification with Few Examples using Controlled Generalization", "abstract": "Training data for text classification is often limited in practice,\nespecially for applications with many output classes or involving many related\nclassification problems. This means classifiers must generalize from limited\nevidence, but the manner and extent of generalization is task dependent.\nCurrent practice primarily relies on pre-trained word embeddings to map words\nunseen in training to similar seen ones. Unfortunately, this squishes many\ncomponents of meaning into highly restricted capacity. Our alternative begins\nwith sparse pre-trained representations derived from unlabeled parsed corpora;\nbased on the available training data, we select features that offers the\nrelevant generalizations. This produces task-specific semantic vectors; here,\nwe show that a feed-forward network over these vectors is especially effective\nin low-data scenarios, compared to existing state-of-the-art methods. By\nfurther pairing this network with a convolutional neural network, we keep this\nedge in low data scenarios and remain competitive when using full training\nsets.", "published": "2020-05-18 06:04:58", "link": "http://arxiv.org/abs/2005.08469v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The presence of occupational structure in online texts based on word\n  embedding NLP models", "abstract": "Research on social stratification is closely linked to analysing the prestige\nassociated with different occupations. This research focuses on the positions\nof occupations in the semantic space represented by large amounts of textual\ndata. The results are compared to standard results in social stratification to\nsee whether the classical results are reproduced and if additional insights can\nbe gained into the social positions of occupations. The paper gives an\naffirmative answer to both questions. The results show fundamental similarity\nof the occupational structure obtained from text analysis to the structure\ndescribed by prestige and social distance scales. While our research reinforces\nmany theories and empirical findings of the traditional body of literature on\nsocial stratification and, in particular, occupational hierarchy, it pointed to\nthe importance of a factor not discussed in the main line of stratification\nliterature so far: the power and organizational aspect.", "published": "2020-05-18 11:49:35", "link": "http://arxiv.org/abs/2005.08612v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Corpus of Chinese Dynastic Histories: Gender Analysis over Two Millennia", "abstract": "Chinese dynastic histories form a large continuous linguistic space of\napproximately 2000 years, from the 3rd century BCE to the 18th century CE. The\nhistories are documented in Classical (Literary) Chinese in a corpus of over 20\nmillion characters, suitable for the computational analysis of historical\nlexicon and semantic change. However, there is no freely available open-source\ncorpus of these histories, making Classical Chinese low-resource. This project\nintroduces a new open-source corpus of twenty-four dynastic histories covered\nby Creative Commons license. An original list of Classical Chinese\ngender-specific terms was developed as a case study for analyzing the\nhistorical linguistic use of male and female terms. The study demonstrates\nconsiderable stability in the usage of these terms, with dominance of male\nterms. Exploration of word meanings uses keyword analysis of focus corpora\ncreated for genderspecific terms. This method yields meaningful semantic\nrepresentations that can be used for future studies of diachronic semantics.", "published": "2020-05-18 15:14:33", "link": "http://arxiv.org/abs/2005.08793v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interaction Matching for Long-Tail Multi-Label Classification", "abstract": "We present an elegant and effective approach for addressing limitations in\nexisting multi-label classification models by incorporating interaction\nmatching, a concept shown to be useful for ad-hoc search result ranking. By\nperforming soft n-gram interaction matching, we match labels with natural\nlanguage descriptions (which are common to have in most multi-labeling tasks).\nOur approach can be used to enhance existing multi-label classification\napproaches, which are biased toward frequently-occurring labels. We evaluate\nour approach on two challenging tasks: automatic medical coding of clinical\nnotes and automatic labeling of entities from software tutorial text. Our\nresults show that our method can yield up to an 11% relative improvement in\nmacro performance, with most of the gains stemming labels that appear\ninfrequently in the training set (i.e., the long tail of labels).", "published": "2020-05-18 15:27:55", "link": "http://arxiv.org/abs/2005.08805v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inflecting when there's no majority: Limitations of encoder-decoder\n  neural networks as cognitive models for German plurals", "abstract": "Can artificial neural networks learn to represent inflectional morphology and\ngeneralize to new words as human speakers do? Kirov and Cotterell (2018) argue\nthat the answer is yes: modern Encoder-Decoder (ED) architectures learn\nhuman-like behavior when inflecting English verbs, such as extending the\nregular past tense form -(e)d to novel words. However, their work does not\naddress the criticism raised by Marcus et al. (1995): that neural models may\nlearn to extend not the regular, but the most frequent class -- and thus fail\non tasks like German number inflection, where infrequent suffixes like -s can\nstill be productively generalized.\n  To investigate this question, we first collect a new dataset from German\nspeakers (production and ratings of plural forms for novel nouns) that is\ndesigned to avoid sources of information unavailable to the ED model. The\nspeaker data show high variability, and two suffixes evince 'regular' behavior,\nappearing more often with phonologically atypical inputs. Encoder-decoder\nmodels do generalize the most frequently produced plural class, but do not show\nhuman-like variability or 'regular' extension of these other plural markers. We\nconclude that modern neural models may still struggle with minority-class\ngeneralization.", "published": "2020-05-18 15:58:28", "link": "http://arxiv.org/abs/2005.08826v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grammatical gender associations outweigh topical gender bias in\n  crosslinguistic word embeddings", "abstract": "Recent research has demonstrated that vector space models of semantics can\nreflect undesirable biases in human culture. Our investigation of\ncrosslinguistic word embeddings reveals that topical gender bias interacts\nwith, and is surpassed in magnitude by, the effect of grammatical gender\nassociations, and both may be attenuated by corpus lemmatization. This finding\nhas implications for downstream applications such as machine translation.", "published": "2020-05-18 16:39:16", "link": "http://arxiv.org/abs/2005.08864v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Span-ConveRT: Few-shot Span Extraction for Dialog with Pretrained\n  Conversational Representations", "abstract": "We introduce Span-ConveRT, a light-weight model for dialog slot-filling which\nframes the task as a turn-based span extraction task. This formulation allows\nfor a simple integration of conversational knowledge coded in large pretrained\nconversational models such as ConveRT (Henderson et al., 2019). We show that\nleveraging such knowledge in Span-ConveRT is especially useful for few-shot\nlearning scenarios: we report consistent gains over 1) a span extractor that\ntrains representations from scratch in the target domain, and 2) a BERT-based\nspan extractor. In order to inspire more work on span extraction for the\nslot-filling task, we also release RESTAURANTS-8K, a new challenging data set\nof 8,198 utterances, compiled from actual conversations in the restaurant\nbooking domain.", "published": "2020-05-18 16:40:30", "link": "http://arxiv.org/abs/2005.08866v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reconstructing Maps from Text", "abstract": "Previous research has demonstrated that Distributional Semantic Models (DSMs)\nare capable of reconstructing maps from news corpora (Louwerse & Zwaan, 2009)\nand novels (Louwerse & Benesh, 2012). The capacity for reproducing maps is\nsurprising since DSMs notoriously lack perceptual grounding (De Vega et al.,\n2012). In this paper we investigate the statistical sources required in\nlanguage to infer maps, and resulting constraints placed on mechanisms of\nsemantic representation. Study 1 brings word co-occurrence under experimental\ncontrol to demonstrate that direct co-occurrence in language is necessary for\ntraditional DSMs to successfully reproduce maps. Study 2 presents an\ninstance-based DSM that is capable of reconstructing maps independent of the\nfrequency of co-occurrence of city names.", "published": "2020-05-18 17:57:24", "link": "http://arxiv.org/abs/2005.08932v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question-Driven Summarization of Answers to Consumer Health Questions", "abstract": "Automatic summarization of natural language is a widely studied area in\ncomputer science, one that is broadly applicable to anyone who routinely needs\nto understand large quantities of information. For example, in the medical\ndomain, recent developments in deep learning approaches to automatic\nsummarization have the potential to make health information more easily\naccessible to patients and consumers. However, to evaluate the quality of\nautomatically generated summaries of health information, gold-standard, human\ngenerated summaries are required. Using answers provided by the National\nLibrary of Medicine's consumer health question answering system, we present the\nMEDIQA Answer Summarization dataset, the first summarization collection\ncontaining question-driven summaries of answers to consumer health questions.\nThis dataset can be used to evaluate single or multi-document summaries\ngenerated by algorithms using extractive or abstractive approaches. In order to\nbenchmark the dataset, we include results of baseline and state-of-the-art deep\nlearning summarization models, demonstrating that this dataset can be used to\neffectively evaluate question-driven machine-generated summaries and promote\nfurther machine learning research in medical question answering.", "published": "2020-05-18 20:36:11", "link": "http://arxiv.org/abs/2005.09067v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are All Languages Created Equal in Multilingual BERT?", "abstract": "Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly\ngood cross-lingual performance on several NLP tasks, even without explicit\ncross-lingual signals. However, these evaluations have focused on cross-lingual\ntransfer with high-resource languages, covering only a third of the languages\ncovered by mBERT. We explore how mBERT performs on a much wider set of\nlanguages, focusing on the quality of representation for low-resource\nlanguages, measured by within-language performance. We consider three tasks:\nNamed Entity Recognition (99 languages), Part-of-speech Tagging, and Dependency\nParsing (54 languages each). mBERT does better than or comparable to baselines\non high resource languages but does much worse for low resource languages.\nFurthermore, monolingual BERT models for these languages do even worse. Paired\nwith similar languages, the performance gap between monolingual BERT and mBERT\ncan be narrowed. We find that better models for low resource languages require\nmore efficient pretraining techniques or more data.", "published": "2020-05-18 21:15:39", "link": "http://arxiv.org/abs/2005.09093v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "(Re)construing Meaning in NLP", "abstract": "Human speakers have an extensive toolkit of ways to express themselves. In\nthis paper, we engage with an idea largely absent from discussions of meaning\nin natural language understanding--namely, that the way something is expressed\nreflects different ways of conceptualizing or construing the information being\nconveyed. We first define this phenomenon more precisely, drawing on\nconsiderable prior work in theoretical cognitive semantics and\npsycholinguistics. We then survey some dimensions of construed meaning and show\nhow insights from construal could inform theoretical and practical work in NLP.", "published": "2020-05-18 21:21:34", "link": "http://arxiv.org/abs/2005.09099v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Generation of Dialogue Response Timings", "abstract": "The timings of spoken response offsets in human dialogue have been shown to\nvary based on contextual elements of the dialogue. We propose neural models\nthat simulate the distributions of these response offsets, taking into account\nthe response turn as well as the preceding turn. The models are designed to be\nintegrated into the pipeline of an incremental spoken dialogue system (SDS). We\nevaluate our models using offline experiments as well as human listening tests.\nWe show that human listeners consider certain response timings to be more\nnatural based on the dialogue context. The introduction of these models into\nSDS pipelines could increase the perceived naturalness of interactions.", "published": "2020-05-18 23:00:57", "link": "http://arxiv.org/abs/2005.09128v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Yseop at SemEval-2020 Task 5: Cascaded BERT Language Model for\n  Counterfactual Statement Analysis", "abstract": "In this paper, we explore strategies to detect and evaluate counterfactual\nsentences. We describe our system for SemEval-2020 Task 5: Modeling Causal\nReasoning in Language: Detecting Counterfactuals. We use a BERT base model for\nthe classification task and build a hybrid BERT Multi-Layer Perceptron system\nto handle the sequence identification task. Our experiments show that while\nintroducing syntactic and semantic features does little in improving the system\nin the classification task, using these types of features as cascaded linear\ninputs to fine-tune the sequence-delimiting ability of the model ensures it\noutperforms other similar-purpose complex systems like BiLSTM-CRF in the second\ntask. Our system achieves an F1 score of 85.00% in Task 1 and 83.90% in Task 2.", "published": "2020-05-18 08:19:18", "link": "http://arxiv.org/abs/2005.08519v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Design Choices for X-vector Based Speaker Anonymization", "abstract": "The recently proposed x-vector based anonymization scheme converts any input\nvoice into that of a random pseudo-speaker. In this paper, we present a\nflexible pseudo-speaker selection technique as a baseline for the first\nVoicePrivacy Challenge. We explore several design choices for the distance\nmetric between speakers, the region of x-vector space where the pseudo-speaker\nis picked, and gender selection. To assess the strength of anonymization\nachieved, we consider attackers using an x-vector based speaker verification\nsystem who may use original or anonymized speech for enrollment, depending on\ntheir knowledge of the anonymization scheme. The Equal Error Rate (EER)\nachieved by the attackers and the decoding Word Error Rate (WER) over\nanonymized data are reported as the measures of privacy and utility.\nExperiments are performed using datasets derived from LibriSpeech to find the\noptimal combination of design choices in terms of privacy and utility.", "published": "2020-05-18 11:32:14", "link": "http://arxiv.org/abs/2005.08601v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Improving Named Entity Recognition in Tor Darknet with Local Distance\n  Neighbor Feature", "abstract": "Name entity recognition in noisy user-generated texts is a difficult task\nusually enhanced by incorporating an external resource of information, such as\ngazetteers. However, gazetteers are task-specific, and they are expensive to\nbuild and maintain. This paper adopts and improves the approach of Aguilar et\nal. by presenting a novel feature, called Local Distance Neighbor, which\nsubstitutes gazetteers. We tested the new approach on the W-NUT-2017 dataset,\nobtaining state-of-the-art results for the Group, Person and Product categories\nof Named Entities. Next, we added 851 manually labeled samples to the\nW-NUT-2017 dataset to account for named entities in the Tor Darknet related to\nweapons and drug selling. Finally, our proposal achieved an entity and surface\nF1 scores of 52.96% and 50.57% on this extended dataset, demonstrating its\nusefulness for Law Enforcement Agencies to detect named entities in the Tor\nhidden services.", "published": "2020-05-18 14:21:22", "link": "http://arxiv.org/abs/2005.08746v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Classification of Spam Emails through Hierarchical Clustering and\n  Supervised Learning", "abstract": "Spammers take advantage of email popularity to send indiscriminately\nunsolicited emails. Although researchers and organizations continuously develop\nanti-spam filters based on binary classification, spammers bypass them through\nnew strategies, like word obfuscation or image-based spam. For the first time\nin literature, we propose to classify spam email in categories to improve the\nhandle of already detected spam emails, instead of just using a binary model.\nFirst, we applied a hierarchical clustering algorithm to create SPEMC-$11$K\n(SPam EMail Classification), the first multi-class dataset, which contains\nthree types of spam emails: Health and Technology, Personal Scams, and Sexual\nContent. Then, we used SPEMC-$11$K to evaluate the combination of TF-IDF and\nBOW encodings with Na\\\"ive Bayes, Decision Trees and SVM classifiers. Finally,\nwe recommend for the task of multi-class spam classification the use of (i)\nTF-IDF combined with SVM for the best micro F1 score performance, $95.39\\%$,\nand (ii) TD-IDF along with NB for the fastest spam classification, analyzing an\nemail in $2.13$ms.", "published": "2020-05-18 14:41:22", "link": "http://arxiv.org/abs/2005.08773v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Critical Impact of Social Networks Infodemic on Defeating Coronavirus\n  COVID-19 Pandemic: Twitter-Based Study and Research Directions", "abstract": "News creation and consumption has been changing since the advent of social\nmedia. An estimated 2.95 billion people in 2019 used social media worldwide.\nThe widespread of the Coronavirus COVID-19 resulted with a tsunami of social\nmedia. Most platforms were used to transmit relevant news, guidelines and\nprecautions to people. According to WHO, uncontrolled conspiracy theories and\npropaganda are spreading faster than the COVID-19 pandemic itself, creating an\ninfodemic and thus causing psychological panic, misleading medical advises, and\neconomic disruption. Accordingly, discussions have been initiated with the\nobjective of moderating all COVID-19 communications, except those initiated\nfrom trusted sources such as the WHO and authorized governmental entities. This\npaper presents a large-scale study based on data mined from Twitter. Extensive\nanalysis has been performed on approximately one million COVID-19 related\ntweets collected over a period of two months. Furthermore, the profiles of\n288,000 users were analyzed including unique users profiles, meta-data and\ntweets context. The study noted various interesting conclusions including the\ncritical impact of the (1) exploitation of the COVID-19 crisis to redirect\nreaders to irrelevant topics and (2) widespread of unauthentic medical\nprecautions and information. Further data analysis revealed the importance of\nusing social networks in a global pandemic crisis by relying on credible users\nwith variety of occupations, content developers and influencers in specific\nfields. In this context, several insights and findings have been provided while\nelaborating computing and non-computing implications and research directions\nfor potential solutions and social networks management strategies during crisis\nperiods.", "published": "2020-05-18 15:53:13", "link": "http://arxiv.org/abs/2005.08820v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "P-SIF: Document Embeddings Using Partition Averaging", "abstract": "Simple weighted averaging of word vectors often yields effective\nrepresentations for sentences which outperform sophisticated seq2seq neural\nmodels in many tasks. While it is desirable to use the same method to represent\ndocuments as well, unfortunately, the effectiveness is lost when representing\nlong documents involving multiple sentences. One of the key reasons is that a\nlonger document is likely to contain words from many different topics; hence,\ncreating a single vector while ignoring all the topical structure is unlikely\nto yield an effective document representation. This problem is less acute in\nsingle sentences and other short text fragments where the presence of a single\ntopic is most likely. To alleviate this problem, we present P-SIF, a\npartitioned word averaging model to represent long documents. P-SIF retains the\nsimplicity of simple weighted word averaging while taking a document's topical\nstructure into account. In particular, P-SIF learns topic-specific vectors from\na document and finally concatenates them all to represent the overall document.\nWe provide theoretical justifications on the correctness of P-SIF. Through a\ncomprehensive set of experiments, we demonstrate P-SIF's effectiveness compared\nto simple weighted averaging and many other baselines.", "published": "2020-05-18 20:41:12", "link": "http://arxiv.org/abs/2005.09069v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Contextual Embeddings: When Are They Worth It?", "abstract": "We study the settings for which deep contextual embeddings (e.g., BERT) give\nlarge improvements in performance relative to classic pretrained embeddings\n(e.g., GloVe), and an even simpler baseline---random word embeddings---focusing\non the impact of the training set size and the linguistic properties of the\ntask. Surprisingly, we find that both of these simpler baselines can match\ncontextual embeddings on industry-scale data, and often perform within 5 to 10%\naccuracy (absolute) on benchmark tasks. Furthermore, we identify properties of\ndata for which contextual embeddings give particularly large gains: language\ncontaining complex structure, ambiguous word usage, and words unseen in\ntraining.", "published": "2020-05-18 22:20:17", "link": "http://arxiv.org/abs/2005.09117v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GPT-too: A language-model-first approach for AMR-to-text generation", "abstract": "Meaning Representations (AMRs) are broad-coverage sentence-level semantic\ngraphs. Existing approaches to generating text from AMR have focused on\ntraining sequence-to-sequence or graph-to-sequence models on AMR annotated data\nonly. In this paper, we propose an alternative approach that combines a strong\npre-trained language model with cycle consistency-based re-scoring. Despite the\nsimplicity of the approach, our experimental results show these models\noutperform all previous techniques on the English LDC2017T10dataset, including\nthe recent use of transformer architectures. In addition to the standard\nevaluation metrics, we provide human evaluation experiments that further\nsubstantiate the strength of our approach.", "published": "2020-05-18 22:50:26", "link": "http://arxiv.org/abs/2005.09123v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Power of Unambiguity in B\u00fcchi Complementation", "abstract": "In this work, we exploit the power of \\emph{unambiguity} for the\ncomplementation problem of B\\\"uchi automata by utilizing reduced run directed\nacyclic graphs (DAGs) over infinite words, in which each vertex has at most one\npredecessor. We then show how to use this type of reduced run DAGs as a\n\\emph{unified tool} to optimize \\emph{both} rank-based and slice-based\ncomplementation constructions for B\\\"uchi automata with a finite degree of\nambiguity. As a result, given a B\\\"uchi automaton with $n$ states and a finite\ndegree of ambiguity, the number of states in the complementary B\\\"uchi\nautomaton constructed by the classical rank-based and slice-based\ncomplementation constructions can be improved, respectively, to $2^{O(n)}$ from\n$2^{O(n\\log n)}$ and to $O(4^n)$ from $O((3n)^n)$.", "published": "2020-05-18 22:51:34", "link": "http://arxiv.org/abs/2005.09125v2", "categories": ["cs.FL", "cs.CL"], "primary_category": "cs.FL"}
{"title": "NEJM-enzh: A Parallel Corpus for English-Chinese Translation in the\n  Biomedical Domain", "abstract": "Machine translation requires large amounts of parallel text. While such\ndatasets are abundant in domains such as newswire, they are less accessible in\nthe biomedical domain. Chinese and English are two of the most widely spoken\nlanguages, yet to our knowledge a parallel corpus in the biomedical domain does\nnot exist for this language pair. In this study, we develop an effective\npipeline to acquire and process an English-Chinese parallel corpus, consisting\nof about 100,000 sentence pairs and 3,000,000 tokens on each side, from the New\nEngland Journal of Medicine (NEJM). We show that training on out-of-domain data\nand fine-tuning with as few as 4,000 NEJM sentence pairs improve translation\nquality by 25.3 (13.4) BLEU for en$\\to$zh (zh$\\to$en) directions. Translation\nquality continues to improve at a slower pace on larger in-domain datasets,\nwith an increase of 33.0 (24.3) BLEU for en$\\to$zh (zh$\\to$en) directions on\nthe full dataset.", "published": "2020-05-18 23:25:15", "link": "http://arxiv.org/abs/2005.09133v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Weak-Attention Suppression For Transformer Based Speech Recognition", "abstract": "Transformers, originally proposed for natural language processing (NLP)\ntasks, have recently achieved great success in automatic speech recognition\n(ASR). However, adjacent acoustic units (i.e., frames) are highly correlated,\nand long-distance dependencies between them are weak, unlike text units. It\nsuggests that ASR will likely benefit from sparse and localized attention. In\nthis paper, we propose Weak-Attention Suppression (WAS), a method that\ndynamically induces sparsity in attention probabilities. We demonstrate that\nWAS leads to consistent Word Error Rate (WER) improvement over strong\ntransformer baselines. On the widely used LibriSpeech benchmark, our proposed\nmethod reduced WER by 10%$ on test-clean and 5% on test-other for streamable\ntransformers, resulting in a new state-of-the-art among streaming models.\nFurther analysis shows that WAS learns to suppress attention of non-critical\nand redundant continuous acoustic frames, and is more likely to suppress past\nframes rather than future ones. It indicates the importance of lookahead in\nattention-based ASR models.", "published": "2020-05-18 23:49:40", "link": "http://arxiv.org/abs/2005.09137v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "The NTNU System at the Interspeech 2020 Non-Native Children's Speech ASR\n  Challenge", "abstract": "This paper describes the NTNU ASR system participating in the Interspeech\n2020 Non-Native Children's Speech ASR Challenge supported by the SIG-CHILD\ngroup of ISCA. This ASR shared task is made much more challenging due to the\ncoexisting diversity of non-native and children speaking characteristics. In\nthe setting of closed-track evaluation, all participants were restricted to\ndevelop their systems merely based on the speech and text corpora provided by\nthe organizer. To work around this under-resourced issue, we built our ASR\nsystem on top of CNN-TDNNF-based acoustic models, meanwhile harnessing the\nsynergistic power of various data augmentation strategies, including both\nutterance- and word-level speed perturbation and spectrogram augmentation,\nalongside a simple yet effective data-cleansing approach. All variants of our\nASR system employed an RNN-based language model to rescore the first-pass\nrecognition hypotheses, which was trained solely on the text dataset released\nby the organizer. Our system with the best configuration came out in second\nplace, resulting in a word error rate (WER) of 17.59 %, while those of the\ntop-performing, second runner-up and official baseline systems are 15.67%,\n18.71%, 35.09%, respectively.", "published": "2020-05-18 02:51:26", "link": "http://arxiv.org/abs/2005.08433v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Effective End-to-End Modeling Approach for Mispronunciation Detection", "abstract": "Recently, end-to-end (E2E) automatic speech recognition (ASR) systems have\ngarnered tremendous attention because of their great success and unified\nmodeling paradigms in comparison to conventional hybrid DNN-HMM ASR systems.\nDespite the widespread adoption of E2E modeling frameworks on ASR, there still\nis a dearth of work on investigating the E2E frameworks for use in\ncomputer-assisted pronunciation learning (CAPT), particularly for\nMispronunciation detection (MD). In response, we first present a novel use of\nhybrid CTCAttention approach to the MD task, taking advantage of the strengths\nof both CTC and the attention-based model meanwhile getting around the need for\nphone-level forced alignment. Second, we perform input augmentation with text\nprompt information to make the resulting E2E model more tailored for the MD\ntask. On the other hand, we adopt two MD decision methods so as to better\ncooperate with the proposed framework: 1) decision-making based on a\nrecognition confidence measure or 2) simply based on speech recognition\nresults. A series of Mandarin MD experiments demonstrate that our approach not\nonly simplifies the processing pipeline of existing hybrid DNN-HMM systems but\nalso brings about systematic and substantial performance improvements.\nFurthermore, input augmentation with text prompts seems to hold excellent\npromise for the E2E-based MD approach.", "published": "2020-05-18 03:37:21", "link": "http://arxiv.org/abs/2005.08440v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Attention-based Transducer for Online Speech Recognition", "abstract": "Recent studies reveal the potential of recurrent neural network transducer\n(RNN-T) for end-to-end (E2E) speech recognition. Among some most popular E2E\nsystems including RNN-T, Attention Encoder-Decoder (AED), and Connectionist\nTemporal Classification (CTC), RNN-T has some clear advantages given that it\nsupports streaming recognition and does not have frame-independency assumption.\nAlthough significant progresses have been made for RNN-T research, it is still\nfacing performance challenges in terms of training speed and accuracy. We\npropose attention-based transducer with modification over RNN-T in two aspects.\nFirst, we introduce chunk-wise attention in the joint network. Second,\nself-attention is introduced in the encoder. Our proposed model outperforms\nRNN-T for both training speed and accuracy. For training, we achieves over 1.7x\nspeedup. With 500 hours LAIX non-native English training data, attention-based\ntransducer yields ~10.6% WER reduction over baseline RNN-T. Trained with full\nset of over 10K hours data, our final system achieves ~5.5% WER reduction over\nthat trained with the best Kaldi TDNN-f recipe. After 8-bit weight quantization\nwithout WER degradation, RTF and latency drop to 0.34~0.36 and 268~409\nmilliseconds respectively on a single CPU core of a production server.", "published": "2020-05-18 07:26:33", "link": "http://arxiv.org/abs/2005.08497v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Question Format Independent Numerical Reasoning: A Set of\n  Prerequisite Tasks", "abstract": "Numerical reasoning is often important to accurately understand the world.\nRecently, several format-specific datasets have been proposed, such as\nnumerical reasoning in the settings of Natural Language Inference (NLI),\nReading Comprehension (RC), and Question Answering (QA). Several\nformat-specific models and architectures in response to those datasets have\nalso been proposed. However, there exists a strong need for a benchmark which\ncan evaluate the abilities of models, in performing question format independent\nnumerical reasoning, as (i) the numerical reasoning capabilities we want to\nteach are not controlled by question formats, (ii) for numerical reasoning\ntechnology to have the best possible application, it must be able to process\nlanguage and reason in a way that is not exclusive to a single format, task,\ndataset or domain. In pursuit of this goal, we introduce NUMBERGAME, a\nmultifaceted benchmark to evaluate model performance across numerical reasoning\ntasks of eight diverse formats. We add four existing question types in our\ncompilation. Two of the new types we add are about questions that require\nexternal numerical knowledge, commonsense knowledge and domain knowledge. For\nbuilding a more practical numerical reasoning system, NUMBERGAME demands four\ncapabilities beyond numerical reasoning: (i) detecting question format directly\nfrom data (ii) finding intermediate common format to which every format can be\nconverted (iii) incorporating commonsense knowledge (iv) handling data\nimbalance across formats. We build several baselines, including a new model\nbased on knowledge hunting using a cheatsheet. However, all baselines perform\npoorly in contrast to the human baselines, indicating the hardness of our\nbenchmark. Our work takes forward the recent progress in generic system\ndevelopment, demonstrating the scope of these under-explored tasks.", "published": "2020-05-18 08:14:04", "link": "http://arxiv.org/abs/2005.08516v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Robust Training of Vector Quantized Bottleneck Models", "abstract": "In this paper we demonstrate methods for reliable and efficient training of\ndiscrete representation using Vector-Quantized Variational Auto-Encoder models\n(VQ-VAEs). Discrete latent variable models have been shown to learn nontrivial\nrepresentations of speech, applicable to unsupervised voice conversion and\nreaching state-of-the-art performance on unit discovery tasks. For unsupervised\nrepresentation learning, they became viable alternatives to continuous latent\nvariable models such as the Variational Auto-Encoder (VAE). However, training\ndeep discrete variable models is challenging, due to the inherent\nnon-differentiability of the discretization operation. In this paper we focus\non VQ-VAE, a state-of-the-art discrete bottleneck model shown to perform on par\nwith its continuous counterparts. It quantizes encoder outputs with on-line\n$k$-means clustering. We show that the codebook learning can suffer from poor\ninitialization and non-stationarity of clustered encoder outputs. We\ndemonstrate that these can be successfully overcome by increasing the learning\nrate for the codebook and periodic date-dependent codeword re-initialization.\nAs a result, we achieve more robust training across different tasks, and\nsignificantly increase the usage of latent codewords even for large codebooks.\nThis has practical benefit, for instance, in unsupervised representation\nlearning, where large codebooks may lead to disentanglement of latent\nrepresentations.", "published": "2020-05-18 08:23:41", "link": "http://arxiv.org/abs/2005.08520v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Audio-visual Multi-channel Recognition of Overlapped Speech", "abstract": "Automatic speech recognition (ASR) of overlapped speech remains a highly\nchallenging task to date. To this end, multi-channel microphone array data are\nwidely used in state-of-the-art ASR systems. Motivated by the invariance of\nvisual modality to acoustic signal corruption, this paper presents an\naudio-visual multi-channel overlapped speech recognition system featuring\ntightly integrated separation front-end and recognition back-end. A series of\naudio-visual multi-channel speech separation front-end components based on\n\\textit{TF masking}, \\textit{filter\\&sum} and \\textit{mask-based MVDR}\nbeamforming approaches were developed. To reduce the error cost mismatch\nbetween the separation and recognition components, they were jointly fine-tuned\nusing the connectionist temporal classification (CTC) loss function, or a\nmulti-task criterion interpolation with scale-invariant signal to noise ratio\n(Si-SNR) error cost. Experiments suggest that the proposed multi-channel AVSR\nsystem outperforms the baseline audio-only ASR system by up to 6.81\\% (26.83\\%\nrelative) and 22.22\\% (56.87\\% relative) absolute word error rate (WER)\nreduction on overlapped speech constructed using either simulation or replaying\nof the lipreading sentence 2 (LRS2) dataset respectively.", "published": "2020-05-18 10:31:19", "link": "http://arxiv.org/abs/2005.08571v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio ALBERT: A Lite BERT for Self-supervised Learning of Audio\n  Representation", "abstract": "For self-supervised speech processing, it is crucial to use pretrained models\nas speech representation extractors. In recent works, increasing the size of\nthe model has been utilized in acoustic model training in order to achieve\nbetter performance. In this paper, we propose Audio ALBERT, a lite version of\nthe self-supervised speech representation model. We use the representations\nwith two downstream tasks, speaker identification, and phoneme classification.\nWe show that Audio ALBERT is capable of achieving competitive performance with\nthose huge models in the downstream tasks while utilizing 91\\% fewer\nparameters. Moreover, we use some simple probing models to measure how much the\ninformation of the speaker and phoneme is encoded in latent representations. In\nprobing experiments, we find that the latent representations encode richer\ninformation of both phoneme and speaker than that of the last layer.", "published": "2020-05-18 10:42:44", "link": "http://arxiv.org/abs/2005.08575v5", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Efficient Wait-k Models for Simultaneous Machine Translation", "abstract": "Simultaneous machine translation consists in starting output generation\nbefore the entire input sequence is available. Wait-k decoders offer a simple\nbut efficient approach for this problem. They first read k source tokens, after\nwhich they alternate between producing a target token and reading another\nsource token. We investigate the behavior of wait-k decoding in low resource\nsettings for spoken corpora using IWSLT datasets. We improve training of these\nmodels using unidirectional encoders, and training across multiple values of k.\nExperiments with Transformer and 2D-convolutional architectures show that our\nwait-k models generalize well across a wide range of latency levels. We also\nshow that the 2D-convolution architecture is competitive with Transformers for\nsimultaneous translation of spoken language.", "published": "2020-05-18 11:14:23", "link": "http://arxiv.org/abs/2005.08595v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Conversational Search -- A Report from Dagstuhl Seminar 19461", "abstract": "Dagstuhl Seminar 19461 \"Conversational Search\" was held on 10-15 November\n2019. 44~researchers in Information Retrieval and Web Search, Natural Language\nProcessing, Human Computer Interaction, and Dialogue Systems were invited to\nshare the latest development in the area of Conversational Search and discuss\nits research agenda and future directions. A 5-day program of the seminar\nconsisted of six introductory and background sessions, three visionary talk\nsessions, one industry talk session, and seven working groups and reporting\nsessions. The seminar also had three social events during the program. This\nreport provides the executive summary, overview of invited talks, and findings\nfrom the seven working groups which cover the definition, evaluation,\nmodelling, explanation, scenarios, applications, and prototype of\nConversational Search. The ideas and findings presented in this report should\nserve as one of the main sources for diverse research programs on\nConversational Search.", "published": "2020-05-18 12:48:33", "link": "http://arxiv.org/abs/2005.08658v1", "categories": ["cs.IR", "cs.CL", "cs.HC"], "primary_category": "cs.IR"}
{"title": "Approaches to Improving Recognition of Underrepresented Named Entities\n  in Hybrid ASR Systems", "abstract": "In this paper, we present a series of complementary approaches to improve the\nrecognition of underrepresented named entities (NE) in hybrid ASR systems\nwithout compromising overall word error rate performance. The underrepresented\nwords correspond to rare or out-of-vocabulary (OOV) words in the training data,\nand thereby can't be modeled reliably. We begin with graphemic lexicon which\nallows to drop the necessity of phonetic models in hybrid ASR. We study it\nunder different settings and demonstrate its effectiveness in dealing with\nunderrepresented NEs. Next, we study the impact of neural language model (LM)\nwith letter-based features derived to handle infrequent words. After that, we\nattempt to enrich representations of underrepresented NEs in pretrained neural\nLM by borrowing the embedding representations of rich-represented words. This\nlet us gain significant performance improvement on underrepresented NE\nrecognition. Finally, we boost the likelihood scores of utterances containing\nNEs in the word lattices rescored by neural LMs and gain further performance\nimprovement. The combination of the aforementioned approaches improves NE\nrecognition by up to 42% relatively.", "published": "2020-05-18 14:11:20", "link": "http://arxiv.org/abs/2005.08742v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Public discourse and sentiment during the COVID-19 pandemic: using\n  Latent Dirichlet Allocation for topic modeling on Twitter", "abstract": "The study aims to understand Twitter users' discourse and psychological\nreactions to COVID-19. We use machine learning techniques to analyze about 1.9\nmillion Tweets (written in English) related to coronavirus collected from\nJanuary 23 to March 7, 2020. A total of salient 11 topics are identified and\nthen categorized into ten themes, including \"updates about confirmed cases,\"\n\"COVID-19 related death,\" \"cases outside China (worldwide),\" \"COVID-19 outbreak\nin South Korea,\" \"early signs of the outbreak in New York,\" \"Diamond Princess\ncruise,\" \"economic impact,\" \"Preventive measures,\" \"authorities,\" and \"supply\nchain.\" Results do not reveal treatments and symptoms related messages as\nprevalent topics on Twitter. Sentiment analysis shows that fear for the unknown\nnature of the coronavirus is dominant in all topics. Implications and\nlimitations of the study are also discussed.", "published": "2020-05-18 15:50:38", "link": "http://arxiv.org/abs/2005.08817v3", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "A Thousand Words are Worth More Than One Recording: NLP Based Speaker\n  Change Point Detection", "abstract": "Speaker Diarization (SD) consists of splitting or segmenting an input audio\nburst according to speaker identities. In this paper, we focus on the crucial\ntask of the SD problem which is the audio segmenting process and suggest a\nsolution for the Change Point Detection (CPD) problem. We empirically\ndemonstrate the negative correlation between an increase in the number of\nspeakers and the Recall and F1-Score measurements. This negative correlation is\nshown to be the outcome of a massive experimental evaluation process, which\naccounts its superiority to recently developed voice based solutions. In order\nto overcome the number of speakers issue, we suggest a robust solution based on\na novel Natural Language Processing (NLP) technique, as well as a metadata\nfeatures extraction process, rather than a vocal based alone. To the best of\nour knowledge, we are the first to propose an intelligent NLP based solution\nthat (I) tackles the CPD problem with a dataset in Hebrew, and (II) solves the\nCPD variant of the SD problem. We empirically show, based on two distinct\ndatasets, that our method is abled to accurately identify the CPDs in an audio\nburst with 82.12% and 89.02% of success in the Recall and F1-score\nmeasurements.", "published": "2020-05-18 17:47:01", "link": "http://arxiv.org/abs/2006.01206v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Augmenting Generative Adversarial Networks for Speech Emotion\n  Recognition", "abstract": "Generative adversarial networks (GANs) have shown potential in learning\nemotional attributes and generating new data samples. However, their\nperformance is usually hindered by the unavailability of larger speech emotion\nrecognition (SER) data. In this work, we propose a framework that utilises the\nmixup data augmentation scheme to augment the GAN in feature learning and\ngeneration. To show the effectiveness of the proposed framework, we present\nresults for SER on (i) synthetic feature vectors, (ii) augmentation of the\ntraining data with synthetic features, (iii) encoded features in compressed\nrepresentation. Our results show that the proposed framework can effectively\nlearn compressed emotional representations as well as it can generate synthetic\nsamples that help improve performance in within-corpus and cross-corpus\nevaluation.", "published": "2020-05-18 04:10:12", "link": "http://arxiv.org/abs/2005.08447v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Architecture Enhancing Robustness to Noise, Adversarial Attacks,\n  and Cross-corpus Setting for Speech Emotion Recognition", "abstract": "Speech emotion recognition systems (SER) can achieve high accuracy when the\ntraining and test data are identically distributed, but this assumption is\nfrequently violated in practice and the performance of SER systems plummet\nagainst unforeseen data shifts. The design of robust models for accurate SER is\nchallenging, which limits its use in practical applications. In this paper we\npropose a deeper neural network architecture wherein we fuse DenseNet, LSTM and\nHighway Network to learn powerful discriminative features which are robust to\nnoise. We also propose data augmentation with our network architecture to\nfurther improve the robustness. We comprehensively evaluate the architecture\ncoupled with data augmentation against (1) noise, (2) adversarial attacks and\n(3) cross-corpus settings. Our evaluations on the widely used IEMOCAP and\nMSP-IMPROV datasets show promising results when compared with existing studies\nand state-of-the-art models.", "published": "2020-05-18 04:21:02", "link": "http://arxiv.org/abs/2005.08453v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Attentron: Few-Shot Text-to-Speech Utilizing Attention-Based\n  Variable-Length Embedding", "abstract": "On account of growing demands for personalization, the need for a so-called\nfew-shot TTS system that clones speakers with only a few data is emerging. To\naddress this issue, we propose Attentron, a few-shot TTS model that clones\nvoices of speakers unseen during training. It introduces two special encoders,\neach serving different purposes. A fine-grained encoder extracts\nvariable-length style information via an attention mechanism, and a\ncoarse-grained encoder greatly stabilizes the speech synthesis, circumventing\nunintelligible gibberish even for synthesizing speech of unseen speakers. In\naddition, the model can scale out to an arbitrary number of reference audios to\nimprove the quality of the synthesized speech. According to our experiments,\nincluding a human evaluation, the proposed model significantly outperforms\nstate-of-the-art models when generating speech for unseen speakers in terms of\nspeaker similarity and quality.", "published": "2020-05-18 06:55:38", "link": "http://arxiv.org/abs/2005.08484v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MoBoAligner: a Neural Alignment Model for Non-autoregressive TTS with\n  Monotonic Boundary Search", "abstract": "To speed up the inference of neural speech synthesis, non-autoregressive\nmodels receive increasing attention recently. In non-autoregressive models,\nadditional durations of text tokens are required to make a hard alignment\nbetween the encoder and the decoder. The duration-based alignment plays a\ncrucial role since it controls the correspondence between text tokens and\nspectrum frames and determines the rhythm and speed of synthesized audio. To\nget better duration-based alignment and improve the quality of\nnon-autoregressive speech synthesis, in this paper, we propose a novel neural\nalignment model named MoboAligner. Given the pairs of the text and mel\nspectrum, MoboAligner tries to identify the boundaries of text tokens in the\ngiven mel spectrum frames based on the token-frame similarity in the neural\nsemantic space with an end-to-end framework. With these boundaries, durations\ncan be extracted and used in the training of non-autoregressive TTS models.\nCompared with the duration extracted by TransformerTTS, MoboAligner brings\nimprovement for the non-autoregressive TTS model on MOS (3.74 comparing to\nFastSpeech's 3.44). Besides, MoboAligner is task-specified and lightweight,\nwhich reduces the parameter number by 45% and the training time consuming by\n30%.", "published": "2020-05-18 08:36:12", "link": "http://arxiv.org/abs/2005.08528v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Quasi-Periodic Parallel WaveGAN Vocoder: A Non-autoregressive\n  Pitch-dependent Dilated Convolution Model for Parametric Speech Generation", "abstract": "In this paper, we propose a parallel WaveGAN (PWG)-like neural vocoder with a\nquasi-periodic (QP) architecture to improve the pitch controllability of PWG.\nPWG is a compact non-autoregressive (non-AR) speech generation model, whose\ngenerative speed is much faster than real time. While utilizing PWG as a\nvocoder to generate speech on the basis of acoustic features such as spectral\nand prosodic features, PWG generates high-fidelity speech. However, when the\ninput acoustic features include unseen pitches, the pitch accuracy of\nPWG-generated speech degrades because of the fixed and generic network of PWG\nwithout prior knowledge of speech periodicity. The proposed QPPWG adopts a\npitch-dependent dilated convolution network (PDCNN) module, which introduces\nthe pitch information into PWG via the dynamically changed network\narchitecture, to improve the pitch controllability and speech modeling\ncapability of vanilla PWG. Both objective and subjective evaluation results\nshow the higher pitch accuracy and comparable speech quality of QPPWG-generated\nspeech when the QPPWG model size is only 70 % of that of vanilla PWG.", "published": "2020-05-18 12:43:38", "link": "http://arxiv.org/abs/2005.08654v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Cyclical Post-filtering Approach to Mismatch Refinement of Neural\n  Vocoder for Text-to-speech Systems", "abstract": "Recently, the effectiveness of text-to-speech (TTS) systems combined with\nneural vocoders to generate high-fidelity speech has been shown. However,\ncollecting the required training data and building these advanced systems from\nscratch are time and resource consuming. An economical approach is to develop a\nneural vocoder to enhance the speech generated by existing or low-cost TTS\nsystems. Nonetheless, this approach usually suffers from two issues: 1)\ntemporal mismatches between TTS and natural waveforms and 2) acoustic\nmismatches between training and testing data. To address these issues, we adopt\na cyclic voice conversion (VC) model to generate temporally matched pseudo-VC\ndata for training and acoustically matched enhanced data for testing the neural\nvocoders. Because of the generality, this framework can be applied to arbitrary\nTTS systems and neural vocoders. In this paper, we apply the proposed method\nwith a state-of-the-art WaveNet vocoder for two different basic TTS systems,\nand both objective and subjective experimental results confirm the\neffectiveness of the proposed framework.", "published": "2020-05-18 12:48:40", "link": "http://arxiv.org/abs/2005.08659v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Mask CTC: Non-Autoregressive End-to-End ASR with CTC and Mask Predict", "abstract": "We present Mask CTC, a novel non-autoregressive end-to-end automatic speech\nrecognition (ASR) framework, which generates a sequence by refining outputs of\nthe connectionist temporal classification (CTC). Neural sequence-to-sequence\nmodels are usually \\textit{autoregressive}: each output token is generated by\nconditioning on previously generated tokens, at the cost of requiring as many\niterations as the output length. On the other hand, non-autoregressive models\ncan simultaneously generate tokens within a constant number of iterations,\nwhich results in significant inference time reduction and better suits\nend-to-end ASR model for real-world scenarios. In this work, Mask CTC model is\ntrained using a Transformer encoder-decoder with joint training of mask\nprediction and CTC. During inference, the target sequence is initialized with\nthe greedy CTC outputs and low-confidence tokens are masked based on the CTC\nprobabilities. Based on the conditional dependence between output tokens, these\nmasked low-confidence tokens are then predicted conditioning on the\nhigh-confidence tokens. Experimental results on different speech recognition\ntasks show that Mask CTC outperforms the standard CTC model (e.g., 17.9% ->\n12.1% WER on WSJ) and approaches the autoregressive model, requiring much less\ninference time using CPUs (0.07 RTF in Python implementation). All of our codes\nwill be publicly available.", "published": "2020-05-18 13:30:16", "link": "http://arxiv.org/abs/2005.08700v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Metric Learning for Keyword Spotting", "abstract": "The goal of this work is to train effective representations for keyword\nspotting via metric learning. Most existing works address keyword spotting as a\nclosed-set classification problem, where both target and non-target keywords\nare predefined. Therefore, prevailing classifier-based keyword spotting systems\nperform poorly on non-target sounds which are unseen during the training stage,\ncausing high false alarm rates in real-world scenarios. In reality, keyword\nspotting is a detection problem where predefined target keywords are detected\nfrom a variety of unknown sounds. This shares many similarities to metric\nlearning problems in that the unseen and unknown non-target sounds must be\nclearly differentiated from the target keywords. However, a key difference is\nthat the target keywords are known and predefined. To this end, we propose a\nnew method based on metric learning that maximises the distance between target\nand non-target keywords, but also learns per-class weights for target keywords\n\\`a la classification objectives. Experiments on the Google Speech Commands\ndataset show that our method significantly reduces false alarms to unseen\nnon-target keywords, while maintaining the overall classification accuracy.", "published": "2020-05-18 14:47:04", "link": "http://arxiv.org/abs/2005.08776v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Many-to-Many Voice Transformer Network", "abstract": "This paper proposes a voice conversion (VC) method based on a\nsequence-to-sequence (S2S) learning framework, which enables simultaneous\nconversion of the voice characteristics, pitch contour, and duration of input\nspeech. We previously proposed an S2S-based VC method using a transformer\nnetwork architecture called the voice transformer network (VTN). The original\nVTN was designed to learn only a mapping of speech feature sequences from one\nspeaker to another. The main idea we propose is an extension of the original\nVTN that can simultaneously learn mappings among multiple speakers. This\nextension called the many-to-many VTN makes it able to fully use available\ntraining data collected from multiple speakers by capturing common latent\nfeatures that can be shared across different speakers. It also allows us to\nintroduce a training loss called the identity mapping loss to ensure that the\ninput feature sequence will remain unchanged when the source and target speaker\nindices are the same. Using this particular loss for model training has been\nfound to be extremely effective in improving the performance of the model at\ntest time. We conducted speaker identity conversion experiments and found that\nour model obtained higher sound quality and speaker similarity than baseline\nmethods. We also found that our model, with a slight modification to its\narchitecture, could handle any-to-many conversion tasks reasonably well.", "published": "2020-05-18 04:02:08", "link": "http://arxiv.org/abs/2005.08445v4", "categories": ["eess.AS", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Unconditional Audio Generation with Generative Adversarial Networks and\n  Cycle Regularization", "abstract": "In a recent paper, we have presented a generative adversarial network\n(GAN)-based model for unconditional generation of the mel-spectrograms of\nsinging voices. As the generator of the model is designed to take a\nvariable-length sequence of noise vectors as input, it can generate\nmel-spectrograms of variable length. However, our previous listening test shows\nthat the quality of the generated audio leaves room for improvement. The\npresent paper extends and expands that previous work in the following aspects.\nFirst, we employ a hierarchical architecture in the generator to induce some\nstructure in the temporal dimension. Second, we introduce a cycle\nregularization mechanism to the generator to avoid mode collapse. Third, we\nevaluate the performance of the new model not only for generating singing\nvoices, but also for generating speech voices. Evaluation result shows that new\nmodel outperforms the prior one both objectively and subjectively. We also\nemploy the model to unconditionally generate sequences of piano and violin\nmusic and find the result promising. Audio examples, as well as the code for\nimplementing our model, will be publicly available online upon paper\npublication.", "published": "2020-05-18 08:35:16", "link": "http://arxiv.org/abs/2005.08526v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Quaternion Neural Networks for Multi-channel Distant Speech Recognition", "abstract": "Despite the significant progress in automatic speech recognition (ASR),\ndistant ASR remains challenging due to noise and reverberation. A common\napproach to mitigate this issue consists of equipping the recording devices\nwith multiple microphones that capture the acoustic scene from different\nperspectives. These multi-channel audio recordings contain specific internal\nrelations between each signal. In this paper, we propose to capture these\ninter- and intra- structural dependencies with quaternion neural networks,\nwhich can jointly process multiple signals as whole quaternion entities. The\nquaternion algebra replaces the standard dot product with the Hamilton one,\nthus offering a simple and elegant way to model dependencies between elements.\nThe quaternion layers are then coupled with a recurrent neural network, which\ncan learn long-term dependencies in the time domain. We show that a quaternion\nlong-short term memory neural network (QLSTM), trained on the concatenated\nmulti-channel speech signals, outperforms equivalent real-valued LSTM on two\ndifferent tasks of multi-channel distant speech recognition.", "published": "2020-05-18 10:26:27", "link": "http://arxiv.org/abs/2005.08566v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Acoustic Integrity Codes: Secure Device Pairing Using Short-Range\n  Acoustic Communication", "abstract": "Secure Device Pairing (SDP) relies on an out-of-band channel to authenticate\ndevices. This requires a common hardware interface, which limits the use of\nexisting SDP systems. We propose to use short-range acoustic communication for\nthe initial pairing. Audio hardware is commonly available on existing\noff-the-shelf devices and can be accessed from user space without requiring\nfirmware or hardware modifications. We improve upon previous approaches by\ndesigning Acoustic Integrity Codes (AICs): a modulation scheme that provides\nmessage authentication on the acoustic physical layer. We analyze their\nsecurity and demonstrate that we can defend against signal cancellation attacks\nby designing signals with low autocorrelation. Our system can detect\novershadowing attacks using a ternary decision function with a threshold. In\nour evaluation of this SDP scheme's security and robustness, we achieve a bit\nerror ratio below 0.1% for a net bit rate of 100 bps with a signal-to-noise\nratio (SNR) of 14 dB. Using our open-source proof-of-concept implementation on\nAndroid smartphones, we demonstrate pairing between different smartphone\nmodels.", "published": "2020-05-18 10:33:26", "link": "http://arxiv.org/abs/2005.08572v2", "categories": ["cs.CR", "cs.NI", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "An Overview on Audio, Signal, Speech, & Language Processing for COVID-19", "abstract": "Recently, there has been an increased attention towards innovating,\nenhancing, building, and deploying applications of speech signal processing for\nproviding assistance and relief to human mankind from the Coronavirus\n(COVID-19) pandemic. Many AI with speech initiatives are taken to combat with\nthe present situation and also to create a safe and secure environment for the\nfuture. This paper summarises all these efforts taken by the re-search\ncommunity towards helping the individuals and the society in the fight against\nCOVID-19 over the past 3-4 months using speech signal processing. We also\nsummarise the deep techniques used in this direction to come up with capable\nsolutions in a short span of time. This paper further gives an overview of the\ncontributions from non-speech modalities that may complement or serve as\ninspiration for audio and speech analysis. In addition, we discuss our\nobservations with respect to solution usability, challenges, and the\nsignificant technology achievements.", "published": "2020-05-18 10:46:30", "link": "http://arxiv.org/abs/2005.08579v1", "categories": ["cs.CY", "cs.SD", "eess.AS"], "primary_category": "cs.CY"}
{"title": "End-to-End Lip Synchronisation Based on Pattern Classification", "abstract": "The goal of this work is to synchronise audio and video of a talking face\nusing deep neural network models. Existing works have trained networks on proxy\ntasks such as cross-modal similarity learning, and then computed similarities\nbetween audio and video frames using a sliding window approach. While these\nmethods demonstrate satisfactory performance, the networks are not trained\ndirectly on the task. To this end, we propose an end-to-end trained network\nthat can directly predict the offset between an audio stream and the\ncorresponding video stream. The similarity matrix between the two modalities is\nfirst computed from the features, then the inference of the offset can be\nconsidered to be a pattern recognition problem where the matrix is considered\nequivalent to an image. The feature extractor and the classifier are trained\njointly. We demonstrate that the proposed approach outperforms the previous\nwork by a large margin on LRS2 and LRS3 datasets.", "published": "2020-05-18 11:42:32", "link": "http://arxiv.org/abs/2005.08606v2", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Defending Your Voice: Adversarial Attack on Voice Conversion", "abstract": "Substantial improvements have been achieved in recent years in voice\nconversion, which converts the speaker characteristics of an utterance into\nthose of another speaker without changing the linguistic content of the\nutterance. Nonetheless, the improved conversion technologies also led to\nconcerns about privacy and authentication. It thus becomes highly desired to be\nable to prevent one's voice from being improperly utilized with such voice\nconversion technologies. This is why we report in this paper the first known\nattempt to perform adversarial attack on voice conversion. We introduce human\nimperceptible noise into the utterances of a speaker whose voice is to be\ndefended. Given these adversarial examples, voice conversion models cannot\nconvert other utterances so as to sound like being produced by the defended\nspeaker. Preliminary experiments were conducted on two currently\nstate-of-the-art zero-shot voice conversion models. Objective and subjective\nevaluation results in both white-box and black-box scenarios are reported. It\nwas shown that the speaker characteristics of the converted utterances were\nmade obviously different from those of the defended speaker, while the\nadversarial examples of the defended speaker are not distinguishable from the\nauthentic utterances.", "published": "2020-05-18 14:51:54", "link": "http://arxiv.org/abs/2005.08781v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Surfboard: Audio Feature Extraction for Modern Machine Learning", "abstract": "We introduce Surfboard, an open-source Python library for extracting audio\nfeatures with application to the medical domain. Surfboard is written with the\naim of addressing pain points of existing libraries and facilitating joint use\nwith modern machine learning frameworks. The package can be accessed both\nprogrammatically in Python and via its command line interface, allowing it to\nbe easily integrated within machine learning workflows. It builds on\nstate-of-the-art audio analysis packages and offers multiprocessing support for\nprocessing large workloads. We review similar frameworks and describe\nSurfboard's architecture, including the clinical motivation for its features.\nUsing the mPower dataset, we illustrate Surfboard's application to a\nParkinson's disease classification task, highlighting common pitfalls in\nexisting research. The source code is opened up to the research community to\nfacilitate future audio research in the clinical domain.", "published": "2020-05-18 16:20:20", "link": "http://arxiv.org/abs/2005.08848v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning Deep Models from Synthetic Data for Extracting Dolphin Whistle\n  Contours", "abstract": "We present a learning-based method for extracting whistles of toothed whales\n(Odontoceti) in hydrophone recordings. Our method represents audio signals as\ntime-frequency spectrograms and decomposes each spectrogram into a set of\ntime-frequency patches. A deep neural network learns archetypical patterns\n(e.g., crossings, frequency modulated sweeps) from the spectrogram patches and\npredicts time-frequency peaks that are associated with whistles. We also\ndeveloped a comprehensive method to synthesize training samples from background\nenvironments and train the network with minimal human annotation effort. We\napplied the proposed learn-from-synthesis method to a subset of the public\nDetection, Classification, Localization, and Density Estimation (DCLDE) 2011\nworkshop data to extract whistle confidence maps, which we then processed with\nan existing contour extractor to produce whistle annotations. The F1-score of\nour best synthesis method was 0.158 greater than our baseline whistle\nextraction algorithm (~25% improvement) when applied to common dolphin\n(Delphinus spp.) and bottlenose dolphin (Tursiops truncatus) whistles.", "published": "2020-05-18 17:09:34", "link": "http://arxiv.org/abs/2005.08894v1", "categories": ["q-bio.QM", "cs.SD", "eess.AS"], "primary_category": "q-bio.QM"}
{"title": "Learning to rank music tracks using triplet loss", "abstract": "Most music streaming services rely on automatic recommendation algorithms to\nexploit their large music catalogs. These algorithms aim at retrieving a ranked\nlist of music tracks based on their similarity with a target music track. In\nthis work, we propose a method for direct recommendation based on the audio\ncontent without explicitly tagging the music tracks. To that aim, we propose\nseveral strategies to perform triplet mining from ranked lists. We train a\nConvolutional Neural Network to learn the similarity via triplet loss. These\ndifferent strategies are compared and validated on a large-scale experiment\nagainst an auto-tagging based approach. The results obtained highlight the\nefficiency of our system, especially when associated with an Auto-pooling\nlayer.", "published": "2020-05-18 08:20:54", "link": "http://arxiv.org/abs/2005.12977v1", "categories": ["cs.IR", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
