{"title": "Bioinformatics and Classical Literary Study", "abstract": "This paper describes the Quantitative Criticism Lab, a collaborative\ninitiative between classicists, quantitative biologists, and computer\nscientists to apply ideas and methods drawn from the sciences to the study of\nliterature. A core goal of the project is the use of computational biology,\nnatural language processing, and machine learning techniques to investigate\nauthorial style, intertextuality, and related phenomena of literary\nsignificance. As a case study in our approach, here we review the use of\nsequence alignment, a common technique in genomics and computational\nlinguistics, to detect intertextuality in Latin literature. Sequence alignment\nis distinguished by its ability to find inexact verbal similarities, which\nmakes it ideal for identifying phonetic echoes in large corpora of Latin texts.\nAlthough especially suited to Latin, sequence alignment in principle can be\nextended to many other languages.", "published": "2016-02-29 07:20:59", "link": "http://arxiv.org/abs/1602.08844v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Representation of linguistic form and function in recurrent neural\n  networks", "abstract": "We present novel methods for analyzing the activation patterns of RNNs from a\nlinguistic point of view and explore the types of linguistic structure they\nlearn. As a case study, we use a multi-task gated recurrent network\narchitecture consisting of two parallel pathways with shared word embeddings\ntrained on predicting the representations of the visual scene corresponding to\nan input sentence, and predicting the next word in the same sentence. Based on\nour proposed method to estimate the amount of contribution of individual tokens\nin the input to the final prediction of the networks we show that the image\nprediction pathway: a) is sensitive to the information structure of the\nsentence b) pays selective attention to lexical categories and grammatical\nfunctions that carry semantic information c) learns to treat the same input\ntoken differently depending on its grammatical functions in the sentence. In\ncontrast the language model is comparatively more sensitive to words with a\nsyntactic function. Furthermore, we propose methods to ex- plore the function\nof individual hidden units in RNNs and show that the two pathways of the\narchitecture in our case study contain specialized units tuned to patterns\ninformative for the task, some of which can carry activations to later time\nsteps to encode long-term dependencies.", "published": "2016-02-29 13:31:17", "link": "http://arxiv.org/abs/1602.08952v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
