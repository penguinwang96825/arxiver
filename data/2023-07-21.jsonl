{"title": "Generator-Retriever-Generator Approach for Open-Domain Question\n  Answering", "abstract": "Open-domain question answering (QA) tasks usually require the retrieval of\nrelevant information from a large corpus to generate accurate answers. We\npropose a novel approach called Generator-Retriever-Generator (GRG) that\ncombines document retrieval techniques with a large language model (LLM), by\nfirst prompting the model to generate contextual documents based on a given\nquestion. In parallel, a dual-encoder network retrieves documents that are\nrelevant to the question from an external corpus. The generated and retrieved\ndocuments are then passed to the second LLM, which generates the final answer.\nBy combining document retrieval and LLM generation, our approach addresses the\nchallenges of open-domain QA, such as generating informative and contextually\nrelevant answers. GRG outperforms the state-of-the-art generate-then-read and\nretrieve-then-read pipelines (GENREAD and RFiD) improving their performance by\nat least by +5.2, +4.2, and +1.6 on TriviaQA, NQ, and WebQ datasets,\nrespectively. We provide code, datasets, and checkpoints at\nhttps://github.com/abdoelsayed2016/GRG.", "published": "2023-07-21 00:34:38", "link": "http://arxiv.org/abs/2307.11278v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is ChatGPT Involved in Texts? Measure the Polish Ratio to Detect\n  ChatGPT-Generated Text", "abstract": "The remarkable capabilities of large-scale language models, such as ChatGPT,\nin text generation have impressed readers and spurred researchers to devise\ndetectors to mitigate potential risks, including misinformation, phishing, and\nacademic dishonesty. Despite this, most previous studies have been\npredominantly geared towards creating detectors that differentiate between\npurely ChatGPT-generated texts and human-authored texts. This approach,\nhowever, fails to work on discerning texts generated through human-machine\ncollaboration, such as ChatGPT-polished texts. Addressing this gap, we\nintroduce a novel dataset termed HPPT (ChatGPT-polished academic abstracts),\nfacilitating the construction of more robust detectors. It diverges from extant\ncorpora by comprising pairs of human-written and ChatGPT-polished abstracts\ninstead of purely ChatGPT-generated texts. Additionally, we propose the \"Polish\nRatio\" method, an innovative measure of the degree of modification made by\nChatGPT compared to the original human-written text. It provides a mechanism to\nmeasure the degree of ChatGPT influence in the resulting text. Our experimental\nresults show our proposed model has better robustness on the HPPT dataset and\ntwo existing datasets (HC3 and CDB). Furthermore, the \"Polish Ratio\" we\nproposed offers a more comprehensive explanation by quantifying the degree of\nChatGPT involvement.", "published": "2023-07-21 06:38:37", "link": "http://arxiv.org/abs/2307.11380v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OUTFOX: LLM-Generated Essay Detection Through In-Context Learning with\n  Adversarially Generated Examples", "abstract": "Large Language Models (LLMs) have achieved human-level fluency in text\ngeneration, making it difficult to distinguish between human-written and\nLLM-generated texts. This poses a growing risk of misuse of LLMs and demands\nthe development of detectors to identify LLM-generated texts. However, existing\ndetectors lack robustness against attacks: they degrade detection accuracy by\nsimply paraphrasing LLM-generated texts. Furthermore, a malicious user might\nattempt to deliberately evade the detectors based on detection results, but\nthis has not been assumed in previous studies. In this paper, we propose\nOUTFOX, a framework that improves the robustness of LLM-generated-text\ndetectors by allowing both the detector and the attacker to consider each\nother's output. In this framework, the attacker uses the detector's prediction\nlabels as examples for in-context learning and adversarially generates essays\nthat are harder to detect, while the detector uses the adversarially generated\nessays as examples for in-context learning to learn to detect essays from a\nstrong attacker. Experiments in the domain of student essays show that the\nproposed detector improves the detection performance on the attacker-generated\ntexts by up to +41.3 points F1-score. Furthermore, the proposed detector shows\na state-of-the-art detection performance: up to 96.9 points F1-score, beating\nexisting detectors on non-attacked texts. Finally, the proposed attacker\ndrastically degrades the performance of detectors by up to -57.0 points\nF1-score, massively outperforming the baseline paraphrasing method for evading\ndetection.", "published": "2023-07-21 17:40:47", "link": "http://arxiv.org/abs/2307.11729v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GPT-4 Can't Reason", "abstract": "GPT-4 was released in March 2023 to wide acclaim, marking a very substantial\nimprovement across the board over GPT-3.5 (OpenAI's previously best model,\nwhich had powered the initial release of ChatGPT). However, despite the\ngenuinely impressive improvement, there are good reasons to be highly skeptical\nof GPT-4's ability to reason. This position paper discusses the nature of\nreasoning; criticizes the current formulation of reasoning problems in the NLP\ncommunity, as well as the way in which LLM reasoning performance is currently\nevaluated; introduces a small collection of 21 diverse reasoning problems; and\nperforms a detailed qualitative evaluation of GPT-4's performance on those\nproblems. Based on this analysis, the paper concludes that, despite its\noccasional flashes of analytical brilliance, GPT-4 at present is utterly\nincapable of reasoning.", "published": "2023-07-21 17:04:25", "link": "http://arxiv.org/abs/2308.03762v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GIST: Generating Image-Specific Text for Fine-grained Object\n  Classification", "abstract": "Recent vision-language models outperform vision-only models on many image\nclassification tasks. However, because of the absence of paired text/image\ndescriptions, it remains difficult to fine-tune these models for fine-grained\nimage classification. In this work, we propose a method, GIST, for generating\nimage-specific fine-grained text descriptions from image-only datasets, and\nshow that these text descriptions can be used to improve classification. Key\nparts of our method include 1. prompting a pretrained large language model with\ndomain-specific prompts to generate diverse fine-grained text descriptions for\neach class and 2. using a pretrained vision-language model to match each image\nto label-preserving text descriptions that capture relevant visual features in\nthe image. We demonstrate the utility of GIST by fine-tuning vision-language\nmodels on the image-and-generated-text pairs to learn an aligned\nvision-language representation space for improved classification. We evaluate\nour learned representation space in full-shot and few-shot scenarios across\nfour diverse fine-grained classification datasets, each from a different\ndomain. Our method achieves an average improvement of $4.1\\%$ in accuracy over\nCLIP linear probes and an average of $1.1\\%$ improvement in accuracy over the\nprevious state-of-the-art image-text classification method on the full-shot\ndatasets. Our method achieves similar improvements across few-shot regimes.\nCode is available at https://github.com/emu1729/GIST.", "published": "2023-07-21 02:47:18", "link": "http://arxiv.org/abs/2307.11315v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Making Pre-trained Language Models both Task-solvers and\n  Self-calibrators", "abstract": "Pre-trained language models (PLMs) serve as backbones for various real-world\nsystems. For high-stake applications, it's equally essential to have reasonable\nconfidence estimations in predictions. While the vanilla confidence scores of\nPLMs can already be effectively utilized, PLMs consistently become\noverconfident in their wrong predictions, which is not desirable in practice.\nPrevious work shows that introducing an extra calibration task can mitigate\nthis issue. The basic idea involves acquiring additional data to train models\nin predicting the confidence of their initial predictions. However, it only\ndemonstrates the feasibility of this kind of method, assuming that there are\nabundant extra available samples for the introduced calibration task. In this\nwork, we consider the practical scenario that we need to effectively utilize\ntraining samples to make PLMs both task-solvers and self-calibrators. Three\nchallenges are presented, including limited training samples, data imbalance,\nand distribution shifts. We first conduct pilot experiments to quantify various\ndecisive factors in the calibration task. Based on the empirical analysis\nresults, we propose a training algorithm LM-TOAST to tackle the challenges.\nExperimental results show that LM-TOAST can effectively utilize the training\ndata to make PLMs have reasonable confidence estimations while maintaining the\noriginal task performance. Further, we consider three downstream applications,\nnamely selective classification, adversarial defense, and model cascading, to\nshow the practical usefulness of LM-TOAST. The code will be made public at\n\\url{https://github.com/Yangyi-Chen/LM-TOAST}.", "published": "2023-07-21 02:51:41", "link": "http://arxiv.org/abs/2307.11316v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DEFTri: A Few-Shot Label Fused Contextual Representation Learning For\n  Product Defect Triage in e-Commerce", "abstract": "Defect Triage is a time-sensitive and critical process in a large-scale agile\nsoftware development lifecycle for e-commerce. Inefficiencies arising from\nhuman and process dependencies in this domain have motivated research in\nautomated approaches using machine learning to accurately assign defects to\nqualified teams. This work proposes a novel framework for automated defect\ntriage (DEFTri) using fine-tuned state-of-the-art pre-trained BERT on labels\nfused text embeddings to improve contextual representations from\nhuman-generated product defects. For our multi-label text classification defect\ntriage task, we also introduce a Walmart proprietary dataset of product defects\nusing weak supervision and adversarial learning, in a few-shot setting.", "published": "2023-07-21 04:22:43", "link": "http://arxiv.org/abs/2307.11344v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "CohortGPT: An Enhanced GPT for Participant Recruitment in Clinical Study", "abstract": "Participant recruitment based on unstructured medical texts such as clinical\nnotes and radiology reports has been a challenging yet important task for the\ncohort establishment in clinical research. Recently, Large Language Models\n(LLMs) such as ChatGPT have achieved tremendous success in various downstream\ntasks thanks to their promising performance in language understanding,\ninference, and generation. It is then natural to test their feasibility in\nsolving the cohort recruitment task, which involves the classification of a\ngiven paragraph of medical text into disease label(s). However, when applied to\nknowledge-intensive problem settings such as medical text classification, where\nthe LLMs are expected to understand the decision made by human experts and\naccurately identify the implied disease labels, the LLMs show a mediocre\nperformance. A possible explanation is that, by only using the medical text,\nthe LLMs neglect to use the rich context of additional information that\nlanguages afford. To this end, we propose to use a knowledge graph as auxiliary\ninformation to guide the LLMs in making predictions. Moreover, to further boost\nthe LLMs adapt to the problem setting, we apply a chain-of-thought (CoT) sample\nselection strategy enhanced by reinforcement learning, which selects a set of\nCoT samples given each individual medical report. Experimental results and\nvarious ablation studies show that our few-shot learning method achieves\nsatisfactory performance compared with fine-tuning strategies and gains superb\nadvantages when the available data is limited. The code and sample dataset of\nthe proposed CohortGPT model is available at:\nhttps://anonymous.4open.science/r/CohortGPT-4872/", "published": "2023-07-21 04:43:00", "link": "http://arxiv.org/abs/2307.11346v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MeetEval: A Toolkit for Computation of Word Error Rates for Meeting\n  Transcription Systems", "abstract": "MeetEval is an open-source toolkit to evaluate all kinds of meeting\ntranscription systems. It provides a unified interface for the computation of\ncommonly used Word Error Rates (WERs), specifically cpWER, ORC-WER and MIMO-WER\nalong other WER definitions. We extend the cpWER computation by a temporal\nconstraint to ensure that only words are identified as correct when the\ntemporal alignment is plausible. This leads to a better quality of the matching\nof the hypothesis string to the reference string that more closely resembles\nthe actual transcription quality, and a system is penalized if it provides poor\ntime annotations. Since word-level timing information is often not available,\nwe present a way to approximate exact word-level timings from segment-level\ntimings (e.g., a sentence) and show that the approximation leads to a similar\nWER as a matching with exact word-level annotations. At the same time, the time\nconstraint leads to a speedup of the matching algorithm, which outweighs the\nadditional overhead caused by processing the time stamps.", "published": "2023-07-21 07:22:18", "link": "http://arxiv.org/abs/2307.11394v3", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Topic Identification For Spontaneous Speech: Enriching Audio Features\n  With Embedded Linguistic Information", "abstract": "Traditional topic identification solutions from audio rely on an automatic\nspeech recognition system (ASR) to produce transcripts used as input to a\ntext-based model. These approaches work well in high-resource scenarios, where\nthere are sufficient data to train both components of the pipeline. However, in\nlow-resource situations, the ASR system, even if available, produces\nlow-quality transcripts, leading to a bad text-based classifier. Moreover,\nspontaneous speech containing hesitations can further degrade the performance\nof the ASR model. In this paper, we investigate alternatives to the standard\ntext-only solutions by comparing audio-only and hybrid techniques of jointly\nutilising text and audio features. The models evaluated on spontaneous Finnish\nspeech demonstrate that purely audio-based solutions are a viable option when\nASR components are not available, while the hybrid multi-modal solutions\nachieve the best results.", "published": "2023-07-21 09:30:46", "link": "http://arxiv.org/abs/2307.11450v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Incorporating Human Translator Style into English-Turkish Literary\n  Machine Translation", "abstract": "Although machine translation systems are mostly designed to serve in the\ngeneral domain, there is a growing tendency to adapt these systems to other\ndomains like literary translation. In this paper, we focus on English-Turkish\nliterary translation and develop machine translation models that take into\naccount the stylistic features of translators. We fine-tune a pre-trained\nmachine translation model by the manually-aligned works of a particular\ntranslator. We make a detailed analysis of the effects of manual and automatic\nalignments, data augmentation methods, and corpus size on the translations. We\npropose an approach based on stylistic features to evaluate the style of a\ntranslator in the output translations. We show that the human translator style\ncan be highly recreated in the target machine translations by adapting the\nmodels to the style of the translator.", "published": "2023-07-21 09:39:50", "link": "http://arxiv.org/abs/2307.11457v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IndigoVX: Where Human Intelligence Meets AI for Optimal Decision Making", "abstract": "This paper defines a new approach for augmenting human intelligence with AI\nfor optimal goal solving. Our proposed AI, Indigo, is an acronym for Informed\nNumerical Decision-making through Iterative Goal-Oriented optimization. When\ncombined with a human collaborator, we term the joint system IndigoVX, for\nVirtual eXpert. The system is conceptually simple. We envisage this method\nbeing applied to games or business strategies, with the human providing\nstrategic context and the AI offering optimal, data-driven moves. Indigo\noperates through an iterative feedback loop, harnessing the human expert's\ncontextual knowledge and the AI's data-driven insights to craft and refine\nstrategies towards a well-defined goal. Using a quantified three-score schema,\nthis hybridization allows the combined team to evaluate strategies and refine\ntheir plan, while adapting to challenges and changes in real-time.", "published": "2023-07-21 11:54:53", "link": "http://arxiv.org/abs/2307.11516v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Bridging Vision and Language Encoders: Parameter-Efficient Tuning for\n  Referring Image Segmentation", "abstract": "Parameter Efficient Tuning (PET) has gained attention for reducing the number\nof parameters while maintaining performance and providing better hardware\nresource savings, but few studies investigate dense prediction tasks and\ninteraction between modalities. In this paper, we do an investigation of\nefficient tuning problems on referring image segmentation. We propose a novel\nadapter called Bridger to facilitate cross-modal information exchange and\ninject task-specific information into the pre-trained model. We also design a\nlightweight decoder for image segmentation. Our approach achieves comparable or\nsuperior performance with only 1.61\\% to 3.38\\% backbone parameter updates,\nevaluated on challenging benchmarks. The code is available at\n\\url{https://github.com/kkakkkka/ETRIS}.", "published": "2023-07-21 12:46:15", "link": "http://arxiv.org/abs/2307.11545v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Advancing Visual Grounding with Scene Knowledge: Benchmark and Method", "abstract": "Visual grounding (VG) aims to establish fine-grained alignment between vision\nand language. Ideally, it can be a testbed for vision-and-language models to\nevaluate their understanding of the images and texts and their reasoning\nabilities over their joint space. However, most existing VG datasets are\nconstructed using simple description texts, which do not require sufficient\nreasoning over the images and texts. This has been demonstrated in a recent\nstudy~\\cite{luo2022goes}, where a simple LSTM-based text encoder without\npretraining can achieve state-of-the-art performance on mainstream VG datasets.\nTherefore, in this paper, we propose a novel benchmark of \\underline{S}cene\n\\underline{K}nowledge-guided \\underline{V}isual \\underline{G}rounding (SK-VG),\nwhere the image content and referring expressions are not sufficient to ground\nthe target objects, forcing the models to have a reasoning ability on the\nlong-form scene knowledge. To perform this task, we propose two approaches to\naccept the triple-type input, where the former embeds knowledge into the image\nfeatures before the image-query interaction; the latter leverages linguistic\nstructure to assist in computing the image-text matching. We conduct extensive\nexperiments to analyze the above methods and show that the proposed approaches\nachieve promising results but still leave room for improvement, including\nperformance and interpretability. The dataset and code are available at\n\\url{https://github.com/zhjohnchan/SK-VG}.", "published": "2023-07-21 13:06:02", "link": "http://arxiv.org/abs/2307.11558v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CausE: Towards Causal Knowledge Graph Embedding", "abstract": "Knowledge graph embedding (KGE) focuses on representing the entities and\nrelations of a knowledge graph (KG) into the continuous vector spaces, which\ncan be employed to predict the missing triples to achieve knowledge graph\ncompletion (KGC). However, KGE models often only briefly learn structural\ncorrelations of triple data and embeddings would be misled by the trivial\npatterns and noisy links in real-world KGs. To address this issue, we build the\nnew paradigm of KGE in the context of causality and embedding disentanglement.\nWe further propose a Causality-enhanced knowledge graph Embedding (CausE)\nframework. CausE employs causal intervention to estimate the causal effect of\nthe confounder embeddings and design new training objectives to make stable\npredictions. Experimental results demonstrate that CausE could outperform the\nbaseline models and achieve state-of-the-art KGC performance. We release our\ncode in https://github.com/zjukg/CausE.", "published": "2023-07-21 14:25:39", "link": "http://arxiv.org/abs/2307.11610v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?", "abstract": "This paper presents OxfordTVG-HIC (Humorous Image Captions), a large-scale\ndataset for humour generation and understanding. Humour is an abstract,\nsubjective, and context-dependent cognitive construct involving several\ncognitive factors, making it a challenging task to generate and interpret.\nHence, humour generation and understanding can serve as a new task for\nevaluating the ability of deep-learning methods to process abstract and\nsubjective information. Due to the scarcity of data, humour-related generation\ntasks such as captioning remain under-explored. To address this gap,\nOxfordTVG-HIC offers approximately 2.9M image-text pairs with humour scores to\ntrain a generalizable humour captioning model. Contrary to existing captioning\ndatasets, OxfordTVG-HIC features a wide range of emotional and semantic\ndiversity resulting in out-of-context examples that are particularly conducive\nto generating humour. Moreover, OxfordTVG-HIC is curated devoid of offensive\ncontent. We also show how OxfordTVG-HIC can be leveraged for evaluating the\nhumour of a generated text. Through explainability analysis of the trained\nmodels, we identify the visual and linguistic cues influential for evoking\nhumour prediction (and generation). We observe qualitatively that these cues\nare aligned with the benign violation theory of humour in cognitive psychology.", "published": "2023-07-21 14:58:44", "link": "http://arxiv.org/abs/2307.11636v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MythQA: Query-Based Large-Scale Check-Worthy Claim Detection through\n  Multi-Answer Open-Domain Question Answering", "abstract": "Check-worthy claim detection aims at providing plausible misinformation to\ndownstream fact-checking systems or human experts to check. This is a crucial\nstep toward accelerating the fact-checking process. Many efforts have been put\ninto how to identify check-worthy claims from a small scale of pre-collected\nclaims, but how to efficiently detect check-worthy claims directly from a\nlarge-scale information source, such as Twitter, remains underexplored. To fill\nthis gap, we introduce MythQA, a new multi-answer open-domain question\nanswering(QA) task that involves contradictory stance mining for query-based\nlarge-scale check-worthy claim detection. The idea behind this is that\ncontradictory claims are a strong indicator of misinformation that merits\nscrutiny by the appropriate authorities. To study this task, we construct\nTweetMythQA, an evaluation dataset containing 522 factoid multi-answer\nquestions based on controversial topics. Each question is annotated with\nmultiple answers. Moreover, we collect relevant tweets for each distinct\nanswer, then classify them into three categories: \"Supporting\", \"Refuting\", and\n\"Neutral\". In total, we annotated 5.3K tweets. Contradictory evidence is\ncollected for all answers in the dataset. Finally, we present a baseline system\nfor MythQA and evaluate existing NLP models for each system component using the\nTweetMythQA dataset. We provide initial benchmarks and identify key challenges\nfor future models to improve upon. Code and data are available at:\nhttps://github.com/TonyBY/Myth-QA", "published": "2023-07-21 18:35:24", "link": "http://arxiv.org/abs/2307.11848v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CARTIER: Cartographic lAnguage Reasoning Targeted at Instruction\n  Execution for Robots", "abstract": "This work explores the capacity of large language models (LLMs) to address\nproblems at the intersection of spatial planning and natural language\ninterfaces for navigation. We focus on following complex instructions that are\nmore akin to natural conversation than traditional explicit procedural\ndirectives typically seen in robotics. Unlike most prior work where navigation\ndirectives are provided as simple imperative commands (e.g., \"go to the\nfridge\"), we examine implicit directives obtained through conversational\ninteractions.We leverage the 3D simulator AI2Thor to create household query\nscenarios at scale, and augment it by adding complex language queries for 40\nobject types. We demonstrate that a robot using our method CARTIER\n(Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots)\ncan parse descriptive language queries up to 42% more reliably than existing\nLLM-enabled methods by exploiting the ability of LLMs to interpret the user\ninteraction in the context of the objects in the scenario.", "published": "2023-07-21 19:09:37", "link": "http://arxiv.org/abs/2307.11865v3", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "A Change of Heart: Improving Speech Emotion Recognition through\n  Speech-to-Text Modality Conversion", "abstract": "Speech Emotion Recognition (SER) is a challenging task. In this paper, we\nintroduce a modality conversion concept aimed at enhancing emotion recognition\nperformance on the MELD dataset. We assess our approach through two\nexperiments: first, a method named Modality-Conversion that employs automatic\nspeech recognition (ASR) systems, followed by a text classifier; second, we\nassume perfect ASR output and investigate the impact of modality conversion on\nSER, this method is called Modality-Conversion++. Our findings indicate that\nthe first method yields substantial results, while the second method\noutperforms state-of-the-art (SOTA) speech-based approaches in terms of SER\nweighted-F1 (WF1) score on the MELD dataset. This research highlights the\npotential of modality conversion for tasks that can be conducted in alternative\nmodalities.", "published": "2023-07-21 13:48:11", "link": "http://arxiv.org/abs/2307.11584v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts", "abstract": "Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have\nrevolutionized visual representation learning by providing good performance on\ndownstream datasets. VLMs are 0-shot adapted to a downstream dataset by\ndesigning prompts that are relevant to the dataset. Such prompt engineering\nmakes use of domain expertise and a validation dataset. Meanwhile, recent\ndevelopments in generative pretrained models like GPT-4 mean they can be used\nas advanced internet search tools. They can also be manipulated to provide\nvisual information in any structure. In this work, we show that GPT-4 can be\nused to generate text that is visually descriptive and how this can be used to\nadapt CLIP to downstream tasks. We show considerable improvements in 0-shot\ntransfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD\n(~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt.\nWe also design a simple few-shot adapter that learns to choose the best\npossible sentences to construct generalizable classifiers that outperform the\nrecently proposed CoCoOP by ~2% on average and by over 4% on 4 specialized\nfine-grained datasets. The code, prompts, and auxiliary text dataset is\navailable at https://github.com/mayug/VDT-Adapter.", "published": "2023-07-21 15:49:59", "link": "http://arxiv.org/abs/2307.11661v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Prompting Large Language Models with Speech Recognition Abilities", "abstract": "Large language models have proven themselves highly flexible, able to solve a\nwide range of generative tasks, such as abstractive summarization and\nopen-ended question answering. In this paper we extend the capabilities of LLMs\nby directly attaching a small audio encoder allowing it to perform speech\nrecognition. By directly prepending a sequence of audial embeddings to the text\ntoken embeddings, the LLM can be converted to an automatic speech recognition\n(ASR) system, and be used in the exact same manner as its textual counterpart.\nExperiments on Multilingual LibriSpeech (MLS) show that incorporating a\nconformer encoder into the open sourced LLaMA-7B allows it to outperform\nmonolingual baselines by 18% and perform multilingual speech recognition\ndespite LLaMA being trained overwhelmingly on English text. Furthermore, we\nperform ablation studies to investigate whether the LLM can be completely\nfrozen during training to maintain its original capabilities, scaling up the\naudio encoder, and increasing the audio encoder striding to generate fewer\nembeddings. The results from these studies show that multilingual ASR is\npossible even when the LLM is frozen or when strides of almost 1 second are\nused in the audio encoder opening up the possibility for LLMs to operate on\nlong-form audio.", "published": "2023-07-21 08:39:15", "link": "http://arxiv.org/abs/2307.11795v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Multimodal Document Analytics for Banking Process Automation", "abstract": "Traditional banks face increasing competition from FinTechs in the rapidly\nevolving financial ecosystem. Raising operational efficiency is vital to\naddress this challenge. Our study aims to improve the efficiency of\ndocument-intensive business processes in banking. To that end, we first review\nthe landscape of business documents in the retail segment. Banking documents\noften contain text, layout, and visuals, suggesting that document analytics and\nprocess automation require more than plain natural language processing (NLP).\nTo verify this and assess the incremental value of visual cues when processing\nbusiness documents, we compare a recently proposed multimodal model called\nLayoutXLM to powerful text classifiers (e.g., BERT) and large language models\n(e.g., GPT) in a case study related to processing company register extracts.\nThe results confirm that incorporating layout information in a model\nsubstantially increases its performance. Interestingly, we also observed that\nmore than 75% of the best model performance (in terms of the F1 score) can be\nachieved with as little as 30% of the training data. This shows that the demand\nfor data labeled data to set up a multi-modal model can be moderate, which\nsimplifies real-world applications of multimodal document analytics. Our study\nalso sheds light on more specific practices in the scope of calibrating a\nmultimodal banking document classifier, including the need for fine-tuning. In\nsum, the paper contributes original empirical evidence on the effectiveness and\nefficiency of multi-model models for document processing in the banking\nbusiness and offers practical guidance on how to unlock this potential in\nday-to-day operations.", "published": "2023-07-21 18:29:04", "link": "http://arxiv.org/abs/2307.11845v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "q-fin.CP"], "primary_category": "cs.CL"}
{"title": "The Looming Threat of Fake and LLM-generated LinkedIn Profiles:\n  Challenges and Opportunities for Detection and Prevention", "abstract": "In this paper, we present a novel method for detecting fake and Large\nLanguage Model (LLM)-generated profiles in the LinkedIn Online Social Network\nimmediately upon registration and before establishing connections. Early fake\nprofile identification is crucial to maintaining the platform's integrity since\nit prevents imposters from acquiring the private and sensitive information of\nlegitimate users and from gaining an opportunity to increase their credibility\nfor future phishing and scamming activities. This work uses textual information\nprovided in LinkedIn profiles and introduces the Section and Subsection Tag\nEmbedding (SSTE) method to enhance the discriminative characteristics of these\ndata for distinguishing between legitimate profiles and those created by\nimposters manually or by using an LLM. Additionally, the dearth of a large\npublicly available LinkedIn dataset motivated us to collect 3600 LinkedIn\nprofiles for our research. We will release our dataset publicly for research\npurposes. This is, to the best of our knowledge, the first large publicly\navailable LinkedIn dataset for fake LinkedIn account detection. Within our\nparadigm, we assess static and contextualized word embeddings, including GloVe,\nFlair, BERT, and RoBERTa. We show that the suggested method can distinguish\nbetween legitimate and fake profiles with an accuracy of about 95% across all\nword embeddings. In addition, we show that SSTE has a promising accuracy for\nidentifying LLM-generated profiles, despite the fact that no LLM-generated\nprofiles were employed during the training phase, and can achieve an accuracy\nof approximately 90% when only 20 LLM-generated profiles are added to the\ntraining set. It is a significant finding since the proliferation of several\nLLMs in the near future makes it extremely challenging to design a single\nsystem that can identify profiles created with various LLMs.", "published": "2023-07-21 19:09:24", "link": "http://arxiv.org/abs/2307.11864v1", "categories": ["cs.SI", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Selective Perception: Optimizing State Descriptions with Reinforcement\n  Learning for Language Model Actors", "abstract": "Large language models (LLMs) are being applied as actors for sequential\ndecision making tasks in domains such as robotics and games, utilizing their\ngeneral world knowledge and planning abilities. However, previous work does\nlittle to explore what environment state information is provided to LLM actors\nvia language. Exhaustively describing high-dimensional states can impair\nperformance and raise inference costs for LLM actors. Previous LLM actors avoid\nthe issue by relying on hand-engineered, task-specific protocols to determine\nwhich features to communicate about a state and which to leave out. In this\nwork, we propose Brief Language INputs for DEcision-making Responses (BLINDER),\na method for automatically selecting concise state descriptions by learning a\nvalue function for task-conditioned state descriptions. We evaluate BLINDER on\nthe challenging video game NetHack and a robotic manipulation task. Our method\nimproves task success rate, reduces input size and compute costs, and\ngeneralizes between LLM actors.", "published": "2023-07-21 22:02:50", "link": "http://arxiv.org/abs/2307.11922v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Who should I Collaborate with? A Comparative Study of Academia and\n  Industry Research Collaboration in NLP", "abstract": "The goal of our research was to investigate the effects of collaboration\nbetween academia and industry on Natural Language Processing (NLP). To do this,\nwe created a pipeline to extract affiliations and citations from NLP papers and\ndivided them into three categories: academia, industry, and hybrid\n(collaborations between academia and industry). Our empirical analysis found\nthat there is a trend towards an increase in industry and academia-industry\ncollaboration publications and that these types of publications tend to have a\nhigher impact compared to those produced solely within academia.", "published": "2023-07-21 01:26:29", "link": "http://arxiv.org/abs/2308.04524v1", "categories": ["cs.DL", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.DL"}
