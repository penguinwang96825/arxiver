{"title": "CLUSE: Cross-Lingual Unsupervised Sense Embeddings", "abstract": "This paper proposes a modularized sense induction and representation learning\nmodel that jointly learns bilingual sense embeddings that align well in the\nvector space, where the cross-lingual signal in the English-Chinese parallel\ncorpus is exploited to capture the collocation and distributed characteristics\nin the language pair. The model is evaluated on the Stanford Contextual Word\nSimilarity (SCWS) dataset to ensure the quality of monolingual sense\nembeddings. In addition, we introduce Bilingual Contextual Word Similarity\n(BCWS), a large and high-quality dataset for evaluating cross-lingual sense\nembeddings, which is the first attempt of measuring whether the learned\nembeddings are indeed aligned well in the vector space. The proposed approach\nshows the superior quality of sense embeddings evaluated in both monolingual\nand bilingual spaces.", "published": "2018-09-15 10:42:41", "link": "http://arxiv.org/abs/1809.05694v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Abstractive Dialogue Summarization with Sentence-Gated Modeling\n  Optimized by Dialogue Acts", "abstract": "Neural abstractive summarization has been increasingly studied, where the\nprior work mainly focused on summarizing single-speaker documents (news,\nscientific publications, etc). In dialogues, there are different interactions\nbetween speakers, which are usually defined as dialogue acts. The interactive\nsignals may provide informative cues for better summarizing dialogues. This\npaper proposes to explicitly leverage dialogue acts in a neural summarization\nmodel, where a sentence-gated mechanism is designed for modeling the\nrelationship between dialogue acts and the summary. The experiments show that\nour proposed model significantly improves the abstractive summarization\nperformance compared to the state-of-the-art baselines on AMI meeting corpus,\ndemonstrating the usefulness of the interactive signal provided by dialogue\nacts.", "published": "2018-09-15 13:30:03", "link": "http://arxiv.org/abs/1809.05715v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Networks and Quantifier Conservativity: Does Data Distribution\n  Affect Learnability?", "abstract": "All known natural language determiners are conservative. Psycholinguistic\nexperiments indicate that children exhibit a corresponding learnability bias\nwhen faced with the task of learning new determiners. However, recent work\nindicates that this bias towards conservativity is not observed during the\ntraining stage of artificial neural networks. In this work, we investigate\nwhether the learnability bias exhibited by children is in part due to the\ndistribution of quantifiers in natural language. We share results of five\nexperiments, contrasted by the distribution of conservative vs.\nnon-conservative determiners in the training data. We demonstrate that the\naquisitional issues with non-conservative quantifiers can not be explained by\nthe distribution of natural language data, which favors conservative\nquantifiers. This finding indicates that the bias in language acquisition data\nmight be innate or representational.", "published": "2018-09-15 15:44:54", "link": "http://arxiv.org/abs/1809.05733v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finding the way from \u00e4 to a: Sub-character morphological inflection\n  for the SIGMORPHON 2018 Shared Task", "abstract": "In this paper we describe the system submitted by UHH to the\nCoNLL--SIGMORPHON 2018 Shared Task: Universal Morphological Reinflection. We\npropose a neural architecture based on the concepts of UZH (Makarov et al.,\n2017), adding new ideas and techniques to their key concept and evaluating\ndifferent combinations of parameters. The resulting system is a\nlanguage-agnostic network model that aims to reduce the number of learned edit\noperations by introducing equivalence classes over graphical features of\nindividual characters. We try to pinpoint advantages and drawbacks of this\napproach by comparing different network configurations and evaluating our\nresults over a wide range of languages.", "published": "2018-09-15 16:42:24", "link": "http://arxiv.org/abs/1809.05742v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysis of Risk Factor Domains in Psychosis Patient Health Records", "abstract": "Readmission after discharge from a hospital is disruptive and costly,\nregardless of the reason. However, it can be particularly problematic for\npsychiatric patients, so predicting which patients may be readmitted is\ncritically important but also very difficult. Clinical narratives in\npsychiatric electronic health records (EHRs) span a wide range of topics and\nvocabulary; therefore, a psychiatric readmission prediction model must begin\nwith a robust and interpretable topic extraction component. We created a data\npipeline for using document vector similarity metrics to perform topic\nextraction on psychiatric EHR data in service of our long-term goal of creating\na readmission risk classifier. We show initial results for our topic extraction\nmodel and identify additional features we will be incorporating in the future.", "published": "2018-09-15 18:27:59", "link": "http://arxiv.org/abs/1809.05752v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Geo-Text Data and Data-Driven Geospatial Semantics", "abstract": "Many datasets nowadays contain links between geographic locations and natural\nlanguage texts. These links can be geotags, such as geotagged tweets or\ngeotagged Wikipedia pages, in which location coordinates are explicitly\nattached to texts. These links can also be place mentions, such as those in\nnews articles, travel blogs, or historical archives, in which texts are\nimplicitly connected to the mentioned places. This kind of data is referred to\nas geo-text data. The availability of large amounts of geo-text data brings\nboth challenges and opportunities. On the one hand, it is challenging to\nautomatically process this kind of data due to the unstructured texts and the\ncomplex spatial footprints of some places. On the other hand, geo-text data\noffers unique research opportunities through the rich information contained in\ntexts and the special links between texts and geography. As a result, geo-text\ndata facilitates various studies especially those in data-driven geospatial\nsemantics. This paper discusses geo-text data and related concepts. With a\nfocus on data-driven research, this paper systematically reviews a large number\nof studies that have discovered multiple types of knowledge from geo-text data.\nBased on the literature review, a generalized workflow is extracted and key\nchallenges for future work are discussed.", "published": "2018-09-15 02:54:34", "link": "http://arxiv.org/abs/1809.05636v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Graph Convolutional Networks for Text Classification", "abstract": "Text classification is an important and classical problem in natural language\nprocessing. There have been a number of studies that applied convolutional\nneural networks (convolution on regular grid, e.g., sequence) to\nclassification. However, only a limited number of studies have explored the\nmore flexible graph convolutional neural networks (convolution on non-grid,\ne.g., arbitrary graph) for the task. In this work, we propose to use graph\nconvolutional networks for text classification. We build a single text graph\nfor a corpus based on word co-occurrence and document word relations, then\nlearn a Text Graph Convolutional Network (Text GCN) for the corpus. Our Text\nGCN is initialized with one-hot representation for word and document, it then\njointly learns the embeddings for both words and documents, as supervised by\nthe known class labels for documents. Our experimental results on multiple\nbenchmark datasets demonstrate that a vanilla Text GCN without any external\nword embeddings or knowledge outperforms state-of-the-art methods for text\nclassification. On the other hand, Text GCN also learns predictive word and\ndocument embeddings. In addition, experimental results show that the\nimprovement of Text GCN over state-of-the-art comparison methods become more\nprominent as we lower the percentage of training data, suggesting the\nrobustness of Text GCN to less training data in text classification.", "published": "2018-09-15 09:13:12", "link": "http://arxiv.org/abs/1809.05679v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Answering Science Exam Questions Using Query Rewriting with Background\n  Knowledge", "abstract": "Open-domain question answering (QA) is an important problem in AI and NLP\nthat is emerging as a bellwether for progress on the generalizability of AI\nmethods and techniques. Much of the progress in open-domain QA systems has been\nrealized through advances in information retrieval methods and corpus\nconstruction. In this paper, we focus on the recently introduced ARC Challenge\ndataset, which contains 2,590 multiple choice questions authored for\ngrade-school science exams. These questions are selected to be the most\nchallenging for current QA systems, and current state of the art performance is\nonly slightly better than random chance. We present a system that rewrites a\ngiven question into queries that are used to retrieve supporting text from a\nlarge corpus of science-related text. Our rewriter is able to incorporate\nbackground knowledge from ConceptNet and -- in tandem with a generic textual\nentailment system trained on SciTail that identifies support in the retrieved\nresults -- outperforms several strong baselines on the end-to-end QA task\ndespite only being trained to identify essential terms in the original source\nquestion. We use a generalizable decision methodology over the retrieved\nevidence and answer candidates to select the best answer. By combining query\nrewriting, background knowledge, and textual entailment our system is able to\noutperform several strong baselines on the ARC dataset.", "published": "2018-09-15 14:49:23", "link": "http://arxiv.org/abs/1809.05726v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "AUEB at BioASQ 6: Document and Snippet Retrieval", "abstract": "We present AUEB's submissions to the BioASQ 6 document and snippet retrieval\ntasks (parts of Task 6b, Phase A). Our models use novel extensions to deep\nlearning architectures that operate solely over the text of the query and\ncandidate document/snippets. Our systems scored at the top or near the top for\nall batches of the challenge, highlighting the effectiveness of deep learning\nfor these tasks.", "published": "2018-09-15 22:11:03", "link": "http://arxiv.org/abs/1809.06366v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Improving Natural Language Inference Using External Knowledge in the\n  Science Questions Domain", "abstract": "Natural Language Inference (NLI) is fundamental to many Natural Language\nProcessing (NLP) applications including semantic search and question answering.\nThe NLI problem has gained significant attention thanks to the release of large\nscale, challenging datasets. Present approaches to the problem largely focus on\nlearning-based methods that use only textual information in order to classify\nwhether a given premise entails, contradicts, or is neutral with respect to a\ngiven hypothesis. Surprisingly, the use of methods based on structured\nknowledge -- a central topic in artificial intelligence -- has not received\nmuch attention vis-a-vis the NLI problem. While there are many open knowledge\nbases that contain various types of reasoning information, their use for NLI\nhas not been well explored. To address this, we present a combination of\ntechniques that harness knowledge graphs to improve performance on the NLI\nproblem in the science questions domain. We present the results of applying our\ntechniques on text, graph, and text-to-graph based models, and discuss\nimplications for the use of external knowledge in solving the NLI problem. Our\nmodel achieves the new state-of-the-art performance on the NLI problem over the\nSciTail science questions dataset.", "published": "2018-09-15 14:37:46", "link": "http://arxiv.org/abs/1809.05724v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Document Informed Neural Autoregressive Topic Models with Distributional\n  Prior", "abstract": "We address two challenges in topic models: (1) Context information around\nwords helps in determining their actual meaning, e.g., \"networks\" used in the\ncontexts \"artificial neural networks\" vs. \"biological neuron networks\".\nGenerative topic models infer topic-word distributions, taking no or only\nlittle context into account. Here, we extend a neural autoregressive topic\nmodel to exploit the full context information around words in a document in a\nlanguage modeling fashion. The proposed model is named as iDocNADE. (2) Due to\nthe small number of word occurrences (i.e., lack of context) in short text and\ndata sparsity in a corpus of few documents, the application of topic models is\nchallenging on such texts. Therefore, we propose a simple and efficient way of\nincorporating external knowledge into neural autoregressive topic models: we\nuse embeddings as a distributional prior. The proposed variants are named as\nDocNADEe and iDocNADEe.\n  We present novel neural autoregressive topic model variants that consistently\noutperform state-of-the-art generative topic models in terms of generalization,\ninterpretability (topic coherence) and applicability (retrieval and\nclassification) over 7 long-text and 8 short-text datasets from diverse\ndomains.", "published": "2018-09-15 12:48:16", "link": "http://arxiv.org/abs/1809.06709v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Attention as a Perspective for Learning Tempo-invariant Audio Queries", "abstract": "Current models for audio--sheet music retrieval via multimodal embedding\nspace learning use convolutional neural networks with a fixed-size window for\nthe input audio. Depending on the tempo of a query performance, this window\ncaptures more or less musical content, while notehead density in the score is\nlargely tempo-independent. In this work we address this disparity with a soft\nattention mechanism, which allows the model to encode only those parts of an\naudio excerpt that are most relevant with respect to efficient query codes.\nEmpirical results on classical piano music indicate that attention is\nbeneficial for retrieval performance, and exhibits intuitively appealing\nbehavior.", "published": "2018-09-15 10:03:15", "link": "http://arxiv.org/abs/1809.05689v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
