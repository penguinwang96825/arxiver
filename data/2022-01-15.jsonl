{"title": "Extracting Space Situational Awareness Events from News Text", "abstract": "Space situational awareness typically makes use of physical measurements from\nradar, telescopes, and other assets to monitor satellites and other spacecraft\nfor operational, navigational, and defense purposes. In this work we explore\nusing textual input for the space situational awareness task. We construct a\ncorpus of 48.5k news articles spanning all known active satellites between 2009\nand 2020. Using a dependency-rule-based extraction system designed to target\nthree high-impact events -- spacecraft launches, failures, and\ndecommissionings, we identify 1,787 space-event sentences that are then\nannotated by humans with 15.9k labels for event slots. We empirically\ndemonstrate a state-of-the-art neural extraction system achieves an overall F1\nbetween 53 and 91 per slot for event extraction in this low-resource,\nhigh-impact domain.", "published": "2022-01-15 00:55:00", "link": "http://arxiv.org/abs/2201.05721v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ensemble Transformer for Efficient and Accurate Ranking Tasks: an\n  Application to Question Answering Systems", "abstract": "Large transformer models can highly improve Answer Sentence Selection (AS2)\ntasks, but their high computational costs prevent their use in many real-world\napplications. In this paper, we explore the following research question: How\ncan we make the AS2 models more accurate without significantly increasing their\nmodel complexity? To address the question, we propose a Multiple Heads Student\narchitecture (named CERBERUS), an efficient neural network designed to distill\nan ensemble of large transformers into a single smaller model. CERBERUS\nconsists of two components: a stack of transformer layers that is used to\nencode inputs, and a set of ranking heads; unlike traditional distillation\ntechnique, each of them is trained by distilling a different large transformer\narchitecture in a way that preserves the diversity of the ensemble members. The\nresulting model captures the knowledge of heterogeneous transformer models by\nusing just a few extra parameters. We show the effectiveness of CERBERUS on\nthree English datasets for AS2; our proposed approach outperforms all\nsingle-model distillations we consider, rivaling the state-of-the-art large AS2\nmodels that have 2.7x more parameters and run 2.5x slower. Code for our model\nis available at https://github.com/amazon-research/wqa-cerberus", "published": "2022-01-15 06:21:01", "link": "http://arxiv.org/abs/2201.05767v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Dual Prompt Learning Framework for Few-Shot Dialogue State Tracking", "abstract": "Dialogue state tracking (DST) module is an important component for\ntask-oriented dialog systems to understand users' goals and needs. Collecting\ndialogue state labels including slots and values can be costly, especially with\nthe wide application of dialogue systems in more and more new-rising domains.\nIn this paper, we focus on how to utilize the language understanding and\ngeneration ability of pre-trained language models for DST. We design a dual\nprompt learning framework for few-shot DST. Specifically, we consider the\nlearning of slot generation and value generation as dual tasks, and two prompts\nare designed based on such a dual structure to incorporate task-related\nknowledge of these two tasks respectively. In this way, the DST task can be\nformulated as a language modeling task efficiently under few-shot settings.\nExperimental results on two task-oriented dialogue datasets show that the\nproposed method not only outperforms existing state-of-the-art few-shot\nmethods, but also can generate unseen slots. It indicates that DST-related\nknowledge can be probed from PLM and utilized to address low-resource DST\nefficiently with the help of prompt learning.", "published": "2022-01-15 07:37:33", "link": "http://arxiv.org/abs/2201.05780v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Lexical Simplification for Turkish", "abstract": "In this paper, we present the first automatic lexical simplification system\nfor the Turkish language. Recent text simplification efforts rely on manually\ncrafted simplified corpora and comprehensive NLP tools that can analyse the\ntarget text both in word and sentence levels. Turkish is a morphologically rich\nagglutinative language that requires unique considerations such as the proper\nhandling of inflectional cases. Being a low-resource language in terms of\navailable resources and industrial-strength tools, it makes the text\nsimplification task harder to approach. We present a new text simplification\npipeline based on pretrained representation model BERT together with\nmorphological features to generate grammatically correct and semantically\nappropriate word-level simplifications.", "published": "2022-01-15 15:58:44", "link": "http://arxiv.org/abs/2201.05878v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reasoning over Hybrid Chain for Table-and-Text Open Domain QA", "abstract": "Tabular and textual question answering requires systems to perform reasoning\nover heterogeneous information, considering table structure, and the\nconnections among table and text. In this paper, we propose a ChAin-centric\nReasoning and Pre-training framework (CARP). CARP utilizes hybrid chain to\nmodel the explicit intermediate reasoning process across table and text for\nquestion answering. We also propose a novel chain-centric pre-training method,\nto enhance the pre-trained model in identifying the cross-modality reasoning\nprocess and alleviating the data sparsity problem. This method constructs the\nlarge-scale reasoning corpus by synthesizing pseudo heterogeneous reasoning\npaths from Wikipedia and generating corresponding questions. We evaluate our\nsystem on OTT-QA, a large-scale table-and-text open-domain question answering\nbenchmark, and our system achieves the state-of-the-art performance. Further\nanalyses illustrate that the explicit hybrid chain offers substantial\nperformance improvement and interpretablity of the intermediate reasoning\nprocess, and the chain-centric pre-training boosts the performance on the chain\nextraction.", "published": "2022-01-15 16:11:55", "link": "http://arxiv.org/abs/2201.05880v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Correction of Syntactic Dependency Annotation Differences", "abstract": "Annotation inconsistencies between data sets can cause problems for\nlow-resource NLP, where noisy or inconsistent data cannot be as easily replaced\ncompared with resource-rich languages. In this paper, we propose a method for\nautomatically detecting annotation mismatches between dependency parsing\ncorpora, as well as three related methods for automatically converting the\nmismatches. All three methods rely on comparing an unseen example in a new\ncorpus with similar examples in an existing corpus. These three methods include\na simple lexical replacement using the most frequent tag of the example in the\nexisting corpus, a GloVe embedding-based replacement that considers a wider\npool of examples, and a BERT embedding-based replacement that uses\ncontextualized embeddings to provide examples fine-tuned to our specific data.\nWe then evaluate these conversions by retraining two dependency parsers --\nStanza (Qi et al. 2020) and Parsing as Tagging (PaT) (Vacareanu et al. 2020) --\non the converted and unconverted data. We find that applying our conversions\nyields significantly better performance in many cases. Some differences\nobserved between the two parsers are observed. Stanza has a more complex\narchitecture with a quadratic algorithm, so it takes longer to train, but it\ncan generalize better with less data. The PaT parser has a simpler architecture\nwith a linear algorithm, speeding up training time but requiring more training\ndata to reach comparable or better performance.", "published": "2022-01-15 17:17:55", "link": "http://arxiv.org/abs/2201.05891v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unobserved Local Structures Make Compositional Generalization Hard", "abstract": "While recent work has convincingly showed that sequence-to-sequence models\nstruggle to generalize to new compositions (termed compositional\ngeneralization), little is known on what makes compositional generalization\nhard on a particular test instance. In this work, we investigate what are the\nfactors that make generalization to certain test instances challenging. We\nfirst substantiate that indeed some examples are more difficult than others by\nshowing that different models consistently fail or succeed on the same test\ninstances. Then, we propose a criterion for the difficulty of an example: a\ntest instance is hard if it contains a local structure that was not observed at\ntraining time. We formulate a simple decision rule based on this criterion and\nempirically show it predicts instance-level generalization well across 5\ndifferent semantic parsing datasets, substantially better than alternative\ndecision rules. Last, we show local structures can be leveraged for creating\ndifficult adversarial compositional splits and also to improve compositional\ngeneralization under limited training budgets by strategically selecting\nexamples for the training set.", "published": "2022-01-15 18:03:29", "link": "http://arxiv.org/abs/2201.05899v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Addressing the Challenges of Cross-Lingual Hate Speech Detection", "abstract": "The goal of hate speech detection is to filter negative online content aiming\nat certain groups of people. Due to the easy accessibility of social media\nplatforms it is crucial to protect everyone which requires building hate speech\ndetection systems for a wide range of languages. However, the available labeled\nhate speech datasets are limited making it problematic to build systems for\nmany languages. In this paper we focus on cross-lingual transfer learning to\nsupport hate speech detection in low-resource languages. We leverage\ncross-lingual word embeddings to train our neural network systems on the source\nlanguage and apply it to the target language, which lacks labeled examples, and\nshow that good performance can be achieved. We then incorporate unlabeled\ntarget language data for further model improvements by bootstrapping labels\nusing an ensemble of different model architectures. Furthermore, we investigate\nthe issue of label imbalance of hate speech datasets, since the high ratio of\nnon-hate examples compared to hate examples often leads to low model\nperformance. We test simple data undersampling and oversampling techniques and\nshow their effectiveness.", "published": "2022-01-15 20:48:14", "link": "http://arxiv.org/abs/2201.05922v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Benchmark for Generalizable and Interpretable Temporal Question\n  Answering over Knowledge Bases", "abstract": "Knowledge Base Question Answering (KBQA) tasks that involve complex reasoning\nare emerging as an important research direction. However, most existing KBQA\ndatasets focus primarily on generic multi-hop reasoning over explicit facts,\nlargely ignoring other reasoning types such as temporal, spatial, and taxonomic\nreasoning. In this paper, we present a benchmark dataset for temporal\nreasoning, TempQA-WD, to encourage research in extending the present approaches\nto target a more challenging set of complex reasoning tasks. Specifically, our\nbenchmark is a temporal question answering dataset with the following\nadvantages: (a) it is based on Wikidata, which is the most frequently curated,\nopenly available knowledge base, (b) it includes intermediate sparql queries to\nfacilitate the evaluation of semantic parsing based approaches for KBQA, and\n(c) it generalizes to multiple knowledge bases: Freebase and Wikidata. The\nTempQA-WD dataset is available at https://github.com/IBM/tempqa-wd.", "published": "2022-01-15 08:49:09", "link": "http://arxiv.org/abs/2201.05793v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KazakhTTS2: Extending the Open-Source Kazakh TTS Corpus With More Data,\n  Speakers, and Topics", "abstract": "We present an expanded version of our previously released Kazakh\ntext-to-speech (KazakhTTS) synthesis corpus. In the new KazakhTTS2 corpus, the\noverall size has increased from 93 hours to 271 hours, the number of speakers\nhas risen from two to five (three females and two males), and the topic\ncoverage has been diversified with the help of new sources, including a book\nand Wikipedia articles. This corpus is necessary for building high-quality TTS\nsystems for Kazakh, a Central Asian agglutinative language from the Turkic\nfamily, which presents several linguistic challenges. We describe the corpus\nconstruction process and provide the details of the training and evaluation\nprocedures for the TTS system. Our experimental results indicate that the\nconstructed corpus is sufficient to build robust TTS models for real-world\napplications, with a subjective mean opinion score ranging from 3.6 to 4.2 for\nall the five speakers. We believe that our corpus will facilitate speech and\nlanguage research for Kazakh and other Turkic languages, which are widely\nconsidered to be low-resource due to the limited availability of free\nlinguistic data. The constructed corpus, code, and pretrained models are\npublicly available in our GitHub repository.", "published": "2022-01-15 06:54:30", "link": "http://arxiv.org/abs/2201.05771v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Machine Learning for Food Review and Recommendation", "abstract": "Food reviews and recommendations have always been important for online food\nservice websites. However, reviewing and recommending food is not simple as it\nis likely to be overwhelmed by disparate contexts and meanings. In this paper,\nwe use different deep learning approaches to address the problems of sentiment\nanalysis, automatic review tag generation, and retrieval of food reviews. We\npropose to develop a web-based food review system at Nanyang Technological\nUniversity (NTU) named NTU Food Hunter, which incorporates different deep\nlearning approaches that help users with food selection. First, we implement\nthe BERT and LSTM deep learning models into the system for sentiment analysis\nof food reviews. Then, we develop a Part-of-Speech (POS) algorithm to\nautomatically identify and extract adjective-noun pairs from the review content\nfor review tag generation based on POS tagging and dependency parsing. Finally,\nwe also train a RankNet model for the re-ranking of the retrieval results to\nimprove the accuracy in our Solr-based food reviews search system. The\nexperimental results show that our proposed deep learning approaches are\npromising for the applications of real-world problems.", "published": "2022-01-15 02:33:59", "link": "http://arxiv.org/abs/2201.10978v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks", "abstract": "Contrastive language-image pretraining (CLIP) links vision and language\nmodalities into a unified embedding space, yielding the tremendous potential\nfor vision-language (VL) tasks. While early concurrent works have begun to\nstudy this potential on a subset of tasks, important questions remain: 1) What\nis the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in\nlow-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches\nwithout impacting inference or pretraining complexity? In this work, we seek to\nanswer these questions through two key contributions. First, we introduce an\nevaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual\nEntailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of\ndata availability constraints and conditions of domain shift. Second, we\npropose an approach, named CLIP Targeted Distillation (CLIP-TD), to\nintelligently distill knowledge from CLIP into existing architectures using a\ndynamically weighted objective applied to adaptively selected tokens per\ninstance. Experiments demonstrate that our proposed CLIP-TD leads to\nexceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to\n71.3%) conditions of VCR, while simultaneously improving performance under\nstandard fully-supervised conditions (up to 2%), achieving state-of-art\nperformance on VCR compared to other single models that are pretrained with\nimage-text data only. On SNLI-VE, CLIP-TD produces significant gains in\nlow-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On\nVQA, CLIP-TD provides improvement in low-shot (up to 9%), and in\nfully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works\nutilizing CLIP for finetuning, as well as baseline naive distillation\napproaches. Code will be made available.", "published": "2022-01-15 01:54:01", "link": "http://arxiv.org/abs/2201.05729v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Kformer: Knowledge Injection in Transformer Feed-Forward Layers", "abstract": "Recent days have witnessed a diverse set of knowledge injection models for\npre-trained language models (PTMs); however, most previous studies neglect the\nPTMs' own ability with quantities of implicit knowledge stored in parameters. A\nrecent study has observed knowledge neurons in the Feed Forward Network (FFN),\nwhich are responsible for expressing factual knowledge. In this work, we\npropose a simple model, Kformer, which takes advantage of the knowledge stored\nin PTMs and external knowledge via knowledge injection in Transformer FFN\nlayers. Empirically results on two knowledge-intensive tasks, commonsense\nreasoning (i.e., SocialIQA) and medical question answering (i.e., MedQA-USMLE),\ndemonstrate that Kformer can yield better performance than other knowledge\ninjection technologies such as concatenation or attention-based injection. We\nthink the proposed simple model and empirical findings may be helpful for the\ncommunity to develop more powerful knowledge injection methods. Code available\nin https://github.com/zjunlp/Kformer.", "published": "2022-01-15 03:00:27", "link": "http://arxiv.org/abs/2201.05742v2", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ConvMixer: Feature Interactive Convolution with Curriculum Learning for\n  Small Footprint and Noisy Far-field Keyword Spotting", "abstract": "Building efficient architecture in neural speech processing is paramount to\nsuccess in keyword spotting deployment. However, it is very challenging for\nlightweight models to achieve noise robustness with concise neural operations.\nIn a real-world application, the user environment is typically noisy and may\nalso contain reverberations. We proposed a novel feature interactive\nconvolutional model with merely 100K parameters to tackle this under the noisy\nfar-field condition. The interactive unit is proposed in place of the attention\nmodule that promotes the flow of information with more efficient computations.\nMoreover, curriculum-based multi-condition training is adopted to attain better\nnoise robustness. Our model achieves 98.2% top-1 accuracy on Google Speech\nCommand V2-12 and is competitive against large transformer models under the\ndesigned noise condition.", "published": "2022-01-15 14:38:36", "link": "http://arxiv.org/abs/2201.05863v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Recent Progress in the CUHK Dysarthric Speech Recognition System", "abstract": "Despite the rapid progress of automatic speech recognition (ASR) technologies\nin the past few decades, recognition of disordered speech remains a highly\nchallenging task to date. Disordered speech presents a wide spectrum of\nchallenges to current data intensive deep neural networks (DNNs) based ASR\ntechnologies that predominantly target normal speech. This paper presents\nrecent research efforts at the Chinese University of Hong Kong (CUHK) to\nimprove the performance of disordered speech recognition systems on the largest\npublicly available UASpeech dysarthric speech corpus. A set of novel modelling\ntechniques including neural architectural search, data augmentation using\nspectra-temporal perturbation, model based speaker adaptation and cross-domain\ngeneration of visual features within an audio-visual speech recognition (AVSR)\nsystem framework were employed to address the above challenges. The combination\nof these techniques produced the lowest published word error rate (WER) of\n25.21% on the UASpeech test set 16 dysarthric speakers, and an overall WER\nreduction of 5.4% absolute (17.6% relative) over the CUHK 2018 dysarthric\nspeech recognition system featuring a 6-way DNN system combination and cross\nadaptation of out-of-domain normal speech data trained systems. Bayesian model\nadaptation further allows rapid adaptation to individual dysarthric speakers to\nbe performed using as little as 3.06 seconds of speech. The efficacy of these\ntechniques were further demonstrated on a CUDYS Cantonese dysarthric speech\nrecognition task.", "published": "2022-01-15 13:02:40", "link": "http://arxiv.org/abs/2201.05845v2", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Common Phone: A Multilingual Dataset for Robust Acoustic Modelling", "abstract": "Current state of the art acoustic models can easily comprise more than 100\nmillion parameters. This growing complexity demands larger training datasets to\nmaintain a decent generalization of the final decision function. An ideal\ndataset is not necessarily large in size, but large with respect to the amount\nof unique speakers, utilized hardware and varying recording conditions. This\nenables a machine learning model to explore as much of the domain-specific\ninput space as possible during parameter estimation. This work introduces\nCommon Phone, a gender-balanced, multilingual corpus recorded from more than\n11.000 contributors via Mozilla's Common Voice project. It comprises around 116\nhours of speech enriched with automatically generated phonetic segmentation. A\nWav2Vec 2.0 acoustic model was trained with the Common Phone to perform\nphonetic symbol recognition and validate the quality of the generated phonetic\nannotation. The architecture achieved a PER of 18.1 % on the entire test set,\ncomputed with all 101 unique phonetic symbols, showing slight differences\nbetween the individual languages. We conclude that Common Phone provides\nsufficient variability and reliable phonetic annotation to help bridging the\ngap between research and application of acoustic models.", "published": "2022-01-15 19:02:46", "link": "http://arxiv.org/abs/2201.05912v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Novel Multi-Task Learning Method for Symbolic Music Emotion\n  Recognition", "abstract": "Symbolic Music Emotion Recognition(SMER) is to predict music emotion from\nsymbolic data, such as MIDI and MusicXML. Previous work mainly focused on\nlearning better representation via (mask) language model pre-training but\nignored the intrinsic structure of the music, which is extremely important to\nthe emotional expression of music. In this paper, we present a simple\nmulti-task framework for SMER, which incorporates the emotion recognition task\nwith other emotion-related auxiliary tasks derived from the intrinsic structure\nof the music. The results show that our multi-task framework can be adapted to\ndifferent models. Moreover, the labels of auxiliary tasks are easy to be\nobtained, which means our multi-task methods do not require manually annotated\nlabels other than emotion. Conducting on two publicly available datasets\n(EMOPIA and VGMIDI), the experiments show that our methods perform better in\nSMER task. Specifically, accuracy has been increased by 4.17 absolute point to\n67.58 in EMOPIA dataset, and 1.97 absolute point to 55.85 in VGMIDI dataset.\nAblation studies also show the effectiveness of multi-task methods designed in\nthis paper.", "published": "2022-01-15 07:45:10", "link": "http://arxiv.org/abs/2201.05782v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
