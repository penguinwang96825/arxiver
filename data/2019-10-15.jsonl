{"title": "Detecting Machine-Translated Text using Back Translation", "abstract": "Machine-translated text plays a crucial role in the communication of people\nusing different languages. However, adversaries can use such text for malicious\npurposes such as plagiarism and fake review. The existing methods detected a\nmachine-translated text only using the text's intrinsic content, but they are\nunsuitable for classifying the machine-translated and human-written texts with\nthe same meanings. We have proposed a method to extract features used to\ndistinguish machine/human text based on the similarity between the intrinsic\ntext and its back-translation. The evaluation of detecting translated sentences\nwith French shows that our method achieves 75.0% of both accuracy and F-score.\nIt outperforms the existing methods whose the best accuracy is 62.8% and the\nF-score is 62.7%. The proposed method even detects more efficiently the\nback-translated text with 83.4% of accuracy, which is higher than 66.7% of the\nbest previous accuracy. We also achieve similar results not only with F-score\nbut also with similar experiments related to Japanese. Moreover, we prove that\nour detector can recognize both machine-translated and machine-back-translated\ntexts without the language information which is used to generate these machine\ntexts. It demonstrates the persistence of our method in various applications in\nboth low- and rich-resource languages.", "published": "2019-10-15 06:54:28", "link": "http://arxiv.org/abs/1910.06558v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text2Math: End-to-end Parsing Text into Math Expressions", "abstract": "We propose Text2Math, a model for semantically parsing text into math\nexpressions. The model can be used to solve different math related problems\nincluding arithmetic word problems and equation parsing problems. Unlike\nprevious approaches, we tackle the problem from an end-to-end structured\nprediction perspective where our algorithm aims to predict the complete math\nexpression at once as a tree structure, where minimal manual efforts are\ninvolved in the process. Empirical results on benchmark datasets demonstrate\nthe efficacy of our approach.", "published": "2019-10-15 07:44:54", "link": "http://arxiv.org/abs/1910.06571v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aligning Cross-Lingual Entities with Multi-Aspect Information", "abstract": "Multilingual knowledge graphs (KGs), such as YAGO and DBpedia, represent\nentities in different languages. The task of cross-lingual entity alignment is\nto match entities in a source language with their counterparts in target\nlanguages. In this work, we investigate embedding-based approaches to encode\nentities from multilingual KGs into the same vector space, where equivalent\nentities are close to each other. Specifically, we apply graph convolutional\nnetworks (GCNs) to combine multi-aspect information of entities, including\ntopological connections, relations, and attributes of entities, to learn entity\nembeddings. To exploit the literal descriptions of entities expressed in\ndifferent languages, we propose two uses of a pretrained multilingual BERT\nmodel to bridge cross-lingual gaps. We further propose two strategies to\nintegrate GCN-based and BERT-based modules to boost performance. Extensive\nexperiments on two benchmark datasets demonstrate that our method significantly\noutperforms existing systems.", "published": "2019-10-15 07:54:12", "link": "http://arxiv.org/abs/1910.06575v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NumNet: Machine Reading Comprehension with Numerical Reasoning", "abstract": "Numerical reasoning, such as addition, subtraction, sorting and counting is a\ncritical skill in human's reading comprehension, which has not been well\nconsidered in existing machine reading comprehension (MRC) systems. To address\nthis issue, we propose a numerical MRC model named as NumNet, which utilizes a\nnumerically-aware graph neural network to consider the comparing information\nand performs numerical reasoning over numbers in the question and passage. Our\nsystem achieves an EM-score of 64.56% on the DROP dataset, outperforming all\nexisting machine reading comprehension models by considering the numerical\nrelations among numbers.", "published": "2019-10-15 13:09:06", "link": "http://arxiv.org/abs/1910.06701v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Importance of Word Boundaries in Character-level Neural Machine\n  Translation", "abstract": "Neural Machine Translation (NMT) models generally perform translation using a\nfixed-size lexical vocabulary, which is an important bottleneck on their\ngeneralization capability and overall translation quality. The standard\napproach to overcome this limitation is to segment words into subword units,\ntypically using some external tools with arbitrary heuristics, resulting in\nvocabulary units not optimized for the translation task. Recent studies have\nshown that the same approach can be extended to perform NMT directly at the\nlevel of characters, which can deliver translation accuracy on-par with\nsubword-based models, on the other hand, this requires relatively deeper\nnetworks. In this paper, we propose a more computationally-efficient solution\nfor character-level NMT which implements a hierarchical decoding architecture\nwhere translations are subsequently generated at the level of words and\ncharacters. We evaluate different methods for open-vocabulary NMT in the\nmachine translation task from English into five languages with distinct\nmorphological typology, and show that the hierarchical decoding model can reach\nhigher translation accuracy than the subword-level NMT model using\nsignificantly fewer parameters, while demonstrating better capacity in learning\nlonger-distance contextual and grammatical dependencies than the standard\ncharacter-level NMT model.", "published": "2019-10-15 13:54:44", "link": "http://arxiv.org/abs/1910.06753v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Facebook AI's WAT19 Myanmar-English Translation Task Submission", "abstract": "This paper describes Facebook AI's submission to the WAT 2019 Myanmar-English\ntranslation task. Our baseline systems are BPE-based transformer models. We\nexplore methods to leverage monolingual data to improve generalization,\nincluding self-training, back-translation and their combination. We further\nimprove results by using noisy channel re-ranking and ensembling. We\ndemonstrate that these techniques can significantly improve not only a system\ntrained with additional monolingual data, but even the baseline system trained\nexclusively on the provided small parallel dataset. Our system ranks first in\nboth directions according to human evaluation and BLEU, with a gain of over 8\nBLEU points above the second best system.", "published": "2019-10-15 15:10:16", "link": "http://arxiv.org/abs/1910.06848v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Answering Complex Open-domain Questions Through Iterative Query\n  Generation", "abstract": "It is challenging for current one-step retrieve-and-read question answering\n(QA) systems to answer questions like \"Which novel by the author of 'Armada'\nwill be adapted as a feature film by Steven Spielberg?\" because the question\nseldom contains retrievable clues about the missing entity (here, the author).\nAnswering such a question requires multi-hop reasoning where one must gather\ninformation about the missing entity (or facts) to proceed with further\nreasoning. We present GoldEn (Gold Entity) Retriever, which iterates between\nreading context and retrieving more supporting documents to answer open-domain\nmulti-hop questions. Instead of using opaque and computationally expensive\nneural retrieval models, GoldEn Retriever generates natural language search\nqueries given the question and available context, and leverages off-the-shelf\ninformation retrieval systems to query for missing entities. This allows GoldEn\nRetriever to scale up efficiently for open-domain multi-hop reasoning while\nmaintaining interpretability. We evaluate GoldEn Retriever on the recently\nproposed open-domain multi-hop QA dataset, HotpotQA, and demonstrate that it\noutperforms the best previously published model despite not using pretrained\nlanguage models such as BERT.", "published": "2019-10-15 18:44:47", "link": "http://arxiv.org/abs/1910.07000v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Iterative Delexicalization for Improved Spoken Language Understanding", "abstract": "Recurrent neural network (RNN) based joint intent classification and slot\ntagging models have achieved tremendous success in recent years for building\nspoken language understanding and dialog systems. However, these models suffer\nfrom poor performance for slots which often encounter large semantic\nvariability in slot values after deployment (e.g. message texts, partial\nmovie/artist names). While greedy delexicalization of slots in the input\nutterance via substring matching can partly improve performance, it often\nproduces incorrect input. Moreover, such techniques cannot delexicalize slots\nwith out-of-vocabulary slot values not seen at training. In this paper, we\npropose a novel iterative delexicalization algorithm, which can accurately\ndelexicalize the input, even with out-of-vocabulary slot values. Based on model\nconfidence of the current delexicalized input, our algorithm improves\ndelexicalization in every iteration to converge to the best input having the\nhighest confidence. We show on benchmark and in-house datasets that our\nalgorithm can greatly improve parsing performance for RNN based models,\nespecially for out-of-distribution slot values.", "published": "2019-10-15 21:28:13", "link": "http://arxiv.org/abs/1910.07060v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comprehend Medical: a Named Entity Recognition and Relationship\n  Extraction Web Service", "abstract": "Comprehend Medical is a stateless and Health Insurance Portability and\nAccountability Act (HIPAA) eligible Named Entity Recognition (NER) and\nRelationship Extraction (RE) service launched under Amazon Web Services (AWS)\ntrained using state-of-the-art deep learning models. Contrary to many existing\nopen source tools, Comprehend Medical is scalable and does not require steep\nlearning curve, dependencies, pipeline configurations, or installations.\nCurrently, Comprehend Medical performs NER in five medical categories: Anatomy,\nMedical Condition, Medications, Protected Health Information (PHI) and\nTreatment, Test and Procedure (TTP). Additionally, the service provides\nrelationship extraction for the detected entities as well as contextual\ninformation such as negation and temporality in the form of traits. Comprehend\nMedical provides two Application Programming Interfaces (API): 1) the NERe API\nwhich returns all the extracted named entities, their traits and the\nrelationships between them and 2) the PHId API which returns just the protected\nhealth information contained in the text. Furthermore, Comprehend Medical is\naccessible through AWS Console, Java and Python Software Development Kit (SDK),\nmaking it easier for non-developers and developers to use.", "published": "2019-10-15 16:38:55", "link": "http://arxiv.org/abs/1910.07419v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Semantic Correspondence Learning for Post-Discharge Patient\n  Mortality Prediction", "abstract": "Predicting patient mortality is an important and challenging problem in the\nhealthcare domain, especially for intensive care unit (ICU) patients.\nElectronic health notes serve as a rich source for learning patient\nrepresentations, that can facilitate effective risk assessment. However, a\nlarge portion of clinical notes are unstructured and also contain domain\nspecific terminologies, from which we need to extract structured information.\nIn this paper, we introduce an embedding framework to learn\nsemantically-plausible distributed representations of clinical notes that\nexploits the semantic correspondence between the unstructured texts and their\ncorresponding structured knowledge, known as semantic frame, in a hierarchical\nfashion. Our approach integrates text modeling and semantic correspondence\nlearning into a single model that comprises 1) an unstructured embedding module\nthat makes use of self-similarity matrix representations in order to inject\nstructural regularities of different segments inherent in clinical texts to\npromote local coherence, 2) a structured embedding module to embed the semantic\nframes (e.g., UMLS semantic types) with deep ConvNet and 3) a hierarchical\nsemantic correspondence module that embeds by enhancing the interactions\nbetween text-semantic frame embedding pairs at multiple levels (i.e., words,\nsentence, note). Evaluations on multiple embedding benchmarks on post discharge\nintensive care patient mortality prediction tasks demonstrate its effectiveness\ncompared to approaches that do not exploit the semantic interactions between\nstructured and unstructured information present in clinical notes.", "published": "2019-10-15 02:40:29", "link": "http://arxiv.org/abs/1910.06492v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FacTweet: Profiling Fake News Twitter Accounts", "abstract": "We present an approach to detect fake news in Twitter at the account level\nusing a neural recurrent model and a variety of different semantic and\nstylistic features. Our method extracts a set of features from the timelines of\nnews Twitter accounts by reading their posts as chunks, rather than dealing\nwith each tweet independently. We show the experimental benefits of modeling\nlatent stylistic signatures of mixed fake and real news with a sequential model\nover a wide range of strong baselines.", "published": "2019-10-15 08:43:47", "link": "http://arxiv.org/abs/1910.06592v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Language Identification on Massive Datasets of Short Message using an\n  Attention Mechanism CNN", "abstract": "Language Identification (LID) is a challenging task, especially when the\ninput texts are short and noisy such as posts and statuses on social media or\nchat logs on gaming forums. The task has been tackled by either designing a\nfeature set for a traditional classifier (e.g. Naive Bayes) or applying a deep\nneural network classifier (e.g. Bi-directional Gated Recurrent Unit,\nEncoder-Decoder). These methods are usually trained and tested on a huge amount\nof private data, then used and evaluated as off-the-shelf packages by other\nresearchers using their own datasets, and consequently the various results\npublished are not directly comparable. In this paper, we first create a new\nmassive labelled dataset based on one year of Twitter data. We use this dataset\nto test several existing language identification systems, in order to obtain a\nset of coherent benchmarks, and we make our dataset publicly available so that\nothers can add to this set of benchmarks. Finally, we propose a shallow but\nefficient neural LID system, which is a ngram-regional convolution neural\nnetwork enhanced with an attention mechanism. Experimental results show that\nour architecture is able to predict tens of thousands of samples per second and\nsurpasses all state-of-the-art systems with an improvement of 5%.", "published": "2019-10-15 13:51:24", "link": "http://arxiv.org/abs/1910.06748v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "MIMO-SPEECH: End-to-End Multi-Channel Multi-Speaker Speech Recognition", "abstract": "Recently, the end-to-end approach has proven its efficacy in monaural\nmulti-speaker speech recognition. However, high word error rates (WERs) still\nprevent these systems from being used in practical applications. On the other\nhand, the spatial information in multi-channel signals has proven helpful in\nfar-field speech recognition tasks. In this work, we propose a novel neural\nsequence-to-sequence (seq2seq) architecture, MIMO-Speech, which extends the\noriginal seq2seq to deal with multi-channel input and multi-channel output so\nthat it can fully model multi-channel multi-speaker speech separation and\nrecognition. MIMO-Speech is a fully neural end-to-end framework, which is\noptimized only via an ASR criterion. It is comprised of: 1) a monaural masking\nnetwork, 2) a multi-source neural beamformer, and 3) a multi-output speech\nrecognition model. With this processing, the input overlapped speech is\ndirectly mapped to text sequences. We further adopted a curriculum learning\nstrategy, making the best use of the training set to improve the performance.\nThe experiments on the spatialized wsj1-2mix corpus show that our model can\nachieve more than 60% WER reduction compared to the single-channel system with\nhigh quality enhanced signals (SI-SDR = 23.1 dB) obtained by the above\nseparation function.", "published": "2019-10-15 04:45:34", "link": "http://arxiv.org/abs/1910.06522v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Context Matters: Recovering Human Semantic Structure from Machine\n  Learning Analysis of Large-Scale Text Corpora", "abstract": "Applying machine learning algorithms to large-scale, text-based corpora\n(embeddings) presents a unique opportunity to investigate at scale how human\nsemantic knowledge is organized and how people use it to judge fundamental\nrelationships, such as similarity between concepts. However, efforts to date\nhave shown a substantial discrepancy between algorithm predictions and\nempirical judgments. Here, we introduce a novel approach of generating\nembeddings motivated by the psychological theory that semantic context plays a\ncritical role in human judgments. Specifically, we train state-of-the-art\nmachine learning algorithms using contextually-constrained text corpora and\nshow that this greatly improves predictions of similarity judgments and feature\nratings. By improving the correspondence between representations derived using\nembeddings generated by machine learning methods and empirical measurements of\nhuman judgments, the approach we describe helps advance the use of large-scale\ntext corpora to understand the structure of human semantic representations.", "published": "2019-10-15 17:51:01", "link": "http://arxiv.org/abs/1910.06954v3", "categories": ["cs.CL", "cs.IR", "cs.LG", "J.4; I.2.7"], "primary_category": "cs.CL"}
{"title": "Seeing and Hearing Egocentric Actions: How Much Can We Learn?", "abstract": "Our interaction with the world is an inherently multimodal experience.\nHowever, the understanding of human-to-object interactions has historically\nbeen addressed focusing on a single modality. In particular, a limited number\nof works have considered to integrate the visual and audio modalities for this\npurpose. In this work, we propose a multimodal approach for egocentric action\nrecognition in a kitchen environment that relies on audio and visual\ninformation. Our model combines a sparse temporal sampling strategy with a late\nfusion of audio, spatial, and temporal streams. Experimental results on the\nEPIC-Kitchens dataset show that multimodal integration leads to better\nperformance than unimodal approaches. In particular, we achieved a 5.18%\nimprovement over the state of the art on verb classification.", "published": "2019-10-15 12:55:49", "link": "http://arxiv.org/abs/1910.06693v1", "categories": ["cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.CV"}
{"title": "VFNet: A Convolutional Architecture for Accent Classification", "abstract": "Understanding accent is an issue which can derail any human-machine\ninteraction. Accent classification makes this task easier by identifying the\naccent being spoken by a person so that the correct words being spoken can be\nidentified by further processing, since same noises can mean entirely different\nwords in different accents of the same language. In this paper, we present\nVFNet (Variable Filter Net), a convolutional neural network (CNN) based\narchitecture which captures a hierarchy of features to beat the previous\nbenchmarks of accent classification, through a novel and elegant technique of\napplying variable filter sizes along the frequency band of the audio\nutterances.", "published": "2019-10-15 13:04:45", "link": "http://arxiv.org/abs/1910.06697v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Analyzing Large Receptive Field Convolutional Networks for Distant\n  Speech Recognition", "abstract": "Despite significant efforts over the last few years to build a robust\nautomatic speech recognition (ASR) system for different acoustic settings, the\nperformance of the current state-of-the-art technologies significantly degrades\nin noisy reverberant environments.\n  Convolutional Neural Networks (CNNs) have been successfully used to achieve\nsubstantial improvements in many speech processing applications including\ndistant speech recognition (DSR). However, standard CNN architectures were not\nefficient in capturing long-term speech dynamics, which are essential in the\ndesign of a robust DSR system. In the present study, we address this issue by\ninvestigating variants of large receptive field CNNs (LRF-CNNs) which include\ndeeply recursive networks, dilated convolutional neural networks, and stacked\nhourglass networks. To compare the efficacy of the aforementioned architectures\nwith the standard CNN for Wall Street Journal (WSJ) corpus, we use a hybrid\nDNN-HMM based speech recognition system. We extend the study to evaluate the\nsystem performances for distant speech simulated using realistic room impulse\nresponses (RIRs). Our experiments show that with fixed number of parameters\nacross all architectures, the large receptive field networks show consistent\nimprovements over the standard CNNs for distant speech. Amongst the explored\nLRF-CNNs, stacked hourglass network has shown improvements with a 8.9% relative\nreduction in word error rate (WER) and 10.7% relative improvement in frame\naccuracy compared to the standard CNNs for distant simulated speech signals.", "published": "2019-10-15 20:47:29", "link": "http://arxiv.org/abs/1910.07047v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
