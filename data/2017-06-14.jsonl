{"title": "Fine-grained human evaluation of neural versus phrase-based machine\n  translation", "abstract": "We compare three approaches to statistical machine translation (pure\nphrase-based, factored phrase-based and neural) by performing a fine-grained\nmanual evaluation via error annotation of the systems' outputs. The error types\nin our annotation are compliant with the multidimensional quality metrics\n(MQM), and the annotation is performed by two annotators. Inter-annotator\nagreement is high for such a task, and results show that the best performing\nsystem (neural) reduces the errors produced by the worst system (phrase-based)\nby 54%.", "published": "2017-06-14 09:59:47", "link": "http://arxiv.org/abs/1706.04389v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Idea density for predicting Alzheimer's disease from transcribed speech", "abstract": "Idea Density (ID) measures the rate at which ideas or elementary predications\nare expressed in an utterance or in a text. Lower ID is found to be associated\nwith an increased risk of developing Alzheimer's disease (AD) (Snowdon et al.,\n1996; Engelman et al., 2010). ID has been used in two different versions:\npropositional idea density (PID) counts the expressed ideas and can be applied\nto any text while semantic idea density (SID) counts pre-defined information\ncontent units and is naturally more applicable to normative domains, such as\npicture description tasks. In this paper, we develop DEPID, a novel\ndependency-based method for computing PID, and its version DEPID-R that enables\nto exclude repeating ideas---a feature characteristic to AD speech. We conduct\nthe first comparison of automatically extracted PID and SID in the diagnostic\nclassification task on two different AD datasets covering both closed-topic and\nfree-recall domains. While SID performs better on the normative dataset, adding\nPID leads to a small but significant improvement (+1.7 F-score). On the\nfree-topic dataset, PID performs better than SID as expected (77.6 vs 72.3 in\nF-score) but adding the features derived from the word embedding clustering\nunderlying the automatic SID increases the results considerably, leading to an\nF-score of 84.8.", "published": "2017-06-14 13:18:08", "link": "http://arxiv.org/abs/1706.04473v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer Learning for Neural Semantic Parsing", "abstract": "The goal of semantic parsing is to map natural language to a machine\ninterpretable meaning representation language (MRL). One of the constraints\nthat limits full exploration of deep learning technologies for semantic parsing\nis the lack of sufficient annotation training data. In this paper, we propose\nusing sequence-to-sequence in a multi-task setup for semantic parsing with a\nfocus on transfer learning. We explore three multi-task architectures for\nsequence-to-sequence modeling and compare their performance with an\nindependently trained model. Our experiments show that the multi-task setup\naids transfer learning from an auxiliary task with large labeled data to a\ntarget task with smaller labeled data. We see absolute accuracy gains ranging\nfrom 1.0% to 4.4% in our in- house data set, and we also see good gains ranging\nfrom 2.5% to 7.0% on the ATIS semantic parsing tasks with syntactic and\nsemantic auxiliary tasks.", "published": "2017-06-14 05:53:51", "link": "http://arxiv.org/abs/1706.04326v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Number game", "abstract": "CLARIN (Common Language Resources and Technology Infrastructure) is regarded\nas one of the most important European research infrastructures, offering and\npromoting a wide array of useful services for (digital) research in linguistics\nand humanities. However, the assessment of the users for its core technical\ndevelopment has been highly limited, therefore, it is unclear if the community\nis thoroughly aware of the status-quo of the growing infrastructure. In\naddition, CLARIN does not seem to be fully materialised marketing and business\nplans and strategies despite its strong technical assets. This article analyses\nthe web traffic of the Virtual Language Observatory, one of the main web\napplications of CLARIN and a symbol of pan-European re-search cooperation, to\nevaluate the users and performance of the service in a transparent and\nscientific way. It is envisaged that the paper can raise awareness of the\npressing issues on objective and transparent operation of the infrastructure\nthough Open Evaluation, and the synergy between marketing and technical\ndevelopment. It also investigates the \"science of web analytics\" in an attempt\nto document the research process for the purpose of reusability and\nreproducibility, thus to find universal lessons for the use of a web analytics,\nrather than to merely produce a statistical report of a particular website\nwhich loses its value outside its context.", "published": "2017-06-14 13:00:08", "link": "http://arxiv.org/abs/1706.05089v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Is Natural Language a Perigraphic Process? The Theorem about Facts and\n  Words Revisited", "abstract": "As we discuss, a stationary stochastic process is nonergodic when a random\npersistent topic can be detected in the infinite random text sampled from the\nprocess, whereas we call the process strongly nonergodic when an infinite\nsequence of independent random bits, called probabilistic facts, is needed to\ndescribe this topic completely. Replacing probabilistic facts with an\nalgorithmically random sequence of bits, called algorithmic facts, we adapt\nthis property back to ergodic processes. Subsequently, we call a process\nperigraphic if the number of algorithmic facts which can be inferred from a\nfinite text sampled from the process grows like a power of the text length. We\npresent a simple example of such a process. Moreover, we demonstrate an\nassertion which we call the theorem about facts and words. This proposition\nstates that the number of probabilistic or algorithmic facts which can be\ninferred from a text drawn from a process must be roughly smaller than the\nnumber of distinct word-like strings detected in this text by means of the PPM\ncompression algorithm. We also observe that the number of the word-like strings\nfor a sample of plays by Shakespeare follows an empirical stepwise power law,\nin a stark contrast to Markov processes. Hence we suppose that natural language\nconsidered as a process is not only non-Markov but also perigraphic.", "published": "2017-06-14 12:22:38", "link": "http://arxiv.org/abs/1706.04432v2", "categories": ["cs.IT", "cs.CL", "math.IT", "94A17, 60G10, 94A29, 68Q30, 68T50"], "primary_category": "cs.IT"}
{"title": "Neural Models for Key Phrase Detection and Question Generation", "abstract": "We propose a two-stage neural model to tackle question generation from\ndocuments. First, our model estimates the probability that word sequences in a\ndocument are ones that a human would pick when selecting candidate answers by\ntraining a neural key-phrase extractor on the answers in a question-answering\ncorpus. Predicted key phrases then act as target answers and condition a\nsequence-to-sequence question-generation model with a copy mechanism.\nEmpirically, our key-phrase extraction model significantly outperforms an\nentity-tagging baseline and existing rule-based approaches. We further\ndemonstrate that our question generation system formulates fluent, answerable\nquestions from key phrases. This two-stage system could be used to augment or\ngenerate reading comprehension datasets, which may be leveraged to improve\nmachine reading systems or in educational settings.", "published": "2017-06-14 16:06:18", "link": "http://arxiv.org/abs/1706.04560v3", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
