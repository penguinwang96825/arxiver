{"title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled", "abstract": "Recent advancements in neural language modelling make it possible to rapidly\ngenerate vast amounts of human-sounding text. The capabilities of humans and\nautomatic discriminators to detect machine-generated text have been a large\nsource of research interest, but humans and machines rely on different cues to\nmake their decisions. Here, we perform careful benchmarking and analysis of\nthree popular sampling-based decoding strategies---top-$k$, nucleus sampling,\nand untruncated random sampling---and show that improvements in decoding\nmethods have primarily optimized for fooling humans. This comes at the expense\nof introducing statistical abnormalities that make detection easy for automatic\nsystems. We also show that though both human and automatic detector performance\nimprove with longer excerpt length, even multi-sentence excerpts can fool\nexpert human raters over 30% of the time. Our findings reveal the importance of\nusing both human and automatic detectors to assess the humanness of text\ngeneration systems.", "published": "2019-11-02 04:52:00", "link": "http://arxiv.org/abs/1911.00650v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Translation Evaluation using Bi-directional Entailment", "abstract": "In this paper, we propose a new metric for Machine Translation (MT)\nevaluation, based on bi-directional entailment. We show that machine generated\ntranslation can be evaluated by determining paraphrasing with a reference\ntranslation provided by a human translator. We hypothesize, and show through\nexperiments, that paraphrasing can be detected by evaluating entailment\nrelationship in the forward and backward direction. Unlike conventional\nmetrics, like BLEU or METEOR, our approach uses deep learning to determine the\nsemantic similarity between candidate and reference translation for generating\nscores rather than relying upon simple n-gram overlap. We use BERT's\npre-trained implementation of transformer networks, fine-tuned on MNLI corpus,\nfor natural language inferencing. We apply our evaluation metric on WMT'14 and\nWMT'17 dataset to evaluate systems participating in the translation task and\nfind that our metric has a better correlation with the human annotated score\ncompared to the other traditional metrics at system level.", "published": "2019-11-02 08:45:24", "link": "http://arxiv.org/abs/1911.00681v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram\n  Representations", "abstract": "The pre-training of text encoders normally processes text as a sequence of\ntokens corresponding to small text units, such as word pieces in English and\ncharacters in Chinese. It omits information carried by larger text granularity,\nand thus the encoders cannot easily adapt to certain combinations of\ncharacters. This leads to a loss of important semantic information, which is\nespecially problematic for Chinese because the language does not have explicit\nword boundaries. In this paper, we propose ZEN, a BERT-based Chinese (Z) text\nencoder Enhanced by N-gram representations, where different combinations of\ncharacters are considered during training. As a result, potential word or phase\nboundaries are explicitly pre-trained and fine-tuned with the character encoder\n(BERT). Therefore ZEN incorporates the comprehensive information of both the\ncharacter sequence and words or phrases it contains. Experimental results\nillustrated the effectiveness of ZEN on a series of Chinese NLP tasks. We show\nthat ZEN, using less resource than other published encoders, can achieve\nstate-of-the-art performance on most tasks. Moreover, it is shown that\nreasonable performance can be obtained when ZEN is trained on a small corpus,\nwhich is important for applying pre-training techniques to scenarios with\nlimited data. The code and pre-trained models of ZEN are available at\nhttps://github.com/sinovation/zen.", "published": "2019-11-02 14:32:08", "link": "http://arxiv.org/abs/1911.00720v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence-Level BERT and Multi-Task Learning of Age and Gender in Social\n  Media", "abstract": "Social media currently provide a window on our lives, making it possible to\nlearn how people from different places, with different backgrounds, ages, and\ngenders use language. In this work we exploit a newly-created Arabic dataset\nwith ground truth age and gender labels to learn these attributes both\nindividually and in a multi-task setting at the sentence level. Our models are\nbased on variations of deep bidirectional neural networks. More specifically,\nwe build models with gated recurrent units and bidirectional encoder\nrepresentations from transformers (BERT). We show the utility of multi-task\nlearning (MTL) on the two tasks and identify task-specific attention as a\nsuperior choice in this context. We also find that a single-task BERT model\noutperform our best MTL models on the two tasks. We report tweet-level accuracy\nof 51.43% for the age task (three-way) and 65.30% on the gender task (binary),\nboth of which outperforms our baselines with a large margin. Our models are\nlanguage-agnostic, and so can be applied to other languages.", "published": "2019-11-02 03:39:19", "link": "http://arxiv.org/abs/1911.00637v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Credibility-based Fake News Detection", "abstract": "Fake news can significantly misinform people who often rely on online sources\nand social media for their information. Current research on fake news detection\nhas mostly focused on analyzing fake news content and how it propagates on a\nnetwork of users. In this paper, we emphasize the detection of fake news by\nassessing its credibility. By analyzing public fake news data, we show that\ninformation on news sources (and authors) can be a strong indicator of\ncredibility. Our findings suggest that an author's history of association with\nfake news, and the number of authors of a news article, can play a significant\nrole in detecting fake news. Our approach can help improve traditional fake\nnews detection methods, wherein content features are often used to detect fake\nnews.", "published": "2019-11-02 04:06:30", "link": "http://arxiv.org/abs/1911.00643v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "How to Pre-Train Your Model? Comparison of Different Pre-Training Models\n  for Biomedical Question Answering", "abstract": "Using deep learning models on small scale datasets would result in\noverfitting. To overcome this problem, the process of pre-training a model and\nfine-tuning it to the small scale dataset has been used extensively in domains\nsuch as image processing. Similarly for question answering, pre-training and\nfine-tuning can be done in several ways. Commonly reading comprehension models\nare used for pre-training, but we show that other types of pre-training can\nwork better. We compare two pre-training models based on reading comprehension\nand open domain question answering models and determine the performance when\nfine-tuned and tested over BIOASQ question answering dataset. We find open\ndomain question answering model to be a better fit for this task rather than\nreading comprehension model.", "published": "2019-11-02 13:25:15", "link": "http://arxiv.org/abs/1911.00712v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GRAPHENE: A Precise Biomedical Literature Retrieval Engine with Graph\n  Augmented Deep Learning and External Knowledge Empowerment", "abstract": "Effective biomedical literature retrieval (BLR) plays a central role in\nprecision medicine informatics. In this paper, we propose GRAPHENE, which is a\ndeep learning based framework for precise BLR. GRAPHENE consists of three main\ndifferent modules 1) graph-augmented document representation learning; 2) query\nexpansion and representation learning and 3) learning to rank biomedical\narticles. The graph-augmented document representation learning module\nconstructs a document-concept graph containing biomedical concept nodes and\ndocument nodes so that global biomedical related concept from external\nknowledge source can be captured, which is further connected to a BiLSTM so\nboth local and global topics can be explored. Query expansion and\nrepresentation learning module expands the query with abbreviations and\ndifferent names, and then builds a CNN-based model to convolve the expanded\nquery and obtain a vector representation for each query. Learning to rank\nminimizes a ranking loss between biomedical articles with the query to learn\nthe retrieval function. Experimental results on applying our system to TREC\nPrecision Medicine track data are provided to demonstrate its effectiveness.", "published": "2019-11-02 18:05:20", "link": "http://arxiv.org/abs/1911.00760v2", "categories": ["cs.IR", "cs.CL", "cs.DL"], "primary_category": "cs.IR"}
{"title": "Design and Challenges of Cloze-Style Reading Comprehension Tasks on\n  Multiparty Dialogue", "abstract": "This paper analyzes challenges in cloze-style reading comprehension on\nmultiparty dialogue and suggests two new tasks for more comprehensive\npredictions of personal entities in daily conversations. We first demonstrate\nthat there are substantial limitations to the evaluation methods of previous\nwork, namely that randomized assignment of samples to training and test data\nsubstantially decreases the complexity of cloze-style reading comprehension.\nAccording to our analysis, replacing the random data split with a chronological\ndata split reduces test accuracy on previous single-variable passage completion\ntask from 72\\% to 34\\%, that leaves much more room to improve. Our proposed\ntasks extend the previous single-variable passage completion task by replacing\nmore character mentions with variables. Several deep learning models are\ndeveloped to validate these three tasks. A thorough error analysis is provided\nto understand the challenges and guide the future direction of this research.", "published": "2019-11-02 19:19:28", "link": "http://arxiv.org/abs/1911.00773v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
