{"title": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning", "abstract": "This work explores enabling Chain-of-Thought (CoT) reasoning to link visual\ncues across multiple images. A straightforward solution is to adapt rule-based\nreinforcement learning for Vision-Language Models (VLMs). However, such methods\ntypically rely on manually curated question-answer pairs, which can be\nparticularly challenging when dealing with fine grained visual details and\ncomplex logic across images. Inspired by self-supervised visual representation\nlearning, we observe that images contain inherent constraints that can serve as\nsupervision. Based on this insight, we construct image triplets comprising two\naugmented views of the same image and a third, similar but distinct image.\nDuring training, the model is prompted to generate a reasoning process to\ncompare these images (i.e., determine same or different). Then we optimize the\nmodel with rule-based reinforcement learning. Due to the high visual similarity\nand the presence of augmentations, the model must attend to subtle visual\nchanges and perform logical reasoning to succeed. Experiments show that,\nalthough trained solely on visual comparison tasks, the learned reasoning\nability generalizes effectively to a wide range of questions. Without relying\non any human-annotated question-answer pairs, our method achieves significant\nimprovements on multi-image reasoning benchmarks and shows strong performance\non general vision tasks.", "published": "2025-06-27 17:59:27", "link": "http://arxiv.org/abs/2506.22434v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "WarpRF: Multi-View Consistency for Training-Free Uncertainty Quantification and Applications in Radiance Fields", "abstract": "We introduce WarpRF, a training-free general-purpose framework for\nquantifying the uncertainty of radiance fields. Built upon the assumption that\nphotometric and geometric consistency should hold among images rendered by an\naccurate model, WarpRF quantifies its underlying uncertainty from an unseen\npoint of view by leveraging backward warping across viewpoints, projecting\nreliable renderings to the unseen viewpoint and measuring the consistency with\nimages rendered there. WarpRF is simple and inexpensive, does not require any\ntraining, and can be applied to any radiance field implementation for free.\nWarpRF excels at both uncertainty quantification and downstream tasks, e.g.,\nactive view selection and active mapping, outperforming any existing method\ntailored to specific frameworks.", "published": "2025-06-27 17:59:13", "link": "http://arxiv.org/abs/2506.22433v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy", "abstract": "Recent advances in deep generative modeling have unlocked unprecedented\nopportunities for video synthesis. In real-world applications, however, users\noften seek tools to faithfully realize their creative editing intentions with\nprecise and consistent control. Despite the progress achieved by existing\nmethods, ensuring fine-grained alignment with user intentions remains an open\nand challenging problem. In this work, we present Shape-for-Motion, a novel\nframework that incorporates a 3D proxy for precise and consistent video\nediting. Shape-for-Motion achieves this by converting the target object in the\ninput video to a time-consistent mesh, i.e., a 3D proxy, allowing edits to be\nperformed directly on the proxy and then inferred back to the video frames. To\nsimplify the editing process, we design a novel Dual-Propagation Strategy that\nallows users to perform edits on the 3D mesh of a single frame, and the edits\nare then automatically propagated to the 3D meshes of the other frames. The 3D\nmeshes for different frames are further projected onto the 2D space to produce\nthe edited geometry and texture renderings, which serve as inputs to a\ndecoupled video diffusion model for generating edited results. Our framework\nsupports various precise and physically-consistent manipulations across the\nvideo frames, including pose editing, rotation, scaling, translation, texture\nmodification, and object composition. Our approach marks a key step toward\nhigh-quality, controllable video editing workflows. Extensive experiments\ndemonstrate the superiority and effectiveness of our approach. Project page:\nhttps://shapeformotion.github.io/", "published": "2025-06-27 17:59:01", "link": "http://arxiv.org/abs/2506.22432v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Single-shot HDR using conventional image sensor shutter functions and optical randomization", "abstract": "High-dynamic-range (HDR) imaging is an essential technique for overcoming the\ndynamic range limits of image sensors. The classic method relies on multiple\nexposures, which slows capture time, resulting in motion artifacts when imaging\ndynamic scenes. Single-shot HDR imaging alleviates this issue by encoding HDR\ndata into a single exposure, then computationally recovering it. Many\nestablished methods use strong image priors to recover improperly exposed image\ndetail. These approaches struggle with extended highlight regions. We utilize\nthe global reset release (GRR) shutter mode of an off-the-shelf sensor. GRR\nshutter mode applies a longer exposure time to rows closer to the bottom of the\nsensor. We use optics that relay a randomly permuted (shuffled) image onto the\nsensor, effectively creating spatially randomized exposures across the scene.\nThe exposure diversity allows us to recover HDR data by solving an optimization\nproblem with a simple total variation image prior. In simulation, we\ndemonstrate that our method outperforms other single-shot methods when many\nsensor pixels are saturated (10% or more), and is competitive at a modest\nsaturation (1%). Finally, we demonstrate a physical lab prototype that uses an\noff-the-shelf random fiber bundle for the optical shuffling. The fiber bundle\nis coupled to a low-cost commercial sensor operating in GRR shutter mode. Our\nprototype achieves a dynamic range of up to 73dB using an 8-bit sensor with\n48dB dynamic range.", "published": "2025-06-27 17:48:21", "link": "http://arxiv.org/abs/2506.22426v1", "categories": ["eess.IV", "cs.CV", "cs.GR", "eess.SP", "physics.optics"], "primary_category": "eess.IV"}
{"title": "Dehazing Light Microscopy Images with Guided Conditional Flow Matching: finding a sweet spot between fidelity and realism", "abstract": "Fluorescence microscopy is a major driver of scientific progress in the life\nsciences. Although high-end confocal microscopes are capable of filtering\nout-of-focus light, cheaper and more accessible microscopy modalities, such as\nwidefield microscopy, can not, which consequently leads to hazy image data.\nComputational dehazing is trying to combine the best of both worlds, leading to\ncheap microscopy but crisp-looking images. The perception-distortion trade-off\ntells us that we can optimize either for data fidelity, e.g. low MSE or high\nPSNR, or for data realism, measured by perceptual metrics such as LPIPS or FID.\nExisting methods either prioritize fidelity at the expense of realism, or\nproduce perceptually convincing results that lack quantitative accuracy. In\nthis work, we propose HazeMatching, a novel iterative method for dehazing light\nmicroscopy images, which effectively balances these objectives. Our goal was to\nfind a balanced trade-off between the fidelity of the dehazing results and the\nrealism of individual predictions (samples). We achieve this by adapting the\nconditional flow matching framework by guiding the generative process with a\nhazy observation in the conditional velocity field. We evaluate HazeMatching on\n5 datasets, covering both synthetic and real data, assessing both distortion\nand perceptual quality. Our method is compared against 7 baselines, achieving a\nconsistent balance between fidelity and realism on average. Additionally, with\ncalibration analysis, we show that HazeMatching produces well-calibrated\npredictions. Note that our method does not need an explicit degradation\noperator to exist, making it easily applicable on real microscopy data. All\ndata used for training and evaluation and our code will be publicly available\nunder a permissive license.", "published": "2025-06-27 17:10:43", "link": "http://arxiv.org/abs/2506.22397v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Test-Time Consistency in Vision Language Models", "abstract": "Vision-Language Models (VLMs) have achieved impressive performance across a\nwide range of multimodal tasks, yet they often exhibit inconsistent behavior\nwhen faced with semantically equivalent inputs, undermining their reliability\nand robustness. Recent benchmarks, such as MM-R3, highlight that even\nstate-of-the-art VLMs can produce divergent predictions across semantically\nequivalent inputs, despite maintaining high average accuracy. Prior work\naddresses this issue by modifying model architectures or conducting large-scale\nfine-tuning on curated datasets. In contrast, we propose a simple and effective\ntest-time consistency framework that enhances semantic consistency without\nsupervised re-training. Our method is entirely post-hoc, model-agnostic, and\napplicable to any VLM with access to its weights. Given a single test point, we\nenforce consistent predictions via two complementary objectives: (i) a\nCross-Entropy Agreement Loss that aligns predictive distributions across\nsemantically equivalent inputs, and (ii) a Pseudo-Label Consistency Loss that\ndraws outputs toward a self-averaged consensus. Our method is plug-and-play and\nleverages information from a single test input itself to improve consistency.\nExperiments on the MM-R3 benchmark show that our framework yields substantial\ngains in consistency across state-of-the-art models, establishing a new\ndirection for inference-time adaptation in multimodal learning.", "published": "2025-06-27 17:09:44", "link": "http://arxiv.org/abs/2506.22395v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment", "abstract": "Video Large Multimodal Models (VLMMs) have made impressive strides in\nunderstanding video content, but they often struggle with abstract and adaptive\nreasoning-the ability to revise their interpretations when new information\nemerges. In reality, conclusions are rarely set in stone; additional context\ncan strengthen or weaken an initial inference. To address this, we introduce\nDefeasible Video Entailment (DVidE), a new task that challenges models to think\nlike doubters, constantly updating their reasoning based on evolving evidence.\nIn DVidE, given a video premise and a textual hypothesis, models must determine\nwhether a new update strengthens or weakens the hypothesis (classification\nversion) or generate a coherent update that modifies the entailment\nrelationship (generation version). For solving the classification task, we\npropose the Chain of Counterfactual Thought framework, utilizing counterfactual\nreasoning, ASR-enhanced video content, and rationale refinement to reduce\ninference bias. For the generation task, we develop a framework that combines\nASR output with a Large Language Model (LLM) to produce coherent, contextually\nrelevant updates aligned with the intended strengthener or weakener goals.\nAdditionally, we introduce a novel benchmark dataset, with\nstrengthener/weakener annotations and an LLM-based evaluation metric\nspecifically designed for assessing generative performance. Experimental\nresults demonstrate significant improvements, highlighting our proposed method\nin enhancing dynamic reasoning capabilities of VLMMs.", "published": "2025-06-27 16:51:15", "link": "http://arxiv.org/abs/2506.22385v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Exploiting Vision Language Model for Training-Free 3D Point Cloud OOD Detection via Graph Score Propagation", "abstract": "Out-of-distribution (OOD) detection in 3D point cloud data remains a\nchallenge, particularly in applications where safe and robust perception is\ncritical. While existing OOD detection methods have shown progress for 2D image\ndata, extending these to 3D environments involves unique obstacles. This paper\nintroduces a training-free framework that leverages Vision-Language Models\n(VLMs) for effective OOD detection in 3D point clouds. By constructing a graph\nbased on class prototypes and testing data, we exploit the data manifold\nstructure to enhancing the effectiveness of VLMs for 3D OOD detection. We\npropose a novel Graph Score Propagation (GSP) method that incorporates prompt\nclustering and self-training negative prompting to improve OOD scoring with\nVLM. Our method is also adaptable to few-shot scenarios, providing options for\npractical applications. We demonstrate that GSP consistently outperforms\nstate-of-the-art methods across synthetic and real-world datasets 3D point\ncloud OOD detection.", "published": "2025-06-27 16:42:45", "link": "http://arxiv.org/abs/2506.22375v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based Vehicle Classification with Potential UAV Applications", "abstract": "This study investigates the performance of the two most relevant computer\nvision deep learning architectures, Convolutional Neural Network and Vision\nTransformer, for event-based cameras. These cameras capture scene changes,\nunlike traditional frame-based cameras with capture static images, and are\nparticularly suited for dynamic environments such as UAVs and autonomous\nvehicles. The deep learning models studied in this work are ResNet34 and ViT\nB16, fine-tuned on the GEN1 event-based dataset. The research evaluates and\ncompares these models under both standard conditions and in the presence of\nsimulated noise. Initial evaluations on the clean GEN1 dataset reveal that\nResNet34 and ViT B16 achieve accuracies of 88% and 86%, respectively, with\nResNet34 showing a slight advantage in classification accuracy. However, the\nViT B16 model demonstrates notable robustness, particularly given its\npre-training on a smaller dataset. Although this study focuses on ground-based\nvehicle classification, the methodologies and findings hold significant promise\nfor adaptation to UAV contexts, including aerial object classification and\nevent-based vision systems for aviation-related tasks.", "published": "2025-06-27 16:21:00", "link": "http://arxiv.org/abs/2506.22360v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Closing the Performance Gap in Biometric Cryptosystems: A Deeper Analysis on Unlinkable Fuzzy Vaults", "abstract": "This paper analyses and addresses the performance gap in the fuzzy\nvault-based \\ac{BCS}. We identify unstable error correction capabilities, which\nare caused by variable feature set sizes and their influence on similarity\nthresholds, as a key source of performance degradation. This issue is further\ncompounded by information loss introduced through feature type transformations.\nTo address both problems, we propose a novel feature quantization method based\non \\it{equal frequent intervals}. This method guarantees fixed feature set\nsizes and supports training-free adaptation to any number of intervals. The\nproposed approach significantly reduces the performance gap introduced by\ntemplate protection. Additionally, it integrates seamlessly with existing\nsystems to minimize the negative effects of feature transformation. Experiments\non state-of-the-art face, fingerprint, and iris recognition systems confirm\nthat only minimal performance degradation remains, demonstrating the\neffectiveness of the method across major biometric modalities.", "published": "2025-06-27 15:57:58", "link": "http://arxiv.org/abs/2506.22347v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "QuKAN: A Quantum Circuit Born Machine approach to Quantum Kolmogorov Arnold Networks", "abstract": "Kolmogorov Arnold Networks (KANs), built upon the Kolmogorov Arnold\nrepresentation theorem (KAR), have demonstrated promising capabilities in\nexpressing complex functions with fewer neurons. This is achieved by\nimplementing learnable parameters on the edges instead of on the nodes, unlike\ntraditional networks such as Multi-Layer Perceptrons (MLPs). However, KANs\npotential in quantum machine learning has not yet been well explored. In this\nwork, we present an implementation of these KAN architectures in both hybrid\nand fully quantum forms using a Quantum Circuit Born Machine (QCBM). We adapt\nthe KAN transfer using pre-trained residual functions, thereby exploiting the\nrepresentational power of parametrized quantum circuits. In the hybrid model we\ncombine classical KAN components with quantum subroutines, while the fully\nquantum version the entire architecture of the residual function is translated\nto a quantum model. We demonstrate the feasibility, interpretability and\nperformance of the proposed Quantum KAN (QuKAN) architecture.", "published": "2025-06-27 15:51:19", "link": "http://arxiv.org/abs/2506.22340v1", "categories": ["quant-ph", "cs.CV", "cs.LG"], "primary_category": "quant-ph"}
{"title": "A Deep Learning framework for building damage assessment using VHR SAR and geospatial data: demonstration on the 2023 Turkiye Earthquake", "abstract": "Building damage identification shortly after a disaster is crucial for\nguiding emergency response and recovery efforts. Although optical satellite\nimagery is commonly used for disaster mapping, its effectiveness is often\nhampered by cloud cover or the absence of pre-event acquisitions. To overcome\nthese challenges, we introduce a novel multimodal deep learning (DL) framework\nfor detecting building damage using single-date very high resolution (VHR)\nSynthetic Aperture Radar (SAR) imagery from the Italian Space Agency (ASI)\nCOSMO SkyMed (CSK) constellation, complemented by auxiliary geospatial data.\nOur method integrates SAR image patches, OpenStreetMap (OSM) building\nfootprints, digital surface model (DSM) data, and structural and exposure\nattributes from the Global Earthquake Model (GEM) to improve detection accuracy\nand contextual interpretation. Unlike existing approaches that depend on pre\nand post event imagery, our model utilizes only post event data, facilitating\nrapid deployment in critical scenarios. The framework effectiveness is\ndemonstrated using a new dataset from the 2023 earthquake in Turkey, covering\nmultiple cities with diverse urban settings. Results highlight that\nincorporating geospatial features significantly enhances detection performance\nand generalizability to previously unseen areas. By combining SAR imagery with\ndetailed vulnerability and exposure information, our approach provides reliable\nand rapid building damage assessments without the dependency from available\npre-event data. Moreover, the automated and scalable data generation process\nensures the framework's applicability across diverse disaster-affected regions,\nunderscoring its potential to support effective disaster management and\nrecovery efforts. Code and data will be made available upon acceptance of the\npaper.", "published": "2025-06-27 15:49:58", "link": "http://arxiv.org/abs/2506.22338v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "MatChA: Cross-Algorithm Matching with Feature Augmentation", "abstract": "State-of-the-art methods fail to solve visual localization in scenarios where\ndifferent devices use different sparse feature extraction algorithms to obtain\nkeypoints and their corresponding descriptors. Translating feature descriptors\nis enough to enable matching. However, performance is drastically reduced in\ncross-feature detector cases, because current solutions assume common\nkeypoints. This means that the same detector has to be used, which is rarely\nthe case in practice when different descriptors are used. The low repeatability\nof keypoints, in addition to non-discriminatory and non-distinctive\ndescriptors, make the identification of true correspondences extremely\nchallenging. We present the first method tackling this problem, which performs\nfeature descriptor augmentation targeting cross-detector feature matching, and\nthen feature translation to a latent space. We show that our method\nsignificantly improves image matching and visual localization in the\ncross-feature scenario and evaluate the proposed method on several benchmarks.", "published": "2025-06-27 15:43:51", "link": "http://arxiv.org/abs/2506.22336v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Unfolding Generative Flows with Koopman Operators: Fast and Interpretable Sampling", "abstract": "Conditional Flow Matching (CFM) offers a simulation-free framework for\ntraining continuous-time generative models, bridging diffusion and flow-based\napproaches. However, sampling from CFM still relies on numerically solving\nnon-linear ODEs which can be computationally expensive and difficult to\ninterpret. Recent alternatives address sampling speed via trajectory\nstraightening, mini-batch coupling or distillation. However, these methods\ntypically do not shed light on the underlying \\textit{structure} of the\ngenerative process. In this work, we propose to accelerate CFM and introduce an\ninterpretable representation of its dynamics by integrating Koopman operator\ntheory, which models non-linear flows as linear evolution in a learned space of\nobservables. We introduce a decoder-free Koopman-CFM architecture that learns\nan embedding where the generative dynamics become linear, enabling closed-form,\none-step sampling via matrix exponentiation. This results in significant\nspeedups over traditional CFM as demonstrated on controlled 2D datasets and\nreal-world benchmarks, MNIST, Fashion-MNIST (F-MNIST), and the Toronto Face\nDataset (TFD). Unlike previous methods, our approach leads to a well-structured\nKoopman generator, whose spectral properties, eigenvalues, and eigenfunctions\noffer principled tools for analyzing generative behavior such as temporal\nscaling, mode stability, and decomposition in Koopman latent space. By\ncombining sampling efficiency with analytical structure, Koopman-enhanced flow\nmatching offers a potential step toward fast and interpretable generative\nmodeling.", "published": "2025-06-27 15:16:16", "link": "http://arxiv.org/abs/2506.22304v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "OutDreamer: Video Outpainting with a Diffusion Transformer", "abstract": "Video outpainting is a challenging task that generates new video content by\nextending beyond the boundaries of an original input video, requiring both\ntemporal and spatial consistency. Many state-of-the-art methods utilize latent\ndiffusion models with U-Net backbones but still struggle to achieve high\nquality and adaptability in generated content. Diffusion transformers (DiTs)\nhave emerged as a promising alternative because of their superior performance.\nWe introduce OutDreamer, a DiT-based video outpainting framework comprising two\nmain components: an efficient video control branch and a conditional\noutpainting branch. The efficient video control branch effectively extracts\nmasked video information, while the conditional outpainting branch generates\nmissing content based on these extracted conditions. Additionally, we propose a\nmask-driven self-attention layer that dynamically integrates the given mask\ninformation, further enhancing the model's adaptability to outpainting tasks.\nFurthermore, we introduce a latent alignment loss to maintain overall\nconsistency both within and between frames. For long video outpainting, we\nemploy a cross-video-clip refiner to iteratively generate missing content,\nensuring temporal consistency across video clips. Extensive evaluations\ndemonstrate that our zero-shot OutDreamer outperforms state-of-the-art\nzero-shot methods on widely recognized benchmarks.", "published": "2025-06-27 15:08:54", "link": "http://arxiv.org/abs/2506.22298v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "RoomCraft: Controllable and Complete 3D Indoor Scene Generation", "abstract": "Generating realistic 3D indoor scenes from user inputs remains a challenging\nproblem in computer vision and graphics, requiring careful balance of geometric\nconsistency, spatial relationships, and visual realism. While neural generation\nmethods often produce repetitive elements due to limited global spatial\nreasoning, procedural approaches can leverage constraints for controllable\ngeneration but struggle with multi-constraint scenarios. When constraints\nbecome numerous, object collisions frequently occur, forcing the removal of\nfurniture items and compromising layout completeness.\n  To address these limitations, we propose RoomCraft, a multi-stage pipeline\nthat converts real images, sketches, or text descriptions into coherent 3D\nindoor scenes. Our approach combines a scene generation pipeline with a\nconstraint-driven optimization framework. The pipeline first extracts\nhigh-level scene information from user inputs and organizes it into a\nstructured format containing room type, furniture items, and spatial relations.\nIt then constructs a spatial relationship network to represent furniture\narrangements and generates an optimized placement sequence using a\nheuristic-based depth-first search (HDFS) algorithm to ensure layout coherence.\nTo handle complex multi-constraint scenarios, we introduce a unified constraint\nrepresentation that processes both formal specifications and natural language\ninputs, enabling flexible constraint-oriented adjustments through a\ncomprehensive action space design. Additionally, we propose a Conflict-Aware\nPositioning Strategy (CAPS) that dynamically adjusts placement weights to\nminimize furniture collisions and ensure layout completeness.\n  Extensive experiments demonstrate that RoomCraft significantly outperforms\nexisting methods in generating realistic, semantically coherent, and visually\nappealing room layouts across diverse input modalities.", "published": "2025-06-27 15:03:17", "link": "http://arxiv.org/abs/2506.22291v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Rethinking Visual Token Reduction in LVLMs under Cross-modal Misalignment", "abstract": "Large Vision-Language Models (LVLMs) encode visual inputs as dense sequences\nof patch-level tokens to capture fine-grained semantics. These visual tokens\noften outnumber their textual counterparts by a large margin, leading to\nsubstantial computational overhead and limiting the scalability of LVLMs in\npractice. Previous efforts have explored visual token reduction either prior to\nor within the large language models (LLM). However, most in-LLM reduction\napproaches rely on text-conditioned interactions, implicitly assuming that\ntextual tokens can reliably capture the importance of visual tokens. In this\nwork, we revisit this assumption and reveal causal, semantic, and spatial forms\nof cross-modal misalignment. These misalignments undermine the effectiveness of\ntext-guided visual token reduction. To address this, we introduce VisionDrop, a\ntraining-free, visual-only pruning framework that selects informative visual\ntokens based on intra-modal (visual-to-visual) attention, without relying on\ntextual signals. To further suppress redundancy throughout the model hierarchy,\nwe treat the visual encoder and the LLM as a unified system and design a\nprogressive pruning pipeline. Our method performs dominant token selection and\nlightweight contextual merging at multiple stages, enabling fine-grained visual\ninformation to be retained even under aggressive token budgets. Extensive\nexperiments across diverse benchmarks show that VisionDrop achieves consistent\nimprovements over existing methods, despite requiring no additional training or\ncomplex modifications. Its simple yet effective design enables efficient\ninference while preserving strong performance across tasks.", "published": "2025-06-27 14:55:40", "link": "http://arxiv.org/abs/2506.22283v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "DIGS: Dynamic CBCT Reconstruction using Deformation-Informed 4D Gaussian Splatting and a Low-Rank Free-Form Deformation Model", "abstract": "3D Cone-Beam CT (CBCT) is widely used in radiotherapy but suffers from motion\nartifacts due to breathing. A common clinical approach mitigates this by\nsorting projections into respiratory phases and reconstructing images per\nphase, but this does not account for breathing variability. Dynamic CBCT\ninstead reconstructs images at each projection, capturing continuous motion\nwithout phase sorting. Recent advancements in 4D Gaussian Splatting (4DGS)\noffer powerful tools for modeling dynamic scenes, yet their application to\ndynamic CBCT remains underexplored. Existing 4DGS methods, such as HexPlane,\nuse implicit motion representations, which are computationally expensive. While\nexplicit low-rank motion models have been proposed, they lack spatial\nregularization, leading to inconsistencies in Gaussian motion. To address these\nlimitations, we introduce a free-form deformation (FFD)-based spatial basis\nfunction and a deformation-informed framework that enforces consistency by\ncoupling the temporal evolution of Gaussian's mean position, scale, and\nrotation under a unified deformation field. We evaluate our approach on six\nCBCT datasets, demonstrating superior image quality with a 6x speedup over\nHexPlane. These results highlight the potential of deformation-informed 4DGS\nfor efficient, motion-compensated CBCT reconstruction. The code is available at\nhttps://github.com/Yuliang-Huang/DIGS.", "published": "2025-06-27 14:48:59", "link": "http://arxiv.org/abs/2506.22280v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "COOCO -- Common Objects Out-of-Context -- Semantic Violation in Scenes: Investigating Multimodal Context in Referential Communication", "abstract": "Natural scenes provide us with rich contexts for object recognition and\nreference. In particular, knowing what type of scene one is looking at\ngenerates expectations about which objects will occur, and what their spatial\nconfiguration should be. Do Vision-Language Models (VLMs) learn to rely on\nscene contexts in a similar way, when generating references to objects? To\naddress this question, we introduce the \\textit{Common Objects Out-of-Context\n(COOCO)} dataset and test to what extent VLMs rely on scene context to refer to\nobjects under different degrees of scene-object congruency, and different\nperturbations. Our findings show that models leverage scene context adaptively,\ndepending on both the semantic relatedness between object and scene and the\nlevel of noise. In particular, models rely more on context under high\ntarget-scene congruence or when objects are degraded. Attention analysis\nreveals that successful object categorisation involves increased focus on the\ntarget in mid-level layers, especially under moderate noise, suggesting that\nVLMs dynamically balance local and contextual information for reference\ngeneration. We make our dataset, code and models available at\n\\href{https://github.com/cs-nlp-uu/scenereg}{https://github.com/cs-nlp-uu/scenereg}.", "published": "2025-06-27 14:44:45", "link": "http://arxiv.org/abs/2506.22274v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "EAMamba: Efficient All-Around Vision State Space Model for Image Restoration", "abstract": "Image restoration is a key task in low-level computer vision that aims to\nreconstruct high-quality images from degraded inputs. The emergence of Vision\nMamba, which draws inspiration from the advanced state space model Mamba, marks\na significant advancement in this field. Vision Mamba demonstrates excellence\nin modeling long-range dependencies with linear complexity, a crucial advantage\nfor image restoration tasks. Despite its strengths, Vision Mamba encounters\nchallenges in low-level vision tasks, including computational complexity that\nscales with the number of scanning sequences and local pixel forgetting. To\naddress these limitations, this study introduces Efficient All-Around Mamba\n(EAMamba), an enhanced framework that incorporates a Multi-Head Selective Scan\nModule (MHSSM) with an all-around scanning mechanism. MHSSM efficiently\naggregates multiple scanning sequences, which avoids increases in computational\ncomplexity and parameter count. The all-around scanning strategy implements\nmultiple patterns to capture holistic information and resolves the local pixel\nforgetting issue. Our experimental evaluations validate these innovations\nacross several restoration tasks, including super resolution, denoising,\ndeblurring, and dehazing. The results validate that EAMamba achieves a\nsignificant 31-89% reduction in FLOPs while maintaining favorable performance\ncompared to existing low-level Vision Mamba methods.", "published": "2025-06-27 14:12:58", "link": "http://arxiv.org/abs/2506.22246v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration", "abstract": "Leveraging diverse robotic data for pretraining remains a critical challenge.\nExisting methods typically model the dataset's action distribution using simple\nobservations as inputs. However, these inputs are often incomplete, resulting\nin a dispersed conditional action distribution-an issue we refer to as\ncoordinate system chaos and state chaos. This inconsistency significantly\nhampers pretraining efficiency. To address this, we propose 4D-VLA, a novel\napproach that effectively integrates 4D information into the input to mitigate\nthese sources of chaos. Our model introduces depth and temporal information\ninto visual features with sequential RGB-D inputs, aligning the coordinate\nsystems of the robot and the scene. This alignment endows the model with strong\nspatiotemporal reasoning capabilities while minimizing training overhead.\nAdditionally, we introduce memory bank sampling, a frame sampling strategy\ndesigned to extract informative frames from historical images, further\nimproving effectiveness and efficiency. Experimental results demonstrate that\nour pretraining method and architectural components substantially enhance model\nperformance. In both simulated and real-world experiments, our model achieves a\nsignificant increase in success rate over OpenVLA. To further assess spatial\nperception and generalization to novel views, we introduce MV-Bench, a\nmulti-view simulation benchmark. Our model consistently outperforms existing\nmethods, demonstrating stronger spatial understanding and adaptability.", "published": "2025-06-27 14:09:29", "link": "http://arxiv.org/abs/2506.22242v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Boosting Classification with Quantum-Inspired Augmentations", "abstract": "Understanding the impact of small quantum gate perturbations, which are\ncommon in quantum digital devices but absent in classical computers, is crucial\nfor identifying potential advantages in quantum machine learning. While these\nperturbations are typically seen as detrimental to quantum computation, they\ncan actually enhance performance by serving as a natural source of data\naugmentation. Additionally, they can often be efficiently simulated on\nclassical hardware, enabling quantum-inspired approaches to improve classical\nmachine learning methods. In this paper, we investigate random Bloch sphere\nrotations, which are fundamental SU(2) transformations, as a simple yet\neffective quantum-inspired data augmentation technique. Unlike conventional\naugmentations such as flipping, rotating, or cropping, quantum transformations\nlack intuitive spatial interpretations, making their application to tasks like\nimage classification less straightforward. While common quantum augmentation\nmethods rely on applying quantum models or trainable quanvolutional layers to\nclassical datasets, we focus on the direct application of small-angle Bloch\nrotations and their effect on classical data. Using the large-scale ImageNet\ndataset, we demonstrate that our quantum-inspired augmentation method improves\nimage classification performance, increasing Top-1 accuracy by 3%, Top-5\naccuracy by 2.5%, and the F$_1$ score from 8% to 12% compared to standard\nclassical augmentation methods. Finally, we examine the use of stronger unitary\naugmentations. Although these transformations preserve information in\nprinciple, they result in visually unrecognizable images with potential\napplications for privacy computations. However, we show that our augmentation\napproach and simple SU(2) transformations do not enhance differential privacy\nand discuss the implications of this limitation.", "published": "2025-06-27 14:08:43", "link": "http://arxiv.org/abs/2506.22241v1", "categories": ["cs.CV", "cond-mat.dis-nn", "cs.LG", "quant-ph"], "primary_category": "cs.CV"}
{"title": "Cardiovascular disease classification using radiomics and geometric features from cardiac CT", "abstract": "Automatic detection and classification of Cardiovascular disease (CVD) from\nComputed Tomography (CT) images play an important part in facilitating\nbetter-informed clinical decisions. However, most of the recent deep learning\nbased methods either directly work on raw CT data or utilize it in pair with\nanatomical cardiac structure segmentation by training an end-to-end classifier.\nAs such, these approaches become much more difficult to interpret from a\nclinical perspective. To address this challenge, in this work, we break down\nthe CVD classification pipeline into three components: (i) image segmentation,\n(ii) image registration, and (iii) downstream CVD classification. Specifically,\nwe utilize the Atlas-ISTN framework and recent segmentation foundational models\nto generate anatomical structure segmentation and a normative healthy atlas.\nThese are further utilized to extract clinically interpretable radiomic\nfeatures as well as deformation field based geometric features (through atlas\nregistration) for CVD classification. Our experiments on the publicly available\nASOCA dataset show that utilizing these features leads to better CVD\nclassification accuracy (87.50\\%) when compared against classification model\ntrained directly on raw CT images (67.50\\%). Our code is publicly available:\nhttps://github.com/biomedia-mira/grc-net", "published": "2025-06-27 13:43:05", "link": "http://arxiv.org/abs/2506.22226v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Advanced Deep Learning Techniques for Automated Segmentation of Type B Aortic Dissections", "abstract": "Purpose: Aortic dissections are life-threatening cardiovascular conditions\nrequiring accurate segmentation of true lumen (TL), false lumen (FL), and false\nlumen thrombosis (FLT) from CTA images for effective management. Manual\nsegmentation is time-consuming and variable, necessitating automated solutions.\nMaterials and Methods: We developed four deep learning-based pipelines for Type\nB aortic dissection segmentation: a single-step model, a sequential model, a\nsequential multi-task model, and an ensemble model, utilizing 3D U-Net and\nSwin-UnetR architectures. A dataset of 100 retrospective CTA images was split\ninto training (n=80), validation (n=10), and testing (n=10). Performance was\nassessed using the Dice Coefficient and Hausdorff Distance. Results: Our\napproach achieved superior segmentation accuracy, with Dice Coefficients of\n0.91 $\\pm$ 0.07 for TL, 0.88 $\\pm$ 0.18 for FL, and 0.47 $\\pm$ 0.25 for FLT,\noutperforming Yao et al. (1), who reported 0.78 $\\pm$ 0.20, 0.68 $\\pm$ 0.18,\nand 0.25 $\\pm$ 0.31, respectively. Conclusion: The proposed pipelines provide\naccurate segmentation of TBAD features, enabling derivation of morphological\nparameters for surveillance and treatment planning", "published": "2025-06-27 13:38:33", "link": "http://arxiv.org/abs/2506.22222v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "ReF-LLE: Personalized Low-Light Enhancement via Reference-Guided Deep Reinforcement Learning", "abstract": "Low-light image enhancement presents two primary challenges: 1) Significant\nvariations in low-light images across different conditions, and 2) Enhancement\nlevels influenced by subjective preferences and user intent. To address these\nissues, we propose ReF-LLE, a novel personalized low-light image enhancement\nmethod that operates in the Fourier frequency domain and incorporates deep\nreinforcement learning. ReF-LLE is the first to integrate deep reinforcement\nlearning into this domain. During training, a zero-reference image evaluation\nstrategy is introduced to score enhanced images, providing reward signals that\nguide the model to handle varying degrees of low-light conditions effectively.\nIn the inference phase, ReF-LLE employs a personalized adaptive iterative\nstrategy, guided by the zero-frequency component in the Fourier domain, which\nrepresents the overall illumination level. This strategy enables the model to\nadaptively adjust low-light images to align with the illumination distribution\nof a user-provided reference image, ensuring personalized enhancement results.\nExtensive experiments on benchmark datasets demonstrate that ReF-LLE\noutperforms state-of-the-art methods, achieving superior perceptual quality and\nadaptability in personalized low-light image enhancement.", "published": "2025-06-27 13:35:34", "link": "http://arxiv.org/abs/2506.22216v1", "categories": ["cs.CV", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Robust and Accurate Multi-view 2D/3D Image Registration with Differentiable X-ray Rendering and Dual Cross-view Constraints", "abstract": "Robust and accurate 2D/3D registration, which aligns preoperative models with\nintraoperative images of the same anatomy, is crucial for successful\ninterventional navigation. To mitigate the challenge of a limited field of view\nin single-image intraoperative scenarios, multi-view 2D/3D registration is\nrequired by leveraging multiple intraoperative images. In this paper, we\npropose a novel multi-view 2D/3D rigid registration approach comprising two\nstages. In the first stage, a combined loss function is designed, incorporating\nboth the differences between predicted and ground-truth poses and the\ndissimilarities (e.g., normalized cross-correlation) between simulated and\nobserved intraoperative images. More importantly, additional cross-view\ntraining loss terms are introduced for both pose and image losses to explicitly\nenforce cross-view constraints. In the second stage, test-time optimization is\nperformed to refine the estimated poses from the coarse stage. Our method\nexploits the mutual constraints of multi-view projection poses to enhance the\nrobustness of the registration process. The proposed framework achieves a mean\ntarget registration error (mTRE) of $0.79 \\pm 2.17$ mm on six specimens from\nthe DeepFluoro dataset, demonstrating superior performance compared to\nstate-of-the-art registration algorithms.", "published": "2025-06-27 12:57:58", "link": "http://arxiv.org/abs/2506.22191v1", "categories": ["cs.CV", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Frequency-Semantic Enhanced Variational Autoencoder for Zero-Shot Skeleton-based Action Recognition", "abstract": "Zero-shot skeleton-based action recognition aims to develop models capable of\nidentifying actions beyond the categories encountered during training. Previous\napproaches have primarily focused on aligning visual and semantic\nrepresentations but often overlooked the importance of fine-grained action\npatterns in the semantic space (e.g., the hand movements in drinking water and\nbrushing teeth). To address these limitations, we propose a Frequency-Semantic\nEnhanced Variational Autoencoder (FS-VAE) to explore the skeleton semantic\nrepresentation learning with frequency decomposition. FS-VAE consists of three\nkey components: 1) a frequency-based enhancement module with high- and\nlow-frequency adjustments to enrich the skeletal semantics learning and improve\nthe robustness of zero-shot action recognition; 2) a semantic-based action\ndescription with multilevel alignment to capture both local details and global\ncorrespondence, effectively bridging the semantic gap and compensating for the\ninherent loss of information in skeleton sequences; 3) a calibrated\ncross-alignment loss that enables valid skeleton-text pairs to counterbalance\nambiguous ones, mitigating discrepancies and ambiguities in skeleton and text\nfeatures, thereby ensuring robust alignment. Evaluations on the benchmarks\ndemonstrate the effectiveness of our approach, validating that\nfrequency-enhanced semantic features enable robust differentiation of visually\nand semantically similar action clusters, improving zero-shot action\nrecognition.", "published": "2025-06-27 12:44:08", "link": "http://arxiv.org/abs/2506.22179v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "KnotDLO: Toward Interpretable Knot Tying", "abstract": "This work presents KnotDLO, a method for one-handed Deformable Linear Object\n(DLO) knot tying that is robust to occlusion, repeatable for varying rope\ninitial configurations, interpretable for generating motion policies, and\nrequires no human demonstrations or training. Grasp and target waypoints for\nfuture DLO states are planned from the current DLO shape. Grasp poses are\ncomputed from indexing the tracked piecewise linear curve representing the DLO\nstate based on the current curve shape and are piecewise continuous. KnotDLO\ncomputes intermediate waypoints from the geometry of the current DLO state and\nthe desired next state. The system decouples visual reasoning from control. In\n16 trials of knot tying, KnotDLO achieves a 50% success rate in tying an\noverhand knot from previously unseen configurations.", "published": "2025-06-27 12:43:05", "link": "http://arxiv.org/abs/2506.22176v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Attention-disentangled Uniform Orthogonal Feature Space Optimization for Few-shot Object Detection", "abstract": "Few-shot object detection (FSOD) aims to detect objects with limited samples\nfor novel classes, while relying on abundant data for base classes. Existing\nFSOD approaches, predominantly built on the Faster R-CNN detector, entangle\nobjectness recognition and foreground classification within shared feature\nspaces. This paradigm inherently establishes class-specific objectness criteria\nand suffers from unrepresentative novel class samples. To resolve this\nlimitation, we propose a Uniform Orthogonal Feature Space (UOFS) optimization\nframework. First, UOFS decouples the feature space into two orthogonal\ncomponents, where magnitude encodes objectness and angle encodes\nclassification. This decoupling enables transferring class-agnostic objectness\nknowledge from base classes to novel classes. Moreover, implementing the\ndisentanglement requires careful attention to two challenges: (1) Base set\nimages contain unlabeled foreground instances, causing confusion between\npotential novel class instances and backgrounds. (2) Angular optimization\ndepends exclusively on base class foreground instances, inducing overfitting of\nangular distributions to base classes. To address these challenges, we propose\na Hybrid Background Optimization (HBO) strategy: (1) Constructing a pure\nbackground base set by removing unlabeled instances in original images to\nprovide unbiased magnitude-based objectness supervision. (2) Incorporating\nunlabeled foreground instances in the original base set into angular\noptimization to enhance distribution uniformity. Additionally, we propose a\nSpatial-wise Attention Disentanglement and Association (SADA) module to address\ntask conflicts between class-agnostic and class-specific tasks. Experiments\ndemonstrate that our method significantly outperforms existing approaches based\non entangled feature spaces.", "published": "2025-06-27 12:17:04", "link": "http://arxiv.org/abs/2506.22161v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Hardware acceleration for ultra-fast Neural Network training on FPGA for MRF map reconstruction", "abstract": "Magnetic Resonance Fingerprinting (MRF) is a fast quantitative MR Imaging\ntechnique that provides multi-parametric maps with a single acquisition. Neural\nNetworks (NNs) accelerate reconstruction but require significant resources for\ntraining. We propose an FPGA-based NN for real-time brain parameter\nreconstruction from MRF data. Training the NN takes an estimated 200 seconds,\nsignificantly faster than standard CPU-based training, which can be up to 250\ntimes slower. This method could enable real-time brain analysis on mobile\ndevices, revolutionizing clinical decision-making and telemedicine.", "published": "2025-06-27 12:09:35", "link": "http://arxiv.org/abs/2506.22156v1", "categories": ["cs.AR", "cs.CV", "physics.ins-det"], "primary_category": "cs.AR"}
{"title": "RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation Models", "abstract": "The rise of imaging techniques such as optical coherence tomography (OCT) and\nadvances in deep learning (DL) have enabled clinicians and researchers to\nstreamline retinal disease staging. A popular DL approach is self-supervised\nlearning (SSL), where models learn from vast amounts of unlabeled data,\navoiding costly annotation. SSL has allowed the development of foundation\nmodels (FMs), large models that can be used for a variety of downstream tasks.\nHowever, existing FMs for OCT, trained solely on image data, lack a\ncomprehensive and robust semantic understanding of images, as evidenced by\ntheir downstream performance (especially for complex tasks), and thus require\nsupervised fine-tuning (which may be unfeasible) to better adapt to specific\napplications and populations. To address this, we propose RetFiner, an SSL\nvision-language refinement scheme that improves the representations of existing\nFMs and enables their efficient and direct adaptation to specific populations\nfor improved downstream performance. Our method uses a diverse set of training\nobjectives which take advantage of the rich supervisory signal found in textual\ndata. We tested RetFiner on the retinal FMs RETFound, UrFound, and VisionFM,\nshowing significant improvements in linear probing performance on seven highly\ndiverse OCT classification tasks, with an average increase of 5.8, 3.9, and 2.1\npercentage points over their baselines, respectively. Our code and model\nweights are publicly available at https://github.com/ronnief1/RetFiner.", "published": "2025-06-27 11:53:54", "link": "http://arxiv.org/abs/2506.22149v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs", "abstract": "Despite progress in Vision-Language Models (VLMs), their capacity for visual\nreasoning is often limited by the \\textit{binding problem}: the failure to\nreliably associate perceptual features with their correct visual referents.\nThis limitation underlies persistent errors in tasks such as counting, visual\nsearch, scene description, and spatial relationship understanding. A key factor\nis that current VLMs process visual features largely in parallel, lacking\nmechanisms for spatially grounded, serial attention. This paper introduces a\nsimple yet effective intervention: augmenting visual inputs with low-level\nspatial structures (e.g., horizontal lines) and pairing this with a textual\nprompt that encourages sequential, spatially-aware parsing. We empirically\ndemonstrate substantial performance improvements across core visual reasoning\ntasks. Specifically, our method improves GPT-4o visual search accuracy by\n25.00%, increases counting accuracy by 26.83%, reduces edit distance error in\nscene description by 0.32, and enhances performance on spatial relationship\ntasks by 9.50% on a a 2D synthetic dataset. Furthermore, we find that the\nvisual modification is essential for these gains; purely textual strategies,\nincluding Chain-of-Thought prompting, are insufficient and can even degrade\nperformance. Our method enhances binding only with a single-query inference,\nunderscoring the importance of visual input design over purely\nlinguistically-based approaches. These findings suggest that low-level visual\nstructuring is a powerful and underexplored direction for improving\ncompositional visual reasoning and could serve as a general strategy for\nenhancing VLM performance on spatially grounded tasks.", "published": "2025-06-27 11:44:40", "link": "http://arxiv.org/abs/2506.22146v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs", "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated significant\nsuccess in visual understanding tasks. However, challenges persist in adapting\nthese models for video comprehension due to the large volume of data and\ntemporal complexity. Existing Video-LLMs using uniform frame sampling often\nstruggle to capture the query-related crucial spatiotemporal clues of videos\neffectively. In this paper, we introduce Q-Frame, a novel approach for adaptive\nframe selection and multi-resolution scaling tailored to the video's content\nand the specific query. Q-Frame employs a training-free, plug-and-play strategy\ngenerated by a text-image matching network like CLIP, utilizing the Gumbel-Max\ntrick for efficient frame selection. Q-Frame allows Video-LLMs to process more\nframes without exceeding computational limits, thereby preserving critical\ntemporal and spatial information. We demonstrate Q-Frame's effectiveness\nthrough extensive experiments on benchmark datasets, including MLVU,\nLongVideoBench, and Video-MME, illustrating its superiority over existing\nmethods and its applicability across various video understanding tasks.", "published": "2025-06-27 11:30:51", "link": "http://arxiv.org/abs/2506.22139v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Low-Rank Implicit Neural Representation via Schatten-p Quasi-Norm and Jacobian Regularization", "abstract": "Higher-order tensors are well-suited for representing multi-dimensional data,\nsuch as color images and videos. Low-rank tensor representation has become\nessential in machine learning and computer vision, but existing methods like\nTucker decomposition offer flexibility at the expense of interpretability. In\ncontrast, while the CANDECOMP/PARAFAC (CP) decomposition provides a more\nnatural and interpretable tensor structure, obtaining sparse solutions remains\nchallenging. Leveraging the rich properties of CP decomposition, we propose a\nCP-based low-rank tensor function parameterized by neural networks for implicit\nneural representation (CP-INR). This approach enables continuous data\nrepresentation beyond structured grids, fully exploiting the non-linearity of\ntensor data with theoretical guarantees on excess risk bounds. To achieve a\nsparse CP decomposition, we introduce a variational form of the Schatten-p\nquasi-norm and prove its relationship to multilinear rank minimization. For\nsmoothness, we propose a regularization term based on the spectral norm of the\nJacobian and Hutchinson's trace estimator. Our proposed smoothness\nregularization is SVD-free and avoids explicit chain rule derivations. It can\nserve as an alternative to Total Variation (TV) regularization in image\ndenoising tasks and is naturally applicable to continuous data. Extensive\nexperiments on multi-dimensional data recovery tasks, including image\ninpainting, denoising, and point cloud upsampling, demonstrate the superiority\nand versatility of our method compared to state-of-the-art approaches.", "published": "2025-06-27 11:23:10", "link": "http://arxiv.org/abs/2506.22134v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Pipe Reconstruction from Point Cloud Data", "abstract": "Accurate digital twins of industrial assets, such as ships and offshore\nplatforms, rely on the precise reconstruction of complex pipe networks.\nHowever, manual modelling of pipes from laser scan data is a time-consuming and\nlabor-intensive process. This paper presents a pipeline for automated pipe\nreconstruction from incomplete laser scan data. The approach estimates a\nskeleton curve using Laplacian-based contraction, followed by curve elongation.\nThe skeleton axis is then recentred using a rolling sphere technique combined\nwith 2D circle fitting, and refined with a 3D smoothing step. This enables the\ndetermination of pipe properties, including radius, length and orientation, and\nfacilitates the creation of detailed 3D models of complex pipe networks. By\nautomating pipe reconstruction, this approach supports the development of\ndigital twins, allowing for rapid and accurate modeling while reducing costs.", "published": "2025-06-27 10:54:51", "link": "http://arxiv.org/abs/2506.22118v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Evaluating Pointing Gestures for Target Selection in Human-Robot Collaboration", "abstract": "Pointing gestures are a common interaction method used in Human-Robot\nCollaboration for various tasks, ranging from selecting targets to guiding\nindustrial processes. This study introduces a method for localizing pointed\ntargets within a planar workspace. The approach employs pose estimation, and a\nsimple geometric model based on shoulder-wrist extension to extract gesturing\ndata from an RGB-D stream. The study proposes a rigorous methodology and\ncomprehensive analysis for evaluating pointing gestures and target selection in\ntypical robotic tasks. In addition to evaluating tool accuracy, the tool is\nintegrated into a proof-of-concept robotic system, which includes object\ndetection, speech transcription, and speech synthesis to demonstrate the\nintegration of multiple modalities in a collaborative application. Finally, a\ndiscussion over tool limitations and performance is provided to understand its\nrole in multimodal robotic systems. All developments are available at:\nhttps://github.com/NMKsas/gesture_pointer.git.", "published": "2025-06-27 10:51:31", "link": "http://arxiv.org/abs/2506.22116v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Pedestrian Intention and Trajectory Prediction in Unstructured Traffic Using IDD-PeD", "abstract": "With the rapid advancements in autonomous driving, accurately predicting\npedestrian behavior has become essential for ensuring safety in complex and\nunpredictable traffic conditions. The growing interest in this challenge\nhighlights the need for comprehensive datasets that capture unstructured\nenvironments, enabling the development of more robust prediction models to\nenhance pedestrian safety and vehicle navigation. In this paper, we introduce\nan Indian driving pedestrian dataset designed to address the complexities of\nmodeling pedestrian behavior in unstructured environments, such as illumination\nchanges, occlusion of pedestrians, unsignalized scene types and\nvehicle-pedestrian interactions. The dataset provides high-level and detailed\nlow-level comprehensive annotations focused on pedestrians requiring the\nego-vehicle's attention. Evaluation of the state-of-the-art intention\nprediction methods on our dataset shows a significant performance drop of up to\n$\\mathbf{15\\%}$, while trajectory prediction methods underperform with an\nincrease of up to $\\mathbf{1208}$ MSE, defeating standard pedestrian datasets.\nAdditionally, we present exhaustive quantitative and qualitative analysis of\nintention and trajectory baselines. We believe that our dataset will open new\nchallenges for the pedestrian behavior research community to build robust\nmodels. Project Page:\nhttps://cvit.iiit.ac.in/research/projects/cvit-projects/iddped", "published": "2025-06-27 10:41:18", "link": "http://arxiv.org/abs/2506.22111v1", "categories": ["cs.CV", "cs.HC"], "primary_category": "cs.CV"}
{"title": "Tied Prototype Model for Few-Shot Medical Image Segmentation", "abstract": "Common prototype-based medical image few-shot segmentation (FSS) methods\nmodel foreground and background classes using class-specific prototypes.\nHowever, given the high variability of the background, a more promising\ndirection is to focus solely on foreground modeling, treating the background as\nan anomaly -- an approach introduced by ADNet. Yet, ADNet faces three key\nlimitations: dependence on a single prototype per class, a focus on binary\nclassification, and fixed thresholds that fail to adapt to patient and organ\nvariability. To address these shortcomings, we propose the Tied Prototype Model\n(TPM), a principled reformulation of ADNet with tied prototype locations for\nforeground and background distributions. Building on its probabilistic\nfoundation, TPM naturally extends to multiple prototypes and multi-class\nsegmentation while effectively separating non-typical background features.\nNotably, both extensions lead to improved segmentation accuracy. Finally, we\nleverage naturally occurring class priors to define an ideal target for\nadaptive thresholds, boosting segmentation performance. Taken together, TPM\nprovides a fresh perspective on prototype-based FSS for medical image\nsegmentation. The code can be found at https://github.com/hjk92g/TPM-FSS.", "published": "2025-06-27 10:33:55", "link": "http://arxiv.org/abs/2506.22101v1", "categories": ["cs.CV", "cs.LG", "stat.ML"], "primary_category": "cs.CV"}
{"title": "B\u00e9zierGS: Dynamic Urban Scene Reconstruction with B\u00e9zier Curve Gaussian Splatting", "abstract": "The realistic reconstruction of street scenes is critical for developing\nreal-world simulators in autonomous driving. Most existing methods rely on\nobject pose annotations, using these poses to reconstruct dynamic objects and\nmove them during the rendering process. This dependence on high-precision\nobject annotations limits large-scale and extensive scene reconstruction. To\naddress this challenge, we propose B\\'ezier curve Gaussian splatting\n(B\\'ezierGS), which represents the motion trajectories of dynamic objects using\nlearnable B\\'ezier curves. This approach fully leverages the temporal\ninformation of dynamic objects and, through learnable curve modeling,\nautomatically corrects pose errors. By introducing additional supervision on\ndynamic object rendering and inter-curve consistency constraints, we achieve\nreasonable and accurate separation and reconstruction of scene elements.\nExtensive experiments on the Waymo Open Dataset and the nuPlan benchmark\ndemonstrate that B\\'ezierGS outperforms state-of-the-art alternatives in both\ndynamic and static scene components reconstruction and novel view synthesis.", "published": "2025-06-27 10:30:16", "link": "http://arxiv.org/abs/2506.22099v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Towards Accurate Heart Rate Measurement from Ultra-Short Video Clips via Periodicity-Guided rPPG Estimation and Signal Reconstruction", "abstract": "Many remote Heart Rate (HR) measurement methods focus on estimating remote\nphotoplethysmography (rPPG) signals from video clips lasting around 10 seconds\nbut often overlook the need for HR estimation from ultra-short video clips. In\nthis paper, we aim to accurately measure HR from ultra-short 2-second video\nclips by specifically addressing two key challenges. First, to overcome the\nlimited number of heartbeat cycles in ultra-short video clips, we propose an\neffective periodicity-guided rPPG estimation method that enforces consistent\nperiodicity between rPPG signals estimated from ultra-short clips and their\nmuch longer ground truth signals. Next, to mitigate estimation inaccuracies due\nto spectral leakage, we propose including a generator to reconstruct longer\nrPPG signals from ultra-short ones while preserving their periodic consistency\nto enable more accurate HR measurement. Extensive experiments on four rPPG\nestimation benchmark datasets demonstrate that our proposed method not only\naccurately measures HR from ultra-short video clips but also outperform\nprevious rPPG estimation techniques to achieve state-of-the-art performance.", "published": "2025-06-27 10:05:00", "link": "http://arxiv.org/abs/2506.22078v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Reasoning in machine vision: learning to think fast and slow", "abstract": "Reasoning is a hallmark of human intelligence, enabling adaptive\ndecision-making in complex and unfamiliar scenarios. In contrast, machine\nintelligence remains bound to training data, lacking the ability to dynamically\nrefine solutions at inference time. While some recent advances have explored\nreasoning in machines, these efforts are largely limited to verbal domains such\nas mathematical problem-solving, where explicit rules govern step-by-step\nreasoning. Other critical real-world tasks - including visual perception,\nspatial reasoning, and radiological diagnosis - require non-verbal reasoning,\nwhich remains an open challenge. Here we present a novel learning paradigm that\nenables machine reasoning in vision by allowing performance improvement with\nincreasing thinking time (inference-time compute), even under conditions where\nlabelled data is very limited. Inspired by dual-process theories of human\ncognition in psychology, our approach integrates a fast-thinking System I\nmodule for familiar tasks, with a slow-thinking System II module that\niteratively refines solutions using self-play reinforcement learning. This\nparadigm mimics human reasoning by proposing, competing over, and refining\nsolutions in data-scarce scenarios. We demonstrate superior performance through\nextended thinking time, compared not only to large-scale supervised learning\nbut also foundation models and even human experts, in real-world vision tasks.\nThese tasks include computer-vision benchmarks and cancer localisation on\nmedical images across five organs, showcasing transformative potential for\nnon-verbal machine reasoning.", "published": "2025-06-27 10:03:05", "link": "http://arxiv.org/abs/2506.22075v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Single-Scanline Relative Pose Estimation for Rolling Shutter Cameras", "abstract": "We propose a novel approach for estimating the relative pose between rolling\nshutter cameras using the intersections of line projections with a single\nscanline per image. This allows pose estimation without explicitly modeling\ncamera motion. Alternatively, scanlines can be selected within a single image,\nenabling single-view relative pose estimation for scanlines of rolling shutter\ncameras. Our approach is designed as a foundational building block for rolling\nshutter structure-from-motion (SfM), where no motion model is required, and\neach scanline's pose can be computed independently. % We classify minimal\nsolvers for this problem in both generic and specialized settings, including\ncases with parallel lines and known gravity direction, assuming known\nintrinsics and no lens distortion. Furthermore, we develop minimal solvers for\nthe parallel-lines scenario, both with and without gravity priors, by\nleveraging connections between this problem and the estimation of 2D structure\nfrom 1D cameras. % Experiments on rolling shutter images from the Fastec\ndataset demonstrate the feasibility of our approach for initializing rolling\nshutter SfM, highlighting its potential for further development. % The code\nwill be made publicly available.", "published": "2025-06-27 10:00:21", "link": "http://arxiv.org/abs/2506.22069v1", "categories": ["cs.CV", "68T45", "I.4.5"], "primary_category": "cs.CV"}
{"title": "MirrorMe: Towards Realtime and High Fidelity Audio-Driven Halfbody Animation", "abstract": "Audio-driven portrait animation, which synthesizes realistic videos from\nreference images using audio signals, faces significant challenges in real-time\ngeneration of high-fidelity, temporally coherent animations. While recent\ndiffusion-based methods improve generation quality by integrating audio into\ndenoising processes, their reliance on frame-by-frame UNet architectures\nintroduces prohibitive latency and struggles with temporal consistency. This\npaper introduces MirrorMe, a real-time, controllable framework built on the LTX\nvideo model, a diffusion transformer that compresses video spatially and\ntemporally for efficient latent space denoising. To address LTX's trade-offs\nbetween compression and semantic fidelity, we propose three innovations: 1. A\nreference identity injection mechanism via VAE-encoded image concatenation and\nself-attention, ensuring identity consistency; 2. A causal audio encoder and\nadapter tailored to LTX's temporal structure, enabling precise audio-expression\nsynchronization; and 3. A progressive training strategy combining close-up\nfacial training, half-body synthesis with facial masking, and hand pose\nintegration for enhanced gesture control. Extensive experiments on the EMTD\nBenchmark demonstrate MirrorMe's state-of-the-art performance in fidelity,\nlip-sync accuracy, and temporal stability.", "published": "2025-06-27 09:57:23", "link": "http://arxiv.org/abs/2506.22065v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "EnLVAM: Enhanced Left Ventricle Linear Measurements Utilizing Anatomical Motion Mode", "abstract": "Linear measurements of the left ventricle (LV) in the Parasternal Long Axis\n(PLAX) view using B-mode echocardiography are crucial for cardiac assessment.\nThese involve placing 4-6 landmarks along a virtual scanline (SL) perpendicular\nto the LV axis near the mitral valve tips. Manual placement is time-consuming\nand error-prone, while existing deep learning methods often misalign landmarks,\ncausing inaccurate measurements. We propose a novel framework that enhances LV\nmeasurement accuracy by enforcing straight-line constraints. A landmark\ndetector is trained on Anatomical M-Mode (AMM) images, computed in real time\nfrom B-mode videos, then transformed back to B-mode space. This approach\naddresses misalignment and reduces measurement errors. Experiments show\nimproved accuracy over standard B-mode methods, and the framework generalizes\nwell across network architectures. Our semi-automatic design includes a\nhuman-in-the-loop step where the user only places the SL, simplifying\ninteraction while preserving alignment flexibility and clinical relevance.", "published": "2025-06-27 09:57:17", "link": "http://arxiv.org/abs/2506.22063v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Few-Shot Identity Adaptation for 3D Talking Heads via Global Gaussian Field", "abstract": "Reconstruction and rendering-based talking head synthesis methods achieve\nhigh-quality results with strong identity preservation but are limited by their\ndependence on identity-specific models. Each new identity requires training\nfrom scratch, incurring high computational costs and reduced scalability\ncompared to generative model-based approaches. To overcome this limitation, we\npropose FIAG, a novel 3D speaking head synthesis framework that enables\nefficient identity-specific adaptation using only a few training footage. FIAG\nincorporates Global Gaussian Field, which supports the representation of\nmultiple identities within a shared field, and Universal Motion Field, which\ncaptures the common motion dynamics across diverse identities. Benefiting from\nthe shared facial structure information encoded in the Global Gaussian Field\nand the general motion priors learned in the motion field, our framework\nenables rapid adaptation from canonical identity representations to specific\nones with minimal data. Extensive comparative and ablation experiments\ndemonstrate that our method outperforms existing state-of-the-art approaches,\nvalidating both the effectiveness and generalizability of the proposed\nframework. Code is available at: \\textit{https://github.com/gme-hong/FIAG}.", "published": "2025-06-27 09:42:30", "link": "http://arxiv.org/abs/2506.22044v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Towards Scalable and Robust White Matter Lesion Localization via Multimodal Deep Learning", "abstract": "White matter hyperintensities (WMH) are radiological markers of small vessel\ndisease and neurodegeneration, whose accurate segmentation and spatial\nlocalization are crucial for diagnosis and monitoring. While multimodal MRI\noffers complementary contrasts for detecting and contextualizing WM lesions,\nexisting approaches often lack flexibility in handling missing modalities and\nfail to integrate anatomical localization efficiently. We propose a deep\nlearning framework for WM lesion segmentation and localization that operates\ndirectly in native space using single- and multi-modal MRI inputs. Our study\nevaluates four input configurations: FLAIR-only, T1-only, concatenated FLAIR\nand T1, and a modality-interchangeable setup. It further introduces a\nmulti-task model for jointly predicting lesion and anatomical region masks to\nestimate region-wise lesion burden. Experiments conducted on the MICCAI WMH\nSegmentation Challenge dataset demonstrate that multimodal input significantly\nimproves the segmentation performance, outperforming unimodal models. While the\nmodality-interchangeable setting trades accuracy for robustness, it enables\ninference in cases with missing modalities. Joint lesion-region segmentation\nusing multi-task learning was less effective than separate models, suggesting\nrepresentational conflict between tasks. Our findings highlight the utility of\nmultimodal fusion for accurate and robust WMH analysis, and the potential of\njoint modeling for integrated predictions.", "published": "2025-06-27 09:39:26", "link": "http://arxiv.org/abs/2506.22041v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Partial CLIP is Enough: Chimera-Seg for Zero-shot Semantic Segmentation", "abstract": "Zero-shot Semantic Segmentation (ZSS) aims to segment both seen and unseen\nclasses using supervision from only seen classes. Beyond adaptation-based\nmethods, distillation-based approaches transfer vision-language alignment of\nvision-language model, e.g., CLIP, to segmentation models. However, such\nknowledge transfer remains challenging due to: (1) the difficulty of aligning\nvision-based features with the textual space, which requires combining spatial\nprecision with vision-language alignment; and (2) the semantic gap between\nCLIP's global representations and the local, fine-grained features of\nsegmentation models. To address challenge (1), we propose Chimera-Seg, which\nintegrates a segmentation backbone as the body and a CLIP-based semantic head\nas the head, like the Chimera in Greek mythology, combining spatial precision\nwith vision-language alignment. Specifically, Chimera-Seg comprises a trainable\nsegmentation model and a CLIP Semantic Head (CSH), which maps dense features\ninto the CLIP-aligned space. The CSH incorporates a frozen subnetwork and fixed\nprojection layers from the CLIP visual encoder, along with lightweight\ntrainable components. The partial module from CLIP visual encoder, paired with\nthe segmentation model, retains segmentation capability while easing the\nmapping to CLIP's semantic space. To address challenge (2), we propose\nSelective Global Distillation (SGD), which distills knowledge from dense\nfeatures exhibiting high similarity to the CLIP CLS token, while gradually\nreducing the number of features used for alignment as training progresses.\nBesides, we also use a Semantic Alignment Module (SAM) to further align dense\nvisual features with semantic embeddings extracted from the frozen CLIP text\nencoder. Experiments on two benchmarks show improvements of 0.9% and 1.2% in\nhIoU.", "published": "2025-06-27 09:26:50", "link": "http://arxiv.org/abs/2506.22032v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Cross-modal Ship Re-Identification via Optical and SAR Imagery: A Novel Dataset and Method", "abstract": "Detecting and tracking ground objects using earth observation imagery remains\na significant challenge in the field of remote sensing. Continuous maritime\nship tracking is crucial for applications such as maritime search and rescue,\nlaw enforcement, and shipping analysis. However, most current ship tracking\nmethods rely on geostationary satellites or video satellites. The former offer\nlow resolution and are susceptible to weather conditions, while the latter have\nshort filming durations and limited coverage areas, making them less suitable\nfor the real-world requirements of ship tracking. To address these limitations,\nwe present the Hybrid Optical and Synthetic Aperture Radar (SAR) Ship\nRe-Identification Dataset (HOSS ReID dataset), designed to evaluate the\neffectiveness of ship tracking using low-Earth orbit constellations of optical\nand SAR sensors. This approach ensures shorter re-imaging cycles and enables\nall-weather tracking. HOSS ReID dataset includes images of the same ship\ncaptured over extended periods under diverse conditions, using different\nsatellites of different modalities at varying times and angles. Furthermore, we\npropose a baseline method for cross-modal ship re-identification, TransOSS,\nwhich is built on the Vision Transformer architecture. It refines the patch\nembedding structure to better accommodate cross-modal tasks, incorporates\nadditional embeddings to introduce more reference information, and employs\ncontrastive learning to pre-train on large-scale optical-SAR image pairs,\nensuring the model's ability to extract modality-invariant features. Our\ndataset and baseline method are publicly available on\nhttps://github.com/Alioth2000/Hoss-ReID.", "published": "2025-06-27 09:13:22", "link": "http://arxiv.org/abs/2506.22027v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Advancing Facial Stylization through Semantic Preservation Constraint and Pseudo-Paired Supervision", "abstract": "Facial stylization aims to transform facial images into appealing,\nhigh-quality stylized portraits, with the critical challenge of accurately\nlearning the target style while maintaining content consistency with the\noriginal image. Although previous StyleGAN-based methods have made significant\nadvancements, the generated results still suffer from artifacts or insufficient\nfidelity to the source image. We argue that these issues stem from neglecting\nsemantic shift of the generator during stylization. Therefore, we propose a\nfacial stylization method that integrates semantic preservation constraint and\npseudo-paired supervision to enhance the content correspondence and improve the\nstylization effect. Additionally, we develop a methodology for creating\nmulti-level pseudo-paired datasets to implement supervisory constraint.\nFurthermore, building upon our facial stylization framework, we achieve more\nflexible multimodal and reference-guided stylization without complex network\narchitecture designs or additional training. Experimental results demonstrate\nthat our approach produces high-fidelity, aesthetically pleasing facial style\ntransfer that surpasses previous methods.", "published": "2025-06-27 08:44:31", "link": "http://arxiv.org/abs/2506.22022v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Towards Universal & Efficient Model Compression via Exponential Torque Pruning", "abstract": "The rapid growth in complexity and size of modern deep neural networks (DNNs)\nhas increased challenges related to computational costs and memory usage,\nspurring a growing interest in efficient model compression techniques. Previous\nstate-of-the-art approach proposes using a Torque-inspired regularization which\nforces the weights of neural modules around a selected pivot point. Whereas, we\nobserve that the pruning effect of this approach is far from perfect, as the\npost-trained network is still dense and also suffers from high accuracy drop.\nIn this work, we attribute such ineffectiveness to the default linear force\napplication scheme, which imposes inappropriate force on neural module of\ndifferent distances. To efficiently prune the redundant and distant modules\nwhile retaining those that are close and necessary for effective inference, in\nthis work, we propose Exponential Torque Pruning (ETP), which adopts an\nexponential force application scheme for regularization. Experimental results\non a broad range of domains demonstrate that, though being extremely simple,\nETP manages to achieve significantly higher compression rate than the previous\nstate-of-the-art pruning strategies with negligible accuracy drop.", "published": "2025-06-27 08:28:21", "link": "http://arxiv.org/abs/2506.22015v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Noise-Inspired Diffusion Model for Generalizable Low-Dose CT Reconstruction", "abstract": "The generalization of deep learning-based low-dose computed tomography (CT)\nreconstruction models to doses unseen in the training data is important and\nremains challenging. Previous efforts heavily rely on paired data to improve\nthe generalization performance and robustness through collecting either diverse\nCT data for re-training or a few test data for fine-tuning. Recently, diffusion\nmodels have shown promising and generalizable performance in low-dose CT (LDCT)\nreconstruction, however, they may produce unrealistic structures due to the CT\nimage noise deviating from Gaussian distribution and imprecise prior\ninformation from the guidance of noisy LDCT images. In this paper, we propose a\nnoise-inspired diffusion model for generalizable LDCT reconstruction, termed\nNEED, which tailors diffusion models for noise characteristics of each domain.\nFirst, we propose a novel shifted Poisson diffusion model to denoise projection\ndata, which aligns the diffusion process with the noise model in pre-log LDCT\nprojections. Second, we devise a doubly guided diffusion model to refine\nreconstructed images, which leverages LDCT images and initial reconstructions\nto more accurately locate prior information and enhance reconstruction\nfidelity. By cascading these two diffusion models for dual-domain\nreconstruction, our NEED requires only normal-dose data for training and can be\neffectively extended to various unseen dose levels during testing via a time\nstep matching strategy. Extensive qualitative, quantitative, and\nsegmentation-based evaluations on two datasets demonstrate that our NEED\nconsistently outperforms state-of-the-art methods in reconstruction and\ngeneralization performance. Source code is made available at\nhttps://github.com/qgao21/NEED.", "published": "2025-06-27 08:24:55", "link": "http://arxiv.org/abs/2506.22012v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "RoboEnvision: A Long-Horizon Video Generation Model for Multi-Task Robot Manipulation", "abstract": "We address the problem of generating long-horizon videos for robotic\nmanipulation tasks. Text-to-video diffusion models have made significant\nprogress in photorealism, language understanding, and motion generation but\nstruggle with long-horizon robotic tasks. Recent works use video diffusion\nmodels for high-quality simulation data and predictive rollouts in robot\nplanning. However, these works predict short sequences of the robot achieving\none task and employ an autoregressive paradigm to extend to the long horizon,\nleading to error accumulations in the generated video and in the execution. To\novercome these limitations, we propose a novel pipeline that bypasses the need\nfor autoregressive generation. We achieve this through a threefold\ncontribution: 1) we first decompose the high-level goals into smaller atomic\ntasks and generate keyframes aligned with these instructions. A second\ndiffusion model then interpolates between each of the two generated frames,\nachieving the long-horizon video. 2) We propose a semantics preserving\nattention module to maintain consistency between the keyframes. 3) We design a\nlightweight policy model to regress the robot joint states from generated\nvideos. Our approach achieves state-of-the-art results on two benchmarks in\nvideo quality and consistency while outperforming previous policy models on\nlong-horizon tasks.", "published": "2025-06-27 08:21:55", "link": "http://arxiv.org/abs/2506.22007v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "R1-Track: Direct Application of MLLMs to Visual Object Tracking via Reinforcement Learning", "abstract": "Visual single object tracking aims to continuously localize and estimate the\nscale of a target in subsequent video frames, given only its initial state in\nthe first frame. This task has traditionally been framed as a template matching\nproblem, evolving through major phases including correlation filters,\ntwo-stream networks, and one-stream networks with significant progress\nachieved. However, these methods typically require explicit classification and\nregression modeling, depend on supervised training with large-scale datasets,\nand are limited to the single task of tracking, lacking flexibility. In recent\nyears, multi-modal large language models (MLLMs) have advanced rapidly.\nOpen-source models like Qwen2.5-VL, a flagship MLLMs with strong foundational\ncapabilities, demonstrate excellent performance in grounding tasks. This has\nspurred interest in applying such models directly to visual tracking. However,\nexperiments reveal that Qwen2.5-VL struggles with template matching between\nimage pairs (i.e., tracking tasks). Inspired by deepseek-R1, we fine-tuned\nQwen2.5-VL using the group relative policy optimization (GRPO) reinforcement\nlearning method on a small-scale dataset with a rule-based reward function. The\nresulting model, R1-Track, achieved notable performance on the GOT-10k\nbenchmark. R1-Track supports flexible initialization via bounding boxes or text\ndescriptions while retaining most of the original model's general capabilities.\nAnd we further discuss potential improvements for R1-Track. This rough\ntechnical report summarizes our findings as of May 2025.", "published": "2025-06-27 07:41:15", "link": "http://arxiv.org/abs/2506.21980v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "StableCodec: Taming One-Step Diffusion for Extreme Image Compression", "abstract": "Diffusion-based image compression has shown remarkable potential for\nachieving ultra-low bitrate coding (less than 0.05 bits per pixel) with high\nrealism, by leveraging the generative priors of large pre-trained text-to-image\ndiffusion models. However, current approaches require a large number of\ndenoising steps at the decoder to generate realistic results under extreme\nbitrate constraints, limiting their application in real-time compression\nscenarios. Additionally, these methods often sacrifice reconstruction fidelity,\nas diffusion models typically fail to guarantee pixel-level consistency. To\naddress these challenges, we introduce StableCodec, which enables one-step\ndiffusion for high-fidelity and high-realism extreme image compression with\nimproved coding efficiency. To achieve ultra-low bitrates, we first develop an\nefficient Deep Compression Latent Codec to transmit a noisy latent\nrepresentation for a single-step denoising process. We then propose a\nDual-Branch Coding Structure, consisting of a pair of auxiliary encoder and\ndecoder, to enhance reconstruction fidelity. Furthermore, we adopt end-to-end\noptimization with joint bitrate and pixel-level constraints. Extensive\nexperiments on the CLIC 2020, DIV2K, and Kodak dataset demonstrate that\nStableCodec outperforms existing methods in terms of FID, KID and DISTS by a\nsignificant margin, even at bitrates as low as 0.005 bits per pixel, while\nmaintaining strong fidelity. Additionally, StableCodec achieves inference\nspeeds comparable to mainstream transform coding schemes. All source code are\navailable at https://github.com/LuizScarlet/StableCodec.", "published": "2025-06-27 07:39:21", "link": "http://arxiv.org/abs/2506.21977v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model", "abstract": "The goal of traffic simulation is to augment a potentially limited amount of\nmanually-driven miles that is available for testing and validation, with a much\nlarger amount of simulated synthetic miles. The culmination of this vision\nwould be a generative simulated city, where given a map of the city and an\nautonomous vehicle (AV) software stack, the simulator can seamlessly simulate\nthe trip from point A to point B by populating the city around the AV and\ncontrolling all aspects of the scene, from animating the dynamic agents (e.g.,\nvehicles, pedestrians) to controlling the traffic light states. We refer to\nthis vision as CitySim, which requires an agglomeration of simulation\ntechnologies: scene generation to populate the initial scene, agent behavior\nmodeling to animate the scene, occlusion reasoning, dynamic scene generation to\nseamlessly spawn and remove agents, and environment simulation for factors such\nas traffic lights. While some key technologies have been separately studied in\nvarious works, others such as dynamic scene generation and environment\nsimulation have received less attention in the research community. We propose\nSceneDiffuser++, the first end-to-end generative world model trained on a\nsingle loss function capable of point A-to-B simulation on a city scale\nintegrating all the requirements above. We demonstrate the city-scale traffic\nsimulation capability of SceneDiffuser++ and study its superior realism under\nlong simulation conditions. We evaluate the simulation quality on an augmented\nversion of the Waymo Open Motion Dataset (WOMD) with larger map regions to\nsupport trip-level simulation.", "published": "2025-06-27 07:35:04", "link": "http://arxiv.org/abs/2506.21976v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA", "cs.RO"], "primary_category": "cs.LG"}
{"title": "TASeg: Text-aware RGB-T Semantic Segmentation based on Fine-tuning Vision Foundation Models", "abstract": "Reliable semantic segmentation of open environments is essential for\nintelligent systems, yet significant problems remain: 1) Existing RGB-T\nsemantic segmentation models mainly rely on low-level visual features and lack\nhigh-level textual information, which struggle with accurate segmentation when\ncategories share similar visual characteristics. 2) While SAM excels in\ninstance-level segmentation, integrating it with thermal images and text is\nhindered by modality heterogeneity and computational inefficiency. To address\nthese, we propose TASeg, a text-aware RGB-T segmentation framework by using\nLow-Rank Adaptation (LoRA) fine-tuning technology to adapt vision foundation\nmodels. Specifically, we propose a Dynamic Feature Fusion Module (DFFM) in the\nimage encoder, which effectively merges features from multiple visual\nmodalities while freezing SAM's original transformer blocks. Additionally, we\nincorporate CLIP-generated text embeddings in the mask decoder to enable\nsemantic alignment, which further rectifies the classification error and\nimproves the semantic understanding accuracy. Experimental results across\ndiverse datasets demonstrate that our method achieves superior performance in\nchallenging scenarios with fewer trainable parameters.", "published": "2025-06-27 07:34:28", "link": "http://arxiv.org/abs/2506.21975v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Exploring Semantic Masked Autoencoder for Self-supervised Point Cloud Understanding", "abstract": "Point cloud understanding aims to acquire robust and general feature\nrepresentations from unlabeled data. Masked point modeling-based methods have\nrecently shown significant performance across various downstream tasks. These\npre-training methods rely on random masking strategies to establish the\nperception of point clouds by restoring corrupted point cloud inputs, which\nleads to the failure of capturing reasonable semantic relationships by the\nself-supervised models. To address this issue, we propose Semantic Masked\nAutoencoder, which comprises two main components: a prototype-based component\nsemantic modeling module and a component semantic-enhanced masking strategy.\nSpecifically, in the component semantic modeling module, we design a component\nsemantic guidance mechanism to direct a set of learnable prototypes in\ncapturing the semantics of different components from objects. Leveraging these\nprototypes, we develop a component semantic-enhanced masking strategy that\naddresses the limitations of random masking in effectively covering complete\ncomponent structures. Furthermore, we introduce a component semantic-enhanced\nprompt-tuning strategy, which further leverages these prototypes to improve the\nperformance of pre-trained models in downstream tasks. Extensive experiments\nconducted on datasets such as ScanObjectNN, ModelNet40, and ShapeNetPart\ndemonstrate the effectiveness of our proposed modules.", "published": "2025-06-27 06:58:59", "link": "http://arxiv.org/abs/2506.21957v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SDRNET: Stacked Deep Residual Network for Accurate Semantic Segmentation of Fine-Resolution Remotely Sensed Images", "abstract": "Land cover maps generated from semantic segmentation of high-resolution\nremotely sensed images have drawn mucon in the photogrammetry and remote\nsensing research community. Currently, massive fine-resolution remotely sensed\n(FRRS) images acquired by improving sensing and imaging technologies become\navailable. However, accurate semantic segmentation of such FRRS images is\ngreatly affected by substantial class disparities, the invisibility of key\nground objects due to occlusion, and object size variation. Despite the\nextraordinary potential in deep convolutional neural networks (DCNNs) in image\nfeature learning and representation, extracting sufficient features from FRRS\nimages for accurate semantic segmentation is still challenging. These\nchallenges demand the deep learning models to learn robust features and\ngenerate sufficient feature descriptors. Specifically, learning\nmulti-contextual features to guarantee adequate coverage of varied object sizes\nfrom the ground scene and harnessing global-local contexts to overcome class\ndisparities challenge even profound networks. Deeper networks significantly\nlose spatial details due to gradual downsampling processes resulting in poor\nsegmentation results and coarse boundaries. This article presents a stacked\ndeep residual network (SDRNet) for semantic segmentation from FRRS images. The\nproposed framework utilizes two stacked encoder-decoder networks to harness\nlong-range semantics yet preserve spatial information and dilated residual\nblocks (DRB) between each encoder and decoder network to capture sufficient\nglobal dependencies thus improving segmentation performance. Our experimental\nresults obtained using the ISPRS Vaihingen and Potsdam datasets demonstrate\nthat the SDRNet performs effectively and competitively against current DCNNs in\nsemantic segmentation.", "published": "2025-06-27 06:40:30", "link": "http://arxiv.org/abs/2506.21945v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware Layout Design", "abstract": "Automated content-aware layout generation -- the task of arranging visual\nelements such as text, logos, and underlays on a background canvas -- remains a\nfundamental yet under-explored problem in intelligent design systems. While\nrecent advances in deep generative models and large language models (LLMs) have\nshown promise in structured content generation, most existing approaches lack\ngrounding in contextual design exemplars and fall short in handling semantic\nalignment and visual coherence. In this work we introduce CAL-RAG, a\nretrieval-augmented, agentic framework for content-aware layout generation that\nintegrates multimodal retrieval, large language models, and collaborative\nagentic reasoning. Our system retrieves relevant layout examples from a\nstructured knowledge base and invokes an LLM-based layout recommender to\npropose structured element placements. A vision-language grader agent evaluates\nthe layout with visual metrics, and a feedback agent provides targeted\nrefinements, enabling iterative improvement. We implement our framework using\nLangGraph and evaluate it on the PKU PosterLayout dataset, a benchmark rich in\nsemantic and structural variability. CAL-RAG achieves state-of-the-art\nperformance across multiple layout metrics -- including underlay effectiveness,\nelement alignment, and overlap -- substantially outperforming strong baselines\nsuch as LayoutPrompter. These results demonstrate that combining retrieval\naugmentation with agentic multi-step reasoning yields a scalable,\ninterpretable, and high-fidelity solution for automated layout generation.", "published": "2025-06-27 06:09:56", "link": "http://arxiv.org/abs/2506.21934v1", "categories": ["cs.IR", "cs.CV", "I.3.3; I.2.11; H.5.2"], "primary_category": "cs.IR"}
{"title": "Quality Assessment and Distortion-aware Saliency Prediction for AI-Generated Omnidirectional Images", "abstract": "With the rapid advancement of Artificial Intelligence Generated Content\n(AIGC) techniques, AI generated images (AIGIs) have attracted widespread\nattention, among which AI generated omnidirectional images (AIGODIs) hold\nsignificant potential for Virtual Reality (VR) and Augmented Reality (AR)\napplications. AI generated omnidirectional images exhibit unique quality\nissues, however, research on the quality assessment and optimization of\nAI-generated omnidirectional images is still lacking. To this end, this work\nfirst studies the quality assessment and distortion-aware saliency prediction\nproblems for AIGODIs, and further presents a corresponding optimization\nprocess. Specifically, we first establish a comprehensive database to reflect\nhuman feedback for AI-generated omnidirectionals, termed OHF2024, which\nincludes both subjective quality ratings evaluated from three perspectives and\ndistortion-aware salient regions. Based on the constructed OHF2024 database, we\npropose two models with shared encoders based on the BLIP-2 model to evaluate\nthe human visual experience and predict distortion-aware saliency for\nAI-generated omnidirectional images, which are named as BLIP2OIQA and\nBLIP2OISal, respectively. Finally, based on the proposed models, we present an\nautomatic optimization process that utilizes the predicted visual experience\nscores and distortion regions to further enhance the visual quality of an\nAI-generated omnidirectional image. Extensive experiments show that our\nBLIP2OIQA model and BLIP2OISal model achieve state-of-the-art (SOTA) results in\nthe human visual experience evaluation task and the distortion-aware saliency\nprediction task for AI generated omnidirectional images, and can be effectively\nused in the optimization process. The database and codes will be released on\nhttps://github.com/IntMeGroup/AIGCOIQA to facilitate future research.", "published": "2025-06-27 05:36:04", "link": "http://arxiv.org/abs/2506.21925v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SPAZER: Spatial-Semantic Progressive Reasoning Agent for Zero-shot 3D Visual Grounding", "abstract": "3D Visual Grounding (3DVG) aims to localize target objects within a 3D scene\nbased on natural language queries. To alleviate the reliance on costly 3D\ntraining data, recent studies have explored zero-shot 3DVG by leveraging the\nextensive knowledge and powerful reasoning capabilities of pre-trained LLMs and\nVLMs. However, existing paradigms tend to emphasize either spatial (3D-based)\nor semantic (2D-based) understanding, limiting their effectiveness in complex\nreal-world applications. In this work, we introduce SPAZER - a VLM-driven agent\nthat combines both modalities in a progressive reasoning framework. It first\nholistically analyzes the scene and produces a 3D rendering from the optimal\nviewpoint. Based on this, anchor-guided candidate screening is conducted to\nperform a coarse-level localization of potential objects. Furthermore,\nleveraging retrieved relevant 2D camera images, 3D-2D joint decision-making is\nefficiently performed to determine the best-matching object. By bridging\nspatial and semantic reasoning neural streams, SPAZER achieves robust zero-shot\ngrounding without training on 3D-labeled data. Extensive experiments on\nScanRefer and Nr3D benchmarks demonstrate that SPAZER significantly outperforms\nprevious state-of-the-art zero-shot methods, achieving notable gains of 9.0%\nand 10.9% in accuracy.", "published": "2025-06-27 05:34:57", "link": "http://arxiv.org/abs/2506.21924v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "ZeroReg3D: A Zero-shot Registration Pipeline for 3D Consecutive Histopathology Image Reconstruction", "abstract": "Histological analysis plays a crucial role in understanding tissue structure\nand pathology. While recent advancements in registration methods have improved\n2D histological analysis, they often struggle to preserve critical 3D spatial\nrelationships, limiting their utility in both clinical and research\napplications. Specifically, constructing accurate 3D models from 2D slices\nremains challenging due to tissue deformation, sectioning artifacts,\nvariability in imaging techniques, and inconsistent illumination. Deep\nlearning-based registration methods have demonstrated improved performance but\nsuffer from limited generalizability and require large-scale training data. In\ncontrast, non-deep-learning approaches offer better generalizability but often\ncompromise on accuracy. In this study, we introduced ZeroReg3D, a novel\nzero-shot registration pipeline tailored for accurate 3D reconstruction from\nserial histological sections. By combining zero-shot deep learning-based\nkeypoint matching with optimization-based affine and non-rigid registration\ntechniques, ZeroReg3D effectively addresses critical challenges such as tissue\ndeformation, sectioning artifacts, staining variability, and inconsistent\nillumination without requiring retraining or fine-tuning. The code has been\nmade publicly available at https://github.com/hrlblab/ZeroReg3D", "published": "2025-06-27 05:31:23", "link": "http://arxiv.org/abs/2506.21923v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SepFormer: Coarse-to-fine Separator Regression Network for Table Structure Recognition", "abstract": "The automated reconstruction of the logical arrangement of tables from image\ndata, termed Table Structure Recognition (TSR), is fundamental for semantic\ndata extraction. Recently, researchers have explored a wide range of techniques\nto tackle this problem, demonstrating significant progress. Each table is a set\nof vertical and horizontal separators. Following this realization, we present\nSepFormer, which integrates the split-and-merge paradigm into a single step\nthrough separator regression with a DETR-style architecture, improving speed\nand robustness. SepFormer is a coarse-to-fine approach that predicts table\nseparators from single-line to line-strip separators with a stack of two\ntransformer decoders. In the coarse-grained stage, the model learns to\ngradually refine single-line segments through decoder layers with additional\nangle loss. At the end of the fine-grained stage, the model predicts line-strip\nseparators by refining sampled points from each single-line segment. Our\nSepFormer can run on average at 25.6 FPS while achieving comparable performance\nwith state-of-the-art methods on several benchmark datasets, including SciTSR,\nPubTabNet, WTW, and iFLYTAB.", "published": "2025-06-27 05:20:42", "link": "http://arxiv.org/abs/2506.21920v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Generating Attribute-Aware Human Motions from Textual Prompt", "abstract": "Text-driven human motion generation has recently attracted considerable\nattention, allowing models to generate human motions based on textual\ndescriptions. However, current methods neglect the influence of human\nattributes (such as age, gender, weight, and height) which are key factors\nshaping human motion patterns. This work represents a pilot exploration for\nbridging this gap. We conceptualize each motion as comprising both attribute\ninformation and action semantics, where textual descriptions align exclusively\nwith action semantics. To achieve this, a new framework inspired by Structural\nCausal Models is proposed to decouple action semantics from human attributes,\nenabling text-to-semantics prediction and attribute-controlled generation. The\nresulting model is capable of generating realistic, attribute-aware motion\naligned with the user's text and attribute inputs. For evaluation, we introduce\nHumanAttr, a comprehensive dataset containing attribute annotations for\ntext-motion pairs, setting the first benchmark for attribute-aware\ntext-to-motion generation. Extensive experiments on the new dataset validate\nour model's effectiveness.", "published": "2025-06-27 04:56:54", "link": "http://arxiv.org/abs/2506.21912v1", "categories": ["cs.CV", "cs.MM"], "primary_category": "cs.CV"}
{"title": "CERBERUS: Crack Evaluation & Recognition Benchmark for Engineering Reliability & Urban Stability", "abstract": "CERBERUS is a synthetic benchmark designed to help train and evaluate AI\nmodels for detecting cracks and other defects in infrastructure. It includes a\ncrack image generator and realistic 3D inspection scenarios built in Unity. The\nbenchmark features two types of setups: a simple Fly-By wall inspection and a\nmore complex Underpass scene with lighting and geometry challenges. We tested a\npopular object detection model (YOLO) using different combinations of synthetic\nand real crack data. Results show that combining synthetic and real data\nimproves performance on real-world images. CERBERUS provides a flexible,\nrepeatable way to test defect detection systems and supports future research in\nautomated infrastructure inspection. CERBERUS is publicly available at\nhttps://github.com/justinreinman/Cerberus-Defect-Generator.", "published": "2025-06-27 04:52:52", "link": "http://arxiv.org/abs/2506.21909v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "RAUM-Net: Regional Attention and Uncertainty-aware Mamba Network", "abstract": "Fine Grained Visual Categorization (FGVC) remains a challenging task in\ncomputer vision due to subtle inter class differences and fragile feature\nrepresentations. Existing methods struggle in fine grained scenarios,\nespecially when labeled data is scarce. We propose a semi supervised method\ncombining Mamba based feature modeling, region attention, and Bayesian\nuncertainty. Our approach enhances local to global feature modeling while\nfocusing on key areas during learning. Bayesian inference selects high quality\npseudo labels for stability. Experiments show strong performance on FGVC\nbenchmarks with occlusions, demonstrating robustness when labeled data is\nlimited. Code is available at https://github.com/wxqnl/RAUM Net.", "published": "2025-06-27 04:48:19", "link": "http://arxiv.org/abs/2506.21905v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Visual Content Detection in Educational Videos with Transfer Learning and Dataset Enrichment", "abstract": "Video is transforming education with online courses and recorded lectures\nsupplementing and replacing classroom teaching. Recent research has focused on\nenhancing information retrieval for video lectures with advanced navigation,\nsearchability, summarization, as well as question answering chatbots. Visual\nelements like tables, charts, and illustrations are central to comprehension,\nretention, and data presentation in lecture videos, yet their full potential\nfor improving access to video content remains underutilized. A major factor is\nthat accurate automatic detection of visual elements in a lecture video is\nchallenging; reasons include i) most visual elements, such as charts, graphs,\ntables, and illustrations, are artificially created and lack any standard\nstructure, and ii) coherent visual objects may lack clear boundaries and may be\ncomposed of connected text and visual components. Despite advancements in deep\nlearning based object detection, current models do not yield satisfactory\nperformance due to the unique nature of visual content in lectures and scarcity\nof annotated datasets. This paper reports on a transfer learning approach for\ndetecting visual elements in lecture video frames. A suite of state of the art\nobject detection models were evaluated for their performance on lecture video\ndatasets. YOLO emerged as the most promising model for this task. Subsequently\nYOLO was optimized for lecture video object detection with training on multiple\nbenchmark datasets and deploying a semi-supervised auto labeling strategy.\nResults evaluate the success of this approach, also in developing a general\nsolution to the problem of object detection in lecture videos. Paper\ncontributions include a publicly released benchmark of annotated lecture video\nframes, along with the source code to facilitate future research.", "published": "2025-06-27 04:43:05", "link": "http://arxiv.org/abs/2506.21903v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Exploring Task-Solving Paradigm for Generalized Cross-Domain Face Anti-Spoofing via Reinforcement Fine-Tuning", "abstract": "Recently the emergence of novel presentation attacks has drawn increasing\nattention to face anti-spoofing. However, existing methods tend to memorize\ndata patterns from the training set, resulting in poor generalization to\nunknown attack types across different scenarios and limited interpretability.\nTo address these challenges, this paper presents a reinforcement\nfine-tuning-based face anti-spoofing method that stimulates the capabilities of\nmultimodal large language models to think and learn how to solve the\nanti-spoofing task itself, rather than relying on the memorization of\nauthenticity patterns. We design verifiable class consistent reward and\nreasoning consistent reward, and employ a GRPO-based optimization strategy to\nguide the model in exploring reasoning policies from multiple perspectives to\nmaximize expected rewards. As a result, through iterative trial-and-error\nlearning while retaining only high-reward trajectories, the model distills\nhighly generalizable decision-making rules from the extensive solution space to\neffectively address cross-domain face anti-spoofing tasks. Extensive\nexperimental results demonstrate that our method achieves state-of-the-art\ncross-domain generalization performance. It generalizes well to diverse unknown\nattack types in unseen target domains while providing interpretable reasoning\nfor its authenticity decisions without requiring labor-intensive textual\nannotations for training.", "published": "2025-06-27 04:28:29", "link": "http://arxiv.org/abs/2506.21895v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SODA: Out-of-Distribution Detection in Domain-Shifted Point Clouds via Neighborhood Propagation", "abstract": "As point cloud data increases in prevalence in a variety of applications, the\nability to detect out-of-distribution (OOD) point cloud objects becomes\ncritical for ensuring model safety and reliability. However, this problem\nremains under-explored in existing research. Inspired by success in the image\ndomain, we propose to exploit advances in 3D vision-language models (3D VLMs)\nfor OOD detection in point cloud objects. However, a major challenge is that\npoint cloud datasets used to pre-train 3D VLMs are drastically smaller in size\nand object diversity than their image-based counterparts. Critically, they\noften contain exclusively computer-designed synthetic objects. This leads to a\nsubstantial domain shift when the model is transferred to practical tasks\ninvolving real objects scanned from the physical environment. In this paper,\nour empirical experiments show that synthetic-to-real domain shift\nsignificantly degrades the alignment of point cloud with their associated text\nembeddings in the 3D VLM latent space, hindering downstream performance. To\naddress this, we propose a novel methodology called SODA which improves the\ndetection of OOD point clouds through a neighborhood-based score propagation\nscheme. SODA is inference-based, requires no additional model training, and\nachieves state-of-the-art performance over existing approaches across datasets\nand problem settings.", "published": "2025-06-27 04:05:55", "link": "http://arxiv.org/abs/2506.21892v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "DIVE: Deep-search Iterative Video Exploration A Technical Report for the CVRR Challenge at CVPR 2025", "abstract": "In this report, we present the winning solution that achieved the 1st place\nin the Complex Video Reasoning & Robustness Evaluation Challenge 2025. This\nchallenge evaluates the ability to generate accurate natural language answers\nto questions about diverse, real-world video clips. It uses the Complex Video\nReasoning and Robustness Evaluation Suite (CVRR-ES) benchmark, which consists\nof 214 unique videos and 2,400 question-answer pairs spanning 11 categories.\nOur method, DIVE (Deep-search Iterative Video Exploration), adopts an iterative\nreasoning approach, in which each input question is semantically decomposed and\nsolved through stepwise reasoning and progressive inference. This enables our\nsystem to provide highly accurate and contextually appropriate answers to even\nthe most complex queries. Applied to the CVRR-ES benchmark, our approach\nachieves 81.44% accuracy on the test set, securing the top position among all\nparticipants. This report details our methodology and provides a comprehensive\nanalysis of the experimental results, demonstrating the effectiveness of our\niterative reasoning framework in achieving robust video question answering. The\ncode is available at https://github.com/PanasonicConnect/DIVE", "published": "2025-06-27 04:05:12", "link": "http://arxiv.org/abs/2506.21891v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Integrating Multi-Modal Sensors: A Review of Fusion Techniques for Intelligent Vehicles", "abstract": "Multi-sensor fusion plays a critical role in enhancing perception for\nautonomous driving, overcoming individual sensor limitations, and enabling\ncomprehensive environmental understanding. This paper first formalizes\nmulti-sensor fusion strategies into data-level, feature-level, and\ndecision-level categories and then provides a systematic review of deep\nlearning-based methods corresponding to each strategy. We present key\nmulti-modal datasets and discuss their applicability in addressing real-world\nchallenges, particularly in adverse weather conditions and complex urban\nenvironments. Additionally, we explore emerging trends, including the\nintegration of Vision-Language Models (VLMs), Large Language Models (LLMs), and\nthe role of sensor fusion in end-to-end autonomous driving, highlighting its\npotential to enhance system adaptability and robustness. Our work offers\nvaluable insights into current methods and future directions for multi-sensor\nfusion in autonomous driving.", "published": "2025-06-27 03:43:48", "link": "http://arxiv.org/abs/2506.21885v1", "categories": ["cs.CV", "cs.MM", "cs.RO"], "primary_category": "cs.CV"}
{"title": "UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields", "abstract": "Neural Radiance Field (NeRF)-based segmentation methods focus on object\nsemantics and rely solely on RGB data, lacking intrinsic material properties.\nThis limitation restricts accurate material perception, which is crucial for\nrobotics, augmented reality, simulation, and other applications. We introduce\nUnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling\njoint hyperspectral novel view synthesis and unsupervised material\nsegmentation. Our method models spectral reflectance via diffuse and specular\ncomponents, where a learned dictionary of global endmembers represents pure\nmaterial signatures, and per-point abundances capture their distribution. For\nmaterial segmentation, we use spectral signature predictions along learned\nendmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF\nenables scene editing by modifying learned endmember dictionaries for flexible\nmaterial-based appearance manipulation. Extensive experiments validate our\napproach, demonstrating superior spectral reconstruction and material\nsegmentation to existing methods. Project page:\nhttps://www.factral.co/UnMix-NeRF.", "published": "2025-06-27 03:42:49", "link": "http://arxiv.org/abs/2506.21884v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "eess.SP"], "primary_category": "eess.IV"}
{"title": "GRASP-PsONet: Gradient-based Removal of Spurious Patterns for PsOriasis Severity Classification", "abstract": "Psoriasis (PsO) severity scoring is important for clinical trials but is\nhindered by inter-rater variability and the burden of in person clinical\nevaluation. Remote imaging using patient captured mobile photos offers\nscalability but introduces challenges, such as variation in lighting,\nbackground, and device quality that are often imperceptible to humans but can\nimpact model performance. These factors, along with inconsistencies in\ndermatologist annotations, reduce the reliability of automated severity\nscoring. We propose a framework to automatically flag problematic training\nimages that introduce spurious correlations which degrade model generalization,\nusing a gradient based interpretability approach. By tracing the gradients of\nmisclassified validation images, we detect training samples where model errors\nalign with inconsistently rated examples or are affected by subtle, nonclinical\nartifacts. We apply this method to a ConvNeXT based weakly supervised model\ndesigned to classify PsO severity from phone images. Removing 8.2% of flagged\nimages improves model AUC-ROC by 5% (85% to 90%) on a held out test set.\nCommonly, multiple annotators and an adjudication process ensure annotation\naccuracy, which is expensive and time consuming. Our method detects training\nimages with annotation inconsistencies, potentially removing the need for\nmanual review. When applied to a subset of training data rated by two\ndermatologists, the method identifies over 90% of cases with inter-rater\ndisagreement by reviewing only the top 30% of samples. This improves automated\nscoring for remote assessments, ensuring robustness despite data collection\nvariability.", "published": "2025-06-27 03:42:09", "link": "http://arxiv.org/abs/2506.21883v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Physical Degradation Model-Guided Interferometric Hyperspectral Reconstruction with Unfolding Transformer", "abstract": "Interferometric Hyperspectral Imaging (IHI) is a critical technique for\nlarge-scale remote sensing tasks due to its advantages in flux and spectral\nresolution. However, IHI is susceptible to complex errors arising from imaging\nsteps, and its quality is limited by existing signal processing-based\nreconstruction algorithms. Two key challenges hinder performance enhancement:\n1) the lack of training datasets. 2) the difficulty in eliminating IHI-specific\ndegradation components through learning-based methods. To address these\nchallenges, we propose a novel IHI reconstruction pipeline. First, based on\nimaging physics and radiometric calibration data, we establish a simplified yet\naccurate IHI degradation model and a parameter estimation method. This model\nenables the synthesis of realistic IHI training datasets from hyperspectral\nimages (HSIs), bridging the gap between IHI reconstruction and deep learning.\nSecond, we design the Interferometric Hyperspectral Reconstruction Unfolding\nTransformer (IHRUT), which achieves effective spectral correction and detail\nrestoration through a stripe-pattern enhancement mechanism and a\nspatial-spectral transformer architecture. Experimental results demonstrate the\nsuperior performance and generalization capability of our method.", "published": "2025-06-27 03:36:00", "link": "http://arxiv.org/abs/2506.21880v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation", "abstract": "Internal world models (WMs) enable agents to understand the world's state and\npredict transitions, serving as the basis for advanced deliberative reasoning.\nRecent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and\nGemini, exhibit potential as general-purpose WMs. While the latest studies have\nevaluated and shown limitations in specific capabilities such as visual\nunderstanding, a systematic evaluation of VLMs' fundamental WM abilities\nremains absent. Drawing on comparative psychology and cognitive science, we\npropose a two-stage framework that assesses Perception (visual, spatial,\ntemporal, quantitative, and motion) and Prediction (mechanistic simulation,\ntransitive inference, compositional inference) to provide an atomic evaluation\nof VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale\nbenchmark comprising 23 fine-grained evaluation dimensions across 6 diverse\nsimulated environments with controlled counterfactual simulations. Through 660\nexperiments on 15 latest commercial and open-source VLMs, we find that these\nmodels exhibit striking limitations in basic world modeling abilities. For\ninstance, almost all models perform at near-random accuracy when distinguishing\nmotion trajectories. Additionally, they lack disentangled understanding --\ne.g., some models tend to believe blue objects move faster than green ones.\nMore rich results and analyses reveal significant gaps between VLMs and\nhuman-level world modeling.", "published": "2025-06-27 03:24:29", "link": "http://arxiv.org/abs/2506.21876v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning", "abstract": "Recent Multimodal Large Language Models (MLLMs) have demonstrated strong\nperformance in visual grounding, establishing themselves as a general interface\nfor various vision-language applications. This progress has driven the\ndevelopment of token pruning methods to mitigate the high computational costs\nassociated with processing numerous visual tokens. However, we observe that\npruning significantly weakens the model's grounding ability, leading to\nincorrect predictions and drastic performance degradation. In Referring\nExpression Comprehension (REC), for instance, pruning causes the accuracy of\nLLaVA on the RefCOCO validation set to drop from 56.14% to 15.34%. Our analysis\nidentifies misaligned position IDs after pruning as the primary cause of this\ndegradation, as both the order and value of these IDs are crucial for\nmaintaining performance in grounding tasks. To address this issue, we propose\nGrounding-Aware Token Pruning (GAP), a simple yet effective adjustment to\nposition IDs that recovers REC accuracy back to 51.42%, which is 90% of the\noriginal performance in the without pruning setting, all while requiring no\nadditional training, memory, or computational overhead. Applied to models such\nas Shikra, MiniGPTv2, and the LLaVA series, our method consistently improves\nperformance across various token pruning strategies.", "published": "2025-06-27 03:11:22", "link": "http://arxiv.org/abs/2506.21873v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Dual-Perspective United Transformer for Object Segmentation in Optical Remote Sensing Images", "abstract": "Automatically segmenting objects from optical remote sensing images (ORSIs)\nis an important task. Most existing models are primarily based on either\nconvolutional or Transformer features, each offering distinct advantages.\nExploiting both advantages is valuable research, but it presents several\nchallenges, including the heterogeneity between the two types of features, high\ncomplexity, and large parameters of the model. However, these issues are often\noverlooked in existing the ORSIs methods, causing sub-optimal segmentation. For\nthat, we propose a novel Dual-Perspective United Transformer (DPU-Former) with\na unique structure designed to simultaneously integrate long-range dependencies\nand spatial details. In particular, we design the global-local mixed attention,\nwhich captures diverse information through two perspectives and introduces a\nFourier-space merging strategy to obviate deviations for efficient fusion.\nFurthermore, we present a gated linear feed-forward network to increase the\nexpressive ability. Additionally, we construct a DPU-Former decoder to\naggregate and strength features at different layers. Consequently, the\nDPU-Former model outperforms the state-of-the-art methods on multiple datasets.\nCode: https://github.com/CSYSI/DPU-Former.", "published": "2025-06-27 02:40:48", "link": "http://arxiv.org/abs/2506.21866v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Remote Sensing Large Vision-Language Model: Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling", "abstract": "Large Vision and Language Models (LVLMs) have shown strong performance across\nvarious vision-language tasks in natural image domains. However, their\napplication to remote sensing (RS) remains underexplored due to significant\ndomain differences in visual appearances, object scales, and semantics. These\ndiscrepancies hider the effective understanding of RS scenes, which contain\nrich, multi-level semantic information spanning from coarse-to-fine levels.\nHence, it limits the direct adaptation of existing LVLMs to RS imagery. To\naddress this gap, we propose a novel LVLM framework tailored for RS\nunderstanding, incorporating two core components: Semantic-augmented\nMulti-level Alignment and Semantic-aware Expert Modeling. First, to align\nmulti-level visual features, we introduce the retrieval-based Semantic\nAugmentation Module which enriches the visual features with relevant semantics\nacross fine-to-coarse levels (e.g., object- and scene-level information). It is\ndesigned to retrieve relevant semantic cues from a RS semantic knowledge\ndatabase, followed by aggregation of semantic cues with user query and\nmulti-level visual features, resulting in semantically enriched representation\nacross multiple levels. Second, for Semantic-aware Expert Modeling, we design\nsemantic experts, where each expert is responsible for processing semantic\nrepresentation at different levels separately. This enables hierarchical\nsemantic understanding from coarse to fine levels. Evaluations across multiple\nRS tasks-including scene classification and VQA, etc.-demonstrate that the\nproposed framework achieves consistent improvements across multiple semantic\nlevels. This highlights its capability and effectiveness in bridging the gap\nbetween general LVLMs and unique demands of RS-specific vision-language\nunderstanding.", "published": "2025-06-27 02:31:37", "link": "http://arxiv.org/abs/2506.21863v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs", "abstract": "In this paper, we present LLaVA-Scissor, a training-free token compression\nstrategy designed for video multimodal large language models. Previous methods\nmostly attempt to compress tokens based on attention scores, but fail to\neffectively capture all semantic regions and often lead to token redundancy.\nDifferently, we propose to leverage the Semantic Connected Components (SCC)\napproach that assigns tokens to distinct semantic regions within the token set,\nensuring comprehensive semantic coverage. The outcome is a two-step\nspatio-temporal token compression strategy that utilizes SCC in both spatial\nand temporal domains. This strategy can effectively compress tokens by\nrepresenting the entire video with a set of non-overlapping semantic tokens. We\nconduct extensive evaluations of the token compression capabilities of\nLLaVA-Scissor across diverse video understanding benchmarks, including video\nquestion answering, long video understanding, and comprehensive multi-choices\nbenchmarks. Experimental results show that the proposed LLaVA-Scissor\noutperforms other token compression methods, achieving superior performance in\nvarious video understanding benchmarks, particularly at low token retention\nratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.", "published": "2025-06-27 02:29:58", "link": "http://arxiv.org/abs/2506.21862v1", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Embodied Domain Adaptation for Object Detection", "abstract": "Mobile robots rely on object detectors for perception and object localization\nin indoor environments. However, standard closed-set methods struggle to handle\nthe diverse objects and dynamic conditions encountered in real homes and labs.\nOpen-vocabulary object detection (OVOD), driven by Vision Language Models\n(VLMs), extends beyond fixed labels but still struggles with domain shifts in\nindoor environments. We introduce a Source-Free Domain Adaptation (SFDA)\napproach that adapts a pre-trained model without accessing source data. We\nrefine pseudo labels via temporal clustering, employ multi-scale threshold\nfusion, and apply a Mean Teacher framework with contrastive learning. Our\nEmbodied Domain Adaptation for Object Detection (EDAOD) benchmark evaluates\nadaptation under sequential changes in lighting, layout, and object diversity.\nOur experiments show significant gains in zero-shot detection performance and\nflexible adaptation to dynamic indoor conditions.", "published": "2025-06-27 02:28:19", "link": "http://arxiv.org/abs/2506.21860v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture of Data Experts for an Expressive Latent Space", "abstract": "The rapid growth of digital pathology and advances in self-supervised deep\nlearning have enabled the development of foundational models for various\npathology tasks across diverse diseases. While multimodal approaches\nintegrating diverse data sources have emerged, a critical gap remains in the\ncomprehensive integration of whole-slide images (WSIs) with spatial\ntranscriptomics (ST), which is crucial for capturing critical molecular\nheterogeneity beyond standard hematoxylin & eosin (H&E) staining. We introduce\nSPADE, a foundation model that integrates histopathology with ST data to guide\nimage representation learning within a unified framework, in effect creating an\nST-informed latent space. SPADE leverages a mixture-of-data experts technique,\nwhere experts, created via two-stage feature-space clustering, use contrastive\nlearning to learn representations of co-registered WSI patches and gene\nexpression profiles. Pre-trained on the comprehensive HEST-1k dataset, SPADE is\nevaluated on 14 downstream tasks, demonstrating significantly superior few-shot\nperformance compared to baseline models, highlighting the benefits of\nintegrating morphological and molecular information into one latent space.", "published": "2025-06-27 02:20:51", "link": "http://arxiv.org/abs/2506.21857v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Periodic-MAE: Periodic Video Masked Autoencoder for rPPG Estimation", "abstract": "In this paper, we propose a method that learns a general representation of\nperiodic signals from unlabeled facial videos by capturing subtle changes in\nskin tone over time. The proposed framework employs the video masked\nautoencoder to learn a high-dimensional spatio-temporal representation of the\nfacial region through self-supervised learning. Capturing quasi-periodic\nsignals in the video is crucial for remote photoplethysmography (rPPG)\nestimation. To account for signal periodicity, we apply frame masking in terms\nof video sampling, which allows the model to capture resampled quasi-periodic\nsignals during the pre-training stage. Moreover, the framework incorporates\nphysiological bandlimit constraints, leveraging the property that physiological\nsignals are sparse within their frequency bandwidth to provide pulse cues to\nthe model. The pre-trained encoder is then transferred to the rPPG task, where\nit is used to extract physiological signals from facial videos. We evaluate the\nproposed method through extensive experiments on the PURE, UBFC-rPPG, MMPD, and\nV4V datasets. Our results demonstrate significant performance improvements,\nparticularly in challenging cross-dataset evaluations. Our code is available at\nhttps://github.com/ziiho08/Periodic-MAE.", "published": "2025-06-27 02:18:10", "link": "http://arxiv.org/abs/2506.21855v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "End-to-End RGB-IR Joint Image Compression With Channel-wise Cross-modality Entropy Model", "abstract": "RGB-IR(RGB-Infrared) image pairs are frequently applied simultaneously in\nvarious applications like intelligent surveillance. However, as the number of\nmodalities increases, the required data storage and transmission costs also\ndouble. Therefore, efficient RGB-IR data compression is essential. This work\nproposes a joint compression framework for RGB-IR image pair. Specifically, to\nfully utilize cross-modality prior information for accurate context probability\nmodeling within and between modalities, we propose a Channel-wise\nCross-modality Entropy Model (CCEM). Among CCEM, a Low-frequency Context\nExtraction Block (LCEB) and a Low-frequency Context Fusion Block (LCFB) are\ndesigned for extracting and aggregating the global low-frequency information\nfrom both modalities, which assist the model in predicting entropy parameters\nmore accurately. Experimental results demonstrate that our approach outperforms\nexisting RGB-IR image pair and single-modality compression methods on LLVIP and\nKAIST datasets. For instance, the proposed framework achieves a 23.1% bit rate\nsaving on LLVIP dataset compared to the state-of-the-art RGB-IR image codec\npresented at CVPR 2022.", "published": "2025-06-27 02:04:21", "link": "http://arxiv.org/abs/2506.21851v1", "categories": ["cs.CV", "cs.MM", "eess.IV"], "primary_category": "cs.CV"}
{"title": "3D-Telepathy: Reconstructing 3D Objects from EEG Signals", "abstract": "Reconstructing 3D visual stimuli from Electroencephalography (EEG) data holds\nsignificant potential for applications in Brain-Computer Interfaces (BCIs) and\naiding individuals with communication disorders. Traditionally, efforts have\nfocused on converting brain activity into 2D images, neglecting the translation\nof EEG data into 3D objects. This limitation is noteworthy, as the human brain\ninherently processes three-dimensional spatial information regardless of\nwhether observing 2D images or the real world. The neural activities captured\nby EEG contain rich spatial information that is inevitably lost when\nreconstructing only 2D images, thus limiting its practical applications in BCI.\nThe transition from EEG data to 3D object reconstruction faces considerable\nobstacles. These include the presence of extensive noise within EEG signals and\na scarcity of datasets that include both EEG and 3D information, which\ncomplicates the extraction process of 3D visual data. Addressing this\nchallenging task, we propose an innovative EEG encoder architecture that\nintegrates a dual self-attention mechanism. We use a hybrid training strategy\nto train the EEG Encoder, which includes cross-attention, contrastive learning,\nand self-supervised learning techniques. Additionally, by employing stable\ndiffusion as a prior distribution and utilizing Variational Score Distillation\nto train a neural radiation field, we successfully generate 3D objects with\nsimilar content and structure from EEG data.", "published": "2025-06-27 01:26:52", "link": "http://arxiv.org/abs/2506.21843v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles", "abstract": "We challenge text-to-image models with generating escape room puzzle images\nthat are visually appealing, logically solid, and intellectually stimulating.\nWhile base image models struggle with spatial relationships and affordance\nreasoning, we propose a hierarchical multi-agent framework that decomposes this\ntask into structured stages: functional design, symbolic scene graph reasoning,\nlayout synthesis, and local image editing. Specialized agents collaborate\nthrough iterative feedback to ensure the scene is visually coherent and\nfunctionally solvable. Experiments show that agent collaboration improves\noutput quality in terms of solvability, shortcut avoidance, and affordance\nclarity, while maintaining visual quality.", "published": "2025-06-27 01:08:37", "link": "http://arxiv.org/abs/2506.21839v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ProSAM: Enhancing the Robustness of SAM-based Visual Reference Segmentation with Probabilistic Prompts", "abstract": "The recent advancements in large foundation models have driven the success of\nopen-set image segmentation, a task focused on segmenting objects beyond\npredefined categories. Among various prompt types (such as points, boxes,\ntexts, and visual references), visual reference segmentation stands out for its\nunique flexibility and strong zero-shot capabilities. Recently, several\nSAM-based methods have made notable progress in this task by automatically\ngenerating prompts to guide SAM. However, these methods often generate prompts\nat object boundaries due to suboptimal prompt encoder, which results in\ninstability and reduced robustness. In this work, we introduce ProSAM, a simple\nbut effective method to address the stability challenges we identified in\nexisting SAM-based visual reference segmentation approaches. By learning a\nvariational prompt encoder to predict multivariate prompt distributions, ProSAM\navoids generating prompts that lie in unstable regions, overcoming the\ninstability caused by less robust prompts. Our approach consistently surpasses\nstate-of-the-art methods on the Pascal-5$^i$ and COCO-20$^i$ datasets,\nproviding a more robust solution for visual reference segmentation.", "published": "2025-06-27 00:50:15", "link": "http://arxiv.org/abs/2506.21835v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "PrefPaint: Enhancing Image Inpainting through Expert Human Feedback", "abstract": "Inpainting, the process of filling missing or corrupted image parts, has\nbroad applications, including medical imaging. However, in specialized fields\nlike medical polyps imaging, where accuracy and reliability are critical,\ninpainting models can generate inaccurate images, leading to significant errors\nin medical diagnosis and treatment. To ensure reliability, medical images\nshould be annotated by experts like oncologists for effective model training.\nWe propose PrefPaint, an approach that incorporates human feedback into the\ntraining process of Stable Diffusion Inpainting, bypassing the need for\ncomputationally expensive reward models. In addition, we develop a web-based\ninterface streamlines training, fine-tuning, and inference. This interactive\ninterface provides a smooth and intuitive user experience, making it easier to\noffer feedback and manage the fine-tuning process. User study on various\ndomains shows that PrefPaint outperforms existing methods, reducing visual\ninconsistencies and improving image rendering, particularly in medical\ncontexts, where our model generates more realistic polyps images.", "published": "2025-06-27 00:47:07", "link": "http://arxiv.org/abs/2506.21834v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "TaleForge: Interactive Multimodal System for Personalized Story Creation", "abstract": "Storytelling is a deeply personal and creative process, yet existing methods\noften treat users as passive consumers, offering generic plots with limited\npersonalization. This undermines engagement and immersion, especially where\nindividual style or appearance is crucial. We introduce TaleForge, a\npersonalized story-generation system that integrates large language models\n(LLMs) and text-to-image diffusion to embed users' facial images within both\nnarratives and illustrations. TaleForge features three interconnected modules:\nStory Generation, where LLMs create narratives and character descriptions from\nuser prompts; Personalized Image Generation, merging users' faces and outfit\nchoices into character illustrations; and Background Generation, creating scene\nbackdrops that incorporate personalized characters. A user study demonstrated\nheightened engagement and ownership when individuals appeared as protagonists.\nParticipants praised the system's real-time previews and intuitive controls,\nthough they requested finer narrative editing tools. TaleForge advances\nmultimodal storytelling by aligning personalized text and imagery to create\nimmersive, user-centric experiences.", "published": "2025-06-27 00:45:38", "link": "http://arxiv.org/abs/2506.21832v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Few-Shot Segmentation of Historical Maps via Linear Probing of Vision Foundation Models", "abstract": "As rich sources of history, maps provide crucial insights into historical\nchanges, yet their diverse visual representations and limited annotated data\npose significant challenges for automated processing. We propose a simple yet\neffective approach for few-shot segmentation of historical maps, leveraging the\nrich semantic embeddings of large vision foundation models combined with\nparameter-efficient fine-tuning. Our method outperforms the state-of-the-art on\nthe Siegfried benchmark dataset in vineyard and railway segmentation, achieving\n+5% and +13% relative improvements in mIoU in 10-shot scenarios and around +20%\nin the more challenging 5-shot setting. Additionally, it demonstrates strong\nperformance on the ICDAR 2021 competition dataset, attaining a mean PQ of 67.3%\nfor building block segmentation, despite not being optimized for this\nshape-sensitive metric, underscoring its generalizability. Notably, our\napproach maintains high performance even in extremely low-data regimes (10- &\n5-shot), while requiring only 689k trainable parameters - just 0.21% of the\ntotal model size. Our approach enables precise segmentation of diverse\nhistorical maps while drastically reducing the need for manual annotations,\nadvancing automated processing and analysis in the field. Our implementation is\npublicly available at:\nhttps://github.com/RafaelSterzinger/few-shot-map-segmentation.", "published": "2025-06-27 00:07:21", "link": "http://arxiv.org/abs/2506.21826v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Conceptual Topic Aggregation", "abstract": "The vast growth of data has rendered traditional manual inspection\ninfeasible, necessitating the adoption of computational methods for efficient\ndata exploration. Topic modeling has emerged as a powerful tool for analyzing\nlarge-scale textual datasets, enabling the extraction of latent semantic\nstructures. However, existing methods for topic modeling often struggle to\nprovide interpretable representations that facilitate deeper insights into data\nstructure and content. In this paper, we propose FAT-CAT, an approach based on\nFormal Concept Analysis (FCA) to enhance meaningful topic aggregation and\nvisualization of discovered topics. Our approach can handle diverse topics and\nfile types -- grouped by directories -- to construct a concept lattice that\noffers a structured, hierarchical representation of their topic distribution.\nIn a case study on the ETYNTKE dataset, we evaluate the effectiveness of our\napproach against other representation methods to demonstrate that FCA-based\naggregation provides more meaningful and interpretable insights into dataset\ncomposition than existing topic modeling techniques.", "published": "2025-06-27 15:19:38", "link": "http://arxiv.org/abs/2506.22309v1", "categories": ["cs.AI", "cs.CL", "cs.DM", "cs.LG", "06B99", "I.2.4; I.2.7"], "primary_category": "cs.AI"}
{"title": "Fault-Tolerant Matroid Bases", "abstract": "We investigate the problem of constructing fault-tolerant bases in matroids.\nGiven a matroid M and a redundancy parameter k, a k-fault-tolerant basis is a\nminimum-size set of elements such that, even after the removal of any k\nelements, the remaining subset still spans the entire ground set. Since\nmatroids generalize linear independence across structures such as vector\nspaces, graphs, and set systems, this problem unifies and extends several\nfault-tolerant concepts appearing in prior research.\n  Our main contribution is a fixed-parameter tractable (FPT) algorithm for the\nk-fault-tolerant basis problem, parameterized by both k and the rank r of the\nmatroid. This two-variable parameterization by k + r is shown to be tight in\nthe following sense. On the one hand, the problem is already NP-hard for k=1.\nOn the other hand, it is Para-NP-hard for r \\geq 3 and polynomial-time solvable\nfor r \\leq 2.", "published": "2025-06-27 08:24:27", "link": "http://arxiv.org/abs/2506.22010v1", "categories": ["cs.DS", "cs.DM"], "primary_category": "cs.DS"}
{"title": "Exploring Modularity of Agentic Systems for Drug Discovery", "abstract": "Large-language models (LLMs) and agentic systems present exciting\nopportunities to accelerate drug discovery and design. In this study, we\ncritically examine the modularity of LLM-based agentic systems for drug\ndiscovery, i.e., whether parts of the agentic system such as the LLM are\ninterchangeable, a topic that has received limited attention in drug discovery\napplications. We compare the performance of different large language models\n(LLMs) and the effectiveness of tool-calling agents versus code-generating\nagents in this domain. Our case study, comparing performance in orchestrating\ntools for chemistry and drug discovery using an LLM-as-a-judge score, shows\nthat Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative\nlanguage models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and\nNova-Micro. Although we confirm that code-generating agents outperform the\ntool-calling ones on average, we show that this is highly question and model\ndependent. Furthermore, the impact of replacing system prompts is dependent on\nthe specific question asked and the model used, underscoring that -- even in\nthis particular domain -- one cannot just replace language models without\nconsidering prompt re-engineering. Our study highlights the necessity of\nfurther research into the modularity of agentic systems to enable the\ndevelopment of stable and scalable solutions for real-world problems.", "published": "2025-06-27 12:57:00", "link": "http://arxiv.org/abs/2506.22189v1", "categories": ["cs.LG", "cs.CL", "cs.MA"], "primary_category": "cs.LG"}
{"title": "ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation", "abstract": "Retrieval-Augmented Generation (RAG) has shown promise in enhancing\nrecommendation systems by incorporating external context into large language\nmodel prompts. However, existing RAG-based approaches often rely on static\nretrieval heuristics and fail to capture nuanced user preferences in dynamic\nrecommendation scenarios. In this work, we introduce ARAG, an Agentic\nRetrieval-Augmented Generation framework for Personalized Recommendation, which\nintegrates a multi-agent collaboration mechanism into the RAG pipeline. To\nbetter understand the long-term and session behavior of the user, ARAG\nleverages four specialized LLM-based agents: a User Understanding Agent that\nsummarizes user preferences from long-term and session contexts, a Natural\nLanguage Inference (NLI) Agent that evaluates semantic alignment between\ncandidate items retrieved by RAG and inferred intent, a context summary agent\nthat summarizes the findings of NLI agent, and an Item Ranker Agent that\ngenerates a ranked list of recommendations based on contextual fit. We evaluate\nARAG accross three datasets. Experimental results demonstrate that ARAG\nsignificantly outperforms standard RAG and recency-based baselines, achieving\nup to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an\nablation study to analyse the effect by different components of ARAG. Our\nfindings highlight the effectiveness of integrating agentic reasoning into\nretrieval-augmented recommendation and provide new directions for LLM-based\npersonalization.", "published": "2025-06-27 05:45:59", "link": "http://arxiv.org/abs/2506.21931v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.MA", "I.2.11; I.2.7; H.3.3"], "primary_category": "cs.IR"}
{"title": "Optimal Benchmark Design under Costly Manipulation", "abstract": "Price benchmarks are used to incorporate market price trends into contracts,\nbut their use can create opportunities for manipulation by parties involved in\nthe contract. This paper examines this issue using a realistic and tractable\nmodel inspired by smart contracts on blockchains like Ethereum. In our model,\nmanipulation costs depend on two factors: the magnitude of adjustments to\nindividual prices (variable costs) and the number of prices adjusted (fixed\ncosts). We find that a weighted mean is the optimal benchmark when fixed costs\nare negligible, while the median is optimal when variable costs are negligible.\nIn cases where both fixed and variable costs are significant, the optimal\nbenchmark can be implemented as a trimmed mean, with the degree of trimming\nincreasing as fixed costs become more important relative to variable costs.\nFurthermore, we show that the optimal weights for a mean-based benchmark are\nproportional to the marginal manipulation costs, whereas the median remains\noptimal without weighting, even when fixed costs differ across prices.", "published": "2025-06-27 11:39:18", "link": "http://arxiv.org/abs/2506.22142v1", "categories": ["q-fin.TR", "econ.TH", "91"], "primary_category": "q-fin.TR"}
{"title": "DiffSoundStream: Efficient Speech Tokenization via Diffusion Decoding", "abstract": "Token-based language modeling is a prominent approach for speech generation,\nwhere tokens are obtained by quantizing features from self-supervised learning\n(SSL) models and extracting codes from neural speech codecs, generally referred\nto as semantic tokens and acoustic tokens. These tokens are often modeled\nautoregressively, with the inference speed being constrained by the token rate.\nIn this work, we propose DiffSoundStream, a solution that improves the\nefficiency of speech tokenization in non-streaming scenarios through two\ntechniques: (1) conditioning the neural codec on semantic tokens to minimize\nredundancy between semantic and acoustic tokens, and (2) leveraging latent\ndiffusion models to synthesize high-quality waveforms from semantic and\ncoarse-level acoustic tokens. Experiments show that at 50 tokens per second,\nDiffSoundStream achieves speech quality on par with a standard SoundStream\nmodel operating at twice the token rate. Additionally, we achieve step-size\ndistillation using just four diffusion sampling steps with only a minor quality\nloss.", "published": "2025-06-27 16:23:07", "link": "http://arxiv.org/abs/2506.22362v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "A Practical Approach to Power Saving in Hearables Using Sub-Nyquist Sampling with Bandwidth Extension", "abstract": "Hearables are wearable computers that are worn on the ear. Bone conduction\nmicrophones (BCMs) are used with air conduction microphones (ACMs) in hearables\nas a supporting modality for multimodal speech enhancement (SE) in noisy\nconditions. However, existing works don't consider the following practical\naspects for low-power implementations on hearables: (i) They do not explore how\nlowering the sampling frequencies and bit resolutions in analog-to-digital\nconverters (ADCs) of hearables jointly impact low-power processing and\nmultimodal SE in terms of speech quality and intelligibility. (ii) They don't\ndiscuss how GAN-like audio quality can be achieved without using actual GAN\ndiscriminators. And (iii) They don't process signals from ACMs/BCMs at\nsub-Nyquist sampling rate because, in their frameworks, they lack a wideband\nreconstruction methodology from their narrowband parts. We propose SUBARU\n(\\textbf{Sub}-Nyquist \\textbf{A}udio \\textbf{R}esolution \\textbf{U}psampling),\nwhich achieves the following: SUBARU (i) intentionally uses sub-Nyquist\nsampling and low bit resolution in ADCs, achieving a 3.31x reduction in power\nconsumption; (ii) introduces novel multi-scale and multi-period virtual\ndiscriminators, which achieve GAN-like audio quality without using GANs'\nadversarial training; and (iii) achieves streaming operations on mobile\nplatforms and SE in in-the-wild noisy conditions with an inference time of\n1.74ms and a memory footprint of less than 13.77MB.", "published": "2025-06-27 15:35:04", "link": "http://arxiv.org/abs/2506.22321v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Reconstructing Intelligible Speech from the Pressure Sensor Data in HVACs", "abstract": "Pressure sensors are an integrated component of modern Heating, Ventilation,\nand Air Conditioning (HVAC) systems. As these pressure sensors operate within\nthe 0-10 Pa range, support high sampling frequencies of 0.5-2 kHz, and are\noften placed close to human proximity, they can be used to eavesdrop on\nconfidential conversation, since human speech has a similar audible range of\n0-10 Pa and a bandwidth of 4 kHz for intelligible quality. This paper presents\nWaLi, which reconstructs intelligible speech from the low-resolution and noisy\npressure sensor data by providing the following technical contributions: (i)\nWaLi reconstructs intelligible speech from a minimum of 0.5 kHz sampling\nfrequency of pressure sensors, whereas previous work can only detect hot\nwords/phrases. WaLi uses complex-valued conformer and Complex Global Attention\nBlock (CGAB) to capture inter-phoneme and intra-phoneme dependencies that exist\nin the low-resolution pressure sensor data. (ii) WaLi handles the transient\nnoise injected from HVAC fans and duct vibrations, by reconstructing both the\nclean magnitude and phase of the missing frequencies of the low-frequency\naliased components. Extensive measurement studies on real-world pressure\nsensors show an LSD of 1.24 and NISQA-MOS of 1.78 for 0.5 kHz to 8 kHz\nupsampling. We believe that such levels of accuracy pose a significant threat\nwhen viewed from a privacy perspective that has not been addressed before for\npressure sensors.", "published": "2025-06-27 15:20:25", "link": "http://arxiv.org/abs/2506.22311v1", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fine-Tuning MIDI-to-Audio Alignment using a Neural Network on Piano Roll and CQT Representations", "abstract": "In this paper, we present a neural network approach for synchronizing audio\nrecordings of human piano performances with their corresponding loosely aligned\nMIDI files. The task is addressed using a Convolutional Recurrent Neural\nNetwork (CRNN) architecture, which effectively captures spectral and temporal\nfeatures by processing an unaligned piano roll and a spectrogram as inputs to\nestimate the aligned piano roll. To train the network, we create a dataset of\npiano pieces with augmented MIDI files that simulate common human timing\nerrors. The proposed model achieves up to 20% higher alignment accuracy than\nthe industry-standard Dynamic Time Warping (DTW) method across various\ntolerance windows. Furthermore, integrating DTW with the CRNN yields additional\nimprovements, offering enhanced robustness and consistency. These findings\ndemonstrate the potential of neural networks in advancing state-of-the-art\nMIDI-to-audio alignment.", "published": "2025-06-27 13:59:50", "link": "http://arxiv.org/abs/2506.22237v1", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cross-lingual Data Selection Using Clip-level Acoustic Similarity for Enhancing Low-resource Automatic Speech Recognition", "abstract": "This paper presents a novel donor data selection method to enhance\nlow-resource automatic speech recognition (ASR). While ASR performs well in\nhigh-resource languages, its accuracy declines in low-resource settings due to\nlimited training data. A common solution is to leverage multilingual\nself-supervised learning (SSL) models with donor languages. However, existing\nmethods rely on language-level similarity, overlooking clip-level variations.\nTo address this limitation, we propose clip-wise acoustic token distribution\nsimilarity (CATDS), a fine-grained selection method that identifies\nacoustically relevant donor clips for better alignment with the target\nlanguage. Unlike existing clip-level selection methods, our method aligns with\nthe representation of SSL models and offers more challenging yet valuable\nsamples. Experimental results show that CATDS outperforms traditional selection\nmethods and can even utilize donor languages previously considered detrimental.", "published": "2025-06-27 13:00:53", "link": "http://arxiv.org/abs/2506.22194v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in Low-Resource Arabic-English Code-Switched Speech Recognition", "abstract": "This paper investigates the performance of various speech SSL models on\ndialectal Arabic (DA) and Arabic-English code-switched (CS) speech. To address\ndata scarcity, a modified audio-splicing approach is introduced to generate\nartificial CS speech data. Fine-tuning an already fine-tuned SSL model with the\nproposed Spliced-Audio Generated (SAGE) data results in an absolute improvement\non Word Error Rate (WER) of 7.8% on Arabic and English CS benchmarks.\nAdditionally, an Experience Replay (ER) inspired approach is proposed to\nenhance generalisation across DA and CS speech while mitigating catastrophic\nforgetting. Integrating an out-of-domain 3-gram language model reduces the\noverall mean WER from 31.7% to 26.6%. Few-shot fine-tuning for code-switching\nbenchmarks further improves WER by 4.9%. A WER of 31.1% on Arabic-English CS\nbenchmarks surpasses large-scale multilingual models, including USM and\nWhisper-large-v2 (both over ten times larger) by an absolute margin of 5.5% and\n8.4%, respectively.", "published": "2025-06-27 11:42:43", "link": "http://arxiv.org/abs/2506.22143v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Robust and Efficient Autoregressive Speech Synthesis with Dynamic Chunk-wise Prediction Policy", "abstract": "Recently, autoregressive (AR) language models have emerged as a dominant\napproach in speech synthesis, offering expressive generation and scalable\ntraining. However, conventional AR speech synthesis models relying on the\nnext-token prediction paradigm often encounter significant challenges when\nhandling long speech sequences. These models often struggle to construct stable\nframe-to-frame attention, leading to increased latency and degraded synthesis\nquality, thereby limiting their feasibility for real-time applications. To\naddress these limitations, we introduce a novel dynamic chunk-wise\nautoregressive synthesis framework, termed DCAR, designed to enhance both\nefficiency and intelligibility robustness in AR speech generation. DCAR\nintroduces a chunk-to-frame attention mechanism through training with\nmulti-token prediction, enabling dynamic chunk prediction in variable speech\ncontexts using a lightweight module trained on-policy. DCAR dynamically adjusts\nthe token prediction span, significantly reducing the sequence length\ndependency while obtaining high synthesis quality. Comprehensive empirical\nevaluations demonstrate that DCAR substantially outperforms traditional\nnext-token prediction models, achieving up to 72.27% intelligibility\nimprovement and 2.61x inference speedup simultaneously on the test set.\nFurthermore, we conduct comprehensive analysis to support it as a versatile\nfoundation for next-generation speech synthesis systems.", "published": "2025-06-27 08:45:21", "link": "http://arxiv.org/abs/2506.22023v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "WTFormer: A Wavelet Conformer Network for MIMO Speech Enhancement with Spatial Cues Peservation", "abstract": "Current multi-channel speech enhancement systems mainly adopt single-output\narchitecture, which face significant challenges in preserving spatio-temporal\nsignal integrity during multiple-input multiple-output (MIMO) processing. To\naddress this limitation, we propose a novel neural network, termed WTFormer,\nfor MIMO speech enhancement that leverages the multi-resolution characteristics\nof wavelet transform and multi-dimensional collaborative attention to\neffectively capture globally distributed spatial features, while using\nConformer for time-frequency modeling. A multi task loss strategy accompanying\nMUSIC algorithm is further proposed for optimization training to protect\nspatial information to the greatest extent. Experimental results on the\nLibriSpeech dataset show that WTFormer can achieve comparable denoising\nperformance to advanced systems while preserving more spatial information with\nonly 0.98M parameters.", "published": "2025-06-27 08:14:17", "link": "http://arxiv.org/abs/2506.22001v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech Transcription in the Cockpit", "abstract": "The developments in transformer encoder-decoder architectures have led to\nsignificant breakthroughs in machine translation, Automatic Speech Recognition\n(ASR), and instruction-based chat machines, among other applications. The\npre-trained models were trained on vast amounts of generic data over a few\nepochs (fewer than five in most cases), resulting in their strong\ngeneralization capabilities. Nevertheless, the performance of these models does\nsuffer when applied to niche domains like transcribing pilot speech in the\ncockpit, which involves a lot of specific vocabulary and multilingual\nconversations. This paper investigates and improves the transcription accuracy\nof cockpit conversations with Whisper models. We have collected around 85\nminutes of cockpit simulator recordings and 130 minutes of interview recordings\nwith pilots and manually labeled them. The speakers are middle aged men\nspeaking both German and English. To improve the accuracy of transcriptions, we\npropose multiple normalization schemes to refine the transcripts and improve\nWord Error Rate (WER). We then employ fine-tuning to enhance ASR performance,\nutilizing performance-efficient fine-tuning with Low-Rank Adaptation (LoRA).\nHereby, WER decreased from 68.49 \\% (pretrained whisper Large model without\nnormalization baseline) to 26.26\\% (finetuned whisper Large model with the\nproposed normalization scheme).", "published": "2025-06-27 07:57:13", "link": "http://arxiv.org/abs/2506.21990v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "HighRateMOS: Sampling-Rate Aware Modeling for Speech Quality Assessment", "abstract": "Modern speech quality prediction models are trained on audio data resampled\nto a specific sampling rate. When faced with higher-rate audio at test time,\nthese models can produce biased scores. We introduce HighRateMOS, the first\nnon-intrusive mean opinion score (MOS) model that explicitly considers sampling\nrate. HighRateMOS ensembles three model variants that exploit the following\ninformation: (i) a learnable embedding of speech sampling rate, (ii) Wav2vec\n2.0 self-supervised embeddings, (iii) multi-scale CNN spectral features, and\n(iv) MFCC features. In AudioMOS 2025 Track3, HighRateMOS ranked first in five\nout of eight metrics. Our experiments confirm that modeling the sampling rate\ndirectly leads to more robust and sampling-rate-agnostic speech quality\npredictions.", "published": "2025-06-27 06:46:31", "link": "http://arxiv.org/abs/2506.21951v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Explainable anomaly detection for sound spectrograms using pooling statistics with quantile differences", "abstract": "Anomaly detection is the task of identifying rarely occurring (i.e. anormal\nor anomalous) samples that differ from almost all other samples in a dataset.\nAs the patterns of anormal samples are usually not known a priori, this task is\nhighly challenging. Consequently, anomaly detection lies between semi- and\nunsupervised learning. The detection of anomalies in sound data, often called\n'ASD' (Anomalous Sound Detection), is a sub-field that deals with the\nidentification of new and yet unknown effects in acoustic recordings. It is of\ngreat importance for various applications in Industry 4.0. Here, vibrational or\nacoustic data are typically obtained from standard sensor signals used for\npredictive maintenance. Examples cover machine condition monitoring or quality\nassurance to track the state of components or products. However, the use of\nintelligent algorithms remains a controversial topic. Management generally aims\nfor cost-reduction and automation, while quality and maintenance experts\nemphasize the need for human expertise and comprehensible solutions. In this\nwork, we present an anomaly detection approach specifically designed for\nspectrograms. The approach is based on statistical evaluations and is\ntheoretically motivated. In addition, it features intrinsic explainability,\nmaking it particularly suitable for applications in industrial settings. Thus,\nthis algorithm is of relevance for applications in which black-box algorithms\nare unwanted or unsuitable.", "published": "2025-06-27 05:21:20", "link": "http://arxiv.org/abs/2506.21921v1", "categories": ["stat.AP", "cs.SD", "eess.AS", "stat.CO", "62", "G.3"], "primary_category": "stat.AP"}
