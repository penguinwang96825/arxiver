{"title": "A Benchmark Corpus for the Detection of Automatically Generated Text in\n  Academic Publications", "abstract": "Automatic text generation based on neural language models has achieved\nperformance levels that make the generated text almost indistinguishable from\nthose written by humans. Despite the value that text generation can have in\nvarious applications, it can also be employed for malicious tasks. The\ndiffusion of such practices represent a threat to the quality of academic\npublishing. To address these problems, we propose in this paper two datasets\ncomprised of artificially generated research content: a completely synthetic\ndataset and a partial text substitution dataset. In the first case, the content\nis completely generated by the GPT-2 model after a short prompt extracted from\noriginal papers. The partial or hybrid dataset is created by replacing several\nsentences of abstracts with sentences that are generated by the Arxiv-NLP\nmodel. We evaluate the quality of the datasets comparing the generated texts to\naligned original texts using fluency metrics such as BLEU and ROUGE. The more\nnatural the artificial texts seem, the more difficult they are to detect and\nthe better is the benchmark. We also evaluate the difficulty of the task of\ndistinguishing original from generated text by using state-of-the-art\nclassification models.", "published": "2022-02-04 08:16:56", "link": "http://arxiv.org/abs/2202.02013v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Temporal Attention for Language Models", "abstract": "Pretrained language models based on the transformer architecture have shown\ngreat success in NLP. Textual training data often comes from the web and is\nthus tagged with time-specific information, but most language models ignore\nthis information. They are trained on the textual data alone, limiting their\nability to generalize temporally. In this work, we extend the key component of\nthe transformer architecture, i.e., the self-attention mechanism, and propose\ntemporal attention - a time-aware self-attention mechanism. Temporal attention\ncan be applied to any transformer model and requires the input texts to be\naccompanied with their relevant time points. It allows the transformer to\ncapture this temporal information and create time-specific contextualized word\nrepresentations. We leverage these representations for the task of semantic\nchange detection; we apply our proposed mechanism to BERT and experiment on\nthree datasets in different languages (English, German, and Latin) that also\nvary in time, size, and genre. Our proposed model achieves state-of-the-art\nresults on all the datasets.", "published": "2022-02-04 11:55:34", "link": "http://arxiv.org/abs/2202.02093v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Ecological Footprint of Neural Machine Translation Systems", "abstract": "Over the past decade, deep learning (DL) has led to significant advancements\nin various fields of artificial intelligence, including machine translation\n(MT). These advancements would not be possible without the ever-growing volumes\nof data and the hardware that allows large DL models to be trained efficiently.\nDue to the large amount of computing cores as well as dedicated memory,\ngraphics processing units (GPUs) are a more effective hardware solution for\ntraining and inference with DL models than central processing units (CPUs).\nHowever, the former is very power demanding. The electrical power consumption\nhas economical as well as ecological implications.\n  This chapter focuses on the ecological footprint of neural MT systems. It\nstarts from the power drain during the training of and the inference with\nneural MT models and moves towards the environment impact, in terms of carbon\ndioxide emissions. Different architectures (RNN and Transformer) and different\nGPUs (consumer-grate NVidia 1080Ti and workstation-grade NVidia P100) are\ncompared. Then, the overall CO2 offload is calculated for Ireland and the\nNetherlands. The NMT models and their ecological impact are compared to common\nhousehold appliances to draw a more clear picture.\n  The last part of this chapter analyses quantization, a technique for reducing\nthe size and complexity of models, as a way to reduce power consumption. As\nquantized models can run on CPUs, they present a power-efficient inference\nsolution without depending on a GPU.", "published": "2022-02-04 14:56:41", "link": "http://arxiv.org/abs/2202.02170v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pir\u00e1: A Bilingual Portuguese-English Dataset for Question-Answering\n  about the Ocean", "abstract": "Current research in natural language processing is highly dependent on\ncarefully produced corpora. Most existing resources focus on English; some\nresources focus on languages such as Chinese and French; few resources deal\nwith more than one language. This paper presents the Pir\\'a dataset, a large\nset of questions and answers about the ocean and the Brazilian coast both in\nPortuguese and English. Pir\\'a is, to the best of our knowledge, the first QA\ndataset with supporting texts in Portuguese, and, perhaps more importantly, the\nfirst bilingual QA dataset that includes this language. The Pir\\'a dataset\nconsists of 2261 properly curated question/answer (QA) sets in both languages.\nThe QA sets were manually created based on two corpora: abstracts related to\nthe Brazilian coast and excerpts of United Nation reports about the ocean. The\nQA sets were validated in a peer-review process with the dataset contributors.\nWe discuss some of the advantages as well as limitations of Pir\\'a, as this new\nresource can support a set of tasks in NLP such as question-answering,\ninformation retrieval, and machine translation.", "published": "2022-02-04 21:29:45", "link": "http://arxiv.org/abs/2202.02398v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Aspect-Based Sentiment Analysis", "abstract": "Aspect-based sentiment analysis (ABSA) typically requires in-domain annotated\ndata for supervised training/fine-tuning. It is a big challenge to scale ABSA\nto a large number of new domains. This paper aims to train a unified model that\ncan perform zero-shot ABSA without using any annotated data for a new domain.\nWe propose a method called contrastive post-training on review Natural Language\nInference (CORN). Later ABSA tasks can be cast into NLI for zero-shot transfer.\nWe evaluate CORN on ABSA tasks, ranging from aspect extraction (AE), aspect\nsentiment classification (ASC), to end-to-end aspect-based sentiment analysis\n(E2E ABSA), which show ABSA can be conducted without any human annotated ABSA\ndata.", "published": "2022-02-04 00:51:46", "link": "http://arxiv.org/abs/2202.01924v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Grounding Answers for Visual Questions Asked by Visually Impaired People", "abstract": "Visual question answering is the task of answering questions about images. We\nintroduce the VizWiz-VQA-Grounding dataset, the first dataset that visually\ngrounds answers to visual questions asked by people with visual impairments. We\nanalyze our dataset and compare it with five VQA-Grounding datasets to\ndemonstrate what makes it similar and different. We then evaluate the SOTA VQA\nand VQA-Grounding models and demonstrate that current SOTA algorithms often\nfail to identify the correct visual evidence where the answer is located. These\nmodels regularly struggle when the visual evidence occupies a small fraction of\nthe image, for images that are higher quality, as well as for visual questions\nthat require skills in text recognition. The dataset, evaluation server, and\nleaderboard all can be found at the following link:\nhttps://vizwiz.org/tasks-and-datasets/answer-grounding-for-vqa/.", "published": "2022-02-04 06:47:16", "link": "http://arxiv.org/abs/2202.01993v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Data Scaling Laws in NMT: The Effect of Noise and Architecture", "abstract": "In this work, we study the effect of varying the architecture and training\ndata quality on the data scaling properties of Neural Machine Translation\n(NMT). First, we establish that the test loss of encoder-decoder transformer\nmodels scales as a power law in the number of training samples, with a\ndependence on the model size. Then, we systematically vary aspects of the\ntraining setup to understand how they impact the data scaling laws. In\nparticular, we change the following (1) Architecture and task setup: We compare\nto a transformer-LSTM hybrid, and a decoder-only transformer with a language\nmodeling loss (2) Noise level in the training distribution: We experiment with\nfiltering, and adding iid synthetic noise. In all the above cases, we find that\nthe data scaling exponents are minimally impacted, suggesting that marginally\nworse architectures or training data can be compensated for by adding more\ndata. Lastly, we find that using back-translated data instead of parallel data,\ncan significantly degrade the scaling exponent.", "published": "2022-02-04 06:53:49", "link": "http://arxiv.org/abs/2202.01994v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Tracking Discourse Influence in Darknet Forums", "abstract": "This technical report documents our efforts in addressing the tasks set forth\nby the 2021 AMoC (Advanced Modelling of Cyber Criminal Careers) Hackathon. Our\nmain contribution is a joint visualisation of semantic and temporal features,\ngenerating insight into the supplied data on darknet cybercrime through the\naspects of novelty, transience, and resonance, which describe the potential\nimpact a message might have on the overall discourse in darknet communities.\nAll code and data produced by us as part of this hackathon is publicly\navailable.", "published": "2022-02-04 11:23:27", "link": "http://arxiv.org/abs/2202.02081v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Webly Supervised Concept Expansion for General Purpose Vision Models", "abstract": "General Purpose Vision (GPV) systems are models that are designed to solve a\nwide array of visual tasks without requiring architectural changes. Today, GPVs\nprimarily learn both skills and concepts from large fully supervised datasets.\nScaling GPVs to tens of thousands of concepts by acquiring data to learn each\nconcept for every skill quickly becomes prohibitive. This work presents an\neffective and inexpensive alternative: learn skills from supervised datasets,\nlearn concepts from web image search, and leverage a key characteristic of\nGPVs: the ability to transfer visual knowledge across skills. We use a dataset\nof 1M+ images spanning 10k+ visual concepts to demonstrate webly-supervised\nconcept expansion for two existing GPVs (GPV-1 and VL-T5) on 3 benchmarks: 5\nCOCO-based datasets (80 primary concepts), a newly curated series of 5 datasets\nbased on the OpenImages and VisualGenome repositories (~500 concepts), and the\nWeb-derived dataset (10k+ concepts). We also propose a new architecture, GPV-2\nthat supports a variety of tasks -- from vision tasks like classification and\nlocalization to vision+language tasks like QA and captioning, to more niche\nones like human-object interaction detection. GPV-2 benefits hugely from web\ndata and outperforms GPV-1 and VL-T5 across these benchmarks. Our data, code,\nand web demo are available at https://prior.allenai.org/projects/gpv2.", "published": "2022-02-04 18:58:36", "link": "http://arxiv.org/abs/2202.02317v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Fairness for Text Classification Tasks with Identity Information Data\n  Augmentation Methods", "abstract": "Counterfactual fairness methods address the question: How would the\nprediction change if the sensitive identity attributes referenced in the text\ninstance were different? These methods are entirely based on generating\ncounterfactuals for the given training and test set instances. Counterfactual\ninstances are commonly prepared by replacing sensitive identity terms, i.e.,\nthe identity terms present in the instance are replaced with other identity\nterms that fall under the same sensitive category. Therefore, the efficacy of\nthese methods depends heavily on the quality and comprehensiveness of identity\npairs. In this paper, we offer a two-step data augmentation process where (1)\nthe former stage consists of a novel method for preparing a comprehensive list\nof identity pairs with word embeddings, and (2) the latter consists of\nleveraging prepared identity pairs list to enhance the training instances by\napplying three simple operations (namely identity pair replacement, identity\nterm blindness, and identity pair swap). We empirically show that the two-stage\naugmentation process leads to diverse identity pairs and an enhanced training\nset, with an improved counterfactual token-based fairness metric score on two\nwell-known text classification tasks.", "published": "2022-02-04 07:08:30", "link": "http://arxiv.org/abs/2203.03541v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "StonkBERT: Can Language Models Predict Medium-Run Stock Price Movements?", "abstract": "To answer this question, we fine-tune transformer-based language models,\nincluding BERT, on different sources of company-related text data for a\nclassification task to predict the one-year stock price performance. We use\nthree different types of text data: News articles, blogs, and annual reports.\nThis allows us to analyze to what extent the performance of language models is\ndependent on the type of the underlying document. StonkBERT, our\ntransformer-based stock performance classifier, shows substantial improvement\nin predictive accuracy compared to traditional language models. The highest\nperformance was achieved with news articles as text source. Performance\nsimulations indicate that these improvements in classification accuracy also\ntranslate into above-average stock market returns.", "published": "2022-02-04 17:50:53", "link": "http://arxiv.org/abs/2202.02268v1", "categories": ["cs.CL", "cs.AI", "cs.CE", "q-fin.CP"], "primary_category": "cs.CL"}
{"title": "Pre-Trained Neural Language Models for Automatic Mobile App User\n  Feedback Answer Generation", "abstract": "Studies show that developers' answers to the mobile app users' feedbacks on\napp stores can increase the apps' star rating. To help app developers generate\nanswers that are related to the users' issues, recent studies develop models to\ngenerate the answers automatically. Aims: The app response generation models\nuse deep neural networks and require training data. Pre-Trained neural language\nModels (PTM) used in Natural Language Processing (NLP) take advantage of the\ninformation they learned from a large corpora in an unsupervised manner, and\ncan reduce the amount of required training data. In this paper, we evaluate\nPTMs to generate replies to the mobile app user feedbacks. Method: We train a\nTransformer model from scratch and fine-tune two PTMs to evaluate the generated\nresponses, which are compared to RRGEN, a current app response model. We also\nevaluate the models with different portions of the training data. Results: The\nresults on a large dataset evaluated by automatic metrics show that PTMs obtain\nlower scores than the baselines. However, our human evaluation confirms that\nPTMs can generate more relevant and meaningful responses to the posted\nfeedbacks. Moreover, the performance of PTMs has less drop compared to other\nmodels when the amount of training data is reduced to 1/3. Conclusion: PTMs are\nuseful in generating responses to app reviews and are more robust models to the\namount of training data provided. However, the prediction time is 19X than\nRRGEN. This study can provide new avenues for research in adapting the PTMs for\nanalyzing mobile app user feedbacks. Index Terms-mobile app user feedback\nanalysis, neural pre-trained language models, automatic answer generation", "published": "2022-02-04 18:26:55", "link": "http://arxiv.org/abs/2202.02294v1", "categories": ["cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "A Dataset for Interactive Vision-Language Navigation with Unknown\n  Command Feasibility", "abstract": "Vision-language navigation (VLN), in which an agent follows language\ninstruction in a visual environment, has been studied under the premise that\nthe input command is fully feasible in the environment. Yet in practice, a\nrequest may not be possible due to language ambiguity or environment changes.\nTo study VLN with unknown command feasibility, we introduce a new dataset\nMobile app Tasks with Iterative Feedback (MoTIF), where the goal is to complete\na natural language command in a mobile app. Mobile apps provide a scalable\ndomain to study real downstream uses of VLN methods. Moreover, mobile app\ncommands provide instruction for interactive navigation, as they result in\naction sequences with state changes via clicking, typing, or swiping. MoTIF is\nthe first to include feasibility annotations, containing both binary\nfeasibility labels and fine-grained labels for why tasks are unsatisfiable. We\nfurther collect follow-up questions for ambiguous queries to enable research on\ntask uncertainty resolution. Equipped with our dataset, we propose the new\nproblem of feasibility prediction, in which a natural language instruction and\nmultimodal app environment are used to predict command feasibility. MoTIF\nprovides a more realistic app dataset as it contains many diverse environments,\nhigh-level goals, and longer action sequences than prior work. We evaluate\ninteractive VLN methods using MoTIF, quantify the generalization ability of\ncurrent approaches to new app environments, and measure the effect of task\nfeasibility on navigation performance.", "published": "2022-02-04 18:51:50", "link": "http://arxiv.org/abs/2202.02312v3", "categories": ["cs.CL", "cs.CV", "cs.HC"], "primary_category": "cs.CL"}
{"title": "JARVix at SemEval-2022 Task 2: It Takes One to Know One? Idiomaticity\n  Detection using Zero and One-Shot Learning", "abstract": "Large Language Models have been successful in a wide variety of Natural\nLanguage Processing tasks by capturing the compositionality of the text\nrepresentations. In spite of their great success, these vector representations\nfail to capture meaning of idiomatic multi-word expressions (MWEs). In this\npaper, we focus on the detection of idiomatic expressions by using binary\nclassification. We use a dataset consisting of the literal and idiomatic usage\nof MWEs in English and Portuguese. Thereafter, we perform the classification in\ntwo different settings: zero shot and one shot, to determine if a given\nsentence contains an idiom or not. N shot classification for this task is\ndefined by N number of common idioms between the training and testing sets. In\nthis paper, we train multiple Large Language Models in both the settings and\nachieve an F1 score (macro) of 0.73 for the zero shot setting and an F1 score\n(macro) of 0.85 for the one shot setting. An implementation of our work can be\nfound at\nhttps://github.com/ashwinpathak20/Idiomaticity_Detection_Using_Few_Shot_Learning.", "published": "2022-02-04 21:17:41", "link": "http://arxiv.org/abs/2202.02394v6", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transformers and the representation of biomedical background knowledge", "abstract": "Specialised transformers-based models (such as BioBERT and BioMegatron) are\nadapted for the biomedical domain based on publicly available biomedical\ncorpora. As such, they have the potential to encode large-scale biological\nknowledge. We investigate the encoding and representation of biological\nknowledge in these models, and its potential utility to support inference in\ncancer precision medicine - namely, the interpretation of the clinical\nsignificance of genomic alterations. We compare the performance of different\ntransformer baselines; we use probing to determine the consistency of encodings\nfor distinct entities; and we use clustering methods to compare and contrast\nthe internal properties of the embeddings for genes, variants, drugs and\ndiseases. We show that these models do indeed encode biological knowledge,\nalthough some of this is lost in fine-tuning for specific tasks. Finally, we\nanalyse how the models behave with regard to biases and imbalances in the\ndataset.", "published": "2022-02-04 23:24:18", "link": "http://arxiv.org/abs/2202.02432v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "From Discrimination to Generation: Knowledge Graph Completion with\n  Generative Transformer", "abstract": "Knowledge graph completion aims to address the problem of extending a KG with\nmissing triples. In this paper, we provide an approach GenKGC, which converts\nknowledge graph completion to sequence-to-sequence generation task with the\npre-trained language model. We further introduce relation-guided demonstration\nand entity-aware hierarchical decoding for better representation learning and\nfast inference. Experimental results on three datasets show that our approach\ncan obtain better or comparable performance than baselines and achieve faster\ninference speed compared with previous methods with pre-trained language\nmodels. We also release a new large-scale Chinese knowledge graph dataset\nAliopenKG500 for research purpose. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKG/tree/main/GenKGC.", "published": "2022-02-04 12:52:32", "link": "http://arxiv.org/abs/2202.02113v7", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Logic Analogy Learning", "abstract": "Letter-string analogy is an important analogy learning task which seems to be\neasy for humans but very challenging for machines. The main idea behind current\napproaches to solving letter-string analogies is to design heuristic rules for\nextracting analogy structures and constructing analogy mappings. However, one\nkey problem is that it is difficult to build a comprehensive and exhaustive set\nof analogy structures which can fully describe the subtlety of analogies. This\nproblem makes current approaches unable to handle complicated letter-string\nanalogy problems. In this paper, we propose Neural logic analogy learning\n(Noan), which is a dynamic neural architecture driven by differentiable logic\nreasoning to solve analogy problems. Each analogy problem is converted into\nlogical expressions consisting of logical variables and basic logical\noperations (AND, OR, and NOT). More specifically, Noan learns the logical\nvariables as vector embeddings and learns each logical operation as a neural\nmodule. In this way, the model builds computational graph integrating neural\nnetwork with logical reasoning to capture the internal logical structure of the\ninput letter strings. The analogy learning problem then becomes a True/False\nevaluation problem of the logical expressions. Experiments show that our\nmachine learning-based Noan approach outperforms state-of-the-art approaches on\nstandard letter-string analogy benchmark datasets.", "published": "2022-02-04 23:35:53", "link": "http://arxiv.org/abs/2202.02436v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.LO"], "primary_category": "cs.CL"}
{"title": "The CUHK-TENCENT speaker diarization system for the ICASSP 2022\n  multi-channel multi-party meeting transcription challenge", "abstract": "This paper describes our speaker diarization system submitted to the\nMulti-channel Multi-party Meeting Transcription (M2MeT) challenge, where\nMandarin meeting data were recorded in multi-channel format for diarization and\nautomatic speech recognition (ASR) tasks. In these meeting scenarios, the\nuncertainty of the speaker number and the high ratio of overlapped speech\npresent great challenges for diarization. Based on the assumption that there is\nvaluable complementary information between acoustic features, spatial-related\nand speaker-related features, we propose a multi-level feature fusion mechanism\nbased target-speaker voice activity detection (FFM-TS-VAD) system to improve\nthe performance of the conventional TS-VAD system. Furthermore, we propose a\ndata augmentation method during training to improve the system robustness when\nthe angular difference between two speakers is relatively small. We provide\ncomparisons for different sub-systems we used in M2MeT challenge. Our\nsubmission is a fusion of several sub-systems and ranks second in the\ndiarization task.", "published": "2022-02-04 05:43:38", "link": "http://arxiv.org/abs/2202.01986v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Polyphonic pitch detection with convolutional recurrent neural networks", "abstract": "Recent directions in automatic speech recognition (ASR) research have shown\nthat applying deep learning models from image recognition challenges in\ncomputer vision is beneficial. As automatic music transcription (AMT) is\nsuperficially similar to ASR, in the sense that methods often rely on\ntransforming spectrograms to symbolic sequences of events (e.g. words or\nnotes), deep learning should benefit AMT as well. In this work, we outline an\nonline polyphonic pitch detection system that streams audio to MIDI by\nConvLSTMs. Our system achieves state-of-the-art results on the 2007 MIREX\nmulti-F0 development set, with an F-measure of 83\\% on the bassoon, clarinet,\nflute, horn and oboe ensemble recording without requiring any musical language\nmodelling or assumptions of instrument timbre.", "published": "2022-02-04 12:58:02", "link": "http://arxiv.org/abs/2202.02115v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Musical Audio Similarity with Self-supervised Convolutional Neural\n  Networks", "abstract": "We have built a music similarity search engine that lets video producers\nsearch by listenable music excerpts, as a complement to traditional full-text\nsearch. Our system suggests similar sounding track segments in a large music\ncatalog by training a self-supervised convolutional neural network with triplet\nloss terms and musical transformations. Semi-structured user interviews\ndemonstrate that we can successfully impress professional video producers with\nthe quality of the search experience, and perceived similarities to query\ntracks averaged 7.8/10 in user testing. We believe this search tool will make\nfor a more natural search experience that is easier to find music to soundtrack\nvideos with.", "published": "2022-02-04 12:51:16", "link": "http://arxiv.org/abs/2202.02112v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
