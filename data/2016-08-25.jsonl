{"title": "A Context-aware Natural Language Generator for Dialogue Systems", "abstract": "We present a novel natural language generation system for spoken dialogue\nsystems capable of entraining (adapting) to users' way of speaking, providing\ncontextually appropriate responses. The generator is based on recurrent neural\nnetworks and the sequence-to-sequence approach. It is fully trainable from data\nwhich include preceding context along with responses to be generated. We show\nthat the context-aware generator yields significant improvements over the\nbaseline in both automatic metrics and a human pairwise preference test.", "published": "2016-08-25 10:43:56", "link": "http://arxiv.org/abs/1608.07076v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Aligning Packed Dependency Trees: a theory of composition for\n  distributional semantics", "abstract": "We present a new framework for compositional distributional semantics in\nwhich the distributional contexts of lexemes are expressed in terms of anchored\npacked dependency trees. We show that these structures have the potential to\ncapture the full sentential contexts of a lexeme and provide a uniform basis\nfor the composition of distributional knowledge in a way that captures both\nmutual disambiguation and generalization.", "published": "2016-08-25 12:44:05", "link": "http://arxiv.org/abs/1608.07115v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Term_Class Relevance Measure for Text Categorization", "abstract": "In this paper, we introduce a new measure called Term_Class relevance to\ncompute the relevancy of a term in classifying a document into a particular\nclass. The proposed measure estimates the degree of relevance of a given term,\nin placing an unlabeled document to be a member of a known class, as a product\nof Class_Term weight and Class_Term density; where the Class_Term weight is the\nratio of the number of documents of the class containing the term to the total\nnumber of documents containing the term and the Class_Term density is the\nrelative density of occurrence of the term in the class to the total occurrence\nof the term in the entire population. Unlike the other existing term weighting\nschemes such as TF-IDF and its variants, the proposed relevance measure takes\ninto account the degree of relative participation of the term across all\ndocuments of the class to the entire population. To demonstrate the\nsignificance of the proposed measure experimentation has been conducted on the\n20 Newsgroups dataset. Further, the superiority of the novel measure is brought\nout through a comparative analysis.", "published": "2016-08-25 11:46:06", "link": "http://arxiv.org/abs/1608.07094v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Semantics derived automatically from language corpora contain human-like\n  biases", "abstract": "Artificial intelligence and machine learning are in a period of astounding\ngrowth. However, there are concerns that these technologies may be used, either\nwith or without intention, to perpetuate the prejudice and unfairness that\nunfortunately characterizes many human institutions. Here we show for the first\ntime that human-like semantic biases result from the application of standard\nmachine learning to ordinary language---the same sort of language humans are\nexposed to every day. We replicate a spectrum of standard human biases as\nexposed by the Implicit Association Test and other well-known psychological\nstudies. We replicate these using a widely used, purely statistical\nmachine-learning model---namely, the GloVe word embedding---trained on a corpus\nof text from the Web. Our results indicate that language itself contains\nrecoverable and accurate imprints of our historic biases, whether these are\nmorally neutral as towards insects or flowers, problematic as towards race or\ngender, or even simply veridical, reflecting the {\\em status quo} for the\ndistribution of gender with respect to careers or first names. These\nregularities are captured by machine learning along with the rest of semantics.\nIn addition to our empirical findings concerning language, we also contribute\nnew methods for evaluating bias in text, the Word Embedding Association Test\n(WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results\nhave implications not only for AI and machine learning, but also for the fields\nof psychology, sociology, and human ethics, since they raise the possibility\nthat mere exposure to everyday language can account for the biases we replicate\nhere.", "published": "2016-08-25 15:07:17", "link": "http://arxiv.org/abs/1608.07187v4", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Learning Latent Vector Spaces for Product Search", "abstract": "We introduce a novel latent vector space model that jointly learns the latent\nrepresentations of words, e-commerce products and a mapping between the two\nwithout the need for explicit annotations. The power of the model lies in its\nability to directly model the discriminative relation between products and a\nparticular word. We compare our method to existing latent vector space models\n(LSI, LDA and word2vec) and evaluate it as a feature in a learning to rank\nsetting. Our latent vector space model achieves its enhanced performance as it\nlearns better product representations. Furthermore, the mapping from words to\nproducts and the representations of words benefit directly from the errors\npropagated back from the product representations during parameter estimation.\nWe provide an in-depth analysis of the performance of our model and analyze the\nstructure of the learned representations.", "published": "2016-08-25 18:57:50", "link": "http://arxiv.org/abs/1608.07253v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
