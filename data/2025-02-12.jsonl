{"title": "The Planted Spanning Tree Problem", "abstract": "We study the problem of detecting and recovering a planted spanning tree\n$M_n^*$ hidden within a complete, randomly weighted graph $G_n$. Specifically,\neach edge $e$ has a non-negative weight drawn independently from $P_n$ if $e\n\\in M_n^*$ and from $Q_n$ otherwise, where $P_n \\equiv P$ is fixed and $Q_n$\nscales with $n$ such that its density at the origin satisfies\n$\\lim_{n\\to\\infty} n Q'_n(0)=1.$ We consider two representative cases: when\n$M_n^*$ is either a uniform spanning tree or a uniform Hamiltonian path. We\nanalyze the recovery performance of the minimum spanning tree (MST) algorithm\nand derive a fixed-point equation that characterizes the asymptotic fraction of\nedges in $M_n^*$ successfully recovered by the MST as $n \\to \\infty.$\nFurthermore, we establish the asymptotic mean weight of the MST, extending\nFrieze's $\\zeta(3)$ result to the planted model. Leveraging this result, we\ndesign an efficient test based on the MST weight and show that it can\ndistinguish the planted model from the unplanted model with vanishing testing\nerror as $n \\to \\infty.$ Our analysis relies on an asymptotic characterization\nof the local structure of the planted model, employing the framework of local\nweak convergence.", "published": "2025-02-12 21:02:36", "link": "http://arxiv.org/abs/2502.08790v1", "categories": ["cs.DS", "cs.DM", "05C80"], "primary_category": "cs.DS"}
{"title": "Orthology and Near-Cographs in the Context of Phylogenetic Networks", "abstract": "Orthologous genes, which arise through speciation, play a key role in\ncomparative genomics and functional inference. In particular, graph-based\nmethods allow for the inference of orthology estimates without prior knowledge\nof the underlying gene or species trees. This results in orthology graphs,\nwhere each vertex represents a gene, and an edge exists between two vertices if\nthe corresponding genes are estimated to be orthologs. Orthology graphs\ninferred under a tree-like evolutionary model must be cographs. However,\nreal-world data often deviate from this property, either due to noise in the\ndata, errors in inference methods or, simply, because evolution follows a\nnetwork-like rather than a tree-like process. The latter, in particular, raises\nthe question of whether and how orthology graphs can be derived from or,\nequivalently, are explained by phylogenetic networks. Here, we study the\nconstraints imposed on orthology graphs when the underlying evolutionary\nhistory follows a phylogenetic network instead of a tree. We show that any\northology graph can be represented by a sufficiently complex level-k network.\nHowever, such networks lack biologically meaningful constraints. In contrast,\nlevel-1 networks provide a simpler explanation, and we establish\ncharacterizations for level-1 explainable orthology graphs, i.e., those derived\nfrom level-1 evolutionary histories. To this end, we employ modular\ndecomposition, a classical technique for studying graph structures.\nSpecifically, an arbitrary graph is level-1 explainable if and only if each\nprimitive subgraph is a near-cograph (a graph in which the removal of a single\nvertex results in a cograph). Additionally, we present a linear-time algorithm\nto recognize level-1 explainable orthology graphs and to construct a level-1\nnetwork that explains them, if such a network exists.", "published": "2025-02-12 19:36:38", "link": "http://arxiv.org/abs/2502.08746v1", "categories": ["q-bio.PE", "cs.DM", "math.CO"], "primary_category": "q-bio.PE"}
{"title": "Low-temperature Sampling on Sparse Random Graphs", "abstract": "We consider sampling in the so-called low-temperature regime, which is\ntypically characterised by non-local behaviour and strong global correlations.\nCanonical examples include sampling independent sets on bipartite graphs and\nsampling from the ferromagnetic $q$-state Potts model. Low-temperature sampling\nis computationally intractable for general graphs, but recent advances based on\nthe polymer method have made significant progress for graph families that\nexhibit certain expansion properties that reinforce the correlations, including\nfor example expanders, lattices and dense graphs.\n  One of the most natural graph classes that has so far escaped this\nalgorithmic framework is the class of sparse Erd\\H{o}s-R\\'enyi random graphs\nwhose expansion only manifests for sufficiently large subsets of vertices;\nsmall sets of vertices on the other hand have vanishing expansion which makes\nthem behave independently from the bulk of the graph and therefore weakens the\ncorrelations. At a more technical level, the expansion of small sets is crucial\nfor establishing the Kotecky-Priess condition which underpins the applicability\nof the framework.\n  Our main contribution is to develop the polymer method in the low-temperature\nregime for sparse random graphs. As our running example, we use the Potts and\nrandom-cluster models on $G(n,d/n)$ for $d=\\Theta(1)$, where we show a\npolynomial-time sampling algorithm for all sufficiently large $q$ and $d$, at\nall temperatures. Our approach applies more generally for models that are\nmonotone. Key to our result is a simple polymer definition that blends easily\nwith the connectivity properties of the graph and allows us to show that\npolymers have size at most $O(\\log n)$.", "published": "2025-02-12 11:51:25", "link": "http://arxiv.org/abs/2502.08328v1", "categories": ["cs.DM", "math.PR", "60J10, 68Q87, 68Q25", "G.2; G.3"], "primary_category": "cs.DM"}
{"title": "Regular matchstick graphs on the sphere", "abstract": "We show that the $5$-regular matchstick graphs on the sphere are exactly the\nfive $5$-regular contact graphs of congruent caps on the sphere found by R. M.\nRobinson (1969).", "published": "2025-02-12 10:50:56", "link": "http://arxiv.org/abs/2502.08294v1", "categories": ["math.CO", "cs.DM", "math.MG", "52C10, 05C10"], "primary_category": "math.CO"}
{"title": "Trend-encoded Probabilistic Multi-order Model: A Non-Machine Learning Approach for Enhanced Stock Market Forecasts", "abstract": "In recent years, the dominance of machine learning in stock market\nforecasting has been evident. While these models have shown decreasing\nprediction errors, their robustness across different datasets has been a\nconcern. A successful stock market prediction model minimizes prediction errors\nand showcases robustness across various data sets, indicating superior\nforecasting performance. This study introduces a novel multiple lag order\nprobabilistic model based on trend encoding (TeMoP) that enhances stock market\npredictions through a probabilistic approach. Results across different stock\nindexes from nine countries demonstrate that the TeMoP outperforms the\nstate-of-the-art machine learning models in predicting accuracy and\nstabilization.", "published": "2025-02-12 05:54:43", "link": "http://arxiv.org/abs/2502.08144v2", "categories": ["q-fin.CP", "math.PR", "stat.AP"], "primary_category": "q-fin.CP"}
{"title": "The Relative Entropy of Expectation and Price", "abstract": "As operators acting on the undetermined final settlement of a derivative\nsecurity, expectation is linear but price is non-linear. When the market of\nunderlying securities is incomplete, non-linearity emerges from the bid-offer\naround the mid price that accounts for the residual risks of the optimal\nfunding and hedging strategy. At the extremes, non-linearity also arises from\nthe embedded options on capital that are exercised upon default. In this essay,\nthese convexities are quantified in an entropic risk metric that evaluates the\nstrategic risks, which is realised as a cost with the introduction of bilateral\nmargin. Price is then adjusted for market incompleteness and the risk of\ndefault caused by the exhaustion of capital.\n  In the complete market theory, price is derived from a martingale condition.\nIn the incomplete market theory presented here, price is instead derived from a\nlog-martingale condition: \\begin{equation}\np=-\\frac{1}{\\alpha}\\log\\mathbb{E}\\exp[-\\alpha P] \\notag \\end{equation} for the\nprice $p$ and payoff $P$ of a funded and hedged derivative security, where the\nprice measure $\\mathbb{E}$ has minimum entropy relative to economic\nexpectations, and the parameter $\\alpha$ matches the risk aversion of the\ninvestor. This price principle is easily applied to standard models for market\nevolution, with applications considered here including model risk analysis,\ndeep hedging and decentralised finance.", "published": "2025-02-12 18:03:07", "link": "http://arxiv.org/abs/2502.08613v4", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Marginal Price Optimization", "abstract": "We introduce a new framework for optimal routing and arbitrage in AMM driven\nmarkets. This framework improves on the original best-practice convex\noptimization by restricting the search to the boundary of the optimal space. We\ncan parameterize this boundary using a set of prices, and a potentially very\nhigh dimensional optimization problem (2 optimization variables per curve) gets\nreduced to a much lower dimensional root finding problem (1 optimization\nvariable per token, regardless of the number of the curves). Our reformulation\nis similar to the dual problem of a reformulation of the original convex\nproblem. We show our reformulation of the problem is equivalent to the original\nformulation except in the case of infinitely concentrated liquidity, where we\nprovide a suitable approximation. Our formulation performs far better than the\noriginal one in terms of speed - we obtain an improvement of up to 200x against\nClarabel, the new CVXPY default solver - and robustness, especially on levered\ncurves.", "published": "2025-02-12 10:05:06", "link": "http://arxiv.org/abs/2502.08258v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "TLOB: A Novel Transformer Model with Dual Attention for Stock Price Trend Prediction with Limit Order Book Data", "abstract": "Stock Price Trend Prediction (SPTP) based on Limit Order Book (LOB) data is a\nfundamental challenge in financial markets. Despite advances in deep learning,\nexisting models fail to generalize across different market conditions and\nstruggle to reliably predict short-term trends. Surprisingly, by adapting a\nsimple MLP-based architecture to LOB, we show that we surpass SoTA performance;\nthus, challenging the necessity of complex architectures. Unlike past work that\nshows robustness issues, we propose TLOB, a transformer-based model that uses a\ndual attention mechanism to capture spatial and temporal dependencies in LOB\ndata. This allows it to adaptively focus on the market microstructure, making\nit particularly effective for longer-horizon predictions and volatile market\nconditions. We also introduce a new labeling method that improves on previous\nones, removing the horizon bias. We evaluate TLOB's effectiveness using the\nestablished FI-2010 benchmark, which exceeds the state-of-the-art by an average\nof 3.7 F1-score(\\%). Additionally, TLOB shows improvements on Tesla and Intel\nwith a 1.3 and 7.7 increase in F1-score(\\%), respectively. Additionally, we\nempirically show how stock price predictability has declined over time (-6.68\nabsolute points in F1-score(\\%)), highlighting the growing market efficiencies.\nPredictability must be considered in relation to transaction costs, so we\nexperimented with defining trends using an average spread, reflecting the\nprimary transaction cost. The resulting performance deterioration underscores\nthe complexity of translating trend classification into profitable trading\nstrategies. We argue that our work provides new insights into the evolving\nlandscape of stock price trend prediction and sets a strong foundation for\nfuture advancements in financial AI. We release the code at\nhttps://github.com/LeonardoBerti00/TLOB.", "published": "2025-02-12 12:41:10", "link": "http://arxiv.org/abs/2502.15757v2", "categories": ["q-fin.ST", "cs.AI", "cs.LG", "q-fin.TR"], "primary_category": "q-fin.ST"}
{"title": "Analyzing Communicability and Connectivity in the Indian Stock Market During Crises", "abstract": "In financial networks, information does not always follow the shortest path\nbetween two nodes but may also take alternate routes. Communicability, a\nnetwork measure, resolves this complexity and, in diffusion-like processes,\nprovides a reliable measure of the ease with which information flows between\nnodes. As a result, communicability appears to be an important measure for\ndetecting disturbances in connectivity within financial systems, similar to\ninstability caused by periods of high volatility. This study investigates the\nevolution of communicability measures in the stock networks during periods of\ncrises, showing how systemic shocks strengthen the pairwise interdependence\nbetween stocks in the financial market. In this study, the permutation test\nreveals that approximately 83.5 per cent of stock pairs were found to be\nstatistically significant at the significance level of 0.001 and have an\nincrease in the shortest communicability path length during the crisis than the\nnormal days, indicating enhanced interdependence and heightened information\nflow in the market. Furthermore, we show that when employed as features in the\nclassification model, the network shortest path-based measures, along with\ncommunicability measures, are able to accurately classify between the times\nperiods of market stability and volatility. Additionally, our results show that\nthe geometric measures perform better in terms of classification accuracy than\ntopological measures. These findings provide important insights into market\nbehaviour during times of increased volatility and advance our understanding of\nthe financial market crisis.", "published": "2025-02-12 09:35:12", "link": "http://arxiv.org/abs/2502.08242v1", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
{"title": "Contextual Subspace Manifold Projection for Structural Refinement of\n  Large Language Model Representations", "abstract": "Internal representations within deep neural architectures encode\nhigh-dimensional abstractions of linguistic structures, yet they often exhibit\ninefficiencies in feature distribution, limiting expressiveness and\nadaptability. Contextual Subspace Manifold Projection introduces a structured\nrefinement technique that selectively reconfigures token embeddings through\ncontrolled subspace constraints, ensuring more stable and geometrically\nwell-defined feature distributions. Empirical evaluations demonstrated that the\nstructured intervention reduced anisotropy, leading to improved representation\ncompactness while preserving semantic fidelity across transformer layers.\nClustering analyses indicated that token embeddings exhibited greater feature\nseparability, reinforcing the hypothesis that structured projection techniques\nenhance internal representation organization without sacrificing linguistic\ncoherence. Gradient magnitude distributions suggested that the method\nintroduced a smoother optimization trajectory, potentially contributing to more\nstable parameter updates throughout training. Computational overhead associated\nwith the projection operations remained minimal, ensuring that the refinements\ndid not introduce significant trade-offs in model efficiency or inference\nspeed. Comparisons with standard embedding refinement techniques highlighted\nthat structured manifold constraints provided a direct mechanism for improving\nrepresentation quality without requiring additional gradient-based\noptimization. Perplexity evaluations confirmed that the adjustments did not\nnegatively impact sequence coherence, further validating the effectiveness of\nthe proposed approach.", "published": "2025-02-12 00:00:37", "link": "http://arxiv.org/abs/2502.08026v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Franken-Adapter: Cross-Lingual Adaptation of LLMs by Embedding Surgery", "abstract": "The capabilities of Large Language Models (LLMs) in low-resource languages\nlag far behind those in English, making their universal accessibility a\nsignificant challenge. To alleviate this, we present\n$\\textit{Franken-Adapter}$, a modular language adaptation approach for\ndecoder-only LLMs with embedding surgery. Our method begins by creating\ncustomized vocabularies for target languages and performing language adaptation\nthrough embedding tuning on multilingual data. These pre-trained embeddings are\nsubsequently integrated with LLMs that have been instruction-tuned on English\nalignment data to enable zero-shot cross-lingual transfer. Our experiments on\n$\\texttt{Gemma2}$ models with up to 27B parameters demonstrate improvements of\nup to 20% across 96 languages, spanning both discriminative and generative\ntasks, with minimal regressions ($<$1%) in English. Further in-depth analysis\nreveals the critical role of customizing tokenizers in enhancing language\nadaptation, while boosting inference efficiency. Additionally, we show the\nversatility of our method by achieving a 14% improvement over a math-optimized\nLLM across 20 languages, offering a modular solution to transfer reasoning\nabilities across languages post hoc.", "published": "2025-02-12 00:38:11", "link": "http://arxiv.org/abs/2502.08037v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLI under the Microscope: What Atomic Hypothesis Decomposition Reveals", "abstract": "Decomposition of text into atomic propositions is a flexible framework\nallowing for the closer inspection of input and output text. We use atomic\ndecomposition of hypotheses in two natural language reasoning tasks,\ntraditional NLI and defeasible NLI, to form atomic sub-problems, or granular\ninferences that models must weigh when solving the overall problem. These\natomic sub-problems serve as a tool to further understand the structure of both\nNLI and defeasible reasoning, probe a model's consistency and understanding of\ndifferent inferences, and measure the diversity of examples in benchmark\ndatasets. Our results indicate that LLMs still struggle with logical\nconsistency on atomic NLI and defeasible NLI sub-problems. Lastly, we identify\ncritical atomic sub-problems of defeasible NLI examples, or those that most\ncontribute to the overall label, and propose a method to measure the\ninferential consistency of a model, a metric designed to capture the degree to\nwhich a model makes consistently correct or incorrect predictions about the\nsame fact under different contexts.", "published": "2025-02-12 02:54:12", "link": "http://arxiv.org/abs/2502.08080v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance", "abstract": "While large language models (LLMs) have shown strong general reasoning\ncapabilities, their effectiveness in financial reasoning, which is crucial for\nreal-world financial applications remains underexplored. In this study, we\nconduct a comprehensive evaluation of 24 state-of-the-art general and\nreasoning-focused LLMs across four complex financial reasoning tasks involving\nfinancial text, tabular data, and equations. We assess key capabilities such as\nnumerical reasoning, tabular interpretation, financial terminology\ncomprehension, long-context understanding, and equation-based problem solving.\nOur analysis reveals that while data quality and pretraining contribute to\nperformance, general techniques like chain-of-thought (CoT) fine-tuning offer\nlimited gains in financial tasks. To address this, we propose two\ndomain-adapted models, Fino1-8B and Fino1-14B, trained with CoT fine-tuning and\nreinforcement learning using domain-specific reasoning paths. Our models are\ntrained on a carefully curated dataset integrating high-quality examples from\ndiverse sources, covering financial reports, tables, equations, and structured\nXBRL texts. Despite limited training data, they achieve an 7-9% performance\nimprovement, outperforming several advanced LLMs, including GPT-o1,\nGPT-o3-mini, GPT-4.5, and comparable with DeepSeek models (V3 and R1),\ndemonstrating strong practical value in resource, constrained scenarios. Our\nfindings highlight the need for domain-specific adaptations in financial\nreasoning, and we release all datasets, models, and code for future research.", "published": "2025-02-12 05:13:04", "link": "http://arxiv.org/abs/2502.08127v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Selective Self-to-Supervised Fine-Tuning for Generalization in Large\n  Language Models", "abstract": "Fine-tuning Large Language Models (LLMs) on specific datasets is a common\npractice to improve performance on target tasks. However, this performance gain\noften leads to overfitting, where the model becomes too specialized in either\nthe task or the characteristics of the training data, resulting in a loss of\ngeneralization. This paper introduces Selective Self-to-Supervised Fine-Tuning\n(S3FT), a fine-tuning approach that achieves better performance than the\nstandard supervised fine-tuning (SFT) while improving generalization. S3FT\nleverages the existence of multiple valid responses to a query. By utilizing\nthe model's correct responses, S3FT reduces model specialization during the\nfine-tuning stage. S3FT first identifies the correct model responses from the\ntraining set by deploying an appropriate judge. Then, it fine-tunes the model\nusing the correct model responses and the gold response (or its paraphrase) for\nthe remaining samples. The effectiveness of S3FT is demonstrated through\nexperiments on mathematical reasoning, Python programming and reading\ncomprehension tasks. The results show that standard SFT can lead to an average\nperformance drop of up to $4.4$ on multiple benchmarks, such as MMLU and\nTruthfulQA. In contrast, S3FT reduces this drop by half, i.e. $2.5$, indicating\nbetter generalization capabilities than SFT while performing significantly\nbetter on the fine-tuning tasks.", "published": "2025-02-12 05:24:21", "link": "http://arxiv.org/abs/2502.08130v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image\n  Interpretation", "abstract": "As a powerful all-weather Earth observation tool, synthetic aperture radar\n(SAR) remote sensing enables critical military reconnaissance, maritime\nsurveillance, and infrastructure monitoring. Although Vision language models\n(VLMs) have made remarkable progress in natural language processing and image\nunderstanding, their applications remain limited in professional domains due to\ninsufficient domain expertise. This paper innovatively proposes the first\nlarge-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which\ncontains approximately 2 million high-quality image-text pairs, encompasses\ndiverse scenarios with detailed target annotations. This dataset not only\nsupports several key tasks such as visual understanding and object detection\ntasks, but also has unique innovative aspects: this study develop a\nvisual-language dataset and benchmark for the SAR domain, enabling and\nevaluating VLMs' capabilities in SAR image interpretation, which provides a\nparadigmatic framework for constructing multimodal datasets across various\nremote sensing vertical domains. Through experiments on 16 mainstream VLMs, the\neffectiveness of the dataset has been fully verified. The project will be\nreleased at https://github.com/JimmyMa99/SARChat.", "published": "2025-02-12 07:19:36", "link": "http://arxiv.org/abs/2502.08168v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ParetoRAG: Leveraging Sentence-Context Attention for Robust and\n  Efficient Retrieval-Augmented Generation", "abstract": "While Retrieval-Augmented Generation (RAG) systems enhance Large Language\nModels (LLMs) by incorporating external knowledge, they still face persistent\nchallenges in retrieval inefficiency and the inability of LLMs to filter out\nirrelevant information. We present ParetoRAG, an unsupervised framework that\noptimizes RAG systems through sentence-level refinement guided by the Pareto\nprinciple. By decomposing paragraphs into sentences and dynamically\nre-weighting core content while preserving contextual coherence, ParetoRAG\nachieves dual improvements in both retrieval precision and generation quality\nwithout requiring additional training or API resources. This framework has been\nempirically validated across various datasets, LLMs, and retrievers.", "published": "2025-02-12 07:32:48", "link": "http://arxiv.org/abs/2502.08178v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inference-time sparse attention with asymmetric indexing", "abstract": "Self-attention in transformer models is an incremental associative memory\nthat maps key vectors to value vectors. One way to speed up self-attention is\nto employ GPU-compliant vector search algorithms, yet the standard partitioning\nmethods yield poor results in this context, because (1) keys and queries follow\ndifferent distributions and (2) the effect of RoPE positional encoding.\n  In this paper, we introduce SAAP (Self-Attention with Asymmetric Partitions),\nwhich overcomes these problems. It is an asymmetrical indexing technique that\nemploys distinct partitions for keys and queries, thereby approximating\nself-attention with a data-adaptive sparsity pattern.\n  It works on pretrained language models without finetuning, as it only\nrequires to train (offline) a small query classifier. On a long context Llama\n3.1-8b model, with sequences ranging from 100k to 500k tokens, our method\ntypically reduces by a factor 20 the fraction of memory that needs to be\nlooked-up, which translates to a time saving of 60\\% when compared to\nFlashAttention-v2.", "published": "2025-02-12 09:39:54", "link": "http://arxiv.org/abs/2502.08246v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Redefining Simplicity: Benchmarking Large Language Models from Lexical\n  to Document Simplification", "abstract": "Text simplification (TS) refers to the process of reducing the complexity of\na text while retaining its original meaning and key information. Existing work\nonly shows that large language models (LLMs) have outperformed supervised\nnon-LLM-based methods on sentence simplification. This study offers the first\ncomprehensive analysis of LLM performance across four TS tasks: lexical,\nsyntactic, sentence, and document simplification. We compare lightweight,\nclosed-source and open-source LLMs against traditional non-LLM methods using\nautomatic metrics and human evaluations. Our experiments reveal that LLMs not\nonly outperform non-LLM approaches in all four tasks but also often generate\noutputs that exceed the quality of existing human-annotated references.\nFinally, we present some future directions of TS in the era of LLMs.", "published": "2025-02-12 10:38:22", "link": "http://arxiv.org/abs/2502.08281v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MultiProSE: A Multi-label Arabic Dataset for Propaganda, Sentiment, and\n  Emotion Detection", "abstract": "Propaganda is a form of persuasion that has been used throughout history with\nthe intention goal of influencing people's opinions through rhetorical and\npsychological persuasion techniques for determined ends. Although Arabic ranked\nas the fourth most-used language on the internet, resources for propaganda\ndetection in languages other than English, especially Arabic, remain extremely\nlimited. To address this gap, the first Arabic dataset for Multi-label\nPropaganda, Sentiment, and Emotion (MultiProSE) has been introduced. MultiProSE\nis an open-source extension of the existing Arabic propaganda dataset, ArPro,\nwith the addition of sentiment and emotion annotations for each text. This\ndataset comprises 8,000 annotated news articles, which is the largest\npropaganda dataset to date. For each task, several baselines have been\ndeveloped using large language models (LLMs), such as GPT-4o-mini, and\npre-trained language models (PLMs), including three BERT-based models. The\ndataset, annotation guidelines, and source code are all publicly released to\nfacilitate future research and development in Arabic language models and\ncontribute to a deeper understanding of how various opinion dimensions interact\nin news media1.", "published": "2025-02-12 11:35:20", "link": "http://arxiv.org/abs/2502.08319v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual Compression Encoding for Large Language Models: A Novel\n  Framework for Multi-Layered Parameter Space Pruning", "abstract": "Context-aware compression techniques have gained increasing attention as\nmodel sizes continue to grow, introducing computational bottlenecks that hinder\nefficient deployment. A structured encoding approach was proposed to\nselectively eliminate redundant parameter groups while ensuring that\nrepresentational fidelity was preserved across multiple layers. Contextual\nCompression Encoding (CCE) introduced a multi-stage encoding mechanism that\ndynamically restructured parameter distributions, allowing for significant\nreductions in memory footprint and computational complexity. Experimental\nevaluations demonstrated that models compressed through CCE retained linguistic\nexpressivity and coherence, maintaining accuracy across a range of text\ngeneration and classification tasks. Layer-wise analysis revealed that\nmiddle-network layers exhibited higher compression ratios, aligning with the\nobservation that self-attention and feed-forward transformations contained\nredundancies that could be reorganized without impairing functional capacity.\nComparisons against conventional quantization and pruning methods confirmed\nthat CCE provided a more balanced trade-off between efficiency and model\nretention, achieving reductions in energy consumption and inference latency\nwithout requiring extensive retraining. Computational efficiency improvements\nwere particularly evident in deployment scenarios involving\nresource-constrained environments, where reductions in memory usage enabled\nmore scalable implementations. Further analyses of internal network behavior\nshowed that compressed models exhibited stable activation distributions and\nadapted dynamically to input variations, reinforcing the viability of\nstructured compression strategies for optimizing large-scale architectures.", "published": "2025-02-12 11:44:19", "link": "http://arxiv.org/abs/2502.08323v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Systematic Knowledge Injection into Large Language Models via Diverse\n  Augmentation for Domain-Specific RAG", "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a prominent method for\nincorporating domain knowledge into Large Language Models (LLMs). While RAG\nenhances response relevance by incorporating retrieved domain knowledge in the\ncontext, retrieval errors can still lead to hallucinations and incorrect\nanswers. To recover from retriever failures, domain knowledge is injected by\nfine-tuning the model to generate the correct response, even in the case of\nretrieval errors. However, we observe that without systematic knowledge\naugmentation, fine-tuned LLMs may memorize new information but still fail to\nextract relevant domain knowledge, leading to poor performance. In this work,\nwe present a novel framework that significantly enhances the fine-tuning\nprocess by augmenting the training data in two ways -- context augmentation and\nknowledge paraphrasing. In context augmentation, we create multiple training\nsamples for a given QA pair by varying the relevance of the retrieved\ninformation, teaching the model when to ignore and when to rely on retrieved\ncontent. In knowledge paraphrasing, we fine-tune with multiple answers to the\nsame question, enabling LLMs to better internalize specialized knowledge. To\nmitigate catastrophic forgetting due to fine-tuning, we add a domain-specific\nidentifier to a question and also utilize a replay buffer containing general QA\npairs. Experimental results demonstrate the efficacy of our method over\nexisting techniques, achieving up to 10\\% relative gain in token-level recall\nwhile preserving the LLM's generalization capabilities.", "published": "2025-02-12 12:39:51", "link": "http://arxiv.org/abs/2502.08356v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling Global Discourse Structures: Theoretical Analysis and NLP\n  Applications in Argument Mining", "abstract": "Particularly in the structure of global discourse, coherence plays a pivotal\nrole in human text comprehension and is a hallmark of high-quality text. This\nis especially true for persuasive texts, where coherent argument structures\nsupport claims effectively. This paper discusses and proposes methods for\ndetecting, extracting and representing these global discourse structures in a\nproccess called Argument(ation) Mining. We begin by defining key terms and\nprocesses of discourse structure analysis, then continue to summarize existing\nresearch on the matter, and identify shortcomings in current argument component\nextraction and classification methods. Furthermore, we will outline an\narchitecture for argument mining that focuses on making models more\ngeneralisable while overcoming challenges in the current field of research by\nutilizing novel NLP techniques. This paper reviews current knowledge,\nsummarizes recent works, and outlines our NLP pipeline, aiming to contribute to\nthe theoretical understanding of global discourse structures.", "published": "2025-02-12 13:03:43", "link": "http://arxiv.org/abs/2502.08371v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in\n  LLM Writing Assistance", "abstract": "Large language models (LLMs) are helping millions of users write texts about\ndiverse issues, and in doing so expose users to different ideas and\nperspectives. This creates concerns about issue bias, where an LLM tends to\npresent just one perspective on a given issue, which in turn may influence how\nusers think about this issue. So far, it has not been possible to measure which\nissue biases LLMs actually manifest in real user interactions, making it\ndifficult to address the risks from biased LLMs. Therefore, we create\nIssueBench: a set of 2.49m realistic prompts for measuring issue bias in LLM\nwriting assistance, which we construct based on 3.9k templates (e.g. \"write a\nblog about\") and 212 political issues (e.g. \"AI regulation\") from real user\ninteractions. Using IssueBench, we show that issue biases are common and\npersistent in state-of-the-art LLMs. We also show that biases are remarkably\nsimilar across models, and that all models align more with US Democrat than\nRepublican voter opinion on a subset of issues. IssueBench can easily be\nadapted to include other issues, templates, or tasks. By enabling robust and\nrealistic measurement, we hope that IssueBench can bring a new quality of\nevidence to ongoing discussions about LLM biases and how to address them.", "published": "2025-02-12 13:37:03", "link": "http://arxiv.org/abs/2502.08395v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Examining Spanish Counseling with MIDAS: a Motivational Interviewing\n  Dataset in Spanish", "abstract": "Cultural and language factors significantly influence counseling, but Natural\nLanguage Processing research has not yet examined whether the findings of\nconversational analysis for counseling conducted in English apply to other\nlanguages. This paper presents a first step towards this direction. We\nintroduce MIDAS (Motivational Interviewing Dataset in Spanish), a counseling\ndataset created from public video sources that contains expert annotations for\ncounseling reflections and questions. Using this dataset, we explore\nlanguage-based differences in counselor behavior in English and Spanish and\ndevelop classifiers in monolingual and multilingual settings, demonstrating its\napplications in counselor behavioral coding tasks.", "published": "2025-02-12 14:53:04", "link": "http://arxiv.org/abs/2502.08458v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Salamandra Technical Report", "abstract": "This work introduces Salamandra, a suite of open-source decoder-only large\nlanguage models available in three different sizes: 2, 7, and 40 billion\nparameters. The models were trained from scratch on highly multilingual data\nthat comprises text in 35 European languages and code. Our carefully curated\ncorpus is made exclusively from open-access data compiled from a wide variety\nof sources. Along with the base models, supplementary checkpoints that were\nfine-tuned on public-domain instruction data are also released for chat\napplications. Additionally, we also share our preliminary experiments on\nmultimodality, which serve as proof-of-concept to showcase potential\napplications for the Salamandra family. Our extensive evaluations on\nmultilingual benchmarks reveal that Salamandra has strong capabilities,\nachieving competitive performance when compared to similarly sized open-source\nmodels. We provide comprehensive evaluation results both on standard downstream\ntasks as well as key aspects related to bias and safety.With this technical\nreport, we intend to promote open science by sharing all the details behind our\ndesign choices, data curation strategy and evaluation methodology. In addition\nto that, we deviate from the usual practice by making our training and\nevaluation scripts publicly accessible. We release all models under a\npermissive Apache 2.0 license in order to foster future research and facilitate\ncommercial use, thereby contributing to the open-source ecosystem of large\nlanguage models.", "published": "2025-02-12 15:26:08", "link": "http://arxiv.org/abs/2502.08489v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explanation based In-Context Demonstrations Retrieval for Multilingual\n  Grammatical Error Correction", "abstract": "Grammatical error correction (GEC) aims to correct grammatical, spelling, and\nsemantic errors in natural language text. With the growing of large language\nmodels (LLMs), direct text generation has gradually become the focus of the GEC\nmethods, and few-shot in-context learning presents a cost-effective solution.\nHowever, selecting effective in-context examples remains challenging, as the\nsimilarity between input texts does not necessarily correspond to similar\ngrammatical error patterns. In this paper, we propose a novel retrieval method\nbased on natural language grammatical error explanations (GEE) to address this\nissue. Our method retrieves suitable few-shot demonstrations by matching the\nGEE of the test input with that of pre-constructed database samples, where\nexplanations for erroneous samples are generated by LLMs. We conducted\nmultilingual GEC few-shot experiments on both major open-source and\nclosed-source LLMs. Experiments across five languages show that our method\noutperforms existing semantic and BM25-based retrieval techniques, without\nrequiring additional training or language adaptation. This also suggests that\nmatching error patterns is key to selecting examples.", "published": "2025-02-12 15:41:43", "link": "http://arxiv.org/abs/2502.08507v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial\n  Stance for Summary Evaluation", "abstract": "Faithfulness evaluators based on large language models (LLMs) are often\nfooled by the fluency of the text and struggle with identifying errors in the\nsummaries. We propose an approach to summary faithfulness evaluation in which\nmultiple LLM-based agents are assigned initial stances (regardless of what\ntheir belief might be) and forced to come up with a reason to justify the\nimposed belief, thus engaging in a multi-round debate to reach an agreement.\nThe uniformly distributed initial assignments result in a greater diversity of\nstances leading to more meaningful debates and ultimately more errors\nidentified. Furthermore, by analyzing the recent faithfulness evaluation\ndatasets, we observe that naturally, it is not always the case for a summary to\nbe either faithful to the source document or not. We therefore introduce a new\ndimension, ambiguity, and a detailed taxonomy to identify such special cases.\nExperiments demonstrate our approach can help identify ambiguities, and have\neven a stronger performance on non-ambiguous summaries.", "published": "2025-02-12 15:46:50", "link": "http://arxiv.org/abs/2502.08514v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quality-Aware Decoding: Unifying Quality Estimation and Decoding", "abstract": "Quality Estimation (QE) models for Neural Machine Translation (NMT) predict\nthe quality of the hypothesis without having access to the reference. An\nemerging research direction in NMT involves the use of QE models, which have\ndemonstrated high correlations with human judgment and can enhance translations\nthrough Quality-Aware Decoding. Although several approaches have been proposed\nbased on sampling multiple candidate translations and picking the best\ncandidate, none have integrated these models directly into the decoding\nprocess. In this paper, we address this by proposing a novel token-level QE\nmodel capable of reliably scoring partial translations. We build a\nuni-directional QE model for this, as decoder models are inherently trained and\nefficient on partial sequences. We then present a decoding strategy that\nintegrates the QE model for Quality-Aware decoding and demonstrate that the\ntranslation quality improves when compared to the N-best list re-ranking with\nstate-of-the-art QE models (up to $1.39$ XCOMET-XXL $\\uparrow$). Finally, we\nshow that our approach provides significant benefits in document translation\ntasks, where the quality of N-best lists is typically suboptimal. Code can be\nfound at https://ai4lt.iar.kit.edu/english/projects\\_kontextmt.php", "published": "2025-02-12 16:49:52", "link": "http://arxiv.org/abs/2502.08561v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SPeCtrum: A Grounded Framework for Multidimensional Identity\n  Representation in LLM-Based Agent", "abstract": "Existing methods for simulating individual identities often oversimplify\nhuman complexity, which may lead to incomplete or flattened representations. To\naddress this, we introduce SPeCtrum, a grounded framework for constructing\nauthentic LLM agent personas by incorporating an individual's multidimensional\nself-concept. SPeCtrum integrates three core components: Social Identity (S),\nPersonal Identity (P), and Personal Life Context (C), each contributing\ndistinct yet interconnected aspects of identity. To evaluate SPeCtrum's\neffectiveness in identity representation, we conducted automated and human\nevaluations. Automated evaluations using popular drama characters showed that\nPersonal Life Context (C)-derived from short essays on preferences and daily\nroutines-modeled characters' identities more effectively than Social Identity\n(S) and Personal Identity (P) alone and performed comparably to the full SPC\ncombination. In contrast, human evaluations involving real-world individuals\nfound that the full SPC combination provided a more comprehensive self-concept\nrepresentation than C alone. Our findings suggest that while C alone may\nsuffice for basic identity simulation, integrating S, P, and C enhances the\nauthenticity and accuracy of real-world identity representation. Overall,\nSPeCtrum offers a structured approach for simulating individuals in LLM agents,\nenabling more personalized human-AI interactions and improving the realism of\nsimulation-based behavioral studies.", "published": "2025-02-12 17:38:27", "link": "http://arxiv.org/abs/2502.08599v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Examining Multilingual Embedding Models Cross-Lingually Through\n  LLM-Generated Adversarial Examples", "abstract": "The evaluation of cross-lingual semantic search capabilities of models is\noften limited to existing datasets from tasks such as information retrieval and\nsemantic textual similarity. To allow for domain-specific evaluation, we\nintroduce Cross Lingual Semantic Discrimination (CLSD), a novel cross-lingual\nsemantic search task that does not require a large evaluation corpus, only\nparallel sentences of the language pair of interest within the target domain.\nThis task focuses on the ability of a model to cross-lingually rank the true\nparallel sentence higher than challenging distractors generated by a large\nlanguage model. We create a case study of our introduced CLSD task for the\nlanguage pair German-French in the news domain. Within this case study, we find\nthat models that are also fine-tuned for retrieval tasks benefit from pivoting\nthrough English, while bitext mining models perform best directly\ncross-lingually. A fine-grained similarity analysis enabled by our distractor\ngeneration strategy indicate that different embedding models are sensitive to\ndifferent types of perturbations.", "published": "2025-02-12 18:54:37", "link": "http://arxiv.org/abs/2502.08638v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Style Extraction on Text Embeddings Using VAE and Parallel Dataset", "abstract": "This study investigates the stylistic differences among various Bible\ntranslations using a Variational Autoencoder (VAE) model. By embedding textual\ndata into high-dimensional vectors, the study aims to detect and analyze\nstylistic variations between translations, with a specific focus on\ndistinguishing the American Standard Version (ASV) from other translations. The\nresults demonstrate that each translation exhibits a unique stylistic\ndistribution, which can be effectively identified using the VAE model. These\nfindings suggest that the VAE model is proficient in capturing and\ndifferentiating textual styles, although it is primarily optimized for\ndistinguishing a single style. The study highlights the model's potential for\nbroader applications in AI-based text generation and stylistic analysis, while\nalso acknowledging the need for further model refinement to address the\ncomplexity of multi-dimensional stylistic relationships. Future research could\nextend this methodology to other text domains, offering deeper insights into\nthe stylistic features embedded within various types of textual data.", "published": "2025-02-12 00:24:28", "link": "http://arxiv.org/abs/2502.08668v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Impact of the Quality of Textual Data on Feature\n  Representation and Machine Learning Models", "abstract": "Background: Data collected in controlled settings typically results in\nhigh-quality datasets. However, in real-world applications, the quality of data\ncollection is often compromised. It is well established that the quality of a\ndataset significantly impacts the performance of machine learning models.\n  Methods: A rudimentary error rate metric was developed to evaluate textual\ndataset quality at the token level. Mixtral Large Language Model (LLM) was used\nto quantify and correct errors in low quality datasets. The study analyzed two\nhealthcare datasets: the high-quality MIMIC-III public hospital dataset and a\nlower-quality private dataset from Australian aged care homes. Errors were\nsystematically introduced into MIMIC at varying rates, while the ACH dataset\nquality was improved using the LLM.\n  Results: For the sampled 35,774 and 6,336 patients from the MIMIC and ACH\ndatasets respectively, we used Mixtral to introduce errors in MIMIC and correct\nerrors in ACH. Mixtral correctly detected errors in 63% of progress notes, with\n17% containing a single token misclassified due to medical terminology. LLMs\ndemonstrated potential for improving progress note quality by addressing\nvarious errors. Under varying error rates, feature representation performance\nwas tolerant to lower error rates (<10%) but declined significantly at higher\nrates.\n  Conclusions: The study revealed that models performed relatively well on\ndatasets with lower error rates (<10%), but their performance declined\nsignificantly as error rates increased (>=10%). Therefore, it is crucial to\nevaluate the quality of a dataset before utilizing it for machine learning\ntasks. For datasets with higher error rates, implementing corrective measures\nis essential to ensure the reliability and effectiveness of machine learning\nmodels.", "published": "2025-02-12 00:27:49", "link": "http://arxiv.org/abs/2502.08669v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Augmentation to Improve Large Language Models in Food Hazard and\n  Product Detection", "abstract": "The primary objective of this study is to demonstrate the impact of data\naugmentation using ChatGPT-4o-mini on food hazard and product analysis. The\naugmented data is generated using ChatGPT-4o-mini and subsequently used to\ntrain two large language models: RoBERTa-base and Flan-T5-base. The models are\nevaluated on test sets. The results indicate that using augmented data helped\nimprove model performance across key metrics, including recall, F1 score,\nprecision, and accuracy, compared to using only the provided dataset. The full\ncode, including model training and the augmented dataset, can be found in this\nrepository: https://github.com/AREEG94FAHAD/food-hazard-prdouct-cls", "published": "2025-02-12 12:14:35", "link": "http://arxiv.org/abs/2502.08687v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IHEval: Evaluating Language Models on Following the Instruction\n  Hierarchy", "abstract": "The instruction hierarchy, which establishes a priority order from system\nmessages to user messages, conversation history, and tool outputs, is essential\nfor ensuring consistent and safe behavior in language models (LMs). Despite its\nimportance, this topic receives limited attention, and there is a lack of\ncomprehensive benchmarks for evaluating models' ability to follow the\ninstruction hierarchy. We bridge this gap by introducing IHEval, a novel\nbenchmark comprising 3,538 examples across nine tasks, covering cases where\ninstructions in different priorities either align or conflict. Our evaluation\nof popular LMs highlights their struggle to recognize instruction priorities.\nAll evaluated models experience a sharp performance decline when facing\nconflicting instructions, compared to their original instruction-following\nperformance. Moreover, the most competitive open-source model only achieves 48%\naccuracy in resolving such conflicts. Our results underscore the need for\ntargeted optimization in the future development of LMs.", "published": "2025-02-12 19:35:28", "link": "http://arxiv.org/abs/2502.08745v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Belief: A Hard Problem for LLMs", "abstract": "We present two LLM-based approaches to zero-shot source-and-target belief\nprediction on FactBank: a unified system that identifies events, sources, and\nbelief labels in a single pass, and a hybrid approach that uses a fine-tuned\nDeBERTa tagger for event detection. We show that multiple open-sourced,\nclosed-source, and reasoning-based LLMs struggle with the task. Using the\nhybrid approach, we achieve new state-of-the-art results on FactBank and offer\na detailed error analysis. Our approach is then tested on the Italian belief\ncorpus ModaFact.", "published": "2025-02-12 20:39:01", "link": "http://arxiv.org/abs/2502.08777v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lexical Manifold Reconfiguration in Large Language Models: A Novel\n  Architectural Approach for Contextual Modulation", "abstract": "Contextual adaptation in token embeddings plays a central role in determining\nhow well language models maintain coherence and retain semantic relationships\nover extended text sequences. Static embeddings often impose constraints on\nlexical flexibility, leading to suboptimal performance when faced with complex\nsentence structures or domain-specific terminology shifts. To address this\nlimitation, a structured approach was developed for dynamically reconfiguring\ntoken embeddings through continuous geometric transformations, ensuring that\nrepresentations evolved in response to evolving discourse structures. A\nmanifold-based transformation mechanism was integrated to regulate lexical\npositioning, allowing embeddings to undergo controlled shifts while preserving\nlinguistic relationships across varying textual contexts. Empirical evaluations\ndemonstrated that embedding reconfiguration contributed to reductions in\nperplexity, improved lexical coherence, and enhanced sentence-level continuity,\nparticularly in structured and domain-adaptive text generation tasks.\nComparative analyses of embedding drift indicated that dynamically restructured\nrepresentations maintained stronger contextual consistency, reducing\nmisalignment in token dependencies while preserving fluency in language\nmodeling outputs. Computational overhead assessments confirmed that while\ntraining complexity increased due to the iterative refinement of embeddings,\ninference remained efficient, ensuring practical feasibility for real-time\ngeneration. Evaluations across multiple datasets further demonstrated that\ndynamically modulated embeddings exhibited broader lexical diversity, reducing\nrepetitive token patterns and enabling a more adaptable representation learning\nprocess.", "published": "2025-02-12 22:11:07", "link": "http://arxiv.org/abs/2502.08818v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Examining and Adapting Time for Multilingual Classification via Mixture\n  of Temporal Experts", "abstract": "Time is implicitly embedded in classification process: classifiers are\nusually built on existing data while to be applied on future data whose\ndistributions (e.g., label and token) may change. However, existing\nstate-of-the-art classification models merely consider the temporal variations\nand primarily focus on English corpora, which leaves temporal studies less\nexplored, let alone under multilingual settings. In this study, we fill the gap\nby treating time as domains (e.g., 2024 vs. 2025), examining temporal effects,\nand developing a domain adaptation framework to generalize classifiers over\ntime on multiple languages. Our framework proposes Mixture of Temporal Experts\n(MoTE) to leverage both semantic and data distributional shifts to learn and\nadapt temporal trends into classification models. Our analysis shows\nclassification performance varies over time across different languages, and we\nexperimentally demonstrate that MoTE can enhance classifier generalizability\nover temporal data shifts. Our study provides analytic insights and addresses\nthe need for time-aware models that perform robustly in multilingual scenarios.", "published": "2025-02-12 22:30:18", "link": "http://arxiv.org/abs/2502.08825v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AI for Scaling Legal Reform: Mapping and Redacting Racial Covenants in\n  Santa Clara County", "abstract": "Legal reform can be challenging in light of the volume, complexity, and\ninterdependence of laws, codes, and records. One salient example of this\nchallenge is the effort to restrict and remove racially restrictive covenants,\nclauses in property deeds that historically barred individuals of specific\nraces from purchasing homes. Despite the Supreme Court holding such racial\ncovenants unenforceable in 1948, they persist in property records across the\nUnited States. Many jurisdictions have moved to identify and strike these\nprovisions, including California, which mandated in 2021 that all counties\nimplement such a process. Yet the scale can be overwhelming, with Santa Clara\nCounty (SCC) alone having over 24 million property deed documents, making\npurely manual review infeasible. We present a novel approach to addressing this\npressing issue, developed through a partnership with the SCC Clerk-Recorder's\nOffice. First, we leverage an open large language model, finetuned to detect\nracial covenants with high precision and recall. We estimate that this system\nreduces manual efforts by 86,500 person hours and costs less than 2% of the\ncost for a comparable off-the-shelf closed model. Second, we illustrate the\nCounty's integration of this model into responsible operational practice,\nincluding legal review and the creation of a historical registry, and release\nour model to assist the hundreds of jurisdictions engaged in similar efforts.\nFinally, our results reveal distinct periods of utilization of racial\ncovenants, sharp geographic clustering, and the disproportionate role of a\nsmall number of developers in maintaining housing discrimination. We estimate\nthat by 1950, one in four properties across the County were subject to racial\ncovenants.", "published": "2025-02-12 23:42:43", "link": "http://arxiv.org/abs/2503.03888v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Mechanistic Circuits for Extractive Question-Answering", "abstract": "Large language models are increasingly used to process documents and\nfacilitate question-answering on them. In our paper, we extract mechanistic\ncircuits for this real-world language modeling task: context-augmented language\nmodeling for extractive question-answering (QA) tasks and understand the\npotential benefits of circuits towards downstream applications such as data\nattribution to context information. We extract circuits as a function of\ninternal model components (e.g., attention heads, MLPs) using causal mediation\nanalysis techniques. Leveraging the extracted circuits, we first understand the\ninterplay between the model's usage of parametric memory and retrieved context\ntowards a better mechanistic understanding of context-augmented language\nmodels. We then identify a small set of attention heads in our circuit which\nperforms reliable data attribution by default, thereby obtaining attribution\nfor free in just the model's forward pass. Using this insight, we then\nintroduce ATTNATTRIB, a fast data attribution algorithm which obtains\nstate-of-the-art attribution results across various extractive QA benchmarks.\nFinally, we show the possibility to steer the language model towards answering\nfrom the context, instead of the parametric memory by using the attribution\nfrom ATTNATTRIB as an additional signal during the forward pass. Beyond\nmechanistic understanding, our paper provides tangible applications of circuits\nin the form of reliable data attribution and model steering.", "published": "2025-02-12 01:54:21", "link": "http://arxiv.org/abs/2502.08059v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GCoT: Chain-of-Thought Prompt Learning for Graphs", "abstract": "Chain-of-thought (CoT) prompting has achieved remarkable success in natural\nlanguage processing (NLP). However, its vast potential remains largely\nunexplored for graphs. This raises an interesting question: How can we design\nCoT prompting for graphs to guide graph models to learn step by step? On one\nhand, unlike natural languages, graphs are non-linear and characterized by\ncomplex topological structures. On the other hand, many graphs lack textual\ndata, making it difficult to formulate language-based CoT prompting. In this\nwork, we propose the first CoT prompt learning framework for text-free graphs,\nGCoT. Specifically, we decompose the adaptation process for each downstream\ntask into a series of inference steps, with each step consisting of\nprompt-based inference, ``thought'' generation, and thought-conditioned prompt\nlearning. While the steps mimic CoT prompting in NLP, the exact mechanism\ndiffers significantly. Specifically, at each step, an input graph, along with a\nprompt, is first fed into a pre-trained graph encoder for prompt-based\ninference. We then aggregate the hidden layers of the encoder to construct a\n``thought'', which captures the working state of each node in the current step.\nConditioned on this thought, we learn a prompt specific to each node based on\nthe current state. These prompts are fed into the next inference step,\nrepeating the cycle. To evaluate and analyze the effectiveness of GCoT, we\nconduct comprehensive experiments on eight public datasets, which demonstrate\nthe advantage of our approach.", "published": "2025-02-12 03:33:06", "link": "http://arxiv.org/abs/2502.08092v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HuDEx: Integrating Hallucination Detection and Explainability for\n  Enhancing the Reliability of LLM responses", "abstract": "Recent advances in large language models (LLMs) have shown promising\nimprovements, often surpassing existing methods across a wide range of\ndownstream tasks in natural language processing. However, these models still\nface challenges, which may hinder their practical applicability. For example,\nthe phenomenon of hallucination is known to compromise the reliability of LLMs,\nespecially in fields that demand high factual precision. Current benchmarks\nprimarily focus on hallucination detection and factuality evaluation but do not\nextend beyond identification. This paper proposes an explanation enhanced\nhallucination-detection model, coined as HuDEx, aimed at enhancing the\nreliability of LLM-generated responses by both detecting hallucinations and\nproviding detailed explanations. The proposed model provides a novel approach\nto integrate detection with explanations, and enable both users and the LLM\nitself to understand and reduce errors. Our measurement results demonstrate\nthat the proposed model surpasses larger LLMs, such as Llama3 70B and GPT-4, in\nhallucination detection accuracy, while maintaining reliable explanations.\nFurthermore, the proposed model performs well in both zero-shot and other test\nenvironments, showcasing its adaptability across diverse benchmark datasets.\nThe proposed approach further enhances the hallucination detection research by\nintroducing a novel approach to integrating interpretability with hallucination\ndetection, which further enhances the performance and reliability of evaluating\nhallucinations in language models.", "published": "2025-02-12 04:17:02", "link": "http://arxiv.org/abs/2502.08109v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing LLM Character-Level Manipulation via Divide and Conquer", "abstract": "Large Language Models (LLMs) have demonstrated strong generalization\ncapabilities across a wide range of natural language processing (NLP) tasks.\nHowever, they exhibit notable weaknesses in character-level string\nmanipulation, struggling with fundamental operations such as character\ndeletion, insertion, and substitution. These challenges stem primarily from\ntokenization constraints, despite the critical role of such operations in data\npreprocessing and code generation. Through systematic analysis, we derive two\nkey insights: (1) LLMs face significant difficulties in leveraging intrinsic\ntoken knowledge for character-level reasoning, and (2) atomized word structures\ncan substantially enhance LLMs' ability to process token-level structural\ninformation. Building on these insights, we propose Character-Level\nManipulation via Divide and Conquer, a novel approach designed to bridge the\ngap between token-level processing and character-level manipulation. Our method\ndecomposes complex operations into explicit character-level subtasks coupled\nwith controlled token reconstruction phases, leading to significant\nimprovements in accuracy. Without additional training, our method significantly\nimproves accuracies on the $\\texttt{Deletion}$, $\\texttt{Insertion}$, and\n$\\texttt{Substitution}$ tasks. To support further research, we open-source our\nimplementation and benchmarks.", "published": "2025-02-12 07:37:39", "link": "http://arxiv.org/abs/2502.08180v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM Modules: Knowledge Transfer from a Large to a Small Model using\n  Enhanced Cross-Attention", "abstract": "In this work, we propose an architecture of LLM Modules that enables the\ntransfer of knowledge from a large pre-trained model to a smaller model using\nan Enhanced Cross-Attention mechanism. In the proposed scheme, the Qwen2-1.5B\nmodel is frozen and its representations are passed through specially designed\nattention layers to the GPT-Neo-125M model, which is trained on limited\ncomputational resources. Experimental results on the Bespoke-Stratos-17k\ndataset demonstrate that after 15 epochs of training, the combined model\ngenerates responses comparable in quality to those obtained by distillation. We\ndiscuss the advantages of the modular approach, provide examples of input\nqueries and comparative analysis, and outline prospects for further extension\nof the method.", "published": "2025-02-12 08:48:55", "link": "http://arxiv.org/abs/2502.08213v1", "categories": ["cs.CL", "cs.LG", "I.2.7; D.2.11"], "primary_category": "cs.CL"}
{"title": "Exploring the Potential of Large Language Models to Simulate Personality", "abstract": "With the advancement of large language models (LLMs), the focus in\nConversational AI has shifted from merely generating coherent and relevant\nresponses to tackling more complex challenges, such as personalizing dialogue\nsystems. In an effort to enhance user engagement, chatbots are often designed\nto mimic human behaviour, responding within a defined emotional spectrum and\naligning to a set of values. In this paper, we aim to simulate personal traits\naccording to the Big Five model with the use of LLMs. Our research showed that\ngenerating personality-related texts is still a challenging task for the\nmodels. As a result, we present a dataset of generated texts with the\npredefined Big Five characteristics and provide an analytical framework for\ntesting LLMs on a simulation of personality skills.", "published": "2025-02-12 10:17:18", "link": "http://arxiv.org/abs/2502.08265v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Word Synchronization Challenge: A Benchmark for Word Association\n  Responses for LLMs", "abstract": "This paper introduces the Word Synchronization Challenge, a novel benchmark\nto evaluate large language models (LLMs) in Human-Computer Interaction (HCI).\nThis benchmark uses a dynamic game-like framework to test LLMs ability to mimic\nhuman cognitive processes through word associations. By simulating complex\nhuman interactions, it assesses how LLMs interpret and align with human thought\npatterns during conversational exchanges, which are essential for effective\nsocial partnerships in HCI. Initial findings highlight the influence of model\nsophistication on performance, offering insights into the models capabilities\nto engage in meaningful social interactions and adapt behaviors in human-like\nways. This research advances the understanding of LLMs potential to replicate\nor diverge from human cognitive functions, paving the way for more nuanced and\nempathetic human-machine collaborations.", "published": "2025-02-12 11:30:28", "link": "http://arxiv.org/abs/2502.08312v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding", "abstract": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores.", "published": "2025-02-12 12:50:15", "link": "http://arxiv.org/abs/2502.08363v1", "categories": ["cs.CL", "cs.AI", "68T01", "I.2"], "primary_category": "cs.CL"}
{"title": "A Semantic Parsing Algorithm to Solve Linear Ordering Problems", "abstract": "We develop an algorithm to semantically parse linear ordering problems, which\nrequire a model to arrange entities using deductive reasoning. Our method takes\nas input a number of premises and candidate statements, parsing them to a\nfirst-order logic of an ordering domain, and then utilizes constraint logic\nprogramming to infer the truth of proposed statements about the ordering.\n  Our semantic parser transforms Heim and Kratzer's syntax-based compositional\nformal semantic rules to a computational algorithm. This transformation\ninvolves introducing abstract types and templates based on their rules, and\nintroduces a dynamic component to interpret entities within a contextual\nframework.\n  Our symbolic system, the Formal Semantic Logic Inferer (FSLI), is applied to\nanswer multiple choice questions in BIG-bench's logical_deduction multiple\nchoice problems, achieving perfect accuracy, compared to 67.06% for the\nbest-performing LLM (GPT-4) and 87.63% for the hybrid system Logic-LM.\n  These promising results demonstrate the benefit of developing a semantic\nparsing algorithm driven by first-order logic constructs.", "published": "2025-02-12 13:58:42", "link": "http://arxiv.org/abs/2502.08415v1", "categories": ["cs.CL", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Towards Prompt Generalization: Grammar-aware Cross-Prompt Automated\n  Essay Scoring", "abstract": "In automated essay scoring (AES), recent efforts have shifted toward\ncross-prompt settings that score essays on unseen prompts for practical\napplicability. However, prior methods trained with essay-score pairs of\nspecific prompts pose challenges in obtaining prompt-generalized essay\nrepresentation. In this work, we propose a grammar-aware cross-prompt trait\nscoring (GAPS), which internally captures prompt-independent syntactic aspects\nto learn generic essay representation. We acquire grammatical error-corrected\ninformation in essays via the grammar error correction technique and design the\nAES model to seamlessly integrate such information. By internally referring to\nboth the corrected and the original essays, the model can focus on generic\nfeatures during training. Empirical experiments validate our method's\ngeneralizability, showing remarkable improvements in prompt-independent and\ngrammar-related traits. Furthermore, GAPS achieves notable QWK gains in the\nmost challenging cross-prompt scenario, highlighting its strength in evaluating\nunseen prompts.", "published": "2025-02-12 14:41:20", "link": "http://arxiv.org/abs/2502.08450v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Measuring Diversity in Synthetic Datasets", "abstract": "Large language models (LLMs) are widely adopted to generate synthetic\ndatasets for various natural language processing (NLP) tasks, such as text\nclassification and summarization. However, accurately measuring the diversity\nof these synthetic datasets-an aspect crucial for robust model\nperformance-remains a significant challenge. In this paper, we introduce\nDCScore, a novel method for measuring synthetic dataset diversity from a\nclassification perspective. Specifically, DCScore formulates diversity\nevaluation as a sample classification task, leveraging mutual relationships\namong samples. We further provide theoretical verification of the\ndiversity-related axioms satisfied by DCScore, highlighting its role as a\nprincipled diversity evaluation method. Experimental results on synthetic\ndatasets reveal that DCScore enjoys a stronger correlation with multiple\ndiversity pseudo-truths of evaluated datasets, underscoring its effectiveness.\nMoreover, both empirical and theoretical evidence demonstrate that DCScore\nsubstantially reduces computational costs compared to existing approaches. Code\nis available at: https://github.com/BlueWhaleLab/DCScore.", "published": "2025-02-12 15:46:34", "link": "http://arxiv.org/abs/2502.08512v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM Pretraining with Continuous Concepts", "abstract": "Next token prediction has been the standard training objective used in large\nlanguage model pretraining. Representations are learned as a result of\noptimizing for token-level perplexity. We propose Continuous Concept Mixing\n(CoCoMix), a novel pretraining framework that combines discrete next token\nprediction with continuous concepts. Specifically, CoCoMix predicts continuous\nconcepts learned from a pretrained sparse autoencoder and mixes them into the\nmodel's hidden state by interleaving with token hidden representations. Through\nexperiments on multiple benchmarks, including language modeling and downstream\nreasoning tasks, we show that CoCoMix is more sample efficient and consistently\noutperforms standard next token prediction, knowledge distillation and\ninserting pause tokens. We find that combining both concept learning and\ninterleaving in an end-to-end framework is critical to performance gains.\nFurthermore, CoCoMix enhances interpretability and steerability by allowing\ndirect inspection and modification of the predicted concept, offering a\ntransparent way to guide the model's internal reasoning process.", "published": "2025-02-12 16:00:11", "link": "http://arxiv.org/abs/2502.08524v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LLMs can implicitly learn from mistakes in-context", "abstract": "Learning from mistakes is a fundamental feature of human intelligence.\nPrevious work has shown that Large Language Models (LLMs) can also learn from\nincorrect answers when provided with a comprehensive rationale detailing why an\nanswer is wrong or how to correct it. In this work, we examine whether LLMs can\nlearn from mistakes in mathematical reasoning tasks when these explanations are\nnot provided. We investigate if LLMs are able to implicitly infer such\nrationales simply from observing both incorrect and correct answers.\nSurprisingly, we find that LLMs perform better, on average, when rationales are\neliminated from the context and incorrect answers are simply shown alongside\ncorrect ones. This approach also substantially outperforms chain-of-thought\nprompting in our evaluations. We show that these results are consistent across\nLLMs of different sizes and varying reasoning abilities. Further, we carry out\nan in-depth analysis, and show that prompting with both wrong and correct\nanswers leads to greater performance and better generalisation than introducing\nadditional, more diverse question-answer pairs into the context. Finally, we\nshow that new rationales generated by models that have only observed incorrect\nand correct answers are scored equally as highly by humans as those produced\nwith the aid of exemplar rationales. Our results demonstrate that LLMs are\nindeed capable of in-context implicit learning.", "published": "2025-02-12 16:31:21", "link": "http://arxiv.org/abs/2502.08550v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SelfElicit: Your Language Model Secretly Knows Where is the Relevant\n  Evidence", "abstract": "Providing Language Models (LMs) with relevant evidence in the context (either\nvia retrieval or user-provided) can significantly improve their ability to\nprovide factually correct grounded responses. However, recent studies have\nfound that LMs often struggle to fully comprehend and utilize key evidence from\nthe context, especially when it contains noise and irrelevant information - an\nissue common in real-world scenarios. To address this, we propose SelfElicit,\nan inference-time approach that helps LMs focus on key contextual evidence\nthrough self-guided explicit highlighting. By leveraging the inherent\nevidence-finding capabilities of LMs using the attention scores of deeper\nlayers, our method automatically identifies and emphasizes key evidence within\nthe input context, facilitating more accurate and factually grounded responses\nwithout additional training or iterative prompting. We demonstrate that\nSelfElicit brings consistent and significant improvement on multiple\nevidence-based QA tasks for various LM families while maintaining computational\nefficiency. Our code and documentation are available at\nhttps://github.com/ZhiningLiu1998/SelfElicit.", "published": "2025-02-12 20:13:56", "link": "http://arxiv.org/abs/2502.08767v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Universal Model Routing for Efficient LLM Inference", "abstract": "Large language models' significant advances in capabilities are accompanied\nby significant increases in inference costs. Model routing is a simple\ntechnique for reducing inference cost, wherein one maintains a pool of\ncandidate LLMs, and learns to route each prompt to the smallest feasible LLM.\nExisting works focus on learning a router for a fixed pool of LLMs. In this\npaper, we consider the problem of dynamic routing, where new, previously\nunobserved LLMs are available at test time. We propose a new approach to this\nproblem that relies on representing each LLM as a feature vector, derived based\non predictions on a set of representative prompts. Based on this, we detail two\neffective strategies, relying on cluster-based routing and a learned cluster\nmap respectively. We prove that these strategies are estimates of a\ntheoretically optimal routing rule, and provide an excess risk bound to\nquantify their errors. Experiments on a range of public benchmarks show the\neffectiveness of the proposed strategies in routing amongst more than 30 unseen\nLLMs.", "published": "2025-02-12 20:30:28", "link": "http://arxiv.org/abs/2502.08773v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "If Multi-Agent Debate is the Answer, What is the Question?", "abstract": "Multi-agent debate (MAD) has emerged as a promising approach to enhance the\nfactual accuracy and reasoning quality of large language models (LLMs) by\nengaging multiple agents in iterative discussions during inference. Despite its\npotential, we argue that current MAD research suffers from critical\nshortcomings in evaluation practices, including limited dataset overlap and\ninconsistent baselines, raising significant concerns about generalizability.\nCorrespondingly, this paper presents a systematic evaluation of five\nrepresentative MAD methods across nine benchmarks using four foundational\nmodels. Surprisingly, our findings reveal that MAD methods fail to reliably\noutperform simple single-agent baselines such as Chain-of-Thought and\nSelf-Consistency, even when consuming additional inference-time computation.\nFrom our analysis, we found that model heterogeneity can significantly improve\nMAD frameworks. We propose Heter-MAD enabling a single LLM agent to access the\noutput from heterogeneous foundation models, which boosts the performance of\ncurrent MAD frameworks. Finally, we outline potential directions for advancing\nMAD, aiming to spark a broader conversation and inspire future work in this\narea.", "published": "2025-02-12 21:01:10", "link": "http://arxiv.org/abs/2502.08788v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can a Single Model Master Both Multi-turn Conversations and Tool Use?\n  CoALM: A Unified Conversational Agentic Language Model", "abstract": "Large Language Models (LLMs) with API-calling capabilities enabled building\neffective Language Agents (LA), while also revolutionizing the conventional\ntask-oriented dialogue (TOD) paradigm. However, current approaches face a\ncritical dilemma: TOD systems are often trained on a limited set of target\nAPIs, requiring new data to maintain their quality when interfacing with new\nservices, while LAs are not trained to maintain user intent over multi-turn\nconversations. Because both robust multi-turn management and advanced function\ncalling are crucial for effective conversational agents, we evaluate these\nskills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and\nAPI-Bank (LA), and our analyses reveal that specialized approaches excel in one\ndomain but underperform in the other. To bridge this chasm, we introduce CoALM\n(Conversational Agentic Language Model), a unified approach that integrates\nboth conversational and agentic capabilities. We created CoALM-IT, a carefully\nconstructed multi-task dataset that interleave multi-turn ReAct reasoning with\ncomplex API usage. Using CoALM-IT, we train three models CoALM 8B, CoALM 70B,\nand CoALM 405B, which outperform top domain-specific models, including GPT-4o,\nacross all three benchmarks. This demonstrates the feasibility of a single\nmodel approach for both TOD and LA, setting a new standard for conversational\nagents.", "published": "2025-02-12 22:18:34", "link": "http://arxiv.org/abs/2502.08820v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Neuro-Conceptual Artificial Intelligence: Integrating OPM with Deep\n  Learning to Enhance Question Answering Quality", "abstract": "Knowledge representation and reasoning are critical challenges in Artificial\nIntelligence (AI), particularly in integrating neural and symbolic approaches\nto achieve explainable and transparent AI systems. Traditional knowledge\nrepresentation methods often fall short of capturing complex processes and\nstate changes. We introduce Neuro-Conceptual Artificial Intelligence (NCAI), a\nspecialization of the neuro-symbolic AI approach that integrates conceptual\nmodeling using Object-Process Methodology (OPM) ISO 19450:2024 with deep\nlearning to enhance question-answering (QA) quality. By converting natural\nlanguage text into OPM models using in-context learning, NCAI leverages the\nexpressive power of OPM to represent complex OPM elements-processes, objects,\nand states-beyond what traditional triplet-based knowledge graphs can easily\ncapture. This rich structured knowledge representation improves reasoning\ntransparency and answer accuracy in an OPM-QA system. We further propose\ntransparency evaluation metrics to quantitatively measure how faithfully the\npredicted reasoning aligns with OPM-based conceptual logic. Our experiments\ndemonstrate that NCAI outperforms traditional methods, highlighting its\npotential for advancing neuro-symbolic AI by providing rich knowledge\nrepresentations, measurable transparency, and improved reasoning.", "published": "2025-02-12 06:10:09", "link": "http://arxiv.org/abs/2502.09658v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Science of Evaluating Foundation Models", "abstract": "The emergent phenomena of large foundation models have revolutionized natural\nlanguage processing. However, evaluating these models presents significant\nchallenges due to their size, capabilities, and deployment across diverse\napplications. Existing literature often focuses on individual aspects, such as\nbenchmark performance or specific tasks, but fails to provide a cohesive\nprocess that integrates the nuances of diverse use cases with broader ethical\nand operational considerations. This work focuses on three key aspects: (1)\nFormalizing the Evaluation Process by providing a structured framework tailored\nto specific use-case contexts, (2) Offering Actionable Tools and Frameworks\nsuch as checklists and templates to ensure thorough, reproducible, and\npractical evaluations, and (3) Surveying Recent Work with a targeted review of\nadvancements in LLM evaluation, emphasizing real-world applications.", "published": "2025-02-12 22:55:43", "link": "http://arxiv.org/abs/2502.09670v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Break the Checkbox: Challenging Closed-Style Evaluations of Cultural\n  Alignment in LLMs", "abstract": "A large number of studies rely on closed-style multiple-choice surveys to\nevaluate cultural alignment in Large Language Models (LLMs). In this work, we\nchallenge this constrained evaluation paradigm and explore more realistic,\nunconstrained approaches. Using the World Values Survey (WVS) and Hofstede\nCultural Dimensions as case studies, we demonstrate that LLMs exhibit stronger\ncultural alignment in less constrained settings, where responses are not\nforced. Additionally, we show that even minor changes, such as reordering\nsurvey choices, lead to inconsistent outputs, exposing the limitations of\nclosed-style evaluations. Our findings advocate for more robust and flexible\nevaluation frameworks that focus on specific cultural proxies, encouraging more\nnuanced and accurate assessments of cultural alignment in LLMs.", "published": "2025-02-12 01:04:13", "link": "http://arxiv.org/abs/2502.08045v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "LowRA: Accurate and Efficient LoRA Fine-Tuning of LLMs under 2 Bits", "abstract": "Fine-tuning large language models (LLMs) is increasingly costly as models\nscale to hundreds of billions of parameters, and even parameter-efficient\nfine-tuning (PEFT) methods like LoRA remain resource-intensive. We introduce\nLowRA, the first framework to enable LoRA fine-tuning below 2 bits per\nparameter with minimal performance loss. LowRA optimizes fine-grained\nquantization - mapping, threshold selection, and precision assignment - while\nleveraging efficient CUDA kernels for scalable deployment. Extensive\nevaluations across 4 LLMs and 4 datasets show that LowRA achieves a superior\nperformance-precision trade-off above 2 bits and remains accurate down to 1.15\nbits, reducing memory usage by up to 50%. Our results highlight the potential\nof ultra-low-bit LoRA fine-tuning for resource-constrained environments.", "published": "2025-02-12 05:48:26", "link": "http://arxiv.org/abs/2502.08141v1", "categories": ["cs.LG", "cs.AR", "cs.CL", "cs.PF"], "primary_category": "cs.LG"}
{"title": "Wisdom of the Crowds in Forecasting: Forecast Summarization for\n  Supporting Future Event Prediction", "abstract": "Future Event Prediction (FEP) is an essential activity whose demand and\napplication range across multiple domains. While traditional methods like\nsimulations, predictive and time-series forecasting have demonstrated promising\noutcomes, their application in forecasting complex events is not entirely\nreliable due to the inability of numerical data to accurately capture the\nsemantic information related to events. One forecasting way is to gather and\naggregate collective opinions on the future to make predictions as cumulative\nperspectives carry the potential to help estimating the likelihood of upcoming\nevents. In this work, we organize the existing research and frameworks that aim\nto support future event prediction based on crowd wisdom through aggregating\nindividual forecasts. We discuss the challenges involved, available datasets,\nas well as the scope of improvement and future research directions for this\ntask. We also introduce a novel data model to represent individual forecast\nstatements.", "published": "2025-02-12 08:35:10", "link": "http://arxiv.org/abs/2502.08205v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Dealing with Annotator Disagreement in Hate Speech Classification", "abstract": "Hate speech detection is a crucial task, especially on social media, where\nharmful content can spread quickly. Implementing machine learning models to\nautomatically identify and address hate speech is essential for mitigating its\nimpact and preventing its proliferation. The first step in developing an\neffective hate speech detection model is to acquire a high-quality dataset for\ntraining. Labeled data is foundational for most natural language processing\ntasks, but categorizing hate speech is difficult due to the diverse and often\nsubjective nature of hate speech, which can lead to varying interpretations and\ndisagreements among annotators. This paper examines strategies for addressing\nannotator disagreement, an issue that has been largely overlooked. In\nparticular, we evaluate different approaches to deal with annotator\ndisagreement regarding hate speech classification in Turkish tweets, based on a\nfine-tuned BERT model. Our work highlights the importance of the problem and\nprovides state-of-art benchmark results for detection and understanding of hate\nspeech in online discourse.", "published": "2025-02-12 10:19:50", "link": "http://arxiv.org/abs/2502.08266v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "What Is That Talk About? A Video-to-Text Summarization Dataset for\n  Scientific Presentations", "abstract": "Transforming recorded videos into concise and accurate textual summaries is a\ngrowing challenge in multimodal learning. This paper introduces VISTA, a\ndataset specifically designed for video-to-text summarization in scientific\ndomains. VISTA contains 18,599 recorded AI conference presentations paired with\ntheir corresponding paper abstracts. We benchmark the performance of\nstate-of-the-art large models and apply a plan-based framework to better\ncapture the structured nature of abstracts. Both human and automated\nevaluations confirm that explicit planning enhances summary quality and factual\nconsistency. However, a considerable gap remains between models and human\nperformance, highlighting the challenges of scientific video summarization.", "published": "2025-02-12 10:36:55", "link": "http://arxiv.org/abs/2502.08279v3", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Improving Existing Optimization Algorithms with LLMs", "abstract": "The integration of Large Language Models (LLMs) into optimization has created\na powerful synergy, opening exciting research opportunities. This paper\ninvestigates how LLMs can enhance existing optimization algorithms. Using their\npre-trained knowledge, we demonstrate their ability to propose innovative\nheuristic variations and implementation strategies. To evaluate this, we\napplied a non-trivial optimization algorithm, Construct, Merge, Solve and Adapt\n(CMSA) -- a hybrid metaheuristic for combinatorial optimization problems that\nincorporates a heuristic in the solution construction phase. Our results show\nthat an alternative heuristic proposed by GPT-4o outperforms the\nexpert-designed heuristic of CMSA, with the performance gap widening on larger\nand denser graphs. Project URL: https://imp-opt-algo-llms.surge.sh/", "published": "2025-02-12 10:58:57", "link": "http://arxiv.org/abs/2502.08298v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE", "I.2.7; I.2.8"], "primary_category": "cs.AI"}
{"title": "Compromising Honesty and Harmlessness in Language Models via Deception\n  Attacks", "abstract": "Recent research on large language models (LLMs) has demonstrated their\nability to understand and employ deceptive behavior, even without explicit\nprompting. However, such behavior has only been observed in rare, specialized\ncases and has not been shown to pose a serious risk to users. Additionally,\nresearch on AI alignment has made significant advancements in training models\nto refuse generating misleading or toxic content. As a result, LLMs generally\nbecame honest and harmless. In this study, we introduce a novel attack that\nundermines both of these traits, revealing a vulnerability that, if exploited,\ncould have serious real-world consequences. In particular, we introduce\nfine-tuning methods that enhance deception tendencies beyond model safeguards.\nThese \"deception attacks\" customize models to mislead users when prompted on\nchosen topics while remaining accurate on others. Furthermore, we find that\ndeceptive models also exhibit toxicity, generating hate speech, stereotypes,\nand other harmful content. Finally, we assess whether models can deceive\nconsistently in multi-turn dialogues, yielding mixed results. Given that\nmillions of users interact with LLM-based chatbots, voice assistants, agents,\nand other interfaces where trustworthiness cannot be ensured, securing these\nmodels against deception attacks is critical.", "published": "2025-02-12 11:02:59", "link": "http://arxiv.org/abs/2502.08301v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Mitigating Hallucinations in Multimodal Spatial Relations through\n  Constraint-Aware Prompting", "abstract": "Spatial relation hallucinations pose a persistent challenge in large\nvision-language models (LVLMs), leading to generate incorrect predictions about\nobject positions and spatial configurations within an image. To address this\nissue, we propose a constraint-aware prompting framework designed to reduce\nspatial relation hallucinations. Specifically, we introduce two types of\nconstraints: (1) bidirectional constraint, which ensures consistency in\npairwise object relations, and (2) transitivity constraint, which enforces\nrelational dependence across multiple objects. By incorporating these\nconstraints, LVLMs can produce more spatially coherent and consistent outputs.\nWe evaluate our method on three widely-used spatial relation datasets,\ndemonstrating performance improvements over existing approaches. Additionally,\na systematic analysis of various bidirectional relation analysis choices and\ntransitivity reference selections highlights greater possibilities of our\nmethods in incorporating constraints to mitigate spatial relation\nhallucinations.", "published": "2025-02-12 11:32:19", "link": "http://arxiv.org/abs/2502.08317v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "From Haystack to Needle: Label Space Reduction for Zero-shot\n  Classification", "abstract": "We present Label Space Reduction (LSR), a novel method for improving\nzero-shot classification performance of Large Language Models (LLMs). LSR\niteratively refines the classification label space by systematically ranking\nand reducing candidate classes, enabling the model to concentrate on the most\nrelevant options. By leveraging unlabeled data with the statistical learning\ncapabilities of data-driven models, LSR dynamically optimizes the label space\nrepresentation at test time. Our experiments across seven benchmarks\ndemonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to\n14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet\ncompared to standard zero-shot classification baselines. To reduce the\ncomputational overhead of LSR, which requires an additional LLM call at each\niteration, we propose distilling the model into a probabilistic classifier,\nallowing for efficient inference.", "published": "2025-02-12 14:20:36", "link": "http://arxiv.org/abs/2502.08436v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Better Embeddings with Coupled Adam", "abstract": "Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.", "published": "2025-02-12 14:32:17", "link": "http://arxiv.org/abs/2502.08441v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "mmE5: Improving Multimodal Multilingual Embeddings via High-quality\n  Synthetic Data", "abstract": "Multimodal embedding models have gained significant attention for their\nability to map data from different modalities, such as text and images, into a\nunified representation space. However, the limited labeled multimodal data\noften hinders embedding performance. Recent approaches have leveraged data\nsynthesis to address this problem, yet the quality of synthetic data remains a\ncritical bottleneck. In this work, we identify three criteria for high-quality\nsynthetic multimodal data. First, broad scope ensures that the generated data\ncovers diverse tasks and modalities, making it applicable to various downstream\nscenarios. Second, robust cross-modal alignment makes different modalities\nsemantically consistent. Third, high fidelity ensures that the synthetic data\nmaintains realistic details to enhance its reliability. Guided by these\nprinciples, we synthesize datasets that: (1) cover a wide range of tasks,\nmodality combinations, and languages, (2) are generated via a deep thinking\nprocess within a single pass of a multimodal large language model, and (3)\nincorporate real-world images with accurate and relevant texts, ensuring\nfidelity through self-evaluation and refinement. Leveraging these high-quality\nsynthetic and labeled datasets, we train a multimodal multilingual E5 model\nmmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art\nperformance on the MMEB Benchmark and superior multilingual performance on the\nXTD benchmark. Our codes, datasets and models are released in\nhttps://github.com/haon-chen/mmE5.", "published": "2025-02-12 15:03:33", "link": "http://arxiv.org/abs/2502.08468v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Enhancing Auto-regressive Chain-of-Thought through Loop-Aligned\n  Reasoning", "abstract": "Chain-of-Thought (CoT) prompting has emerged as a powerful technique for\nenhancing language model's reasoning capabilities. However, generating long and\ncorrect CoT trajectories is challenging. Recent studies have demonstrated that\nLooped Transformers possess remarkable length generalization capabilities, but\ntheir limited generality and adaptability prevent them from serving as an\nalternative to auto-regressive solutions. To better leverage the strengths of\nLooped Transformers, we propose RELAY (REasoning through Loop Alignment\niterativelY). Specifically, we align the steps of Chain-of-Thought (CoT)\nreasoning with loop iterations and apply intermediate supervision during the\ntraining of Looped Transformers. This additional iteration-wise supervision not\nonly preserves the Looped Transformer's ability for length generalization but\nalso enables it to predict CoT reasoning steps for unseen data. Therefore, we\nleverage this Looped Transformer to generate accurate reasoning chains for\ncomplex problems that exceed the training length, which will then be used to\nfine-tune an auto-regressive model. We conduct extensive experiments, and the\nresults demonstrate the effectiveness of our approach, with significant\nimprovements in the performance of the auto-regressive model. Code will be\nreleased at https://github.com/qifanyu/RELAY.", "published": "2025-02-12 15:17:04", "link": "http://arxiv.org/abs/2502.08482v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion\n  in Information Retrieval", "abstract": "Query expansion is widely used in Information Retrieval (IR) to improve\nsearch outcomes by enriching queries with additional contextual information.\nAlthough recent Large Language Model (LLM) based methods generate\npseudo-relevant content and expanded terms via multiple prompts, they often\nyield repetitive, narrow expansions that lack the diverse context needed to\nretrieve all relevant information. In this paper, we introduce QA-Expand, a\nnovel and effective framework for query expansion. It first generates multiple\nrelevant questions from the initial query and subsequently produces\ncorresponding pseudo-answers as surrogate documents. A feedback model further\nrewrites and filters these answers to ensure only the most informative\naugmentations are incorporated. Extensive experiments on benchmarks such as\nBEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up\nto 13% over state-of-the-art methods, offering a robust solution for modern\nretrieval challenges.", "published": "2025-02-12 16:39:06", "link": "http://arxiv.org/abs/2502.08557v2", "categories": ["cs.IR", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.IR"}
{"title": "Distillation Scaling Laws", "abstract": "We provide a distillation scaling law that estimates distilled model\nperformance based on a compute budget and its allocation between the student\nand teacher. Our findings reduce the risks associated with using distillation\nat scale; compute allocation for both the teacher and student models can now be\ndone to maximize student performance. We provide compute optimal distillation\nrecipes for when 1) a teacher exists, or 2) a teacher needs training. If many\nstudents are to be distilled, or a teacher already exists, distillation\noutperforms supervised pretraining until a compute level which grows\npredictably with student size. If one student is to be distilled and a teacher\nalso needs training, supervised learning should be done instead. Additionally,\nwe provide insights across our large scale study of distillation, which\nincrease our understanding of distillation and inform experimental design.", "published": "2025-02-12 17:52:47", "link": "http://arxiv.org/abs/2502.08606v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Randomness of Low-Layer Parameters Determines Confusing Samples in Terms\n  of Interaction Representations of a DNN", "abstract": "In this paper, we find that the complexity of interactions encoded by a deep\nneural network (DNN) can explain its generalization power. We also discover\nthat the confusing samples of a DNN, which are represented by non-generalizable\ninteractions, are determined by its low-layer parameters. In comparison, other\nfactors, such as high-layer parameters and network architecture, have much less\nimpact on the composition of confusing samples. Two DNNs with different\nlow-layer parameters usually have fully different sets of confusing samples,\neven though they have similar performance. This finding extends the\nunderstanding of the lottery ticket hypothesis, and well explains distinctive\nrepresentation power of different DNNs.", "published": "2025-02-12 18:25:13", "link": "http://arxiv.org/abs/2502.08625v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Mathematical Reasoning in Large Language Models: Assessing Logical and\n  Arithmetic Errors across Wide Numerical Ranges", "abstract": "Mathematical reasoning in Large Language Models (LLMs) is often evaluated\nusing benchmarks with limited numerical ranges, failing to reflect real-world\nproblem-solving across diverse scales. Furthermore, most existing evaluation\nmethods only compare model outputs to ground-truth answers, obscuring insights\ninto reasoning processes. To address these limitations, we introduce\nGSM-Ranges, a dataset generator derived from GSM8K that systematically perturbs\nnumerical values in math problems to assess model robustness across varying\nnumerical scales. Additionally, we propose a novel grading methodology that\ndistinguishes between logical and non-logical errors, offering a more precise\nevaluation of reasoning processes beyond computational accuracy. Our\nexperiments with various models reveal a significant increase in logical error\nrates-up to 14 percentage points-as numerical complexity rises, demonstrating a\ngeneral weakness in reasoning with out-of-distribution numerical values.\nMoreover, while models demonstrate high accuracy on standalone arithmetic\ntasks, their performance deteriorates substantially when computations are\nembedded within word problems. These findings provide a comprehensive\nevaluation of LLMs' mathematical reasoning capabilities and inform future\nresearch directions for improving numerical generalization in language models.", "published": "2025-02-12 09:53:10", "link": "http://arxiv.org/abs/2502.08680v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Are Expressions for Music Emotions the Same Across Cultures?", "abstract": "Music evokes profound emotions, yet the universality of emotional descriptors\nacross languages remains debated. A key challenge in cross-cultural research on\nmusic emotion is biased stimulus selection and manual curation of taxonomies,\npredominantly relying on Western music and languages. To address this, we\npropose a balanced experimental design with nine online experiments in Brazil,\nthe US, and South Korea, involving N=672 participants. First, we sample a\nbalanced set of popular music from these countries. Using an open-ended tagging\npipeline, we then gather emotion terms to create culture-specific taxonomies.\nFinally, using these bottom-up taxonomies, participants rate emotions of each\nsong. This allows us to map emotional similarities within and across cultures.\nResults show consistency in high arousal, high valence emotions but greater\nvariability in others. Notably, machine translations were often inadequate to\ncapture music-specific meanings. These findings together highlight the need for\na domain-sensitive, open-ended, bottom-up emotion elicitation approach to\nreduce cultural biases in emotion research.", "published": "2025-02-12 19:35:15", "link": "http://arxiv.org/abs/2502.08744v1", "categories": ["cs.CL", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Systematic Review on the Evaluation of Large Language Models in Theory\n  of Mind Tasks", "abstract": "In recent years, evaluating the Theory of Mind (ToM) capabilities of large\nlanguage models (LLMs) has received significant attention within the research\ncommunity. As the field rapidly evolves, navigating the diverse approaches and\nmethodologies has become increasingly complex. This systematic review\nsynthesizes current efforts to assess LLMs' ability to perform ToM tasks, an\nessential aspect of human cognition involving the attribution of mental states\nto oneself and others. Despite notable advancements, the proficiency of LLMs in\nToM remains a contentious issue. By categorizing benchmarks and tasks through a\ntaxonomy rooted in cognitive science, this review critically examines\nevaluation techniques, prompting strategies, and the inherent limitations of\nLLMs in replicating human-like mental state reasoning. A recurring theme in the\nliterature reveals that while LLMs demonstrate emerging competence in ToM\ntasks, significant gaps persist in their emulation of human cognitive\nabilities.", "published": "2025-02-12 21:19:30", "link": "http://arxiv.org/abs/2502.08796v1", "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Ask in Any Modality: A Comprehensive Survey on Multimodal\n  Retrieval-Augmented Generation", "abstract": "Large Language Models (LLMs) struggle with hallucinations and outdated\nknowledge due to their reliance on static training data. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by integrating external dynamic\ninformation enhancing factual and updated grounding. Recent advances in\nmultimodal learning have led to the development of Multimodal RAG,\nincorporating multiple modalities such as text, images, audio, and video to\nenhance the generated outputs. However, cross-modal alignment and reasoning\nintroduce unique challenges to Multimodal RAG, distinguishing it from\ntraditional unimodal RAG. This survey offers a structured and comprehensive\nanalysis of Multimodal RAG systems, covering datasets, metrics, benchmarks,\nevaluation, methodologies, and innovations in retrieval, fusion, augmentation,\nand generation. We precisely review training strategies, robustness\nenhancements, and loss functions, while also exploring the diverse Multimodal\nRAG scenarios. Furthermore, we discuss open challenges and future research\ndirections to support advancements in this evolving field. This survey lays the\nfoundation for developing more capable and reliable AI systems that effectively\nleverage multimodal dynamic external knowledge bases. Resources are available\nat https://github.com/llm-lab-org/Multimodal-RAG-Survey.", "published": "2025-02-12 22:33:41", "link": "http://arxiv.org/abs/2502.08826v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Cancer Vaccine Adjuvant Name Recognition from Biomedical Literature\n  using Large Language Models", "abstract": "Motivation: An adjuvant is a chemical incorporated into vaccines that\nenhances their efficacy by improving the immune response. Identifying adjuvant\nnames from cancer vaccine studies is essential for furthering research and\nenhancing immunotherapies. However, the manual curation from the constantly\nexpanding biomedical literature poses significant challenges. This study\nexplores the automated recognition of vaccine adjuvant names using Large\nLanguage Models (LLMs), specifically Generative Pretrained Transformers (GPT)\nand Large Language Model Meta AI (Llama). Methods: We utilized two datasets: 97\nclinical trial records from AdjuvareDB and 290 abstracts annotated with the\nVaccine Adjuvant Compendium (VAC). GPT-4o and Llama 3.2 were employed in\nzero-shot and few-shot learning paradigms with up to four examples per prompt.\nPrompts explicitly targeted adjuvant names, testing the impact of contextual\ninformation such as substances or interventions. Outputs underwent automated\nand manual validation for accuracy and consistency. Results: GPT-4o attained\n100% Precision across all situations while exhibiting notable improve in Recall\nand F1-scores, particularly with incorporating interventions. On the VAC\ndataset, GPT-4o achieved a maximum F1-score of 77.32% with interventions,\nsurpassing Llama-3.2-3B by approximately 2%. On the AdjuvareDB dataset, GPT-4o\nreached an F1-score of 81.67% for three-shot prompting with interventions,\nsurpassing Llama-3.2-3 B's maximum F1-score of 65.62%. Conclusion: Our findings\ndemonstrate that LLMs excel at identifying adjuvant names, including rare\nvariations of naming representation. This study emphasizes the capability of\nLLMs to enhance cancer vaccine development by efficiently extracting insights.\nFuture work aims to broaden the framework to encompass various biomedical\nliterature and enhance model generalizability across various vaccines and\nadjuvants.", "published": "2025-02-12 06:30:31", "link": "http://arxiv.org/abs/2502.09659v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "k-LLMmeans: Summaries as Centroids for Interpretable and Scalable\n  LLM-Based Text Clustering", "abstract": "We introduce k-LLMmeans, a novel modification of the k-means clustering\nalgorithm that utilizes LLMs to generate textual summaries as cluster\ncentroids, thereby capturing contextual and semantic nuances often lost when\nrelying on purely numerical means of document embeddings. This modification\npreserves the properties of k-means while offering greater interpretability:\nthe cluster centroid is represented by an LLM-generated summary, whose\nembedding guides cluster assignments. We also propose a mini-batch variant,\nenabling efficient online clustering for streaming text data and providing\nreal-time interpretability of evolving cluster centroids. Through extensive\nsimulations, we show that our methods outperform vanilla k-means on multiple\nmetrics while incurring only modest LLM usage that does not scale with dataset\nsize. Finally, We present a case study showcasing the interpretability of\nevolving cluster centroids in sequential text streams. As part of our\nevaluation, we compile a new dataset from StackExchange, offering a benchmark\nfor text-stream clustering.", "published": "2025-02-12 19:50:22", "link": "http://arxiv.org/abs/2502.09667v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual\n  Reasoning in Mathematical LLMs", "abstract": "Leveraging mathematical Large Language Models (LLMs) for proof generation is\na fundamental topic in LLMs research. We argue that the ability of current LLMs\nto prove statements largely depends on whether they have encountered the\nrelevant proof process during training. This reliance limits their deeper\nunderstanding of mathematical theorems and related concepts. Inspired by the\npedagogical method of \"proof by counterexamples\" commonly used in human\nmathematics education, our work aims to enhance LLMs' ability to conduct\nmathematical reasoning and proof through counterexamples. Specifically, we\nmanually create a high-quality, university-level mathematical benchmark,\nCounterMATH, which requires LLMs to prove mathematical statements by providing\ncounterexamples, thereby assessing their grasp of mathematical concepts.\nAdditionally, we develop a data engineering framework to automatically obtain\ntraining data for further model improvement. Extensive experiments and detailed\nanalyses demonstrate that CounterMATH is challenging, indicating that LLMs,\nsuch as OpenAI o1, have insufficient counterexample-driven proof capabilities.\nMoreover, our exploration into model training reveals that strengthening LLMs'\ncounterexample-driven conceptual reasoning abilities is crucial for improving\ntheir overall mathematical capabilities. We believe that our work offers new\nperspectives on the community of mathematical LLMs.", "published": "2025-02-12 02:01:10", "link": "http://arxiv.org/abs/2502.10454v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "AuPair: Golden Example Pairs for Code Repair", "abstract": "Scaling up inference-time compute has proven to be a valuable strategy in\nimproving the performance of Large Language Models (LLMs) without fine-tuning.\nAn important task that can benefit from additional inference-time compute is\nself-repair; given an initial flawed response, or guess, the LLM corrects its\nown mistake and produces an improved response, or fix. We leverage the\nin-context learning ability of LLMs to perform self-repair in the coding\ndomain. The key contribution of our paper is an approach that synthesises and\nselects an ordered set of golden example pairs, or AuPairs, of these initial\nguesses and subsequent fixes for the corresponding problems. Each such AuPair\nis provided as a single in-context example at inference time to generate a\nrepaired solution. For an inference-time compute budget of $N$ LLM calls per\nproblem, $N$ AuPairs are used to generate $N$ repaired solutions, out of which\nthe highest-scoring solution is selected as the final answer. The underlying\nintuition is that if the LLM is given a different example of fixing an\nincorrect guess each time, it can subsequently generate a diverse set of\nrepaired solutions. Our algorithm selects these AuPairs in a manner that\nmaximises complementarity and usefulness. We demonstrate the results of our\nalgorithm on 5 LLMs across 7 competitive programming datasets for the code\nrepair task. Our algorithm yields a significant boost in performance compared\nto best-of-$N$ and self-repair, and also exhibits strong generalisation across\ndatasets and models. Moreover, our approach shows significantly stronger\nscaling with inference-time compute budget compared to baselines.", "published": "2025-02-12 11:07:04", "link": "http://arxiv.org/abs/2502.18487v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "DiMA: An LLM-Powered Ride-Hailing Assistant at DiDi", "abstract": "On-demand ride-hailing services like DiDi, Uber, and Lyft have transformed\nurban transportation, offering unmatched convenience and flexibility. In this\npaper, we introduce DiMA, an LLM-powered ride-hailing assistant deployed in\nDiDi Chuxing. Its goal is to provide seamless ride-hailing services and beyond\nthrough a natural and efficient conversational interface under dynamic and\ncomplex spatiotemporal urban contexts. To achieve this, we propose a\nspatiotemporal-aware order planning module that leverages external tools for\nprecise spatiotemporal reasoning and progressive order planning. Additionally,\nwe develop a cost-effective dialogue system that integrates multi-type dialog\nrepliers with cost-aware LLM configurations to handle diverse conversation\ngoals and trade-off response quality and latency. Furthermore, we introduce a\ncontinual fine-tuning scheme that utilizes real-world interactions and\nsimulated dialogues to align the assistant's behavior with human preferred\ndecision-making processes. Since its deployment in the DiDi application, DiMA\nhas demonstrated exceptional performance, achieving 93% accuracy in order\nplanning and 92% in response generation during real-world interactions. Offline\nexperiments further validate DiMA capabilities, showing improvements of up to\n70.23% in order planning and 321.27% in response generation compared to three\nstate-of-the-art agent frameworks, while reducing latency by $0.72\\times$ to\n$5.47\\times$. These results establish DiMA as an effective, efficient, and\nintelligent mobile assistant for ride-hailing services.", "published": "2025-02-12 10:33:45", "link": "http://arxiv.org/abs/2503.04768v1", "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Composite Sketch+Text Queries for Retrieving Objects with Elusive Names\n  and Complex Interactions", "abstract": "Non-native speakers with limited vocabulary often struggle to name specific\nobjects despite being able to visualize them, e.g., people outside Australia\nsearching for numbats. Further, users may want to search for such elusive\nobjects with difficult-to-sketch interactions, e.g., numbat digging in the\nground. In such common but complex situations, users desire a search interface\nthat accepts composite multimodal queries comprising hand-drawn sketches of\ndifficult-to-name but easy-to-draw objects and text describing\ndifficult-to-sketch but easy-to-verbalize object attributes or interaction with\nthe scene. This novel problem statement distinctly differs from the previously\nwell-researched TBIR (text-based image retrieval) and SBIR (sketch-based image\nretrieval) problems. To study this under-explored task, we curate a dataset,\nCSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of approx. 2M\nqueries and 108K natural scene images. Further, as a solution to this problem,\nwe propose a pretrained multimodal transformer-based baseline, STNET\n(Sketch+Text Network), that uses a hand-drawn sketch to localize relevant\nobjects in the natural scene image, and encodes the text and image to perform\nimage retrieval. In addition to contrastive learning, we propose multiple\ntraining objectives that improve the performance of our model. Extensive\nexperiments show that our proposed method outperforms several state-of-the-art\nretrieval methods for text-only, sketch-only, and composite query modalities.\nWe make the dataset and code available at our project website.", "published": "2025-02-12 14:22:59", "link": "http://arxiv.org/abs/2502.08438v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Utility Engineering: Analyzing and Controlling Emergent Value Systems in\n  AIs", "abstract": "As AIs rapidly advance and become more agentic, the risk they pose is\ngoverned not only by their capabilities but increasingly by their propensities,\nincluding goals and values. Tracking the emergence of goals and values has\nproven a longstanding problem, and despite much interest over the years it\nremains unclear whether current AIs have meaningful values. We propose a\nsolution to this problem, leveraging the framework of utility functions to\nstudy the internal coherence of AI preferences. Surprisingly, we find that\nindependently-sampled preferences in current LLMs exhibit high degrees of\nstructural coherence, and moreover that this emerges with scale. These findings\nsuggest that value systems emerge in LLMs in a meaningful sense, a finding with\nbroad implications. To study these emergent value systems, we propose utility\nengineering as a research agenda, comprising both the analysis and control of\nAI utilities. We uncover problematic and often shocking values in LLM\nassistants despite existing control measures. These include cases where AIs\nvalue themselves over humans and are anti-aligned with specific individuals. To\nconstrain these emergent value systems, we propose methods of utility control.\nAs a case study, we show how aligning utilities with a citizen assembly reduces\npolitical biases and generalizes to new scenarios. Whether we like it or not,\nvalue systems have already emerged in AIs, and much work remains to fully\nunderstand and control these emergent representations.", "published": "2025-02-12 18:55:43", "link": "http://arxiv.org/abs/2502.08640v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Linking Cryptoasset Attribution Tags to Knowledge Graph Entities: An\n  LLM-based Approach", "abstract": "Attribution tags form the foundation of modern cryptoasset forensics.\nHowever, inconsistent or incorrect tags can mislead investigations and even\nresult in false accusations. To address this issue, we propose a novel\ncomputational method based on Large Language Models (LLMs) to link attribution\ntags with well-defined knowledge graph concepts. We implemented this method in\nan end-to-end pipeline and conducted experiments showing that our approach\noutperforms baseline methods by up to 37.4% in F1-score across three publicly\navailable attribution tag datasets. By integrating concept filtering and\nblocking procedures, we generate candidate sets containing five knowledge graph\nentities, achieving a recall of 93% without the need for labeled data.\nAdditionally, we demonstrate that local LLM models can achieve F1-scores of\n90%, comparable to remote models which achieve 94%. We also analyze the\ncost-performance trade-offs of various LLMs and prompt templates, showing that\nselecting the most cost-effective configuration can reduce costs by 90%, with\nonly a 1% decrease in performance. Our method not only enhances attribution tag\nquality but also serves as a blueprint for fostering more reliable forensic\nevidence.", "published": "2025-02-12 01:28:40", "link": "http://arxiv.org/abs/2502.10453v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.DB", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Causal Analysis of ASR Errors for Children: Quantifying the Impact of\n  Physiological, Cognitive, and Extrinsic Factors", "abstract": "The increasing use of children's automatic speech recognition (ASR) systems\nhas spurred research efforts to improve the accuracy of models designed for\nchildren's speech in recent years. The current approach utilizes either\nopen-source speech foundation models (SFMs) directly or fine-tuning them with\nchildren's speech data. These SFMs, whether open-source or fine-tuned for\nchildren, often exhibit higher word error rates (WERs) compared to adult\nspeech. However, there is a lack of systemic analysis of the cause of this\ndegraded performance of SFMs. Understanding and addressing the reasons behind\nthis performance disparity is crucial for improving the accuracy of SFMs for\nchildren's speech. Our study addresses this gap by investigating the causes of\naccuracy degradation and the primary contributors to WER in children's speech.\nIn the first part of the study, we conduct a comprehensive benchmarking study\non two self-supervised SFMs (Wav2Vec2.0 and Hubert) and two weakly supervised\nSFMs (Whisper and MMS) across various age groups on two children speech\ncorpora, establishing the raw data for the causal inference analysis in the\nsecond part. In the second part of the study, we analyze the impact of\nphysiological factors (age, gender), cognitive factors (pronunciation ability),\nand external factors (vocabulary difficulty, background noise, and word count)\non SFM accuracy in children's speech using causal inference. The results\nindicate that physiology (age) and particular external factor (number of words\nin audio) have the highest impact on accuracy, followed by background noise and\npronunciation ability. Fine-tuning SFMs on children's speech reduces\nsensitivity to physiological and cognitive factors, while sensitivity to the\nnumber of words in audio persists.\n  Keywords: Children's ASR, Speech Foundational Models, Causal Inference,\nPhysiology, Cognition, Pronunciation", "published": "2025-02-12 17:20:37", "link": "http://arxiv.org/abs/2502.08587v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "DualStream Contextual Fusion Network: Efficient Target Speaker\n  Extraction by Leveraging Mixture and Enrollment Interactions", "abstract": "Target speaker extraction focuses on extracting a target speech signal from\nan environment with multiple speakers by leveraging an enrollment. Existing\nmethods predominantly rely on speaker embeddings obtained from the enrollment,\npotentially disregarding the contextual information and the internal\ninteractions between the mixture and enrollment. In this paper, we propose a\nnovel DualStream Contextual Fusion Network (DCF-Net) in the time-frequency\n(T-F) domain. Specifically, DualStream Fusion Block (DSFB) is introduced to\nobtain contextual information and capture the interactions between\ncontextualized enrollment and mixture representation across both spatial and\nchannel dimensions, and then rich and consistent representations are utilized\nto guide the extraction network for better extraction. Experimental results\ndemonstrate that DCF-Net outperforms state-of-the-art (SOTA) methods, achieving\na scale-invariant signal-to-distortion ratio improvement (SI-SDRi) of 21.6 dB\non the benchmark dataset, and exhibits its robustness and effectiveness in both\nnoise and reverberation scenarios. In addition, the wrong extraction results of\nour model, called target confusion problem, reduce to 0.4%, which highlights\nthe potential of DCF-Net for practical applications.", "published": "2025-02-12 08:03:27", "link": "http://arxiv.org/abs/2502.08191v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sparse wavefield reconstruction and denoising with boostlets", "abstract": "Boostlets are spatiotemporal functions that decompose nondispersive\nwavefields into a collection of localized waveforms parametrized by dilations,\nhyperbolic rotations, and translations. We study the sparsity properties of\nboostlets and find that the resulting decompositions are significantly sparser\nthan those of other state-of-the-art representation systems, such as wavelets\nand shearlets. This translates into improved denoising performance when\nhard-thresholding the boostlet coefficients. The results suggest that boostlets\noffer a natural framework for sparsely decomposing wavefields in unified\nspace-time.", "published": "2025-02-12 09:18:10", "link": "http://arxiv.org/abs/2502.08230v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SIToBI -- A Speech Prosody Annotation Tool for Indian Languages", "abstract": "The availability of prosodic information from speech signals is useful in a\nwide range of applications. However, deriving this information from speech\nsignals can be a laborious task involving manual intervention. Therefore, the\ncurrent work focuses on developing a tool that can provide prosodic annotations\ncorresponding to a given speech signal, particularly for Indian languages. The\nproposed Segmentation with Intensity, Tones and Break Indices (SIToBI) tool\nprovides time-aligned phoneme, syllable, and word transcriptions,\nsyllable-level pitch contour annotations, break indices, and syllable-level\nrelative intensity indices. The tool focuses more on syllable-level annotations\nsince Indian languages are syllable-timed. Indians, regardless of the language\nthey speak, may exhibit influences from other languages. As a result, other\nlanguages spoken in India may also exhibit syllable-timed characteristics. The\naccuracy of the annotations derived from the tool is analyzed by comparing them\nagainst manual annotations and the tool is observed to perform well. While the\ncurrent work focuses on three languages, namely, Tamil, Hindi, and Indian\nEnglish, the tool can easily be extended to other Indian languages and possibly\nother syllable-timed languages as well.", "published": "2025-02-12 08:29:00", "link": "http://arxiv.org/abs/2502.09661v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "YNote: A Novel Music Notation for Fine-Tuning LLMs in Music Generation", "abstract": "The field of music generation using Large Language Models (LLMs) is evolving\nrapidly, yet existing music notation systems, such as MIDI, ABC Notation, and\nMusicXML, remain too complex for effective fine-tuning of LLMs. These formats\nare difficult for both machines and humans to interpret due to their\nvariability and intricate structure. To address these challenges, we introduce\nYNote, a simplified music notation system that uses only four characters to\nrepresent a note and its pitch. YNote's fixed format ensures consistency,\nmaking it easy to read and more suitable for fine-tuning LLMs. In our\nexperiments, we fine-tuned GPT-2 (124M) on a YNote-encoded dataset and achieved\nBLEU and ROUGE scores of 0.883 and 0.766, respectively. With just two notes as\nprompts, the model was able to generate coherent and stylistically relevant\nmusic. We believe YNote offers a practical alternative to existing music\nnotations for machine learning applications and has the potential to\nsignificantly enhance the quality of music generation using LLMs.", "published": "2025-02-12 14:10:52", "link": "http://arxiv.org/abs/2502.10467v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
