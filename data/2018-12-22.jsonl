{"title": "A Survey on Deep Learning for Named Entity Recognition", "abstract": "Named entity recognition (NER) is the task to identify mentions of rigid\ndesignators from text belonging to predefined semantic types such as person,\nlocation, organization etc. NER always serves as the foundation for many\nnatural language applications such as question answering, text summarization,\nand machine translation. Early NER systems got a huge success in achieving good\nperformance with the cost of human engineering in designing domain-specific\nfeatures and rules. In recent years, deep learning, empowered by continuous\nreal-valued vector representations and semantic composition through nonlinear\nprocessing, has been employed in NER systems, yielding stat-of-the-art\nperformance. In this paper, we provide a comprehensive review on existing deep\nlearning techniques for NER. We first introduce NER resources, including tagged\nNER corpora and off-the-shelf NER tools. Then, we systematically categorize\nexisting works based on a taxonomy along three axes: distributed\nrepresentations for input, context encoder, and tag decoder. Next, we survey\nthe most representative methods for recent applied techniques of deep learning\nin new NER problem settings and applications. Finally, we present readers with\nthe challenges faced by NER systems and outline future directions in this area.", "published": "2018-12-22 04:54:13", "link": "http://arxiv.org/abs/1812.09449v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Slot Filling and Intent Detection via Capsule Neural Networks", "abstract": "Being able to recognize words as slots and detect the intent of an utterance\nhas been a keen issue in natural language understanding. The existing works\neither treat slot filling and intent detection separately in a pipeline manner,\nor adopt joint models which sequentially label slots while summarizing the\nutterance-level intent without explicitly preserving the hierarchical\nrelationship among words, slots, and intents. To exploit the semantic hierarchy\nfor effective modeling, we propose a capsule-based neural network model which\naccomplishes slot filling and intent detection via a dynamic\nrouting-by-agreement schema. A re-routing schema is proposed to further\nsynergize the slot filling performance using the inferred intent\nrepresentation. Experiments on two real-world datasets show the effectiveness\nof our model when compared with other alternative model architectures, as well\nas existing natural language understanding services.", "published": "2018-12-22 07:49:42", "link": "http://arxiv.org/abs/1812.09471v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distant Supervision for Relation Extraction with Linear Attenuation\n  Simulation and Non-IID Relevance Embedding", "abstract": "Distant supervision for relation extraction is an efficient method to reduce\nlabor costs and has been widely used to seek novel relational facts in large\ncorpora, which can be identified as a multi-instance multi-label problem.\nHowever, existing distant supervision methods suffer from selecting important\nwords in the sentence and extracting valid sentences in the bag. Towards this\nend, we propose a novel approach to address these problems in this paper.\nFirstly, we propose a linear attenuation simulation to reflect the importance\nof words in the sentence with respect to the distances between entities and\nwords. Secondly, we propose a non-independent and identically distributed\n(non-IID) relevance embedding to capture the relevance of sentences in the bag.\nOur method can not only capture complex information of words about hidden\nrelations, but also express the mutual information of instances in the bag.\nExtensive experiments on a benchmark dataset have well-validated the\neffectiveness of the proposed method.", "published": "2018-12-22 12:04:17", "link": "http://arxiv.org/abs/1812.09516v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Cross-Lingual Subword Similarities in Low-Resource Document\n  Classification", "abstract": "Text classification must sometimes be applied in a low-resource language with\nno labeled training data. However, training data may be available in a related\nlanguage. We investigate whether character-level knowledge transfer from a\nrelated language helps text classification. We present a cross-lingual document\nclassification framework (CACO) that exploits cross-lingual subword similarity\nby jointly training a character-based embedder and a word-based classifier. The\nembedder derives vector representations for input words from their written\nforms, and the classifier makes predictions based on the word vectors. We use a\njoint character representation for both the source language and the target\nlanguage, which allows the embedder to generalize knowledge about source\nlanguage words to target language words with similar forms. We propose a\nmulti-task objective that can further improve the model if additional\ncross-lingual or monolingual resources are available. Experiments confirm that\ncharacter-level knowledge transfer is more data-efficient than word-level\ntransfer between related languages.", "published": "2018-12-22 22:53:19", "link": "http://arxiv.org/abs/1812.09617v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Differentiable Supervector Extraction for Encoding Speaker and Phrase\n  Information in Text Dependent Speaker Verification", "abstract": "In this paper, we propose a new differentiable neural network alignment\nmechanism for text-dependent speaker verification which uses alignment models\nto produce a supervector representation of an utterance. Unlike previous works\nwith similar approaches, we do not extract the embedding of an utterance from\nthe mean reduction of the temporal dimension. Our system replaces the mean by a\nphrase alignment model to keep the temporal structure of each phrase which is\nrelevant in this application since the phonetic information is part of the\nidentity in the verification task. Moreover, we can apply a convolutional\nneural network as front-end, and thanks to the alignment process being\ndifferentiable, we can train the whole network to produce a supervector for\neach utterance which will be discriminative with respect to the speaker and the\nphrase simultaneously. As we show, this choice has the advantage that the\nsupervector encodes the phrase and speaker information providing good\nperformance in text-dependent speaker verification tasks. In this work, the\nprocess of verification is performed using a basic similarity metric, due to\nsimplicity, compared to other more elaborate models that are commonly used. The\nnew model using alignment to produce supervectors was tested on the\nRSR2015-Part I database for text-dependent speaker verification, providing\ncompetitive results compared to similar size networks using the mean to extract\nembeddings.", "published": "2018-12-22 09:25:59", "link": "http://arxiv.org/abs/1812.09484v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
