{"title": "Dynamic ETF Portfolio Optimization Using enhanced Transformer-Based Models for Covariance and Semi-Covariance Prediction(Work in Progress)", "abstract": "This study explores the use of Transformer-based models to predict both\ncovariance and semi-covariance matrices for ETF portfolio optimization.\nTraditional portfolio optimization techniques often rely on static covariance\nestimates or impose strict model assumptions, which may fail to capture the\ndynamic and non-linear nature of market fluctuations. Our approach leverages\nthe power of Transformer models to generate adaptive, real-time predictions of\nasset covariances, with a focus on the semi-covariance matrix to account for\ndownside risk. The semi-covariance matrix emphasizes negative correlations\nbetween assets, offering a more nuanced approach to risk management compared to\ntraditional methods that treat all volatility equally.\n  Through a series of experiments, we demonstrate that Transformer-based\npredictions of both covariance and semi-covariance significantly enhance\nportfolio performance. Our results show that portfolios optimized using the\nsemi-covariance matrix outperform those optimized with the standard covariance\nmatrix, particularly in volatile market conditions. Moreover, the use of the\nSortino ratio, a risk-adjusted performance metric that focuses on downside\nrisk, further validates the effectiveness of our approach in managing risk\nwhile maximizing returns.\n  These findings have important implications for asset managers and investors,\noffering a dynamic, data-driven framework for portfolio construction that\nadapts more effectively to shifting market conditions. By integrating\nTransformer-based models with the semi-covariance matrix for improved risk\nmanagement, this research contributes to the growing field of machine learning\nin finance and provides valuable insights for optimizing ETF portfolios.", "published": "2024-11-29 12:05:30", "link": "http://arxiv.org/abs/2411.19649v1", "categories": ["q-fin.PM", "q-fin.CP"], "primary_category": "q-fin.PM"}
{"title": "Capital Asset Pricing Model with Size Factor and Normalizing by Volatility Index", "abstract": "The Capital Asset Pricing Model (CAPM) relates a well-diversified stock\nportfolio to a benchmark portfolio. We insert size effect in CAPM, capturing\nthe observation that small stocks have higher risk and return than large\nstocks, on average. Dividing stock index returns by the Volatility Index makes\nthem independent and normal. In this article, we combine these ideas to create\na new discrete-time model, which includes volatility, relative size, and CAPM.\nWe fit this model using real-world data, prove the long-term stability, and\nconnect this research to Stochastic Portfolio Theory. We fill important gaps in\nour previous article on CAPM with the size factor.", "published": "2024-11-29 03:03:08", "link": "http://arxiv.org/abs/2411.19444v3", "categories": ["q-fin.MF", "math.PR", "q-fin.ST", "60G50, 62J05, 62M10, 62P05, 91G15"], "primary_category": "q-fin.MF"}
{"title": "Ergodic optimal liquidations in DeFi", "abstract": "We address the liquidation problem arising from the credit risk management in\ndecentralised finance (DeFi) by formulating it as an ergodic optimal control\nproblem. In decentralised derivatives exchanges, liquidation is triggered\nwhenever the parties fail to maintain sufficient collateral for their open\npositions. Consequently, effectively managing and liquidating disposal of\npositions accrued through liquidations is a critical concern for decentralised\nderivatives exchanges. By simplifying the model (linear temporary and permanent\nprice impacts, simplified cash balance dynamics), we derive the closed-form\nsolutions for the optimal liquidation strategies, which balance immediate\nexecutions with the temporary and permanent price impacts, and the optimal\nlong-term average reward. Numerical simulations further highlight the\neffectiveness of the proposed optimal strategy and demonstrate that the\nsimplified model closely approximates the original market environment. Finally,\nwe provide the method for calibrating the parameters in the model from the\navailable data.", "published": "2024-11-29 11:39:40", "link": "http://arxiv.org/abs/2411.19637v1", "categories": ["q-fin.TR", "math.OC", "Primary 93E20, Secondary 91G80"], "primary_category": "q-fin.TR"}
{"title": "Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language\n  Models", "abstract": "Iterative retrieval refers to the process in which the model continuously\nqueries the retriever during generation to enhance the relevance of the\nretrieved knowledge, thereby improving the performance of Retrieval-Augmented\nGeneration (RAG). Existing work typically employs few-shot prompting or\nmanually constructed rules to implement iterative retrieval. This introduces\nadditional inference overhead and overlooks the remarkable reasoning\ncapabilities of Large Language Models (LLMs). In this paper, we introduce\nAuto-RAG, an autonomous iterative retrieval model centered on the LLM's\npowerful decision-making capabilities. Auto-RAG engages in multi-turn dialogues\nwith the retriever, systematically planning retrievals and refining queries to\nacquire valuable knowledge. This process continues until sufficient external\ninformation is gathered, at which point the results are presented to the user.\nTo this end, we develop a method for autonomously synthesizing reasoning-based\ndecision-making instructions in iterative retrieval and fine-tuned the latest\nopen-source LLMs. The experimental results indicate that Auto-RAG is capable of\nautonomous iterative interaction with the retriever, effectively leveraging the\nremarkable reasoning and decision-making abilities of LLMs, which lead to\noutstanding performance across six benchmarks. Further analysis reveals that\nAuto-RAG can autonomously adjust the number of iterations based on the\ndifficulty of the questions and the utility of the retrieved knowledge, without\nrequiring any human intervention. Moreover, Auto-RAG expresses the iterative\nretrieval process in natural language, enhancing interpretability while\nproviding users with a more intuitive experience\\footnote{Code is available at\n\\url{https://github.com/ictnlp/Auto-RAG}.", "published": "2024-11-29 03:01:05", "link": "http://arxiv.org/abs/2411.19443v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ensemble Watermarks for Large Language Models", "abstract": "The rapid advancement of large language models (LLMs) has made it\nincreasingly difficult to distinguish between text written by humans and\nmachines. While watermarks already exist for LLMs, they often lack flexibility,\nand struggle with attacks such as paraphrasing. To address these issues, we\npropose a multi-feature method for generating watermarks that combines multiple\ndistinct watermark features into an ensemble watermark. Concretely, we combine\nacrostica and sensorimotor norms with the established red-green watermark to\nachieve a 98% detection rate. After a paraphrasing attack the performance\nremains high with 95% detection rate. The red-green feature alone as baseline\nachieves a detection rate of 49%. The evaluation of all feature combinations\nreveals that the ensemble of all three consistently has the highest detection\nrate across several LLMs and watermark strength settings. Due to the\nflexibility of combining features in the ensemble, various requirements and\ntrade-offs can be addressed. Additionally, for all ensemble configurations the\nsame detection function can be used without adaptations. This method is\nparticularly of interest to facilitate accountability and prevent societal\nharm.", "published": "2024-11-29 09:18:32", "link": "http://arxiv.org/abs/2411.19563v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KV Shifting Attention Enhances Language Modeling", "abstract": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters.", "published": "2024-11-29 09:42:38", "link": "http://arxiv.org/abs/2411.19574v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ICPR 2024 Competition on Multilingual Claim-Span Identification", "abstract": "A lot of claims are made in social media posts, which may contain\nmisinformation or fake news. Hence, it is crucial to identify claims as a first\nstep towards claim verification. Given the huge number of social media posts,\nthe task of identifying claims needs to be automated. This competition deals\nwith the task of 'Claim Span Identification' in which, given a text, parts /\nspans that correspond to claims are to be identified. This task is more\nchallenging than the traditional binary classification of text into claim or\nnot-claim, and requires state-of-the-art methods in Pattern Recognition,\nNatural Language Processing and Machine Learning. For this competition, we used\na newly developed dataset called HECSI containing about 8K posts in English and\nabout 8K posts in Hindi with claim-spans marked by human annotators. This paper\ngives an overview of the competition, and the solutions developed by the\nparticipating teams.", "published": "2024-11-29 09:50:32", "link": "http://arxiv.org/abs/2411.19579v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-Context Learning with Noisy Labels", "abstract": "In-context learning refers to the emerging ability of large language models\n(LLMs) to perform a target task without additional training, utilizing\ndemonstrations of the task. Recent studies aim to enhance in-context learning\nperformance by selecting more useful demonstrations. However, they overlook the\npresence of inevitable noisy labels in task demonstrations that arise during\nthe labeling process in the real-world. In this paper, we propose a new task,\nin-context learning with noisy labels, which aims to solve real-world problems\nfor in-context learning where labels in task demonstrations would be corrupted.\nMoreover, we propose a new method and baseline methods for the new task,\ninspired by studies in learning with noisy labels. Through experiments, we\ndemonstrate that our proposed method can serve as a safeguard against\nperformance degradation in in-context learning caused by noisy labels.", "published": "2024-11-29 09:54:08", "link": "http://arxiv.org/abs/2411.19581v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Reason about the Region Connection Calculus?", "abstract": "Qualitative Spatial Reasoning is a well explored area of Knowledge\nRepresentation and Reasoning and has multiple applications ranging from\nGeographical Information Systems to Robotics and Computer Vision. Recently,\nmany claims have been made for the reasoning capabilities of Large Language\nModels (LLMs). Here, we investigate the extent to which a set of representative\nLLMs can perform classical qualitative spatial reasoning tasks on the\nmereotopological Region Connection Calculus, RCC-8. We conduct three pairs of\nexperiments (reconstruction of composition tables, alignment to human\ncomposition preferences, conceptual neighbourhood reconstruction) using\nstate-of-the-art LLMs; in each pair one experiment uses eponymous relations and\none, anonymous relations (to test the extent to which the LLM relies on\nknowledge about the relation names obtained during training). All instances are\nrepeated 30 times to measure the stochasticity of the LLMs.", "published": "2024-11-29 10:10:16", "link": "http://arxiv.org/abs/2411.19589v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM Teacher-Student Framework for Text Classification With No Manually\n  Annotated Data: A Case Study in IPTC News Topic Classification", "abstract": "With the ever-increasing number of news stories available online, classifying\nthem by topic, regardless of the language they are written in, has become\ncrucial for enhancing readers' access to relevant content. To address this\nchallenge, we propose a teacher-student framework based on large language\nmodels (LLMs) for developing multilingual news classification models of\nreasonable size with no need for manual data annotation. The framework employs\na Generative Pretrained Transformer (GPT) model as the teacher model to develop\nan IPTC Media Topic training dataset through automatic annotation of news\narticles in Slovenian, Croatian, Greek, and Catalan. The teacher model exhibits\na high zero-shot performance on all four languages. Its agreement with human\nannotators is comparable to that between the human annotators themselves. To\nmitigate the computational limitations associated with the requirement of\nprocessing millions of texts daily, smaller BERT-like student models are\nfine-tuned on the GPT-annotated dataset. These student models achieve high\nperformance comparable to the teacher model. Furthermore, we explore the impact\nof the training data size on the performance of the student models and\ninvestigate their monolingual, multilingual and zero-shot cross-lingual\ncapabilities. The findings indicate that student models can achieve high\nperformance with a relatively small number of training instances, and\ndemonstrate strong zero-shot cross-lingual abilities. Finally, we publish the\nbest-performing news topic classifier, enabling multilingual classification\nwith the top-level categories of the IPTC Media Topic schema.", "published": "2024-11-29 11:42:58", "link": "http://arxiv.org/abs/2411.19638v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-Oasis", "abstract": "After the introduction of Large Language Models (LLMs), there have been\nsubstantial improvements in the performance of Natural Language Generation\n(NLG) tasks, including Text Summarization and Machine Translation. However,\nLLMs still produce outputs containing hallucinations, that is, content not\ngrounded in factual information. Therefore, developing methods to assess the\nfactuality of LLMs has become urgent.\n  Indeed, resources for factuality evaluation have recently emerged. Although\nchallenging, these resources face one or more of the following limitations: (i)\nthey are tailored to a specific task or domain; (ii) they are limited in size,\nthereby preventing the training of new factuality evaluators; (iii) they are\ndesigned for simpler verification tasks, such as claim verification.\n  To address these issues, we introduce LLM-Oasis, to the best of our knowledge\nthe largest resource for training end-to-end factuality evaluators. LLM-Oasis\nis constructed by extracting claims from Wikipedia, falsifying a subset of\nthese claims, and generating pairs of factual and unfactual texts. We then rely\non human annotators to both validate the quality of our dataset and to create a\ngold standard test set for benchmarking factuality evaluation systems.\n  Our experiments demonstrate that LLM-Oasis presents a significant challenge\nfor state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our\nproposed end-to-end factuality evaluation task, highlighting its potential to\ndrive future research in the field.", "published": "2024-11-29 12:21:15", "link": "http://arxiv.org/abs/2411.19655v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MIMDE: Exploring the Use of Synthetic vs Human Data for Evaluating\n  Multi-Insight Multi-Document Extraction Tasks", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\ntext analysis tasks, yet their evaluation on complex, real-world applications\nremains challenging. We define a set of tasks, Multi-Insight Multi-Document\nExtraction (MIMDE) tasks, which involves extracting an optimal set of insights\nfrom a document corpus and mapping these insights back to their source\ndocuments. This task is fundamental to many practical applications, from\nanalyzing survey responses to processing medical records, where identifying and\ntracing key insights across documents is crucial. We develop an evaluation\nframework for MIMDE and introduce a novel set of complementary human and\nsynthetic datasets to examine the potential of synthetic data for LLM\nevaluation. After establishing optimal metrics for comparing extracted\ninsights, we benchmark 20 state-of-the-art LLMs on both datasets. Our analysis\nreveals a strong correlation (0.71) between the ability of LLMs to extracts\ninsights on our two datasets but synthetic data fails to capture the complexity\nof document-level analysis. These findings offer crucial guidance for the use\nof synthetic data in evaluating text analysis systems, highlighting both its\npotential and limitations.", "published": "2024-11-29 13:24:10", "link": "http://arxiv.org/abs/2411.19689v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Deep Learning Approach to Language-independent Gender Prediction on\n  Twitter", "abstract": "This work presents a set of experiments conducted to predict the gender of\nTwitter users based on language-independent features extracted from the text of\nthe users' tweets. The experiments were performed on a version of TwiSty\ndataset including tweets written by the users of six different languages:\nPortuguese, French, Dutch, English, German, and Italian. Logistic regression\n(LR), and feed-forward neural networks (FFNN) with back-propagation were used\nto build models in two different settings: Inter-Lingual (IL) and Cross-Lingual\n(CL). In the IL setting, the training and testing were performed on the same\nlanguage whereas in the CL, Italian and German datasets were set aside and only\nused as test sets and the rest were combined to compose training and\ndevelopment sets. In the IL, the highest accuracy score belongs to LR whereas\nin the CL, FFNN with three hidden layers yields the highest score. The results\nshow that neural network based models underperform traditional models when the\nsize of the training set is small; however, they beat traditional models by a\nnon-trivial margin, when they are fed with large enough data. Finally, the\nfeature analysis confirms that men and women have different writing styles\nindependent of their language.", "published": "2024-11-29 14:26:34", "link": "http://arxiv.org/abs/2411.19733v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "INCLUDE: Evaluating Multilingual Language Understanding with Regional\n  Knowledge", "abstract": "The performance differential of large language models (LLM) between languages\nhinders their effective deployment in many regions, inhibiting the potential\neconomic and societal value of generative AI tools in many communities.\nHowever, the development of functional LLMs in many languages (\\ie,\nmultilingual LLMs) is bottlenecked by the lack of high-quality evaluation\nresources in languages other than English. Moreover, current practices in\nmultilingual benchmark construction often translate English resources, ignoring\nthe regional and cultural knowledge of the environments in which multilingual\nsystems would be used. In this work, we construct an evaluation suite of\n197,243 QA pairs from local exam sources to measure the capabilities of\nmultilingual LLMs in a variety of regional contexts. Our novel resource,\nINCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across\n44 written languages that evaluates multilingual LLMs for performance in the\nactual language environments where they would be deployed.", "published": "2024-11-29 16:03:14", "link": "http://arxiv.org/abs/2411.19799v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SDR-GNN: Spectral Domain Reconstruction Graph Neural Network for\n  Incomplete Multimodal Learning in Conversational Emotion Recognition", "abstract": "Multimodal Emotion Recognition in Conversations (MERC) aims to classify\nutterance emotions using textual, auditory, and visual modal features. Most\nexisting MERC methods assume each utterance has complete modalities,\noverlooking the common issue of incomplete modalities in real-world scenarios.\nRecently, graph neural networks (GNNs) have achieved notable results in\nIncomplete Multimodal Emotion Recognition in Conversations (IMERC). However,\ntraditional GNNs focus on binary relationships between nodes, limiting their\nability to capture more complex, higher-order information. Moreover, repeated\nmessage passing can cause over-smoothing, reducing their capacity to preserve\nessential high-frequency details. To address these issues, we propose a\nSpectral Domain Reconstruction Graph Neural Network (SDR-GNN) for incomplete\nmultimodal learning in conversational emotion recognition. SDR-GNN constructs\nan utterance semantic interaction graph using a sliding window based on both\nspeaker and context relationships to model emotional dependencies. To capture\nhigher-order and high-frequency information, SDR-GNN utilizes weighted\nrelationship aggregation, ensuring consistent semantic feature extraction\nacross utterances. Additionally, it performs multi-frequency aggregation in the\nspectral domain, enabling efficient recovery of incomplete modalities by\nextracting both high- and low-frequency information. Finally, multi-head\nattention is applied to fuse and optimize features for emotion recognition.\nExtensive experiments on various real-world datasets demonstrate that our\napproach is effective in incomplete multimodal learning and outperforms current\nstate-of-the-art methods.", "published": "2024-11-29 16:31:50", "link": "http://arxiv.org/abs/2411.19822v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sensitive Content Classification in Social Media: A Holistic Resource\n  and Evaluation", "abstract": "The detection of sensitive content in large datasets is crucial for ensuring\nthat shared and analysed data is free from harmful material. However, current\nmoderation tools, such as external APIs, suffer from limitations in\ncustomisation, accuracy across diverse sensitive categories, and privacy\nconcerns. Additionally, existing datasets and open-source models focus\npredominantly on toxic language, leaving gaps in detecting other sensitive\ncategories such as substance abuse or self-harm. In this paper, we put forward\na unified dataset tailored for social media content moderation across six\nsensitive categories: conflictual language, profanity, sexually explicit\nmaterial, drug-related content, self-harm, and spam. By collecting and\nannotating data with consistent retrieval strategies and guidelines, we address\nthe shortcomings of previous focalised research. Our analysis demonstrates that\nfine-tuning large language models (LLMs) on this novel dataset yields\nsignificant improvements in detection performance compared to open\noff-the-shelf models such as LLaMA, and even proprietary OpenAI models, which\nunderperform by 10-15% overall. This limitation is even more pronounced on\npopular moderation APIs, which cannot be easily tailored to specific sensitive\ncontent categories, among others.", "published": "2024-11-29 16:44:02", "link": "http://arxiv.org/abs/2411.19832v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Artificial intelligence contribution to translation industry: looking\n  back and forward", "abstract": "This study provides a comprehensive analysis of artificial intelligence (AI)\ncontribution to translation industry (ACTI) research, synthesizing it over\nforty-one years from 1980-2024. 13220 articles were retrieved from three\nsources, namely WoS, Scopus, and Lens. We provided two types of analysis, viz.,\nscientometric and thematic, focusing on cluster, subject categories, keywords,\nburstness, centrality and research centers as for the former. For the latter,\nwe thematically review 18 articles, selected purposefully from the articles\ninvolved, centering on purpose, approach, findings, and contribution to ACTI\nfuture directions. The findings reveal that in the past AI contribution to\ntranslation industry was not rigorous, resulting in rule-based machine\ntranslation and statistical machine translation whose output was not\nsatisfactory. However, the more AI develops, the more machine translation\ndevelops, incorporating Neural Networking Algorithms and (Deep) Language\nLearning Models like ChatGPT whose translation output has developed\nconsiderably. However, much rigorous research is still needed to overcome\nseveral problems encountering translation industry, specifically concerning\nlow-source languages, multi-dialectical and free word order languages, and\ncultural and religious registers.", "published": "2024-11-29 17:10:33", "link": "http://arxiv.org/abs/2411.19855v2", "categories": ["cs.CL", "cs-CL", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "What fifty-one years of Linguistics and Artificial Intelligence research\n  tell us about their correlation: A scientometric review", "abstract": "There is a strong correlation between linguistics and artificial intelligence\n(AI), best manifested by deep learning language models. This study provides a\nthorough scientometric analysis of this correlation, synthesizing the\nintellectual production during 51 years, from 1974 to 2024. It involves 5750\nWeb of Science-indexed articles published in 2124 journals, which are written\nby 20835 authors belonging to 13773 research centers in 794 countries. Two\npowerful software, viz., CiteSpace and VOSviewer, were used to generate mapping\nvisualizations of the intellectual landscape, trending issues and (re)emerging\nhotspots. The results indicate that in the 1980s and 1990s, linguistics and AI\nresearch was not robust, characterized by unstable publication over time. It\nhas, however, witnessed a remarkable increase of publication since then,\nreaching 1478 articles in 2023, and 546 articles in January-March timespan in\n2024, involving emerging issues and hotspots, addressing new horizons, new\ntopics, and launching new applications and powerful deep learning language\nmodels including ChatGPT.", "published": "2024-11-29 17:12:06", "link": "http://arxiv.org/abs/2411.19858v1", "categories": ["cs.CL", "cs-CL", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Train Once for All: A Transitional Approach for Efficient Aspect\n  Sentiment Triplet Extraction", "abstract": "Aspect-Opinion Pair Extraction (AOPE) and Aspect Sentiment Triplet Extraction\n(ASTE) have drawn growing attention in NLP. However, most existing approaches\nextract aspects and opinions independently, optionally adding pairwise\nrelations, often leading to error propagation and high time complexity. To\naddress these challenges and being inspired by transition-based dependency\nparsing, we propose the first transition-based model for AOPE and ASTE that\nperforms aspect and opinion extraction jointly, which also better captures\nposition-aware aspect-opinion relations and mitigates entity-level bias. By\nintegrating contrastive-augmented optimization, our model delivers more\naccurate action predictions and jointly optimizes separate subtasks in linear\ntime. Extensive experiments on 4 commonly used ASTE/AOPE datasets show that,\nwhile performing worse when trained on a single dataset than some previous\nmodels, our model achieves the best performance on both ASTE and AOPE if\ntrained on combined datasets, outperforming the strongest previous models in\nF1-measures (often by a large margin). We hypothesize that this is due to our\nmodel's ability to learn transition actions from multiple datasets and domains.\nOur code is available at https://anonymous.4open.science/r/trans_aste-8FCF.", "published": "2024-11-29 19:10:41", "link": "http://arxiv.org/abs/2412.00208v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clinical Document Corpora -- Real Ones, Translated and Synthetic\n  Substitutes, and Assorted Domain Proxies: A Survey of Diversity in Corpus\n  Design, with Focus on German Text Data", "abstract": "We survey clinical document corpora, with focus on German textual data. Due\nto rigid data privacy legislation in Germany these resources, with only few\nexceptions, are stored in safe clinical data spaces and locked against\nclinic-external researchers. This situation stands in stark contrast with\nestablished workflows in the field of natural language processing where easy\naccessibility and reuse of data collections are common practice. Hence,\nalternative corpus designs have been examined to escape from this data poverty.\nBesides machine translation of English clinical datasets and the generation of\nsynthetic corpora with fictitious clinical contents, several other types of\ndomain proxies have come up as substitutes for clinical documents. Common\ninstances of close proxies are medical journal publications, therapy\nguidelines, drug labels, etc., more distant proxies include online encyclopedic\nmedical articles or medical contents from social media channels. After\nPRISM-conformant identification of 362 hits from 4 bibliographic systems, 78\nrelevant documents were finally selected for this review. They contained\noverall 92 different published versions of corpora from which 71 were truly\nunique in terms of their underlying document sets. Out of these, the majority\nwere clinical corpora -- 46 real ones, 5 translated ones, and 6 synthetic ones.\nAs to domain proxies, we identified 18 close and 17 distant ones. There is a\nclear divide between the large number of non-accessible authentic clinical\nGerman-language corpora and their publicly accessible substitutes: translated\nor synthetic, close or more distant proxies. So on first sight, the data\nbottleneck seems broken. Yet differences in genre-specific writing style,\nwording and medical domain expertise in this typological space are also\nobvious. This raises the question how valid alternative corpus designs really\nare.", "published": "2024-11-29 19:56:58", "link": "http://arxiv.org/abs/2412.00230v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Actions and Objects Pathways for Domain Adaptation in Video Question\n  Answering", "abstract": "In this paper, we introduce the Actions and Objects Pathways (AOPath) for\nout-of-domain generalization in video question answering tasks. AOPath\nleverages features from a large pretrained model to enhance generalizability\nwithout the need for explicit training on the unseen domains. Inspired by human\nbrain, AOPath dissociates the pretrained features into action and object\nfeatures, and subsequently processes them through separate reasoning pathways.\nIt utilizes a novel module which converts out-of-domain features into\ndomain-agnostic features without introducing any trainable weights. We validate\nthe proposed approach on the TVQA dataset, which is partitioned into multiple\nsubsets based on genre to facilitate the assessment of generalizability. The\nproposed approach demonstrates 5% and 4% superior performance over conventional\nclassifiers on out-of-domain and in-domain datasets, respectively. It also\noutperforms prior methods that involve training millions of parameters, whereas\nthe proposed approach trains very few parameters.", "published": "2024-11-29 02:14:05", "link": "http://arxiv.org/abs/2411.19434v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Beyond Surface Structure: A Causal Assessment of LLMs' Comprehension\n  Ability", "abstract": "Large language models (LLMs) have shown remarkable capability in natural\nlanguage tasks, yet debate persists on whether they truly comprehend deep\nstructure (i.e., core semantics) or merely rely on surface structure (e.g.,\npresentation format). Prior studies observe that LLMs' performance declines\nwhen intervening on surface structure, arguing their success relies on surface\nstructure recognition. However, surface structure sensitivity does not prevent\ndeep structure comprehension. Rigorously evaluating LLMs' capability requires\nanalyzing both, yet deep structure is often overlooked. To this end, we assess\nLLMs' comprehension ability using causal mediation analysis, aiming to fully\ndiscover the capability of using both deep and surface structures.\nSpecifically, we formulate the comprehension of deep structure as direct causal\neffect (DCE) and that of surface structure as indirect causal effect (ICE),\nrespectively. To address the non-estimability of original DCE and ICE --\nstemming from the infeasibility of isolating mutual influences of deep and\nsurface structures, we develop the corresponding quantifiable surrogates,\nincluding approximated DCE (ADCE) and approximated ICE (AICE). We further apply\nthe ADCE to evaluate a series of mainstream LLMs, showing that most of them\nexhibit deep structure comprehension ability, which grows along with the\nprediction accuracy. Comparing ADCE and AICE demonstrates closed-source LLMs\nrely more on deep structure, while open-source LLMs are more surface-sensitive,\nwhich decreases with model scale. Theoretically, ADCE is a bidirectional\nevaluation, which measures both the sufficiency and necessity of deep structure\nchanges in causing output variations, thus offering a more comprehensive\nassessment than accuracy, a common evaluation in LLMs. Our work provides new\ninsights into LLMs' deep structure comprehension and offers novel methods for\nLLMs evaluation.", "published": "2024-11-29 03:57:26", "link": "http://arxiv.org/abs/2411.19456v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Training Agents with Weakly Supervised Feedback from Large Language\n  Models", "abstract": "Large Language Models (LLMs) offer a promising basis for creating agents that\ncan tackle complex tasks through iterative environmental interaction. Existing\nmethods either require these agents to mimic expert-provided trajectories or\nrely on definitive environmental feedback for reinforcement learning which\nlimits their application to specific scenarios like gaming or code generation.\nThis paper introduces a novel training method for LLM-based agents using weakly\nsupervised signals from a critic LLM, bypassing the need for expert\ntrajectories or definitive feedback. Our agents are trained in iterative\nmanner, where they initially generate trajectories through environmental\ninteraction. Subsequently, a critic LLM selects a subset of good trajectories,\nwhich are then used to update the agents, enabling them to generate improved\ntrajectories in the next iteration. Extensive tests on the API-bank dataset\nshow consistent improvement in our agents' capabilities and comparable\nperformance to GPT-4, despite using open-source models with much fewer\nparameters.", "published": "2024-11-29 08:47:04", "link": "http://arxiv.org/abs/2411.19547v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with\n  Multi-dimensional and fine-grained information", "abstract": "During the development of large language models (LLMs), pre-training data\nplay a critical role in shaping LLMs' capabilities. In recent years several\nlarge-scale and high-quality pre-training datasets have been released to\naccelerate the research of LLMs, including ChineseWebText1.0, C4, Pile,\nWanJuan, MAPCC and others. However, as LLMs continue to evolve, focus has\nincreasingly shifted to domain-specific capabilities and safety concerns,\nmaking those previous coarse-grained texts insufficient for meeting training\nrequirements. Furthermore, fine-grained information, such as quality, domain\nand toxicity, is becoming increasingly important in building powerful and\nreliable LLMs for various scenarios. To address these challenges, in this paper\nwe propose a new tool-chain called MDFG-tool for constructing large-scale and\nhigh-quality Chinese datasets with multi-dimensional and fine-grained\ninformation. First, we employ manually crafted rules to discard explicit noisy\ntexts from raw contents. Second, the quality evaluation model, domain\nclassifier, and toxicity evaluation model are well-designed to assess the\nremaining cleaned data respectively. Finally, we integrate these three types of\nfine-grained information for each text. With this approach, we release the\nlargest, high-quality and fine-grained Chinese text ChineseWebText2.0, which\nconsists of 3.8TB and each text is associated with a quality score, domain\nlabels, a toxicity label and a toxicity score, facilitating the LLM researchers\nto select data based on various types of fine-grained information. The data,\ncodes and the tool-chain are available on this website\nhttps://github.com/CASIA-LM/ChineseWebText-2.0", "published": "2024-11-29 12:48:49", "link": "http://arxiv.org/abs/2411.19668v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TakeLab Retriever: AI-Driven Search Engine for Articles from Croatian\n  News Outlets", "abstract": "TakeLab Retriever is an AI-driven search engine designed to discover,\ncollect, and semantically analyze news articles from Croatian news outlets. It\noffers a unique perspective on the history and current landscape of Croatian\nonline news media, making it an essential tool for researchers seeking to\nuncover trends, patterns, and correlations that general-purpose search engines\ncannot provide. TakeLab retriever utilizes cutting-edge natural language\nprocessing (NLP) methods, enabling users to sift through articles using named\nentities, phrases, and topics through the web application. This technical\nreport is divided into two parts: the first explains how TakeLab Retriever is\nutilized, while the second provides a detailed account of its design. In the\nsecond part, we also address the software engineering challenges involved and\npropose solutions for developing a microservice-based semantic search engine\ncapable of handling over ten million news articles published over the past two\ndecades.", "published": "2024-11-29 14:08:32", "link": "http://arxiv.org/abs/2411.19718v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Towards Santali Linguistic Inclusion: Building the First\n  Santali-to-English Translation Model using mT5 Transformer and Data\n  Augmentation", "abstract": "Around seven million individuals in India, Bangladesh, Bhutan, and Nepal\nspeak Santali, positioning it as nearly the third most commonly used\nAustroasiatic language. Despite its prominence among the Austroasiatic language\nfamily's Munda subfamily, Santali lacks global recognition. Currently, no\ntranslation models exist for the Santali language. Our paper aims to include\nSantali to the NPL spectrum. We aim to examine the feasibility of building\nSantali translation models based on available Santali corpora. The paper\nsuccessfully addressed the low-resource problem and, with promising results,\nexamined the possibility of creating a functional Santali machine translation\nmodel in a low-resource setup. Our study shows that Santali-English parallel\ncorpus performs better when in transformers like mt5 as opposed to untrained\ntransformers, proving that transfer learning can be a viable technique that\nworks with Santali language. Besides the mT5 transformer, Santali-English\nperforms better than Santali-Bangla parallel corpus as the mT5 has been trained\nin way more English data than Bangla data. Lastly, our study shows that with\ndata augmentation, our model performs better.", "published": "2024-11-29 14:17:33", "link": "http://arxiv.org/abs/2411.19726v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AIDetx: a compression-based method for identification of\n  machine-learning generated text", "abstract": "This paper introduces AIDetx, a novel method for detecting machine-generated\ntext using data compression techniques. Traditional approaches, such as deep\nlearning classifiers, often suffer from high computational costs and limited\ninterpretability. To address these limitations, we propose a compression-based\nclassification framework that leverages finite-context models (FCMs). AIDetx\nconstructs distinct compression models for human-written and AI-generated text,\nclassifying new inputs based on which model achieves a higher compression\nratio. We evaluated AIDetx on two benchmark datasets, achieving F1 scores\nexceeding 97% and 99%, respectively, highlighting its high accuracy. Compared\nto current methods, such as large language models (LLMs), AIDetx offers a more\ninterpretable and computationally efficient solution, significantly reducing\nboth training time and hardware requirements (e.g., no GPUs needed). The full\nimplementation is publicly available at https://github.com/AIDetx/AIDetx.", "published": "2024-11-29 17:31:42", "link": "http://arxiv.org/abs/2411.19869v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "To Ensemble or Not: Assessing Majority Voting Strategies for Phishing\n  Detection with Large Language Models", "abstract": "The effectiveness of Large Language Models (LLMs) significantly relies on the\nquality of the prompts they receive. However, even when processing identical\nprompts, LLMs can yield varying outcomes due to differences in their training\nprocesses. To leverage the collective intelligence of multiple LLMs and enhance\ntheir performance, this study investigates three majority voting strategies for\ntext classification, focusing on phishing URL detection. The strategies are:\n(1) a prompt-based ensemble, which utilizes majority voting across the\nresponses generated by a single LLM to various prompts; (2) a model-based\nensemble, which entails aggregating responses from multiple LLMs to a single\nprompt; and (3) a hybrid ensemble, which combines the two methods by sending\ndifferent prompts to multiple LLMs and then aggregating their responses. Our\nanalysis shows that ensemble strategies are most suited in cases where\nindividual components exhibit equivalent performance levels. However, when\nthere is a significant discrepancy in individual performance, the effectiveness\nof the ensemble method may not exceed that of the highest-performing single LLM\nor prompt. In such instances, opting for ensemble techniques is not\nrecommended.", "published": "2024-11-29 14:42:23", "link": "http://arxiv.org/abs/2412.00166v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NushuRescue: Revitalization of the Endangered Nushu Language with AI", "abstract": "The preservation and revitalization of endangered and extinct languages is a\nmeaningful endeavor, conserving cultural heritage while enriching fields like\nlinguistics and anthropology. However, these languages are typically\nlow-resource, making their reconstruction labor-intensive and costly. This\nchallenge is exemplified by Nushu, a rare script historically used by Yao women\nin China for self-expression within a patriarchal society. To address this\nchallenge, we introduce NushuRescue, an AI-driven framework designed to train\nlarge language models (LLMs) on endangered languages with minimal data.\nNushuRescue automates evaluation and expands target corpora to accelerate\nlinguistic revitalization. As a foundational component, we developed NCGold, a\n500-sentence Nushu-Chinese parallel corpus, the first publicly available\ndataset of its kind. Leveraging GPT-4-Turbo, with no prior exposure to Nushu\nand only 35 short examples from NCGold, NushuRescue achieved 48.69% translation\naccuracy on 50 withheld sentences and generated NCSilver, a set of 98 newly\ntranslated modern Chinese sentences of varying lengths. A sample of both NCGold\nand NCSilver is included in the Supplementary Materials. Additionally, we\ndeveloped FastText-based and Seq2Seq models to further support research on\nNushu. NushuRescue provides a versatile and scalable tool for the\nrevitalization of endangered languages, minimizing the need for extensive human\ninput.", "published": "2024-11-29 19:25:00", "link": "http://arxiv.org/abs/2412.00218v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AI-assisted summary of suicide risk Formulation", "abstract": "Background: Formulation, associated with suicide risk assessment, is an\nindividualised process that seeks to understand the idiosyncratic nature and\ndevelopment of an individual's problems. Auditing clinical documentation on an\nelectronic health record (EHR) is challenging as it requires resource-intensive\nmanual efforts to identify keywords in relevant sections of specific forms.\nFurthermore, clinicians and healthcare professionals often do not use keywords;\ntheir clinical language can vary greatly and may contain various jargon and\nacronyms. Also, the relevant information may be recorded elsewhere. This study\ndescribes how we developed advanced Natural Language Processing (NLP)\nalgorithms, a branch of Artificial Intelligence (AI), to analyse EHR data\nautomatically. Method: Advanced Optical Character Recognition techniques were\nused to process unstructured data sets, such as portable document format (pdf)\nfiles. Free text data was cleaned and pre-processed using Normalisation of Free\nText techniques. We developed algorithms and tools to unify the free text.\nFinally, the formulation was checked for the presence of each concept based on\nsimilarity using NLP-powered semantic matching techniques. Results: We\nextracted information indicative of formulation and assessed it to cover the\nrelevant concepts. This was achieved using a Weighted Score to obtain a\nConfidence Level. Conclusion: The rigour to which formulation is completed is\ncrucial to effectively using EHRs, ensuring correct and timely identification,\nengagement and interventions that may potentially avoid many suicide attempts\nand suicides.", "published": "2024-11-29 16:40:28", "link": "http://arxiv.org/abs/2412.10388v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Simple and Provable Scaling Laws for the Test-Time Compute of Large\n  Language Models", "abstract": "We propose two simple yet principled algorithms that enjoy provable scaling\nlaws for the test-time compute of large language models (LLMs), which require a\nblack-box LLM and nothing else (e.g., no external verifier or reward model) for\na minimalistic implementation. (i) The first one is a two-stage knockout-style\nalgorithm: given an input problem, it first generates multiple candidate\nsolutions, and then aggregate them for a final output, via a knockout\ntournament where pairwise comparisons among the candidates are conducted.\nAssuming that the LLM can generate a correct solution with non-zero probability\nand do better than a random guess in comparing a pair of correct and incorrect\nsolutions, we prove theoretically that the failure probability of this\nalgorithm decays to zero exponentially or by a power law (depending on the\nspecific way of scaling) as its test-time compute grows. (ii) The second one is\na two-stage league-style algorithm, where each candidate solution is evaluated\nby its average win rate against multiple opponents, rather than eliminated upon\nloss to a single opponent. Under certain technical assumptions that are\nanalogous to but more robust than those required by the knockout-style\nalgorithm, we prove theoretically that the failure probability of the\nleague-style algorithm also decays to zero exponentially as its test-time\ncompute grows. Through extensive experiments with two challenging benchmarks,\nnamely GPQA and MMLU-Pro, we validate the proposed theories and demonstrate the\noutstanding scaling properties of both algorithms.", "published": "2024-11-29 05:29:47", "link": "http://arxiv.org/abs/2411.19477v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "COLD: Causal reasOning in cLosed Daily activities", "abstract": "Large Language Models (LLMs) have shown state-of-the-art performance in a\nvariety of tasks, including arithmetic and reasoning; however, to gauge the\nintellectual capabilities of LLMs, causal reasoning has become a reliable proxy\nfor validating a general understanding of the mechanics and intricacies of the\nworld similar to humans. Previous works in natural language processing (NLP)\nhave either focused on open-ended causal reasoning via causal commonsense\nreasoning (CCR) or framed a symbolic representation-based question answering\nfor theoretically backed-up analysis via a causal inference engine. The former\nadds an advantage of real-world grounding but lacks theoretically backed-up\nanalysis/validation, whereas the latter is far from real-world grounding. In\nthis work, we bridge this gap by proposing the COLD (Causal reasOning in cLosed\nDaily activities) framework, which is built upon human understanding of daily\nreal-world activities to reason about the causal nature of events. We show that\nthe proposed framework facilitates the creation of enormous causal queries (~ 9\nmillion) and comes close to the mini-turing test, simulating causal reasoning\nto evaluate the understanding of a daily real-world task. We evaluate multiple\nLLMs on the created causal queries and find that causal reasoning is\nchallenging even for activities trivial to humans. We further explore (the\ncausal reasoning abilities of LLMs) using the backdoor criterion to determine\nthe causal strength between events.", "published": "2024-11-29 06:37:13", "link": "http://arxiv.org/abs/2411.19500v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TQA-Bench: Evaluating LLMs for Multi-Table Question Answering with\n  Scalable Context and Symbolic Extension", "abstract": "The advent of large language models (LLMs) has unlocked great opportunities\nin complex data management tasks, particularly in question answering (QA) over\ncomplicated multi-table relational data. Despite significant progress,\nsystematically evaluating LLMs on multi-table QA remains a critical challenge\ndue to the inherent complexity of analyzing heterogeneous table structures and\npotential large scale of serialized relational data. Existing benchmarks\nprimarily focus on single-table QA, failing to capture the intricacies of\nreasoning across multiple relational tables, as required in real-world domains\nsuch as finance, healthcare, and e-commerce. To address this gap, we present\nTQA-Bench, a new multi-table QA benchmark designed to evaluate the capabilities\nof LLMs in tackling complex QA tasks over relational data. Our benchmark\nincorporates diverse relational database instances sourced from real-world\npublic datasets and introduces a flexible sampling mechanism to create tasks\nwith varying multi-table context lengths, ranging from 8K to 64K tokens. To\nensure robustness and reliability, we integrate symbolic extensions into the\nevaluation framework, enabling the assessment of LLM reasoning capabilities\nbeyond simple data retrieval or probabilistic pattern matching. We\nsystematically evaluate a range of LLMs, both open-source and closed-source,\nspanning model scales from 7 billion to 70 billion parameters. Our extensive\nexperiments reveal critical insights into the performance of LLMs in\nmulti-table QA, highlighting both challenges and opportunities for advancing\ntheir application in complex, data-driven environments. Our benchmark\nimplementation and results are available at\nhttps://github.com/Relaxed-System-Lab/TQA-Bench.", "published": "2024-11-29 06:48:13", "link": "http://arxiv.org/abs/2411.19504v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Knowledge Management for Automobile Failure Analysis Using Graph RAG", "abstract": "This paper presents a knowledge management system for automobile failure\nanalysis using retrieval-augmented generation (RAG) with large language models\n(LLMs) and knowledge graphs (KGs). In the automotive industry, there is a\ngrowing demand for knowledge transfer of failure analysis from experienced\nengineers to young engineers. However, failure events are phenomena that occur\nin a chain reaction, making them difficult for beginners to analyze them. While\nknowledge graphs, which can describe semantic relationships and structure\ninformation is effective in representing failure events, due to their\ncapability of representing the relationships between components, there is much\ninformation in KGs, so it is challenging for young engineers to extract and\nunderstand sub-graphs from the KG. On the other hand, there is increasing\ninterest in the use of Graph RAG, a type of RAG that combines LLMs and KGs for\nknowledge management. However, when using the current Graph RAG framework with\nan existing knowledge graph for automobile failures, several issues arise\nbecause it is difficult to generate executable queries for a knowledge graph\ndatabase which is not constructed by LLMs. To address this, we focused on\noptimizing the Graph RAG pipeline for existing knowledge graphs. Using an\noriginal Q&A dataset, the ROUGE F1 score of the sentences generated by the\nproposed method showed an average improvement of 157.6% compared to the current\nmethod. This highlights the effectiveness of the proposed method for automobile\nfailure analysis.", "published": "2024-11-29 08:34:07", "link": "http://arxiv.org/abs/2411.19539v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Initialization using Update Approximation is a Silver Bullet for\n  Extremely Efficient Low-Rank Fine-Tuning", "abstract": "Low-rank adapters have become standard for efficiently fine-tuning large\nlanguage models (LLMs), but they often fall short of achieving the performance\nof full fine-tuning. We propose a method, LoRA Silver Bullet or LoRA-SB, that\napproximates full fine-tuning within low-rank subspaces using a carefully\ndesigned initialization strategy. We theoretically demonstrate that the\narchitecture of LoRA-XS, which inserts a learnable (r x r) matrix between B and\nA while keeping other matrices fixed, provides the precise conditions needed\nfor this approximation. We leverage its constrained update space to achieve\noptimal scaling for high-rank gradient updates while removing the need for\nhyperparameter tuning. We prove that our initialization offers an optimal\nlow-rank approximation of the initial gradient and preserves update directions\nthroughout training. Extensive experiments across mathematical reasoning,\ncommonsense reasoning, and language understanding tasks demonstrate that our\napproach exceeds the performance of standard LoRA while using \\textbf{27-90}\ntimes fewer learnable parameters, and comprehensively outperforms LoRA-XS. Our\nfindings establish that it is possible to simulate full fine-tuning in low-rank\nsubspaces, and achieve significant efficiency gains without sacrificing\nperformance. Our code is publicly available at\nhttps://github.com/RaghavSinghal10/lora-sb.", "published": "2024-11-29 09:10:30", "link": "http://arxiv.org/abs/2411.19557v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Accelerating Multimodal Large Language Models via Dynamic Visual-Token\n  Exit and the Empirical Findings", "abstract": "The excessive use of visual tokens in existing Multimoal Large Language\nModels (MLLMs) often exhibits obvious redundancy and brings in prohibitively\nexpensive computation. To gain insights into this problem, we first conduct\nextensive empirical studies on the attention behaviors of MLLMs, and summarize\nthree main inference stages in MLLMs: (i) Early fusion between tokens is first\naccomplished quickly. (ii) Intra-modality modeling then comes to play. (iii)\nMultimodal reasoning} resumes and lasts until the end of inference. In\nparticular, we reveal that visual tokens will stop contributing to reasoning\nwhen the text tokens receive enough image information, yielding obvious visual\nredundancy. Based on these generalized observations, we propose a simple yet\neffective method to improve the efficiency of MLLMs, termed dynamic\nvisual-token exit (DyVTE). DyVTE uses lightweight hyper-networks to perceive\nthe text token status and decide the removal of all visual tokens after a\ncertain layer, thereby addressing the observed visual redundancy. To validate\nVTE, we apply it to a set of MLLMs, including LLaVA, VILA, Eagle and InternVL,\nand conduct extensive experiments on a bunch of benchmarks. The experiment\nresults not only show the effectiveness of our VTE in improving MLLMs'\nefficiency, but also yield the general modeling patterns of MLLMs, well\nfacilitating the in-depth understanding of MLLMs. Our code is anonymously\nreleased at https://github.com/DoubtedSteam/DyVTE.", "published": "2024-11-29 11:24:23", "link": "http://arxiv.org/abs/2411.19628v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Noro: A Noise-Robust One-shot Voice Conversion System with Hidden\n  Speaker Representation Capabilities", "abstract": "One-shot voice conversion (VC) aims to alter the timbre of speech from a\nsource speaker to match that of a target speaker using just a single reference\nspeech from the target, while preserving the semantic content of the original\nsource speech. Despite advancements in one-shot VC, its effectiveness decreases\nin real-world scenarios where reference speeches, often sourced from the\ninternet, contain various disturbances like background noise. To address this\nissue, we introduce Noro, a Noise Robust One-shot VC system. Noro features\ninnovative components tailored for VC using noisy reference speeches, including\na dual-branch reference encoding module and a noise-agnostic contrastive\nspeaker loss. Experimental results demonstrate that Noro outperforms our\nbaseline system in both clean and noisy scenarios, highlighting its efficacy\nfor real-world applications. Additionally, we investigate the hidden speaker\nrepresentation capabilities of our baseline system by repurposing its reference\nencoder as a speaker encoder. The results shows that it is competitive with\nseveral advanced self-supervised learning models for speaker representation\nunder the SUPERB settings, highlighting the potential for advancing speaker\nrepresentation learning through one-shot VC task.", "published": "2024-11-29 15:18:01", "link": "http://arxiv.org/abs/2411.19770v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware\n  Omni-Modal Perception of Long Videos", "abstract": "Despite impressive advancements in video understanding, most efforts remain\nlimited to coarse-grained or visual-only video tasks. However, real-world\nvideos encompass omni-modal information (vision, audio, and speech) with a\nseries of events forming a cohesive storyline. The lack of multi-modal video\ndata with fine-grained event annotations and the high cost of manual labeling\nare major obstacles to comprehensive omni-modality video perception. To address\nthis gap, we propose an automatic pipeline consisting of high-quality\nmulti-modal video filtering, semantically coherent omni-modal event boundary\ndetection, and cross-modal correlation-aware event captioning. In this way, we\npresent LongVALE, the first-ever Vision-Audio-Language Event understanding\nbenchmark comprising 105K omni-modal events with precise temporal boundaries\nand detailed relation-aware captions within 8.4K high-quality long videos.\nFurther, we build a baseline that leverages LongVALE to enable video large\nlanguage models (LLMs) for omni-modality fine-grained temporal video\nunderstanding for the first time. Extensive experiments demonstrate the\neffectiveness and great potential of LongVALE in advancing comprehensive\nmulti-modal video understanding.", "published": "2024-11-29 15:18:06", "link": "http://arxiv.org/abs/2411.19772v3", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "PerLA: Perceptive 3D Language Assistant", "abstract": "Enabling Large Language Models (LLMs) to understand the 3D physical world is\nan emerging yet challenging research direction. Current strategies for\nprocessing point clouds typically downsample the scene or divide it into\nsmaller parts for separate analysis. However, both approaches risk losing key\nlocal details or global contextual information. In this paper, we introduce\nPerLA, a 3D language assistant designed to be more perceptive to both details\nand context, making visual representations more informative for the LLM. PerLA\ncaptures high-resolution (local) details in parallel from different point cloud\nareas and integrates them with (global) context obtained from a\nlower-resolution whole point cloud. We present a novel algorithm that preserves\npoint cloud locality through the Hilbert curve and effectively aggregates\nlocal-to-global information via cross-attention and a graph neural network.\nLastly, we introduce a novel loss for local representation consensus to promote\ntraining stability. PerLA outperforms state-of-the-art 3D language assistants,\nwith gains of up to +1.34 CiDEr on ScanQA for question answering, and +4.22 on\nScanRefer and +3.88 on Nr3D for dense captioning.\nhttps://gfmei.github.io/PerLA/", "published": "2024-11-29 15:20:29", "link": "http://arxiv.org/abs/2411.19774v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "MoTe: Learning Motion-Text Diffusion Model for Multiple Generation Tasks", "abstract": "Recently, human motion analysis has experienced great improvement due to\ninspiring generative models such as the denoising diffusion model and large\nlanguage model. While the existing approaches mainly focus on generating\nmotions with textual descriptions and overlook the reciprocal task. In this\npaper, we present~\\textbf{MoTe}, a unified multi-modal model that could handle\ndiverse tasks by learning the marginal, conditional, and joint distributions of\nmotion and text simultaneously. MoTe enables us to handle the paired\ntext-motion generation, motion captioning, and text-driven motion generation by\nsimply modifying the input context. Specifically, MoTe is composed of three\ncomponents: Motion Encoder-Decoder (MED), Text Encoder-Decoder (TED), and\nMoti-on-Text Diffusion Model (MTDM). In particular, MED and TED are trained for\nextracting latent embeddings, and subsequently reconstructing the motion\nsequences and textual descriptions from the extracted embeddings, respectively.\nMTDM, on the other hand, performs an iterative denoising process on the input\ncontext to handle diverse tasks. Experimental results on the benchmark datasets\ndemonstrate the superior performance of our proposed method on text-to-motion\ngeneration and competitive performance on motion captioning.", "published": "2024-11-29 15:48:24", "link": "http://arxiv.org/abs/2411.19786v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Reverse Thinking Makes LLMs Stronger Reasoners", "abstract": "Reverse thinking plays a crucial role in human reasoning. Humans can reason\nnot only from a problem to a solution but also in reverse, i.e., start from the\nsolution and reason towards the problem. This often enhances overall reasoning\nperformance as it enables consistency checks between their forward and backward\nthinking. To enable Large Language Models (LLMs) to perform reverse thinking,\nwe introduce Reverse-Enhanced Thinking (RevThink), a framework composed of data\naugmentation and learning objectives. In RevThink, we augment the dataset by\ncollecting structured forward-backward reasoning from a teacher model,\nconsisting of: (1) the original question, (2) forward reasoning, (3) backward\nquestion, and (4) backward reasoning. We then employ three objectives to train\na smaller student model in a multi-task learning fashion: (a) generate forward\nreasoning from a question, (b) generate a backward question from a question,\nand (c) generate backward reasoning from the backward question. Experiments\nacross 12 datasets covering commonsense, math, and logical reasoning show an\naverage 13.53% improvement over the student model's zero-shot performance and a\n6.84% improvement over the strongest knowledge distillation baselines.\nMoreover, our method demonstrates sample efficiency -- using only 10% of the\ncorrect forward reasoning from the training data, it outperforms a standard\nfine-tuning method trained on 10x more forward reasoning. RevThink also\nexhibits strong generalization to out-of-distribution held-out datasets.", "published": "2024-11-29 17:27:05", "link": "http://arxiv.org/abs/2411.19865v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SIMS: Simulating Stylized Human-Scene Interactions with\n  Retrieval-Augmented Script Generation", "abstract": "Simulating stylized human-scene interactions (HSI) in physical environments\nis a challenging yet fascinating task. Prior works emphasize long-term\nexecution but fall short in achieving both diverse style and physical\nplausibility. To tackle this challenge, we introduce a novel hierarchical\nframework named SIMS that seamlessly bridges highlevel script-driven intent\nwith a low-level control policy, enabling more expressive and diverse\nhuman-scene interactions. Specifically, we employ Large Language Models with\nRetrieval-Augmented Generation (RAG) to generate coherent and diverse long-form\nscripts, providing a rich foundation for motion planning. A versatile\nmulticondition physics-based control policy is also developed, which leverages\ntext embeddings from the generated scripts to encode stylistic cues,\nsimultaneously perceiving environmental geometries and accomplishing task\ngoals. By integrating the retrieval-augmented script generation with the\nmulti-condition controller, our approach provides a unified solution for\ngenerating stylized HSI motions. We further introduce a comprehensive planning\ndataset produced by RAG and a stylized motion dataset featuring diverse\nlocomotions and interactions. Extensive experiments demonstrate SIMS's\neffectiveness in executing various tasks and generalizing across different\nscenarios, significantly outperforming previous methods.", "published": "2024-11-29 18:36:15", "link": "http://arxiv.org/abs/2411.19921v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.GR"], "primary_category": "cs.CV"}
{"title": "On Domain-Specific Post-Training for Multimodal Large Language Models", "abstract": "Adapting general multimodal large language models (MLLMs) to specific\ndomains, such as scientific and industrial fields, is highly significant in\npromoting their practical applications. This paper systematically investigates\ndomain adaptation of MLLMs through post-training, focusing on data synthesis,\ntraining pipelines, and task evaluation. (1) Data Synthesis: Using only\nopen-source models, we develop a generate-then-filter pipeline that curates\ndiverse visual instruction tasks based on domain-specific image-caption pairs.\nThe resulting data surpass the data synthesized by manual rules or strong\nclosed-source models (e.g., GPT-4V) in enhancing domain-specific performance.\n(2) Training Pipeline: While the two-stage training--initially on image-caption\npairs followed by visual instruction tasks--is commonly adopted for developing\ngeneral MLLMs, we apply a single-stage training pipeline to enhance task\ndiversity for domain-specific post-training. (3) Task Evaluation: We conduct\nextensive experiments in high-impact domains such as biomedicine, food, and\nremote sensing, by post-training a variety of MLLMs and then evaluating MLLM\nperformance on various domain-specific tasks. Furthermore, we fully open-source\nour models, code, and data to encourage future research in this area.", "published": "2024-11-29 18:42:28", "link": "http://arxiv.org/abs/2411.19930v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VLSBench: Unveiling Visual Leakage in Multimodal Safety", "abstract": "Safety concerns of Multimodal large language models (MLLMs) have gradually\nbecome an important problem in various applications. Surprisingly, previous\nworks indicate a counter-intuitive phenomenon that using textual unlearning to\nalign MLLMs achieves comparable safety performances with MLLMs trained with\nimage-text pairs. To explain such a counter-intuitive phenomenon, we discover a\nvisual safety information leakage (VSIL) problem in existing multimodal safety\nbenchmarks, i.e., the potentially risky and sensitive content in the image has\nbeen revealed in the textual query. In this way, MLLMs can easily refuse these\nsensitive text-image queries according to textual queries. However, image-text\npairs without VSIL are common in real-world scenarios and are overlooked by\nexisting multimodal safety benchmarks. To this end, we construct multimodal\nvisual leakless safety benchmark (VLSBench) preventing visual safety leakage\nfrom image to textual query with 2.4k image-text pairs. Experimental results\nindicate that VLSBench poses a significant challenge to both open-source and\nclose-source MLLMs, including LLaVA, Qwen2-VL, Llama3.2-Vision, and GPT-4o.\nThis study demonstrates that textual alignment is enough for multimodal safety\nscenarios with VSIL, while multimodal alignment is a more promising solution\nfor multimodal safety scenarios without VSIL. Please see our code and data at:\nhttps://hxhcreate.github.io/vlsbench.github.io/", "published": "2024-11-29 18:56:37", "link": "http://arxiv.org/abs/2411.19939v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.CR"}
{"title": "Perception Test 2024: Challenge Summary and a Novel Hour-Long VideoQA\n  Benchmark", "abstract": "Following the successful 2023 edition, we organised the Second Perception\nTest challenge as a half-day workshop alongside the IEEE/CVF European\nConference on Computer Vision (ECCV) 2024, with the goal of benchmarking\nstate-of-the-art video models and measuring the progress since last year using\nthe Perception Test benchmark. This year, the challenge had seven tracks (up\nfrom six last year) and covered low-level and high-level tasks, with language\nand non-language interfaces, across video, audio, and text modalities; the\nadditional track covered hour-long video understanding and introduced a novel\nvideo QA benchmark 1h-walk VQA. Overall, the tasks in the different tracks\nwere: object tracking, point tracking, temporal action localisation, temporal\nsound localisation, multiple-choice video question-answering, grounded video\nquestion-answering, and hour-long video question-answering. We summarise in\nthis report the challenge tasks and results, and introduce in detail the novel\nhour-long video QA benchmark 1h-walk VQA.", "published": "2024-11-29 18:57:25", "link": "http://arxiv.org/abs/2411.19941v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhances\n  LLM's Reasoning Capability", "abstract": "Mathematical reasoning tasks pose significant challenges for large language\nmodels (LLMs) because they require precise logical deduction and sequence\nanalysis. In this work, we introduce the concept of critical tokens -- elements\nwithin reasoning trajectories that significantly influence incorrect outcomes.\nWe present a novel framework for identifying these tokens through rollout\nsampling and demonstrate their substantial divergence from traditional error\ntokens. Through extensive experiments on datasets such as GSM8K and MATH500, we\nshow that identifying and replacing critical tokens significantly improves\nmodel accuracy. We propose an efficient methodology for pinpointing these\ntokens in large-scale datasets using contrastive estimation and extend this\nframework to enhance model training processes with direct preference\noptimization (DPO). Experimental results on GSM8K and MATH500 benchmarks with\nthe widely used models Llama-3 (8B and 70B) and Deepseek-math (7B) demonstrate\nthe effectiveness of the proposed approach, cDPO. Our results underscore the\npotential of leveraging critical tokens to reduce errors in reasoning tasks,\nadvancing the development of AI systems capable of robust logical deduction.\nOur code, annotated datasets, and trained models are available at\nhttps://github.com/chenzhiling9954/Critical-Tokens-Matter to support and\nencourage future research in this promising field.", "published": "2024-11-29 18:58:22", "link": "http://arxiv.org/abs/2411.19943v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sparrow: Data-Efficient Video-LLM with Text-to-Image Augmentation", "abstract": "Recent years have witnessed the success of Multimodal Large Language Models\n(MLLMs) in the vision understanding domain. The success of these models can\nlargely be attributed to the dominant scaling law, which states that larger\nparameter sizes and data volumes contribute to better performance. Notably,\ndata scaling has mainly been powered by automatic data pipelines, which center\naround the self-instruction of LLMs. The paradigm has been taken for granted\nfor quite some time, but the study of the effectiveness of scaling with these\ndata has been neglected for a long time. In this context, this work revisits\nscaling with synthetic data and focuses on developing video-LLMs from a\ndata-centric perspective. Our main study approach is fine-tuning pre-trained\nimage-LLMs with video data and investigating learning efficiency through data\nscaling. Results from our preliminary experiments reveal a low learning\nefficiency phenomenon when simply scaling up video data samples, which, through\nour probing, can be ascribed to a lack of instruction diversity. Aiming at this\nissue, we propose a data augmentation method called Sparrow, which synthesizes\nvideo-like samples from pure text instruction data. Mixing these synthetic\nsamples with the video data enables a more efficient training scheme. Through\ncomprehensive experiments, we demonstrate that our proposed method achieves\nperformance comparable to or even superior to baselines trained with many more\nsamples. Meanwhile, we find that incorporating these synthetic samples can\nboost the performance of long video understanding without training with long\nvideo data. The code and data examples are available at\nhttps://github.com/VITA-MLLM/Sparrow.", "published": "2024-11-29 18:59:54", "link": "http://arxiv.org/abs/2411.19951v4", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching", "abstract": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments.", "published": "2024-11-29 05:57:37", "link": "http://arxiv.org/abs/2412.03594v2", "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CogACT: A Foundational Vision-Language-Action Model for Synergizing\n  Cognition and Action in Robotic Manipulation", "abstract": "The advancement of large Vision-Language-Action (VLA) models has\nsignificantly improved robotic manipulation in terms of language-guided task\nexecution and generalization to unseen scenarios. While existing VLAs adapted\nfrom pretrained large Vision-Language-Models (VLM) have demonstrated promising\ngeneralizability, their task performance is still unsatisfactory as indicated\nby the low tasks success rates in different environments. In this paper, we\npresent a new advanced VLA architecture derived from VLM. Unlike previous works\nthat directly repurpose VLM for action prediction by simple action\nquantization, we propose a omponentized VLA architecture that has a specialized\naction module conditioned on VLM output. We systematically study the design of\nthe action module and demonstrates the strong performance enhancement with\ndiffusion action transformers for action sequence modeling, as well as their\nfavorable scaling behaviors. We also conduct comprehensive experiments and\nablation studies to evaluate the efficacy of our models with varied designs.\nThe evaluation on 5 robot embodiments in simulation and real work shows that\nour model not only significantly surpasses existing VLAs in task performance\nand but also exhibits remarkable adaptation to new robots and generalization to\nunseen objects and backgrounds. It exceeds the average success rates of OpenVLA\nwhich has similar model size (7B) with ours by over 35% in simulated evaluation\nand 55% in real robot experiments. It also outperforms the large RT-2-X model\n(55B) by 18% absolute success rates in simulation. Code and models can be found\non our project page (https://cogact.github.io/).", "published": "2024-11-29 12:06:03", "link": "http://arxiv.org/abs/2411.19650v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Classical and Quantum Algorithms for the Deterministic L-system\n  Inductive Inference Problem", "abstract": "L-systems can be made to model and create simulations of many biological\nprocesses, such as plant development. Finding an L-system for a given process\nis typically solved by hand, by experts, in a massively time-consuming process.\nIt would be significant if this could be done automatically from data, such as\nfrom sequences of images. In this paper, we are interested in inferring a\nparticular type of L-system, deterministic context-free L-system (D0L-system)\nfrom a sequence of strings. We introduce the characteristic graph of a sequence\nof strings, which we then utilize to translate our problem (inferring\nD0L-system) in polynomial time into the maximum independent set problem (MIS)\nand the SAT problem. After that, we offer a classical exact algorithm and an\napproximate quantum algorithm for the problem.", "published": "2024-11-29 18:11:39", "link": "http://arxiv.org/abs/2411.19906v2", "categories": ["quant-ph", "cs.CL", "cs.DS", "cs.FL", "cs.LG"], "primary_category": "quant-ph"}
{"title": "SSDM 2.0: Time-Accurate Speech Rich Transcription with Non-Fluencies", "abstract": "Speech is a hierarchical collection of text, prosody, emotions, dysfluencies,\netc. Automatic transcription of speech that goes beyond text (words) is an\nunderexplored problem. We focus on transcribing speech along with non-fluencies\n(dysfluencies). The current state-of-the-art pipeline SSDM suffers from complex\narchitecture design, training complexity, and significant shortcomings in the\nlocal sequence aligner, and it does not explore in-context learning capacity.\nIn this work, we propose SSDM 2.0, which tackles those shortcomings via four\nmain contributions: (1) We propose a novel \\textit{neural articulatory flow} to\nderive highly scalable speech representations. (2) We developed a\n\\textit{full-stack connectionist subsequence aligner} that captures all types\nof dysfluencies. (3) We introduced a mispronunciation prompt pipeline and\nconsistency learning module into LLM to leverage dysfluency \\textit{in-context\npronunciation learning} abilities. (4) We curated Libri-Dys and open-sourced\nthe current largest-scale co-dysfluency corpus, \\textit{Libri-Co-Dys}, for\nfuture research endeavors. In clinical experiments on pathological speech\ntranscription, we tested SSDM 2.0 using nfvPPA corpus primarily characterized\nby \\textit{articulatory dysfluencies}. Overall, SSDM 2.0 outperforms SSDM and\nall other dysfluency transcription models by a large margin. See our project\ndemo page at \\url{https://berkeley-speech-group.github.io/SSDM2.0/}.", "published": "2024-11-29 21:39:21", "link": "http://arxiv.org/abs/2412.00265v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "V2SFlow: Video-to-Speech Generation with Speech Decomposition and\n  Rectified Flow", "abstract": "In this paper, we introduce V2SFlow, a novel Video-to-Speech (V2S) framework\ndesigned to generate natural and intelligible speech directly from silent\ntalking face videos. While recent V2S systems have shown promising results on\nconstrained datasets with limited speakers and vocabularies, their performance\noften degrades on real-world, unconstrained datasets due to the inherent\nvariability and complexity of speech signals. To address these challenges, we\ndecompose the speech signal into manageable subspaces (content, pitch, and\nspeaker information), each representing distinct speech attributes, and predict\nthem directly from the visual input. To generate coherent and realistic speech\nfrom these predicted attributes, we employ a rectified flow matching decoder\nbuilt on a Transformer architecture, which models efficient probabilistic\npathways from random noise to the target speech distribution. Extensive\nexperiments demonstrate that V2SFlow significantly outperforms state-of-the-art\nmethods, even surpassing the naturalness of ground truth utterances.", "published": "2024-11-29 05:55:20", "link": "http://arxiv.org/abs/2411.19486v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Ditto: Motion-Space Diffusion for Controllable Realtime Talking Head\n  Synthesis", "abstract": "Recent advances in diffusion models have revolutionized audio-driven talking\nhead synthesis. Beyond precise lip synchronization, diffusion-based methods\nexcel in generating subtle expressions and natural head movements that are\nwell-aligned with the audio signal. However, these methods are confronted by\nslow inference speed, insufficient fine-grained control over facial motions,\nand occasional visual artifacts largely due to an implicit latent space derived\nfrom Variational Auto-Encoders (VAE), which prevent their adoption in realtime\ninteraction applications. To address these issues, we introduce Ditto, a\ndiffusion-based framework that enables controllable realtime talking head\nsynthesis. Our key innovation lies in bridging motion generation and\nphotorealistic neural rendering through an explicit identity-agnostic motion\nspace, replacing conventional VAE representations. This design substantially\nreduces the complexity of diffusion learning while enabling precise control\nover the synthesized talking heads. We further propose an inference strategy\nthat jointly optimizes three key components: audio feature extraction, motion\ngeneration, and video synthesis. This optimization enables streaming\nprocessing, realtime inference, and low first-frame delay, which are the\nfunctionalities crucial for interactive applications such as AI assistants.\nExtensive experimental results demonstrate that Ditto generates compelling\ntalking head videos and substantially outperforms existing methods in both\nmotion control and realtime performance.", "published": "2024-11-29 07:01:31", "link": "http://arxiv.org/abs/2411.19509v2", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Zero-shot Musical Stem Retrieval with Joint-Embedding Predictive\n  Architectures", "abstract": "In this paper, we tackle the task of musical stem retrieval. Given a musical\nmix, it consists in retrieving a stem that would fit with it, i.e., that would\nsound pleasant if played together. To do so, we introduce a new method based on\nJoint-Embedding Predictive Architectures, where an encoder and a predictor are\njointly trained to produce latent representations of a context and predict\nlatent representations of a target. In particular, we design our predictor to\nbe conditioned on arbitrary instruments, enabling our model to perform\nzero-shot stem retrieval. In addition, we discover that pretraining the encoder\nusing contrastive learning drastically improves the model's performance.\n  We validate the retrieval performances of our model using the MUSDB18 and\nMoisesDB datasets. We show that it significantly outperforms previous baselines\non both datasets, showcasing its ability to support more or less precise (and\npossibly unseen) conditioning. We also evaluate the learned embeddings on a\nbeat tracking task, demonstrating that they retain temporal structure and local\ninformation.", "published": "2024-11-29 16:11:47", "link": "http://arxiv.org/abs/2411.19806v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Parallel Stacked Aggregated Network for Voice Authentication in\n  IoT-Enabled Smart Devices", "abstract": "Voice authentication on IoT-enabled smart devices has gained prominence in\nrecent years due to increasing concerns over user privacy and security. The\ncurrent authentication systems are vulnerable to different voice-spoofing\nattacks (e.g., replay, voice cloning, and audio deepfakes) that mimic\nlegitimate voices to deceive authentication systems and enable fraudulent\nactivities (e.g., impersonation, unauthorized access, financial fraud, etc.).\nExisting solutions are often designed to tackle a single type of attack,\nleading to compromised performance against unseen attacks. On the other hand,\nexisting unified voice anti-spoofing solutions, not designed specifically for\nIoT, possess complex architectures and thus cannot be deployed on IoT-enabled\nsmart devices. Additionally, most of these unified solutions exhibit\nsignificant performance issues, including higher equal error rates or lower\naccuracy for specific attacks. To overcome these issues, we present the\nparallel stacked aggregation network (PSA-Net), a lightweight framework\ndesigned as an anti-spoofing defense system for voice-controlled smart IoT\ndevices. The PSA-Net processes raw audios directly and eliminates the need for\ndataset-dependent handcrafted features or pre-computed spectrograms.\nFurthermore, PSA-Net employs a split-transform-aggregate approach, which\ninvolves the segmentation of utterances, the extraction of intrinsic\ndifferentiable embeddings through convolutions, and the aggregation of them to\ndistinguish legitimate from spoofed audios. In contrast to existing deep\nResnet-oriented solutions, we incorporate cardinality as an additional\ndimension in our network, which enhances the PSA-Net ability to generalize\nacross diverse attacks. The results show that the PSA-Net achieves more\nconsistent performance for different attacks that exist in current\nanti-spoofing solutions.", "published": "2024-11-29 16:57:31", "link": "http://arxiv.org/abs/2411.19841v1", "categories": ["cs.SD", "cs.CR", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Musical composition and 2D cellular automata based on music intervals", "abstract": "This study is a theoretical approach for exploring the applicability of a 2D\ncellular automaton based on melodic and harmonic intervals in random arrays of\nmusical notes. The aim of this study was to explore alternatives uses for a\ncellular automaton in the musical context for better understanding the musical\ncreativity. We used the complex systems and humanities approaches as a\nframework for capturing the essence of creating music based on rules of music\ntheory. Findings suggested that such rules matter for generating large-scale\npatterns of organized notes. Therefore, our formulation provides a novel\napproach for understanding and replicating aspects of the musical creativity.", "published": "2024-11-29 17:03:41", "link": "http://arxiv.org/abs/2411.19844v1", "categories": ["cs.SD", "cs.CY", "eess.AS", "stat.AP"], "primary_category": "cs.SD"}
{"title": "Memristive Nanowire Network for Energy Efficient Audio Classification:\n  Pre-Processing-Free Reservoir Computing with Reduced Latency", "abstract": "Speech recognition is a key challenge in natural language processing,\nrequiring low latency, efficient computation, and strong generalization for\nreal-time applications. While software-based artificial neural networks (ANNs)\nexcel at this task, they are computationally intensive and depend heavily on\ndata pre-processing. Neuromorphic computing, with its low-latency and\nenergy-efficient advantages, holds promise for audio classification. Memristive\nnanowire networks, combined with pre-processing techniques like Mel-Frequency\nCepstrum Coefficient extraction, have been widely used for associative\nlearning, but such pre-processing can be power-intensive, undermining latency\nbenefits. This study pioneers the use of memristive and spatio-temporal\nproperties of nanowire networks for audio signal classification without\npre-processing. A nanowire network simulation is paired with three linear\nclassifiers for 10-class MNIST audio classification and binary speaker\ngeneralization tests. The hybrid system achieves significant benefits:\nexcellent data compression with only 3% of nanowire output utilized, a 10-fold\nreduction in computational latency, and up to 28.5% improved classification\naccuracy (using a logistic regression classifier). Precision and recall improve\nby 10% and 17% for multispeaker datasets, and by 24% and 17% for individual\nspeaker datasets, compared to raw data classifiers. This work provides a\nfoundational proof of concept for utilizing memristive nanowire networks (NWN)\nin edge-computing devices, showcasing their potential for efficient, real-time\naudio signal processing with reduced computational overhead and power\nconsumption, and enabling the development of advanced neuromorphic computing\nsolutions.", "published": "2024-11-29 10:58:51", "link": "http://arxiv.org/abs/2411.19611v1", "categories": ["cs.SD", "cond-mat.dis-nn", "eess.AS", "physics.app-ph", "stat.CO"], "primary_category": "cs.SD"}
{"title": "Scaling Transformers for Low-Bitrate High-Quality Speech Coding", "abstract": "The tokenization of speech with neural audio codec models is a vital part of\nmodern AI pipelines for the generation or understanding of speech, alone or in\na multimodal context. Traditionally such tokenization models have concentrated\non low parameter-count architectures using only components with strong\ninductive biases. In this work we show that by scaling a transformer\narchitecture with large parameter count to this problem, and applying a\nflexible Finite Scalar Quantization (FSQ) based bottleneck, it is possible to\nreach state-of-the-art speech quality at extremely low bit-rates of $400$ or\n$700$ bits-per-second. The trained models strongly out-perform existing\nbaselines in both objective and subjective tests.", "published": "2024-11-29 16:58:02", "link": "http://arxiv.org/abs/2411.19842v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Circumventing shortcuts in audio-visual deepfake detection datasets with\n  unsupervised learning", "abstract": "Good datasets are essential for developing and benchmarking any machine\nlearning system. Their importance is even more extreme for safety critical\napplications such as deepfake detection - the focus of this paper. Here we\nreveal that two of the most widely used audio-video deepfake datasets suffer\nfrom a previously unidentified spurious feature: the leading silence. Fake\nvideos start with a very brief moment of silence and based on this feature\nalone, we can separate the real and fake samples almost perfectly. As such,\nprevious audio-only and audio-video models exploit the presence of silence in\nthe fake videos and consequently perform worse when the leading silence is\nremoved. To circumvent latching on such unwanted artifact and possibly other\nunrevealed ones we propose a shift from supervised to unsupervised learning by\ntraining models exclusively on real data. We show that by aligning\nself-supervised audio-video representations we remove the risk of relying on\ndataset-specific biases and improve robustness in deepfake detection.", "published": "2024-11-29 18:58:20", "link": "http://arxiv.org/abs/2412.00175v2", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Deepfake Media Generation and Detection in the Generative AI Era: A\n  Survey and Outlook", "abstract": "With the recent advancements in generative modeling, the realism of deepfake\ncontent has been increasing at a steady pace, even reaching the point where\npeople often fail to detect manipulated media content online, thus being\ndeceived into various kinds of scams. In this paper, we survey deepfake\ngeneration and detection techniques, including the most recent developments in\nthe field, such as diffusion models and Neural Radiance Fields. Our literature\nreview covers all deepfake media types, comprising image, video, audio and\nmultimodal (audio-visual) content. We identify various kinds of deepfakes,\naccording to the procedure used to alter or generate the fake content. We\nfurther construct a taxonomy of deepfake generation and detection methods,\nillustrating the important groups of methods and the domains where these\nmethods are applied. Next, we gather datasets used for deepfake detection and\nprovide updated rankings of the best performing deepfake detectors on the most\npopular datasets. In addition, we develop a novel multimodal benchmark to\nevaluate deepfake detectors on out-of-distribution content. The results\nindicate that state-of-the-art detectors fail to generalize to deepfake content\ngenerated by unseen deepfake generators. Finally, we propose future directions\nto obtain robust and powerful deepfake detectors. Our project page and new\nbenchmark are available at https://github.com/CroitoruAlin/biodeep.", "published": "2024-11-29 08:29:25", "link": "http://arxiv.org/abs/2411.19537v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
