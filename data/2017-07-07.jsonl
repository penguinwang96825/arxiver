{"title": "A Nested Attention Neural Hybrid Model for Grammatical Error Correction", "abstract": "Grammatical error correction (GEC) systems strive to correct both global\nerrors in word order and usage, and local errors in spelling and inflection.\nFurther developing upon recent work on neural machine translation, we propose a\nnew hybrid neural model with nested attention layers for GEC. Experiments show\nthat the new model can effectively correct errors of both types by\nincorporating word and character-level information,and that the model\nsignificantly outperforms previous neural models for GEC as measured on the\nstandard CoNLL-14 benchmark dataset. Further analysis also shows that the\nsuperiority of the proposed model can be largely attributed to the use of the\nnested attention mechanism, which has proven particularly effective in\ncorrecting local errors that involve small edits in orthography.", "published": "2017-07-07 03:10:32", "link": "http://arxiv.org/abs/1707.02026v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "External Evaluation of Event Extraction Classifiers for Automatic\n  Pathway Curation: An extended study of the mTOR pathway", "abstract": "This paper evaluates the impact of various event extraction systems on\nautomatic pathway curation using the popular mTOR pathway. We quantify the\nimpact of training data sets as well as different machine learning classifiers\nand show that some improve the quality of automatically extracted pathways.", "published": "2017-07-07 07:46:54", "link": "http://arxiv.org/abs/1707.02063v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Computational Models of Tutor Feedback in Language Acquisition", "abstract": "This paper investigates the role of tutor feedback in language learning using\ncomputational models. We compare two dominant paradigms in language learning:\ninteractive learning and cross-situational learning - which differ primarily in\nthe role of social feedback such as gaze or pointing. We analyze the\nrelationship between these two paradigms and propose a new mixed paradigm that\ncombines the two paradigms and allows to test algorithms in experiments that\ncombine no feedback and social feedback. To deal with mixed feedback\nexperiments, we develop new algorithms and show how they perform with respect\nto traditional knn and prototype approaches.", "published": "2017-07-07 15:34:08", "link": "http://arxiv.org/abs/1707.02230v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Summarization Techniques: A Brief Survey", "abstract": "In recent years, there has been a explosion in the amount of text data from a\nvariety of sources. This volume of text is an invaluable source of information\nand knowledge which needs to be effectively summarized to be useful. In this\nreview, the main approaches to automatic text summarization are described. We\nreview the different processes for summarization and describe the effectiveness\nand shortcomings of the different methods.", "published": "2017-07-07 16:55:56", "link": "http://arxiv.org/abs/1707.02268v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A parallel corpus of Python functions and documentation strings for\n  automated code documentation and code generation", "abstract": "Automated documentation of programming source code and automated code\ngeneration from natural language are challenging tasks of both practical and\nscientific interest. Progress in these areas has been limited by the low\navailability of parallel corpora of code and natural language descriptions,\nwhich tend to be small and constrained to specific domains.\n  In this work we introduce a large and diverse parallel corpus of a hundred\nthousands Python functions with their documentation strings (\"docstrings\")\ngenerated by scraping open source repositories on GitHub. We describe baseline\nresults for the code documentation and code generation tasks obtained by neural\nmachine translation. We also experiment with data augmentation techniques to\nfurther increase the amount of training data.\n  We release our datasets and processing scripts in order to stimulate research\nin these areas.", "published": "2017-07-07 17:15:27", "link": "http://arxiv.org/abs/1707.02275v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Zero-Shot Frame Semantic Parsing for Domain Scaling", "abstract": "State-of-the-art slot filling models for goal-oriented human/machine\nconversational language understanding systems rely on deep learning methods.\nWhile multi-task training of such models alleviates the need for large\nin-domain annotated datasets, bootstrapping a semantic parsing model for a new\ndomain using only the semantic frame, such as the back-end API or knowledge\ngraph schema, is still one of the holy grail tasks of language understanding\nfor dialogue systems. This paper proposes a deep learning based approach that\ncan utilize only the slot description in context without the need for any\nlabeled or unlabeled in-domain examples, to quickly bootstrap a new domain. The\nmain idea of this paper is to leverage the encoding of the slot names and\ndescriptions within a multi-task deep learned slot filling model, to implicitly\nalign slots across domains. The proposed approach is promising for solving the\ndomain scaling problem and eliminating the need for any manually annotated data\nor explicit schema alignment. Furthermore, our experiments on multiple domains\nshow that this approach results in significantly better slot-filling\nperformance when compared to using only in-domain data, especially in the low\ndata regime.", "published": "2017-07-07 21:21:33", "link": "http://arxiv.org/abs/1707.02363v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
