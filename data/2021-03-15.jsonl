{"title": "Generating CCG Categories", "abstract": "Previous CCG supertaggers usually predict categories using multi-class\nclassification. Despite their simplicity, internal structures of categories are\nusually ignored. The rich semantics inside these structures may help us to\nbetter handle relations among categories and bring more robustness into\nexisting supertaggers. In this work, we propose to generate categories rather\nthan classify them: each category is decomposed into a sequence of smaller\natomic tags, and the tagger aims to generate the correct sequence. We show that\nwith this finer view on categories, annotations of different categories could\nbe shared and interactions with sentence contexts could be enhanced. The\nproposed category generator is able to achieve state-of-the-art tagging (95.5%\naccuracy) and parsing (89.8% labeled F1) performances on the standard CCGBank.\nFurthermore, its performances on infrequent (even unseen) categories,\nout-of-domain texts and low resource language give promising results on\nintroducing generation models to the general CCG analyses.", "published": "2021-03-15 05:01:48", "link": "http://arxiv.org/abs/2103.08139v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards the evaluation of automatic simultaneous speech translation from\n  a communicative perspective", "abstract": "In recent years, automatic speech-to-speech and speech-to-text translation\nhas gained momentum thanks to advances in artificial intelligence, especially\nin the domains of speech recognition and machine translation. The quality of\nsuch applications is commonly tested with automatic metrics, such as BLEU,\nprimarily with the goal of assessing improvements of releases or in the context\nof evaluation campaigns. However, little is known about how the output of such\nsystems is perceived by end users or how they compare to human performances in\nsimilar communicative tasks.\n  In this paper, we present the results of an experiment aimed at evaluating\nthe quality of a real-time speech translation engine by comparing it to the\nperformance of professional simultaneous interpreters. To do so, we adopt a\nframework developed for the assessment of human interpreters and use it to\nperform a manual evaluation on both human and machine performances. In our\nsample, we found better performance for the human interpreters in terms of\nintelligibility, while the machine performs slightly better in terms of\ninformativeness. The limitations of the study and the possible enhancements of\nthe chosen framework are discussed. Despite its intrinsic limitations, the use\nof this framework represents a first step towards a user-centric and\ncommunication-oriented methodology for evaluating real-time automatic speech\ntranslation.", "published": "2021-03-15 13:09:00", "link": "http://arxiv.org/abs/2103.08364v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-view Subword Regularization", "abstract": "Multilingual pretrained representations generally rely on subword\nsegmentation algorithms to create a shared multilingual vocabulary. However,\nstandard heuristic algorithms often lead to sub-optimal segmentation,\nespecially for languages with limited amounts of data. In this paper, we take\ntwo major steps towards alleviating this problem. First, we demonstrate\nempirically that applying existing subword regularization methods(Kudo, 2018;\nProvilkov et al., 2020) during fine-tuning of pre-trained multilingual\nrepresentations improves the effectiveness of cross-lingual transfer. Second,\nto take full advantage of different possible input segmentations, we propose\nMulti-view Subword Regularization (MVR), a method that enforces the consistency\nbetween predictions of using inputs tokenized by the standard and probabilistic\nsegmentations. Results on the XTREME multilingual benchmark(Hu et al., 2020)\nshow that MVR brings consistent improvements of up to 2.5 points over using\nstandard segmentation algorithms.", "published": "2021-03-15 16:07:42", "link": "http://arxiv.org/abs/2103.08490v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Effect of Domain and Diacritics in Yor\u00f9b\u00e1-English Neural Machine\n  Translation", "abstract": "Massively multilingual machine translation (MT) has shown impressive\ncapabilities, including zero and few-shot translation between low-resource\nlanguage pairs. However, these models are often evaluated on high-resource\nlanguages with the assumption that they generalize to low-resource ones. The\ndifficulty of evaluating MT models on low-resource pairs is often due to lack\nof standardized evaluation datasets. In this paper, we present MENYO-20k, the\nfirst multi-domain parallel corpus with a special focus on clean orthography\nfor Yor\\`ub\\'a--English with standardized train-test splits for benchmarking.\nWe provide several neural MT benchmarks and compare them to the performance of\npopular pre-trained (massively multilingual) MT models both for the\nheterogeneous test set and its subdomains. Since these pre-trained models use\nhuge amounts of data with uncertain quality, we also analyze the effect of\ndiacritics, a major characteristic of Yor\\`ub\\'a, in the training data. We\ninvestigate how and when this training condition affects the final quality and\nintelligibility of a translation. Our models outperform massively multilingual\nmodels such as Google ($+8.7$ BLEU) and Facebook M2M ($+9.1$ BLEU) when\ntranslating to Yor\\`ub\\'a, setting a high quality benchmark for future\nresearch.", "published": "2021-03-15 18:52:32", "link": "http://arxiv.org/abs/2103.08647v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Transition-based Parser for Unscoped Episodic Logical Forms", "abstract": "\"Episodic Logic:Unscoped Logical Form\" (EL-ULF) is a semantic representation\ncapturing predicate-argument structure as well as more challenging aspects of\nlanguage within the Episodic Logic formalism. We present the first learned\napproach for parsing sentences into ULFs, using a growing set of annotated\nexamples. The results provide a strong baseline for future improvement. Our\nmethod learns a sequence-to-sequence model for predicting the transition action\nsequence within a modified cache transition system. We evaluate the efficacy of\ntype grammar-based constraints, a word-to-symbol lexicon, and transition system\nstate features in this task. Our system is available at\nhttps://github.com/genelkim/ulf-transition-parser We also present the first\nofficial annotated ULF dataset at\nhttps://www.cs.rochester.edu/u/gkim21/ulf/resources/.", "published": "2021-03-15 23:09:32", "link": "http://arxiv.org/abs/2103.08759v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Double Articulation Analyzer with Prosody for Unsupervised Word and\n  Phoneme Discovery", "abstract": "Infants acquire words and phonemes from unsegmented speech signals using\nsegmentation cues, such as distributional, prosodic, and co-occurrence cues.\nMany pre-existing computational models that represent the process tend to focus\non distributional or prosodic cues. This paper proposes a nonparametric\nBayesian probabilistic generative model called the prosodic hierarchical\nDirichlet process-hidden language model (Prosodic HDP-HLM). Prosodic HDP-HLM,\nan extension of HDP-HLM, considers both prosodic and distributional cues within\na single integrative generative model. We conducted three experiments on\ndifferent types of datasets, and demonstrate the validity of the proposed\nmethod. The results show that the Prosodic DAA successfully uses prosodic cues\nand outperforms a method that solely uses distributional cues. The main\ncontributions of this study are as follows: 1) We develop a probabilistic\ngenerative model for time series data including prosody that potentially has a\ndouble articulation structure; 2) We propose the Prosodic DAA by deriving the\ninference procedure for Prosodic HDP-HLM and show that Prosodic DAA can\ndiscover words directly from continuous human speech signals using statistical\ninformation and prosodic information in an unsupervised manner; 3) We show that\nprosodic cues contribute to word segmentation more in naturally distributed\ncase words, i.e., they follow Zipf's law.", "published": "2021-03-15 08:17:44", "link": "http://arxiv.org/abs/2103.08199v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mention-centered Graph Neural Network for Document-level Relation\n  Extraction", "abstract": "Document-level relation extraction aims to discover relations between\nentities across a whole document. How to build the dependency of entities from\ndifferent sentences in a document remains to be a great challenge. Current\napproaches either leverage syntactic trees to construct document-level graphs\nor aggregate inference information from different sentences. In this paper, we\nbuild cross-sentence dependencies by inferring compositional relations between\ninter-sentence mentions. Adopting aggressive linking strategy, intermediate\nrelations are reasoned on the document-level graphs by mention convolution. We\nfurther notice the generalization problem of NA instances, which is caused by\nincomplete annotation and worsened by fully-connected mention pairs. An\nimproved ranking loss is proposed to attend this problem. Experiments show the\nconnections between different mentions are crucial to document-level relation\nextraction, which enables the model to extract more meaningful higher-level\ncompositional relations.", "published": "2021-03-15 08:19:44", "link": "http://arxiv.org/abs/2103.08200v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sent2Matrix: Folding Character Sequences in Serpentine Manifolds for\n  Two-Dimensional Sentence", "abstract": "We study text representation methods using deep models. Current methods, such\nas word-level embedding and character-level embedding schemes, treat texts as\neither a sequence of atomic words or a sequence of characters. These methods\neither ignore word morphologies or word boundaries. To overcome these\nlimitations, we propose to convert texts into 2-D representations and develop\nthe Sent2Matrix method. Our method allows for the explicit incorporation of\nboth word morphologies and boundaries. When coupled with a novel serpentine\npadding method, our Sent2Matrix method leads to an interesting visualization in\nwhich 1-D character sequences are folded into 2-D serpentine manifolds.\nNotably, our method is the first attempt to represent texts in 2-D formats.\nExperimental results on text classification tasks shown that our method\nconsistently outperforms prior embedding methods.", "published": "2021-03-15 13:52:47", "link": "http://arxiv.org/abs/2103.08387v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Study of Automatic Metrics for the Evaluation of Natural Language\n  Explanations", "abstract": "As transparency becomes key for robotics and AI, it will be necessary to\nevaluate the methods through which transparency is provided, including\nautomatically generated natural language (NL) explanations. Here, we explore\nparallels between the generation of such explanations and the much-studied\nfield of evaluation of Natural Language Generation (NLG). Specifically, we\ninvestigate which of the NLG evaluation measures map well to explanations. We\npresent the ExBAN corpus: a crowd-sourced corpus of NL explanations for\nBayesian Networks. We run correlations comparing human subjective ratings with\nNLG automatic measures. We find that embedding-based automatic NLG evaluation\nmethods, such as BERTScore and BLEURT, have a higher correlation with human\nratings, compared to word-overlap metrics, such as BLEU and ROUGE. This work\nhas implications for Explainable AI and transparent robotic and autonomous\nsystems.", "published": "2021-03-15 17:10:39", "link": "http://arxiv.org/abs/2103.08545v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Discriminative Learning for Probabilistic Context-Free Grammars based on\n  Generalized H-Criterion", "abstract": "We present a formal framework for the development of a family of\ndiscriminative learning algorithms for Probabilistic Context-Free Grammars\n(PCFGs) based on a generalization of criterion-H. First of all, we propose the\nH-criterion as the objective function and the Growth Transformations as the\noptimization method, which allows us to develop the final expressions for the\nestimation of the parameters of the PCFGs. And second, we generalize the\nH-criterion to take into account the set of reference interpretations and the\nset of competing interpretations, and we propose a new family of objective\nfunctions that allow us to develop the expressions of the estimation\ntransformations for PCFGs.", "published": "2021-03-15 19:07:17", "link": "http://arxiv.org/abs/2103.08656v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "XLST: Cross-lingual Self-training to Learn Multilingual Representation\n  for Low Resource Speech Recognition", "abstract": "In this paper, we propose a weakly supervised multilingual representation\nlearning framework, called cross-lingual self-training (XLST). XLST is able to\nutilize a small amount of annotated data from high-resource languages to\nimprove the representation learning on multilingual un-annotated data.\nSpecifically, XLST uses a supervised trained model to produce initial\nrepresentations and another model to learn from them, by maximizing the\nsimilarity between output embeddings of these two models. Furthermore, the\nmoving average mechanism and multi-view data augmentation are employed, which\nare experimentally shown to be crucial to XLST. Comprehensive experiments have\nbeen conducted on the CommonVoice corpus to evaluate the effectiveness of XLST.\nResults on 5 downstream low-resource ASR tasks shows that our multilingual\npretrained model achieves relatively 18.6% PER reduction over the\nstate-of-the-art self-supervised method, with leveraging additional 100 hours\nof annotated English data.", "published": "2021-03-15 08:33:50", "link": "http://arxiv.org/abs/2103.08207v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence", "abstract": "Typical fact verification models use retrieved written evidence to verify\nclaims. Evidence sources, however, often change over time as more information\nis gathered and revised. In order to adapt, models must be sensitive to subtle\ndifferences in supporting evidence. We present VitaminC, a benchmark infused\nwith challenging cases that require fact verification models to discern and\nadjust to slight factual changes. We collect over 100,000 Wikipedia revisions\nthat modify an underlying fact, and leverage these revisions, together with\nadditional synthetically constructed ones, to create a total of over 400,000\nclaim-evidence pairs. Unlike previous resources, the examples in VitaminC are\ncontrastive, i.e., they contain evidence pairs that are nearly identical in\nlanguage and content, with the exception that one supports a given claim while\nthe other does not. We show that training using this design increases\nrobustness -- improving accuracy by 10% on adversarial fact verification and 6%\non adversarial natural language inference (NLI). Moreover, the structure of\nVitaminC leads us to define additional tasks for fact-checking resources:\ntagging relevant words in the evidence for verifying the claim, identifying\nfactual revisions, and providing automatic edits via factually consistent text\ngeneration.", "published": "2021-03-15 17:05:13", "link": "http://arxiv.org/abs/2103.08541v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Discriminator Sobolev Defense-GAN Against Adversarial Attacks for\n  End-to-End Speech Systems", "abstract": "This paper introduces a defense approach against end-to-end adversarial\nattacks developed for cutting-edge speech-to-text systems. The proposed defense\nalgorithm has four major steps. First, we represent speech signals with 2D\nspectrograms using the short-time Fourier transform. Second, we iteratively\nfind a safe vector using a spectrogram subspace projection operation. This\noperation minimizes the chordal distance adjustment between spectrograms with\nan additional regularization term. Third, we synthesize a spectrogram with such\na safe vector using a novel GAN architecture trained with Sobolev integral\nprobability metric. To improve the model's performance in terms of stability\nand the total number of learned modes, we impose an additional constraint on\nthe generator network. Finally, we reconstruct the signal from the synthesized\nspectrogram and the Griffin-Lim phase approximation technique. We evaluate the\nproposed defense approach against six strong white and black-box adversarial\nattacks benchmarked on DeepSpeech, Kaldi, and Lingvo models. Our experimental\nresults show that our algorithm outperforms other state-of-the-art defense\nalgorithms both in terms of accuracy and signal quality.", "published": "2021-03-15 01:11:13", "link": "http://arxiv.org/abs/2103.08086v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Robust Speech-to-Text Adversarial Attack", "abstract": "This paper introduces a novel adversarial algorithm for attacking the\nstate-of-the-art speech-to-text systems, namely DeepSpeech, Kaldi, and Lingvo.\nOur approach is based on developing an extension for the conventional\ndistortion condition of the adversarial optimization formulation using the\nCram\\`er integral probability metric. Minimizing over this metric, which\nmeasures the discrepancies between original and adversarial samples'\ndistributions, contributes to crafting signals very close to the subspace of\nlegitimate speech recordings. This helps to yield more robust adversarial\nsignals against playback over-the-air without employing neither costly\nexpectation over transformation operations nor static room impulse response\nsimulations. Our approach outperforms other targeted and non-targeted\nalgorithms in terms of word error rate and sentence-level-accuracy with\ncompetitive performance on the crafted adversarial signals' quality. Compared\nto seven other strong white and black-box adversarial attacks, our proposed\napproach is considerably more resilient against multiple consecutive playbacks\nover-the-air, corroborating its higher robustness in noisy environments.", "published": "2021-03-15 01:51:41", "link": "http://arxiv.org/abs/2103.08095v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Computational timbre and tonal system similarity analysis of the music\n  of Northern Myanmar-based Kachin compared to Xinjiang-based Uyghur ethnic\n  groups", "abstract": "The music of Northern Myanmar Kachin ethnic group is compared to the music of\nwestern China, Xijiang based Uyghur music, using timbre and pitch feature\nextraction and machine learning. Although separated by Tibet, the muqam\ntradition of Xinjiang might be found in Kachin music due to myths of Kachin\norigin, as well as linguistic similarities, e.g., the Kachin term 'makan' for a\nmusical piece. Extractions were performed using the apollon and COMSAR\n(Computational Music and Sound Archiving) frameworks, on which the Ethnographic\nSound Recordings Archive (ESRA) is based, using ethnographic recordings from\nESRA next to additional pieces. In terms of pitch, tonal systems were compared\nusing Kohonen self-organizing map (SOM), which clearly clusters Kachin and\nUyghur musical pieces. This is mainly caused by the Xinjiang muqam music\nshowing just fifth and fourth, while Kachin pieces tend to have a higher fifth\nand fourth, next to other dissimilarities. Also, the timbre features of\nspectral centroid and spectral sharpness standard deviation clearly tells\nUyghur from Kachin pieces, where Uyghur music shows much larger deviations.\nAlthough more features will be compared in the future, like rhythm or melody,\nthese already strong findings might introduce an alternative comparison\nmethodology of ethnic groups beyond traditional linguistic definitions.", "published": "2021-03-15 08:23:14", "link": "http://arxiv.org/abs/2103.08203v1", "categories": ["cs.SD", "eess.AS", "q-bio.NC"], "primary_category": "cs.SD"}
{"title": "Lightweight and interpretable neural modeling of an audio distortion\n  effect using hyperconditioned differentiable biquads", "abstract": "In this work, we propose using differentiable cascaded biquads to model an\naudio distortion effect. We extend trainable infinite impulse response (IIR)\nfilters to the hyperconditioned case, in which a transformation is learned to\ndirectly map external parameters of the distortion effect to its internal\nfilter and gain parameters, along with activations necessary to ensure filter\nstability. We propose a novel, efficient training scheme of IIR filters by\nmeans of a Fourier transform. Our models have significantly fewer parameters\nand reduced complexity relative to more traditional black-box neural audio\neffect modeling methodologies using finite impulse response filters. Our\nsmallest, best-performing model adequately models a BOSS MT-2 pedal at 44.1\nkHz, using a total of 40 biquads and only 210 parameters. Its model parameters\nare interpretable, can be related back to the original analog audio circuit,\nand can even be intuitively altered by machine learning non-specialists after\nmodel training. Quantitative and qualitative results illustrate the\neffectiveness of the proposed method.", "published": "2021-03-15 20:46:31", "link": "http://arxiv.org/abs/2103.08709v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
