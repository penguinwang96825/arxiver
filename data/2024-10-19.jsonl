{"title": "Hierarchical Reinforced Trader (HRT): A Bi-Level Approach for Optimizing Stock Selection and Execution", "abstract": "Leveraging Deep Reinforcement Learning (DRL) in automated stock trading has\nshown promising results, yet its application faces significant challenges,\nincluding the curse of dimensionality, inertia in trading actions, and\ninsufficient portfolio diversification. Addressing these challenges, we\nintroduce the Hierarchical Reinforced Trader (HRT), a novel trading strategy\nemploying a bi-level Hierarchical Reinforcement Learning framework. The HRT\nintegrates a Proximal Policy Optimization (PPO)-based High-Level Controller\n(HLC) for strategic stock selection with a Deep Deterministic Policy Gradient\n(DDPG)-based Low-Level Controller (LLC) tasked with optimizing trade executions\nto enhance portfolio value. In our empirical analysis, comparing the HRT agent\nwith standalone DRL models and the S&P 500 benchmark during both bullish and\nbearish market conditions, we achieve a positive and higher Sharpe ratio. This\nadvancement not only underscores the efficacy of incorporating hierarchical\nstructures into DRL strategies but also mitigates the aforementioned\nchallenges, paving the way for designing more profitable and robust trading\nalgorithms in complex markets.", "published": "2024-10-19 01:29:38", "link": "http://arxiv.org/abs/2410.14927v1", "categories": ["q-fin.TR", "cs.CE", "cs.LG"], "primary_category": "q-fin.TR"}
{"title": "ChronoFact: Timeline-based Temporal Fact Verification", "abstract": "Automated fact verification plays an essential role in fostering trust in the\ndigital space. Despite the growing interest, the verification of temporal facts\nhas not received much attention in the community. Temporal fact verification\nbrings new challenges where cues of the temporal information need to be\nextracted and temporal reasoning involving various temporal aspects of the text\nmust be applied. In this work, we propose an end-to-end solution for temporal\nfact verification that considers the temporal information in claims to obtain\nrelevant evidence sentences and harness the power of large language model for\ntemporal reasoning. Recognizing that temporal facts often involve events, we\nmodel these events in the claim and evidence sentences. We curate two temporal\nfact datasets to learn time-sensitive representations that encapsulate not only\nthe semantic relationships among the events, but also their chronological\nproximity. This allows us to retrieve the top-k relevant evidence sentences and\nprovide the context for a large language model to perform temporal reasoning\nand outputs whether a claim is supported or refuted by the retrieved evidence\nsentences. Experiment results demonstrate that the proposed approach\nsignificantly enhances the accuracy of temporal claim verification, thereby\nadvancing current state-of-the-art in automated fact verification.", "published": "2024-10-19 03:44:19", "link": "http://arxiv.org/abs/2410.14964v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Subversive Characters and Stereotyping Readers: Characterizing Queer\n  Relationalities with Dialogue-Based Relation Extraction", "abstract": "Television is often seen as a site for subcultural identification and\nsubversive fantasy, including in queer cultures. How might we measure\nsubversion, or the degree to which the depiction of social relationship between\na dyad (e.g. two characters who are colleagues) deviates from its typical\nrepresentation on TV? To explore this question, we introduce the task of\nstereotypic relationship extraction. Built on cognitive stylistics, linguistic\nanthropology, and dialogue relation extraction, in this paper, we attempt to\nmodel the cognitive process of stereotyping TV characters in dialogic\ninteractions. Given a dyad, we want to predict: what social relationship do the\nspeakers exhibit through their words? Subversion is then characterized by the\ndiscrepancy between the distribution of the model's predictions and the ground\ntruth labels. To demonstrate the usefulness of this task and gesture at a\nmethodological intervention, we enclose four case studies to characterize the\nrepresentation of queer relationalities in the Big Bang Theory, Frasier, and\nGilmore Girls, as we explore the suspicious and reparative modes of reading\nwith our computational methods.", "published": "2024-10-19 05:01:56", "link": "http://arxiv.org/abs/2410.14978v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CAP: Data Contamination Detection via Consistency Amplification", "abstract": "Large language models (LLMs) are widely used, but concerns about data\ncontamination challenge the reliability of LLM evaluations. Existing\ncontamination detection methods are often task-specific or require extra\nprerequisites, limiting practicality. We propose a novel framework, Consistency\nAmplification-based Data Contamination Detection (CAP), which introduces the\nPerformance Consistency Ratio (PCR) to measure dataset leakage by leveraging LM\nconsistency. To the best of our knowledge, this is the first method to\nexplicitly differentiate between fine-tuning and contamination, which is\ncrucial for detecting contamination in domain-specific models. Additionally,\nCAP is applicable to various benchmarks and works for both white-box and\nblack-box models. We validate CAP's effectiveness through experiments on seven\nLLMs and four domain-specific benchmarks. Our findings also show that composite\nbenchmarks from various dataset sources are particularly prone to unintentional\ncontamination. Codes will be publicly available soon.", "published": "2024-10-19 06:33:33", "link": "http://arxiv.org/abs/2410.15005v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Ontology Expansion for Conversational Understanding", "abstract": "In the rapidly evolving field of conversational AI, Ontology Expansion\n(OnExp) is crucial for enhancing the adaptability and robustness of\nconversational agents. Traditional models rely on static, predefined\nontologies, limiting their ability to handle new and unforeseen user needs.\nThis survey paper provides a comprehensive review of the state-of-the-art\ntechniques in OnExp for conversational understanding. It categorizes the\nexisting literature into three main areas: (1) New Intent Discovery, (2) New\nSlot-Value Discovery, and (3) Joint OnExp. By examining the methodologies,\nbenchmarks, and challenges associated with these areas, we highlight several\nemerging frontiers in OnExp to improve agent performance in real-world\nscenarios and discuss their corresponding challenges. This survey aspires to be\na foundational reference for researchers and practitioners, promoting further\nexploration and innovation in this crucial domain.", "published": "2024-10-19 07:27:30", "link": "http://arxiv.org/abs/2410.15019v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Theoretical Aspects of Bias and Diversity in Minimum Bayes Risk Decoding", "abstract": "Text generation commonly relies on greedy and beam decoding that limit the\nsearch space and degrade output quality. Minimum Bayes Risk (MBR) decoding can\nmitigate this problem by utilizing automatic evaluation metrics and\nmodel-generated pseudo-references. Previous studies have conducted empirical\nanalyses to reveal the improvement by MBR decoding, and reported various\nobservations. However, despite these observations, the theoretical relationship\nbetween them remains uncertain. To address this, we present a novel theoretical\ninterpretation of MBR decoding from the perspective of bias-diversity\ndecomposition. We decompose errors in the estimated quality of generated\nhypotheses in MBR decoding into two key factors: bias, which reflects the\ncloseness between utility functions and human evaluations, and diversity, which\nrepresents the variation in the estimated quality of utility functions. Our\ntheoretical analysis reveals the difficulty in simultaneously improving both\nbias and diversity, and highlights the effectiveness of increasing diversity to\nenhance MBR decoding performance. This analysis verifies the alignment between\nour theoretical insights and the empirical results reported in previous work.\nFurthermore, to support our theoretical findings, we propose a new metric,\npseudo-bias, which approximates the bias term using gold references. We also\nintroduce a new MBR approach, Metric-augmented MBR (MAMBR), which increases\ndiversity by adjusting the behavior of utility functions without altering the\npseudo-references. Experimental results across multiple NLP tasks show that the\ndecomposed terms in the bias-diversity decomposition correlate well with\nperformance, and that MAMBR improves text generation quality by modifying\nutility function behavior. Our code will be available at\nhttps://github.com/naist-nlp/mbr-bias-diversity.", "published": "2024-10-19 07:32:10", "link": "http://arxiv.org/abs/2410.15021v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving General Text Embedding Model: Tackling Task Conflict and Data\n  Imbalance through Model Merging", "abstract": "Text embeddings are vital for tasks such as text retrieval and semantic\ntextual similarity (STS). Recently, the advent of pretrained language models,\nalong with unified benchmarks like the Massive Text Embedding Benchmark (MTEB),\nhas facilitated the development of versatile general-purpose text embedding\nmodels. Advanced embedding models are typically developed using large-scale\nmulti-task data and joint training across multiple tasks. However, our\nexperimental analysis reveals two significant drawbacks of joint training: 1)\nTask Conflict: Gradients from different tasks interfere with each other,\nleading to negative transfer. 2) Data Imbalance: Disproportionate data\ndistribution introduces biases that negatively impact performance across tasks.\nTo overcome these challenges, we explore model merging-a technique that\ncombines independently trained models to mitigate gradient conflicts and\nbalance data distribution. We introduce a novel method, Self Positioning, which\nefficiently searches for optimal model combinations within the interpolation\nspace of task vectors using stochastic gradient descent. Our experiments\ndemonstrate that Self Positioning significantly enhances multi-task performance\non the MTEB dataset, achieving an absolute improvement of 0.7 points. It\noutperforms traditional resampling methods while reducing computational costs.\nThis work offers a robust approach to building generalized text embedding\nmodels with superior performance across diverse embedding-related tasks.", "published": "2024-10-19 08:39:21", "link": "http://arxiv.org/abs/2410.15035v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "mHumanEval -- A Multilingual Benchmark to Evaluate Large Language Models\n  for Code Generation", "abstract": "Recent advancements in large language models (LLMs) have significantly\nenhanced code generation from natural language prompts. The HumanEval\nBenchmark, developed by OpenAI, remains the most widely used code generation\nbenchmark. However, this and other Code LLM benchmarks face critical\nlimitations, particularly in task diversity, test coverage, and linguistic\nscope. Current evaluations primarily focus on English-to-Python conversion\ntasks with limited test cases, potentially overestimating model performance.\nWhile recent works have addressed test coverage and programming language (PL)\ndiversity, code generation from low-resource language prompts remains largely\nunexplored. To address this gap, we introduce mHumanEval, an extended benchmark\nsupporting prompts in over 200 natural languages. We employ established machine\ntranslation methods to compile the benchmark, coupled with a quality assurance\nprocess. Furthermore, we provide expert human translations for 15 diverse\nnatural languages (NLs). We conclude by analyzing the multilingual code\ngeneration capabilities of state-of-the-art (SOTA) Code LLMs, offering insights\ninto the current landscape of cross-lingual code generation.", "published": "2024-10-19 08:44:26", "link": "http://arxiv.org/abs/2410.15037v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are LLMs Good Zero-Shot Fallacy Classifiers?", "abstract": "Fallacies are defective arguments with faulty reasoning. Detecting and\nclassifying them is a crucial NLP task to prevent misinformation, manipulative\nclaims, and biased decisions. However, existing fallacy classifiers are limited\nby the requirement for sufficient labeled data for training, which hinders\ntheir out-of-distribution (OOD) generalization abilities. In this paper, we\nfocus on leveraging Large Language Models (LLMs) for zero-shot fallacy\nclassification. To elicit fallacy-related knowledge and reasoning abilities of\nLLMs, we propose diverse single-round and multi-round prompting schemes,\napplying different task-specific instructions such as extraction,\nsummarization, and Chain-of-Thought reasoning. With comprehensive experiments\non benchmark datasets, we suggest that LLMs could be potential zero-shot\nfallacy classifiers. In general, LLMs under single-round prompting schemes have\nachieved acceptable zero-shot performances compared to the best full-shot\nbaselines and can outperform them in all OOD inference scenarios and some\nopen-domain tasks. Our novel multi-round prompting schemes can effectively\nbring about more improvements, especially for small LLMs. Our analysis further\nunderlines the future research on zero-shot fallacy classification. Codes and\ndata are available at: https://github.com/panFJCharlotte98/Fallacy_Detection.", "published": "2024-10-19 09:38:55", "link": "http://arxiv.org/abs/2410.15050v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Robust RALMs: Revealing the Impact of Imperfect Retrieval on\n  Retrieval-Augmented Language Models", "abstract": "Retrieval Augmented Language Models (RALMs) have gained significant attention\nfor their ability to generate accurate answer and improve efficiency. However,\nRALMs are inherently vulnerable to imperfect information due to their reliance\non the imperfect retriever or knowledge source. We identify three common\nscenarios-unanswerable, adversarial, conflicting-where retrieved document sets\ncan confuse RALM with plausible real-world examples. We present the first\ncomprehensive investigation to assess how well RALMs detect and handle such\nproblematic scenarios. Among these scenarios, to systematically examine\nadversarial robustness we propose a new adversarial attack method, Generative\nmodel-based ADVersarial attack (GenADV) and a novel metric Robustness under\nAdditional Document (RAD). Our findings reveal that RALMs often fail to\nidentify the unanswerability or contradiction of a document set, which\nfrequently leads to hallucinations. Moreover, we show the addition of an\nadversary significantly degrades RALM's performance, with the model becoming\neven more vulnerable when the two scenarios overlap (adversarial+unanswerable).\nOur research identifies critical areas for assessing and enhancing the\nrobustness of RALMs, laying the foundation for the development of more robust\nmodels.", "published": "2024-10-19 13:40:33", "link": "http://arxiv.org/abs/2410.15107v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Verification with Transparency: The TrendFact Benchmark for Auditable\n  Fact-Checking via Natural Language Explanation", "abstract": "While fact verification remains fundamental, explanation generation serves as\na critical enabler for trustworthy fact-checking systems by producing\ninterpretable rationales and facilitating comprehensive verification processes.\nHowever, current benchmarks exhibit critical limitations in three dimensions:\n(1) absence of explanatory annotations, (2) English-centric language bias, and\n(3) inadequate temporal relevance. To bridge these gaps, we present TrendFact,\nthe first Chinese fact-checking benchmark incorporating structured natural\nlanguage explanations. TrendFact comprises 7,643 carefully curated samples from\ntrending social media content and professional fact-checking repositories,\ncovering domains such as public health, political discourse, and economic\nclaims. It supports various forms of reasoning, including numerical\ncomputation, logical reasoning, and common sense verification. The rigorous\nmultistage construction process ensures high data quality and provides\nsignificant challenges. Furthermore, we propose the ECS to complement existing\nevaluation metrics. To establish effective baselines for TrendFact, we propose\nFactISR, a dual-component method integrating evidence triangulation and\niterative self-reflection mechanism. Experimental results demonstrate that\ncurrent leading reasoning models (e.g., DeepSeek-R1, o1) have significant\nlimitations on TrendFact, underscoring the real-world challenges it presents.\nFactISR significantly enhances reasoning model performance, offering new\ninsights for explainable and complex fact-checking.", "published": "2024-10-19 15:25:19", "link": "http://arxiv.org/abs/2410.15135v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CAST: Corpus-Aware Self-similarity Enhanced Topic modelling", "abstract": "Topic modelling is a pivotal unsupervised machine learning technique for\nextracting valuable insights from large document collections. Existing neural\ntopic modelling methods often encode contextual information of documents, while\nignoring contextual details of candidate centroid words, leading to the\ninaccurate selection of topic words due to the contextualization gap. In\nparallel, it is found that functional words are frequently selected over\ntopical words. To address these limitations, we introduce CAST: Corpus-Aware\nSelf-similarity Enhanced Topic modelling, a novel topic modelling method that\nbuilds upon candidate centroid word embeddings contextualized on the dataset,\nand a novel self-similarity-based method to filter out less meaningful tokens.\nInspired by findings in contrastive learning that self-similarities of\nfunctional token embeddings in different contexts are much lower than topical\ntokens, we find self-similarity to be an effective metric to prevent functional\nwords from acting as candidate topic words. Our approach significantly enhances\nthe coherence and diversity of generated topics, as well as the topic model's\nability to handle noisy data. Experiments on news benchmark datasets and one\nTwitter dataset demonstrate the method's superiority in generating coherent,\ndiverse topics, and handling noisy data, outperforming strong baselines.", "published": "2024-10-19 15:27:11", "link": "http://arxiv.org/abs/2410.15136v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A survey of neural-network-based methods utilising comparable data for\n  finding translation equivalents", "abstract": "The importance of inducing bilingual dictionary components in many natural\nlanguage processing (NLP) applications is indisputable. However, the dictionary\ncompilation process requires extensive work and combines two disciplines, NLP\nand lexicography, while the former often omits the latter. In this paper, we\npresent the most common approaches from NLP that endeavour to automatically\ninduce one of the essential dictionary components, translation equivalents and\nfocus on the neural-network-based methods using comparable data. We analyse\nthem from a lexicographic perspective since their viewpoints are crucial for\nimproving the described methods. Moreover, we identify the methods that\nintegrate these viewpoints and can be further exploited in various applications\nthat require them. This survey encourages a connection between the NLP and\nlexicography fields as the NLP field can benefit from lexicographic insights,\nand it serves as a helping and inspiring material for further research in the\ncontext of neural-network-based methods utilising comparable data.", "published": "2024-10-19 16:10:41", "link": "http://arxiv.org/abs/2410.15144v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Deep Unlearning in Large Language Models", "abstract": "Machine unlearning is a key requirement of many data protection regulations\nsuch as GDPR. Prior work on unlearning has mostly considered superficial\nunlearning tasks where a single or a few related pieces of information are\nrequired to be removed. However, the task of unlearning a fact is much more\nchallenging in recent large language models (LLMs), because the facts in LLMs\ncan be deduced from each other. In this work, we investigate whether current\nunlearning methods for LLMs succeed beyond superficial unlearning of facts.\nSpecifically, we formally propose a framework and a definition for deep\nunlearning facts that are interrelated. We design the metric, recall, to\nquantify the extent of deep unlearning. To systematically evaluate deep\nunlearning, we construct a synthetic dataset EDU-RELAT, which consists of a\nsynthetic knowledge base of family relationships and biographies, together with\na realistic logical rule set that connects them. We use this dataset to test\nfour unlearning methods in four LLMs at different sizes. Our findings reveal\nthat in the task of deep unlearning only a single fact, they either fail to\nproperly unlearn with high recall, or end up unlearning many other irrelevant\nfacts. Our dataset and code are publicly available at:\nhttps://github.com/wrh14/deep_unlearning.", "published": "2024-10-19 16:40:08", "link": "http://arxiv.org/abs/2410.15153v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Electoral Approach to Diversify LLM-based Multi-Agent Collective\n  Decision-Making", "abstract": "Modern large language models (LLMs) have exhibited cooperative synergy on\ncomplex task-solving, and collective decision-making (CDM) is a pivotal\ncomponent in LLM-based multi-agent collaboration frameworks. Our survey on 52\nrecent such systems uncovers a severe lack of diversity, with a heavy reliance\non dictatorial and plurality voting for CDM. Through the lens of social choice\ntheory, we scrutinize widely-adopted CDM methods and identify their\nlimitations. To enrich current landscape of LLM-based CDM, we present GEDI, an\nelectoral CDM module that incorporates various ordinal preferential voting\nmechanisms. Our empirical case study across three benchmarks shows that the\nintegration of certain CDM methods can markedly improve the reasoning\ncapabilities and robustness of some leading LLMs, all without requiring\nintricate system designs. Additionally, we find that some CDM mechanisms\ngenerate positive synergies even with as few as three agents. The voting-based\nmethods also demonstrate robustness against single points of failure, as well\nas diversity in terms of hit-rate@k and subject-wise impacts.", "published": "2024-10-19 17:42:49", "link": "http://arxiv.org/abs/2410.15168v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Diversity of Synthetic Data and its Impact on Training Large\n  Language Models", "abstract": "The rise of Large Language Models (LLMs) has accentuated the need for\ndiverse, high-quality pre-training data. Synthetic data emerges as a viable\nsolution to the challenges of data scarcity and inaccessibility. While previous\nliterature has focused predominantly on the quality and quantity of real data,\nour work enables the measurement of diversity in synthetic data and explores\nits impact on LLM performance. We study the downstream effects of synthetic\ndata diversity during both the pre-training and fine-tuning stages by\nintroducing a new diversity metric, \\textit{LLM cluster-agent}, designed to\nevaluate the diversity of synthetic datasets. Through a series of controlled\nexperiments with models of 350M and 1.4B parameters, we demonstrate that the\nproposed cluster-based LLM scoring of diversity correlates positively with both\npre-training and supervised fine-tuning performance. Our findings also reveal\nthat synthetic data diversity in pre-training affects supervised fine-tuning\nmore significantly than pre-training itself, even for smaller models. We hope\nthis study advances our understanding of the optimal use of synthetic data in\nLLM training and opens new avenues for efficient data generation processes.", "published": "2024-10-19 22:14:07", "link": "http://arxiv.org/abs/2410.15226v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Baichuan Alignment Technical Report", "abstract": "We introduce Baichuan Alignment, a detailed analysis of the alignment\ntechniques employed in the Baichuan series of models. This represents the\nindustry's first comprehensive account of alignment methodologies, offering\nvaluable insights for advancing AI research. We investigate the critical\ncomponents that enhance model performance during the alignment process,\nincluding optimization methods, data strategies, capability enhancements, and\nevaluation processes. The process spans three key stages: Prompt Augmentation\nSystem(PAS), Supervised Fine-Tuning(SFT), and Preference Alignment. The\nproblems encountered, the solutions applied, and the improvements made are\nthoroughly recorded.\n  Through comparisons across well-established benchmarks, we highlight the\ntechnological advancements enabled by Baichuan Alignment. Baichuan-Instruct is\nan internal model, while Qwen2-Nova-72B and Llama3-PBM-Nova-70B are instruct\nversions of the Qwen2-72B and Llama-3-70B base models, optimized through\nBaichuan Alignment. Baichuan-Instruct demonstrates significant improvements in\ncore capabilities, with user experience gains ranging from 17% to 28%, and\nperforms exceptionally well on specialized benchmarks. In open-source benchmark\nevaluations, both Qwen2-Nova-72B and Llama3-PBM-Nova-70B consistently\noutperform their respective official instruct versions across nearly all\ndatasets. This report aims to clarify the key technologies behind the alignment\nprocess, fostering a deeper understanding within the community.\nLlama3-PBM-Nova-70B model is available at\nhttps://huggingface.co/PKU-Baichuan-MLSystemLab/Llama3-PBM-Nova-70B.", "published": "2024-10-19 02:07:33", "link": "http://arxiv.org/abs/2410.14940v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SemiHVision: Enhancing Medical Multimodal Models with a Semi-Human\n  Annotated Dataset and Fine-Tuned Instruction Generation", "abstract": "Multimodal large language models (MLLMs) have made significant strides, yet\nthey face challenges in the medical domain due to limited specialized\nknowledge. While recent medical MLLMs demonstrate strong performance in lab\nsettings, they often struggle in real-world applications, highlighting a\nsubstantial gap between research and practice. In this paper, we seek to\naddress this gap at various stages of the end-to-end learning pipeline,\nincluding data collection, model fine-tuning, and evaluation. At the data\ncollection stage, we introduce SemiHVision, a dataset that combines human\nannotations with automated augmentation techniques to improve both medical\nknowledge representation and diagnostic reasoning. For model fine-tuning, we\ntrained PMC-Cambrian-8B-AN over 2400 H100 GPU hours, resulting in performance\nthat surpasses public medical models like HuatuoGPT-Vision-34B (79.0% vs.\n66.7%) and private general models like Claude3-Opus (55.7%) on traditional\nbenchmarks such as SLAKE and VQA-RAD. In the evaluation phase, we observed that\ntraditional benchmarks cannot accurately reflect realistic clinical task\ncapabilities. To overcome this limitation and provide more targeted guidance\nfor model evaluation, we introduce the JAMA Clinical Challenge, a novel\nbenchmark specifically designed to evaluate diagnostic reasoning. On this\nbenchmark, PMC-Cambrian-AN achieves state-of-the-art performance with a GPT-4\nscore of 1.29, significantly outperforming HuatuoGPT-Vision-34B (1.13) and\nClaude3-Opus (1.17), demonstrating its superior diagnostic reasoning abilities.", "published": "2024-10-19 02:35:35", "link": "http://arxiv.org/abs/2410.14948v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "ChitroJera: A Regionally Relevant Visual Question Answering Dataset for\n  Bangla", "abstract": "Visual Question Answer (VQA) poses the problem of answering a natural\nlanguage question about a visual context. Bangla, despite being a widely spoken\nlanguage, is considered low-resource in the realm of VQA due to the lack of a\nproper benchmark dataset. The absence of such datasets challenges models that\nare known to be performant in other languages. Furthermore, existing Bangla VQA\ndatasets offer little cultural relevance and are largely adapted from their\nforeign counterparts. To address these challenges, we introduce a large-scale\nBangla VQA dataset titled ChitroJera, totaling over 15k samples where diverse\nand locally relevant data sources are used. We assess the performance of text\nencoders, image encoders, multimodal models, and our novel dual-encoder models.\nThe experiments reveal that the pre-trained dual-encoders outperform other\nmodels of its scale. We also evaluate the performance of large language models\n(LLMs) using prompt-based techniques, with LLMs achieving the best performance.\nGiven the underdeveloped state of existing datasets, we envision ChitroJera\nexpanding the scope of Vision-Language tasks in Bangla.", "published": "2024-10-19 05:45:21", "link": "http://arxiv.org/abs/2410.14991v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Enhancing Multimodal Sentiment Analysis for Missing Modality through\n  Self-Distillation and Unified Modality Cross-Attention", "abstract": "In multimodal sentiment analysis, collecting text data is often more\nchallenging than video or audio due to higher annotation costs and inconsistent\nautomatic speech recognition (ASR) quality. To address this challenge, our\nstudy has developed a robust model that effectively integrates multimodal\nsentiment information, even in the absence of text modality. Specifically, we\nhave developed a Double-Flow Self-Distillation Framework, including Unified\nModality Cross-Attention (UMCA) and Modality Imagination Autoencoder (MIA),\nwhich excels at processing both scenarios with complete modalities and those\nwith missing text modality. In detail, when the text modality is missing, our\nframework uses the LLM-based model to simulate the text representation from the\naudio modality, while the MIA module supplements information from the other two\nmodalities to make the simulated text representation similar to the real text\nrepresentation. To further align the simulated and real representations, and to\nenable the model to capture the continuous nature of sample orders in sentiment\nvalence regression tasks, we have also introduced the Rank-N Contrast (RNC)\nloss function. When testing on the CMU-MOSEI, our model achieved outstanding\nperformance on MAE and significantly outperformed other models when text\nmodality is missing. The code is available at:\nhttps://github.com/WarmCongee/SDUMC", "published": "2024-10-19 07:59:41", "link": "http://arxiv.org/abs/2410.15029v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Weakly-supervised diagnosis identification from Italian discharge\n  letters", "abstract": "Objective: Recognizing diseases from discharge letters is crucial for cohort\nselection and epidemiological analyses, as this is the only type of data\nconsistently produced across hospitals. This is a classic document\nclassification problem, typically requiring supervised learning. However,\nmanual annotation of large datasets of discharge letters is uncommon since it\nis extremely time-consuming. We propose a novel weakly-supervised pipeline to\nrecognize diseases from Italian discharge letters. Methods: Our Natural\nLanguage Processing pipeline is based on a fine-tuned version of the Italian\nUmberto model. The pipeline extracts diagnosis-related sentences from a subset\nof letters and applies a two-level clustering using the embeddings generated by\nthe fine-tuned Umberto model. These clusters are summarized and those mapped to\nthe diseases of interest are selected as weak labels. Finally, the same\nBERT-based model is trained using these weak labels to detect the targeted\ndiseases. Results: A case study related to the identification of bronchiolitis\nwith 33'176 Italian discharge letters from 44 hospitals in the Veneto Region\nshows the potential of our method, with an AUC of 77.7 % and an F1-Score of\n75.1 % on manually annotated labels, improving compared to other non-supervised\nmethods and with a limited loss compared to fully supervised methods. Results\nare robust to the cluster selection and the identified clusters highlight the\npotential to recognize a variety of diseases. Conclusions: This study\ndemonstrates the feasibility of diagnosis identification from Italian discharge\nletters in the absence of labelled data. Our pipeline showed strong performance\nand robustness, and its flexibility allows for easy adaptation to various\ndiseases. This approach offers a scalable solution for clinical text\nclassification, reducing the need for manual annotation while maintaining good\naccuracy.", "published": "2024-10-19 09:42:20", "link": "http://arxiv.org/abs/2410.15051v1", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large\n  Language Models", "abstract": "Generation of plausible but incorrect factual information, often termed\nhallucination, has attracted significant research interest. Retrieval-augmented\nlanguage model (RALM) -- which enhances models with up-to-date knowledge --\nemerges as a promising method to reduce hallucination. However, existing RALMs\nmay instead exacerbate hallucination when retrieving lengthy contexts. To\naddress this challenge, we propose COFT, a novel\n\\textbf{CO}arse-to-\\textbf{F}ine highligh\\textbf{T}ing method to focus on\ndifferent granularity-level key texts, thereby avoiding getting lost in lengthy\ncontexts. Specifically, COFT consists of three components: \\textit{recaller},\n\\textit{scorer}, and \\textit{selector}. First, \\textit{recaller} applies a\nknowledge graph to extract potential key entities in a given context. Second,\n\\textit{scorer} measures the importance of each entity by calculating its\ncontextual weight. Finally, \\textit{selector} selects high contextual weight\nentities with a dynamic threshold algorithm and highlights the corresponding\nparagraphs, sentences, or words in a coarse-to-fine manner. Extensive\nexperiments on the knowledge hallucination benchmark demonstrate the\neffectiveness of COFT, leading to a superior performance over $30\\%$ in the F1\nscore metric. Moreover, COFT also exhibits remarkable versatility across\nvarious long-form tasks, such as reading comprehension and question answering.", "published": "2024-10-19 13:59:48", "link": "http://arxiv.org/abs/2410.15116v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MELT: Materials-aware Continued Pre-training for Language Model\n  Adaptation to Materials Science", "abstract": "We introduce a novel continued pre-training method, MELT (MatEriaLs-aware\ncontinued pre-Training), specifically designed to efficiently adapt the\npre-trained language models (PLMs) for materials science. Unlike previous\nadaptation strategies that solely focus on constructing domain-specific corpus,\nMELT comprehensively considers both the corpus and the training strategy, given\nthat materials science corpus has distinct characteristics from other domains.\nTo this end, we first construct a comprehensive materials knowledge base from\nthe scientific corpus by building semantic graphs. Leveraging this extracted\nknowledge, we integrate a curriculum into the adaptation process that begins\nwith familiar and generalized concepts and progressively moves toward more\nspecialized terms. We conduct extensive experiments across diverse benchmarks\nto verify the effectiveness and generality of MELT. A comprehensive evaluation\nconvincingly supports the strength of MELT, demonstrating superior performance\ncompared to existing continued pre-training methods. The in-depth analysis also\nshows that MELT enables PLMs to effectively represent materials entities\ncompared to the existing adaptation methods, thereby highlighting its broad\napplicability across a wide spectrum of materials science.", "published": "2024-10-19 14:49:03", "link": "http://arxiv.org/abs/2410.15126v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Less is More: Parameter-Efficient Selection of Intermediate Tasks for\n  Transfer Learning", "abstract": "Intermediate task transfer learning can greatly improve model performance.\nIf, for example, one has little training data for emotion detection, first\nfine-tuning a language model on a sentiment classification dataset may improve\nperformance strongly. But which task to choose for transfer learning? Prior\nmethods producing useful task rankings are infeasible for large source pools,\nas they require forward passes through all source language models. We overcome\nthis by introducing Embedding Space Maps (ESMs), light-weight neural networks\nthat approximate the effect of fine-tuning a language model. We conduct the\nlargest study on NLP task transferability and task selection with 12k\nsource-target pairs. We find that applying ESMs on a prior method reduces\nexecution time and disk space usage by factors of 10 and 278, respectively,\nwhile retaining high selection performance (avg. regret@5 score of 2.95).", "published": "2024-10-19 16:22:04", "link": "http://arxiv.org/abs/2410.15148v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluation Of P300 Speller Performance Using Large Language Models Along\n  With Cross-Subject Training", "abstract": "Amyotrophic lateral sclerosis (ALS), a progressive neuromuscular degenerative\ndisease, severely restricts patient communication capacity within a few years\nof onset, resulting in a significant deterioration of quality of life. The P300\nspeller brain computer interface (BCI) offers an alternative communication\nmedium by leveraging a subject's EEG response to characters traditionally\nhighlighted on a character grid on a graphical user interface (GUI). A\nrecurring theme in P300-based research is enhancing performance to enable\nfaster subject interaction. This study builds on that theme by addressing key\nlimitations, particularly in the training of multi-subject classifiers, and by\nintegrating advanced language models to optimize stimuli presentation and word\nprediction, thereby improving communication efficiency. Furthermore, various\nadvanced large language models such as Generative Pre-Trained Transformer\n(GPT2), BERT, and BART, alongside Dijkstra's algorithm, are utilized to\noptimize stimuli and provide word completion choices based on the spelling\nhistory. In addition, a multi-layered smoothing approach is applied to allow\nfor out-of-vocabulary (OOV) words. By conducting extensive simulations based on\nrandomly sampled EEG data from subjects, we show substantial speed improvements\nin typing passages that include rare and out-of-vocabulary (OOV) words, with\nthe extent of improvement varying depending on the language model utilized. The\ngains through such character-level interface optimizations are approximately\n10%, and GPT2 for multi-word prediction provides gains of around 40%. In\nparticular, some large language models achieve performance levels within 10% of\nthe theoretical performance limits established in this study. In addition, both\nwithin and across subjects, training techniques are explored, and speed\nimprovements are shown to hold in both cases.", "published": "2024-10-19 17:23:16", "link": "http://arxiv.org/abs/2410.15161v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Uncovering Autoregressive LLM Knowledge of Thematic Fit in Event\n  Representation", "abstract": "The thematic fit estimation task measures the compatibility between a\npredicate (typically a verb), an argument (typically a noun phrase), and a\nspecific semantic role assigned to the argument. Previous state-of-the-art work\nhas focused on modeling thematic fit through distributional or neural models of\nevent representation, trained in a supervised fashion with indirect labels. In\nthis work, we assess whether pre-trained autoregressive LLMs possess\nconsistent, expressible knowledge about thematic fit. We evaluate both closed\nand open state-of-the-art LLMs on several psycholinguistic datasets, along\nthree axes: (1) Reasoning Form: multi-step logical reasoning (chain-of-thought\nprompting) vs. simple prompting. (2) Input Form: providing context (generated\nsentences) vs. raw tuples <predicate, argument, role>. (3) Output Form:\ncategorical vs. numeric. Our results show that chain-of-thought reasoning is\nmore effective on datasets with self-explanatory semantic role labels,\nespecially Location. Generated sentences helped only in few settings, and\nlowered results in many others. Predefined categorical (compared to numeric)\noutput raised GPT's results across the board with few exceptions, but lowered\nLlama's. We saw that semantically incoherent generated sentences, which the\nmodels lack the ability to consistently filter out, hurt reasoning and overall\nperformance too. Our GPT-powered methods set new state-of-the-art on all tested\ndatasets.", "published": "2024-10-19 18:25:30", "link": "http://arxiv.org/abs/2410.15173v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fine-tuning foundational models to code diagnoses from veterinary health\n  records", "abstract": "Veterinary medical records represent a large data resource for application to\nveterinary and One Health clinical research efforts. Use of the data is limited\nby interoperability challenges including inconsistent data formats and data\nsiloing. Clinical coding using standardized medical terminologies enhances the\nquality of medical records and facilitates their interoperability with\nveterinary and human health records from other sites. Previous studies, such as\nDeepTag and VetTag, evaluated the application of Natural Language Processing\n(NLP) to automate veterinary diagnosis coding, employing long short-term memory\n(LSTM) and transformer models to infer a subset of Systemized Nomenclature of\nMedicine - Clinical Terms (SNOMED-CT) diagnosis codes from free-text clinical\nnotes. This study expands on these efforts by incorporating all 7,739 distinct\nSNOMED-CT diagnosis codes recognized by the Colorado State University (CSU)\nVeterinary Teaching Hospital (VTH) and by leveraging the increasing\navailability of pre-trained large language models (LLMs). Ten freely-available\npre-trained LLMs were fine-tuned on the free-text notes from 246,473\nmanually-coded veterinary patient visits included in the CSU VTH's electronic\nhealth records (EHRs), which resulted in superior performance relative to\nprevious efforts. The most accurate results were obtained when expansive\nlabeled data were used to fine-tune relatively large clinical LLMs, but the\nstudy also showed that comparable results can be obtained using more limited\nresources and non-clinical LLMs. The results of this study contribute to the\nimprovement of the quality of veterinary EHRs by investigating accessible\nmethods for automated coding and support both animal and human health research\nby paving the way for more integrated and comprehensive health databases that\nspan species and institutions.", "published": "2024-10-19 19:30:29", "link": "http://arxiv.org/abs/2410.15186v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Chasing Random: Instruction Selection Strategies Fail to Generalize", "abstract": "Prior work has shown that language models can be tuned to follow user\ninstructions using only a small set of high-quality instructions. This has\naccelerated the development of methods that filter a large, noisy\ninstruction-tuning datasets down to high-quality subset which works just as\nwell. However, typically, the performance of these methods is not demonstrated\nacross a uniform experimental setup and thus their generalization capabilities\nare not well established. In this work, we analyze popular selection strategies\nacross different source datasets, selection budgets and evaluation benchmarks:\nOur results indicate that selection strategies generalize poorly, often failing\nto consistently outperform even random baselines. We also analyze the\ncost-performance trade-offs of using data selection. Our findings reveal that\ndata selection can often exceed the cost of fine-tuning on the full dataset,\nyielding only marginal and sometimes no gains compared to tuning on the full\ndataset or a random subset.", "published": "2024-10-19 22:10:49", "link": "http://arxiv.org/abs/2410.15225v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "End-to-End Transformer-based Automatic Speech Recognition for Northern\n  Kurdish: A Pioneering Approach", "abstract": "Automatic Speech Recognition (ASR) for low-resource languages remains a\nchallenging task due to limited training data. This paper introduces a\ncomprehensive study exploring the effectiveness of Whisper, a pre-trained ASR\nmodel, for Northern Kurdish (Kurmanji) an under-resourced language spoken in\nthe Middle East. We investigate three fine-tuning strategies: vanilla, specific\nparameters, and additional modules. Using a Northern Kurdish fine-tuning speech\ncorpus containing approximately 68 hours of validated transcribed data, our\nexperiments demonstrate that the additional module fine-tuning strategy\nsignificantly improves ASR accuracy on a specialized test set, achieving a Word\nError Rate (WER) of 10.5% and Character Error Rate (CER) of 5.7% with Whisper\nversion 3. These results underscore the potential of sophisticated transformer\nmodels for low-resource ASR and emphasize the importance of tailored\nfine-tuning techniques for optimal performance.", "published": "2024-10-19 11:46:30", "link": "http://arxiv.org/abs/2410.16330v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Team Ryu's Submission to SIGMORPHON 2024 Shared Task on Subword\n  Tokenization", "abstract": "This papers presents the submission of team Ryu to the canceled SIGMORPHON\n2024 shared task on subword tokenization. My submission explores whether\nmorphological segmentation methods can be used as a part of subword tokenizers.\nI adopt two approaches: the statistical segmentation method Morfessor and a\ntransformer based sequence-to-sequence (seq2seq) segmentation model in\ntokenizers. The prediction results show that morphological segmentation could\nbe as effective as commonly used subword tokenizers. Additionally, I\ninvestigate how a tokenizer's vocabulary influences the performance of language\nmodels. A tokenizer with a balanced token frequency distribution tends to work\nbetter. A balanced token vocabulary can be achieved by keeping frequent words\nas unique tokens.", "published": "2024-10-19 04:06:09", "link": "http://arxiv.org/abs/2410.17094v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BrainECHO: Semantic Brain Signal Decoding through Vector-Quantized\n  Spectrogram Reconstruction for Whisper-Enhanced Text Generation", "abstract": "Recent advances in decoding language from brain signals (EEG and MEG) have\nbeen significantly driven by pre-trained language models, leading to remarkable\nprogress on publicly available non-invasive EEG/MEG datasets. However, previous\nworks predominantly utilize teacher forcing during text generation, leading to\nsignificant performance drops without its use. A fundamental issue is the\ninability to establish a unified feature space correlating textual data with\nthe corresponding evoked brain signals. Although some recent studies attempt to\nmitigate this gap using an audio-text pre-trained model, Whisper, which is\nfavored for its signal input modality, they still largely overlook the inherent\ndifferences between audio signals and brain signals in directly applying\nWhisper to decode brain signals. To address these limitations, we propose a new\nmulti-stage strategy for semantic brain signal decoding via vEctor-quantized\nspeCtrogram reconstruction for WHisper-enhanced text generatiOn, termed\nBrainECHO. Specifically, BrainECHO successively conducts: 1) Discrete\nautoencoding of the audio spectrogram; 2) Brain-audio latent space alignment;\nand 3) Semantic text generation via Whisper finetuning. Through this\nautoencoding--alignment--finetuning process, BrainECHO outperforms\nstate-of-the-art methods under the same data split settings on two widely\naccepted resources: the EEG dataset (Brennan) and the MEG dataset (GWilliams).\nThe innovation of BrainECHO, coupled with its robustness and superiority at the\nsentence, session, and subject-independent levels across public datasets,\nunderscores its significance for language-based brain-computer interfaces.", "published": "2024-10-19 04:29:03", "link": "http://arxiv.org/abs/2410.14971v1", "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Do Large Language Models Truly Grasp Mathematics? An Empirical\n  Exploration From Cognitive Psychology", "abstract": "The cognitive mechanism by which Large Language Models (LLMs) solve\nmathematical problems remains a widely debated and unresolved issue. Currently,\nthere is little interpretable experimental evidence that connects LLMs'\nproblem-solving with human cognitive psychology.To determine if LLMs possess\nhuman-like mathematical reasoning, we modified the problems used in the human\nCognitive Reflection Test (CRT). Our results show that, even with the use of\nChains of Thought (CoT) prompts, mainstream LLMs, including the latest o1 model\n(noted for its reasoning capabilities), have a high error rate when solving\nthese modified CRT problems. Specifically, the average accuracy rate dropped by\nup to 50% compared to the original questions.Further analysis of LLMs'\nincorrect answers suggests that they primarily rely on pattern matching from\ntheir training data, which aligns more with human intuition (System 1 thinking)\nrather than with human-like reasoning (System 2 thinking). This finding\nchallenges the belief that LLMs have genuine mathematical reasoning abilities\ncomparable to humans. As a result, this work may adjust overly optimistic views\non LLMs' progress towards artificial general intelligence.", "published": "2024-10-19 05:01:56", "link": "http://arxiv.org/abs/2410.14979v5", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "DM-Codec: Distilling Multimodal Representations for Speech Tokenization", "abstract": "Recent advancements in speech-language models have yielded significant\nimprovements in speech tokenization and synthesis. However, effectively mapping\nthe complex, multidimensional attributes of speech into discrete tokens remains\nchallenging. This process demands acoustic, semantic, and contextual\ninformation for precise speech representations. Existing speech representations\ngenerally fall into two categories: acoustic tokens from audio codecs and\nsemantic tokens from speech self-supervised learning models. Although recent\nefforts have unified acoustic and semantic tokens for improved performance,\nthey overlook the crucial role of contextual representation in comprehensive\nspeech modeling. Our empirical investigations reveal that the absence of\ncontextual representations results in elevated Word Error Rate (WER) and Word\nInformation Lost (WIL) scores in speech transcriptions. To address these\nlimitations, we propose two novel distillation approaches: (1) a language model\n(LM)-guided distillation method that incorporates contextual information, and\n(2) a combined LM and self-supervised speech model (SM)-guided distillation\ntechnique that effectively distills multimodal representations (acoustic,\nsemantic, and contextual) into a comprehensive speech tokenizer, termed\nDM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder\nframework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM\nduring the training process. Experiments show DM-Codec significantly\noutperforms state-of-the-art speech tokenization models, reducing WER by up to\n13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility\nby 1.85% on the LibriSpeech benchmark dataset. The code, samples, and model\ncheckpoints are available at https://github.com/mubtasimahasan/DM-Codec.", "published": "2024-10-19 07:14:14", "link": "http://arxiv.org/abs/2410.15017v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "On Designing Effective RL Reward at Training Time for LLM Reasoning", "abstract": "Reward models have been increasingly critical for improving the reasoning\ncapability of LLMs. Existing research has shown that a well-trained reward\nmodel can substantially improve model performances at inference time via\nsearch. However, the potential of reward models during RL training time still\nremains largely under-explored. It is currently unclear whether these reward\nmodels can provide additional training signals to enhance the reasoning\ncapabilities of LLMs in RL training that uses sparse success rewards, which\nverify the correctness of solutions. In this work, we evaluate popular reward\nmodels for RL training, including the Outcome-supervised Reward Model (ORM) and\nthe Process-supervised Reward Model (PRM), and train a collection of LLMs for\nmath problems using RL by combining these learned rewards with success rewards.\nSurprisingly, even though these learned reward models have strong\ninference-time performances, they may NOT help or even hurt RL training,\nproducing worse performances than LLMs trained with the success reward only.\nOur analysis reveals that an LLM can receive high rewards from some of these\nreward models by repeating correct but unnecessary reasoning steps, leading to\na severe reward hacking issue. Therefore, we introduce two novel reward\nrefinement techniques, including Clipping and Delta. The key idea is to ensure\nthe accumulative reward of any reasoning trajectory is upper-bounded to keep a\nlearned reward model effective without being exploited. We evaluate our\ntechniques with multiple reward models over a set of 1.5B and 7B LLMs on MATH\nand GSM8K benchmarks and demonstrate that with a carefully designed reward\nfunction, RL training without any additional supervised tuning can improve all\nthe evaluated LLMs, including the state-of-the-art 7B LLM\nQwen2.5-Math-7B-Instruct on MATH and GSM8K benchmarks.", "published": "2024-10-19 13:53:50", "link": "http://arxiv.org/abs/2410.15115v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Explaining Graph Neural Networks with Large Language Models: A\n  Counterfactual Perspective for Molecular Property Prediction", "abstract": "In recent years, Graph Neural Networks (GNNs) have become successful in\nmolecular property prediction tasks such as toxicity analysis. However, due to\nthe black-box nature of GNNs, their outputs can be concerning in high-stakes\ndecision-making scenarios, e.g., drug discovery. Facing such an issue, Graph\nCounterfactual Explanation (GCE) has emerged as a promising approach to improve\nGNN transparency. However, current GCE methods usually fail to take\ndomain-specific knowledge into consideration, which can result in outputs that\nare not easily comprehensible by humans. To address this challenge, we propose\na novel GCE method, LLM-GCE, to unleash the power of large language models\n(LLMs) in explaining GNNs for molecular property prediction. Specifically, we\nutilize an autoencoder to generate the counterfactual graph topology from a set\nof counterfactual text pairs (CTPs) based on an input graph. Meanwhile, we also\nincorporate a CTP dynamic feedback module to mitigate LLM hallucination, which\nprovides intermediate feedback derived from the generated counterfactuals as an\nattempt to give more faithful guidance. Extensive experiments demonstrate the\nsuperior performance of LLM-GCE. Our code is released on\nhttps://github.com/YinhanHe123/new\\_LLM4GNNExplanation.", "published": "2024-10-19 17:34:36", "link": "http://arxiv.org/abs/2410.15165v1", "categories": ["cs.LG", "cs.CL", "q-bio.BM"], "primary_category": "cs.LG"}
{"title": "The Computational Anatomy of Humility: Modeling Intellectual Humility in\n  Online Public Discourse", "abstract": "The ability for individuals to constructively engage with one another across\nlines of difference is a critical feature of a healthy pluralistic society.\nThis is also true in online discussion spaces like social media platforms. To\ndate, much social media research has focused on preventing ills -- like\npolitical polarization and the spread of misinformation. While this is\nimportant, enhancing the quality of online public discourse requires not just\nreducing ills but also promoting foundational human virtues. In this study, we\nfocus on one particular virtue: ``intellectual humility'' (IH), or\nacknowledging the potential limitations in one's own beliefs. Specifically, we\nexplore the development of computational methods for measuring IH at scale. We\nmanually curate and validate an IH codebook on 350 posts about religion drawn\nfrom subreddits and use them to develop LLM-based models for automating this\nmeasurement. Our best model achieves a Macro-F1 score of 0.64 across labels\n(and 0.70 when predicting IH/IA/Neutral at the coarse level), higher than an\nexpected naive baseline of 0.51 (0.32 for IH/IA/Neutral) but lower than a human\nannotator-informed upper bound of 0.85 (0.83 for IH/IA/Neutral). Our results\nboth highlight the challenging nature of detecting IH online -- opening the\ndoor to new directions in NLP research -- and also lay a foundation for\ncomputational social science researchers interested in analyzing and fostering\nmore IH in online public discourse.", "published": "2024-10-19 19:09:20", "link": "http://arxiv.org/abs/2410.15182v1", "categories": ["cs.CY", "cs.CL", "cs.DB"], "primary_category": "cs.CY"}
{"title": "Towards Safer Heuristics With XPlain", "abstract": "Many problems that cloud operators solve are computationally expensive, and\noperators often use heuristic algorithms (that are faster and scale better than\noptimal) to solve them more efficiently. Heuristic analyzers enable operators\nto find when and by how much their heuristics underperform. However, these\ntools do not provide enough detail for operators to mitigate the heuristic's\nimpact in practice: they only discover a single input instance that causes the\nheuristic to underperform (and not the full set), and they do not explain why.\n  We propose XPlain, a tool that extends these analyzers and helps operators\nunderstand when and why their heuristics underperform. We present promising\ninitial results that show such an extension is viable.", "published": "2024-10-19 12:21:42", "link": "http://arxiv.org/abs/2410.15086v1", "categories": ["cs.AI", "cs.CL", "cs.DC", "cs.NI", "cs.PF"], "primary_category": "cs.AI"}
{"title": "Transit Pulse: Utilizing Social Media as a Source for Customer Feedback\n  and Information Extraction with Large Language Model", "abstract": "Users of the transit system flood social networks daily with messages that\ncontain valuable insights crucial for improving service quality. These posts\nhelp transit agencies quickly identify emerging issues. Parsing topics and\nsentiments is key to gaining comprehensive insights to foster service\nexcellence. However, the volume of messages makes manual analysis impractical,\nand standard NLP techniques like Term Frequency-Inverse Document Frequency\n(TF-IDF) fall short in nuanced interpretation. Traditional sentiment analysis\nseparates topics and sentiments before integrating them, often missing the\ninteraction between them. This incremental approach complicates classification\nand reduces analytical productivity. To address these challenges, we propose a\nnovel approach to extracting and analyzing transit-related information,\nincluding sentiment and sarcasm detection, identification of unusual system\nproblems, and location data from social media. Our method employs Large\nLanguage Models (LLM), specifically Llama 3, for a streamlined analysis free\nfrom pre-established topic labels. To enhance the model's domain-specific\nknowledge, we utilize Retrieval-Augmented Generation (RAG), integrating\nexternal knowledge sources into the information extraction pipeline. We\nvalidated our method through extensive experiments comparing its performance\nwith traditional NLP approaches on user tweet data from the real world transit\nsystem. Our results demonstrate the potential of LLMs to transform social media\ndata analysis in the public transit domain, providing actionable insights and\nenhancing transit agencies' responsiveness by extracting a broader range of\ninformation.", "published": "2024-10-19 07:08:40", "link": "http://arxiv.org/abs/2410.15016v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "PAT: Parameter-Free Audio-Text Aligner to Boost Zero-Shot Audio\n  Classification", "abstract": "Audio-Language Models (ALMs) have demonstrated remarkable performance in\nzero-shot audio classification. In this paper, we introduce PAT (Parameter-free\nAudio-Text aligner), a simple and training-free method aimed at boosting the\nzero-shot audio classification performance of CLAP-like ALMs. To achieve this,\nwe propose to improve the cross-modal interaction between audio and language\nmodalities by enhancing the representations for both modalities using mutual\nfeedback. Precisely, to enhance textual representations, we propose a prompt\nensemble algorithm that automatically selects and combines the most relevant\nprompts from a datastore with a large pool of handcrafted prompts and weighs\nthem according to their relevance to the audio. On the other hand, to enhance\naudio representations, we reweigh the frame-level audio features based on the\nenhanced textual information. Our proposed method does not require any\nadditional modules or parameters and can be used with any existing CLAP-like\nALM to improve zero-shot audio classification performance. We experiment across\n18 diverse benchmark datasets and 6 ALMs and show that the PAT outperforms\nvanilla zero-shot evaluation with significant margins of 0.42%-27.0%.\nAdditionally, we demonstrate that PAT maintains robust performance even when\ninput audio is degraded by varying levels of noise. Our code will be\nopen-sourced upon acceptance.", "published": "2024-10-19 10:52:42", "link": "http://arxiv.org/abs/2410.15062v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Independent Feature Enhanced Crossmodal Fusion for Match-Mismatch\n  Classification of Speech Stimulus and EEG Response", "abstract": "It is crucial for auditory attention decoding to classify matched and\nmismatched speech stimuli with corresponding EEG responses by exploring their\nrelationship. However, existing methods often adopt two independent networks to\nencode speech stimulus and EEG response, which neglect the relationship between\nthese signals from the two modalities. In this paper, we propose an independent\nfeature enhanced crossmodal fusion model (IFE-CF) for match-mismatch\nclassification, which leverages the fusion feature of the speech stimulus and\nthe EEG response to achieve auditory EEG decoding. Specifically, our IFE-CF\ncontains a crossmodal encoder to encode the speech stimulus and the EEG\nresponse with a two-branch structure connected via crossmodal attention\nmechanism in the encoding process, a multi-channel fusion module to fuse\nfeatures of two modalities by aggregating the interaction feature obtained from\nthe crossmodal encoder and the independent feature obtained from the speech\nstimulus and EEG response, and a predictor to give the matching result. In\naddition, the causal mask is introduced to consider the time delay of the\nspeech-EEG pair in the crossmodal encoder, which further enhances the feature\nrepresentation for match-mismatch classification. Experiments demonstrate our\nmethod's effectiveness with better classification accuracy, as compared with\nthe baseline of the Auditory EEG Decoding Challenge 2023.", "published": "2024-10-19 11:50:36", "link": "http://arxiv.org/abs/2410.15078v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "ImmerseDiffusion: A Generative Spatial Audio Latent Diffusion Model", "abstract": "We introduce ImmerseDiffusion, an end-to-end generative audio model that\nproduces 3D immersive soundscapes conditioned on the spatial, temporal, and\nenvironmental conditions of sound objects. ImmerseDiffusion is trained to\ngenerate first-order ambisonics (FOA) audio, which is a conventional spatial\naudio format comprising four channels that can be rendered to multichannel\nspatial output. The proposed generative system is composed of a spatial audio\ncodec that maps FOA audio to latent components, a latent diffusion model\ntrained based on various user input types, namely, text prompts, spatial,\ntemporal and environmental acoustic parameters, and optionally a spatial audio\nand text encoder trained in a Contrastive Language and Audio Pretraining (CLAP)\nstyle. We propose metrics to evaluate the quality and spatial adherence of the\ngenerated spatial audio. Finally, we assess the model performance in terms of\ngeneration quality and spatial conformance, comparing the two proposed modes:\n``descriptive\", which uses spatial text prompts) and ``parametric\", which uses\nnon-spatial text prompts and spatial parameters. Our evaluations demonstrate\npromising results that are consistent with the user conditions and reflect\nreliable spatial fidelity.", "published": "2024-10-19 02:28:53", "link": "http://arxiv.org/abs/2410.14945v2", "categories": ["cs.SD", "cs.ET", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Processing using Pattern Recognition for Music Genre\n  Classification", "abstract": "This project explores the application of machine learning techniques for\nmusic genre classification using the GTZAN dataset, which contains 100 audio\nfiles per genre. Motivated by the growing demand for personalized music\nrecommendations, we focused on classifying five genres-Blues, Classical, Jazz,\nHip Hop, and Country-using a variety of algorithms including Logistic\nRegression, K-Nearest Neighbors (KNN), Random Forest, and Artificial Neural\nNetworks (ANN) implemented via Keras. The ANN model demonstrated the best\nperformance, achieving a validation accuracy of 92.44%. We also analyzed key\naudio features such as spectral roll-off, spectral centroid, and MFCCs, which\nhelped enhance the model's accuracy. Future work will expand the model to cover\nall ten genres, investigate advanced methods like Long Short-Term Memory (LSTM)\nnetworks and ensemble approaches, and develop a web application for real-time\ngenre classification and playlist generation. This research aims to contribute\nto improving music recommendation systems and content curation.", "published": "2024-10-19 05:44:05", "link": "http://arxiv.org/abs/2410.14990v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Pronunciation and Accent Conversion through Knowledge\n  Distillation And Synthetic Ground-Truth from Native TTS", "abstract": "Previous approaches on accent conversion (AC) mainly aimed at making\nnon-native speech sound more native while maintaining the original content and\nspeaker identity. However, non-native speakers sometimes have pronunciation\nissues, which can make it difficult for listeners to understand them. Hence, we\ndeveloped a new AC approach that not only focuses on accent conversion but also\nimproves pronunciation of non-native accented speaker. By providing the\nnon-native audio and the corresponding transcript, we generate the ideal\nground-truth audio with native-like pronunciation with original duration and\nprosody. This ground-truth data aids the model in learning a direct mapping\nbetween accented and native speech. We utilize the end-to-end VITS framework to\nachieve high-quality waveform reconstruction for the AC task. As a result, our\nsystem not only produces audio that closely resembles native accents and while\nretaining the original speaker's identity but also improve pronunciation, as\ndemonstrated by evaluation results.", "published": "2024-10-19 06:12:31", "link": "http://arxiv.org/abs/2410.14997v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
