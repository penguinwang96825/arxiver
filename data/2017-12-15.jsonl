{"title": "Learning when to skim and when to read", "abstract": "Many recent advances in deep learning for natural language processing have\ncome at increasing computational cost, but the power of these state-of-the-art\nmodels is not needed for every example in a dataset. We demonstrate two\napproaches to reducing unnecessary computation in cases where a fast but weak\nbaseline classier and a stronger, slower model are both available. Applying an\nAUC-based metric to the task of sentiment classification, we find significant\nefficiency gains with both a probability-threshold method for reducing\ncomputational cost and one that uses a secondary decision network.", "published": "2017-12-15 00:12:47", "link": "http://arxiv.org/abs/1712.05483v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Avoiding Echo-Responses in a Retrieval-Based Conversation System", "abstract": "Retrieval-based conversation systems generally tend to highly rank responses\nthat are semantically similar or even identical to the given conversation\ncontext. While the system's goal is to find the most appropriate response,\nrather than the most semantically similar one, this tendency results in\nlow-quality responses. We refer to this challenge as the echoing problem. To\nmitigate this problem, we utilize a hard negative mining approach at the\ntraining stage. The evaluation shows that the resulting model reduces echoing\nand achieves better results in terms of Average Precision and Recall@N metrics,\ncompared to the models trained without the proposed approach.", "published": "2017-12-15 11:32:29", "link": "http://arxiv.org/abs/1712.05626v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Text Generation and Planning for Strategic Dialogue", "abstract": "End-to-end models for goal-orientated dialogue are challenging to train,\nbecause linguistic and strategic aspects are entangled in latent state vectors.\nWe introduce an approach to learning representations of messages in dialogues\nby maximizing the likelihood of subsequent sentences and actions, which\ndecouples the semantics of the dialogue utterance from its linguistic\nrealization. We then use these latent sentence representations for hierarchical\nlanguage generation, planning and reinforcement learning. Experiments show that\nour approach increases the end-task reward achieved by the model, improves the\neffectiveness of long-term planning using rollouts, and allows self-play\nreinforcement learning to improve decision making without diverging from human\nlanguage. Our hierarchical latent-variable model outperforms previous work both\nlinguistically and strategically.", "published": "2017-12-15 21:33:07", "link": "http://arxiv.org/abs/1712.05846v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoDraw: Collaborative Drawing as a Testbed for Grounded Goal-driven\n  Communication", "abstract": "In this work, we propose a goal-driven collaborative task that combines\nlanguage, perception, and action. Specifically, we develop a Collaborative\nimage-Drawing game between two agents, called CoDraw. Our game is grounded in a\nvirtual world that contains movable clip art objects. The game involves two\nplayers: a Teller and a Drawer. The Teller sees an abstract scene containing\nmultiple clip art pieces in a semantically meaningful configuration, while the\nDrawer tries to reconstruct the scene on an empty canvas using available clip\nart pieces. The two players communicate with each other using natural language.\nWe collect the CoDraw dataset of ~10K dialogs consisting of ~138K messages\nexchanged between human players. We define protocols and metrics to evaluate\nlearned agents in this testbed, highlighting the need for a novel \"crosstalk\"\nevaluation condition which pairs agents trained independently on disjoint\nsubsets of the training data. We present models for our task and benchmark them\nusing both fully automated evaluation and by having them play the game live\nwith humans.", "published": "2017-12-15 06:38:15", "link": "http://arxiv.org/abs/1712.05558v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Novel Approach for Effective Learning in Low Resourced Scenarios", "abstract": "Deep learning based discriminative methods, being the state-of-the-art\nmachine learning techniques, are ill-suited for learning from lower amounts of\ndata. In this paper, we propose a novel framework, called simultaneous two\nsample learning (s2sL), to effectively learn the class discriminative\ncharacteristics, even from very low amount of data. In s2sL, more than one\nsample (here, two samples) are simultaneously considered to both, train and\ntest the classifier. We demonstrate our approach for speech/music\ndiscrimination and emotion classification through experiments. Further, we also\nshow the effectiveness of s2sL approach for classification in low-resource\nscenario, and for imbalanced data.", "published": "2017-12-15 10:33:57", "link": "http://arxiv.org/abs/1712.05608v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Sockeye: A Toolkit for Neural Machine Translation", "abstract": "We describe Sockeye (version 1.12), an open-source sequence-to-sequence\ntoolkit for Neural Machine Translation (NMT). Sockeye is a production-ready\nframework for training and applying models as well as an experimental platform\nfor researchers. Written in Python and built on MXNet, the toolkit offers\nscalable training and inference for the three most prominent encoder-decoder\narchitectures: attentional recurrent neural networks, self-attentional\ntransformers, and fully convolutional networks. Sockeye also supports a wide\nrange of optimizers, normalization and regularization techniques, and inference\nimprovements from current NMT literature. Users can easily run standard\ntraining recipes, explore different model settings, and incorporate new ideas.\nIn this paper, we highlight Sockeye's features and benchmark it against other\nNMT toolkits on two language arcs from the 2017 Conference on Machine\nTranslation (WMT): English-German and Latvian-English. We report competitive\nBLEU scores across all three architectures, including an overall best score for\nSockeye's transformer implementation. To facilitate further comparison, we\nrelease all system outputs and training scripts used in our experiments. The\nSockeye toolkit is free software released under the Apache 2.0 license.", "published": "2017-12-15 14:44:28", "link": "http://arxiv.org/abs/1712.05690v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Sentiment Predictability for Stocks", "abstract": "In this work, we present our findings and experiments for stock-market\nprediction using various textual sentiment analysis tools, such as mood\nanalysis and event extraction, as well as prediction models, such as LSTMs and\nspecific convolutional architectures.", "published": "2017-12-15 18:41:53", "link": "http://arxiv.org/abs/1712.05785v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
