{"title": "Modeling Global and Local Node Contexts for Text Generation from\n  Knowledge Graphs", "abstract": "Recent graph-to-text models generate text from graph-based data using either\nglobal or local aggregation to learn node representations. Global node encoding\nallows explicit communication between two distant nodes, thereby neglecting\ngraph topology as all nodes are directly connected. In contrast, local node\nencoding considers the relations between neighbor nodes capturing the graph\nstructure, but it can fail to capture long-range relations. In this work, we\ngather both encoding strategies, proposing novel neural models which encode an\ninput graph combining both global and local node contexts, in order to learn\nbetter contextualized node embeddings. In our experiments, we demonstrate that\nour approaches lead to significant improvements on two graph-to-text datasets\nachieving BLEU scores of 18.01 on AGENDA dataset, and 63.69 on the WebNLG\ndataset for seen categories, outperforming state-of-the-art models by 3.7 and\n3.1 points, respectively.", "published": "2020-01-29 18:24:14", "link": "http://arxiv.org/abs/2001.11003v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AMR Similarity Metrics from Principles", "abstract": "Different metrics have been proposed to compare Abstract Meaning\nRepresentation (AMR) graphs. The canonical Smatch metric (Cai and Knight, 2013)\naligns the variables of two graphs and assesses triple matches. The recent\nSemBleu metric (Song and Gildea, 2019) is based on the machine-translation\nmetric Bleu (Papineni et al., 2002) and increases computational efficiency by\nablating the variable-alignment.\n  In this paper, i) we establish criteria that enable researchers to perform a\nprincipled assessment of metrics comparing meaning representations like AMR;\nii) we undertake a thorough analysis of Smatch and SemBleu where we show that\nthe latter exhibits some undesirable properties. For example, it does not\nconform to the identity of indiscernibles rule and introduces biases that are\nhard to control; iii) we propose a novel metric S$^2$match that is more\nbenevolent to only very slight meaning deviations and targets the fulfilment of\nall established criteria. We assess its suitability and show its advantages\nover Smatch and SemBleu.", "published": "2020-01-29 16:19:44", "link": "http://arxiv.org/abs/2001.10929v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ABSent: Cross-Lingual Sentence Representation Mapping with Bidirectional\n  GANs", "abstract": "A number of cross-lingual transfer learning approaches based on neural\nnetworks have been proposed for the case when large amounts of parallel text\nare at our disposal. However, in many real-world settings, the size of parallel\nannotated training data is restricted. Additionally, prior cross-lingual\nmapping research has mainly focused on the word level. This raises the question\nof whether such techniques can also be applied to effortlessly obtain\ncross-lingually aligned sentence representations. To this end, we propose an\nAdversarial Bi-directional Sentence Embedding Mapping (ABSent) framework, which\nlearns mappings of cross-lingual sentence representations from limited\nquantities of parallel data.", "published": "2020-01-29 22:44:05", "link": "http://arxiv.org/abs/2001.11121v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interpretable Rumor Detection in Microblogs by Attending to User\n  Interactions", "abstract": "We address rumor detection by learning to differentiate between the\ncommunity's response to real and fake claims in microblogs. Existing\nstate-of-the-art models are based on tree models that model conversational\ntrees. However, in social media, a user posting a reply might be replying to\nthe entire thread rather than to a specific user. We propose a post-level\nattention model (PLAN) to model long distance interactions between tweets with\nthe multi-head attention mechanism in a transformer network. We investigated\nvariants of this model: (1) a structure aware self-attention model (StA-PLAN)\nthat incorporates tree structure information in the transformer network, and\n(2) a hierarchical token and post-level attention model (StA-HiTPLAN) that\nlearns a sentence representation with token-level self-attention. To the best\nof our knowledge, we are the first to evaluate our models on two rumor\ndetection data sets: the PHEME data set as well as the Twitter15 and Twitter16\ndata sets. We show that our best models outperform current state-of-the-art\nmodels for both data sets. Moreover, the attention mechanism allows us to\nexplain rumor detection predictions at both token-level and post-level.", "published": "2020-01-29 02:37:11", "link": "http://arxiv.org/abs/2001.10667v1", "categories": ["cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Learning Robust and Multilingual Speech Representations", "abstract": "Unsupervised speech representation learning has shown remarkable success at\nfinding representations that correlate with phonetic structures and improve\ndownstream speech recognition performance. However, most research has been\nfocused on evaluating the representations in terms of their ability to improve\nthe performance of speech recognition systems on read English (e.g. Wall Street\nJournal and LibriSpeech). This evaluation methodology overlooks two important\ndesiderata that speech representations should have: robustness to domain shifts\nand transferability to other languages. In this paper we learn representations\nfrom up to 8000 hours of diverse and noisy speech data and evaluate the\nrepresentations by looking at their robustness to domain shifts and their\nability to improve recognition performance in many languages. We find that our\nrepresentations confer significant robustness advantages to the resulting\nrecognition systems: we see significant improvements in out-of-domain transfer\nrelative to baseline feature sets and the features likewise provide\nimprovements in 25 phonetically diverse languages including tonal languages and\nlow-resource languages.", "published": "2020-01-29 23:24:56", "link": "http://arxiv.org/abs/2001.11128v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "D\u00e9j\u00e0 vu: A Contextualized Temporal Attention Mechanism for\n  Sequential Recommendation", "abstract": "Predicting users' preferences based on their sequential behaviors in history\nis challenging and crucial for modern recommender systems. Most existing\nsequential recommendation algorithms focus on transitional structure among the\nsequential actions, but largely ignore the temporal and context information,\nwhen modeling the influence of a historical event to current prediction.\n  In this paper, we argue that the influence from the past events on a user's\ncurrent action should vary over the course of time and under different context.\nThus, we propose a Contextualized Temporal Attention Mechanism that learns to\nweigh historical actions' influence on not only what action it is, but also\nwhen and how the action took place. More specifically, to dynamically calibrate\nthe relative input dependence from the self-attention mechanism, we deploy\nmultiple parameterized kernel functions to learn various temporal dynamics, and\nthen use the context information to determine which of these reweighing kernels\nto follow for each input. In empirical evaluations on two large public\nrecommendation datasets, our model consistently outperformed an extensive set\nof state-of-the-art sequential recommendation methods.", "published": "2020-01-29 20:27:42", "link": "http://arxiv.org/abs/2002.00741v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Environment-aware Reconfigurable Noise Suppression", "abstract": "The paper proposes an efficient, robust, and reconfigurable technique to\nsuppress various types of noises for any sampling rate. The theoretical\nanalyses, subjective and objective test results show that the proposed noise\nsuppression (NS) solution significantly enhances the speech transmission index\n(STI), speech intelligibility (SI), signal-to-noise ratio (SNR), and subjective\nlistening experience. The STI and SI consists of 5 levels, i.e., bad, poor,\nfair, good, and excellent. The most common noisy condition is of SNR ranging\nfrom -5 to 8 dB. For the input SNR between -5 and 2.5 dB, the proposed NS\nimproves the STI and SI from \"fair\" to \"good\". For the input SNR between 2.5 to\n8 dB, the STI and SI are improved from \"good\" to \"excellent\" by the proposed\nNS. The proposed NS can be adopted in both capture and playback paths for voice\nover internet protocol, voice-trigger, and automatic speech recognition\napplications.", "published": "2020-01-29 08:22:51", "link": "http://arxiv.org/abs/2001.10718v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Compact recurrent neural networks for acoustic event detection on\n  low-energy low-complexity platforms", "abstract": "Outdoor acoustic events detection is an exciting research field but\nchallenged by the need for complex algorithms and deep learning techniques,\ntypically requiring many computational, memory, and energy resources. This\nchallenge discourages IoT implementation, where an efficient use of resources\nis required. However, current embedded technologies and microcontrollers have\nincreased their capabilities without penalizing energy efficiency. This paper\naddresses the application of sound event detection at the edge, by optimizing\ndeep learning techniques on resource-constrained embedded platforms for the\nIoT. The contribution is two-fold: firstly, a two-stage student-teacher\napproach is presented to make state-of-the-art neural networks for sound event\ndetection fit on current microcontrollers; secondly, we test our approach on an\nARM Cortex M4, particularly focusing on issues related to 8-bits quantization.\nOur embedded implementation can achieve 68% accuracy in recognition on\nUrbansound8k, not far from state-of-the-art performance, with an inference time\nof 125 ms for each second of the audio stream, and power consumption of 5.5 mW\nin just 34.3 kB of RAM.", "published": "2020-01-29 14:56:52", "link": "http://arxiv.org/abs/2001.10876v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Language Identification for Multilingual Speakers", "abstract": "Spoken language identification (LID) technologies have improved in recent\nyears from discriminating largely distinct languages to discriminating highly\nsimilar languages or even dialects of the same language. One aspect that has\nbeen mostly neglected, however, is discrimination of languages for multilingual\nspeakers, despite being a primary target audience of many systems that utilize\nLID technologies. As we show in this work, LID systems can have a high average\naccuracy for most combinations of languages while greatly underperforming for\nothers when accented speech is present. We address this by using\ncoarser-grained targets for the acoustic LID model and integrating its outputs\nwith interaction context signals in a context-aware model to tailor the system\nto each user. This combined system achieves an average 97% accuracy across all\nlanguage combinations while improving worst-case accuracy by over 60% relative\nto our baseline.", "published": "2020-01-29 18:58:11", "link": "http://arxiv.org/abs/2001.11019v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Audio-Visual Decision Fusion for WFST-based and seq2seq Models", "abstract": "Under noisy conditions, speech recognition systems suffer from high Word\nError Rates (WER). In such cases, information from the visual modality\ncomprising the speaker lip movements can help improve the performance. In this\nwork, we propose novel methods to fuse information from audio and visual\nmodalities at inference time. This enables us to train the acoustic and visual\nmodels independently. First, we train separate RNN-HMM based acoustic and\nvisual models. A common WFST generated by taking a special union of the HMM\ncomponents is used for decoding using a modified Viterbi algorithm. Second, we\ntrain separate seq2seq acoustic and visual models. The decoding step is\nperformed simultaneously for both modalities using shallow fusion while\nmaintaining a common hypothesis beam. We also present results for a novel\nseq2seq fusion without the weighing parameter. We present results at varying\nSNR and show that our methods give significant improvements over acoustic-only\nWER.", "published": "2020-01-29 13:45:08", "link": "http://arxiv.org/abs/2001.10832v1", "categories": ["eess.AS", "cs.LG", "cs.MM", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
