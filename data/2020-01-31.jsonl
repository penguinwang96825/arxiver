{"title": "Teaching Machines to Converse", "abstract": "The ability of a machine to communicate with humans has long been associated\nwith the general success of AI. This dates back to Alan Turing's epoch-making\nwork in the early 1950s, which proposes that a machine's intelligence can be\ntested by how well it, the machine, can fool a human into believing that the\nmachine is a human through dialogue conversations. Many systems learn\ngeneration rules from a minimal set of authored rules or labels on top of\nhand-coded rules or templates, and thus are both expensive and difficult to\nextend to open-domain scenarios. Recently, the emergence of neural network\nmodels the potential to solve many of the problems in dialogue learning that\nearlier systems cannot tackle: the end-to-end neural frameworks offer the\npromise of scalability and language-independence, together with the ability to\ntrack the dialogue state and then mapping between states and dialogue actions\nin a way not possible with conventional systems. On the other hand, neural\nsystems bring about new challenges: they tend to output dull and generic\nresponses; they lack a consistent or a coherent persona; they are usually\noptimized through single-turn conversations and are incapable of handling the\nlong-term success of a conversation; and they are not able to take the\nadvantage of the interactions with humans. This dissertation attempts to tackle\nthese challenges: Contributions are two-fold: (1) we address new challenges\npresented by neural network models in open-domain dialogue generation systems;\n(2) we develop interactive question-answering dialogue systems by (a) giving\nthe agent the ability to ask questions and (b) training a conversation agent\nthrough interactions with humans in an online fashion, where a bot improves\nthrough communicating with humans and learning from the mistakes that it makes.", "published": "2020-01-31 08:28:07", "link": "http://arxiv.org/abs/2001.11701v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Break It Down: A Question Understanding Benchmark", "abstract": "Understanding natural language questions entails the ability to break down a\nquestion into the requisite steps for computing its answer. In this work, we\nintroduce a Question Decomposition Meaning Representation (QDMR) for questions.\nQDMR constitutes the ordered list of steps, expressed through natural language,\nthat are necessary for answering a question. We develop a crowdsourcing\npipeline, showing that quality QDMRs can be annotated at scale, and release the\nBreak dataset, containing over 83K pairs of questions and their QDMRs. We\ndemonstrate the utility of QDMR by showing that (a) it can be used to improve\nopen-domain question answering on the HotpotQA dataset, (b) it can be\ndeterministically converted to a pseudo-SQL formal language, which can\nalleviate annotation in semantic parsing applications. Last, we use Break to\ntrain a sequence-to-sequence model with copying that parses questions into QDMR\nstructures, and show that it substantially outperforms several natural\nbaselines.", "published": "2020-01-31 11:04:52", "link": "http://arxiv.org/abs/2001.11770v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hybrid Tiled Convolutional Neural Networks for Text Sentiment\n  Classification", "abstract": "The tiled convolutional neural network (tiled CNN) has been applied only to\ncomputer vision for learning invariances. We adjust its architecture to NLP to\nimprove the extraction of the most salient features for sentiment analysis.\nKnowing that the major drawback of the tiled CNN in the NLP field is its\ninflexible filter structure, we propose a novel architecture called hybrid\ntiled CNN that applies a filter only on the words that appear in the similar\ncontexts and on their neighbor words (a necessary step for preventing the loss\nof some n-grams). The experiments on the datasets of IMDB movie reviews and\nSemEval 2017 demonstrate the efficiency of the hybrid tiled CNN that performs\nbetter than both CNN and tiled CNN.", "published": "2020-01-31 14:08:15", "link": "http://arxiv.org/abs/2001.11857v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An efficient automated data analytics approach to large scale\n  computational comparative linguistics", "abstract": "This research project aimed to overcome the challenge of analysing human\nlanguage relationships, facilitate the grouping of languages and formation of\ngenealogical relationship between them by developing automated comparison\ntechniques. Techniques were based on the phonetic representation of certain key\nwords and concept. Example word sets included numbers 1-10 (curated), large\ndatabase of numbers 1-10 and sheep counting numbers 1-10 (other sources),\ncolours (curated), basic words (curated).\n  To enable comparison within the sets the measure of Edit distance was\ncalculated based on Levenshtein distance metric. This metric between two\nstrings is the minimum number of single-character edits, operations including:\ninsertions, deletions or substitutions. To explore which words exhibit more or\nless variation, which words are more preserved and examine how languages could\nbe grouped based on linguistic distances within sets, several data analytics\ntechniques were involved. Those included density evaluation, hierarchical\nclustering, silhouette, mean, standard deviation and Bhattacharya coefficient\ncalculations. These techniques lead to the development of a workflow which was\nlater implemented by combining Unix shell scripts, a developed R package and\nSWI Prolog. This proved to be computationally efficient and permitted the fast\nexploration of large language sets and their analysis.", "published": "2020-01-31 15:25:56", "link": "http://arxiv.org/abs/2001.11899v1", "categories": ["cs.CL", "J.m"], "primary_category": "cs.CL"}
{"title": "Unsupervised Bilingual Lexicon Induction Across Writing Systems", "abstract": "Recent embedding-based methods in unsupervised bilingual lexicon induction\nhave shown good results, but generally have not leveraged orthographic\n(spelling) information, which can be helpful for pairs of related languages.\nThis work augments a state-of-the-art method with orthographic features, and\nextends prior work in this space by proposing methods that can learn and\nutilize orthographic correspondences even between languages with different\nscripts. We demonstrate this by experimenting on three language pairs with\ndifferent scripts and varying degrees of lexical similarity.", "published": "2020-01-31 19:48:58", "link": "http://arxiv.org/abs/2002.00037v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Popular Classification Models' Robustness to Random and\n  Targeted Corruptions", "abstract": "Text classification models, especially neural networks based models, have\nreached very high accuracy on many popular benchmark datasets. Yet, such models\nwhen deployed in real world applications, tend to perform badly. The primary\nreason is that these models are not tested against sufficient real world\nnatural data. Based on the application users, the vocabulary and the style of\nthe model's input may greatly vary. This emphasizes the need for a model\nagnostic test dataset, which consists of various corruptions that are natural\nto appear in the wild. Models trained and tested on such benchmark datasets,\nwill be more robust against real world data. However, such data sets are not\neasily available. In this work, we address this problem, by extending the\nbenchmark datasets along naturally occurring corruptions such as Spelling\nErrors, Text Noise and Synonyms and making them publicly available. Through\nextensive experiments, we compare random and targeted corruption strategies\nusing Local Interpretable Model-Agnostic Explanations(LIME). We report the\nvulnerabilities in two popular text classification models along these\ncorruptions and also find that targeted corruptions can expose vulnerabilities\nof a model better than random choices in most cases.", "published": "2020-01-31 11:54:46", "link": "http://arxiv.org/abs/2002.00754v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Similarit\u00e0 per la ricerca del dominio di una frase", "abstract": "English. This document aims to study the best algorithms to verify the\nbelonging of a specific document to a related domain by comparing different\nmethods for calculating the distance between two vectors. This study has been\nmade possible with the help of the structures made available by the Apache\nSpark framework. Starting from the study illustrated in the publication \"New\nfrontier of textual classification: Big data and distributed calculus\" by\nMassimiliano Morrelli et al., We wanted to carry out a study on the possible\nimplementation of a solution capable of calculating the Similarity of a\nsentence using the distributed environment.\n  Italiano. Il presente documento persegue l'obiettivo di studiare gli\nalgoritmi migliori per verificare l'appartenenza di un determinato documento a\nun relativo dominio tramite un confronto di diversi metodi per il calcolo della\ndistanza fra due vettori. Tale studio \\`e stato condotto con l'ausilio delle\nstrutture messe a disposizione dal framework Apache Spark. Partendo dallo\nstudio illustrato nella pubblicazione \"Nuova frontiera della classificazione\ntestuale: Big data e calcolo distribuito\" di Massimiliano Morrelli et al., si\n\\`e voluto realizzare uno studio sulla possibile implementazione di una\nsoluzione in grado di calcolare la Similarit\\`a di una frase sfruttando\nl'ambiente distribuito.", "published": "2020-01-31 09:37:00", "link": "http://arxiv.org/abs/2002.00757v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparison Between Traditional Machine Learning Models And Neural\n  Network Models For Vietnamese Hate Speech Detection", "abstract": "Hate-speech detection on social network language has become one of the main\nresearching fields recently due to the spreading of social networks like\nFacebook and Twitter. In Vietnam, the threat of offensive and harassment cause\nbad impacts for online user. The VLSP - Shared task about Hate Speech Detection\non social networks showed many proposed approaches for detecting whatever\ncomment is clean or not. However, this problem still needs further researching.\nConsequently, we compare traditional machine learning and deep learning on a\nlarge dataset about the user's comments on social network in Vietnamese and\nfind out what is the advantage and disadvantage of each model by comparing\ntheir accuracy on F1-score, then we pick two models in which has highest\naccuracy in traditional machine learning models and deep neural models\nrespectively. Next, we compare these two models capable of predicting the right\nlabel by referencing their confusion matrices and considering the advantages\nand disadvantages of each model. Finally, from the comparison result, we\npropose our ensemble method that concentrates the abilities of traditional\nmethods and deep learning methods.", "published": "2020-01-31 09:28:57", "link": "http://arxiv.org/abs/2002.00759v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Adversarial Learning with Comparative Discrimination for Text\n  Generation", "abstract": "Conventional Generative Adversarial Networks (GANs) for text generation tend\nto have issues of reward sparsity and mode collapse that affect the quality and\ndiversity of generated samples. To address the issues, we propose a novel\nself-adversarial learning (SAL) paradigm for improving GANs' performance in\ntext generation. In contrast to standard GANs that use a binary classifier as\nits discriminator to predict whether a sample is real or generated, SAL employs\na comparative discriminator which is a pairwise classifier for comparing the\ntext quality between a pair of samples. During training, SAL rewards the\ngenerator when its currently generated sentence is found to be better than its\npreviously generated samples. This self-improvement reward mechanism allows the\nmodel to receive credits more easily and avoid collapsing towards the limited\nnumber of real samples, which not only helps alleviate the reward sparsity\nissue but also reduces the risk of mode collapse. Experiments on text\ngeneration benchmark datasets show that our proposed approach substantially\nimproves both the quality and the diversity, and yields more stable performance\ncompared to the previous GANs for text generation.", "published": "2020-01-31 07:50:25", "link": "http://arxiv.org/abs/2001.11691v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pseudo-Bidirectional Decoding for Local Sequence Transduction", "abstract": "Local sequence transduction (LST) tasks are sequence transduction tasks where\nthere exists massive overlapping between the source and target sequences, such\nas Grammatical Error Correction (GEC) and spell or OCR correction. Previous\nwork generally tackles LST tasks with standard sequence-to-sequence (seq2seq)\nmodels that generate output tokens from left to right and suffer from the issue\nof unbalanced outputs. Motivated by the characteristic of LST tasks, in this\npaper, we propose a simple but versatile approach named Pseudo-Bidirectional\nDecoding (PBD) for LST tasks. PBD copies the corresponding representation of\nsource tokens to the decoder as pseudo future context to enable the decoder to\nattends to its bi-directional context. In addition, the bidirectional decoding\nscheme and the characteristic of LST tasks motivate us to share the encoder and\nthe decoder of seq2seq models. The proposed PBD approach provides right side\ncontext information for the decoder and models the inductive bias of LST tasks,\nreducing the number of parameters by half and providing good regularization\neffects. Experimental results on several benchmark datasets show that our\napproach consistently improves the performance of standard seq2seq models on\nLST tasks.", "published": "2020-01-31 07:55:39", "link": "http://arxiv.org/abs/2001.11694v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FastWordBug: A Fast Method To Generate Adversarial Text Against NLP\n  Applications", "abstract": "In this paper, we present a novel algorithm, FastWordBug, to efficiently\ngenerate small text perturbations in a black-box setting that forces a\nsentiment analysis or text classification mode to make an incorrect prediction.\nBy combining the part of speech attributes of words, we propose a scoring\nmethod that can quickly identify important words that affect text\nclassification. We evaluate FastWordBug on three real-world text datasets and\ntwo state-of-the-art machine learning models under black-box setting. The\nresults show that our method can significantly reduce the accuracy of the\nmodel, and at the same time, we can call the model as little as possible, with\nthe highest attack efficiency. We also attack two popular real-world cloud\nservices of NLP, and the results show that our method works as well.", "published": "2020-01-31 07:39:45", "link": "http://arxiv.org/abs/2002.00760v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Enhancement of Short Text Clustering by Iterative Classification", "abstract": "Short text clustering is a challenging task due to the lack of signal\ncontained in such short texts. In this work, we propose iterative\nclassification as a method to b o ost the clustering quality (e.g., accuracy)\nof short texts. Given a clustering of short texts obtained using an arbitrary\nclustering algorithm, iterative classification applies outlier removal to\nobtain outlier-free clusters. Then it trains a classification algorithm using\nthe non-outliers based on their cluster distributions. Using the trained\nclassification model, iterative classification reclassifies the outliers to\nobtain a new set of clusters. By repeating this several times, we obtain a much\nimproved clustering of texts. Our experimental results show that the proposed\nclustering enhancement method not only improves the clustering quality of\ndifferent clustering methods (e.g., k-means, k-means--, and hierarchical\nclustering) but also outperforms the state-of-the-art short text clustering\nmethods on several short text datasets by a statistically significant margin.", "published": "2020-01-31 02:12:05", "link": "http://arxiv.org/abs/2001.11631v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Augmenting Visual Question Answering with Semantic Frame Information in\n  a Multitask Learning Approach", "abstract": "Visual Question Answering (VQA) concerns providing answers to Natural\nLanguage questions about images. Several deep neural network approaches have\nbeen proposed to model the task in an end-to-end fashion. Whereas the task is\ngrounded in visual processing, if the question focuses on events described by\nverbs, the language understanding component becomes crucial. Our hypothesis is\nthat models should be aware of verb semantics, as expressed via semantic role\nlabels, argument types, and/or frame elements. Unfortunately, no VQA dataset\nexists that includes verb semantic information. Our first contribution is a new\nVQA dataset (imSituVQA) that we built by taking advantage of the imSitu\nannotations. The imSitu dataset consists of images manually labeled with\nsemantic frame elements, mostly taken from FrameNet. Second, we propose a\nmultitask CNN-LSTM VQA model that learns to classify the answers as well as the\nsemantic frame elements. Our experiments show that semantic frame element\nclassification helps the VQA system avoid inconsistent responses and improves\nperformance.", "published": "2020-01-31 06:31:39", "link": "http://arxiv.org/abs/2001.11673v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Pretrained Transformers for Simple Question Answering over Knowledge\n  Graphs", "abstract": "Answering simple questions over knowledge graphs is a well-studied problem in\nquestion answering. Previous approaches for this task built on recurrent and\nconvolutional neural network based architectures that use pretrained word\nembeddings. It was recently shown that finetuning pretrained transformer\nnetworks (e.g. BERT) can outperform previous approaches on various natural\nlanguage processing tasks. In this work, we investigate how well BERT performs\non SimpleQuestions and provide an evaluation of both BERT and BiLSTM-based\nmodels in datasparse scenarios.", "published": "2020-01-31 18:14:17", "link": "http://arxiv.org/abs/2001.11985v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Massively Multilingual Document Alignment with Cross-lingual\n  Sentence-Mover's Distance", "abstract": "Document alignment aims to identify pairs of documents in two distinct\nlanguages that are of comparable content or translations of each other. Such\naligned data can be used for a variety of NLP tasks from training cross-lingual\nrepresentations to mining parallel data for machine translation. In this paper\nwe develop an unsupervised scoring function that leverages cross-lingual\nsentence embeddings to compute the semantic distance between documents in\ndifferent languages. These semantic distances are then used to guide a document\nalignment algorithm to properly pair cross-lingual web documents across a\nvariety of low, mid, and high-resource language pairs. Recognizing that our\nproposed scoring function and other state of the art methods are\ncomputationally intractable for long web documents, we utilize a more tractable\ngreedy algorithm that performs comparably. We experimentally demonstrate that\nour distance metric performs better alignment than current baselines\noutperforming them by 7% on high-resource language pairs, 15% on mid-resource\nlanguage pairs, and 22% on low-resource language pairs.", "published": "2020-01-31 05:14:16", "link": "http://arxiv.org/abs/2002.00761v2", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Two-path Deep Semi-supervised Learning for Timely Fake News Detection", "abstract": "News in social media such as Twitter has been generated in high volume and\nspeed. However, very few of them are labeled (as fake or true news) by\nprofessionals in near real time. In order to achieve timely detection of fake\nnews in social media, a novel framework of two-path deep semi-supervised\nlearning is proposed where one path is for supervised learning and the other is\nfor unsupervised learning. The supervised learning path learns on the limited\namount of labeled data while the unsupervised learning path is able to learn on\na huge amount of unlabeled data. Furthermore, these two paths implemented with\nconvolutional neural networks (CNN) are jointly optimized to complete\nsemi-supervised learning. In addition, we build a shared CNN to extract the low\nlevel features on both labeled data and unlabeled data to feed them into these\ntwo paths. To verify this framework, we implement a Word CNN based\nsemi-supervised learning model and test it on two datasets, namely, LIAR and\nPHEME. Experimental results demonstrate that the model built on the proposed\nframework can recognize fake news effectively with very few labeled data.", "published": "2020-01-31 02:28:35", "link": "http://arxiv.org/abs/2002.00763v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Improving LPCNet-based Text-to-Speech with Linear Prediction-structured\n  Mixture Density Network", "abstract": "In this paper, we propose an improved LPCNet vocoder using a linear\nprediction (LP)-structured mixture density network (MDN). The recently proposed\nLPCNet vocoder has successfully achieved high-quality and lightweight speech\nsynthesis systems by combining a vocal tract LP filter with a WaveRNN-based\nvocal source (i.e., excitation) generator. However, the quality of synthesized\nspeech is often unstable because the vocal source component is insufficiently\nrepresented by the mu-law quantization method, and the model is trained without\nconsidering the entire speech production mechanism. To address this problem, we\nfirst introduce LP-MDN, which enables the autoregressive neural vocoder to\nstructurally represent the interactions between the vocal tract and vocal\nsource components. Then, we propose to incorporate the LP-MDN to the LPCNet\nvocoder by replacing the conventional discretized output with continuous\ndensity distribution. The experimental results verify that the proposed system\nprovides high quality synthetic speech by achieving a mean opinion score of\n4.41 within a text-to-speech framework.", "published": "2020-01-31 07:43:01", "link": "http://arxiv.org/abs/2001.11686v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A study on the role of subsidiary information in replay attack spoofing\n  detection", "abstract": "In this study, we analyze the role of various categories of subsidiary\ninformation in conducting replay attack spoofing detection: `Room Size',\n`Reverberation', `Speaker-to-ASV distance, `Attacker-to-Speaker distance', and\n`Replay Device Quality'. As a means of analyzing subsidiary information, we use\ntwo frameworks to either subtract or include a category of subsidiary\ninformation to the code extracted from a deep neural network. For subtraction,\nwe utilize an adversarial process framework which makes the code orthogonal to\nthe basis vectors of the subsidiary information. For addition, we utilize the\nmulti-task learning framework to include subsidiary information to the code.\nAll experiments are conducted using the ASVspoof 2019 physical access scenario\nwith the provided meta data. Through the analysis of the result of the two\napproaches, we conclude that various categories of subsidiary information does\nnot reside enough in the code when the deep neural network is trained for\nbinary classification. Explicitly including various categories of subsidiary\ninformation through the multi-task learning framework can help improve\nperformance in closed set condition.", "published": "2020-01-31 07:45:03", "link": "http://arxiv.org/abs/2001.11688v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Training Keyword Spotters with Limited and Synthesized Speech Data", "abstract": "With the rise of low power speech-enabled devices, there is a growing demand\nto quickly produce models for recognizing arbitrary sets of keywords. As with\nmany machine learning tasks, one of the most challenging parts in the model\ncreation process is obtaining a sufficient amount of training data. In this\npaper, we explore the effectiveness of synthesized speech data in training\nsmall, spoken term detection models of around 400k parameters. Instead of\ntraining such models directly on the audio or low level features such as MFCCs,\nwe use a pre-trained speech embedding model trained to extract useful features\nfor keyword spotting models. Using this speech embedding, we show that a model\nwhich detects 10 keywords when trained on only synthetic speech is equivalent\nto a model trained on over 500 real examples. We also show that a model without\nour speech embeddings would need to be trained on over 4000 real examples to\nreach the same accuracy.", "published": "2020-01-31 07:50:42", "link": "http://arxiv.org/abs/2002.01322v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Detecting Emotion Primitives from Speech and their use in discerning\n  Categorical Emotions", "abstract": "Emotion plays an essential role in human-to-human communication, enabling us\nto convey feelings such as happiness, frustration, and sincerity. While modern\nspeech technologies rely heavily on speech recognition and natural language\nunderstanding for speech content understanding, the investigation of vocal\nexpression is increasingly gaining attention. Key considerations for building\nrobust emotion models include characterizing and improving the extent to which\na model, given its training data distribution, is able to generalize to unseen\ndata conditions. This work investigated a long-shot-term memory (LSTM) network\nand a time convolution - LSTM (TC-LSTM) to detect primitive emotion attributes\nsuch as valence, arousal, and dominance, from speech. It was observed that\ntraining with multiple datasets and using robust features improved the\nconcordance correlation coefficient (CCC) for valence, by 30\\% with respect to\nthe baseline system. Additionally, this work investigated how emotion\nprimitives can be used to detect categorical emotions such as happiness,\ndisgust, contempt, anger, and surprise from neutral speech, and results\nindicated that arousal, followed by dominance was a better detector of such\nemotions.", "published": "2020-01-31 03:11:24", "link": "http://arxiv.org/abs/2002.01323v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
