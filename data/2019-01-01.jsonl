{"title": "Improving Tree-LSTM with Tree Attention", "abstract": "In Natural Language Processing (NLP), we often need to extract information\nfrom tree topology. Sentence structure can be represented via a dependency tree\nor a constituency tree structure. For this reason, a variant of LSTMs, named\nTree-LSTM, was proposed to work on tree topology. In this paper, we design a\ngeneralized attention framework for both dependency and constituency trees by\nencoding variants of decomposable attention inside a Tree-LSTM cell. We\nevaluated our models on a semantic relatedness task and achieved notable\nresults compared to Tree-LSTM based methods with no attention as well as other\nneural and non-neural methods and good results compared to Tree-LSTM based\nmethods with attention.", "published": "2019-01-01 00:15:45", "link": "http://arxiv.org/abs/1901.00066v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring spectro-temporal features in end-to-end convolutional neural\n  networks", "abstract": "Triangular, overlapping Mel-scaled filters (\"f-banks\") are the current\nstandard input for acoustic models that exploit their input's time-frequency\ngeometry, because they provide a psycho-acoustically motivated time-frequency\ngeometry for a speech signal. F-bank coefficients are provably robust to small\ndeformations in the scale. In this paper, we explore two ways in which filter\nbanks can be adjusted for the purposes of speech recognition. First, triangular\nfilters can be replaced with Gabor filters, a compactly supported filter that\nbetter localizes events in time, or Gammatone filters, a\npsychoacoustically-motivated filter. Second, by rearranging the order of\noperations in computing filter bank features, features can be integrated over\nsmaller time scales while simultaneously providing better frequency resolution.\nWe make all feature implementations available online through open-source\nrepositories. Initial experimentation with a modern end-to-end CNN phone\nrecognizer yielded no significant improvements to phone error rate due to\neither modification. The result, and its ramifications with respect to learned\nfilter banks, is discussed.", "published": "2019-01-01 01:17:26", "link": "http://arxiv.org/abs/1901.00072v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Text Infilling", "abstract": "Recent years have seen remarkable progress of text generation in different\ncontexts, such as the most common setting of generating text from scratch, and\nthe emerging paradigm of retrieval-and-rewriting. Text infilling, which fills\nmissing text portions of a sentence or paragraph, is also of numerous use in\nreal life, yet is under-explored. Previous work has focused on restricted\nsettings by either assuming single word per missing portion or limiting to a\nsingle missing portion to the end of the text. This paper studies the general\ntask of text infilling, where the input text can have an arbitrary number of\nportions to be filled, each of which may require an arbitrary unknown number of\ntokens. We study various approaches for the task, including a self-attention\nmodel with segment-aware position encoding and bidirectional context modeling.\nWe create extensive supervised data by masking out text with varying\nstrategies. Experiments show the self-attention model greatly outperforms\nothers, creating a strong baseline for future research.", "published": "2019-01-01 14:41:17", "link": "http://arxiv.org/abs/1901.00158v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Transfer learning from language models to image caption generators:\n  Better models may not transfer better", "abstract": "When designing a neural caption generator, a convolutional neural network can\nbe used to extract image features. Is it possible to also use a neural language\nmodel to extract sentence prefix features? We answer this question by trying\ndifferent ways to transfer the recurrent neural network and embedding layer\nfrom a neural language model to an image caption generator. We find that image\ncaption generators with transferred parameters perform better than those\ntrained from scratch, even when simply pre-training them on the text of the\nsame captions dataset it will later be trained on. We also find that the best\nlanguage models (in terms of perplexity) do not result in the best caption\ngenerators after transfer learning.", "published": "2019-01-01 20:23:40", "link": "http://arxiv.org/abs/1901.01216v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
