{"title": "IERL: Interpretable Ensemble Representation Learning -- Combining\n  CrowdSourced Knowledge and Distributed Semantic Representations", "abstract": "Large Language Models (LLMs) encode meanings of words in the form of\ndistributed semantics. Distributed semantics capture common statistical\npatterns among language tokens (words, phrases, and sentences) from large\namounts of data. LLMs perform exceedingly well across General Language\nUnderstanding Evaluation (GLUE) tasks designed to test a model's understanding\nof the meanings of the input tokens. However, recent studies have shown that\nLLMs tend to generate unintended, inconsistent, or wrong texts as outputs when\nprocessing inputs that were seen rarely during training, or inputs that are\nassociated with diverse contexts (e.g., well-known hallucination phenomenon in\nlanguage generation tasks). Crowdsourced and expert-curated knowledge graphs\nsuch as ConceptNet are designed to capture the meaning of words from a compact\nset of well-defined contexts. Thus LLMs may benefit from leveraging such\nknowledge contexts to reduce inconsistencies in outputs. We propose a novel\nensemble learning method, Interpretable Ensemble Representation Learning\n(IERL), that systematically combines LLM and crowdsourced knowledge\nrepresentations of input tokens. IERL has the distinct advantage of being\ninterpretable by design (when was the LLM context used vs. when was the\nknowledge context used?) over state-of-the-art (SOTA) methods, allowing\nscrutiny of the inputs in conjunction with the parameters of the model,\nfacilitating the analysis of models' inconsistent or irrelevant outputs.\nAlthough IERL is agnostic to the choice of LLM and crowdsourced knowledge, we\ndemonstrate our approach using BERT and ConceptNet. We report improved or\ncompetitive results with IERL across GLUE tasks over current SOTA methods and\nsignificantly enhanced model interpretability.", "published": "2023-06-24 05:02:34", "link": "http://arxiv.org/abs/2306.13865v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Estimating the Causal Effect of Early ArXiving on Paper Acceptance", "abstract": "What is the effect of releasing a preprint of a paper before it is submitted\nfor peer review? No randomized controlled trial has been conducted, so we turn\nto observational data to answer this question. We use data from the ICLR\nconference (2018--2022) and apply methods from causal inference to estimate the\neffect of arXiving a paper before the reviewing period (early arXiving) on its\nacceptance to the conference. Adjusting for confounders such as topic, authors,\nand quality, we may estimate the causal effect. However, since quality is a\nchallenging construct to estimate, we use the negative outcome control method,\nusing paper citation count as a control variable to debias the quality\nconfounding effect. Our results suggest that early arXiving may have a small\neffect on a paper's chances of acceptance. However, this effect (when existing)\ndoes not differ significantly across different groups of authors, as grouped by\nauthor citation count and institute rank. This suggests that early arXiving\ndoes not provide an advantage to any particular group.", "published": "2023-06-24 07:45:38", "link": "http://arxiv.org/abs/2306.13891v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can GPT-4 Support Analysis of Textual Data in Tasks Requiring Highly\n  Specialized Domain Expertise?", "abstract": "We evaluated the capability of generative pre-trained transformers~(GPT-4) in\nanalysis of textual data in tasks that require highly specialized domain\nexpertise. Specifically, we focused on the task of analyzing court opinions to\ninterpret legal concepts. We found that GPT-4, prompted with annotation\nguidelines, performs on par with well-trained law student annotators. We\nobserved that, with a relatively minor decrease in performance, GPT-4 can\nperform batch predictions leading to significant cost reductions. However,\nemploying chain-of-thought prompting did not lead to noticeably improved\nperformance on this task. Further, we demonstrated how to analyze GPT-4's\npredictions to identify and mitigate deficiencies in annotation guidelines, and\nsubsequently improve the performance of the model. Finally, we observed that\nthe model is quite brittle, as small formatting related changes in the prompt\nhad a high impact on the predictions. These findings can be leveraged by\nresearchers and practitioners who engage in semantic/pragmatic annotations of\ntexts in the context of the tasks requiring highly specialized domain\nexpertise.", "published": "2023-06-24 08:48:24", "link": "http://arxiv.org/abs/2306.13906v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Mapping of Arguments of Deverbal Nouns to Their\n  Corresponding Verbal Labels", "abstract": "Deverbal nouns are nominal forms of verbs commonly used in written English\ntexts to describe events or actions, as well as their arguments. However, many\nNLP systems, and in particular pattern-based ones, neglect to handle such\nnominalized constructions. The solutions that do exist for handling arguments\nof nominalized constructions are based on semantic annotation and require\nsemantic ontologies, making their applications restricted to a small set of\nnouns. We propose to adopt instead a more syntactic approach, which maps the\narguments of deverbal nouns to the universal-dependency relations of the\ncorresponding verbal construction. We present an unsupervised mechanism --\nbased on contextualized word representations -- which allows to enrich\nuniversal-dependency trees with dependency arcs denoting arguments of deverbal\nnouns, using the same labels as the corresponding verbal cases. By sharing the\nsame label set as in the verbal case, patterns that were developed for verbs\ncan be applied without modification but with high accuracy also to the nominal\nconstructions.", "published": "2023-06-24 10:07:01", "link": "http://arxiv.org/abs/2306.13922v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Robust Aspect-based Sentiment Analysis through\n  Non-counterfactual Augmentations", "abstract": "While state-of-the-art NLP models have demonstrated excellent performance for\naspect based sentiment analysis (ABSA), substantial evidence has been presented\non their lack of robustness. This is especially manifested as significant\ndegradation in performance when faced with out-of-distribution data. Recent\nsolutions that rely on counterfactually augmented datasets show promising\nresults, but they are inherently limited because of the lack of access to\nexplicit causal structure. In this paper, we present an alternative approach\nthat relies on non-counterfactual data augmentation. Our proposal instead\nrelies on using noisy, cost-efficient data augmentations that preserve\nsemantics associated with the target aspect. Our approach then relies on\nmodelling invariances between different versions of the data to improve\nrobustness. A comprehensive suite of experiments shows that our proposal\nsignificantly improves upon strong pre-trained baselines on both standard and\nrobustness-specific datasets. Our approach further establishes a new\nstate-of-the-art on the ABSA robustness benchmark and transfers well across\ndomains.", "published": "2023-06-24 13:57:32", "link": "http://arxiv.org/abs/2306.13971v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models as Sous Chefs: Revising Recipes with GPT-3", "abstract": "With their remarkably improved text generation and prompting capabilities,\nlarge language models can adapt existing written information into forms that\nare easier to use and understand. In our work, we focus on recipes as an\nexample of complex, diverse, and widely used instructions. We develop a prompt\ngrounded in the original recipe and ingredients list that breaks recipes down\ninto simpler steps. We apply this prompt to recipes from various world\ncuisines, and experiment with several large language models (LLMs), finding\nbest results with GPT-3.5. We also contribute an Amazon Mechanical Turk task\nthat is carefully designed to reduce fatigue while collecting human judgment of\nthe quality of recipe revisions. We find that annotators usually prefer the\nrevision over the original, demonstrating a promising application of LLMs in\nserving as digital sous chefs for recipes and beyond. We release our prompt,\ncode, and MTurk template for public use.", "published": "2023-06-24 14:42:43", "link": "http://arxiv.org/abs/2306.13986v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Symbolic Chain-of-Thought Distillation: Small Models Can Also \"Think\"\n  Step-by-Step", "abstract": "Chain-of-thought prompting (e.g., \"Let's think step-by-step\") primes large\nlanguage models to verbalize rationalization for their predictions. While\nchain-of-thought can lead to dramatic performance gains, benefits appear to\nemerge only for sufficiently large models (beyond 50B parameters). We show that\norders-of-magnitude smaller models (125M -- 1.3B parameters) can still benefit\nfrom chain-of-thought prompting. To achieve this, we introduce Symbolic\nChain-of-Thought Distillation (SCoTD), a method to train a smaller student\nmodel on rationalizations sampled from a significantly larger teacher model.\nExperiments across several commonsense benchmarks show that: 1) SCoTD enhances\nthe performance of the student model in both supervised and few-shot settings,\nand especially for challenge sets; 2) sampling many reasoning chains per\ninstance from the teacher is paramount; and 3) after distillation, student\nchain-of-thoughts are judged by humans as comparable to the teacher, despite\norders of magnitude fewer parameters. We test several hypotheses regarding what\nproperties of chain-of-thought samples are important, e.g., diversity vs.\nteacher likelihood vs. open-endedness. We release our corpus of\nchain-of-thought samples and code.", "published": "2023-06-24 20:15:07", "link": "http://arxiv.org/abs/2306.14050v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UAlberta at SemEval-2023 Task 1: Context Augmentation and Translation\n  for Multilingual Visual Word Sense Disambiguation", "abstract": "We describe the systems of the University of Alberta team for the\nSemEval-2023 Visual Word Sense Disambiguation (V-WSD) Task. We present a novel\nalgorithm that leverages glosses retrieved from BabelNet, in combination with\ntext and image encoders. Furthermore, we compare language-specific encoders\nagainst the application of English encoders to translated texts. As the\ncontexts given in the task datasets are extremely short, we also experiment\nwith augmenting these contexts with descriptions generated by a language model.\nThis yields substantial improvements in accuracy. We describe and evaluate\nadditional V-WSD methods which use image generation and text-conditioned image\nsegmentation. Overall, the results of our official submission rank us 18 out of\n56 teams. Some of our unofficial results are even better than the official\nones. Our code is publicly available at https://github.com/UAlberta-NLP/v-wsd.", "published": "2023-06-24 22:00:06", "link": "http://arxiv.org/abs/2306.14067v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "L3Cube-MahaSent-MD: A Multi-domain Marathi Sentiment Analysis Dataset\n  and Transformer Models", "abstract": "The exploration of sentiment analysis in low-resource languages, such as\nMarathi, has been limited due to the availability of suitable datasets. In this\nwork, we present L3Cube-MahaSent-MD, a multi-domain Marathi sentiment analysis\ndataset, with four different domains - movie reviews, general tweets, TV show\nsubtitles, and political tweets. The dataset consists of around 60,000 manually\ntagged samples covering 3 distinct sentiments - positive, negative, and\nneutral. We create a sub-dataset for each domain comprising 15k samples. The\nMahaSent-MD is the first comprehensive multi-domain sentiment analysis dataset\nwithin the Indic sentiment landscape. We fine-tune different monolingual and\nmultilingual BERT models on these datasets and report the best accuracy with\nthe MahaBERT model. We also present an extensive in-domain and cross-domain\nanalysis thus highlighting the need for low-resource multi-domain datasets. The\ndata and models are available at https://github.com/l3cube-pune/MarathiNLP .", "published": "2023-06-24 07:27:53", "link": "http://arxiv.org/abs/2306.13888v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Math Word Problem Solving by Generating Linguistic Variants of Problem\n  Statements", "abstract": "The art of mathematical reasoning stands as a fundamental pillar of\nintellectual progress and is a central catalyst in cultivating human ingenuity.\nResearchers have recently published a plethora of works centered around the\ntask of solving Math Word Problems (MWP) $-$ a crucial stride towards general\nAI. These existing models are susceptible to dependency on shallow heuristics\nand spurious correlations to derive the solution expressions. In order to\nameliorate this issue, in this paper, we propose a framework for MWP solvers\nbased on the generation of linguistic variants of the problem text. The\napproach involves solving each of the variant problems and electing the\npredicted expression with the majority of the votes. We use DeBERTa\n(Decoding-enhanced BERT with disentangled attention) as the encoder to leverage\nits rich textual representations and enhanced mask decoder to construct the\nsolution expressions. Furthermore, we introduce a challenging dataset,\n$\\mathrm{P\\small{ARA}\\normalsize{MAWPS}}$, consisting of paraphrased,\nadversarial, and inverse variants of selectively sampled MWPs from the\nbenchmark $\\mathrm{M\\small{AWPS}}$ dataset. We extensively experiment on this\ndataset along with other benchmark datasets using some baseline MWP solver\nmodels. We show that training on linguistic variants of problem statements and\nvoting on candidate predictions improve the mathematical reasoning and\nrobustness of the model. We make our code and data publicly available.", "published": "2023-06-24 08:27:39", "link": "http://arxiv.org/abs/2306.13899v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Comparison of Pre-trained Language Models for Turkish Address Parsing", "abstract": "Transformer based pre-trained models such as BERT and its variants, which are\ntrained on large corpora, have demonstrated tremendous success for natural\nlanguage processing (NLP) tasks. Most of academic works are based on the\nEnglish language; however, the number of multilingual and language specific\nstudies increase steadily. Furthermore, several studies claimed that language\nspecific models outperform multilingual models in various tasks. Therefore, the\ncommunity tends to train or fine-tune the models for the language of their case\nstudy, specifically. In this paper, we focus on Turkish maps data and\nthoroughly evaluate both multilingual and Turkish based BERT, DistilBERT,\nELECTRA and RoBERTa. Besides, we also propose a MultiLayer Perceptron (MLP) for\nfine-tuning BERT in addition to the standard approach of one-layer fine-tuning.\nFor the dataset, a mid-sized Address Parsing corpus taken with a relatively\nhigh quality is constructed. Conducted experiments on this dataset indicate\nthat Turkish language specific models with MLP fine-tuning yields slightly\nbetter results when compared to the multilingual fine-tuned models. Moreover,\nvisualization of address tokens' representations further indicates the\neffectiveness of BERT variants for classifying a variety of addresses.", "published": "2023-06-24 12:09:43", "link": "http://arxiv.org/abs/2306.13947v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Characterizing the Emotion Carriers of COVID-19 Misinformation and Their\n  Impact on Vaccination Outcomes in India and the United States", "abstract": "The COVID-19 Infodemic had an unprecedented impact on health behaviors and\noutcomes at a global scale. While many studies have focused on a qualitative\nand quantitative understanding of misinformation, including sentiment analysis,\nthere is a gap in understanding the emotion-carriers of misinformation and\ntheir differences across geographies. In this study, we characterized emotion\ncarriers and their impact on vaccination rates in India and the United States.\nA manually labelled dataset was created from 2.3 million tweets and collated\nwith three publicly available datasets (CoAID, AntiVax, CMU) to train deep\nlearning models for misinformation classification. Misinformation labelled\ntweets were further analyzed for behavioral aspects by leveraging Plutchik\nTransformers to determine the emotion for each tweet. Time series analysis was\nconducted to study the impact of misinformation on spatial and temporal\ncharacteristics. Further, categorical classification was performed using\ntransformer models to assign categories for the misinformation tweets.\nWord2Vec+BiLSTM was the best model for misinformation classification, with an\nF1-score of 0.92. The US had the highest proportion of misinformation tweets\n(58.02%), followed by the UK (10.38%) and India (7.33%). Disgust, anticipation,\nand anger were associated with an increased prevalence of misinformation\ntweets. Disgust was the predominant emotion associated with misinformation\ntweets in the US, while anticipation was the predominant emotion in India. For\nIndia, the misinformation rate exhibited a lead relationship with vaccination,\nwhile in the US it lagged behind vaccination. Our study deciphered that\nemotions acted as differential carriers of misinformation across geography and\ntime. These carriers can be monitored to develop strategic interventions for\ncountering misinformation, leading to improved public health.", "published": "2023-06-24 12:56:56", "link": "http://arxiv.org/abs/2306.13954v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Emotion Flip Reasoning in Multiparty Conversations", "abstract": "In a conversational dialogue, speakers may have different emotional states\nand their dynamics play an important role in understanding dialogue's emotional\ndiscourse. However, simply detecting emotions is not sufficient to entirely\ncomprehend the speaker-specific changes in emotion that occur during a\nconversation. To understand the emotional dynamics of speakers in an efficient\nmanner, it is imperative to identify the rationale or instigator behind any\nchanges or flips in emotion expressed by the speaker. In this paper, we explore\nthe task called Instigator based Emotion Flip Reasoning (EFR), which aims to\nidentify the instigator behind a speaker's emotion flip within a conversation.\nFor example, an emotion flip from joy to anger could be caused by an instigator\nlike threat. To facilitate this task, we present MELD-I, a dataset that\nincludes ground-truth EFR instigator labels, which are in line with emotional\npsychology. To evaluate the dataset, we propose a novel neural architecture\ncalled TGIF, which leverages Transformer encoders and stacked GRUs to capture\nthe dialogue context, speaker dynamics, and emotion sequence in a conversation.\nOur evaluation demonstrates state-of-the-art performance (+4-12% increase in\nF1-score) against five baselines used for the task. Further, we establish the\ngeneralizability of TGIF on an unseen dataset in a zero-shot setting.\nAdditionally, we provide a detailed analysis of the competing models,\nhighlighting the advantages and limitations of our neural architecture.", "published": "2023-06-24 13:22:02", "link": "http://arxiv.org/abs/2306.13959v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fusing Multimodal Signals on Hyper-complex Space for Extreme Abstractive\n  Text Summarization (TL;DR) of Scientific Contents", "abstract": "The realm of scientific text summarization has experienced remarkable\nprogress due to the availability of annotated brief summaries and ample data.\nHowever, the utilization of multiple input modalities, such as videos and\naudio, has yet to be thoroughly explored. At present, scientific\nmultimodal-input-based text summarization systems tend to employ longer target\nsummaries like abstracts, leading to an underwhelming performance in the task\nof text summarization.\n  In this paper, we deal with a novel task of extreme abstractive text\nsummarization (aka TL;DR generation) by leveraging multiple input modalities.\nTo this end, we introduce mTLDR, a first-of-its-kind dataset for the\naforementioned task, comprising videos, audio, and text, along with both\nauthor-composed summaries and expert-annotated summaries. The mTLDR dataset\naccompanies a total of 4,182 instances collected from various academic\nconference proceedings, such as ICLR, ACL, and CVPR. Subsequently, we present\nmTLDRgen, an encoder-decoder-based model that employs a novel dual-fused\nhyper-complex Transformer combined with a Wasserstein Riemannian Encoder\nTransformer, to dexterously capture the intricacies between different\nmodalities in a hyper-complex latent geometric space. The hyper-complex\nTransformer captures the intrinsic properties between the modalities, while the\nWasserstein Riemannian Encoder Transformer captures the latent structure of the\nmodalities in the latent space geometry, thereby enabling the model to produce\ndiverse sentences. mTLDRgen outperforms 20 baselines on mTLDR as well as\nanother non-scientific dataset (How2) across three Rouge-based evaluation\nmeasures. Furthermore, based on the qualitative metrics, BERTScore and FEQA,\nand human evaluations, we demonstrate that the summaries generated by mTLDRgen\nare fluent and congruent to the original source material.", "published": "2023-06-24 13:51:42", "link": "http://arxiv.org/abs/2306.13968v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models\n  and Evaluation Benchmarks", "abstract": "The research on code-mixed data is limited due to the unavailability of\ndedicated code-mixed datasets and pre-trained language models. In this work, we\nfocus on the low-resource Indian language Marathi which lacks any prior work in\ncode-mixing. We present L3Cube-MeCorpus, a large code-mixed Marathi-English\n(Mr-En) corpus with 10 million social media sentences for pretraining. We also\nrelease L3Cube-MeBERT and MeRoBERTa, code-mixed BERT-based transformer models\npre-trained on MeCorpus. Furthermore, for benchmarking, we present three\nsupervised datasets MeHate, MeSent, and MeLID for downstream tasks like\ncode-mixed Mr-En hate speech detection, sentiment analysis, and language\nidentification respectively. These evaluation datasets individually consist of\nmanually annotated \\url{~}12,000 Marathi-English code-mixed tweets. Ablations\nshow that the models trained on this novel corpus significantly outperform the\nexisting state-of-the-art BERT models. This is the first work that presents\nartifacts for code-mixed Marathi research. All datasets and models are publicly\nreleased at https://github.com/l3cube-pune/MarathiNLP .", "published": "2023-06-24 18:17:38", "link": "http://arxiv.org/abs/2306.14030v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Weighted Automata Extraction and Explanation of Recurrent Neural\n  Networks for Natural Language Tasks", "abstract": "Recurrent Neural Networks (RNNs) have achieved tremendous success in\nprocessing sequential data, yet understanding and analyzing their behaviours\nremains a significant challenge. To this end, many efforts have been made to\nextract finite automata from RNNs, which are more amenable for analysis and\nexplanation. However, existing approaches like exact learning and compositional\napproaches for model extraction have limitations in either scalability or\nprecision. In this paper, we propose a novel framework of Weighted Finite\nAutomata (WFA) extraction and explanation to tackle the limitations for natural\nlanguage tasks. First, to address the transition sparsity and context loss\nproblems we identified in WFA extraction for natural language tasks, we propose\nan empirical method to complement missing rules in the transition diagram, and\nadjust transition matrices to enhance the context-awareness of the WFA. We also\npropose two data augmentation tactics to track more dynamic behaviours of RNN,\nwhich further allows us to improve the extraction precision. Based on the\nextracted model, we propose an explanation method for RNNs including a word\nembedding method -- Transition Matrix Embeddings (TME) and TME-based task\noriented explanation for the target RNN. Our evaluation demonstrates the\nadvantage of our method in extraction precision than existing approaches, and\nthe effectiveness of TME-based explanation method in applications to\npretraining and adversarial example generation.", "published": "2023-06-24 19:16:56", "link": "http://arxiv.org/abs/2306.14040v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond Scale: The Diversity Coefficient as a Data Quality Metric for\n  Variability in Natural Language Data", "abstract": "Current trends in pre-training Large Language Models (LLMs) primarily focus\non the scaling of model and dataset size. While the quality of pre-training\ndata is considered an important factor for training powerful LLMs, it remains a\nnebulous concept that has not been rigorously characterized. To this end, we\npropose a formalization of one key aspect of data quality -- measuring the\nvariability of natural language data -- specifically via a measure we call the\ndiversity coefficient. Our empirical analysis shows that the proposed diversity\ncoefficient aligns with the intuitive properties of diversity and variability,\ne.g., it increases as the number of latent concepts increases. Then, we measure\nthe diversity coefficient of publicly available pre-training datasets and\ndemonstrate that their formal diversity is high compared to theoretical lower\nand upper bounds. Finally, we conduct a comprehensive set of controlled\ninterventional experiments with GPT-2 and LLaMAv2 that demonstrate the\ndiversity coefficient of pre-training data characterizes useful aspects of\ndownstream model evaluation performance -- totaling 44 models of various sizes\n(51M to 7B parameters). We conclude that our formal notion of diversity is an\nimportant aspect of data quality that captures variability and causally leads\nto improved evaluation performance.", "published": "2023-06-24 02:25:56", "link": "http://arxiv.org/abs/2306.13840v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Is Pre-training Truly Better Than Meta-Learning?", "abstract": "In the context of few-shot learning, it is currently believed that a fixed\npre-trained (PT) model, along with fine-tuning the final layer during\nevaluation, outperforms standard meta-learning algorithms. We re-evaluate these\nclaims under an in-depth empirical examination of an extensive set of formally\ndiverse datasets and compare PT to Model Agnostic Meta-Learning (MAML). Unlike\nprevious work, we emphasize a fair comparison by using: the same architecture,\nthe same optimizer, and all models trained to convergence. Crucially, we use a\nmore rigorous statistical tool -- the effect size (Cohen's d) -- to determine\nthe practical significance of the difference between a model trained with PT\nvs. a MAML. We then use a previously proposed metric -- the diversity\ncoefficient -- to compute the average formal diversity of a dataset. Using this\nanalysis, we demonstrate the following: 1. when the formal diversity of a data\nset is low, PT beats MAML on average and 2. when the formal diversity is high,\nMAML beats PT on average. The caveat is that the magnitude of the average\ndifference between a PT vs. MAML using the effect size is low (according to\nclassical statistical thresholds) -- less than 0.2. Nevertheless, this\nobservation is contrary to the currently held belief that a pre-trained model\nis always better than a meta-learning model. Our extensive experiments consider\n21 few-shot learning benchmarks, including the large-scale few-shot learning\ndataset Meta-Data set. We also show no significant difference between a MAML\nmodel vs. a PT model with GPT-2 on Openwebtext. We, therefore, conclude that a\npre-trained model does not always beat a meta-learned model and that the formal\ndiversity of a dataset is a driving factor.", "published": "2023-06-24 02:26:45", "link": "http://arxiv.org/abs/2306.13841v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Spatio-temporal Storytelling? Leveraging Generative Models for Semantic\n  Trajectory Analysis", "abstract": "In this paper, we lay out a vision for analysing semantic trajectory traces\nand generating synthetic semantic trajectory data (SSTs) using generative\nlanguage model. Leveraging the advancements in deep learning, as evident by\nprogress in the field of natural language processing (NLP), computer vision,\netc. we intend to create intelligent models that can study the semantic\ntrajectories in various contexts, predicting future trends, increasing machine\nunderstanding of the movement of animals, humans, goods, etc. enhancing\nhuman-computer interactions, and contributing to an array of applications\nranging from urban-planning to personalized recommendation engines and business\nstrategy.", "published": "2023-06-24 08:45:47", "link": "http://arxiv.org/abs/2306.13905v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Weakly Supervised Multi-Label Classification of Full-Text Scientific\n  Papers", "abstract": "Instead of relying on human-annotated training samples to build a classifier,\nweakly supervised scientific paper classification aims to classify papers only\nusing category descriptions (e.g., category names, category-indicative\nkeywords). Existing studies on weakly supervised paper classification are less\nconcerned with two challenges: (1) Papers should be classified into not only\ncoarse-grained research topics but also fine-grained themes, and potentially\ninto multiple themes, given a large and fine-grained label space; and (2) full\ntext should be utilized to complement the paper title and abstract for\nclassification. Moreover, instead of viewing the entire paper as a long linear\nsequence, one should exploit the structural information such as citation links\nacross papers and the hierarchy of sections and paragraphs in each paper. To\ntackle these challenges, in this study, we propose FUTEX, a framework that uses\nthe cross-paper network structure and the in-paper hierarchy structure to\nclassify full-text scientific papers under weak supervision. A network-aware\ncontrastive fine-tuning module and a hierarchy-aware aggregation module are\ndesigned to leverage the two types of structural signals, respectively.\nExperiments on two benchmark datasets demonstrate that FUTEX significantly\noutperforms competitive baselines and is on par with fully supervised\nclassifiers that use 1,000 to 60,000 ground-truth training samples.", "published": "2023-06-24 15:27:55", "link": "http://arxiv.org/abs/2306.14003v1", "categories": ["cs.CL", "cs.DL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DesCo: Learning Object Recognition with Rich Language Descriptions", "abstract": "Recent development in vision-language approaches has instigated a paradigm\nshift in learning visual recognition models from language supervision. These\napproaches align objects with language queries (e.g. \"a photo of a cat\") and\nimprove the models' adaptability to identify novel objects and domains.\nRecently, several studies have attempted to query these models with complex\nlanguage expressions that include specifications of fine-grained semantic\ndetails, such as attributes, shapes, textures, and relations. However, simply\nincorporating language descriptions as queries does not guarantee accurate\ninterpretation by the models. In fact, our experiments show that GLIP, the\nstate-of-the-art vision-language model for object detection, often disregards\ncontextual information in the language descriptions and instead relies heavily\non detecting objects solely by their names. To tackle the challenges, we\npropose a new description-conditioned (DesCo) paradigm of learning object\nrecognition models with rich language descriptions consisting of two major\ninnovations: 1) we employ a large language model as a commonsense knowledge\nengine to generate rich language descriptions of objects based on object names\nand the raw image-text caption; 2) we design context-sensitive queries to\nimprove the model's ability in deciphering intricate nuances embedded within\ndescriptions and enforce the model to focus on context rather than object names\nalone. On two novel object detection benchmarks, LVIS and OminiLabel, under the\nzero-shot detection setting, our approach achieves 34.8 APr minival (+9.1) and\n29.3 AP (+3.6), respectively, surpassing the prior state-of-the-art models,\nGLIP and FIBER, by a large margin.", "published": "2023-06-24 21:05:02", "link": "http://arxiv.org/abs/2306.14060v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Improving End-to-End Neural Diarization Using Conversational Summary\n  Representations", "abstract": "Speaker diarization is a task concerned with partitioning an audio recording\nby speaker identity. End-to-end neural diarization with encoder-decoder based\nattractor calculation (EEND-EDA) aims to solve this problem by directly\noutputting diarization results for a flexible number of speakers. Currently,\nthe EDA module responsible for generating speaker-wise attractors is\nconditioned on zero vectors providing no relevant information to the network.\nIn this work, we extend EEND-EDA by replacing the input zero vectors to the\ndecoder with learned conversational summary representations. The updated EDA\nmodule sequentially generates speaker-wise attractors based on utterance-level\ninformation. We propose three methods to initialize the summary vector and\nconduct an investigation into varying input recording lengths. On a range of\npublicly available test sets, our model achieves an absolute DER performance\nimprovement of 1.90 % when compared to the baseline.", "published": "2023-06-24 04:43:26", "link": "http://arxiv.org/abs/2306.13863v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Analysis of Personalized Speech Recognition System Development for\n  the Deaf and Hard-of-Hearing", "abstract": "Deaf or hard-of-hearing (DHH) speakers typically have atypical speech caused\nby deafness. With the growing support of speech-based devices and software\napplications, more work needs to be done to make these devices inclusive to\neveryone. To do so, we analyze the use of openly-available automatic speech\nrecognition (ASR) tools with a DHH Japanese speaker dataset. As these\nout-of-the-box ASR models typically do not perform well on DHH speech, we\nprovide a thorough analysis of creating personalized ASR systems. We collected\na large DHH speaker dataset of four speakers totaling around 28.05 hours and\nthoroughly analyzed the performance of different training frameworks by varying\nthe training data sizes. Our findings show that 1000 utterances (or 1-2 hours)\nfrom a target speaker can already significantly improve the model performance\nwith minimal amount of work needed, thus we recommend researchers to collect at\nleast 1000 utterances to make an efficient personalized ASR system. In cases\nwhere 1000 utterances is difficult to collect, we also discover significant\nimprovements in using previously proposed data augmentation techniques such as\nintermediate fine-tuning when only 200 utterances are available.", "published": "2023-06-24 12:49:49", "link": "http://arxiv.org/abs/2306.13953v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
