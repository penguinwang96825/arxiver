{"title": "T3: Tree-Autoencoder Constrained Adversarial Text Generation for\n  Targeted Attack", "abstract": "Adversarial attacks against natural language processing systems, which\nperform seemingly innocuous modifications to inputs, can induce arbitrary\nmistakes to the target models. Though raised great concerns, such adversarial\nattacks can be leveraged to estimate the robustness of NLP models. Compared\nwith the adversarial example generation in continuous data domain (e.g.,\nimage), generating adversarial text that preserves the original meaning is\nchallenging since the text space is discrete and non-differentiable. To handle\nthese challenges, we propose a target-controllable adversarial attack framework\nT3, which is applicable to a range of NLP tasks. In particular, we propose a\ntree-based autoencoder to embed the discrete text data into a continuous\nrepresentation space, upon which we optimize the adversarial perturbation. A\nnovel tree-based decoder is then applied to regularize the syntactic\ncorrectness of the generated text and manipulate it on either sentence\n(T3(Sent)) or word (T3(Word)) level. We consider two most representative NLP\ntasks: sentiment analysis and question answering (QA). Extensive experimental\nresults and human studies show that T3 generated adversarial texts can\nsuccessfully manipulate the NLP models to output the targeted incorrect answer\nwithout misleading the human. Moreover, we show that the generated adversarial\ntexts have high transferability which enables the black-box attacks in\npractice. Our work sheds light on an effective and general way to examine the\nrobustness of NLP models. Our code is publicly available at\nhttps://github.com/AI-secure/T3/.", "published": "2019-12-22 03:02:42", "link": "http://arxiv.org/abs/1912.10375v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hybrid Machine Learning Models of Classifying Residential Requests for\n  Smart Dispatching", "abstract": "This paper presents a hybrid machine learning method of classifying\nresidential requests in natural language to responsible departments that\nprovide timely responses back to residents under the vision of digital\ngovernment services in smart cities. Residential requests in natural language\ndescriptions cover almost every aspect of a city's daily operation. Hence the\nresponsible departments are fine-grained to even the level of local\ncommunities. There are no specific general categories or labels for each\nrequest sample. This causes two issues for supervised classification solutions,\nnamely (1) the request sample data is unbalanced and (2) lack of specific\nlabels for training. To solve these issues, we investigate a hybrid machine\nlearning method that generates meta-class labels by means of unsupervised\nclustering algorithms; applies two-word embedding methods with three\nclassifiers (including two hierarchical classifiers and one residual\nconvolutional neural network); and selects the best performing classifier as\nthe classification result. We demonstrate our approach performing better\nclassification tasks compared to two benchmarking machine learning models,\nNaive Bayes classifier and a Multiple Layer Perceptron (MLP). In addition, the\nhierarchical classification method provides insights into the source of\nclassification errors.", "published": "2019-12-22 21:47:05", "link": "http://arxiv.org/abs/1912.10546v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Emotion Recognition from Speech", "abstract": "In this work, we conduct an extensive comparison of various approaches to\nspeech based emotion recognition systems. The analyses were carried out on\naudio recordings from Ryerson Audio-Visual Database of Emotional Speech and\nSong (RAVDESS). After pre-processing the raw audio files, features such as\nLog-Mel Spectrogram, Mel-Frequency Cepstral Coefficients (MFCCs), pitch and\nenergy were considered. The significance of these features for emotion\nclassification was compared by applying methods such as Long Short Term Memory\n(LSTM), Convolutional Neural Networks (CNNs), Hidden Markov Models (HMMs) and\nDeep Neural Networks (DNNs). On the 14-class (2 genders x 7 emotions)\nclassification task, an accuracy of 68% was achieved with a 4-layer 2\ndimensional CNN using the Log-Mel Spectrogram features. We also observe that,\nin emotion recognition, the choice of audio features impacts the results much\nmore than the model complexity.", "published": "2019-12-22 14:43:14", "link": "http://arxiv.org/abs/1912.10458v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Tag-less Back-Translation", "abstract": "An effective method to generate a large number of parallel sentences for\ntraining improved neural machine translation (NMT) systems is the use of the\nback-translations of the target-side monolingual data. The standard\nback-translation method has been shown to be unable to efficiently utilize the\navailable huge amount of existing monolingual data because of the inability of\ntranslation models to differentiate between the authentic and synthetic\nparallel data during training. Tagging, or using gates, has been used to enable\ntranslation models to distinguish between synthetic and authentic data,\nimproving standard back-translation and also enabling the use of iterative\nback-translation on language pairs that underperformed using standard\nback-translation. In this work, we approach back-translation as a domain\nadaptation problem, eliminating the need for explicit tagging. In the approach\n-- \\emph{tag-less back-translation} -- the synthetic and authentic parallel\ndata are treated as out-of-domain and in-domain data respectively and, through\npre-training and fine-tuning, the translation model is shown to be able to\nlearn more efficiently from them during training. Experimental results have\nshown that the approach outperforms the standard and tagged back-translation\napproaches on low resource English-Vietnamese and English-German neural machine\ntranslation.", "published": "2019-12-22 19:20:10", "link": "http://arxiv.org/abs/1912.10514v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Harnessing Evolution of Multi-Turn Conversations for Effective Answer\n  Retrieval", "abstract": "With the improvements in speech recognition and voice generation technologies\nover the last years, a lot of companies have sought to develop conversation\nunderstanding systems that run on mobile phones or smart home devices through\nnatural language interfaces. Conversational assistants, such as Google\nAssistant and Microsoft Cortana, can help users to complete various types of\ntasks. This requires an accurate understanding of the user's information need\nas the conversation evolves into multiple turns. Finding relevant context in a\nconversation's history is challenging because of the complexity of natural\nlanguage and the evolution of a user's information need. In this work, we\npresent an extensive analysis of language, relevance, dependency of user\nutterances in a multi-turn information-seeking conversation. To this aim, we\nhave annotated relevant utterances in the conversations released by the TREC\nCaST 2019 track. The annotation labels determine which of the previous\nutterances in a conversation can be used to improve the current one.\nFurthermore, we propose a neural utterance relevance model based on BERT\nfine-tuning, outperforming competitive baselines. We study and compare the\nperformance of multiple retrieval models, utilizing different strategies to\nincorporate the user's context. The experimental results on both classification\nand retrieval tasks show that our proposed approach can effectively identify\nand incorporate the conversation context. We show that processing the current\nutterance using the predicted relevant utterance leads to a 38% relative\nimprovement in terms of nDCG@20. Finally, to foster research in this area, we\nhave released the dataset of the annotations.", "published": "2019-12-22 22:39:04", "link": "http://arxiv.org/abs/1912.10554v2", "categories": ["cs.CL", "cs.HC", "cs.IR"], "primary_category": "cs.CL"}
{"title": "End-Point Detection with State Transition Model based on Chunk-Wise\n  Classification", "abstract": "A state transition model (STM) based on chunk-wise classification was\nproposed for end-point detection (EPD). In general, EPD is developed using\nframe-wise voice activity detection (VAD) with additional STM, in which the\nstate transition is conducted based on VAD's frame-level decision (speech or\nnon-speech). However, VAD errors frequently occur in noisy environments, even\nthough we use state-of-the-art deep neural network based VAD, which causes the\nundesired state transition of STM. In this work, to build robust STM, a state\ntransition is conducted based on chunk-wise classification as EPD does not need\nto be conducted in frame-level. The chunk consists of multiple frames and the\nclassification of chunk between speech and non-speech is done by aggregating\nthe decisions of VAD for multiple frames, so that some undesired VAD errors in\na chunk can be smoothed by other correct VAD decisions. Finally, the model was\nevaluated in both qualitative and quantitative measures including phone error\nrate.", "published": "2019-12-22 13:09:36", "link": "http://arxiv.org/abs/1912.10442v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "power-law nonlinearity with maximally uniform distribution criterion for\n  improved neural network training in automatic speech recognition", "abstract": "In this paper, we describe the Maximum Uniformity of Distribution (MUD)\nalgorithm with the power-law nonlinearity. In this approach, we hypothesize\nthat neural network training will become more stable if feature distribution is\nnot too much skewed. We propose two different types of MUD approaches: power\nfunction-based MUD and histogram-based MUD. In these approaches, we first\nobtain the mel filterbank coefficients and apply nonlinearity functions for\neach filterbank channel. With the power function-based MUD, we apply a\npower-function based nonlinearity where power function coefficients are chosen\nto maximize the likelihood assuming that nonlinearity outputs follow the\nuniform distribution. With the histogram-based MUD, the empirical Cumulative\nDensity Function (CDF) from the training database is employed to transform the\noriginal distribution into a uniform distribution. In MUD processing, we do not\nuse any prior knowledge (e.g. logarithmic relation) about the energy of the\nincoming signal and the perceived intensity by a human. Experimental results\nusing an end-to-end speech recognition system demonstrate that power-function\nbased MUD shows better result than the conventional Mel Filterbank Cepstral\nCoefficients (MFCCs). On the LibriSpeech database, we could achieve 4.02 % WER\non test-clean and 13.34 % WER on test-other without using any Language Models\n(LMs). The major contribution of this work is that we developed a new algorithm\nfor designing the compressive nonlinearity in a data-driven way, which is much\nmore flexible than the previous approaches and may be extended to other domains\nas well.", "published": "2019-12-22 04:40:40", "link": "http://arxiv.org/abs/1912.11041v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "end-to-end training of a large vocabulary end-to-end speech recognition\n  system", "abstract": "In this paper, we present an end-to-end training framework for building\nstate-of-the-art end-to-end speech recognition systems. Our training system\nutilizes a cluster of Central Processing Units(CPUs) and Graphics Processing\nUnits (GPUs). The entire data reading, large scale data augmentation, neural\nnetwork parameter updates are all performed \"on-the-fly\". We use vocal tract\nlength perturbation [1] and an acoustic simulator [2] for data augmentation.\nThe processed features and labels are sent to the GPU cluster. The Horovod\nallreduce approach is employed to train neural network parameters. We evaluated\nthe effectiveness of our system on the standard Librispeech corpus [3] and the\n10,000-hr anonymized Bixby English dataset. Our end-to-end speech recognition\nsystem built using this training infrastructure showed a 2.44 % WER on\ntest-clean of the LibriSpeech test set after applying shallow fusion with a\nTransformer language model (LM). For the proprietary English Bixby open domain\ntest set, we obtained a WER of 7.92 % using a Bidirectional Full Attention\n(BFA) end-to-end model after applying shallow fusion with an RNN-LM. When the\nmonotonic chunckwise attention (MoCha) based approach is employed for streaming\nspeech recognition, we obtained a WER of 9.95 % on the same Bixby open domain\ntest set.", "published": "2019-12-22 02:59:28", "link": "http://arxiv.org/abs/1912.11040v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP", "stat.ML"], "primary_category": "eess.AS"}
