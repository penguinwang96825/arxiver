{"title": "Using Machine Translation to Augment Multilingual Classification", "abstract": "An all-too-present bottleneck for text classification model development is\nthe need to annotate training data and this need is multiplied for multilingual\nclassifiers. Fortunately, contemporary machine translation models are both\neasily accessible and have dependable translation quality, making it possible\nto translate labeled training data from one language into another. Here, we\nexplore the effects of using machine translation to fine-tune a multilingual\nmodel for a classification task across multiple languages. We also investigate\nthe benefits of using a novel technique, originally proposed in the field of\nimage captioning, to account for potential negative effects of tuning models on\ntranslated data. We show that translated data are of sufficient quality to tune\nmultilingual classifiers and that this novel loss technique is able to offer\nsome improvement over models tuned without it.", "published": "2024-05-09 00:31:59", "link": "http://arxiv.org/abs/2405.05478v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Boosting Large Language Models with Continual Learning for Aspect-based\n  Sentiment Analysis", "abstract": "Aspect-based sentiment analysis (ABSA) is an important subtask of sentiment\nanalysis, which aims to extract the aspects and predict their sentiments. Most\nexisting studies focus on improving the performance of the target domain by\nfine-tuning domain-specific models (trained on source domains) based on the\ntarget domain dataset. Few works propose continual learning tasks for ABSA,\nwhich aim to learn the target domain's ability while maintaining the history\ndomains' abilities. In this paper, we propose a Large Language Model-based\nContinual Learning (\\texttt{LLM-CL}) model for ABSA. First, we design a domain\nknowledge decoupling module to learn a domain-invariant adapter and separate\ndomain-variant adapters dependently with an orthogonal constraint. Then, we\nintroduce a domain knowledge warmup strategy to align the representation\nbetween domain-invariant and domain-variant knowledge. In the test phase, we\nindex the corresponding domain-variant knowledge via domain positioning to not\nrequire each sample's domain ID. Extensive experiments over 19 datasets\nindicate that our \\texttt{LLM-CL} model obtains new state-of-the-art\nperformance.", "published": "2024-05-09 02:00:07", "link": "http://arxiv.org/abs/2405.05496v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Care: Assessing the Healthcare Implications of Pre-training Data\n  on Language Model Bias", "abstract": "Large language models (LLMs) are increasingly essential in processing natural\nlanguages, yet their application is frequently compromised by biases and\ninaccuracies originating in their training data. In this study, we introduce\nCross-Care, the first benchmark framework dedicated to assessing biases and\nreal world knowledge in LLMs, specifically focusing on the representation of\ndisease prevalence across diverse demographic groups. We systematically\nevaluate how demographic biases embedded in pre-training corpora like $ThePile$\ninfluence the outputs of LLMs. We expose and quantify discrepancies by\njuxtaposing these biases against actual disease prevalences in various U.S.\ndemographic groups. Our results highlight substantial misalignment between LLM\nrepresentation of disease prevalence and real disease prevalence rates across\ndemographic subgroups, indicating a pronounced risk of bias propagation and a\nlack of real-world grounding for medical applications of LLMs. Furthermore, we\nobserve that various alignment methods minimally resolve inconsistencies in the\nmodels' representation of disease prevalence across different languages. For\nfurther exploration and analysis, we make all data and a data visualization\ntool available at: www.crosscare.net.", "published": "2024-05-09 02:33:14", "link": "http://arxiv.org/abs/2405.05506v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OpenFactCheck: Building, Benchmarking Customized Fact-Checking Systems\n  and Evaluating the Factuality of Claims and LLMs", "abstract": "The increased use of large language models (LLMs) across a variety of\nreal-world applications calls for mechanisms to verify the factual accuracy of\ntheir outputs. Difficulties lie in assessing the factuality of free-form\nresponses in open domains. Also, different papers use disparate evaluation\nbenchmarks and measurements, which renders them hard to compare and hampers\nfuture progress. To mitigate these issues, we propose OpenFactCheck, a unified\nframework for building customized automatic fact-checking systems, benchmarking\ntheir accuracy, evaluating factuality of LLMs, and verifying claims in a\ndocument. OpenFactCheck consists of three modules: (i) CUSTCHECKER allows users\nto easily customize an automatic fact-checker and verify the factual\ncorrectness of documents and claims, (ii) LLMEVAL, a unified evaluation\nframework assesses LLM's factuality ability from various perspectives fairly,\nand (iii) CHECKEREVAL is an extensible solution for gauging the reliability of\nautomatic fact-checkers' verification results using human-annotated datasets.\nData and code are publicly available at\nhttps://github.com/yuxiaw/openfactcheck.", "published": "2024-05-09 07:15:19", "link": "http://arxiv.org/abs/2405.05583v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Dialect Robustness of Language Models via Conversation\n  Understanding", "abstract": "With an evergrowing number of LLMs reporting superlative performance for\nEnglish, their ability to perform equitably for different dialects of English\n($\\textit{i.e.}$, dialect robustness) needs to be ascertained. Specifically, we\nuse English language (US English or Indian English) conversations between\nhumans who play the word-guessing game of 'taboo'. We formulate two evaluative\ntasks: target word prediction (TWP) ($\\textit{i.e.}$, predict the masked target\nword in a conversation) and target word selection (TWS) ($\\textit{i.e.}$,\nselect the most likely masked target word in a conversation, from among a set\nof candidate words). Extending MD3, an existing dialectic dataset of\ntaboo-playing conversations, we introduce M-MD3, a target-word-masked version\nof MD3 with the en-US and en-IN subsets. We create two subsets: en-MV (where\nen-US is transformed to include dialectal information) and en-TR (where\ndialectal information is removed from en-IN). We evaluate one open-source\n(Llama3) and two closed-source (GPT-4/3.5) LLMs. LLMs perform significantly\nbetter for US English than Indian English for both TWP and TWS tasks, for all\nsettings, exhibiting marginalisation against the Indian dialect of English.\nWhile GPT-based models perform the best, the comparatively smaller models work\nmore equitably after fine-tuning. Our error analysis shows that the LLMs can\nunderstand the dialect better after fine-tuning using dialectal data. Our\nevaluation methodology exhibits a novel way to examine attributes of language\nmodels using pre-existing dialogue datasets.", "published": "2024-05-09 11:38:23", "link": "http://arxiv.org/abs/2405.05688v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Statements in Text: A Domain-Agnostic Few-Shot Solution", "abstract": "Many tasks related to Computational Social Science and Web Content Analysis\ninvolve classifying pieces of text based on the claims they contain.\nState-of-the-art approaches usually involve fine-tuning models on large\nannotated datasets, which are costly to produce. In light of this, we propose\nand release a qualitative and versatile few-shot learning methodology as a\ncommon paradigm for any claim-based textual classification task. This\nmethodology involves defining the classes as arbitrarily sophisticated\ntaxonomies of claims, and using Natural Language Inference models to obtain the\ntextual entailment between these and a corpus of interest. The performance of\nthese models is then boosted by annotating a minimal sample of data points,\ndynamically sampled using the well-established statistical heuristic of\nProbabilistic Bisection. We illustrate this methodology in the context of three\ntasks: climate change contrarianism detection, topic/stance classification and\ndepression-relates symptoms detection. This approach rivals traditional\npre-train/fine-tune approaches while drastically reducing the need for data\nannotation.", "published": "2024-05-09 12:03:38", "link": "http://arxiv.org/abs/2405.05705v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Experimental Pragmatics with Machines: Testing LLM Predictions for the\n  Inferences of Plain and Embedded Disjunctions", "abstract": "Human communication is based on a variety of inferences that we draw from\nsentences, often going beyond what is literally said. While there is wide\nagreement on the basic distinction between entailment, implicature, and\npresupposition, the status of many inferences remains controversial. In this\npaper, we focus on three inferences of plain and embedded disjunctions, and\ncompare them with regular scalar implicatures. We investigate this comparison\nfrom the novel perspective of the predictions of state-of-the-art large\nlanguage models, using the same experimental paradigms as recent studies\ninvestigating the same inferences with humans. The results of our best\nperforming models mostly align with those of humans, both in the large\ndifferences we find between those inferences and implicatures, as well as in\nfine-grained distinctions among different aspects of those inferences.", "published": "2024-05-09 13:54:15", "link": "http://arxiv.org/abs/2405.05776v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient LLM Comparative Assessment: a Product of Experts Framework for\n  Pairwise Comparisons", "abstract": "LLM-as-a-judge approaches are a practical and effective way of assessing a\nrange of text tasks. However, when using pairwise comparisons to rank a set of\ncandidates, the computational cost scales quadratically with the number of\ncandidates, which has practical limitations. This paper introduces a Product of\nExpert (PoE) framework for efficient LLM Comparative Assessment. Here\nindividual comparisons are considered experts that provide information on a\npair's score difference. The PoE framework combines the information from these\nexperts to yield an expression that can be maximized with respect to the\nunderlying set of candidates, and is highly flexible where any form of expert\ncan be assumed. When Gaussian experts are used one can derive simple\nclosed-form solutions for the optimal candidate ranking, and expressions for\nselecting which comparisons should be made to maximize the probability of this\nranking. Our approach enables efficient comparative assessment, where by using\nonly a small subset of the possible comparisons, one can generate score\npredictions that correlate well with human judgements. We evaluate the approach\non multiple NLG tasks and demonstrate that our framework can yield considerable\ncomputational savings when performing pairwise comparative assessment. With\nmany candidate texts, using as few as 2% of comparisons the PoE solution can\nachieve similar performance to when all comparisons are used.", "published": "2024-05-09 16:45:27", "link": "http://arxiv.org/abs/2405.05894v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?", "abstract": "When large language models are aligned via supervised fine-tuning, they may\nencounter new factual information that was not acquired through pre-training.\nIt is often conjectured that this can teach the model the behavior of\nhallucinating factually incorrect responses, as the model is trained to\ngenerate facts that are not grounded in its pre-existing knowledge. In this\nwork, we study the impact of such exposure to new knowledge on the capability\nof the fine-tuned model to utilize its pre-existing knowledge. To this end, we\ndesign a controlled setup, focused on closed-book QA, where we vary the\nproportion of the fine-tuning examples that introduce new knowledge. We\ndemonstrate that large language models struggle to acquire new factual\nknowledge through fine-tuning, as fine-tuning examples that introduce new\nknowledge are learned significantly slower than those consistent with the\nmodel's knowledge. However, we also find that as the examples with new\nknowledge are eventually learned, they linearly increase the model's tendency\nto hallucinate. Taken together, our results highlight the risk in introducing\nnew factual knowledge through fine-tuning, and support the view that large\nlanguage models mostly acquire factual knowledge through pre-training, whereas\nfine-tuning teaches them to use it more efficiently.", "published": "2024-05-09 17:00:22", "link": "http://arxiv.org/abs/2405.05904v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DOLOMITES: Domain-Specific Long-Form Methodical Tasks", "abstract": "Experts in various fields routinely perform methodical writing tasks to plan,\norganize, and report their work. From a clinician writing a differential\ndiagnosis for a patient, to a teacher writing a lesson plan for students, these\ntasks are pervasive, requiring to methodically generate structured long-form\noutput for a given input. We develop a typology of methodical tasks structured\nin the form of a task objective, procedure, input, and output, and introduce\nDoLoMiTes, a novel benchmark with specifications for 519 such tasks elicited\nfrom hundreds of experts from across 25 fields. Our benchmark further contains\nspecific instantiations of methodical tasks with concrete input and output\nexamples (1,857 in total) which we obtain by collecting expert revisions of up\nto 10 model-generated examples of each task. We use these examples to evaluate\ncontemporary language models highlighting that automating methodical tasks is a\nchallenging long-form generation problem, as it requires performing complex\ninferences, while drawing upon the given context as well as domain knowledge.", "published": "2024-05-09 17:25:31", "link": "http://arxiv.org/abs/2405.05938v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency\n  for Tool Planning", "abstract": "The emergence of large language models (LLMs) has opened up unprecedented\npossibilities for automating complex tasks that are often comparable to human\nperformance. Despite their capabilities, LLMs still encounter difficulties in\ncompleting tasks that require high levels of accuracy and complexity due to\ntheir inherent limitations in handling multifaceted problems single-handedly.\nThis paper introduces `Smurfs', a cutting-edge multi-agent framework designed\nto revolutionize the application of LLMs. By seamlessly transforming a\nconventional LLM into a synergistic multi-agent ensemble, Smurfs can enhance\nthe model's ability to solve complex tasks at no additional cost. This is\nachieved through innovative prompting strategies that allocate distinct roles\nwithin the model, thereby facilitating collaboration among specialized agents\nand forming an intelligent multi-agent system. Our empirical investigation on\nboth open-ended task of StableToolBench and closed-ended task on HotpotQA\nshowcases Smurfs' superior capability in intricate tool utilization scenarios.\nNotably, Smurfs outmatches all the baseline methods in both experiments,\nsetting new state-of-the-art performance. Furthermore, through comprehensive\nablation studies, we dissect the contribution of the core components of the\nmulti-agent framework to its overall efficacy. This not only verifies the\neffectiveness of the framework, but also sets a route for future exploration of\nmulti-agent LLM systems.", "published": "2024-05-09 17:49:04", "link": "http://arxiv.org/abs/2405.05955v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OpenBA-V2: Reaching 77.3% High Compression Ratio with Fast Multi-Stage\n  Pruning", "abstract": "Large Language Models (LLMs) have played an important role in many fields due\nto their powerful capabilities.However, their massive number of parameters\nleads to high deployment requirements and incurs significant inference costs,\nwhich impedes their practical applications. Training smaller models is an\neffective way to address this problem. Therefore, we introduce OpenBA-V2, a\n3.4B model derived from multi-stage compression and continual pre-training from\nthe original 15B OpenBA model. OpenBA-V2 utilizes more data, more flexible\ntraining objectives, and techniques such as layer pruning, neural pruning, and\nvocabulary pruning to achieve a compression rate of 77.3\\% with minimal\nperformance loss. OpenBA-V2 demonstrates competitive performance compared to\nother open-source models of similar size, achieving results close to or on par\nwith the 15B OpenBA model in downstream tasks such as common sense reasoning\nand Named Entity Recognition (NER). OpenBA-V2 illustrates that LLMs can be\ncompressed into smaller ones with minimal performance loss by employing\nadvanced training objectives and data strategies, which may help deploy LLMs in\nresource-limited scenarios.", "published": "2024-05-09 17:53:28", "link": "http://arxiv.org/abs/2405.05957v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Perplexity Reflect Large Language Model's Ability in Long Text\n  Understanding?", "abstract": "Recent studies have shown that Large Language Models (LLMs) have the\npotential to process extremely long text. Many works only evaluate LLMs'\nlong-text processing ability on the language modeling task, with perplexity\n(PPL) as the evaluation metric. However, in our study, we find that there is no\ncorrelation between PPL and LLMs' long-text understanding ability. Besides, PPL\nmay only reflect the model's ability to model local information instead of\ncatching long-range dependency. Therefore, only using PPL to prove the model\ncould process long text is inappropriate. The local focus feature of PPL could\nalso explain some existing phenomena, such as the great extrapolation ability\nof the position method ALiBi. When evaluating a model's ability in long text,\nwe might pay more attention to PPL's limitation and avoid overly relying on it.", "published": "2024-05-09 21:15:49", "link": "http://arxiv.org/abs/2405.06105v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Fine-Tuning With Adapters", "abstract": "In the arena of language model fine-tuning, the traditional approaches, such\nas Domain-Adaptive Pretraining (DAPT) and Task-Adaptive Pretraining (TAPT),\nalthough effective, but computational intensive. This research introduces a\nnovel adaptation method utilizing the UniPELT framework as a base and added a\nPromptTuning Layer, which significantly reduces the number of trainable\nparameters while maintaining competitive performance across various benchmarks.\nOur method employs adapters, which enable efficient transfer of pretrained\nmodels to new tasks with minimal retraining of the base model parameters. We\nevaluate our approach using three diverse datasets: the GLUE benchmark, a\ndomain-specific dataset comprising four distinct areas, and the Stanford\nQuestion Answering Dataset 1.1 (SQuAD). Our results demonstrate that our\ncustomized adapter-based method achieves performance comparable to full model\nfine-tuning, DAPT+TAPT and UniPELT strategies while requiring fewer or\nequivalent amount of parameters. This parameter efficiency not only alleviates\nthe computational burden but also expedites the adaptation process. The study\nunderlines the potential of adapters in achieving high performance with\nsignificantly reduced resource consumption, suggesting a promising direction\nfor future research in parameter-efficient fine-tuning.", "published": "2024-05-09 01:40:38", "link": "http://arxiv.org/abs/2405.05493v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic question generation for propositional logical equivalences", "abstract": "The increase in academic dishonesty cases among college students has raised\nconcern, particularly due to the shift towards online learning caused by the\npandemic. We aim to develop and implement a method capable of generating\ntailored questions for each student. The use of Automatic Question Generation\n(AQG) is a possible solution. Previous studies have investigated AQG frameworks\nin education, which include validity, user-defined difficulty, and personalized\nproblem generation. Our new AQG approach produces logical equivalence problems\nfor Discrete Mathematics, which is a core course for year-one computer science\nstudents. This approach utilizes a syntactic grammar and a semantic attribute\nsystem through top-down parsing and syntax tree transformations. Our\nexperiments show that the difficulty level of questions generated by our AQG\napproach is similar to the questions presented to students in the textbook [1].\nThese results confirm the practicality of our AQG approach for automated\nquestion generation in education, with the potential to significantly enhance\nlearning experiences.", "published": "2024-05-09 02:44:42", "link": "http://arxiv.org/abs/2405.05513v1", "categories": ["cs.CL", "cs.DM"], "primary_category": "cs.CL"}
{"title": "From Human Judgements to Predictive Models: Unravelling Acceptability in\n  Code-Mixed Sentences", "abstract": "Current computational approaches for analysing or generating code-mixed\nsentences do not explicitly model \"naturalness\" or \"acceptability\" of\ncode-mixed sentences, but rely on training corpora to reflect distribution of\nacceptable code-mixed sentences. Modelling human judgement for the\nacceptability of code-mixed text can help in distinguishing natural code-mixed\ntext and enable quality-controlled generation of code-mixed text. To this end,\nwe construct Cline - a dataset containing human acceptability judgements for\nEnglish-Hindi (en-hi) code-mixed text. Cline is the largest of its kind with\n16,642 sentences, consisting of samples sourced from two sources: synthetically\ngenerated code-mixed text and samples collected from online social media. Our\nanalysis establishes that popular code-mixing metrics such as CMI, Number of\nSwitch Points, Burstines, which are used to filter/curate/compare code-mixed\ncorpora have low correlation with human acceptability judgements, underlining\nthe necessity of our dataset. Experiments using Cline demonstrate that simple\nMultilayer Perceptron (MLP) models trained solely on code-mixing metrics are\noutperformed by fine-tuned pre-trained Multilingual Large Language Models\n(MLLMs). Specifically, XLM-Roberta and Bernice outperform IndicBERT across\ndifferent configurations in challenging data settings. Comparison with\nChatGPT's zero and fewshot capabilities shows that MLLMs fine-tuned on larger\ndata outperform ChatGPT, providing scope for improvement in code-mixed tasks.\nZero-shot transfer from English-Hindi to English-Telugu acceptability judgments\nusing our model checkpoints proves superior to random baselines, enabling\napplication to other code-mixed language pairs and providing further avenues of\nresearch. We publicly release our human-annotated dataset, trained checkpoints,\ncode-mix corpus, and code for data generation and model training.", "published": "2024-05-09 06:40:39", "link": "http://arxiv.org/abs/2405.05572v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can We Use Large Language Models to Fill Relevance Judgment Holes?", "abstract": "Incomplete relevance judgments limit the re-usability of test collections.\nWhen new systems are compared against previous systems used to build the pool\nof judged documents, they often do so at a disadvantage due to the ``holes'' in\ntest collection (i.e., pockets of un-assessed documents returned by the new\nsystem). In this paper, we take initial steps towards extending existing test\ncollections by employing Large Language Models (LLM) to fill the holes by\nleveraging and grounding the method using existing human judgments. We explore\nthis problem in the context of Conversational Search using TREC iKAT, where\ninformation needs are highly dynamic and the responses (and, the results\nretrieved) are much more varied (leaving bigger holes). While previous work has\nshown that automatic judgments from LLMs result in highly correlated rankings,\nwe find substantially lower correlates when human plus automatic judgments are\nused (regardless of LLM, one/two/few shot, or fine-tuned). We further find\nthat, depending on the LLM employed, new runs will be highly favored (or\npenalized), and this effect is magnified proportionally to the size of the\nholes. Instead, one should generate the LLM annotations on the whole document\npool to achieve more consistent rankings with human-generated labels. Future\nwork is required to prompt engineering and fine-tuning LLMs to reflect and\nrepresent the human annotations, in order to ground and align the models, such\nthat they are more fit for purpose.", "published": "2024-05-09 07:39:19", "link": "http://arxiv.org/abs/2405.05600v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "G-SAP: Graph-based Structure-Aware Prompt Learning over Heterogeneous\n  Knowledge for Commonsense Reasoning", "abstract": "Commonsense question answering has demonstrated considerable potential across\nvarious applications like assistants and social robots. Although fully\nfine-tuned pre-trained Language Models(LM) have achieved remarkable performance\nin commonsense reasoning, their tendency to excessively prioritize textual\ninformation hampers the precise transfer of structural knowledge and undermines\ninterpretability. Some studies have explored combining LMs with Knowledge\nGraphs(KGs) by coarsely fusing the two modalities to perform Graph Neural\nNetwork(GNN)-based reasoning that lacks a profound interaction between\nheterogeneous modalities. In this paper, we propose a novel Graph-based\nStructure-Aware Prompt Learning Model for commonsense reasoning, named G-SAP,\naiming to maintain a balance between heterogeneous knowledge and enhance the\ncross-modal interaction within the LM+GNNs model. In particular, an evidence\ngraph is constructed by integrating multiple knowledge sources, i.e.\nConceptNet, Wikipedia, and Cambridge Dictionary to boost the performance.\nAfterward, a structure-aware frozen PLM is employed to fully incorporate the\nstructured and textual information from the evidence graph, where the\ngeneration of prompts is driven by graph entities and relations. Finally, a\nheterogeneous message-passing reasoning module is used to facilitate deep\ninteraction of knowledge between the LM and graph-based networks. Empirical\nvalidation, conducted through extensive experiments on three benchmark\ndatasets, demonstrates the notable performance of the proposed model. The\nresults reveal a significant advancement over the existing models, especially,\nwith 6.12% improvement over the SoTA LM+GNNs model on the OpenbookQA dataset.", "published": "2024-05-09 08:28:12", "link": "http://arxiv.org/abs/2405.05616v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Automatic Prompt Generation System for Tabular Data Tasks", "abstract": "Efficient processing of tabular data is important in various industries,\nespecially when working with datasets containing a large number of columns.\nLarge language models (LLMs) have demonstrated their ability on several tasks\nthrough carefully crafted prompts. However, creating effective prompts for\ntabular datasets is challenging due to the structured nature of the data and\nthe need to manage numerous columns. This paper presents an innovative\nauto-prompt generation system suitable for multiple LLMs, with minimal\ntraining. It proposes two novel methods; 1) A Reinforcement Learning-based\nalgorithm for identifying and sequencing task-relevant columns 2) Cell-level\nsimilarity-based approach for enhancing few-shot example selection. Our\napproach has been extensively tested across 66 datasets, demonstrating improved\nperformance in three downstream tasks: data imputation, error detection, and\nentity matching using two distinct LLMs; Google flan-t5-xxl and Mixtral 8x7B.", "published": "2024-05-09 08:32:55", "link": "http://arxiv.org/abs/2405.05618v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Beyond Prompts: Learning from Human Communication for Enhanced AI Intent\n  Alignment", "abstract": "AI intent alignment, ensuring that AI produces outcomes as intended by users,\nis a critical challenge in human-AI interaction. The emergence of generative\nAI, including LLMs, has intensified the significance of this problem, as\ninteractions increasingly involve users specifying desired results for AI\nsystems. In order to support better AI intent alignment, we aim to explore\nhuman strategies for intent specification in human-human communication. By\nstudying and comparing human-human and human-LLM communication, we identify key\nstrategies that can be applied to the design of AI systems that are more\neffective at understanding and aligning with user intent. This study aims to\nadvance toward a human-centered AI system by bringing together human\ncommunication strategies for the design of AI systems.", "published": "2024-05-09 11:10:29", "link": "http://arxiv.org/abs/2405.05678v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Can large language models understand uncommon meanings of common words?", "abstract": "Large language models (LLMs) like ChatGPT have shown significant advancements\nacross diverse natural language understanding (NLU) tasks, including\nintelligent dialogue and autonomous agents. Yet, lacking widely acknowledged\ntesting mechanisms, answering `whether LLMs are stochastic parrots or genuinely\ncomprehend the world' remains unclear, fostering numerous studies and sparking\nheated debates. Prevailing research mainly focuses on surface-level NLU,\nneglecting fine-grained explorations. However, such explorations are crucial\nfor understanding their unique comprehension mechanisms, aligning with human\ncognition, and finally enhancing LLMs' general NLU capacities. To address this\ngap, our study delves into LLMs' nuanced semantic comprehension capabilities,\nparticularly regarding common words with uncommon meanings. The idea stems from\nfoundational principles of human communication within psychology, which\nunderscore accurate shared understandings of word semantics. Specifically, this\npaper presents the innovative construction of a Lexical Semantic Comprehension\n(LeSC) dataset with novel evaluation metrics, the first benchmark encompassing\nboth fine-grained and cross-lingual dimensions. Introducing models of both\nopen-source and closed-source, varied scales and architectures, our extensive\nempirical experiments demonstrate the inferior performance of existing models\nin this basic lexical-meaning understanding task. Notably, even the\nstate-of-the-art LLMs GPT-4 and GPT-3.5 lag behind 16-year-old humans by 3.9%\nand 22.3%, respectively. Additionally, multiple advanced prompting techniques\nand retrieval-augmented generation are also introduced to help alleviate this\ntrouble, yet limitations persist. By highlighting the above critical\nshortcomings, this research motivates further investigation and offers novel\ninsights for developing more intelligent LLMs.", "published": "2024-05-09 12:58:22", "link": "http://arxiv.org/abs/2405.05741v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Similarity Guided Multimodal Fusion Transformer for Semantic Location\n  Prediction in Social Media", "abstract": "Semantic location prediction aims to derive meaningful location insights from\nmultimodal social media posts, offering a more contextual understanding of\ndaily activities than using GPS coordinates. This task faces significant\nchallenges due to the noise and modality heterogeneity in \"text-image\" posts.\nExisting methods are generally constrained by inadequate feature\nrepresentations and modal interaction, struggling to effectively reduce noise\nand modality heterogeneity. To address these challenges, we propose a\nSimilarity-Guided Multimodal Fusion Transformer (SG-MFT) for predicting the\nsemantic locations of users from their multimodal posts. First, we incorporate\nhigh-quality text and image representations by utilizing a pre-trained large\nvision-language model. Then, we devise a Similarity-Guided Interaction Module\n(SIM) to alleviate modality heterogeneity and noise interference by\nincorporating both coarse-grained and fine-grained similarity guidance for\nimproving modality interactions. Specifically, we propose a novel\nsimilarity-aware feature interpolation attention mechanism at the\ncoarse-grained level, leveraging modality-wise similarity to mitigate\nheterogeneity and reduce noise within each modality. At the fine-grained level,\nwe utilize a similarity-aware feed-forward block and element-wise similarity to\nfurther address the issue of modality heterogeneity. Finally, building upon\npre-processed features with minimal noise and modal interference, we devise a\nSimilarity-aware Fusion Module (SFM) to fuse two modalities with a\ncross-attention mechanism. Comprehensive experimental results clearly\ndemonstrate the superior performance of our proposed method.", "published": "2024-05-09 13:32:26", "link": "http://arxiv.org/abs/2405.05760v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Towards a More Inclusive AI: Progress and Perspectives in Large Language\n  Model Training for the S\u00e1mi Language", "abstract": "S\\'ami, an indigenous language group comprising multiple languages, faces\ndigital marginalization due to the limited availability of data and\nsophisticated language models designed for its linguistic intricacies. This\nwork focuses on increasing technological participation for the S\\'ami language.\nWe draw the attention of the ML community towards the language modeling problem\nof Ultra Low Resource (ULR) languages. ULR languages are those for which the\namount of available textual resources is very low, and the speaker count for\nthem is also very low. ULRLs are also not supported by mainstream Large\nLanguage Models (LLMs) like ChatGPT, due to which gathering artificial training\ndata for them becomes even more challenging. Mainstream AI foundational model\ndevelopment has given less attention to this category of languages. Generally,\nthese languages have very few speakers, making it hard to find them. However,\nit is important to develop foundational models for these ULR languages to\npromote inclusion and the tangible abilities and impact of LLMs. To this end,\nwe have compiled the available S\\'ami language resources from the web to create\na clean dataset for training language models. In order to study the behavior of\nmodern LLM models with ULR languages (S\\'ami), we have experimented with\ndifferent kinds of LLMs, mainly at the order of $\\sim$ seven billion\nparameters. We have also explored the effect of multilingual LLM training for\nULRLs. We found that the decoder-only models under a sequential multilingual\ntraining scenario perform better than joint multilingual training, whereas\nmultilingual training with high semantic overlap, in general, performs better\nthan training from scratch.This is the first study on the S\\'ami language for\nadapting non-statistical language models that use the latest developments in\nthe field of natural language processing (NLP).", "published": "2024-05-09 13:54:22", "link": "http://arxiv.org/abs/2405.05777v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing RELIES on Linguistics", "abstract": "Large Language Models (LLMs) have become capable of generating highly fluent\ntext in certain languages, without modules specially designed to capture\ngrammar or semantic coherence. What does this mean for the future of linguistic\nexpertise in NLP? We highlight several aspects in which NLP (still) relies on\nlinguistics, or where linguistic thinking can illuminate new directions. We\nargue our case around the acronym RELIES that encapsulates six major facets\nwhere linguistics contributes to NLP: Resources, Evaluation, Low-resource\nsettings, Interpretability, Explanation, and the Study of language. This list\nis not exhaustive, nor is linguistics the main point of reference for every\neffort under these themes; but at a macro level, these facets highlight the\nenduring importance of studying machine systems vis-\\`a-vis systems of human\nlanguage.", "published": "2024-05-09 17:59:32", "link": "http://arxiv.org/abs/2405.05966v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Mixture-of-Experts Approach to Few-Shot Task Transfer in Open-Ended\n  Text Worlds", "abstract": "Open-ended worlds are those in which there are no pre-specified goals or\nenvironmental reward signal. As a consequence, an agent must know how to\nperform a multitude of tasks. However, when a new task is presented to an\nagent, we expect it to be able to reuse some of what it knows from previous\ntasks to rapidly learn that new task. We introduce a novel technique whereby\npolicies for different a priori known tasks are combined into a\nMixture-of-Experts model with an attention mechanism across a mix of frozen and\nunfrozen experts. The model learns when to attend to frozen task-specific\nexperts when appropriate and learns new experts to handle novel situations. We\nwork in an open-ended text-based environment in which the agent is tasked with\nbehaving like different types of character roles and must rapidly learn\nbehaviors associated with new character role types. We show that our agent both\nobtains more rewards in the zero-shot setting, and discovers these rewards with\ngreater sample efficiency in the few-shot learning settings.", "published": "2024-05-09 19:02:56", "link": "http://arxiv.org/abs/2405.06059v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HMT: Hierarchical Memory Transformer for Efficient Long Context Language\n  Processing", "abstract": "Transformer-based large language models (LLM) have been widely used in\nlanguage processing applications. However, due to the memory constraints of the\ndevices, most of them restrict the context window. Even though recurrent models\nin previous works can memorize past tokens to enable unlimited context and\nmaintain effectiveness, they have ``flat'' memory architectures. Such\narchitectures have limitations in selecting and filtering information. Since\nhumans are good at learning and self-adjustment, we believe that imitating\nbrain memory hierarchy is beneficial for model memorization. Thus, we propose\nthe Hierarchical Memory Transformer (HMT), a novel framework that facilitates a\nmodel's long-context processing ability by imitating human memorization\nbehavior. Leveraging memory-augmented segment-level recurrence, we organize the\nmemory hierarchy by preserving tokens from early input segments, passing memory\nembeddings along the sequence, and recalling relevant information from history.\nEvaluating general language modeling, question-answering tasks, and the\nsummarization task, we show that HMT consistently improves the long-context\nprocessing ability of existing models. Furthermore, HMT achieves a comparable\nor superior generation quality to long-context LLMs with $2 \\sim 57\\times$\nfewer parameters and $2.5 \\sim 116\\times$ less inference memory, significantly\noutperforming previous memory-augmented models. Code on Github:\nhttps://github.com/OswaldHe/HMT-pytorch.", "published": "2024-05-09 19:32:49", "link": "http://arxiv.org/abs/2405.06067v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on Human\n  Annotation: A Case Study Using Schedule-of-Event Table Detection", "abstract": "Large Language Models (LLMs) have demonstrated their efficacy across a broad\nspectrum of tasks in healthcare applications. However, often LLMs need to be\nfine-tuned on task-specific expert annotated data to achieve optimal\nperformance, which can be expensive and time consuming. In this study, we\nfine-tune PaLM-2 with parameter efficient fine-tuning (PEFT) using noisy labels\nobtained from gemini-pro 1.0 for the detection of Schedule-of-Event (SoE)\ntables, which specify care plan in clinical trial protocols. We introduce a\nfiltering mechanism to select high-confidence labels for this table\nclassification task, thereby reducing the noise in the auto-generated labels.\nWe show that fine-tuned PaLM-2 with those labels achieves performance that\nexceeds the gemini-pro 1.0 and other LLMs. Furthermore, its performance is\nclose to a PaLM-2 fine-tuned on labels obtained from non-expert annotators. Our\nresults show that leveraging LLM-generated labels through powerful models like\ngemini-pro can potentially serve as a viable strategy for improving LLM\nperformance through fine-tuning in specialized tasks, particularly in domains\nwhere expert annotations are scarce, expensive, or time-consuming to obtain.", "published": "2024-05-09 20:45:58", "link": "http://arxiv.org/abs/2405.06093v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Narrative to Trajectory (N2T+): Extracting Routes of Life or Death from\n  Human Trafficking Text Corpora", "abstract": "Climate change and political unrest in certain regions of the world are\nimposing extreme hardship on many communities and are forcing millions of\nvulnerable populations to abandon their homelands and seek refuge in safer\nlands. As international laws are not fully set to deal with the migration\ncrisis, people are relying on networks of exploiting smugglers to escape the\ndevastation in order to live in stability. During the smuggling journey,\nmigrants can become victims of human trafficking if they fail to pay the\nsmuggler and may be forced into coerced labor. Government agencies and\nanti-trafficking organizations try to identify the trafficking routes based on\nstories of survivors in order to gain knowledge and help prevent such crimes.\nIn this paper, we propose a system called Narrative to Trajectory (N2T+), which\nextracts trajectories of trafficking routes. N2T+ uses Data Science and Natural\nLanguage Processing techniques to analyze trafficking narratives, automatically\nextract relevant location names, disambiguate possible name ambiguities, and\nplot the trafficking route on a map. In a comparative evaluation we show that\nthe proposed multi-dimensional approach offers significantly higher geolocation\ndetection than other state of the art techniques.", "published": "2024-05-09 22:21:40", "link": "http://arxiv.org/abs/2405.06129v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Enhanced Review Detection and Recognition: A Platform-Agnostic Approach\n  with Application to Online Commerce", "abstract": "Online commerce relies heavily on user generated reviews to provide unbiased\ninformation about products that they have not physically seen. The importance\nof reviews has attracted multiple exploitative online behaviours and requires\nmethods for monitoring and detecting reviews. We present a machine learning\nmethodology for review detection and extraction, and demonstrate that it\ngeneralises for use across websites that were not contained in the training\ndata. This method promises to drive applications for automatic detection and\nevaluation of reviews, regardless of their source. Furthermore, we showcase the\nversatility of our method by implementing and discussing three key applications\nfor analysing reviews: Sentiment Inconsistency Analysis, which detects and\nfilters out unreliable reviews based on inconsistencies between ratings and\ncomments; Multi-language support, enabling the extraction and translation of\nreviews from various languages without relying on HTML scraping; and Fake\nreview detection, achieved by integrating a trained NLP model to identify and\ndistinguish between genuine and fake reviews.", "published": "2024-05-09 00:32:22", "link": "http://arxiv.org/abs/2405.06704v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMs can Find Mathematical Reasoning Mistakes by Pedagogical\n  Chain-of-Thought", "abstract": "Self-correction is emerging as a promising approach to mitigate the issue of\nhallucination in Large Language Models (LLMs). To facilitate effective\nself-correction, recent research has proposed mistake detection as its initial\nstep. However, current literature suggests that LLMs often struggle with\nreliably identifying reasoning mistakes when using simplistic prompting\nstrategies. To address this challenge, we introduce a unique prompting\nstrategy, termed the Pedagogical Chain-of-Thought (PedCoT), which is\nspecifically designed to guide the identification of reasoning mistakes,\nparticularly mathematical reasoning mistakes. PedCoT consists of pedagogical\nprinciples for prompts (PPP) design, two-stage interaction process (TIP) and\ngrounded PedCoT prompts, all inspired by the educational theory of the Bloom\nCognitive Model (BCM). We evaluate our approach on two public datasets\nfeaturing math problems of varying difficulty levels. The experiments\ndemonstrate that our zero-shot prompting strategy significantly outperforms\nstrong baselines. The proposed method can achieve the goal of reliable\nmathematical mistake identification and provide a foundation for automatic math\nanswer grading. The results underscore the significance of educational theory,\nserving as domain knowledge, in guiding prompting strategy design for\naddressing challenging tasks with LLMs effectively.", "published": "2024-05-09 07:37:34", "link": "http://arxiv.org/abs/2405.06705v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring the Capabilities of Large Multimodal Models on Dense Text", "abstract": "While large multi-modal models (LMM) have shown notable progress in\nmulti-modal tasks, their capabilities in tasks involving dense textual content\nremains to be fully explored. Dense text, which carries important information,\nis often found in documents, tables, and product descriptions. Understanding\ndense text enables us to obtain more accurate information, assisting in making\nbetter decisions. To further explore the capabilities of LMM in complex text\ntasks, we propose the DT-VQA dataset, with 170k question-answer pairs. In this\npaper, we conduct a comprehensive evaluation of GPT4V, Gemini, and various\nopen-source LMMs on our dataset, revealing their strengths and weaknesses.\nFurthermore, we evaluate the effectiveness of two strategies for LMM: prompt\nengineering and downstream fine-tuning. We find that even with automatically\nlabeled training datasets, significant improvements in model performance can be\nachieved. We hope that this research will promote the study of LMM in dense\ntext tasks. Code will be released at\nhttps://github.com/Yuliang-Liu/MultimodalOCR.", "published": "2024-05-09 07:47:25", "link": "http://arxiv.org/abs/2405.06706v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hypothesis Testing Prompting Improves Deductive Reasoning in Large\n  Language Models", "abstract": "Combining different forms of prompts with pre-trained large language models\nhas yielded remarkable results on reasoning tasks (e.g. Chain-of-Thought\nprompting). However, along with testing on more complex reasoning, these\nmethods also expose problems such as invalid reasoning and fictional reasoning\npaths. In this paper, we develop \\textit{Hypothesis Testing Prompting}, which\nadds conclusion assumptions, backward reasoning, and fact verification during\nintermediate reasoning steps. \\textit{Hypothesis Testing prompting} involves\nmultiple assumptions and reverses validation of conclusions leading to its\nunique correct answer. Experiments on two challenging deductive reasoning\ndatasets ProofWriter and RuleTaker show that hypothesis testing prompting not\nonly significantly improves the effect, but also generates a more reasonable\nand standardized reasoning process.", "published": "2024-05-09 08:46:17", "link": "http://arxiv.org/abs/2405.06707v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating the Efficacy of AI Techniques in Textual Anonymization: A\n  Comparative Study", "abstract": "In the digital era, with escalating privacy concerns, it's imperative to\ndevise robust strategies that protect private data while maintaining the\nintrinsic value of textual information. This research embarks on a\ncomprehensive examination of text anonymisation methods, focusing on\nConditional Random Fields (CRF), Long Short-Term Memory (LSTM), Embeddings from\nLanguage Models (ELMo), and the transformative capabilities of the Transformers\narchitecture. Each model presents unique strengths since LSTM is modeling\nlong-term dependencies, CRF captures dependencies among word sequences, ELMo\ndelivers contextual word representations using deep bidirectional language\nmodels and Transformers introduce self-attention mechanisms that provide\nenhanced scalability. Our study is positioned as a comparative analysis of\nthese models, emphasising their synergistic potential in addressing text\nanonymisation challenges. Preliminary results indicate that CRF, LSTM, and ELMo\nindividually outperform traditional methods. The inclusion of Transformers,\nwhen compared alongside with the other models, offers a broader perspective on\nachieving optimal text anonymisation in contemporary settings.", "published": "2024-05-09 11:29:25", "link": "http://arxiv.org/abs/2405.06709v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Digital Diagnostics: The Potential Of Large Language Models In\n  Recognizing Symptoms Of Common Illnesses", "abstract": "The recent swift development of LLMs like GPT-4, Gemini, and GPT-3.5 offers a\ntransformative opportunity in medicine and healthcare, especially in digital\ndiagnostics. This study evaluates each model diagnostic abilities by\ninterpreting a user symptoms and determining diagnoses that fit well with\ncommon illnesses, and it demonstrates how each of these models could\nsignificantly increase diagnostic accuracy and efficiency. Through a series of\ndiagnostic prompts based on symptoms from medical databases, GPT-4 demonstrates\nhigher diagnostic accuracy from its deep and complete history of training on\nmedical data. Meanwhile, Gemini performs with high precision as a critical tool\nin disease triage, demonstrating its potential to be a reliable model when\nphysicians are trying to make high-risk diagnoses. GPT-3.5, though slightly\nless advanced, is a good tool for medical diagnostics. This study highlights\nthe need to study LLMs for healthcare and clinical practices with more care and\nattention, ensuring that any system utilizing LLMs promotes patient privacy and\ncomplies with health information privacy laws such as HIPAA compliance, as well\nas the social consequences that affect the varied individuals in complex\nhealthcare contexts. This study marks the start of a larger future effort to\nstudy the various ways in which assigning ethical concerns to LLMs task of\nlearning from human biases could unearth new ways to apply AI in complex\nmedical settings.", "published": "2024-05-09 15:12:24", "link": "http://arxiv.org/abs/2405.06712v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unveiling the Competitive Dynamics: A Comparative Evaluation of American\n  and Chinese LLMs", "abstract": "The strategic significance of Large Language Models (LLMs) in economic\nexpansion, innovation, societal development, and national security has been\nincreasingly recognized since the advent of ChatGPT. This study provides a\ncomprehensive comparative evaluation of American and Chinese LLMs in both\nEnglish and Chinese contexts. We proposed a comprehensive evaluation framework\nthat encompasses natural language proficiency, disciplinary expertise, and\nsafety and responsibility, and systematically assessed 16 prominent models from\nthe US and China under various operational tasks and scenarios. Our key\nfindings show that GPT 4-Turbo is at the forefront in English contexts, whereas\nErnie-Bot 4 stands out in Chinese contexts. The study also highlights\ndisparities in LLM performance across languages and tasks, stressing the\nnecessity for linguistically and culturally nuanced model development. The\ncomplementary strengths of American and Chinese LLMs point to the value of\nSino-US collaboration in advancing LLM technology. The research presents the\ncurrent LLM competition landscape and offers valuable insights for policymakers\nand businesses regarding strategic LLM investments and development. Future work\nwill expand on this framework to include emerging LLM multimodal capabilities\nand business application assessments.", "published": "2024-05-09 15:39:19", "link": "http://arxiv.org/abs/2405.06713v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards a Path Dependent Account of Category Fluency", "abstract": "Category fluency is a widely studied cognitive phenomenon, yet two\nconflicting accounts have been proposed as the underlying retrieval mechanism\n-- an optimal foraging process deliberately searching through memory (Hills et\nal., 2012) and a random walk sampling from a semantic network (Abbott et al.,\n2015). Evidence for both accounts has centered around predicting human patch\nswitches, where both existing models of category fluency produce paradoxically\nidentical results. We begin by peeling back the assumptions made by existing\nmodels, namely that each named example only depends on the previous example, by\n(i) adding an additional bias to model the category transition probability\ndirectly and (ii) relying on a large language model to predict based on the\nentire existing sequence. Then, we present evidence towards resolving the\ndisagreement between each account of foraging by reformulating models as\nsequence generators. To evaluate, we compare generated category fluency runs to\na bank of human-written sequences by proposing a metric based on n-gram\noverlap. We find category switch predictors do not necessarily produce\nhuman-like sequences, in fact the additional biases used by the Hills et al.\n(2012) model are required to improve generation quality, which are later\nimproved by our category modification. Even generating exclusively with an LLM\nrequires an additional global cue to trigger the patch switching behavior\nduring production. Further tests on only the search process on top of the\nsemantic network highlight the importance of deterministic search to replicate\nhuman behavior.", "published": "2024-05-09 16:36:56", "link": "http://arxiv.org/abs/2405.06714v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Creativity in Large Language Models through Associative\n  Thinking Strategies", "abstract": "This paper explores the enhancement of creativity in Large Language Models\n(LLMs) like vGPT-4 through associative thinking, a cognitive process where\ncreative ideas emerge from linking seemingly unrelated concepts. Associative\nthinking strategies have been found to effectively help humans boost\ncreativity. However, whether the same strategies can help LLMs become more\ncreative remains under-explored. In this work, we investigate whether prompting\nLLMs to connect disparate concepts can augment their creative outputs. Focusing\non three domains -- Product Design, Storytelling, and Marketing -- we introduce\ncreativity tasks designed to assess vGPT-4's ability to generate original and\nuseful content. By challenging the models to form novel associations, we\nevaluate the potential of associative thinking to enhance the creative\ncapabilities of LLMs. Our findings show that leveraging associative thinking\ntechniques can significantly improve the originality of vGPT-4's responses.", "published": "2024-05-09 16:42:29", "link": "http://arxiv.org/abs/2405.06715v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "One vs. Many: Comprehending Accurate Information from Multiple Erroneous\n  and Inconsistent AI Generations", "abstract": "As Large Language Models (LLMs) are nondeterministic, the same input can\ngenerate different outputs, some of which may be incorrect or hallucinated. If\nrun again, the LLM may correct itself and produce the correct answer.\nUnfortunately, most LLM-powered systems resort to single results which, correct\nor not, users accept. Having the LLM produce multiple outputs may help identify\ndisagreements or alternatives. However, it is not obvious how the user will\ninterpret conflicts or inconsistencies. To this end, we investigate how users\nperceive the AI model and comprehend the generated information when they\nreceive multiple, potentially inconsistent, outputs. Through a preliminary\nstudy, we identified five types of output inconsistencies. Based on these\ncategories, we conducted a study (N=252) in which participants were given one\nor more LLM-generated passages to an information-seeking question. We found\nthat inconsistency within multiple LLM-generated outputs lowered the\nparticipants' perceived AI capacity, while also increasing their comprehension\nof the given information. Specifically, we observed that this positive effect\nof inconsistencies was most significant for participants who read two passages,\ncompared to those who read three. Based on these findings, we present design\nimplications that, instead of regarding LLM output inconsistencies as a\ndrawback, we can reveal the potential inconsistencies to transparently indicate\nthe limitations of these models and promote critical LLM usage.", "published": "2024-05-09 07:12:45", "link": "http://arxiv.org/abs/2405.05581v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for\n  LLM", "abstract": "Large language models (LLMs) have achieved remarkable performance in various\nnatural language processing tasks, especially in dialogue systems. However, LLM\nmay also pose security and moral threats, especially in multi round\nconversations where large models are more easily guided by contextual content,\nresulting in harmful or biased responses. In this paper, we present a novel\nmethod to attack LLMs in multi-turn dialogues, called CoA (Chain of Attack).\nCoA is a semantic-driven contextual multi-turn attack method that adaptively\nadjusts the attack policy through contextual feedback and semantic relevance\nduring multi-turn of dialogue with a large model, resulting in the model\nproducing unreasonable or harmful content. We evaluate CoA on different LLMs\nand datasets, and show that it can effectively expose the vulnerabilities of\nLLMs, and outperform existing attack methods. Our work provides a new\nperspective and tool for attacking and defending LLMs, and contributes to the\nsecurity and ethical assessment of dialogue systems.", "published": "2024-05-09 08:15:21", "link": "http://arxiv.org/abs/2405.05610v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Memory-Space Visual Prompting for Efficient Vision-Language Fine-Tuning", "abstract": "Current solutions for efficiently constructing large vision-language (VL)\nmodels follow a two-step paradigm: projecting the output of pre-trained vision\nencoders to the input space of pre-trained language models as visual prompts;\nand then transferring the models to downstream VL tasks via end-to-end\nparameter-efficient fine-tuning (PEFT). However, this paradigm still exhibits\ninefficiency since it significantly increases the input length of the language\nmodels. In this paper, in contrast to integrating visual prompts into inputs,\nwe regard visual prompts as additional knowledge that facilitates language\nmodels in addressing tasks associated with visual information. Motivated by the\nfinding that Feed-Forward Network (FFN) of language models acts as \"key-value\nmemory\", we introduce a novel approach termed memory-space visual prompting\n(MemVP), wherein visual prompts are concatenated with the weights of FFN for\nvisual knowledge injection. Experimental results across various VL tasks and\nlanguage models reveal that MemVP significantly reduces the training time and\ninference latency of the finetuned VL models and surpasses the performance of\nprevious PEFT methods. Code: https://github.com/JieShibo/MemVP", "published": "2024-05-09 08:23:20", "link": "http://arxiv.org/abs/2405.05615v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Computational lexical analysis of Flamenco genres", "abstract": "Flamenco, recognized by UNESCO as part of the Intangible Cultural Heritage of\nHumanity, is a profound expression of cultural identity rooted in Andalusia,\nSpain. However, there is a lack of quantitative studies that help identify\ncharacteristic patterns in this long-lived music tradition. In this work, we\npresent a computational analysis of Flamenco lyrics, employing natural language\nprocessing and machine learning to categorize over 2000 lyrics into their\nrespective Flamenco genres, termed as $\\textit{palos}$. Using a Multinomial\nNaive Bayes classifier, we find that lexical variation across styles enables to\naccurately identify distinct $\\textit{palos}$. More importantly, from an\nautomatic method of word usage, we obtain the semantic fields that characterize\neach style. Further, applying a metric that quantifies the inter-genre distance\nwe perform a network analysis that sheds light on the relationship between\nFlamenco styles. Remarkably, our results suggest historical connections and\n$\\textit{palo}$ evolutions. Overall, our work illuminates the intricate\nrelationships and cultural significance embedded within Flamenco lyrics,\ncomplementing previous qualitative discussions with quantitative analyses and\nsparking new discussions on the origin and development of traditional music\ngenres.", "published": "2024-05-09 12:35:33", "link": "http://arxiv.org/abs/2405.05723v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Exploring the Potential of Human-LLM Synergy in Advancing Qualitative\n  Analysis: A Case Study on Mental-Illness Stigma", "abstract": "Qualitative analysis is a challenging, yet crucial aspect of advancing\nresearch in the field of Human-Computer Interaction (HCI). Recent studies show\nthat large language models (LLMs) can perform qualitative coding within\nexisting schemes, but their potential for collaborative human-LLM discovery and\nnew insight generation in qualitative analysis is still underexplored. To\nbridge this gap and advance qualitative analysis by harnessing the power of\nLLMs, we propose CHALET, a novel methodology that leverages the human-LLM\ncollaboration paradigm to facilitate conceptualization and empower qualitative\nresearch. The CHALET approach involves LLM-supported data collection,\nperforming both human and LLM deductive coding to identify disagreements, and\nperforming collaborative inductive coding on these disagreement cases to derive\nnew conceptual insights. We validated the effectiveness of CHALET through its\napplication to the attribution model of mental-illness stigma, uncovering\nimplicit stigmatization themes on cognitive, emotional and behavioral\ndimensions. We discuss the implications for future research, methodology, and\nthe transdisciplinary opportunities CHALET presents for the HCI community and\nbeyond.", "published": "2024-05-09 13:27:22", "link": "http://arxiv.org/abs/2405.05758v1", "categories": ["cs.HC", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "The Perspectivist Paradigm Shift: Assumptions and Challenges of\n  Capturing Human Labels", "abstract": "Longstanding data labeling practices in machine learning involve collecting\nand aggregating labels from multiple annotators. But what should we do when\nannotators disagree? Though annotator disagreement has long been seen as a\nproblem to minimize, new perspectivist approaches challenge this assumption by\ntreating disagreement as a valuable source of information. In this position\npaper, we examine practices and assumptions surrounding the causes of\ndisagreement--some challenged by perspectivist approaches, and some that remain\nto be addressed--as well as practical and normative challenges for work\noperating under these assumptions. We conclude with recommendations for the\ndata labeling pipeline and avenues for future research engaging with\nsubjectivity and disagreement.", "published": "2024-05-09 15:48:07", "link": "http://arxiv.org/abs/2405.05860v1", "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Special Characters Attack: Toward Scalable Training Data Extraction From\n  Large Language Models", "abstract": "Large language models (LLMs) have achieved remarkable performance on a wide\nrange of tasks. However, recent studies have shown that LLMs can memorize\ntraining data and simple repeated tokens can trick the model to leak the data.\nIn this paper, we take a step further and show that certain special characters\nor their combinations with English letters are stronger memory triggers,\nleading to more severe data leakage. The intuition is that, since LLMs are\ntrained with massive data that contains a substantial amount of special\ncharacters (e.g. structural symbols {, } of JSON files, and @, # in emails and\nonline posts), the model may memorize the co-occurrence between these special\ncharacters and the raw texts. This motivates us to propose a simple but\neffective Special Characters Attack (SCA) to induce training data leakage. Our\nexperiments verify the high effectiveness of SCA against state-of-the-art LLMs:\nthey can leak diverse training data, such as code corpus, web pages, and\npersonally identifiable information, and sometimes generate non-stop outputs as\na byproduct. We further show that the composition of the training data corpus\ncan be revealed by inspecting the leaked data -- one crucial piece of\ninformation for pre-training high-performance LLMs. Our work can help\nunderstand the sensitivity of LLMs to special characters and identify potential\nareas for improvement.", "published": "2024-05-09 02:35:32", "link": "http://arxiv.org/abs/2405.05990v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "LLMC: Benchmarking Large Language Model Quantization with a Versatile\n  Compression Toolkit", "abstract": "Recent advancements in large language models (LLMs) are propelling us toward\nartificial general intelligence with their remarkable emergent abilities and\nreasoning capabilities. However, the substantial computational and memory\nrequirements limit the widespread adoption. Quantization, a key compression\ntechnique, can effectively mitigate these demands by compressing and\naccelerating LLMs, albeit with potential risks to accuracy. Numerous studies\nhave aimed to minimize the accuracy loss associated with quantization. However,\ntheir quantization configurations vary from each other and cannot be fairly\ncompared. In this paper, we present LLMC, a plug-and-play compression toolkit,\nto fairly and systematically explore the impact of quantization. LLMC\nintegrates dozens of algorithms, models, and hardwares, offering high\nextensibility from integer to floating-point quantization, from LLM to\nvision-language (VLM) model, from fixed-bit to mixed precision, and from\nquantization to sparsification. Powered by this versatile toolkit, our\nbenchmark covers three key aspects: calibration data, algorithms (three\nstrategies), and data formats, providing novel insights and detailed analyses\nfor further research and practical guidance for users. Our toolkit is available\nat https://github.com/ModelTC/llmc.", "published": "2024-05-09 11:49:05", "link": "http://arxiv.org/abs/2405.06001v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Large Language Models Show Human-like Social Desirability Biases in\n  Survey Responses", "abstract": "As Large Language Models (LLMs) become widely used to model and simulate\nhuman behavior, understanding their biases becomes critical. We developed an\nexperimental framework using Big Five personality surveys and uncovered a\npreviously undetected social desirability bias in a wide range of LLMs. By\nsystematically varying the number of questions LLMs were exposed to, we\ndemonstrate their ability to infer when they are being evaluated. When\npersonality evaluation is inferred, LLMs skew their scores towards the\ndesirable ends of trait dimensions (i.e., increased extraversion, decreased\nneuroticism, etc). This bias exists in all tested models, including GPT-4/3.5,\nClaude 3, Llama 3, and PaLM-2. Bias levels appear to increase in more recent\nmodels, with GPT-4's survey responses changing by 1.20 (human) standard\ndeviations and Llama 3's by 0.98 standard deviations-very large effects. This\nbias is robust to randomization of question order and paraphrasing.\nReverse-coding all the questions decreases bias levels but does not eliminate\nthem, suggesting that this effect cannot be attributed to acquiescence bias.\nOur findings reveal an emergent social desirability bias and suggest\nconstraints on profiling LLMs with psychometric tests and on using LLMs as\nproxies for human participants.", "published": "2024-05-09 19:02:53", "link": "http://arxiv.org/abs/2405.06058v2", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.AI"}
{"title": "LLMs for XAI: Future Directions for Explaining Explanations", "abstract": "In response to the demand for Explainable Artificial Intelligence (XAI), we\ninvestigate the use of Large Language Models (LLMs) to transform ML\nexplanations into natural, human-readable narratives. Rather than directly\nexplaining ML models using LLMs, we focus on refining explanations computed\nusing existing XAI algorithms. We outline several research directions,\nincluding defining evaluation metrics, prompt design, comparing LLM models,\nexploring further training methods, and integrating external data. Initial\nexperiments and user study suggest that LLMs offer a promising way to enhance\nthe interpretability and usability of XAI.", "published": "2024-05-09 19:17:47", "link": "http://arxiv.org/abs/2405.06064v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Muting Whisper: A Universal Acoustic Adversarial Attack on Speech\n  Foundation Models", "abstract": "Recent developments in large speech foundation models like Whisper have led\nto their widespread use in many automatic speech recognition (ASR)\napplications. These systems incorporate `special tokens' in their vocabulary,\nsuch as $\\texttt{<|endoftext|>}$, to guide their language generation process.\nHowever, we demonstrate that these tokens can be exploited by adversarial\nattacks to manipulate the model's behavior. We propose a simple yet effective\nmethod to learn a universal acoustic realization of Whisper's\n$\\texttt{<|endoftext|>}$ token, which, when prepended to any speech signal,\nencourages the model to ignore the speech and only transcribe the special\ntoken, effectively `muting' the model. Our experiments demonstrate that the\nsame, universal 0.64-second adversarial audio segment can successfully mute a\ntarget Whisper ASR model for over 97\\% of speech samples. Moreover, we find\nthat this universal adversarial audio segment often transfers to new datasets\nand tasks. Overall this work demonstrates the vulnerability of Whisper models\nto `muting' adversarial attacks, where such attacks can pose both risks and\npotential benefits in real-world settings: for example the attack can be used\nto bypass speech moderation systems, or conversely the attack can also be used\nto protect private speech data.", "published": "2024-05-09 22:59:23", "link": "http://arxiv.org/abs/2405.06134v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Reddit-Impacts: A Named Entity Recognition Dataset for Analyzing\n  Clinical and Social Effects of Substance Use Derived from Social Media", "abstract": "Substance use disorders (SUDs) are a growing concern globally, necessitating\nenhanced understanding of the problem and its trends through data-driven\nresearch. Social media are unique and important sources of information about\nSUDs, particularly since the data in such sources are often generated by people\nwith lived experiences. In this paper, we introduce Reddit-Impacts, a\nchallenging Named Entity Recognition (NER) dataset curated from subreddits\ndedicated to discussions on prescription and illicit opioids, as well as\nmedications for opioid use disorder. The dataset specifically concentrates on\nthe lesser-studied, yet critically important, aspects of substance use--its\nclinical and social impacts. We collected data from chosen subreddits using the\npublicly available Application Programming Interface for Reddit. We manually\nannotated text spans representing clinical and social impacts reported by\npeople who also reported personal nonmedical use of substances including but\nnot limited to opioids, stimulants and benzodiazepines. Our objective is to\ncreate a resource that can enable the development of systems that can\nautomatically detect clinical and social impacts of substance use from\ntext-based social media data. The successful development of such systems may\nenable us to better understand how nonmedical use of substances affects\nindividual health and societal dynamics, aiding the development of effective\npublic health strategies. In addition to creating the annotated data set, we\napplied several machine learning models to establish baseline performances.\nSpecifically, we experimented with transformer models like BERT, and RoBERTa,\none few-shot learning model DANN by leveraging the full training dataset, and\nGPT-3.5 by using one-shot learning, for automatic NER of clinical and social\nimpacts. The dataset has been made available through the 2024 SMM4H shared\ntasks.", "published": "2024-05-09 23:43:57", "link": "http://arxiv.org/abs/2405.06145v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LangCell: Language-Cell Pre-training for Cell Identity Understanding", "abstract": "Cell identity encompasses various semantic aspects of a cell, including cell\ntype, pathway information, disease information, and more, which are essential\nfor biologists to gain insights into its biological characteristics.\nUnderstanding cell identity from the transcriptomic data, such as annotating\ncell types, has become an important task in bioinformatics. As these semantic\naspects are determined by human experts, it is impossible for AI models to\neffectively carry out cell identity understanding tasks without the supervision\nsignals provided by single-cell and label pairs. The single-cell pre-trained\nlanguage models (PLMs) currently used for this task are trained only on a\nsingle modality, transcriptomics data, lack an understanding of cell identity\nknowledge. As a result, they have to be fine-tuned for downstream tasks and\nstruggle when lacking labeled data with the desired semantic labels. To address\nthis issue, we propose an innovative solution by constructing a unified\nrepresentation of single-cell data and natural language during the pre-training\nphase, allowing the model to directly incorporate insights related to cell\nidentity. More specifically, we introduce $\\textbf{LangCell}$, the first\n$\\textbf{Lang}$uage-$\\textbf{Cell}$ pre-training framework. LangCell utilizes\ntexts enriched with cell identity information to gain a profound comprehension\nof cross-modal knowledge. Results from experiments conducted on different\nbenchmarks show that LangCell is the only single-cell PLM that can work\neffectively in zero-shot cell identity understanding scenarios, and also\nsignificantly outperforms existing models in few-shot and fine-tuning cell\nidentity understanding scenarios.", "published": "2024-05-09 10:04:05", "link": "http://arxiv.org/abs/2405.06708v5", "categories": ["q-bio.GN", "cs.AI", "cs.CL"], "primary_category": "q-bio.GN"}
{"title": "Mobile Sequencers", "abstract": "The article is an attempt to contribute to explorations of a common origin\nfor language and planned-collaborative action. It gives `semantics of change'\nthe central stage in the synthesis, from its history and recordkeeping to its\ndevelopment, its syntax, delivery and reception, including substratal aspects.\n  It is suggested that to arrive at a common core, linguistic semantics must be\nunderstood as studying through syntax mobile agent's representing, tracking and\ncoping with change and no change. Semantics of actions can be conceived the\nsame way, but through plans instead of syntax. The key point is the following:\nSequencing itself, of words and action sequences, brings in more structural\ninterpretation to the sequence than which is immediately evident from the\nsequents themselves. Mobile sequencers can be understood as subjects\nstructuring reporting, understanding and keeping track of change and no change.\nThe idea invites rethinking of the notion of category, both in language and in\nplanning.\n  Understanding understanding change by mobile agents is suggested to be about\nhuman extended practice, not extended-human practice. That's why linguistics is\nas important as computer science in the synthesis. It must rely on\nrepresentational history of acts, thoughts and expressions, personal and\npublic, crosscutting overtness and covertness of these phenomena. It has\nimplication for anthropology in the extended practice, which is covered\nbriefly.", "published": "2024-05-09 12:39:50", "link": "http://arxiv.org/abs/2405.06710v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Pre-trained Text-to-Image Diffusion Models Are Versatile Representation\n  Learners for Control", "abstract": "Embodied AI agents require a fine-grained understanding of the physical world\nmediated through visual and language inputs. Such capabilities are difficult to\nlearn solely from task-specific data. This has led to the emergence of\npre-trained vision-language models as a tool for transferring representations\nlearned from internet-scale data to downstream tasks and new domains. However,\ncommonly used contrastively trained representations such as in CLIP have been\nshown to fail at enabling embodied agents to gain a sufficiently fine-grained\nscene understanding -- a capability vital for control. To address this\nshortcoming, we consider representations from pre-trained text-to-image\ndiffusion models, which are explicitly optimized to generate images from text\nprompts and as such, contain text-conditioned representations that reflect\nhighly fine-grained visuo-spatial information. Using pre-trained text-to-image\ndiffusion models, we construct Stable Control Representations which allow\nlearning downstream control policies that generalize to complex, open-ended\nenvironments. We show that policies learned using Stable Control\nRepresentations are competitive with state-of-the-art representation learning\napproaches across a broad range of simulated control settings, encompassing\nchallenging manipulation and navigation tasks. Most notably, we show that\nStable Control Representations enable learning policies that exhibit\nstate-of-the-art performance on OVMM, a difficult open-vocabulary navigation\nbenchmark.", "published": "2024-05-09 15:39:54", "link": "http://arxiv.org/abs/2405.05852v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO", "stat.ML"], "primary_category": "cs.CV"}
{"title": "The RoyalFlush Automatic Speech Diarization and Recognition System for\n  In-Car Multi-Channel Automatic Speech Recognition Challenge", "abstract": "This paper presents our system submission for the In-Car Multi-Channel\nAutomatic Speech Recognition (ICMC-ASR) Challenge, which focuses on speaker\ndiarization and speech recognition in complex multi-speaker scenarios. To\naddress these challenges, we develop end-to-end speaker diarization models that\nnotably decrease the diarization error rate (DER) by 49.58\\% compared to the\nofficial baseline on the development set. For speech recognition, we utilize\nself-supervised learning representations to train end-to-end ASR models. By\nintegrating these models, we achieve a character error rate (CER) of 16.93\\% on\nthe track 1 evaluation set, and a concatenated minimum permutation character\nerror rate (cpCER) of 25.88\\% on the track 2 evaluation set.", "published": "2024-05-09 02:03:51", "link": "http://arxiv.org/abs/2405.05498v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound training platform applied to astronomy", "abstract": "The convergence between astronomy and data sonification represents a\nsignificant advancement in the approach and analysis of cosmic information. By\nsurpassing the visual exclusivity in data analysis in astronomy, innovative\nprojects have developed software that goes beyond visual representation,\ntransforming data into auditory and tactile displays. However, it has been\nevidenced that this novel technique requires specialized training, particularly\nfor audio format data. This work describes the initial development of a\nplatform aimed at providing training for data analysis in astronomy through\nsonification. The integration of these tools in astronomical education and\nresearch opens new horizons, facilitating a more inclusive and multisensory\nparticipation in the exploration of space science.", "published": "2024-05-09 18:23:59", "link": "http://arxiv.org/abs/2405.06042v1", "categories": ["astro-ph.IM", "cs.SD", "eess.AS"], "primary_category": "astro-ph.IM"}
