{"title": "e-SNLI: Natural Language Inference with Natural Language Explanations", "abstract": "In order for machine learning to garner widespread public adoption, models\nmust be able to provide interpretable and robust explanations for their\ndecisions, as well as learn from human-provided explanations at train time. In\nthis work, we extend the Stanford Natural Language Inference dataset with an\nadditional layer of human-annotated natural language explanations of the\nentailment relations. We further implement models that incorporate these\nexplanations into their training process and output them at test time. We show\nhow our corpus of explanations, which we call e-SNLI, can be used for various\ngoals, such as obtaining full sentence justifications of a model's decisions,\nimproving universal sentence representations and transferring to out-of-domain\nNLI datasets. Our dataset thus opens up a range of research directions for\nusing natural language explanations, both for improving models and for\nasserting their trust.", "published": "2018-12-04 03:15:38", "link": "http://arxiv.org/abs/1812.01193v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Practical Text Classification With Large Pre-Trained Language Models", "abstract": "Multi-emotion sentiment classification is a natural language processing (NLP)\nproblem with valuable use cases on real-world data. We demonstrate that\nlarge-scale unsupervised language modeling combined with finetuning offers a\npractical solution to this task on difficult datasets, including those with\nlabel class imbalance and domain-specific context. By training an\nattention-based Transformer network (Vaswani et al. 2017) on 40GB of text\n(Amazon reviews) (McAuley et al. 2015) and fine-tuning on the training set, our\nmodel achieves a 0.69 F1 score on the SemEval Task 1:E-c multi-dimensional\nemotion classification problem (Mohammad et al. 2018), based on the Plutchik\nwheel of emotions (Plutchik 1979). These results are competitive with state of\nthe art models, including strong F1 scores on difficult (emotion) categories\nsuch as Fear (0.73), Disgust (0.77) and Anger (0.78), as well as competitive\nresults on rare categories such as Anticipation (0.42) and Surprise (0.37).\nFurthermore, we demonstrate our application on a real world text classification\ntask. We create a narrowly collected text dataset of real tweets on several\ntopics, and show that our finetuned model outperforms general purpose\ncommercially available APIs for sentiment and multidimensional emotion\nclassification on this dataset by a significant margin. We also perform a\nvariety of additional studies, investigating properties of deep learning\narchitectures, datasets and algorithms for achieving practical multidimensional\nsentiment classification. Overall, we find that unsupervised language modeling\nand finetuning is a simple framework for achieving high quality results on\nreal-world sentiment classification.", "published": "2018-12-04 04:04:56", "link": "http://arxiv.org/abs/1812.01207v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Multi-grained Sentiment Lexicon Information for Neural\n  Sequence Models", "abstract": "Neural sequence models have achieved great success in sentence-level\nsentiment classification. However, some models are exceptionally complex or\nbased on expensive features. Some other models recognize the value of existed\nlinguistic resource but utilize it insufficiently. This paper proposes a novel\nand general method to incorporate lexicon information, including sentiment\nlexicons(+/-), negation words and intensifiers. Words are annotated in\nfine-grained and coarse-grained labels. The proposed method first encodes the\nfine-grained labels into sentiment embedding and concatenates it with word\nembedding. Second, the coarse-grained labels are utilized to enhance the\nattention mechanism to give large weight on sentiment-related words.\nExperimental results show that our method can increase classification accuracy\nfor neural sequence models on both SST-5 and MR dataset. Specifically, the\nenhanced Bi-LSTM model can even compare with a Tree-LSTM which uses expensive\nphrase-level annotations. Further analysis shows that in most cases the lexicon\nresource can offer the right annotations. Besides, the proposed method is\ncapable of overcoming the effect from inevitably wrong annotations.", "published": "2018-12-04 17:01:52", "link": "http://arxiv.org/abs/1812.01527v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Impact of Sentiment Detection to Recognize Toxic and Subversive Online\n  Comments", "abstract": "The presence of toxic content has become a major problem for many online\ncommunities. Moderators try to limit this problem by implementing more and more\nrefined comment filters, but toxic users are constantly finding new ways to\ncircumvent them. Our hypothesis is that while modifying toxic content and\nkeywords to fool filters can be easy, hiding sentiment is harder. In this\npaper, we explore various aspects of sentiment detection and their correlation\nto toxicity, and use our results to implement a toxicity detection tool. We\nthen test how adding the sentiment information helps detect toxicity in three\ndifferent real-world datasets, and incorporate subversion to these datasets to\nsimulate a user trying to circumvent the system. Our results show sentiment\ninformation has a positive impact on toxicity detection against a subversive\nuser.", "published": "2018-12-04 21:44:36", "link": "http://arxiv.org/abs/1812.01704v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantification and Analysis of Scientific Language Variation Across\n  Research Fields", "abstract": "Quantifying differences in terminologies from various academic domains has\nbeen a longstanding problem yet to be solved. We propose a computational\napproach for analyzing linguistic variation among scientific research fields by\ncapturing the semantic change of terms based on a neural language model. The\nmodel is trained on a large collection of literature in five computer science\nresearch fields, for which we obtain field-specific vector representations for\nkey terms, and global vector representations for other words. Several\nquantitative approaches are introduced to identify the terms whose semantics\nhave drastically changed, or remain unchanged across different research fields.\nWe also propose a metric to quantify the overall linguistic variation of\nresearch fields. After quantitative evaluation on human annotated data and\nqualitative comparison with other methods, we show that our model can improve\ncross-disciplinary data collaboration by identifying terms that potentially\ninduce confusion during interdisciplinary studies.", "published": "2018-12-04 07:05:47", "link": "http://arxiv.org/abs/1812.01250v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tartan: A retrieval-based socialbot powered by a dynamic finite-state\n  machine architecture", "abstract": "This paper describes the Tartan conversational agent built for the 2018 Alexa\nPrize Competition. Tartan is a non-goal-oriented socialbot focused around\nproviding users with an engaging and fluent casual conversation. Tartan's key\nfeatures include an emphasis on structured conversation based on flexible\nfinite-state models and an approach focused on understanding and using\nconversational acts. To provide engaging conversations, Tartan blends\nscript-like yet dynamic responses with data-based generative and retrieval\nmodels. Unique to Tartan is that our dialog manager is modeled as a dynamic\nFinite State Machine. To our knowledge, no other conversational agent\nimplementation has followed this specific structure.", "published": "2018-12-04 07:48:00", "link": "http://arxiv.org/abs/1812.01260v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transferable Natural Language Interface to Structured Queries aided by\n  Adversarial Generation", "abstract": "A natural language interface (NLI) to structured query is intriguing due to\nits wide industrial applications and high economical values. In this work, we\ntackle the problem of domain adaptation for NLI with limited data on target\ndomain. Two important approaches are considered: (a) effective\ngeneral-knowledge-learning on source domain semantic parsing, and (b) data\naugmentation on target domain. We present a Structured Query Inference Network\n(SQIN) to enhance learning for domain adaptation, by separating schema\ninformation from NL and decoding SQL in a more structural-aware manner; we also\npropose a GAN-based augmentation technique (AugmentGAN) to mitigate the issue\nof lacking target domain data. We report solid results on GeoQuery, Overnight,\nand WikiSQL to demonstrate state-of-the-art performances for both in-domain and\ndomain-transfer tasks.", "published": "2018-12-04 06:42:49", "link": "http://arxiv.org/abs/1812.01245v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Face-to-Face Neural Conversation Model", "abstract": "Neural networks have recently become good at engaging in dialog. However,\ncurrent approaches are based solely on verbal text, lacking the richness of a\nreal face-to-face conversation. We propose a neural conversation model that\naims to read and generate facial gestures alongside with text. This allows our\nmodel to adapt its response based on the \"mood\" of the conversation. In\nparticular, we introduce an RNN encoder-decoder that exploits the movement of\nfacial muscles, as well as the verbal conversation. The decoder consists of two\nlayers, where the lower layer aims at generating the verbal response and coarse\nfacial expressions, while the second layer fills in the subtle gestures, making\nthe generated output more smooth and natural. We train our neural network by\nhaving it \"watch\" 250 movies. We showcase our joint face-text model in\ngenerating more natural conversations through automatic metrics and a human\nstudy. We demonstrate an example application with a face-to-face chatting\navatar.", "published": "2018-12-04 16:55:25", "link": "http://arxiv.org/abs/1812.01525v1", "categories": ["cs.CV", "cs.CL", "cs.GR"], "primary_category": "cs.CV"}
{"title": "Playing Text-Adventure Games with Graph-Based Deep Reinforcement\n  Learning", "abstract": "Text-based adventure games provide a platform on which to explore\nreinforcement learning in the context of a combinatorial action space, such as\nnatural language. We present a deep reinforcement learning architecture that\nrepresents the game state as a knowledge graph which is learned during\nexploration. This graph is used to prune the action space, enabling more\nefficient exploration. The question of which action to take can be reduced to a\nquestion-answering task, a form of transfer learning that pre-trains certain\nparts of our architecture. In experiments using the TextWorld framework, we\nshow that our proposed technique can learn a control policy faster than\nbaseline alternatives. We have also open-sourced our code at\nhttps://github.com/rajammanabrolu/KG-DQN.", "published": "2018-12-04 19:06:00", "link": "http://arxiv.org/abs/1812.01628v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to match transient sound events using attentional similarity\n  for few-shot sound recognition", "abstract": "In this paper, we introduce a novel attentional similarity module for the\nproblem of few-shot sound recognition. Given a few examples of an unseen sound\nevent, a classifier must be quickly adapted to recognize the new sound event\nwithout much fine-tuning. The proposed attentional similarity module can be\nplugged into any metric-based learning method for few-shot learning, allowing\nthe resulting model to especially match related short sound events. Extensive\nexperiments on two datasets shows that the proposed module consistently\nimproves the performance of five different metric-based learning methods for\nfew-shot sound recognition. The relative improvement ranges from +4.1% to +7.7%\nfor 5-shot 5-way accuracy for the ESC-50 dataset, and from +2.1% to +6.5% for\nnoiseESC-50. Qualitative results demonstrate that our method contributes in\nparticular to the recognition of transient sound events.", "published": "2018-12-04 08:22:09", "link": "http://arxiv.org/abs/1812.01269v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LSTM based AE-DNN constraint for better late reverb suppression in\n  multi-channel LP formulation", "abstract": "Prediction of late reverberation component using multi-channel linear\nprediction (MCLP) in short-time Fourier transform (STFT) domain is an effective\nmeans to enhance reverberant speech. Traditionally, a speech power spectral\ndensity (PSD) weighted prediction error (WPE) minimization approach is used to\nestimate the prediction filters. The method is sensitive to the estimate of the\ndesired signal PSD. In this paper, we propose a deep neural network (DNN) based\nnon-linear estimate for the desired signal PSD. An auto encoder trained on\nclean speech STFT coefficients is used as the desired signal prior. We explore\ntwo different architectures based on (i) fully-connected (FC) feed-forward, and\n(ii) recurrent long short-term memory (LSTM) layers. Experiments using real\nroom impulse responses show that the LSTM-DNN based PSD estimate performs\nbetter than the traditional methods for late reverb suppression.", "published": "2018-12-04 11:37:25", "link": "http://arxiv.org/abs/1812.01346v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Localization and Tracking of an Acoustic Source using a Diagonal\n  Unloading Beamforming and a Kalman Filter", "abstract": "We present the signal processing framework and some results for the IEEE AASP\nchallenge on acoustic source localization and tracking (LOCATA). The system is\ndesigned for the direction of arrival (DOA) estimation in single-source\nscenarios. The proposed framework consists of four main building blocks:\npre-processing, voice activity detection (VAD), localization, tracking. The\nsignal pre-processing pipeline includes the short-time Fourier transform (STFT)\nof the multichannel input captured by the array and the cross power spectral\ndensity (CPSD) matrices estimation. The VAD is calculated with a trace-based\nthreshold of the CPSD matrices. The localization is then computed using our\nrecently proposed diagonal unloading (DU) beamforming, which has low-complexity\nand high resolution. The DOA estimation is finally smoothed with a Kalman filer\n(KF). Experimental results on the LOCATA development dataset are reported in\nterms of the root mean square error (RMSE) for a 7-microphone linear array, the\n12-microphone pseudo-spherical array integrated in a prototype head for a\nhumanoid robot, and the 32-microphone spherical array.", "published": "2018-12-04 16:48:52", "link": "http://arxiv.org/abs/1812.01521v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Intensity Particle Flow SMC-PHD Filter For Audio Speaker Tracking", "abstract": "Non-zero diffusion particle flow Sequential Monte Carlo probability\nhypothesis density (NPF-SMC-PHD) filtering has been recently introduced for\nmulti-speaker tracking. However, the NPF does not consider the missing\ndetection which plays a key role in estimation of the number of speakers with\ntheir states. To address this limitation, we propose to use intensity particle\nflow (IPF) in NPFSMC-PHD filter. The proposed method, IPF-SMC-PHD, considers\nthe clutter intensity and detection probability while no data association\nalgorithms are used for the calculation of particle flow. Experiments on the\nLOCATA (acoustic source Localization and Tracking) dataset with the sequences\nof task 4 show that our proposed IPF-SMC-PHD filter improves the tracking\nperformance in terms of estimation accuracy as compared to its baseline\ncounterparts.", "published": "2018-12-04 18:20:27", "link": "http://arxiv.org/abs/1812.01570v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Domain Mismatch Robust Acoustic Scene Classification using Channel\n  Information Conversion", "abstract": "In a recent acoustic scene classification (ASC) research field, training and\ntest device channel mismatch have become an issue for the real world\nimplementation. To address the issue, this paper proposes a channel domain\nconversion using factorized hierarchical variational autoencoder. Proposed\nmethod adapts both the source and target domain to a pre-defined specific\ndomain. Unlike the conventional approach, the relationship between the target\nand source domain and information of each domain are not required in the\nadaptation process. Based on the experimental results using the IEEE detection\nand classification of acoustic scenes and event 2018 task 1-B dataset and the\nbaseline system, it is shown that the proposed approach can mitigate the\nchannel mismatching issue of different recording devices.", "published": "2018-12-04 22:43:33", "link": "http://arxiv.org/abs/1812.01731v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Domain Attentive Fusion for End-to-end Dialect Identification with\n  Unknown Target Domain", "abstract": "End-to-end deep learning language or dialect identification systems operate\non the spectrogram or other acoustic feature and directly generate\nidentification scores for each class. An important issue for end-to-end systems\nis to have some knowledge of the application domain, because the system can be\nvulnerable to use cases that were not seen in the training phase; such a\nscenario is often referred to as a domain mismatched condition. In general, we\nassume that there is enough variation in the training dataset to expose the\nsystem to multiple domains. In this work, we study how to best make use a\ntraining dataset in order to have maximum effectiveness on unknown target\ndomains. Our goal is to process the input without any knowledge of the target\ndomain while preserving robust performance on other domains as well. To\naccomplish this objective, we propose a domain attentive fusion approach for\nend-to-end dialect/language identification systems. To help with\nexperimentation, we collect a dataset from three different domains, and create\nexperimental protocols for a domain mismatched condition. The results of our\nproposed approach, which were tested on a variety of broadcast and YouTube\ndata, shows significant performance gain compared to traditional approaches,\neven without any prior target domain information.", "published": "2018-12-04 16:05:35", "link": "http://arxiv.org/abs/1812.01501v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Singing Voice Separation Using a Deep Convolutional Neural Network\n  Trained by Ideal Binary Mask and Cross Entropy", "abstract": "Separating a singing voice from its music accompaniment remains an important\nchallenge in the field of music information retrieval. We present a unique\nneural network approach inspired by a technique that has revolutionized the\nfield of vision: pixel-wise image classification, which we combine with cross\nentropy loss and pretraining of the CNN as an autoencoder on singing voice\nspectrograms. The pixel-wise classification technique directly estimates the\nsound source label for each time-frequency (T-F) bin in our spectrogram image,\nthus eliminating common pre- and postprocessing tasks. The proposed network is\ntrained by using the Ideal Binary Mask (IBM) as the target output label. The\nIBM identifies the dominant sound source in each T-F bin of the magnitude\nspectrogram of a mixture signal, by considering each T-F bin as a pixel with a\nmulti-label (for each sound source). Cross entropy is used as the training\nobjective, so as to minimize the average probability error between the target\nand predicted label for each pixel. By treating the singing voice separation\nproblem as a pixel-wise classification task, we additionally eliminate one of\nthe commonly used, yet not easy to comprehend, postprocessing steps: the Wiener\nfilter postprocessing.\n  The proposed CNN outperforms the first runner up in the Music Information\nRetrieval Evaluation eXchange (MIREX) 2016 and the winner of MIREX 2014 with a\ngain of 2.2702 ~ 5.9563 dB global normalized source to distortion ratio (GNSDR)\nwhen applied to the iKala dataset. An experiment with the DSD100 dataset on the\nfull-tracks song evaluation task also shows that our model is able to compete\nwith cutting-edge singing voice separation systems which use multi-channel\nmodeling, data augmentation, and model blending.", "published": "2018-12-04 08:47:41", "link": "http://arxiv.org/abs/1812.01278v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "stat.ML", "68-XX, 68Txx"], "primary_category": "cs.SD"}
{"title": "Voice Disorder Detection Using Long Short Term Memory (LSTM) Model", "abstract": "Automated detection of voice disorders with computational methods is a recent\nresearch area in the medical domain since it requires a rigorous endoscopy for\nthe accurate diagnosis. Efficient screening methods are required for the\ndiagnosis of voice disorders so as to provide timely medical facilities in\nminimal resources. Detecting Voice disorder using computational methods is a\nchallenging problem since audio data is continuous due to which extracting\nrelevant features and applying machine learning is hard and unreliable. This\npaper proposes a Long short term memory model (LSTM) to detect pathological\nvoice disorders and evaluates its performance in a real 400 testing samples\nwithout any labels. Different feature extraction methods are used to provide\nthe best set of features before applying LSTM model for classification. The\npaper describes the approach and experiments that show promising results with\n22% sensitivity, 97% specificity and 56% unweighted average recall.", "published": "2018-12-04 00:37:33", "link": "http://arxiv.org/abs/1812.01779v1", "categories": ["q-bio.QM", "cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "q-bio.QM"}
