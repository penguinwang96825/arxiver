{"title": "Opinion Recommendation using Neural Memory Model", "abstract": "We present opinion recommendation, a novel task of jointly predicting a\ncustom review with a rating score that a certain user would give to a certain\nproduct or service, given existing reviews and rating scores to the product or\nservice by other users, and the reviews that the user has given to other\nproducts and services. A characteristic of opinion recommendation is the\nreliance of multiple data sources for multi-task joint learning, which is the\nstrength of neural models. We use a single neural network to model users and\nproducts, capturing their correlation and generating customised product\nrepresentations using a deep memory network, from which customised ratings and\nreviews are constructed jointly. Results show that our opinion recommendation\nsystem gives ratings that are closer to real user ratings on Yelp.com data\ncompared with Yelp's own ratings, and our methods give better results compared\nto several pipelines baselines using state-of-the-art sentiment rating and\nsummarization systems.", "published": "2017-02-06 07:29:01", "link": "http://arxiv.org/abs/1702.01517v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Semantic Parsing over Multiple Knowledge-bases", "abstract": "A fundamental challenge in developing semantic parsers is the paucity of\nstrong supervision in the form of language utterances annotated with logical\nform. In this paper, we propose to exploit structural regularities in language\nin different domains, and train semantic parsers over multiple knowledge-bases\n(KBs), while sharing information across datasets. We find that we can\nsubstantially improve parsing accuracy by training a single\nsequence-to-sequence model over multiple KBs, when providing an encoding of the\ndomain at decoding time. Our model achieves state-of-the-art performance on the\nOvernight dataset (containing eight domains), improves performance over a\nsingle KB baseline from 75.6% to 79.6%, while obtaining a 7x reduction in the\nnumber of model parameters.", "published": "2017-02-06 11:22:15", "link": "http://arxiv.org/abs/1702.01569v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Hybrid Approach For Hindi-English Machine Translation", "abstract": "In this paper, an extended combined approach of phrase based statistical\nmachine translation (SMT), example based MT (EBMT) and rule based MT (RBMT) is\nproposed to develop a novel hybrid data driven MT system capable of\noutperforming the baseline SMT, EBMT and RBMT systems from which it is derived.\nIn short, the proposed hybrid MT process is guided by the rule based MT after\ngetting a set of partial candidate translations provided by EBMT and SMT\nsubsystems. Previous works have shown that EBMT systems are capable of\noutperforming the phrase-based SMT systems and RBMT approach has the strength\nof generating structurally and morphologically more accurate results. This\nhybrid approach increases the fluency, accuracy and grammatical precision which\nimprove the quality of a machine translation system. A comparison of the\nproposed hybrid machine translation (HTM) model with renowned translators i.e.\nGoogle, BING and Babylonian is also presented which shows that the proposed\nmodel works better on sentences with ambiguity as well as comprised of idioms\nthan others.", "published": "2017-02-06 12:19:14", "link": "http://arxiv.org/abs/1702.01587v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Q-WordNet PPV: Simple, Robust and (almost) Unsupervised Generation of\n  Polarity Lexicons for Multiple Languages", "abstract": "This paper presents a simple, robust and (almost) unsupervised\ndictionary-based method, qwn-ppv (Q-WordNet as Personalized PageRanking Vector)\nto automatically generate polarity lexicons. We show that qwn-ppv outperforms\nother automatically generated lexicons for the four extrinsic evaluations\npresented here. It also shows very competitive and robust results with respect\nto manually annotated ones. Results suggest that no single lexicon is best for\nevery task and dataset and that the intrinsic evaluation of polarity lexicons\nis not a good performance indicator on a Sentiment Analysis task. The qwn-ppv\nmethod allows to easily create quality polarity lexicons whenever no\ndomain-based annotated corpora are available for a given language.", "published": "2017-02-06 17:14:29", "link": "http://arxiv.org/abs/1702.01711v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DNN adaptation by automatic quality estimation of ASR hypotheses", "abstract": "In this paper we propose to exploit the automatic Quality Estimation (QE) of\nASR hypotheses to perform the unsupervised adaptation of a deep neural network\nmodeling acoustic probabilities. Our hypothesis is that significant\nimprovements can be achieved by: i)automatically transcribing the evaluation\ndata we are currently trying to recognise, and ii) selecting from it a subset\nof \"good quality\" instances based on the word error rate (WER) scores predicted\nby a QE component. To validate this hypothesis, we run several experiments on\nthe evaluation data sets released for the CHiME-3 challenge. First, we operate\nin oracle conditions in which manual transcriptions of the evaluation data are\navailable, thus allowing us to compute the \"true\" sentence WER. In this\nscenario, we perform the adaptation with variable amounts of data, which are\ncharacterised by different levels of quality. Then, we move to realistic\nconditions in which the manual transcriptions of the evaluation data are not\navailable. In this case, the adaptation is performed on data selected according\nto the WER scores \"predicted\" by a QE component. Our results indicate that: i)\nQE predictions allow us to closely approximate the adaptation results obtained\nin oracle conditions, and ii) the overall ASR performance based on the proposed\nQE-driven adaptation method is significantly better than the strong, most\nrecent, CHiME-3 baseline.", "published": "2017-02-06 17:21:39", "link": "http://arxiv.org/abs/1702.01714v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-task memory networks for category-specific aspect and opinion\n  terms co-extraction", "abstract": "In aspect-based sentiment analysis, most existing methods either focus on\naspect/opinion terms extraction or aspect terms categorization. However, each\ntask by itself only provides partial information to end users. To generate more\ndetailed and structured opinion analysis, we propose a finer-grained problem,\nwhich we call category-specific aspect and opinion terms extraction. This\nproblem involves the identification of aspect and opinion terms within each\nsentence, as well as the categorization of the identified terms. To this end,\nwe propose an end-to-end multi-task attention model, where each task\ncorresponds to aspect/opinion terms extraction for a specific category. Our\nmodel benefits from exploring the commonalities and relationships among\ndifferent tasks to address the data sparsity issue. We demonstrate its\nstate-of-the-art performance on three benchmark datasets.", "published": "2017-02-06 19:55:51", "link": "http://arxiv.org/abs/1702.01776v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ensemble Distillation for Neural Machine Translation", "abstract": "Knowledge distillation describes a method for training a student network to\nperform better by learning from a stronger teacher network. Translating a\nsentence with an Neural Machine Translation (NMT) engine is time expensive and\nhaving a smaller model speeds up this process. We demonstrate how to transfer\nthe translation quality of an ensemble and an oracle BLEU teacher network into\na single NMT system. Further, we present translation improvements from a\nteacher network that has the same architecture and dimensions of the student\nnetwork. As the training of the student model is still expensive, we introduce\na data filtering method based on the knowledge of the teacher model that not\nonly speeds up the training, but also leads to better translation quality. Our\ntechniques need no code change and can be easily reproduced with any NMT\narchitecture to speed up the decoding process.", "published": "2017-02-06 21:49:12", "link": "http://arxiv.org/abs/1702.01802v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beam Search Strategies for Neural Machine Translation", "abstract": "The basic concept in Neural Machine Translation (NMT) is to train a large\nNeural Network that maximizes the translation performance on a given parallel\ncorpus. NMT is then using a simple left-to-right beam-search decoder to\ngenerate new translations that approximately maximize the trained conditional\nprobability. The current beam search strategy generates the target sentence\nword by word from left-to- right while keeping a fixed amount of active\ncandidates at each time step. First, this simple search is less adaptive as it\nalso expands candidates whose scores are much worse than the current best.\nSecondly, it does not expand hypotheses if they are not within the best scoring\ncandidates, even if their scores are close to the best one. The latter one can\nbe avoided by increasing the beam size until no performance improvement can be\nobserved. While you can reach better performance, this has the draw- back of a\nslower decoding speed. In this paper, we concentrate on speeding up the decoder\nby applying a more flexible beam search strategy whose candidate size may vary\nat each time step depending on the candidate scores. We speed up the original\ndecoder by up to 43% for the two language pairs German-English and\nChinese-English without losing any translation quality.", "published": "2017-02-06 22:08:46", "link": "http://arxiv.org/abs/1702.01806v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Living a discrete life in a continuous world: Reference with distributed\n  representations", "abstract": "Reference is a crucial property of language that allows us to connect\nlinguistic expressions to the world. Modeling it requires handling both\ncontinuous and discrete aspects of meaning. Data-driven models excel at the\nformer, but struggle with the latter, and the reverse is true for symbolic\nmodels.\n  This paper (a) introduces a concrete referential task to test both aspects,\ncalled cross-modal entity tracking; (b) proposes a neural network architecture\nthat uses external memory to build an entity library inspired in the DRSs of\nDRT, with a mechanism to dynamically introduce new referents or add information\nto referents that are already in the library.\n  Our model shows promise: it beats traditional neural network architectures on\nthe task. However, it is still outperformed by Memory Networks, another model\nwith external memory.", "published": "2017-02-06 22:50:49", "link": "http://arxiv.org/abs/1702.01815v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
