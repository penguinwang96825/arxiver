{"title": "SJTU-NICT's Supervised and Unsupervised Neural Machine Translation\n  Systems for the WMT20 News Translation Task", "abstract": "In this paper, we introduced our joint team SJTU-NICT 's participation in the\nWMT 2020 machine translation shared task. In this shared task, we participated\nin four translation directions of three language pairs: English-Chinese,\nEnglish-Polish on supervised machine translation track, German-Upper Sorbian on\nlow-resource and unsupervised machine translation tracks. Based on different\nconditions of language pairs, we have experimented with diverse neural machine\ntranslation (NMT) techniques: document-enhanced NMT, XLM pre-trained language\nmodel enhanced NMT, bidirectional translation as a pre-training, reference\nlanguage based UNMT, data-dependent gaussian prior objective, and BT-BLEU\ncollaborative filtering self-training. We also used the TF-IDF algorithm to\nfilter the training set to obtain a domain more similar set with the test set\nfor finetuning. In our submissions, the primary systems won the first place on\nEnglish to Chinese, Polish to English, and German to Upper Sorbian translation\ndirections.", "published": "2020-10-11 00:40:05", "link": "http://arxiv.org/abs/2010.05122v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Document-Level Definition Detection in Scholarly Documents: Existing\n  Models, Error Analyses, and Future Directions", "abstract": "The task of definition detection is important for scholarly papers, because\npapers often make use of technical terminology that may be unfamiliar to\nreaders. Despite prior work on definition detection, current approaches are far\nfrom being accurate enough to use in real-world applications. In this paper, we\nfirst perform in-depth error analysis of the current best performing definition\ndetection system and discover major causes of errors. Based on this analysis,\nwe develop a new definition detection system, HEDDEx, that utilizes syntactic\nfeatures, transformer encoders, and heuristic filters, and evaluate it on a\nstandard sentence-level benchmark. Because current benchmarks evaluate randomly\nsampled sentences, we propose an alternative evaluation that assesses every\nsentence within a document. This allows for evaluating recall in addition to\nprecision. HEDDEx outperforms the leading system on both the sentence-level and\nthe document-level tasks, by 12.7 F1 points and 14.4 F1 points, respectively.\nWe note that performance on the high-recall document-level task is much lower\nthan in the standard evaluation approach, due to the necessity of incorporation\nof document structure as features. We discuss remaining challenges in\ndocument-level definition detection, ideas for improvements, and potential\nissues for the development of reading aid applications.", "published": "2020-10-11 01:16:10", "link": "http://arxiv.org/abs/2010.05129v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CDEvalSumm: An Empirical Study of Cross-Dataset Evaluation for Neural\n  Summarization Systems", "abstract": "Neural network-based models augmented with unsupervised pre-trained knowledge\nhave achieved impressive performance on text summarization. However, most\nexisting evaluation methods are limited to an in-domain setting, where\nsummarizers are trained and evaluated on the same dataset. We argue that this\napproach can narrow our understanding of the generalization ability for\ndifferent summarization systems. In this paper, we perform an in-depth analysis\nof characteristics of different datasets and investigate the performance of\ndifferent summarization models under a cross-dataset setting, in which a\nsummarizer trained on one corpus will be evaluated on a range of out-of-domain\ncorpora. A comprehensive study of 11 representative summarization systems on 5\ndatasets from different domains reveals the effect of model architectures and\ngeneration ways (i.e. abstractive and extractive) on model generalization\nability. Further, experimental results shed light on the limitations of\nexisting summarizers. Brief introduction and supplementary code can be found in\nhttps://github.com/zide05/CDEvalSumm.", "published": "2020-10-11 02:19:15", "link": "http://arxiv.org/abs/2010.05139v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Plan ahead: Self-Supervised Text Planning for Paragraph Completion Task", "abstract": "Despite the recent success of contextualized language models on various NLP\ntasks, language model itself cannot capture textual coherence of a long,\nmulti-sentence document (e.g., a paragraph). Humans often make structural\ndecisions on what and how to say about before making utterances. Guiding\nsurface realization with such high-level decisions and structuring text in a\ncoherent way is essentially called a planning process. Where can the model\nlearn such high-level coherence? A paragraph itself contains various forms of\ninductive coherence signals called self-supervision in this work, such as\nsentence orders, topical keywords, rhetorical structures, and so on. Motivated\nby that, this work proposes a new paragraph completion task PARCOM; predicting\nmasked sentences in a paragraph. However, the task suffers from predicting and\nselecting appropriate topical content with respect to the given context. To\naddress that, we propose a self-supervised text planner SSPlanner that predicts\nwhat to say first (content prediction), then guides the pretrained language\nmodel (surface realization) using the predicted content. SSPlanner outperforms\nthe baseline generation models on the paragraph completion task in both\nautomatic and human evaluation. We also find that a combination of noun and\nverb types of keywords is the most effective for content selection. As more\nnumber of content keywords are provided, overall generation quality also\nincreases.", "published": "2020-10-11 02:38:21", "link": "http://arxiv.org/abs/2010.05141v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PHICON: Improving Generalization of Clinical Text De-identification\n  Models via Data Augmentation", "abstract": "De-identification is the task of identifying protected health information\n(PHI) in the clinical text. Existing neural de-identification models often fail\nto generalize to a new dataset. We propose a simple yet effective data\naugmentation method PHICON to alleviate the generalization issue. PHICON\nconsists of PHI augmentation and Context augmentation, which creates augmented\ntraining corpora by replacing PHI entities with named-entities sampled from\nexternal sources, and by changing background context with synonym replacement\nor random word insertion, respectively. Experimental results on the i2b2 2006\nand 2014 de-identification challenge datasets show that PHICON can help three\nselected de-identification models boost F1-score (by at most 8.6%) on\ncross-dataset test setting. We also discuss how much augmentation to use and\nhow each augmentation method influences the performance.", "published": "2020-10-11 02:57:11", "link": "http://arxiv.org/abs/2010.05143v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A General Model of Conversational Dynamics and an Example Application in\n  Serious Illness Communication", "abstract": "Conversation has been a primary means for the exchange of information since\nancient times. Understanding patterns of information flow in conversations is a\ncritical step in assessing and improving communication quality. In this paper,\nwe describe COnversational DYnamics Model (CODYM) analysis, a novel approach\nfor studying patterns of information flow in conversations. CODYMs are Markov\nModels that capture sequential dependencies in the lengths of speaker turns.\nThe proposed method is automated and scalable, and preserves the privacy of the\nconversational participants. The primary function of CODYM analysis is to\nquantify and visualize patterns of information flow, concisely summarized over\nsequential turns from one or more conversations. Our approach is general and\ncomplements existing methods, providing a new tool for use in the analysis of\nany type of conversation. As an important first application, we demonstrate the\nmodel on transcribed conversations between palliative care clinicians and\nseriously ill patients. These conversations are dynamic and complex, taking\nplace amidst heavy emotions, and include difficult topics such as end-of-life\npreferences and patient values. We perform a versatile set of CODYM analyses\nthat (a) establish the validity of the model by confirming known patterns of\nconversational turn-taking and word usage, (b) identify normative patterns of\ninformation flow in serious illness conversations, and (c) show how these\npatterns vary across narrative time and differ under expressions of anger, fear\nand sadness. Potential applications of CODYMs range from assessment and\ntraining of effective healthcare communication to comparing conversational\ndynamics across language and culture, with the prospect of identifying\nuniversal similarities and unique \"fingerprints\" of information flow.", "published": "2020-10-11 04:33:03", "link": "http://arxiv.org/abs/2010.05164v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Foodborne Illness Complaints in Multiple Languages Using\n  English Annotations Only", "abstract": "Health departments have been deploying text classification systems for the\nearly detection of foodborne illness complaints in social media documents such\nas Yelp restaurant reviews. Current systems have been successfully applied for\ndocuments in English and, as a result, a promising direction is to increase\ncoverage and recall by considering documents in additional languages, such as\nSpanish or Chinese. Training previous systems for more languages, however,\nwould be expensive, as it would require the manual annotation of many documents\nfor each new target language. To address this challenge, we consider\ncross-lingual learning and train multilingual classifiers using only the\nannotations for English-language reviews. Recent zero-shot approaches based on\npre-trained multi-lingual BERT (mBERT) have been shown to effectively align\nlanguages for aspects such as sentiment. Interestingly, we show that those\napproaches are less effective for capturing the nuances of foodborne illness,\nour public health application of interest. To improve performance without extra\nannotations, we create artificial training documents in the target language\nthrough machine translation and train mBERT jointly for the source (English)\nand target language. Furthermore, we show that translating labeled documents to\nmultiple languages leads to additional performance improvements for some target\nlanguages. We demonstrate the benefits of our approach through extensive\nexperiments with Yelp restaurant reviews in seven languages. Our classifiers\nidentify foodborne illness complaints in multilingual reviews from the Yelp\nChallenge dataset, which highlights the potential of our general approach for\ndeployment in health departments.", "published": "2020-10-11 08:46:17", "link": "http://arxiv.org/abs/2010.05194v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Machine Translation of Mathematical Text", "abstract": "We have implemented a machine translation system, the PolyMath Translator,\nfor LaTeX documents containing mathematical text. The current implementation\ntranslates English LaTeX to French LaTeX, attaining a BLEU score of 53.5 on a\nheld-out test corpus of mathematical sentences. It produces LaTeX documents\nthat can be compiled to PDF without further editing. The system first converts\nthe body of an input LaTeX document into English sentences containing math\ntokens, using the pandoc universal document converter to parse LaTeX input. We\nhave trained a Transformer-based translator model, using OpenNMT, on a combined\ncorpus containing a small proportion of domain-specific sentences. Our full\nsystem uses both this Transformer model and Google Translate, the latter being\nused as a backup to better handle linguistic features that do not appear in our\ntraining dataset. If the Transformer model does not have confidence in its\ntranslation, as determined by a high perplexity score, then we use Google\nTranslate with a custom glossary. This backup was used 26% of the time on our\ntest corpus of mathematical sentences. The PolyMath Translator is available as\na web service at www.polymathtrans.ai.", "published": "2020-10-11 11:59:40", "link": "http://arxiv.org/abs/2010.05229v1", "categories": ["cs.CL", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Controllable Multi-Character Psychology-Oriented Story Generation", "abstract": "Story generation, which aims to generate a long and coherent story\nautomatically based on the title or an input sentence, is an important research\narea in the field of natural language generation. There is relatively little\nwork on story generation with appointed emotions. Most existing works focus on\nusing only one specific emotion to control the generation of a whole story and\nignore the emotional changes in the characters in the course of the story. In\nour work, we aim to design an emotional line for each character that considers\nmultiple emotions common in psychological theories, with the goal of generating\nstories with richer emotional changes in the characters. To the best of our\nknowledge, this work is first to focuses on characters' emotional lines in\nstory generation. We present a novel model-based attention mechanism that we\ncall SoCP (Storytelling of multi-Character Psychology). We show that the\nproposed model can generate stories considering the changes in the\npsychological state of different characters. To take into account the\nparticularity of the model, in addition to commonly used evaluation\nindicators(BLEU, ROUGE, etc.), we introduce the accuracy rate of psychological\nstate control as a novel evaluation metric. The new indicator reflects the\neffect of the model on the psychological state control of story characters.\nExperiments show that with SoCP, the generated stories follow the psychological\nstate for each character according to both automatic and human evaluations.", "published": "2020-10-11 12:05:00", "link": "http://arxiv.org/abs/2010.05230v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Accurate and Reliable Energy Measurement of NLP Models", "abstract": "Accurate and reliable measurement of energy consumption is critical for\nmaking well-informed design choices when choosing and training large scale NLP\nmodels. In this work, we show that existing software-based energy measurements\nare not accurate because they do not take into account hardware differences and\nhow resource utilization affects energy consumption. We conduct energy\nmeasurement experiments with four different models for a question answering\ntask. We quantify the error of existing software-based energy measurements by\nusing a hardware power meter that provides highly accurate energy measurements.\nOur key takeaway is the need for a more accurate energy estimation model that\ntakes into account hardware variabilities and the non-linear relationship\nbetween resource utilization and energy consumption. We release the code and\ndata at https://github.com/csarron/sustainlp2020-energy.", "published": "2020-10-11 13:44:52", "link": "http://arxiv.org/abs/2010.05248v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Prediction of Medieval Arabic Diacritics", "abstract": "This study uses a character level neural machine translation approach trained\non a long short-term memory-based bi-directional recurrent neural network\narchitecture for diacritization of Medieval Arabic. The results improve from\nthe online tool used as a baseline. A diacritization model have been published\nopenly through an easy to use Python package available on PyPi and Zenodo. We\nhave found that context size should be considered when optimizing a feasible\nprediction model.", "published": "2020-10-11 15:21:01", "link": "http://arxiv.org/abs/2010.05269v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TransQuest at WMT2020: Sentence-Level Direct Assessment", "abstract": "This paper presents the team TransQuest's participation in Sentence-Level\nDirect Assessment shared task in WMT 2020. We introduce a simple QE framework\nbased on cross-lingual transformers, and we use it to implement and evaluate\ntwo different neural architectures. The proposed methods achieve\nstate-of-the-art results surpassing the results obtained by OpenKiwi, the\nbaseline used in the shared task. We further fine tune the QE framework by\nperforming ensemble and data augmentation. Our approach is the winning solution\nin all of the language pairs according to the WMT 2020 official results.", "published": "2020-10-11 18:53:05", "link": "http://arxiv.org/abs/2010.05318v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incremental Processing in the Age of Non-Incremental Encoders: An\n  Empirical Assessment of Bidirectional Models for Incremental NLU", "abstract": "While humans process language incrementally, the best language encoders\ncurrently used in NLP do not. Both bidirectional LSTMs and Transformers assume\nthat the sequence that is to be encoded is available in full, to be processed\neither forwards and backwards (BiLSTMs) or as a whole (Transformers). We\ninvestigate how they behave under incremental interfaces, when partial output\nmust be provided based on partial input seen up to a certain time step, which\nmay happen in interactive systems. We test five models on various NLU datasets\nand compare their performance using three incremental evaluation metrics. The\nresults support the possibility of using bidirectional encoders in incremental\nmode while retaining most of their non-incremental quality. The\n\"omni-directional\" BERT model, which achieves better non-incremental\nperformance, is impacted more by the incremental access. This can be alleviated\nby adapting the training regime (truncated training), or the testing procedure,\nby delaying the output until some right context is available or by\nincorporating hypothetical right contexts generated by a language model like\nGPT-2.", "published": "2020-10-11 19:51:21", "link": "http://arxiv.org/abs/2010.05330v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation Doesn't Translate Gender Coreference Right\n  Unless You Make It", "abstract": "Neural Machine Translation (NMT) has been shown to struggle with grammatical\ngender that is dependent on the gender of human referents, which can cause\ngender bias effects. Many existing approaches to this problem seek to control\ngender inflection in the target language by explicitly or implicitly adding a\ngender feature to the source sentence, usually at the sentence level.\n  In this paper we propose schemes for incorporating explicit word-level gender\ninflection tags into NMT. We explore the potential of this gender-inflection\ncontrolled translation when the gender feature can be determined from a human\nreference, or when a test sentence can be automatically gender-tagged,\nassessing on English-to-Spanish and English-to-German translation.\n  We find that simple existing approaches can over-generalize a gender-feature\nto multiple entities in a sentence, and suggest effective alternatives in the\nform of tagged coreference adaptation data. We also propose an extension to\nassess translations of gender-neutral entities from English given a\ncorresponding linguistic convention, such as a non-binary inflection, in the\ntarget language.", "published": "2020-10-11 20:05:42", "link": "http://arxiv.org/abs/2010.05332v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Addressing Exposure Bias With Document Minimum Risk Training: Cambridge\n  at the WMT20 Biomedical Translation Task", "abstract": "The 2020 WMT Biomedical translation task evaluated Medline abstract\ntranslations. This is a small-domain translation task, meaning limited relevant\ntraining data with very distinct style and vocabulary. Models trained on such\ndata are susceptible to exposure bias effects, particularly when training\nsentence pairs are imperfect translations of each other. This can result in\npoor behaviour during inference if the model learns to neglect the source\nsentence.\n  The UNICAM entry addresses this problem during fine-tuning using a robust\nvariant on Minimum Risk Training. We contrast this approach with data-filtering\nto remove `problem' training examples. Under MRT fine-tuning we obtain good\nresults for both directions of English-German and English-Spanish biomedical\ntranslation. In particular we achieve the best English-to-Spanish translation\nresult and second-best Spanish-to-English result, despite using only single\nmodels with no ensembling.", "published": "2020-10-11 20:09:43", "link": "http://arxiv.org/abs/2010.05333v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "We Can Detect Your Bias: Predicting the Political Ideology of News\n  Articles", "abstract": "We explore the task of predicting the leading political ideology or bias of\nnews articles. First, we collect and release a large dataset of 34,737 articles\nthat were manually annotated for political ideology -left, center, or right-,\nwhich is well-balanced across both topics and media. We further use a\nchallenging experimental setup where the test examples come from media that\nwere not seen during training, which prevents the model from learning to detect\nthe source of the target news article instead of predicting its political\nideology. From a modeling perspective, we propose an adversarial media\nadaptation, as well as a specially adapted triplet loss. We further add\nbackground information about the source, and we show that it is quite helpful\nfor improving article-level prediction. Our experimental results show very\nsizable improvements over using state-of-the-art pre-trained Transformers in\nthis challenging setup.", "published": "2020-10-11 20:27:55", "link": "http://arxiv.org/abs/2010.05338v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Language Embeddings Capture Scales?", "abstract": "Pretrained Language Models (LMs) have been shown to possess significant\nlinguistic, common sense, and factual knowledge. One form of knowledge that has\nnot been studied yet in this context is information about the scalar magnitudes\nof objects. We show that pretrained language models capture a significant\namount of this information but are short of the capability required for general\ncommon-sense reasoning. We identify contextual information in pre-training and\nnumeracy as two key factors affecting their performance and show that a simple\nmethod of canonicalizing numbers can have a significant effect on the results.", "published": "2020-10-11 21:11:09", "link": "http://arxiv.org/abs/2010.05345v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Learning Which Features Matter: RoBERTa Acquires a Preference for\n  Linguistic Generalizations (Eventually)", "abstract": "One reason pretraining on self-supervised linguistic tasks is effective is\nthat it teaches models features that are helpful for language understanding.\nHowever, we want pretrained models to learn not only to represent linguistic\nfeatures, but also to use those features preferentially during fine-turning.\nWith this goal in mind, we introduce a new English-language diagnostic set\ncalled MSGS (the Mixed Signals Generalization Set), which consists of 20\nambiguous binary classification tasks that we use to test whether a pretrained\nmodel prefers linguistic or surface generalizations during fine-tuning. We\npretrain RoBERTa models from scratch on quantities of data ranging from 1M to\n1B words and compare their performance on MSGS to the publicly available\nRoBERTa-base. We find that models can learn to represent linguistic features\nwith little pretraining data, but require far more data to learn to prefer\nlinguistic generalizations over surface ones. Eventually, with about 30B words\nof pretraining data, RoBERTa-base does demonstrate a linguistic bias with some\nregularity. We conclude that while self-supervised pretraining is an effective\nway to learn helpful inductive biases, there is likely room to improve the rate\nat which models learn which features matter.", "published": "2020-10-11 22:09:27", "link": "http://arxiv.org/abs/2010.05358v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantitative Argument Summarization and Beyond: Cross-Domain Key Point\n  Analysis", "abstract": "When summarizing a collection of views, arguments or opinions on some topic,\nit is often desirable not only to extract the most salient points, but also to\nquantify their prevalence. Work on multi-document summarization has\ntraditionally focused on creating textual summaries, which lack this\nquantitative aspect. Recent work has proposed to summarize arguments by mapping\nthem to a small set of expert-generated key points, where the salience of each\nkey point corresponds to the number of its matching arguments. The current work\nadvances key point analysis in two important respects: first, we develop a\nmethod for automatic extraction of key points, which enables fully automatic\nanalysis, and is shown to achieve performance comparable to a human expert.\nSecond, we demonstrate that the applicability of key point analysis goes well\nbeyond argumentation data. Using models trained on publicly available\nargumentation datasets, we achieve promising results in two additional domains:\nmunicipal surveys and user reviews. An additional contribution is an in-depth\nevaluation of argument-to-key point matching models, where we substantially\noutperform previous results.", "published": "2020-10-11 23:01:51", "link": "http://arxiv.org/abs/2010.05369v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "fairseq S2T: Fast Speech-to-Text Modeling with fairseq", "abstract": "We introduce fairseq S2T, a fairseq extension for speech-to-text (S2T)\nmodeling tasks such as end-to-end speech recognition and speech-to-text\ntranslation. It follows fairseq's careful design for scalability and\nextensibility. We provide end-to-end workflows from data pre-processing, model\ntraining to offline (online) inference. We implement state-of-the-art\nRNN-based, Transformer-based as well as Conformer-based models and open-source\ndetailed training recipes. Fairseq's machine translation models and language\nmodels can be seamlessly integrated into S2T workflows for multi-task learning\nor transfer learning. Fairseq S2T documentation and examples are available at\nhttps://github.com/pytorch/fairseq/tree/master/examples/speech_to_text.", "published": "2020-10-11 05:36:54", "link": "http://arxiv.org/abs/2010.05171v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Lexically Cohesive Neural Machine Translation with Copy Mechanism", "abstract": "Lexically cohesive translations preserve consistency in word choices in\ndocument-level translation. We employ a copy mechanism into a context-aware\nneural machine translation model to allow copying words from previous\ntranslation outputs. Different from previous context-aware neural machine\ntranslation models that handle all the discourse phenomena implicitly, our\nmodel explicitly addresses the lexical cohesion problem by boosting the\nprobabilities to output words consistently. We conduct experiments on Japanese\nto English translation using an evaluation dataset for discourse translation.\nThe results showed that the proposed model significantly improved lexical\ncohesion compared to previous context-aware models.", "published": "2020-10-11 08:39:02", "link": "http://arxiv.org/abs/2010.05193v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Connecting the Dots Between Fact Verification and Fake News Detection", "abstract": "Fact verification models have enjoyed a fast advancement in the last two\nyears with the development of pre-trained language models like BERT and the\nrelease of large scale datasets such as FEVER. However, the challenging problem\nof fake news detection has not benefited from the improvement of fact\nverification models, which is closely related to fake news detection. In this\npaper, we propose a simple yet effective approach to connect the dots between\nfact verification and fake news detection. Our approach first employs a text\nsummarization model pre-trained on news corpora to summarize the long news\narticle into a short claim. Then we use a fact verification model pre-trained\non the FEVER dataset to detect whether the input news article is real or fake.\nOur approach makes use of the recent success of fact verification models and\nenables zero-shot fake news detection, alleviating the need of large-scale\ntraining data to train fake news detection models. Experimental results on\nFakenewsNet, a benchmark dataset for fake news detection, demonstrate the\neffectiveness of our proposed approach.", "published": "2020-10-11 09:28:52", "link": "http://arxiv.org/abs/2010.05202v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "End to End Binarized Neural Networks for Text Classification", "abstract": "Deep neural networks have demonstrated their superior performance in almost\nevery Natural Language Processing task, however, their increasing complexity\nraises concerns. In particular, these networks require high expenses on\ncomputational hardware, and training budget is a concern for many. Even for a\ntrained network, the inference phase can be too demanding for\nresource-constrained devices, thus limiting its applicability. The\nstate-of-the-art transformer models are a vivid example. Simplifying the\ncomputations performed by a network is one way of relaxing the complexity\nrequirements. In this paper, we propose an end to end binarized neural network\narchitecture for the intent classification task. In order to fully utilize the\npotential of end to end binarization, both input representations (vector\nembeddings of tokens statistics) and the classifier are binarized. We\ndemonstrate the efficiency of such architecture on the intent classification of\nshort texts over three datasets and for text classification with a larger\ndataset. The proposed architecture achieves comparable to the state-of-the-art\nresults on standard intent classification datasets while utilizing ~ 20-40%\nlesser memory and training time. Furthermore, the individual components of the\narchitecture, such as binarized vector embeddings of documents or binarized\nclassifiers, can be used separately with not necessarily fully binary\narchitectures.", "published": "2020-10-11 11:21:53", "link": "http://arxiv.org/abs/2010.05223v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Data Agnostic RoBERTa-based Natural Language to SQL Query Generation", "abstract": "Relational databases are among the most widely used architectures to store\nmassive amounts of data in the modern world. However, there is a barrier\nbetween these databases and the average user. The user often lacks the\nknowledge of a query language such as SQL required to interact with the\ndatabase. The NL2SQL task aims at finding deep learning approaches to solve\nthis problem by converting natural language questions into valid SQL queries.\nGiven the sensitive nature of some databases and the growing need for data\nprivacy, we have presented an approach with data privacy at its core. We have\npassed RoBERTa embeddings and data-agnostic knowledge vectors into LSTM based\nsubmodels to predict the final query. Although we have not achieved state of\nthe art results, we have eliminated the need for the table data, right from the\ntraining of the model, and have achieved a test set execution accuracy of\n76.7%. By eliminating the table data dependency while training we have created\na model capable of zero shot learning based on the natural language question\nand table schema alone.", "published": "2020-10-11 13:18:46", "link": "http://arxiv.org/abs/2010.05243v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Few-shot Learning for Multi-label Intent Detection", "abstract": "In this paper, we study the few-shot multi-label classification for user\nintent detection. For multi-label intent detection, state-of-the-art work\nestimates label-instance relevance scores and uses a threshold to select\nmultiple associated intent labels. To determine appropriate thresholds with\nonly a few examples, we first learn universal thresholding experience on\ndata-rich domains, and then adapt the thresholds to certain few-shot domains\nwith a calibration based on nonparametric learning. For better calculation of\nlabel-instance relevance score, we introduce label name embedding as anchor\npoints in representation space, which refines representations of different\nclasses to be well-separated from each other. Experiments on two datasets show\nthat the proposed model significantly outperforms strong baselines in both\none-shot and five-shot settings.", "published": "2020-10-11 14:42:18", "link": "http://arxiv.org/abs/2010.05256v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Distillation of Syntactic Information from Contextualized\n  Word Representations", "abstract": "Contextualized word representations, such as ELMo and BERT, were shown to\nperform well on various semantic and syntactic tasks. In this work, we tackle\nthe task of unsupervised disentanglement between semantics and structure in\nneural language representations: we aim to learn a transformation of the\ncontextualized vectors, that discards the lexical semantics, but keeps the\nstructural information. To this end, we automatically generate groups of\nsentences which are structurally similar but semantically different, and use\nmetric-learning approach to learn a transformation that emphasizes the\nstructural component that is encoded in the vectors. We demonstrate that our\ntransformation clusters vectors in space by structural properties, rather than\nby lexical semantics. Finally, we demonstrate the utility of our distilled\nrepresentations by showing that they outperform the original contextualized\nrepresentations in a few-shot parsing setting.", "published": "2020-10-11 15:13:18", "link": "http://arxiv.org/abs/2010.05265v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilingual Offensive Language Identification with Cross-lingual\n  Embeddings", "abstract": "Offensive content is pervasive in social media and a reason for concern to\ncompanies and government organizations. Several studies have been recently\npublished investigating methods to detect the various forms of such content\n(e.g. hate speech, cyberbulling, and cyberaggression). The clear majority of\nthese studies deal with English partially because most annotated datasets\navailable contain English data. In this paper, we take advantage of English\ndata available by applying cross-lingual contextual word embeddings and\ntransfer learning to make predictions in languages with less resources. We\nproject predictions on comparable data in Bengali, Hindi, and Spanish and we\nreport results of 0.8415 F1 macro for Bengali, 0.8568 F1 macro for Hindi, and\n0.7513 F1 macro for Spanish. Finally, we show that our approach compares\nfavorably to the best systems submitted to recent shared tasks on these three\nlanguages, confirming the robustness of cross-lingual contextual embeddings and\ntransfer learning for this task.", "published": "2020-10-11 19:17:24", "link": "http://arxiv.org/abs/2010.05324v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "InfoMiner at WNUT-2020 Task 2: Transformer-based Covid-19 Informative\n  Tweet Extraction", "abstract": "Identifying informative tweets is an important step when building information\nextraction systems based on social media. WNUT-2020 Task 2 was organised to\nrecognise informative tweets from noise tweets. In this paper, we present our\napproach to tackle the task objective using transformers. Overall, our approach\nachieves 10th place in the final rankings scoring 0.9004 F1 score for the test\nset.", "published": "2020-10-11 19:31:18", "link": "http://arxiv.org/abs/2010.05327v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ComStreamClust: a communicative multi-agent approach to text clustering\n  in streaming data", "abstract": "Topic detection is the task of determining and tracking hot topics in social\nmedia. Twitter is arguably the most popular platform for people to share their\nideas with others about different issues. One such prevalent issue is the\nCOVID-19 pandemic. Detecting and tracking topics on these kinds of issues would\nhelp governments and healthcare companies deal with this phenomenon. In this\npaper, we propose a novel, multi-agent, communicative clustering approach,\nso-called ComStreamClust for clustering sub-topics inside a broader topic,\ne.g., COVID-19. The proposed approach is parallelizable, and can simultaneously\nhandle several data-point. The LaBSE sentence embedding is used to measure the\nsemantic similarity between two tweets. ComStreamClust has been evaluated on\ntwo datasets: the COVID-19 and the FA CUP. The results obtained from\nComStreamClust approve the effectiveness of the proposed approach when compared\nto existing methods.", "published": "2020-10-11 21:19:19", "link": "http://arxiv.org/abs/2010.05349v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Knowledge-Driven Approach to Classifying Object and Attribute\n  Coreferences in Opinion Mining", "abstract": "Classifying and resolving coreferences of objects (e.g., product names) and\nattributes (e.g., product aspects) in opinionated reviews is crucial for\nimproving the opinion mining performance. However, the task is challenging as\none often needs to consider domain-specific knowledge (e.g., iPad is a tablet\nand has aspect resolution) to identify coreferences in opinionated reviews.\nAlso, compiling a handcrafted and curated domain-specific knowledge base for\neach domain is very time consuming and arduous. This paper proposes an approach\nto automatically mine and leverage domain-specific knowledge for classifying\nobjects and attribute coreferences. The approach extracts domain-specific\nknowledge from unlabeled review data and trains a knowledgeaware neural\ncoreference classification model to leverage (useful) domain knowledge together\nwith general commonsense knowledge for the task. Experimental evaluation on\nrealworld datasets involving five domains (product types) shows the\neffectiveness of the approach.", "published": "2020-10-11 22:08:43", "link": "http://arxiv.org/abs/2010.05357v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Safe Reinforcement Learning with Natural Language Constraints", "abstract": "While safe reinforcement learning (RL) holds great promise for many practical\napplications like robotics or autonomous cars, current approaches require\nspecifying constraints in mathematical form. Such specifications demand domain\nexpertise, limiting the adoption of safe RL. In this paper, we propose learning\nto interpret natural language constraints for safe RL. To this end, we first\nintroduce HazardWorld, a new multi-task benchmark that requires an agent to\noptimize reward while not violating constraints specified in free-form text. We\nthen develop an agent with a modular architecture that can interpret and adhere\nto such textual constraints while learning new tasks. Our model consists of (1)\na constraint interpreter that encodes textual constraints into spatial and\ntemporal representations of forbidden states, and (2) a policy network that\nuses these representations to produce a policy achieving minimal constraint\nviolations during training. Across different domains in HazardWorld, we show\nthat our method achieves higher rewards (up to11x) and fewer constraint\nviolations (by 1.8x) compared to existing approaches. However, in terms of\nabsolute performance, HazardWorld still poses significant challenges for agents\nto learn efficiently, motivating the need for future work.", "published": "2020-10-11 03:41:56", "link": "http://arxiv.org/abs/2010.05150v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Constructing a Visual Relationship Authenticity Dataset", "abstract": "A visual relationship denotes a relationship between two objects in an image,\nwhich can be represented as a triplet of (subject; predicate; object). Visual\nrelationship detection is crucial for scene understanding in images. Existing\nvisual relationship detection datasets only contain true relationships that\ncorrectly describe the content in an image. However, distinguishing false\nvisual relationships from true ones is also crucial for image understanding and\ngrounded natural language processing. In this paper, we construct a visual\nrelationship authenticity dataset, where both true and false relationships\namong all objects appeared in the captions in the Flickr30k entities image\ncaption dataset are annotated. The dataset is available at\nhttps://github.com/codecreator2053/VR_ClassifiedDataset. We hope that this\ndataset can promote the study on both vision and language understanding.", "published": "2020-10-11 07:38:33", "link": "http://arxiv.org/abs/2010.05185v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Learning Adaptive Language Interfaces through Decomposition", "abstract": "Our goal is to create an interactive natural language interface that\nefficiently and reliably learns from users to complete tasks in simulated\nrobotics settings. We introduce a neural semantic parsing system that learns\nnew high-level abstractions through decomposition: users interactively teach\nthe system by breaking down high-level utterances describing novel behavior\ninto low-level steps that it can understand. Unfortunately, existing methods\neither rely on grammars which parse sentences with limited flexibility, or\nneural sequence-to-sequence models that do not learn efficiently or reliably\nfrom individual examples. Our approach bridges this gap, demonstrating the\nflexibility of modern neural systems, as well as the one-shot reliable\ngeneralization of grammar-based methods. Our crowdsourced interactive\nexperiments suggest that over time, users complete complex tasks more\nefficiently while using our system by leveraging what they just taught. At the\nsame time, getting users to trust the system enough to be incentivized to teach\nhigh-level utterances is still an ongoing challenge. We end with a discussion\nof some of the obstacles we need to overcome to fully realize the potential of\nthe interactive paradigm.", "published": "2020-10-11 08:27:07", "link": "http://arxiv.org/abs/2010.05190v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "A Defeasible Calculus for Zetetic Agents", "abstract": "The study of defeasible reasoning unites epistemologists with those working\nin AI, in part, because both are interested in epistemic rationality. While it\nis traditionally thought to govern the formation and (with)holding of beliefs,\nepistemic rationality may also apply to the interrogative attitudes associated\nwith our core epistemic practice of inquiry, such as wondering, investigating,\nand curiosity. Since generally intelligent systems should be capable of\nrational inquiry, AI researchers have a natural interest in the norms that\ngovern interrogative attitudes. Following its recent coinage, we use the term\n\"zetetic\" to refer to the properties and norms associated with the capacity to\ninquire. In this paper, we argue that zetetic norms can be modeled via\ndefeasible inferences to and from questions---a.k.a erotetic inferences---in a\nmanner similar to the way norms of epistemic rationality are represented by\ndefeasible inference rules. We offer a sequent calculus that accommodates the\nunique features of \"erotetic defeat\" and that exhibits the computational\nproperties needed to inform the design of zetetic agents. The calculus\npresented here is an improved version of the one presented in Millson (2019),\nextended to cover a new class of defeasible erotetic inferences.", "published": "2020-10-11 17:39:03", "link": "http://arxiv.org/abs/2010.05293v1", "categories": ["cs.AI", "cs.CL", "math.LO"], "primary_category": "cs.AI"}
{"title": "Weakly Supervised Medication Regimen Extraction from Medical\n  Conversations", "abstract": "Automated Medication Regimen (MR) extraction from medical conversations can\nnot only improve recall and help patients follow through with their care plan,\nbut also reduce the documentation burden for doctors. In this paper, we focus\non extracting spans for frequency, route and change, corresponding to\nmedications discussed in the conversation. We first describe a unique dataset\nof annotated doctor-patient conversations and then present a weakly supervised\nmodel architecture that can perform span extraction using noisy classification\ndata. The model utilizes an attention bottleneck inside a classification model\nto perform the extraction. We experiment with several variants of attention\nscoring and projection functions and propose a novel transformer-based\nattention scoring function (TAScore). The proposed combination of TAScore and\nFusedmax projection achieves a 10 point increase in Longest Common Substring F1\ncompared to the baseline of additive scoring plus softmax projection.", "published": "2020-10-11 18:53:03", "link": "http://arxiv.org/abs/2010.05317v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GiantMIDI-Piano: A large-scale MIDI dataset for classical piano music", "abstract": "Symbolic music datasets are important for music information retrieval and\nmusical analysis. However, there is a lack of large-scale symbolic datasets for\nclassical piano music. In this article, we create a GiantMIDI-Piano (GP)\ndataset containing 38,700,838 transcribed notes and 10,855 unique solo piano\nworks composed by 2,786 composers. We extract the names of music works and the\nnames of composers from the International Music Score Library Project (IMSLP).\nWe search and download their corresponding audio recordings from the internet.\nWe further create a curated subset containing 7,236 works composed by 1,787\ncomposers by constraining the titles of downloaded audio recordings containing\nthe surnames of composers. We apply a convolutional neural network to detect\nsolo piano works. Then, we transcribe those solo piano recordings into Musical\nInstrument Digital Interface (MIDI) files using a high-resolution piano\ntranscription system. Each transcribed MIDI file contains the onset, offset,\npitch, and velocity attributes of piano notes and pedals. GiantMIDI-Piano\nincludes 90% live performance MIDI files and 10\\% sequence input MIDI files. We\nanalyse the statistics of GiantMIDI-Piano and show pitch class, interval,\ntrichord, and tetrachord frequencies of six composers from different eras to\nshow that GiantMIDI-Piano can be used for musical analysis. We evaluate the\nquality of GiantMIDI-Piano in terms of solo piano detection F1 scores, metadata\naccuracy, and transcription error rates. We release the source code for\nacquiring the GiantMIDI-Piano dataset at\nhttps://github.com/bytedance/GiantMIDI-Piano", "published": "2020-10-11 01:23:43", "link": "http://arxiv.org/abs/2010.07061v3", "categories": ["cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
