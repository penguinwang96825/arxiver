{"title": "Rethinking ChatGPT's Success: Usability and Cognitive Behaviors Enabled\n  by Auto-regressive LLMs' Prompting", "abstract": "Over the last decade, a wide range of training and deployment strategies for\nLarge Language Models (LLMs) have emerged. Among these, the prompting paradigms\nof Auto-regressive LLMs (AR-LLMs) have catalyzed a significant surge in\nArtificial Intelligence (AI). This paper aims to emphasize the significance of\nutilizing free-form modalities (forms of input and output) and verbal free-form\ncontexts as user-directed channels (methods for transforming modalities) for\ndownstream deployment. Specifically, we analyze the structure of modalities\nwithin both two types of LLMs and six task-specific channels during deployment.\nFrom the perspective of users, our analysis introduces and applies the\nanalytical metrics of task customizability, transparency, and complexity to\ngauge their usability, highlighting the superior nature of AR-LLMs' prompting\nparadigms. Moreover, we examine the stimulation of diverse cognitive behaviors\nin LLMs through the adoption of free-form text and verbal contexts, mirroring\nhuman linguistic expressions of such behaviors. We then detail four common\ncognitive behaviors to underscore how AR-LLMs' prompting successfully imitate\nhuman-like behaviors using this free-form modality and channel. Lastly, the\npotential for improving LLM deployment, both as autonomous agents and within\nmulti-agent systems, is identified via cognitive behavior concepts and\nprinciples.", "published": "2024-05-17 00:19:41", "link": "http://arxiv.org/abs/2405.10474v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Better Question Generation in QA-based Event Extraction", "abstract": "Event Extraction (EE) is an essential information extraction task that aims\nto extract event-related information from unstructured texts. The paradigm of\nthis task has shifted from conventional classification-based methods to more\ncontemporary question-answering-based (QA-based) approaches. However, in\nQA-based EE, the quality of the questions dramatically affects the extraction\naccuracy, and how to generate high-quality questions for QA-based EE remains a\nchallenge. In this work, to tackle this challenge, we suggest four criteria to\nevaluate the quality of a question and propose a reinforcement learning method,\nRLQG, for QA-based EE that can generate generalizable, high-quality, and\ncontext-dependent questions and provides clear guidance to QA models. The\nextensive experiments conducted on ACE and RAMS datasets have strongly\nvalidated our approach's effectiveness, which also demonstrates its robustness\nin scenarios with limited training data. The corresponding code of RLQG is\nreleased for further research.", "published": "2024-05-17 03:52:01", "link": "http://arxiv.org/abs/2405.10517v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptable and Reliable Text Classification using Large Language Models", "abstract": "Text classification is fundamental in Natural Language Processing (NLP), and\nthe advent of Large Language Models (LLMs) has revolutionized the field. This\npaper introduces an adaptable and reliable text classification paradigm, which\nleverages LLMs as the core component to address text classification tasks. Our\nsystem simplifies the traditional text classification workflows, reducing the\nneed for extensive preprocessing and domain-specific expertise to deliver\nadaptable and reliable text classification results. We evaluated the\nperformance of several LLMs, machine learning algorithms, and neural\nnetwork-based architectures on four diverse datasets. Results demonstrate that\ncertain LLMs surpass traditional methods in sentiment analysis, spam SMS\ndetection, and multi-label classification. Furthermore, it is shown that the\nsystem's performance can be further enhanced through few-shot or fine-tuning\nstrategies, making the fine-tuned model the top performer across all datasets.\nSource code and datasets are available in this GitHub repository:\nhttps://github.com/yeyimilk/llm-zero-shot-classifiers.", "published": "2024-05-17 04:05:05", "link": "http://arxiv.org/abs/2405.10523v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models can Exploit Cross-Task In-context Learning for\n  Data-Scarce Novel Tasks", "abstract": "Large Language Models (LLMs) have transformed NLP with their remarkable\nIn-context Learning (ICL) capabilities. Automated assistants based on LLMs are\ngaining popularity; however, adapting them to novel tasks is still challenging.\nWhile colossal models excel in zero-shot performance, their computational\ndemands limit widespread use, and smaller language models struggle without\ncontext. This paper investigates whether LLMs can generalize from labeled\nexamples of predefined tasks to novel tasks. Drawing inspiration from\nbiological neurons and the mechanistic interpretation of the Transformer\narchitecture, we explore the potential for information sharing across tasks. We\ndesign a cross-task prompting setup with three LLMs and show that LLMs achieve\nsignificant performance improvements despite no examples from the target task\nin the context. Cross-task prompting leads to a remarkable performance boost of\n107% for LLaMA-2 7B, 18.6% for LLaMA-2 13B, and 3.2% for GPT 3.5 on average\nover zero-shot prompting, and performs comparable to standard in-context\nlearning. The effectiveness of generating pseudo-labels for in-task examples is\ndemonstrated, and our analyses reveal a strong correlation between the effect\nof cross-task examples and model activation similarities in source and target\ninput tokens. This paper offers a first-of-its-kind exploration of LLMs'\nability to solve novel tasks based on contextual signals from different task\nexamples.", "published": "2024-05-17 05:20:49", "link": "http://arxiv.org/abs/2405.10548v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Hard Nut to Crack: Idiom Detection with Conversational Large Language\n  Models", "abstract": "In this work, we explore idiomatic language processing with Large Language\nModels (LLMs). We introduce the Idiomatic language Test Suite IdioTS, a new\ndataset of difficult examples specifically designed by language experts to\nassess the capabilities of LLMs to process figurative language at sentence\nlevel. We propose a comprehensive evaluation methodology based on an idiom\ndetection task, where LLMs are prompted with detecting an idiomatic expression\nin a given English sentence. We present a thorough automatic and manual\nevaluation of the results and an extensive error analysis.", "published": "2024-05-17 07:08:13", "link": "http://arxiv.org/abs/2405.10579v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic data sampler for cross-language transfer learning in large\n  language models", "abstract": "Large Language Models (LLMs) have gained significant attention in the field\nof natural language processing (NLP) due to their wide range of applications.\nHowever, training LLMs for languages other than English poses significant\nchallenges, due to the difficulty in acquiring large-scale corpus and the\nrequisite computing resources. In this paper, we propose ChatFlow, a\ncross-language transfer-based LLM, to address these challenges and train large\nChinese language models in a cost-effective manner. We employ a mix of Chinese,\nEnglish, and parallel corpus to continuously train the LLaMA2 model, aiming to\nalign cross-language representations and facilitate the knowledge transfer\nspecifically to the Chinese language model. In addition, we use a dynamic data\nsampler to progressively transition the model from unsupervised pre-training to\nsupervised fine-tuning. Experimental results demonstrate that our approach\naccelerates model convergence and achieves superior performance. We evaluate\nChatFlow on popular Chinese and English benchmarks, the results indicate that\nit outperforms other Chinese models post-trained on LLaMA-2-7B.", "published": "2024-05-17 08:40:51", "link": "http://arxiv.org/abs/2405.10626v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Layer-Condensed KV Cache for Efficient Inference of Large Language\n  Models", "abstract": "Huge memory consumption has been a major bottleneck for deploying\nhigh-throughput large language models in real-world applications. In addition\nto the large number of parameters, the key-value (KV) cache for the attention\nmechanism in the transformer architecture consumes a significant amount of\nmemory, especially when the number of layers is large for deep language models.\nIn this paper, we propose a novel method that only computes and caches the KVs\nof a small number of layers, thus significantly saving memory consumption and\nimproving inference throughput. Our experiments on large language models show\nthat our method achieves up to 26$\\times$ higher throughput than standard\ntransformers and competitive performance in language modeling and downstream\ntasks. In addition, our method is orthogonal to existing transformer\nmemory-saving techniques, so it is straightforward to integrate them with our\nmodel, achieving further improvement in inference efficiency. Our code is\navailable at https://github.com/whyNLP/LCKV.", "published": "2024-05-17 08:59:46", "link": "http://arxiv.org/abs/2405.10637v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SPOR: A Comprehensive and Practical Evaluation Method for Compositional\n  Generalization in Data-to-Text Generation", "abstract": "Compositional generalization is an important ability of language models and\nhas many different manifestations. For data-to-text generation, previous\nresearch on this ability is limited to a single manifestation called\nSystematicity and lacks consideration of large language models (LLMs), which\ncannot fully cover practical application scenarios. In this work, we propose\nSPOR, a comprehensive and practical evaluation method for compositional\ngeneralization in data-to-text generation. SPOR includes four aspects of\nmanifestations (Systematicity, Productivity, Order invariance, and Rule\nlearnability) and allows high-quality evaluation without additional manual\nannotations based on existing datasets. We demonstrate SPOR on two different\ndatasets and evaluate some existing language models including LLMs. We find\nthat the models are deficient in various aspects of the evaluation and need\nfurther improvement. Our work shows the necessity for comprehensive research on\ndifferent manifestations of compositional generalization in data-to-text\ngeneration and provides a framework for evaluation.", "published": "2024-05-17 09:25:30", "link": "http://arxiv.org/abs/2405.10650v8", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revolutionizing Process Mining: A Novel Architecture for ChatGPT\n  Integration and Enhanced User Experience through Optimized Prompt Engineering", "abstract": "In the rapidly evolving field of business process management, there is a\ngrowing need for analytical tools that can transform complex data into\nactionable insights. This research introduces a novel approach by integrating\nLarge Language Models (LLMs), such as ChatGPT, into process mining tools,\nmaking process analytics more accessible to a wider audience. The study aims to\ninvestigate how ChatGPT enhances analytical capabilities, improves user\nexperience, increases accessibility, and optimizes the architectural frameworks\nof process mining tools. The key innovation of this research lies in developing\na tailored prompt engineering strategy for each process mining submodule,\nensuring that the AI-generated outputs are accurate and relevant to the\ncontext. The integration architecture follows an Extract, Transform, Load (ETL)\nprocess, which includes various process mining engine modules and utilizes\nzero-shot and optimized prompt engineering techniques. ChatGPT is connected via\nAPIs and receives structured outputs from the process mining modules, enabling\nconversational interactions. To validate the effectiveness of this approach,\nthe researchers used data from 17 companies that employ BehfaLab's Process\nMining Tool. The results showed significant improvements in user experience,\nwith an expert panel rating 72% of the results as \"Good\". This research\ncontributes to the advancement of business process analysis methodologies by\ncombining process mining with artificial intelligence. Future research\ndirections include further optimization of prompt engineering, exploration of\nintegration with other AI technologies, and assessment of scalability across\nvarious business environments. This study paves the way for continuous\ninnovation at the intersection of process mining and artificial intelligence,\npromising to revolutionize the way businesses analyze and optimize their\nprocesses.", "published": "2024-05-17 10:48:14", "link": "http://arxiv.org/abs/2405.10689v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Feature-Adaptive and Data-Scalable In-Context Learning", "abstract": "In-context learning (ICL), which promotes inference with several\ndemonstrations, has become a widespread paradigm to stimulate LLM capabilities\nfor downstream tasks. Due to context length constraints, it cannot be further\nimproved in spite of more training data, and general features directly from\nLLMs in ICL are not adaptive to the specific downstream task. In this paper, we\npropose a feature-adaptive and data-scalable in-context learning framework\n(FADS-ICL), which can leverage task-adaptive features to promote inference on\nthe downstream task, with the supervision of beyond-context samples.\nSpecifically, it first extracts general features of beyond-context samples via\nthe LLM with ICL input form one by one, and introduces a task-specific\nmodulator to perform feature refinement and prediction after fitting a specific\ndownstream task. We conduct extensive experiments on FADS-ICL under varying\ndata settings (4$\\sim$128 shots) and LLM scale (0.8$\\sim$70B) settings.\nExperimental results show that FADS-ICL consistently outperforms previous\nstate-of-the-art methods by a significant margin under all settings, verifying\nthe effectiveness and superiority of FADS-ICL. For example, under the 1.5B and\n32 shots setting, FADS-ICL can achieve \\textbf{+14.3} average accuracy from\nfeature adaptation over vanilla ICL on 10 datasets, with \\textbf{+6.2} average\naccuracy over the previous state-of-the-art method, and the performance can\nfurther improve with increasing training data. Code and data are publicly\navailable at \\url{https://github.com/jiahaozhenbang/FADS-ICL}.", "published": "2024-05-17 12:32:53", "link": "http://arxiv.org/abs/2405.10738v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SBAAM! Eliminating Transcript Dependency in Automatic Subtitling", "abstract": "Subtitling plays a crucial role in enhancing the accessibility of audiovisual\ncontent and encompasses three primary subtasks: translating spoken dialogue,\nsegmenting translations into concise textual units, and estimating timestamps\nthat govern their on-screen duration. Past attempts to automate this process\nrely, to varying degrees, on automatic transcripts, employed diversely for the\nthree subtasks. In response to the acknowledged limitations associated with\nthis reliance on transcripts, recent research has shifted towards\ntranscription-free solutions for translation and segmentation, leaving the\ndirect generation of timestamps as uncharted territory. To fill this gap, we\nintroduce the first direct model capable of producing automatic subtitles,\nentirely eliminating any dependence on intermediate transcripts also for\ntimestamp prediction. Experimental results, backed by manual evaluation,\nshowcase our solution's new state-of-the-art performance across multiple\nlanguage pairs and diverse conditions.", "published": "2024-05-17 12:42:56", "link": "http://arxiv.org/abs/2405.10741v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ActiveLLM: Large Language Model-based Active Learning for Textual\n  Few-Shot Scenarios", "abstract": "Active learning is designed to minimize annotation efforts by prioritizing\ninstances that most enhance learning. However, many active learning strategies\nstruggle with a 'cold start' problem, needing substantial initial data to be\neffective. This limitation often reduces their utility for pre-trained models,\nwhich already perform well in few-shot scenarios. To address this, we introduce\nActiveLLM, a novel active learning approach that leverages large language\nmodels such as GPT-4, Llama 3, and Mistral Large for selecting instances. We\ndemonstrate that ActiveLLM significantly enhances the classification\nperformance of BERT classifiers in few-shot scenarios, outperforming both\ntraditional active learning methods and the few-shot learning method SetFit.\nAdditionally, ActiveLLM can be extended to non-few-shot scenarios, allowing for\niterative selections. In this way, ActiveLLM can even help other active\nlearning strategies to overcome their cold start problem. Our results suggest\nthat ActiveLLM offers a promising solution for improving model performance\nacross various learning setups.", "published": "2024-05-17 14:23:54", "link": "http://arxiv.org/abs/2405.10808v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ECR-Chain: Advancing Generative Language Models to Better Emotion-Cause\n  Reasoners through Reasoning Chains", "abstract": "Understanding the process of emotion generation is crucial for analyzing the\ncauses behind emotions. Causal Emotion Entailment (CEE), an\nemotion-understanding task, aims to identify the causal utterances in a\nconversation that stimulate the emotions expressed in a target utterance.\nHowever, current works in CEE mainly focus on modeling semantic and emotional\ninteractions in conversations, neglecting the exploration of the\nemotion-generation process. This hinders the models from deeply understanding\nemotions, restricting their ability to produce explainable predictions. In this\nwork, inspired by the emotion generation process of\n\"stimulus-appraisal-emotion\" in the cognitive appraisal theory, we introduce a\nstep-by-step reasoning method, Emotion-Cause Reasoning Chain (ECR-Chain), to\ninfer the stimulus from the target emotional expressions in conversations.\nSpecifically, we first introduce the ECR-Chain to ChatGPT via few-shot\nprompting, which significantly improves its performance on the CEE task. We\nfurther propose an automated construction process to utilize ChatGPT in\nbuilding an ECR-Chain set, which can enhance the reasoning abilities of smaller\nmodels through supervised training and assist the Vicuna-7B model in achieving\nstate-of-the-art CEE performance. Moreover, our methods can enable these\ngenerative language models to effectively perform emotion-cause reasoning in an\nexplainable manner. Our code, data and more details are at\nhttps://github.com/hzp3517/ECR-Chain.", "published": "2024-05-17 15:45:08", "link": "http://arxiv.org/abs/2405.10860v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Arabic Noun System Generation", "abstract": "In this paper, we show that the multiple-stem approach to nouns with a broken\nplural pattern allows for greater generalizations to be stated in the\nmorphological system. Such an approach dispenses with truncating/deleting rules\nand other complex rules that are required to account for the highly allomorphic\nbroken plural system. The generation of inflected sound nouns necessitates a\npre-specification of the affixes denoting the sound plural masculine and the\nsound plural feminine, namely uwna and aAt, in the lexicon. The first\nsubsection of section one provides an evaluation of some of the previous\nanalyses of the Arabic broken plural. We provide both linguistic and\nstatistical evidence against deriving broken plurals from the singular or the\nroot. In subsection two, we propose a multiple stem approach to the Arabic Noun\nPlural System within the Lexeme-based Morphology framework. In section two, we\nlook at the noun inflection of Arabic. Section three provides an implementation\nof the Arabic Noun system in MORPHE. In this context, we show how the\ngeneralizations discussed in the linguistic analysis section are captured in\nMorphe using the equivalencing nodes.", "published": "2024-05-17 17:33:10", "link": "http://arxiv.org/abs/2405.11014v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Unappreciated Role of Intent in Algorithmic Moderation of Social\n  Media Content", "abstract": "As social media has become a predominant mode of communication globally, the\nrise of abusive content threatens to undermine civil discourse. Recognizing the\ncritical nature of this issue, a significant body of research has been\ndedicated to developing language models that can detect various types of online\nabuse, e.g., hate speech, cyberbullying. However, there exists a notable\ndisconnect between platform policies, which often consider the author's\nintention as a criterion for content moderation, and the current capabilities\nof detection models, which typically lack efforts to capture intent. This paper\nexamines the role of intent in content moderation systems. We review state of\nthe art detection models and benchmark training datasets for online abuse to\nassess their awareness and ability to capture intent. We propose strategic\nchanges to the design and development of automated detection and moderation\nsystems to improve alignment with ethical and policy conceptualizations of\nabuse.", "published": "2024-05-17 18:05:13", "link": "http://arxiv.org/abs/2405.11030v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CC-GPX: Extracting High-Quality Annotated Geospatial Data from Common\n  Crawl", "abstract": "The Common Crawl (CC) corpus is the largest open web crawl dataset containing\n9.5+ petabytes of data captured since 2008. The dataset is instrumental in\ntraining large language models, and as such it has been studied for\n(un)desirable content, and distilled for smaller, domain-specific datasets.\nHowever, to our knowledge, no research has been dedicated to using CC as a\nsource of annotated geospatial data. In this paper, we introduce an efficient\npipeline to extract annotated user-generated tracks from GPX files found in CC,\nand the resulting multimodal dataset with 1,416 pairings of human-written\ndescriptions and MultiLineString vector data from the 6 most recent CC\nreleases. The dataset can be used to study people's outdoor activity patterns,\nthe way people talk about their outdoor experiences, as well as for developing\ntrajectory generation or track annotation models, or for various other problems\nin place of synthetically generated routes. Our reproducible code is available\non GitHub: https://github.com/ilyankou/cc-gpx", "published": "2024-05-17 18:31:26", "link": "http://arxiv.org/abs/2405.11039v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Substitution-based Word Sense Induction", "abstract": "Word Sense Induction (WSI) is the task of discovering senses of an ambiguous\nword by grouping usages of this word into clusters corresponding to these\nsenses. Many approaches were proposed to solve WSI in English and a few other\nlanguages, but these approaches are not easily adaptable to new languages. We\npresent multilingual substitution-based WSI methods that support any of 100\nlanguages covered by the underlying multilingual language model with minimal to\nno adaptation required. Despite the multilingual capabilities, our methods\nperform on par with the existing monolingual approaches on popular English WSI\ndatasets. At the same time, they will be most useful for lower-resourced\nlanguages which miss lexical resources available for English, thus, have higher\ndemand for unsupervised methods like WSI.", "published": "2024-05-17 20:38:56", "link": "http://arxiv.org/abs/2405.11086v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Reproducibility Study on Quantifying Language Similarity: The Impact\n  of Missing Values in the URIEL Knowledge Base", "abstract": "In the pursuit of supporting more languages around the world, tools that\ncharacterize properties of languages play a key role in expanding the existing\nmultilingual NLP research. In this study, we focus on a widely used typological\nknowledge base, URIEL, which aggregates linguistic information into numeric\nvectors. Specifically, we delve into the soundness and reproducibility of the\napproach taken by URIEL in quantifying language similarity. Our analysis\nreveals URIEL's ambiguity in calculating language distances and in handling\nmissing values. Moreover, we find that URIEL does not provide any information\nabout typological features for 31\\% of the languages it represents, undermining\nthe reliabilility of the database, particularly on low-resource languages. Our\nliterature review suggests URIEL and lang2vec are used in papers on diverse NLP\ntasks, which motivates us to rigorously verify the database as the\neffectiveness of these works depends on the reliability of the information the\ntool provides.", "published": "2024-05-17 23:53:48", "link": "http://arxiv.org/abs/2405.11125v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CNER: A tool Classifier of Named-Entity Relationships", "abstract": "We introduce CNER, an ensemble of capable tools for extraction of semantic\nrelationships between named entities in Spanish language. Built upon a\ncontainer-based architecture, CNER integrates different Named entity\nrecognition and relation extraction tools with a user-friendly interface that\nallows users to input free text or files effortlessly, facilitating streamlined\nanalysis. Developed as a prototype version for the Natural Language Processing\n(NLP) Group at Universidad del Valle, CNER serves as a practical educational\nresource, illustrating how machine learning techniques can effectively tackle\ndiverse NLP tasks in Spanish. Our preliminary results reveal the promising\npotential of CNER in advancing the understanding and development of NLP tools,\nparticularly within Spanish-language contexts.", "published": "2024-05-17 01:16:58", "link": "http://arxiv.org/abs/2405.10485v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Automatic News Generation and Fact-Checking System Based on Language\n  Processing", "abstract": "This paper explores an automatic news generation and fact-checking system\nbased on language processing, aimed at enhancing the efficiency and quality of\nnews production while ensuring the authenticity and reliability of the news\ncontent. With the rapid development of Natural Language Processing (NLP) and\ndeep learning technologies, automatic news generation systems are capable of\nextracting key information from massive data and generating well-structured,\nfluent news articles. Meanwhile, by integrating fact-checking technology, the\nsystem can effectively prevent the spread of false news and improve the\naccuracy and credibility of news. This study details the key technologies\ninvolved in automatic news generation and factchecking, including text\ngeneration, information extraction, and the application of knowledge graphs,\nand validates the effectiveness of these technologies through experiments.\nAdditionally, the paper discusses the future development directions of\nautomatic news generation and fact-checking systems, emphasizing the importance\nof further integration and innovation of technologies. The results show that\nwith continuous technological optimization and practical application, these\nsystems will play an increasingly important role in the future news industry,\nproviding more efficient and reliable news services.", "published": "2024-05-17 01:58:23", "link": "http://arxiv.org/abs/2405.10492v2", "categories": ["cs.CL", "cs.LG", "I.5; H.4"], "primary_category": "cs.CL"}
{"title": "Language Models can Evaluate Themselves via Probability Discrepancy", "abstract": "In this paper, we initiate our discussion by demonstrating how Large Language\nModels (LLMs), when tasked with responding to queries, display a more even\nprobability distribution in their answers if they are more adept, as opposed to\ntheir less skilled counterparts. Expanding on this foundational insight, we\npropose a new self-evaluation method ProbDiff for assessing the efficacy of\nvarious LLMs. This approach obviates the necessity for an additional evaluation\nmodel or the dependence on external, proprietary models like GPT-4 for\njudgment. It uniquely utilizes the LLMs being tested to compute the probability\ndiscrepancy between the initial response and its revised versions. A higher\ndiscrepancy for a given query between two LLMs indicates a relatively weaker\ncapability. Our findings reveal that ProbDiff achieves results on par with\nthose obtained from evaluations based on GPT-4, spanning a range of scenarios\nthat include natural language generation (NLG) tasks such as translation,\nsummarization, and our proposed Xiaohongshu blog writing task, and benchmarks\nfor LLM evaluation like AlignBench, MT-Bench, and AlpacaEval, across LLMs of\nvarying magnitudes.", "published": "2024-05-17 03:50:28", "link": "http://arxiv.org/abs/2405.10516v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Benchmarking Large Language Models on CFLUE -- A Chinese Financial\n  Language Understanding Evaluation Dataset", "abstract": "In light of recent breakthroughs in large language models (LLMs) that have\nrevolutionized natural language processing (NLP), there is an urgent need for\nnew benchmarks to keep pace with the fast development of LLMs. In this paper,\nwe propose CFLUE, the Chinese Financial Language Understanding Evaluation\nbenchmark, designed to assess the capability of LLMs across various dimensions.\nSpecifically, CFLUE provides datasets tailored for both knowledge assessment\nand application assessment. In knowledge assessment, it consists of 38K+\nmultiple-choice questions with associated solution explanations. These\nquestions serve dual purposes: answer prediction and question reasoning. In\napplication assessment, CFLUE features 16K+ test instances across distinct\ngroups of NLP tasks such as text classification, machine translation, relation\nextraction, reading comprehension, and text generation. Upon CFLUE, we conduct\na thorough evaluation of representative LLMs. The results reveal that only\nGPT-4 and GPT-4-turbo achieve an accuracy exceeding 60\\% in answer prediction\nfor knowledge assessment, suggesting that there is still substantial room for\nimprovement in current LLMs. In application assessment, although GPT-4 and\nGPT-4-turbo are the top two performers, their considerable advantage over\nlightweight LLMs is noticeably diminished. The datasets and scripts associated\nwith CFLUE are openly accessible at https://github.com/aliyun/cflue.", "published": "2024-05-17 05:03:40", "link": "http://arxiv.org/abs/2405.10542v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RDRec: Rationale Distillation for LLM-based Recommendation", "abstract": "Large language model (LLM)-based recommender models that bridge users and\nitems through textual prompts for effective semantic reasoning have gained\nconsiderable attention. However, few methods consider the underlying rationales\nbehind interactions, such as user preferences and item attributes, limiting the\nreasoning capability of LLMs for recommendations. This paper proposes a\nrationale distillation recommender (RDRec), a compact model designed to learn\nrationales generated by a larger language model (LM). By leveraging rationales\nfrom reviews related to users and items, RDRec remarkably specifies their\nprofiles for recommendations. Experiments show that RDRec achieves\nstate-of-the-art (SOTA) performance in both top-N and sequential\nrecommendations. Our source code is released at\nhttps://github.com/WangXFng/RDRec.", "published": "2024-05-17 07:22:02", "link": "http://arxiv.org/abs/2405.10587v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Adaptive Feature-based Low-Rank Compression of Large Language Models via\n  Bayesian Optimization", "abstract": "In recent years, large language models (LLMs) have driven advances in natural\nlanguage processing. Still, their growing scale has increased the computational\nburden, necessitating a balance between efficiency and performance. Low-rank\ncompression, a promising technique, reduces non-essential parameters by\ndecomposing weight matrices into products of two low-rank matrices. Yet, its\napplication in LLMs has not been extensively studied. The key to low-rank\ncompression lies in low-rank factorization and low-rank dimensions allocation.\nTo address the challenges of low-rank compression in LLMs, we conduct empirical\nresearch on the low-rank characteristics of large models. We propose a low-rank\ncompression method suitable for LLMs. This approach involves precise estimation\nof feature distributions through pooled covariance matrices and a Bayesian\noptimization strategy for allocating low-rank dimensions. Experiments on the\nLLaMA-2 models demonstrate that our method outperforms existing strong\nstructured pruning and low-rank compression techniques in maintaining model\nperformance at the same compression ratio.", "published": "2024-05-17 08:27:12", "link": "http://arxiv.org/abs/2405.10616v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DeepPavlov at SemEval-2024 Task 8: Leveraging Transfer Learning for\n  Detecting Boundaries of Machine-Generated Texts", "abstract": "The Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated\nText Detection shared task in the SemEval-2024 competition aims to tackle the\nproblem of misusing collaborative human-AI writing. Although there are a lot of\nexisting detectors of AI content, they are often designed to give a binary\nanswer and thus may not be suitable for more nuanced problem of finding the\nboundaries between human-written and machine-generated texts, while hybrid\nhuman-AI writing becomes more and more popular. In this paper, we address the\nboundary detection problem. Particularly, we present a pipeline for augmenting\ndata for supervised fine-tuning of DeBERTaV3. We receive new best MAE score,\naccording to the leaderboard of the competition, with this pipeline.", "published": "2024-05-17 08:44:48", "link": "http://arxiv.org/abs/2405.10629v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Medical Dialogue: A Survey of Categories, Methods, Evaluation and\n  Challenges", "abstract": "This paper surveys and organizes research works on medical dialog systems,\nwhich is an important yet challenging task. Although these systems have been\nsurveyed in the medical community from an application perspective, a systematic\nreview from a rigorous technical perspective has to date remained noticeably\nabsent. As a result, an overview of the categories, methods, and evaluation of\nmedical dialogue systems remain limited and underspecified, hindering the\nfurther improvement of this area. To fill this gap, we investigate an initial\npool of 325 papers from well-known computer science, and natural language\nprocessing conferences and journals, and make an overview. Recently, large\nlanguage models have shown strong model capacity on downstream tasks, which\nalso reshaped medical dialog systems' foundation. Despite the alluring\npractical application value, current medical dialogue systems still suffer from\nproblems. To this end, this paper lists the grand challenges of medical dialog\nsystems, especially of large language models.", "published": "2024-05-17 08:46:15", "link": "http://arxiv.org/abs/2405.10630v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Realistic Evaluation of Toxicity in Large Language Models", "abstract": "Large language models (LLMs) have become integral to our professional\nworkflows and daily lives. Nevertheless, these machine companions of ours have\na critical flaw: the huge amount of data which endows them with vast and\ndiverse knowledge, also exposes them to the inevitable toxicity and bias. While\nmost LLMs incorporate defense mechanisms to prevent the generation of harmful\ncontent, these safeguards can be easily bypassed with minimal prompt\nengineering. In this paper, we introduce the new Thoroughly Engineered Toxicity\n(TET) dataset, comprising manually crafted prompts designed to nullify the\nprotective layers of such models. Through extensive evaluations, we demonstrate\nthe pivotal role of TET in providing a rigorous benchmark for evaluation of\ntoxicity awareness in several popular LLMs: it highlights the toxicity in the\nLLMs that might remain hidden when using normal prompts, thus revealing subtler\nissues in their behavior.", "published": "2024-05-17 09:42:59", "link": "http://arxiv.org/abs/2405.10659v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Empowering Prior to Court Legal Analysis: A Transparent and Accessible\n  Dataset for Defensive Statement Classification and Interpretation", "abstract": "The classification of statements provided by individuals during police\ninterviews is a complex and significant task within the domain of natural\nlanguage processing (NLP) and legal informatics. The lack of extensive\ndomain-specific datasets raises challenges to the advancement of NLP methods in\nthe field. This paper aims to address some of the present challenges by\nintroducing a novel dataset tailored for classification of statements made\nduring police interviews, prior to court proceedings. Utilising the curated\ndataset for training and evaluation, we introduce a fine-tuned DistilBERT model\nthat achieves state-of-the-art performance in distinguishing truthful from\ndeceptive statements. To enhance interpretability, we employ explainable\nartificial intelligence (XAI) methods to offer explainability through saliency\nmaps, that interpret the model's decision-making process. Lastly, we present an\nXAI interface that empowers both legal professionals and non-specialists to\ninteract with and benefit from our system. Our model achieves an accuracy of\n86%, and is shown to outperform a custom transformer architecture in a\ncomparative study. This holistic approach advances the accessibility,\ntransparency, and effectiveness of statement analysis, with promising\nimplications for both legal practice and research.", "published": "2024-05-17 11:22:27", "link": "http://arxiv.org/abs/2405.10702v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Persian Pronoun Resolution: Leveraging Neural Networks and Language\n  Models", "abstract": "Coreference resolution, critical for identifying textual entities referencing\nthe same entity, faces challenges in pronoun resolution, particularly\nidentifying pronoun antecedents. Existing methods often treat pronoun\nresolution as a separate task from mention detection, potentially missing\nvaluable information. This study proposes the first end-to-end neural network\nsystem for Persian pronoun resolution, leveraging pre-trained Transformer\nmodels like ParsBERT. Our system jointly optimizes both mention detection and\nantecedent linking, achieving a 3.37 F1 score improvement over the previous\nstate-of-the-art system (which relied on rule-based and statistical methods) on\nthe Mehr corpus. This significant improvement demonstrates the effectiveness of\ncombining neural networks with linguistic models, potentially marking a\nsignificant advancement in Persian pronoun resolution and paving the way for\nfurther research in this under-explored area.", "published": "2024-05-17 11:56:00", "link": "http://arxiv.org/abs/2405.10714v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SignLLM: Sign Language Production Large Language Models", "abstract": "In this paper, we propose SignLLM, a multilingual Sign Language Production\n(SLP) large language model, which includes two novel multilingual SLP modes\nMLSF and Prompt2LangGloss that allow sign language gestures generation from\nquery texts input and question-style prompts input respectively. Both modes can\nuse a new RL loss based on reinforcement learning and a new RL module named\nPriority Learning Channel. These RL components can accelerate the training by\nenhancing the model's capability to sample high-quality data. For SignLLM's\ntraining, we introduce Prompt2Sign, a comprehensive multilingual sign language\ndataset, which builds from public data, including American Sign Language (ASL)\nand seven others. This dataset standardizes information by extracting pose\ninformation from sign language videos into a unified compressed format. We\nextensively evaluate SignLLM, demonstrating that our model achieves\nstate-of-the-art performance on SLP tasks across eight sign languages.", "published": "2024-05-17 12:01:43", "link": "http://arxiv.org/abs/2405.10718v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "INDUS: Effective and Efficient Language Models for Scientific\n  Applications", "abstract": "Large language models (LLMs) trained on general domain corpora showed\nremarkable results on natural language processing (NLP) tasks. However,\nprevious research demonstrated LLMs trained using domain-focused corpora\nperform better on specialized tasks. Inspired by this insight, we developed\nINDUS, a comprehensive suite of LLMs tailored for the closely-related domains\nof Earth science, biology, physics, heliophysics, planetary sciences and\nastrophysics, and trained using curated scientific corpora drawn from diverse\ndata sources. The suite of models include: (1) an encoder model trained using\ndomain-specific vocabulary and corpora to address NLP tasks, (2) a\ncontrastive-learning based text embedding model trained using a diverse set of\ndatasets to address information retrieval tasks and (3) smaller versions of\nthese models created using knowledge distillation for applications which have\nlatency or resource constraints. We also created three new scientific benchmark\ndatasets, CLIMATE-CHANGE NER (entity-recognition), NASA-QA (extractive QA) and\nNASA-IR (IR) to accelerate research in these multi-disciplinary fields. We show\nthat our models outperform both general-purpose (RoBERTa) and domain-specific\n(SCIBERT) encoders on these new tasks as well as existing tasks in the domains\nof interest. Furthermore, we demonstrate the use of these models in two\nindustrial settings -- as a retrieval model for large-scale vector search\napplications and in automatic content tagging systems.", "published": "2024-05-17 12:15:07", "link": "http://arxiv.org/abs/2405.10725v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "COGNET-MD, an evaluation framework and dataset for Large Language Model\n  benchmarks in the medical domain", "abstract": "Large Language Models (LLMs) constitute a breakthrough state-of-the-art\nArtificial Intelligence (AI) technology which is rapidly evolving and promises\nto aid in medical diagnosis either by assisting doctors or by simulating a\ndoctor's workflow in more advanced and complex implementations. In this\ntechnical paper, we outline Cognitive Network Evaluation Toolkit for Medical\nDomains (COGNET-MD), which constitutes a novel benchmark for LLM evaluation in\nthe medical domain. Specifically, we propose a scoring-framework with increased\ndifficulty to assess the ability of LLMs in interpreting medical text. The\nproposed framework is accompanied with a database of Multiple Choice Quizzes\n(MCQs). To ensure alignment with current medical trends and enhance safety,\nusefulness, and applicability, these MCQs have been constructed in\ncollaboration with several associated medical experts in various medical\ndomains and are characterized by varying degrees of difficulty. The current\n(first) version of the database includes the medical domains of Psychiatry,\nDentistry, Pulmonology, Dermatology and Endocrinology, but it will be\ncontinuously extended and expanded to include additional medical domains.", "published": "2024-05-17 16:31:56", "link": "http://arxiv.org/abs/2405.10893v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey on Large Language Models with Multilingualism: Recent Advances\n  and New Frontiers", "abstract": "The rapid development of Large Language Models (LLMs) demonstrates remarkable\nmultilingual capabilities in natural language processing, attracting global\nattention in both academia and industry. To mitigate potential discrimination\nand enhance the overall usability and accessibility for diverse language user\ngroups, it is important for the development of language-fair technology.\nDespite the breakthroughs of LLMs, the investigation into the multilingual\nscenario remains insufficient, where a comprehensive survey to summarize recent\napproaches, developments, limitations, and potential solutions is desirable. To\nthis end, we provide a survey with multiple perspectives on the utilization of\nLLMs in the multilingual scenario. We first rethink the transitions between\nprevious and current research on pre-trained language models. Then we introduce\nseveral perspectives on the multilingualism of LLMs, including training and\ninference methods, information retrieval, model security, multi-domain with\nlanguage culture, and usage of datasets. We also discuss the major challenges\nthat arise in these aspects, along with possible solutions. Besides, we\nhighlight future research directions that aim at further enhancing LLMs with\nmultilingualism. The survey aims to help the research community address\nmultilingual problems and provide a comprehensive understanding of the core\nconcepts, key techniques, and latest developments in multilingual natural\nlanguage processing based on LLMs.", "published": "2024-05-17 17:47:39", "link": "http://arxiv.org/abs/2405.10936v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Petri nets in modelling glucose regulating processes in the liver", "abstract": "Diabetes is a chronic condition, considered one of the civilization diseases,\nthat is characterized by sustained high blood sugar levels. There is no doubt\nthat more and more people is going to suffer from diabetes, hence it is crucial\nto understand better its biological foundations. The essential processes\nrelated to the control of glucose levels in the blood are: glycolysis (process\nof breaking down of glucose) and glucose synthesis, both taking place in the\nliver. The glycolysis occurs during feeding and it is stimulated by insulin. On\nthe other hand, the glucose synthesis arises during fasting and it is\nstimulated by glucagon. In the paper we present a Petri net model of glycolysis\nand glucose synthesis in the liver. The model is created based on medical\nliterature. Standard Petri nets techniques are used to analyse the properties\nof the model: traps, reachability graphs, tokens dynamics, deadlocks analysis.\nThe results are described in the paper. Our analysis shows that the model\ncaptures the interactions between different enzymes and substances, which is\nconsistent with the biological processes occurring during fasting and feeding.\nThe model constitutes the first element of our long-time goal to create the\nwhole body model of the glucose regulation in a healthy human and a person with\ndiabetes.", "published": "2024-05-17 13:15:01", "link": "http://arxiv.org/abs/2405.11009v1", "categories": ["q-bio.OT", "cs.CL", "68", "F.1.1"], "primary_category": "q-bio.OT"}
{"title": "From Generalist to Specialist: Improving Large Language Models for\n  Medical Physics Using ARCoT", "abstract": "Large Language Models (LLMs) have achieved remarkable progress, yet their\napplication in specialized fields, such as medical physics, remains challenging\ndue to the need for domain-specific knowledge. This study introduces ARCoT\n(Adaptable Retrieval-based Chain of Thought), a framework designed to enhance\nthe domain-specific accuracy of LLMs without requiring fine-tuning or extensive\nretraining. ARCoT integrates a retrieval mechanism to access relevant\ndomain-specific information and employs step-back and chain-of-thought\nprompting techniques to guide the LLM's reasoning process, ensuring more\naccurate and context-aware responses. Benchmarking on a medical physics\nmultiple-choice exam, our model outperformed standard LLMs and reported average\nhuman performance, demonstrating improvements of up to 68% and achieving a high\nscore of 90%. This method reduces hallucinations and increases domain-specific\nperformance. The versatility and model-agnostic nature of ARCoT make it easily\nadaptable to various domains, showcasing its significant potential for\nenhancing the accuracy and reliability of LLMs in specialized fields.", "published": "2024-05-17 18:31:38", "link": "http://arxiv.org/abs/2405.11040v1", "categories": ["cs.CL", "physics.med-ph"], "primary_category": "cs.CL"}
{"title": "Leveraging Discourse Structure for Extractive Meeting Summarization", "abstract": "We introduce an extractive summarization system for meetings that leverages\ndiscourse structure to better identify salient information from complex\nmulti-party discussions. Using discourse graphs to represent semantic relations\nbetween the contents of utterances in a meeting, we train a GNN-based node\nclassification model to select the most important utterances, which are then\ncombined to create an extractive summary. Experimental results on AMI and ICSI\ndemonstrate that our approach surpasses existing text-based and graph-based\nextractive summarization systems, as measured by both classification and\nsummarization metrics. Additionally, we conduct ablation studies on discourse\nstructure and relation type to provide insights for future NLP applications\nleveraging discourse analysis theory.", "published": "2024-05-17 19:06:20", "link": "http://arxiv.org/abs/2405.11055v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prompt Exploration with Prompt Regression", "abstract": "In the advent of democratized usage of large language models (LLMs), there is\na growing desire to systematize LLM prompt creation and selection processes\nbeyond iterative trial-and-error. Prior works majorly focus on searching the\nspace of prompts without accounting for relations between prompt variations.\nHere we propose a framework, Prompt Exploration with Prompt Regression (PEPR),\nto predict the effect of prompt combinations given results for individual\nprompt elements as well as a simple method to select an effective prompt for a\ngiven use-case. We evaluate our approach with open-source LLMs of different\nsizes on several different tasks.", "published": "2024-05-17 20:30:49", "link": "http://arxiv.org/abs/2405.11083v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Are Large Language Models Moral Hypocrites? A Study Based on Moral\n  Foundations", "abstract": "Large language models (LLMs) have taken centre stage in debates on Artificial\nIntelligence. Yet there remains a gap in how to assess LLMs' conformity to\nimportant human values. In this paper, we investigate whether state-of-the-art\nLLMs, GPT-4 and Claude 2.1 (Gemini Pro and LLAMA 2 did not generate valid\nresults) are moral hypocrites. We employ two research instruments based on the\nMoral Foundations Theory: (i) the Moral Foundations Questionnaire (MFQ), which\ninvestigates which values are considered morally relevant in abstract moral\njudgements; and (ii) the Moral Foundations Vignettes (MFVs), which evaluate\nmoral cognition in concrete scenarios related to each moral foundation. We\ncharacterise conflicts in values between these different abstractions of moral\nevaluation as hypocrisy. We found that both models displayed reasonable\nconsistency within each instrument compared to humans, but they displayed\ncontradictory and hypocritical behaviour when we compared the abstract values\npresent in the MFQ to the evaluation of concrete moral violations of the MFV.", "published": "2024-05-17 21:27:32", "link": "http://arxiv.org/abs/2405.11100v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Dynamic Embeddings with Task-Oriented prompting", "abstract": "This paper introduces Dynamic Embeddings with Task-Oriented prompting\n(DETOT), a novel approach aimed at improving the adaptability and efficiency of\nmachine learning models by implementing a flexible embedding layer. Unlike\ntraditional static embeddings [14], DETOT dynamically adjusts embeddings based\non task-specific requirements and performance feedback, optimizing input data\nrepresentation for individual tasks [4]. This method enhances both accuracy and\ncomputational performance by tailoring the representation layer to meet the\nunique needs of each task. The structure of DETOT is detailed, highlighting its\ntask-specific adaptation, continuous feedback loop, and mechanisms for\npreventing overfitting. Empirical evaluations demonstrate its superiority over\nexisting methods.", "published": "2024-05-17 23:18:15", "link": "http://arxiv.org/abs/2405.11117v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Dialogue State Tracking Models through LLM-backed User-Agents\n  Simulation", "abstract": "Dialogue State Tracking (DST) is designed to monitor the evolving dialogue\nstate in the conversations and plays a pivotal role in developing task-oriented\ndialogue systems. However, obtaining the annotated data for the DST task is\nusually a costly endeavor. In this paper, we focus on employing LLMs to\ngenerate dialogue data to reduce dialogue collection and annotation costs.\nSpecifically, GPT-4 is used to simulate the user and agent interaction,\ngenerating thousands of dialogues annotated with DST labels. Then a two-stage\nfine-tuning on LLaMA 2 is performed on the generated data and the real data for\nthe DST prediction. Experimental results on two public DST benchmarks show that\nwith the generated dialogue data, our model performs better than the baseline\ntrained solely on real data. In addition, our approach is also capable of\nadapting to the dynamic demands in real-world scenarios, generating dialogues\nin new domains swiftly. After replacing dialogue segments in any domain with\nthe corresponding generated ones, the model achieves comparable performance to\nthe model trained on real data.", "published": "2024-05-17 07:00:05", "link": "http://arxiv.org/abs/2405.13037v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Surgical Feature-Space Decomposition of LLMs: Why, When and How?", "abstract": "Low-rank approximations, of the weight and feature space can enhance the\nperformance of deep learning models, whether in terms of improving\ngeneralization or reducing the latency of inference. However, there is no clear\nconsensus yet on \\emph{how}, \\emph{when} and \\emph{why} these approximations\nare helpful for large language models (LLMs). In this work, we empirically\nstudy the efficacy of weight and feature space decomposition in\ntransformer-based LLMs. We demonstrate that surgical decomposition not only\nprovides critical insights into the trade-off between compression and language\nmodelling performance, but also sometimes enhances commonsense reasoning\nperformance of LLMs. Our empirical analysis identifies specific network\nsegments that intrinsically exhibit a low-rank structure. Furthermore, we\nextend our investigation to the implications of low-rank approximations on\nmodel bias. Overall, our findings offer a novel perspective on optimizing LLMs,\npresenting the low-rank approximation not only as a tool for performance\nenhancements, but also as a means to potentially rectify biases within these\nmodels. Our code is available at\n\\href{https://github.com/nyunAI/SFSD-LLM}{GitHub}.", "published": "2024-05-17 07:34:03", "link": "http://arxiv.org/abs/2405.13039v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Assessing Political Bias in Large Language Models", "abstract": "The assessment of bias within Large Language Models (LLMs) has emerged as a\ncritical concern in the contemporary discourse surrounding Artificial\nIntelligence (AI) in the context of their potential impact on societal\ndynamics. Recognizing and considering political bias within LLM applications is\nespecially important when closing in on the tipping point toward performative\nprediction. Then, being educated about potential effects and the societal\nbehavior LLMs can drive at scale due to their interplay with human operators.\nIn this way, the upcoming elections of the European Parliament will not remain\nunaffected by LLMs. We evaluate the political bias of the currently most\npopular open-source LLMs (instruct or assistant models) concerning political\nissues within the European Union (EU) from a German voter's perspective. To do\nso, we use the \"Wahl-O-Mat,\" a voting advice application used in Germany. From\nthe voting advice of the \"Wahl-O-Mat\" we quantize the degree of alignment of\nLLMs with German political parties. We show that larger models, such as\nLlama3-70B, tend to align more closely with left-leaning political parties,\nwhile smaller models often remain neutral, particularly when prompted in\nEnglish. The central finding is that LLMs are similarly biased, with low\nvariances in the alignment concerning a specific party. Our findings underline\nthe importance of rigorously assessing and making bias transparent in LLMs to\nsafeguard the integrity and trustworthiness of applications that employ the\ncapabilities of performative prediction and the invisible hand of machine\nlearning prediction and language generation.", "published": "2024-05-17 15:30:18", "link": "http://arxiv.org/abs/2405.13041v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Hybrid Deep Learning Framework for Stock Price Prediction Considering\n  the Investor Sentiment of Online Forum Enhanced by Popularity", "abstract": "Stock price prediction has always been a difficult task for forecasters.\nUsing cutting-edge deep learning techniques, stock price prediction based on\ninvestor sentiment extracted from online forums has become feasible. We propose\na novel hybrid deep learning framework for predicting stock prices. The\nframework leverages the XLNET model to analyze the sentiment conveyed in user\nposts on online forums, combines these sentiments with the post popularity\nfactor to compute daily group sentiments, and integrates this information with\nstock technical indicators into an improved BiLSTM-highway model for stock\nprice prediction. Through a series of comparative experiments involving four\nstocks on the Chinese stock market, it is demonstrated that the hybrid\nframework effectively predicts stock prices. This study reveals the necessity\nof analyzing investors' textual views for stock price prediction.", "published": "2024-05-17 07:18:08", "link": "http://arxiv.org/abs/2405.10584v1", "categories": ["cs.LG", "cs.CL", "q-fin.ST"], "primary_category": "cs.LG"}
{"title": "UniCL: A Universal Contrastive Learning Framework for Large Time Series\n  Models", "abstract": "Time-series analysis plays a pivotal role across a range of critical\napplications, from finance to healthcare, which involves various tasks, such as\nforecasting and classification. To handle the inherent complexities of\ntime-series data, such as high dimensionality and noise, traditional supervised\nlearning methods first annotate extensive labels for time-series data in each\ntask, which is very costly and impractical in real-world applications. In\ncontrast, pre-trained foundation models offer a promising alternative by\nleveraging unlabeled data to capture general time series patterns, which can\nthen be fine-tuned for specific tasks. However, existing approaches to\npre-training such models typically suffer from high-bias and low-generality\nissues due to the use of predefined and rigid augmentation operations and\ndomain-specific data training. To overcome these limitations, this paper\nintroduces UniCL, a universal and scalable contrastive learning framework\ndesigned for pretraining time-series foundation models across cross-domain\ndatasets. Specifically, we propose a unified and trainable time-series\naugmentation operation to generate pattern-preserved, diverse, and low-bias\ntime-series data by leveraging spectral information. Besides, we introduce a\nscalable augmentation algorithm capable of handling datasets with varying\nlengths, facilitating cross-domain pretraining. Extensive experiments on two\nbenchmark datasets across eleven domains validate the effectiveness of UniCL,\ndemonstrating its high generalization on time-series analysis across various\nfields.", "published": "2024-05-17 07:47:11", "link": "http://arxiv.org/abs/2405.10597v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MC-GPT: Empowering Vision-and-Language Navigation with Memory Map and\n  Reasoning Chains", "abstract": "In the Vision-and-Language Navigation (VLN) task, the agent is required to\nnavigate to a destination following a natural language instruction. While\nlearning-based approaches have been a major solution to the task, they suffer\nfrom high training costs and lack of interpretability. Recently, Large Language\nModels (LLMs) have emerged as a promising tool for VLN due to their strong\ngeneralization capabilities. However, existing LLM-based methods face\nlimitations in memory construction and diversity of navigation strategies. To\naddress these challenges, we propose a suite of techniques. Firstly, we\nintroduce a method to maintain a topological map that stores navigation\nhistory, retaining information about viewpoints, objects, and their spatial\nrelationships. This map also serves as a global action space. Additionally, we\npresent a Navigation Chain of Thoughts module, leveraging human navigation\nexamples to enrich navigation strategy diversity. Finally, we establish a\npipeline that integrates navigational memory and strategies with perception and\naction prediction modules. Experimental results on the REVERIE and R2R datasets\nshow that our method effectively enhances the navigation ability of the LLM and\nimproves the interpretability of navigation reasoning.", "published": "2024-05-17 08:33:27", "link": "http://arxiv.org/abs/2405.10620v2", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Specialising and Analysing Instruction-Tuned and Byte-Level Language\n  Models for Organic Reaction Prediction", "abstract": "Transformer-based encoder-decoder models have demonstrated impressive results\nin chemical reaction prediction tasks. However, these models typically rely on\npretraining using tens of millions of unlabelled molecules, which can be\ntime-consuming and GPU-intensive. One of the central questions we aim to answer\nin this work is: Can FlanT5 and ByT5, the encode-decoder models pretrained\nsolely on language data, be effectively specialised for organic reaction\nprediction through task-specific fine-tuning? We conduct a systematic empirical\nstudy on several key issues of the process, including tokenisation, the impact\nof (SMILES-oriented) pretraining, fine-tuning sample efficiency, and decoding\nalgorithms at inference. Our key findings indicate that although being\npretrained only on language tasks, FlanT5 and ByT5 provide a solid foundation\nto fine-tune for reaction prediction, and thus become `chemistry domain\ncompatible' in the process. This suggests that GPU-intensive and expensive\npretraining on a large dataset of unlabelled molecules may be useful yet not\nessential to leverage the power of language models for chemistry. All our\nmodels achieve comparable Top-1 and Top-5 accuracy although some variation\nacross different models does exist. Notably, tokenisation and vocabulary\ntrimming slightly affect final performance but can speed up training and\ninference; The most efficient greedy decoding strategy is very competitive\nwhile only marginal gains can be achieved from more sophisticated decoding\nalgorithms. In summary, we evaluate FlanT5 and ByT5 across several dimensions\nand benchmark their impact on organic reaction prediction, which may guide more\neffective use of these state-of-the-art language models for chemistry-related\ntasks in the future.", "published": "2024-05-17 08:39:56", "link": "http://arxiv.org/abs/2405.10625v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.BM"], "primary_category": "cs.CL"}
{"title": "SynDy: Synthetic Dynamic Dataset Generation Framework for Misinformation\n  Tasks", "abstract": "Diaspora communities are disproportionately impacted by off-the-radar\nmisinformation and often neglected by mainstream fact-checking efforts,\ncreating a critical need to scale-up efforts of nascent fact-checking\ninitiatives. In this paper we present SynDy, a framework for Synthetic Dynamic\nDataset Generation to leverage the capabilities of the largest frontier Large\nLanguage Models (LLMs) to train local, specialized language models. To the best\nof our knowledge, SynDy is the first paper utilizing LLMs to create\nfine-grained synthetic labels for tasks of direct relevance to misinformation\nmitigation, namely Claim Matching, Topical Clustering, and Claim Relationship\nClassification. SynDy utilizes LLMs and social media queries to automatically\ngenerate distantly-supervised, topically-focused datasets with synthetic labels\non these three tasks, providing essential tools to scale up human-led\nfact-checking at a fraction of the cost of human-annotated data. Training on\nSynDy's generated labels shows improvement over a standard baseline and is not\nsignificantly worse compared to training on human labels (which may be\ninfeasible to acquire). SynDy is being integrated into Meedan's chatbot\ntiplines that are used by over 50 organizations, serve over 230K users\nannually, and automatically distribute human-written fact-checks via messaging\napps such as WhatsApp. SynDy will also be integrated into our deployed\nCo-Insights toolkit, enabling low-resource organizations to launch tiplines for\ntheir communities. Finally, we envision SynDy enabling additional fact-checking\ntools such as matching new misinformation claims to high-quality explainers on\ncommon misinformation topics.", "published": "2024-05-17 11:14:55", "link": "http://arxiv.org/abs/2405.10700v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.IR"}
{"title": "Empowering Small-Scale Knowledge Graphs: A Strategy of Leveraging\n  General-Purpose Knowledge Graphs for Enriched Embeddings", "abstract": "Knowledge-intensive tasks pose a significant challenge for Machine Learning\n(ML) techniques. Commonly adopted methods, such as Large Language Models\n(LLMs), often exhibit limitations when applied to such tasks. Nevertheless,\nthere have been notable endeavours to mitigate these challenges, with a\nsignificant emphasis on augmenting LLMs through Knowledge Graphs (KGs). While\nKGs provide many advantages for representing knowledge, their development costs\ncan deter extensive research and applications. Addressing this limitation, we\nintroduce a framework for enriching embeddings of small-scale domain-specific\nKnowledge Graphs with well-established general-purpose KGs. Adopting our\nmethod, a modest domain-specific KG can benefit from a performance boost in\ndownstream tasks when linked to a substantial general-purpose KG. Experimental\nevaluations demonstrate a notable enhancement, with up to a 44% increase\nobserved in the Hits@10 metric. This relatively unexplored research direction\ncan catalyze more frequent incorporation of KGs in knowledge-intensive tasks,\nresulting in more robust, reliable ML implementations, which hallucinates less\nthan prevalent LLM solutions.\n  Keywords: knowledge graph, knowledge graph completion, entity alignment,\nrepresentation learning, machine learning", "published": "2024-05-17 12:46:23", "link": "http://arxiv.org/abs/2405.10745v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Tailoring Vaccine Messaging with Common-Ground Opinions", "abstract": "One way to personalize chatbot interactions is by establishing common ground\nwith the intended reader. A domain where establishing mutual understanding\ncould be particularly impactful is vaccine concerns and misinformation. Vaccine\ninterventions are forms of messaging which aim to answer concerns expressed\nabout vaccination. Tailoring responses in this domain is difficult, since\nopinions often have seemingly little ideological overlap. We define the task of\ntailoring vaccine interventions to a Common-Ground Opinion (CGO). Tailoring\nresponses to a CGO involves meaningfully improving the answer by relating it to\nan opinion or belief the reader holds. In this paper we introduce TAILOR-CGO, a\ndataset for evaluating how well responses are tailored to provided CGOs. We\nbenchmark several major LLMs on this task; finding GPT-4-Turbo performs\nsignificantly better than others. We also build automatic evaluation metrics,\nincluding an efficient and accurate BERT model that outperforms finetuned LLMs,\ninvestigate how to successfully tailor vaccine messaging to CGOs, and provide\nactionable recommendations from this investigation.\n  Code and model weights: https://github.com/rickardstureborg/tailor-cgo\nDataset: https://huggingface.co/datasets/DukeNLP/tailor-cgo", "published": "2024-05-17 15:48:30", "link": "http://arxiv.org/abs/2405.10861v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50 (Primary) 68T01, 68T37, 91F20 (Secondary)", "I.2; I.2.7; I.7"], "primary_category": "cs.CL"}
{"title": "A Framework for Leveraging Partially-Labeled Data for Product\n  Attribute-Value Identification", "abstract": "In the e-commerce domain, the accurate extraction of attribute-value pairs\n(e.g., Brand: Apple) from product titles and user search queries is crucial for\nenhancing search and recommendation systems. A major challenge with neural\nmodels for this task is the lack of high-quality training data, as the\nannotations for attribute-value pairs in the available datasets are often\nincomplete. To address this, we introduce GenToC, a model designed for training\ndirectly with partially-labeled data, eliminating the necessity for a fully\nannotated dataset. GenToC employs a marker-augmented generative model to\nidentify potential attributes, followed by a token classification model that\ndetermines the associated values for each attribute. GenToC outperforms\nexisting state-of-the-art models, exhibiting upto 56.3% increase in the number\nof accurate extractions. Furthermore, we utilize GenToC to regenerate the\ntraining dataset to expand attribute-value annotations. This bootstrapping\nsubstantially improves the data quality for training other standard NER models,\nwhich are typically faster but less capable in handling partially-labeled data,\nenabling them to achieve comparable performance to GenToC. Our results\ndemonstrate GenToC's unique ability to learn from a limited set of\npartially-labeled data and improve the training of more efficient models,\nadvancing the automated extraction of attribute-value pairs. Finally, our model\nhas been successfully integrated into IndiaMART, India's largest B2B e-commerce\nplatform, achieving a significant increase of 20.2% in the number of correctly\nidentified attribute-value pairs over the existing deployed system while\nachieving a high precision of 89.5%.", "published": "2024-05-17 17:09:45", "link": "http://arxiv.org/abs/2405.10918v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Observational Scaling Laws and the Predictability of Language Model\n  Performance", "abstract": "Understanding how language model performance varies with scale is critical to\nbenchmark and algorithm development. Scaling laws are one approach to building\nthis understanding, but the requirement of training models across many\ndifferent scales has limited their use. We propose an alternative,\nobservational approach that bypasses model training and instead builds scaling\nlaws from ~100 publically available models. Building a single scaling law from\nmultiple model families is challenging due to large variations in their\ntraining compute efficiencies and capabilities. However, we show that these\nvariations are consistent with a simple, generalized scaling law where language\nmodel performance is a function of a low-dimensional capability space, and\nmodel families only vary in their efficiency in converting training compute to\ncapabilities. Using this approach, we show the surprising predictability of\ncomplex scaling phenomena: we show that several emergent phenomena follow a\nsmooth, sigmoidal behavior and are predictable from small models; we show that\nthe agent performance of models such as GPT-4 can be precisely predicted from\nsimpler non-agentic benchmarks; and we show how to predict the impact of\npost-training interventions like Chain-of-Thought and Self-Consistency as\nlanguage model capabilities continue to improve.", "published": "2024-05-17 17:49:44", "link": "http://arxiv.org/abs/2405.10938v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Generative Artificial Intelligence: A Systematic Review and Applications", "abstract": "In recent years, the study of artificial intelligence (AI) has undergone a\nparadigm shift. This has been propelled by the groundbreaking capabilities of\ngenerative models both in supervised and unsupervised learning scenarios.\nGenerative AI has shown state-of-the-art performance in solving perplexing\nreal-world conundrums in fields such as image translation, medical diagnostics,\ntextual imagery fusion, natural language processing, and beyond. This paper\ndocuments the systematic review and analysis of recent advancements and\ntechniques in Generative AI with a detailed discussion of their applications\nincluding application-specific models. Indeed, the major impact that generative\nAI has made to date, has been in language generation with the development of\nlarge language models, in the field of image translation and several other\ninterdisciplinary applications of generative AI. Moreover, the primary\ncontribution of this paper lies in its coherent synthesis of the latest\nadvancements in these areas, seamlessly weaving together contemporary\nbreakthroughs in the field. Particularly, how it shares an exploration of the\nfuture trajectory for generative AI. In conclusion, the paper ends with a\ndiscussion of Responsible AI principles, and the necessary ethical\nconsiderations for the sustainability and growth of these generative models.", "published": "2024-05-17 18:03:59", "link": "http://arxiv.org/abs/2405.11029v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Jill Watson: A Virtual Teaching Assistant powered by ChatGPT", "abstract": "Conversational AI agents often require extensive datasets for training that\nare not publicly released, are limited to social chit-chat or handling a\nspecific domain, and may not be easily extended to accommodate the latest\nadvances in AI technologies. This paper introduces Jill Watson, a\nconversational Virtual Teaching Assistant (VTA) leveraging the capabilities of\nChatGPT. Jill Watson based on ChatGPT requires no prior training and uses a\nmodular design to allow the integration of new APIs using a skill-based\narchitecture inspired by XiaoIce. Jill Watson is also well-suited for\nintelligent textbooks as it can process and converse using multiple large\ndocuments. We exclusively utilize publicly available resources for\nreproducibility and extensibility. Comparative analysis shows that our system\noutperforms the legacy knowledge-based Jill Watson as well as the OpenAI\nAssistants service. We employ many safety measures that reduce instances of\nhallucinations and toxicity. The paper also includes real-world examples from a\nclassroom setting that demonstrate different features of Jill Watson and its\neffectiveness.", "published": "2024-05-17 19:55:57", "link": "http://arxiv.org/abs/2405.11070v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "AudioSetMix: Enhancing Audio-Language Datasets with LLM-Assisted\n  Augmentations", "abstract": "Multi-modal learning in the audio-language domain has seen significant\nadvancements in recent years. However, audio-language learning faces challenges\ndue to limited and lower-quality data compared to image-language tasks.\nExisting audio-language datasets are notably smaller, and manual labeling is\nhindered by the need to listen to entire audio clips for accurate labeling.\n  Our method systematically generates audio-caption pairs by augmenting audio\nclips with natural language labels and corresponding audio signal processing\noperations. Leveraging a Large Language Model, we generate descriptions of\naugmented audio clips with a prompt template. This scalable method produces\nAudioSetMix, a high-quality training dataset for text-and-audio related models.\n  Integration of our dataset improves models performance on benchmarks by\nproviding diversified and better-aligned examples. Notably, our dataset\naddresses the absence of modifiers (adjectives and adverbs) in existing\ndatasets. By enabling models to learn these concepts, and generating hard\nnegative examples during training, we achieve state-of-the-art performance on\nmultiple benchmarks.", "published": "2024-05-17 21:08:58", "link": "http://arxiv.org/abs/2405.11093v2", "categories": ["eess.AS", "cs.CL", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Watermarking Language Models for Many Adaptive Users", "abstract": "We study watermarking schemes for language models with provable guarantees.\nAs we show, prior works offer no robustness guarantees against adaptive\nprompting: when a user queries a language model more than once, as even benign\nusers do. And with just a single exception (Christ and Gunn, 2024), prior works\nare restricted to zero-bit watermarking: machine-generated text can be detected\nas such, but no additional information can be extracted from the watermark.\nUnfortunately, merely detecting AI-generated text may not prevent future\nabuses.\n  We introduce multi-user watermarks, which allow tracing model-generated text\nto individual users or to groups of colluding users, even in the face of\nadaptive prompting. We construct multi-user watermarking schemes from\nundetectable, adaptively robust, zero-bit watermarking schemes (and prove that\nthe undetectable zero-bit scheme of Christ, Gunn, and Zamir (2024) is\nadaptively robust). Importantly, our scheme provides both zero-bit and\nmulti-user assurances at the same time. It detects shorter snippets just as\nwell as the original scheme, and traces longer excerpts to individuals.\n  The main technical component is a construction of message-embedding\nwatermarks from zero-bit watermarks. Ours is the first generic reduction\nbetween watermarking schemes for language models. A challenge for such\nreductions is the lack of a unified abstraction for robustness -- that marked\ntext is detectable even after edits. We introduce a new unifying abstraction\ncalled AEB-robustness. AEB-robustness provides that the watermark is detectable\nwhenever the edited text \"approximates enough blocks\" of model-generated\noutput.", "published": "2024-05-17 22:15:30", "link": "http://arxiv.org/abs/2405.11109v2", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "LLM-based Multi-Agent Reinforcement Learning: Current and Future\n  Directions", "abstract": "In recent years, Large Language Models (LLMs) have shown great abilities in\nvarious tasks, including question answering, arithmetic problem solving, and\npoem writing, among others. Although research on LLM-as-an-agent has shown that\nLLM can be applied to Reinforcement Learning (RL) and achieve decent results,\nthe extension of LLM-based RL to Multi-Agent System (MAS) is not trivial, as\nmany aspects, such as coordination and communication between agents, are not\nconsidered in the RL frameworks of a single agent. To inspire more research on\nLLM-based MARL, in this letter, we survey the existing LLM-based single-agent\nand multi-agent RL frameworks and provide potential research directions for\nfuture research. In particular, we focus on the cooperative tasks of multiple\nagents with a common goal and communication among them. We also consider\nhuman-in/on-the-loop scenarios enabled by the language component in the\nframework.", "published": "2024-05-17 22:10:23", "link": "http://arxiv.org/abs/2405.11106v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.MA"}
{"title": "Distinctive and Natural Speaker Anonymization via Singular Value\n  Transformation-assisted Matrix", "abstract": "Speaker anonymization is an effective privacy protection solution that aims\nto conceal the speaker's identity while preserving the naturalness and\ndistinctiveness of the original speech. Mainstream approaches use an\nutterance-level vector from a pre-trained automatic speaker verification (ASV)\nmodel to represent speaker identity, which is then averaged or modified for\nanonymization. However, these systems suffer from deterioration in the\nnaturalness of anonymized speech, degradation in speaker distinctiveness, and\nsevere privacy leakage against powerful attackers. To address these issues and\nespecially generate more natural and distinctive anonymized speech, we propose\na novel speaker anonymization approach that models a matrix related to speaker\nidentity and transforms it into an anonymized singular value\ntransformation-assisted matrix to conceal the original speaker identity. Our\napproach extracts frame-level speaker vectors from a pre-trained ASV model and\nemploys an attention mechanism to create a speaker-score matrix and\nspeaker-related tokens. Notably, the speaker-score matrix acts as the weight\nfor the corresponding speaker-related token, representing the speaker's\nidentity. The singular value transformation-assisted matrix is generated by\nrecomposing the decomposed orthonormal eigenvectors matrix and non-linear\ntransformed singular through Singular Value Decomposition (SVD). Experiments on\nVoicePrivacy Challenge datasets demonstrate the effectiveness of our approach\nin protecting speaker privacy under all attack scenarios while maintaining\nspeech naturalness and distinctiveness.", "published": "2024-05-17 13:48:43", "link": "http://arxiv.org/abs/2405.10786v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Acoustic modeling for Overlapping Speech Recognition: JHU Chime-5\n  Challenge System", "abstract": "This paper summarizes our acoustic modeling efforts in the Johns Hopkins\nUniversity speech recognition system for the CHiME-5 challenge to recognize\nhighly-overlapped dinner party speech recorded by multiple microphone arrays.\nWe explore data augmentation approaches, neural network architectures,\nfront-end speech dereverberation, beamforming and robust i-vector extraction\nwith comparisons of our in-house implementations and publicly available tools.\nWe finally achieved a word error rate of 69.4% on the development set, which is\na 11.7% absolute improvement over the previous baseline of 81.1%, and release\nthis improved baseline with refined techniques/tools as an advanced CHiME-5\nrecipe.", "published": "2024-05-17 20:20:41", "link": "http://arxiv.org/abs/2405.11078v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Implementation of the Feedforward Multichannel Virtual Sensing Active\n  Noise Control (MVANC) by Using MATLAB", "abstract": "The multichannel virtual sensing active noise control (MVANC) methodology is\nan advanced approach that may provide a wide area of silence at specific\nvirtual positions that are distant from the physical error microphones.\nCurrently, there is a scarcity of open-source programs available for the MVANC\nalgorithm. This work presents a MATLAB code for the MVANC approach, utilizing\nthe multichannel filtered-x least mean square (MCFxLMS) algorithm. The code is\ndesigned to be applicable to systems with any number of channels. The code can\nbe found on GitHub.", "published": "2024-05-17 03:28:33", "link": "http://arxiv.org/abs/2405.10510v1", "categories": ["eess.SP", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Enhancing DMI Interactions by Integrating Haptic Feedback for Intricate\n  Vibrato Technique", "abstract": "This paper investigates the integration of force feedback in Digital Musical\nInstruments (DMI), specifically evaluating the reproduction of intricate\nvibrato techniques using haptic feedback controllers. We introduce our system\nfor vibrato modulation using force feedback, composed of Bend-aid (a web-based\nsequencer platform using pre-designed haptic feedback models) and TorqueTuner\n(an open-source 1 Degree-of-Freedom (DoF) rotary haptic device for generating\nprogrammable haptic effects). We designed a formal user study to assess the\nimpact of each haptic mode on user experience in a vibrato mimicry task. Twenty\nmusically trained participants rated their user experience for the three haptic\nmodes (Smooth, Detent, and Spring) using four Likert-scale scores: comfort,\nflexibility, ease of control, and helpfulness for the task. Finally, we asked\nparticipants to share their reflections. Our research indicates that while the\nSpring mode can help with light vibrato, preferences for haptic modes vary\nbased on musical training background. This emphasizes the need for adaptable\ntask interfaces and flexible haptic feedback in DMI design.", "published": "2024-05-17 02:54:44", "link": "http://arxiv.org/abs/2405.10502v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Enhancing the analysis of murine neonatal ultrasonic vocalizations:\n  Development, evaluation, and application of different mathematical models", "abstract": "Rodents employ a broad spectrum of ultrasonic vocalizations (USVs) for social\ncommunication. As these vocalizations offer valuable insights into affective\nstates, social interactions, and developmental stages of animals, various deep\nlearning approaches have aimed to automate both the quantitative (detection)\nand qualitative (classification) analysis of USVs. Here, we present the first\nsystematic evaluation of different types of neural networks for USV\nclassification. We assessed various feedforward networks, including a\ncustom-built, fully-connected network and convolutional neural network,\ndifferent residual neural networks (ResNets), an EfficientNet, and a Vision\nTransformer (ViT). Paired with a refined, entropy-based detection algorithm\n(achieving recall of 94.9% and precision of 99.3%), the best architecture\n(achieving 86.79% accuracy) was integrated into a fully automated pipeline\ncapable of analyzing extensive USV datasets with high reliability.\nAdditionally, users can specify an individual minimum accuracy threshold based\non their research needs. In this semi-automated setup, the pipeline selectively\nclassifies calls with high pseudo-probability, leaving the rest for manual\ninspection. Our study focuses exclusively on neonatal USVs. As part of an\nongoing phenotyping study, our pipeline has proven to be a valuable tool for\nidentifying key differences in USVs produced by mice with autism-like\nbehaviors.", "published": "2024-05-17 07:46:05", "link": "http://arxiv.org/abs/2405.12957v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
