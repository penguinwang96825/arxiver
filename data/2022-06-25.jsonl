{"title": "Probing Causes of Hallucinations in Neural Machine Translations", "abstract": "Hallucination, one kind of pathological translations that bothers Neural\nMachine Translation, has recently drawn much attention. In simple terms,\nhallucinated translations are fluent sentences but barely related to source\ninputs. Arguably, it remains an open problem how hallucination occurs. In this\npaper, we propose to use probing methods to investigate the causes of\nhallucinations from the perspective of model architecture, aiming to avoid such\nproblems in future architecture designs. By conducting experiments over various\nNMT datasets, we find that hallucination is often accompanied by the deficient\nencoder, especially embeddings, and vulnerable cross-attentions, while,\ninterestingly, cross-attention mitigates some errors caused by the encoder.", "published": "2022-06-25 01:57:22", "link": "http://arxiv.org/abs/2206.12529v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph Component Contrastive Learning for Concept Relatedness Estimation", "abstract": "Concept relatedness estimation (CRE) aims to determine whether two given\nconcepts are related. Existing methods only consider the pairwise relationship\nbetween concepts, while overlooking the higher-order relationship that could be\nencoded in a concept-level graph structure. We discover that this underlying\ngraph satisfies a set of intrinsic properties of CRE, including reflexivity,\ncommutativity, and transitivity. In this paper, we formalize the CRE properties\nand introduce a graph structure named ConcreteGraph. To address the data\nscarcity issue in CRE, we introduce a novel data augmentation approach to\nsample new concept pairs from the graph. As it is intractable for data\naugmentation to fully capture the structural information of the ConcreteGraph\ndue to a large amount of potential concept pairs, we further introduce a novel\nGraph Component Contrastive Learning framework to implicitly learn the complete\nstructure of the ConcreteGraph. Empirical results on three datasets show\nsignificant improvement over the state-of-the-art model. Detailed ablation\nstudies demonstrate that our proposed approach can effectively capture the\nhigh-order relationship among concepts.", "published": "2022-06-25 04:52:54", "link": "http://arxiv.org/abs/2206.12556v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Construct a Sentence with Multiple Specified Words", "abstract": "This paper demonstrates a task to finetune a BART model so it can construct a\nsentence from an arbitrary set of words, which used to be a difficult NLP task.\nThe training task is making sentences with four words, but the trained model\ncan generate sentences when fewer or more words are provided. The output\nsentences have high quality in general. The model can have some real-world\napplications, and this task can be used as an evaluation mechanism for any\nlanguage model as well.", "published": "2022-06-25 05:49:51", "link": "http://arxiv.org/abs/2206.12565v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adversarial Self-Attention for Language Understanding", "abstract": "Deep neural models (e.g. Transformer) naturally learn spurious features,\nwhich create a ``shortcut'' between the labels and inputs, thus impairing the\ngeneralization and robustness. This paper advances the self-attention mechanism\nto its robust variant for Transformer-based pre-trained language models (e.g.\nBERT). We propose \\textit{Adversarial Self-Attention} mechanism (ASA), which\nadversarially biases the attentions to effectively suppress the model reliance\non features (e.g. specific keywords) and encourage its exploration of broader\nsemantics. We conduct a comprehensive evaluation across a wide range of tasks\nfor both pre-training and fine-tuning stages. For pre-training, ASA unfolds\nremarkable performance gains compared to naive training for longer steps. For\nfine-tuning, ASA-empowered models outweigh naive models by a large margin\nconsidering both generalization and robustness.", "published": "2022-06-25 09:18:10", "link": "http://arxiv.org/abs/2206.12608v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Protoformer: Embedding Prototypes for Transformers", "abstract": "Transformers have been widely applied in text classification. Unfortunately,\nreal-world data contain anomalies and noisy labels that cause challenges for\nstate-of-art Transformers. This paper proposes Protoformer, a novel\nself-learning framework for Transformers that can leverage problematic samples\nfor text classification. Protoformer features a selection mechanism for\nembedding samples that allows us to efficiently extract and utilize anomalies\nprototypes and difficult class prototypes. We demonstrated such capabilities on\ndatasets with diverse textual structures (e.g., Twitter, IMDB, ArXiv). We also\napplied the framework to several models. The results indicate that Protoformer\ncan improve current Transformers in various empirical settings.", "published": "2022-06-25 18:51:53", "link": "http://arxiv.org/abs/2206.12710v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-supervised Context-aware Style Representation for Expressive Speech\n  Synthesis", "abstract": "Expressive speech synthesis, like audiobook synthesis, is still challenging\nfor style representation learning and prediction. Deriving from reference audio\nor predicting style tags from text requires a huge amount of labeled data,\nwhich is costly to acquire and difficult to define and annotate accurately. In\nthis paper, we propose a novel framework for learning style representation from\nabundant plain text in a self-supervised manner. It leverages an emotion\nlexicon and uses contrastive learning and deep clustering. We further integrate\nthe style representation as a conditioned embedding in a multi-style\nTransformer TTS. Comparing with multi-style TTS by predicting style tags\ntrained on the same dataset but with human annotations, our method achieves\nimproved results according to subjective evaluations on both in-domain and\nout-of-domain test sets in audiobook speech. Moreover, with implicit\ncontext-aware style representation, the emotion transition of synthesized audio\nin a long paragraph appears more natural. The audio samples are available on\nthe demo web.", "published": "2022-06-25 05:29:48", "link": "http://arxiv.org/abs/2206.12559v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Language Models as Knowledge Embeddings", "abstract": "Knowledge embeddings (KE) represent a knowledge graph (KG) by embedding\nentities and relations into continuous vector spaces. Existing methods are\nmainly structure-based or description-based. Structure-based methods learn\nrepresentations that preserve the inherent structure of KGs. They cannot well\nrepresent abundant long-tail entities in real-world KGs with limited structural\ninformation. Description-based methods leverage textual information and\nlanguage models. Prior approaches in this direction barely outperform\nstructure-based ones, and suffer from problems like expensive negative sampling\nand restrictive description demand. In this paper, we propose LMKE, which\nadopts Language Models to derive Knowledge Embeddings, aiming at both enriching\nrepresentations of long-tail entities and solving problems of prior\ndescription-based methods. We formulate description-based KE learning with a\ncontrastive learning framework to improve efficiency in training and\nevaluation. Experimental results show that LMKE achieves state-of-the-art\nperformance on KE benchmarks of link prediction and triple classification,\nespecially for long-tail entities.", "published": "2022-06-25 10:39:12", "link": "http://arxiv.org/abs/2206.12617v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Distilling a Pretrained Language Model to a Multilingual ASR Model", "abstract": "Multilingual speech data often suffer from long-tailed language distribution,\nresulting in performance degradation. However, multilingual text data is much\neasier to obtain, yielding a more useful general language model. Hence, we are\nmotivated to distill the rich knowledge embedded inside a well-trained teacher\ntext model to the student speech model. We propose a novel method called the\nDistilling a Language model to a Speech model (Distill-L2S), which aligns the\nlatent representations of two different modalities. The subtle differences are\nhandled by the shrinking mechanism, nearest-neighbor interpolation, and a\nlearnable linear projection layer. We demonstrate the effectiveness of our\ndistillation method by applying it to the multilingual automatic speech\nrecognition (ASR) task. We distill the transformer-based cross-lingual language\nmodel (InfoXLM) while fine-tuning the large-scale multilingual ASR model\n(XLSR-wav2vec 2.0) for each language. We show the superiority of our method on\n20 low-resource languages of the CommonVoice dataset with less than 100 hours\nof speech data.", "published": "2022-06-25 12:36:11", "link": "http://arxiv.org/abs/2206.12638v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis with R: Natural Language Processing for\n  Semi-Automated Assessments of Qualitative Data", "abstract": "Sentiment analysis is a sub-discipline in the field of natural language\nprocessing and computational linguistics and can be used for automated or\nsemi-automated analyses of text documents. One of the aims of these analyses is\nto recognize an expressed attitude as positive or negative as it can be\ncontained in comments on social media platforms or political documents and\nspeeches as well as fictional and nonfictional texts. Regarding analyses of\ncomments on social media platforms, this is an extension of the previous\ntutorial on semi-automated screenings of social media network data. A\nlongitudinal perspective regarding social media comments as well as\ncross-sectional perspectives regarding fictional and nonfictional texts, e.g.\nentire books and libraries, can lead to extensive text documents. Their\nanalyses can be simplified and accelerated by using sentiment analysis with\nacceptable inter-rater reliability. Therefore, this tutorial introduces the\nbasic functions for performing a sentiment analysis with R and explains how\ntext documents can be analysed step by step - regardless of their underlying\nformatting. All prerequisites and steps are described in detail and associated\ncodes are available on GitHub. A comparison of two political speeches\nillustrates a possible use case.", "published": "2022-06-25 13:25:39", "link": "http://arxiv.org/abs/2206.12649v1", "categories": ["cs.CL", "cs.CV", "stat.AP", "D.1.5; D.3.0; I.7; J.4; K.4"], "primary_category": "cs.CL"}
{"title": "Synthesizing Personalized Non-speech Vocalization from Discrete Speech\n  Representations", "abstract": "We formulated non-speech vocalization (NSV) modeling as a text-to-speech task\nand verified its viability. Specifically, we evaluated the phonetic\nexpressivity of HUBERT speech units on NSVs and verified our model's ability to\ncontrol over speaker timbre even though the training data is speaker few-shot.\nIn addition, we substantiated that the heterogeneity in recording conditions is\nthe major obstacle for NSV modeling. Finally, we discussed five improvements\nover our method for future research. Audio samples of synthesized NSVs are\navailable on our demo page: https://resemble-ai.github.io/reLaugh.", "published": "2022-06-25 14:27:10", "link": "http://arxiv.org/abs/2206.12662v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Evaluation of Semantic Answer Similarity Metrics", "abstract": "There are several issues with the existing general machine translation or\nnatural language generation evaluation metrics, and question-answering (QA)\nsystems are indifferent in that context. To build robust QA systems, we need\nthe ability to have equivalently robust evaluation systems to verify whether\nmodel predictions to questions are similar to ground-truth annotations. The\nability to compare similarity based on semantics as opposed to pure string\noverlap is important to compare models fairly and to indicate more realistic\nacceptance criteria in real-life applications. We build upon the first to our\nknowledge paper that uses transformer-based model metrics to assess semantic\nanswer similarity and achieve higher correlations to human judgement in the\ncase of no lexical overlap. We propose cross-encoder augmented bi-encoder and\nBERTScore models for semantic answer similarity, trained on a new dataset\nconsisting of name pairs of US-American public figures. As far as we are\nconcerned, we provide the first dataset of co-referent name string pairs along\nwith their similarities, which can be used for training.", "published": "2022-06-25 14:40:36", "link": "http://arxiv.org/abs/2206.12664v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TEVR: Improving Speech Recognition by Token Entropy Variance Reduction", "abstract": "This paper presents TEVR, a speech recognition model designed to minimize the\nvariation in token entropy w.r.t. to the language model. This takes advantage\nof the fact that if the language model will reliably and accurately predict a\ntoken anyway, then the acoustic model doesn't need to be accurate in\nrecognizing it. We train German ASR models with 900 million parameters and show\nthat on CommonVoice German, TEVR scores a very competitive 3.64% word error\nrate, which outperforms the best reported results by a relative 16.89%\nreduction in word error rate. We hope that releasing our fully trained speech\nrecognition pipeline to the community will lead to privacy-preserving offline\nvirtual assistants in the future.", "published": "2022-06-25 16:42:05", "link": "http://arxiv.org/abs/2206.12693v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "F.2.1; I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Generating Diverse Vocal Bursts with StyleGAN2 and MEL-Spectrograms", "abstract": "We describe our approach for the generative emotional vocal burst task (ExVo\nGenerate) of the ICML Expressive Vocalizations Competition. We train a\nconditional StyleGAN2 architecture on mel-spectrograms of preprocessed versions\nof the audio samples. The mel-spectrograms generated by the model are then\ninverted back to the audio domain. As a result, our generated samples\nsubstantially improve upon the baseline provided by the competition from a\nqualitative and quantitative perspective for all emotions. More precisely, even\nfor our worst-performing emotion (awe), we obtain an FAD of 1.76 compared to\nthe baseline of 4.81 (as a reference, the FAD between the train/validation sets\nfor awe is 0.776).", "published": "2022-06-25 05:39:52", "link": "http://arxiv.org/abs/2206.12563v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-supervision and Learnable STRFs for Age, Emotion, and Country\n  Prediction", "abstract": "This work presents a multitask approach to the simultaneous estimation of\nage, country of origin, and emotion given vocal burst audio for the 2022 ICML\nExpressive Vocalizations Challenge ExVo-MultiTask track. The method of choice\nutilized a combination of spectro-temporal modulation and self-supervised\nfeatures, followed by an encoder-decoder network organized in a multitask\nparadigm. We evaluate the complementarity between the tasks posed by examining\nindependent task-specific and joint models, and explore the relative strengths\nof different feature sets. We also introduce a simple score fusion mechanism to\nleverage the complementarity of different feature sets for this task.\n  We find that robust data preprocessing in conjunction with score fusion over\nspectro-temporal receptive field and HuBERT models achieved our best\nExVo-MultiTask test score of 0.412.", "published": "2022-06-25 06:09:10", "link": "http://arxiv.org/abs/2206.12568v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
