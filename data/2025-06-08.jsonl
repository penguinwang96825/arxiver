{"title": "Reward Model Interpretability via Optimal and Pessimal Tokens", "abstract": "Reward modeling has emerged as a crucial component in aligning large language\nmodels with human values. Significant attention has focused on using reward\nmodels as a means for fine-tuning generative models. However, the reward models\nthemselves -- which directly encode human value judgments by turning\nprompt-response pairs into scalar rewards -- remain relatively understudied. We\npresent a novel approach to reward model interpretability through exhaustive\nanalysis of their responses across their entire vocabulary space. By examining\nhow different reward models score every possible single-token response to\nvalue-laden prompts, we uncover several striking findings: (i) substantial\nheterogeneity between models trained on similar objectives, (ii) systematic\nasymmetries in how models encode high- vs low-scoring tokens, (iii) significant\nsensitivity to prompt framing that mirrors human cognitive biases, and (iv)\novervaluation of more frequent tokens. We demonstrate these effects across ten\nrecent open-source reward models of varying parameter counts and architectures.\nOur results challenge assumptions about the interchangeability of reward\nmodels, as well as their suitability as proxies of complex and\ncontext-dependent human values. We find that these models can encode concerning\nbiases toward certain identity groups, which may emerge as unintended\nconsequences of harmlessness training -- distortions that risk propagating\nthrough the downstream large language models now deployed to millions.", "published": "2025-06-08 23:56:58", "link": "http://arxiv.org/abs/2506.07326v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "I.2.6; I.2.7; H.5.2; J.4; K.4.2"], "primary_category": "cs.CL"}
{"title": "ConfQA: Answer Only If You Are Confident", "abstract": "Can we teach Large Language Models (LLMs) to refrain from hallucinating\nfactual statements? In this paper we present a fine-tuning strategy that we\ncall ConfQA, which can reduce hallucination rate from 20-40% to under 5% across\nmultiple factuality benchmarks. The core idea is simple: when the LLM answers a\nquestion correctly, it is trained to continue with the answer; otherwise, it is\ntrained to admit \"I am unsure\". But there are two key factors that make the\ntraining highly effective. First, we introduce a dampening prompt \"answer only\nif you are confident\" to explicitly guide the behavior, without which\nhallucination remains high as 15%-25%. Second, we leverage simple factual\nstatements, specifically attribute values from knowledge graphs, to help LLMs\ncalibrate the confidence, resulting in robust generalization across domains and\nquestion types. Building on this insight, we propose the Dual Neural Knowledge\nframework, which seamlessly select between internally parameterized neural\nknowledge and externally recorded symbolic knowledge based on ConfQA's\nconfidence. The framework enables potential accuracy gains to beyond 95%, while\nreducing unnecessary external retrievals by over 30%.", "published": "2025-06-08 22:51:46", "link": "http://arxiv.org/abs/2506.07309v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Subjectivity in the Annotation of Bridging Anaphora", "abstract": "Bridging refers to the associative relationship between inferable entities in\na discourse and the antecedents which allow us to understand them, such as\nunderstanding what \"the door\" means with respect to an aforementioned \"house\".\nAs identifying associative relations between entities is an inherently\nsubjective task, it is difficult to achieve consistent agreement in the\nannotation of bridging anaphora and their antecedents. In this paper, we\nexplore the subjectivity involved in the annotation of bridging instances at\nthree levels: anaphor recognition, antecedent resolution, and bridging subtype\nselection. To do this, we conduct an annotation pilot on the test set of the\nexisting GUM corpus, and propose a newly developed classification system for\nbridging subtypes, which we compare to previously proposed schemes. Our results\nsuggest that some previous resources are likely to be severely under-annotated.\nWe also find that while agreement on the bridging subtype category was\nmoderate, annotator overlap for exhaustively identifying instances of bridging\nis low, and that many disagreements resulted from subjective understanding of\nthe entities involved.", "published": "2025-06-08 21:40:09", "link": "http://arxiv.org/abs/2506.07297v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Exploring the Impact of Temperature on Large Language Models:Hot or Cold?", "abstract": "The sampling temperature, a critical hyperparameter in large language models\n(LLMs), modifies the logits before the softmax layer, thereby reshaping the\ndistribution of output tokens. Recent studies have challenged the Stochastic\nParrots analogy by demonstrating that LLMs are capable of understanding\nsemantics rather than merely memorizing data and that randomness, modulated by\nsampling temperature, plays a crucial role in model inference. In this study,\nwe systematically evaluated the impact of temperature in the range of 0 to 2 on\ndata sets designed to assess six different capabilities, conducting statistical\nanalyses on open source models of three different sizes: small (1B--4B), medium\n(6B--13B), and large (40B--80B). Our findings reveal distinct skill-specific\neffects of temperature on model performance, highlighting the complexity of\noptimal temperature selection in practical applications. To address this\nchallenge, we propose a BERT-based temperature selector that takes advantage of\nthese observed effects to identify the optimal temperature for a given prompt.\nWe demonstrate that this approach can significantly improve the performance of\nsmall and medium models in the SuperGLUE datasets. Furthermore, our study\nextends to FP16 precision inference, revealing that temperature effects are\nconsistent with those observed in 4-bit quantized models. By evaluating\ntemperature effects up to 4.0 in three quantized models, we find that the\nMutation Temperature -- the point at which significant performance changes\noccur -- increases with model size.", "published": "2025-06-08 21:36:26", "link": "http://arxiv.org/abs/2506.07295v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parsing the Switch: LLM-Based UD Annotation for Complex Code-Switched and Low-Resource Languages", "abstract": "Code-switching presents a complex challenge for syntactic analysis,\nespecially in low-resource language settings where annotated data is scarce.\nWhile recent work has explored the use of large language models (LLMs) for\nsequence-level tagging, few approaches systematically investigate how well\nthese models capture syntactic structure in code-switched contexts. Moreover,\nexisting parsers trained on monolingual treebanks often fail to generalize to\nmultilingual and mixed-language input. To address this gap, we introduce the\nBiLingua Parser, an LLM-based annotation pipeline designed to produce Universal\nDependencies (UD) annotations for code-switched text. First, we develop a\nprompt-based framework for Spanish-English and Spanish-Guaran\\'i data,\ncombining few-shot LLM prompting with expert review. Second, we release two\nannotated datasets, including the first Spanish-Guaran\\'i UD-parsed corpus.\nThird, we conduct a detailed syntactic analysis of switch points across\nlanguage pairs and communicative contexts. Experimental results show that\nBiLingua Parser achieves up to 95.29% LAS after expert revision, significantly\noutperforming prior baselines and multilingual parsers. These results show that\nLLMs, when carefully guided, can serve as practical tools for bootstrapping\nsyntactic resources in under-resourced, code-switched environments. Data and\nsource code are available at https://github.com/N3mika/ParsingProject", "published": "2025-06-08 20:23:57", "link": "http://arxiv.org/abs/2506.07274v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Question Answering under Temporal Conflict: Evaluating and Organizing Evolving Knowledge with LLMs", "abstract": "Large language models (LLMs) exhibit remarkable capabilities in question\nanswering and reasoning thanks to their extensive parametric memory. However,\ntheir knowledge is inherently limited by the scope of their pre-training data,\nwhile real-world information evolves continuously. Updating this knowledge\ntypically requires costly and brittle re-training, or in-context learning\n(ICL), which becomes impractical at scale given the volume and volatility of\nmodern information. Motivated by these limitations, we investigate how LLMs\nperform when exposed to temporal text corpora, or documents that reflect\nevolving knowledge over time, such as sports biographies where facts like a\nplayer's \"current team\" change year by year. To this end, we introduce two new\nbenchmarks: Temporal Wiki, which captures factual drift across historical\nWikipedia snapshots, and Unified Clark, which aggregates timestamped news\narticles to simulate real-world information accumulation. Our analysis reveals\nthat LLMs often struggle to reconcile conflicting or outdated facts and can be\nmisled when multiple versions of a fact appear in context. To address these\nissues, we propose a lightweight, agentic framework that incrementally builds a\nstructured, external memory from source documents without requiring\nre-training. This knowledge organization strategy enables models to retrieve\nand reason over temporally filtered, relevant information at inference time.\nEmpirically, our method outperforms ICL and RAG baselines across both\nbenchmarks, especially on questions requiring more complex reasoning or\nintegration of conflicting facts.", "published": "2025-06-08 20:13:33", "link": "http://arxiv.org/abs/2506.07270v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bias Attribution in Filipino Language Models: Extending a Bias Interpretability Metric for Application on Agglutinative Languages", "abstract": "Emerging research on bias attribution and interpretability have revealed how\ntokens contribute to biased behavior in language models processing English\ntexts. We build on this line of inquiry by adapting the information-theoretic\nbias attribution score metric for implementation on models handling\nagglutinative languages, particularly Filipino. We then demonstrate the\neffectiveness of our adapted method by using it on a purely Filipino model and\non three multilingual models: one trained on languages worldwide and two on\nSoutheast Asian data. Our results show that Filipino models are driven towards\nbias by words pertaining to people, objects, and relationships, entity-based\nthemes that stand in contrast to the action-heavy nature of bias-contributing\nthemes in English (i.e., criminal, sexual, and prosocial behaviors). These\nfindings point to differences in how English and non-English models process\ninputs linked to sociodemographic groups and bias.", "published": "2025-06-08 18:13:18", "link": "http://arxiv.org/abs/2506.07249v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving the Efficiency of Long Document Classification using Sentence Ranking Approach", "abstract": "Long document classification poses challenges due to the computational\nlimitations of transformer-based models, particularly BERT, which are\nconstrained by fixed input lengths and quadratic attention complexity.\nMoreover, using the full document for classification is often redundant, as\nonly a subset of sentences typically carries the necessary information. To\naddress this, we propose a TF-IDF-based sentence ranking method that improves\nefficiency by selecting the most informative content. Our approach explores\nfixed-count and percentage-based sentence selection, along with an enhanced\nscoring strategy combining normalized TF-IDF scores and sentence length.\nEvaluated on the MahaNews LDC dataset of long Marathi news articles, the method\nconsistently outperforms baselines such as first, last, and random sentence\nselection. With MahaBERT-v2, we achieve near-identical classification accuracy\nwith just a 0.33 percent drop compared to the full-context baseline, while\nreducing input size by over 50 percent and inference latency by 43 percent.\nThis demonstrates that significant context reduction is possible without\nsacrificing performance, making the method practical for real-world long\ndocument classification tasks.", "published": "2025-06-08 18:09:43", "link": "http://arxiv.org/abs/2506.07248v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes", "abstract": "Recent advancements in large language models (LLMs) have significantly\nimproved performance on the Text-to-SQL task. However, prior approaches\ntypically rely on static, pre-processed database information provided at\ninference time, which limits the model's ability to fully understand the\ndatabase contents. Without dynamic interaction, LLMs are constrained to fixed,\nhuman-provided context and cannot autonomously explore the underlying data. To\naddress this limitation, we propose SDE-SQL, a framework that enables large\nlanguage models to perform self-driven exploration of databases during\ninference. This is accomplished by generating and executing SQL probes, which\nallow the model to actively retrieve information from the database and\niteratively update its understanding of the data. Unlike prior methods, SDE-SQL\noperates in a zero-shot setting, without relying on any question-SQL pairs as\nin-context demonstrations. When evaluated on the BIRD benchmark with\nQwen2.5-72B-Instruct, SDE-SQL achieves an 8.02% relative improvement in\nexecution accuracy over the vanilla Qwen2.5-72B-Instruct baseline, establishing\na new state-of-the-art among methods based on open-source models without\nsupervised fine-tuning (SFT) or model ensembling. Moreover, with SFT, the\nperformance of SDE-SQL can be further enhanced, yielding an additional 0.52%\nimprovement.", "published": "2025-06-08 18:01:26", "link": "http://arxiv.org/abs/2506.07245v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Step Visual Reasoning with Visual Tokens Scaling and Verification", "abstract": "Multi-modal large language models (MLLMs) have achieved remarkable\ncapabilities by integrating visual perception with language understanding,\nenabling applications such as image-grounded dialogue, visual question\nanswering, and scientific analysis. However, most MLLMs adopt a static\ninference paradigm, encoding the entire image into fixed visual tokens upfront,\nwhich limits their ability to iteratively refine understanding or adapt to\ncontext during inference. This contrasts sharply with human perception, which\nis dynamic, selective, and feedback-driven. In this work, we introduce a novel\nframework for inference-time visual token scaling that enables MLLMs to perform\niterative, verifier-guided reasoning over visual content. We formulate the\nproblem as a Markov Decision Process, involving a reasoner that proposes visual\nactions and a verifier, which is trained via multi-step Direct Preference\nOptimization (DPO), that evaluates these actions and determines when reasoning\nshould terminate. To support this, we present a new dataset, VTS, comprising\nsupervised reasoning trajectories (VTS-SFT) and preference-labeled reasoning\ncomparisons (VTS-DPO). Our method significantly outperforms existing approaches\nacross diverse visual reasoning benchmarks, offering not only improved accuracy\nbut also more interpretable and grounded reasoning processes. These results\ndemonstrate the promise of dynamic inference mechanisms for enabling\nfine-grained, context-aware visual reasoning in next-generation MLLMs.", "published": "2025-06-08 17:38:49", "link": "http://arxiv.org/abs/2506.07235v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Reducing Object Hallucination in Large Audio-Language Models via Audio-Aware Decoding", "abstract": "Large Audio-Language Models (LALMs) can take audio and text as the inputs and\nanswer questions about the audio. While prior LALMs have shown strong\nperformance on standard benchmarks, there has been alarming evidence that LALMs\ncan hallucinate what is presented in the audio. To mitigate the hallucination\nof LALMs, we introduce Audio-Aware Decoding (AAD), a lightweight inference-time\nstrategy that uses contrastive decoding to compare the token prediction logits\nwith and without the audio context. By contrastive decoding, AAD promotes the\ntokens whose probability increases when the audio is present. We conduct our\nexperiment on object hallucination datasets with three LALMs and show that AAD\nimproves the F1 score by 0.046 to 0.428. We also show that AAD can improve the\naccuracy on general audio QA datasets like Clotho-AQA by 5.4% to 10.3%. We\nconduct thorough ablation studies to understand the effectiveness of each\ncomponent in AAD.", "published": "2025-06-08 17:36:50", "link": "http://arxiv.org/abs/2506.07233v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning", "abstract": "Multimodal large language models (MLLMs) have achieved strong performance on\nvision-language tasks but still struggle with fine-grained visual differences,\nleading to hallucinations or missed semantic shifts. We attribute this to\nlimitations in both training data and learning objectives. To address these\nissues, we propose a controlled data generation pipeline that produces\nminimally edited image pairs with semantically aligned captions. Using this\npipeline, we construct the Micro Edit Dataset (MED), containing over 50K\nimage-text pairs spanning 11 fine-grained edit categories, including attribute,\ncount, position, and object presence changes. Building on MED, we introduce a\nsupervised fine-tuning (SFT) framework with a feature-level consistency loss\nthat promotes stable visual embeddings under small edits. We evaluate our\napproach on the Micro Edit Detection benchmark, which includes carefully\nbalanced evaluation pairs designed to test sensitivity to subtle visual\nvariations across the same edit categories. Our method improves difference\ndetection accuracy and reduces hallucinations compared to strong baselines,\nincluding GPT-4o. Moreover, it yields consistent gains on standard\nvision-language tasks such as image captioning and visual question answering.\nThese results demonstrate the effectiveness of combining targeted data and\nalignment objectives for enhancing fine-grained visual reasoning in MLLMs.", "published": "2025-06-08 17:23:36", "link": "http://arxiv.org/abs/2506.07227v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical Action Planning", "abstract": "Effective evaluation is critical for driving advancements in MLLM research.\nThe surgical action planning (SAP) task, which aims to generate future action\nsequences from visual inputs, demands precise and sophisticated analytical\ncapabilities. Unlike mathematical reasoning, surgical decision-making operates\nin life-critical domains and requires meticulous, verifiable processes to\nensure reliability and patient safety. This task demands the ability to\ndistinguish between atomic visual actions and coordinate complex, long-horizon\nprocedures, capabilities that are inadequately evaluated by current benchmarks.\nTo address this gap, we introduce SAP-Bench, a large-scale, high-quality\ndataset designed to enable multimodal large language models (MLLMs) to perform\ninterpretable surgical action planning. Our SAP-Bench benchmark, derived from\nthe cholecystectomy procedures context with the mean duration of 1137.5s, and\nintroduces temporally-grounded surgical action annotations, comprising the\n1,226 clinically validated action clips (mean duration: 68.7s) capturing five\nfundamental surgical actions across 74 procedures. The dataset provides 1,152\nstrategically sampled current frames, each paired with the corresponding next\naction as multimodal analysis anchors. We propose the MLLM-SAP framework that\nleverages MLLMs to generate next action recommendations from the current\nsurgical scene and natural language instructions, enhanced with injected\nsurgical domain knowledge. To assess our dataset's effectiveness and the\nbroader capabilities of current models, we evaluate seven state-of-the-art\nMLLMs (e.g., OpenAI-o1, GPT-4o, QwenVL2.5-72B, Claude-3.5-Sonnet, GeminiPro2.5,\nStep-1o, and GLM-4v) and reveal critical gaps in next action prediction\nperformance.", "published": "2025-06-08 15:30:04", "link": "http://arxiv.org/abs/2506.07196v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images", "abstract": "While multimodal large language models excel at various tasks, they still\nsuffer from hallucinations, which limit their reliability and scalability for\nbroader domain applications. To address this issue, recent research mainly\nfocuses on objective hallucination. However, for sequential images, besides\nobjective hallucination, there is also behavioral hallucination, which is less\nstudied. This work aims to fill in the gap. We first reveal that behavioral\nhallucinations mainly arise from two key factors: prior-driven bias and the\nsnowball effect. Based on these observations, we introduce SHE (Sequence\nHallucination Eradication), a lightweight, two-stage framework that (1) detects\nhallucinations via visual-textual alignment check using our proposed adaptive\ntemporal window and (2) mitigates them via orthogonal projection onto the joint\nembedding space. We also propose a new metric (BEACH) to quantify behavioral\nhallucination severity. Empirical results on standard benchmarks demonstrate\nthat SHE reduces behavioral hallucination by over 10% on BEACH while\nmaintaining descriptive accuracy.", "published": "2025-06-08 15:08:52", "link": "http://arxiv.org/abs/2506.07184v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs", "abstract": "As video large language models (Video-LLMs) become increasingly integrated\ninto real-world applications that demand grounded multimodal reasoning,\nensuring their factual consistency and reliability is of critical importance.\nHowever, sycophancy, the tendency of these models to align with user input even\nwhen it contradicts the visual evidence, undermines their trustworthiness in\nsuch contexts. Current sycophancy research has largely overlooked its specific\nmanifestations in the video-language domain, resulting in a notable absence of\nsystematic benchmarks and targeted evaluations to understand how Video-LLMs\nrespond under misleading user input. To fill this gap, we propose VISE\n(Video-LLM Sycophancy Benchmarking and Evaluation), the first dedicated\nbenchmark designed to evaluate sycophantic behavior in state-of-the-art\nVideo-LLMs across diverse question formats, prompt biases, and visual reasoning\ntasks. Specifically, VISE pioneeringly brings linguistic perspectives on\nsycophancy into the visual domain, enabling fine-grained analysis across\nmultiple sycophancy types and interaction patterns. In addition, we explore\nkey-frame selection as an interpretable, training-free mitigation strategy,\nwhich reveals potential paths for reducing sycophantic bias by strengthening\nvisual grounding.", "published": "2025-06-08 15:00:21", "link": "http://arxiv.org/abs/2506.07180v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "RULE: Reinforcement UnLEarning Achieves Forget-Retain Pareto Optimality", "abstract": "The widespread deployment of Large Language Models (LLMs) trained on massive,\nuncurated corpora has raised growing concerns about the inclusion of sensitive,\ncopyrighted, or illegal content. This has led to increasing interest in LLM\nunlearning: the task of selectively removing specific information from a model\nwithout retraining from scratch or degrading overall utility. However, existing\nmethods often rely on large-scale forget and retain datasets, and suffer from\nunnatural responses, poor generalization, or catastrophic utility loss. In this\nwork, we propose Reinforcement UnLearning (RULE), an efficient framework that\nformulates unlearning as a refusal boundary optimization problem. RULE is\ntrained with a small portion of the forget set and synthesized boundary\nqueries, using a verifiable reward function that encourages safe refusal on\nforget--related queries while preserving helpful responses on permissible\ninputs. We provide both theoretical and empirical evidence demonstrating the\neffectiveness of RULE in achieving targeted unlearning without compromising\nmodel utility. Experimental results show that, with only $12%$ forget set and\n$8%$ synthesized boundary data, RULE outperforms existing baselines by up to\n$17.5%$ forget quality and $16.3%$ naturalness response while maintaining\ngeneral utility, achieving forget--retain Pareto optimality. Remarkably, we\nfurther observe that RULE improves the naturalness of model outputs, enhances\ntraining efficiency, and exhibits strong generalization ability, generalizing\nrefusal behavior to semantically related but unseen queries.", "published": "2025-06-08 14:38:39", "link": "http://arxiv.org/abs/2506.07171v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CTDGSI: A comprehensive exploitation of instance selection methods for automatic text classification. VII Concurso de Teses, Disserta\u00e7\u00f5es e Trabalhos de Gradua\u00e7\u00e3o em SI -- XXI Simp\u00f3sio Brasileiro de Sistemas de Informa\u00e7\u00e3o", "abstract": "Progress in Natural Language Processing (NLP) has been dictated by the rule\nof more: more data, more computing power and more complexity, best exemplified\nby the Large Language Models. However, training (or fine-tuning) large dense\nmodels for specific applications usually requires significant amounts of\ncomputing resources. This \\textbf{Ph.D. dissertation} focuses on an\nunder-investi\\-gated NLP data engineering technique, whose potential is\nenormous in the current scenario known as Instance Selection (IS). The IS goal\nis to reduce the training set size by removing noisy or redundant instances\nwhile maintaining the effectiveness of the trained models and reducing the\ntraining process cost. We provide a comprehensive and scientifically sound\ncomparison of IS methods applied to an essential NLP task -- Automatic Text\nClassification (ATC), considering several classification solutions and many\ndatasets. Our findings reveal a significant untapped potential for IS\nsolutions. We also propose two novel IS solutions that are noise-oriented and\nredundancy-aware, specifically designed for large datasets and transformer\narchitectures. Our final solution achieved an average reduction of 41\\% in\ntraining sets, while maintaining the same levels of effectiveness in all\ndatasets. Importantly, our solutions demonstrated speedup improvements of 1.67x\n(up to 2.46x), making them scalable for datasets with hundreds of thousands of\ndocuments.", "published": "2025-06-08 14:34:57", "link": "http://arxiv.org/abs/2506.07169v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Text-Attributed Graph Learning through Selective Annotation and Graph Alignment", "abstract": "In the realm of Text-attributed Graphs (TAGs), traditional graph neural\nnetworks (GNNs) often fall short due to the complex textual information\nassociated with each node. Recent methods have improved node representations by\nleveraging large language models (LLMs) to enhance node text features, but\nthese approaches typically require extensive annotations or fine-tuning across\nall nodes, which is both time-consuming and costly. To overcome these\nchallenges, we introduce GAGA, an efficient framework for TAG representation\nlearning. GAGA reduces annotation time and cost by focusing on annotating only\nrepresentative nodes and edges. It constructs an annotation graph that captures\nthe topological relationships among these annotations. Furthermore, GAGA\nemploys a two-level alignment module to effectively integrate the annotation\ngraph with the TAG, aligning their underlying structures. Experiments show that\nGAGA achieves classification accuracies on par with or surpassing\nstate-of-the-art methods while requiring only 1% of the data to be annotated,\ndemonstrating its high efficiency.", "published": "2025-06-08 14:34:29", "link": "http://arxiv.org/abs/2506.07168v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "GeometryZero: Improving Geometry Solving for LLM with Group Contrastive Policy Optimization", "abstract": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities across diverse domains, particularly in mathematical reasoning,\namid which geometry problem solving remains a challenging area where auxiliary\nconstruction plays a enssential role. Existing approaches either achieve\nsuboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring\nmassive computational costs. We posit that reinforcement learning with\nverifiable reward (e.g., GRPO) offers a promising direction for training\nsmaller models that effectively combine auxiliary construction with robust\ngeometric reasoning. However, directly applying GRPO to geometric reasoning\npresents fundamental limitations due to its dependence on unconditional\nrewards, which leads to indiscriminate and counterproductive auxiliary\nconstructions. To address these challenges, we propose Group Contrastive Policy\nOptimization (GCPO), a novel reinforcement learning framework featuring two key\ninnovations: (1) Group Contrastive Masking, which adaptively provides positive\nor negative reward signals for auxiliary construction based on contextual\nutility, and a (2) length reward that promotes longer reasoning chains.\nBuilding on GCPO, we develop GeometryZero, a family of affordable-size\ngeometric reasoning models that judiciously determine when to employ auxiliary\nconstruction. Our extensive empirical evaluation across popular geometric\nbenchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models\nconsistently outperform baselines (e.g. GRPO), achieving an average improvement\nof 4.29% across all benchmarks.", "published": "2025-06-08 14:18:15", "link": "http://arxiv.org/abs/2506.07160v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntactic Control of Language Models by Posterior Inference", "abstract": "Controlling the syntactic structure of text generated by language models is\nvaluable for applications requiring clarity, stylistic consistency, or\ninterpretability, yet it remains a challenging task. In this paper, we argue\nthat sampling algorithms based on the posterior inference can effectively\nenforce a target constituency structure during generation. Our approach\ncombines sequential Monte Carlo, which estimates the posterior distribution by\nsampling from a proposal distribution, with a syntactic tagger that ensures\nthat each generated token aligns with the desired syntactic structure. Our\nexperiments with GPT2 and Llama3-8B models show that with an appropriate\nproposal distribution, we can improve syntactic accuracy, increasing the F1\nscore from $12.31$ (GPT2-large) and $35.33$ (Llama3-8B) to about $93$ in both\ncases without compromising the language model's fluency. These results\nunderscore both the complexity of syntactic control and the effectiveness of\nsampling algorithms, offering a promising approach for applications where\nprecise control over syntax is essential.", "published": "2025-06-08 14:01:34", "link": "http://arxiv.org/abs/2506.07154v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semantic-preserved Augmentation with Confidence-weighted Fine-tuning for Aspect Category Sentiment Analysis", "abstract": "Large language model (LLM) is an effective approach to addressing data\nscarcity in low-resource scenarios. Recent existing research designs\nhand-crafted prompts to guide LLM for data augmentation. We introduce a data\naugmentation strategy for the aspect category sentiment analysis (ACSA) task\nthat preserves the original sentence semantics and has linguistic diversity,\nspecifically by providing a structured prompt template for an LLM to generate\npredefined content. In addition, we employ a post-processing technique to\nfurther ensure semantic consistency between the generated sentence and the\noriginal sentence. The augmented data increases the semantic coverage of the\ntraining distribution, enabling the model better to understand the relationship\nbetween aspect categories and sentiment polarities, enhancing its inference\ncapabilities. Furthermore, we propose a confidence-weighted fine-tuning\nstrategy to encourage the model to generate more confident and accurate\nsentiment polarity predictions. Compared with powerful and recent works, our\nmethod consistently achieves the best performance on four benchmark datasets\nover all baselines.", "published": "2025-06-08 13:53:28", "link": "http://arxiv.org/abs/2506.07148v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompting Science Report 2: The Decreasing Value of Chain of Thought in Prompting", "abstract": "This is the second in a series of short reports that seek to help business,\neducation, and policy leaders understand the technical details of working with\nAI through rigorous testing. In this report, we investigate Chain-of-Thought\n(CoT) prompting, a technique that encourages a large language model (LLM) to\n\"think step by step\" (Wei et al., 2022). CoT is a widely adopted method for\nimproving reasoning tasks, however, our findings reveal a more nuanced picture\nof its effectiveness. We demonstrate two things:\n  - The effectiveness of Chain-of-Thought prompting can vary greatly depending\non the type of task and model. For non-reasoning models, CoT generally improves\naverage performance by a small amount, particularly if the model does not\ninherently engage in step-by-step processing by default. However, CoT can\nintroduce more variability in answers, sometimes triggering occasional errors\nin questions the model would otherwise get right. We also found that many\nrecent models perform some form of CoT reasoning even if not asked; for these\nmodels, a request to perform CoT had little impact. Performing CoT generally\nrequires far more tokens (increasing cost and time) than direct answers.\n  - For models designed with explicit reasoning capabilities, CoT prompting\noften results in only marginal, if any, gains in answer accuracy. However, it\nsignificantly increases the time and tokens needed to generate a response.", "published": "2025-06-08 13:41:25", "link": "http://arxiv.org/abs/2506.07142v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Compact Vision Tokens for Efficient Large Multimodal Models", "abstract": "Large multimodal models (LMMs) suffer significant computational challenges\ndue to the high cost of Large Language Models (LLMs) and the quadratic\ncomplexity of processing long vision token sequences. In this paper, we explore\nthe spatial redundancy among vision tokens and shorten the length of vision\ntoken sequences for inference acceleration. Specifically, we propose a Spatial\nToken Fusion (STF) method to learn compact vision tokens for short vision token\nsequence, where spatial-adjacent tokens are fused into one. Meanwhile,\nweight-frozen vision encoder can not well adapt to the demand of extensive\ndownstream vision-language tasks. To this end, we further introduce a\nMulti-Block Token Fusion (MBTF) module to supplement multi-granularity features\nfor the reduced token sequence. Overall, we combine STF and MBTF module to\nbalance token reduction and information preservation, thereby improving\ninference efficiency without sacrificing multimodal reasoning capabilities.\nExperimental results demonstrate that our method based on LLaVA-1.5 achieves\ncomparable or even superior performance to the baseline on 8 popular\nvision-language benchmarks with only $25\\%$ vision tokens of baseline. The\nsource code and trained weights are available at\nhttps://github.com/visresearch/LLaVA-STF.", "published": "2025-06-08 13:36:06", "link": "http://arxiv.org/abs/2506.07138v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models", "abstract": "Large language models (LLMs) have shown strong performance across natural\nlanguage reasoning tasks, yet their reasoning processes remain brittle and\ndifficult to interpret. Prompting techniques like Chain-of-Thought (CoT)\nenhance reliability by eliciting intermediate reasoning steps or aggregating\nmultiple outputs. However, they lack mechanisms for enforcing logical structure\nand assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a\nnovel framework that models reasoning as collaboration among three parallel\nagents, each simulating a distinct mode of inference: abductive, deductive, and\ninductive. Each agent produces a reasoning trace, which is structured into a\nformal reasoning graph. To evaluate consistency, we apply Bayesian belief\npropagation guided by natural language inference (NLI), assigning confidence\nscores to each step. The most coherent graph is selected to derive the final\nanswer. Experiments on symbolic (WebOfLies) and numerical (MultiArith)\nreasoning benchmarks show that ToTh consistently outperforms CoT,\nSelf-Consistency, and CoT-Decoding across multiple LLMs, while producing\ninterpretable and logically grounded reasoning chains. Our findings suggest a\npromising direction for building more robust and cognitively inspired LLM\nreasoning. The implementation is available at\nhttps://github.com/KurbanIntelligenceLab/theorem-of-thought.", "published": "2025-06-08 12:28:38", "link": "http://arxiv.org/abs/2506.07106v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Far Are We from Optimal Reasoning Efficiency?", "abstract": "Large Reasoning Models (LRMs) demonstrate remarkable problem-solving\ncapabilities through extended Chain-of-Thought (CoT) reasoning but often\nproduce excessively verbose and redundant reasoning traces. This inefficiency\nincurs high inference costs and limits practical deployment. While existing\nfine-tuning methods aim to improve reasoning efficiency, assessing their\nefficiency gains remains challenging due to inconsistent evaluations. In this\nwork, we introduce the reasoning efficiency frontiers, empirical upper bounds\nderived from fine-tuning base LRMs across diverse approaches and training\nconfigurations. Based on these frontiers, we propose the Reasoning Efficiency\nGap (REG), a unified metric quantifying deviations of any fine-tuned LRMs from\nthese frontiers. Systematic evaluation on challenging mathematical benchmarks\nreveals significant gaps in current methods: they either sacrifice accuracy for\nshort length or still remain inefficient under tight token budgets. To reduce\nthe efficiency gap, we propose REO-RL, a class of Reinforcement Learning\nalgorithms that minimizes REG by targeting a sparse set of token budgets.\nLeveraging numerical integration over strategically selected budgets, REO-RL\napproximates the full efficiency objective with low error using a small set of\ntoken budgets. Through systematic benchmarking, we demonstrate that our\nefficiency metric, REG, effectively captures the accuracy-length trade-off,\nwith low-REG methods reducing length while maintaining accuracy. Our approach,\nREO-RL, consistently reduces REG by >=50 across all evaluated LRMs and matching\nQwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy\nloss. Ablation studies confirm the effectiveness of our exponential token\nbudget strategy. Finally, our findings highlight that fine-tuning LRMs to\nperfectly align with the efficiency frontiers remains an open challenge.", "published": "2025-06-08 12:18:50", "link": "http://arxiv.org/abs/2506.07104v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Representation Decomposition for Learning Similarity and Contrastness Across Modalities for Affective Computing", "abstract": "Multi-modal affective computing aims to automatically recognize and interpret\nhuman attitudes from diverse data sources such as images and text, thereby\nenhancing human-computer interaction and emotion understanding. Existing\napproaches typically rely on unimodal analysis or straightforward fusion of\ncross-modal information that fail to capture complex and conflicting evidence\npresented across different modalities. In this paper, we propose a novel\nLLM-based approach for affective computing that explicitly deconstructs visual\nand textual representations into shared (modality-invariant) and\nmodality-specific components. Specifically, our approach firstly encodes and\naligns input modalities using pre-trained multi-modal encoders, then employs a\nrepresentation decomposition framework to separate common emotional content\nfrom unique cues, and finally integrates these decomposed signals via an\nattention mechanism to form a dynamic soft prompt for a multi-modal LLM.\nExtensive experiments on three representative tasks for affective computing,\nnamely, multi-modal aspect-based sentiment analysis, multi-modal emotion\nanalysis, and hateful meme detection, demonstrate the effectiveness of our\napproach, which consistently outperforms strong baselines and state-of-the-art\nmodels.", "published": "2025-06-08 11:15:57", "link": "http://arxiv.org/abs/2506.07086v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Com$^2$: A Causal-Guided Benchmark for Exploring Complex Commonsense Reasoning in Large Language Models", "abstract": "Large language models (LLMs) have mastered abundant simple and explicit\ncommonsense knowledge through pre-training, enabling them to achieve human-like\nperformance in simple commonsense reasoning. Nevertheless, LLMs struggle to\nreason with complex and implicit commonsense knowledge that is derived from\nsimple ones (such as understanding the long-term effects of certain events), an\naspect humans tend to focus on more. Existing works focus on complex tasks like\nmath and code, while complex commonsense reasoning remains underexplored due to\nits uncertainty and lack of structure. To fill this gap and align with\nreal-world concerns, we propose a benchmark Com$^2$ focusing on complex\ncommonsense reasoning. We first incorporate causal event graphs to serve as\nstructured complex commonsense. Then we adopt causal theory~(e.g.,\nintervention) to modify the causal event graphs and obtain different scenarios\nthat meet human concerns. Finally, an LLM is employed to synthesize examples\nwith slow thinking, which is guided by the logical relationships in the\nmodified causal graphs. Furthermore, we use detective stories to construct a\nmore challenging subset. Experiments show that LLMs struggle in reasoning depth\nand breadth, while post-training and slow thinking can alleviate this. The code\nand data are available at https://github.com/Waste-Wood/Com2.", "published": "2025-06-08 09:53:08", "link": "http://arxiv.org/abs/2506.07064v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs", "abstract": "The rapid advancement of image generation technologies intensifies the demand\nfor interpretable and robust detection methods. Although existing approaches\noften attain high accuracy, they typically operate as black boxes without\nproviding human-understandable justifications. Multi-modal Large Language\nModels (MLLMs), while not originally intended for forgery detection, exhibit\nstrong analytical and reasoning capabilities. When properly fine-tuned, they\ncan effectively identify AI-generated images and offer meaningful explanations.\nHowever, existing MLLMs still struggle with hallucination and often fail to\nalign their visual interpretations with actual image content and human\nreasoning. To bridge this gap, we construct a dataset of AI-generated images\nannotated with bounding boxes and descriptive captions that highlight synthesis\nartifacts, establishing a foundation for human-aligned visual-textual grounded\nreasoning. We then finetune MLLMs through a multi-stage optimization strategy\nthat progressively balances the objectives of accurate detection, visual\nlocalization, and coherent textual explanation. The resulting model achieves\nsuperior performance in both detecting AI-generated images and localizing\nvisual flaws, significantly outperforming baseline methods.", "published": "2025-06-08 08:47:44", "link": "http://arxiv.org/abs/2506.07045v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CNFs and DNFs with Exactly $k$ Solutions", "abstract": "Model counting is a fundamental problem that consists of determining the\nnumber of satisfying assignments for a given Boolean formula. The weighted\nvariant, which computes the weighted sum of satisfying assignments, has\nextensive applications in probabilistic reasoning, network reliability,\nstatistical physics, and formal verification. A common approach for solving\nweighted model counting is to reduce it to unweighted model counting, which\nraises an important question: {\\em What is the minimum number of terms (or\nclauses) required to construct a DNF (or CNF) formula with exactly $k$\nsatisfying assignments?}\n  In this paper, we establish both upper and lower bounds on this question. We\nprove that for any natural number $k$, one can construct a monotone DNF formula\nwith exactly $k$ satisfying assignments using at most $O(\\sqrt{\\log k}\\log\\log\nk)$ terms. This construction represents the first $o(\\log k)$ upper bound for\nthis problem. We complement this result by showing that there exist infinitely\nmany values of $k$ for which any DNF or CNF representation requires at least\n$\\Omega(\\log\\log k)$ terms or clauses. These results have significant\nimplications for the efficiency of model counting algorithms based on formula\ntransformations.", "published": "2025-06-08 20:11:20", "link": "http://arxiv.org/abs/2506.07268v1", "categories": ["cs.DM", "cs.DS", "cs.LO", "math.CO", "math.LO"], "primary_category": "cs.DM"}
{"title": "Rectangular Duals on the Cylinder and the Torus", "abstract": "A rectangular dual of a plane graph $G$ is a contact representation of $G$ by\ninterior-disjoint rectangles such that (i) no four rectangles share a point,\nand (ii) the union of all rectangles is a rectangle. In this paper, we study\nrectangular duals of graphs that are embedded in surfaces other than the plane.\nIn particular, we fully characterize when a graph embedded on a cylinder admits\na cylindrical rectangular dual. For graphs embedded on the flat torus, we can\ntest whether the graph has a toroidal rectangular dual if we are additionally\ngiven a \\textit{regular edge labeling}, i.e. a combinatorial description of\nrectangle adjacencies. Furthermore we can test whether there exists a toroidal\nrectangular dual that respects the embedding and that resides on a flat torus\nfor which the sides are axis-aligned. Testing and constructing the rectangular\ndual, if applicable, can be done efficiently.", "published": "2025-06-08 14:35:51", "link": "http://arxiv.org/abs/2506.07170v1", "categories": ["cs.CG", "cs.DM", "68R10, 05C62"], "primary_category": "cs.CG"}
{"title": "HotelMatch-LLM: Joint Multi-Task Training of Small and Large Language Models for Efficient Multimodal Hotel Retrieval", "abstract": "We present HotelMatch-LLM, a multimodal dense retrieval model for the travel\ndomain that enables natural language property search, addressing the\nlimitations of traditional travel search engines which require users to start\nwith a destination and editing search parameters. HotelMatch-LLM features three\nkey innovations: (1) Domain-specific multi-task optimization with three novel\nretrieval, visual, and language modeling objectives; (2) Asymmetrical dense\nretrieval architecture combining a small language model (SLM) for efficient\nonline query processing and a large language model (LLM) for embedding hotel\ndata; and (3) Extensive image processing to handle all property image\ngalleries. Experiments on four diverse test sets show HotelMatch-LLM\nsignificantly outperforms state-of-the-art models, including VISTA and MARVEL.\nSpecifically, on the test set -- main query type -- we achieve 0.681 for\nHotelMatch-LLM compared to 0.603 for the most effective baseline, MARVEL. Our\nanalysis highlights the impact of our multi-task optimization, the\ngeneralizability of HotelMatch-LLM across LLM architectures, and its\nscalability for processing large image galleries.", "published": "2025-06-08 21:39:08", "link": "http://arxiv.org/abs/2506.07296v1", "categories": ["cs.IR", "cs.AI", "cs.CV"], "primary_category": "cs.IR"}
{"title": "Research Knowledge Graphs: the Shifting Paradigm of Scholarly Information Representation", "abstract": "Sharing and reusing research artifacts, such as datasets, publications, or\nmethods is a fundamental part of scientific activity, where heterogeneity of\nresources and metadata and the common practice of capturing information in\nunstructured publications pose crucial challenges. Reproducibility of research\nand finding state-of-the-art methods or data have become increasingly\nchallenging. In this context, the concept of Research Knowledge Graphs (RKGs)\nhas emerged, aiming at providing an easy to use and machine-actionable\nrepresentation of research artifacts and their relations. That is facilitated\nthrough the use of established principles for data representation, the\nconsistent adoption of globally unique persistent identifiers and the reuse and\nlinking of vocabularies and data. This paper provides the first\nconceptualisation of the RKG vision, a categorisation of in-use RKGs together\nwith a description of RKG building blocks and principles. We also survey\nreal-world RKG implementations differing with respect to scale, schema, data,\nused vocabulary, and reliability of the contained data. We also characterise\ndifferent RKG construction methodologies and provide a forward-looking\nperspective on the diverse applications, opportunities, and challenges\nassociated with the RKG vision.", "published": "2025-06-08 21:10:30", "link": "http://arxiv.org/abs/2506.07285v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "RADAR: Recall Augmentation through Deferred Asynchronous Retrieval", "abstract": "Modern large-scale recommender systems employ multi-stage ranking funnel\n(Retrieval, Pre-ranking, Ranking) to balance engagement and computational\nconstraints (latency, CPU). However, the initial retrieval stage, often relying\non efficient but less precise methods like K-Nearest Neighbors (KNN), struggles\nto effectively surface the most engaging items from billion-scale catalogs,\nparticularly distinguishing highly relevant and engaging candidates from merely\nrelevant ones. We introduce Recall Augmentation through Deferred Asynchronous\nRetrieval (RADAR), a novel framework that leverages asynchronous, offline\ncomputation to pre-rank a significantly larger candidate set for users using\nthe full complexity ranking model. These top-ranked items are stored and\nutilized as a high-quality retrieval source during online inference, bypassing\nonline retrieval and pre-ranking stages for these candidates. We demonstrate\nthrough offline experiments that RADAR significantly boosts recall (2X\nRecall@200 vs DNN retrieval baseline) by effectively combining a larger\nretrieved candidate set with a more powerful ranking model. Online A/B tests\nconfirm a +0.8% lift in topline engagement metrics, validating RADAR as a\npractical and effective method to improve recommendation quality under strict\nonline serving constraints.", "published": "2025-06-08 19:21:46", "link": "http://arxiv.org/abs/2506.07261v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "From Swath to Full-Disc: Advancing Precipitation Retrieval with Multimodal Knowledge Expansion", "abstract": "Accurate near-real-time precipitation retrieval has been enhanced by\nsatellite-based technologies. However, infrared-based algorithms have low\naccuracy due to weak relations with surface precipitation, whereas passive\nmicrowave and radar-based methods are more accurate but limited in range. This\nchallenge motivates the Precipitation Retrieval Expansion (PRE) task, which\naims to enable accurate, infrared-based full-disc precipitation retrievals\nbeyond the scanning swath. We introduce Multimodal Knowledge Expansion, a\ntwo-stage pipeline with the proposed PRE-Net model. In the Swath-Distilling\nstage, PRE-Net transfers knowledge from a multimodal data integration model to\nan infrared-based model within the scanning swath via Coordinated Masking and\nWavelet Enhancement (CoMWE). In the Full-Disc Adaptation stage, Self-MaskTune\nrefines predictions across the full disc by balancing multimodal and full-disc\ninfrared knowledge. Experiments on the introduced PRE benchmark demonstrate\nthat PRE-Net significantly advanced precipitation retrieval performance,\noutperforming leading products like PERSIANN-CCS, PDIR, and IMERG. The code\nwill be available at https://github.com/Zjut-MultimediaPlus/PRE-Net.", "published": "2025-06-08 09:15:46", "link": "http://arxiv.org/abs/2506.07050v1", "categories": ["cs.CV", "cs.IR", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Correcting for Position Bias in Learning to Rank: A Control Function Approach", "abstract": "Implicit feedback data, such as user clicks, is commonly used in\nlearning-to-rank (LTR) systems because it is easy to collect and it often\nreflects user preferences. However, this data is prone to various biases, and\ntraining an LTR system directly on biased data can result in suboptimal ranking\nperformance. One of the most prominent and well-studied biases in implicit\nfeedback data is position bias, which occurs because users are more likely to\ninteract with higher-ranked documents regardless of their true relevance. In\nthis paper, we propose a novel control function-based method that accounts for\nposition bias in a two-stage process. The first stage uses exogenous variation\nfrom the residuals of the ranking process to correct for position bias in the\nsecond stage click equation. Unlike previous position bias correction methods,\nour method does not require knowledge of the click or propensity model and\nallows for nonlinearity in the underlying ranking model. Moreover, our method\nis general and allows for debiasing any state-of-the-art ranking algorithm by\nplugging it into the second stage. We also introduce a technique to debias\nvalidation clicks for hyperparameter tuning to select the optimal model in the\nabsence of unbiased validation data. Experimental results demonstrate that our\nmethod outperforms state-of-the-art approaches in correcting for position bias.", "published": "2025-06-08 04:10:14", "link": "http://arxiv.org/abs/2506.06989v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Energy Efficiency Maximization for Movable Antenna Communication Systems", "abstract": "This paper investigates energy efficiency maximization for movable antenna\n(MA)-aided multi-user uplink communication systems by considering the time\ndelay and energy consumption incurred by practical antenna movement. We first\nexamine the special case with a single user and propose an optimization\nalgorithm based on the one-dimensional (1D) exhaustive search to maximize the\nuser's energy efficiency. Moreover, we derive an upper bound on the energy\nefficiency and analyze the conditions required to achieve this performance\nbound under different numbers of channel paths. Then, for the general\nmulti-user scenario, we propose an iterative algorithm to fairly maximize the\nminimum energy efficiency among all users. Simulation results demonstrate the\neffectiveness of the proposed scheme in improving energy efficiency compared to\nexisting MA schemes that do not account for movement-related costs, as well as\nthe conventional fixed-position antenna (FPA) scheme. In addition, the results\nshow the robustness of the proposed scheme to imperfect channel state\ninformation (CSI) and provide valuable insights for practical system\ndeployment.", "published": "2025-06-08 13:15:34", "link": "http://arxiv.org/abs/2506.07129v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Passive Detection in Multi-Static ISAC Systems: Performance Analysis and Joint Beamforming Optimization", "abstract": "This paper investigates the passive detection problem in multi-static\nintegrated sensing and communication (ISAC) systems, where multiple sensing\nreceivers (SRs) jointly detect a target using random unknown communication\nsignals transmitted by a collaborative base station. Unlike traditional active\ndetection, the considered passive detection does not require complete prior\nknowledge of the transmitted communication signals at each SR. First, we derive\na generalized likelihood ratio test detector and conduct an asymptotic analysis\nof the detection statistic under the large-sample regime. We examine how the\nsignal-to-noise ratios (SNRs) of the target paths and direct paths influence\nthe detection performance. Then, we propose two joint transmit beamforming\ndesigns based on the analyses. In the first design, the asymptotic detection\nprobability is maximized while satisfying the signal-to-interference-plus-noise\nratio requirement for each communication user under the total transmit power\nconstraint. Given the non-convex nature of the problem, we develop an\nalternating optimization algorithm based on the quadratic transform and\nsemi-definite relaxation. The second design adopts a heuristic approach that\naims to maximize the target energy, subject to a minimum SNR threshold on the\ndirect path, and offers lower computational complexity. Numerical results\nvalidate the asymptotic analysis and demonstrate the superiority of the\nproposed beamforming designs in balancing passive detection performance and\ncommunication quality. This work highlights the promise of target detection\nusing unknown communication data signals in multi-static ISAC systems.", "published": "2025-06-08 06:54:57", "link": "http://arxiv.org/abs/2506.07019v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Very Large-scale Multi-Robot Task Allocation in Challenging Environments via Robot Redistribution", "abstract": "We consider the Multi-Robot Task Allocation (MRTA) problem that aims to\noptimize an assignment of multiple robots to multiple tasks in challenging\nenvironments which are with densely populated obstacles and narrow passages. In\nsuch environments, conventional methods optimizing the sum-of-cost are often\nineffective because the conflicts between robots incur additional costs (e.g.,\ncollision avoidance, waiting). Also, an allocation that does not incorporate\nthe actual robot paths could cause deadlocks, which significantly degrade the\ncollective performance of the robots.\n  We propose a scalable MRTA method that considers the paths of the robots to\navoid collisions and deadlocks which result in a fast completion of all tasks\n(i.e., minimizing the \\textit{makespan}). To incorporate robot paths into task\nallocation, the proposed method constructs a roadmap using a Generalized\nVoronoi Diagram. The method partitions the roadmap into several components to\nknow how to redistribute robots to achieve all tasks with less conflicts\nbetween the robots. In the redistribution process, robots are transferred to\ntheir final destinations according to a push-pop mechanism with the first-in\nfirst-out principle. From the extensive experiments, we show that our method\ncan handle instances with hundreds of robots in dense clutter while competitors\nare unable to compute a solution within a time limit.", "published": "2025-06-08 21:34:31", "link": "http://arxiv.org/abs/2506.07293v1", "categories": ["cs.RO", "cs.MA"], "primary_category": "cs.RO"}
{"title": "Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied Environments", "abstract": "Large language models (LLMs) possess extensive knowledge bases and strong\nreasoning capabilities, making them promising tools for complex, multi-agent\nplanning in embodied environments. However, despite LLMs' advanced abilities\nand the sophisticated modular design of agentic methods, existing LLM-based\nplanning algorithms remain limited by weak adaptation capabilities to\nmulti-agent embodied scenarios. We address this limitation by introducing a\nframework that enables LLM agents to learn and evolve both before and during\ntest time, equipping them with environment-relevant knowledge for better\nplanning and enhanced communication for improved cooperation. Inspired by\ncentralized training with decentralized execution in multi-agent reinforcement\nlearning, we propose a \\textit{Learn as Individuals, Evolve as a Team (LIET)}\nparadigm for multi-agent LLMs adaptation. At the individual level, LLM agents\nlearn a local utility function from exploratory datasets to better comprehend\nthe embodied environment, which is then queried during test time to support\ninformed decision-making. At the team level, LLM agents collaboratively and\niteratively maintain and update a shared cooperation knowledge list based on\nnew experiences, using it to guide more effective communication. By combining\nindividual learning with team evolution, LIET enables comprehensive and\nflexible adaptation for LLM agents. Our experiments on Communicative\nWatch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate\nthat LIET, instantiated with both LLaMA and GPT-4o, outperforms existing\nbaselines and exhibits strong cooperative planning abilities.", "published": "2025-06-08 17:32:03", "link": "http://arxiv.org/abs/2506.07232v1", "categories": ["cs.MA", "cs.AI", "cs.LG"], "primary_category": "cs.MA"}
{"title": "Position: Simulating Society Requires Simulating Thought", "abstract": "Simulating society with large language models (LLMs), we argue, requires more\nthan generating plausible behavior -- it demands cognitively grounded reasoning\nthat is structured, revisable, and traceable. LLM-based agents are increasingly\nused to emulate individual and group behavior -- primarily through prompting\nand supervised fine-tuning. Yet they often lack internal coherence, causal\nreasoning, and belief traceability -- making them unreliable for analyzing how\npeople reason, deliberate, or respond to interventions.\n  To address this, we present a conceptual modeling paradigm, Generative Minds\n(GenMinds), which draws from cognitive science to support structured belief\nrepresentations in generative agents. To evaluate such agents, we introduce the\nRECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess\nreasoning fidelity via causal traceability, demographic grounding, and\nintervention consistency. These contributions advance a broader shift: from\nsurface-level mimicry to generative agents that simulate thought -- not just\nlanguage -- for social simulations.", "published": "2025-06-08 00:59:02", "link": "http://arxiv.org/abs/2506.06958v1", "categories": ["cs.CY", "cs.AI", "cs.MA"], "primary_category": "cs.CY"}
{"title": "Highly efficient linear energy stable methods for preserving the original energy dissipation law of the incompressible Navier-Stokes equation", "abstract": "In this paper, we introduce a comprehensive computational framework to\nconstruct highly efficient linear energy stable methods for the incompressible\nNavier-Stokes equation, which preserve the original energy dissipation law. By\nmultiplying the convection term by an identity-one term and incorporating a\nzero stabilization term, we recast the original model as a strongly equivalent\nsystem, while ensuring the retention of the original energy dissipation law.\nSuch nonlinear system is then discretized in time based on the Crank-Nicolson\nschemes and the backward differentiation formulas, resulting in highly\nefficient time-discrete schemes. The proposed schemes are designed to preserve\nthe original energy dissipation law while requiring only the solutions of three\nlinear Stokes systems and a $2\\times 2$ linear system at each time step. The\nfinite difference approximation on a staggered grid is employed for the\ntime-discrete systems to derive fully discrete energy stable schemes, which are\nproven to preserve the original energy dissipation law and be uniquely\nsolvable. We present the efficient implementation of these methods. Various\nnumerical experiments are carried out to verify the accuracy, efficacy, and\nadvantageous performance of our newly developed methods.", "published": "2025-06-08 13:39:43", "link": "http://arxiv.org/abs/2506.07141v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "New highly efficient and accurate numerical scheme for the Cahn-Hilliard-Brinkman system", "abstract": "In this paper, based on a generalized scalar auxiliary variable approach with\nrelaxation (R-GSAV), we construct a class of high-order backward\ndifferentiation formula (BDF) schemes with variable time steps for the\nCahn-Hilliard-Brinkman(CHB) system. In theory, it is strictly proved that the\ndesigned schemes are unconditionally energy-stable. With the delicate treatment\nof adaptive strategies, we propose several adaptive time-step algorithms to\nenhance the robustness of the schemes. More importantly, a novel hybrid-order\nadaptive time steps algorithm performs outstanding for the coupled system. The\nhybrid-order algorithm inherits the advantages of some traditional high-order\nBDF adaptive strategies. A comprehensive comparison with some adaptive\ntime-step algorithms is given, and the advantages of the new adaptive time-step\nalgorithms are emphasized. Finally, the effectiveness and accuracy of the new\nmethods are validated through a series of numerical experiments.", "published": "2025-06-08 13:15:01", "link": "http://arxiv.org/abs/2506.07128v1", "categories": ["math.NA", "cs.NA", "physics.comp-ph", "74A50, 65M12, 65M70"], "primary_category": "math.NA"}
{"title": "Computational homogenization of parabolic equations with memory effects for a periodic heterogeneous medium", "abstract": "In homogenization theory, mathematical models at the macro level are\nconstructed based on the solution of auxiliary cell problems at the micro level\nwithin a single periodicity cell. These problems are formulated using\nasymptotic expansions of the solution with respect to a small parameter, which\nrepresents the characteristic size of spatial heterogeneity. When studying\ndiffusion equations with contrasting coefficients, special attention is given\nto nonlocal models with weakly conducting inclusions. In this case, macro-level\nprocesses are described by integro-differential equations, where the difference\nkernel is determined by the solution of a nonstationary cell problem. The main\ncontribution of this work is the development of a computational framework for\nthe homogenization of nonstationary processes, accounting for memory effects.\nThe effective diffusion tensor is computed using a standard numerical procedure\nbased on finite element discretization in space. The memory kernel is\napproximated by a sum of exponentials obtained from solving a partial spectral\nproblem on the periodicity cell. The nonlocal macro-level problem is\ntransformed into a local one, where memory effects are incorporated through the\nsolution of auxiliary nonstationary problems. Standard two-level time\ndiscretization schemes are employed, and unconditional stability of the\ndiscrete solutions is proved in appropriate norms. Key aspects of the proposed\ncomputational homogenization technique are illustrated by solving a\ntwo-dimensional model problem.", "published": "2025-06-08 12:43:59", "link": "http://arxiv.org/abs/2506.07111v1", "categories": ["math.NA", "cs.NA", "35B27, 45K05, 65M12, 65M60"], "primary_category": "math.NA"}
{"title": "CIR bridge for modeling of fish migration on sub-hourly scale", "abstract": "Bridges, which are stochastic processes with pinned initial and terminal\nconditions, have recently been applied to solve various problems. We show that\na bridge based on the Cox-Ingersoll-Ross process, called a CIR bridge in this\npaper, reasonably models the intraday number of migrating fish at an\nobservation point in a river. The studied fish migrates between sunrise and\nsunset each day, which are considered the initial and terminal times,\nrespectively. The CIR bridge is well-defined as a unique pathwise continuous\nsolution to a stochastic differential equation with unbounded drift and\ndiffusion coefficients and potentially represents the on-off intermittency of\nthe fish count data. Our bridge is theoretically novel in that it admits\nclosed-form time-dependent averages and variances, with which the model\nparameters can be identified efficiently, and is computable by a\nrecently-developed one-step numerical method. The CIR bridge is applied to the\nsub-hourly migration data of the diadromous fish Plecoglossus altivelis\naltivelis in the Nagara River, Japan, from February to June.", "published": "2025-06-08 11:35:34", "link": "http://arxiv.org/abs/2506.07094v1", "categories": ["math.PR", "cs.NA", "math.NA"], "primary_category": "math.PR"}
{"title": "The PML method for calculating the propagating modes of electromagnetic wave in periodic structures", "abstract": "When the electromagnetic wave is incident on the periodic structures, in\naddition to the scattering field, some propagating modes that are traveling in\nthe periodic medium could be generated. In the present paper, we study the\ncalculation of propagating modes. We formulate the problem as a nonlinear\neigenvalue problem in an unbounded periodic domain. Then we use perfectly\nmatched layers to truncate the unbounded domain, recast the problem to a\nquadratic eigenvalue problem, and prove the approximation property of the\ntruncation. Finally, we formulate the quadratic eigenvalue problem to a general\neigenvalue problem, use the finite element method to discrete the truncation\nproblem, and show numerical examples to verify theoretical results.", "published": "2025-06-08 11:08:16", "link": "http://arxiv.org/abs/2506.07084v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "A novel efficient structure-preserving exponential integrator for Hamiltonian systems", "abstract": "We propose a linearly implicit structure-preserving numerical method for\nsemilinear Hamiltonian systems with polynomial nonlinearities, combining\nKahan's method and exponential integrator. This approach efficiently balances\ncomputational cost, accuracy and the preservation of key geometric properties,\nincluding symmetry and near-preservation of energy. By requiring only the\nsolution of a single linear system per time step, the proposed method offers\nsignificant computational advantages while comparing with the state-of-the-art\nsymmetric energy-preserving exponential integrators. The stability, efficiency\nand long-term accuracy of the method are demonstrated through numerical\nexperiments on systems such as the Henon-Heiles system, the Fermi-Pasta-Ulam\nsystem and the two-dimensional Zakharov-Kuznestov equation.", "published": "2025-06-08 10:25:42", "link": "http://arxiv.org/abs/2506.07072v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Deep regularization networks for inverse problems with noisy operators", "abstract": "A supervised learning approach is proposed for regularization of large\ninverse problems where the main operator is built from noisy data. This is\ngermane to superresolution imaging via the sampling indicators of the inverse\nscattering theory. We aim to accelerate the spatiotemporal regularization\nprocess for this class of inverse problems to enable real-time imaging. In this\napproach, a neural operator maps each pattern on the right-hand side of the\nscattering equation to its affiliated regularization parameter. The network is\ntrained in two steps which entails: (1) training on low-resolution\nregularization maps furnished by the Morozov discrepancy principle with\nnonoptimal thresholds, and (2) optimizing network predictions through\nminimization of the Tikhonov loss function regulated by the validation loss.\nStep 2 allows for tailoring of the approximate maps of Step 1 toward\nconstruction of higher quality images. This approach enables direct learning\nfrom test data and dispenses with the need for a-priori knowledge of the\noptimal regularization maps. The network, trained on low-resolution data,\nquickly generates dense regularization maps for high-resolution imaging. We\nhighlight the importance of the training loss function on the network's\ngeneralizability. In particular, we demonstrate that networks informed by the\nlogic of discrepancy principle lead to images of higher contrast. In this case,\nthe training process involves many-objective optimization. We propose a new\nmethod to adaptively select the appropriate loss weights during training\nwithout requiring an additional optimization process. The proposed approach is\nsynthetically examined for imaging damage evolution in an elastic plate. The\nresults indicate that the discrepancy-informed regularization networks not only\naccelerate the imaging process, but also remarkably enhance the image quality\nin complex environments.", "published": "2025-06-08 06:19:18", "link": "http://arxiv.org/abs/2506.07008v1", "categories": ["math.NA", "cs.AI", "cs.NA", "eess.SP"], "primary_category": "math.NA"}
{"title": "Uncertainty-Aware Strategies: A Model-Agnostic Framework for Robust Financial Optimization through Subsampling", "abstract": "This paper addresses the challenge of model uncertainty in quantitative\nfinance, where decisions in portfolio allocation, derivative pricing, and risk\nmanagement rely on estimating stochastic models from limited data. In practice,\nthe unavailability of the true probability measure forces reliance on an\nempirical approximation, and even small misestimations can lead to significant\ndeviations in decision quality. Building on the framework of Klibanoff et al.\n(2005), we enhance the conventional objective - whether this is expected\nutility in an investing context or a hedging metric - by superimposing an outer\n\"uncertainty measure\", motivated by traditional monetary risk measures, on the\nspace of models. In scenarios where a natural model distribution is lacking or\nBayesian methods are impractical, we propose an ad hoc subsampling strategy,\nanalogous to bootstrapping in statistical finance and related to mini-batch\nsampling in deep learning, to approximate model uncertainty. To address the\nquadratic memory demands of naive implementations, we also present an adapted\nstochastic gradient descent algorithm that enables efficient parallelization.\nThrough analytical, simulated, and empirical studies - including multi-period,\nreal data and high-dimensional examples - we demonstrate that uncertainty\nmeasures outperform traditional mixture of measures strategies and our\nmodel-agnostic subsampling-based approach not only enhances robustness against\nmodel risk but also achieves performance comparable to more elaborate Bayesian\nmethods.", "published": "2025-06-08 21:55:00", "link": "http://arxiv.org/abs/2506.07299v1", "categories": ["q-fin.CP", "cs.LG", "q-fin.MF", "q-fin.RM"], "primary_category": "q-fin.CP"}
{"title": "From Axioms to Algorithms: Mechanized Proofs of the vNM Utility Theorem", "abstract": "This paper presents a comprehensive formalization of the von\nNeumann-Morgenstern (vNM) expected utility theorem using the Lean 4 interactive\ntheorem prover. We implement the classical axioms of preference-completeness,\ntransitivity, continuity, and independence-enabling machine-verified proofs of\nboth the existence and uniqueness of utility representations. Our formalization\ncaptures the mathematical structure of preference relations over lotteries,\nverifying that preferences satisfying the vNM axioms can be represented by\nexpected utility maximization.\n  Our contributions include a granular implementation of the independence\naxiom, formally verified proofs of fundamental claims about mixture lotteries,\nconstructive demonstrations of utility existence, and computational experiments\nvalidating the results. We prove equivalence to classical presentations while\noffering greater precision at decision boundaries.\n  This formalization provides a rigorous foundation for applications in\neconomic modeling, AI alignment, and management decision systems, bridging the\ngap between theoretical decision theory and computational implementation.", "published": "2025-06-08 10:09:54", "link": "http://arxiv.org/abs/2506.07066v1", "categories": ["econ.TH", "cs.AI", "q-fin.CP"], "primary_category": "econ.TH"}
{"title": "PASS: Private Attributes Protection with Stochastic Data Substitution", "abstract": "The growing Machine Learning (ML) services require extensive collections of\nuser data, which may inadvertently include people's private information\nirrelevant to the services. Various studies have been proposed to protect\nprivate attributes by removing them from the data while maintaining the\nutilities of the data for downstream tasks. Nevertheless, as we theoretically\nand empirically show in the paper, these methods reveal severe vulnerability\nbecause of a common weakness rooted in their adversarial training based\nstrategies. To overcome this limitation, we propose a novel approach, PASS,\ndesigned to stochastically substitute the original sample with another one\naccording to certain probabilities, which is trained with a novel loss function\nsoundly derived from information-theoretic objective defined for\nutility-preserving private attributes protection. The comprehensive evaluation\nof PASS on various datasets of different modalities, including facial images,\nhuman activity sensory signals, and voice recording datasets, substantiates\nPASS's effectiveness and generalizability.", "published": "2025-06-08 22:48:07", "link": "http://arxiv.org/abs/2506.07308v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "ALINE: Joint Amortization for Bayesian Inference and Active Data Acquisition", "abstract": "Many critical applications, from autonomous scientific discovery to\npersonalized medicine, demand systems that can both strategically acquire the\nmost informative data and instantaneously perform inference based upon it.\nWhile amortized methods for Bayesian inference and experimental design offer\npart of the solution, neither approach is optimal in the most general and\nchallenging task, where new data needs to be collected for instant inference.\nTo tackle this issue, we introduce the Amortized Active Learning and Inference\nEngine (ALINE), a unified framework for amortized Bayesian inference and active\ndata acquisition. ALINE leverages a transformer architecture trained via\nreinforcement learning with a reward based on self-estimated information gain\nprovided by its own integrated inference component. This allows it to\nstrategically query informative data points while simultaneously refining its\npredictions. Moreover, ALINE can selectively direct its querying strategy\ntowards specific subsets of model parameters or designated predictive tasks,\noptimizing for posterior estimation, data prediction, or a mixture thereof.\nEmpirical results on regression-based active learning, classical Bayesian\nexperimental design benchmarks, and a psychometric model with selectively\ntargeted parameters demonstrate that ALINE delivers both instant and accurate\ninference along with efficient selection of informative points.", "published": "2025-06-08 19:15:34", "link": "http://arxiv.org/abs/2506.07259v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Strongly Consistent Community Detection in Popularity Adjusted Block Models", "abstract": "The Popularity Adjusted Block Model (PABM) provides a flexible framework for\ncommunity detection in network data by allowing heterogeneous node popularity\nacross communities. However, this flexibility increases model complexity and\nraises key unresolved challenges, particularly in effectively adapting spectral\nclustering techniques and efficiently achieving strong consistency in label\nrecovery. To address these challenges, we first propose the Thresholded Cosine\nSpectral Clustering (TCSC) algorithm and establish its weak consistency under\nthe PABM. We then introduce the one-step Refined TCSC algorithm and prove that\nit achieves strong consistency under the PABM, correctly recovering all\ncommunity labels with high probability. We further show that the two-step\nRefined TCSC accelerates clustering error convergence, especially with small\nsample sizes. Additionally, we propose a data-driven approach for selecting the\nnumber of communities, which outperforms existing methods under the PABM. The\neffectiveness and robustness of our methods are validated through extensive\nsimulations and real-world applications.", "published": "2025-06-08 17:17:41", "link": "http://arxiv.org/abs/2506.07224v1", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "primary_category": "stat.ME"}
{"title": "Quantile-Optimal Policy Learning under Unmeasured Confounding", "abstract": "We study quantile-optimal policy learning where the goal is to find a policy\nwhose reward distribution has the largest $\\alpha$-quantile for some $\\alpha\n\\in (0, 1)$. We focus on the offline setting whose generating process involves\nunobserved confounders. Such a problem suffers from three main challenges: (i)\nnonlinearity of the quantile objective as a functional of the reward\ndistribution, (ii) unobserved confounding issue, and (iii) insufficient\ncoverage of the offline dataset. To address these challenges, we propose a\nsuite of causal-assisted policy learning methods that provably enjoy strong\ntheoretical guarantees under mild conditions. In particular, to address (i) and\n(ii), using causal inference tools such as instrumental variables and negative\ncontrols, we propose to estimate the quantile objectives by solving nonlinear\nfunctional integral equations. Then we adopt a minimax estimation approach with\nnonparametric models to solve these integral equations, and propose to\nconstruct conservative policy estimates that address (iii). The final policy is\nthe one that maximizes these pessimistic estimates. In addition, we propose a\nnovel regularized policy learning method that is more amenable to computation.\nFinally, we prove that the policies learned by these methods are\n$\\tilde{\\mathscr{O}}(n^{-1/2})$ quantile-optimal under a mild coverage\nassumption on the offline dataset. Here, $\\tilde{\\mathscr{O}}(\\cdot)$ omits\npoly-logarithmic factors. To the best of our knowledge, we propose the first\nsample-efficient policy learning algorithms for estimating the quantile-optimal\npolicy when there exist unmeasured confounding.", "published": "2025-06-08 13:37:38", "link": "http://arxiv.org/abs/2506.07140v1", "categories": ["stat.ML", "cs.LG", "econ.EM"], "primary_category": "stat.ML"}
{"title": "Pointwise confidence estimation in the non-linear $\\ell^2$-regularized least squares", "abstract": "We consider a high-probability non-asymptotic confidence estimation in the\n$\\ell^2$-regularized non-linear least-squares setting with fixed design. In\nparticular, we study confidence estimation for local minimizers of the\nregularized training loss. We show a pointwise confidence bound, meaning that\nit holds for the prediction on any given fixed test input $x$. Importantly, the\nproposed confidence bound scales with similarity of the test input to the\ntraining data in the implicit feature space of the predictor (for instance,\nbecoming very large when the test input lies far outside of the training data).\nThis desirable last feature is captured by the weighted norm involving the\ninverse-Hessian matrix of the objective function, which is a generalized\nversion of its counterpart in the linear setting, $x^{\\top} \\text{Cov}^{-1} x$.\nOur generalized result can be regarded as a non-asymptotic counterpart of the\nclassical confidence interval based on asymptotic normality of the MLE\nestimator. We propose an efficient method for computing the weighted norm,\nwhich only mildly exceeds the cost of a gradient computation of the loss\nfunction. Finally, we complement our analysis with empirical evidence showing\nthat the proposed confidence bound provides better coverage/width trade-off\ncompared to a confidence estimation by bootstrapping, which is a gold-standard\nmethod in many applications involving non-linear predictors such as neural\nnetworks.", "published": "2025-06-08 11:23:49", "link": "http://arxiv.org/abs/2506.07088v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "State Entropy Regularization for Robust Reinforcement Learning", "abstract": "State entropy regularization has empirically shown better exploration and\nsample complexity in reinforcement learning (RL). However, its theoretical\nguarantees have not been studied. In this paper, we show that state entropy\nregularization improves robustness to structured and spatially correlated\nperturbations. These types of variation are common in transfer learning but\noften overlooked by standard robust RL methods, which typically focus on small,\nuncorrelated changes. We provide a comprehensive characterization of these\nrobustness properties, including formal guarantees under reward and transition\nuncertainty, as well as settings where the method performs poorly. Much of our\nanalysis contrasts state entropy with the widely used policy entropy\nregularization, highlighting their different benefits. Finally, from a\npractical standpoint, we illustrate that compared with policy entropy, the\nrobustness advantages of state entropy are more sensitive to the number of\nrollouts used for policy evaluation.", "published": "2025-06-08 11:15:31", "link": "http://arxiv.org/abs/2506.07085v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Efficient $Q$-Learning and Actor-Critic Methods for Robust Average Reward Reinforcement Learning", "abstract": "We present the first $Q$-learning and actor-critic algorithms for robust\naverage reward Markov Decision Processes (MDPs) with non-asymptotic convergence\nunder contamination, TV distance and Wasserstein distance uncertainty sets. We\nshow that the robust $Q$ Bellman operator is a strict contractive mapping with\nrespect to a carefully constructed semi-norm with constant functions being\nquotiented out. This property supports a stochastic approximation update, that\nlearns the optimal robust $Q$ function in $\\tilde{\\cO}(\\epsilon^{-2})$ samples.\nWe also show that the same idea can be used for robust $Q$ function estimation,\nwhich can be further used for critic estimation. Coupling it with theories in\nrobust policy mirror descent update, we present a natural actor-critic\nalgorithm that attains an $\\epsilon$-optimal robust policy in\n$\\tilde{\\cO}(\\epsilon^{-3})$ samples. These results advance the theory of\ndistributionally robust reinforcement learning in the average reward setting.", "published": "2025-06-08 08:26:27", "link": "http://arxiv.org/abs/2506.07040v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Half-AVAE: Adversarial-Enhanced Factorized and Structured Encoder-Free VAE for Underdetermined Independent Component Analysis", "abstract": "This study advances the Variational Autoencoder (VAE) framework by addressing\nchallenges in Independent Component Analysis (ICA) under both determined and\nunderdetermined conditions, focusing on enhancing the independence and\ninterpretability of latent variables. Traditional VAEs map observed data to\nlatent variables and back via an encoder-decoder architecture, but struggle\nwith underdetermined ICA where the number of latent variables exceeds observed\nsignals. The proposed Half Adversarial VAE (Half-AVAE) builds on the\nencoder-free Half-VAE framework, eliminating explicit inverse mapping to tackle\nunderdetermined scenarios. By integrating adversarial networks and External\nEnhancement (EE) terms, Half-AVAE promotes mutual independence among latent\ndimensions, achieving factorized and interpretable representations. Experiments\nwith synthetic signals demonstrate that Half-AVAE outperforms baseline models,\nincluding GP-AVAE and Half-VAE, in recovering independent components under\nunderdetermined conditions, as evidenced by lower root mean square errors. The\nstudy highlights the flexibility of VAEs in variational inference, showing that\nencoder omission, combined with adversarial training and structured priors,\nenables effective solutions for complex ICA tasks, advancing applications in\ndisentanglement, causal inference, and generative modeling.", "published": "2025-06-08 06:29:53", "link": "http://arxiv.org/abs/2506.07011v1", "categories": ["stat.ML", "cs.LG", "eess.SP"], "primary_category": "stat.ML"}
{"title": "Towards Physics-informed Diffusion for Anomaly Detection in Trajectories", "abstract": "Given trajectory data, a domain-specific study area, and a user-defined\nthreshold, we aim to find anomalous trajectories indicative of possible GPS\nspoofing (e.g., fake trajectory). The problem is societally important to curb\nillegal activities in international waters, such as unauthorized fishing and\nillicit oil transfers. The problem is challenging due to advances in AI\ngenerated in deep fakes generation (e.g., additive noise, fake trajectories)\nand lack of adequate amount of labeled samples for ground-truth verification.\nRecent literature shows promising results for anomalous trajectory detection\nusing generative models despite data sparsity. However, they do not consider\nfine-scale spatiotemporal dependencies and prior physical knowledge, resulting\nin higher false-positive rates. To address these limitations, we propose a\nphysics-informed diffusion model that integrates kinematic constraints to\nidentify trajectories that do not adhere to physical laws. Experimental results\non real-world datasets in the maritime and urban domains show that the proposed\nframework results in higher prediction accuracy and lower estimation error rate\nfor anomaly detection and trajectory generation methods, respectively. Our\nimplementation is available at\nhttps://github.com/arunshar/Physics-Informed-Diffusion-Probabilistic-Model.", "published": "2025-06-08 05:10:20", "link": "http://arxiv.org/abs/2506.06999v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Certified Unlearning for Neural Networks", "abstract": "We address the problem of machine unlearning, where the goal is to remove the\ninfluence of specific training data from a model upon request, motivated by\nprivacy concerns and regulatory requirements such as the \"right to be\nforgotten.\" Unfortunately, existing methods rely on restrictive assumptions or\nlack formal guarantees. To this end, we propose a novel method for certified\nmachine unlearning, leveraging the connection between unlearning and privacy\namplification by stochastic post-processing. Our method uses noisy fine-tuning\non the retain data, i.e., data that does not need to be removed, to ensure\nprovable unlearning guarantees. This approach requires no assumptions about the\nunderlying loss function, making it broadly applicable across diverse settings.\nWe analyze the theoretical trade-offs in efficiency and accuracy and\ndemonstrate empirically that our method not only achieves formal unlearning\nguarantees but also performs effectively in practice, outperforming existing\nbaselines. Our code is available at\nhttps://github.com/stair-lab/certified-unlearningneural-networks-icml-2025", "published": "2025-06-08 03:55:28", "link": "http://arxiv.org/abs/2506.06985v1", "categories": ["cs.LG", "cs.CR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Near Optimal Non-asymptotic Sample Complexity of 1-Identification", "abstract": "Motivated by an open direction in existing literature, we study the\n1-identification problem, a fundamental multi-armed bandit formulation on pure\nexploration. The goal is to determine whether there exists an arm whose mean\nreward is at least a known threshold $\\mu_0$, or to output None if it believes\nsuch an arm does not exist. The agent needs to guarantee its output is correct\nwith probability at least $1-\\delta$. Degenne & Koolen 2019 has established the\nasymptotically tight sample complexity for the 1-identification problem, but\nthey commented that the non-asymptotic analysis remains unclear. We design a\nnew algorithm Sequential-Exploration-Exploitation (SEE), and conduct\ntheoretical analysis from the non-asymptotic perspective. Novel to the\nliterature, we achieve near optimality, in the sense of matching upper and\nlower bounds on the pulling complexity. The gap between the upper and lower\nbounds is up to a polynomial logarithmic factor. The numerical result also\nindicates the effectiveness of our algorithm, compared to existing benchmarks.", "published": "2025-06-08 03:23:45", "link": "http://arxiv.org/abs/2506.06978v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Speech Recognition on TV Series with Video-guided Post-Correction", "abstract": "Automatic Speech Recognition (ASR) has achieved remarkable success with deep\nlearning, driving advancements in conversational artificial intelligence, media\ntranscription, and assistive technologies. However, ASR systems still struggle\nin complex environments such as TV series, where overlapping speech,\ndomain-specific terminology, and long-range contextual dependencies pose\nsignificant challenges to transcription accuracy. Existing multimodal\napproaches fail to correct ASR outputs with the rich temporal and contextual\ninformation available in video. To address this limitation, we propose a novel\nmultimodal post-correction framework that refines ASR transcriptions by\nleveraging contextual cues extracted from video. Our framework consists of two\nstages: ASR Generation and Video-based Post-Correction, where the first stage\nproduces the initial transcript and the second stage corrects errors using\nVideo-based Contextual Information Extraction and Context-aware ASR Correction.\nWe employ the Video-Large Multimodal Model (VLMM) to extract key contextual\ninformation using tailored prompts, which is then integrated with a Large\nLanguage Model (LLM) to refine the ASR output. We evaluate our method on a\nmultimodal benchmark for TV series ASR and demonstrate its effectiveness in\nimproving ASR performance by leveraging video-based context to enhance\ntranscription accuracy in complex multimedia environments.", "published": "2025-06-08 23:36:31", "link": "http://arxiv.org/abs/2506.07323v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Generalized Source Tracing for Codec-Based Deepfake Speech", "abstract": "Recent attempts at source tracing for codec-based deepfake speech\n(CodecFake), generated by neural audio codec-based speech generation (CoSG)\nmodels, have exhibited suboptimal performance. However, how to train source\ntracing models using simulated CoSG data while maintaining strong performance\non real CoSG-generated audio remains an open challenge. In this paper, we show\nthat models trained solely on codec-resynthesized data tend to overfit to\nnon-speech regions and struggle to generalize to unseen content. To mitigate\nthese challenges, we introduce the Semantic-Acoustic Source Tracing Network\n(SASTNet), which jointly leverages Whisper for semantic feature encoding and\nWav2vec2 with AudioMAE for acoustic feature encoding. Our proposed SASTNet\nachieves state-of-the-art performance on the CoSG test set of the CodecFake+\ndataset, demonstrating its effectiveness for reliable source tracing.", "published": "2025-06-08 21:36:10", "link": "http://arxiv.org/abs/2506.07294v1", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-Distillation from Speech and Music Representation Models", "abstract": "Real-world audio often mixes speech and music, yet models typically handle\nonly one domain. This paper introduces a multi-teacher distillation framework\nthat unifies speech and music models into a single one while significantly\nreducing model size. Our approach leverages the strengths of domain-specific\nteacher models, such as HuBERT for speech and MERT for music, and explores\nvarious strategies to balance both domains. Experiments across diverse tasks\ndemonstrate that our model matches the performance of domain-specific models,\nshowing the effectiveness of cross-domain distillation. Additionally, we\nconduct few-shot learning experiments, highlighting the need for general models\nin real-world scenarios where labeled data is limited. Our results show that\nour model not only performs on par with specialized models but also outperforms\nthem in few-shot scenarios, proving that a cross-domain approach is essential\nand effective for diverse tasks with limited data.", "published": "2025-06-08 17:45:46", "link": "http://arxiv.org/abs/2506.07237v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Methods for pitch analysis in contemporary popular music: Vitalic's use of tones that do not operate on the principle of acoustic resonance", "abstract": "Vitalic is an electronic music producer who has been active since 2001.\nVitalic's 2005 track \"No Fun\" features a main synthesiser part built from a\nsequence of single inharmonic tones that evoke two simultaneous melodies. This\npart serves as a starting point for examining Vitalic's use of tones that do\nnot operate on the principle of acoustic resonance. The study considers tones\nthat evoke two or more simultaneous pitches and examines various inharmonic\npartial layouts. Examples outside Vitalic's music are also provided to suggest\nthat similar tone properties can be found elsewhere in contemporary popular\nmusic.", "published": "2025-06-08 16:14:28", "link": "http://arxiv.org/abs/2506.07207v1", "categories": ["cs.SD", "eess.AS", "00A65", "J.5"], "primary_category": "cs.SD"}
{"title": "Audio synthesizer inversion in symmetric parameter spaces with approximately equivariant flow matching", "abstract": "Many audio synthesizers can produce the same signal given different parameter\nconfigurations, meaning the inversion from sound to parameters is an inherently\nill-posed problem. We show that this is largely due to intrinsic symmetries of\nthe synthesizer, and focus in particular on permutation invariance. First, we\ndemonstrate on a synthetic task that regressing point estimates under\npermutation symmetry degrades performance, even when using a\npermutation-invariant loss function or symmetry-breaking heuristics. Then,\nviewing equivalent solutions as modes of a probability distribution, we show\nthat a conditional generative model substantially improves performance.\nFurther, acknowledging the invariance of the implicit parameter distribution,\nwe find that performance is further improved by using a permutation equivariant\ncontinuous normalizing flow. To accommodate intricate symmetries in real\nsynthesizers, we also propose a relaxed equivariance strategy that adaptively\ndiscovers relevant symmetries from data. Applying our method to Surge XT, a\nfull-featured open source synthesizer used in real world audio production, we\nfind our method outperforms regression and generative baselines across audio\nreconstruction metrics.", "published": "2025-06-08 15:47:44", "link": "http://arxiv.org/abs/2506.07199v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Technical Report: A Practical Guide to Kaldi ASR Optimization", "abstract": "This technical report introduces innovative optimizations for Kaldi-based\nAutomatic Speech Recognition (ASR) systems, focusing on acoustic model\nenhancement, hyperparameter tuning, and language model efficiency. We developed\na custom Conformer block integrated with a multistream TDNN-F structure,\nenabling superior feature extraction and temporal modeling. Our approach\nincludes advanced data augmentation techniques and dynamic hyperparameter\noptimization to boost performance and reduce overfitting. Additionally, we\npropose robust strategies for language model management, employing Bayesian\noptimization and $n$-gram pruning to ensure relevance and computational\nefficiency. These systematic improvements significantly elevate ASR accuracy\nand robustness, outperforming existing methods and offering a scalable solution\nfor diverse speech recognition scenarios. This report underscores the\nimportance of strategic optimizations in maintaining Kaldi's adaptability and\ncompetitiveness in rapidly evolving technological landscapes.", "published": "2025-06-08 13:53:55", "link": "http://arxiv.org/abs/2506.07149v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RBA-FE: A Robust Brain-Inspired Audio Feature Extractor for Depression Diagnosis", "abstract": "This article proposes a robust brain-inspired audio feature extractor\n(RBA-FE) model for depression diagnosis, using an improved hierarchical network\narchitecture. Most deep learning models achieve state-of-the-art performance\nfor image-based diagnostic tasks, ignoring the counterpart audio features. In\norder to tailor the noise challenge, RBA-FE leverages six acoustic features\nextracted from the raw audio, capturing both spatial characteristics and\ntemporal dependencies. This hybrid attribute helps alleviate the precision\nlimitation in audio feature extraction within other learning models like deep\nresidual shrinkage networks. To deal with the noise issues, our model\nincorporates an improved spiking neuron model, called adaptive rate smooth\nleaky integrate-and-fire (ARSLIF). The ARSLIF model emulates the mechanism of\n``retuning of cellular signal selectivity\" in the brain attention systems,\nwhich enhances the model robustness against environmental noises in audio data.\nExperimental results demonstrate that RBA-FE achieves state-of-the-art accuracy\non the MODMA dataset, respectively with 0.8750, 0.8974, 0.8750 and 0.8750 in\nprecision, accuracy, recall and F1 score. Extensive experiments on the AVEC2014\nand DAIC-WOZ datasets both show enhancements in noise robustness. It is further\nindicated by comparison that the ARSLIF neuron model suggest the abnormal\nfiring pattern within the feature extraction on depressive audio data, offering\nbrain-inspired interpretability.", "published": "2025-06-08 13:00:45", "link": "http://arxiv.org/abs/2506.07118v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Streaming Endpointer for Spoken Dialogue using Neural Audio Codecs and Label-Delayed Training", "abstract": "Accurate, low-latency endpointing is crucial for effective spoken dialogue\nsystems. While traditional endpointers often rely on spectrum-based audio\nfeatures, this work proposes real-time speech endpointing for multi-turn\ndialogues using streaming, low-bitrate Neural Audio Codec (NAC) features,\nbuilding upon recent advancements in neural audio codecs. To further reduce\ncutoff errors, we introduce a novel label delay training scheme. At a fixed\nmedian latency of 160 ms, our combined NAC and label delay approach achieves\nsignificant relative cutoff error reductions: 42.7% for a single-stream\nendpointer and 37.5% for a two-stream configuration, compared to baseline\nmethods. Finally, we demonstrate efficient integration with a codec-based\npretrained speech large language model, improving its median response time by\n1200 ms and reducing its cutoff error by 35%.", "published": "2025-06-08 10:54:23", "link": "http://arxiv.org/abs/2506.07081v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "E-BATS: Efficient Backpropagation-Free Test-Time Adaptation for Speech Foundation Models", "abstract": "Speech Foundation Models encounter significant performance degradation when\ndeployed in real-world scenarios involving acoustic domain shifts, such as\nbackground noise and speaker accents. Test-time adaptation (TTA) has recently\nemerged as a viable strategy to address such domain shifts at inference time\nwithout requiring access to source data or labels. However, existing TTA\napproaches, particularly those relying on backpropagation, are\nmemory-intensive, limiting their applicability in speech tasks and\nresource-constrained settings. Although backpropagation-free methods offer\nimproved efficiency, existing ones exhibit poor accuracy. This is because they\nare predominantly developed for vision tasks, which fundamentally differ from\nspeech task formulations, noise characteristics, and model architecture, posing\nunique transferability challenges. In this paper, we introduce E-BATS, the\nfirst Efficient BAckpropagation-free TTA framework designed explicitly for\nspeech foundation models. E-BATS achieves a balance between adaptation\neffectiveness and memory efficiency through three key components: (i)\nlightweight prompt adaptation for a forward-pass-based feature alignment, (ii)\na multi-scale loss to capture both global (utterance-level) and local\ndistribution shifts (token-level) and (iii) a test-time exponential moving\naverage mechanism for stable adaptation across utterances. Experiments\nconducted on four noisy speech datasets spanning sixteen acoustic conditions\ndemonstrate consistent improvements, with 4.1%-13.5% accuracy gains over\nbackpropagation-free baselines and 2.0-6.4 times GPU memory savings compared to\nbackpropagation-based methods. By enabling scalable and robust adaptation under\nacoustic variability, this work paves the way for developing more efficient\nadaptation approaches for practical speech processing systems in real-world\nenvironments.", "published": "2025-06-08 10:33:37", "link": "http://arxiv.org/abs/2506.07078v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Insights on Harmonic Tones from a Generative Music Experiment", "abstract": "The ultimate purpose of generative music AI is music production. The\nstudio-lab, a social form within the art-science branch of\ncross-disciplinarity, is a way to advance music production with AI music\nmodels. During a studio-lab experiment involving researchers, music producers,\nand an AI model for music generating bass-like audio, it was observed that the\nproducers used the model's output to convey two or more pitches with a single\nharmonic complex tone, which in turn revealed that the model had learned to\ngenerate structured and coherent simultaneous melodic lines using monophonic\nsequences of harmonic complex tones. These findings prompt a reconsideration of\nthe long-standing debate on whether humans can perceive harmonics as distinct\npitches and highlight how generative AI can not only enhance musical creativity\nbut also contribute to a deeper understanding of music.", "published": "2025-06-08 10:25:55", "link": "http://arxiv.org/abs/2506.07073v1", "categories": ["cs.SD", "cs.HC", "eess.AS", "68T01", "J.5"], "primary_category": "cs.SD"}
{"title": "\"In This Environment, As That Speaker\": A Text-Driven Framework for Multi-Attribute Speech Conversion", "abstract": "We propose TES-VC (Text-driven Environment and Speaker controllable Voice\nConversion), a text-driven voice conversion framework with independent control\nof speaker timbre and environmental acoustics. TES-VC processes simultaneous\ntext inputs for target voice and environment, accurately generating speech\nmatching described timbre/environment while preserving source content. Trained\non synthetic data with decoupled vocal/environment features via latent\ndiffusion modeling, our method eliminates interference between attributes. The\nRetrieval-Based Timbre Control (RBTC) module enables precise manipulation using\nabstract descriptions without paired data. Experiments confirm TES-VC\neffectively generates contextually appropriate speech in both timbre and\nenvironment with high content retention and superior controllability which\ndemonstrates its potential for widespread applications.", "published": "2025-06-08 08:00:56", "link": "http://arxiv.org/abs/2506.07036v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Theoretical and Experimental Evaluation of AoA Estimation in Single-Anchor 5G Uplink Positioning", "abstract": "As we move towards 6G, the demand for high-precision, cost-effective\npositioning solutions becomes increasingly critical. Single-anchor positioning\noffers a promising alternative to traditional multi-anchor approaches,\nparticularly in complex propagation environments where infrastructure costs and\ndeployment constraints present significant challenges. This paper provides a\ncomprehensive evaluation of key algorithmic choices in the development of a\nsingle-anchor 5G uplink positioning testbed. Our developed testbed uses angle\nof arrival (AoA) estimation combined with range measurements from an\nultra-wideband pair, to derive the position. The simulations conducted assess\nthe impact of the selected algorithms on channel order and AoA estimation,\nwhile the influence of antenna calibration errors on AoA estimation is also\nexamined. Finally, we compare simulations and results obtained from our\ndeveloped platform.", "published": "2025-06-08 21:56:26", "link": "http://arxiv.org/abs/2506.07300v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Real-Time High-Accuracy Digital Wireless Time, Frequency, and Phase Calibration For Coherent Distributed Antenna Arrays", "abstract": "This work presents a fully-digital high-accuracy real-time calibration\nprocedure for frequency and time alignment of open-loop wirelessly coordinated\ncoherent distributed antenna array (CDA) modems, enabling RF phase coherence of\nspatially separated commercial off-the-shelf (COTS) software-defined radios\n(SDRs) without any cables or external references such as global navigation\nsatellite system (GNSS). Building on previous work using high-accuracy\nspectrally-sparse time of arrival (ToA) waveforms and a multi-step ToA\nrefinement process, a high-accuracy two-way time transfer (TWTT)-based\ntime-frequency coordination approach is demonstrated. By using a high-accuracy\ntime estimation approach, frequency estimates can be derived over long\nobservation intervals leading to a high-accuracy frequency estimate, without\nthe requirement for long pulse durations as is required for direct spectral\nfrequency estimation techniques, minimizing coordination overhead. Furthermore,\ndue to the two-way nature of the high-accuracy TWTT approach, the time and\nfrequency estimates are Doppler and multi-path tolerant, so long as the channel\nis reciprocal over the synchronization epoch.\n  This technique is experimentally verified by demonstrating wireless\ndistributed array coordination using COTS SDRs in a lab environment in static\nand dynamic scenarios and with significant multipath scatterers. Time,\nfrequency, and phase stability were measured over coaxial cables to an\noscilloscope and achieved time and phase coordination precision of ~60-70 ps,\nwith median coherent gains above 99% using optimized parameters, and a\nbeamforming frequency RMSE of 3.73 ppb in a dynamic scenario. Finally,\nexperiments are conducted to compare the performance of this technique with\nprevious work works using an analog continuous-wave two-tone (CWTT) frequency\nreference technique in both static and dynamic settings as a benchmark.", "published": "2025-06-08 20:09:44", "link": "http://arxiv.org/abs/2506.07267v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Performance Evaluation of Beyond Diagonal RIS under Hardware Impairments", "abstract": "Beyond diagonal reconfigurable intelligent surface (BD-RIS) improves the\ntraditional reconfigurable intelligent surface (RIS) architecture functionality\nby interconnecting elements for advanced wave control. However, real-world\nimplementations face hardware imperfections, such as impedance mismatches and\nvaractor nonidealities, which can degrade overall system performance. In this\npaper, we propose three hardware impairment models that directly affect the\nBD-RIS scattering matrix structure and evaluate their impact on the channel\nestimation accuracy using the normalized mean square error (NMSE) as a\nperformance metric. The proposed impairment models consider imperfections\naffecting self-impedances, mutual impedances, or both. Our results reveal how\neach impairment type degrades the system performance, allowing us to identify\nscenarios where the traditional RIS can outperform the BD-RIS.", "published": "2025-06-08 20:05:28", "link": "http://arxiv.org/abs/2506.07266v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Fourier-Domain CFO Estimation Using Jutted Binary Modulation on Conjugate-Reciprocal Zeros", "abstract": "In this work, we propose jutted binary modulation on conjugate-reciprocal\nzeros (J-BMOCZ) for non-coherent communication under a carrier frequency offset\n(CFO). By introducing asymmetry to the Huffman BMOCZ zero constellation, we\nexploit the identical aperiodic auto-correlation function of BMOCZ to derive a\nFourier-domain metric for CFO estimation. Unlike the existing methods for\nHuffman BMOCZ, which require a cyclically permutable code (CPC) for pilot-free\nCFO correction, J-BMOCZ enables the estimation of a CFO without the use of\npilots or channel coding. Through numerical simulations in additive white\nGaussian noise and fading channels, we show that the bit error rate (BER) loss\nof J-BMOCZ under a CFO is just 1 dB over Huffman BMOCZ without a CFO.\nFurthermore, the results show that coded J-BMOCZ achieves better BER\nperformance than Huffman BMOCZ with a CPC.", "published": "2025-06-08 18:54:06", "link": "http://arxiv.org/abs/2506.07256v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Joint Channel and Symbol Estimation for Communication Systems with Movable Antennas", "abstract": "Communication systems aided by movable antennas have been the subject of\nrecent research due to their potentially increased spatial degrees of freedom\noffered by optimizing the antenna positioning at the transmitter and/or\nreceiver. In this context, a topic that deserves attention is channel\nestimation. Conventional methods reported recently rely on pilot-assisted\nstrategies to estimate the channel coefficients. In this work, we address the\njoint channel and symbol estimation problem for an uplink multi-user\ncommunication system, where the base station is equipped with a movable antenna\narray. A semi-blind receiver based on the PARAFAC2 model is formulated to\nexploit the tensor decomposition structure for the received signals, from which\nchannel and symbol estimates can be jointly obtained via an alternating\nestimation algorithm. Compared with reference schemes, our preliminary\nnumerical simulations yield remarkable results for the proposed method.", "published": "2025-06-08 15:08:24", "link": "http://arxiv.org/abs/2506.07183v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Circuit-Based Modeling Approach for Channel Estimation in RIS-Assisted Communications", "abstract": "Reconfigurable intelligent surface (RIS) has been explored as a supportive\ntechnology for wireless communication since around 2019. While the literature\nhighlights the potential of RIS in different modern applications, two key\nissues have gained significant attention from the research community: channel\nestimation and phase shift optimization. The performance gains of RIS-assisted\nsystems rely heavily on optimal phase shifts, which, in turn, depend on\naccurate channel estimation. Several studies have addressed these challenges\nunder different assumptions. Some works consider a range of continuous phase\nshifts, while others propose a limited number of discrete phase values for the\nRIS elements. Many studies present an idealized perspective, whereas others aim\nto approximate more practical aspects by considering circuit system responses\nand employing phase shifts derived from a Discrete Fourier Transform (DFT) or\nother lookup tables. However, to our knowledge, no study has examined the\ninfluence of circuit system parameters on channel estimation and subsequent\nphase shift optimization. This paper models each RIS element as an equivalent\nresonant circuit composed of resistance, capacitance, and inductance. We\npropose that resistance and capacitance parameters can be dynamically and\nindependently configured, leading to the formulation of an impedance matrix.\nFurthermore, we construct a circuit-based RIS phase shift matrix that accounts\nfor the response of the resonant circuit, which changes with variations in the\nphysical parameters of resistance and capacitance. We investigate the impact of\nthis circuit-based RIS phase shift within a tensor-based channel estimation\napproach. Our results indicate a performance loss compared to ideal scenarios,\nsuch as those using the DFT design. However, we found that increasing the\ntraining time can mitigate this performance degradation.", "published": "2025-06-08 13:11:27", "link": "http://arxiv.org/abs/2506.07124v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Near-Field Integrated Sensing and Communication for Multi-Target Indication", "abstract": "Integrated sensing and communication (ISAC) in the near-field regime offers\nthe potential to jointly support high-rate downlink transmission and\nhigh-resolution multi-target detection by exploiting the spherical-wave nature\nof electromagnetic propagation. In this paper, we propose a unified beamforming\nframework for a multi-user multi-target near-field ISAC system. In this system,\na multi-antenna base station simultaneously serves multiple single-antenna\nusers and senses multiple point-targets without prior knowledge of their radar\ncross sections. By optimizing the transmit covariance matrix, our design\nmaximizes the minimum weighted transmit beampattern gain across all targets to\nensure accurate sensing while strictly limiting inter-target cross-correlations\nand guaranteeing per-user communication rate and total power constraints. We\nextend classical far-field beampattern and cross-correlation measures to the\nnear-field by incorporating both angle and range dependencies, enabling\ndiscrimination of targets along the same direction but at different distances.\nThe resulting non-convex program is efficiently relaxed to a semidefinite\nprogram via rank-one lifting. We then develop a closed-form reconstruction to\nrecover optimal rank-one beamformers. Numerical simulations demonstrate that\nour near-field ISAC design can simultaneously resolve and serve users/targets\nalong the same direction but at different distances, achieving significant\ngains over far-field and single-target benchmarks.", "published": "2025-06-08 09:25:40", "link": "http://arxiv.org/abs/2506.07052v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Towards Generalized Source Tracing for Codec-Based Deepfake Speech", "abstract": "Recent attempts at source tracing for codec-based deepfake speech\n(CodecFake), generated by neural audio codec-based speech generation (CoSG)\nmodels, have exhibited suboptimal performance. However, how to train source\ntracing models using simulated CoSG data while maintaining strong performance\non real CoSG-generated audio remains an open challenge. In this paper, we show\nthat models trained solely on codec-resynthesized data tend to overfit to\nnon-speech regions and struggle to generalize to unseen content. To mitigate\nthese challenges, we introduce the Semantic-Acoustic Source Tracing Network\n(SASTNet), which jointly leverages Whisper for semantic feature encoding and\nWav2vec2 with AudioMAE for acoustic feature encoding. Our proposed SASTNet\nachieves state-of-the-art performance on the CoSG test set of the CodecFake+\ndataset, demonstrating its effectiveness for reliable source tracing.", "published": "2025-06-08 21:36:10", "link": "http://arxiv.org/abs/2506.07294v2", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Theoretical and Experimental Evaluation of AoA Estimation in Single-Anchor 5G Uplink Positioning", "abstract": "As we move towards 6G, the demand for high-precision, cost-effective\npositioning solutions becomes increasingly critical. Single-anchor positioning\noffers a promising alternative to traditional multi-anchor approaches,\nparticularly in complex propagation environments where infrastructure costs and\ndeployment constraints present significant challenges. This paper provides a\ncomprehensive evaluation of key algorithmic choices in the development of a\nsingle-anchor 5G uplink positioning testbed. Our developed testbed uses angle\nof arrival (AoA) estimation combined with range measurements from an\nultra-wideband pair, to derive the position. The simulations conducted assess\nthe impact of the selected algorithms on channel order and AoA estimation,\nwhile the influence of antenna calibration errors on AoA estimation is also\nexamined. Finally, we compare simulations and results obtained from our\ndeveloped platform.", "published": "2025-06-08 21:56:26", "link": "http://arxiv.org/abs/2506.07300v2", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Fourier-Domain CFO Estimation Using Jutted Binary Modulation on Conjugate-Reciprocal Zeros", "abstract": "In this work, we propose jutted binary modulation on conjugate-reciprocal\nzeros (J-BMOCZ) for non-coherent communication under a carrier frequency offset\n(CFO). By introducing asymmetry to the Huffman BMOCZ zero constellation, we\nexploit the identical aperiodic auto-correlation function of BMOCZ sequences to\nderive a Fourier-domain metric for CFO estimation. Unlike the existing methods\nfor Huffman BMOCZ, which require a cyclically permutable code (CPC) for\npilot-free CFO correction, J-BMOCZ enables the estimation of a CFO without the\nuse of pilots or channel coding. Through numerical simulations in additive\nwhite Gaussian noise and fading channels, we show that the bit error rate (BER)\nloss of J-BMOCZ under a CFO is just 1 dB over Huffman BMOCZ without a CFO.\nFurthermore, the results show that coded J-BMOCZ achieves better BER\nperformance than Huffman BMOCZ with a CPC.", "published": "2025-06-08 18:54:06", "link": "http://arxiv.org/abs/2506.07256v2", "categories": ["eess.SP"], "primary_category": "eess.SP"}
