{"title": "Translating Phrases in Neural Machine Translation", "abstract": "Phrases play an important role in natural language understanding and machine\ntranslation (Sag et al., 2002; Villavicencio et al., 2005). However, it is\ndifficult to integrate them into current neural machine translation (NMT) which\nreads and generates sentences word by word. In this work, we propose a method\nto translate phrases in NMT by integrating a phrase memory storing target\nphrases from a phrase-based statistical machine translation (SMT) system into\nthe encoder-decoder architecture of NMT. At each decoding step, the phrase\nmemory is first re-written by the SMT model, which dynamically generates\nrelevant target phrases with contextual information provided by the NMT model.\nThen the proposed model reads the phrase memory to make probability estimations\nfor all phrases in the phrase memory. If phrase generation is carried on, the\nNMT decoder selects an appropriate phrase from the memory to perform phrase\ntranslation and updates its decoding state by consuming the words in the\nselected phrase. Otherwise, the NMT decoder generates a word from the\nvocabulary as the general NMT decoder does. Experiment results on the Chinese\nto English translation show that the proposed model achieves significant\nimprovements over the baseline on various test sets.", "published": "2017-08-07 03:46:48", "link": "http://arxiv.org/abs/1708.01980v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Memory-augmented Neural Machine Translation", "abstract": "Neural machine translation (NMT) has achieved notable success in recent\ntimes, however it is also widely recognized that this approach has limitations\nwith handling infrequent words and word pairs. This paper presents a novel\nmemory-augmented NMT (M-NMT) architecture, which stores knowledge about how\nwords (usually infrequently encountered ones) should be translated in a memory\nand then utilizes them to assist the neural model. We use this memory mechanism\nto combine the knowledge learned from a conventional statistical machine\ntranslation system and the rules learned by an NMT system, and also propose a\nsolution for out-of-vocabulary (OOV) words based on this framework. Our\nexperiments on two Chinese-English translation tasks demonstrated that the\nM-NMT architecture outperformed the NMT baseline by $9.0$ and $2.7$ BLEU points\non the two tasks, respectively. Additionally, we found this architecture\nresulted in a much more effective OOV treatment compared to competitive\nmethods.", "published": "2017-08-07 06:47:23", "link": "http://arxiv.org/abs/1708.02005v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ISS-MULT: Intelligent Sample Selection for Multi-Task Learning in\n  Question Answering", "abstract": "Transferring knowledge from a source domain to another domain is useful,\nespecially when gathering new data is very expensive and time-consuming. Deep\nnetworks have been well-studied for question answering tasks in recent years;\nhowever, no prominent research for transfer learning through deep neural\nnetworks exists in the question answering field. In this paper, two main\nmethods (INIT and MULT) in this field are examined. Then, a new method named\nIntelligent sample selection (ISS-MULT) is proposed to improve the MULT method\nfor question answering tasks. Different datasets, specificay SQuAD, SelQA,\nWikiQA, NewWikiQA and InforBoxQA, are used for evaluation. Moreover, two\ndifferent tasks of question answering - answer selection and answer triggering\n- are evaluated to examine the effectiveness of transfer learning. The results\nshow that using transfer learning generally improves the performance if the\ncorpora are related and are based on the same policy. In addition, using\nISS-MULT could finely improve the MULT method for question answering tasks, and\nthese improvements prove more significant in the answer triggering task.", "published": "2017-08-07 19:00:25", "link": "http://arxiv.org/abs/1708.02267v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Corpus-level Fine-grained Entity Typing", "abstract": "This paper addresses the problem of corpus-level entity typing, i.e.,\ninferring from a large corpus that an entity is a member of a class such as\n\"food\" or \"artist\". The application of entity typing we are interested in is\nknowledge base completion, specifically, to learn which classes an entity is a\nmember of. We propose FIGMENT to tackle this problem. FIGMENT is embedding-\nbased and combines (i) a global model that scores based on aggregated\ncontextual information of an entity and (ii) a context model that first scores\nthe individual occurrences of an entity and then aggregates the scores. Each of\nthe two proposed models has some specific properties. For the global model,\nlearning high quality entity representations is crucial because it is the only\nsource used for the predictions. Therefore, we introduce representations using\nname and contexts of entities on the three levels of entity, word, and\ncharacter. We show each has complementary information and a multi-level\nrepresentation is the best. For the context model, we need to use distant\nsupervision since the context-level labels are not available for entities.\nDistant supervised labels are noisy and this harms the performance of models.\nTherefore, we introduce and apply new algorithms for noise mitigation using\nmulti-instance learning. We show the effectiveness of our models in a large\nentity typing dataset, built from Freebase.", "published": "2017-08-07 19:41:42", "link": "http://arxiv.org/abs/1708.02275v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Video Highlights Detection and Summarization with Lag-Calibration based\n  on Concept-Emotion Mapping of Crowd-sourced Time-Sync Comments", "abstract": "With the prevalence of video sharing, there are increasing demands for\nautomatic video digestion such as highlight detection. Recently, platforms with\ncrowdsourced time-sync video comments have emerged worldwide, providing a good\nopportunity for highlight detection. However, this task is non-trivial: (1)\ntime-sync comments often lag behind their corresponding shot; (2) time-sync\ncomments are semantically sparse and noisy; (3) to determine which shots are\nhighlights is highly subjective. The present paper aims to tackle these\nchallenges by proposing a framework that (1) uses concept-mapped lexical-chains\nfor lag calibration; (2) models video highlights based on comment intensity and\ncombination of emotion and concept concentration of each shot; (3) summarize\neach detected highlight using improved SumBasic with emotion and concept\nmapping. Experiments on large real-world datasets show that our highlight\ndetection method and summarization method both outperform other benchmarks with\nconsiderable margins.", "published": "2017-08-07 17:21:20", "link": "http://arxiv.org/abs/1708.02210v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption\n  Generator?", "abstract": "In neural image captioning systems, a recurrent neural network (RNN) is\ntypically viewed as the primary `generation' component. This view suggests that\nthe image features should be `injected' into the RNN. This is in fact the\ndominant view in the literature. Alternatively, the RNN can instead be viewed\nas only encoding the previously generated words. This view suggests that the\nRNN should only be used to encode linguistic features and that only the final\nrepresentation should be `merged' with the image features at a later stage.\nThis paper compares these two architectures. We find that, in general, late\nmerging outperforms injection, suggesting that RNNs are better viewed as\nencoders, rather than generators.", "published": "2017-08-07 09:01:35", "link": "http://arxiv.org/abs/1708.02043v2", "categories": ["cs.CL", "cs.CV", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Multimodal Classification for Analysing Social Media", "abstract": "Classification of social media data is an important approach in understanding\nuser behavior on the Web. Although information on social media can be of\ndifferent modalities such as texts, images, audio or videos, traditional\napproaches in classification usually leverage only one prominent modality.\nTechniques that are able to leverage multiple modalities are often complex and\nsusceptible to the absence of some modalities. In this paper, we present simple\nmodels that combine information from different modalities to classify social\nmedia content and are able to handle the above problems with existing\ntechniques. Our models combine information from different modalities using a\npooling layer and an auxiliary learning task is used to learn a common feature\nspace. We demonstrate the performance of our models and their robustness to the\nmissing of some modalities in the emotion classification domain. Our\napproaches, although being simple, can not only achieve significantly higher\naccuracies than traditional fusion approaches but also have comparable results\nwhen only one modality is available.", "published": "2017-08-07 12:50:09", "link": "http://arxiv.org/abs/1708.02099v1", "categories": ["cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Regularizing and Optimizing LSTM Language Models", "abstract": "Recurrent neural networks (RNNs), such as long short-term memory networks\n(LSTMs), serve as a fundamental building block for many sequence learning\ntasks, including machine translation, language modeling, and question\nanswering. In this paper, we consider the specific problem of word-level\nlanguage modeling and investigate strategies for regularizing and optimizing\nLSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on\nhidden-to-hidden weights as a form of recurrent regularization. Further, we\nintroduce NT-ASGD, a variant of the averaged stochastic gradient method,\nwherein the averaging trigger is determined using a non-monotonic condition as\nopposed to being tuned by the user. Using these and other regularization\nstrategies, we achieve state-of-the-art word level perplexities on two data\nsets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the\neffectiveness of a neural cache in conjunction with our proposed model, we\nachieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and\n52.0 on WikiText-2.", "published": "2017-08-07 16:03:44", "link": "http://arxiv.org/abs/1708.02182v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "LitStoryTeller: An Interactive System for Visual Exploration of\n  Scientific Papers Leveraging Named entities and Comparative Sentences", "abstract": "The present study proposes LitStoryTeller, an interactive system for visually\nexploring the semantic structure of a scientific article. We demonstrate how\nLitStoryTeller could be used to answer some of the most fundamental research\nquestions, such as how a new method was built on top of existing methods, based\non what theoretical proof and experimental evidences. More importantly,\nLitStoryTeller can assist users to understand the full and interesting story a\nscientific paper, with a concise outline and important details. The proposed\nsystem borrows a metaphor from screen play, and visualizes the storyline of a\nscientific paper by arranging its characters (scientific concepts or\nterminologies) and scenes (paragraphs/sentences) into a progressive and\ninteractive storyline. Such storylines help to preserve the semantic structure\nand logical thinking process of a scientific paper. Semantic structures, such\nas scientific concepts and comparative sentences, are extracted using existing\nnamed entity recognition APIs and supervised classifiers, from a scientific\npaper automatically. Two supplementary views, ranked entity frequency view and\nentity co-occurrence network view, are provided to help users identify the\n\"main plot\" of such scientific storylines. When collective documents are ready,\nLitStoryTeller also provides a temporal entity evolution view and entity\ncommunity view for collection digestion.", "published": "2017-08-07 17:32:56", "link": "http://arxiv.org/abs/1708.02214v3", "categories": ["cs.HC", "cs.CL", "cs.DL"], "primary_category": "cs.HC"}
{"title": "Generative Statistical Models with Self-Emergent Grammar of Chord\n  Sequences", "abstract": "Generative statistical models of chord sequences play crucial roles in music\nprocessing. To capture syntactic similarities among certain chords (e.g. in C\nmajor key, between G and G7 and between F and Dm), we study hidden Markov\nmodels and probabilistic context-free grammar models with latent variables\ndescribing syntactic categories of chord symbols and their unsupervised\nlearning techniques for inducing the latent grammar from data. Surprisingly, we\nfind that these models often outperform conventional Markov models in\npredictive power, and the self-emergent categories often correspond to\ntraditional harmonic functions. This implies the need for chord categories in\nharmony models from the informatics perspective.", "published": "2017-08-07 18:00:42", "link": "http://arxiv.org/abs/1708.02255v3", "categories": ["cs.AI", "cs.CL", "cs.SD"], "primary_category": "cs.AI"}
{"title": "Reinforced Video Captioning with Entailment Rewards", "abstract": "Sequence-to-sequence models have shown promising improvements on the temporal\ntask of video captioning, but they optimize word-level cross-entropy loss\nduring training. First, using policy gradient and mixed-loss methods for\nreinforcement learning, we directly optimize sentence-level task-based metrics\n(as rewards), achieving significant improvements over the baseline, based on\nboth automatic metrics and human evaluation on multiple datasets. Next, we\npropose a novel entailment-enhanced reward (CIDEnt) that corrects\nphrase-matching based metrics (such as CIDEr) to only allow for\nlogically-implied partial matches and avoid contradictions, achieving further\nsignificant improvements over the CIDEr-reward model. Overall, our\nCIDEnt-reward model achieves the new state-of-the-art on the MSR-VTT dataset.", "published": "2017-08-07 20:50:24", "link": "http://arxiv.org/abs/1708.02300v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Shortcut-Stacked Sentence Encoders for Multi-Domain Inference", "abstract": "We present a simple sequential sentence encoder for multi-domain natural\nlanguage inference. Our encoder is based on stacked bidirectional LSTM-RNNs\nwith shortcut connections and fine-tuning of word embeddings. The overall\nsupervised model uses the above encoder to encode two input sentences into two\nvectors, and then uses a classifier over the vector combination to label the\nrelationship between these two sentences as that of entailment, contradiction,\nor neural. Our Shortcut-Stacked sentence encoders achieve strong improvements\nover existing encoders on matched and mismatched multi-domain natural language\ninference (top non-ensemble single-model result in the EMNLP RepEval 2017\nShared Task (Nangia et al., 2017)). Moreover, they achieve the new\nstate-of-the-art encoding result on the original SNLI dataset (Bowman et al.,\n2015).", "published": "2017-08-07 21:33:11", "link": "http://arxiv.org/abs/1708.02312v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Asking Too Much? The Rhetorical Role of Questions in Political Discourse", "abstract": "Questions play a prominent role in social interactions, performing rhetorical\nfunctions that go beyond that of simple informational exchange. The surface\nform of a question can signal the intention and background of the person asking\nit, as well as the nature of their relation with the interlocutor. While the\ninformational nature of questions has been extensively examined in the context\nof question-answering applications, their rhetorical aspects have been largely\nunderstudied.\n  In this work we introduce an unsupervised methodology for extracting surface\nmotifs that recur in questions, and for grouping them according to their latent\nrhetorical role. By applying this framework to the setting of question sessions\nin the UK parliament, we show that the resulting typology encodes key aspects\nof the political discourse---such as the bifurcation in questioning behavior\nbetween government and opposition parties---and reveals new insights into the\neffects of a legislator's tenure and political career ambitions.", "published": "2017-08-07 18:00:32", "link": "http://arxiv.org/abs/1708.02254v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI", "physics.soc-ph"], "primary_category": "cs.CL"}
