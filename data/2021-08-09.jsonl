{"title": "The HW-TSC's Offline Speech Translation Systems for IWSLT 2021\n  Evaluation", "abstract": "This paper describes our work in participation of the IWSLT-2021 offline\nspeech translation task. Our system was built in a cascade form, including a\nspeaker diarization module, an Automatic Speech Recognition (ASR) module and a\nMachine Translation (MT) module. We directly use the LIUM SpkDiarization tool\nas the diarization module. The ASR module is trained with three ASR datasets\nfrom different sources, by multi-source training, using a modified Transformer\nencoder. The MT module is pretrained on the large-scale WMT news translation\ndataset and fine-tuned on the TED corpus. Our method achieves 24.6 BLEU score\non the 2021 test set.", "published": "2021-08-09 07:28:04", "link": "http://arxiv.org/abs/2108.03845v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Images really do the Talking? Analysing the significance of Images in\n  Tamil Troll meme classification", "abstract": "A meme is an part of media created to share an opinion or emotion across the\ninternet. Due to its popularity, memes have become the new forms of\ncommunication on social media. However, due to its nature, they are being used\nin harmful ways such as trolling and cyberbullying progressively. Various data\nmodelling methods create different possibilities in feature extraction and\nturning them into beneficial information. The variety of modalities included in\ndata plays a significant part in predicting the results. We try to explore the\nsignificance of visual features of images in classifying memes. Memes are a\nblend of both image and text, where the text is embedded into the image. We try\nto incorporate the memes as troll and non-trolling memes based on the images\nand the text on them. However, the images are to be analysed and combined with\nthe text to increase performance. Our work illustrates different textual\nanalysis methods and contrasting multimodal methods ranging from simple merging\nto cross attention to utilising both worlds' - best visual and textual\nfeatures. The fine-tuned cross-lingual language model, XLM, performed the best\nin textual analysis, and the multimodal transformer performs the best in\nmultimodal analysis.", "published": "2021-08-09 09:04:42", "link": "http://arxiv.org/abs/2108.03886v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Not quite there yet: Combining analogical patterns and encoder-decoder\n  networks for cognitively plausible inflection", "abstract": "The paper presents four models submitted to Part 2 of the SIGMORPHON 2021\nShared Task 0, which aims at replicating human judgements on the inflection of\nnonce lexemes. Our goal is to explore the usefulness of combining pre-compiled\nanalogical patterns with an encoder-decoder architecture. Two models are\ndesigned using such patterns either in the input or the output of the network.\nTwo extra models controlled for the role of raw similarity of nonce inflected\nforms to existing inflected forms in the same paradigm cell, and the role of\nthe type frequency of analogical patterns. Our strategy is entirely endogenous\nin the sense that the models appealing solely to the data provided by the\nSIGMORPHON organisers, without using external resources. Our model 2 ranks\nsecond among all submitted systems, suggesting that the inclusion of analogical\npatterns in the network architecture is useful in mimicking speakers'\npredictions.", "published": "2021-08-09 12:03:27", "link": "http://arxiv.org/abs/2108.03968v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT-based distractor generation for Swedish reading comprehension\n  questions using a small-scale dataset", "abstract": "An important part when constructing multiple-choice questions (MCQs) for\nreading comprehension assessment are the distractors, the incorrect but\npreferably plausible answer options. In this paper, we present a new BERT-based\nmethod for automatically generating distractors using only a small-scale\ndataset. We also release a new such dataset of Swedish MCQs (used for training\nthe model), and propose a methodology for assessing the generated distractors.\nEvaluation shows that from a student's perspective, our method generated one or\nmore plausible distractors for more than 50% of the MCQs in our test set. From\na teacher's perspective, about 50% of the generated distractors were deemed\nappropriate. We also do a thorough analysis of the results.", "published": "2021-08-09 12:15:47", "link": "http://arxiv.org/abs/2108.03973v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aspect-based Sentiment Analysis in Document -- FOMC Meeting Minutes on\n  Economic Projection", "abstract": "The Federal Open Market Committee within the Federal Reserve System is\nresponsible for managing inflation, maximizing employment, and stabilizing\ninterest rates. Meeting minutes play an important role for market movements\nbecause they provide the birds eye view of how this economic complexity is\nconstantly re-weighed. Therefore, There has been growing interest in analyzing\nand extracting sentiments on various aspects from large financial texts for\neconomic projection. However, Aspect-based Sentiment Analysis is not widely\nused on financial data due to the lack of large labeled dataset. In this paper,\nI propose a model to train ABSA on financial documents under weak supervision\nand analyze its predictive power on various macroeconomic indicators.", "published": "2021-08-09 14:29:58", "link": "http://arxiv.org/abs/2108.04080v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COMPARE: A Taxonomy and Dataset of Comparison Discussions in Peer\n  Reviews", "abstract": "Comparing research papers is a conventional method to demonstrate progress in\nexperimental research. We present COMPARE, a taxonomy and a dataset of\ncomparison discussions in peer reviews of research papers in the domain of\nexperimental deep learning. From a thorough observation of a large set of\nreview sentences, we build a taxonomy of categories in comparison discussions\nand present a detailed annotation scheme to analyze this. Overall, we annotate\n117 reviews covering 1,800 sentences. We experiment with various methods to\nidentify comparison sentences in peer reviews and report a maximum F1 Score of\n0.49. We also pretrain two language models specifically on ML, NLP, and CV\npaper abstracts and reviews to learn informative representations of peer\nreviews. The annotated dataset and the pretrained models are available at\nhttps://github.com/shruti-singh/COMPARE .", "published": "2021-08-09 21:24:28", "link": "http://arxiv.org/abs/2108.04366v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Transferability of Neural Models of Morphological Analogies", "abstract": "Analogical proportions are statements expressed in the form \"A is to B as C\nis to D\" and are used for several reasoning and classification tasks in\nartificial intelligence and natural language processing (NLP). In this paper,\nwe focus on morphological tasks and we propose a deep learning approach to\ndetect morphological analogies. We present an empirical study to see how our\nframework transfers across languages, and that highlights interesting\nsimilarities and differences between these languages. In view of these results,\nwe also discuss the possibility of building a multilingual morphological model.", "published": "2021-08-09 11:08:33", "link": "http://arxiv.org/abs/2108.03938v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-modal Retrieval of Tables and Texts Using Tri-encoder Models", "abstract": "Open-domain extractive question answering works well on textual data by first\nretrieving candidate texts and then extracting the answer from those\ncandidates. However, some questions cannot be answered by text alone but\nrequire information stored in tables. In this paper, we present an approach for\nretrieving both texts and tables relevant to a question by jointly encoding\ntexts, tables and questions into a single vector space. To this end, we create\na new multi-modal dataset based on text and table datasets from related work\nand compare the retrieval performance of different encoding schemata. We find\nthat dense vector embeddings of transformer models outperform sparse embeddings\non four out of six evaluation datasets. Comparing different dense embedding\nmodels, tri-encoders with one encoder for each question, text and table,\nincrease retrieval performance compared to bi-encoders with one encoder for the\nquestion and one for both text and tables. We release the newly created\nmulti-modal dataset to the community so that it can be used for training and\nevaluation.", "published": "2021-08-09 14:02:00", "link": "http://arxiv.org/abs/2108.04049v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Noisy Channel Language Model Prompting for Few-Shot Text Classification", "abstract": "We introduce a noisy channel approach for language model prompting in\nfew-shot text classification. Instead of computing the likelihood of the label\ngiven the input (referred as direct models), channel models compute the\nconditional probability of the input given the label, and are thereby required\nto explain every word in the input. We use channel models for recently proposed\nfew-shot learning methods with no or very limited updates to the language model\nparameters, via either in-context demonstration or prompt tuning. Our\nexperiments show that, for both methods, channel models significantly\noutperform their direct counterparts, which we attribute to their stability,\ni.e., lower variance and higher worst-case accuracy. We also present extensive\nablations that provide recommendations for when to use channel prompt tuning\ninstead of other competitive methods (e.g., direct head tuning): channel prompt\ntuning is preferred when the number of training examples is small, labels in\nthe training data are imbalanced, or generalization to unseen labels is\nrequired.", "published": "2021-08-09 15:06:26", "link": "http://arxiv.org/abs/2108.04106v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Making Transformers Solve Compositional Tasks", "abstract": "Several studies have reported the inability of Transformer models to\ngeneralize compositionally, a key type of generalization in many NLP tasks such\nas semantic parsing. In this paper we explore the design space of Transformer\nmodels showing that the inductive biases given to the model by several design\ndecisions significantly impact compositional generalization. Through this\nexploration, we identified Transformer configurations that generalize\ncompositionally significantly better than previously reported in the literature\nin a diverse set of compositional tasks, and that achieve state-of-the-art\nresults in a semantic parsing compositional generalization benchmark (COGS),\nand a string edit operation composition benchmark (PCFG).", "published": "2021-08-09 22:38:29", "link": "http://arxiv.org/abs/2108.04378v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "An Interpretable Approach to Hateful Meme Detection", "abstract": "Hateful memes are an emerging method of spreading hate on the internet,\nrelying on both images and text to convey a hateful message. We take an\ninterpretable approach to hateful meme detection, using machine learning and\nsimple heuristics to identify the features most important to classifying a meme\nas hateful. In the process, we build a gradient-boosted decision tree and an\nLSTM-based model that achieve comparable performance (73.8 validation and 72.7\ntest auROC) to the gold standard of humans and state-of-the-art transformer\nmodels on this challenging task.", "published": "2021-08-09 18:28:56", "link": "http://arxiv.org/abs/2108.10069v1", "categories": ["cs.LG", "cs.CL", "I.2.7; I.2.10"], "primary_category": "cs.LG"}
{"title": "KGAP: Knowledge Graph Augmented Political Perspective Detection in News\n  Media", "abstract": "Identifying political perspectives in news media has become an important task\ndue to the rapid growth of political commentary and the increasingly polarized\npolitical ideologies. Previous approaches focus on textual content and leave\nout the rich social and political context that is essential in the perspective\ndetection process. To address this limitation, we propose KGAP, a political\nperspective detection method that incorporates external domain knowledge.\nSpecifically, we construct a political knowledge graph to serve as\ndomain-specific external knowledge. We then construct heterogeneous information\nnetworks to represent news documents, which jointly model news text and\nexternal knowledge. Finally, we adopt relational graph neural networks and\nconduct political perspective detection as graph-level classification.\nExtensive experiments demonstrate that our method consistently achieves the\nbest performance on two real-world perspective detection benchmarks. Ablation\nstudies further bear out the necessity of external knowledge and the\neffectiveness of our graph-based approach.", "published": "2021-08-09 08:05:56", "link": "http://arxiv.org/abs/2108.03861v4", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Legislator Representation Learning with Social Context and Expert\n  Knowledge", "abstract": "Modeling the ideological perspectives of political actors is an essential\ntask in computational political science with applications in many downstream\ntasks. Existing approaches are generally limited to textual data and voting\nrecords, while they neglect the rich social context and valuable expert\nknowledge for holistic evaluation. In this paper, we propose a representation\nlearning framework of political actors that jointly leverages social context\nand expert knowledge. Specifically, we retrieve and extract factual statements\nabout legislators to leverage social context information. We then construct a\nheterogeneous information network to incorporate social context and use\nrelational graph neural networks to learn legislator representations. Finally,\nwe train our model with three objectives to align representation learning with\nexpert knowledge, model ideological stance consistency, and simulate the echo\nchamber phenomenon. Extensive experiments demonstrate that our learned\nrepresentations successfully advance the state-of-the-art in three downstream\ntasks. Further analysis proves the correlation between learned legislator\nrepresentations and various socio-political factors, as well as bearing out the\nnecessity of social context and expert knowledge in modeling political actors.", "published": "2021-08-09 08:59:43", "link": "http://arxiv.org/abs/2108.03881v3", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "A Neural Approach for Detecting Morphological Analogies", "abstract": "Analogical proportions are statements of the form \"A is to B as C is to D\"\nthat are used for several reasoning and classification tasks in artificial\nintelligence and natural language processing (NLP). For instance, there are\nanalogy based approaches to semantics as well as to morphology. In fact,\nsymbolic approaches were developed to solve or to detect analogies between\ncharacter strings, e.g., the axiomatic approach as well as that based on\nKolmogorov complexity. In this paper, we propose a deep learning approach to\ndetect morphological analogies, for instance, with reinflexion or conjugation.\nWe present empirical results that show that our framework is competitive with\nthe above-mentioned state of the art symbolic approaches. We also explore\nempirically its transferability capacity across languages, which highlights\ninteresting similarities between them.", "published": "2021-08-09 11:21:55", "link": "http://arxiv.org/abs/2108.03945v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Image Retrieval on Real-life Images with Pre-trained Vision-and-Language\n  Models", "abstract": "We extend the task of composed image retrieval, where an input query consists\nof an image and short textual description of how to modify the image. Existing\nmethods have only been applied to non-complex images within narrow domains,\nsuch as fashion products, thereby limiting the scope of study on in-depth\nvisual reasoning in rich image and language contexts. To address this issue, we\ncollect the Compose Image Retrieval on Real-life images (CIRR) dataset, which\nconsists of over 36,000 pairs of crowd-sourced, open-domain images with\nhuman-generated modifying text. To extend current methods to the open-domain,\nwe propose CIRPLANT, a transformer based model that leverages rich pre-trained\nvision-and-language (V&L) knowledge for modifying visual features conditioned\non natural language. Retrieval is then done by nearest neighbor lookup on the\nmodified features. We demonstrate that with a relatively simple architecture,\nCIRPLANT outperforms existing methods on open-domain images, while matching\nstate-of-the-art accuracy on the existing narrow datasets, such as fashion.\nTogether with the release of CIRR, we believe this work will inspire further\nresearch on composed image retrieval.", "published": "2021-08-09 13:25:06", "link": "http://arxiv.org/abs/2108.04024v1", "categories": ["cs.CV", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "Disentangling Hate in Online Memes", "abstract": "Hateful and offensive content detection has been extensively explored in a\nsingle modality such as text. However, such toxic information could also be\ncommunicated via multimodal content such as online memes. Therefore, detecting\nmultimodal hateful content has recently garnered much attention in academic and\nindustry research communities. This paper aims to contribute to this emerging\nresearch topic by proposing DisMultiHate, which is a novel framework that\nperformed the classification of multimodal hateful content. Specifically,\nDisMultiHate is designed to disentangle target entities in multimodal memes to\nimprove hateful content classification and explainability. We conduct extensive\nexperiments on two publicly available hateful and offensive memes datasets. Our\nexperiment results show that DisMultiHate is able to outperform\nstate-of-the-art unimodal and multimodal baselines in the hateful meme\nclassification task. Empirical case studies were also conducted to demonstrate\nDisMultiHate's ability to disentangle target entities in memes and ultimately\nshowcase DisMultiHate's explainability of the multimodal hateful content\nclassification task.", "published": "2021-08-09 06:46:28", "link": "http://arxiv.org/abs/2108.06207v1", "categories": ["cs.IR", "cs.CL", "cs.MM"], "primary_category": "cs.IR"}
{"title": "FiLMing Multimodal Sarcasm Detection with Attention", "abstract": "Sarcasm detection identifies natural language expressions whose intended\nmeaning is different from what is implied by its surface meaning. It finds\napplications in many NLP tasks such as opinion mining, sentiment analysis, etc.\nToday, social media has given rise to an abundant amount of multimodal data\nwhere users express their opinions through text and images. Our paper aims to\nleverage multimodal data to improve the performance of the existing systems for\nsarcasm detection. So far, various approaches have been proposed that uses text\nand image modality and a fusion of both. We propose a novel architecture that\nuses the RoBERTa model with a co-attention layer on top to incorporate context\nincongruity between input text and image attributes. Further, we integrate\nfeature-wise affine transformation by conditioning the input image through\nFiLMed ResNet blocks with the textual features using the GRU network to capture\nthe multimodal information. The output from both the models and the CLS token\nfrom RoBERTa is concatenated and used for the final prediction. Our results\ndemonstrate that our proposed model outperforms the existing state-of-the-art\nmethod by 6.14% F1 score on the public Twitter multimodal sarcasm detection\ndataset.", "published": "2021-08-09 06:33:29", "link": "http://arxiv.org/abs/2110.00416v1", "categories": ["cs.MM", "cs.CL", "cs.LG"], "primary_category": "cs.MM"}
{"title": "GAN Computers Generate Arts? A Survey on Visual Arts, Music, and\n  Literary Text Generation using Generative Adversarial Network", "abstract": "\"Art is the lie that enables us to realize the truth.\" - Pablo Picasso. For\ncenturies, humans have dedicated themselves to producing arts to convey their\nimagination. The advancement in technology and deep learning in particular, has\ncaught the attention of many researchers trying to investigate whether art\ngeneration is possible by computers and algorithms. Using generative\nadversarial networks (GANs), applications such as synthesizing photorealistic\nhuman faces and creating captions automatically from images were realized. This\nsurvey takes a comprehensive look at the recent works using GANs for generating\nvisual arts, music, and literary text. A performance comparison and description\nof the various GAN architecture are also presented. Finally, some of the key\nchallenges in art generation using GANs are highlighted along with\nrecommendations for future work.", "published": "2021-08-09 07:59:04", "link": "http://arxiv.org/abs/2108.03857v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "stat.ML"], "primary_category": "cs.AI"}
{"title": "End-to-End Speech Recognition With Joint Dereverberation Of Sub-Band\n  Autoregressive Envelopes", "abstract": "The end-to-end (E2E) automatic speech recognition (ASR) systems are often\nrequired to operate in reverberant conditions, where the long-term sub-band\nenvelopes of the speech are temporally smeared. In this paper, we develop a\nfeature enhancement approach using a neural model operating on sub-band\ntemporal envelopes. The temporal envelopes are modeled using the framework of\nfrequency domain linear prediction (FDLP). The neural enhancement model\nproposed in this paper performs an envelope gain based enhancement of temporal\nenvelopes. The model architecture consists of a combination of convolutional\nand long short term memory (LSTM) neural network layers. Further, the envelope\ndereverberation, feature extraction and acoustic modeling using transformer\nbased E2E ASR can all be jointly optimized for the speech recognition task. The\njoint optimization ensures that the dereverberation model targets the ASR cost\nfunction. We perform E2E speech recognition experiments on the REVERB challenge\ndataset as well as on the VOiCES dataset. In these experiments, the proposed\njoint modeling approach yields significant improvements compared to the\nbaseline E2E ASR system (average relative improvements of 21% on the REVERB\nchallenge dataset and about 10% on the VOiCES dataset).", "published": "2021-08-09 12:20:01", "link": "http://arxiv.org/abs/2108.03975v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Time-Frequency Localization Using Deep Convolutional Maxout Neural\n  Network in Persian Speech Recognition", "abstract": "In this paper, a CNN-based structure for the time-frequency localization of\ninformation is proposed for Persian speech recognition. Research has shown that\nthe receptive fields' spectrotemporal plasticity of some neurons in mammals'\nprimary auditory cortex and midbrain makes localization facilities improve\nrecognition performance. Over the past few years, much work has been done to\nlocalize time-frequency information in ASR systems, using the spatial or\ntemporal immutability properties of methods such as HMMs, TDNNs, CNNs, and\nLSTM-RNNs. However, most of these models have large parameter volumes and are\nchallenging to train. For this purpose, we have presented a structure called\nTime-Frequency Convolutional Maxout Neural Network (TFCMNN) in which parallel\ntime-domain and frequency-domain 1D-CMNNs are applied simultaneously and\nindependently to the spectrogram, and then their outputs are concatenated and\napplied jointly to a fully connected Maxout network for classification. To\nimprove the performance of this structure, we have used newly developed methods\nand models such as Dropout, maxout, and weight normalization. Two sets of\nexperiments were designed and implemented on the FARSDAT dataset to evaluate\nthe performance of this model compared to conventional 1D-CMNN models.\nAccording to the experimental results, the average recognition score of TFCMNN\nmodels is about 1.6% higher than the average of conventional 1D-CMNN models. In\naddition, the average training time of the TFCMNN models is about 17 hours\nlower than the average training time of traditional models. Therefore, as\nproven in other sources, time-frequency localization in ASR systems increases\nsystem accuracy and speeds up the training process.", "published": "2021-08-09 05:46:58", "link": "http://arxiv.org/abs/2108.03818v4", "categories": ["cs.SD", "cs.AI", "eess.AS", "I.2.7"], "primary_category": "cs.SD"}
{"title": "A Streamwise GAN Vocoder for Wideband Speech Coding at Very Low Bit Rate", "abstract": "Recently, GAN vocoders have seen rapid progress in speech synthesis, starting\nto outperform autoregressive models in perceptual quality with much higher\ngeneration speed. However, autoregressive vocoders are still the common choice\nfor neural generation of speech signals coded at very low bit rates. In this\npaper, we present a GAN vocoder which is able to generate wideband speech\nwaveforms from parameters coded at 1.6 kbit/s. The proposed model is a modified\nversion of the StyleMelGAN vocoder that can run in frame-by-frame manner,\nmaking it suitable for streaming applications. The experimental results show\nthat the proposed model significantly outperforms prior autoregressive vocoders\nlike LPCNet for very low bit rate speech coding, with computational complexity\nof about 5 GMACs, providing a new state of the art in this domain. Moreover,\nthis streamwise adversarial vocoder delivers quality competitive to advanced\nspeech codecs such as EVS at 5.9 kbit/s on clean speech, which motivates\nfurther usage of feed-forward fully-convolutional models for low bit rate\nspeech coding.", "published": "2021-08-09 14:03:07", "link": "http://arxiv.org/abs/2108.04051v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "AnyoneNet: Synchronized Speech and Talking Head Generation for Arbitrary\n  Person", "abstract": "Automatically generating videos in which synthesized speech is synchronized\nwith lip movements in a talking head has great potential in many human-computer\ninteraction scenarios. In this paper, we present an automatic method to\ngenerate synchronized speech and talking-head videos on the basis of text and a\nsingle face image of an arbitrary person as input. In contrast to previous\ntext-driven talking head generation methods, which can only synthesize the\nvoice of a specific person, the proposed method is capable of synthesizing\nspeech for any person that is inaccessible in the training stage. Specifically,\nthe proposed method decomposes the generation of synchronized speech and\ntalking head videos into two stages, i.e., a text-to-speech (TTS) stage and a\nspeech-driven talking head generation stage. The proposed TTS module is a\nface-conditioned multi-speaker TTS model that gets the speaker identity\ninformation from face images instead of speech, which allows us to synthesize a\npersonalized voice on the basis of the input face image. To generate the\ntalking head videos from the face images, a facial landmark-based method that\ncan predict both lip movements and head rotations is proposed. Extensive\nexperiments demonstrate that the proposed method is able to generate\nsynchronized speech and talking head videos for arbitrary persons and\nnon-persons. Synthesized speech shows consistency with the given face regarding\nto the synthesized voice's timbre and one's appearance in the image, and the\nproposed landmark-based talking head method outperforms the state-of-the-art\nlandmark-based method on generating natural talking head videos.", "published": "2021-08-09 19:58:38", "link": "http://arxiv.org/abs/2108.04325v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
