{"title": "Recurrent Dropout without Memory Loss", "abstract": "This paper presents a novel approach to recurrent neural network (RNN)\nregularization. Differently from the widely adopted dropout method, which is\napplied to \\textit{forward} connections of feed-forward architectures or RNNs,\nwe propose to drop neurons directly in \\textit{recurrent} connections in a way\nthat does not cause loss of long-term memory. Our approach is as easy to\nimplement and apply as the regular feed-forward dropout and we demonstrate its\neffectiveness for Long Short-Term Memory network, the most popular type of RNN\ncells. Our experiments on NLP benchmarks show consistent improvements even when\ncombined with conventional feed-forward dropout.", "published": "2016-03-16 14:33:47", "link": "http://arxiv.org/abs/1603.05118v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparing Convolutional Neural Networks to Traditional Models for Slot\n  Filling", "abstract": "We address relation classification in the context of slot filling, the task\nof finding and evaluating fillers like \"Steve Jobs\" for the slot X in \"X\nfounded Apple\". We propose a convolutional neural network which splits the\ninput sentence into three parts according to the relation arguments and compare\nit to state-of-the-art and traditional approaches of relation classification.\nFinally, we combine different methods and show that the combination is better\nthan individual approaches. We also analyze the effect of genre differences on\nperformance.", "published": "2016-03-16 16:02:03", "link": "http://arxiv.org/abs/1603.05157v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
