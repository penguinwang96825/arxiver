{"title": "FinBERT: A Pretrained Language Model for Financial Communications", "abstract": "Contextual pretrained language models, such as BERT (Devlin et al., 2019),\nhave made significant breakthrough in various NLP tasks by training on large\nscale of unlabeled text re-sources.Financial sector also accumulates large\namount of financial communication text.However, there is no pretrained finance\nspecific language models available. In this work,we address the need by\npretraining a financial domain specific BERT models, FinBERT, using a large\nscale of financial communication corpora. Experiments on three financial\nsentiment classification tasks confirm the advantage of FinBERT over generic\ndomain BERT model. The code and pretrained models are available at\nhttps://github.com/yya518/FinBERT. We hope this will be useful for\npractitioners and researchers working on financial NLP tasks.", "published": "2020-06-15 02:51:06", "link": "http://arxiv.org/abs/2006.08097v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evidence-Aware Inferential Text Generation with Vector Quantised\n  Variational AutoEncoder", "abstract": "Generating inferential texts about an event in different perspectives\nrequires reasoning over different contexts that the event occurs. Existing\nworks usually ignore the context that is not explicitly provided, resulting in\na context-independent semantic representation that struggles to support the\ngeneration. To address this, we propose an approach that automatically finds\nevidence for an event from a large text corpus, and leverages the evidence to\nguide the generation of inferential texts. Our approach works in an\nencoder-decoder manner and is equipped with a Vector Quantised-Variational\nAutoencoder, where the encoder outputs representations from a distribution over\ndiscrete variables. Such discrete representations enable automatically\nselecting relevant evidence, which not only facilitates evidence-aware\ngeneration, but also provides a natural way to uncover rationales behind the\ngeneration. Our approach provides state-of-the-art performance on both\nEvent2Mind and ATOMIC datasets. More importantly, we find that with discrete\nrepresentations, our model selectively uses evidence to generate different\ninferential texts.", "published": "2020-06-15 02:59:52", "link": "http://arxiv.org/abs/2006.08101v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting N-ary Cross-sentence Relations using Constrained Subsequence\n  Kernel", "abstract": "Most of the past work in relation extraction deals with relations occurring\nwithin a sentence and having only two entity arguments. We propose a new\nformulation of the relation extraction task where the relations are more\ngeneral than intra-sentence relations in the sense that they may span multiple\nsentences and may have more than two arguments. Moreover, the relations are\nmore specific than corpus-level relations in the sense that their scope is\nlimited only within a document and not valid globally throughout the corpus. We\npropose a novel sequence representation to characterize instances of such\nrelations. We then explore various classifiers whose features are derived from\nthis sequence representation. For SVM classifier, we design a Constrained\nSubsequence Kernel which is a variant of Generalized Subsequence Kernel. We\nevaluate our approach on three datasets across two domains: biomedical and\ngeneral domain.", "published": "2020-06-15 07:23:58", "link": "http://arxiv.org/abs/2006.08185v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-grained Human Evaluation of Transformer and Recurrent Approaches to\n  Neural Machine Translation for English-to-Chinese", "abstract": "This research presents a fine-grained human evaluation to compare the\nTransformer and recurrent approaches to neural machine translation (MT), on the\ntranslation direction English-to-Chinese. To this end, we develop an error\ntaxonomy compliant with the Multidimensional Quality Metrics (MQM) framework\nthat is customised to the relevant phenomena of this translation direction. We\nthen conduct an error annotation using this customised error taxonomy on the\noutput of state-of-the-art recurrent- and Transformer-based MT systems on a\nsubset of WMT2019's news test set. The resulting annotation shows that,\ncompared to the best recurrent system, the best Transformer system results in a\n31% reduction of the total number of errors and it produced significantly less\nerrors in 10 out of 22 error categories. We also note that two of the systems\nevaluated do not produce any error for a category that was relevant for this\ntranslation direction prior to the advent of NMT systems: Chinese classifiers.", "published": "2020-06-15 11:47:00", "link": "http://arxiv.org/abs/2006.08297v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Catplayinginthesnow: Impact of Prior Segmentation on a Model of Visually\n  Grounded Speech", "abstract": "The language acquisition literature shows that children do not build their\nlexicon by segmenting the spoken input into phonemes and then building up words\nfrom them, but rather adopt a top-down approach and start by segmenting\nword-like units and then break them down into smaller units. This suggests that\nthe ideal way of learning a language is by starting from full semantic units.\nIn this paper, we investigate if this is also the case for a neural model of\nVisually Grounded Speech trained on a speech-image retrieval task. We evaluated\nhow well such a network is able to learn a reliable speech-to-image mapping\nwhen provided with phone, syllable, or word boundary information. We present a\nsimple way to introduce such information into an RNN-based model and\ninvestigate which type of boundary is the most efficient. We also explore at\nwhich level of the network's architecture such information should be introduced\nso as to maximise its performances. Finally, we show that using multiple\nboundary types at once in a hierarchical structure, by which low-level segments\nare used to recompose high-level segments, is beneficial and yields better\nresults than using low-level or high-level segments in isolation.", "published": "2020-06-15 13:20:13", "link": "http://arxiv.org/abs/2006.08387v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DynE: Dynamic Ensemble Decoding for Multi-Document Summarization", "abstract": "Sequence-to-sequence (s2s) models are the basis for extensive work in natural\nlanguage processing. However, some applications, such as multi-document\nsummarization, multi-modal machine translation, and the automatic post-editing\nof machine translation, require mapping a set of multiple distinct inputs into\na single output sequence. Recent work has introduced bespoke architectures for\nthese multi-input settings, and developed models which can handle increasingly\nlonger inputs; however, the performance of special model architectures is\nlimited by the available in-domain training data. In this work we propose a\nsimple decoding methodology which ensembles the output of multiple instances of\nthe same model on different inputs. Our proposed approach allows models trained\nfor vanilla s2s tasks to be directly used in multi-input settings. This works\nparticularly well when each of the inputs has significant overlap with the\nothers, as when compressing a cluster of news articles about the same event\ninto a single coherent summary, and we obtain state-of-the-art results on\nseveral multi-document summarization datasets.", "published": "2020-06-15 20:40:06", "link": "http://arxiv.org/abs/2006.08748v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Multi-Property Extraction and Beyond", "abstract": "In this paper, we investigate the Dual-source Transformer architecture on the\nWikiReading information extraction and machine reading comprehension dataset.\nThe proposed model outperforms the current state-of-the-art by a large margin.\nNext, we introduce WikiReading Recycled - a newly developed public dataset,\nsupporting the task of multiple property extraction. It keeps the spirit of the\noriginal WikiReading but does not inherit the identified disadvantages of its\npredecessor.", "published": "2020-06-15 11:07:52", "link": "http://arxiv.org/abs/2006.08281v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Regularized Forward-Backward Decoder for Attention Models", "abstract": "Nowadays, attention models are one of the popular candidates for speech\nrecognition. So far, many studies mainly focus on the encoder structure or the\nattention module to enhance the performance of these models. However, mostly\nignore the decoder. In this paper, we propose a novel regularization technique\nincorporating a second decoder during the training phase. This decoder is\noptimized on time-reversed target labels beforehand and supports the standard\ndecoder during training by adding knowledge from future context. Since it is\nonly added during training, we are not changing the basic structure of the\nnetwork or adding complexity during decoding. We evaluate our approach on the\nsmaller TEDLIUMv2 and the larger LibriSpeech dataset, achieving consistent\nimprovements on both of them.", "published": "2020-06-15 16:04:16", "link": "http://arxiv.org/abs/2006.08506v2", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Automatic Validation of Textual Attribute Values in E-commerce Catalog\n  by Learning with Limited Labeled Data", "abstract": "Product catalogs are valuable resources for eCommerce website. In the\ncatalog, a product is associated with multiple attributes whose values are\nshort texts, such as product name, brand, functionality and flavor. Usually\nindividual retailers self-report these key values, and thus the catalog\ninformation unavoidably contains noisy facts. Although existing deep neural\nnetwork models have shown success in conducting cross-checking between two\npieces of texts, their success has to be dependent upon a large set of quality\nlabeled data, which are hard to obtain in this validation task: products span a\nvariety of categories. To address the aforementioned challenges, we propose a\nnovel meta-learning latent variable approach, called MetaBridge, which can\nlearn transferable knowledge from a subset of categories with limited labeled\ndata and capture the uncertainty of never-seen categories with unlabeled data.\nMore specifically, we make the following contributions. (1) We formalize the\nproblem of validating the textual attribute values of products from a variety\nof categories as a natural language inference task in the few-shot learning\nsetting, and propose a meta-learning latent variable model to jointly process\nthe signals obtained from product profiles and textual attribute values. (2) We\npropose to integrate meta learning and latent variable in a unified model to\neffectively capture the uncertainty of various categories. (3) We propose a\nnovel objective function based on latent variable model in the few-shot\nlearning setting, which ensures distribution consistency between unlabeled and\nlabeled data and prevents overfitting by sampling from the learned\ndistribution. Extensive experiments on real eCommerce datasets from hundreds of\ncategories demonstrate the effectiveness of MetaBridge on textual attribute\nvalidation and its outstanding performance compared with state-of-the-art\napproaches.", "published": "2020-06-15 21:31:05", "link": "http://arxiv.org/abs/2006.08779v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Hybrid Natural Language Generation System Integrating Rules and Deep\n  Learning Algorithms", "abstract": "This paper proposes an enhanced natural language generation system combining\nthe merits of both rule-based approaches and modern deep learning algorithms,\nboosting its performance to the extent where the generated textual content is\ncapable of exhibiting agile human-writing styles and the content logic of which\nis highly controllable. We also come up with a novel approach called HMCU to\nmeasure the performance of the natural language processing comprehensively and\nprecisely.", "published": "2020-06-15 00:50:41", "link": "http://arxiv.org/abs/2006.09213v2", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Document Classification for COVID-19 Literature", "abstract": "The global pandemic has made it more important than ever to quickly and\naccurately retrieve relevant scientific literature for effective consumption by\nresearchers in a wide range of fields. We provide an analysis of several\nmulti-label document classification models on the LitCovid dataset, a growing\ncollection of 23,000 research papers regarding the novel 2019 coronavirus. We\nfind that pre-trained language models fine-tuned on this dataset outperform all\nother baselines and that BioBERT surpasses the others by a small margin with\nmicro-F1 and accuracy scores of around 86% and 75% respectively on the test\nset. We evaluate the data efficiency and generalizability of these models as\nessential features of any system prepared to deal with an urgent situation like\nthe current health crisis. Finally, we explore 50 errors made by the best\nperforming models on LitCovid documents and find that they often (1) correlate\ncertain labels too closely together and (2) fail to focus on discriminative\nsections of the articles; both of which are important issues to address in\nfuture work. Both data and code are available on GitHub.", "published": "2020-06-15 20:03:28", "link": "http://arxiv.org/abs/2006.13816v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Exploration of End-to-End ASR for OpenSTT -- Russian Open Speech-to-Text\n  Dataset", "abstract": "This paper presents an exploration of end-to-end automatic speech recognition\nsystems (ASR) for the largest open-source Russian language data set -- OpenSTT.\nWe evaluate different existing end-to-end approaches such as joint\nCTC/Attention, RNN-Transducer, and Transformer. All of them are compared with\nthe strong hybrid ASR system based on LF-MMI TDNN-F acoustic model. For the\nthree available validation sets (phone calls, YouTube, and books), our best\nend-to-end model achieves word error rate (WER) of 34.8%, 19.1%, and 18.1%,\nrespectively. Under the same conditions, the hybridASR system demonstrates\n33.5%, 20.9%, and 18.6% WER.", "published": "2020-06-15 10:35:31", "link": "http://arxiv.org/abs/2006.08274v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SD-RSIC: Summarization Driven Deep Remote Sensing Image Captioning", "abstract": "Deep neural networks (DNNs) have been recently found popular for image\ncaptioning problems in remote sensing (RS). Existing DNN based approaches rely\non the availability of a training set made up of a high number of RS images\nwith their captions. However, captions of training images may contain redundant\ninformation (they can be repetitive or semantically similar to each other),\nresulting in information deficiency while learning a mapping from the image\ndomain to the language domain. To overcome this limitation, in this paper, we\npresent a novel Summarization Driven Remote Sensing Image Captioning (SD-RSIC)\napproach. The proposed approach consists of three main steps. The first step\nobtains the standard image captions by jointly exploiting convolutional neural\nnetworks (CNNs) with long short-term memory (LSTM) networks. The second step,\nunlike the existing RS image captioning methods, summarizes the ground-truth\ncaptions of each training image into a single caption by exploiting sequence to\nsequence neural networks and eliminates the redundancy present in the training\nset. The third step automatically defines the adaptive weights associated to\neach RS image to combine the standard captions with the summarized captions\nbased on the semantic content of the image. This is achieved by a novel\nadaptive weighting strategy defined in the context of LSTM networks.\nExperimental results obtained on the RSCID, UCM-Captions and Sydney-Captions\ndatasets show the effectiveness of the proposed approach compared to the\nstate-of-the-art RS image captioning approaches. The code of the proposed\napproach is publicly available at\nhttps://gitlab.tubit.tu-berlin.de/rsim/SD-RSIC.", "published": "2020-06-15 14:29:12", "link": "http://arxiv.org/abs/2006.08432v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "To Pretrain or Not to Pretrain: Examining the Benefits of Pretraining on\n  Resource Rich Tasks", "abstract": "Pretraining NLP models with variants of Masked Language Model (MLM)\nobjectives has recently led to a significant improvements on many tasks. This\npaper examines the benefits of pretrained models as a function of the number of\ntraining samples used in the downstream task. On several text classification\ntasks, we show that as the number of training examples grow into the millions,\nthe accuracy gap between finetuning BERT-based model and training vanilla LSTM\nfrom scratch narrows to within 1%. Our findings indicate that MLM-based models\nmight reach a diminishing return point as the supervised data size increases\nsignificantly.", "published": "2020-06-15 18:18:59", "link": "http://arxiv.org/abs/2006.08671v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "On the use of human reference data for evaluating automatic image\n  descriptions", "abstract": "Automatic image description systems are commonly trained and evaluated using\ncrowdsourced, human-generated image descriptions. The best-performing system is\nthen determined using some measure of similarity to the reference data (BLEU,\nMeteor, CIDER, etc). Thus, both the quality of the systems as well as the\nquality of the evaluation depends on the quality of the descriptions. As\nSection 2 will show, the quality of current image description datasets is\ninsufficient. I argue that there is a need for more detailed guidelines that\ntake into account the needs of visually impaired users, but also the\nfeasibility of generating suitable descriptions. With high-quality data,\nevaluation of image description systems could use reference descriptions, but\nwe should also look for alternatives.", "published": "2020-06-15 21:57:27", "link": "http://arxiv.org/abs/2006.08792v1", "categories": ["cs.CL", "cs.CV", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Cooking Is All About People: Comment Classification On Cookery Channels\n  Using BERT and Classification Models (Malayalam-English Mix-Code)", "abstract": "The scope of a lucrative career promoted by Google through its video\ndistribution platform YouTube has attracted a large number of users to become\ncontent creators. An important aspect of this line of work is the feedback\nreceived in the form of comments which show how well the content is being\nreceived by the audience. However, volume of comments coupled with spam and\nlimited tools for comment classification makes it virtually impossible for a\ncreator to go through each and every comment and gather constructive feedback.\nAutomatic classification of comments is a challenge even for established\nclassification models, since comments are often of variable lengths riddled\nwith slang, symbols and abbreviations. This is a greater challenge where\ncomments are multilingual as the messages are often rife with the respective\nvernacular. In this work, we have evaluated top-performing classification\nmodels for classifying comments which are a mix of different combinations of\nEnglish and Malayalam (only English, only Malayalam and Mix of English and\nMalayalam). The statistical analysis of results indicates that Multinomial\nNaive Bayes, K-Nearest Neighbors (KNN), Support Vector Machine (SVM), Random\nForest and Decision Trees offer similar level of accuracy in comment\nclassification. Further, we have also evaluated 3 multilingual transformer\nbased language models (BERT, DISTILBERT and XLM) and compared their performance\nto the traditional machine learning classification techniques. XLM was the\ntop-performing BERT model with an accuracy of 67.31. Random Forest with Term\nFrequency Vectorizer was the best performing model out of all the traditional\nclassification models with an accuracy of 63.59.", "published": "2020-06-15 19:07:06", "link": "http://arxiv.org/abs/2007.04249v3", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "An Iterative Graph Spectral Subtraction Method for Speech Enhancement", "abstract": "In this paper, we investigate the application of graph signal processing\n(GSP) theory in speech enhancement. We first propose a set of shift operators\nto construct graph speech signals, and then analyze their spectrum in the graph\nFourier domain. By leveraging the differences between the spectrum of graph\nspeech and graph noise signals, we further propose the graph spectral\nsubtraction (GSS) method to suppress the noise interference in noisy speech.\nMoreover, based on GSS, we propose the iterative graph spectral subtraction\n(IGSS) method to further improve the speech enhancement performance. Our\nexperimental results show that the proposed operators are suitable for graph\nspeech signals, and the proposed methods outperform the traditional basic\nspectral subtraction (BSS) method and iterative basic spectral subtraction\n(IBSS) method in terms of both signal-to-noise ratios (SNR) and mean Perceptual\nEvaluation of Speech Quality (PESQ).", "published": "2020-06-15 15:55:39", "link": "http://arxiv.org/abs/2006.08497v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Emotion Recognition in Audio and Video Using Deep Neural Networks", "abstract": "Humans are able to comprehend information from multiple domains for e.g.\nspeech, text and visual. With advancement of deep learning technology there has\nbeen significant improvement of speech recognition. Recognizing emotion from\nspeech is important aspect and with deep learning technology emotion\nrecognition has improved in accuracy and latency. There are still many\nchallenges to improve accuracy. In this work, we attempt to explore different\nneural networks to improve accuracy of emotion recognition. With different\narchitectures explored, we find (CNN+RNN) + 3DCNN multi-model architecture\nwhich processes audio spectrograms and corresponding video frames giving\nemotion prediction accuracy of 54.0% among 4 emotions and 71.75% among 3\nemotions using IEMOCAP[2] dataset.", "published": "2020-06-15 04:50:18", "link": "http://arxiv.org/abs/2006.08129v1", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.SD", "I.4.9; I.2.m"], "primary_category": "eess.AS"}
{"title": "COALA: Co-Aligned Autoencoders for Learning Semantically Enriched Audio\n  Representations", "abstract": "Audio representation learning based on deep neural networks (DNNs) emerged as\nan alternative approach to hand-crafted features. For achieving high\nperformance, DNNs often need a large amount of annotated data which can be\ndifficult and costly to obtain. In this paper, we propose a method for learning\naudio representations, aligning the learned latent representations of audio and\nassociated tags. Aligning is done by maximizing the agreement of the latent\nrepresentations of audio and tags, using a contrastive loss. The result is an\naudio embedding model which reflects acoustic and semantic characteristics of\nsounds. We evaluate the quality of our embedding model, measuring its\nperformance as a feature extractor on three different tasks (namely, sound\nevent recognition, and music genre and musical instrument classification), and\ninvestigate what type of characteristics the model captures. Our results are\npromising, sometimes in par with the state-of-the-art in the considered tasks\nand the embeddings produced with our method are well correlated with some\nacoustic descriptors.", "published": "2020-06-15 13:17:18", "link": "http://arxiv.org/abs/2006.08386v2", "categories": ["cs.LG", "cs.IR", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
