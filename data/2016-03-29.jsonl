{"title": "Prepositional Attachment Disambiguation Using Bilingual Parsing and\n  Alignments", "abstract": "In this paper, we attempt to solve the problem of Prepositional Phrase (PP)\nattachments in English. The motivation for the work comes from NLP applications\nlike Machine Translation, for which, getting the correct attachment of\nprepositions is very crucial. The idea is to correct the PP-attachments for a\nsentence with the help of alignments from parallel data in another language.\nThe novelty of our work lies in the formulation of the problem into a dual\ndecomposition based algorithm that enforces agreement between the parse trees\nfrom two languages as a constraint. Experiments were performed on the\nEnglish-Hindi language pair and the performance improved by 10% over the\nbaseline, where the baseline is the attachment predicted by the MSTParser model\ntrained for English.", "published": "2016-03-29 00:06:11", "link": "http://arxiv.org/abs/1603.08594v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What a Nerd! Beating Students and Vector Cosine in the ESL and TOEFL\n  Datasets", "abstract": "In this paper, we claim that Vector Cosine, which is generally considered one\nof the most efficient unsupervised measures for identifying word similarity in\nVector Space Models, can be outperformed by a completely unsupervised measure\nthat evaluates the extent of the intersection among the most associated\ncontexts of two target words, weighting such intersection according to the rank\nof the shared contexts in the dependency ranked lists. This claim comes from\nthe hypothesis that similar words do not simply occur in similar contexts, but\nthey share a larger portion of their most relevant contexts compared to other\nrelated words. To prove it, we describe and evaluate APSyn, a variant of\nAverage Precision that, independently of the adopted parameters, outperforms\nthe Vector Cosine and the co-occurrence on the ESL and TOEFL test sets. In the\nbest setting, APSyn reaches 0.73 accuracy on the ESL dataset and 0.70 accuracy\nin the TOEFL dataset, beating therefore the non-English US college applicants\n(whose average, as reported in the literature, is 64.50%) and several\nstate-of-the-art approaches.", "published": "2016-03-29 10:00:27", "link": "http://arxiv.org/abs/1603.08701v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Nine Features in a Random Forest to Learn Taxonomical Semantic Relations", "abstract": "ROOT9 is a supervised system for the classification of hypernyms, co-hyponyms\nand random words that is derived from the already introduced ROOT13 (Santus et\nal., 2016). It relies on a Random Forest algorithm and nine unsupervised\ncorpus-based features. We evaluate it with a 10-fold cross validation on 9,600\npairs, equally distributed among the three classes and involving several\nParts-Of-Speech (i.e. adjectives, nouns and verbs). When all the classes are\npresent, ROOT9 achieves an F1 score of 90.7%, against a baseline of 57.2%\n(vector cosine). When the classification is binary, ROOT9 achieves the\nfollowing results against the baseline: hypernyms-co-hyponyms 95.7% vs. 69.8%,\nhypernyms-random 91.8% vs. 64.1% and co-hyponyms-random 97.8% vs. 79.4%. In\norder to compare the performance with the state-of-the-art, we have also\nevaluated ROOT9 in subsets of the Weeds et al. (2014) datasets, proving that it\nis in fact competitive. Finally, we investigated whether the system learns the\nsemantic relation or it simply learns the prototypical hypernyms, as claimed by\nLevy et al. (2015). The second possibility seems to be the most likely, even\nthough ROOT9 can be trained on negative examples (i.e., switched hypernyms) to\ndrastically reduce this bias.", "published": "2016-03-29 10:00:40", "link": "http://arxiv.org/abs/1603.08702v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ROOT13: Spotting Hypernyms, Co-Hyponyms and Randoms", "abstract": "In this paper, we describe ROOT13, a supervised system for the classification\nof hypernyms, co-hyponyms and random words. The system relies on a Random\nForest algorithm and 13 unsupervised corpus-based features. We evaluate it with\na 10-fold cross validation on 9,600 pairs, equally distributed among the three\nclasses and involving several Parts-Of-Speech (i.e. adjectives, nouns and\nverbs). When all the classes are present, ROOT13 achieves an F1 score of 88.3%,\nagainst a baseline of 57.6% (vector cosine). When the classification is binary,\nROOT13 achieves the following results: hypernyms-co-hyponyms (93.4% vs. 60.2%),\nhypernymsrandom (92.3% vs. 65.5%) and co-hyponyms-random (97.3% vs. 81.5%). Our\nresults are competitive with stateof-the-art models.", "published": "2016-03-29 10:05:05", "link": "http://arxiv.org/abs/1603.08705v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compilation as a Typed EDSL-to-EDSL Transformation", "abstract": "This article is about an implementation and compilation technique that is\nused in RAW-Feldspar which is a complete rewrite of the Feldspar embedded\ndomain-specific language (EDSL) (Axelsson et al. 2010). Feldspar is high-level\nfunctional language that generates efficient C code to run on embedded targets.\nThe gist of the technique presented in this post is the following: rather\nwriting a back end that converts pure Feldspar expressions directly to C, we\ntranslate them to a low-level monadic EDSL. From the low-level EDSL, C code is\nthen generated. This approach has several advantages:\n  1. The translation is simpler to write than a complete C back end.\n  2. The translation is between two typed EDSLs, which rules out many potential\nerrors.\n  3. The low-level EDSL is reusable and can be shared between several\nhigh-level EDSLs.\n  Although the article contains a lot of code, most of it is in fact reusable.\nAs mentioned in Discussion, we can write the same implementation in less than\n50 lines of code using generic libraries that we have developed to support\nFeldspar.", "published": "2016-03-29 17:58:46", "link": "http://arxiv.org/abs/1603.08865v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Readable Read: Automatic Assessment of Language Learning Materials\n  based on Linguistic Complexity", "abstract": "Corpora and web texts can become a rich language learning resource if we have\na means of assessing whether they are linguistically appropriate for learners\nat a given proficiency level. In this paper, we aim at addressing this issue by\npresenting the first approach for predicting linguistic complexity for Swedish\nsecond language learning material on a 5-point scale. After showing that the\ntraditional Swedish readability measure, L\\\"asbarhetsindex (LIX), is not\nsuitable for this task, we propose a supervised machine learning model, based\non a range of linguistic features, that can reliably classify texts according\nto their difficulty level. Our model obtained an accuracy of 81.3% and an\nF-score of 0.8, which is comparable to the state of the art in English and is\nconsiderably higher than previously reported results for other languages. We\nfurther studied the utility of our features with single sentences instead of\nfull texts since sentences are a common linguistic unit in language learning\nexercises. We trained a separate model on sentence-level data with five\nclasses, which yielded 63.4% accuracy. Although this is lower than the document\nlevel performance, we achieved an adjacent accuracy of 92%. Furthermore, we\nfound that using a combination of different features, compared to using lexical\nfeatures alone, resulted in 7% improvement in classification accuracy at the\nsentence level, whereas at the document level, lexical features were more\ndominant. Our models are intended for use in a freely accessible web-based\nlanguage learning platform for the automatic generation of exercises.", "published": "2016-03-29 18:12:28", "link": "http://arxiv.org/abs/1603.08868v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data", "abstract": "Understanding unstructured text is a major goal within natural language\nprocessing. Comprehension tests pose questions based on short text passages to\nevaluate such understanding. In this work, we investigate machine comprehension\non the challenging {\\it MCTest} benchmark. Partly because of its limited size,\nprior work on {\\it MCTest} has focused mainly on engineering better features.\nWe tackle the dataset with a neural approach, harnessing simple neural networks\narranged in a parallel hierarchy. The parallel hierarchy enables our model to\ncompare the passage, question, and answer from a variety of trainable\nperspectives, as opposed to using a manually designed, rigid feature set.\nPerspectives range from the word level to sentence fragments to sequences of\nsentences; the networks operate only on word-embedding representations of text.\nWhen trained with a methodology designed to help cope with limited training\ndata, our Parallel-Hierarchical model sets a new state of the art for {\\it\nMCTest}, outperforming previous feature-engineered approaches slightly and\nprevious neural approaches by a significant margin (over 15\\% absolute).", "published": "2016-03-29 18:52:46", "link": "http://arxiv.org/abs/1603.08884v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Learning-Based Single-Document Summarization with Compression and\n  Anaphoricity Constraints", "abstract": "We present a discriminative model for single-document summarization that\nintegrally combines compression and anaphoricity constraints. Our model selects\ntextual units to include in the summary based on a rich set of sparse features\nwhose weights are learned on a large corpus. We allow for the deletion of\ncontent within a sentence when that deletion is licensed by compression rules;\nin our framework, these are implemented as dependencies between subsentential\nunits of text. Anaphoricity constraints then improve cross-sentence coherence\nby guaranteeing that, for each pronoun included in the summary, the pronoun's\nantecedent is included as well or the pronoun is rewritten as a full mention.\nWhen trained end-to-end, our final system outperforms prior work on both ROUGE\nas well as on human judgments of linguistic quality.", "published": "2016-03-29 18:58:42", "link": "http://arxiv.org/abs/1603.08887v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards an Automated Requirements-driven Development of Smart\n  Cyber-Physical Systems", "abstract": "The Invariant Refinement Method for Self Adaptation (IRM-SA) is a design\nmethod targeting development of smart Cyber-Physical Systems (sCPS). It allows\nfor a systematic translation of the system requirements into the system\narchitecture expressed as an ensemble-based component system (EBCS). However,\nsince the requirements are captured using natural language, there exists the\ndanger of their misinterpretation due to natural language requirements'\nambiguity, which could eventually lead to design errors. Thus, automation and\nvalidation of the design process is desirable. In this paper, we (i) analyze\nthe translation process of natural language requirements into the IRM-SA model,\n(ii) identify individual steps that can be automated and/or validated using\nnatural language processing techniques, and (iii) propose suitable methods.", "published": "2016-03-29 04:34:39", "link": "http://arxiv.org/abs/1603.08636v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Shirtless and Dangerous: Quantifying Linguistic Signals of Gender Bias\n  in an Online Fiction Writing Community", "abstract": "Imagine a princess asleep in a castle, waiting for her prince to slay the\ndragon and rescue her. Tales like the famous Sleeping Beauty clearly divide up\ngender roles. But what about more modern stories, borne of a generation\nincreasingly aware of social constructs like sexism and racism? Do these\nstories tend to reinforce gender stereotypes, or counter them? In this paper,\nwe present a technique that combines natural language processing with a\ncrowdsourced lexicon of stereotypes to capture gender biases in fiction. We\napply this technique across 1.8 billion words of fiction from the Wattpad\nonline writing community, investigating gender representation in stories, how\nmale and female characters behave and are described, and how authors' use of\ngender stereotypes is associated with the community's ratings. We find that\nmale over-representation and traditional gender stereotypes (e.g., dominant men\nand submissive women) are common throughout nearly every genre in our corpus.\nHowever, only some of these stereotypes, like sexual or violent men, are\nassociated with highly rated stories. Finally, despite women often being the\ntarget of negative stereotypes, female authors are equally likely to write such\nstereotypes as men.", "published": "2016-03-29 16:24:46", "link": "http://arxiv.org/abs/1603.08832v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
