{"title": "A Deep Generative Framework for Paraphrase Generation", "abstract": "Paraphrase generation is an important problem in NLP, especially in question\nanswering, information retrieval, information extraction, conversation systems,\nto name a few. In this paper, we address the problem of generating paraphrases\nautomatically. Our proposed method is based on a combination of deep generative\nmodels (VAE) with sequence-to-sequence models (LSTM) to generate paraphrases,\ngiven an input sentence. Traditional VAEs when combined with recurrent neural\nnetworks can generate free text but they are not suitable for paraphrase\ngeneration for a given sentence. We address this problem by conditioning the\nboth, encoder and decoder sides of VAE, on the original sentence, so that it\ncan generate the given sentence's paraphrases. Unlike most existing models, our\nmodel is simple, modular and can generate multiple paraphrases, for a given\nsentence. Quantitative evaluation of the proposed method on a benchmark\nparaphrase dataset demonstrates its efficacy, and its performance improvement\nover the state-of-the-art methods by a significant margin, whereas qualitative\nhuman evaluation indicate that the generated paraphrases are well-formed,\ngrammatically correct, and are relevant to the input sentence. Furthermore, we\nevaluate our method on a newly released question paraphrase dataset, and\nestablish a new baseline for future research.", "published": "2017-09-15 06:58:13", "link": "http://arxiv.org/abs/1709.05074v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Aspect Term Extraction with B-LSTM & CRF using\n  Automatically Labelled Datasets", "abstract": "Aspect Term Extraction (ATE) identifies opinionated aspect terms in texts and\nis one of the tasks in the SemEval Aspect Based Sentiment Analysis (ABSA)\ncontest. The small amount of available datasets for supervised ATE and the\ncostly human annotation for aspect term labelling give rise to the need for\nunsupervised ATE. In this paper, we introduce an architecture that achieves\ntop-ranking performance for supervised ATE. Moreover, it can be used\nefficiently as feature extractor and classifier for unsupervised ATE. Our\nsecond contribution is a method to automatically construct datasets for ATE. We\ntrain a classifier on our automatically labelled datasets and evaluate it on\nthe human annotated SemEval ABSA test sets. Compared to a strong rule-based\nbaseline, we obtain a dramatically higher F-score and attain precision values\nabove 80%. Our unsupervised method beats the supervised ABSA baseline from\nSemEval, while preserving high precision scores.", "published": "2017-09-15 08:12:58", "link": "http://arxiv.org/abs/1709.05094v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transcribing Against Time", "abstract": "We investigate the problem of manually correcting errors from an automatic\nspeech transcript in a cost-sensitive fashion. This is done by specifying a\nfixed time budget, and then automatically choosing location and size of\nsegments for correction such that the number of corrected errors is maximized.\nThe core components, as suggested by previous research [1], are a utility model\nthat estimates the number of errors in a particular segment, and a cost model\nthat estimates annotation effort for the segment. In this work we propose a\ndynamic updating framework that allows for the training of cost models during\nthe ongoing transcription process. This removes the need for transcriber\nenrollment prior to the actual transcription, and improves correction\nefficiency by allowing highly transcriber-adaptive cost modeling. We first\nconfirm and analyze the improvements afforded by this method in a simulated\nstudy. We then conduct a realistic user study, observing efficiency\nimprovements of 15% relative on average, and 42% for the participants who\ndeviated most strongly from our initial, transcriber-agnostic cost model.\nMoreover, we find that our updating framework can capture dynamically changing\nfactors, such as transcriber fatigue and topic familiarity, which we observe to\nhave a large influence on the transcriber's working behavior.", "published": "2017-09-15 14:19:29", "link": "http://arxiv.org/abs/1709.05227v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "And That's A Fact: Distinguishing Factual and Emotional Argumentation in\n  Online Dialogue", "abstract": "We investigate the characteristics of factual and emotional argumentation\nstyles observed in online debates. Using an annotated set of \"factual\" and\n\"feeling\" debate forum posts, we extract patterns that are highly correlated\nwith factual and emotional arguments, and then apply a bootstrapping\nmethodology to find new patterns in a larger pool of unannotated forum posts.\nThis process automatically produces a large set of patterns representing\nlinguistic expressions that are highly correlated with factual and emotional\nlanguage. Finally, we analyze the most discriminating patterns to better\nunderstand the defining characteristics of factual and emotional arguments.", "published": "2017-09-15 16:28:46", "link": "http://arxiv.org/abs/1709.05295v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are you serious?: Rhetorical Questions and Sarcasm in Social Media\n  Dialog", "abstract": "Effective models of social dialog must understand a broad range of rhetorical\nand figurative devices. Rhetorical questions (RQs) are a type of figurative\nlanguage whose aim is to achieve a pragmatic goal, such as structuring an\nargument, being persuasive, emphasizing a point, or being ironic. While there\nare computational models for other forms of figurative language, rhetorical\nquestions have received little attention to date. We expand a small dataset\nfrom previous work, presenting a corpus of 10,270 RQs from debate forums and\nTwitter that represent different discourse functions. We show that we can\nclearly distinguish between RQs and sincere questions (0.76 F1). We then show\nthat RQs can be used both sarcastically and non-sarcastically, observing that\nnon-sarcastic (other) uses of RQs are frequently argumentative in forums, and\npersuasive in tweets. We present experiments to distinguish between these uses\nof RQs using SVM and LSTM models that represent linguistic features and\npost-level context, achieving results as high as 0.76 F1 for \"sarcastic\" and\n0.77 F1 for \"other\" in forums, and 0.83 F1 for both \"sarcastic\" and \"other\" in\ntweets. We supplement our quantitative experiments with an in-depth\ncharacterization of the linguistic variation in RQs.", "published": "2017-09-15 16:54:02", "link": "http://arxiv.org/abs/1709.05305v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Harvesting Creative Templates for Generating Stylistically Varied\n  Restaurant Reviews", "abstract": "Many of the creative and figurative elements that make language exciting are\nlost in translation in current natural language generation engines. In this\npaper, we explore a method to harvest templates from positive and negative\nreviews in the restaurant domain, with the goal of vastly expanding the types\nof stylistic variation available to the natural language generator. We learn\nhyperbolic adjective patterns that are representative of the strongly-valenced\nexpressive language commonly used in either positive or negative reviews. We\nthen identify and delexicalize entities, and use heuristics to extract\ngeneration templates from review sentences. We evaluate the learned templates\nagainst more traditional review templates, using subjective measures of\n\"convincingness\", \"interestingness\", and \"naturalness\". Our results show that\nthe learned templates score highly on these measures. Finally, we analyze the\nlinguistic categories that characterize the learned positive and negative\ntemplates. We plan to use the learned templates to improve the conversational\nstyle of dialogue systems in the restaurant domain.", "published": "2017-09-15 16:59:20", "link": "http://arxiv.org/abs/1709.05308v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Creating and Characterizing a Diverse Corpus of Sarcasm in Dialogue", "abstract": "The use of irony and sarcasm in social media allows us to study them at scale\nfor the first time. However, their diversity has made it difficult to construct\na high-quality corpus of sarcasm in dialogue. Here, we describe the process of\ncreating a large- scale, highly-diverse corpus of online debate forums\ndialogue, and our novel methods for operationalizing classes of sarcasm in the\nform of rhetorical questions and hyperbole. We show that we can use\nlexico-syntactic cues to reliably retrieve sarcastic utterances with high\naccuracy. To demonstrate the properties and quality of our corpus, we conduct\nsupervised learning experiments with simple features, and show that we achieve\nboth higher precision and F than previous work on sarcasm in debate forums\ndialogue. We apply a weakly-supervised linguistic pattern learner and\nqualitatively analyze the linguistic differences in each class.", "published": "2017-09-15 21:01:57", "link": "http://arxiv.org/abs/1709.05404v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Search with Structured Data to Create a More Engaging User\n  Experience in Open Domain Dialogue", "abstract": "The greatest challenges in building sophisticated open-domain conversational\nagents arise directly from the potential for ongoing mixed-initiative\nmulti-turn dialogues, which do not follow a particular plan or pursue a\nparticular fixed information need. In order to make coherent conversational\ncontributions in this context, a conversational agent must be able to track the\ntypes and attributes of the entities under discussion in the conversation and\nknow how they are related. In some cases, the agent can rely on structured\ninformation sources to help identify the relevant semantic relations and\nproduce a turn, but in other cases, the only content available comes from\nsearch, and it may be unclear which semantic relations hold between the search\nresults and the discourse context. A further constraint is that the system must\nproduce its contribution to the ongoing conversation in real-time. This paper\ndescribes our experience building SlugBot for the 2017 Alexa Prize, and\ndiscusses how we leveraged search and structured data from different sources to\nhelp SlugBot produce dialogic turns and carry on conversations whose length\nover the semi-finals user evaluation period averaged 8:17 minutes.", "published": "2017-09-15 21:12:59", "link": "http://arxiv.org/abs/1709.05411v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"How May I Help You?\": Modeling Twitter Customer Service Conversations\n  Using Fine-Grained Dialogue Acts", "abstract": "Given the increasing popularity of customer service dialogue on Twitter,\nanalysis of conversation data is essential to understand trends in customer and\nagent behavior for the purpose of automating customer service interactions. In\nthis work, we develop a novel taxonomy of fine-grained \"dialogue acts\"\nfrequently observed in customer service, showcasing acts that are more suited\nto the domain than the more generic existing taxonomies. Using a sequential\nSVM-HMM model, we model conversation flow, predicting the dialogue act of a\ngiven turn in real-time. We characterize differences between customer and agent\nbehavior in Twitter customer service conversations, and investigate the effect\nof testing our system on different customer service industries. Finally, we use\na data-driven approach to predict important conversation outcomes: customer\nsatisfaction, customer frustration, and overall problem resolution. We show\nthat the type and location of certain dialogue acts in a conversation have a\nsignificant effect on the probability of desirable and undesirable outcomes,\nand present actionable rules based on our findings. The patterns and rules we\nderive can be used as guidelines for outcome-driven automated customer service\nplatforms.", "published": "2017-09-15 21:24:48", "link": "http://arxiv.org/abs/1709.05413v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WOAH: Preliminaries to Zero-shot Ontology Learning for Conversational\n  Agents", "abstract": "The present paper presents the Weighted Ontology Approximation Heuristic\n(WOAH), a novel zero-shot approach to ontology estimation for conversational\nagents development environments. This methodology extracts verbs and nouns\nseparately from data by distilling the dependencies obtained and applying\nsimilarity and sparsity metrics to generate an ontology estimation configurable\nin terms of the level of generalization.", "published": "2017-09-15 00:12:08", "link": "http://arxiv.org/abs/1709.05014v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Query-based Attention CNN for Text Similarity Map", "abstract": "In this paper, we introduce Query-based Attention CNN(QACNN) for Text\nSimilarity Map, an end-to-end neural network for question answering. This\nnetwork is composed of compare mechanism, two-staged CNN architecture with\nattention mechanism, and a prediction layer. First, the compare mechanism\ncompares between the given passage, query, and multiple answer choices to build\nsimilarity maps. Then, the two-staged CNN architecture extracts features\nthrough word-level and sentence-level. At the same time, attention mechanism\nhelps CNN focus more on the important part of the passage based on the query\ninformation. Finally, the prediction layer find out the most possible answer\nchoice. We conduct this model on the MovieQA dataset using Plot Synopses only,\nand achieve 79.99% accuracy which is the state of the art on the dataset.", "published": "2017-09-15 02:25:57", "link": "http://arxiv.org/abs/1709.05036v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Learning Intrinsic Sparse Structures within Long Short-Term Memory", "abstract": "Model compression is significant for the wide adoption of Recurrent Neural\nNetworks (RNNs) in both user devices possessing limited resources and business\nclusters requiring quick responses to large-scale service requests. This work\naims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the\nsizes of basic structures within LSTM units, including input updates, gates,\nhidden states, cell states and outputs. Independently reducing the sizes of\nbasic structures can result in inconsistent dimensions among them, and\nconsequently, end up with invalid LSTM units. To overcome the problem, we\npropose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS\nwill simultaneously decrease the sizes of all basic structures by one and\nthereby always maintain the dimension consistency. By learning ISS within LSTM\nunits, the obtained LSTMs remain regular while having much smaller basic\nstructures. Based on group Lasso regularization, our method achieves 10.59x\nspeedup without losing any perplexity of a language modeling of Penn TreeBank\ndataset. It is also successfully evaluated through a compact model with only\n2.69M weights for machine Question Answering of SQuAD dataset. Our approach is\nsuccessfully extended to non- LSTM RNNs, like Recurrent Highway Networks\n(RHNs). Our source code is publicly available at\nhttps://github.com/wenwei202/iss-rnns", "published": "2017-09-15 01:10:23", "link": "http://arxiv.org/abs/1709.05027v7", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Self-Guiding Multimodal LSTM - when we do not have a perfect training\n  dataset for image captioning", "abstract": "In this paper, a self-guiding multimodal LSTM (sg-LSTM) image captioning\nmodel is proposed to handle uncontrolled imbalanced real-world image-sentence\ndataset. We collect FlickrNYC dataset from Flickr as our testbed with 306,165\nimages and the original text descriptions uploaded by the users are utilized as\nthe ground truth for training. Descriptions in FlickrNYC dataset vary\ndramatically ranging from short term-descriptions to long\nparagraph-descriptions and can describe any visual aspects, or even refer to\nobjects that are not depicted. To deal with the imbalanced and noisy situation\nand to fully explore the dataset itself, we propose a novel guiding textual\nfeature extracted utilizing a multimodal LSTM (m-LSTM) model. Training of\nm-LSTM is based on the portion of data in which the image content and the\ncorresponding descriptions are strongly bonded. Afterwards, during the training\nof sg-LSTM on the rest training data, this guiding information serves as\nadditional input to the network along with the image representations and the\nground-truth descriptions. By integrating these input components into a\nmultimodal block, we aim to form a training scheme with the textual information\ntightly coupled with the image content. The experimental results demonstrate\nthat the proposed sg-LSTM model outperforms the traditional state-of-the-art\nmultimodal RNN captioning framework in successfully describing the key\ncomponents of the input images.", "published": "2017-09-15 02:53:16", "link": "http://arxiv.org/abs/1709.05038v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Algorithms and Architecture for Real-time Recommendations at News UK", "abstract": "Recommendation systems are recognised as being hugely important in industry,\nand the area is now well understood. At News UK, there is a requirement to be\nable to quickly generate recommendations for users on news items as they are\npublished. However, little has been published about systems that can generate\nrecommendations in response to changes in recommendable items and user\nbehaviour in a very short space of time. In this paper we describe a new\nalgorithm for updating collaborative filtering models incrementally, and\ndemonstrate its effectiveness on clickstream data from The Times. We also\ndescribe the architecture that allows recommendations to be generated on the\nfly, and how we have made each component scalable. The system is currently\nbeing used in production at News UK.", "published": "2017-09-15 15:47:23", "link": "http://arxiv.org/abs/1709.05278v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
