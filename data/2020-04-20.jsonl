{"title": "Adversarial Training for Large Neural Language Models", "abstract": "Generalization and robustness are both key desiderata for designing machine\nlearning methods. Adversarial training can enhance robustness, but past work\noften finds it hurts generalization. In natural language processing (NLP),\npre-training large neural language models such as BERT have demonstrated\nimpressive gain in generalization for a variety of tasks, with further\nimprovement from adversarial fine-tuning. However, these models are still\nvulnerable to adversarial attacks. In this paper, we show that adversarial\npre-training can improve both generalization and robustness. We propose a\ngeneral algorithm ALUM (Adversarial training for large neural LangUage Models),\nwhich regularizes the training objective by applying perturbations in the\nembedding space that maximizes the adversarial loss. We present the first\ncomprehensive study of adversarial training in all stages, including\npre-training from scratch, continual pre-training on a well-trained model, and\ntask-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide\nrange of NLP tasks, in both regular and adversarial scenarios. Even for models\nthat have been well trained on extremely large text corpora, such as RoBERTa,\nALUM can still produce significant gains from continual pre-training, whereas\nconventional non-adversarial methods can not. ALUM can be further combined with\ntask-specific fine-tuning to attain additional gains. The ALUM code is publicly\navailable at https://github.com/namisan/mt-dnn.", "published": "2020-04-20 00:07:18", "link": "http://arxiv.org/abs/2004.08994v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating External Knowledge through Pre-training for Natural\n  Language to Code Generation", "abstract": "Open-domain code generation aims to generate code in a general-purpose\nprogramming language (such as Python) from natural language (NL) intents.\nMotivated by the intuition that developers usually retrieve resources on the\nweb when writing code, we explore the effectiveness of incorporating two\nvarieties of external knowledge into NL-to-code generation: automatically mined\nNL-code pairs from the online programming QA forum StackOverflow and\nprogramming language API documentation. Our evaluations show that combining the\ntwo sources with data augmentation and retrieval-based data re-sampling\nimproves the current state-of-the-art by up to 2.2% absolute BLEU score on the\ncode generation testbed CoNaLa. The code and resources are available at\nhttps://github.com/neulab/external-knowledge-codegen.", "published": "2020-04-20 01:45:27", "link": "http://arxiv.org/abs/2004.09015v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptation of a Lexical Organization for Social Engineering Detection\n  and Response Generation", "abstract": "We present a paradigm for extensible lexicon development based on Lexical\nConceptual Structure to support social engineering detection and response\ngeneration. We leverage the central notions of ask (elicitation of behaviors\nsuch as providing access to money) and framing (risk/reward implied by the\nask). We demonstrate improvements in ask/framing detection through refinements\nto our lexical organization and show that response generation qualitatively\nimproves as ask/framing detection performance improves. The paradigm presents a\nsystematic and efficient approach to resource adaptation for improved\ntask-specific performance.", "published": "2020-04-20 04:37:55", "link": "http://arxiv.org/abs/2004.09050v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The State and Fate of Linguistic Diversity and Inclusion in the NLP\n  World", "abstract": "Language technologies contribute to promoting multilingualism and linguistic\ndiversity around the world. However, only a very small number of the over 7000\nlanguages of the world are represented in the rapidly evolving language\ntechnologies and applications. In this paper we look at the relation between\nthe types of languages, resources, and their representation in NLP conferences\nto understand the trajectory that different languages have followed over time.\nOur quantitative investigation underlines the disparity between languages,\nespecially in terms of their resources, and calls into question the \"language\nagnostic\" status of current models and systems. Through this paper, we attempt\nto convince the ACL community to prioritise the resolution of the predicaments\nhighlighted here, so that no language is left behind.", "published": "2020-04-20 07:19:22", "link": "http://arxiv.org/abs/2004.09095v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Variational Inference for Learning Representations of Natural Language\n  Edits", "abstract": "Document editing has become a pervasive component of the production of\ninformation, with version control systems enabling edits to be efficiently\nstored and applied. In light of this, the task of learning distributed\nrepresentations of edits has been recently proposed. With this in mind, we\npropose a novel approach that employs variational inference to learn a\ncontinuous latent space of vector representations to capture the underlying\nsemantic information with regard to the document editing process. We achieve\nthis by introducing a latent variable to explicitly model the aforementioned\nfeatures. This latent variable is then combined with a document representation\nto guide the generation of an edited version of this document. Additionally, to\nfacilitate standardized automatic evaluation of edit representations, which has\nheavily relied on direct human input thus far, we also propose a suite of\ndownstream tasks, PEER, specifically designed to measure the quality of edit\nrepresentations in the context of natural language processing.", "published": "2020-04-20 09:08:59", "link": "http://arxiv.org/abs/2004.09143v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Study of Cross-Lingual Ability and Language-specific Information in\n  Multilingual BERT", "abstract": "Recently, multilingual BERT works remarkably well on cross-lingual transfer\ntasks, superior to static non-contextualized word embeddings. In this work, we\nprovide an in-depth experimental study to supplement the existing literature of\ncross-lingual ability. We compare the cross-lingual ability of\nnon-contextualized and contextualized representation model with the same data.\nWe found that datasize and context window size are crucial factors to the\ntransferability. We also observe the language-specific information in\nmultilingual BERT. By manipulating the latent representations, we can control\nthe output languages of multilingual BERT, and achieve unsupervised token\ntranslation. We further show that based on the observation, there is a\ncomputationally cheap but effective approach to improve the cross-lingual\nability of multilingual BERT.", "published": "2020-04-20 11:13:16", "link": "http://arxiv.org/abs/2004.09205v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PHINC: A Parallel Hinglish Social Media Code-Mixed Corpus for Machine\n  Translation", "abstract": "Code-mixing is the phenomenon of using more than one language in a sentence.\nIt is a very frequently observed pattern of communication on social media\nplatforms. Flexibility to use multiple languages in one text message might help\nto communicate efficiently with the target audience. But, it adds to the\nchallenge of processing and understanding natural language to a much larger\nextent. This paper presents a parallel corpus of the 13,738 code-mixed\nEnglish-Hindi sentences and their corresponding translation in English. The\ntranslations of sentences are done manually by the annotators. We are releasing\nthe parallel corpus to facilitate future research opportunities in code-mixed\nmachine translation. The annotated corpus is available at\nhttps://doi.org/10.5281/zenodo.3605597.", "published": "2020-04-20 17:04:22", "link": "http://arxiv.org/abs/2004.09447v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grounding Conversations with Improvised Dialogues", "abstract": "Effective dialogue involves grounding, the process of establishing mutual\nknowledge that is essential for communication between people. Modern dialogue\nsystems are not explicitly trained to build common ground, and therefore\noverlook this important aspect of communication. Improvisational theater\n(improv) intrinsically contains a high proportion of dialogue focused on\nbuilding common ground, and makes use of the yes-and principle, a strong\ngrounding speech act, to establish coherence and an actionable objective\nreality. We collect a corpus of more than 26,000 yes-and turns, transcribing\nthem from improv dialogues and extracting them from larger, but more sparsely\npopulated movie script dialogue corpora, via a bootstrapped classifier. We\nfine-tune chit-chat dialogue systems with our corpus to encourage more\ngrounded, relevant conversation and confirm these findings with human\nevaluations.", "published": "2020-04-20 18:05:53", "link": "http://arxiv.org/abs/2004.09544v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Automated Pipeline for Character and Relationship Extraction from\n  Readers' Literary Book Reviews on Goodreads.com", "abstract": "Reader reviews of literary fiction on social media, especially those in\npersistent, dedicated forums, create and are in turn driven by underlying\nnarrative frameworks. In their comments about a novel, readers generally\ninclude only a subset of characters and their relationships, thus offering a\nlimited perspective on that work. Yet in aggregate, these reviews capture an\nunderlying narrative framework comprised of different actants (people, places,\nthings), their roles, and interactions that we label the \"consensus narrative\nframework\". We represent this framework in the form of an actant-relationship\nstory graph. Extracting this graph is a challenging computational problem,\nwhich we pose as a latent graphical model estimation problem. Posts and reviews\nare viewed as samples of sub graphs/networks of the hidden narrative framework.\nInspired by the qualitative narrative theory of Greimas, we formulate a\ngraphical generative Machine Learning (ML) model where nodes represent actants,\nand multi-edges and self-loops among nodes capture context-specific\nrelationships. We develop a pipeline of interlocking automated methods to\nextract key actants and their relationships, and apply it to thousands of\nreviews and comments posted on Goodreads.com. We manually derive the ground\ntruth narrative framework from SparkNotes, and then use word embedding tools to\ncompare relationships in ground truth networks with our extracted networks. We\nfind that our automated methodology generates highly accurate consensus\nnarrative frameworks: for our four target novels, with approximately 2900\nreviews per novel, we report average coverage/recall of important relationships\nof > 80% and an average edge detection rate of >89\\%. These extracted narrative\nframeworks can generate insight into how people (or classes of people) read and\nhow they recount what they have read to others.", "published": "2020-04-20 19:57:37", "link": "http://arxiv.org/abs/2004.09601v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Revised Generative Evaluation of Visual Dialogue", "abstract": "Evaluating Visual Dialogue, the task of answering a sequence of questions\nrelating to a visual input, remains an open research challenge. The current\nevaluation scheme of the VisDial dataset computes the ranks of ground-truth\nanswers in predefined candidate sets, which Massiceti et al. (2018) show can be\nsusceptible to the exploitation of dataset biases. This scheme also does little\nto account for the different ways of expressing the same answer--an aspect of\nlanguage that has been well studied in NLP. We propose a revised evaluation\nscheme for the VisDial dataset leveraging metrics from the NLP literature to\nmeasure consensus between answers generated by the model and a set of relevant\nanswers. We construct these relevant answer sets using a simple and effective\nsemi-supervised method based on correlation, which allows us to automatically\nextend and scale sparse relevance annotations from humans to the entire\ndataset. We release these sets and code for the revised evaluation scheme as\nDenseVisDial, and intend them to be an improvement to the dataset in the face\nof its existing constraints and design choices.", "published": "2020-04-20 13:26:45", "link": "http://arxiv.org/abs/2004.09272v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MPNet: Masked and Permuted Pre-training for Language Understanding", "abstract": "BERT adopts masked language modeling (MLM) for pre-training and is one of the\nmost successful pre-training models. Since BERT neglects dependency among\npredicted tokens, XLNet introduces permuted language modeling (PLM) for\npre-training to address this problem. However, XLNet does not leverage the full\nposition information of a sentence and thus suffers from position discrepancy\nbetween pre-training and fine-tuning. In this paper, we propose MPNet, a novel\npre-training method that inherits the advantages of BERT and XLNet and avoids\ntheir limitations. MPNet leverages the dependency among predicted tokens\nthrough permuted language modeling (vs. MLM in BERT), and takes auxiliary\nposition information as input to make the model see a full sentence and thus\nreducing the position discrepancy (vs. PLM in XLNet). We pre-train MPNet on a\nlarge-scale dataset (over 160GB text corpora) and fine-tune on a variety of\ndown-streaming tasks (GLUE, SQuAD, etc). Experimental results show that MPNet\noutperforms MLM and PLM by a large margin, and achieves better results on these\ntasks compared with previous state-of-the-art pre-trained methods (e.g., BERT,\nXLNet, RoBERTa) under the same model setting. The code and the pre-trained\nmodels are available at: https://github.com/microsoft/MPNet.", "published": "2020-04-20 13:54:12", "link": "http://arxiv.org/abs/2004.09297v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Panacea Threat Intelligence and Active Defense Platform", "abstract": "We describe Panacea, a system that supports natural language processing (NLP)\ncomponents for active defenses against social engineering attacks. We deploy a\npipeline of human language technology, including Ask and Framing Detection,\nNamed Entity Recognition, Dialogue Engineering, and Stylometry. Panacea\nprocesses modern message formats through a plug-in architecture to accommodate\ninnovative approaches for message analysis, knowledge representation and\ndialogue generation. The novelty of the Panacea system is that uses NLP for\ncyber defense and engages the attacker using bots to elicit evidence to\nattribute to the attacker and to waste the attacker's time and resources.", "published": "2020-04-20 22:08:08", "link": "http://arxiv.org/abs/2004.09662v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "A Baseline for the Commands For Autonomous Vehicles Challenge", "abstract": "The Commands For Autonomous Vehicles (C4AV) challenge requires participants\nto solve an object referral task in a real-world setting. More specifically, we\nconsider a scenario where a passenger can pass free-form natural language\ncommands to a self-driving car. This problem is particularly challenging, as\nthe language is much less constrained compared to existing benchmarks, and\nobject references are often implicit. The challenge is based on the recent\n\\texttt{Talk2Car} dataset. This document provides a technical overview of a\nmodel that we released to help participants get started in the competition. The\ncode can be found at https://github.com/talk2car/Talk2Car.", "published": "2020-04-20 13:35:47", "link": "http://arxiv.org/abs/2004.13822v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gated Convolutional Bidirectional Attention-based Model for Off-topic\n  Spoken Response Detection", "abstract": "Off-topic spoken response detection, the task aiming at predicting whether a\nresponse is off-topic for the corresponding prompt, is important for an\nautomated speaking assessment system. In many real-world educational\napplications, off-topic spoken response detectors are required to achieve high\nrecall for off-topic responses not only on seen prompts but also on prompts\nthat are unseen during training. In this paper, we propose a novel approach for\noff-topic spoken response detection with high off-topic recall on both seen and\nunseen prompts. We introduce a new model, Gated Convolutional Bidirectional\nAttention-based Model (GCBiA), which applies bi-attention mechanism and\nconvolutions to extract topic words of prompts and key-phrases of responses,\nand introduces gated unit and residual connections between major layers to\nbetter represent the relevance of responses and prompts. Moreover, a new\nnegative sampling method is proposed to augment training data. Experiment\nresults demonstrate that our novel approach can achieve significant\nimprovements in detecting off-topic responses with extremely high on-topic\nrecall, for both seen and unseen prompts.", "published": "2020-04-20 03:16:06", "link": "http://arxiv.org/abs/2004.09036v4", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Taming the Expressiveness and Programmability of Graph Analytical\n  Queries", "abstract": "Graph database has enjoyed a boom in the last decade, and graph queries\naccordingly gain a lot of attentions from both the academia and industry. We\nfocus on analytical queries in this paper. While analyzing existing\ndomain-specific languages (DSLs) for analytical queries regarding the\nperspectives of completeness, expressiveness and programmability, we find out\nthat none of existing work has achieved a satisfactory coverage of these\nperspectives. Motivated by this, we propose the \\flash DSL, which is named\nafter the three primitive operators Filter, LocAl and PuSH. We prove that\n\\flash is Turing complete (completeness), and show that it achieves both good\nexpressiveness and programmability for analytical queries. We provide an\nimplementation of \\flash based on code generation, and compare it with native\nC++ codes and existing DSL using representative queries. The experiment results\ndemonstrate \\flash's expressiveness, and its capability of programming complex\nalgorithms that achieve satisfactory runtime.", "published": "2020-04-20 04:08:28", "link": "http://arxiv.org/abs/2004.09045v2", "categories": ["cs.CL", "cs.DB", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Compositionality and Generalization in Emergent Languages", "abstract": "Natural language allows us to refer to novel composite concepts by combining\nexpressions denoting their parts according to systematic rules, a property\nknown as \\emph{compositionality}. In this paper, we study whether the language\nemerging in deep multi-agent simulations possesses a similar ability to refer\nto novel primitive combinations, and whether it accomplishes this feat by\nstrategies akin to human-language compositionality. Equipped with new ways to\nmeasure compositionality in emergent languages inspired by disentanglement in\nrepresentation learning, we establish three main results. First, given\nsufficiently large input spaces, the emergent language will naturally develop\nthe ability to refer to novel composite concepts. Second, there is no\ncorrelation between the degree of compositionality of an emergent language and\nits ability to generalize. Third, while compositionality is not necessary for\ngeneralization, it provides an advantage in terms of language transmission: The\nmore compositional a language is, the more easily it will be picked up by new\nlearners, even when the latter differ in architecture from the original agents.\nWe conclude that compositionality does not arise from simple generalization\npressure, but if an emergent language does chance upon it, it will be more\nlikely to survive and thrive.", "published": "2020-04-20 08:30:14", "link": "http://arxiv.org/abs/2004.09124v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CheXbert: Combining Automatic Labelers and Expert Annotations for\n  Accurate Radiology Report Labeling Using BERT", "abstract": "The extraction of labels from radiology text reports enables large-scale\ntraining of medical imaging models. Existing approaches to report labeling\ntypically rely either on sophisticated feature engineering based on medical\ndomain knowledge or manual annotations by experts. In this work, we introduce a\nBERT-based approach to medical image report labeling that exploits both the\nscale of available rule-based systems and the quality of expert annotations. We\ndemonstrate superior performance of a biomedically pretrained BERT model first\ntrained on annotations of a rule-based labeler and then finetuned on a small\nset of expert annotations augmented with automated backtranslation. We find\nthat our final model, CheXbert, is able to outperform the previous best\nrules-based labeler with statistical significance, setting a new SOTA for\nreport labeling on one of the largest datasets of chest x-rays.", "published": "2020-04-20 09:46:40", "link": "http://arxiv.org/abs/2004.09167v3", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Encoder-Decoder Incompatibility in Variational Text Modeling and\n  Beyond", "abstract": "Variational autoencoders (VAEs) combine latent variables with amortized\nvariational inference, whose optimization usually converges into a trivial\nlocal optimum termed posterior collapse, especially in text modeling. By\ntracking the optimization dynamics, we observe the encoder-decoder\nincompatibility that leads to poor parameterizations of the data manifold. We\nargue that the trivial local optimum may be avoided by improving the encoder\nand decoder parameterizations since the posterior network is part of a\ntransition map between them. To this end, we propose Coupled-VAE, which couples\na VAE model with a deterministic autoencoder with the same structure and\nimproves the encoder and decoder parameterizations via encoder weight sharing\nand decoder signal matching. We apply the proposed Coupled-VAE approach to\nvarious VAE models with different regularization, posterior family, decoder\nstructure, and optimization strategy. Experiments on benchmark datasets (i.e.,\nPTB, Yelp, and Yahoo) show consistently improved results in terms of\nprobability estimation and richness of the latent space. We also generalize our\nmethod to conditional language modeling and propose Coupled-CVAE, which largely\nimproves the diversity of dialogue generation on the Switchboard dataset.", "published": "2020-04-20 10:34:10", "link": "http://arxiv.org/abs/2004.09189v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Practical Guide to Studying Emergent Communication through Grounded\n  Language Games", "abstract": "The question of how an effective and efficient communication system can\nemerge in a population of agents that need to solve a particular task attracts\nmore and more attention from researchers in many fields, including artificial\nintelligence, linguistics and statistical physics. A common methodology for\nstudying this question consists of carrying out multi-agent experiments in\nwhich a population of agents takes part in a series of scripted and\ntask-oriented communicative interactions, called 'language games'. While each\nindividual language game is typically played by two agents in the population, a\nlarge series of games allows the population to converge on a shared\ncommunication system. Setting up an experiment in which a rich system for\ncommunicating about the real world emerges is a major enterprise, as it\nrequires a variety of software components for running multi-agent experiments,\nfor interacting with sensors and actuators, for conceptualising and\ninterpreting semantic structures, and for mapping between these semantic\nstructures and linguistic utterances. The aim of this paper is twofold. On the\none hand, it introduces a high-level robot interface that extends the Babel\nsoftware system, presenting for the first time a toolkit that provides flexible\nmodules for dealing with each subtask involved in running advanced grounded\nlanguage game experiments. On the other hand, it provides a practical guide to\nusing the toolkit for implementing such experiments, taking a grounded colour\nnaming game experiment as a didactic example.", "published": "2020-04-20 11:48:24", "link": "http://arxiv.org/abs/2004.09218v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Learning Geometric Word Meta-Embeddings", "abstract": "We propose a geometric framework for learning meta-embeddings of words from\ndifferent embedding sources. Our framework transforms the embeddings into a\ncommon latent space, where, for example, simple averaging of different\nembeddings (of a given word) is more amenable. The proposed latent space arises\nfrom two particular geometric transformations - the orthogonal rotations and\nthe Mahalanobis metric scaling. Empirical results on several word similarity\nand word analogy benchmarks illustrate the efficacy of the proposed framework.", "published": "2020-04-20 11:49:04", "link": "http://arxiv.org/abs/2004.09219v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "CHiME-6 Challenge:Tackling Multispeaker Speech Recognition for\n  Unsegmented Recordings", "abstract": "Following the success of the 1st, 2nd, 3rd, 4th and 5th CHiME challenges we\norganize the 6th CHiME Speech Separation and Recognition Challenge (CHiME-6).\nThe new challenge revisits the previous CHiME-5 challenge and further considers\nthe problem of distant multi-microphone conversational speech diarization and\nrecognition in everyday home environments. Speech material is the same as the\nprevious CHiME-5 recordings except for accurate array synchronization. The\nmaterial was elicited using a dinner party scenario with efforts taken to\ncapture data that is representative of natural conversational speech. This\npaper provides a baseline description of the CHiME-6 challenge for both\nsegmented multispeaker speech recognition (Track 1) and unsegmented\nmultispeaker speech recognition (Track 2). Of note, Track 2 is the first\nchallenge activity in the community to tackle an unsegmented multispeaker\nspeech recognition scenario with a complete set of reproducible open source\nbaselines providing speech enhancement, speaker diarization, and speech\nrecognition modules.", "published": "2020-04-20 12:59:07", "link": "http://arxiv.org/abs/2004.09249v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic\n  Speech Recognition of Contact Centers", "abstract": "Automatic speech recognition (ASR) via call is essential for various\napplications, including AI for contact center (AICC) services. Despite the\nadvancement of ASR, however, most publicly available call-based speech corpora\nsuch as Switchboard are old-fashioned. Also, most existing call corpora are in\nEnglish and mainly focus on open domain dialog or general scenarios such as\naudiobooks. Here we introduce a new large-scale Korean call-based speech corpus\nunder a goal-oriented dialog scenario from more than 11,000 people, i.e.,\nClovaCall corpus. ClovaCall includes approximately 60,000 pairs of a short\nsentence and its corresponding spoken utterance in a restaurant reservation\ndomain. We validate the effectiveness of our dataset with intensive experiments\nusing two standard ASR models. Furthermore, we release our ClovaCall dataset\nand baseline source codes to be available via\nhttps://github.com/ClovaAI/ClovaCall.", "published": "2020-04-20 15:12:29", "link": "http://arxiv.org/abs/2004.09367v2", "categories": ["cs.LG", "cs.CL", "cs.SD", "stat.ML"], "primary_category": "cs.LG"}
{"title": "StereoSet: Measuring stereotypical bias in pretrained language models", "abstract": "A stereotype is an over-generalized belief about a particular group of\npeople, e.g., Asians are good at math or Asians are bad drivers. Such beliefs\n(biases) are known to hurt target groups. Since pretrained language models are\ntrained on large real world data, they are known to capture stereotypical\nbiases. In order to assess the adverse effects of these models, it is important\nto quantify the bias captured in them. Existing literature on quantifying bias\nevaluates pretrained language models on a small set of artificially constructed\nbias-assessing sentences. We present StereoSet, a large-scale natural dataset\nin English to measure stereotypical biases in four domains: gender, profession,\nrace, and religion. We evaluate popular models like BERT, GPT-2, RoBERTa, and\nXLNet on our dataset and show that these models exhibit strong stereotypical\nbiases. We also present a leaderboard with a hidden test set to track the bias\nof future language models at https://stereoset.mit.edu", "published": "2020-04-20 17:14:33", "link": "http://arxiv.org/abs/2004.09456v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Fine-tuning Multi-hop Question Answering with Hierarchical Graph Network", "abstract": "In this paper, we present a two stage model for multi-hop question answering.\nThe first stage is a hierarchical graph network, which is used to reason over\nmulti-hop question and is capable to capture different levels of granularity\nusing the nature structure(i.e., paragraphs, questions, sentences and entities)\nof documents. The reasoning process is convert to node classify task(i.e.,\nparagraph nodes and sentences nodes). The second stage is a language model\nfine-tuning task. In a word, stage one use graph neural network to select and\nconcatenate support sentences as one paragraph, and stage two find the answer\nspan in language model fine-tuning paradigm.", "published": "2020-04-20 09:34:16", "link": "http://arxiv.org/abs/2004.13821v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "End-to-End Whisper to Natural Speech Conversion using Modified\n  Transformer Network", "abstract": "Machine recognition of an atypical speech like whispered speech, is a\nchallenging task. We introduce whisper-to-natural-speech conversion using\nsequence-to-sequence approach by proposing enhanced transformer architecture,\nwhich uses both parallel and non-parallel data. We investigate different\nfeatures like Mel frequency cepstral coefficients and smoothed spectral\nfeatures. The proposed networks are trained end-to-end using supervised\napproach for feature-to-feature transformation. Further, we also investigate\nthe effectiveness of embedded auxillary decoder used after N encoder\nsub-layers, trained with the frame-level objective function for identifying\nsource phoneme labels. We show results on opensource wTIMIT and CHAINS datasets\nby measuring word error rate using end-to-end ASR and also BLEU scores for the\ngenerated speech. Alternatively, we also propose a novel method to measure\nspectral shape of it by measuring formant distributions w.r.t. reference\nspeech, as formant divergence metric. We have found whisper-to-natural\nconverted speech formants probability distribution is similar to the\ngroundtruth distribution. To the authors' best knowledge, this is the first\ntime enhanced transformer has been proposed, both with and without auxiliary\ndecoder for whisper-to-natural-speech conversion and vice versa.", "published": "2020-04-20 14:47:46", "link": "http://arxiv.org/abs/2004.09347v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Language-agnostic Multilingual Modeling", "abstract": "Multilingual Automated Speech Recognition (ASR) systems allow for the joint\ntraining of data-rich and data-scarce languages in a single model. This enables\ndata and parameter sharing across languages, which is especially beneficial for\nthe data-scarce languages. However, most state-of-the-art multilingual models\nrequire the encoding of language information and therefore are not as flexible\nor scalable when expanding to newer languages. Language-independent\nmultilingual models help to address this issue, and are also better suited for\nmulticultural societies where several languages are frequently used together\n(but often rendered with different writing systems). In this paper, we propose\na new approach to building a language-agnostic multilingual ASR system which\ntransforms all languages to one writing system through a many-to-one\ntransliteration transducer. Thus, similar sounding acoustics are mapped to a\nsingle, canonical target sequence of graphemes, effectively separating the\nmodeling and rendering problems. We show with four Indic languages, namely,\nHindi, Bengali, Tamil and Kannada, that the language-agnostic multilingual\nmodel achieves up to 10% relative reduction in Word Error Rate (WER) over a\nlanguage-dependent multilingual model.", "published": "2020-04-20 18:57:43", "link": "http://arxiv.org/abs/2004.09571v1", "categories": ["eess.AS", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "ViSQOL v3: An Open Source Production Ready Objective Speech and Audio\n  Metric", "abstract": "Estimation of perceptual quality in audio and speech is possible using a\nvariety of methods. The combined v3 release of ViSQOL and ViSQOLAudio (for\nspeech and audio, respectively,) provides improvements upon previous versions,\nin terms of both design and usage. As an open source C++ library or binary with\npermissive licensing, ViSQOL can now be deployed beyond the research context\ninto production usage. The feedback from internal production teams at Google\nhas helped to improve this new release, and serves to show cases where it is\nmost applicable, as well as to highlight limitations. The new model is\nbenchmarked against real-world data for evaluation purposes. The trends and\ndirection of future work is discussed.", "published": "2020-04-20 19:19:26", "link": "http://arxiv.org/abs/2004.09584v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Data Processing for Optimizing Naturalness of Vietnamese Text-to-speech\n  System", "abstract": "Abstract End-to-end text-to-speech (TTS) systems has proved its great success\nin the presence of a large amount of high-quality training data recorded in\nanechoic room with high-quality microphone. Another approach is to use\navailable source of found data like radio broadcast news. We aim to optimize\nthe naturalness of TTS system on the found data using a novel data processing\nmethod. The data processing method includes 1) utterance selection and 2)\nprosodic punctuation insertion to prepare training data which can optimize the\nnaturalness of TTS systems. We showed that using the processing data method, an\nend-to-end TTS achieved a mean opinion score (MOS) of 4.1 compared to 4.3 of\nnatural speech. We showed that the punctuation insertion contributed the most\nto the result. To facilitate the research and development of TTS systems, we\ndistributed the processed data of one speaker at\nhttps://forms.gle/6Hk5YkqgDxAaC2BU6.", "published": "2020-04-20 20:11:53", "link": "http://arxiv.org/abs/2004.09607v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Music Gesture for Visual Sound Separation", "abstract": "Recent deep learning approaches have achieved impressive performance on\nvisual sound separation tasks. However, these approaches are mostly built on\nappearance and optical flow like motion feature representations, which exhibit\nlimited abilities to find the correlations between audio signals and visual\npoints, especially when separating multiple instruments of the same types, such\nas multiple violins in a scene. To address this, we propose \"Music Gesture,\" a\nkeypoint-based structured representation to explicitly model the body and\nfinger movements of musicians when they perform music. We first adopt a\ncontext-aware graph network to integrate visual semantic context with body\ndynamics, and then apply an audio-visual fusion model to associate body\nmovements with the corresponding audio signals. Experimental results on three\nmusic performance datasets show: 1) strong improvements upon benchmark metrics\nfor hetero-musical separation tasks (i.e. different instruments); 2) new\nability for effective homo-musical separation for piano, flute, and trumpet\nduets, which to our best knowledge has never been achieved with alternative\nmethods. Project page: http://music-gesture.csail.mit.edu.", "published": "2020-04-20 17:53:46", "link": "http://arxiv.org/abs/2004.09476v1", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
