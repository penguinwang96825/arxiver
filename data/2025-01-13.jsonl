{"title": "Improving DeFi Accessibility through Efficient Liquidity Provisioning with Deep Reinforcement Learning", "abstract": "This paper applies deep reinforcement learning (DRL) to optimize liquidity\nprovisioning in Uniswap v3, a decentralized finance (DeFi) protocol\nimplementing an automated market maker (AMM) model with concentrated liquidity.\nWe model the liquidity provision task as a Markov Decision Process (MDP) and\ntrain an active liquidity provider (LP) agent using the Proximal Policy\nOptimization (PPO) algorithm. The agent dynamically adjusts liquidity positions\nby using information about price dynamics to balance fee maximization and\nimpermanent loss mitigation. We use a rolling window approach for training and\ntesting, reflecting realistic market conditions and regime shifts. This study\ncompares the data-driven performance of the DRL-based strategy against common\nheuristics adopted by small retail LP actors that do not systematically modify\ntheir liquidity positions. By promoting more efficient liquidity management,\nthis work aims to make DeFi markets more accessible and inclusive for a broader\nrange of participants. Through a data-driven approach to liquidity management,\nthis work seeks to contribute to the ongoing development of more efficient and\nuser-friendly DeFi markets.", "published": "2025-01-13 17:27:11", "link": "http://arxiv.org/abs/2501.07508v1", "categories": ["q-fin.CP", "cs.LG"], "primary_category": "q-fin.CP"}
{"title": "Pricing Quanto and Composite Contracts with Local-Correlation Models", "abstract": "Pricing composite and quanto contracts requires a joint model of both the\nunderlying asset and the exchange rate. In this contribution, we explore the\npotential of local-correlation models to address the challenges of calibrating\nsynthetic quanto forward contracts and composite options quoted in the market.\nSpecifically, we design on-line calibration procedures for generic local and\nstochastic volatility models. The paper concludes with a numerical study\nassessing the calibration performance of these methodologies and comparing them\nto simpler approximations of the correlation structure.", "published": "2025-01-13 10:52:29", "link": "http://arxiv.org/abs/2501.07200v1", "categories": ["q-fin.PR", "q-fin.CP", "65C05, 91G20, 91G60"], "primary_category": "q-fin.PR"}
{"title": "How low-cost AI universal approximators reshape market efficiency", "abstract": "The efficient market hypothesis (EMH) famously stated that prices fully\nreflect the information available to traders. This critically depends on the\ntransfer of information into prices through trading strategies. Traders\noptimise their strategy with models of increasing complexity that identify the\nrelationship between information and profitable trades more and more\naccurately. Under specific conditions, the increased availability of low-cost\nuniversal approximators, such as AI systems, should be naturally pushing\ntowards more advanced trading strategies, potentially making it harder and\nharder for inefficient traders to profit. In this paper, we leverage on a\ngeneralised notion of market efficiency, based on the definition of an\nequilibrium price process, that allows us to distinguish different levels of\nmodel complexity through investors' beliefs, and trading strategies\noptimisation, and discuss the relationship between AI-powered trading and the\ntime-evolution of market efficiency. Finally, we outline the need for and the\nchallenge of describing out-of-equilibrium market dynamics in an adaptive\nmulti-agent environment.", "published": "2025-01-13 17:06:25", "link": "http://arxiv.org/abs/2501.07489v1", "categories": ["q-fin.MF", "q-fin.GN", "q-fin.TR"], "primary_category": "q-fin.MF"}
{"title": "Follow the Leader: Enhancing Systematic Trend-Following Using Network Momentum", "abstract": "We present a systematic, trend-following strategy, applied to commodity\nfutures markets, that combines univariate trend indicators with cross-sectional\ntrend indicators that capture so-called {\\em momentum spillover}, which can\noccur when there is a lead-lag relationship between the trending behaviour of\ndifferent markets. Our strategy utilises two methods for detecting lead-lag\nrelationships, with a method for computing {\\em network momentum}, to produce a\nnovel trend-following indicator. We use our new trend indicator to construct a\nportfolio whose performance we compare to a baseline model which uses only\nunivariate indicators, and demonstrate statistically significant improvements\nin Sharpe ratio, skewness of returns, and downside performance, using synthetic\nbootstrapped data samples taken from time-series of actual prices.", "published": "2025-01-13 08:53:44", "link": "http://arxiv.org/abs/2501.07135v1", "categories": ["q-fin.TR", "q-fin.MF"], "primary_category": "q-fin.TR"}
{"title": "Boosting Text-To-Image Generation via Multilingual Prompting in Large\n  Multimodal Models", "abstract": "Previous work on augmenting large multimodal models (LMMs) for text-to-image\n(T2I) generation has focused on enriching the input space of in-context\nlearning (ICL). This includes providing a few demonstrations and optimizing\nimage descriptions to be more detailed and logical. However, as demand for more\ncomplex and flexible image descriptions grows, enhancing comprehension of input\ntext within the ICL paradigm remains a critical yet underexplored area. In this\nwork, we extend this line of research by constructing parallel multilingual\nprompts aimed at harnessing the multilingual capabilities of LMMs. More\nspecifically, we translate the input text into several languages and provide\nthe models with both the original text and the translations. Experiments on two\nLMMs across 3 benchmarks show that our method, PMT2I, achieves superior\nperformance in general, compositional, and fine-grained assessments, especially\nin human preference alignment. Additionally, with its advantage of generating\nmore diverse images, PMT2I significantly outperforms baseline prompts when\nincorporated with reranking methods. Our code and parallel multilingual data\ncan be found at https://github.com/takagi97/PMT2I.", "published": "2025-01-13 06:41:23", "link": "http://arxiv.org/abs/2501.07086v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When lies are mostly truthful: automated verbal deception detection for\n  embedded lies", "abstract": "Background: Verbal deception detection research relies on narratives and\ncommonly assumes statements as truthful or deceptive. A more realistic\nperspective acknowledges that the veracity of statements exists on a continuum\nwith truthful and deceptive parts being embedded within the same statement.\nHowever, research on embedded lies has been lagging behind. Methods: We\ncollected a novel dataset of 2,088 truthful and deceptive statements with\nannotated embedded lies. Using a within-subjects design, participants provided\na truthful account of an autobiographical event. They then rewrote their\nstatement in a deceptive manner by including embedded lies, which they\nhighlighted afterwards and judged on lie centrality, deceptiveness, and source.\nResults: We show that a fined-tuned language model (Llama-3-8B) can classify\ntruthful statements and those containing embedded lies with 64% accuracy.\nIndividual differences, linguistic properties and explainability analysis\nsuggest that the challenge of moving the dial towards embedded lies stems from\ntheir resemblance to truthful statements. Typical deceptive statements\nconsisted of 2/3 truthful information and 1/3 embedded lies, largely derived\nfrom past personal experiences and with minimal linguistic differences with\ntheir truthful counterparts. Conclusion: We present this dataset as a novel\nresource to address this challenge and foster research on embedded lies in\nverbal deception detection.", "published": "2025-01-13 11:16:05", "link": "http://arxiv.org/abs/2501.07217v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FinerWeb-10BT: Refining Web Data with LLM-Based Line-Level Filtering", "abstract": "Data quality is crucial for training Large Language Models (LLMs).\nTraditional heuristic filters often miss low-quality text or mistakenly remove\nvaluable content. In this paper, we introduce an LLM-based line-level filtering\nmethod to enhance training data quality. We use GPT-4o mini to label a\n20,000-document sample from FineWeb at the line level, allowing the model to\ncreate descriptive labels for low-quality lines. These labels are grouped into\nnine main categories, and we train a DeBERTa-v3 classifier to scale the\nfiltering to a 10B-token subset of FineWeb. To test the impact of our\nfiltering, we train GPT-2 models on both the original and the filtered\ndatasets. The results show that models trained on the filtered data achieve\nhigher accuracy on the HellaSwag benchmark and reach their performance targets\nfaster, even with up to 25\\% less data. This demonstrates that LLM-based\nline-level filtering can significantly improve data quality and training\nefficiency for LLMs. We release our quality-annotated dataset, FinerWeb-10BT,\nand the codebase to support further work in this area.", "published": "2025-01-13 13:26:50", "link": "http://arxiv.org/abs/2501.07314v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Large Language Models in Inferring Personality Traits from\n  User Conversations", "abstract": "Large Language Models (LLMs) are demonstrating remarkable human like\ncapabilities across diverse domains, including psychological assessment. This\nstudy evaluates whether LLMs, specifically GPT-4o and GPT-4o mini, can infer\nBig Five personality traits and generate Big Five Inventory-10 (BFI-10) item\nscores from user conversations under zero-shot prompting conditions. Our\nfindings reveal that incorporating an intermediate step--prompting for BFI-10\nitem scores before calculating traits--enhances accuracy and aligns more\nclosely with the gold standard than direct trait inference. This structured\napproach underscores the importance of leveraging psychological frameworks in\nimproving predictive precision. Additionally, a group comparison based on\ndepressive symptom presence revealed differential model performance.\nParticipants were categorized into two groups: those experiencing at least one\ndepressive symptom and those without symptoms. GPT-4o mini demonstrated\nheightened sensitivity to depression-related shifts in traits such as\nNeuroticism and Conscientiousness within the symptom-present group, whereas\nGPT-4o exhibited strengths in nuanced interpretation across groups. These\nfindings underscore the potential of LLMs to analyze real-world psychological\ndata effectively, offering a valuable foundation for interdisciplinary research\nat the intersection of artificial intelligence and psychology.", "published": "2025-01-13 18:09:58", "link": "http://arxiv.org/abs/2501.07532v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GPT as a Monte Carlo Language Tree: A Probabilistic Perspective", "abstract": "Large Language Models (LLMs), such as GPT, are considered to learn the latent\ndistributions within large-scale web-crawl datasets and accomplish natural\nlanguage processing (NLP) tasks by predicting the next token. However, this\nmechanism of latent distribution modeling lacks quantitative understanding and\nanalysis. In this paper, we propose a novel perspective that any language\ndataset can be represented by a Monte Carlo Language Tree (abbreviated as\n``Data-Tree''), where each node denotes a token, each edge denotes a token\ntransition probability, and each sequence has a unique path. Any GPT-like\nlanguage model can also be flattened into another Monte Carlo Language Tree\n(abbreviated as ``GPT-Tree''). Our experiments show that different GPT models\ntrained on the same dataset exhibit significant structural similarity in\nGPT-Tree visualization, and larger models converge more closely to the\nData-Tree. More than 87\\% GPT output tokens can be recalled by Data-Tree. These\nfindings may confirm that the reasoning process of LLMs is more likely to be\nprobabilistic pattern-matching rather than formal reasoning, as each model\ninference seems to find a context pattern with maximum probability from the\nData-Tree. Furthermore, we provide deeper insights into issues such as\nhallucination, Chain-of-Thought (CoT) reasoning, and token bias in LLMs.", "published": "2025-01-13 19:04:57", "link": "http://arxiv.org/abs/2501.07641v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Talent Employment Insights Through Feature Extraction with LLM\n  Finetuning", "abstract": "This paper explores the application of large language models (LLMs) to\nextract nuanced and complex job features from unstructured job postings. Using\na dataset of 1.2 million job postings provided by AdeptID, we developed a\nrobust pipeline to identify and classify variables such as remote work\navailability, remuneration structures, educational requirements, and work\nexperience preferences. Our methodology combines semantic chunking,\nretrieval-augmented generation (RAG), and fine-tuning DistilBERT models to\novercome the limitations of traditional parsing tools. By leveraging these\ntechniques, we achieved significant improvements in identifying variables often\nmislabeled or overlooked, such as non-salary-based compensation and inferred\nremote work categories. We present a comprehensive evaluation of our fine-tuned\nmodels and analyze their strengths, limitations, and potential for scaling.\nThis work highlights the promise of LLMs in labor market analytics, providing a\nfoundation for more accurate and actionable insights into job data.", "published": "2025-01-13 19:49:49", "link": "http://arxiv.org/abs/2501.07663v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Abstractive Summarisation: A Dataset of Human-authored\n  Summaries of Norwegian News Articles", "abstract": "We introduce a dataset of high-quality human-authored summaries of news\narticles in Norwegian. The dataset is intended for benchmarking the abstractive\nsummarisation capabilities of generative language models. Each document in the\ndataset is provided with three different candidate gold-standard summaries\nwritten by native Norwegian speakers, and all summaries are provided in both of\nthe written variants of Norwegian -- Bokm{\\aa}l and Nynorsk. The paper\ndescribes details on the data creation effort as well as an evaluation of\nexisting open LLMs for Norwegian on the dataset. We also provide insights from\na manual human evaluation, comparing human-authored to model-generated\nsummaries. Our results indicate that the dataset provides a challenging LLM\nbenchmark for Norwegian summarisation capabilities", "published": "2025-01-13 22:08:29", "link": "http://arxiv.org/abs/2501.07718v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entailed Between the Lines: Incorporating Implication into NLI", "abstract": "Much of human communication depends on implication, conveying meaning beyond\nliteral words to express a wider range of thoughts, intentions, and feelings.\nFor models to better understand and facilitate human communication, they must\nbe responsive to the text's implicit meaning. We focus on Natural Language\nInference (NLI), a core tool for many language tasks, and find that\nstate-of-the-art NLI models and datasets struggle to recognize a range of cases\nwhere entailment is implied, rather than explicit from the text. We formalize\nimplied entailment as an extension of the NLI task and introduce the Implied\nNLI dataset (INLI) to help today's LLMs both recognize a broader variety of\nimplied entailments and to distinguish between implicit and explicit\nentailment. We show how LLMs fine-tuned on INLI understand implied entailment\nand can generalize this understanding across datasets and domains.", "published": "2025-01-13 22:09:44", "link": "http://arxiv.org/abs/2501.07719v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMic: Romanian Foundation Language Model", "abstract": "Recent advances in Large Language Models (LLMs) have demonstrated remarkable\ncapabilities across various tasks with commercial models leading the way. While\nopen models usually operate at a smaller scale, they maintain competitiveness\nthrough specialization and fine-tuning. However, a significant challenge\npersists: open models often underperform in low-resource languages due to\nlimited representation in the training corpus. In this paper, we present LLMic,\na bilingual foundation language model designed specifically for the Romanian\nLanguage. We document the complete process of pretraining a foundation model\nfor a low-resource language, including corpus construction, architecture\nselection, and hyper-parameter optimization. Our evaluation demonstrates that\nLLMic can be specialized for tasks in the target language, achieving results\ncomparable to other much larger open models. We show that fine-tuning LLMic for\nlanguage translation after the initial pretraining phase outperforms existing\nsolutions in English-to-Romanian translation tasks. This opens the path for\nefficient large-scale processing for the Romanian language community, using the\nmuch smaller LLMic model", "published": "2025-01-13 22:14:45", "link": "http://arxiv.org/abs/2501.07721v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the encoding of linguistic representations in the\n  Fully-Connected Layer of generative CNNs for Speech", "abstract": "Interpretability work on the convolutional layers of CNNs has primarily\nfocused on computer vision, but some studies also explore correspondences\nbetween the latent space and the output in the audio domain. However, it has\nnot been thoroughly examined how acoustic and linguistic information is\nrepresented in the fully connected (FC) layer that bridges the latent space and\nconvolutional layers. The current study presents the first exploration of how\nthe FC layer of CNNs for speech synthesis encodes linguistically relevant\ninformation. We propose two techniques for exploration of the fully connected\nlayer. In Experiment 1, we use weight matrices as inputs into convolutional\nlayers. In Experiment 2, we manipulate the FC layer to explore how\nsymbolic-like representations are encoded in CNNs. We leverage the fact that\nthe FC layer outputs a feature map and that variable-specific weight matrices\nare temporally structured to (1) demonstrate how the distribution of learned\nweights varies between latent variables in systematic ways and (2) demonstrate\nhow manipulating the FC layer while holding constant subsequent model\nparameters affects the output. We ultimately present an FC manipulation that\ncan output a single segment. Using this technique, we show that lexically\nspecific latent codes in generative CNNs (ciwGAN) have shared lexically\ninvariant sublexical representations in the FC-layer weights, showing that\nciwGAN encodes lexical information in a linguistically principled manner.", "published": "2025-01-13 22:24:52", "link": "http://arxiv.org/abs/2501.07726v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advancing Student Writing Through Automated Syntax Feedback", "abstract": "This study underscores the pivotal role of syntax feedback in augmenting the\nsyntactic proficiency of students. Recognizing the challenges faced by learners\nin mastering syntactic nuances, we introduce a specialized dataset named\nEssay-Syntax-Instruct designed to enhance the understanding and application of\nEnglish syntax among these students. Leveraging the capabilities of Large\nLanguage Models (LLMs) such as GPT3.5-Turbo, Llama-2-7b-chat-hf,\nLlama-2-13b-chat-hf, and Mistral-7B-Instruct-v0.2, this work embarks on a\ncomprehensive fine-tuning process tailored to the syntax improvement task.\nThrough meticulous evaluation, we demonstrate that the fine-tuned LLMs exhibit\na marked improvement in addressing syntax-related challenges, thereby serving\nas a potent tool for students to identify and rectify their syntactic errors.\nThe findings not only highlight the effectiveness of the proposed dataset in\nelevating the performance of LLMs for syntax enhancement but also illuminate a\npromising path for utilizing advanced language models to support language\nacquisition efforts. This research contributes to the broader field of language\nlearning technology by showcasing the potential of LLMs in facilitating the\nlinguistic development of Students.", "published": "2025-01-13 23:10:02", "link": "http://arxiv.org/abs/2501.07740v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LEO: Boosting Mixture of Vision Encoders for Multimodal Large Language\n  Models", "abstract": "Enhanced visual understanding serves as a cornerstone for multimodal large\nlanguage models (MLLMs). Recent hybrid MLLMs incorporate a mixture of vision\nexperts to address the limitations of using a single vision encoder and\nexcessively long visual tokens. Despite the progress of these MLLMs, a research\ngap remains in effectively integrating diverse vision encoders. This work\nexplores fusion strategies of visual tokens for hybrid MLLMs, leading to the\ndesign of LEO, a novel MLLM with a dual-branch vision encoder framework that\nincorporates a post-adaptation fusion strategy and adaptive tiling: for each\nsegmented tile of the input images, LEO sequentially interleaves the visual\ntokens from its two vision encoders. Extensive evaluation across 13\nvision-language benchmarks reveals that LEO outperforms state-of-the-art\nopen-source MLLMs and hybrid MLLMs on the majority of tasks. Furthermore, we\nshow that LEO can be adapted to the specialized domain of autonomous driving\nwithout altering the model architecture or training recipe, achieving\ncompetitive performance compared to existing baselines. The code and model will\nbe publicly available.", "published": "2025-01-13 00:29:55", "link": "http://arxiv.org/abs/2501.06986v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ViSoLex: An Open-Source Repository for Vietnamese Social Media Lexical\n  Normalization", "abstract": "ViSoLex is an open-source system designed to address the unique challenges of\nlexical normalization for Vietnamese social media text. The platform provides\ntwo core services: Non-Standard Word (NSW) Lookup and Lexical Normalization,\nenabling users to retrieve standard forms of informal language and standardize\ntext containing NSWs. ViSoLex's architecture integrates pre-trained language\nmodels and weakly supervised learning techniques to ensure accurate and\nefficient normalization, overcoming the scarcity of labeled data in Vietnamese.\nThis paper details the system's design, functionality, and its applications for\nresearchers and non-technical users. Additionally, ViSoLex offers a flexible,\ncustomizable framework that can be adapted to various datasets and research\nrequirements. By publishing the source code, ViSoLex aims to contribute to the\ndevelopment of more robust Vietnamese natural language processing tools and\nencourage further research in lexical normalization. Future directions include\nexpanding the system's capabilities for additional languages and improving the\nhandling of more complex non-standard linguistic patterns.", "published": "2025-01-13 02:47:13", "link": "http://arxiv.org/abs/2501.07020v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Research on the Online Update Method for Retrieval-Augmented Generation\n  (RAG) Model with Incremental Learning", "abstract": "In the contemporary context of rapid advancements in information technology\nand the exponential growth of data volume, language models are confronted with\nsignificant challenges in effectively navigating the dynamic and ever-evolving\ninformation landscape to update and adapt to novel knowledge in real time. In\nthis work, an online update method is proposed, which is based on the existing\nRetrieval Enhanced Generation (RAG) model with multiple innovation mechanisms.\nFirstly, the dynamic memory is used to capture the emerging data samples, and\nthen gradually integrate them into the core model through a tunable knowledge\ndistillation strategy. At the same time, hierarchical indexing and multi-layer\ngating mechanism are introduced into the retrieval module to ensure that the\nretrieved content is more targeted and accurate. Finally, a multi-stage network\nstructure is established for different types of inputs in the generation stage,\nand cross-attention matching and screening are carried out on the intermediate\nrepresentations of each stage to ensure the effective integration and iterative\nupdate of new and old knowledge. Experimental results show that the proposed\nmethod is better than the existing mainstream comparison models in terms of\nknowledge retention and inference accuracy.", "published": "2025-01-13 05:16:14", "link": "http://arxiv.org/abs/2501.07063v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "ListConRanker: A Contrastive Text Reranker with Listwise Encoding", "abstract": "Reranker models aim to re-rank the passages based on the semantics similarity\nbetween the given query and passages, which have recently received more\nattention due to the wide application of the Retrieval-Augmented Generation.\nMost previous methods apply pointwise encoding, meaning that it can only encode\nthe context of the query for each passage input into the model. However, for\nthe reranker model, given a query, the comparison results between passages are\neven more important, which is called listwise encoding. Besides, previous\nmodels are trained using the cross-entropy loss function, which leads to issues\nof unsmooth gradient changes during training and low training efficiency. To\naddress these issues, we propose a novel Listwise-encoded Contrastive text\nreRanker (ListConRanker). It can help the passage to be compared with other\npassages during the encoding process, and enhance the contrastive information\nbetween positive examples and between positive and negative examples. At the\nsame time, we use the circle loss to train the model to increase the\nflexibility of gradients and solve the problem of training efficiency.\nExperimental results show that ListConRanker achieves state-of-the-art\nperformance on the reranking benchmark of Chinese Massive Text Embedding\nBenchmark, including the cMedQA1.0, cMedQA2.0, MMarcoReranking, and T2Reranking\ndatasets.", "published": "2025-01-13 07:51:46", "link": "http://arxiv.org/abs/2501.07111v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and\n  Vision-Language Models Derived from Scientific Literature", "abstract": "The development of vision-language models (VLMs) is driven by large-scale and\ndiverse multimodal datasets. However, progress toward generalist biomedical\nVLMs is limited by the lack of annotated, publicly accessible datasets across\nbiology and medicine. Existing efforts are restricted to narrow domains,\nmissing the full diversity of biomedical knowledge encoded in scientific\nliterature. To address this gap, we introduce BIOMEDICA, a scalable,\nopen-source framework to extract, annotate, and serialize the entirety of the\nPubMed Central Open Access subset into an easy-to-use, publicly accessible\ndataset. Our framework produces a comprehensive archive with over 24 million\nunique image-text pairs from over 6 million articles. Metadata and\nexpert-guided annotations are also provided. We demonstrate the utility and\naccessibility of our resource by releasing BMCA-CLIP, a suite of CLIP-style\nmodels continuously pre-trained on the BIOMEDICA dataset via streaming,\neliminating the need to download 27 TB of data locally. On average, our models\nachieve state-of-the-art performance across 40 tasks - spanning pathology,\nradiology, ophthalmology, dermatology, surgery, molecular biology,\nparasitology, and cell biology - excelling in zero-shot classification with a\n6.56% average improvement (as high as 29.8% and 17.5% in dermatology and\nophthalmology, respectively), and stronger image-text retrieval, all while\nusing 10x less compute. To foster reproducibility and collaboration, we release\nour codebase and dataset for the broader research community.", "published": "2025-01-13 09:58:03", "link": "http://arxiv.org/abs/2501.07171v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Can Vision-Language Models Evaluate Handwritten Math?", "abstract": "Recent advancements in Vision-Language Models (VLMs) have opened new\npossibilities in automatic grading of handwritten student responses,\nparticularly in mathematics. However, a comprehensive study to test the ability\nof VLMs to evaluate and reason over handwritten content remains absent. To\naddress this gap, we introduce FERMAT, a benchmark designed to assess the\nability of VLMs to detect, localize and correct errors in handwritten\nmathematical content. FERMAT spans four key error dimensions - computational,\nconceptual, notational, and presentation - and comprises over 2,200 handwritten\nmath solutions derived from 609 manually curated problems from grades 7-12 with\nintentionally introduced perturbations. Using FERMAT we benchmark nine VLMs\nacross three tasks: error detection, localization, and correction. Our results\nreveal significant shortcomings in current VLMs in reasoning over handwritten\ntext, with Gemini-1.5-Pro achieving the highest error correction rate (77%). We\nalso observed that some models struggle with processing handwritten content, as\ntheir accuracy improves when handwritten inputs are replaced with printed text\nor images. These findings highlight the limitations of current VLMs and reveal\nnew avenues for improvement. We release FERMAT and all the associated resources\nin the open-source to drive further research.", "published": "2025-01-13 11:52:55", "link": "http://arxiv.org/abs/2501.07244v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Comparative analysis of optical character recognition methods for S\u00e1mi\n  texts from the National Library of Norway", "abstract": "Optical Character Recognition (OCR) is crucial to the National Library of\nNorway's (NLN) digitisation process as it converts scanned documents into\nmachine-readable text. However, for the S\\'ami documents in NLN's collection,\nthe OCR accuracy is insufficient. Given that OCR quality affects downstream\nprocesses, evaluating and improving OCR for text written in S\\'ami languages is\nnecessary to make these resources accessible. To address this need, this work\nfine-tunes and evaluates three established OCR approaches, Transkribus,\nTesseract and TrOCR, for transcribing S\\'ami texts from NLN's collection. Our\nresults show that Transkribus and TrOCR outperform Tesseract on this task,\nwhile Tesseract achieves superior performance on an out-of-domain dataset.\nFurthermore, we show that fine-tuning pre-trained models and supplementing\nmanual annotations with machine annotations and synthetic text images can yield\naccurate OCR for S\\'ami languages, even with a moderate amount of manually\nannotated data.", "published": "2025-01-13 13:07:51", "link": "http://arxiv.org/abs/2501.07300v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Emergent effects of scaling on the functional hierarchies within large\n  language models", "abstract": "Large language model (LLM) architectures are often described as functionally\nhierarchical: Early layers process syntax, middle layers begin to parse\nsemantics, and late layers integrate information. The present work revisits\nthese ideas. This research submits simple texts to an LLM (e.g., \"A church and\norgan\") and extracts the resulting activations. Then, for each layer, support\nvector machines and ridge regressions are fit to predict a text's label and\nthus examine whether a given layer encodes some information. Analyses using a\nsmall model (Llama-3.2-3b; 28 layers) partly bolster the common hierarchical\nperspective: Item-level semantics are most strongly represented early (layers\n2-7), then two-item relations (layers 8-12), and then four-item analogies\n(layers 10-15). Afterward, the representation of items and simple relations\ngradually decreases in deeper layers that focus on more global information.\nHowever, several findings run counter to a steady hierarchy view: First,\nalthough deep layers can represent document-wide abstractions, deep layers also\ncompress information from early portions of the context window without\nmeaningful abstraction. Second, when examining a larger model\n(Llama-3.3-70b-Instruct), stark fluctuations in abstraction level appear: As\ndepth increases, two-item relations and four-item analogies initially increase\nin their representation, then markedly decrease, and afterward increase again\nmomentarily. This peculiar pattern consistently emerges across several\nexperiments. Third, another emergent effect of scaling is coordination between\nthe attention mechanisms of adjacent layers. Across multiple experiments using\nthe larger model, adjacent layers fluctuate between what information they each\nspecialize in representing. In sum, an abstraction hierarchy often manifests\nacross layers, but large models also deviate from this structure in curious\nways.", "published": "2025-01-13 14:27:39", "link": "http://arxiv.org/abs/2501.07359v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Retrieval-Augmented Generation: A Study of Best Practices", "abstract": "Retrieval-Augmented Generation (RAG) systems have recently shown remarkable\nadvancements by integrating retrieval mechanisms into language models,\nenhancing their ability to produce more accurate and contextually relevant\nresponses. However, the influence of various components and configurations\nwithin RAG systems remains underexplored. A comprehensive understanding of\nthese elements is essential for tailoring RAG systems to complex retrieval\ntasks and ensuring optimal performance across diverse applications. In this\npaper, we develop several advanced RAG system designs that incorporate query\nexpansion, various novel retrieval strategies, and a novel Contrastive\nIn-Context Learning RAG. Our study systematically investigates key factors,\nincluding language model size, prompt design, document chunk size, knowledge\nbase size, retrieval stride, query expansion techniques, Contrastive In-Context\nLearning knowledge bases, multilingual knowledge bases, and Focus Mode\nretrieving relevant context at sentence-level. Through extensive\nexperimentation, we provide a detailed analysis of how these factors influence\nresponse quality. Our findings offer actionable insights for developing RAG\nsystems, striking a balance between contextual richness and\nretrieval-generation efficiency, thereby paving the way for more adaptable and\nhigh-performing RAG frameworks in diverse real-world scenarios. Our code and\nimplementation details are publicly available.", "published": "2025-01-13 15:07:55", "link": "http://arxiv.org/abs/2501.07391v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TiEBe: A Benchmark for Assessing the Current Knowledge of Large Language\n  Models", "abstract": "In a rapidly evolving knowledge landscape and the increasing adoption of\nlarge language models, a need has emerged to keep these models continuously\nupdated with current events. While existing benchmarks evaluate general factual\nrecall, they often overlook two critical aspects: the ability of models to\nintegrate evolving knowledge through continual learning and the significant\nregional disparities in their performance. To address these gaps, we introduce\nthe Timely Events Benchmark (TiEBe), a dataset containing over 11,000\nquestion-answer pairs focused on globally and regionally significant events.\nTiEBe leverages structured retrospective data from Wikipedia, enabling\ncontinuous updates to assess LLMs' knowledge of evolving global affairs and\ntheir understanding of events across different regions. Our benchmark\ndemonstrates that LLMs exhibit substantial geographic disparities in factual\nrecall, emphasizing the need for more balanced global knowledge representation.\nFurthermore, TiEBe serves as a tool for evaluating continual learning\nstrategies, providing insights into models' ability to acquire new information\nwithout forgetting past knowledge.", "published": "2025-01-13 16:58:32", "link": "http://arxiv.org/abs/2501.07482v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Parallel Key-Value Cache Fusion for Position Invariant RAG", "abstract": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines.", "published": "2025-01-13 17:50:30", "link": "http://arxiv.org/abs/2501.07523v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal\n  Aspects in Video Editing", "abstract": "Video editing models have advanced significantly, but evaluating their\nperformance remains challenging. Traditional metrics, such as CLIP text and\nimage scores, often fall short: text scores are limited by inadequate training\ndata and hierarchical dependencies, while image scores fail to assess temporal\nconsistency. We present SST-EM (Semantic, Spatial, and Temporal Evaluation\nMetric), a novel evaluation framework that leverages modern Vision-Language\nModels (VLMs), Object Detection, and Temporal Consistency checks. SST-EM\ncomprises four components: (1) semantic extraction from frames using a VLM, (2)\nprimary object tracking with Object Detection, (3) focused object refinement\nvia an LLM agent, and (4) temporal consistency assessment using a Vision\nTransformer (ViT). These components are integrated into a unified metric with\nweights derived from human evaluations and regression analysis. The name SST-EM\nreflects its focus on Semantic, Spatial, and Temporal aspects of video\nevaluation. SST-EM provides a comprehensive evaluation of semantic fidelity and\ntemporal smoothness in video editing. The source code is available in the\n\\textbf{\\href{https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git}{GitHub\nRepository}}.", "published": "2025-01-13 18:37:08", "link": "http://arxiv.org/abs/2501.07554v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "WebWalker: Benchmarking LLMs in Web Traversal", "abstract": "Retrieval-augmented generation (RAG) demonstrates remarkable performance\nacross tasks in open-domain question-answering. However, traditional search\nengines may retrieve shallow content, limiting the ability of LLMs to handle\ncomplex, multi-layered information. To address it, we introduce WebWalkerQA, a\nbenchmark designed to assess the ability of LLMs to perform web traversal. It\nevaluates the capacity of LLMs to traverse a website's subpages to extract\nhigh-quality data systematically. We propose WebWalker, which is a multi-agent\nframework that mimics human-like web navigation through an explore-critic\nparadigm. Extensive experimental results show that WebWalkerQA is challenging\nand demonstrates the effectiveness of RAG combined with WebWalker, through the\nhorizontal and vertical integration in real-world scenarios.", "published": "2025-01-13 18:58:07", "link": "http://arxiv.org/abs/2501.07572v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey of Early Exit Deep Neural Networks in NLP", "abstract": "Deep Neural Networks (DNNs) have grown increasingly large in size to achieve\nstate of the art performance across a wide range of tasks. However, their high\ncomputational requirements make them less suitable for resource-constrained\napplications. Also, real-world datasets often consist of a mixture of easy and\ncomplex samples, necessitating adaptive inference mechanisms that account for\nsample difficulty. Early exit strategies offer a promising solution by enabling\nadaptive inference, where simpler samples are classified using the initial\nlayers of the DNN, thereby accelerating the overall inference process. By\nattaching classifiers at different layers, early exit methods not only reduce\ninference latency but also improve the model robustness against adversarial\nattacks. This paper presents a comprehensive survey of early exit methods and\ntheir applications in NLP.", "published": "2025-01-13 20:08:52", "link": "http://arxiv.org/abs/2501.07670v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ESURF: Simple and Effective EDU Segmentation", "abstract": "Segmenting text into Elemental Discourse Units (EDUs) is a fundamental task\nin discourse parsing. We present a new simple method for identifying EDU\nboundaries, and hence segmenting them, based on lexical and character n-gram\nfeatures, using random forest classification. We show that the method, despite\nits simplicity, outperforms other methods both for segmentation and within a\nstate of the art discourse parser. This indicates the importance of such\nfeatures for identifying basic discourse elements, pointing towards potentially\nmore training-efficient methods for discourse analysis.", "published": "2025-01-13 22:18:52", "link": "http://arxiv.org/abs/2501.07723v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging ASIC AI Chips for Homomorphic Encryption", "abstract": "Cloud-based services are making the outsourcing of sensitive client data\nincreasingly common. Although homomorphic encryption (HE) offers strong privacy\nguarantee, it requires substantially more resources than computing on\nplaintext, often leading to unacceptably large latencies in getting the\nresults. HE accelerators have emerged to mitigate this latency issue, but with\nthe high cost of ASICs. In this paper we show that HE primitives can be\nconverted to AI operators and accelerated on existing ASIC AI accelerators,\nlike TPUs, which are already widely deployed in the cloud. Adapting such\naccelerators for HE requires (1) supporting modular multiplication, (2)\nhigh-precision arithmetic in software, and (3) efficient mapping on matrix\nengines. We introduce the CROSS compiler (1) to adopt Barrett reduction to\nprovide modular reduction support using multiplier and adder, (2) Basis Aligned\nTransformation (BAT) to convert high-precision multiplication as low-precision\nmatrix-vector multiplication, (3) Matrix Aligned Transformation (MAT) to covert\nvectorized modular operation with reduction into matrix multiplication that can\nbe efficiently processed on 2D spatial matrix engine. Our evaluation of CROSS\non a Google TPUv4 demonstrates significant performance improvements, with up to\n161x and 5x speedup compared to the previous work on many-core CPUs and V100.\nThe kernel-level codes are open-sourced at\nhttps://github.com/google/jaxite/tree/main/jaxite_word.", "published": "2025-01-13 04:08:14", "link": "http://arxiv.org/abs/2501.07047v2", "categories": ["cs.CR", "cs.AR", "cs.CL", "cs.PL"], "primary_category": "cs.CR"}
{"title": "AdaCS: Adaptive Normalization for Enhanced Code-Switching ASR", "abstract": "Intra-sentential code-switching (CS) refers to the alternation between\nlanguages that happens within a single utterance and is a significant challenge\nfor Automatic Speech Recognition (ASR) systems. For example, when a Vietnamese\nspeaker uses foreign proper names or specialized terms within their speech. ASR\nsystems often struggle to accurately transcribe intra-sentential CS due to\ntheir training on monolingual data and the unpredictable nature of CS. This\nissue is even more pronounced for low-resource languages, where limited data\navailability hinders the development of robust models. In this study, we\npropose AdaCS, a normalization model integrates an adaptive bias attention\nmodule (BAM) into encoder-decoder network. This novel approach provides a\nrobust solution to CS ASR in unseen domains, thereby significantly enhancing\nour contribution to the field. By utilizing BAM to both identify and normalize\nCS phrases, AdaCS enhances its adaptive capabilities with a biased list of\nwords provided during inference. Our method demonstrates impressive performance\nand the ability to handle unseen CS phrases across various domains. Experiments\nshow that AdaCS outperforms previous state-of-the-art method on Vietnamese CS\nASR normalization by considerable WER reduction of 56.2% and 36.8% on the two\nproposed test sets.", "published": "2025-01-13 07:27:00", "link": "http://arxiv.org/abs/2501.07102v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Audio-CoT: Exploring Chain-of-Thought Reasoning in Large Audio Language\n  Model", "abstract": "Large Audio-Language Models (LALMs) have demonstrated remarkable performance\nin tasks involving audio perception and understanding, such as speech\nrecognition and audio captioning. However, their reasoning capabilities -\ncritical for solving complex real-world problems - remain underexplored. In\nthis work, we conduct the first exploration into integrating Chain-of-Thought\n(CoT) reasoning into LALMs to enhance their reasoning ability across auditory\nmodalities. We evaluate representative CoT methods, analyzing their performance\nin both information extraction and reasoning tasks across sound, music, and\nspeech domains. Our findings reveal that CoT methods significantly improve\nperformance on easy and medium tasks but encounter challenges with hard tasks,\nwhere reasoning chains can confuse the model rather than improve accuracy.\nAdditionally, we identify a positive correlation between reasoning path length\nand accuracy, demonstrating the potential of scaling inference for advanced\ninstruction-following and reasoning. This study not only highlights the promise\nof CoT in enhancing LALM reasoning capabilities but also identifies key\nlimitations and provides actionable directions for future research.", "published": "2025-01-13 11:54:40", "link": "http://arxiv.org/abs/2501.07246v1", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Lessons of Developing Process Reward Models in Mathematical\n  Reasoning", "abstract": "Process Reward Models (PRMs) emerge as a promising approach for process\nsupervision in mathematical reasoning of Large Language Models (LLMs), which\naim to identify and mitigate intermediate errors in the reasoning processes.\nHowever, the development of effective PRMs faces significant challenges,\nparticularly in data annotation and evaluation methodologies. In this paper,\nthrough extensive experiments, we demonstrate that commonly used Monte Carlo\n(MC) estimation-based data synthesis for PRMs typically yields inferior\nperformance and generalization compared to LLM-as-a-judge and human annotation\nmethods. MC estimation relies on completion models to evaluate current-step\ncorrectness, leading to inaccurate step verification. Furthermore, we identify\npotential biases in conventional Best-of-N (BoN) evaluation strategies for\nPRMs: (1) The unreliable policy models generate responses with correct answers\nbut flawed processes, leading to a misalignment between the evaluation criteria\nof BoN and the PRM objectives of process verification. (2) The tolerance of\nPRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a\nsignificant proportion of minimum scores concentrated on the final answer\nsteps, revealing the shift from process to outcome-based assessment in BoN\nOptimized PRMs. To address these challenges, we develop a consensus filtering\nmechanism that effectively integrates MC estimation with LLM-as-a-judge and\nadvocates a more comprehensive evaluation framework that combines\nresponse-level and step-level metrics. Based on the mechanisms, we\nsignificantly improve both model performance and data efficiency in the BoN\nevaluation and the step-wise error identification task. Finally, we release a\nnew state-of-the-art PRM that outperforms existing open-source alternatives and\nprovides practical guidelines for future research in building process\nsupervision models.", "published": "2025-01-13 13:10:16", "link": "http://arxiv.org/abs/2501.07301v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Joint Automatic Speech Recognition And Structure Learning For Better\n  Speech Understanding", "abstract": "Spoken language understanding (SLU) is a structure prediction task in the\nfield of speech. Recently, many works on SLU that treat it as a\nsequence-to-sequence task have achieved great success. However, This method is\nnot suitable for simultaneous speech recognition and understanding. In this\npaper, we propose a joint speech recognition and structure learning framework\n(JSRSL), an end-to-end SLU model based on span, which can accurately transcribe\nspeech and extract structured content simultaneously. We conduct experiments on\nname entity recognition and intent classification using the Chinese dataset\nAISHELL-NER and the English dataset SLURP. The results show that our proposed\nmethod not only outperforms the traditional sequence-to-sequence method in both\ntranscription and extraction capabilities but also achieves state-of-the-art\nperformance on the two datasets.", "published": "2025-01-13 13:43:46", "link": "http://arxiv.org/abs/2501.07329v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Imagine while Reasoning in Space: Multimodal Visualization-of-Thought", "abstract": "Chain-of-Thought (CoT) prompting has proven highly effective for enhancing\ncomplex reasoning in Large Language Models (LLMs) and Multimodal Large Language\nModels (MLLMs). Yet, it struggles in complex spatial reasoning tasks.\nNonetheless, human cognition extends beyond language alone, enabling the\nremarkable capability to think in both words and images. Inspired by this\nmechanism, we propose a new reasoning paradigm, Multimodal\nVisualization-of-Thought (MVoT). It enables visual thinking in MLLMs by\ngenerating image visualizations of their reasoning traces. To ensure\nhigh-quality visualization, we introduce token discrepancy loss into\nautoregressive MLLMs. This innovation significantly improves both visual\ncoherence and fidelity. We validate this approach through several dynamic\nspatial reasoning tasks. Experimental results reveal that MVoT demonstrates\ncompetitive performance across tasks. Moreover, it exhibits robust and reliable\nimprovements in the most challenging scenarios where CoT fails. Ultimately,\nMVoT establishes new possibilities for complex reasoning tasks where visual\nthinking can effectively complement verbal reasoning.", "published": "2025-01-13 18:23:57", "link": "http://arxiv.org/abs/2501.07542v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Heterogeneous Multimodal Graph Learning Framework for Recognizing User\n  Emotions in Social Networks", "abstract": "The rapid expansion of social media platforms has provided unprecedented\naccess to massive amounts of multimodal user-generated content. Comprehending\nuser emotions can provide valuable insights for improving communication and\nunderstanding of human behaviors. Despite significant advancements in Affective\nComputing, the diverse factors influencing user emotions in social networks\nremain relatively understudied. Moreover, there is a notable lack of deep\nlearning-based methods for predicting user emotions in social networks, which\ncould be addressed by leveraging the extensive multimodal data available. This\nwork presents a novel formulation of personalized emotion prediction in social\nnetworks based on heterogeneous graph learning. Building upon this formulation,\nwe design HMG-Emo, a Heterogeneous Multimodal Graph Learning Framework that\nutilizes deep learning-based features for user emotion recognition.\nAdditionally, we include a dynamic context fusion module in HMG-Emo that is\ncapable of adaptively integrating the different modalities in social media\ndata. Through extensive experiments, we demonstrate the effectiveness of\nHMG-Emo and verify the superiority of adopting a graph neural network-based\napproach, which outperforms existing baselines that use rich hand-crafted\nfeatures. To the best of our knowledge, HMG-Emo is the first multimodal and\ndeep-learning-based approach to predict personalized emotions within online\nsocial networks. Our work highlights the significance of exploiting advanced\ndeep learning techniques for less-explored problems in Affective Computing.", "published": "2025-01-13 23:21:33", "link": "http://arxiv.org/abs/2501.07746v1", "categories": ["cs.SI", "cs.CL", "cs.CV"], "primary_category": "cs.SI"}
{"title": "MathReader : Text-to-Speech for Mathematical Documents", "abstract": "TTS (Text-to-Speech) document reader from Microsoft, Adobe, Apple, and OpenAI\nhave been serviced worldwide. They provide relatively good TTS results for\ngeneral plain text, but sometimes skip contents or provide unsatisfactory\nresults for mathematical expressions. This is because most modern academic\npapers are written in LaTeX, and when LaTeX formulas are compiled, they are\nrendered as distinctive text forms within the document. However, traditional\nTTS document readers output only the text as it is recognized, without\nconsidering the mathematical meaning of the formulas. To address this issue, we\npropose MathReader, which effectively integrates OCR, a fine-tuned T5 model,\nand TTS. MathReader demonstrated a lower Word Error Rate (WER) than existing\nTTS document readers, such as Microsoft Edge and Adobe Acrobat, when processing\ndocuments containing mathematical formulas. MathReader reduced the WER from\n0.510 to 0.281 compared to Microsoft Edge, and from 0.617 to 0.281 compared to\nAdobe Acrobat. This will significantly contribute to alleviating the\ninconvenience faced by users who want to listen to documents, especially those\nwho are visually impaired. The code is available at\nhttps://github.com/hyeonsieun/MathReader.", "published": "2025-01-13 06:47:05", "link": "http://arxiv.org/abs/2501.07088v2", "categories": ["cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.AI"}
{"title": "Microphone Array Signal Processing and Deep Learning for Speech\n  Enhancement", "abstract": "Multi-channel acoustic signal processing is a well-established and powerful\ntool to exploit the spatial diversity between a target signal and non-target or\nnoise sources for signal enhancement. However, the textbook solutions for\noptimal data-dependent spatial filtering rest on the knowledge of second-order\nstatistical moments of the signals, which have traditionally been difficult to\nacquire. In this contribution, we compare model-based, purely data-driven, and\nhybrid approaches to parameter estimation and filtering, where the latter tries\nto combine the benefits of model-based signal processing and data-driven deep\nlearning to overcome their individual deficiencies. We illustrate the\nunderlying design principles with examples from noise reduction, source\nseparation, and dereverberation.", "published": "2025-01-13 11:13:47", "link": "http://arxiv.org/abs/2501.07215v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Estimating Musical Surprisal in Audio", "abstract": "In modeling musical surprisal expectancy with computational methods, it has\nbeen proposed to use the information content (IC) of one-step predictions from\nan autoregressive model as a proxy for surprisal in symbolic music. With an\nappropriately chosen model, the IC of musical events has been shown to\ncorrelate with human perception of surprise and complexity aspects, including\ntonal and rhythmic complexity. This work investigates whether an analogous\nmethodology can be applied to music audio. We train an autoregressive\nTransformer model to predict compressed latent audio representations of a\npretrained autoencoder network. We verify learning effects by estimating the\ndecrease in IC with repetitions. We investigate the mean IC of musical segment\ntypes (e.g., A or B) and find that segment types appearing later in a piece\nhave a higher IC than earlier ones on average. We investigate the IC's relation\nto audio and musical features and find it correlated with timbral variations\nand loudness and, to a lesser extent, dissonance, rhythmic complexity, and\nonset density related to audio and musical features. Finally, we investigate if\nthe IC can predict EEG responses to songs and thus model humans' surprisal in\nmusic. We provide code for our method on github.com/sonycslparis/audioic.", "published": "2025-01-13 16:46:45", "link": "http://arxiv.org/abs/2501.07474v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Completing Sets of Prototype Transfer Functions for Subspace-based\n  Direction of Arrival Estimation of Multiple Speakers", "abstract": "To estimate the direction of arrival (DOA) of multiple speakers,\nsubspace-based prototype transfer function matching methods such as multiple\nsignal classification (MUSIC) or relative transfer function (RTF) vector\nmatching are commonly employed. In general, these methods require calibrated\nmicrophone arrays, which are characterized by a known array geometry or a set\nof known prototype transfer functions for several directions. In this paper, we\nconsider a partially calibrated microphone array, composed of a calibrated\nbinaural hearing aid and a (non-calibrated) external microphone at an unknown\nlocation with no available set of prototype transfer functions. We propose a\nprocedure for completing sets of prototype transfer functions by exploiting the\northogonality of subspaces, allowing to apply matching-based DOA estimation\nmethods with partially calibrated microphone arrays. For the MUSIC and RTF\nvector matching methods, experimental results for two speakers in noisy and\nreverberant environments clearly demonstrate that for all locations of the\nexternal microphone DOAs can be estimated more accurately with completed sets\nof prototype transfer functions than with incomplete sets.\n  \\c{opyright}20XX IEEE. Personal use of this material is permitted. Permission\nfrom IEEE must be obtained for all other uses, in any current or future media,\nincluding reprinting/republishing this material for advertising or promotional\npurposes, creating new collective works, for resale or redistribution to\nservers or lists, or reuse of any copyrighted component of this work in other\nworks.", "published": "2025-01-13 17:53:34", "link": "http://arxiv.org/abs/2501.07524v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Decoding Musical Evolution Through Network Science", "abstract": "Music has always been central to human culture, reflecting and shaping\ntraditions, emotions, and societal changes. Technological advancements have\ntransformed how music is created and consumed, influencing tastes and the music\nitself. In this study, we use Network Science to analyze musical complexity.\nDrawing on $\\approx20,000$ MIDI files across six macro-genres spanning nearly\nfour centuries, we represent each composition as a weighted directed network to\nstudy its structural properties. Our results show that Classical and Jazz\ncompositions have higher complexity and melodic diversity than recently\ndeveloped genres. However, a temporal analysis reveals a trend toward\nsimplification, with even Classical and Jazz nearing the complexity levels of\nmodern genres. This study highlights how digital tools and streaming platforms\nshape musical evolution, fostering new genres while driving homogenization and\nsimplicity.", "published": "2025-01-13 18:39:44", "link": "http://arxiv.org/abs/2501.07557v1", "categories": ["cs.SD", "cs.CY", "eess.AS", "physics.soc-ph"], "primary_category": "cs.SD"}
{"title": "Discrimination loss vs. SRT: A model-based approach towards harmonizing\n  speech test interpretations", "abstract": "Objective: Speech tests aim to estimate discrimination loss or speech\nrecognition threshold (SRT). This paper investigates the potential to estimate\nSRTs from clinical data that target at characterizing the discrimination loss.\nKnowledge about the relationship between the speech test outcome\nvariables--conceptually linked via the psychometric function--is important\ntowards integration of data from different databases.\n  Design: Depending on the available data, different SRT estimation procedures\nwere compared and evaluated. A novel, model-based SRT estimation procedure was\nproposed that deals with incomplete patient data. Interpretations of\nsupra-threshold deficits were assessed for the two interpretation modes.\n  Study sample: Data for 27009 patients with Freiburg monosyllabic speech test\n(FMST) and audiogram (AG) results from the same day were included in the\nretrospective analysis.\n  Results: The model-based SRT estimation procedure provided accurate SRTs, but\nwith large deviations in the estimated slope. Supra-threshold hearing loss\ncomponents differed between the two interpretation modes.\n  Conclusions: The model-based procedure can be used for SRT estimation, and\nits properties relate to data availability for individual patients. All SRT\nprocedures are influenced by the uncertainty of the word recognition scores. In\nthe future, the proposed approach can be used to assess additional differences\nbetween speech tests.", "published": "2025-01-13 09:39:37", "link": "http://arxiv.org/abs/2501.08921v1", "categories": ["cs.SD", "eess.AS", "physics.med-ph"], "primary_category": "cs.SD"}
