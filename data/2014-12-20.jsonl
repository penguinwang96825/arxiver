{"title": "Embedding Entities and Relations for Learning and Inference in Knowledge\n  Bases", "abstract": "We consider learning representations of entities and relations in KBs using\nthe neural-embedding approach. We show that most existing models, including NTN\n(Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized\nunder a unified learning framework, where entities are low-dimensional vectors\nlearned from a neural network and relations are bilinear and/or linear mapping\nfunctions. Under this framework, we compare a variety of embedding models on\nthe link prediction task. We show that a simple bilinear formulation achieves\nnew state-of-the-art results for the task (achieving a top-10 accuracy of 73.2%\nvs. 54.7% by TransE on Freebase). Furthermore, we introduce a novel approach\nthat utilizes the learned relation embeddings to mine logical rules such as\n\"BornInCity(a,b) and CityInCountry(b,c) => Nationality(a,c)\". We find that\nembeddings learned from the bilinear objective are particularly good at\ncapturing relational semantics and that the composition of relations is\ncharacterized by matrix multiplication. More interestingly, we demonstrate that\nour embedding-based rule extraction approach successfully outperforms a\nstate-of-the-art confidence-based rule mining approach in mining Horn rules\nthat involve compositional reasoning.", "published": "2014-12-20 01:37:16", "link": "http://arxiv.org/abs/1412.6575v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving zero-shot learning by mitigating the hubness problem", "abstract": "The zero-shot paradigm exploits vector-based word representations extracted\nfrom text corpora with unsupervised methods to learn general mapping functions\nfrom other feature spaces onto word space, where the words associated to the\nnearest neighbours of the mapped vectors are used as their linguistic labels.\nWe show that the neighbourhoods of the mapped elements are strongly polluted by\nhubs, vectors that tend to be near a high proportion of items, pushing their\ncorrect labels down the neighbour list. After illustrating the problem\nempirically, we propose a simple method to correct it by taking the proximity\ndistribution of potential neighbours across many mapped vectors into account.\nWe show that this correction leads to consistent improvements in realistic\nzero-shot experiments in the cross-lingual, image labeling and image retrieval\ndomains.", "published": "2014-12-20 01:03:46", "link": "http://arxiv.org/abs/1412.6568v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Outperforming Word2Vec on Analogy Tasks with Random Projections", "abstract": "We present a distributed vector representation based on a simplification of\nthe BEAGLE system, designed in the context of the Sigma cognitive architecture.\nOur method does not require gradient-based training of neural networks, matrix\ndecompositions as with LSA, or convolutions as with BEAGLE. All that is\ninvolved is a sum of random vectors and their pointwise products. Despite the\nsimplicity of this technique, it gives state-of-the-art results on analogy\nproblems, in most cases better than Word2Vec. To explain this success, we\ninterpret it as a dimension reduction via random projection.", "published": "2014-12-20 07:07:29", "link": "http://arxiv.org/abs/1412.6616v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Word Representations via Gaussian Embedding", "abstract": "Current work in lexical distributed representations maps each word to a point\nvector in low-dimensional space. Mapping instead to a density provides many\ninteresting advantages, including better capturing uncertainty about a\nrepresentation and its relationships, expressing asymmetries more naturally\nthan dot product or cosine similarity, and enabling more expressive\nparameterization of decision boundaries. This paper advocates for density-based\ndistributed embeddings and presents a method for learning representations in\nthe space of Gaussian distributions. We compare performance on various word\nembedding benchmarks, investigate the ability of these embeddings to model\nentailment and other asymmetric relationships, and explore novel properties of\nthe representation.", "published": "2014-12-20 07:42:40", "link": "http://arxiv.org/abs/1412.6623v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Modeling Compositionality with Multiplicative Recurrent Neural Networks", "abstract": "We present the multiplicative recurrent neural network as a general model for\ncompositional meaning in language, and evaluate it on the task of fine-grained\nsentiment analysis. We establish a connection to the previously investigated\nmatrix-space models for compositionality, and show they are special cases of\nthe multiplicative recurrent net. Our experiments show that these models\nperform comparably or better than Elman-type additive recurrent neural networks\nand outperform matrix-space models on a standard fine-grained sentiment\nanalysis corpus. Furthermore, they yield comparable results to structural deep\nmodels on the recently published Stanford Sentiment Treebank without the need\nfor generating parse trees.", "published": "2014-12-20 01:53:22", "link": "http://arxiv.org/abs/1412.6577v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)", "abstract": "In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model\nfor generating novel image captions. It directly models the probability\ndistribution of generating a word given previous words and an image. Image\ncaptions are generated by sampling from this distribution. The model consists\nof two sub-networks: a deep recurrent neural network for sentences and a deep\nconvolutional network for images. These two sub-networks interact with each\nother in a multimodal layer to form the whole m-RNN model. The effectiveness of\nour model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K,\nFlickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In\naddition, we apply the m-RNN model to retrieval tasks for retrieving images or\nsentences, and achieves significant performance improvement over the\nstate-of-the-art methods which directly optimize the ranking objective function\nfor retrieval. The project page of this work is:\nwww.stat.ucla.edu/~junhua.mao/m-RNN.html .", "published": "2014-12-20 08:10:04", "link": "http://arxiv.org/abs/1412.6632v5", "categories": ["cs.CV", "cs.CL", "cs.LG", "I.2.6; I.2.7; I.2.10"], "primary_category": "cs.CV"}
{"title": "Weakly Supervised Multi-Embeddings Learning of Acoustic Models", "abstract": "We trained a Siamese network with multi-task same/different information on a\nspeech dataset, and found that it was possible to share a network for both\ntasks without a loss in performance. The first task was to discriminate between\ntwo same or different words, and the second was to discriminate between two\nsame or different talkers.", "published": "2014-12-20 11:54:41", "link": "http://arxiv.org/abs/1412.6645v3", "categories": ["cs.SD", "cs.CL", "cs.LG", "I.2.6; I.2.7; I.5.1"], "primary_category": "cs.SD"}
{"title": "Incremental Adaptation Strategies for Neural Network Language Models", "abstract": "It is today acknowledged that neural network language models outperform\nbackoff language models in applications like speech recognition or statistical\nmachine translation. However, training these models on large amounts of data\ncan take several days. We present efficient techniques to adapt a neural\nnetwork language model to new data. Instead of training a completely new model\nor relying on mixture approaches, we propose two new methods: continued\ntraining on resampled data or insertion of adaptation layers. We present\nexperimental results in an CAT environment where the post-edits of professional\ntranslators are used to improve an SMT system. Both methods are very fast and\nachieve significant improvements without overfitting the small adaptation data.", "published": "2014-12-20 13:06:05", "link": "http://arxiv.org/abs/1412.6650v4", "categories": ["cs.NE", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
