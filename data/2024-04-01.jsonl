{"title": "TM-TREK at SemEval-2024 Task 8: Towards LLM-Based Automatic Boundary\n  Detection for Human-Machine Mixed Text", "abstract": "With the increasing prevalence of text generated by large language models\n(LLMs), there is a growing concern about distinguishing between LLM-generated\nand human-written texts in order to prevent the misuse of LLMs, such as the\ndissemination of misleading information and academic dishonesty. Previous\nresearch has primarily focused on classifying text as either entirely\nhuman-written or LLM-generated, neglecting the detection of mixed texts that\ncontain both types of content. This paper explores LLMs' ability to identify\nboundaries in human-written and machine-generated mixed texts. We approach this\ntask by transforming it into a token classification problem and regard the\nlabel turning point as the boundary. Notably, our ensemble model of LLMs\nachieved first place in the 'Human-Machine Mixed Text Detection' sub-task of\nthe SemEval'24 Competition Task 8. Additionally, we investigate factors that\ninfluence the capability of LLMs in detecting boundaries within mixed texts,\nincluding the incorporation of extra layers on top of LLMs, combination of\nsegmentation loss, and the impact of pretraining. Our findings aim to provide\nvaluable insights for future research in this area.", "published": "2024-04-01 03:54:42", "link": "http://arxiv.org/abs/2404.00899v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PSYDIAL: Personality-based Synthetic Dialogue Generation using Large\n  Language Models", "abstract": "We present a novel end-to-end personality-based synthetic dialogue data\ngeneration pipeline, specifically designed to elicit responses from large\nlanguage models via prompting. We design the prompts to generate more\nhuman-like dialogues considering real-world scenarios when users engage with\nchatbots. We introduce PSYDIAL, the first Korean dialogue dataset focused on\npersonality-based dialogues, curated using our proposed pipeline. Notably, we\nfocus on the Extraversion dimension of the Big Five personality model in our\nresearch. Experimental results indicate that while pre-trained models and those\nfine-tuned with a chit-chat dataset struggle to generate responses reflecting\npersonality, models trained with PSYDIAL show significant improvements. The\nversatility of our pipeline extends beyond dialogue tasks, offering potential\nfor other non-dialogue related applications. This research opens doors for more\nnuanced, personality-driven conversational AI in Korean and potentially other\nlanguages. Our code is publicly available at\nhttps://github.com/jiSilverH/psydial.", "published": "2024-04-01 05:19:34", "link": "http://arxiv.org/abs/2404.00930v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChatGLM-RLHF: Practices of Aligning Large Language Models with Human\n  Feedback", "abstract": "ChatGLM is a free-to-use AI service powered by the ChatGLM family of large\nlanguage models (LLMs). In this paper, we present the ChatGLM-RLHF pipeline --\na reinforcement learning from human feedback (RLHF) system -- designed to\nenhance ChatGLM's alignment with human preferences. ChatGLM-RLHF encompasses\nthree major components: the collection of human preference data, the training\nof the reward model, and the optimization of policies. Throughout the process\nof integrating ChatGLM-RLHF into production, we encountered and addressed\nseveral unprecedented challenges. We introduce the strategies to mitigate\nreward variance for stabilized large-scale training, implement model\nparallelism with fused gradient-descent, and design regularization constraints\nto avoid catastrophic forgetting in LLMs. Experiments show that ChatGLM-RLHF\nbrings significant improvements in alignment tasks compared to the supervised\nfine-tuned (SFT) version of ChatGLM. For instance, it achieves on average 15\\%\nmore wins against ChatGLM-SFT in Chinese alignment tasks. The work presents our\npractices of aligning LLMs with human preferences, offering insights into the\nchallenges and solutions in RLHF implementations.", "published": "2024-04-01 05:39:36", "link": "http://arxiv.org/abs/2404.00934v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AISPACE at SemEval-2024 task 8: A Class-balanced Soft-voting System for\n  Detecting Multi-generator Machine-generated Text", "abstract": "SemEval-2024 Task 8 provides a challenge to detect human-written and\nmachine-generated text. There are 3 subtasks for different detection scenarios.\nThis paper proposes a system that mainly deals with Subtask B. It aims to\ndetect if given full text is written by human or is generated by a specific\nLarge Language Model (LLM), which is actually a multi-class text classification\ntask. Our team AISPACE conducted a systematic study of fine-tuning\ntransformer-based models, including encoderonly, decoder-only and\nencoder-decoder models. We compared their performance on this task and\nidentified that encoder-only models performed exceptionally well. We also\napplied a weighted Cross Entropy loss function to address the issue of data\nimbalance of different class samples. Additionally, we employed softvoting\nstrategy over multi-models ensemble to enhance the reliability of our\npredictions. Our system ranked top 1 in Subtask B, which sets a\nstate-of-the-art benchmark for this new challenge.", "published": "2024-04-01 06:25:47", "link": "http://arxiv.org/abs/2404.00950v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prior Constraints-based Reward Model Training for Aligning Large\n  Language Models", "abstract": "Reinforcement learning with human feedback for aligning large language models\n(LLMs) trains a reward model typically using ranking loss with comparison\npairs.However, the training procedure suffers from an inherent problem: the\nuncontrolled scaling of reward scores during reinforcement learning due to the\nlack of constraints while training the reward model.This paper proposes a Prior\nConstraints-based Reward Model (namely PCRM) training method to mitigate this\nproblem. PCRM incorporates prior constraints, specifically, length ratio and\ncosine similarity between outputs of each comparison pair, during reward model\ntraining to regulate optimization magnitude and control score margins. We\ncomprehensively evaluate PCRM by examining its rank correlation with human\npreferences and its effectiveness in aligning LLMs via RL. Experimental results\ndemonstrate that PCRM significantly improves alignment performance by\neffectively constraining reward score scaling. As another bonus, our method is\neasily integrated into arbitrary rank-based alignment methods, such as direct\npreference optimization, and can yield consistent improvement.", "published": "2024-04-01 07:49:11", "link": "http://arxiv.org/abs/2404.00978v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Nexus of Large Language Models and Legal Systems: A Short\n  Survey", "abstract": "With the advancement of Artificial Intelligence (AI) and Large Language\nModels (LLMs), there is a profound transformation occurring in the realm of\nnatural language processing tasks within the legal domain. The capabilities of\nLLMs are increasingly demonstrating unique roles in the legal sector, bringing\nboth distinctive benefits and various challenges. This survey delves into the\nsynergy between LLMs and the legal system, such as their applications in tasks\nlike legal text comprehension, case retrieval, and analysis. Furthermore, this\nsurvey highlights key challenges faced by LLMs in the legal domain, including\nbias, interpretability, and ethical considerations, as well as how researchers\nare addressing these issues. The survey showcases the latest advancements in\nfine-tuned legal LLMs tailored for various legal systems, along with legal\ndatasets available for fine-tuning LLMs in various languages. Additionally, it\nproposes directions for future research and development.", "published": "2024-04-01 08:35:56", "link": "http://arxiv.org/abs/2404.00990v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Causes the Failure of Explicit to Implicit Discourse Relation\n  Recognition?", "abstract": "We consider an unanswered question in the discourse processing community: why\ndo relation classifiers trained on explicit examples (with connectives removed)\nperform poorly in real implicit scenarios? Prior work claimed this is due to\nlinguistic dissimilarity between explicit and implicit examples but provided no\nempirical evidence. In this study, we show that one cause for such failure is a\nlabel shift after connectives are eliminated. Specifically, we find that the\ndiscourse relations expressed by some explicit instances will change when\nconnectives disappear. Unlike previous work manually analyzing a few examples,\nwe present empirical evidence at the corpus level to prove the existence of\nsuch shift. Then, we analyze why label shift occurs by considering factors such\nas the syntactic role played by connectives, ambiguity of connectives, and\nmore. Finally, we investigate two strategies to mitigate the label shift:\nfiltering out noisy data and joint learning with connectives. Experiments on\nPDTB 2.0, PDTB 3.0, and the GUM dataset demonstrate that classifiers trained\nwith our strategies outperform strong baselines.", "published": "2024-04-01 09:08:53", "link": "http://arxiv.org/abs/2404.00999v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constructing and Expanding Low-Resource and Underrepresented Parallel\n  Datasets for Indonesian Local Languages", "abstract": "In Indonesia, local languages play an integral role in the culture. However,\nthe available Indonesian language resources still fall into the category of\nlimited data in the Natural Language Processing (NLP) field. This is become\nproblematic when build NLP model for these languages. To address this gap, we\nintroduce Bhinneka Korpus, a multilingual parallel corpus featuring five\nIndonesian local languages. Our goal is to enhance access and utilization of\nthese resources, extending their reach within the country. We explained in a\ndetail the dataset collection process and associated challenges. Additionally,\nwe experimented with translation task using the IBM Model 1 due to data\nconstraints. The result showed that the performance of each language already\nshows good indications for further development. Challenges such as lexical\nvariation, smoothing effects, and cross-linguistic variability are discussed.\nWe intend to evaluate the corpus using advanced NLP techniques for low-resource\nlanguages, paving the way for multilingual translation models.", "published": "2024-04-01 09:24:06", "link": "http://arxiv.org/abs/2404.01009v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PairEval: Open-domain Dialogue Evaluation with Pairwise Comparison", "abstract": "Building a reliable and automated evaluation metric is a necessary but\nchallenging problem for open-domain dialogue systems. Recent studies proposed\nevaluation metrics that assess generated responses by considering their\nrelevance to previous dialogue histories. Although effective, these metrics\nevaluate individual responses directly rather than considering their relative\nquality compared to other responses. To handle this, we propose PairEval, a\nnovel dialogue evaluation metric for assessing responses by comparing their\nquality against responses in different conversations. PairEval is built on top\nof open-sourced and moderate-size language models, and we make them specialized\nin pairwise comparison between dialogue responses. Extensive experiments on\nmultiple benchmarks demonstrate that our metric exhibits a higher correlation\nwith human judgments than baseline metrics. We also find that the proposed\ncomparative metric is more robust in detecting common failures from open-domain\ndialogue systems, including repetition and speaker insensitivity.", "published": "2024-04-01 09:35:06", "link": "http://arxiv.org/abs/2404.01015v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Verifying Claims About Metaphors with Large-Scale Automatic Metaphor\n  Identification", "abstract": "There are several linguistic claims about situations where words are more\nlikely to be used as metaphors. However, few studies have sought to verify such\nclaims with large corpora. This study entails a large-scale, corpus-based\nanalysis of certain existing claims about verb metaphors, by applying metaphor\ndetection to sentences extracted from Common Crawl and using the statistics\nobtained from the results. The verification results indicate that the direct\nobjects of verbs used as metaphors tend to have lower degrees of concreteness,\nimageability, and familiarity, and that metaphors are more likely to be used in\nemotional and subjective sentences.", "published": "2024-04-01 10:17:45", "link": "http://arxiv.org/abs/2404.01029v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Mystery of Influential Data for Mathematical Reasoning", "abstract": "Selecting influential data for fine-tuning on downstream tasks is a key\nfactor for both performance and computation efficiency. Recent works have shown\nthat training with only limited data can show a superior performance on general\ntasks. However, the feasibility on mathematical reasoning tasks has not been\nvalidated. To go further, there exist two open questions for mathematical\nreasoning: how to select influential data and what is an influential data\ncomposition. For the former one, we propose a Quality-aware Diverse Selection\n(QaDS) strategy adaptable for mathematical reasoning. A comparison with other\nselection strategies validates the superiority of QaDS. For the latter one, we\nfirst enlarge our setting and explore the influential data composition. We\nconduct a series of experiments and highlight: scaling up reasoning data, and\ntraining with general data selected by QaDS is helpful. Then, we define our\noptimal mixture as OpenMathMix, an influential data mixture with open-source\ndata selected by QaDS. With OpenMathMix, we achieve a state-of-the-art 48.8%\naccuracy on MATH with 7B base model. Additionally, we showcase the use of QaDS\nin creating efficient fine-tuning mixtures with various selection ratios, and\nanalyze the quality of a wide range of open-source datasets, which can perform\nas a reference for future works on mathematical reasoning tasks.", "published": "2024-04-01 12:01:06", "link": "http://arxiv.org/abs/2404.01067v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Prompting Methods for Large Language Models: A Survey", "abstract": "Prompting is a mainstream paradigm for adapting large language models to\nspecific natural language processing tasks without modifying internal\nparameters. Therefore, detailed supplementary knowledge needs to be integrated\ninto external prompts, which inevitably brings extra human efforts and\ncomputational burdens for practical applications. As an effective solution to\nmitigate resource consumption, Efficient Prompting Methods have attracted a\nwide range of attention. We provide mathematical expressions at a high level to\ndeeply discuss Automatic Prompt Engineering for different prompt components and\nPrompt Compression in continuous and discrete spaces. Finally, we highlight\npromising future directions to inspire researchers interested in this field.", "published": "2024-04-01 12:19:08", "link": "http://arxiv.org/abs/2404.01077v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SentiCSE: A Sentiment-aware Contrastive Sentence Embedding Framework\n  with Sentiment-guided Textual Similarity", "abstract": "Recently, sentiment-aware pre-trained language models (PLMs) demonstrate\nimpressive results in downstream sentiment analysis tasks. However, they\nneglect to evaluate the quality of their constructed sentiment representations;\nthey just focus on improving the fine-tuning performance, which overshadows the\nrepresentation quality. We argue that without guaranteeing the representation\nquality, their downstream performance can be highly dependent on the\nsupervision of the fine-tuning data rather than representation quality. This\nproblem would make them difficult to foray into other sentiment-related\ndomains, especially where labeled data is scarce. We first propose\nSentiment-guided Textual Similarity (SgTS), a novel metric for evaluating the\nquality of sentiment representations, which is designed based on the degree of\nequivalence in sentiment polarity between two sentences. We then propose\nSentiCSE, a novel Sentiment-aware Contrastive Sentence Embedding framework for\nconstructing sentiment representations via combined word-level and\nsentence-level objectives, whose quality is guaranteed by SgTS. Qualitative and\nquantitative comparison with the previous sentiment-aware PLMs shows the\nsuperiority of our work. Our code is available at:\nhttps://github.com/nayohan/SentiCSE", "published": "2024-04-01 13:24:20", "link": "http://arxiv.org/abs/2404.01104v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Emphasising Structured Information: Integrating Abstract Meaning\n  Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation", "abstract": "Automatic open-domain dialogue evaluation has attracted increasing attention.\nTrainable evaluation metrics, typically trained with true positive and randomly\nselected negative responses, tend to assign higher scores to responses that\nshare greater content similarity with a given context. However, adversarial\nnegative responses, despite possessing high content similarity with the\ncontexts, are semantically different. Consequently, existing evaluation metrics\nare not robust enough to evaluate such responses, resulting in low correlations\nwith human judgments. While recent studies have demonstrated the effectiveness\nof Large Language Models (LLMs) for open-domain dialogue evaluation, they still\nface challenges in effectively handling adversarial negative examples. In this\npaper, we propose an effective framework for open-domain dialogue evaluation,\nwhich combines domain-specific language models (SLMs) enhanced with Abstract\nMeaning Representation (AMR) knowledge with LLMs. The SLMs can explicitly\nincorporate AMR graph information of the dialogue through a gating mechanism\nfor enhanced dialogue semantic representation learning. Both the evaluation\nresult from the SLMs and the AMR graph information are incorporated into the\nLLM's prompt for enhanced evaluation performance. Experimental results on\nopen-domain dialogue evaluation tasks demonstrate the superiority of our method\ncompared to a wide range of state-of-the-art baselines, especially in\ndiscriminating adversarial negative responses. Our code and data are publicly\navailable at https://github.com/Bernard-Yang/SIMAMR.", "published": "2024-04-01 14:11:45", "link": "http://arxiv.org/abs/2404.01129v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KoCoNovel: Annotated Dataset of Character Coreference in Korean Novels", "abstract": "In this paper, we present KoCoNovel, a novel character coreference dataset\nderived from Korean literary texts, complete with detailed annotation\nguidelines. Comprising 178K tokens from 50 modern and contemporary novels,\nKoCoNovel stands as one of the largest public coreference resolution corpora in\nKorean, and the first to be based on literary texts. KoCoNovel offers four\ndistinct versions to accommodate a wide range of literary coreference analysis\nneeds. These versions are designed to support perspectives of the omniscient\nauthor or readers, and to manage multiple entities as either separate or\noverlapping, thereby broadening its applicability. One of KoCoNovel's\ndistinctive features is that 24% of all character mentions are single common\nnouns, lacking possessive markers or articles. This feature is particularly\ninfluenced by the nuances of Korean address term culture, which favors the use\nof terms denoting social relationships and kinship over personal names. In\nexperiments with a BERT-based coreference model, we observe notable performance\nenhancements with KoCoNovel in character coreference tasks within literary\ntexts, compared to a larger non-literary coreference dataset. Such findings\nunderscore KoCoNovel's potential to significantly enhance coreference\nresolution models through the integration of Korean cultural and linguistic\ndynamics.", "published": "2024-04-01 14:36:35", "link": "http://arxiv.org/abs/2404.01140v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LITE: Modeling Environmental Ecosystems with Multimodal Large Language\n  Models", "abstract": "The modeling of environmental ecosystems plays a pivotal role in the\nsustainable management of our planet. Accurate prediction of key environmental\nvariables over space and time can aid in informed policy and decision-making,\nthus improving people's livelihood. Recently, deep learning-based methods have\nshown promise in modeling the spatial-temporal relationships for predicting\nenvironmental variables. However, these approaches often fall short in handling\nincomplete features and distribution shifts, which are commonly observed in\nenvironmental data due to the substantial cost of data collection and\nmalfunctions in measuring instruments. To address these issues, we propose LITE\n-- a multimodal large language model for environmental ecosystems modeling.\nSpecifically, LITE unifies different environmental variables by transforming\nthem into natural language descriptions and line graph images. Then, LITE\nutilizes unified encoders to capture spatial-temporal dynamics and correlations\nin different modalities. During this step, the incomplete features are imputed\nby a sparse Mixture-of-Experts framework, and the distribution shift is handled\nby incorporating multi-granularity information from past observations. Finally,\nguided by domain instructions, a language model is employed to fuse the\nmultimodal representations for the prediction. Our experiments demonstrate that\nLITE significantly enhances performance in environmental spatial-temporal\nprediction across different domains compared to the best baseline, with a\n41.25% reduction in prediction error. This justifies its effectiveness. Our\ndata and code are available at https://github.com/hrlics/LITE.", "published": "2024-04-01 15:14:07", "link": "http://arxiv.org/abs/2404.01165v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Faithful and Complete Hospital-Course Summaries from the\n  Electronic Health Record", "abstract": "The rapid adoption of Electronic Health Records (EHRs) has been instrumental\nin streamlining administrative tasks, increasing transparency, and enabling\ncontinuity of care across providers. An unintended consequence of the increased\ndocumentation burden, however, has been reduced face-time with patients and,\nconcomitantly, a dramatic rise in clinician burnout. In this thesis, we\npinpoint a particularly time-intensive, yet critical, documentation task:\ngenerating a summary of a patient's hospital admissions, and propose and\nevaluate automated solutions. In Chapter 2, we construct a dataset based on\n109,000 hospitalizations (2M source notes) and perform exploratory analyses to\nmotivate future work on modeling and evaluation [NAACL 2021]. In Chapter 3, we\naddress faithfulness from a modeling perspective by revising noisy references\n[EMNLP 2022] and, to reduce the reliance on references, directly calibrating\nmodel outputs to metrics [ACL 2023]. These works relied heavily on automatic\nmetrics as human annotations were limited. To fill this gap, in Chapter 4, we\nconduct a fine-grained expert annotation of system errors in order to\nmeta-evaluate existing metrics and better understand task-specific issues of\ndomain adaptation and source-summary alignments. To learn a metric less\ncorrelated to extractiveness (copy-and-paste), we derive noisy faithfulness\nlabels from an ensemble of existing metrics and train a faithfulness classifier\non these pseudo labels [MLHC 2023]. Finally, in Chapter 5, we demonstrate that\nfine-tuned LLMs (Mistral and Zephyr) are highly prone to entity hallucinations\nand cover fewer salient entities. We improve both coverage and faithfulness by\nperforming sentence-level entity planning based on a set of pre-computed\nsalient entities from the source text, which extends our work on entity-guided\nnews summarization [ACL, 2023], [EMNLP, 2023].", "published": "2024-04-01 15:47:21", "link": "http://arxiv.org/abs/2404.01189v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Estimating Lexical Complexity from Document-Level Distributions", "abstract": "Existing methods for complexity estimation are typically developed for entire\ndocuments. This limitation in scope makes them inapplicable for shorter pieces\nof text, such as health assessment tools. These typically consist of lists of\nindependent sentences, all of which are too short for existing methods to\napply. The choice of wording in these assessment tools is crucial, as both the\ncognitive capacity and the linguistic competency of the intended patient groups\ncould vary substantially. As a first step towards creating better tools for\nsupporting health practitioners, we develop a two-step approach for estimating\nlexical complexity that does not rely on any pre-annotated data. We implement\nour approach for the Norwegian language and verify its effectiveness using\nstatistical testing and a qualitative evaluation of samples from real\nassessment tools. We also investigate the relationship between our complexity\nmeasure and certain features typically associated with complexity in the\nliterature, such as word length, frequency, and the number of syllables.", "published": "2024-04-01 15:55:18", "link": "http://arxiv.org/abs/2404.01196v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Fine Line: Navigating Large Language Model Pretraining with\n  Down-streaming Capability Analysis", "abstract": "Uncovering early-stage metrics that reflect final model performance is one\ncore principle for large-scale pretraining. The existing scaling law\ndemonstrates the power-law correlation between pretraining loss and training\nflops, which serves as an important indicator of the current training state for\nlarge language models. However, this principle only focuses on the model's\ncompression properties on the training data, resulting in an inconsistency with\nthe ability improvements on the downstream tasks. Some follow-up works\nattempted to extend the scaling-law to more complex metrics (such as\nhyperparameters), but still lacked a comprehensive analysis of the dynamic\ndifferences among various capabilities during pretraining. To address the\naforementioned limitations, this paper undertakes a comprehensive comparison of\nmodel capabilities at various pretraining intermediate checkpoints. Through\nthis analysis, we confirm that specific downstream metrics exhibit similar\ntraining dynamics across models of different sizes, up to 67 billion\nparameters. In addition to our core findings, we've reproduced Amber and\nOpenLLaMA, releasing their intermediate checkpoints. This initiative offers\nvaluable resources to the research community and facilitates the verification\nand exploration of LLM pretraining by open-source researchers. Besides, we\nprovide empirical summaries, including performance comparisons of different\nmodels and capabilities, and tuition of key metrics for different training\nphases. Based on these findings, we provide a more user-friendly strategy for\nevaluating the optimization state, offering guidance for establishing a stable\npretraining process.", "published": "2024-04-01 16:00:01", "link": "http://arxiv.org/abs/2404.01204v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AILS-NTUA at SemEval-2024 Task 6: Efficient model tuning for\n  hallucination detection and analysis", "abstract": "In this paper, we present our team's submissions for SemEval-2024 Task-6 -\nSHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration\nMistakes. The participants were asked to perform binary classification to\nidentify cases of fluent overgeneration hallucinations. Our experimentation\nincluded fine-tuning a pre-trained model on hallucination detection and a\nNatural Language Inference (NLI) model. The most successful strategy involved\ncreating an ensemble of these models, resulting in accuracy rates of 77.8% and\n79.9% on model-agnostic and model-aware datasets respectively, outperforming\nthe organizers' baseline and achieving notable results when contrasted with the\ntop-performing results in the competition, which reported accuracies of 84.7%\nand 81.3% correspondingly.", "published": "2024-04-01 16:10:15", "link": "http://arxiv.org/abs/2404.01210v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stable Code Technical Report", "abstract": "We introduce Stable Code, the first in our new-generation of code language\nmodels series, which serves as a general-purpose base code language model\ntargeting code completion, reasoning, math, and other software\nengineering-based tasks. Additionally, we introduce an instruction variant\nnamed Stable Code Instruct that allows conversing with the model in a natural\nchat interface for performing question-answering and instruction-based tasks.\nIn this technical report, we detail the data and training procedure leading to\nboth models. Their weights are available via Hugging Face for anyone to\ndownload and use at https://huggingface.co/stabilityai/stable-code-3b and\nhttps://huggingface.co/stabilityai/stable-code-instruct-3b. This report\ncontains thorough evaluations of the models, including multilingual programming\nbenchmarks, and the MT benchmark focusing on multi-turn dialogues. At the time\nof its release, Stable Code is the state-of-the-art open model under 3B\nparameters and even performs comparably to larger models of sizes 7 billion and\n15 billion parameters on the popular Multi-PL benchmark. Stable Code Instruct\nalso exhibits state-of-the-art performance on the MT-Bench coding tasks and on\nMulti-PL completion compared to other instruction tuned models. Given its\nappealing small size, we also provide throughput measurements on a number of\nedge devices. In addition, we open source several quantized checkpoints and\nprovide their performance metrics compared to the original model.", "published": "2024-04-01 16:39:36", "link": "http://arxiv.org/abs/2404.01226v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language\n  Models", "abstract": "This paper presents a comprehensive survey of the current status and\nopportunities for Large Language Models (LLMs) in strategic reasoning, a\nsophisticated form of reasoning that necessitates understanding and predicting\nadversary actions in multi-agent settings while adjusting strategies\naccordingly. Strategic reasoning is distinguished by its focus on the dynamic\nand uncertain nature of interactions among multi-agents, where comprehending\nthe environment and anticipating the behavior of others is crucial. We explore\nthe scopes, applications, methodologies, and evaluation metrics related to\nstrategic reasoning with LLMs, highlighting the burgeoning development in this\narea and the interdisciplinary approaches enhancing their decision-making\nperformance. It aims to systematize and clarify the scattered literature on\nthis subject, providing a systematic review that underscores the importance of\nstrategic reasoning as a critical cognitive capability and offers insights into\nfuture research directions and potential improvements.", "published": "2024-04-01 16:50:54", "link": "http://arxiv.org/abs/2404.01230v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effectively Prompting Small-sized Language Models for Cross-lingual\n  Tasks via Winning Tickets", "abstract": "Current soft prompt methods yield limited performance when applied to\nsmall-sized models (fewer than a billion parameters). Deep prompt-tuning, which\nentails prepending parameters in each layer for enhanced efficacy, presents a\nsolution for prompting small-sized models, albeit requiring carefully designed\nimplementation. In this paper, we introduce the Lottery Ticket Prompt-learning\n(LTP) framework that integrates winning tickets with soft prompts. The LTP\noffers a simpler implementation and requires only a one-time execution. We\ndemonstrate LTP on cross-lingual tasks, where prior works rely on external\ntools like human-designed multilingual templates and bilingual dictionaries,\nwhich may not be feasible in a low-resource regime. Specifically, we select a\nsubset of parameters that have been changed the most during the fine-tuning\nwith the Masked Language Modeling objective. Then, we prepend soft prompts to\nthe original pre-trained language model and only update the selected parameters\ntogether with prompt-related parameters when adapting to the downstream tasks.\nWe verify the effectiveness of our LTP framework on cross-lingual tasks,\nspecifically targeting low-resource languages. Our approach outperforms the\nbaselines by only updating 20\\% of the original parameters.", "published": "2024-04-01 17:03:16", "link": "http://arxiv.org/abs/2404.01242v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniArk: Improving Generalisation and Consistency for Factual Knowledge\n  Extraction through Debiasing", "abstract": "Several recent papers have investigated the potential of language models as\nknowledge bases as well as the existence of severe biases when extracting\nfactual knowledge. In this work, we focus on the factual probing performance\nover unseen prompts from tuning, and using a probabilistic view we show the\ninherent misalignment between pre-training and downstream tuning objectives in\nlanguage models for probing knowledge. We hypothesize that simultaneously\ndebiasing these objectives can be the key to generalisation over unseen\nprompts. We propose an adapter-based framework, UniArk, for generalised and\nconsistent factual knowledge extraction through simple methods without\nintroducing extra parameters. Extensive experiments show that UniArk can\nsignificantly improve the model's out-of-domain generalisation as well as\nconsistency under various prompts. Additionally, we construct ParaTrex, a\nlarge-scale and diverse dataset for measuring the inconsistency and\nout-of-domain generation of models. Further, ParaTrex offers a reference method\nfor constructing paraphrased datasets using large language models.", "published": "2024-04-01 17:22:07", "link": "http://arxiv.org/abs/2404.01253v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Artificial Intelligence and the Spatial Documentation of Languages", "abstract": "The advancement in technology has made interdisciplinary research more\naccessible. Particularly the breakthrough in Artificial Intelligence AI has\ngiven huge advantages to researchers working in interdisciplinary and\nmultidisciplinary fields. This study investigates the ability of AI models,\nparticularly GPT4 and GPT Data Analyst in creating language maps for language\ndocumentation. The study Integrates documentary linguistics linguistic\ngeography and AI by showcasing how AI models facilitate the spatial\ndocumentation of languages through the creation of language maps with minimal\ncartographic expertise. The study is conducted using a CSV file and a GeoJSON\nfile both obtained from HDX and from the researchers fieldwork. The study data\nis then applied in realtime conversations with the AI models in order to\ngenerate the language distribution maps. The study highlights the two AI models\ncapabilities in generating highquality static and interactive web maps and\nstreamlining the mapmaking process, despite facing challenges like\ninconsistencies and difficulties in adding legends. The findings suggest a\npromising future for AI in generating language maps and enhancing the work of\ndocumentary linguists as they collect their data in the field pointing towards\nthe need for further development to fully harness AI potential in this field.", "published": "2024-04-01 17:35:57", "link": "http://arxiv.org/abs/2404.01263v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Capable of Offering Cognitive Reappraisal, if\n  Guided", "abstract": "Large language models (LLMs) have offered new opportunities for emotional\nsupport, and recent work has shown that they can produce empathic responses to\npeople in distress. However, long-term mental well-being requires emotional\nself-regulation, where a one-time empathic response falls short. This work\ntakes a first step by engaging with cognitive reappraisals, a strategy from\npsychology practitioners that uses language to targetedly change negative\nappraisals that an individual makes of the situation; such appraisals is known\nto sit at the root of human emotional experience. We hypothesize that\npsychologically grounded principles could enable such advanced psychology\ncapabilities in LLMs, and design RESORT which consists of a series of\nreappraisal constitutions across multiple dimensions that can be used as LLM\ninstructions. We conduct a first-of-its-kind expert evaluation (by clinical\npsychologists with M.S. or Ph.D. degrees) of an LLM's zero-shot ability to\ngenerate cognitive reappraisal responses to medium-length social media messages\nasking for support. This fine-grained evaluation showed that even LLMs at the\n7B scale guided by RESORT are capable of generating empathic responses that can\nhelp users reappraise their situations.", "published": "2024-04-01 17:56:30", "link": "http://arxiv.org/abs/2404.01288v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Developing Safe and Responsible Large Language Model : Can We Balance\n  Bias Reduction and Language Understanding in Large Language Models?", "abstract": "Large Language Models (LLMs) have advanced various Natural Language\nProcessing (NLP) tasks, such as text generation and translation, among others.\nHowever, these models often generate texts that can perpetuate biases. Existing\napproaches to mitigate these biases usually compromise knowledge retention.\nThis study explores whether LLMs can produce safe, unbiased outputs without\nsacrificing knowledge or comprehension. We introduce the Safe and Responsible\nLarge Language Model (\\textbf{SR}$_{\\text{LLM}}$), which has been instruction\nfine-tuned atop of a safe fine-tuned auto-regressive decoder-only LLM to reduce\nbiases in generated texts. We developed a specialized dataset with examples of\nunsafe and corresponding safe variations to train \\textbf{SR}$_{\\text{LLM}}$ to\nidentify and correct biased text. Experiments on our specialized dataset and\nout-of-distribution test sets reveal that \\textbf{SR}$_{\\text{LLM}}$\neffectively reduces biases while preserving knowledge integrity. This\nperformance surpasses that of traditional fine-tuning of smaller language\nmodels and base LLMs that merely reply on prompting techniques. Our findings\ndemonstrate that instruction fine-tuning on custom datasets tailored for tasks\nsuch as debiasing is a highly effective strategy for minimizing bias in LLM\nwhile preserving their inherent knowledge and capabilities. The code and\ndataset are accessible at\n\\href{https://github.com/shainarazavi/Safe-Responsible-LLM}{SR-LLM}", "published": "2024-04-01 18:10:05", "link": "http://arxiv.org/abs/2404.01399v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enterprise Use Cases Combining Knowledge Graphs and Natural Language\n  Processing", "abstract": "Knowledge management is a critical challenge for enterprises in today's\ndigital world, as the volume and complexity of data being generated and\ncollected continue to grow incessantly. Knowledge graphs (KG) emerged as a\npromising solution to this problem by providing a flexible, scalable, and\nsemantically rich way to organize and make sense of data. This paper builds\nupon a recent survey of the research literature on combining KGs and Natural\nLanguage Processing (NLP). Based on selected application scenarios from\nenterprise context, we discuss synergies that result from such a combination.\nWe cover various approaches from the three core areas of KG construction,\nreasoning as well as KG-based NLP tasks. In addition to explaining innovative\nenterprise use cases, we assess their maturity in terms of practical\napplicability and conclude with an outlook on emergent application areas for\nthe future.", "published": "2024-04-01 19:28:52", "link": "http://arxiv.org/abs/2404.01443v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finding Replicable Human Evaluations via Stable Ranking Probability", "abstract": "Reliable human evaluation is critical to the development of successful\nnatural language generation models, but achieving it is notoriously difficult.\nStability is a crucial requirement when ranking systems by quality: consistent\nranking of systems across repeated evaluations is not just desirable, but\nessential. Without it, there is no reliable foundation for hill-climbing or\nproduct launch decisions. In this paper, we use machine translation and its\nstate-of-the-art human evaluation framework, MQM, as a case study to understand\nhow to set up reliable human evaluations that yield stable conclusions. We\ninvestigate the optimal configurations for item allocation to raters, number of\nratings per item, and score normalization. Our study on two language pairs\nprovides concrete recommendations for designing replicable human evaluation\nstudies. We also collect and release the largest publicly available dataset of\nmulti-segment translations rated by multiple professional translators,\nconsisting of nearly 140,000 segment annotations across two language pairs.", "published": "2024-04-01 20:50:13", "link": "http://arxiv.org/abs/2404.01474v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Study on Scaling Up Multilingual News Framing Analysis", "abstract": "Media framing is the study of strategically selecting and presenting specific\naspects of political issues to shape public opinion. Despite its relevance to\nalmost all societies around the world, research has been limited due to the\nlack of available datasets and other resources. This study explores the\npossibility of dataset creation through crowdsourcing, utilizing non-expert\nannotators to develop training corpora. We first extend framing analysis beyond\nEnglish news to a multilingual context (12 typologically diverse languages)\nthrough automatic translation. We also present a novel benchmark in Bengali and\nPortuguese on the immigration and same-sex marriage domains. Additionally, we\nshow that a system trained on our crowd-sourced dataset, combined with other\nexisting ones, leads to a 5.32 percentage point increase from the baseline,\nshowing that crowdsourcing is a viable option. Last, we study the performance\nof large language models (LLMs) for this task, finding that task-specific\nfine-tuning is a better approach than employing bigger non-specialized models.", "published": "2024-04-01 21:02:18", "link": "http://arxiv.org/abs/2404.01481v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AAdaM at SemEval-2024 Task 1: Augmentation and Adaptation for\n  Multilingual Semantic Textual Relatedness", "abstract": "This paper presents our system developed for the SemEval-2024 Task 1:\nSemantic Textual Relatedness for African and Asian Languages. The shared task\naims at measuring the semantic textual relatedness between pairs of sentences,\nwith a focus on a range of under-represented languages. In this work, we\npropose using machine translation for data augmentation to address the\nlow-resource challenge of limited training data. Moreover, we apply\ntask-adaptive pre-training on unlabeled task data to bridge the gap between\npre-training and task adaptation. For model training, we investigate both full\nfine-tuning and adapter-based tuning, and adopt the adapter framework for\neffective zero-shot cross-lingual transfer. We achieve competitive results in\nthe shared task: our system performs the best among all ranked teams in both\nsubtask A (supervised learning) and subtask C (cross-lingual transfer).", "published": "2024-04-01 21:21:15", "link": "http://arxiv.org/abs/2404.01490v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diverse Perspectives, Divergent Models: Cross-Cultural Evaluation of\n  Depression Detection on Twitter", "abstract": "Social media data has been used for detecting users with mental disorders,\nsuch as depression. Despite the global significance of cross-cultural\nrepresentation and its potential impact on model performance, publicly\navailable datasets often lack crucial metadata related to this aspect. In this\nwork, we evaluate the generalization of benchmark datasets to build AI models\non cross-cultural Twitter data. We gather a custom geo-located Twitter dataset\nof depressed users from seven countries as a test dataset. Our results show\nthat depression detection models do not generalize globally. The models perform\nworse on Global South users compared to Global North. Pre-trained language\nmodels achieve the best generalization compared to Logistic Regression, though\nstill show significant gaps in performance on depressed and non-Western users.\nWe quantify our findings and provide several actionable suggestions to mitigate\nthis issue.", "published": "2024-04-01 03:59:12", "link": "http://arxiv.org/abs/2406.15362v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring LLM Multi-Agents for ICD Coding", "abstract": "To address the limitations of Large Language Models (LLMs) in the\nInternational Classification of Diseases (ICD) coding task, where they often\nproduce inaccurate and incomplete prediction results due to the\nhigh-dimensional and skewed distribution of the ICD codes, and often lack\ninterpretability and reliability as well. We introduce an innovative\nmulti-agent approach for ICD coding which mimics the ICD coding assignment\nprocedure in real-world settings, comprising five distinct agents: the patient,\nphysician, coder, reviewer, and adjuster. Each agent utilizes an LLM-based\nmodel tailored to their specific role within the coding process. We also\nintegrate the system with Electronic Health Record (HER)'s SOAP (subjective,\nobjective, assessment and plan) structure to boost the performances. We compare\nour method with a system of agents designed solely by LLMs and other strong\nbaselines and evaluate it using the Medical Information Mart for Intensive Care\nIII (MIMIC-III) dataset. Our multi-agent coding framework significantly\noutperforms Zero-shot Chain of Thought (CoT) prompting and self-consistency\nwith CoT (CoT-SC) in coding common and rare ICD codes. An ablation study\nvalidates the effectiveness of the designated agent roles. it also outperforms\nthe LLM-designed agent system. Moreover, our method achieves comparable results\nto state-of-the-art ICD coding methods that require extensive pre-training or\nfine-tuning, and outperforms them in rare code accuracy, and explainability.\nAdditionally, we demonstrate the method's practical applicability by presenting\nits performance in scenarios not limited by the common or rare ICD code\nconstraints.The proposed multi-agent method for ICD coding effectively mimics\nthe real-world coding process and improves performance on both common and rare\ncodes.", "published": "2024-04-01 15:17:39", "link": "http://arxiv.org/abs/2406.15363v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do language models plan ahead for future tokens?", "abstract": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.", "published": "2024-04-01 02:01:28", "link": "http://arxiv.org/abs/2404.00859v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie\n  Embedding", "abstract": "Large language models (LLMs) have demonstrated exceptional performance in\nvarious NLP applications. However, the majority of existing open-source LLMs\nare pre-trained primarily on English data and little part of other languages.\nThis deficiency in multilingual training data results in suboptimal performance\nwhen applied to languages with fewer available resources. Furthermore,\nenhancing the performance of LLMs on low-resource languages by full-parameter\nfine-tuning with additional data requires substantial computational resources,\nposing computational barriers for research organizations and individual\nresearchers. Consequently, several techniques such as parameter-efficient\ntuning and advanced embedding initialization have been proposed to address\nthese challenges. In this work, we combine them to facilitate cross-lingual\ntransfer on English-dominated open-source LLM. To effectively enhance the\nmodel's proficiency in Traditional Chinese, we conduct secondary pre-training\non Llama 2 7B with Traditional Chinese data by leveraging QLoRA and our\nproposed zip-tie embedding initialization. The resulting model called Bailong,\nwhich stands for Bilingual trAnsfer learnIng based on qLOra and zip-tie\nembeddiNG. We present Bailong-instruct 7B, a fine-tuned version of Bailong 7B\noptimized for multi-turn dialogue scenarios. Recognizing the inadequacy of\nbenchmark datasets in Traditional Chinese, we further introduce Bailong-bench\nto assess the alignment of models with human preferences and the capability to\nfollow instructions in both Traditional Chinese and English tasks. In our\nevaluation, Bailong-instruct 7B exhibits competitive performance on\nBailong-bench and other benchmark datasets when compared to other open-source\nmodels of similar or even larger parameter sizes. Bailong-instruct 7B and\nBailong-bench are publicly available with the aim of empowering the community\nto build upon our efforts.", "published": "2024-04-01 02:04:44", "link": "http://arxiv.org/abs/2404.00862v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large\n  Language Models", "abstract": "Large language models (LLMs) have shown promising abilities of in-context\nlearning (ICL), adapting swiftly to new tasks with only few-shot\ndemonstrations. However, current few-shot methods heavily depend on\nhigh-quality, query-specific demos, which are often lacking. When faced with\nout-of-demonstration (OOD) queries, methods that rely on hand-crafted demos or\nexternal retrievers might fail. To bridge the gap between limited demos and OOD\nqueries, we propose Self-Demos, a novel prompting method that elicits the\ninherent generalizability in LLMs by query-aware demo generation. The generated\ndemos strategically interpolate between existing demos and the given query,\ntransforming the query from OOD to ID. To evaluate the effectiveness of our\napproach, we manually constructed OOD-Toolset, a dataset in the tool-using\nscenario with over 300 real-world APIs and 1000 instances, each consisting of\nthree tool-use cases as demos and an OOD query. Thorough experiments on our\ndataset and two public math benchmarks have shown that our method can\noutperform state-of-the-art baselines in the OOD setting. Moreover, we conduct\na range of analyses to validate Self-Demos's generalization and provide more\ninsights.", "published": "2024-04-01 03:25:06", "link": "http://arxiv.org/abs/2404.00884v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMs are Good Sign Language Translators", "abstract": "Sign Language Translation (SLT) is a challenging task that aims to translate\nsign videos into spoken language. Inspired by the strong translation\ncapabilities of large language models (LLMs) that are trained on extensive\nmultilingual text corpora, we aim to harness off-the-shelf LLMs to handle SLT.\nIn this paper, we regularize the sign videos to embody linguistic\ncharacteristics of spoken language, and propose a novel SignLLM framework to\ntransform sign videos into a language-like representation for improved\nreadability by off-the-shelf LLMs. SignLLM comprises two key modules: (1) The\nVector-Quantized Visual Sign module converts sign videos into a sequence of\ndiscrete character-level sign tokens, and (2) the Codebook Reconstruction and\nAlignment module converts these character-level tokens into word-level sign\nrepresentations using an optimal transport formulation. A sign-text alignment\nloss further bridges the gap between sign and text tokens, enhancing semantic\ncompatibility. We achieve state-of-the-art gloss-free results on two\nwidely-used SLT benchmarks.", "published": "2024-04-01 05:07:13", "link": "http://arxiv.org/abs/2404.00925v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and\n  Bias", "abstract": "Based on the foundation of Large Language Models (LLMs), Multilingual LLMs\n(MLLMs) have been developed to address the challenges faced in multilingual\nnatural language processing, hoping to achieve knowledge transfer from\nhigh-resource languages to low-resource languages. However, significant\nlimitations and challenges still exist, such as language imbalance,\nmultilingual alignment, and inherent bias. In this paper, we aim to provide a\ncomprehensive analysis of MLLMs, delving deeply into discussions surrounding\nthese critical issues. First of all, we start by presenting an overview of\nMLLMs, covering their evolutions, key techniques, and multilingual capacities.\nSecondly, we explore the multilingual training corpora of MLLMs and the\nmultilingual datasets oriented for downstream tasks that are crucial to enhance\nthe cross-lingual capability of MLLMs. Thirdly, we survey the state-of-the-art\nstudies of multilingual representations and investigate whether the current\nMLLMs can learn a universal language representation. Fourthly, we discuss bias\non MLLMs, including its categories, evaluation metrics, and debiasing\ntechniques. Finally, we discuss existing challenges and point out promising\nresearch directions of MLLMs.", "published": "2024-04-01 05:13:56", "link": "http://arxiv.org/abs/2404.00929v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evalverse: Unified and Accessible Library for Large Language Model\n  Evaluation", "abstract": "This paper introduces Evalverse, a novel library that streamlines the\nevaluation of Large Language Models (LLMs) by unifying disparate evaluation\ntools into a single, user-friendly framework. Evalverse enables individuals\nwith limited knowledge of artificial intelligence to easily request LLM\nevaluations and receive detailed reports, facilitated by an integration with\ncommunication platforms like Slack. Thus, Evalverse serves as a powerful tool\nfor the comprehensive assessment of LLMs, offering both researchers and\npractitioners a centralized and easily accessible evaluation framework.\nFinally, we also provide a demo video for Evalverse, showcasing its\ncapabilities and implementation in a two-minute format.", "published": "2024-04-01 06:03:39", "link": "http://arxiv.org/abs/2404.00943v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report\n  Generation", "abstract": "Evaluating generated radiology reports is crucial for the development of\nradiology AI, but existing metrics fail to reflect the task's clinical\nrequirements. This study proposes a novel evaluation framework using large\nlanguage models (LLMs) to compare radiology reports for assessment. We compare\nthe performance of various LLMs and demonstrate that, when using GPT-4, our\nproposed metric achieves evaluation consistency close to that of radiologists.\nFurthermore, to reduce costs and improve accessibility, making this method\npractical, we construct a dataset using LLM evaluation results and perform\nknowledge distillation to train a smaller model. The distilled model achieves\nevaluation capabilities comparable to GPT-4. Our framework and distilled model\noffer an accessible and efficient evaluation method for radiology report\ngeneration, facilitating the development of more clinically relevant models.\nThe model will be further open-sourced and accessible.", "published": "2024-04-01 09:02:12", "link": "http://arxiv.org/abs/2404.00998v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Source-Aware Training Enables Knowledge Attribution in Language Models", "abstract": "Large language models (LLMs) learn a vast amount of knowledge during\npretraining, but they are often oblivious to the source(s) of such knowledge.\nWe investigate the problem of intrinsic source citation, where LLMs are\nrequired to cite the pretraining source supporting a generated response.\nIntrinsic source citation can enhance LLM transparency, interpretability, and\nverifiability. To give LLMs such ability, we explore source-aware training -- a\nrecipe that involves (i) training the LLM to associate unique source document\nidentifiers with the knowledge in each document, followed by (ii) an\ninstruction-tuning stage to teach the LLM to cite a supporting pretraining\nsource when prompted. Source-aware training borrows from existing\npretraining/fine-tuning frameworks and requires minimal changes to the model\narchitecture or implementation. Through experiments on synthetic data, we\ndemonstrate that our training recipe can enable faithful attribution to the\npretraining data without a substantial impact on the model's perplexity\ncompared to standard pretraining. Our findings also highlight the importance of\npretraining data augmentation in achieving attribution. Code and data available\nhere: \\url{https://github.com/mukhal/intrinsic-source-citation}", "published": "2024-04-01 09:39:38", "link": "http://arxiv.org/abs/2404.01019v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ARAGOG: Advanced RAG Output Grading", "abstract": "Retrieval-Augmented Generation (RAG) is essential for integrating external\nknowledge into Large Language Model (LLM) outputs. While the literature on RAG\nis growing, it primarily focuses on systematic reviews and comparisons of new\nstate-of-the-art (SoTA) techniques against their predecessors, with a gap in\nextensive experimental comparisons. This study begins to address this gap by\nassessing various RAG methods' impacts on retrieval precision and answer\nsimilarity. We found that Hypothetical Document Embedding (HyDE) and LLM\nreranking significantly enhance retrieval precision. However, Maximal Marginal\nRelevance (MMR) and Cohere rerank did not exhibit notable advantages over a\nbaseline Naive RAG system, and Multi-query approaches underperformed. Sentence\nWindow Retrieval emerged as the most effective for retrieval precision, despite\nits variable performance on answer similarity. The study confirms the potential\nof the Document Summary Index as a competent retrieval approach. All resources\nrelated to this research are publicly accessible for further investigation\nthrough our GitHub repository ARAGOG (https://github.com/predlico/ARAGOG). We\nwelcome the community to further this exploratory study in RAG systems.", "published": "2024-04-01 10:43:52", "link": "http://arxiv.org/abs/2404.01037v1", "categories": ["cs.CL", "cs.IR", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Regularized Best-of-N Sampling with Minimum Bayes Risk Objective for\n  Language Model Alignment", "abstract": "Best-of-N (BoN) sampling with a reward model has been shown to be an\neffective strategy for aligning Large Language Models (LLMs) to human\npreferences at the time of decoding. BoN sampling is susceptible to a problem\nknown as reward hacking when the accuracy of the reward model is not high\nenough due to the quality or the quantity of the preference dataset. Because\nthe reward model is an imperfect proxy for the true objective, over-optimizing\nits value can compromise its performance on the true objective. In this\nresearch, we propose MBR-BoN, a variant of BoN that aims to mitigate reward\nhacking at inference time by incorporating the Minimum Bayes Risk (MBR)\nobjective as a proximity regularization term. We show empirically and\nanalytically that the MBR objective quantifies the proximity of the response to\nthe reference policy, serving as a proximity regularizer. We evaluate MBR-BoN\non the AlpacaFarm and Anthropic's hh-rlhf datasets and show that it outperforms\nboth BoN sampling and MBR decoding. We also evaluate MBR-BoN to generate a\npairwise preference learning dataset for Direct Preference Optimization (DPO).\nEmpirical results show that models trained on a dataset generated with MBR-BoN\noutperform those with vanilla BoN. Our code is available at\nhttps://github.com/CyberAgentAILab/regularized-bon", "published": "2024-04-01 11:26:50", "link": "http://arxiv.org/abs/2404.01054v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Advancing AI with Integrity: Ethical Challenges and Solutions in Neural\n  Machine Translation", "abstract": "This paper addresses the ethical challenges of Artificial Intelligence in\nNeural Machine Translation (NMT) systems, emphasizing the imperative for\ndevelopers to ensure fairness and cultural sensitivity. We investigate the\nethical competence of AI models in NMT, examining the Ethical considerations at\neach stage of NMT development, including data handling, privacy, data\nownership, and consent. We identify and address ethical issues through\nempirical studies. These include employing Transformer models for\nLuganda-English translations and enhancing efficiency with sentence\nmini-batching. And complementary studies that refine data labeling techniques\nand fine-tune BERT and Longformer models for analyzing Luganda and English\nsocial media content. Our second approach is a literature review from databases\nsuch as Google Scholar and platforms like GitHub. Additionally, the paper\nprobes the distribution of responsibility between AI systems and humans,\nunderscoring the essential role of human oversight in upholding NMT ethical\nstandards. Incorporating a biblical perspective, we discuss the societal impact\nof NMT and the broader ethical responsibilities of developers, positing them as\nstewards accountable for the societal repercussions of their creations.", "published": "2024-04-01 12:03:35", "link": "http://arxiv.org/abs/2404.01070v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AILS-NTUA at SemEval-2024 Task 9: Cracking Brain Teasers: Transformer\n  Models for Lateral Thinking Puzzles", "abstract": "In this paper, we outline our submission for the SemEval-2024 Task 9\ncompetition: 'BRAINTEASER: A Novel Task Defying Common Sense'. We engage in\nboth sub-tasks: Sub-task A-Sentence Puzzle and Sub-task B-Word Puzzle. We\nevaluate a plethora of pre-trained transformer-based language models of\ndifferent sizes through fine-tuning. Subsequently, we undertake an analysis of\ntheir scores and responses to aid future researchers in understanding and\nutilizing these models effectively. Our top-performing approaches secured\ncompetitive positions on the competition leaderboard across both sub-tasks. In\nthe evaluation phase, our best submission attained an average accuracy score of\n81.7% in the Sentence Puzzle, and 85.4% in the Word Puzzle, significantly\noutperforming the best neural baseline (ChatGPT) by more than 20% and 30%\nrespectively.", "published": "2024-04-01 12:27:55", "link": "http://arxiv.org/abs/2404.01084v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do LLMs Find Human Answers To Fact-Driven Questions Perplexing? A Case\n  Study on Reddit", "abstract": "Large language models (LLMs) have been shown to be proficient in correctly\nanswering questions in the context of online discourse. However, the study of\nusing LLMs to model human-like answers to fact-driven social media questions is\nstill under-explored. In this work, we investigate how LLMs model the wide\nvariety of human answers to fact-driven questions posed on several\ntopic-specific Reddit communities, or subreddits. We collect and release a\ndataset of 409 fact-driven questions and 7,534 diverse, human-rated answers\nfrom 15 r/Ask{Topic} communities across 3 categories: profession, social\nidentity, and geographic location. We find that LLMs are considerably better at\nmodeling highly-rated human answers to such questions, as opposed to\npoorly-rated human answers. We present several directions for future research\nbased on our initial findings.", "published": "2024-04-01 14:46:20", "link": "http://arxiv.org/abs/2404.01147v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Green AI: Exploring Carbon Footprints, Mitigation Strategies, and Trade\n  Offs in Large Language Model Training", "abstract": "Prominent works in the field of Natural Language Processing have long\nattempted to create new innovative models by improving upon previous model\ntraining approaches, altering model architecture, and developing more in-depth\ndatasets to better their performance. However, with the quickly advancing field\nof NLP comes increased greenhouse gas emissions, posing concerns over the\nenvironmental damage caused by training LLMs. Gaining a comprehensive\nunderstanding of the various costs, particularly those pertaining to\nenvironmental aspects, that are associated with artificial intelligence serves\nas the foundational basis for ensuring safe AI models. Currently,\ninvestigations into the CO2 emissions of AI models remain an emerging area of\nresearch, and as such, in this paper, we evaluate the CO2 emissions of\nwell-known large language models, which have an especially high carbon\nfootprint due to their significant amount of model parameters. We argue for the\ntraining of LLMs in a way that is responsible and sustainable by suggesting\nmeasures for reducing carbon emissions. Furthermore, we discuss how the choice\nof hardware affects CO2 emissions by contrasting the CO2 emissions during model\ntraining for two widely used GPUs. Based on our results, we present the\nbenefits and drawbacks of our proposed solutions and make the argument for the\npossibility of training more environmentally safe AI models without sacrificing\ntheir robustness and performance.", "published": "2024-04-01 15:01:45", "link": "http://arxiv.org/abs/2404.01157v1", "categories": ["cs.CL", "cs.PF"], "primary_category": "cs.CL"}
{"title": "Dialogue with Robots: Proposals for Broadening Participation and\n  Research in the SLIVAR Community", "abstract": "The ability to interact with machines using natural human language is\nbecoming not just commonplace, but expected. The next step is not just text\ninterfaces, but speech interfaces and not just with computers, but with all\nmachines including robots. In this paper, we chronicle the recent history of\nthis growing field of spoken dialogue with robots and offer the community three\nproposals, the first focused on education, the second on benchmarks, and the\nthird on the modeling of language when it comes to spoken interaction with\nrobots. The three proposals should act as white papers for any researcher to\ntake and build upon.", "published": "2024-04-01 15:03:27", "link": "http://arxiv.org/abs/2404.01158v1", "categories": ["cs.CL", "cs.RO"], "primary_category": "cs.CL"}
{"title": "A Neuro-Symbolic Approach to Monitoring Salt Content in Food", "abstract": "We propose a dialogue system that enables heart failure patients to inquire\nabout salt content in foods and help them monitor and reduce salt intake.\nAddressing the lack of specific datasets for food-based salt content inquiries,\nwe develop a template-based conversational dataset. The dataset is structured\nto ask clarification questions to identify food items and their salt content.\nOur findings indicate that while fine-tuning transformer-based models on the\ndataset yields limited performance, the integration of Neuro-Symbolic Rules\nsignificantly enhances the system's performance. Our experiments show that by\nintegrating neuro-symbolic rules, our system achieves an improvement in joint\ngoal accuracy of over 20% across different data sizes compared to naively\nfine-tuning transformer-based models.", "published": "2024-04-01 15:34:24", "link": "http://arxiv.org/abs/2404.01182v1", "categories": ["cs.CL", "cs.SC"], "primary_category": "cs.CL"}
{"title": "Open-Vocabulary Federated Learning with Multimodal Prototyping", "abstract": "Existing federated learning (FL) studies usually assume the training label\nspace and test label space are identical. However, in real-world applications,\nthis assumption is too ideal to be true. A new user could come up with queries\nthat involve data from unseen classes, and such open-vocabulary queries would\ndirectly defect such FL systems. Therefore, in this work, we explicitly focus\non the under-explored open-vocabulary challenge in FL. That is, for a new user,\nthe global server shall understand her/his query that involves arbitrary\nunknown classes. To address this problem, we leverage the pre-trained\nvision-language models (VLMs). In particular, we present a novel adaptation\nframework tailored for VLMs in the context of FL, named as Federated Multimodal\nPrototyping (Fed-MP). Fed-MP adaptively aggregates the local model weights\nbased on light-weight client residuals, and makes predictions based on a novel\nmultimodal prototyping mechanism. Fed-MP exploits the knowledge learned from\nthe seen classes, and robustifies the adapted VLM to unseen categories. Our\nempirical evaluation on various datasets validates the effectiveness of Fed-MP.", "published": "2024-04-01 16:51:13", "link": "http://arxiv.org/abs/2404.01232v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "GFLean: An Autoformalisation Framework for Lean via GF", "abstract": "We present an autoformalisation framework for the Lean theorem prover, called\nGFLean. GFLean uses a high-level grammar writing tool called Grammatical\nFramework (GF) for parsing and linearisation. GFLean is implemented in Haskell.\nWe explain the functionalities of GFLean, its inner working and discuss its\nlimitations. We also discuss how we can use neural network based translation\nprograms and rule based translation programs together complimenting each other\nto build robust autoformalisation frameworks.", "published": "2024-04-01 16:51:50", "link": "http://arxiv.org/abs/2404.01234v1", "categories": ["cs.CL", "math.LO", "I.2.7"], "primary_category": "cs.CL"}
{"title": "An image speaks a thousand words, but can everyone listen? On image\n  transcreation for cultural relevance", "abstract": "Given the rise of multimedia content, human translators increasingly focus on\nculturally adapting not only words but also other modalities such as images to\nconvey the same meaning. While several applications stand to benefit from this,\nmachine translation systems remain confined to dealing with language in speech\nand text. In this work, we take a first step towards translating images to make\nthem culturally relevant. First, we build three pipelines comprising\nstate-of-the-art generative models to do the task. Next, we build a two-part\nevaluation dataset: i) concept: comprising 600 images that are cross-culturally\ncoherent, focusing on a single concept per image, and ii) application:\ncomprising 100 images curated from real-world applications. We conduct a\nmulti-faceted human evaluation of translated images to assess for cultural\nrelevance and meaning preservation. We find that as of today, image-editing\nmodels fail at this task, but can be improved by leveraging LLMs and retrievers\nin the loop. Best pipelines can only translate 5% of images for some countries\nin the easier concept dataset and no translation is successful for some\ncountries in the application dataset, highlighting the challenging nature of\nthe task. Our code and data is released here:\nhttps://github.com/simran-khanuja/image-transcreation.", "published": "2024-04-01 17:08:50", "link": "http://arxiv.org/abs/2404.01247v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "FABLES: Evaluating faithfulness and content selection in book-length\n  summarization", "abstract": "While long-context large language models (LLMs) can technically summarize\nbook-length documents (>100K tokens), the length and complexity of the\ndocuments have so far prohibited evaluations of input-dependent aspects like\nfaithfulness. In this paper, we conduct the first large-scale human evaluation\nof faithfulness and content selection on LLM-generated summaries of fictional\nbooks. Our study mitigates the issue of data contamination by focusing on\nsummaries of books published in 2023 or 2024, and we hire annotators who have\nfully read each book prior to the annotation task to minimize cost and\ncognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims\nmade in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which\nallows us to rank LLM summarizers based on faithfulness: Claude-3-Opus\nsignificantly outperforms all closed-source LLMs, while the open-source Mixtral\nis on par with GPT-3.5-Turbo. An analysis of the annotations reveals that most\nunfaithful claims relate to events and character states, and they generally\nrequire indirect reasoning over the narrative to invalidate. While LLM-based\nauto-raters have proven reliable for factuality and coherence in other\nsettings, we implement several LLM raters of faithfulness and find that none\ncorrelates strongly with human annotations, especially with regard to detecting\nunfaithful claims. Our experiments suggest that detecting unfaithful claims is\nan important future direction not only for summarization evaluation but also as\na testbed for long-context understanding. Finally, we move beyond faithfulness\nby exploring content selection errors in book-length summarization: we develop\na typology of omission errors related to crucial narrative elements and also\nidentify a systematic over-emphasis on events occurring towards the end of the\nbook.", "published": "2024-04-01 17:33:38", "link": "http://arxiv.org/abs/2404.01261v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic\n  Representations", "abstract": "Current foundation models exhibit impressive capabilities when prompted\neither with text only or with both image and text inputs. But do their\ncapabilities change depending on the input modality? In this work, we propose\n$\\textbf{IsoBench}$, a benchmark dataset containing problems from four major\nareas: math, science, algorithms, and games. Each example is presented with\nmultiple $\\textbf{isomorphic representations}$ of inputs, such as visual,\ntextual, and mathematical presentations. IsoBench provides fine-grained\nfeedback to diagnose performance gaps caused by the form of the representation.\nAcross various foundation models, we observe that on the same problem, models\nhave a consistent preference towards textual representations. Most prominently,\nwhen evaluated on all IsoBench problems, Claude-3 Opus performs 28.7 points\nworse when provided with images instead of text; similarly, GPT-4 Turbo is 18.7\npoints worse and Gemini Pro is 14.9 points worse. Finally, we present two\nprompting techniques, $\\textit{IsoCombination}$ and $\\textit{IsoScratchPad}$,\nwhich improve model performance by considering combinations of, and\ntranslations between, different input representations.", "published": "2024-04-01 17:43:27", "link": "http://arxiv.org/abs/2404.01266v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Towards Safety and Helpfulness Balanced Responses via Controllable Large\n  Language Models", "abstract": "As large language models (LLMs) become easily accessible nowadays, the\ntrade-off between safety and helpfulness can significantly impact user\nexperience. A model that prioritizes safety will cause users to feel less\nengaged and assisted while prioritizing helpfulness will potentially cause\nharm. Possible harms include teaching people how to build a bomb, exposing\nyouth to inappropriate content, and hurting users' mental health. In this work,\nwe propose to balance safety and helpfulness in diverse use cases by\ncontrolling both attributes in LLM. We explore training-free and fine-tuning\nmethods that do not require extra human annotations and analyze the challenges\nof controlling safety and helpfulness in LLMs. Our experiments demonstrate that\nour method can rewind a learned model and unlock its controllability.", "published": "2024-04-01 17:59:06", "link": "http://arxiv.org/abs/2404.01295v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unveiling Divergent Inductive Biases of LLMs on Temporal Data", "abstract": "Unraveling the intricate details of events in natural language necessitates a\nsubtle understanding of temporal dynamics. Despite the adeptness of Large\nLanguage Models (LLMs) in discerning patterns and relationships from data,\ntheir inherent comprehension of temporal dynamics remains a formidable\nchallenge. This research meticulously explores these intrinsic challenges\nwithin LLMs, with a specific emphasis on evaluating the performance of GPT-3.5\nand GPT-4 models in the analysis of temporal data. Employing two distinct\nprompt types, namely Question Answering (QA) format and Textual Entailment (TE)\nformat, our analysis probes into both implicit and explicit events. The\nfindings underscore noteworthy trends, revealing disparities in the performance\nof GPT-3.5 and GPT-4. Notably, biases toward specific temporal relationships\ncome to light, with GPT-3.5 demonstrating a preference for \"AFTER'' in the QA\nformat for both implicit and explicit events, while GPT-4 leans towards\n\"BEFORE''. Furthermore, a consistent pattern surfaces wherein GPT-3.5 tends\ntowards \"TRUE'', and GPT-4 exhibits a preference for \"FALSE'' in the TE format\nfor both implicit and explicit events. This persistent discrepancy between\nGPT-3.5 and GPT-4 in handling temporal data highlights the intricate nature of\ninductive bias in LLMs, suggesting that the evolution of these models may not\nmerely mitigate bias but may introduce new layers of complexity.", "published": "2024-04-01 19:56:41", "link": "http://arxiv.org/abs/2404.01453v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Will the Real Linda Please Stand up...to Large Language Models?\n  Examining the Representativeness Heuristic in LLMs", "abstract": "Although large language models (LLMs) have demonstrated remarkable\nproficiency in modeling text and generating human-like text, they may exhibit\nbiases acquired from training data in doing so. Specifically, LLMs may be\nsusceptible to a common cognitive trap in human decision-making called the\nrepresentativeness heuristic. This is a concept in psychology that refers to\njudging the likelihood of an event based on how closely it resembles a\nwell-known prototype or typical example, versus considering broader facts or\nstatistical evidence. This research investigates the impact of the\nrepresentativeness heuristic on LLM reasoning. We created ReHeAT\n(Representativeness Heuristic AI Testing), a dataset containing a series of\nproblems spanning six common types of representativeness heuristics.\nExperiments reveal that four LLMs applied to ReHeAT all exhibited\nrepresentativeness heuristic biases. We further identify that the model's\nreasoning steps are often incorrectly based on a stereotype rather than on the\nproblem's description. Interestingly, the performance improves when adding a\nhint in the prompt to remind the model to use its knowledge. This suggests the\nuniqueness of the representativeness heuristic compared to traditional biases.\nIt can occur even when LLMs possess the correct knowledge while falling into a\ncognitive trap. This highlights the importance of future research focusing on\nthe representativeness heuristic in model reasoning and decision-making and on\ndeveloping solutions to address it.", "published": "2024-04-01 20:15:06", "link": "http://arxiv.org/abs/2404.01461v4", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Set-Aligning Framework for Auto-Regressive Event Temporal Graph\n  Generation", "abstract": "Event temporal graphs have been shown as convenient and effective\nrepresentations of complex temporal relations between events in text. Recent\nstudies, which employ pre-trained language models to auto-regressively generate\nlinearised graphs for constructing event temporal graphs, have shown promising\nresults. However, these methods have often led to suboptimal graph generation\nas the linearised graphs exhibit set characteristics which are instead treated\nsequentially by language models. This discrepancy stems from the conventional\ntext generation objectives, leading to erroneous penalisation of correct\npredictions caused by the misalignment of elements in target sequences. To\naddress these challenges, we reframe the task as a conditional set generation\nproblem, proposing a Set-aligning Framework tailored for the effective\nutilisation of Large Language Models (LLMs). The framework incorporates data\naugmentations and set-property regularisations designed to alleviate text\ngeneration loss penalties associated with the linearised graph edge sequences,\nthus encouraging the generation of more relation edges. Experimental results\nshow that our framework surpasses existing baselines for event temporal graph\ngeneration. Furthermore, under zero-shot settings, the structural knowledge\nintroduced through our framework notably improves model generalisation,\nparticularly when the training examples available are limited.", "published": "2024-04-01 23:46:00", "link": "http://arxiv.org/abs/2404.01532v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "LLaMA-Excitor: General Instruction Tuning via Indirect Feature\n  Interaction", "abstract": "Existing methods to fine-tune LLMs, like Adapter, Prefix-tuning, and LoRA,\nwhich introduce extra modules or additional input sequences to inject new\nskills or knowledge, may compromise the innate abilities of LLMs. In this\npaper, we propose LLaMA-Excitor, a lightweight method that stimulates the LLMs'\npotential to better follow instructions by gradually paying more attention to\nworthwhile information. Specifically, the LLaMA-Excitor does not directly\nchange the intermediate hidden state during the self-attention calculation of\nthe transformer structure. We designed the Excitor block as a bypass module for\nthe similarity score computation in LLMs' self-attention to reconstruct keys\nand change the importance of values by learnable prompts. LLaMA-Excitor ensures\na self-adaptive allocation of additional attention to input instructions, thus\neffectively preserving LLMs' pre-trained knowledge when fine-tuning LLMs on\nlow-quality instruction-following datasets. Furthermore, we unify the modeling\nof multi-modal tuning and language-only tuning, extending LLaMA-Excitor to a\npowerful visual instruction follower without the need for complex multi-modal\nalignment. Our proposed approach is evaluated in language-only and multi-modal\ntuning experimental scenarios. Notably, LLaMA-Excitor is the only method that\nmaintains basic capabilities while achieving a significant improvement (+6%) on\nthe MMLU benchmark. In the visual instruction tuning, we achieve a new\nstate-of-the-art image captioning performance of 157.5 CIDEr on MSCOCO, and a\ncomparable performance (88.39%) on ScienceQA to cutting-edge models with more\nparameters and extensive vision-language pertaining.", "published": "2024-04-01 04:39:21", "link": "http://arxiv.org/abs/2404.00913v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Token-Efficient Leverage Learning in Large Language Models", "abstract": "Large Language Models (LLMs) have excelled in various tasks but perform\nbetter in high-resource scenarios, which presents challenges in low-resource\nscenarios. Data scarcity and the inherent difficulty of adapting LLMs to\nspecific tasks compound the challenge. To address the twin hurdles, we\nintroduce \\textbf{Leverage Learning}. We present a streamlined implement of\nthis methodology called Token-Efficient Leverage Learning (TELL). TELL\nshowcases the potential of Leverage Learning, demonstrating effectiveness\nacross various LLMs and low-resource tasks, ranging from $10^4$ to $10^6$\ntokens. It reduces task data requirements by up to nearly an order of magnitude\ncompared to conventional Supervised Fine-Tuning (SFT) while delivering\ncompetitive performance. With the same amount of task data, TELL leads in\nimproving task performance compared to SFT. We discuss the mechanism of\nLeverage Learning, suggesting it aligns with quantization hypothesis and\nexplore its promising potential through empirical testing.", "published": "2024-04-01 04:39:44", "link": "http://arxiv.org/abs/2404.00914v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How Can Large Language Models Enable Better Socially Assistive\n  Human-Robot Interaction: A Brief Survey", "abstract": "Socially assistive robots (SARs) have shown great success in providing\npersonalized cognitive-affective support for user populations with special\nneeds such as older adults, children with autism spectrum disorder (ASD), and\nindividuals with mental health challenges. The large body of work on SAR\ndemonstrates its potential to provide at-home support that complements\nclinic-based interventions delivered by mental health professionals, making\nthese interventions more effective and accessible. However, there are still\nseveral major technical challenges that hinder SAR-mediated interactions and\ninterventions from reaching human-level social intelligence and efficacy. With\nthe recent advances in large language models (LLMs), there is an increased\npotential for novel applications within the field of SAR that can significantly\nexpand the current capabilities of SARs. However, incorporating LLMs introduces\nnew risks and ethical concerns that have not yet been encountered, and must be\ncarefully be addressed to safely deploy these more advanced systems. In this\nwork, we aim to conduct a brief survey on the use of LLMs in SAR technologies,\nand discuss the potentials and risks of applying LLMs to the following three\nmajor technical challenges of SAR: 1) natural language dialog; 2) multimodal\nunderstanding; 3) LLMs as robot policies.", "published": "2024-04-01 05:50:56", "link": "http://arxiv.org/abs/2404.00938v2", "categories": ["cs.HC", "cs.CL", "cs.CV", "cs.RO"], "primary_category": "cs.HC"}
{"title": "Evaluating the Factuality of Large Language Models using Large-Scale\n  Knowledge Graphs", "abstract": "The advent of Large Language Models (LLMs) has significantly transformed the\nAI landscape, enhancing machine learning and AI capabilities. Factuality issue\nis a critical concern for LLMs, as they may generate factually incorrect\nresponses. In this paper, we propose GraphEval to evaluate an LLM's performance\nusing a substantially large test dataset. Specifically, the test dataset is\nretrieved from a large knowledge graph with more than 10 million facts without\nexpensive human efforts. Unlike conventional methods that evaluate LLMs based\non generated responses, GraphEval streamlines the evaluation process by\ncreating a judge model to estimate the correctness of the answers given by the\nLLM. Our experiments demonstrate that the judge model's factuality assessment\naligns closely with the correctness of the LLM's generated outputs, while also\nsubstantially reducing evaluation costs. Besides, our findings offer valuable\ninsights into LLM performance across different metrics and highlight the\npotential for future improvements in ensuring the factual integrity of LLM\noutputs. The code is publicly available at https://github.com/xz-liu/GraphEval.", "published": "2024-04-01 06:01:17", "link": "http://arxiv.org/abs/2404.00942v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Query Performance Prediction using Relevance Judgments Generated by\n  Large Language Models", "abstract": "Query performance prediction (QPP) aims to estimate the retrieval quality of\na search system for a query without human relevance judgments. Previous QPP\nmethods typically return a single scalar value and do not require the predicted\nvalues to approximate a specific information retrieval (IR) evaluation measure,\nleading to certain drawbacks: (i) a single scalar is insufficient to accurately\nrepresent different IR evaluation measures, especially when metrics do not\nhighly correlate, and (ii) a single scalar limits the interpretability of QPP\nmethods because solely using a scalar is insufficient to explain QPP results.\nTo address these issues, we propose a QPP framework using automatically\ngenerated relevance judgments (QPP-GenRE), which decomposes QPP into\nindependent subtasks of predicting the relevance of each item in a ranked list\nto a given query. This allows us to predict any IR evaluation measure using the\ngenerated relevance judgments as pseudo-labels. This also allows us to\ninterpret predicted IR evaluation measures, and identify, track and rectify\nerrors in generated relevance judgments to improve QPP quality. We predict an\nitem's relevance by using open-source large language models (LLMs) to ensure\nscientific reproducibility.\n  We face two main challenges: (i) excessive computational costs of judging an\nentire corpus for predicting a metric considering recall, and (ii) limited\nperformance in prompting open-source LLMs in a zero-/few-shot manner. To solve\nthe challenges, we devise an approximation strategy to predict an IR measure\nconsidering recall and propose to fine-tune open-source LLMs using\nhuman-labeled relevance judgments. Experiments on the TREC 2019-2022 deep\nlearning tracks show that QPP-GenRE achieves state-of-the-art QPP quality for\nboth lexical and neural rankers.", "published": "2024-04-01 09:33:05", "link": "http://arxiv.org/abs/2404.01012v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "H.3.3"], "primary_category": "cs.IR"}
{"title": "What is in Your Safe Data? Identifying Benign Data that Breaks Safety", "abstract": "Current Large Language Models (LLMs), even those tuned for safety and\nalignment, are susceptible to jailbreaking. Some have found that just further\nfine-tuning an aligned model with benign data (i.e., data without harmful\ncontent) surprisingly leads to substantial degradation in safety. We delve into\nthe data-centric aspects of why benign fine-tuning inadvertently contributes to\njailbreaking. First, we represent fine-tuning data through two lenses:\nrepresentation and gradient spaces. Additionally, we propose a bi-directional\nanchoring method that, during the selection process, prioritizes data points\nthat are close to harmful examples and far from benign ones. Our approach\neffectively identifies subsets of benign data that are more likely to degrade\nthe model's safety after fine-tuning. Training on just 100 of these seemingly\nbenign datapoints surprisingly leads to the fine-tuned model affirmatively\nresponding to >70% of tested harmful requests, compared to <20% after\nfine-tuning on randomly selected data. We also observe that the selected data\nfrequently appear as lists, bullet points, or math questions, indicating a\nsystematic pattern in fine-tuning data that contributes to jailbreaking.", "published": "2024-04-01 13:12:30", "link": "http://arxiv.org/abs/2404.01099v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "AURORA: Navigating UI Tarpits via Automated Neural Screen Understanding", "abstract": "Nearly a decade of research in software engineering has focused on automating\nmobile app testing to help engineers in overcoming the unique challenges\nassociated with the software platform. Much of this work has come in the form\nof Automated Input Generation tools (AIG tools) that dynamically explore app\nscreens. However, such tools have repeatedly been demonstrated to achieve\nlower-than-expected code coverage - particularly on sophisticated proprietary\napps. Prior work has illustrated that a primary cause of these coverage\ndeficiencies is related to so-called tarpits, or complex screens that are\ndifficult to navigate.\n  In this paper, we take a critical step toward enabling AIG tools to\neffectively navigate tarpits during app exploration through a new form of\nautomated semantic screen understanding. We introduce AURORA, a technique that\nlearns from the visual and textual patterns that exist in mobile app UIs to\nautomatically detect common screen designs and navigate them accordingly. The\nkey idea of AURORA is that there are a finite number of mobile app screen\ndesigns, albeit with subtle variations, such that the general patterns of\ndifferent categories of UI designs can be learned. As such, AURORA employs a\nmulti-modal, neural screen classifier that is able to recognize the most common\ntypes of UI screen designs. After recognizing a given screen, it then applies a\nset of flexible and generalizable heuristics to properly navigate the screen.\nWe evaluated AURORA both on a set of 12 apps with known tarpits from prior\nwork, and on a new set of five of the most popular apps from the Google Play\nstore. Our results indicate that AURORA is able to effectively navigate tarpit\nscreens, outperforming prior approaches that avoid tarpits by 19.6% in terms of\nmethod coverage. The improvements can be attributed to AURORA's UI design\nclassification and heuristic navigation techniques.", "published": "2024-04-01 16:58:32", "link": "http://arxiv.org/abs/2404.01240v1", "categories": ["cs.SE", "cs.CL", "cs.CV", "cs.HC"], "primary_category": "cs.SE"}
{"title": "TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model", "abstract": "Clinical trials are indispensable for medical research and the development of\nnew treatments. However, clinical trials often involve thousands of\nparticipants and can span several years to complete, with a high probability of\nfailure during the process. Recently, there has been a burgeoning interest in\nvirtual clinical trials, which simulate real-world scenarios and hold the\npotential to significantly enhance patient safety, expedite development, reduce\ncosts, and contribute to the broader scientific knowledge in healthcare.\nExisting research often focuses on leveraging electronic health records (EHRs)\nto support clinical trial outcome prediction. Yet, trained with limited\nclinical trial outcome data, existing approaches frequently struggle to perform\naccurate predictions. Some research has attempted to generate EHRs to augment\nmodel development but has fallen short in personalizing the generation for\nindividual patient profiles. Recently, the emergence of large language models\nhas illuminated new possibilities, as their embedded comprehensive clinical\nknowledge has proven beneficial in addressing medical issues. In this paper, we\npropose a large language model-based digital twin creation approach, called\nTWIN-GPT. TWIN-GPT can establish cross-dataset associations of medical\ninformation given limited data, generating unique personalized digital twins\nfor different patients, thereby preserving individual patient characteristics.\nComprehensive experiments show that using digital twins created by TWIN-GPT can\nboost the clinical trial outcome prediction, exceeding various previous\nprediction approaches.", "published": "2024-04-01 17:48:55", "link": "http://arxiv.org/abs/2404.01273v2", "categories": ["cs.LG", "cs.CL", "stat.ME"], "primary_category": "cs.LG"}
{"title": "CausalChaos! Dataset for Comprehensive Causal Action Question Answering\n  Over Longer Causal Chains Grounded in Dynamic Visual Scenes", "abstract": "Causal video question answering (QA) has garnered increasing interest, yet\nexisting datasets often lack depth in causal reasoning. To address this gap, we\ncapitalize on the unique properties of cartoons and construct CausalChaos!, a\nnovel, challenging causal Why-QA dataset built upon the iconic \"Tom and Jerry\"\ncartoon series. Cartoons use the principles of animation that allow animators\nto create expressive, unambiguous causal relationships between events to form a\ncoherent storyline. Utilizing these properties, along with thought-provoking\nquestions and multi-level answers (answer and detailed causal explanation), our\nquestions involve causal chains that interconnect multiple dynamic interactions\nbetween characters and visual scenes. These factors demand models to solve more\nchallenging, yet well-defined causal relationships. We also introduce hard\nincorrect answer mining, including a causally confusing version that is even\nmore challenging. While models perform well, there is much room for\nimprovement, especially, on open-ended answers. We identify more\nadvanced/explicit causal relationship modeling & joint modeling of vision and\nlanguage as the immediate areas for future efforts to focus upon. Along with\nthe other complementary datasets, our new challenging dataset will pave the way\nfor these developments in the field.", "published": "2024-04-01 17:59:53", "link": "http://arxiv.org/abs/2404.01299v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Efficiently Distilling LLMs for Edge Applications", "abstract": "Supernet training of LLMs is of great interest in industrial applications as\nit confers the ability to produce a palette of smaller models at constant cost,\nregardless of the number of models (of different size / latency) produced. We\npropose a new method called Multistage Low-rank Fine-tuning of\nSuper-transformers (MLFS) for parameter-efficient supernet training. We show\nthat it is possible to obtain high-quality encoder models that are suitable for\ncommercial edge applications, and that while decoder-only models are resistant\nto a comparable degree of compression, decoders can be effectively sliced for a\nsignificant reduction in training time.", "published": "2024-04-01 07:35:15", "link": "http://arxiv.org/abs/2404.01353v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LLM Attributor: Interactive Visual Attribution for LLM Generation", "abstract": "While large language models (LLMs) have shown remarkable capability to\ngenerate convincing text across diverse domains, concerns around its potential\nrisks have highlighted the importance of understanding the rationale behind\ntext generation. We present LLM Attributor, a Python library that provides\ninteractive visualizations for training data attribution of an LLM's text\ngeneration. Our library offers a new way to quickly attribute an LLM's text\ngeneration to training data points to inspect model behaviors, enhance its\ntrustworthiness, and compare model-generated text with user-provided text. We\ndescribe the visual and interactive design of our tool and highlight usage\nscenarios for LLaMA2 models fine-tuned with two different datasets: online\narticles about recent disasters and finance-related question-answer pairs.\nThanks to LLM Attributor's broad support for computational notebooks, users can\neasily integrate it into their workflow to interactively visualize attributions\nof their models. For easier access and extensibility, we open-source LLM\nAttributor at https://github.com/poloclub/ LLM-Attribution. The video demo is\navailable at https://youtu.be/mIG2MDQKQxM.", "published": "2024-04-01 13:16:34", "link": "http://arxiv.org/abs/2404.01361v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prompt-prompted Adaptive Structured Pruning for Efficient LLM Generation", "abstract": "With the development of transformer-based large language models (LLMs), they\nhave been applied to many fields due to their remarkable utility, but this\ncomes at a considerable computational cost at deployment. Fortunately, some\nmethods such as pruning or constructing a mixture of experts (MoE) aim at\nexploiting sparsity in transformer feedforward (FF) blocks to gain boosts in\nspeed and reduction in memory requirements. However, these techniques can be\nvery costly and inflexible in practice, as they often require training or are\nrestricted to specific types of architectures. To address this, we introduce\nGRIFFIN, a novel training-free and calibration-free method that selects unique\nFF experts at the sequence level for efficient generation across a plethora of\nLLMs with different non-ReLU activation functions. This is possible due to a\ncritical observation that many trained LLMs naturally produce highly structured\nFF activation patterns within a sequence, which we call flocking. Despite our\nmethod's simplicity, we show with 50% of the FF parameters, GRIFFIN maintains\nthe original model's performance with little to no degradation on a variety of\nclassification and generation tasks, all while improving latency (e.g.\n1.29$\\times$ and 1.25$\\times$ speed-ups in Gemma 7B and Llama 2 13B,\nrespectively, on an NVIDIA L40). Code is available at\nhttps://github.com/hdong920/GRIFFIN.", "published": "2024-04-01 17:56:06", "link": "http://arxiv.org/abs/2404.01365v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing\n  Positional Bias in LLMs", "abstract": "Recent advances in large language models (LLMs) have enhanced their ability\nto process long input contexts. This development is particularly crucial for\ntasks that involve retrieving knowledge from an external datastore, which can\nresult in long inputs. However, recent studies show a positional bias in LLMs,\ndemonstrating varying performance depending on the location of useful\ninformation within the input sequence. In this study, we conduct extensive\nexperiments to investigate the root causes of positional bias. Our findings\nindicate that the primary contributor to LLM positional bias stems from the\ninherent positional preferences of different models. We demonstrate that merely\nemploying prompt-based solutions is inadequate for overcoming the positional\npreferences. To address this positional bias issue of a pre-trained LLM, we\ndeveloped a Position-Aware Parameter Efficient Fine-Tuning (PAPEFT) approach\nwhich is composed of a data augmentation technique and a parameter efficient\nadapter, enhancing a uniform attention distribution across the input context.\nOur experiments demonstrate that the proposed approach effectively reduces\npositional bias, improving LLMs' effectiveness in handling long context\nsequences for various tasks that require externally retrieved knowledge.", "published": "2024-04-01 19:04:17", "link": "http://arxiv.org/abs/2404.01430v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Creating emoji lexica from unsupervised sentiment analysis of their\n  descriptions", "abstract": "Online media, such as blogs and social networking sites, generate massive\nvolumes of unstructured data of great interest to analyze the opinions and\nsentiments of individuals and organizations. Novel approaches beyond Natural\nLanguage Processing are necessary to quantify these opinions with polarity\nmetrics. So far, the sentiment expressed by emojis has received little\nattention. The use of symbols, however, has boomed in the past four years.\nAbout twenty billion are typed in Twitter nowadays, and new emojis keep\nappearing in each new Unicode version, making them increasingly relevant to\nsentiment analysis tasks. This has motivated us to propose a novel approach to\npredict the sentiments expressed by emojis in online textual messages, such as\ntweets, that does not require human effort to manually annotate data and saves\nvaluable time for other analysis tasks. For this purpose, we automatically\nconstructed a novel emoji sentiment lexicon using an unsupervised sentiment\nanalysis system based on the definitions given by emoji creators in Emojipedia.\nAdditionally, we automatically created lexicon variants by also considering the\nsentiment distribution of the informal texts accompanying emojis. All these\nlexica are evaluated and compared regarding the improvement obtained by\nincluding them in sentiment analysis of the annotated datasets provided by\nKralj Novak et al. (2015). The results confirm the competitiveness of our\napproach.", "published": "2024-04-01 19:22:58", "link": "http://arxiv.org/abs/2404.01439v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OpenChemIE: An Information Extraction Toolkit For Chemistry Literature", "abstract": "Information extraction from chemistry literature is vital for constructing\nup-to-date reaction databases for data-driven chemistry. Complete extraction\nrequires combining information across text, tables, and figures, whereas prior\nwork has mainly investigated extracting reactions from single modalities. In\nthis paper, we present OpenChemIE to address this complex challenge and enable\nthe extraction of reaction data at the document level. OpenChemIE approaches\nthe problem in two steps: extracting relevant information from individual\nmodalities and then integrating the results to obtain a final list of\nreactions. For the first step, we employ specialized neural models that each\naddress a specific task for chemistry information extraction, such as parsing\nmolecules or reactions from text or figures. We then integrate the information\nfrom these modules using chemistry-informed algorithms, allowing for the\nextraction of fine-grained reaction data from reaction condition and substrate\nscope investigations. Our machine learning models attain state-of-the-art\nperformance when evaluated individually, and we meticulously annotate a\nchallenging dataset of reaction schemes with R-groups to evaluate our pipeline\nas a whole, achieving an F1 score of 69.5%. Additionally, the reaction\nextraction results of \\ours attain an accuracy score of 64.3% when directly\ncompared against the Reaxys chemical database. We provide OpenChemIE freely to\nthe public as an open-source package, as well as through a web interface.", "published": "2024-04-01 20:16:21", "link": "http://arxiv.org/abs/2404.01462v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "TraveLER: A Modular Multi-LMM Agent Framework for Video\n  Question-Answering", "abstract": "Recently, image-based Large Multimodal Models (LMMs) have made significant\nprogress in video question-answering (VideoQA) using a frame-wise approach by\nleveraging large-scale pretraining in a zero-shot manner. Nevertheless, these\nmodels need to be capable of finding relevant information, extracting it, and\nanswering the question simultaneously. Currently, existing methods perform all\nof these steps in a single pass without being able to adapt if insufficient or\nincorrect information is collected. To overcome this, we introduce a modular\nmulti-LMM agent framework based on several agents with different roles,\ninstructed by a Planner agent that updates its instructions using shared\nfeedback from the other agents. Specifically, we propose TraveLER, a method\nthat can create a plan to \"Traverse\" through the video, ask questions about\nindividual frames to \"Locate\" and store key information, and then \"Evaluate\" if\nthere is enough information to answer the question. Finally, if there is not\nenough information, our method is able to \"Replan\" based on its collected\nknowledge. Through extensive experiments, we find that the proposed TraveLER\napproach improves performance on several VideoQA benchmarks without the need to\nfine-tune on specific datasets. Our code is available at\nhttps://github.com/traveler-framework/TraveLER.", "published": "2024-04-01 20:58:24", "link": "http://arxiv.org/abs/2404.01476v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "BERT-Enhanced Retrieval Tool for Homework Plagiarism Detection System", "abstract": "Text plagiarism detection task is a common natural language processing task\nthat aims to detect whether a given text contains plagiarism or copying from\nother texts. In existing research, detection of high level plagiarism is still\na challenge due to the lack of high quality datasets. In this paper, we propose\na plagiarized text data generation method based on GPT-3.5, which produces\n32,927 pairs of text plagiarism detection datasets covering a wide range of\nplagiarism methods, bridging the gap in this part of research. Meanwhile, we\npropose a plagiarism identification method based on Faiss with BERT with high\nefficiency and high accuracy. Our experiments show that the performance of this\nmodel outperforms other models in several metrics, including 98.86\\%, 98.90%,\n98.86%, and 0.9888 for Accuracy, Precision, Recall, and F1 Score, respectively.\nAt the end, we also provide a user-friendly demo platform that allows users to\nupload a text library and intuitively participate in the plagiarism analysis.", "published": "2024-04-01 12:20:34", "link": "http://arxiv.org/abs/2404.01582v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Stream of Search (SoS): Learning to Search in Language", "abstract": "Language models are rarely shown fruitful mistakes while training. They then\nstruggle to look beyond the next token, suffering from a snowballing of errors\nand struggling to predict the consequence of their actions several steps ahead.\nIn this paper, we show how language models can be taught to search by\nrepresenting the process of search in language, as a flattened string -- a\nstream of search (SoS). We propose a unified language for search that captures\nan array of different symbolic search strategies. We demonstrate our approach\nusing the simple yet difficult game of Countdown, where the goal is to combine\ninput numbers with arithmetic operations to reach a target number. We pretrain\na transformer-based language model from scratch on a dataset of streams of\nsearch generated by heuristic solvers. We find that SoS pretraining increases\nsearch accuracy by 25% over models trained to predict only the optimal search\ntrajectory. We further finetune this model with two policy improvement methods:\nAdvantage-Induced Policy Alignment (APA) and Self-Taught Reasoner (STaR). The\nfinetuned SoS models solve 36% of previously unsolved problems, including\nproblems that cannot be solved by any of the heuristic solvers. Our results\nindicate that language models can learn to solve problems via search,\nself-improve to flexibly use different search strategies, and potentially\ndiscover new ones.", "published": "2024-04-01 06:50:52", "link": "http://arxiv.org/abs/2404.03683v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Mapping the Increasing Use of LLMs in Scientific Papers", "abstract": "Scientific publishing lays the foundation of science by disseminating\nresearch findings, fostering collaboration, encouraging reproducibility, and\nensuring that scientific knowledge is accessible, verifiable, and built upon\nover time. Recently, there has been immense speculation about how many people\nare using large language models (LLMs) like ChatGPT in their academic writing,\nand to what extent this tool might have an effect on global scientific\npractices. However, we lack a precise measure of the proportion of academic\nwriting substantially modified or produced by LLMs. To address this gap, we\nconduct the first systematic, large-scale analysis across 950,965 papers\npublished between January 2020 and February 2024 on the arXiv, bioRxiv, and\nNature portfolio journals, using a population-level statistical framework to\nmeasure the prevalence of LLM-modified content over time. Our statistical\nestimation operates on the corpus level and is more robust than inference on\nindividual instances. Our findings reveal a steady increase in LLM usage, with\nthe largest and fastest growth observed in Computer Science papers (up to\n17.5%). In comparison, Mathematics papers and the Nature portfolio showed the\nleast LLM modification (up to 6.3%). Moreover, at an aggregate level, our\nanalysis reveals that higher levels of LLM-modification are associated with\npapers whose first authors post preprints more frequently, papers in more\ncrowded research areas, and papers of shorter lengths. Our findings suggests\nthat LLMs are being broadly used in scientific writings.", "published": "2024-04-01 17:45:15", "link": "http://arxiv.org/abs/2404.01268v1", "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Evaluating Text-to-Visual Generation with Image-to-Text Generation", "abstract": "Despite significant progress in generative AI, comprehensive evaluation\nremains challenging because of the lack of effective metrics and standardized\nbenchmarks. For instance, the widely-used CLIPScore measures the alignment\nbetween a (generated) image and text prompt, but it fails to produce reliable\nscores for complex prompts involving compositions of objects, attributes, and\nrelations. One reason is that text encoders of CLIP can notoriously act as a\n\"bag of words\", conflating prompts such as \"the horse is eating the grass\" with\n\"the grass is eating the horse\". To address this, we introduce the VQAScore,\nwhich uses a visual-question-answering (VQA) model to produce an alignment\nscore by computing the probability of a \"Yes\" answer to a simple \"Does this\nfigure show '{text}'?\" question. Though simpler than prior art, VQAScore\ncomputed with off-the-shelf models produces state-of-the-art results across\nmany (8) image-text alignment benchmarks. We also compute VQAScore with an\nin-house model that follows best practices in the literature. For example, we\nuse a bidirectional image-question encoder that allows image embeddings to\ndepend on the question being asked (and vice versa). Our in-house model,\nCLIP-FlanT5, outperforms even the strongest baselines that make use of the\nproprietary GPT-4V. Interestingly, although we train with only images, VQAScore\ncan also align text with video and 3D models. VQAScore allows researchers to\nbenchmark text-to-visual generation using complex texts that capture the\ncompositional structure of real-world prompts. We introduce GenAI-Bench, a more\nchallenging benchmark with 1,600 compositional text prompts that require\nparsing scenes, objects, attributes, relationships, and high-order reasoning\nlike comparison and logic. GenAI-Bench also offers over 15,000 human ratings\nfor leading image and video generation models such as Stable Diffusion, DALL-E\n3, and Gen2.", "published": "2024-04-01 17:58:06", "link": "http://arxiv.org/abs/2404.01291v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Is Model Collapse Inevitable? Breaking the Curse of Recursion by\n  Accumulating Real and Synthetic Data", "abstract": "The proliferation of generative models, combined with pretraining on\nweb-scale data, raises a timely question: what happens when these models are\ntrained on their own generated outputs? Recent investigations into model-data\nfeedback loops proposed that such loops would lead to a phenomenon termed model\ncollapse, under which performance progressively degrades with each model-data\nfeedback iteration until fitted models become useless. However, those studies\nlargely assumed that new data replace old data over time, where an arguably\nmore realistic assumption is that data accumulate over time. In this paper, we\nask: what effect does accumulating data have on model collapse? We empirically\nstudy this question by pretraining sequences of language models on text\ncorpora. We confirm that replacing the original real data by each generation's\nsynthetic data does indeed tend towards model collapse, then demonstrate that\naccumulating the successive generations of synthetic data alongside the\noriginal real data avoids model collapse; these results hold across a range of\nmodel sizes, architectures, and hyperparameters. We obtain similar results for\ndeep generative models on other types of real data: diffusion models for\nmolecule conformation generation and variational autoencoders for image\ngeneration. To understand why accumulating data can avoid model collapse, we\nuse an analytically tractable framework introduced by prior work in which a\nsequence of linear models are fit to the previous models' outputs. Previous\nwork used this framework to show that if data are replaced, the test error\nincreases with the number of model-fitting iterations; we extend this argument\nto prove that if data instead accumulate, the test error has a finite upper\nbound independent of the number of iterations, meaning model collapse no longer\noccurs.", "published": "2024-04-01 18:31:24", "link": "http://arxiv.org/abs/2404.01413v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.ET", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Statistical Framework of Watermarks for Large Language Models: Pivot,\n  Detection Efficiency and Optimal Rules", "abstract": "Since ChatGPT was introduced in November 2022, embedding (nearly)\nunnoticeable statistical signals into text generated by large language models\n(LLMs), also known as watermarking, has been used as a principled approach to\nprovable detection of LLM-generated text from its human-written counterpart. In\nthis paper, we introduce a general and flexible framework for reasoning about\nthe statistical efficiency of watermarks and designing powerful detection\nrules. Inspired by the hypothesis testing formulation of watermark detection,\nour framework starts by selecting a pivotal statistic of the text and a secret\nkey -- provided by the LLM to the verifier -- to enable controlling the false\npositive rate (the error of mistakenly detecting human-written text as\nLLM-generated). Next, this framework allows one to evaluate the power of\nwatermark detection rules by obtaining a closed-form expression of the\nasymptotic false negative rate (the error of incorrectly classifying\nLLM-generated text as human-written). Our framework further reduces the problem\nof determining the optimal detection rule to solving a minimax optimization\nprogram. We apply this framework to two representative watermarks -- one of\nwhich has been internally implemented at OpenAI -- and obtain several findings\nthat can be instrumental in guiding the practice of implementing watermarks. In\nparticular, we derive optimal detection rules for these watermarks under our\nframework. These theoretically derived detection rules are demonstrated to be\ncompetitive and sometimes enjoy a higher power than existing detection\napproaches through numerical experiments.", "published": "2024-04-01 17:03:41", "link": "http://arxiv.org/abs/2404.01245v3", "categories": ["math.ST", "cs.CL", "cs.CR", "cs.LG", "stat.ML", "stat.TH"], "primary_category": "math.ST"}
{"title": "Utilizing AI and Social Media Analytics to Discover Adverse Side Effects\n  of GLP-1 Receptor Agonists", "abstract": "Adverse side effects (ASEs) of drugs, revealed after FDA approval, pose a\nthreat to patient safety. To promptly detect overlooked ASEs, we developed a\ndigital health methodology capable of analyzing massive public data from social\nmedia, published clinical research, manufacturers' reports, and ChatGPT. We\nuncovered ASEs associated with the glucagon-like peptide 1 receptor agonists\n(GLP-1 RA), a market expected to grow exponentially to $133.5 billion USD by\n2030. Using a Named Entity Recognition (NER) model, our method successfully\ndetected 21 potential ASEs overlooked upon FDA approval, including irritability\nand numbness. Our data-analytic approach revolutionizes the detection of\nunreported ASEs associated with newly deployed drugs, leveraging cutting-edge\nAI-driven social media analytics. It can increase the safety of new drugs in\nthe marketplace by unlocking the power of social media to support regulators\nand manufacturers in the rapid discovery of hidden ASE risks.", "published": "2024-04-01 09:48:14", "link": "http://arxiv.org/abs/2404.01358v1", "categories": ["q-bio.QM", "cs.AI", "cs.CL", "cs.IR", "cs.LG", "cs.SI", "62"], "primary_category": "q-bio.QM"}
{"title": "Voice Conversion Augmentation for Speaker Recognition on Defective\n  Datasets", "abstract": "Modern speaker recognition system relies on abundant and balanced datasets\nfor classification training. However, diverse defective datasets, such as\npartially-labelled, small-scale, and imbalanced datasets, are common in\nreal-world applications. Previous works usually studied specific solutions for\neach scenario from the algorithm perspective. However, the root cause of these\nproblems lies in dataset imperfections. To address these challenges with a\nunified solution, we propose the Voice Conversion Augmentation (VCA) strategy\nto obtain pseudo speech from the training set. Furthermore, to guarantee\ngeneration quality, we designed the VCA-NN~(nearest neighbours) strategy to\nselect source speech from utterances that are close to the target speech in the\nrepresentation space. Our experimental results on three created datasets\ndemonstrated that VCA-NN effectively mitigates these dataset problems, which\nprovides a new direction for handling the speaker recognition problems from the\ndata aspect.", "published": "2024-04-01 02:05:05", "link": "http://arxiv.org/abs/2404.00863v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "KazEmoTTS: A Dataset for Kazakh Emotional Text-to-Speech Synthesis", "abstract": "This study focuses on the creation of the KazEmoTTS dataset, designed for\nemotional Kazakh text-to-speech (TTS) applications. KazEmoTTS is a collection\nof 54,760 audio-text pairs, with a total duration of 74.85 hours, featuring\n34.23 hours delivered by a female narrator and 40.62 hours by two male\nnarrators. The list of the emotions considered include \"neutral\", \"angry\",\n\"happy\", \"sad\", \"scared\", and \"surprised\". We also developed a TTS model\ntrained on the KazEmoTTS dataset. Objective and subjective evaluations were\nemployed to assess the quality of synthesized speech, yielding an MCD score\nwithin the range of 6.02 to 7.67, alongside a MOS that spanned from 3.51 to\n3.57. To facilitate reproducibility and inspire further research, we have made\nour code, pre-trained model, and dataset accessible in our GitHub repository.", "published": "2024-04-01 10:32:04", "link": "http://arxiv.org/abs/2404.01033v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Enhancing Real-World Active Speaker Detection with Multi-Modal\n  Extraction Pre-Training", "abstract": "Audio-visual active speaker detection (AV-ASD) aims to identify which visible\nface is speaking in a scene with one or more persons. Most existing AV-ASD\nmethods prioritize capturing speech-lip correspondence. However, there is a\nnoticeable gap in addressing the challenges from real-world AV-ASD scenarios.\nDue to the presence of low-quality noisy videos in such cases, AV-ASD systems\nwithout a selective listening ability are short of effectively filtering out\ndisruptive voice components from mixed audio inputs. In this paper, we propose\na Multi-modal Speaker Extraction-to-Detection framework named `MuSED', which is\npre-trained with audio-visual target speaker extraction to learn the denoising\nability, then it is fine-tuned with the AV-ASD task. Meanwhile, to better\ncapture the multi-modal information and deal with real-world problems such as\nmissing modality, MuSED is modelled on the time domain directly and integrates\nthe multi-modal plus-and-minus augmentation strategy. Our experiments\ndemonstrate that MuSED substantially outperforms the state-of-the-art AV-ASD\nmethods and achieves 95.6% mAP on the AVA-ActiveSpeaker dataset, 98.3% AP on\nthe ASW dataset, and 97.9% F1 on the Columbia AV-ASD dataset, respectively. We\nwill publicly release the code in due course.", "published": "2024-04-01 02:01:49", "link": "http://arxiv.org/abs/2404.00861v1", "categories": ["eess.AS", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Removing Speaker Information from Speech Representation using\n  Variable-Length Soft Pooling", "abstract": "Recently, there have been efforts to encode the linguistic information of\nspeech using a self-supervised framework for speech synthesis. However,\npredicting representations from surrounding representations can inadvertently\nentangle speaker information in the speech representation. This paper aims to\nremove speaker information by exploiting the structured nature of speech,\ncomposed of discrete units like phonemes with clear boundaries. A neural\nnetwork predicts these boundaries, enabling variable-length pooling for\nevent-based representation extraction instead of fixed-rate methods. The\nboundary predictor outputs a probability for the boundary between 0 and 1,\nmaking pooling soft. The model is trained to minimize the difference with the\npooled representation of the data augmented by time-stretch and pitch-shift. To\nconfirm that the learned representation includes contents information but is\nindependent of speaker information, the model was evaluated with libri-light's\nphonetic ABX task and SUPERB's speaker identification task.", "published": "2024-04-01 01:49:09", "link": "http://arxiv.org/abs/2404.00856v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Novel Audio Representation for Music Genre Identification in MIR", "abstract": "For Music Information Retrieval downstream tasks, the most common audio\nrepresentation is time-frequency-based, such as Mel spectrograms. In order to\nidentify musical genres, this study explores the possibilities of a new form of\naudio representation one of the most usual MIR downstream tasks. Therefore, to\ndiscretely encoding music using deep vector quantization; a novel audio\nrepresentation was created for the innovative generative music model i.e.\nJukebox. The effectiveness of Jukebox's audio representation is compared to Mel\nspectrograms using a dataset that is almost equivalent to State-of-the-Art\n(SOTA) and an almost same transformer design. The results of this study imply\nthat, at least when the transformers are pretrained using a very modest dataset\nof 20k tracks, Jukebox's audio representation is not superior to Mel\nspectrograms. This could be explained by the fact that Jukebox's audio\nrepresentation does not sufficiently take into account the peculiarities of\nhuman hearing perception. On the other hand, Mel spectrograms are specifically\ncreated with the human auditory sense in mind.", "published": "2024-04-01 11:40:09", "link": "http://arxiv.org/abs/2404.01058v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Analysis and Visualization of Musical Structure using Networks", "abstract": "In this article, a framework for defining and analysing a family of graphs or\nnetworks from symbolic music information is discussed. Such graphs concern\ndifferent types of elements, such as pitches, chords and rhythms, and the\nrelations among them, and are built from quantitative or categorical data\ncontained in digital music scores. They are helpful in visualizing musical\nfeatures at once, thus leading to a computational tool for understanding the\ngeneral structural elements of a music fragment. Data obtained from a digital\nscore undergoes different analytical procedures from graph and network theory,\nsuch as computing their centrality measures and entropy, and detecting their\ncommunities. We analyze pieces of music coming from different styles, and\ncompare some of our results with conclusions from traditional music analysis\ntechniques.", "published": "2024-04-01 20:07:36", "link": "http://arxiv.org/abs/2404.15208v1", "categories": ["cs.SI", "cs.SD", "eess.AS", "physics.soc-ph"], "primary_category": "cs.SI"}
{"title": "360+x: A Panoptic Multi-modal Scene Understanding Dataset", "abstract": "Human perception of the world is shaped by a multitude of viewpoints and\nmodalities. While many existing datasets focus on scene understanding from a\ncertain perspective (e.g. egocentric or third-person views), our dataset offers\na panoptic perspective (i.e. multiple viewpoints with multiple data\nmodalities). Specifically, we encapsulate third-person panoramic and front\nviews, as well as egocentric monocular/binocular views with rich modalities\nincluding video, multi-channel audio, directional binaural delay, location data\nand textual scene descriptions within each scene captured, presenting\ncomprehensive observation of the world. Figure 1 offers a glimpse of all 28\nscene categories of our 360+x dataset. To the best of our knowledge, this is\nthe first database that covers multiple viewpoints with multiple data\nmodalities to mimic how daily information is accessed in the real world.\nThrough our benchmark analysis, we presented 5 different scene understanding\ntasks on the proposed 360+x dataset to evaluate the impact and benefit of each\ndata modality and perspective in panoptic scene understanding. We hope this\nunique dataset could broaden the scope of comprehensive scene understanding and\nencourage the community to approach these problems from more diverse\nperspectives.", "published": "2024-04-01 08:34:42", "link": "http://arxiv.org/abs/2404.00989v2", "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
