{"title": "Viola: A Topic Agnostic Generate-and-Rank Dialogue System", "abstract": "We present Viola, an open-domain dialogue system for spoken conversation that\nuses a topic-agnostic dialogue manager based on a simple generate-and-rank\napproach. Leveraging recent advances of generative dialogue systems powered by\nlarge language models, Viola fetches a batch of response candidates from\nvarious neural dialogue models trained with different datasets and\nknowledge-grounding inputs. Additional responses originating from\ntemplate-based generators are also considered, depending on the user's input\nand detected entities. The hand-crafted generators build on a dynamic knowledge\ngraph injected with rich content that is crawled from the web and automatically\nprocessed on a daily basis. Viola's response ranker is a fine-tuned polyencoder\nthat chooses the best response given the dialogue history. While dedicated\nannotations for the polyencoder alone can indirectly steer it away from\nchoosing problematic responses, we add rule-based safety nets to detect neural\ndegeneration and a dedicated classifier to filter out offensive content. We\nanalyze conversations that Viola took part in for the Alexa Prize Socialbot\nGrand Challenge 4 and discuss the strengths and weaknesses of our approach.\nLastly, we suggest future work with a focus on curating conversation data\nspecifcially for socialbots that will contribute towards a more robust\ndata-driven socialbot.", "published": "2021-08-25 06:20:34", "link": "http://arxiv.org/abs/2108.11063v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ontology-Enhanced Slot Filling", "abstract": "Slot filling is a fundamental task in dialog state tracking in task-oriented\ndialog systems. In multi-domain task-oriented dialog system, user utterances\nand system responses may mention multiple named entities and attributes values.\nA system needs to select those that are confirmed by the user and fill them\ninto destined slots. One difficulty is that since a dialogue session contains\nmultiple system-user turns, feeding in all the tokens into a deep model such as\nBERT can be challenging due to limited capacity of input word tokens and GPU\nmemory. In this paper, we investigate an ontology-enhanced approach by matching\nthe named entities occurred in all dialogue turns using ontology. The matched\nentities in the previous dialogue turns will be accumulated and encoded as\nadditional inputs to a BERT-based dialogue state tracker. In addition, our\nimprovement includes ontology constraint checking and the correction of slot\nname tokenization. Experimental results showed that our ontology-enhanced\ndialogue state tracker improves the joint goal accuracy (slot F1) from 52.63%\n(91.64%) to 53.91% (92%) on MultiWOZ 2.1 corpus.", "published": "2021-08-25 14:54:47", "link": "http://arxiv.org/abs/2108.11275v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ProoFVer: Natural Logic Theorem Proving for Fact Verification", "abstract": "Fact verification systems typically rely on neural network classifiers for\nveracity prediction which lack explainability. This paper proposes ProoFVer,\nwhich uses a seq2seq model to generate natural logic-based inferences as\nproofs. These proofs consist of lexical mutations between spans in the claim\nand the evidence retrieved, each marked with a natural logic operator. Claim\nveracity is determined solely based on the sequence of these operators. Hence,\nthese proofs are faithful explanations, and this makes ProoFVer faithful by\nconstruction. Currently, ProoFVer has the highest label accuracy and the\nsecond-best Score in the FEVER leaderboard. Furthermore, it improves by 13.21%\npoints over the next best model on a dataset with counterfactual instances,\ndemonstrating its robustness. As explanations, the proofs show better overlap\nwith human rationales than attention-based highlights and the proofs help\nhumans predict model decisions correctly more often than using the evidence\ndirectly.", "published": "2021-08-25 17:23:04", "link": "http://arxiv.org/abs/2108.11357v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing Accurately Categorizes Indications, Findings\n  and Pathology Reports from Multicenter Colonoscopy", "abstract": "Colonoscopy is used for colorectal cancer (CRC) screening. Extracting details\nof the colonoscopy findings from free text in electronic health records (EHRs)\ncan be used to determine patient risk for CRC and colorectal screening\nstrategies. We developed and evaluated the accuracy of a deep learning model\nframework to extract information for the clinical decision support system to\ninterpret relevant free-text reports, including indications, pathology, and\nfindings notes. The Bio-Bi-LSTM-CRF framework was developed using Bidirectional\nLong Short-term Memory (Bi-LSTM) and Conditional Random Fields (CRF) to extract\nseveral clinical features from these free-text reports including indications\nfor the colonoscopy, findings during the colonoscopy, and pathology of resected\nmaterial. We trained the Bio-Bi-LSTM-CRF and existing Bi-LSTM-CRF models on 80%\nof 4,000 manually annotated notes from 3,867 patients. These clinical notes\nwere from a group of patients over 40 years of age enrolled in four Veterans\nAffairs Medical Centers. A total of 10% of the remaining annotated notes were\nused to train hyperparameter and the remaining 10% were used to evaluate the\naccuracy of our model Bio-Bi-LSTM-CRF and compare to Bi-LSTM-CRF.", "published": "2021-08-25 03:55:08", "link": "http://arxiv.org/abs/2108.11034v1", "categories": ["cs.CL", "cs.LG", "68Txx"], "primary_category": "cs.CL"}
{"title": "YANMTT: Yet Another Neural Machine Translation Toolkit", "abstract": "In this paper we present our open-source neural machine translation (NMT)\ntoolkit called \"Yet Another Neural Machine Translation Toolkit\" abbreviated as\nYANMTT which is built on top of the Transformers library. Despite the growing\nimportance of sequence to sequence pre-training there surprisingly few, if not\nnone, well established toolkits that allow users to easily do pre-training.\nToolkits such as Fairseq which do allow pre-training, have very large codebases\nand thus they are not beginner friendly. With regards to transfer learning via\nfine-tuning most toolkits do not explicitly allow the user to have control over\nwhat parts of the pre-trained models can be transferred. YANMTT aims to address\nthese issues via the minimum amount of code to pre-train large scale NMT\nmodels, selectively transfer pre-trained parameters and fine-tune them, perform\ntranslation as well as extract representations and attentions for visualization\nand analyses. Apart from these core features our toolkit also provides other\nadvanced functionalities such as but not limited to document/multi-source NMT,\nsimultaneous NMT and model compression via distillation which we believe are\nrelevant to the purpose behind our toolkit.", "published": "2021-08-25 08:47:24", "link": "http://arxiv.org/abs/2108.11126v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring the Promises of Transformer-Based LMs for the Representation\n  of Normative Claims in the Legal Domain", "abstract": "In this article, we explore the potential of transformer-based language\nmodels (LMs) to correctly represent normative statements in the legal domain,\ntaking tax law as our use case. In our experiment, we use a variety of LMs as\nbases for both word- and sentence-based clusterers that are then evaluated on a\nsmall, expert-compiled test-set, consisting of real-world samples from tax law\nresearch literature that can be clearly assigned to one of four normative\ntheories. The results of the experiment show that clusterers based on\nsentence-BERT-embeddings deliver the most promising results. Based on this main\nexperiment, we make first attempts at using the best performing models in a\nbootstrapping loop to build classifiers that map normative claims on one of\nthese four normative theories.", "published": "2021-08-25 13:03:04", "link": "http://arxiv.org/abs/2108.11215v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "How COVID-19 has Impacted American Attitudes Toward China: A Study on\n  Twitter", "abstract": "Past research has studied social determinants of attitudes toward foreign\ncountries. Confounded by potential endogeneity biases due to unobserved factors\nor reverse causality, the causal impact of these factors on public opinion is\nusually difficult to establish. Using social media data, we leverage the\nsuddenness of the COVID-19 pandemic to examine whether a major global event has\ncausally changed American views of another country. We collate a database of\nmore than 297 million posts on the social media platform Twitter about China or\nCOVID-19 up to June 2020, and we treat tweeting about COVID-19 as a proxy for\nindividual awareness of COVID-19. Using regression discontinuity and\ndifference-in-difference estimation, we find that awareness of COVID-19 causes\na sharp rise in anti-China attitudes. Our work has implications for\nunderstanding how self-interest affects policy preference and how Americans\nview migrant communities.", "published": "2021-08-25 04:29:58", "link": "http://arxiv.org/abs/2108.11040v1", "categories": ["cs.SI", "cs.CL", "physics.soc-ph", "stat.AP"], "primary_category": "cs.SI"}
{"title": "Models In a Spelling Bee: Language Models Implicitly Learn the Character\n  Composition of Tokens", "abstract": "Standard pretrained language models operate on sequences of subword tokens\nwithout direct access to the characters that compose each token's string\nrepresentation. We probe the embedding layer of pretrained language models and\nshow that models learn the internal character composition of whole word and\nsubword tokens to a surprising extent, without ever seeing the characters\ncoupled with the tokens. Our results show that the embedding layer of RoBERTa\nholds enough information to accurately spell up to a third of the vocabulary\nand reach high average character ngram overlap on all token types. We further\ntest whether enriching subword models with additional character information can\nimprove language modeling, and observe that this method has a near-identical\nlearning curve as training without spelling-based enrichment. Overall, our\nresults suggest that language modeling objectives incentivize the model to\nimplicitly learn some notion of spelling, and that explicitly teaching the\nmodel how to spell does not appear to enhance its performance on such tasks.", "published": "2021-08-25 11:48:05", "link": "http://arxiv.org/abs/2108.11193v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Detecting Drill Failure in the Small Short-sound Drill Dataset", "abstract": "Monitoring the conditions of machines is vital in the manufacturing industry.\nEarly detection of faulty components in machines for stopping and repairing the\nfailed components can minimize the downtime of the machine. This article\npresents an approach to detect the failure occurring in drill machines based on\ndrill sounds from Valmet AB. The drill dataset includes three classes:\nanomalous sounds, normal sounds, and irrelevant sounds, which are also labeled\nas \"Broken\", \"Normal\", and \"Other\", respectively. Detecting drill failure\neffectively remains a challenge due to the following reasons. The waveform of\ndrill sound is complex and short for detection. Additionally, in realistic\nsoundscapes, there are sounds and noise in the context at the same time.\nMoreover, the balanced dataset is small to apply state-of-the-art deep learning\ntechniques. To overcome these aforementioned difficulties, we augmented sounds\nto increase the number of sounds in the dataset. We then proposed a\nconvolutional neural network (CNN) combined with a long short-term memory\n(LSTM) to extract features from log-Mel spectrograms and learn global\nhigh-level feature representation for the classification of three classes. A\nleaky rectified linear unit (Leaky ReLU) was utilized as the activation\nfunction for our proposed CNN instead of the rectified linear unit (ReLU).\nMoreover, we deployed an attention mechanism at the frame level after the LSTM\nlayer to learn long-term global feature representations. As a result, the\nproposed method reached an overall accuracy of 92.35% for the drill failure\ndetection system.", "published": "2021-08-25 07:25:15", "link": "http://arxiv.org/abs/2108.11089v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Temporal envelope and fine structure cues for dysarthric speech\n  detection using CNNs", "abstract": "Deep learning-based techniques for automatic dysarthric speech detection have\nrecently attracted interest in the research community. State-of-the-art\ntechniques typically learn neurotypical and dysarthric discriminative\nrepresentations by processing time-frequency input representations such as the\nmagnitude spectrum of the short-time Fourier transform (STFT). Although these\ntechniques are expected to leverage perceptual dysarthric cues, representations\nsuch as the magnitude spectrum of the STFT do not necessarily convey perceptual\naspects of complex sounds. Inspired by the temporal processing mechanisms of\nthe human auditory system, in this paper we factor signals into the product of\na slowly varying envelope and a rapidly varying fine structure. Separately\nexploiting the different perceptual cues present in the envelope (i.e.,\nphonetic information, stress, and voicing) and fine structure (i.e., pitch,\nvowel quality, and breathiness), two discriminative representations are learned\nthrough a convolutional neural network and used for automatic dysarthric speech\ndetection. Experimental results show that processing both the envelope and fine\nstructure representations yields a considerably better dysarthric speech\ndetection performance than processing only the envelope, fine structure, or\nmagnitude spectrum of the STFT representation.", "published": "2021-08-25 10:13:40", "link": "http://arxiv.org/abs/2108.11153v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AccoMontage: Accompaniment Arrangement via Phrase Selection and Style\n  Transfer", "abstract": "Accompaniment arrangement is a difficult music generation task involving\nintertwined constraints of melody, harmony, texture, and music structure.\nExisting models are not yet able to capture all these constraints effectively,\nespecially for long-term music generation. To address this problem, we propose\nAccoMontage, an accompaniment arrangement system for whole pieces of music\nthrough unifying phrase selection and neural style transfer. We focus on\ngenerating piano accompaniments for folk/pop songs based on a lead sheet (i.e.,\nmelody with chord progression). Specifically, AccoMontage first retrieves\nphrase montages from a database while recombining them structurally using\ndynamic programming. Second, chords of the retrieved phrases are manipulated to\nmatch the lead sheet via style transfer. Lastly, the system offers controls\nover the generation process. In contrast to pure learning-based approaches,\nAccoMontage introduces a novel hybrid pathway, in which rule-based optimization\nand deep learning are both leveraged to complement each other for high-quality\ngeneration. Experiments show that our model generates well-structured\naccompaniment with delicate texture, significantly outperforming the baselines.", "published": "2021-08-25 13:02:03", "link": "http://arxiv.org/abs/2108.11213v1", "categories": ["cs.SD", "cs.IR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Integrated Speech and Gesture Synthesis", "abstract": "Text-to-speech and co-speech gesture synthesis have until now been treated as\nseparate areas by two different research communities, and applications merely\nstack the two technologies using a simple system-level pipeline. This can lead\nto modeling inefficiencies and may introduce inconsistencies that limit the\nachievable naturalness. We propose to instead synthesize the two modalities in\na single model, a new problem we call integrated speech and gesture synthesis\n(ISG). We also propose a set of models modified from state-of-the-art neural\nspeech-synthesis engines to achieve this goal. We evaluate the models in three\ncarefully-designed user studies, two of which evaluate the synthesized speech\nand gesture in isolation, plus a combined study that evaluates the models like\nthey will be used in real-world applications -- speech and gesture presented\ntogether. The results show that participants rate one of the proposed\nintegrated synthesis models as being as good as the state-of-the-art pipeline\nsystem we compare against, in all three tests. The model is able to achieve\nthis with faster synthesis time and greatly reduced parameter count compared to\nthe pipeline system, illustrating some of the potential benefits of treating\nspeech and gesture synthesis together as a single, unified problem. Videos and\ncode are available on our project page at https://swatsw.github.io/isg_icmi21/", "published": "2021-08-25 19:04:00", "link": "http://arxiv.org/abs/2108.11436v1", "categories": ["cs.HC", "cs.GR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
