{"title": "Relative-error testing of conjunctions and decision lists", "abstract": "We study the relative-error property testing model for Boolean functions that\nwas recently introduced in the work of Chen et al. (SODA 2025). In\nrelative-error testing, the testing algorithm gets uniform random satisfying\nassignments as well as black-box queries to $f$, and it must accept $f$ with\nhigh probability whenever $f$ has the property that is being tested and reject\nany $f$ that is relative-error far from having the property. Here the\nrelative-error distance from $f$ to a function $g$ is measured with respect to\n$|f^{-1}(1)|$ rather than with respect to the entire domain size $2^n$ as in\nthe Hamming distance measure that is used in the standard model; thus, unlike\nthe standard model, relative-error testing allows us to study the testability\nof sparse Boolean functions that have few satisfying assignments. It was shown\nin Chen et al. (SODA 2025) that relative-error testing is at least as difficult\nas standard-model property testing, but for many natural and important Boolean\nfunction classes the precise relationship between the two notions is unknown.\n  In this paper we consider the well-studied and fundamental properties of\nbeing a conjunction and being a decision list. In the relative-error setting,\nwe give an efficient one-sided error tester for conjunctions with running time\nand query complexity $O(1/\\epsilon)$.\n  Secondly, we give a two-sided relative-error $\\tilde{O}$$(1/\\epsilon)$ tester\nfor decision lists, matching the query complexity of the state-of-the-art\nalgorithm in the standard model Bshouty (RANDOM 2020) and Diakonikolas et al.\n(FOCS 2007).", "published": "2025-04-11 21:40:57", "link": "http://arxiv.org/abs/2504.08987v1", "categories": ["cs.CC", "cs.DM", "cs.DS"], "primary_category": "cs.CC"}
{"title": "$\u03c7$-Boundedness and Neighbourhood Complexity of Bounded Merge-Width Graphs", "abstract": "Merge-width, recently introduced by Dreier and Toru\\'nczyk, is a common\ngeneralisation of bounded expansion classes and twin-width for which the\nfirst-order model checking problem remains tractable. We prove that a number of\nbasic properties shared by bounded expansion and bounded twin-width graphs also\nhold for bounded merge-width graphs: they are $\\chi$-bounded, they satisfy the\nstrong Erd\\H{o}s-Hajnal property, and their neighbourhood complexity is linear.", "published": "2025-04-11 05:43:21", "link": "http://arxiv.org/abs/2504.08266v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "RouterKT: Mixture-of-Experts for Knowledge Tracing", "abstract": "Knowledge Tracing (KT) is a fundamental task in Intelligent Tutoring Systems\n(ITS), which aims to model the dynamic knowledge states of students based on\ntheir interaction histories. However, existing KT models often rely on a global\nforgetting decay mechanism for capturing learning patterns, assuming that\nstudents' performance is predominantly influenced by their most recent\ninteractions. Such approaches fail to account for the diverse and complex\nlearning patterns arising from individual differences and varying learning\nstages. To address this limitation, we propose RouterKT, a novel\nMixture-of-Experts (MoE) architecture designed to capture heterogeneous\nlearning patterns by enabling experts to specialize in different patterns\nwithout any handcrafted learning pattern bias such as forgetting decay.\nSpecifically, RouterKT introduces a \\textbf{person-wise routing mechanism} to\neffectively model individual-specific learning behaviors and employs\n\\textbf{multi-heads as experts} to enhance the modeling of complex and diverse\npatterns. Comprehensive experiments on ten benchmark datasets demonstrate that\nRouterKT exhibits significant flexibility and improves the performance of\nvarious KT backbone models, with a maximum average AUC improvement of 3.29\\%\nacross different backbones and datasets, outperforming other state-of-the-art\nmodels. Moreover, RouterKT demonstrates consistently superior inference\nefficiency compared to existing approaches based on handcrafted learning\npattern bias, highlighting its usability for real-world educational\napplications. The source code is available at\nhttps://github.com/derek-liao/RouterKT.git.", "published": "2025-04-11 21:42:08", "link": "http://arxiv.org/abs/2504.08989v1", "categories": ["cs.CY", "cs.IR", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Code-Craft: Hierarchical Graph-Based Code Summarization for Enhanced Context Retrieval", "abstract": "Understanding and navigating large-scale codebases remains a significant\nchallenge in software engineering. Existing methods often treat code as flat\ntext or focus primarily on local structural relationships, limiting their\nability to provide holistic, context-aware information retrieval. We present\nHierarchical Code Graph Summarization (HCGS), a novel approach that constructs\na multi-layered representation of a codebase by generating structured summaries\nin a bottom-up fashion from a code graph. HCGS leverages the Language Server\nProtocol for language-agnostic code analysis and employs a parallel level-based\nalgorithm for efficient summary generation. Through extensive evaluation on\nfive diverse codebases totaling 7,531 functions, HCGS demonstrates significant\nimprovements in code retrieval accuracy, achieving up to 82 percentage relative\nimprovement in top-1 retrieval precision for large codebases like libsignal\n(27.15 percentage points), and perfect Pass@3 scores for smaller repositories.\nThe system's hierarchical approach consistently outperforms traditional\ncode-only retrieval across all metrics, with particularly substantial gains in\nlarger, more complex codebases where understanding function relationships is\ncrucial.", "published": "2025-04-11 20:57:27", "link": "http://arxiv.org/abs/2504.08975v1", "categories": ["cs.SE", "cs.IR"], "primary_category": "cs.SE"}
{"title": "Large Language Model Empowered Recommendation Meets All-domain Continual Pre-Training", "abstract": "Recent research efforts have investigated how to integrate Large Language\nModels (LLMs) into recommendation, capitalizing on their semantic comprehension\nand open-world knowledge for user behavior understanding. These approaches\npredominantly employ supervised fine-tuning on single-domain user interactions\nto adapt LLMs for specific recommendation tasks. However, they typically\nencounter dual challenges: the mismatch between general language\nrepresentations and domain-specific preference patterns, as well as the limited\nadaptability to multi-domain recommendation scenarios. To bridge these gaps, we\nintroduce CPRec -- an All-domain Continual Pre-Training framework for\nRecommendation -- designed to holistically align LLMs with universal user\nbehaviors through the continual pre-training paradigm. Specifically, we first\ndesign a unified prompt template and organize users' multi-domain behaviors\ninto domain-specific behavioral sequences and all-domain mixed behavioral\nsequences that emulate real-world user decision logic. To optimize behavioral\nknowledge infusion, we devise a Warmup-Stable-Annealing learning rate schedule\ntailored for the continual pre-training paradigm in recommendation to\nprogressively enhance the LLM's capability in knowledge adaptation from\nopen-world knowledge to universal recommendation tasks. To evaluate the\neffectiveness of our CPRec, we implement it on a large-scale dataset covering\nseven domains and conduct extensive experiments on five real-world datasets\nfrom two distinct platforms. Experimental results confirm that our continual\npre-training paradigm significantly mitigates the semantic-behavioral\ndiscrepancy and achieves state-of-the-art performance in all recommendation\nscenarios. The source code will be released upon acceptance.", "published": "2025-04-11 20:01:25", "link": "http://arxiv.org/abs/2504.08949v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Comparative Study of Recommender Systems under Big Data Constraints", "abstract": "Recommender Systems (RS) have become essential tools in a wide range of\ndigital services, from e-commerce and streaming platforms to news and social\nmedia. As the volume of user-item interactions grows exponentially, especially\nin Big Data environments, selecting the most appropriate RS model becomes a\ncritical task. This paper presents a comparative study of several\nstate-of-the-art recommender algorithms, including EASE-R, SLIM, SLIM with\nElasticNet regularization, Matrix Factorization (FunkSVD and ALS), P3Alpha, and\nRP3Beta. We evaluate these models according to key criteria such as\nscalability, computational complexity, predictive accuracy, and\ninterpretability. The analysis considers both their theoretical underpinnings\nand practical applicability in large-scale scenarios. Our results highlight\nthat while models like SLIM and SLIM-ElasticNet offer high accuracy and\ninterpretability, they suffer from high computational costs, making them less\nsuitable for real-time applications. In contrast, algorithms such as EASE-R and\nRP3Beta achieve a favorable balance between performance and scalability,\nproving more effective in large-scale environments. This study aims to provide\nguidelines for selecting the most appropriate recommender approach based on\nspecific Big Data constraints and system requirements.", "published": "2025-04-11 11:35:13", "link": "http://arxiv.org/abs/2504.08457v1", "categories": ["cs.IR", "Primary 68T05, 68P20, Secondary 68W20, 68R10"], "primary_category": "cs.IR"}
{"title": "A Reproducibility Study of Graph-Based Legal Case Retrieval", "abstract": "Legal retrieval is a widely studied area in Information Retrieval (IR) and a\nkey task in this domain is retrieving relevant cases based on a given query\ncase, often done by applying language models as encoders to model case\nsimilarity. Recently, Tang et al. proposed CaseLink, a novel graph-based method\nfor legal case retrieval, which models both cases and legal charges as nodes in\na network, with edges representing relationships such as references and shared\nsemantics. This approach offers a new perspective on the task by capturing\nhigher-order relationships of cases going beyond the stand-alone level of\ndocuments. However, while this shift in approaching legal case retrieval is a\npromising direction in an understudied area of graph-based legal IR, challenges\nin reproducing novel results have recently been highlighted, with multiple\nstudies reporting difficulties in reproducing previous findings. Thus, in this\nwork we reproduce CaseLink, a graph-based legal case retrieval method, to\nsupport future research in this area of IR. In particular, we aim to assess its\nreliability and generalizability by (i) first reproducing the original study\nsetup and (ii) applying the approach to an additional dataset. We then build\nupon the original implementations by (iii) evaluating the approach's\nperformance when using a more sophisticated graph data representation and (iv)\nusing an open large language model (LLM) in the pipeline to address limitations\nthat are known to result from using closed models accessed via an API. Our\nfindings aim to improve the understanding of graph-based approaches in legal IR\nand contribute to improving reproducibility in the field. To achieve this, we\nshare all our implementations and experimental artifacts with the community.", "published": "2025-04-11 10:04:12", "link": "http://arxiv.org/abs/2504.08400v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "PCA-RAG: Principal Component Analysis for Efficient Retrieval-Augmented Generation", "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\ngrounding large language models in external knowledge sources, improving the\nprecision of agents responses. However, high-dimensional language model\nembeddings, often in the range of hundreds to thousands of dimensions, can\npresent scalability challenges in terms of storage and latency, especially when\nprocessing massive financial text corpora. This paper investigates the use of\nPrincipal Component Analysis (PCA) to reduce embedding dimensionality, thereby\nmitigating computational bottlenecks without incurring large accuracy losses.\nWe experiment with a real-world dataset and compare different similarity and\ndistance metrics under both full-dimensional and PCA-compressed embeddings. Our\nresults show that reducing vectors from 3,072 to 110 dimensions provides a\nsizeable (up to $60\\times$) speedup in retrieval operations and a $\\sim\n28.6\\times$ reduction in index size, with only moderate declines in correlation\nmetrics relative to human-annotated similarity scores. These findings\ndemonstrate that PCA-based compression offers a viable balance between\nretrieval fidelity and resource efficiency, essential for real-time systems\nsuch as Zanista AI's \\textit{Newswitch} platform. Ultimately, our study\nunderscores the practicality of leveraging classical dimensionality reduction\ntechniques to scale RAG architectures for knowledge-intensive applications in\nfinance and trading, where speed, memory efficiency, and accuracy must jointly\nbe optimized.", "published": "2025-04-11 09:38:12", "link": "http://arxiv.org/abs/2504.08386v1", "categories": ["cs.LG", "cs.AI", "cs.IR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Scholar Inbox: Personalized Paper Recommendations for Scientists", "abstract": "Scholar Inbox is a new open-access platform designed to address the\nchallenges researchers face in staying current with the rapidly expanding\nvolume of scientific literature. We provide personalized recommendations,\ncontinuous updates from open-access archives (arXiv, bioRxiv, etc.), visual\npaper summaries, semantic search, and a range of tools to streamline research\nworkflows and promote open research access. The platform's personalized\nrecommendation system is trained on user ratings, ensuring that recommendations\nare tailored to individual researchers' interests. To further enhance the user\nexperience, Scholar Inbox also offers a map of science that provides an\noverview of research across domains, enabling users to easily explore specific\ntopics. We use this map to address the cold start problem common in recommender\nsystems, as well as an active learning strategy that iteratively prompts users\nto rate a selection of papers, allowing the system to learn user preferences\nquickly. We evaluate the quality of our recommendation system on a novel\ndataset of 800k user ratings, which we make publicly available, as well as via\nan extensive user study. https://www.scholar-inbox.com/", "published": "2025-04-11 09:37:48", "link": "http://arxiv.org/abs/2504.08385v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "OnSET: Ontology and Semantic Exploration Toolkit", "abstract": "Retrieval over knowledge graphs is usually performed using dedicated, complex\nquery languages like SPARQL. We propose a novel system, Ontology and Semantic\nExploration Toolkit (OnSET) that allows non-expert users to easily build\nqueries with visual user guidance provided by topic modelling and semantic\nsearch throughout the application. OnSET allows users without any prior\ninformation about the ontology or networked knowledge to start exploring topics\nof interest over knowledge graphs, including the retrieval and detailed\nexploration of prototypical sub-graphs and their instances. Existing systems\neither focus on direct graph explorations or do not foster further exploration\nof the result set. We, however, provide a node-based editor that can extend on\nthese missing properties of existing systems to support the search over big\nontologies with sub-graph instances. Furthermore, OnSET combines efficient and\nopen platforms to deploy the system on commodity hardware.", "published": "2025-04-11 09:18:06", "link": "http://arxiv.org/abs/2504.08373v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "eST$^2$ Miner -- Process Discovery Based on Firing Partial Orders", "abstract": "Process discovery generates process models from event logs. Traditionally, an\nevent log is defined as a multiset of traces, where each trace is a sequence of\nevents. The total order of the events in a sequential trace is typically based\non their temporal occurrence. However, real-life processes are partially\nordered by nature. Different activities can occur in different parts of the\nprocess and, thus, independently of each other. Therefore, the temporal total\norder of events does not necessarily reflect their causal order, as also\ncausally unrelated events may be ordered in time. Only partial orders allow to\nexpress concurrency, duration, overlap, and uncertainty of events.\nConsequently, there is a growing need for process mining algorithms that can\ndirectly handle partially ordered input. In this paper, we combine two\nwell-established and efficient algorithms, the eST Miner from the process\nmining community and the Firing LPO algorithm from the Petri net community, to\nintroduce the eST$^2$ Miner. The eST$^2$ Miner is a process discovery algorithm\nthat can directly handle partially ordered input, gives strong formal\nguarantees, offers good runtime and excellent space complexity, and can, thus,\nbe used in real-life applications.", "published": "2025-04-11 09:16:54", "link": "http://arxiv.org/abs/2504.08372v1", "categories": ["cs.DB", "cs.IR"], "primary_category": "cs.DB"}
{"title": "RAG-VR: Leveraging Retrieval-Augmented Generation for 3D Question Answering in VR Environments", "abstract": "Recent advances in large language models (LLMs) provide new opportunities for\ncontext understanding in virtual reality (VR). However, VR contexts are often\nhighly localized and personalized, limiting the effectiveness of\ngeneral-purpose LLMs. To address this challenge, we present RAG-VR, the first\n3D question-answering system for VR that incorporates retrieval-augmented\ngeneration (RAG), which augments an LLM with external knowledge retrieved from\na localized knowledge database to improve the answer quality. RAG-VR includes a\npipeline for extracting comprehensive knowledge about virtual environments and\nuser conditions for accurate answer generation. To ensure efficient retrieval,\nRAG-VR offloads the retrieval process to a nearby edge server and uses only\nessential information during retrieval. Moreover, we train the retriever to\neffectively distinguish among relevant, irrelevant, and hard-to-differentiate\ninformation in relation to questions. RAG-VR improves answer accuracy by\n17.9%-41.8% and reduces end-to-end latency by 34.5%-47.3% compared with two\nbaseline systems.", "published": "2025-04-11 04:55:50", "link": "http://arxiv.org/abs/2504.08256v2", "categories": ["cs.IR", "cs.AI", "cs.HC"], "primary_category": "cs.IR"}
{"title": "How Good Are Large Language Models for Course Recommendation in MOOCs?", "abstract": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and are increasingly being integrated into recommendation\nsystems. However, their potential in educational recommendation systems has yet\nto be fully explored. This paper investigates the use of LLMs as a\ngeneral-purpose recommendation model, leveraging their vast knowledge derived\nfrom large-scale corpora for course recommendation tasks. We explore a variety\nof approaches, ranging from prompt-based methods to more advanced fine-tuning\ntechniques, and compare their performance against traditional recommendation\nmodels. Extensive experiments were conducted on a real-world MOOC dataset,\nevaluating using LLMs as course recommendation systems across key dimensions\nsuch as accuracy, diversity, and novelty. Our results demonstrate that LLMs can\nachieve good performance comparable to traditional models, highlighting their\npotential to enhance educational recommendation systems. These findings pave\nthe way for further exploration and development of LLM-based approaches in the\ncontext of educational recommendations.", "published": "2025-04-11 02:19:26", "link": "http://arxiv.org/abs/2504.08208v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "Joint Transmit Waveform and Receive Filter Design for ISAC System with Jamming", "abstract": "In this paper, to suppress jamming in the complex electromagnetic\nenvironment, we propose a joint transmit waveform and receive filter design\nframework for integrated sensing and communications (ISAC). By jointly\noptimizing the transmit waveform and receive filters, we aim at minimizing the\nmultiuser interference (MUI), subject to the constraints of the target\nmainlobe, jamming mainlobe and peak sidelobe level of the receive filter output\nas well as the transmit power of the ISAC base station. We propose two schemes\nto solve the problem, including joint transmit waveform and matched filter\ndesign (JTMD) and joint transmit waveform and mismatched filter design (JTMMD)\nschemes. For both schemes, we adopt the alternating direction method of\nmultipliers to iteratively optimize the transmit waveform and receive filters,\nwhere the number of targets as well as the range and angles of each target can\nalso be estimated. Simulation results show that both the JTMD and JTMMD schemes\nachieve superior performance in terms of communication MUI and radar detection\nperformance.", "published": "2025-04-11 13:32:29", "link": "http://arxiv.org/abs/2504.08520v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Simultaneous Rational Number Codes: Decoding Beyond Half the Minimum Distance with Multiplicities and Bad Primes", "abstract": "In this paper, we extend the work of (Abbondati et al., 2024) on decoding\nsimultaneous rational number codes by addressing two important scenarios:\nmultiplicities and the presence of bad primes (divisors of denominators).\nFirst, we generalize previous results to multiplicity rational codes by\nconsidering modular reductions with respect to prime power moduli. Then, using\nhybrid analysis techniques, we extend our approach to vectors of fractions that\nmay present bad primes. Our contributions include: a decoding algorithm for\nsimultaneous rational number reconstruction with multiplicities, a rigorous\nanalysis of the algorithm's failure probability that generalizes several\nprevious results, an extension to a hybrid model handling situations where not\nall errors can be assumed random, and a unified approach to handle bad primes\nwithin multiplicities. The theoretical results provide a comprehensive\nprobabilistic analysis of reconstruction failure in these more complex\nscenarios, advancing the state of the art in error correction for rational\nnumber codes.", "published": "2025-04-11 12:01:41", "link": "http://arxiv.org/abs/2504.08472v1", "categories": ["cs.IT", "cs.SC", "math.IT"], "primary_category": "cs.IT"}
{"title": "High-dimensional Clustering and Signal Recovery under Block Signals", "abstract": "This paper studies computationally efficient methods and their minimax\noptimality for high-dimensional clustering and signal recovery under block\nsignal structures. We propose two sets of methods, cross-block feature\naggregation PCA (CFA-PCA) and moving average PCA (MA-PCA), designed for sparse\nand dense block signals, respectively. Both methods adaptively utilize block\nsignal structures, applicable to non-Gaussian data with heterogeneous variances\nand non-diagonal covariance matrices. Specifically, the CFA method utilizes a\nblock-wise U-statistic to aggregate and select block signals non-parametrically\nfrom data with unknown cluster labels. We show that the proposed methods are\nconsistent for both clustering and signal recovery under mild conditions and\nweaker signal strengths than the existing methods without considering block\nstructures of signals. Furthermore, we derive both statistical and\ncomputational minimax lower bounds (SMLB and CMLB) for high-dimensional\nclustering and signal recovery under block signals, where the CMLBs are\nrestricted to algorithms with polynomial computation complexity. The minimax\nboundaries partition signals into regions of impossibility and possibility. No\nalgorithm (or no polynomial time algorithm) can achieve consistent clustering\nor signal recovery if the signals fall into the statistical (or computational)\nregion of impossibility. We show that the proposed CFA-PCA and MA-PCA methods\ncan achieve the CMLBs for the sparse and dense block signal regimes,\nrespectively, indicating the proposed methods are computationally minimax\noptimal. A tuning parameter selection method is proposed based on\npost-clustering signal recovery results. Simulation studies are conducted to\nevaluate the proposed methods. A case study on global temperature change\ndemonstrates their utility in practice.", "published": "2025-04-11 07:54:55", "link": "http://arxiv.org/abs/2504.08332v1", "categories": ["stat.ME", "cs.IT", "math.IT", "math.ST", "stat.ML", "stat.TH"], "primary_category": "stat.ME"}
{"title": "A General DoF and Pattern Analyzing Scheme for Electromagnetic Information Theory", "abstract": "Electromagnetic information theory (EIT) is one of the emerging topics for 6G\ncommunication due to its potential to reveal the performance limit of wireless\ncommunication systems. For EIT, one of the most important research directions\nis degree of freedom (DoF) analysis. Existing research works on DoF analysis\nfor EIT focus on asymptotic conclusions of DoF, which do not well fit the\npractical wireless communication systems with finite spatial regions and finite\nfrequency bandwidth. In this paper, we use the theoretical analyzing tools from\nSlepian concentration problem and extend them to three-dimensional space domain\nand four-dimensional space-time domain under electromagnetic constraints. Then\nwe provide asymptotic DoF conclusions and non-asymptotic DoF analyzing scheme,\nwhich suits practical scenarios better, under different scenarios like\nthree-dimensional antenna array. Moreover, we theoretically prove that the\nchannel DoF is upper bounded by the proposed DoF of electromagnetic fields.\nFinally, we use numerical analysis to provide some insights about the optimal\nspatial sampling interval of the antenna array, the DoF of three-dimensional\nantenna array, the impact of unequal antenna spacing, the orthogonal space-time\npatterns, etc.", "published": "2025-04-11 05:27:33", "link": "http://arxiv.org/abs/2504.08262v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Pobogot -- An Open-Hardware Open-Source Low Cost Robot for Swarm Robotics", "abstract": "This paper describes the Pogobot, an open-source and open-hardware platform\nspecifically designed for research involving swarm robotics. Pogobot features\nvibration-based locomotion, infrared communication, and an array of sensors in\na cost-effective package (approx. 250~euros/unit). The platform's modular\ndesign, comprehensive API, and extensible architecture facilitate the\nimplementation of swarm intelligence algorithms and distributed online\nreinforcement learning algorithms. Pogobots offer an accessible alternative to\nexisting platforms while providing advanced capabilities including directional\ncommunication between units. More than 200 Pogobots are already being used on a\ndaily basis at Sorbonne Universit\\'e and PSL to study self-organizing systems,\nprogrammable active matter, discrete reaction-diffusion-advection systems as\nwell as models of social learning and evolution.", "published": "2025-04-11 16:47:59", "link": "http://arxiv.org/abs/2504.08686v1", "categories": ["cs.RO", "cs.AI", "cs.MA"], "primary_category": "cs.RO"}
{"title": "Ready, Bid, Go! On-Demand Delivery Using Fleets of Drones with Unknown, Heterogeneous Energy Storage Constraints", "abstract": "Unmanned Aerial Vehicles (UAVs) are expected to transform logistics, reducing\ndelivery time, costs, and emissions. This study addresses an on-demand delivery\n, in which fleets of UAVs are deployed to fulfil orders that arrive\nstochastically. Unlike previous work, it considers UAVs with heterogeneous,\nunknown energy storage capacities and assumes no knowledge of the energy\nconsumption models. We propose a decentralised deployment strategy that\ncombines auction-based task allocation with online learning. Each UAV\nindependently decides whether to bid for orders based on its energy storage\ncharge level, the parcel mass, and delivery distance. Over time, it refines its\npolicy to bid only for orders within its capability. Simulations using\nrealistic UAV energy models reveal that, counter-intuitively, assigning orders\nto the least confident bidders reduces delivery times and increases the number\nof successfully fulfilled orders. This strategy is shown to outperform\nthreshold-based methods which require UAVs to exceed specific charge levels at\ndeployment. We propose a variant of the strategy which uses learned policies\nfor forecasting. This enables UAVs with insufficient charge levels to commit to\nfulfilling orders at specific future times, helping to prioritise early orders.\nOur work provides new insights into long-term deployment of UAV swarms,\nhighlighting the advantages of decentralised energy-aware decision-making\ncoupled with online learning in real-world dynamic environments.", "published": "2025-04-11 14:39:25", "link": "http://arxiv.org/abs/2504.08585v1", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.RO"}
{"title": "A Hybrid ABM-PDE Framework for Real-World Infectious Disease Simulations", "abstract": "This paper presents a hybrid modeling approach that couples an Agent-Based\nModel (ABM) with a partial differential equation (PDE) model in an epidemic\nsetting to simulate the spatial spread of infectious diseases using a\ncompartmental structure with seven health states. The goal is to reduce the\ncomputational complexity of a full-ABM by introducing a coupled ABM-PDE model\nthat offers significantly faster simulations while maintaining comparable\naccuracy. Our results demonstrate that the hybrid model not only reduces the\noverall simulation runtime (defined as the number of runs required for stable\nresults multiplied by the duration of a single run) but also achieves smaller\nerrors across both 25% and 100% population samples. The coupling mechanism\nensures consistency at the model interface: agents crossing from the ABM into\nthe PDE domain are removed and represented as density contributions at the\ncorresponding grid node, while surplus density in the PDE domain is used to\ngenerate agents with plausible trajectories derived from mobile phone data. We\nevaluate the hybrid model using real-world mobility and infection data for the\nBerlin-Brandenburg region in Germany, showing that it captures the core\nepidemiological dynamics while enabling efficient large-scale simulations.", "published": "2025-04-11 10:44:09", "link": "http://arxiv.org/abs/2504.08430v1", "categories": ["cs.MA", "q-bio.PE"], "primary_category": "cs.MA"}
{"title": "Graph Based Deep Reinforcement Learning Aided by Transformers for Multi-Agent Cooperation", "abstract": "Mission planning for a fleet of cooperative autonomous drones in applications\nthat involve serving distributed target points, such as disaster response,\nenvironmental monitoring, and surveillance, is challenging, especially under\npartial observability, limited communication range, and uncertain environments.\nTraditional path-planning algorithms struggle in these scenarios, particularly\nwhen prior information is not available. To address these challenges, we\npropose a novel framework that integrates Graph Neural Networks (GNNs), Deep\nReinforcement Learning (DRL), and transformer-based mechanisms for enhanced\nmulti-agent coordination and collective task execution. Our approach leverages\nGNNs to model agent-agent and agent-goal interactions through adaptive graph\nconstruction, enabling efficient information aggregation and decision-making\nunder constrained communication. A transformer-based message-passing mechanism,\naugmented with edge-feature-enhanced attention, captures complex interaction\npatterns, while a Double Deep Q-Network (Double DQN) with prioritized\nexperience replay optimizes agent policies in partially observable\nenvironments. This integration is carefully designed to address specific\nrequirements of multi-agent navigation, such as scalability, adaptability, and\nefficient task execution. Experimental results demonstrate superior\nperformance, with 90% service provisioning and 100% grid coverage (node\ndiscovery), while reducing the average steps per episode to 200, compared to\n600 for benchmark methods such as particle swarm optimization (PSO), greedy\nalgorithms and DQN.", "published": "2025-04-11 01:46:18", "link": "http://arxiv.org/abs/2504.08195v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "Toward Super Agent System with Hybrid AI Routers", "abstract": "AI Agents powered by Large Language Models are transforming the world through\nenormous applications. A super agent has the potential to fulfill diverse user\nneeds, such as summarization, coding, and research, by accurately understanding\nuser intent and leveraging the appropriate tools to solve tasks. However, to\nmake such an agent viable for real-world deployment and accessible at scale,\nsignificant optimizations are required to ensure high efficiency and low cost.\nThis paper presents a design of the Super Agent System. Upon receiving a user\nprompt, the system first detects the intent of the user, then routes the\nrequest to specialized task agents with the necessary tools or automatically\ngenerates agentic workflows. In practice, most applications directly serve as\nAI assistants on edge devices such as phones and robots. As different language\nmodels vary in capability and cloud-based models often entail high\ncomputational costs, latency, and privacy concerns, we then explore the hybrid\nmode where the router dynamically selects between local and cloud models based\non task complexity. Finally, we introduce the blueprint of an on-device super\nagent enhanced with cloud. With advances in multi-modality models and edge\nhardware, we envision that most computations can be handled locally, with cloud\ncollaboration only as needed. Such architecture paves the way for super agents\nto be seamlessly integrated into everyday life in the near future.", "published": "2025-04-11 00:54:56", "link": "http://arxiv.org/abs/2504.10519v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Dimension reduction for derivative-informed operator learning: An analysis of approximation errors", "abstract": "We study the derivative-informed learning of nonlinear operators between\ninfinite-dimensional separable Hilbert spaces by neural networks. Such\noperators can arise from the solution of partial differential equations (PDEs),\nand are used in many simulation-based outer-loop tasks in science and\nengineering, such as PDE-constrained optimization, Bayesian inverse problems,\nand optimal experimental design. In these settings, the neural network\napproximations can be used as surrogate models to accelerate the solution of\nthe outer-loop tasks. However, since outer-loop tasks in infinite dimensions\noften require knowledge of the underlying geometry, the approximation accuracy\nof the operator's derivatives can also significantly impact the performance of\nthe surrogate model. Motivated by this, we analyze the approximation errors of\nneural operators in Sobolev norms over infinite-dimensional Gaussian input\nmeasures. We focus on the reduced basis neural operator (RBNO), which uses\nlinear encoders and decoders defined on dominant input/output subspaces spanned\nby reduced sets of orthonormal bases. To this end, we study two methods for\ngenerating the bases; principal component analysis (PCA) and\nderivative-informed subspaces (DIS), which use the dominant eigenvectors of the\ncovariance of the data or the derivatives as the reduced bases, respectively.\nWe then derive bounds for errors arising from both the dimension reduction and\nthe latent neural network approximation, including the sampling errors\nassociated with the empirical estimation of the PCA/DIS. Our analysis is\nvalidated on numerical experiments with elliptic PDEs, where our results show\nthat bases informed by the map (i.e., DIS or output PCA) yield accurate\nreconstructions and generalization errors for both the operator and its\nderivatives, while input PCA may underperform unless ranks and training sample\nsizes are sufficiently large.", "published": "2025-04-11 17:56:52", "link": "http://arxiv.org/abs/2504.08730v1", "categories": ["math.NA", "cs.LG", "cs.NA"], "primary_category": "math.NA"}
{"title": "Discretization Error Analysis of a High Order Unfitted Space-Time Method for moving domain problems", "abstract": "We present a numerical analysis of a higher order unfitted space-time Finite\nElement method applied to a convection-diffusion model problem posed on a\nmoving bulk domain. The method uses isoparametric space-time mappings for the\ngeometry approximation of level set domains and has been presented and\ninvestigated computationally in [Heimann, Lehrenfeld, Preu{\\ss}, SIAM J. Sci.\nComp. 45(2), 2023, B139 - B165]. Recently, in [Heimann, Lehrenfeld, IMA J.\nNumer. Anal., 2025] error bounds for the geometry approximation have been\nproven. In this paper we prove stability and accuracy including the influence\nof the geometry approximation.", "published": "2025-04-11 15:16:20", "link": "http://arxiv.org/abs/2504.08608v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Slicing the Gaussian Mixture Wasserstein Distance", "abstract": "Gaussian mixture models (GMMs) are widely used in machine learning for tasks\nsuch as clustering, classification, image reconstruction, and generative\nmodeling. A key challenge in working with GMMs is defining a computationally\nefficient and geometrically meaningful metric. The mixture Wasserstein (MW)\ndistance adapts the Wasserstein metric to GMMs and has been applied in various\ndomains, including domain adaptation, dataset comparison, and reinforcement\nlearning. However, its high computational cost -- arising from repeated\nWasserstein distance computations involving matrix square root estimations and\nan expensive linear program -- limits its scalability to high-dimensional and\nlarge-scale problems. To address this, we propose multiple novel slicing-based\napproximations to the MW distance that significantly reduce computational\ncomplexity while preserving key optimal transport properties. From a\ntheoretical viewpoint, we establish several weak and strong equivalences\nbetween the introduced metrics, and show the relations to the original MW\ndistance and the well-established sliced Wasserstein distance. Furthermore, we\nvalidate the effectiveness of our approach through numerical experiments,\ndemonstrating computational efficiency and applications in clustering,\nperceptual image comparison, and GMM minimization", "published": "2025-04-11 13:57:09", "link": "http://arxiv.org/abs/2504.08544v1", "categories": ["cs.LG", "cs.NA", "math.NA", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Well-Posedness of Discretizations for Fractional Elasto-Plasticity", "abstract": "We consider a fractional plasticity model based on linear isotropic and\nkinematic hardening as well as a standard von-Mises yield function, where the\nflow rule is replaced by a Riesz--Caputo fractional derivative. The resulting\nmathematical model is typically non-local and non-smooth. Our numerical\nalgorithm is based on the well-known radial return mapping and exploits that\nthe kernel is finitely supported. We propose explicit and implicit\ndiscretizations of the model and show the well-posedness of the explicit in\ntime discretization in combination with a standard finite element approach in\nspace. Our numerical results in 2D and 3D illustrate the performance of the\nalgorithm and the influence of the fractional parameter.", "published": "2025-04-11 11:25:26", "link": "http://arxiv.org/abs/2504.08450v1", "categories": ["math.NA", "cs.NA", "26A33, 74H15, 74H20, 74S05"], "primary_category": "math.NA"}
{"title": "An posteriori error estimator for discontinuous Galerkin discretisations of convection-diffusion problems with application to Earth's mantle convection simulations", "abstract": "We present new aposteriori error estimates for the interior penalty\ndiscontinuous Galerkin method applied to non-stationary convection-diffusion\nequations. The focus is on strongly convection-dominated problems without\nzeroth-order reaction terms, which leads to the absence of positive L^2-like\ncomponents. An important specific example is the energy/temperature equation of\nthe Boussinesq system arising from the modelling of mantle convection of the\nEarth. The key mathematical challenge of mitigating the effects of exponential\nfactors with respect to the final time, arising from the use of Gronwall-type\narguments, is addressed by an exponential fitting technique. The latter results\nto a new class of aposteriori error estimates for the stationary problem, which\nare valid in cases of convection and reaction coefficient combinations not\ncovered by the existing literature. This new class of estimators is combined\nwith an elliptic reconstruction technique to derive new respective estimates\nfor the non-stationary problem, exhibiting reduced dependence on Gronwall-type\nexponents and, thus, offer more accurate estimation for longer time intervals.\nWe showcase the superior performance of the new class of aposteriori error\nestimators in driving mesh adaptivity in Earth's mantle convection simulations,\nin a setting where the energy/temperature equation is discretised by the\ndiscontinuous Galerkin method, coupled with the Taylor-Hood finite element for\nthe momentum and mass conservation equations. We exploit the community code\nASPECT, to present numerical examples showing the effectivity of the proposed\napproach.", "published": "2025-04-11 09:34:50", "link": "http://arxiv.org/abs/2504.08382v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Deep learning-based moment closure for multi-phase computation of semiclassical limit of the Schr\u00f6dinger equation", "abstract": "We present a deep learning approach for computing multi-phase solutions to\nthe semiclassical limit of the Schr\\\"odinger equation. Traditional methods\nrequire deriving a multi-phase ansatz to close the moment system of the\nLiouville equation, a process that is often computationally intensive and\nimpractical. Our method offers an efficient alternative by introducing a novel\ntwo-stage neural network framework to close the $2N\\times 2N$ moment system,\nwhere $N$ represents the number of phases in the solution ansatz. In the first\nstage, we train neural networks to learn the mapping between higher-order\nmoments and lower-order moments (along with their derivatives). The second\nstage incorporates physics-informed neural networks (PINNs), where we\nsubstitute the learned higher-order moments to systematically close the system.\nWe provide theoretical guarantees for the convergence of both the loss\nfunctions and the neural network approximations. Numerical experiments\ndemonstrate the effectiveness of our method for one- and two-dimensional\nproblems with various phase numbers $N$ in the multi-phase solutions. The\nresults confirm the accuracy and computational efficiency of the proposed\napproach compared to conventional techniques.", "published": "2025-04-11 08:13:52", "link": "http://arxiv.org/abs/2504.08341v1", "categories": ["math.NA", "cs.NA", "68T20, 35Q84, 35B40, 82C40"], "primary_category": "math.NA"}
{"title": "Stochastic Momentum ADMM for nonconvex and nonsmooth optimization with application to PnP algorithm", "abstract": "This paper introduces a single-loop Stochastic Momentum Alternating Direction\nMethod of Multipliers (SMADMM) for tackling a class of nonconvex and nonsmooth\noptimization problems. We establish that SMADMM achieves an optimal oracle\ncomplexity of $\\mathcal{O}(\\epsilon^{-\\frac{3}{2}})$ in the online setting,\nwhere only stochastic first-order oracle, is available. In particular, SMADMM\nrequires only $\\mathcal{O}(1)$ stochastic gradient evaluations per iteration\nand avoids the need for restarting with large batch gradient estimates. This is\nthe first stochastic ADMM method achieving optimal oracle complexity for\nnonconvex and nonsmooth problems, requiring $\\mathcal{O}(1)$ batch size.\nFurthermore, we extend our method by integrating it with plug-and-play (PnP)\npriors, resulting in the PnP-SMADMM algorithm. Numerical experiments on\nclassification, CT image reconstruction and phase retrieve demonstrate the\npractical effectiveness of our approach and validate the theoretical findings.", "published": "2025-04-11 03:11:51", "link": "http://arxiv.org/abs/2504.08223v1", "categories": ["math.OC", "cs.NA", "math.NA", "65K05, 65K10, 90C05, 90C26, 90C30"], "primary_category": "math.OC"}
{"title": "International Financial Markets Through 150 Years: Evaluating Stylized Facts", "abstract": "In the theory of financial markets, a stylized fact is a qualitative summary\nof a pattern in financial market data that is observed across multiple assets,\nasset classes and time horizons. In this article, we test a set of eleven\nstylized facts for financial market data. Our main contribution is to consider\na broad range of geographical regions across Asia, continental Europe, and the\nUS over a time period of 150 years, as well as two of the most traded\ncryptocurrencies, thus providing insights into the robustness and\ngeneralizability of commonly known stylized facts.", "published": "2025-04-11 15:19:00", "link": "http://arxiv.org/abs/2504.08611v1", "categories": ["q-fin.ST", "q-fin.GN", "62P05"], "primary_category": "q-fin.ST"}
{"title": "Perfect Clustering in Nonuniform Hypergraphs", "abstract": "While there has been tremendous activity in the area of statistical network\ninference on graphs, hypergraphs have not enjoyed the same attention, on\naccount of their relative complexity and the lack of tractable statistical\nmodels. We introduce a hyper-edge-centric model for analyzing hypergraphs,\ncalled the interaction hypergraph, which models natural sampling methods for\nhypergraphs in neuroscience and communication networks, and accommodates\ninteractions involving different numbers of entities. We define latent\nembeddings for the interactions in such a network, and analyze their\nestimators. In particular, we show that a spectral estimate of the interaction\nlatent positions can achieve perfect clustering once enough interactions are\nobserved.", "published": "2025-04-11 21:12:48", "link": "http://arxiv.org/abs/2504.08980v1", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH", "05C65, 05C80, 60B20, 62F12"], "primary_category": "stat.ME"}
{"title": "Rethinking Few-Shot Image Fusion: Granular Ball Priors Enable General-Purpose Deep Fusion", "abstract": "In image fusion tasks, the absence of real fused images as priors presents a\nfundamental challenge. Most deep learning-based fusion methods rely on\nlarge-scale paired datasets to extract global weighting features from raw\nimages, thereby generating fused outputs that approximate real fused images. In\ncontrast to previous studies, this paper explores few-shot training of neural\nnetworks under the condition of having prior knowledge. We propose a novel\nfusion framework named GBFF, and a Granular Ball Significant Extraction\nalgorithm specifically designed for the few-shot prior setting. All pixel pairs\ninvolved in the fusion process are initially modeled as a Coarse-Grained\nGranular Ball. At the local level, Fine-Grained Granular Balls are used to\nslide through the brightness space to extract Non-Salient Pixel Pairs, and\nperform splitting operations to obtain Salient Pixel Pairs. Pixel-wise weights\nare then computed to generate a pseudo-supervised image. At the global level,\npixel pairs with significant contributions to the fusion process are\ncategorized into the Positive Region, while those whose contributions cannot be\naccurately determined are assigned to the Boundary Region. The Granular Ball\nperforms modality-aware adaptation based on the proportion of the positive\nregion, thereby adjusting the neural network's loss function and enabling it to\ncomplement the information of the boundary region. Extensive experiments\ndemonstrate the effectiveness of both the proposed algorithm and the underlying\ntheory. Compared with state-of-the-art (SOTA) methods, our approach shows\nstrong competitiveness in terms of both fusion time and image expressiveness.\nOur code is publicly available at:", "published": "2025-04-11 19:33:06", "link": "http://arxiv.org/abs/2504.08937v2", "categories": ["cs.GR", "cs.CV", "cs.LG", "eess.IV", "stat.ML"], "primary_category": "cs.GR"}
{"title": "Improving the evaluation of samplers on multi-modal targets", "abstract": "Addressing multi-modality constitutes one of the major challenges of\nsampling. In this reflection paper, we advocate for a more systematic\nevaluation of samplers towards two sources of difficulty that are mode\nseparation and dimension. For this, we propose a synthetic experimental setting\nthat we illustrate on a selection of samplers, focusing on the challenging\ncriterion of recovery of the mode relative importance. These evaluations are\ncrucial to diagnose the potential of samplers to handle multi-modality and\ntherefore to drive progress in the field.", "published": "2025-04-11 18:47:16", "link": "http://arxiv.org/abs/2504.08916v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Surrogate-based optimization of system architectures subject to hidden constraints", "abstract": "The exploration of novel architectures requires physics-based simulation due\nto a lack of prior experience to start from, which introduces two specific\nchallenges for optimization algorithms: evaluations become more expensive (in\ntime) and evaluations might fail. The former challenge is addressed by\nSurrogate-Based Optimization (SBO) algorithms, in particular Bayesian\nOptimization (BO) using Gaussian Process (GP) models. An overview is provided\nof how BO can deal with challenges specific to architecture optimization, such\nas design variable hierarchy and multiple objectives: specific measures include\nensemble infills and a hierarchical sampling algorithm. Evaluations might fail\ndue to non-convergence of underlying solvers or infeasible geometry in certain\nareas of the design space. Such failed evaluations, also known as hidden\nconstraints, pose a particular challenge to SBO/BO, as the surrogate model\ncannot be trained on empty results. This work investigates various strategies\nfor satisfying hidden constraints in BO algorithms. Three high-level strategies\nare identified: rejection of failed points from the training set, replacing\nfailed points based on viable (non-failed) points, and predicting the failure\nregion. Through investigations on a set of test problems including a jet engine\narchitecture optimization problem, it is shown that best performance is\nachieved with a mixed-discrete GP to predict the Probability of Viability\n(PoV), and by ensuring selected infill points satisfy some minimum PoV\nthreshold. This strategy is demonstrated by solving a jet engine architecture\nproblem that features at 50% failure rate and could not previously be solved by\na BO algorithm. The developed BO algorithm and used test problems are available\nin the open-source Python library SBArchOpt.", "published": "2025-04-11 17:35:58", "link": "http://arxiv.org/abs/2504.08721v1", "categories": ["cs.LG", "math.OC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Beyond Black-Box Predictions: Identifying Marginal Feature Effects in Tabular Transformer Networks", "abstract": "In recent years, deep neural networks have showcased their predictive power\nacross a variety of tasks. Beyond natural language processing, the transformer\narchitecture has proven efficient in addressing tabular data problems and\nchallenges the previously dominant gradient-based decision trees in these\nareas. However, this predictive power comes at the cost of intelligibility:\nMarginal feature effects are almost completely lost in the black-box nature of\ndeep tabular transformer networks. Alternative architectures that use the\nadditivity constraints of classical statistical regression models can maintain\nintelligible marginal feature effects, but often fall short in predictive power\ncompared to their more complex counterparts. To bridge the gap between\nintelligibility and performance, we propose an adaptation of tabular\ntransformer networks designed to identify marginal feature effects. We provide\ntheoretical justifications that marginal feature effects can be accurately\nidentified, and our ablation study demonstrates that the proposed model\nefficiently detects these effects, even amidst complex feature interactions. To\ndemonstrate the model's predictive capabilities, we compare it to several\ninterpretable as well as black-box models and find that it can match black-box\nperformances while maintaining intelligibility. The source code is available at\nhttps://github.com/OpenTabular/NAMpy.", "published": "2025-04-11 17:23:09", "link": "http://arxiv.org/abs/2504.08712v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Bayesian optimization for mixed variables using an adaptive dimension reduction process: applications to aircraft design", "abstract": "Multidisciplinary design optimization methods aim at adapting numerical\noptimization techniques to the design of engineering systems involving multiple\ndisciplines. In this context, a large number of mixed continuous, integer and\ncategorical variables might arise during the optimization process and practical\napplications involve a large number of design variables. Recently, there has\nbeen a growing interest in mixed variables constrained Bayesian optimization\nbut most existing approaches severely increase the number of the\nhyperparameters related to the surrogate model. In this paper, we address this\nissue by constructing surrogate models using less hyperparameters. The\nreduction process is based on the partial least squares method. An adaptive\nprocedure for choosing the number of hyperparameters is proposed. The\nperformance of the proposed approach is confirmed on analytical tests as well\nas two real applications related to aircraft design. A significant improvement\nis obtained compared to genetic algorithms.", "published": "2025-04-11 16:43:11", "link": "http://arxiv.org/abs/2504.08682v1", "categories": ["stat.ME", "cs.LG", "math.OC", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Transformer Learns Optimal Variable Selection in Group-Sparse Classification", "abstract": "Transformers have demonstrated remarkable success across various\napplications. However, the success of transformers have not been understood in\ntheory. In this work, we give a case study of how transformers can be trained\nto learn a classic statistical model with \"group sparsity\", where the input\nvariables form multiple groups, and the label only depends on the variables\nfrom one of the groups. We theoretically demonstrate that, a one-layer\ntransformer trained by gradient descent can correctly leverage the attention\nmechanism to select variables, disregarding irrelevant ones and focusing on\nthose beneficial for classification. We also demonstrate that a well-pretrained\none-layer transformer can be adapted to new downstream tasks to achieve good\nprediction accuracy with a limited number of samples. Our study sheds light on\nhow transformers effectively learn structured data.", "published": "2025-04-11 15:39:44", "link": "http://arxiv.org/abs/2504.08638v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Gradient Descent Robustly Learns the Intrinsic Dimension of Data in Training Convolutional Neural Networks", "abstract": "Modern neural networks are usually highly over-parameterized. Behind the wide\nusage of over-parameterized networks is the belief that, if the data are\nsimple, then the trained network will be automatically equivalent to a simple\npredictor. Following this intuition, many existing works have studied different\nnotions of \"ranks\" of neural networks and their relation to the rank of data.\nIn this work, we study the rank of convolutional neural networks (CNNs) trained\nby gradient descent, with a specific focus on the robustness of the rank to\nimage background noises. Specifically, we point out that, when adding\nbackground noises to images, the rank of the CNN trained with gradient descent\nis affected far less compared with the rank of the data. We support our claim\nwith a theoretical case study, where we consider a particular data model to\ncharacterize low-rank clean images with added background noises. We prove that\nCNNs trained by gradient descent can learn the intrinsic dimension of clean\nimages, despite the presence of relatively large background noises. We also\nconduct experiments on synthetic and real datasets to further validate our\nclaim.", "published": "2025-04-11 15:29:55", "link": "http://arxiv.org/abs/2504.08628v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Statistically guided deep learning", "abstract": "We present a theoretically well-founded deep learning algorithm for\nnonparametric regression. It uses over-parametrized deep neural networks with\nlogistic activation function, which are fitted to the given data via gradient\ndescent. We propose a special topology of these networks, a special random\ninitialization of the weights, and a data-dependent choice of the learning rate\nand the number of gradient descent steps. We prove a theoretical bound on the\nexpected $L_2$ error of this estimate, and illustrate its finite sample size\nperformance by applying it to simulated data. Our results show that a\ntheoretical analysis of deep learning which takes into account simultaneously\noptimization, generalization and approximation can result in a new deep\nlearning estimate which has an improved finite sample performance.", "published": "2025-04-11 12:36:06", "link": "http://arxiv.org/abs/2504.08489v1", "categories": ["math.ST", "cs.LG", "stat.ML", "stat.TH"], "primary_category": "math.ST"}
{"title": "Diffusion Models for Robotic Manipulation: A Survey", "abstract": "Diffusion generative models have demonstrated remarkable success in visual\ndomains such as image and video generation. They have also recently emerged as\na promising approach in robotics, especially in robot manipulations. Diffusion\nmodels leverage a probabilistic framework, and they stand out with their\nability to model multi-modal distributions and their robustness to\nhigh-dimensional input and output spaces. This survey provides a comprehensive\nreview of state-of-the-art diffusion models in robotic manipulation, including\ngrasp learning, trajectory planning, and data augmentation. Diffusion models\nfor scene and image augmentation lie at the intersection of robotics and\ncomputer vision for vision-based tasks to enhance generalizability and data\nscarcity. This paper also presents the two main frameworks of diffusion models\nand their integration with imitation learning and reinforcement learning. In\naddition, it discusses the common architectures and benchmarks and points out\nthe challenges and advantages of current state-of-the-art diffusion-based\nmethods.", "published": "2025-04-11 11:01:11", "link": "http://arxiv.org/abs/2504.08438v1", "categories": ["cs.RO", "stat.ML"], "primary_category": "cs.RO"}
{"title": "In almost all shallow analytic neural network optimization landscapes, efficient minimizers have strongly convex neighborhoods", "abstract": "Whether or not a local minimum of a cost function has a strongly convex\nneighborhood greatly influences the asymptotic convergence rate of optimizers.\nIn this article, we rigorously analyze the prevalence of this property for the\nmean squared error induced by shallow, 1-hidden layer neural networks with\nanalytic activation functions when applied to regression problems. The\nparameter space is divided into two domains: the 'efficient domain' (all\nparameters for which the respective realization function cannot be generated by\na network having a smaller number of neurons) and the 'redundant domain' (the\nremaining parameters). In almost all regression problems on the efficient\ndomain the optimization landscape only features local minima that are strongly\nconvex. Formally, we will show that for certain randomly picked regression\nproblems the optimization landscape is almost surely a Morse function on the\nefficient domain. The redundant domain has significantly smaller dimension than\nthe efficient domain and on this domain, potential local minima are never\nisolated.", "published": "2025-04-11 10:43:17", "link": "http://arxiv.org/abs/2504.08867v1", "categories": ["cs.LG", "math.PR", "stat.ML", "60G15, 60G60, 62J02, 62M45, 68T07"], "primary_category": "cs.LG"}
{"title": "Standardization of Weighted Ranking Correlation Coefficients", "abstract": "A relevant problem in statistics is defining the correlation of two rankings\nof a list of items. Kendall's tau and Spearman's rho are two well established\ncorrelation coefficients, characterized by a symmetric form that ensures zero\nexpected value between two pairs of rankings randomly chosen with uniform\nprobability. However, in recent years, several weighted versions of the\noriginal Spearman and Kendall coefficients have emerged that take into account\nthe greater importance of top ranks compared to low ranks, which is common in\nmany contexts. The weighting schemes break the symmetry, causing a non-zero\nexpected value between two random rankings. This issue is very relevant, as it\nundermines the concept of uncorrelation between rankings. In this paper, we\naddress this problem by proposing a standardization function $g(x)$ that maps a\ncorrelation ranking coefficient $\\Gamma$ in a standard form $g(\\Gamma)$ that\nhas zero expected value, while maintaining the relevant statistical properties\nof $\\Gamma$.", "published": "2025-04-11 10:37:19", "link": "http://arxiv.org/abs/2504.08428v1", "categories": ["stat.ME", "cond-mat.stat-mech", "cs.LG", "math.ST", "stat.ML", "stat.TH", "62-08 (Primary) 62-04, 65D10 (Secondary)", "G.3"], "primary_category": "stat.ME"}
{"title": "Proofs as Explanations: Short Certificates for Reliable Predictions", "abstract": "We consider a model for explainable AI in which an explanation for a\nprediction $h(x)=y$ consists of a subset $S'$ of the training data (if it\nexists) such that all classifiers $h' \\in H$ that make at most $b$ mistakes on\n$S'$ predict $h'(x)=y$. Such a set $S'$ serves as a proof that $x$ indeed has\nlabel $y$ under the assumption that (1) the target function $h^\\star$ belongs\nto $H$, and (2) the set $S$ contains at most $b$ corrupted points. For example,\nif $b=0$ and $H$ is the family of linear classifiers in $\\mathbb{R}^d$, and if\n$x$ lies inside the convex hull of the positive data points in $S$ (and hence\nevery consistent linear classifier labels $x$ as positive), then\nCarath\\'eodory's theorem states that $x$ lies inside the convex hull of $d+1$\nof those points. So, a set $S'$ of size $d+1$ could be released as an\nexplanation for a positive prediction, and would serve as a short proof of\ncorrectness of the prediction under the assumption of realizability.\n  In this work, we consider this problem more generally, for general hypothesis\nclasses $H$ and general values $b\\geq 0$. We define the notion of the robust\nhollow star number of $H$ (which generalizes the standard hollow star number),\nand show that it precisely characterizes the worst-case size of the smallest\ncertificate achievable, and analyze its size for natural classes. We also\nconsider worst-case distributional bounds on certificate size, as well as\ndistribution-dependent bounds that we show tightly control the sample size\nneeded to get a certificate for any given test example. In particular, we\ndefine a notion of the certificate coefficient $\\varepsilon_x$ of an example\n$x$ with respect to a data distribution $D$ and target function $h^\\star$, and\nprove matching upper and lower bounds on sample size as a function of\n$\\varepsilon_x$, $b$, and the VC dimension $d$ of $H$.", "published": "2025-04-11 09:26:37", "link": "http://arxiv.org/abs/2504.08377v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Entropic bounds for conditionally Gaussian vectors and applications to neural networks", "abstract": "Using entropic inequalities from information theory, we provide new bounds on\nthe total variation and 2-Wasserstein distances between a conditionally\nGaussian law and a Gaussian law with invertible covariance matrix. We apply our\nresults to quantify the speed of convergence to Gaussian of a randomly\ninitialized fully connected neural network and its derivatives - evaluated in a\nfinite number of inputs - when the initialization is Gaussian and the sizes of\nthe inner layers diverge to infinity. Our results require mild assumptions on\nthe activation function, and allow one to recover optimal rates of convergence\nin a variety of distances, thus improving and extending the findings of Basteri\nand Trevisan (2023), Favaro et al. (2023), Trevisan (2024) and Apollonio et al.\n(2024). One of our main tools are the quantitative cumulant estimates\nestablished in Hanin (2024). As an illustration, we apply our results to bound\nthe total variation distance between the Bayesian posterior law of the neural\nnetwork and its derivatives, and the posterior law of the corresponding\nGaussian limit: this yields quantitative versions of a posterior CLT by Hron et\nal. (2022), and extends several estimates by Trevisan (2024) to the total\nvariation metric.", "published": "2025-04-11 08:00:37", "link": "http://arxiv.org/abs/2504.08335v1", "categories": ["math.PR", "cs.AI", "cs.LG", "stat.ML", "60F05 (Primary) 68T07 (Secondary)", "G.3; I.2"], "primary_category": "math.PR"}
{"title": "An Introduction to Double/Debiased Machine Learning", "abstract": "This paper provides a practical introduction to Double/Debiased Machine\nLearning (DML). DML provides a general approach to performing inference about a\ntarget parameter in the presence of nuisance parameters. The aim of DML is to\nreduce the impact of nuisance parameter estimation on estimators of the\nparameter of interest. We describe DML and its two essential components: Neyman\northogonality and cross-fitting. We highlight that DML reduces functional form\ndependence and accommodates the use of complex data types, such as text data.\nWe illustrate its application through three empirical examples that demonstrate\nDML's applicability in cross-sectional and panel settings.", "published": "2025-04-11 07:48:42", "link": "http://arxiv.org/abs/2504.08324v1", "categories": ["econ.EM", "stat.ME", "stat.ML"], "primary_category": "econ.EM"}
{"title": "All Optical Echo State Network Reservoir Computing", "abstract": "We propose an innovative design for an all-optical Echo State Network (ESN),\nan advanced type of reservoir computer known for its universal computational\ncapabilities. Our design enables fully optical implementation of arbitrary\nESNs, featuring complete flexibility in optical matrix multiplication and\nnonlinear activation. Leveraging the nonlinear characteristics of stimulated\nBrillouin scattering (SBS), the architecture efficiently realizes\nmeasurement-free operations crucial for reservoir computing. The approach\nsignificantly reduces computational overhead and energy consumption compared to\ntraditional software-based methods. Comprehensive simulations validate the\nsystem's memory capacity, nonlinear processing strength, and polynomial algebra\ncapabilities, showcasing performance comparable to software ESNs across key\nbenchmark tasks. Our design establishes a feasible, scalable, and universally\napplicable framework for optical reservoir computing, suitable for diverse\nmachine learning applications.", "published": "2025-04-11 03:12:53", "link": "http://arxiv.org/abs/2504.08224v1", "categories": ["physics.optics", "cs.LG", "stat.ML"], "primary_category": "physics.optics"}
{"title": "Local Distance-Preserving Node Embeddings and Their Performance on Random Graphs", "abstract": "Learning node representations is a fundamental problem in graph machine\nlearning. While existing embedding methods effectively preserve local\nsimilarity measures, they often fail to capture global functions like graph\ndistances. Inspired by Bourgain's seminal work on Hilbert space embeddings of\nmetric spaces (1985), we study the performance of local distance-preserving\nnode embeddings. Known as landmark-based algorithms, these embeddings\napproximate pairwise distances by computing shortest paths from a small subset\nof reference nodes (i.e., landmarks). Our main theoretical contribution shows\nthat random graphs, such as Erd\\H{o}s-R\\'enyi random graphs, require lower\ndimensions in landmark-based embeddings compared to worst-case graphs.\nEmpirically, we demonstrate that the GNN-based approximations for the distances\nto landmarks generalize well to larger networks, offering a scalable\nalternative for graph representation learning.", "published": "2025-04-11 02:47:46", "link": "http://arxiv.org/abs/2504.08216v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Deep Distributional Learning with Non-crossing Quantile Network", "abstract": "In this paper, we introduce a non-crossing quantile (NQ) network for\nconditional distribution learning. By leveraging non-negative activation\nfunctions, the NQ network ensures that the learned distributions remain\nmonotonic, effectively addressing the issue of quantile crossing. Furthermore,\nthe NQ network-based deep distributional learning framework is highly\nadaptable, applicable to a wide range of applications, from classical\nnon-parametric quantile regression to more advanced tasks such as causal effect\nestimation and distributional reinforcement learning (RL). We also develop a\ncomprehensive theoretical foundation for the deep NQ estimator and its\napplication to distributional RL, providing an in-depth analysis that\ndemonstrates its effectiveness across these domains. Our experimental results\nfurther highlight the robustness and versatility of the NQ network.", "published": "2025-04-11 02:46:39", "link": "http://arxiv.org/abs/2504.08215v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "primary_category": "stat.ML"}
{"title": "Optimizing Power Grid Topologies with Reinforcement Learning: A Survey of Methods and Challenges", "abstract": "Power grid operation is becoming increasingly complex due to the rising\nintegration of renewable energy sources and the need for more adaptive control\nstrategies. Reinforcement Learning (RL) has emerged as a promising approach to\npower network control (PNC), offering the potential to enhance decision-making\nin dynamic and uncertain environments. The Learning To Run a Power Network\n(L2RPN) competitions have played a key role in accelerating research by\nproviding standardized benchmarks and problem formulations, leading to rapid\nadvancements in RL-based methods. This survey provides a comprehensive and\nstructured overview of RL applications for power grid topology optimization,\ncategorizing existing techniques, highlighting key design choices, and\nidentifying gaps in current research. Additionally, we present a comparative\nnumerical study evaluating the impact of commonly applied RL-based methods,\noffering insights into their practical effectiveness. By consolidating existing\nresearch and outlining open challenges, this survey aims to provide a\nfoundation for future advancements in RL-driven power grid optimization.", "published": "2025-04-11 02:27:30", "link": "http://arxiv.org/abs/2504.08210v1", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY", "stat.ML"], "primary_category": "eess.SY"}
{"title": "A Piecewise Lyapunov Analysis of Sub-quadratic SGD: Applications to Robust and Quantile Regression", "abstract": "Motivated by robust and quantile regression problems, we investigate the\nstochastic gradient descent (SGD) algorithm for minimizing an objective\nfunction $f$ that is locally strongly convex with a sub--quadratic tail. This\nsetting covers many widely used online statistical methods. We introduce a\nnovel piecewise Lyapunov function that enables us to handle functions $f$ with\nonly first-order differentiability, which includes a wide range of popular loss\nfunctions such as Huber loss. Leveraging our proposed Lyapunov function, we\nderive finite-time moment bounds under general diminishing stepsizes, as well\nas constant stepsizes. We further establish the weak convergence, central limit\ntheorem and bias characterization under constant stepsize, providing the first\ngeometrical convergence result for sub--quadratic SGD. Our results have wide\napplications, especially in online statistical methods. In particular, we\ndiscuss two applications of our results. 1) Online robust regression: We\nconsider a corrupted linear model with sub--exponential covariates and\nheavy--tailed noise. Our analysis provides convergence rates comparable to\nthose for corrupted models with Gaussian covariates and noise. 2) Online\nquantile regression: Importantly, our results relax the common assumption in\nprior work that the conditional density is continuous and provide a more\nfine-grained analysis for the moment bounds.", "published": "2025-04-11 00:20:37", "link": "http://arxiv.org/abs/2504.08178v3", "categories": ["stat.ML", "cs.LG", "math.OC", "math.PR", "math.ST", "stat.TH"], "primary_category": "stat.ML"}
{"title": "Beyond Global Metrics: A Fairness Analysis for Interpretable Voice Disorder Detection Systems", "abstract": "We conducted a comprehensive analysis of an Automatic Voice Disorders\nDetection (AVDD) system using existing voice disorder datasets with available\ndemographic metadata. The study involved analysing system performance across\nvarious demographic groups, particularly focusing on gender and age-based\ncohorts. Performance evaluation was based on multiple metrics, including\nnormalised costs and cross-entropy. We employed calibration techniques trained\nseparately on predefined demographic groups to address group-dependent\nmiscalibration. Analysis revealed significant performance disparities across\ngroups despite strong global metrics. The system showed systematic biases,\nmisclassifying healthy speakers over 55 as having a voice disorder and speakers\nwith disorders aged 14-30 as healthy. Group-specific calibration improved\nposterior probability quality, reducing overconfidence. For young disordered\nspeakers, low severity scores were identified as contributing to poor system\nperformance. For older speakers, age-related voice characteristics and\npotential limitations in the pretrained Hubert model used as feature extractor\nlikely affected results. The study demonstrates that global performance metrics\nare insufficient for evaluating AVDD system performance. Group-specific\nanalysis may unmask problems in system performance which are hidden within\nglobal metrics. Further, group-dependent calibration strategies help mitigate\nbiases, resulting in a more reliable indication of system confidence. These\nfindings emphasize the need for demographic-specific evaluation and calibration\nin voice disorder detection systems, while providing a methodological framework\napplicable to broader biomedical classification tasks where demographic\nmetadata is available.", "published": "2025-04-11 22:17:05", "link": "http://arxiv.org/abs/2504.08997v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Spatial Audio Processing with Large Language Model on Wearable Devices", "abstract": "Integrating spatial context into large language models (LLMs) has the\npotential to revolutionize human-computer interaction, particularly in wearable\ndevices. In this work, we present a novel system architecture that incorporates\nspatial speech understanding into LLMs, enabling contextually aware and\nadaptive applications for wearable technologies. Our approach leverages\nmicrostructure-based spatial sensing to extract precise Direction of Arrival\n(DoA) information using a monaural microphone. To address the lack of existing\ndataset for microstructure-assisted speech recordings, we synthetically create\na dataset called OmniTalk by using the LibriSpeech dataset. This spatial\ninformation is fused with linguistic embeddings from OpenAI's Whisper model,\nallowing each modality to learn complementary contextual representations. The\nfused embeddings are aligned with the input space of LLaMA-3.2 3B model and\nfine-tuned with lightweight adaptation technique LoRA to optimize for on-device\nprocessing. SING supports spatially-aware automatic speech recognition (ASR),\nachieving a mean error of $25.72^\\circ$-a substantial improvement compared to\nthe 88.52$^\\circ$ median error in existing work-with a word error rate (WER) of\n5.3. SING also supports soundscaping, for example, inference how many people\nwere talking and their directions, with up to 5 people and a median DoA error\nof 16$^\\circ$. Our system demonstrates superior performance in spatial speech\nunderstanding while addressing the challenges of power efficiency, privacy, and\nhardware constraints, paving the way for advanced applications in augmented\nreality, accessibility, and immersive experiences.", "published": "2025-04-11 18:19:59", "link": "http://arxiv.org/abs/2504.08907v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "BowelRCNN: Region-based Convolutional Neural Network System for Bowel Sound Auscultation", "abstract": "Sound events representing intestinal activity detection is a diagnostic tool\nwith potential to identify gastrointestinal conditions. This article introduces\nBowelRCNN, a novel bowel sound detection system that uses audio recording,\nspectrogram analysys and region-based convolutional neural network (RCNN)\narchitecture. The system was trained and validated on a real recording dataset\ngathered from 19 patients, comprising 60 minutes of prepared and annotated\naudio data. BowelRCNN achieved a classification accuracy of 96% and an F1 score\nof 71%. This research highlights the feasibility of using CNN architectures for\nbowel sound auscultation, achieving results comparable to those of\nrecurrent-convolutional methods.", "published": "2025-04-11 16:01:04", "link": "http://arxiv.org/abs/2504.08659v1", "categories": ["cs.SD", "eess.AS", "68", "J.3"], "primary_category": "cs.SD"}
{"title": "Reverberation-based Features for Sound Event Localization and Detection with Distance Estimation", "abstract": "Sound event localization and detection (SELD) involves predicting active\nsound event classes over time while estimating their positions. The\nlocalization subtask in SELD is usually treated as a direction of arrival\nestimation problem, ignoring source distance. Only recently, SELD was extended\nto 3D by incorporating distance estimation, enabling the prediction of sound\nevent positions in 3D space (3D SELD). However, existing methods lack input\nfeatures designed for distance estimation. We argue that reverberation encodes\nvaluable information for this task. This paper introduces two novel feature\nformats for 3D SELD based on reverberation: one using direct-to-reverberant\nratio (DRR) and another leveraging signal autocorrelation to provide the model\nwith insights into early reflections. Pre-training on synthetic data improves\nrelative distance error (RDE) and overall SELD score, with\nautocorrelation-based features reducing RDE by over 3 percentage points on the\nSTARSS23 dataset. The code to extract the features is available at\ngithub.com/dberghi/SELD-distance-features.", "published": "2025-04-11 15:43:13", "link": "http://arxiv.org/abs/2504.08644v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "TorchFX: A modern approach to Audio DSP with PyTorch and GPU acceleration", "abstract": "The burgeoning complexity and real-time processing demands of audio signals\nnecessitate optimized algorithms that harness the computational prowess of\nGraphics Processing Units (GPUs). Existing Digital Signal Processing (DSP)\nlibraries often fall short in delivering the requisite efficiency and\nflexibility, particularly in integrating Artificial Intelligence (AI) models.\nIn response, we introduce TorchFX: a GPU-accelerated Python library for DSP,\nspecifically engineered to facilitate sophisticated audio signal processing.\nBuilt atop the PyTorch framework, TorchFX offers an Object-Oriented interface\nthat emulates the usability of torchaudio, enhancing functionality with a novel\npipe operator for intuitive filter chaining. This library provides a\ncomprehensive suite of Finite Impulse Response (FIR) and Infinite Impulse\nResponse (IIR) filters, with a focus on multichannel audio files, thus\nfacilitating the integration of DSP and AI-based approaches. Our benchmarking\nresults demonstrate significant efficiency gains over traditional libraries\nlike SciPy, particularly in multichannel contexts. Despite current limitations\nin GPU compatibility, ongoing developments promise broader support and\nreal-time processing capabilities. TorchFX aims to become a useful tool for the\ncommunity, contributing to innovation and progress in DSP with GPU\nacceleration. TorchFX is publicly available on GitHub at\nhttps://github.com/matteospanio/torchfx.", "published": "2025-04-11 15:26:10", "link": "http://arxiv.org/abs/2504.08624v1", "categories": ["eess.AS", "cs.PF", "cs.SD", "eess.SP", "D.2.10; J.5"], "primary_category": "eess.AS"}
{"title": "On The Landscape of Spoken Language Models: A Comprehensive Survey", "abstract": "The field of spoken language processing is undergoing a shift from training\ncustom-built, task-specific models toward using and optimizing spoken language\nmodels (SLMs) which act as universal speech processing systems. This trend is\nsimilar to the progression toward universal language models that has taken\nplace in the field of (text) natural language processing. SLMs include both\n\"pure\" language models of speech -- models of the distribution of tokenized\nspeech sequences -- and models that combine speech encoders with text language\nmodels, often including both spoken and written input or output. Work in this\narea is very diverse, with a range of terminology and evaluation settings. This\npaper aims to contribute an improved understanding of SLMs via a unifying\nliterature survey of recent work in the context of the evolution of the field.\nOur survey categorizes the work in this area by model architecture, training,\nand evaluation choices, and describes some key challenges and directions for\nfuture work.", "published": "2025-04-11 13:40:53", "link": "http://arxiv.org/abs/2504.08528v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Mitigating Timbre Leakage with Universal Semantic Mapping Residual Block for Voice Conversion", "abstract": "Voice conversion (VC) transforms source speech into a target voice by\npreserving the content. However, timbre information from the source speaker is\ninherently embedded in the content representations, causing significant timbre\nleakage and reducing similarity to the target speaker. To address this, we\nintroduce a residual block to a content extractor. The residual block consists\nof two weighted branches: 1) universal semantic dictionary based Content\nFeature Re-expression (CFR) module, supplying timbre-free content\nrepresentation. 2) skip connection to the original content layer, providing\ncomplementary fine-grained information. In the CFR module, each dictionary\nentry in the universal semantic dictionary represents a phoneme class, computed\nstatistically using speech from multiple speakers, creating a stable,\nspeaker-independent semantic set. We introduce a CFR method to obtain\ntimbre-free content representations by expressing each content frame as a\nweighted linear combination of dictionary entries using corresponding phoneme\nposteriors as weights. Extensive experiments across various VC frameworks\ndemonstrate that our approach effectively mitigates timbre leakage and\nsignificantly improves similarity to the target speaker.", "published": "2025-04-11 13:36:59", "link": "http://arxiv.org/abs/2504.08524v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On the Design of Diffusion-based Neural Speech Codecs", "abstract": "Recently, neural speech codecs (NSCs) trained as generative models have shown\nsuperior performance compared to conventional codecs at low bitrates. Although\nmost state-of-the-art NSCs are trained as Generative Adversarial Networks\n(GANs), Diffusion Models (DMs), a recent class of generative models, represent\na promising alternative due to their superior performance in image generation\nrelative to GANs. Consequently, DMs have been successfully applied for audio\nand speech coding among various other audio generation applications. However,\nthe design of diffusion-based NSCs has not yet been explored in a systematic\nway. We address this by providing a comprehensive analysis of diffusion-based\nNSCs divided into three contributions. First, we propose a categorization based\non the conditioning and output domains of the DM. This simple conceptual\nframework allows us to define a design space for diffusion-based NSCs and to\nassign a category to existing approaches in the literature. Second, we\nsystematically investigate unexplored designs by creating and evaluating new\ndiffusion-based NSCs within the conceptual framework. Finally, we compare the\nproposed models to existing GAN and DM baselines through objective metrics and\nsubjective listening tests.", "published": "2025-04-11 11:58:38", "link": "http://arxiv.org/abs/2504.08470v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Passive Underwater Acoustic Signal Separation based on Feature Decoupling Dual-path Network", "abstract": "Signal separation in the passive underwater acoustic domain has heavily\nrelied on deep learning techniques to isolate ship radiated noise. However, the\nseparation networks commonly used in this domain stem from speech separation\napplications and may not fully consider the unique aspects of underwater\nacoustics beforehand, such as the influence of different propagation media,\nsignal frequencies and modulation characteristics. This oversight highlights\nthe need for tailored approaches that account for the specific characteristics\nof underwater sound propagation. This study introduces a novel temporal network\ndesigned to separate ship radiated noise by employing a dual-path model and a\nfeature decoupling approach. The mixed signals' features are transformed into a\nspace where they exhibit greater independence, with each dimension's\nsignificance decoupled. Subsequently, a fusion of local and global attention\nmechanisms is employed in the separation layer. Extensive comparisons showcase\nthe effectiveness of this method when compared to other prevalent network\nmodels, as evidenced by its performance in the ShipsEar and DeepShip datasets.", "published": "2025-04-11 09:16:22", "link": "http://arxiv.org/abs/2504.08371v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "68T10", "I.5.4; I.2.6; J.2"], "primary_category": "cs.SD"}
{"title": "Location-Oriented Sound Event Localization and Detection with Spatial Mapping and Regression Localization", "abstract": "Sound Event Localization and Detection (SELD) combines the Sound Event\nDetection (SED) with the corresponding Direction Of Arrival (DOA). Recently,\nadopted event oriented multi-track methods affect the generality in polyphonic\nenvironments due to the limitation of the number of tracks. To enhance the\ngenerality in polyphonic environments, we propose Spatial Mapping and\nRegression Localization for SELD (SMRL-SELD). SMRL-SELD segments the 3D spatial\nspace, mapping it to a 2D plane, and a new regression localization loss is\nproposed to help the results converge toward the location of the corresponding\nevent. SMRL-SELD is location-oriented, allowing the model to learn event\nfeatures based on orientation. Thus, the method enables the model to process\npolyphonic sounds regardless of the number of overlapping events. We conducted\nexperiments on STARSS23 and STARSS22 datasets and our proposed SMRL-SELD\noutperforms the existing SELD methods in overall evaluation and polyphony\nenvironments.", "published": "2025-04-11 09:00:53", "link": "http://arxiv.org/abs/2504.08365v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generalized Multilingual Text-to-Speech Generation with Language-Aware Style Adaptation", "abstract": "Text-to-Speech (TTS) models can generate natural, human-like speech across\nmultiple languages by transforming phonemes into waveforms. However,\nmultilingual TTS remains challenging due to discrepancies in phoneme\nvocabularies and variations in prosody and speaking style across languages.\nExisting approaches either train separate models for each language, which\nachieve high performance at the cost of increased computational resources, or\nuse a unified model for multiple languages that struggles to capture\nfine-grained, language-specific style variations. In this work, we propose\nLanStyleTTS, a non-autoregressive, language-aware style adaptive TTS framework\nthat standardizes phoneme representations and enables fine-grained,\nphoneme-level style control across languages. This design supports a unified\nmultilingual TTS model capable of producing accurate and high-quality speech\nwithout the need to train language-specific models. We evaluate LanStyleTTS by\nintegrating it with several state-of-the-art non-autoregressive TTS\narchitectures. Results show consistent performance improvements across\ndifferent model backbones. Furthermore, we investigate a range of acoustic\nfeature representations, including mel-spectrograms and autoencoder-derived\nlatent features. Our experiments demonstrate that latent encodings can\nsignificantly reduce model size and computational cost while preserving\nhigh-quality speech generation.", "published": "2025-04-11 06:12:57", "link": "http://arxiv.org/abs/2504.08274v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "How competitive are pay-as-bid auction games?", "abstract": "Motivated by the current structure of ancillary services markets, we study\nthe pay-as-bid auction game, a supply function model with discriminatory\npricing and asymmetric firms. In this game, strategies are non-decreasing\nsupply functions relating price to quantity and the exact choice of the\nstrategy space turns out to be a crucial issue: when it includes all\nnon-decreasing continuous functions, pure-strategy Nash equilibria often fail\nto exist. To overcome this, we restrict the strategy space to the set of\nLipschitz-continuous functions and we prove that Nash equilibria always exist\n(under standard concavity assumptions) and consist of functions that are affine\non their own support and have slope equal to the maximum allowed Lipschitz\nconstant. We further show that the Nash equilibrium is unique up to the\nmarket-clearing price when the demand is affine and the asymmetric marginal\nproduction costs are homogeneous in zero. For quadratic production costs, we\nderive a closed-form expression and we compute the limit as the allowed\nLipschitz constant grows to infinity. Our results show that in the limit the\npay-as-bid auction game achieves perfect competition with efficient allocation\nand induces a lower market-clearing price compared to supply function models\nbased on uniform price auctions.", "published": "2025-04-11 12:23:31", "link": "http://arxiv.org/abs/2504.13920v1", "categories": ["math.OC", "cs.GT", "cs.MA", "cs.SY", "eess.SY"], "primary_category": "math.OC"}
{"title": "RouterKT: Mixture-of-Experts for Knowledge Tracing", "abstract": "Knowledge Tracing (KT) is a fundamental task in Intelligent Tutoring Systems\n(ITS), which aims to model the dynamic knowledge states of students based on\ntheir interaction histories. However, existing KT models often rely on a global\nforgetting decay mechanism for capturing learning patterns, assuming that\nstudents' performance is predominantly influenced by their most recent\ninteractions. Such approaches fail to account for the diverse and complex\nlearning patterns arising from individual differences and varying learning\nstages. To address this limitation, we propose RouterKT, a novel\nMixture-of-Experts (MoE) architecture designed to capture heterogeneous\nlearning patterns by enabling experts to specialize in different patterns\nwithout any handcrafted learning pattern bias such as forgetting decay.\nSpecifically, RouterKT introduces a \\textbf{person-wise routing mechanism} to\neffectively model individual-specific learning behaviors and employs\n\\textbf{multi-heads as experts} to enhance the modeling of complex and diverse\npatterns. Comprehensive experiments on ten benchmark datasets demonstrate that\nRouterKT exhibits significant flexibility and improves the performance of\nvarious KT backbone models, with a maximum average AUC improvement of 3.29\\%\nacross different backbones and datasets, outperforming other state-of-the-art\nmodels. Moreover, RouterKT demonstrates consistently superior inference\nefficiency compared to existing approaches based on handcrafted learning\npattern bias, highlighting its usability for real-world educational\napplications. The source code is available at\nhttps://github.com/ringotc/RouterKT.git.", "published": "2025-04-11 21:42:08", "link": "http://arxiv.org/abs/2504.08989v2", "categories": ["cs.CY", "cs.IR", "cs.LG"], "primary_category": "cs.CY"}
{"title": "How competitive are pay-as-bid auction games?", "abstract": "We study the pay-as-bid auction game, a supply function model with\ndiscriminatory pricing and asymmetric firms. In this game, strategies are\nnon-decreasing supply functions relating pric to quantity and the exact choice\nof the strategy space turns out to be a crucial issue: when it includes all\nnon-decreasing continuous functions, pure-strategy Nash equilibria often fail\nto exist. To overcome this, we restrict the strategy space to the set of\nLipschitz-continuous functions and we prove that Nash equilibria always exist\n(under standard concavity assumptions) and consist of functions that are affine\non their own support and have slope equal to the maximum allowed Lipschitz\nconstant. We further show that the Nash equilibrium is unique up to the\nmarket-clearing price when the demand is affine and the asymmetric marginal\nproduction costs are homogeneous in zero. For quadratic production costs, we\nderive a closed-form expression and we compute the limit as the allowed\nLipschitz constant grows to infinity. Our results show that in the limit the\npay-as-bid auction game achieves perfect competition with efficient allocation\nand induces a lower market-clearing price compared to supply function models\nbased on uniform price auctions.", "published": "2025-04-11 12:23:31", "link": "http://arxiv.org/abs/2504.13920v2", "categories": ["math.OC", "cs.GT", "cs.MA", "cs.SY", "eess.SY"], "primary_category": "math.OC"}
{"title": "Location-Oriented Sound Event Localization and Detection with Spatial Mapping and Regression Localization", "abstract": "Sound Event Localization and Detection (SELD) combines the Sound Event\nDetection (SED) with the corresponding Direction Of Arrival (DOA). Recently,\nadopted event oriented multi-track methods affect the generality in polyphonic\nenvironments due to the limitation of the number of tracks. To enhance the\ngenerality in polyphonic environments, we propose Spatial Mapping and\nRegression Localization for SELD (SMRL-SELD). SMRL-SELD segments the 3D spatial\nspace, mapping it to a 2D plane, and a new regression localization loss is\nproposed to help the results converge toward the location of the corresponding\nevent. SMRL-SELD is location-oriented, allowing the model to learn event\nfeatures based on orientation. Thus, the method enables the model to process\npolyphonic sounds regardless of the number of overlapping events. We conducted\nexperiments on STARSS23 and STARSS22 datasets and our proposed SMRL-SELD\noutperforms the existing SELD methods in overall evaluation and polyphony\nenvironments.", "published": "2025-04-11 09:00:53", "link": "http://arxiv.org/abs/2504.08365v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spatial Audio Processing with Large Language Model on Wearable Devices", "abstract": "Integrating spatial context into large language models (LLMs) has the\npotential to revolutionize human-computer interaction, particularly in wearable\ndevices. In this work, we present a novel system architecture that incorporates\nspatial speech understanding into LLMs, enabling contextually aware and\nadaptive applications for wearable technologies. Our approach leverages\nmicrostructure-based spatial sensing to extract precise Direction of Arrival\n(DoA) information using a monaural microphone. To address the lack of existing\ndataset for microstructure-assisted speech recordings, we synthetically create\na dataset called OmniTalk by using the LibriSpeech dataset. This spatial\ninformation is fused with linguistic embeddings from OpenAI's Whisper model,\nallowing each modality to learn complementary contextual representations. The\nfused embeddings are aligned with the input space of LLaMA-3.2 3B model and\nfine-tuned with lightweight adaptation technique LoRA to optimize for on-device\nprocessing. SING supports spatially-aware automatic speech recognition (ASR),\nachieving a mean error of $25.72^\\circ$-a substantial improvement compared to\nthe 88.52$^\\circ$ median error in existing work-with a word error rate (WER) of\n5.3. SING also supports soundscaping, for example, inference how many people\nwere talking and their directions, with up to 5 people and a median DoA error\nof 16$^\\circ$. Our system demonstrates superior performance in spatial speech\nunderstanding while addressing the challenges of power efficiency, privacy, and\nhardware constraints, paving the way for advanced applications in augmented\nreality, accessibility, and immersive experiences.", "published": "2025-04-11 18:19:59", "link": "http://arxiv.org/abs/2504.08907v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Mitigating Timbre Leakage with Universal Semantic Mapping Residual Block for Voice Conversion", "abstract": "Voice conversion (VC) transforms source speech into a target voice by\npreserving the content. However, timbre information from the source speaker is\ninherently embedded in the content representations, causing significant timbre\nleakage and reducing similarity to the target speaker. To address this, we\nintroduce a residual block to a content extractor. The residual block consists\nof two weighted branches: 1) universal semantic dictionary based Content\nFeature Re-expression (CFR) module, supplying timbre-free content\nrepresentation. 2) skip connection to the original content layer, providing\ncomplementary fine-grained information. In the CFR module, each dictionary\nentry in the universal semantic dictionary represents a phoneme class, computed\nstatistically using speech from multiple speakers, creating a stable,\nspeaker-independent semantic set. We introduce a CFR method to obtain\ntimbre-free content representations by expressing each content frame as a\nweighted linear combination of dictionary entries using corresponding phoneme\nposteriors as weights. Extensive experiments across various VC frameworks\ndemonstrate that our approach effectively mitigates timbre leakage and\nsignificantly improves similarity to the target speaker.", "published": "2025-04-11 13:36:59", "link": "http://arxiv.org/abs/2504.08524v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
