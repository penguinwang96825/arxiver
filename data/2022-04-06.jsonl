{"title": "Quick Starting Dialog Systems with Paraphrase Generation", "abstract": "Acquiring training data to improve the robustness of dialog systems can be a\npainstakingly long process. In this work, we propose a method to reduce the\ncost and effort of creating new conversational agents by artificially\ngenerating more data from existing examples, using paraphrase generation. Our\nproposed approach can kick-start a dialog system with little human effort, and\nbrings its performance to a level satisfactory enough for allowing actual\ninteractions with real end-users. We experimented with two neural paraphrasing\napproaches, namely Neural Machine Translation and a Transformer-based seq2seq\nmodel. We present the results obtained with two datasets in English and in\nFrench:~a crowd-sourced public intent classification dataset and our own\ncorporate dialog system dataset. We show that our proposed approach increased\nthe generalization capabilities of the intent classification model on both\ndatasets, reducing the effort required to initialize a new dialog system and\nhelping to deploy this technology at scale within an organization.", "published": "2022-04-06 02:35:59", "link": "http://arxiv.org/abs/2204.02546v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "C3KG: A Chinese Commonsense Conversation Knowledge Graph", "abstract": "Existing commonsense knowledge bases often organize tuples in an isolated\nmanner, which is deficient for commonsense conversational models to plan the\nnext steps. To fill the gap, we curate a large-scale multi-turn human-written\nconversation corpus, and create the first Chinese commonsense conversation\nknowledge graph which incorporates both social commonsense knowledge and dialog\nflow information. To show the potential of our graph, we develop a\ngraph-conversation matching approach, and benchmark two graph-grounded\nconversational tasks.", "published": "2022-04-06 02:59:34", "link": "http://arxiv.org/abs/2204.02549v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing Structured Pruning on Multilingual Pre-trained Models: Settings,\n  Algorithms, and Efficiency", "abstract": "Structured pruning has been extensively studied on monolingual pre-trained\nlanguage models and is yet to be fully evaluated on their multilingual\ncounterparts. This work investigates three aspects of structured pruning on\nmultilingual pre-trained language models: settings, algorithms, and efficiency.\nExperiments on nine downstream tasks show several counter-intuitive phenomena:\nfor settings, individually pruning for each language does not induce a better\nresult; for algorithms, the simplest method performs the best; for efficiency,\na fast model does not imply that it is also small. To facilitate the comparison\non all sparsity levels, we present Dynamic Sparsification, a simple approach\nthat allows training the model once and adapting to different model sizes at\ninference. We hope this work fills the gap in the study of structured pruning\non multilingual pre-trained models and sheds light on future research.", "published": "2022-04-06 06:29:52", "link": "http://arxiv.org/abs/2204.02601v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "There Are a Thousand Hamlets in a Thousand People's Eyes: Enhancing\n  Knowledge-grounded Dialogue with Personal Memory", "abstract": "Knowledge-grounded conversation (KGC) shows great potential in building an\nengaging and knowledgeable chatbot, and knowledge selection is a key ingredient\nin it. However, previous methods for knowledge selection only concentrate on\nthe relevance between knowledge and dialogue context, ignoring the fact that\nage, hobby, education and life experience of an interlocutor have a major\neffect on his or her personal preference over external knowledge. Without\ntaking the personalization issue into account, it is difficult to select the\nproper knowledge and generate persona-consistent responses. In this work, we\nintroduce personal memory into knowledge selection in KGC to address the\npersonalization issue. We propose a variational method to model the underlying\nrelationship between one's personal memory and his or her selection of\nknowledge, and devise a learning scheme in which the forward mapping from\npersonal memory to knowledge and its inverse mapping is included in a closed\nloop so that they could teach each other. Experiment results show that our\nmethod outperforms existing KGC methods significantly on both automatic\nevaluation and human evaluation.", "published": "2022-04-06 07:06:37", "link": "http://arxiv.org/abs/2204.02624v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Synthetic Data for Conversational Response Generation in\n  Low-resource Settings", "abstract": "Response generation is a task in natural language processing (NLP) where a\nmodel is trained to respond to human statements. Conversational response\ngenerators take this one step further with the ability to respond within the\ncontext of previous responses. While there are existing techniques for training\nsuch models, they all require an abundance of conversational data which are not\nalways available for low-resource languages. In this research, we make three\ncontributions. First, we released the first Filipino conversational dataset\ncollected from a popular Philippine online forum, which we named the PEx\nConversations Dataset. Second, we introduce a data augmentation (DA)\nmethodology for Filipino data by employing a Tagalog RoBERTa model to increase\nthe size of the existing corpora. Lastly, we published the first Filipino\nconversational response generator capable of generating responses related to\nthe previous 3 responses. With the supplementary synthetic data, we were able\nto improve the performance of the response generator by up to 12.2% in\nBERTScore, 10.7% in perplexity, and 11.7% in content word usage as compared to\ntraining with zero synthetic data.", "published": "2022-04-06 08:11:12", "link": "http://arxiv.org/abs/2204.02653v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Yunshan Cup 2020: Overview of the Part-of-Speech Tagging Task for\n  Low-resourced Languages", "abstract": "The Yunshan Cup 2020 track focused on creating a framework for evaluating\ndifferent methods of part-of-speech (POS). There were two tasks for this track:\n(1) POS tagging for the Indonesian language, and (2) POS tagging for the Lao\ntagging. The Indonesian dataset is comprised of 10000 sentences from Indonesian\nnews within 29 tags. And the Lao dataset consists of 8000 sentences within 27\ntags. 25 teams registered for the task. The methods of participants ranged from\nfeature-based to neural networks using either classical machine learning\ntechniques or ensemble methods. The best performing results achieve an accuracy\nof 95.82% for Indonesian and 93.03%, showing that neural sequence labeling\nmodels significantly outperform classic feature-based methods and rule-based\nmethods.", "published": "2022-04-06 08:16:22", "link": "http://arxiv.org/abs/2204.02658v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "drsphelps at SemEval-2022 Task 2: Learning idiom representations using\n  BERTRAM", "abstract": "This paper describes our system for SemEval-2022 Task 2 Multilingual\nIdiomaticity Detection and Sentence Embedding sub-task B. We modify a standard\nBERT sentence transformer by adding embeddings for each idioms, which are\ncreated using BERTRAM and a small number of contexts. We show that this\ntechnique increases the quality of idiom representations and leads to better\nperformance on the task. We also perform analysis on our final results and show\nthat the quality of the produced idiom embeddings is highly sensitive to the\nquality of the input contexts.", "published": "2022-04-06 13:32:37", "link": "http://arxiv.org/abs/2204.02821v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Resources and Technologies for Non-Scheduled and Endangered\n  Indian Languages", "abstract": "In the present paper, we will present a survey of the language resources and\ntechnologies available for the non-scheduled and endangered languages of India.\nWhile there have been different estimates from different sources about the\nnumber of languages in India, it could be assumed that there are more than\n1,000 languages currently being spoken in India. However barring some of the 22\nlanguages included in the 8th Schedule of the Indian Constitution (called the\nscheduled languages), there is hardly any substantial resource or technology\navailable for the rest of the languages. Nonetheless there have been some\nindividual attempts at developing resources and technologies for the different\nlanguages across the country. Of late, some financial support has also become\navailable for the endangered languages. In this paper, we give a summary of the\nresources and technologies for those Indian languages which are not included in\nthe 8th schedule of the Indian Constitution and/or which are endangered.", "published": "2022-04-06 13:33:24", "link": "http://arxiv.org/abs/2204.02822v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Paying More Attention to Self-attention: Improving Pre-trained Language\n  Models via Attention Guiding", "abstract": "Pre-trained language models (PLM) have demonstrated their effectiveness for a\nbroad range of information retrieval and natural language processing tasks. As\nthe core part of PLM, multi-head self-attention is appealing for its ability to\njointly attend to information from different positions. However, researchers\nhave found that PLM always exhibits fixed attention patterns regardless of the\ninput (e.g., excessively paying attention to [CLS] or [SEP]), which we argue\nmight neglect important information in the other positions. In this work, we\npropose a simple yet effective attention guiding mechanism to improve the\nperformance of PLM by encouraging attention towards the established goals.\nSpecifically, we propose two kinds of attention guiding methods, i.e., map\ndiscrimination guiding (MDG) and attention pattern decorrelation guiding (PDG).\nThe former definitely encourages the diversity among multiple self-attention\nheads to jointly attend to information from different representation subspaces,\nwhile the latter encourages self-attention to attend to as many different\npositions of the input as possible. We conduct experiments with multiple\ngeneral pre-trained models (i.e., BERT, ALBERT, and Roberta) and\ndomain-specific pre-trained models (i.e., BioBERT, ClinicalBERT, BlueBert, and\nSciBERT) on three benchmark datasets (i.e., MultiNLI, MedNLI, and\nCross-genre-IR). Extensive experimental results demonstrate that our proposed\nMDG and PDG bring stable performance improvements on all datasets with high\nefficiency and low cost.", "published": "2022-04-06 16:22:02", "link": "http://arxiv.org/abs/2204.02922v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inducing Positive Perspectives with Text Reframing", "abstract": "Sentiment transfer is one popular example of a text style transfer task,\nwhere the goal is to reverse the sentiment polarity of a text. With a sentiment\nreversal comes also a reversal in meaning. We introduce a different but related\ntask called positive reframing in which we neutralize a negative point of view\nand generate a more positive perspective for the author without contradicting\nthe original meaning. Our insistence on meaning preservation makes positive\nreframing a challenging and semantically rich task. To facilitate rapid\nprogress, we introduce a large-scale benchmark, Positive Psychology Frames,\nwith 8,349 sentence pairs and 12,755 structured annotations to explain positive\nreframing in terms of six theoretically-motivated reframing strategies. Then we\nevaluate a set of state-of-the-art text style transfer models, and conclude by\ndiscussing key challenges and directions for future work.", "published": "2022-04-06 17:37:52", "link": "http://arxiv.org/abs/2204.02952v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems", "abstract": "Conversational agents have come increasingly closer to human competence in\nopen-domain dialogue settings; however, such models can reflect insensitive,\nhurtful, or entirely incoherent viewpoints that erode a user's trust in the\nmoral integrity of the system. Moral deviations are difficult to mitigate\nbecause moral judgments are not universal, and there may be multiple competing\njudgments that apply to a situation simultaneously. In this work, we introduce\na new resource, not to authoritatively resolve moral ambiguities, but instead\nto facilitate systematic understanding of the intuitions, values and moral\njudgments reflected in the utterances of dialogue systems. The Moral Integrity\nCorpus, MIC, is such a resource, which captures the moral assumptions of 38k\nprompt-reply pairs, using 99k distinct Rules of Thumb (RoTs). Each RoT reflects\na particular moral conviction that can explain why a chatbot's reply may appear\nacceptable or problematic. We further organize RoTs with a set of 9 moral and\nsocial attributes and benchmark performance for attribute classification. Most\nimportantly, we show that current neural language models can automatically\ngenerate new RoTs that reasonably describe previously unseen interactions, but\nthey still struggle with certain scenarios. Our findings suggest that MIC will\nbe a useful resource for understanding and language models' implicit moral\nassumptions and flexibly benchmarking the integrity of conversational agents.\nTo download the data, see https://github.com/GT-SALT/mic", "published": "2022-04-06 18:10:53", "link": "http://arxiv.org/abs/2204.03021v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Interactive Feedback to Improve the Accuracy and Explainability of\n  Question Answering Systems Post-Deployment", "abstract": "Most research on question answering focuses on the pre-deployment stage;\ni.e., building an accurate model for deployment. In this paper, we ask the\nquestion: Can we improve QA systems further \\emph{post-}deployment based on\nuser interactions? We focus on two kinds of improvements: 1) improving the QA\nsystem's performance itself, and 2) providing the model with the ability to\nexplain the correctness or incorrectness of an answer. We collect a\nretrieval-based QA dataset, FeedbackQA, which contains interactive feedback\nfrom users. We collect this dataset by deploying a base QA system to\ncrowdworkers who then engage with the system and provide feedback on the\nquality of its answers. The feedback contains both structured ratings and\nunstructured natural language explanations. We train a neural model with this\nfeedback data that can generate explanations and re-score answer candidates. We\nshow that feedback data not only improves the accuracy of the deployed QA\nsystem but also other stronger non-deployed systems. The generated explanations\nalso help users make informed decisions about the correctness of answers.\nProject page: https://mcgill-nlp.github.io/feedbackqa/", "published": "2022-04-06 18:17:09", "link": "http://arxiv.org/abs/2204.03025v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VALUE: Understanding Dialect Disparity in NLU", "abstract": "English Natural Language Understanding (NLU) systems have achieved great\nperformances and even outperformed humans on benchmarks like GLUE and\nSuperGLUE. However, these benchmarks contain only textbook Standard American\nEnglish (SAE). Other dialects have been largely overlooked in the NLP\ncommunity. This leads to biased and inequitable NLU systems that serve only a\nsub-population of speakers. To understand disparities in current models and to\nfacilitate more dialect-competent NLU systems, we introduce the VernAcular\nLanguage Understanding Evaluation (VALUE) benchmark, a challenging variant of\nGLUE that we created with a set of lexical and morphosyntactic transformation\nrules. In this initial release (V.1), we construct rules for 11 features of\nAfrican American Vernacular English (AAVE), and we recruit fluent AAVE speakers\nto validate each feature transformation via linguistic acceptability judgments\nin a participatory design manner. Experiments show that these new dialectal\nfeatures can lead to a drop in model performance. To run the transformation\ncode and download both synthetic and gold-standard dialectal GLUE benchmarks,\nsee https://github.com/SALT-NLP/value", "published": "2022-04-06 18:30:56", "link": "http://arxiv.org/abs/2204.03031v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Abusive and Threatening Language Detection in Urdu using Supervised\n  Machine Learning and Feature Combinations", "abstract": "This paper presents the system descriptions submitted at the FIRE Shared Task\n2021 on Urdu's Abusive and Threatening Language Detection Task. This challenge\naims at automatically identifying abusive and threatening tweets written in\nUrdu. Our submitted results were selected for the third recognition at the\ncompetition. This paper reports a non-exhaustive list of experiments that\nallowed us to reach the submitted results. Moreover, after the result\ndeclaration of the competition, we managed to attain even better results than\nthe submitted results. Our models achieved 0.8318 F1 score on Task A (Abusive\nLanguage Detection for Urdu Tweets) and 0.4931 F1 score on Task B (Threatening\nLanguage Detection for Urdu Tweets). Results show that Support Vector Machines\nwith stopwords removed, lemmatization applied, and features vector created by\nthe combinations of word n-grams for n=1,2,3 produced the best results for Task\nA. For Task B, Support Vector Machines with stopwords removed, lemmatization\nnot applied, feature vector created from a pre-trained Urdu Word2Vec (on word\nunigrams and bigrams), and making the dataset balanced using oversampling\ntechnique produced the best results. The code is made available for\nreproducibility.", "published": "2022-04-06 19:57:31", "link": "http://arxiv.org/abs/2204.03062v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The 2021 Urdu Fake News Detection Task using Supervised Machine Learning\n  and Feature Combinations", "abstract": "This paper presents the system description submitted at the FIRE Shared Task:\n\"The 2021 Fake News Detection in the Urdu Language\". This challenge aims at\nautomatically identifying Fake news written in Urdu. Our submitted results\nranked fifth in the competition. However, after the result declaration of the\ncompetition, we managed to attain even better results than the submitted\nresults. The best F1 Macro score achieved by one of our models is 0.6674,\nhigher than the second-best score in the competition. The result is achieved on\nSupport Vector Machines (polynomial kernel degree 1) with stopwords removed,\nlemmatization applied, and selecting the 20K best features out of 1.557 million\nfeatures in total (which were produced by Word n-grams n=1,2,3,4 and Char\nn-grams n=2,3,4,5,6). The code is made available for reproducibility.", "published": "2022-04-06 20:00:37", "link": "http://arxiv.org/abs/2204.03064v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ByT5 model for massively multilingual grapheme-to-phoneme conversion", "abstract": "In this study, we tackle massively multilingual grapheme-to-phoneme\nconversion through implementing G2P models based on ByT5. We have curated a G2P\ndataset from various sources that covers around 100 languages and trained\nlarge-scale multilingual G2P models based on ByT5. We found that ByT5 operating\non byte-level inputs significantly outperformed the token-based mT5 model in\nterms of multilingual G2P. Pairwise comparison with monolingual models in these\nlanguages suggests that multilingual ByT5 models generally lower the phone\nerror rate by jointly learning from a variety of languages. The pretrained\nmodel can further benefit low resource G2P through zero-shot prediction on\nunseen languages or provides pretrained weights for finetuning, which helps the\nmodel converge to a lower phone error rate than randomly initialized weights.\nTo facilitate future research on multilingual G2P, we make available our code\nand pretrained multilingual G2P models at:\nhttps://github.com/lingjzhu/CharsiuG2P.", "published": "2022-04-06 20:03:38", "link": "http://arxiv.org/abs/2204.03067v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Urdu Morphology, Orthography and Lexicon Extraction", "abstract": "Urdu is a challenging language because of, first, its Perso-Arabic script and\nsecond, its morphological system having inherent grammatical forms and\nvocabulary of Arabic, Persian and the native languages of South Asia. This\npaper describes an implementation of the Urdu language as a software API, and\nwe deal with orthography, morphology and the extraction of the lexicon. The\nmorphology is implemented in a toolkit called Functional Morphology (Forsberg &\nRanta, 2004), which is based on the idea of dealing grammars as software\nlibraries. Therefore this implementation could be reused in applications such\nas intelligent search of keywords, language training and infrastructure for\nsyntax. We also present an implementation of a small part of Urdu syntax to\ndemonstrate this reusability.", "published": "2022-04-06 20:14:01", "link": "http://arxiv.org/abs/2204.03071v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Zero-Shot Event Extraction via Sentence Simplification", "abstract": "The success of sites such as ACLED and Our World in Data have demonstrated\nthe massive utility of extracting events in structured formats from large\nvolumes of textual data in the form of news, social media, blogs and discussion\nforums. Event extraction can provide a window into ongoing geopolitical crises\nand yield actionable intelligence. With the proliferation of large pretrained\nlanguage models, Machine Reading Comprehension (MRC) has emerged as a new\nparadigm for event extraction in recent times. In this approach, event argument\nextraction is framed as an extractive question-answering task. One of the key\nadvantages of the MRC-based approach is its ability to perform zero-shot\nextraction. However, the problem of long-range dependencies, i.e., large\nlexical distance between trigger and argument words and the difficulty of\nprocessing syntactically complex sentences plague MRC-based approaches. In this\npaper, we present a general approach to improve the performance of MRC-based\nevent extraction by performing unsupervised sentence simplification guided by\nthe MRC model itself. We evaluate our approach on the ICEWS geopolitical event\nextraction dataset, with specific attention to `Actor' and `Target' argument\nroles. We show how such context simplification can improve the performance of\nMRC-based event extraction by more than 5% for actor extraction and more than\n10% for target extraction.", "published": "2022-04-06 01:14:50", "link": "http://arxiv.org/abs/2204.02531v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Modeling Motion with Multi-Modal Features for Text-Based Video\n  Segmentation", "abstract": "Text-based video segmentation aims to segment the target object in a video\nbased on a describing sentence. Incorporating motion information from optical\nflow maps with appearance and linguistic modalities is crucial yet has been\nlargely ignored by previous work. In this paper, we design a method to fuse and\nalign appearance, motion, and linguistic features to achieve accurate\nsegmentation. Specifically, we propose a multi-modal video transformer, which\ncan fuse and aggregate multi-modal and temporal features between frames.\nFurthermore, we design a language-guided feature fusion module to progressively\nfuse appearance and motion features in each feature level with guidance from\nlinguistic features. Finally, a multi-modal alignment loss is proposed to\nalleviate the semantic gap between features from different modalities.\nExtensive experiments on A2D Sentences and J-HMDB Sentences verify the\nperformance and the generalization ability of our method compared to the\nstate-of-the-art methods.", "published": "2022-04-06 02:42:33", "link": "http://arxiv.org/abs/2204.02547v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Modeling Temporal-Modal Entity Graph for Procedural Multimodal Machine\n  Comprehension", "abstract": "Procedural Multimodal Documents (PMDs) organize textual instructions and\ncorresponding images step by step. Comprehending PMDs and inducing their\nrepresentations for the downstream reasoning tasks is designated as Procedural\nMultiModal Machine Comprehension (M3C). In this study, we approach Procedural\nM3C at a fine-grained level (compared with existing explorations at a document\nor sentence level), that is, entity. With delicate consideration, we model\nentity both in its temporal and cross-modal relation and propose a novel\nTemporal-Modal Entity Graph (TMEG). Specifically, graph structure is formulated\nto capture textual and visual entities and trace their temporal-modal\nevolution. In addition, a graph aggregation module is introduced to conduct\ngraph encoding and reasoning. Comprehensive experiments across three Procedural\nM3C tasks are conducted on a traditional dataset RecipeQA and our new dataset\nCraftQA, which can better evaluate the generalization of TMEG.", "published": "2022-04-06 03:41:13", "link": "http://arxiv.org/abs/2204.02566v1", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "A Weakly Supervised Propagation Model for Rumor Verification and Stance\n  Detection with Multiple Instance Learning", "abstract": "The diffusion of rumors on microblogs generally follows a propagation tree\nstructure, that provides valuable clues on how an original message is\ntransmitted and responded by users over time. Recent studies reveal that rumor\ndetection and stance detection are two different but relevant tasks which can\njointly enhance each other, e.g., rumors can be debunked by cross-checking the\nstances conveyed by their relevant microblog posts, and stances are also\nconditioned on the nature of the rumor. However, most stance detection methods\nrequire enormous post-level stance labels for training, which are\nlabor-intensive given a large number of posts. Enlightened by Multiple Instance\nLearning (MIL) scheme, we first represent the diffusion of claims with\nbottom-up and top-down trees, then propose two tree-structured weakly\nsupervised frameworks to jointly classify rumors and stances, where only the\nbag-level labels concerning claim's veracity are needed. Specifically, we\nconvert the multi-class problem into a multiple MIL-based binary classification\nproblem where each binary model focuses on differentiating a target stance or\nrumor type and other types. Finally, we propose a hierarchical attention\nmechanism to aggregate the binary predictions, including (1) a bottom-up or\ntop-down tree attention layer to aggregate binary stances into binary veracity;\nand (2) a discriminative attention layer to aggregate the binary class into\nfiner-grained classes. Extensive experiments conducted on three Twitter-based\ndatasets demonstrate promising performance of our model on both claim-level\nrumor detection and post-level stance classification compared with\nstate-of-the-art methods.", "published": "2022-04-06 07:07:06", "link": "http://arxiv.org/abs/2204.02626v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DAGAM: Data Augmentation with Generation And Modification", "abstract": "Text classification is a representative downstream task of natural language\nprocessing, and has exhibited excellent performance since the advent of\npre-trained language models based on Transformer architecture. However, in\npre-trained language models, under-fitting often occurs due to the size of the\nmodel being very large compared to the amount of available training data. Along\nwith significant importance of data collection in modern machine learning\nparadigm, studies have been actively conducted for natural language data\naugmentation. In light of this, we introduce three data augmentation schemes\nthat help reduce underfitting problems of large-scale language models.\nPrimarily we use a generation model for data augmentation, which is defined as\nData Augmentation with Generation (DAG). Next, we augment data using text\nmodification techniques such as corruption and word order change (Data\nAugmentation with Modification, DAM). Finally, we propose Data Augmentation\nwith Generation And Modification (DAGAM), which combines DAG and DAM techniques\nfor a boosted performance. We conduct data augmentation for six benchmark\ndatasets of text classification task, and verify the usefulness of DAG, DAM,\nand DAGAM through BERT-based fine-tuning and evaluation, deriving better\nresults compared to the performance with original datasets.", "published": "2022-04-06 07:20:45", "link": "http://arxiv.org/abs/2204.02633v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mix-and-Match: Scalable Dialog Response Retrieval using Gaussian Mixture\n  Embeddings", "abstract": "Embedding-based approaches for dialog response retrieval embed the\ncontext-response pairs as points in the embedding space. These approaches are\nscalable, but fail to account for the complex, many-to-many relationships that\nexist between context-response pairs. On the other end of the spectrum, there\nare approaches that feed the context-response pairs jointly through multiple\nlayers of neural networks. These approaches can model the complex relationships\nbetween context-response pairs, but fail to scale when the set of responses is\nmoderately large (>100). In this paper, we combine the best of both worlds by\nproposing a scalable model that can learn complex relationships between\ncontext-response pairs. Specifically, the model maps the contexts as well as\nresponses to probability distributions over the embedding space. We train the\nmodels by optimizing the Kullback-Leibler divergence between the distributions\ninduced by context-response pairs in the training data. We show that the\nresultant model achieves better performance as compared to other\nembedding-based approaches on publicly available conversation data.", "published": "2022-04-06 10:21:09", "link": "http://arxiv.org/abs/2204.02710v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A New Dataset for Topic-Based Paragraph Classification in\n  Genocide-Related Court Transcripts", "abstract": "Recent progress in natural language processing has been impressive in many\ndifferent areas with transformer-based approaches setting new benchmarks for a\nwide range of applications. This development has also lowered the barriers for\npeople outside the NLP community to tap into the tools and resources applied to\na variety of domain-specific applications. The bottleneck however still remains\nthe lack of annotated gold-standard collections as soon as one's research or\nprofessional interest falls outside the scope of what is readily available. One\nsuch area is genocide-related research (also including the work of experts who\nhave a professional interest in accessing, exploring and searching large-scale\ndocument collections on the topic, such as lawyers). We present GTC (Genocide\nTranscript Corpus), the first annotated corpus of genocide-related court\ntranscripts which serves three purposes: (1) to provide a first reference\ncorpus for the community, (2) to establish benchmark performances (using\nstate-of-the-art transformer-based approaches) for the new classification task\nof paragraph identification of violence-related witness statements, (3) to\nexplore first steps towards transfer learning within the domain. We consider\nour contribution to be addressing in particular this year's hot topic on\nLanguage Technology for All.", "published": "2022-04-06 10:24:19", "link": "http://arxiv.org/abs/2204.02712v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Annotation-Scheme Reconstruction for \"Fake News\" and Japanese Fake News\n  Dataset", "abstract": "Fake news provokes many societal problems; therefore, there has been\nextensive research on fake news detection tasks to counter it. Many fake news\ndatasets were constructed as resources to facilitate this task. Contemporary\nresearch focuses almost exclusively on the factuality aspect of the news.\nHowever, this aspect alone is insufficient to explain \"fake news,\" which is a\ncomplex phenomenon that involves a wide range of issues. To fully understand\nthe nature of each instance of fake news, it is important to observe it from\nvarious perspectives, such as the intention of the false news disseminator, the\nharmfulness of the news to our society, and the target of the news. We propose\na novel annotation scheme with fine-grained labeling based on detailed\ninvestigations of existing fake news datasets to capture these various aspects\nof fake news. Using the annotation scheme, we construct and publish the first\nJapanese fake news dataset. The annotation scheme is expected to provide an\nin-depth understanding of fake news. We plan to build datasets for both\nJapanese and other languages using our scheme. Our Japanese dataset is\npublished at https://hkefka385.github.io/dataset/fakenews-japanese/.", "published": "2022-04-06 10:42:39", "link": "http://arxiv.org/abs/2204.02718v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Match-Prompt: Improving Multi-task Generalization Ability for Neural\n  Text Matching via Prompt Learning", "abstract": "Text matching is a fundamental technique in both information retrieval and\nnatural language processing. Text matching tasks share the same paradigm that\ndetermines the relationship between two given texts. The relationships vary\nfrom task to task, e.g.~relevance in document retrieval, semantic alignment in\nparaphrase identification and answerable judgment in question answering.\nHowever, the essential signals for text matching remain in a finite scope,\ni.e.~exact matching, semantic matching, and inference matching. Ideally, a good\ntext matching model can learn to capture and aggregate these signals for\ndifferent matching tasks to achieve competitive performance, while recent\nstate-of-the-art text matching models, e.g.~Pre-trained Language Models (PLMs),\nare hard to generalize. It is because the end-to-end supervised learning on\ntask-specific dataset makes model overemphasize the data sample bias and\ntask-specific signals instead of the essential matching signals. To overcome\nthis problem, we adopt a specialization-generalization training strategy and\nrefer to it as Match-Prompt. In specialization stage, descriptions of different\nmatching tasks are mapped to a few prompt tokens. In generalization stage,\nmatching model explores the essential matching signals by being trained on\ndiverse matching tasks. High diverse matching tasks avoid model fitting the\ndata bias on a specific task, so that model can focus on learning the essential\nmatching signals. Meanwhile, the prompt tokens obtained in the first step help\nthe model distinguish different task-specific matching signals. Experimental\nresults on public datasets show that Match-Prompt can improve multi-task\ngeneralization capability of PLMs in text matching and yield better in-domain\nmulti-task, out-of-domain multi-task and new task adaptation performance than\nmulti-task and task-specific models trained by previous fine-tuning paradigm.", "published": "2022-04-06 11:01:08", "link": "http://arxiv.org/abs/2204.02725v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Global Readiness of Language Technology for Healthcare: What would it\n  Take to Combat the Next Pandemic?", "abstract": "The COVID-19 pandemic has brought out both the best and worst of language\ntechnology (LT). On one hand, conversational agents for information\ndissemination and basic diagnosis have seen widespread use, and arguably, had\nan important role in combating the pandemic. On the other hand, it has also\nbecome clear that such technologies are readily available for a handful of\nlanguages, and the vast majority of the global south is completely bereft of\nthese benefits. What is the state of LT, especially conversational agents, for\nhealthcare across the world's languages? And, what would it take to ensure\nglobal readiness of LT before the next pandemic? In this paper, we try to\nanswer these questions through survey of existing literature and resources, as\nwell as through a rapid chatbot building exercise for 15 Asian and African\nlanguages with varying amount of resource-availability. The study confirms the\npitiful state of LT even for languages with large speaker bases, such as\nSinhala and Hausa, and identifies the gaps that could help us prioritize\nresearch and investment strategies in LT for healthcare.", "published": "2022-04-06 13:03:28", "link": "http://arxiv.org/abs/2204.02790v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks", "abstract": "The field of Natural Language Processing has experienced a dramatic leap in\ncapabilities with the recent introduction of huge Language Models. Despite this\nsuccess, natural language problems that involve several compounded steps are\nstill practically unlearnable, even by the largest LMs. This complies with\nexperimental failures for end-to-end learning of composite problems that were\ndemonstrated in a variety of domains. An effective mitigation is to introduce\nintermediate supervision for solving sub-tasks of the compounded problem.\nRecently, several works have demonstrated high gains by taking a\nstraightforward approach for incorporating intermediate supervision in\ncompounded natural language problems: the sequence-to-sequence LM is fed with\nan augmented input, in which the decomposed tasks' labels are simply\nconcatenated to the original input. In this paper, we prove a positive learning\nresult that motivates these recent efforts. We show that when concatenating\nintermediate supervision to the input and training a sequence-to-sequence model\non this modified input, unlearnable composite problems can become learnable. We\nshow that this is true for any family of tasks which on the one hand, are\nunlearnable, and on the other hand, can be decomposed into a polynomial number\nof simple sub-tasks, each of which depends only on O(1) previous sub-task\nresults. Beyond motivating contemporary empirical efforts for incorporating\nintermediate supervision in sequence-to-sequence language models, our positive\ntheoretical result is the first of its kind in the landscape of results on the\nbenefits of intermediate supervision for neural-network learning: Until now,\nall theoretical results on the subject are negative, i.e., show cases where\nlearning is impossible without intermediate supervision, while our result is\npositive, showing that learning is facilitated in the presence of intermediate\nsupervision.", "published": "2022-04-06 15:16:27", "link": "http://arxiv.org/abs/2204.02892v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EMMT: A simultaneous eye-tracking, 4-electrode EEG and audio corpus for\n  multi-modal reading and translation scenarios", "abstract": "We present the Eyetracked Multi-Modal Translation (EMMT) corpus, a dataset\ncontaining monocular eye movement recordings, audio and 4-electrode\nelectroencephalogram (EEG) data of 43 participants. The objective was to\ncollect cognitive signals as responses of participants engaged in a number of\nlanguage intensive tasks involving different text-image stimuli settings when\ntranslating from English to Czech.\n  Each participant was exposed to 32 text-image stimuli pairs and asked to (1)\nread the English sentence, (2) translate it into Czech, (3) consult the image,\n(4) translate again, either updating or repeating the previous translation. The\ntext stimuli consisted of 200 unique sentences with 616 unique words coupled\nwith 200 unique images as the visual stimuli.\n  The recordings were collected over a two week period and all the participants\nincluded in the study were Czech natives with strong English skills. Due to the\nnature of the tasks involved in the study and the relatively large number of\nparticipants involved, the corpus is well suited for research in Translation\nProcess Studies, Cognitive Sciences among other disciplines.", "published": "2022-04-06 15:47:55", "link": "http://arxiv.org/abs/2204.02905v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Knowledge Base Index Compression via Dimensionality and Precision\n  Reduction", "abstract": "Recently neural network based approaches to knowledge-intensive NLP tasks,\nsuch as question answering, started to rely heavily on the combination of\nneural retrievers and readers. Retrieval is typically performed over a large\ntextual knowledge base (KB) which requires significant memory and compute\nresources, especially when scaled up. On HotpotQA we systematically investigate\nreducing the size of the KB index by means of dimensionality (sparse random\nprojections, PCA, autoencoders) and numerical precision reduction.\n  Our results show that PCA is an easy solution that requires very little data\nand is only slightly worse than autoencoders, which are less stable. All\nmethods are sensitive to pre- and post-processing and data should always be\ncentered and normalized both before and after dimension reduction. Finally, we\nshow that it is possible to combine PCA with using 1bit per dimension. Overall\nwe achieve (1) 100$\\times$ compression with 75%, and (2) 24$\\times$ compression\nwith 92% original retrieval performance.", "published": "2022-04-06 15:49:27", "link": "http://arxiv.org/abs/2204.02906v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based\n  Sentiment Analysis", "abstract": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis\ntask that aims to align aspects and corresponding sentiments for\naspect-specific sentiment polarity inference. It is challenging because a\nsentence may contain multiple aspects or complicated (e.g., conditional,\ncoordinating, or adversative) relations. Recently, exploiting dependency syntax\ninformation with graph neural networks has been the most popular trend. Despite\nits success, methods that heavily rely on the dependency tree pose challenges\nin accurately modeling the alignment of the aspects and their words indicative\nof sentiment, since the dependency tree may provide noisy signals of unrelated\nassociations (e.g., the \"conj\" relation between \"great\" and \"dreadful\" in\nFigure 2). In this paper, to alleviate this problem, we propose a Bi-Syntax\naware Graph Attention Network (BiSyn-GAT+). Specifically, BiSyn-GAT+ fully\nexploits the syntax information (e.g., phrase segmentation and hierarchical\nstructure) of the constituent tree of a sentence to model the sentiment-aware\ncontext of every single aspect (called intra-context) and the sentiment\nrelations across aspects (called inter-context) for learning. Experiments on\nfour benchmark datasets demonstrate that BiSyn-GAT+ outperforms the\nstate-of-the-art methods consistently.", "published": "2022-04-06 22:18:12", "link": "http://arxiv.org/abs/2204.03117v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Domain Specific Fine-tuning of Denoising Sequence-to-Sequence Models for\n  Natural Language Summarization", "abstract": "Summarization of long-form text data is a problem especially pertinent in\nknowledge economy jobs such as medicine and finance, that require continuously\nremaining informed on a sophisticated and evolving body of knowledge. As such,\nisolating and summarizing key content automatically using Natural Language\nProcessing (NLP) techniques holds the potential for extensive time savings in\nthese industries. We explore applications of a state-of-the-art NLP model\n(BART), and explore strategies for tuning it to optimal performance using data\naugmentation and various fine-tuning strategies. We show that our end-to-end\nfine-tuning approach can result in a 5-6\\% absolute ROUGE-1 improvement over an\nout-of-the-box pre-trained BART summarizer when tested on domain specific data,\nand make available our end-to-end pipeline to achieve these results on finance,\nmedical, or other user-specified domains.", "published": "2022-04-06 18:17:14", "link": "http://arxiv.org/abs/2204.09716v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Simple and Effective Unsupervised Speech Synthesis", "abstract": "We introduce the first unsupervised speech synthesis system based on a\nsimple, yet effective recipe. The framework leverages recent work in\nunsupervised speech recognition as well as existing neural-based speech\nsynthesis. Using only unlabeled speech audio and unlabeled text as well as a\nlexicon, our method enables speech synthesis without the need for a\nhuman-labeled corpus. Experiments demonstrate the unsupervised system can\nsynthesize speech similar to a supervised counterpart in terms of naturalness\nand intelligibility measured by human evaluation.", "published": "2022-04-06 00:19:13", "link": "http://arxiv.org/abs/2204.02524v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Prosodic Alignment for off-screen automatic dubbing", "abstract": "The goal of automatic dubbing is to perform speech-to-speech translation\nwhile achieving audiovisual coherence. This entails isochrony, i.e.,\ntranslating the original speech by also matching its prosodic structure into\nphrases and pauses, especially when the speaker's mouth is visible. In previous\nwork, we introduced a prosodic alignment model to address isochrone or\non-screen dubbing. In this work, we extend the prosodic alignment model to also\naddress off-screen dubbing that requires less stringent synchronization\nconstraints. We conduct experiments on four dubbing directions - English to\nFrench, Italian, German and Spanish - on a publicly available collection of TED\nTalks and on publicly available YouTube videos. Empirical results show that\ncompared to our previous work the extended prosodic alignment model provides\nsignificantly better subjective viewing experience on videos in which on-screen\nand off-screen automatic dubbing is applied for sentences with speakers mouth\nvisible and not visible, respectively.", "published": "2022-04-06 01:02:58", "link": "http://arxiv.org/abs/2204.02530v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Distributed Transition Systems with Tags for Privacy Analysis", "abstract": "We present a logical framework that formally models how a given private\ninformation P stored on a given database D, can get captured progressively, by\nan agent/adversary querying the database repeatedly. Named DLTTS (Distributed\nLabeled Tagged Transition System), the framework borrows ideas from several\ndomains: Probabilistic Automata of Segala, Probabilistic Concurrent Systems,\nand Probabilistic labelled transition systems. To every node on a DLTTS is\nattached a tag that represents the 'current' knowledge of the adversary,\nacquired from the responses of the answering mechanism of the DBMS to his/her\nqueries, at the nodes traversed earlier, along any given run; this knowledge is\ncompleted at the same node, with further relational deductions, possibly in\ncombination with 'public' information from other databases given in advance. A\n'blackbox' mechanism is also part of a DLTTS, and it is meant as an oracle; its\nrole is to tell if the private information has been deduced by the adversary at\nthe current node, and if so terminate the run. An additional special feature is\nthat the blackbox also gives information on how 'close', or how 'far', the\nknowledge of the adversary is, from the private information P , at the current\nnode. A metric is defined for that purpose, on the set of all 'type compatible'\ntuples from the given database, the data themselves being typed with the\nheaders of the base. Despite the transition systems flavor of our framework,\nthis metric is not 'behavioral' in the sense presented in some other works. It\nis exclusively database oriented, and allows to define new notions of adjacency\nand of indistinguishabilty between databases, more generally than those usually\nbased on the Hamming metric (and a restricted notion of adjacency). Examples\nare given all along to illustrate how our framework works.\n  Keywords:Database, Privacy, Transition System, Probability, Distribution.", "published": "2022-04-06 06:32:08", "link": "http://arxiv.org/abs/2204.02602v2", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "SecureBERT: A Domain-Specific Language Model for Cybersecurity", "abstract": "Natural Language Processing (NLP) has recently gained wide attention in\ncybersecurity, particularly in Cyber Threat Intelligence (CTI) and cyber\nautomation. Increased connection and automation have revolutionized the world's\neconomic and cultural infrastructures, while they have introduced risks in\nterms of cyber attacks. CTI is information that helps cybersecurity analysts\nmake intelligent security decisions, that is often delivered in the form of\nnatural language text, which must be transformed to machine readable format\nthrough an automated procedure before it can be used for automated security\nmeasures.\n  This paper proposes SecureBERT, a cybersecurity language model capable of\ncapturing text connotations in cybersecurity text (e.g., CTI) and therefore\nsuccessful in automation for many critical cybersecurity tasks that would\notherwise rely on human expertise and time-consuming manual efforts. SecureBERT\nhas been trained using a large corpus of cybersecurity text.To make SecureBERT\neffective not just in retaining general English understanding, but also when\napplied to text with cybersecurity implications, we developed a customized\ntokenizer as well as a method to alter pre-trained weights. The SecureBERT is\nevaluated using the standard Masked Language Model (MLM) test as well as two\nadditional standard NLP tasks. Our evaluation studies show that\nSecureBERT\\footnote{\\url{https://github.com/ehsanaghaei/SecureBERT}}\noutperforms existing similar models, confirming its capability for solving\ncrucial NLP tasks in cybersecurity.", "published": "2022-04-06 09:17:21", "link": "http://arxiv.org/abs/2204.02685v3", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "An Algebraic Approach to Learning and Grounding", "abstract": "We consider the problem of learning the semantics of composite algebraic\nexpressions from examples. The outcome is a versatile framework for studying\nlearning tasks that can be put into the following abstract form: The input is a\npartial algebra $\\alg$ and a finite set of examples $(\\varphi_1, O_1),\n(\\varphi_2, O_2), \\ldots$, each consisting of an algebraic term $\\varphi_i$ and\na set of objects~$O_i$. The objective is to simultaneously fill in the missing\nalgebraic operations in $\\alg$ and ground the variables of every $\\varphi_i$ in\n$O_i$, so that the combined value of the terms is optimised. We demonstrate the\napplicability of this framework through case studies in grammatical inference,\npicture-language learning, and the grounding of logic scene descriptions.", "published": "2022-04-06 13:29:11", "link": "http://arxiv.org/abs/2204.02813v2", "categories": ["cs.CL", "cs.FL", "cs.LO", "I.2.4; I.2.7; I.2.10; I.1.3; I.2"], "primary_category": "cs.CL"}
{"title": "Aggression in Hindi and English Speech: Acoustic Correlates and\n  Automatic Identification", "abstract": "In the present paper, we will present the results of an acoustic analysis of\npolitical discourse in Hindi and discuss some of the conventionalised acoustic\nfeatures of aggressive speech regularly employed by the speakers of Hindi and\nEnglish. The study is based on a corpus of slightly over 10 hours of political\ndiscourse and includes debates on news channel and political speeches. Using\nthis study, we develop two automatic classification systems for identifying\naggression in English and Hindi speech, based solely on an acoustic model. The\nHindi classifier, trained using 50 hours of annotated speech, and English\nclassifier, trained using 40 hours of annotated speech, achieve a respectable\naccuracy of over 73% and 66% respectively. In this paper, we discuss the\ndevelopment of this annotated dataset, the experiments for developing the\nclassifier and discuss the errors that it makes.", "published": "2022-04-06 13:29:25", "link": "http://arxiv.org/abs/2204.02814v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Question Generation for Reading Comprehension Assessment by Modeling How\n  and What to Ask", "abstract": "Reading is integral to everyday life, and yet learning to read is a struggle\nfor many young learners. During lessons, teachers can use comprehension\nquestions to increase engagement, test reading skills, and improve retention.\nHistorically such questions were written by skilled teachers, but recently\nlanguage models have been used to generate comprehension questions. However,\nmany existing Question Generation (QG) systems focus on generating literal\nquestions from the text, and have no way to control the type of the generated\nquestion. In this paper, we study QG for reading comprehension where\ninferential questions are critical and extractive techniques cannot be used. We\npropose a two-step model (HTA-WTA) that takes advantage of previous datasets,\nand can generate questions for a specific targeted comprehension skill. We\npropose a new reading comprehension dataset that contains questions annotated\nwith story-based reading comprehension skills (SBRCS), allowing for a more\ncomplete reader assessment. Across several experiments, our results show that\nHTA-WTA outperforms multiple strong baselines on this new dataset. We show that\nthe HTA-WTA model tests for strong SCRS by asking deep inferential questions.", "published": "2022-04-06 15:52:24", "link": "http://arxiv.org/abs/2204.02908v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhanced Direct Speech-to-Speech Translation Using Self-supervised\n  Pre-training and Data Augmentation", "abstract": "Direct speech-to-speech translation (S2ST) models suffer from data scarcity\nissues as there exists little parallel S2ST data, compared to the amount of\ndata available for conventional cascaded systems that consist of automatic\nspeech recognition (ASR), machine translation (MT), and text-to-speech (TTS)\nsynthesis. In this work, we explore self-supervised pre-training with unlabeled\nspeech data and data augmentation to tackle this issue. We take advantage of a\nrecently proposed speech-to-unit translation (S2UT) framework that encodes\ntarget speech into discrete representations, and transfer pre-training and\nefficient partial finetuning techniques that work well for speech-to-text\ntranslation (S2T) to the S2UT domain by studying both speech encoder and\ndiscrete unit decoder pre-training. Our experiments on Spanish-English\ntranslation show that self-supervised pre-training consistently improves model\nperformance compared with multitask learning with an average 6.6-12.1 BLEU\ngain, and it can be further combined with data augmentation techniques that\napply MT to create weakly supervised training data. Audio samples are available\nat:\nhttps://facebookresearch.github.io/speech_translation/enhanced_direct_s2st_units/index.html .", "published": "2022-04-06 17:59:22", "link": "http://arxiv.org/abs/2204.02967v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Hierarchical Annotation for Building A Suite of Clinical Natural\n  Language Processing Tasks: Progress Note Understanding", "abstract": "Applying methods in natural language processing on electronic health records\n(EHR) data is a growing field. Existing corpus and annotation focus on modeling\ntextual features and relation prediction. However, there is a paucity of\nannotated corpus built to model clinical diagnostic thinking, a process\ninvolving text understanding, domain knowledge abstraction and reasoning. This\nwork introduces a hierarchical annotation schema with three stages to address\nclinical text understanding, clinical reasoning, and summarization. We created\nan annotated corpus based on an extensive collection of publicly available\ndaily progress notes, a type of EHR documentation that is collected in time\nseries in a problem-oriented format. The conventional format for a progress\nnote follows a Subjective, Objective, Assessment and Plan heading (SOAP). We\nalso define a new suite of tasks, Progress Note Understanding, with three tasks\nutilizing the three annotation stages. The novel suite of tasks was designed to\ntrain and evaluate future NLP models for clinical text understanding, clinical\nknowledge representation, inference, and summarization.", "published": "2022-04-06 18:38:08", "link": "http://arxiv.org/abs/2204.03035v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "SOMOS: The Samsung Open MOS Dataset for the Evaluation of Neural\n  Text-to-Speech Synthesis", "abstract": "In this work, we present the SOMOS dataset, the first large-scale mean\nopinion scores (MOS) dataset consisting of solely neural text-to-speech (TTS)\nsamples. It can be employed to train automatic MOS prediction systems focused\non the assessment of modern synthesizers, and can stimulate advancements in\nacoustic model evaluation. It consists of 20K synthetic utterances of the LJ\nSpeech voice, a public domain speech dataset which is a common benchmark for\nbuilding neural acoustic models and vocoders. Utterances are generated from 200\nTTS systems including vanilla neural acoustic models as well as models which\nallow prosodic variations. An LPCNet vocoder is used for all systems, so that\nthe samples' variation depends only on the acoustic models. The synthesized\nutterances provide balanced and adequate domain and length coverage. We collect\nMOS naturalness evaluations on 3 English Amazon Mechanical Turk locales and\nshare practices leading to reliable crowdsourced annotations for this task. We\nprovide baseline results of state-of-the-art MOS prediction models on the SOMOS\ndataset and show the limitations that such models face when assigned to\nevaluate TTS utterances.", "published": "2022-04-06 18:45:20", "link": "http://arxiv.org/abs/2204.03040v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fusing finetuned models for better pretraining", "abstract": "Pretrained models are the standard starting point for training. This approach\nconsistently outperforms the use of a random initialization. However,\npretraining is a costly endeavour that few can undertake.\n  In this paper, we create better base models at hardly any cost, by fusing\nmultiple existing fine tuned models into one. Specifically, we fuse by\naveraging the weights of these models. We show that the fused model results\nsurpass the pretrained model ones. We also show that fusing is often better\nthan intertraining.\n  We find that fusing is less dependent on the target task. Furthermore, weight\ndecay nullifies intertraining effects but not those of fusing.", "published": "2022-04-06 18:54:48", "link": "http://arxiv.org/abs/2204.03044v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge Infused Decoding", "abstract": "Pre-trained language models (LMs) have been shown to memorize a substantial\namount of knowledge from the pre-training corpora; however, they are still\nlimited in recalling factually correct knowledge given a certain context.\nHence, they tend to suffer from counterfactual or hallucinatory generation when\nused in knowledge-intensive natural language generation (NLG) tasks. Recent\nremedies to this problem focus on modifying either the pre-training or task\nfine-tuning objectives to incorporate knowledge, which normally require\nadditional costly training or architecture modification of LMs for practical\napplications. We present Knowledge Infused Decoding (KID) -- a novel decoding\nalgorithm for generative LMs, which dynamically infuses external knowledge into\neach step of the LM decoding. Specifically, we maintain a local knowledge\nmemory based on the current context, interacting with a dynamically created\nexternal knowledge trie, and continuously update the local memory as a\nknowledge-aware constraint to guide decoding via reinforcement learning. On six\ndiverse knowledge-intensive NLG tasks, task-agnostic LMs (e.g., GPT-2 and BART)\narmed with KID outperform many task-optimized state-of-the-art models, and show\nparticularly strong performance in few-shot scenarios over seven related\nknowledge-infusion techniques. Human evaluation confirms KID's ability to\ngenerate more relevant and factual language for the input context when compared\nwith multiple baselines. Finally, KID also alleviates exposure bias and\nprovides stable generation quality when generating longer sequences. Code for\nKID is available at https://github.com/microsoft/KID.", "published": "2022-04-06 20:58:32", "link": "http://arxiv.org/abs/2204.03084v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The TalkMoves Dataset: K-12 Mathematics Lesson Transcripts Annotated for\n  Teacher and Student Discursive Moves", "abstract": "Transcripts of teaching episodes can be effective tools to understand\ndiscourse patterns in classroom instruction. According to most educational\nexperts, sustained classroom discourse is a critical component of equitable,\nengaging, and rich learning environments for students. This paper describes the\nTalkMoves dataset, composed of 567 human-annotated K-12 mathematics lesson\ntranscripts (including entire lessons or portions of lessons) derived from\nvideo recordings. The set of transcripts primarily includes in-person lessons\nwith whole-class discussions and/or small group work, as well as some online\nlessons. All of the transcripts are human-transcribed, segmented by the speaker\n(teacher or student), and annotated at the sentence level for ten discursive\nmoves based on accountable talk theory. In addition, the transcripts include\nutterance-level information in the form of dialogue act labels based on the\nSwitchboard Dialog Act Corpus. The dataset can be used by educators,\npolicymakers, and researchers to understand the nature of teacher and student\ndiscourse in K-12 math classrooms. Portions of this dataset have been used to\ndevelop the TalkMoves application, which provides teachers with automated,\nimmediate, and actionable feedback about their mathematics instruction.", "published": "2022-04-06 18:12:30", "link": "http://arxiv.org/abs/2204.09652v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KNN-Diffusion: Image Generation via Large-Scale Retrieval", "abstract": "Recent text-to-image models have achieved impressive results. However, since\nthey require large-scale datasets of text-image pairs, it is impractical to\ntrain them on new domains where data is scarce or not labeled. In this work, we\npropose using large-scale retrieval methods, in particular, efficient\nk-Nearest-Neighbors (kNN), which offers novel capabilities: (1) training a\nsubstantially small and efficient text-to-image diffusion model without any\ntext, (2) generating out-of-distribution images by simply swapping the\nretrieval database at inference time, and (3) performing text-driven local\nsemantic manipulations while preserving object identity. To demonstrate the\nrobustness of our method, we apply our kNN approach on two state-of-the-art\ndiffusion backbones, and show results on several different datasets. As\nevaluated by human studies and automatic metrics, our method achieves\nstate-of-the-art results compared to existing approaches that train\ntext-to-image generation models using images only (without paired text data)", "published": "2022-04-06 14:13:35", "link": "http://arxiv.org/abs/2204.02849v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.GR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound", "abstract": "We introduce an audiovisual method for long-range text-to-video retrieval.\nUnlike previous approaches designed for short video retrieval (e.g., 5-15\nseconds in duration), our approach aims to retrieve minute-long videos that\ncapture complex human actions. One challenge of standard video-only approaches\nis the large computational cost associated with processing hundreds of densely\nextracted frames from such long videos. To address this issue, we propose to\nreplace parts of the video with compact audio cues that succinctly summarize\ndynamic audio events and are cheap to process. Our method, named ECLIPSE\n(Efficient CLIP with Sound Encoding), adapts the popular CLIP model to an\naudiovisual video setting, by adding a unified audiovisual transformer block\nthat captures complementary cues from the video and audio streams. In addition\nto being 2.92x faster and 2.34x memory-efficient than long-range video-only\napproaches, our method also achieves better text-to-video retrieval accuracy on\nseveral diverse long-range video datasets such as ActivityNet, QVHighlights,\nYouCook2, DiDeMo and Charades.", "published": "2022-04-06 14:43:42", "link": "http://arxiv.org/abs/2204.02874v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Emotional Speech Recognition with Pre-trained Deep Visual Models", "abstract": "In this paper, we propose a new methodology for emotional speech recognition\nusing visual deep neural network models. We employ the transfer learning\ncapabilities of the pre-trained computer vision deep models to have a mandate\nfor the emotion recognition in speech task. In order to achieve that, we\npropose to use a composite set of acoustic features and a procedure to convert\nthem into images. Besides, we present a training paradigm for these models\ntaking into consideration the different characteristics between acoustic-based\nimages and regular ones. In our experiments, we use the pre-trained VGG-16\nmodel and test the overall methodology on the Berlin EMO-DB dataset for\nspeaker-independent emotion recognition. We evaluate the proposed model on the\nfull list of the seven emotions and the results set a new state-of-the-art.", "published": "2022-04-06 11:27:59", "link": "http://arxiv.org/abs/2204.03561v1", "categories": ["cs.CV", "cs.CL", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Representation Selective Self-distillation and wav2vec 2.0 Feature\n  Exploration for Spoof-aware Speaker Verification", "abstract": "Text-to-speech and voice conversion studies are constantly improving to the\nextent where they can produce synthetic speech almost indistinguishable from\nbona fide human speech. In this regard, the importance of countermeasures (CM)\nagainst synthetic voice attacks of the automatic speaker verification (ASV)\nsystems emerges. Nonetheless, most end-to-end spoofing detection networks are\nblack-box systems, and the answer to what is an effective representation for\nfinding artifacts remains veiled. In this paper, we examine which feature space\ncan effectively represent synthetic artifacts using wav2vec 2.0, and study\nwhich architecture can effectively utilize the space. Our study allows us to\nanalyze which attribute of speech signals is advantageous for the CM systems.\nThe proposed CM system achieved 0.31% equal error rate (EER) on ASVspoof 2019\nLA evaluation set for the spoof detection task. We further propose a simple yet\neffective spoofing aware speaker verification (SASV) method, which takes\nadvantage of the disentangled representations from our countermeasure system.\nEvaluation performed with the SASV Challenge 2022 database show 1.08% of SASV\nEER. Quantitative analysis shows that using the explored feature space of\nwav2vec 2.0 advantages both spoofing CM and SASV.", "published": "2022-04-06 07:47:36", "link": "http://arxiv.org/abs/2204.02639v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A New Nonlinear speaker parameterization algorithm for speaker\n  identification", "abstract": "In this paper we propose a new parameterization algorithm based on nonlinear\nprediction, which is an extension of the classical LPC parameters. The\nparameters performances are estimated by two different methods: the\nArithmetic-Harmonic Sphericity (AHS) and the Auto-Regressive Vector Model\n(ARVM). Two different methods are proposed for the parameterization based on\nthe Neural Predictive Coding (NPC): classical neural networks initialization\nand linear initialization. We applied these two parameters to speaker\nidentification. The fist parameters obtained smaller rates. We show for the\nfirst parameters how they can be combined with the classical parameters (LPCC,\nMFCC, etc.) in order to improve the results of only one classical\nparameterization (MFCC provides 97.55% and MFCC+NPC 98.78%). For the linear\ninitialization, we obtain 100% which is great improvement. This study opens a\nnew way towards different parameterization schemes that offer better accuracy\non speaker recognition tasks.", "published": "2022-04-06 06:37:08", "link": "http://arxiv.org/abs/2204.02609v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Global HRTF Interpolation via Learned Affine Transformation of\n  Hyper-conditioned Features", "abstract": "Estimating Head-Related Transfer Functions (HRTFs) of arbitrary source points\nis essential in immersive binaural audio rendering. Computing each individual's\nHRTFs is challenging, as traditional approaches require expensive time and\ncomputational resources, while modern data-driven approaches are data-hungry.\nEspecially for the data-driven approaches, existing HRTF datasets differ in\nspatial sampling distributions of source positions, posing a major problem when\ngeneralizing the method across multiple datasets. To alleviate this, we propose\na deep learning method based on a novel conditioning architecture. The proposed\nmethod can predict an HRTF of any position by interpolating the HRTFs of known\ndistributions. Experimental results show that the proposed architecture\nimproves the model's generalizability across datasets with various coordinate\nsystems. Additional demonstrations show that the model robustly reconstructs\nthe target HRTFs from the spatially downsampled HRTFs in both quantitative and\nperceptual measures.", "published": "2022-04-06 07:42:15", "link": "http://arxiv.org/abs/2204.02637v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Multi-Scale Speaking Style Modelling with Hierarchical Context\n  Information for Mandarin Speech Synthesis", "abstract": "Previous works on expressive speech synthesis focus on modelling the\nmono-scale style embedding from the current sentence or context, but the\nmulti-scale nature of speaking style in human speech is neglected. In this\npaper, we propose a multi-scale speaking style modelling method to capture and\npredict multi-scale speaking style for improving the naturalness and\nexpressiveness of synthetic speech. A multi-scale extractor is proposed to\nextract speaking style embeddings at three different levels from the\nground-truth speech, and explicitly guide the training of a multi-scale style\npredictor based on hierarchical context information. Both objective and\nsubjective evaluations on a Mandarin audiobooks dataset demonstrate that our\nproposed method can significantly improve the naturalness and expressiveness of\nthe synthesized speech.", "published": "2022-04-06 11:38:23", "link": "http://arxiv.org/abs/2204.02743v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spectral Denoising for Microphone Classification", "abstract": "In this paper, we propose the use of denoising for microphone classification,\nto enable its usage for several key application domains that involve noisy\nconditions. We describe the proposed analysis pipeline and the baseline\nalgorithm for microphone classification, and discuss various denoising\napproaches which can be applied to it within the time or spectral domain;\nfinally, we determine the best-performing denoising procedure, and evaluate the\nperformance of the overall, integrated approach with several SNR levels of\nadditive input noise. As a result, the proposed method achieves an average\naccuracy increase of about 25% on denoised content over the reference baseline.", "published": "2022-04-06 14:06:04", "link": "http://arxiv.org/abs/2204.02841v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Wav2vec2-Based Experimental Study on Self-Supervised Learning Methods\n  to Improve Child Speech Recognition", "abstract": "Despite recent advancements in deep learning technologies, Child Speech\nRecognition remains a challenging task. Current Automatic Speech Recognition\n(ASR) models require substantial amounts of annotated data for training, which\nis scarce. In this work, we explore using the ASR model, wav2vec2, with\ndifferent pretraining and finetuning configurations for self-supervised\nlearning (SSL) toward improving automatic child speech recognition. The\npretrained wav2vec2 models were finetuned using different amounts of child\nspeech training data, adult speech data, and a combination of both, to discover\nthe optimum amount of data required to finetune the model for the task of child\nASR. Our trained model achieves the best Word Error Rate (WER) of 7.42 on the\nMyST child speech dataset, 2.99 on the PFSTAR dataset and 12.47 on the CMU KIDS\ndataset as compared to any other previous methods. Our models outperformed the\nwav2vec2 BASE 960 on child speech which is considered a state-of-the-art ASR\nmodel on adult speech by just using 10 hours of child speech data in\nfinetuning. The analysis of different types of training data and their effect\non inference is also provided by using a combination of datasets in\npretraining, finetuning and inference.", "published": "2022-04-06 16:00:31", "link": "http://arxiv.org/abs/2204.05419v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Customizable End-to-end Optimization of Online Neural Network-supported\n  Dereverberation for Hearing Devices", "abstract": "This work focuses on online dereverberation for hearing devices using the\nweighted prediction error (WPE) algorithm. WPE filtering requires an estimate\nof the target speech power spectral density (PSD). Recently deep neural\nnetworks (DNNs) have been used for this task. However, these approaches\noptimize the PSD estimate which only indirectly affects the WPE output, thus\npotentially resulting in limited dereverberation. In this paper, we propose an\nend-to-end approach specialized for online processing, that directly optimizes\nthe dereverberated output signal. In addition, we propose to adapt it to the\nneeds of different types of hearing-device users by modifying the optimization\ntarget as well as the WPE algorithm characteristics used in training. We show\nthat the proposed end-to-end approach outperforms the traditional and\nconventional DNN-supported WPEs on a noise-free version of the WHAMR! dataset.", "published": "2022-04-06 09:43:29", "link": "http://arxiv.org/abs/2204.02694v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Neural Network-augmented Kalman Filtering for Robust Online Speech\n  Dereverberation in Noisy Reverberant Environments", "abstract": "In this paper, a neural network-augmented algorithm for noise-robust online\ndereverberation with a Kalman filtering variant of the weighted prediction\nerror (WPE) method is proposed. The filter stochastic variations are predicted\nby a deep neural network (DNN) trained end-to-end using the filter residual\nerror and signal characteristics. The presented framework allows for robust\ndereverberation on a single-channel noisy reverberant dataset similar to\nWHAMR!. The Kalman filtering WPE introduces distortions in the enhanced signal\nwhen predicting the filter variations from the residual error only, if the\ntarget speech power spectral density is not perfectly known and the observation\nis noisy. The proposed approach avoids these distortions by correcting the\nfilter variations estimation in a data-driven way, increasing the robustness of\nthe method to noisy scenarios. Furthermore, it yields a strong dereverberation\nand denoising performance compared to a DNN-supported recursive least squares\nvariant of WPE, especially for highly noisy inputs.", "published": "2022-04-06 11:38:04", "link": "http://arxiv.org/abs/2204.02741v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Federated Self-supervised Speech Representations: Are We There Yet?", "abstract": "The ubiquity of microphone-enabled devices has lead to large amounts of\nunlabelled audio data being produced at the edge. The integration of\nself-supervised learning (SSL) and federated learning (FL) into one coherent\nsystem can potentially offer data privacy guarantees while also advancing the\nquality and robustness of speech representations. In this paper, we provide a\nfirst-of-its-kind systematic study of the feasibility and complexities for\ntraining speech SSL models under FL scenarios from the perspective of\nalgorithms, hardware, and systems limits. Despite the high potential of their\ncombination, we find existing system constraints and algorithmic behaviour make\nSSL and FL systems nearly impossible to build today. Yet critically, our\nresults indicate specific performance bottlenecks and research opportunities\nthat would allow this situation to be reversed. While our analysis suggests\nthat, given existing trends in hardware, hybrid SSL and FL speech systems will\nnot be viable until 2027. We believe this study can act as a roadmap to\naccelerate work towards reaching this milestone much earlier.", "published": "2022-04-06 13:14:15", "link": "http://arxiv.org/abs/2204.02804v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Expression-preserving face frontalization improves visually assisted\n  speech processing", "abstract": "Face frontalization consists of synthesizing a frontally-viewed face from an\narbitrarily-viewed one. The main contribution of this paper is a frontalization\nmethodology that preserves non-rigid facial deformations in order to boost the\nperformance of visually assisted speech communication. The method alternates\nbetween the estimation of (i)~the rigid transformation (scale, rotation, and\ntranslation) and (ii)~the non-rigid deformation between an arbitrarily-viewed\nface and a face model. The method has two important merits: it can deal with\nnon-Gaussian errors in the data and it incorporates a dynamical face\ndeformation model. For that purpose, we use the generalized Student\nt-distribution in combination with a linear dynamic system in order to account\nfor both rigid head motions and time-varying facial deformations caused by\nspeech production. We propose to use the zero-mean normalized cross-correlation\n(ZNCC) score to evaluate the ability of the method to preserve facial\nexpressions. The method is thoroughly evaluated and compared with several state\nof the art methods, either based on traditional geometric models or on deep\nlearning. Moreover, we show that the method, when incorporated into deep\nlearning pipelines, namely lip reading and speech enhancement, improves word\nrecognition and speech intelligibilty scores by a considerable margin.\nSupplemental material is accessible at\nhttps://team.inria.fr/robotlearn/research/facefrontalization/", "published": "2022-04-06 13:22:24", "link": "http://arxiv.org/abs/2204.02810v4", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "A neural network-supported two-stage algorithm for lightweight\n  dereverberation on hearing devices", "abstract": "A two-stage lightweight online dereverberation algorithm for hearing devices\nis presented in this paper. The approach combines a multi-channel multi-frame\nlinear filter with a single-channel single-frame post-filter. Both components\nrely on power spectral density (PSD) estimates provided by deep neural networks\n(DNNs). By deriving new metrics analyzing the dereverberation performance in\nvarious time ranges, we confirm that directly optimizing for a criterion at the\noutput of the multi-channel linear filtering stage results in a more efficient\ndereverberation as compared to placing the criterion at the output of the DNN\nto optimize the PSD estimation. More concretely, we show that training this\nstage end-to-end helps further remove the reverberation in the range accessible\nto the filter, thus increasing the \\textit{early-to-moderate} reverberation\nratio. We argue and demonstrate that it can then be well combined with a\npost-filtering stage to efficiently suppress the residual late reverberation,\nthereby increasing the \\textit{early-to-final} reverberation ratio. This\nproposed two stage procedure is shown to be both very effective in terms of\ndereverberation performance and computational demands, as compared to e.g.\nrecent state-of-the-art DNN approaches. Furthermore, the proposed two-stage\nsystem can be adapted to the needs of different types of hearing-device users\nby controlling the amount of reduction of early reflections.", "published": "2022-04-06 11:08:28", "link": "http://arxiv.org/abs/2204.02978v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FFC-SE: Fast Fourier Convolution for Speech Enhancement", "abstract": "Fast Fourier convolution (FFC) is the recently proposed neural operator\nshowing promising performance in several computer vision problems. The FFC\noperator allows employing large receptive field operations within early layers\nof the neural network. It was shown to be especially helpful for inpainting of\nperiodic structures which are common in audio processing. In this work, we\ndesign neural network architectures which adapt FFC for speech enhancement. We\nhypothesize that a large receptive field allows these networks to produce more\ncoherent phases than vanilla convolutional models, and validate this hypothesis\nexperimentally. We found that neural networks based on Fast Fourier convolution\noutperform analogous convolutional models and show better or comparable results\nwith other speech enhancement baselines.", "published": "2022-04-06 18:52:47", "link": "http://arxiv.org/abs/2204.03042v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-Visual Person-of-Interest DeepFake Detection", "abstract": "Face manipulation technology is advancing very rapidly, and new methods are\nbeing proposed day by day. The aim of this work is to propose a deepfake\ndetector that can cope with the wide variety of manipulation methods and\nscenarios encountered in the real world. Our key insight is that each person\nhas specific characteristics that a synthetic generator likely cannot\nreproduce. Accordingly, we extract audio-visual features which characterize the\nidentity of a person, and use them to create a person-of-interest (POI)\ndeepfake detector. We leverage a contrastive learning paradigm to learn the\nmoving-face and audio segment embeddings that are most discriminative for each\nidentity. As a result, when the video and/or audio of a person is manipulated,\nits representation in the embedding space becomes inconsistent with the real\nidentity, allowing reliable detection. Training is carried out exclusively on\nreal talking-face video; thus, the detector does not depend on any specific\nmanipulation method and yields the highest generalization ability. In addition,\nour method can detect both single-modality (audio-only, video-only) and\nmulti-modality (audio-video) attacks, and is robust to low-quality or corrupted\nvideos. Experiments on a wide variety of datasets confirm that our method\nensures a SOTA performance, especially on low quality videos. Code is publicly\navailable on-line at https://github.com/grip-unina/poi-forensics.", "published": "2022-04-06 20:51:40", "link": "http://arxiv.org/abs/2204.03083v3", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Successes and critical failures of neural networks in capturing\n  human-like speech recognition", "abstract": "Natural and artificial audition can in principle acquire different solutions\nto a given problem. The constraints of the task, however, can nudge the\ncognitive science and engineering of audition to qualitatively converge,\nsuggesting that a closer mutual examination would potentially enrich artificial\nhearing systems and process models of the mind and brain. Speech recognition -\nan area ripe for such exploration - is inherently robust in humans to a number\ntransformations at various spectrotemporal granularities. To what extent are\nthese robustness profiles accounted for by high-performing neural network\nsystems? We bring together experiments in speech recognition under a single\nsynthesis framework to evaluate state-of-the-art neural networks as\nstimulus-computable, optimized observers. In a series of experiments, we (1)\nclarify how influential speech manipulations in the literature relate to each\nother and to natural speech, (2) show the granularities at which machines\nexhibit out-of-distribution robustness, reproducing classical perceptual\nphenomena in humans, (3) identify the specific conditions where model\npredictions of human performance differ, and (4) demonstrate a crucial failure\nof all artificial systems to perceptually recover where humans do, suggesting\nalternative directions for theory and model building. These findings encourage\na tighter synergy between the cognitive science and engineering of audition.", "published": "2022-04-06 06:35:10", "link": "http://arxiv.org/abs/2204.03740v4", "categories": ["cs.SD", "cs.AI", "eess.AS", "q-bio.NC"], "primary_category": "cs.SD"}
{"title": "Late multimodal fusion for image and audio music transcription", "abstract": "Music transcription, which deals with the conversion of music sources into a\nstructured digital format, is a key problem for Music Information Retrieval\n(MIR). When addressing this challenge in computational terms, the MIR community\nfollows two lines of research: music documents, which is the case of Optical\nMusic Recognition (OMR), or audio recordings, which is the case of Automatic\nMusic Transcription (AMT). The different nature of the aforementioned input\ndata has conditioned these fields to develop modality-specific frameworks.\nHowever, their recent definition in terms of sequence labeling tasks leads to a\ncommon output representation, which enables research on a combined paradigm. In\nthis respect, multimodal image and audio music transcription comprises the\nchallenge of effectively combining the information conveyed by image and audio\nmodalities. In this work, we explore this question at a late-fusion level: we\nstudy four combination approaches in order to merge, for the first time, the\nhypotheses regarding end-to-end OMR and AMT systems in a lattice-based search\nspace. The results obtained for a series of performance scenarios -- in which\nthe corresponding single-modality models yield different error rates -- showed\ninteresting benefits of these approaches. In addition, two of the four\nstrategies considered significantly improve the corresponding unimodal standard\nrecognition frameworks.", "published": "2022-04-06 20:00:33", "link": "http://arxiv.org/abs/2204.03063v3", "categories": ["cs.MM", "cs.CV", "cs.IR", "cs.SD", "eess.AS", "H.3; H.4; I.4; I.5; J.6"], "primary_category": "cs.MM"}
