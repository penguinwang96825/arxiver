{"title": "Template-based Approach to Zero-shot Intent Recognition", "abstract": "The recent advances in transfer learning techniques and pre-training of large\ncontextualized encoders foster innovation in real-life applications, including\ndialog assistants. Practical needs of intent recognition require effective data\nusage and the ability to constantly update supported intents, adopting new\nones, and abandoning outdated ones. In particular, the generalized zero-shot\nparadigm, in which the model is trained on the seen intents and tested on both\nseen and unseen intents, is taking on new importance. In this paper, we explore\nthe generalized zero-shot setup for intent recognition. Following best\npractices for zero-shot text classification, we treat the task with a sentence\npair modeling approach. We outperform previous state-of-the-art f1-measure by\nup to 16\\% for unseen intents, using intent labels and user utterances and\nwithout accessing external sources (such as knowledge bases). Further\nenhancement includes lexicalization of intent labels, which improves\nperformance by up to 7\\%. By using task transferring from other sentence pair\ntasks, such as Natural Language Inference, we gain additional improvements.", "published": "2022-06-22 08:44:59", "link": "http://arxiv.org/abs/2206.10914v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparing Formulaic Language in Human and Machine Translation: Insight\n  from a Parliamentary Corpus", "abstract": "A recent study has shown that, compared to human translations, neural machine\ntranslations contain more strongly-associated formulaic sequences made of\nrelatively high-frequency words, but far less strongly-associated formulaic\nsequences made of relatively rare words. These results were obtained on the\nbasis of translations of quality newspaper articles in which human translations\ncan be thought to be not very literal. The present study attempts to replicate\nthis research using a parliamentary corpus. The text were translated from\nFrench to English by three well-known neural machine translation systems:\nDeepL, Google Translate and Microsoft Translator. The results confirm the\nobservations on the news corpus, but the differences are less strong. They\nsuggest that the use of text genres that usually result in more literal\ntranslations, such as parliamentary corpora, might be preferable when comparing\nhuman and machine translations. Regarding the differences between the three\nneural machine systems, it appears that Google translations contain fewer\nhighly collocational bigrams, identified by the CollGram technique, than Deepl\nand Microsoft translations.", "published": "2022-06-22 08:59:10", "link": "http://arxiv.org/abs/2206.10919v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Context Tagging for Utterance Rewriting", "abstract": "Utterance rewriting aims to recover coreferences and omitted information from\nthe latest turn of a multi-turn dialogue. Recently, methods that tag rather\nthan linearly generate sequences have proven stronger in both in- and\nout-of-domain rewriting settings. This is due to a tagger's smaller search\nspace as it can only copy tokens from the dialogue context. However, these\nmethods may suffer from low coverage when phrases that must be added to a\nsource utterance cannot be covered by a single context span. This can occur in\nlanguages like English that introduce tokens such as prepositions into the\nrewrite for grammaticality. We propose a hierarchical context tagger (HCT) that\nmitigates this issue by predicting slotted rules (e.g., \"besides_\") whose slots\nare later filled with context spans. HCT (i) tags the source string with\ntoken-level edit actions and slotted rules and (ii) fills in the resulting rule\nslots with spans from the dialogue context. This rule tagging allows HCT to add\nout-of-context tokens and multiple spans at once; we further cluster the rules\nto truncate the long tail of the rule distribution. Experiments on several\nbenchmarks show that HCT can outperform state-of-the-art rewriting systems by\n~2 BLEU points.", "published": "2022-06-22 17:09:34", "link": "http://arxiv.org/abs/2206.11218v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding the Properties of Generated Corpora", "abstract": "Models for text generation have become focal for many research tasks and\nespecially for the generation of sentence corpora. However, understanding the\nproperties of an automatically generated text corpus remains challenging. We\npropose a set of tools that examine the properties of generated text corpora.\nApplying these tools on various generated corpora allowed us to gain new\ninsights into the properties of the generative models. As part of our\ncharacterization process, we found remarkable differences in the corpora\ngenerated by two leading generative technologies.", "published": "2022-06-22 17:13:52", "link": "http://arxiv.org/abs/2206.11219v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GODEL: Large-Scale Pre-Training for Goal-Directed Dialog", "abstract": "We introduce GODEL (Grounded Open Dialogue Language Model), a large\npre-trained language model for dialog. In contrast with earlier models such as\nDialoGPT, GODEL leverages a new phase of grounded pre-training designed to\nbetter support adapting GODEL to a wide range of downstream dialog tasks that\nrequire information external to the current conversation (e.g., a database or\ndocument) to produce good responses. Experiments against an array of benchmarks\nthat encompass task-oriented dialog, conversational QA, and grounded\nopen-domain dialog show that GODEL outperforms state-of-the-art pre-trained\ndialog models in few-shot fine-tuning setups, in terms of both human and\nautomatic evaluation. A novel feature of our evaluation methodology is the\nintroduction of a notion of utility that assesses the usefulness of responses\n(extrinsic evaluation) in addition to their communicative features (intrinsic\nevaluation). We show that extrinsic evaluation offers improved inter-annotator\nagreement and correlation with automated metrics. Code and data processing\nscripts are publicly available.", "published": "2022-06-22 18:19:32", "link": "http://arxiv.org/abs/2206.11309v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DP-Parse: Finding Word Boundaries from Raw Speech with an Instance\n  Lexicon", "abstract": "Finding word boundaries in continuous speech is challenging as there is\nlittle or no equivalent of a 'space' delimiter between words. Popular Bayesian\nnon-parametric models for text segmentation use a Dirichlet process to jointly\nsegment sentences and build a lexicon of word types. We introduce DP-Parse,\nwhich uses similar principles but only relies on an instance lexicon of word\ntokens, avoiding the clustering errors that arise with a lexicon of word types.\nOn the Zero Resource Speech Benchmark 2017, our model sets a new speech\nsegmentation state-of-the-art in 5 languages. The algorithm monotonically\nimproves with better input representations, achieving yet higher scores when\nfed with weakly supervised inputs. Despite lacking a type lexicon, DP-Parse can\nbe pipelined to a language model and learn semantic and syntactic\nrepresentations as assessed by a new spoken word embedding benchmark.", "published": "2022-06-22 19:15:57", "link": "http://arxiv.org/abs/2206.11332v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient and effective training of language and graph neural network\n  models", "abstract": "Can we combine heterogenous graph structure with text to learn high-quality\nsemantic and behavioural representations? Graph neural networks (GNN)s encode\nnumerical node attributes and graph structure to achieve impressive performance\nin a variety of supervised learning tasks. Current GNN approaches are\nchallenged by textual features, which typically need to be encoded to a\nnumerical vector before provided to the GNN that may incur some information\nloss. In this paper, we put forth an efficient and effective framework termed\nlanguage model GNN (LM-GNN) to jointly train large-scale language models and\ngraph neural networks. The effectiveness in our framework is achieved by\napplying stage-wise fine-tuning of the BERT model first with heterogenous graph\ninformation and then with a GNN model. Several system and design optimizations\nare proposed to enable scalable and efficient training. LM-GNN accommodates\nnode and edge classification as well as link prediction tasks. We evaluate the\nLM-GNN framework in different datasets performance and showcase the\neffectiveness of the proposed approach. LM-GNN provides competitive results in\nan Amazon query-purchase-product application.", "published": "2022-06-22 00:23:37", "link": "http://arxiv.org/abs/2206.10781v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multi-LexSum: Real-World Summaries of Civil Rights Lawsuits at Multiple\n  Granularities", "abstract": "With the advent of large language models, methods for abstractive\nsummarization have made great strides, creating potential for use in\napplications to aid knowledge workers processing unwieldy document collections.\nOne such setting is the Civil Rights Litigation Clearinghouse (CRLC)\n(https://clearinghouse.net),which posts information about large-scale civil\nrights lawsuits, serving lawyers, scholars, and the general public. Today,\nsummarization in the CRLC requires extensive training of lawyers and law\nstudents who spend hours per case understanding multiple relevant documents in\norder to produce high-quality summaries of key events and outcomes. Motivated\nby this ongoing real-world summarization effort, we introduce Multi-LexSum, a\ncollection of 9,280 expert-authored summaries drawn from ongoing CRLC writing.\nMulti-LexSum presents a challenging multi-document summarization task given the\nlength of the source documents, often exceeding two hundred pages per case.\nFurthermore, Multi-LexSum is distinct from other datasets in its multiple\ntarget summaries, each at a different granularity (ranging from one-sentence\n\"extreme\" summaries to multi-paragraph narrations of over five hundred words).\nWe present extensive analysis demonstrating that despite the high-quality\nsummaries in the training data (adhering to strict content and style\nguidelines), state-of-the-art summarization models perform poorly on this task.\nWe release Multi-LexSum for further research in summarization methods as well\nas to facilitate development of applications to assist in the CRLC's mission at\nhttps://multilexsum.github.io.", "published": "2022-06-22 07:26:55", "link": "http://arxiv.org/abs/2206.10883v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Evaluation of Embedding Models for Automatic Extraction and\n  Classification of Acknowledged Entities in Scientific Documents", "abstract": "Acknowledgments in scientific papers may give an insight into aspects of the\nscientific community, such as reward systems, collaboration patterns, and\nhidden research trends. The aim of the paper is to evaluate the performance of\ndifferent embedding models for the task of automatic extraction and\nclassification of acknowledged entities from the acknowledgment text in\nscientific papers. We trained and implemented a named entity recognition (NER)\ntask using the Flair NLP-framework. The training was conducted using three\ndefault Flair NER models with two differently-sized corpora. The Flair\nEmbeddings model trained on the larger training corpus showed the best accuracy\nof 0.77. Our model is able to recognize six entity types: funding agency, grant\nnumber, individuals, university, corporation and miscellaneous. The model works\nmore precise for some entity types than the others, thus, individuals and grant\nnumbers showed very good F1-Score over 0.9. Most of the previous works on\nacknowledgement analysis were limited by the manual evaluation of data and\ntherefore by the amount of processed data. This model can be applied for the\ncomprehensive analysis of the acknowledgement texts and may potentially make a\ngreat contribution to the field of automated acknowledgement analysis.", "published": "2022-06-22 09:32:28", "link": "http://arxiv.org/abs/2206.10939v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Connecting a French Dictionary from the Beginning of the 20th Century to\n  Wikidata", "abstract": "The \\textit{Petit Larousse illustr\\'e} is a French dictionary first published\nin 1905. Its division in two main parts on language and on history and\ngeography corresponds to a major milestone in French lexicography as well as a\nrepository of general knowledge from this period. Although the value of many\nentries from 1905 remains intact, some descriptions now have a dimension that\nis more historical than contemporary. They are nonetheless significant to\nanalyze and understand cultural representations from this time. A comparison\nwith more recent information or a verification of these entries would require a\ntedious manual work. In this paper, we describe a new lexical resource, where\nwe connected all the dictionary entries of the history and geography part to\ncurrent data sources. For this, we linked each of these entries to a wikidata\nidentifier. Using the wikidata links, we can automate more easily the\nidentification, comparison, and verification of historically-situated\nrepresentations. We give a few examples on how to process wikidata identifiers\nand we carried out a small analysis of the entities described in the dictionary\nto outline possible applications. The resource, i.e. the annotation of 20,245\ndictionary entries with wikidata links, is available from GitHub\nurl{https://github.com/pnugues/petit_larousse_1905/", "published": "2022-06-22 12:45:21", "link": "http://arxiv.org/abs/2206.11022v3", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Answer Fast: Accelerating BERT on the Tensor Streaming Processor", "abstract": "Transformers have become a predominant machine learning workload, they are\nnot only the de-facto standard for natural language processing tasks, but they\nare also being deployed in other domains such as vision and speech recognition.\nMany of the transformer-based applications are real-time systems such as\nmachine translation and web search. These real time systems often come with\nstrict end-to-end inference latency requirements. Unfortunately, while the\nmajority of the transformer computation comes from matrix multiplications,\ntransformers also include several non-linear components that tend to become the\nbottleneck during an inference. In this work, we accelerate the inference of\nBERT models on the tensor streaming processor. By carefully fusing all the\nnonlinear components with the matrix multiplication components, we are able to\nefficiently utilize the on-chip matrix multiplication units resulting in a\ndeterministic tail latency of 130 $\\mu$s for a batch-1 inference through\nBERT-base, which is 6X faster than the current state-of-the-art.", "published": "2022-06-22 13:27:27", "link": "http://arxiv.org/abs/2206.11062v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multi-View Clustering for Open Knowledge Base Canonicalization", "abstract": "Open information extraction (OIE) methods extract plenty of OIE triples <noun\nphrase, relation phrase, noun phrase> from unstructured text, which compose\nlarge open knowledge bases (OKBs). Noun phrases and relation phrases in such\nOKBs are not canonicalized, which leads to scattered and redundant facts. It is\nfound that two views of knowledge (i.e., a fact view based on the fact triple\nand a context view based on the fact triple's source context) provide\ncomplementary information that is vital to the task of OKB canonicalization,\nwhich clusters synonymous noun phrases and relation phrases into the same group\nand assigns them unique identifiers. However, these two views of knowledge have\nso far been leveraged in isolation by existing works. In this paper, we propose\nCMVC, a novel unsupervised framework that leverages these two views of\nknowledge jointly for canonicalizing OKBs without the need of manually\nannotated labels. To achieve this goal, we propose a multi-view CH K-Means\nclustering algorithm to mutually reinforce the clustering of view-specific\nembeddings learned from each view by considering their different clustering\nqualities. In order to further enhance the canonicalization performance, we\npropose a training data optimization strategy in terms of data quantity and\ndata quality respectively in each particular view to refine the learned\nview-specific embeddings in an iterative manner. Additionally, we propose a\nLog-Jump algorithm to predict the optimal number of clusters in a data-driven\nway without requiring any labels. We demonstrate the superiority of our\nframework through extensive experiments on multiple real-world OKB data sets\nagainst state-of-the-art methods.", "published": "2022-06-22 14:23:16", "link": "http://arxiv.org/abs/2206.11130v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Modeling Emergent Lexicon Formation with a Self-Reinforcing Stochastic\n  Process", "abstract": "We introduce FiLex, a self-reinforcing stochastic process which models finite\nlexicons in emergent language experiments. The central property of FiLex is\nthat it is a self-reinforcing process, parallel to the intuition that the more\na word is used in a language, the more its use will continue. As a theoretical\nmodel, FiLex serves as a way to both explain and predict the behavior of the\nemergent language system. We empirically test FiLex's ability to capture the\nrelationship between the emergent language's hyperparameters and the lexicon's\nShannon entropy.", "published": "2022-06-22 14:47:24", "link": "http://arxiv.org/abs/2206.11146v1", "categories": ["cs.CL", "cs.MA", "I.2.11; I.2.7; I.6.m"], "primary_category": "cs.CL"}
{"title": "The Problem of Semantic Shift in Longitudinal Monitoring of Social\n  Media: A Case Study on Mental Health During the COVID-19 Pandemic", "abstract": "Social media allows researchers to track societal and cultural changes over\ntime based on language analysis tools. Many of these tools rely on statistical\nalgorithms which need to be tuned to specific types of language. Recent studies\nhave shown the absence of appropriate tuning, specifically in the presence of\nsemantic shift, can hinder robustness of the underlying methods. However,\nlittle is known about the practical effect this sensitivity may have on\ndownstream longitudinal analyses. We explore this gap in the literature through\na timely case study: understanding shifts in depression during the course of\nthe COVID-19 pandemic. We find that inclusion of only a small number of\nsemantically-unstable features can promote significant changes in longitudinal\nestimates of our target outcome. At the same time, we demonstrate that a\nrecently-introduced method for measuring semantic shift may be used to\nproactively identify failure points of language-based models and, in turn,\nimprove predictive generalization.", "published": "2022-06-22 15:09:28", "link": "http://arxiv.org/abs/2206.11160v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Recommendations for Systematic Research on Emergent Language", "abstract": "Emergent language is unique among fields within the discipline of machine\nlearning for its open-endedness, not obviously presenting well-defined problems\nto be solved. As a result, the current research in the field has largely been\nexploratory: focusing on establishing new problems, techniques, and phenomena.\nYet after these problems have been established, subsequent progress requires\nresearch which can measurably demonstrate how it improves on prior approaches.\nThis type of research is what we call systematic research; in this paper, we\nillustrate this mode of research specifically for emergent language. We first\nidentify the overarching goals of emergent language research, categorizing them\nas either science or engineering. Using this distinction, we present core\nmethodological elements of science and engineering, analyze their role in\ncurrent emergent language research, and recommend how to apply these elements.", "published": "2022-06-22 18:10:44", "link": "http://arxiv.org/abs/2206.11302v1", "categories": ["cs.MA", "cs.CL", "I.2.11; I.6.m; K.4.m"], "primary_category": "cs.MA"}
{"title": "Enhancing Networking Cipher Algorithms with Natural Language", "abstract": "This work provides a survey of several networking cipher algorithms and\nproposes a method for integrating natural language processing (NLP) as a\nprotective agent for them. Two main proposals are covered for the use of NLP in\nnetworking. First, NLP is considered as the weakest link in a networking\nencryption model; and, second, as a hefty deterrent when combined as an extra\nlayer over what could be considered a strong type of encryption -- the stream\ncipher. This paper summarizes how languages can be integrated into symmetric\nencryption as a way to assist in the encryption of vulnerable streams that may\nbe found under attack due to the natural frequency distribution of letters or\nwords in a local language stream.", "published": "2022-06-22 09:05:52", "link": "http://arxiv.org/abs/2206.10924v1", "categories": ["cs.CL", "cs.CR", "cs.NI"], "primary_category": "cs.CL"}
{"title": "Toward An Optimal Selection of Dialogue Strategies: A Target-Driven\n  Approach for Intelligent Outbound Robots", "abstract": "With the growth of the economy and society, enterprises, especially in the\nFinTech industry, have increasing demands of outbound calls for customers such\nas debt collection, marketing, anti-fraud calls, and so on. But a large amount\nof repetitive and mechanical work occupies most of the time of human agents, so\nthe cost of equipment and labor for enterprises is increasing accordingly. At\nthe same time, with the development of artificial intelligence technology in\nthe past few decades, it has become quite common for companies to use new\ntechnologies such as Big Data and artificial intelligence to empower outbound\ncall businesses. The intelligent outbound robot is a typical application of the\nartificial intelligence technology in the field of outbound call businesses. It\nis mainly used to communicate with customers in order to accomplish a certain\ntarget. It has the characteristics of low cost, high reuse, and easy\ncompliance, which has attracted more attention from the industry.\n  At present, there are two kinds of intelligent outbound robots in the\nindustry but both of them still leave large room for improvement. One kind of\nthem is based on a finite state machine relying on the configuration of jump\nconditions and corresponding nodes based on manual experience. This kind of\nintelligent outbound robot is also called a flow-based robot. For example, the\nschematic diagram of the working model of a flow-based robot for debt\ncollection is shown in Fig.\\ref{fig:label}. In each round, the robot will reply\nto the user with the words corresponding to each node.", "published": "2022-06-22 09:49:30", "link": "http://arxiv.org/abs/2206.10953v1", "categories": ["cs.CL", "cs.AI", "cs.RO"], "primary_category": "cs.CL"}
{"title": "reStructured Pre-training", "abstract": "In this work, we try to decipher the internal connection of NLP technology\ndevelopment in the past decades, searching for essence, which rewards us with a\n(potential) new learning paradigm for NLP tasks, dubbed as reStructured\nPre-training (RST). In such a paradigm, the role of data will be re-emphasized,\nand model pre-training and fine-tuning of downstream tasks are viewed as a\nprocess of data storing and accessing. Based on that, we operationalize the\nsimple principle that a good storage mechanism should not only have the ability\nto cache a large amount of data but also consider the ease of access. We\nachieve this by pre-training models over restructured data that consist of a\nvariety of valuable information instead of raw data after overcoming several\nengineering challenges. Experimentally, RST models not only surpass strong\ncompetitors (e.g., T0) on 52/55 popular datasets from a variety of NLP tasks,\nbut also achieve superior performance in National College Entrance Examination\n- English (Gaokao-English),the most authoritative examination in China.\nSpecifically, the proposed system Qin achieves 40 points higher than the\naverage scores made by students and 15 points higher than GPT3 with 1/16\nparameters. In particular, Qin gets a high score of 138.5 (the full mark is\n150) in the 2018 English exam (national paper III). We have released the Gaokao\nBenchmark with an online submission platform.\n  In addition, we test our model in the 2022 College Entrance Examination\nEnglish that happened a few days ago (2022.06.08), and it gets a total score of\n134 (v.s. GPT3's 108).", "published": "2022-06-22 14:49:24", "link": "http://arxiv.org/abs/2206.11147v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Then and Now: Quantifying the Longitudinal Validity of Self-Disclosed\n  Depression Diagnoses", "abstract": "Self-disclosed mental health diagnoses, which serve as ground truth\nannotations of mental health status in the absence of clinical measures,\nunderpin the conclusions behind most computational studies of mental health\nlanguage from the last decade. However, psychiatric conditions are dynamic; a\nprior depression diagnosis may no longer be indicative of an individual's\nmental health, either due to treatment or other mitigating factors. We ask: to\nwhat extent are self-disclosures of mental health diagnoses actually relevant\nover time? We analyze recent activity from individuals who disclosed a\ndepression diagnosis on social media over five years ago and, in turn, acquire\na new understanding of how presentations of mental health status on social\nmedia manifest longitudinally. We also provide expanded evidence for the\npresence of personality-related biases in datasets curated using self-disclosed\ndiagnoses. Our findings motivate three practical recommendations for improving\nmental health datasets curated using self-disclosed diagnoses: 1) Annotate\ndiagnosis dates and psychiatric comorbidities; 2) Sample control groups using\npropensity score matching; 3) Identify and remove spurious correlations\nintroduced by selection bias.", "published": "2022-06-22 15:02:03", "link": "http://arxiv.org/abs/2206.11155v1", "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Towards Unsupervised Content Disentanglement in Sentence Representations\n  via Syntactic Roles", "abstract": "Linking neural representations to linguistic factors is crucial in order to\nbuild and analyze NLP models interpretable by humans. Among these factors,\nsyntactic roles (e.g. subjects, direct objects,$\\dots$) and their realizations\nare essential markers since they can be understood as a decomposition of\npredicative structures and thus the meaning of sentences. Starting from a deep\nprobabilistic generative model with attention, we measure the interaction\nbetween latent variables and realizations of syntactic roles and show that it\nis possible to obtain, without supervision, representations of sentences where\ndifferent syntactic roles correspond to clearly identified different latent\nvariables. The probabilistic model we propose is an Attention-Driven\nVariational Autoencoder (ADVAE). Drawing inspiration from Transformer-based\nmachine translation models, ADVAEs enable the analysis of the interactions\nbetween latent variables and input tokens through attention. We also develop an\nevaluation protocol to measure disentanglement with regard to the realizations\nof syntactic roles. This protocol is based on attention maxima for the encoder\nand on latent variable perturbations for the decoder. Our experiments on raw\nEnglish text from the SNLI dataset show that $\\textit{i)}$ disentanglement of\nsyntactic roles can be induced without supervision, $\\textit{ii)}$ ADVAE\nseparates syntactic roles better than classical sequence VAEs and Transformer\nVAEs, $\\textit{iii)}$ realizations of syntactic roles can be separately\nmodified in sentences by mere intervention on the associated latent variables.\nOur work constitutes a first step towards unsupervised controllable content\ngeneration. The code for our work is publicly available.", "published": "2022-06-22 15:50:01", "link": "http://arxiv.org/abs/2206.11184v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VisFIS: Visual Feature Importance Supervision with\n  Right-for-the-Right-Reason Objectives", "abstract": "Many past works aim to improve visual reasoning in models by supervising\nfeature importance (estimated by model explanation techniques) with human\nannotations such as highlights of important image regions. However, recent work\nhas shown that performance gains from feature importance (FI) supervision for\nVisual Question Answering (VQA) tasks persist even with random supervision,\nsuggesting that these methods do not meaningfully align model FI with human FI.\nIn this paper, we show that model FI supervision can meaningfully improve VQA\nmodel accuracy as well as performance on several Right-for-the-Right-Reason\n(RRR) metrics by optimizing for four key model objectives: (1) accurate\npredictions given limited but sufficient information (Sufficiency); (2)\nmax-entropy predictions given no important information (Uncertainty); (3)\ninvariance of predictions to changes in unimportant features (Invariance); and\n(4) alignment between model FI explanations and human FI explanations\n(Plausibility). Our best performing method, Visual Feature Importance\nSupervision (VisFIS), outperforms strong baselines on benchmark VQA datasets in\nterms of both in-distribution and out-of-distribution accuracy. While past work\nsuggests that the mechanism for improved accuracy is through improved\nexplanation plausibility, we show that this relationship depends crucially on\nexplanation faithfulness (whether explanations truly represent the model's\ninternal reasoning). Predictions are more accurate when explanations are\nplausible and faithful, and not when they are plausible but not faithful.\nLastly, we show that, surprisingly, RRR metrics are not predictive of\nout-of-distribution model accuracy when controlling for a model's\nin-distribution accuracy, which calls into question the value of these metrics\nfor evaluating model reasoning. All supporting code is available at\nhttps://github.com/zfying/visfis", "published": "2022-06-22 17:02:01", "link": "http://arxiv.org/abs/2206.11212v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "GEMv2: Multilingual NLG Benchmarking in a Single Line of Code", "abstract": "Evaluation in machine learning is usually informed by past choices, for\nexample which datasets or metrics to use. This standardization enables the\ncomparison on equal footing using leaderboards, but the evaluation choices\nbecome sub-optimal as better alternatives arise. This problem is especially\npertinent in natural language generation which requires ever-improving suites\nof datasets, metrics, and human evaluation to make definitive claims. To make\nfollowing best model evaluation practices easier, we introduce GEMv2. The new\nversion of the Generation, Evaluation, and Metrics Benchmark introduces a\nmodular infrastructure for dataset, model, and metric developers to benefit\nfrom each others work. GEMv2 supports 40 documented datasets in 51 languages.\nModels for all datasets can be evaluated online and our interactive data card\ncreation and rendering tools make it easier to add new datasets to the living\nbenchmark.", "published": "2022-06-22 17:52:30", "link": "http://arxiv.org/abs/2206.11249v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Conformer with dual-mode chunked attention for joint online and offline\n  ASR", "abstract": "In this paper, we present an in-depth study on online attention mechanisms\nand distillation techniques for dual-mode (i.e., joint online and offline) ASR\nusing the Conformer Transducer. In the dual-mode Conformer Transducer model,\nlayers can function in online or offline mode while sharing parameters, and\nin-place knowledge distillation from offline to online mode is applied in\ntraining to improve online accuracy. In our study, we first demonstrate\naccuracy improvements from using chunked attention in the Conformer encoder\ncompared to autoregressive attention with and without lookahead. Furthermore,\nwe explore the efficient KLD and 1-best KLD losses with different shifts\nbetween online and offline outputs in the knowledge distillation. Finally, we\nshow that a simplified dual-mode Conformer that only has mode-specific\nself-attention performs equally well as the one also having mode-specific\nconvolutions and normalization. Our experiments are based on two very different\ndatasets: the Librispeech task and an internal corpus of medical conversations.\nResults show that the proposed dual-mode system using chunked attention yields\n5% and 4% relative WER improvement on the Librispeech and medical tasks,\ncompared to the dual-mode system using autoregressive attention with similar\naverage lookahead.", "published": "2022-06-22 15:04:15", "link": "http://arxiv.org/abs/2206.11157v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Radio2Speech: High Quality Speech Recovery from Radio Frequency Signals", "abstract": "Considering the microphone is easily affected by noise and soundproof\nmaterials, the radio frequency (RF) signal is a promising candidate to recover\naudio as it is immune to noise and can traverse many soundproof objects. In\nthis paper, we introduce Radio2Speech, a system that uses RF signals to recover\nhigh quality speech from the loudspeaker. Radio2Speech can recover speech\ncomparable to the quality of the microphone, advancing from recovering only\nsingle tone music or incomprehensible speech in existing approaches. We use\nRadio UNet to accurately recover speech in time-frequency domain from RF\nsignals with limited frequency band. Also, we incorporate the neural vocoder to\nsynthesize the speech waveform from the estimated time-frequency representation\nwithout using the contaminated phase. Quantitative and qualitative evaluations\nshow that in quiet, noisy and soundproof scenarios, Radio2Speech achieves\nstate-of-the-art performance and is on par with the microphone that works in\nquiet scenarios.", "published": "2022-06-22 13:29:45", "link": "http://arxiv.org/abs/2206.11066v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Jointist: Joint Learning for Multi-instrument Transcription and Its\n  Applications", "abstract": "In this paper, we introduce Jointist, an instrument-aware multi-instrument\nframework that is capable of transcribing, recognizing, and separating multiple\nmusical instruments from an audio clip. Jointist consists of the instrument\nrecognition module that conditions the other modules: the transcription module\nthat outputs instrument-specific piano rolls, and the source separation module\nthat utilizes instrument information and transcription results.\n  The instrument conditioning is designed for an explicit multi-instrument\nfunctionality while the connection between the transcription and source\nseparation modules is for better transcription performance. Our challenging\nproblem formulation makes the model highly useful in the real world given that\nmodern popular music typically consists of multiple instruments. However, its\nnovelty necessitates a new perspective on how to evaluate such a model. During\nthe experiment, we assess the model from various aspects, providing a new\nevaluation perspective for multi-instrument transcription. We also argue that\ntranscription models can be utilized as a preprocessing module for other music\nanalysis tasks. In the experiment on several downstream tasks, the symbolic\nrepresentation provided by our transcription model turned out to be helpful to\nspectrograms in solving downbeat detection, chord recognition, and key\nestimation.", "published": "2022-06-22 02:03:01", "link": "http://arxiv.org/abs/2206.10805v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "UniCon+: ICTCAS-UCAS Submission to the AVA-ActiveSpeaker Task at\n  ActivityNet Challenge 2022", "abstract": "This report presents a brief description of our winning solution to the AVA\nActive Speaker Detection (ASD) task at ActivityNet Challenge 2022. Our\nunderlying model UniCon+ continues to build on our previous work, the Unified\nContext Network (UniCon) and Extended UniCon which are designed for robust\nscene-level ASD. We augment the architecture with a simple GRU-based module\nthat allows information of recurring identities to flow across scenes through\nread and update operations. We report a best result of 94.47% mAP on the\nAVA-ActiveSpeaker test set, which continues to rank first on this year's\nchallenge leaderboard and significantly pushes the state-of-the-art.", "published": "2022-06-22 06:11:07", "link": "http://arxiv.org/abs/2206.10861v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "A Systematic Comparison of Phonetic Aware Techniques for Speech\n  Enhancement", "abstract": "Speech enhancement has seen great improvement in recent years using\nend-to-end neural networks. However, most models are agnostic to the spoken\nphonetic content. Recently, several studies suggested phonetic-aware speech\nenhancement, mostly using perceptual supervision. Yet, injecting phonetic\nfeatures during model optimization can take additional forms (e.g., model\nconditioning). In this paper, we conduct a systematic comparison between\ndifferent methods of incorporating phonetic information in a speech enhancement\nmodel. By conducting a series of controlled experiments, we observe the\ninfluence of different phonetic content models as well as various\nfeature-injection techniques on enhancement performance, considering both\ncausal and non-causal models. Specifically, we evaluate three settings for\ninjecting phonetic information, namely: i) feature conditioning; ii) perceptual\nsupervision; and iii) regularization. Phonetic features are obtained using an\nintermediate layer of either a supervised pre-trained Automatic Speech\nRecognition (ASR) model or by using a pre-trained Self-Supervised Learning\n(SSL) model. We further observe the effect of choosing different embedding\nlayers on performance, considering both manual and learned configurations.\nResults suggest that using a SSL model as phonetic features outperforms the ASR\none in most cases. Interestingly, the conditioning setting performs best among\nthe evaluated configurations.", "published": "2022-06-22 12:00:50", "link": "http://arxiv.org/abs/2206.11000v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dynamic Restrained Uncertainty Weighting Loss for Multitask Learning of\n  Vocal Expression", "abstract": "We propose a novel Dynamic Restrained Uncertainty Weighting Loss to\nexperimentally handle the problem of balancing the contributions of multiple\ntasks on the ICML ExVo 2022 Challenge. The multitask aims to recognize\nexpressed emotions and demographic traits from vocal bursts jointly. Our\nstrategy combines the advantages of Uncertainty Weight and Dynamic Weight\nAverage, by extending weights with a restraint term to make the learning\nprocess more explainable. We use a lightweight multi-exit CNN architecture to\nimplement our proposed loss approach. The experimental H-Mean score (0.394)\nshows a substantial improvement over the baseline H-Mean score (0.335).", "published": "2022-06-22 13:14:09", "link": "http://arxiv.org/abs/2206.11049v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On the Role of Spatial, Spectral, and Temporal Processing for DNN-based\n  Non-linear Multi-channel Speech Enhancement", "abstract": "Employing deep neural networks (DNNs) to directly learn filters for\nmulti-channel speech enhancement has potentially two key advantages over a\ntraditional approach combining a linear spatial filter with an independent\ntempo-spectral post-filter: 1) non-linear spatial filtering allows to overcome\npotential restrictions originating from a linear processing model and 2) joint\nprocessing of spatial and tempo-spectral information allows to exploit\ninterdependencies between different sources of information. A variety of\nDNN-based non-linear filters have been proposed recently, for which good\nenhancement performance is reported. However, little is known about the\ninternal mechanisms which turns network architecture design into a game of\nchance. Therefore, in this paper, we perform experiments to better understand\nthe internal processing of spatial, spectral and temporal information by\nDNN-based non-linear filters. On the one hand, our experiments in a difficult\nspeech extraction scenario confirm the importance of non-linear spatial\nfiltering, which outperforms an oracle linear spatial filter by 0.24 POLQA\nscore. On the other hand, we demonstrate that joint processing results in a\nlarge performance gap of 0.4 POLQA score between network architectures\nexploiting spectral versus temporal information besides spatial information.", "published": "2022-06-22 15:42:44", "link": "http://arxiv.org/abs/2206.11181v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Few-shot Long-Tailed Bird Audio Recognition", "abstract": "It is easier to hear birds than see them. However, they still play an\nessential role in nature and are excellent indicators of deteriorating\nenvironmental quality and pollution. Recent advances in Deep Neural Networks\nallow us to process audio data to detect and classify birds. This technology\ncan assist researchers in monitoring bird populations and biodiversity. We\npropose a sound detection and classification pipeline to analyze complex\nsoundscape recordings and identify birdcalls in the background. Our method\nlearns from weak labels and few data and acoustically recognizes the bird\nspecies. Our solution achieved 18th place of 807 teams at the BirdCLEF 2022\nChallenge hosted on Kaggle.", "published": "2022-06-22 04:14:25", "link": "http://arxiv.org/abs/2206.11260v2", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Simple Baseline for Domain Adaptation in End to End ASR Systems Using\n  Synthetic Data", "abstract": "Automatic Speech Recognition(ASR) has been dominated by deep learning-based\nend-to-end speech recognition models. These approaches require large amounts of\nlabeled data in the form of audio-text pairs. Moreover, these models are more\nsusceptible to domain shift as compared to traditional models. It is common\npractice to train generic ASR models and then adapt them to target domains\nusing comparatively smaller data sets. We consider a more extreme case of\ndomain adaptation where text-only corpus is available. In this work, we propose\na simple baseline technique for domain adaptation in end-to-end speech\nrecognition models. We convert the text-only corpus to audio data using single\nspeaker Text to Speech (TTS) engine. The parallel data in the target domain is\nthen used to fine-tune the final dense layer of generic ASR models. We show\nthat single speaker synthetic TTS data coupled with final dense layer only\nfine-tuning provides reasonable improvements in word error rates. We use text\ndata from address and e-commerce search domains to show the effectiveness of\nour low-cost baseline approach on CTC and attention-based models.", "published": "2022-06-22 12:07:38", "link": "http://arxiv.org/abs/2206.13240v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
