{"title": "Cross-Language Domain Adaptation for Classifying Crisis-Related Short\n  Messages", "abstract": "Rapid crisis response requires real-time analysis of messages. After a\ndisaster happens, volunteers attempt to classify tweets to determine needs,\ne.g., supplies, infrastructure damage, etc. Given labeled data, supervised\nmachine learning can help classify these messages. Scarcity of labeled data\ncauses poor performance in machine training. Can we reuse old tweets to train\nclassifiers? How can we choose labeled tweets for training? Specifically, we\nstudy the usefulness of labeled data of past events. Do labeled tweets in\ndifferent language help? We observe the performance of our classifiers trained\nusing different combinations of training sets obtained from past disasters. We\nperform extensive experimentation on real crisis datasets and show that the\npast labels are useful when both source and target events are of the same type\n(e.g. both earthquakes). For similar languages (e.g., Italian and Spanish),\ncross-language domain adaptation was useful, however, when for different\nlanguages (e.g., Italian and English), the performance decreased.", "published": "2016-02-17 12:29:56", "link": "http://arxiv.org/abs/1602.05388v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Authorship Attribution Using a Neural Network Language Model", "abstract": "In practice, training language models for individual authors is often\nexpensive because of limited data resources. In such cases, Neural Network\nLanguage Models (NNLMs), generally outperform the traditional non-parametric\nN-gram models. Here we investigate the performance of a feed-forward NNLM on an\nauthorship attribution problem, with moderate author set size and relatively\nlimited data. We also consider how the text topics impact performance. Compared\nwith a well-constructed N-gram baseline method with Kneser-Ney smoothing, the\nproposed method achieves nearly 2:5% reduction in perplexity and increases\nauthor classification accuracy by 3:43% on average, given as few as 5 test\nsentences. The performance is very competitive with the state of the art in\nterms of accuracy and demand on test data. The source code, preprocessed\ndatasets, a detailed description of the methodology and results are available\nat https://github.com/zge/authorship-attribution.", "published": "2016-02-17 04:06:28", "link": "http://arxiv.org/abs/1602.05292v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Label Noise Reduction in Entity Typing by Heterogeneous Partial-Label\n  Embedding", "abstract": "Current systems of fine-grained entity typing use distant supervision in\nconjunction with existing knowledge bases to assign categories (type labels) to\nentity mentions. However, the type labels so obtained from knowledge bases are\noften noisy (i.e., incorrect for the entity mention's local context). We define\na new task, Label Noise Reduction in Entity Typing (LNR), to be the automatic\nidentification of correct type labels (type-paths) for training examples, given\nthe set of candidate type labels obtained by distant supervision with a given\ntype hierarchy. The unknown type labels for individual entity mentions and the\nsemantic similarity between entity types pose unique challenges for solving the\nLNR task. We propose a general framework, called PLE, to jointly embed entity\nmentions, text features and entity types into the same low-dimensional space\nwhere, in that space, objects whose types are semantically close have similar\nrepresentations. Then we estimate the type-path for each training example in a\ntop-down manner using the learned embeddings. We formulate a global objective\nfor learning the embeddings from text corpora and knowledge bases, which adopts\na novel margin-based loss that is robust to noisy labels and faithfully models\ntype correlation derived from knowledge bases. Our experiments on three public\ntyping datasets demonstrate the effectiveness and robustness of PLE, with an\naverage of 25% improvement in accuracy compared to next best method.", "published": "2016-02-17 05:26:47", "link": "http://arxiv.org/abs/1602.05307v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Comparative Study of Word and Sentence Similarity\n  Measures", "abstract": "Sentence similarity is considered the basis of many natural language tasks\nsuch as information retrieval, question answering and text summarization. The\nsemantic meaning between compared text fragments is based on the words semantic\nfeatures and their relationships. This article reviews a set of word and\nsentence similarity measures and compares them on benchmark datasets. On the\nstudied datasets, results showed that hybrid semantic measures perform better\nthan both knowledge and corpus based measures.", "published": "2016-02-17 19:33:47", "link": "http://arxiv.org/abs/1610.04533v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
