{"title": "Weakly-supervised Relation Extraction by Pattern-enhanced Embedding\n  Learning", "abstract": "Extracting relations from text corpora is an important task in text mining.\nIt becomes particularly challenging when focusing on weakly-supervised relation\nextraction, that is, utilizing a few relation instances (i.e., a pair of\nentities and their relation) as seeds to extract more instances from corpora.\nExisting distributional approaches leverage the corpus-level co-occurrence\nstatistics of entities to predict their relations, and require large number of\nlabeled instances to learn effective relation classifiers. Alternatively,\npattern-based approaches perform bootstrapping or apply neural networks to\nmodel the local contexts, but still rely on large number of labeled instances\nto build reliable models. In this paper, we study integrating the\ndistributional and pattern-based methods in a weakly-supervised setting, such\nthat the two types of methods can provide complementary supervision for each\nother to build an effective, unified model. We propose a novel co-training\nframework with a distributional module and a pattern module. During training,\nthe distributional module helps the pattern module discriminate between the\ninformative patterns and other patterns, and the pattern module generates some\nhighly-confident instances to improve the distributional module. The whole\nframework can be effectively optimized by iterating between improving the\npattern module and updating the distributional module. We conduct experiments\non two tasks: knowledge base completion with text corpora and corpus-level\nrelation extraction. Experimental results prove the effectiveness of our\nframework in the weakly-supervised setting.", "published": "2017-11-09 01:42:14", "link": "http://arxiv.org/abs/1711.03226v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Analysis of Multiple-Turn Reasoning Strategies in Reading\n  Comprehension Tasks", "abstract": "Reading comprehension (RC) is a challenging task that requires synthesis of\ninformation across sentences and multiple turns of reasoning. Using a\nstate-of-the-art RC model, we empirically investigate the performance of\nsingle-turn and multiple-turn reasoning on the SQuAD and MS MARCO datasets. The\nRC model is an end-to-end neural network with iterative attention, and uses\nreinforcement learning to dynamically control the number of turns. We find that\nmultiple-turn reasoning outperforms single-turn reasoning for all question and\nanswer types; further, we observe that enabling a flexible number of turns\ngenerally improves upon a fixed multiple-turn strategy. %across all question\ntypes, and is particularly beneficial to questions with lengthy, descriptive\nanswers. We achieve results competitive to the state-of-the-art on these two\ndatasets.", "published": "2017-11-09 01:56:00", "link": "http://arxiv.org/abs/1711.03230v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tracking of enriched dialog states for flexible conversational\n  information access", "abstract": "Dialog state tracking (DST) is a crucial component in a task-oriented dialog\nsystem for conversational information access. A common practice in current\ndialog systems is to define the dialog state by a set of slot-value pairs. Such\nrepresentation of dialog states and the slot-filling based DST have been widely\nemployed, but suffer from three drawbacks. (1) The dialog state can contain\nonly a single value for a slot, and (2) can contain only users' affirmative\npreference over the values for a slot. (3) Current task-based dialog systems\nmainly focus on the searching task, while the enquiring task is also very\ncommon in practice. The above observations motivate us to enrich current\nrepresentation of dialog states and collect a brand new dialog dataset about\nmovies, based upon which we build a new DST, called enriched DST (EDST), for\nflexible accessing movie information. The EDST supports the searching task, the\nenquiring task and their mixed task. We show that the new EDST method not only\nachieves good results on Iqiyi dataset, but also outperforms other\nstate-of-the-art DST methods on the traditional dialog datasets, WOZ2.0 and\nDSTC2.", "published": "2017-11-09 14:10:45", "link": "http://arxiv.org/abs/1711.03381v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Modeling for Code-Switched Data: Challenges and Approaches", "abstract": "Lately, the problem of code-switching has gained a lot of attention and has\nemerged as an active area of research. In bilingual communities, the speakers\ncommonly embed the words and phrases of a non-native language into the syntax\nof a native language in their day-to-day communications. The code-switching is\na global phenomenon among multilingual communities, still very limited acoustic\nand linguistic resources are available as yet. For developing effective speech\nbased applications, the ability of the existing language technologies to deal\nwith the code-switched data can not be over emphasized. The code-switching is\nbroadly classified into two modes: inter-sentential and intra-sentential\ncode-switching. In this work, we have studied the intra-sentential problem in\nthe context of code-switching language modeling task. The salient contributions\nof this paper includes: (i) the creation of Hindi-English code-switching text\ncorpus by crawling a few blogging sites educating about the usage of the\nInternet (ii) the exploration of the parts-of-speech features towards more\neffective modeling of Hindi-English code-switched data by the monolingual\nlanguage model (LM) trained on native (Hindi) language data, and (iii) the\nproposal of a novel textual factor referred to as the code-switch factor\n(CS-factor), which allows the LM to predict the code-switching instances. In\nthe context of recognition of the code-switching data, the substantial\nreduction in the PPL is achieved with the use of POS factors and also the\nproposed CS-factor provides independent as well as additive gain in the PPL.", "published": "2017-11-09 06:29:28", "link": "http://arxiv.org/abs/1711.03541v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Lifted Matrix-Space Model for Semantic Composition", "abstract": "Tree-structured neural network architectures for sentence encoding draw\ninspiration from the approach to semantic composition generally seen in formal\nlinguistics, and have shown empirical improvements over comparable sequence\nmodels by doing so. Moreover, adding multiplicative interaction terms to the\ncomposition functions in these models can yield significant further\nimprovements. However, existing compositional approaches that adopt such a\npowerful composition function scale poorly, with parameter counts exploding as\nmodel dimension or vocabulary size grows. We introduce the Lifted Matrix-Space\nmodel, which uses a global transformation to map vector word embeddings to\nmatrices, which can then be composed via an operation based on matrix-matrix\nmultiplication. Its composition function effectively transmits a larger number\nof activations across layers with relatively few model parameters. We evaluate\nour model on the Stanford NLI corpus, the Multi-Genre NLI corpus, and the\nStanford Sentiment Treebank and find that it consistently outperforms TreeLSTM\n(Tai et al., 2015), the previous best known composition function for\ntree-structured models.", "published": "2017-11-09 20:58:07", "link": "http://arxiv.org/abs/1711.03602v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large-scale Cloze Test Dataset Created by Teachers", "abstract": "Cloze tests are widely adopted in language exams to evaluate students'\nlanguage proficiency. In this paper, we propose the first large-scale\nhuman-created cloze test dataset CLOTH, containing questions used in\nmiddle-school and high-school language exams. With missing blanks carefully\ncreated by teachers and candidate choices purposely designed to be nuanced,\nCLOTH requires a deeper language understanding and a wider attention span than\npreviously automatically-generated cloze datasets. We test the performance of\ndedicatedly designed baseline models including a language model trained on the\nOne Billion Word Corpus and show humans outperform them by a significant\nmargin. We investigate the source of the performance gap, trace model\ndeficiencies to some distinct properties of CLOTH, and identify the limited\nability of comprehending the long-term context to be the key bottleneck.", "published": "2017-11-09 01:41:12", "link": "http://arxiv.org/abs/1711.03225v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SemRe-Rank: Improving Automatic Term Extraction By Incorporating\n  Semantic Relatedness With Personalised PageRank", "abstract": "Automatic Term Extraction deals with the extraction of terminology from a\ndomain specific corpus, and has long been an established research area in data\nand knowledge acquisition. ATE remains a challenging task as it is known that\nthere is no existing ATE methods that can consistently outperform others in any\ndomain. This work adopts a refreshed perspective to this problem: instead of\nsearching for such a 'one-size-fit-all' solution that may never exist, we\npropose to develop generic methods to 'enhance' existing ATE methods. We\nintroduce SemRe-Rank, the first method based on this principle, to incorporate\nsemantic relatedness - an often overlooked venue - into an existing ATE method\nto further improve its performance. SemRe-Rank incorporates word embeddings\ninto a personalised PageRank process to compute 'semantic importance' scores\nfor candidate terms from a graph of semantically related words (nodes), which\nare then used to revise the scores of candidate terms computed by a base ATE\nalgorithm. Extensively evaluated with 13 state-of-the-art base ATE methods on\nfour datasets of diverse nature, it is shown to have achieved widespread\nimprovement over all base methods and across all datasets, with up to 15\npercentage points when measured by the Precision in the top ranked K candidate\nterms (the average for a set of K's), or up to 28 percentage points in F1\nmeasured at a K that equals to the expected real terms in the candidates (F1 in\nshort). Compared to an alternative approach built on the well-known TextRank\nalgorithm, SemRe-Rank can potentially outperform by up to 8 points in Precision\nat top K, or up to 17 points in F1.", "published": "2017-11-09 13:39:21", "link": "http://arxiv.org/abs/1711.03373v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Open-World Knowledge Graph Completion", "abstract": "Knowledge Graphs (KGs) have been applied to many tasks including Web search,\nlink prediction, recommendation, natural language processing, and entity\nlinking. However, most KGs are far from complete and are growing at a rapid\npace. To address these problems, Knowledge Graph Completion (KGC) has been\nproposed to improve KGs by filling in its missing connections. Unlike existing\nmethods which hold a closed-world assumption, i.e., where KGs are fixed and new\nentities cannot be easily added, in the present work we relax this assumption\nand propose a new open-world KGC task. As a first attempt to solve this task we\nintroduce an open-world KGC model called ConMask. This model learns embeddings\nof the entity's name and parts of its text-description to connect unseen\nentities to the KG. To mitigate the presence of noisy text descriptions,\nConMask uses a relationship-dependent content masking to extract relevant\nsnippets and then trains a fully convolutional neural network to fuse the\nextracted snippets with entities in the KG. Experiments on large data sets,\nboth old and new, show that ConMask performs well in the open-world KGC task\nand even outperforms existing KGC models on the standard closed-world KGC task.", "published": "2017-11-09 15:58:55", "link": "http://arxiv.org/abs/1711.03438v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Learning Multi-Modal Word Representation Grounded in Visual Context", "abstract": "Representing the semantics of words is a long-standing problem for the\nnatural language processing community. Most methods compute word semantics\ngiven their textual context in large corpora. More recently, researchers\nattempted to integrate perceptual and visual features. Most of these works\nconsider the visual appearance of objects to enhance word representations but\nthey ignore the visual environment and context in which objects appear. We\npropose to unify text-based techniques with vision-based techniques by\nsimultaneously leveraging textual and visual context to learn multimodal word\nembeddings. We explore various choices for what can serve as a visual context\nand present an end-to-end method to integrate visual context elements in a\nmultimodal skip-gram model. We provide experiments and extensive analysis of\nthe obtained results.", "published": "2017-11-09 17:28:07", "link": "http://arxiv.org/abs/1711.03483v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Crafting Adversarial Examples For Speech Paralinguistics Applications", "abstract": "Computational paralinguistic analysis is increasingly being used in a wide\nrange of cyber applications, including security-sensitive applications such as\nspeaker verification, deceptive speech detection, and medical diagnostics.\nWhile state-of-the-art machine learning techniques, such as deep neural\nnetworks, can provide robust and accurate speech analysis, they are susceptible\nto adversarial attacks. In this work, we propose an end-to-end scheme to\ngenerate adversarial examples for computational paralinguistic applications by\nperturbing directly the raw waveform of an audio recording rather than specific\nacoustic features. Our experiments show that the proposed adversarial\nperturbation can lead to a significant performance drop of state-of-the-art\ndeep neural networks, while only minimally impairing the audio quality.", "published": "2017-11-09 07:41:53", "link": "http://arxiv.org/abs/1711.03280v2", "categories": ["cs.LG", "cs.CR", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
