{"title": "PAGE: A Position-Aware Graph-Based Model for Emotion Cause Entailment in\n  Conversation", "abstract": "Conversational Causal Emotion Entailment (C2E2) is a task that aims at\nrecognizing the causes corresponding to a target emotion in a conversation. The\norder of utterances in the conversation affects the causal inference. However,\nmost current position encoding strategies ignore the order relation among\nutterances and speakers. To address the issue, we devise a novel position-aware\ngraph to encode the entire conversation, fully modeling causal relations among\nutterances. The comprehensive experiments show that our method consistently\nachieves state-of-the-art performance on two challenging test sets, proving the\neffectiveness of our model. Our source code is available on Github:\nhttps://github.com/XiaojieGu/PAGE.", "published": "2023-03-03 09:13:23", "link": "http://arxiv.org/abs/2303.01795v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mapping Wordnets on the Fly with Permanent Sense Keys", "abstract": "Most of the major databases on the semantic web have links to Princeton\nWordNet (PWN) synonym set (synset) identifiers, which differ for each PWN\nrelease, and are thus incompatible between versions. On the other hand, both\nPWN and the more recent Open English Wordnet (OEWN) provide permanent word\nsense identifiers (the sense keys), which can solve this interoperability\nproblem.\n  We present an algorithm that runs in linear time, to automatically derive a\nsynset mapping between any pair of Wordnet versions that use PWN sense keys.\nThis allows to update old WordNet links, and seamlessly interoperate with newer\nEnglish Wordnet versions for which no prior mapping exists.\n  By applying the proposed algorithm on the fly, at load time, we combine the\nOpen Multilingual Wordnet (OMW 1.4, which uses old PWN 3.0 identifiers) with\nOEWN Edition 2021, and obtain almost perfect precision and recall. We compare\nthe results of our approach using respectively synset offsets, versus the\nCollaborative InterLingual Index (CILI version 1.0) as synset identifiers, and\nfind that the synset offsets perform better than CILI 1.0 in all cases, except\na few ties.", "published": "2023-03-03 11:01:10", "link": "http://arxiv.org/abs/2303.01847v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating the Translation Performance of a Large Multilingual\n  Language Model: the Case of BLOOM", "abstract": "The NLP community recently saw the release of a new large open-access\nmultilingual language model, BLOOM (BigScience et al., 2022) covering 46\nlanguages. We focus on BLOOM's multilingual ability by evaluating its machine\ntranslation performance across several datasets (WMT, Flores-101 and DiaBLa)\nand language pairs (high- and low-resourced). Our results show that 0-shot\nperformance suffers from overgeneration and generating in the wrong language,\nbut this is greatly improved in the few-shot setting, with very good results\nfor a number of language pairs. We study several aspects including prompt\ndesign, model sizes, cross-lingual transfer and the use of discursive context.", "published": "2023-03-03 13:23:42", "link": "http://arxiv.org/abs/2303.01911v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ancient Chinese Word Segmentation and Part-of-Speech Tagging Using\n  Distant Supervision", "abstract": "Ancient Chinese word segmentation (WSG) and part-of-speech tagging (POS) are\nimportant to study ancient Chinese, but the amount of ancient Chinese WSG and\nPOS tagging data is still rare. In this paper, we propose a novel augmentation\nmethod of ancient Chinese WSG and POS tagging data using distant supervision\nover parallel corpus. However, there are still mislabeled and unlabeled ancient\nChinese words inevitably in distant supervision. To address this problem, we\ntake advantage of the memorization effects of deep neural networks and a small\namount of annotated data to get a model with much knowledge and a little noise,\nand then we use this model to relabel the ancient Chinese sentences in parallel\ncorpus. Experiments show that the model trained over the relabeled data\noutperforms the model trained over the data generated from distant supervision\nand the annotated data. Our code is available at\nhttps://github.com/farlit/ACDS.", "published": "2023-03-03 13:24:17", "link": "http://arxiv.org/abs/2303.01912v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Who could be behind QAnon? Authorship attribution with supervised\n  machine-learning", "abstract": "A series of social media posts signed under the pseudonym \"Q\", started a\nmovement known as QAnon, which led some of its most radical supporters to\nviolent and illegal actions. To identify the person(s) behind Q, we evaluate\nthe coincidence between the linguistic properties of the texts written by Q and\nto those written by a list of suspects provided by journalistic investigation.\nTo identify the authors of these posts, serious challenges have to be\naddressed. The \"Q drops\" are very short texts, written in a way that constitute\na sort of literary genre in itself, with very peculiar features of style. These\ntexts might have been written by different authors, whose other writings are\noften hard to find. After an online ethnology of the movement, necessary to\ncollect enough material written by these thirteen potential authors, we use\nsupervised machine learning to build stylistic profiles for each of them. We\nthen performed a rolling analysis on Q's writings, to see if any of those\nlinguistic profiles match the so-called 'QDrops' in part or entirety. We\nconclude that two different individuals, Paul F. and Ron W., are the closest\nmatch to Q's linguistic signature, and they could have successively written Q's\ntexts. These potential authors are not high-ranked personality from the U.S.\nadministration, but rather social media activists.", "published": "2023-03-03 16:50:12", "link": "http://arxiv.org/abs/2303.02078v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Data Augmentation Methods on Social Media Corpora", "abstract": "Data augmentation has proven widely effective in computer vision. In Natural\nLanguage Processing (NLP) data augmentation remains an area of active research.\nThere is no widely accepted augmentation technique that works well across tasks\nand model architectures. In this paper we explore data augmentation techniques\nin the context of text classification using two social media datasets. We\nexplore popular varieties of data augmentation, starting with oversampling,\nEasy Data Augmentation (Wei and Zou, 2019) and Back-Translation (Sennrich et\nal., 2015). We also consider Greyscaling, a relatively unexplored data\naugmentation technique that seeks to mitigate the intensity of adjectives in\nexamples. Finally, we consider a few-shot learning approach: Pattern-Exploiting\nTraining (PET) (Schick et al., 2020). For the experiments we use a BERT\ntransformer architecture. Results show that augmentation techniques provide\nonly minimal and inconsistent improvements. Synonym replacement provided\nevidence of some performance improvement and adjective scales with Grayscaling\nis an area where further exploration would be valuable. Few-shot learning\nexperiments show consistent improvement over supervised training, and seem very\npromising when classes are easily separable but further exploration would be\nvaluable.", "published": "2023-03-03 20:15:35", "link": "http://arxiv.org/abs/2303.02198v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TrojText: Test-time Invisible Textual Trojan Insertion", "abstract": "In Natural Language Processing (NLP), intelligent neuron models can be\nsusceptible to textual Trojan attacks. Such attacks occur when Trojan models\nbehave normally for standard inputs but generate malicious output for inputs\nthat contain a specific trigger. Syntactic-structure triggers, which are\ninvisible, are becoming more popular for Trojan attacks because they are\ndifficult to detect and defend against. However, these types of attacks require\na large corpus of training data to generate poisoned samples with the necessary\nsyntactic structures for Trojan insertion. Obtaining such data can be difficult\nfor attackers, and the process of generating syntactic poisoned triggers and\ninserting Trojans can be time-consuming. This paper proposes a solution called\nTrojText, which aims to determine whether invisible textual Trojan attacks can\nbe performed more efficiently and cost-effectively without training data. The\nproposed approach, called the Representation-Logit Trojan Insertion (RLI)\nalgorithm, uses smaller sampled test data instead of large training data to\nachieve the desired attack. The paper also introduces two additional\ntechniques, namely the accumulated gradient ranking (AGR) and Trojan Weights\nPruning (TWP), to reduce the number of tuned parameters and the attack\noverhead. The TrojText approach was evaluated on three datasets (AG's News,\nSST-2, and OLID) using three NLP models (BERT, XLNet, and DeBERTa). The\nexperiments demonstrated that the TrojText approach achieved a 98.35\\%\nclassification accuracy for test sentences in the target class on the BERT\nmodel for the AG's News dataset. The source code for TrojText is available at\nhttps://github.com/UCF-ML-Research/TrojText.", "published": "2023-03-03 22:19:22", "link": "http://arxiv.org/abs/2303.02242v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NCL: Textual Backdoor Defense Using Noise-augmented Contrastive Learning", "abstract": "At present, backdoor attacks attract attention as they do great harm to deep\nlearning models. The adversary poisons the training data making the model being\ninjected with a backdoor after being trained unconsciously by victims using the\npoisoned dataset. In the field of text, however, existing works do not provide\nsufficient defense against backdoor attacks. In this paper, we propose a\nNoise-augmented Contrastive Learning (NCL) framework to defend against textual\nbackdoor attacks when training models with untrustworthy data. With the aim of\nmitigating the mapping between triggers and the target label, we add\nappropriate noise perturbing possible backdoor triggers, augment the training\ndataset, and then pull homology samples in the feature space utilizing\ncontrastive learning objective. Experiments demonstrate the effectiveness of\nour method in defending three types of textual backdoor attacks, outperforming\nthe prior works.", "published": "2023-03-03 07:07:04", "link": "http://arxiv.org/abs/2303.01742v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Meme Sentiment Analysis Enhanced with Multimodal Spatial Encoding and\n  Facial Embedding", "abstract": "Internet memes are characterised by the interspersing of text amongst visual\nelements. State-of-the-art multimodal meme classifiers do not account for the\nrelative positions of these elements across the two modalities, despite the\nlatent meaning associated with where text and visual elements are placed.\nAgainst two meme sentiment classification datasets, we systematically show\nperformance gains from incorporating the spatial position of visual objects,\nfaces, and text clusters extracted from memes. In addition, we also present\nfacial embedding as an impactful enhancement to image representation in a\nmultimodal meme classifier. Finally, we show that incorporating this spatial\ninformation allows our fully automated approaches to outperform their\ncorresponding baselines that rely on additional human validation of\nOCR-extracted text.", "published": "2023-03-03 08:44:20", "link": "http://arxiv.org/abs/2303.01781v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Exploiting Language Relatedness in Machine Translation Through Domain\n  Adaptation Techniques", "abstract": "One of the significant challenges of Machine Translation (MT) is the scarcity\nof large amounts of data, mainly parallel sentence aligned corpora. If the\nevaluation is as rigorous as resource-rich languages, both Neural Machine\nTranslation (NMT) and Statistical Machine Translation (SMT) can produce good\nresults with such large amounts of data. However, it is challenging to improve\nthe quality of MT output for low resource languages, especially in NMT and SMT.\nIn order to tackle the challenges faced by MT, we present a novel approach of\nusing a scaled similarity score of sentences, especially for related languages\nbased on a 5-gram KenLM language model with Kneser-ney smoothing technique for\nfiltering in-domain data from out-of-domain corpora that boost the translation\nquality of MT. Furthermore, we employ other domain adaptation techniques such\nas multi-domain, fine-tuning and iterative back-translation approach to compare\nour novel approach on the Hindi-Nepali language pair for NMT and SMT. Our\napproach succeeds in increasing ~2 BLEU point on multi-domain approach, ~3 BLEU\npoint on fine-tuning for NMT and ~2 BLEU point on iterative back-translation\napproach.", "published": "2023-03-03 09:07:30", "link": "http://arxiv.org/abs/2303.01793v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hitachi at SemEval-2023 Task 3: Exploring Cross-lingual Multi-task\n  Strategies for Genre and Framing Detection in Online News", "abstract": "This paper explains the participation of team Hitachi to SemEval-2023 Task 3\n\"Detecting the genre, the framing, and the persuasion techniques in online news\nin a multi-lingual setup.'' Based on the multilingual, multi-task nature of the\ntask and the low-resource setting, we investigated different cross-lingual and\nmulti-task strategies for training the pretrained language models. Through\nextensive experiments, we found that (a) cross-lingual/multi-task training, and\n(b) collecting an external balanced dataset, can benefit the genre and framing\ndetection. We constructed ensemble models from the results and achieved the\nhighest macro-averaged F1 scores in Italian and Russian genre categorization\nsubtasks.", "published": "2023-03-03 09:12:55", "link": "http://arxiv.org/abs/2303.01794v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong\n  Few-shot Learners", "abstract": "Visual recognition in low-data regimes requires deep neural networks to learn\ngeneralized representations from limited training samples. Recently, CLIP-based\nmethods have shown promising few-shot performance benefited from the\ncontrastive language-image pre-training. We then question, if the more diverse\npre-training knowledge can be cascaded to further assist few-shot\nrepresentation learning. In this paper, we propose CaFo, a Cascade of\nFoundation models that incorporates diverse prior knowledge of various\npre-training paradigms for better few-shot learning. Our CaFo incorporates\nCLIP's language-contrastive knowledge, DINO's vision-contrastive knowledge,\nDALL-E's vision-generative knowledge, and GPT-3's language-generative\nknowledge. Specifically, CaFo works by 'Prompt, Generate, then Cache'. Firstly,\nwe leverage GPT-3 to produce textual inputs for prompting CLIP with rich\ndownstream linguistic semantics. Then, we generate synthetic images via DALL-E\nto expand the few-shot training data without any manpower. At last, we\nintroduce a learnable cache model to adaptively blend the predictions from CLIP\nand DINO. By such collaboration, CaFo can fully unleash the potential of\ndifferent pre-training methods and unify them to perform state-of-the-art for\nfew-shot classification. Code is available at\nhttps://github.com/ZrrSkywalker/CaFo.", "published": "2023-03-03 18:58:16", "link": "http://arxiv.org/abs/2303.02151v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Domain Specific Question Answering Over Knowledge Graphs Using Logical\n  Programming and Large Language Models", "abstract": "Answering questions over domain-specific graphs requires a tailored approach\ndue to the limited number of relations and the specific nature of the domain.\nOur approach integrates classic logical programming languages into large\nlanguage models (LLMs), enabling the utilization of logical reasoning\ncapabilities to tackle the KGQA task. By representing the questions as Prolog\nqueries, which are readable and near close to natural language in\nrepresentation, we facilitate the generation of programmatically derived\nanswers. To validate the effectiveness of our approach, we evaluate it using a\nwell-known benchmark dataset, MetaQA. Our experimental results demonstrate that\nour method achieves accurate identification of correct answer entities for all\ntest questions, even when trained on a small fraction of annotated data.\nOverall, our work presents a promising approach to addressing question\nanswering over domain-specific graphs, offering an explainable and robust\nsolution by incorporating logical programming languages.", "published": "2023-03-03 20:35:38", "link": "http://arxiv.org/abs/2303.02206v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Learning to reason over visual objects", "abstract": "A core component of human intelligence is the ability to identify abstract\npatterns inherent in complex, high-dimensional perceptual data, as exemplified\nby visual reasoning tasks such as Raven's Progressive Matrices (RPM). Motivated\nby the goal of designing AI systems with this capacity, recent work has focused\non evaluating whether neural networks can learn to solve RPM-like problems.\nPrevious work has generally found that strong performance on these problems\nrequires the incorporation of inductive biases that are specific to the RPM\nproblem format, raising the question of whether such models might be more\nbroadly useful. Here, we investigated the extent to which a general-purpose\nmechanism for processing visual scenes in terms of objects might help promote\nabstract visual reasoning. We found that a simple model, consisting only of an\nobject-centric encoder and a transformer reasoning module, achieved\nstate-of-the-art results on both of two challenging RPM-like benchmarks (PGM\nand I-RAVEN), as well as a novel benchmark with greater visual complexity\n(CLEVR-Matrices). These results suggest that an inductive bias for\nobject-centric processing may be a key component of abstract visual reasoning,\nobviating the need for problem-specific inductive biases.", "published": "2023-03-03 23:19:42", "link": "http://arxiv.org/abs/2303.02260v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Multi label classification of Artificial Intelligence related patents\n  using Modified D2SBERT and Sentence Attention mechanism", "abstract": "Patent classification is an essential task in patent information management\nand patent knowledge mining. It is very important to classify patents related\nto artificial intelligence, which is the biggest topic these days. However,\nartificial intelligence-related patents are very difficult to classify because\nit is a mixture of complex technologies and legal terms. Moreover, due to the\nunsatisfactory performance of current algorithms, it is still mostly done\nmanually, wasting a lot of time and money. Therefore, we present a method for\nclassifying artificial intelligence-related patents published by the USPTO\nusing natural language processing technique and deep learning methodology. We\nuse deformed BERT and sentence attention overcome the limitations of BERT. Our\nexperiment result is highest performance compared to other deep learning\nmethods.", "published": "2023-03-03 12:27:24", "link": "http://arxiv.org/abs/2303.03165v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Will Affective Computing Emerge from Foundation Models and General AI? A\n  First Evaluation on ChatGPT", "abstract": "ChatGPT has shown the potential of emerging general artificial intelligence\ncapabilities, as it has demonstrated competent performance across many natural\nlanguage processing tasks. In this work, we evaluate the capabilities of\nChatGPT to perform text classification on three affective computing problems,\nnamely, big-five personality prediction, sentiment analysis, and suicide\ntendency detection. We utilise three baselines, a robust language model\n(RoBERTa-base), a legacy word model with pretrained embeddings (Word2Vec), and\na simple bag-of-words baseline (BoW). Results show that the RoBERTa trained for\na specific downstream task generally has a superior performance. On the other\nhand, ChatGPT provides decent results, and is relatively comparable to the\nWord2Vec and BoW baselines. ChatGPT further shows robustness against noisy\ndata, where Word2Vec models achieve worse results due to noise. Results\nindicate that ChatGPT is a good generalist model that is capable of achieving\ngood results across various problems without any specialised training, however,\nit is not as good as a specialised model for a downstream task.", "published": "2023-03-03 16:11:37", "link": "http://arxiv.org/abs/2303.03186v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can ChatGPT-like Generative Models Guarantee Factual Accuracy? On the\n  Mistakes of New Generation Search Engines", "abstract": "Although large conversational AI models such as OpenAI's ChatGPT have\ndemonstrated great potential, we question whether such models can guarantee\nfactual accuracy. Recently, technology companies such as Microsoft and Google\nhave announced new services which aim to combine search engines with\nconversational AI. However, we have found numerous mistakes in the public\ndemonstrations that suggest we should not easily trust the factual claims of\nthe AI models. Rather than criticizing specific models or companies, we hope to\ncall on researchers and developers to improve AI models' transparency and\nfactual correctness.", "published": "2023-03-03 04:27:44", "link": "http://arxiv.org/abs/2304.11076v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "APIContext2Com: Code Comment Generation by Incorporating Pre-Defined API\n  Documentation", "abstract": "Code comments are significantly helpful in comprehending software programs\nand also aid developers to save a great deal of time in software maintenance.\nCode comment generation aims to automatically predict comments in natural\nlanguage given a code snippet. Several works investigate the effect of\nintegrating external knowledge on the quality of generated comments. In this\nstudy, we propose a solution, namely APIContext2Com, to improve the\neffectiveness of generated comments by incorporating the pre-defined\nApplication Programming Interface (API) context. The API context includes the\ndefinition and description of the pre-defined APIs that are used within the\ncode snippets. As the detailed API information expresses the functionality of a\ncode snippet, it can be helpful in better generating the code summary. We\nintroduce a seq-2-seq encoder-decoder neural network model with different sets\nof multiple encoders to effectively transform distinct inputs into target\ncomments. A ranking mechanism is also developed to exclude non-informative\nAPIs, so that we can filter out unrelated APIs. We evaluate our approach using\nthe Java dataset from CodeSearchNet. The findings reveal that the proposed\nmodel improves the best baseline by 1.88 (8.24 %), 2.16 (17.58 %), 1.38 (18.3\n%), 0.73 (14.17 %), 1.58 (14.98 %) and 1.9 (6.92 %) for BLEU1, BLEU2, BLEU3,\nBLEU4, METEOR, ROUGE-L respectively. Human evaluation and ablation studies\nconfirm the quality of the generated comments and the effect of architecture\nand ranking APIs.", "published": "2023-03-03 00:38:01", "link": "http://arxiv.org/abs/2303.01645v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "DWFormer: Dynamic Window transFormer for Speech Emotion Recognition", "abstract": "Speech emotion recognition is crucial to human-computer interaction. The\ntemporal regions that represent different emotions scatter in different parts\nof the speech locally. Moreover, the temporal scales of important information\nmay vary over a large range within and across speech segments. Although\ntransformer-based models have made progress in this field, the existing models\ncould not precisely locate important regions at different temporal scales. To\naddress the issue, we propose Dynamic Window transFormer (DWFormer), a new\narchitecture that leverages temporal importance by dynamically splitting\nsamples into windows. Self-attention mechanism is applied within windows for\ncapturing temporal important information locally in a fine-grained way.\nCross-window information interaction is also taken into account for global\ncommunication. DWFormer is evaluated on both the IEMOCAP and the MELD datasets.\nExperimental results show that the proposed model achieves better performance\nthan the previous state-of-the-art methods.", "published": "2023-03-03 03:26:53", "link": "http://arxiv.org/abs/2303.01694v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Prophet: Prompting Large Language Models with Complementary Answer\n  Heuristics for Knowledge-based Visual Question Answering", "abstract": "Knowledge-based visual question answering (VQA) requires external knowledge\nbeyond the image to answer the question. Early studies retrieve required\nknowledge from explicit knowledge bases (KBs), which often introduces\nirrelevant information to the question, hence restricting the performance of\ntheir models. Recent works have resorted to using a powerful large language\nmodel (LLM) as an implicit knowledge engine to acquire the necessary knowledge\nfor answering. Despite the encouraging results achieved by these methods, we\nargue that they have not fully activated the capacity of the blind LLM as the\nprovided textual input is insufficient to depict the required visual\ninformation to answer the question. In this paper, we present Prophet -- a\nconceptually simple, flexible, and general framework designed to prompt LLM\nwith answer heuristics for knowledge-based VQA. Specifically, we first train a\nvanilla VQA model on a specific knowledge-based VQA dataset without external\nknowledge. After that, we extract two types of complementary answer heuristics\nfrom the VQA model: answer candidates and answer-aware examples. Finally, the\ntwo types of answer heuristics are jointly encoded into a formatted prompt to\nfacilitate the LLM's understanding of both the image and question, thus\ngenerating a more accurate answer. By incorporating the state-of-the-art LLM\nGPT-3, Prophet significantly outperforms existing state-of-the-art methods on\nfour challenging knowledge-based VQA datasets. To demonstrate the generality of\nour approach, we instantiate Prophet with the combinations of different VQA\nmodels (i.e., both discriminative and generative ones) and different LLMs\n(i.e., both commercial and open-source ones).", "published": "2023-03-03 13:05:15", "link": "http://arxiv.org/abs/2303.01903v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Pre-trained Model Representations and their Robustness against Noise for\n  Speech Emotion Analysis", "abstract": "Pre-trained model representations have demonstrated state-of-the-art\nperformance in speech recognition, natural language processing, and other\napplications. Speech models, such as Bidirectional Encoder Representations from\nTransformers (BERT) and Hidden units BERT (HuBERT), have enabled generating\nlexical and acoustic representations to benefit speech recognition\napplications. We investigated the use of pre-trained model representations for\nestimating dimensional emotions, such as activation, valence, and dominance,\nfrom speech. We observed that while valence may rely heavily on lexical\nrepresentations, activation and dominance rely mostly on acoustic information.\nIn this work, we used multi-modal fusion representations from pre-trained\nmodels to generate state-of-the-art speech emotion estimation, and we showed a\n100% and 30% relative improvement in concordance correlation coefficient (CCC)\non valence estimation compared to standard acoustic and lexical baselines.\nFinally, we investigated the robustness of pre-trained model representations\nagainst noise and reverberation degradation and noticed that lexical and\nacoustic representations are impacted differently. We discovered that lexical\nrepresentations are more robust to distortions compared to acoustic\nrepresentations, and demonstrated that knowledge distillation from a\nmulti-modal model helps to improve the noise-robustness of acoustic-based\nmodels.", "published": "2023-03-03 18:22:32", "link": "http://arxiv.org/abs/2303.03177v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-End Speech Recognition: A Survey", "abstract": "In the last decade of automatic speech recognition (ASR) research, the\nintroduction of deep learning brought considerable reductions in word error\nrate of more than 50% relative, compared to modeling without deep learning. In\nthe wake of this transition, a number of all-neural ASR architectures were\nintroduced. These so-called end-to-end (E2E) models provide highly integrated,\ncompletely neural ASR models, which rely strongly on general machine learning\nknowledge, learn more consistently from data, while depending less on ASR\ndomain-specific experience. The success and enthusiastic adoption of deep\nlearning accompanied by more generic model architectures lead to E2E models now\nbecoming the prominent ASR approach. The goal of this survey is to provide a\ntaxonomy of E2E ASR models and corresponding improvements, and to discuss their\nproperties and their relation to the classical hidden Markov model (HMM) based\nASR architecture. All relevant aspects of E2E ASR are covered in this work:\nmodeling, training, decoding, and external language model integration,\naccompanied by discussions of performance and deployment opportunities, as well\nas an outlook into potential future developments.", "published": "2023-03-03 01:46:41", "link": "http://arxiv.org/abs/2303.03329v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer", "abstract": "Knowledge graphs (KG) are essential background knowledge providers in many\ntasks. When designing models for KG-related tasks, one of the key tasks is to\ndevise the Knowledge Representation and Fusion (KRF) module that learns the\nrepresentation of elements from KGs and fuses them with task representations.\nWhile due to the difference of KGs and perspectives to be considered during\nfusion across tasks, duplicate and ad hoc KRF modules design are conducted\namong tasks. In this paper, we propose a novel knowledge graph pretraining\nmodel KGTransformer that could serve as a uniform KRF module in diverse\nKG-related tasks. We pretrain KGTransformer with three self-supervised tasks\nwith sampled sub-graphs as input. For utilization, we propose a general\nprompt-tuning mechanism regarding task data as a triple prompt to allow\nflexible interactions between task KGs and task data. We evaluate pretrained\nKGTransformer on three tasks, triple classification, zero-shot image\nclassification, and question answering. KGTransformer consistently achieves\nbetter results than specifically designed task models. Through experiments, we\njustify that the pretrained KGTransformer could be used off the shelf as a\ngeneral and effective KRF module across KG-related tasks. The code and datasets\nare available at https://github.com/zjukg/KGTransformer.", "published": "2023-03-03 02:58:17", "link": "http://arxiv.org/abs/2303.03922v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Early Warning Signals of Social Instabilities in Twitter Data", "abstract": "The goal of this project is to create and study novel techniques to identify\nearly warning signals for socially disruptive events, like riots, wars, or\nrevolutions using only publicly available data on social media. Such techniques\nneed to be robust enough to work on real-time data: to achieve this goal we\npropose a topological approach together with more standard BERT models. Indeed,\ntopology-based algorithms, being provably stable against deformations and\nnoise, seem to work well in low-data regimes. The general idea is to build a\nbinary classifier that predicts if a given tweet is related to a disruptive\nevent or not. The results indicate that the persistent-gradient approach is\nstable and even more performant than deep-learning-based anomaly detection\nalgorithms. We also benchmark the generalisability of the methodology against\nout-of-samples tasks, with very promising results.", "published": "2023-03-03 11:18:02", "link": "http://arxiv.org/abs/2303.05401v1", "categories": ["cs.CL", "cs.LG", "cs.SI", "68"], "primary_category": "cs.CL"}
{"title": "Cryptocurrency Price Prediction using Twitter Sentiment Analysis", "abstract": "The cryptocurrency ecosystem has been the centre of discussion on many social\nmedia platforms, following its noted volatility and varied opinions. Twitter is\nrapidly being utilised as a news source and a medium for bitcoin discussion.\nOur algorithm seeks to use historical prices and sentiment of tweets to\nforecast the price of Bitcoin. In this study, we develop an end-to-end model\nthat can forecast the sentiment of a set of tweets (using a Bidirectional\nEncoder Representations from Transformers - based Neural Network Model) and\nforecast the price of Bitcoin (using Gated Recurrent Unit) using the predicted\nsentiment and other metrics like historical cryptocurrency price data, tweet\nvolume, a user's following, and whether or not a user is verified. The\nsentiment prediction gave a Mean Absolute Percentage Error of 9.45%, an average\nof real-time data, and test data. The mean absolute percent error for the price\nprediction was 3.6%.", "published": "2023-03-03 18:42:01", "link": "http://arxiv.org/abs/2303.09397v1", "categories": ["q-fin.ST", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "Unified Keyword Spotting and Audio Tagging on Mobile Devices with\n  Transformers", "abstract": "Keyword spotting (KWS) is a core human-machine-interaction front-end task for\nmost modern intelligent assistants. Recently, a unified (UniKW-AT) framework\nhas been proposed that adds additional capabilities in the form of audio\ntagging (AT) to a KWS model. However, previous work did not consider the\nreal-world deployment of a UniKW-AT model, where factors such as model size and\ninference speed are more important than performance alone. This work introduces\nthree mobile-device deployable models named Unified Transformers (UiT). Our\nbest model achieves an mAP of 34.09 on Audioset, and an accuracy of 97.76 on\nthe public Google Speech Commands V1 dataset. Further, we benchmark our\nproposed approaches on four mobile platforms, revealing that the proposed UiT\nmodels can achieve a speedup of 2 - 6 times against a competitive MobileNetV2.", "published": "2023-03-03 09:38:53", "link": "http://arxiv.org/abs/2303.01812v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An investigation into the adaptability of a diffusion-based TTS model", "abstract": "Given the recent success of diffusion in producing natural-sounding synthetic\nspeech, we investigate how diffusion can be used in speaker adaptive TTS.\nTaking cues from more traditional adaptation approaches, we show that\nadaptation can be included in a diffusion pipeline using conditional layer\nnormalization with a step embedding. However, we show experimentally that,\nwhilst the approach has merit, such adaptation alone cannot approach the\nperformance of Transformer-based techniques. In a second experiment, we show\nthat diffusion can be optimally combined with Transformer, with the latter\ntaking the bulk of the adaptation load and the former contributing to improved\nnaturalness.", "published": "2023-03-03 11:06:20", "link": "http://arxiv.org/abs/2303.01849v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Spectrogram Inversion for Audio Source Separation via Consistency,\n  Mixing, and Magnitude Constraints", "abstract": "Audio source separation is often achieved by estimating the magnitude\nspectrogram of each source, and then applying a phase recovery (or spectrogram\ninversion) algorithm to retrieve time-domain signals. Typically, spectrogram\ninversion is treated as an optimization problem involving one or several terms\nin order to promote estimates that comply with a consistency property, a mixing\nconstraint, and/or a target magnitude objective. Nonetheless, it is still\nunclear which set of constraints and problem formulation is the most\nappropriate in practice. In this paper, we design a general framework for\nderiving spectrogram inversion algorithm, which is based on formulating\noptimization problems by combining these objectives either as soft penalties or\nhard constraints. We solve these by means of algorithms that perform\nalternating projections on the subsets corresponding to each\nobjective/constraint. Our framework encompasses existing techniques from the\nliterature as well as novel algorithms. We investigate the potential of these\napproaches for a speech enhancement task. In particular, one of our novel\nalgorithms outperforms other approaches in a realistic setting where the\nmagnitudes are estimated beforehand using a neural network.", "published": "2023-03-03 11:40:14", "link": "http://arxiv.org/abs/2303.01864v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Decoding and Visualising Intended Emotion in an Expressive Piano\n  Performance", "abstract": "Expert musicians can mould a musical piece to convey specific emotions that\nthey intend to communicate. In this paper, we place a mid-level features based\nmusic emotion model in this performer-to-listener communication scenario, and\ndemonstrate via a small visualisation music emotion decoding in real time. We\nalso extend the existing set of mid-level features using analogues of\nperceptual speed and perceived dynamics.", "published": "2023-03-03 12:10:54", "link": "http://arxiv.org/abs/2303.01875v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Low-Complexity Audio Embedding Extractors", "abstract": "Solving tasks such as speaker recognition, music classification, or semantic\naudio event tagging with deep learning models typically requires\ncomputationally demanding networks. General-purpose audio embeddings (GPAEs)\nare dense representations of audio signals that allow lightweight, shallow\nclassifiers to tackle various audio tasks. The idea is that a single complex\nfeature extractor would extract dense GPAEs, while shallow MLPs can produce\ntask-specific predictions. If the extracted dense representations are general\nenough to allow the simple downstream classifiers to generalize to a variety of\ntasks in the audio domain, a single costly forward pass suffices to solve\nmultiple tasks in parallel. In this work, we try to reduce the cost of GPAE\nextractors to make them suitable for resource-constrained devices. We use\nefficient MobileNets trained on AudioSet using Knowledge Distillation from a\nTransformer ensemble as efficient GPAE extractors. We explore how to obtain\nhigh-quality GPAEs from the model, study how model complexity relates to the\nquality of extracted GPAEs, and conclude that low-complexity models can\ngenerate competitive GPAEs, paving the way for analyzing audio streams on edge\ndevices w.r.t. multiple audio classification and recognition tasks.", "published": "2023-03-03 12:17:48", "link": "http://arxiv.org/abs/2303.01879v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "WESPER: Zero-shot and Realtime Whisper to Normal Voice Conversion for\n  Whisper-based Speech Interactions", "abstract": "Recognizing whispered speech and converting it to normal speech creates many\npossibilities for speech interaction. Because the sound pressure of whispered\nspeech is significantly lower than that of normal speech, it can be used as a\nsemi-silent speech interaction in public places without being audible to\nothers. Converting whispers to normal speech also improves the speech quality\nfor people with speech or hearing impairments. However, conventional speech\nconversion techniques do not provide sufficient conversion quality or require\nspeaker-dependent datasets consisting of pairs of whispered and normal speech\nutterances. To address these problems, we propose WESPER, a zero-shot,\nreal-time whisper-to-normal speech conversion mechanism based on\nself-supervised learning. WESPER consists of a speech-to-unit (STU) encoder,\nwhich generates hidden speech units common to both whispered and normal speech,\nand a unit-to-speech (UTS) decoder, which reconstructs speech from the encoded\nspeech units. Unlike the existing methods, this conversion is user-independent\nand does not require a paired dataset for whispered and normal speech. The UTS\ndecoder can reconstruct speech in any target speaker's voice from speech units,\nand it requires only an unlabeled target speaker's speech data. We confirmed\nthat the quality of the speech converted from a whisper was improved while\npreserving its natural prosody. Additionally, we confirmed the effectiveness of\nthe proposed approach to perform speech reconstruction for people with speech\nor hearing disabilities. (project page: http://lab.rekimoto.org/projects/wesper\n)", "published": "2023-03-03 00:10:25", "link": "http://arxiv.org/abs/2303.01639v1", "categories": ["cs.SD", "cs.HC", "eess.AS", "H.5.2; H.1.2; I.2.0; I.3.6"], "primary_category": "cs.SD"}
{"title": "Miipher: A Robust Speech Restoration Model Integrating Self-Supervised\n  Speech and Text Representations", "abstract": "Speech restoration (SR) is a task of converting degraded speech signals into\nhigh-quality ones. In this study, we propose a robust SR model called Miipher,\nand apply Miipher to a new SR application: increasing the amount of\nhigh-quality training data for speech generation by converting speech samples\ncollected from the Web to studio-quality. To make our SR model robust against\nvarious degradation, we use (i) a speech representation extracted from w2v-BERT\nfor the input feature, and (ii) a text representation extracted from\ntranscripts via PnG-BERT as a linguistic conditioning feature. Experiments show\nthat Miipher (i) is robust against various audio degradation and (ii) enable us\nto train a high-quality text-to-speech (TTS) model from restored speech samples\ncollected from the Web. Audio samples are available at our demo page:\ngoogle.github.io/df-conformer/miipher/", "published": "2023-03-03 01:57:16", "link": "http://arxiv.org/abs/2303.01664v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LooperGP: A Loopable Sequence Model for Live Coding Performance using\n  GuitarPro Tablature", "abstract": "Despite their impressive offline results, deep learning models for symbolic\nmusic generation are not widely used in live performances due to a deficit of\nmusically meaningful control parameters and a lack of structured musical form\nin their outputs. To address these issues we introduce LooperGP, a method for\nsteering a Transformer-XL model towards generating loopable musical phrases of\na specified number of bars and time signature, enabling a tool for live coding\nperformances. We show that by training LooperGP on a dataset of 93,681 musical\nloops extracted from the DadaGP dataset, we are able to steer its generative\noutput towards generating 3x as many loopable phrases as our baseline. In a\nsubjective listening test conducted by 31 participants, LooperGP loops achieved\npositive median ratings in originality, musical coherence and loop smoothness,\ndemonstrating its potential as a performance tool.", "published": "2023-03-03 02:00:49", "link": "http://arxiv.org/abs/2303.01665v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AutoMatch: A Large-scale Audio Beat Matching Benchmark for Boosting Deep\n  Learning Assistant Video Editing", "abstract": "The explosion of short videos has dramatically reshaped the manners people\nsocialize, yielding a new trend for daily sharing and access to the latest\ninformation. These rich video resources, on the one hand, benefited from the\npopularization of portable devices with cameras, but on the other, they can not\nbe independent of the valuable editing work contributed by numerous video\ncreators. In this paper, we investigate a novel and practical problem, namely\naudio beat matching (ABM), which aims to recommend the proper transition time\nstamps based on the background music. This technique helps to ease the\nlabor-intensive work during video editing, saving energy for creators so that\nthey can focus more on the creativity of video content. We formally define the\nABM problem and its evaluation protocol. Meanwhile, a large-scale audio\ndataset, i.e., the AutoMatch with over 87k finely annotated background music,\nis presented to facilitate this newly opened research direction. To further lay\nsolid foundations for the following study, we also propose a novel model termed\nBeatX to tackle this challenging task. Alongside, we creatively present the\nconcept of label scope, which eliminates the data imbalance issues and assigns\nadaptive weights for the ground truth during the training procedure in one\nstop. Though plentiful short video platforms have flourished for a long time,\nthe relevant research concerning this scenario is not sufficient, and to the\nbest of our knowledge, AutoMatch is the first large-scale dataset to tackle the\naudio beat matching problem. We hope the released dataset and our competitive\nbaseline can encourage more attention to this line of research. The dataset and\ncodes will be made publicly available.", "published": "2023-03-03 12:30:09", "link": "http://arxiv.org/abs/2303.01884v1", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Utilizing synthetic training data for the supervised classification of\n  rat ultrasonic vocalizations", "abstract": "Murine rodents generate ultrasonic vocalizations (USVs) with frequencies that\nextend to around 120kHz. These calls are important in social behaviour, and so\ntheir analysis can provide insights into the function of vocal communication,\nand its dysfunction. The manual identification of USVs, and subsequent\nclassification into different subcategories is time consuming. Although machine\nlearning approaches for identification and classification can lead to enormous\nefficiency gains, the time and effort required to generate training data can be\nhigh, and the accuracy of current approaches can be problematic. Here we\ncompare the detection and classification performance of a trained human against\ntwo convolutional neural networks (CNNs), DeepSqueak and VocalMat, on audio\ncontaining rat USVs. Furthermore, we test the effect of inserting synthetic\nUSVs into the training data of the VocalMat CNN as a means of reducing the\nworkload associated with generating a training set. Our results indicate that\nVocalMat outperformed the DeepSqueak CNN on measures of call identification,\nand classification. Additionally, we found that the augmentation of training\ndata with synthetic images resulted in a further improvement in accuracy, such\nthat it was sufficiently close to human performance to allow for the use of\nthis software in laboratory conditions.", "published": "2023-03-03 03:17:45", "link": "http://arxiv.org/abs/2303.03183v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SottoVoce: An Ultrasound Imaging-Based Silent Speech Interaction Using\n  Deep Neural Networks", "abstract": "The availability of digital devices operated by voice is expanding rapidly.\nHowever, the applications of voice interfaces are still restricted. For\nexample, speaking in public places becomes an annoyance to the surrounding\npeople, and secret information should not be uttered. Environmental noise may\nreduce the accuracy of speech recognition. To address these limitations, a\nsystem to detect a user's unvoiced utterance is proposed. From internal\ninformation observed by an ultrasonic imaging sensor attached to the underside\nof the jaw, our proposed system recognizes the utterance contents without the\nuser's uttering voice. Our proposed deep neural network model is used to obtain\nacoustic features from a sequence of ultrasound images. We confirmed that audio\nsignals generated by our system can control the existing smart speakers. We\nalso observed that a user can adjust their oral movement to learn and improve\nthe accuracy of their voice recognition.", "published": "2023-03-03 07:46:35", "link": "http://arxiv.org/abs/2303.01758v1", "categories": ["cs.HC", "cs.LG", "cs.SD", "eess.AS", "eess.IV", "H.1.2; I.2.1; I.2.7"], "primary_category": "cs.HC"}
