{"title": "Description Based Text Classification with Reinforcement Learning", "abstract": "The task of text classification is usually divided into two stages: {\\it text\nfeature extraction} and {\\it classification}. In this standard formalization\ncategories are merely represented as indexes in the label vocabulary, and the\nmodel lacks for explicit instructions on what to classify. Inspired by the\ncurrent trend of formalizing NLP problems as question answering tasks, we\npropose a new framework for text classification, in which each category label\nis associated with a category description. Descriptions are generated by\nhand-crafted templates or using abstractive/extractive models from\nreinforcement learning. The concatenation of the description and the text is\nfed to the classifier to decide whether or not the current label should be\nassigned to the text. The proposed strategy forces the model to attend to the\nmost salient texts with respect to the label, which can be regarded as a hard\nversion of attention, leading to better performances. We observe significant\nperformance boosts over strong baselines on a wide range of text classification\ntasks including single-label classification, multi-label classification and\nmulti-aspect sentiment analysis.", "published": "2020-02-08 02:14:28", "link": "http://arxiv.org/abs/2002.03067v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LAVA NAT: A Non-Autoregressive Translation Model with Look-Around\n  Decoding and Vocabulary Attention", "abstract": "Non-autoregressive translation (NAT) models generate multiple tokens in one\nforward pass and is highly efficient at inference stage compared with\nautoregressive translation (AT) methods. However, NAT models often suffer from\nthe multimodality problem, i.e., generating duplicated tokens or missing\ntokens. In this paper, we propose two novel methods to address this issue, the\nLook-Around (LA) strategy and the Vocabulary Attention (VA) mechanism. The\nLook-Around strategy predicts the neighbor tokens in order to predict the\ncurrent token, and the Vocabulary Attention models long-term token dependencies\ninside the decoder by attending the whole vocabulary for each position to\nacquire knowledge of which token is about to generate. %We also propose a\ndynamic bidirectional decoding approach to accelerate the inference process of\nthe LAVA model while preserving the high-quality of the generated output. Our\nproposed model uses significantly less time during inference compared with\nautoregressive models and most other NAT models. Our experiments on four\nbenchmarks (WMT14 En$\\rightarrow$De, WMT14 De$\\rightarrow$En, WMT16\nRo$\\rightarrow$En and IWSLT14 De$\\rightarrow$En) show that the proposed model\nachieves competitive performance compared with the state-of-the-art\nnon-autoregressive and autoregressive models while significantly reducing the\ntime cost in inference phase.", "published": "2020-02-08 04:11:03", "link": "http://arxiv.org/abs/2002.03084v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HHH: An Online Medical Chatbot System based on Knowledge Graph and\n  Hierarchical Bi-Directional Attention", "abstract": "This paper proposes a chatbot framework that adopts a hybrid model which\nconsists of a knowledge graph and a text similarity model. Based on this\nchatbot framework, we build HHH, an online question-and-answer (QA) Healthcare\nHelper system for answering complex medical questions. HHH maintains a\nknowledge graph constructed from medical data collected from the Internet. HHH\nalso implements a novel text representation and similarity deep learning model,\nHierarchical BiLSTM Attention Model (HBAM), to find the most similar question\nfrom a large QA dataset. We compare HBAM with other state-of-the-art language\nmodels such as bidirectional encoder representation from transformers (BERT)\nand Manhattan LSTM Model (MaLSTM). We train and test the models with a subset\nof the Quora duplicate questions dataset in the medical area. The experimental\nresults show that our model is able to achieve a superior performance than\nthese existing methods.", "published": "2020-02-08 11:06:27", "link": "http://arxiv.org/abs/2002.03140v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mining Commonsense Facts from the Physical World", "abstract": "Textual descriptions of the physical world implicitly mention commonsense\nfacts, while the commonsense knowledge bases explicitly represent such facts as\ntriples. Compared to dramatically increased text data, the coverage of existing\nknowledge bases is far away from completion. Most of the prior studies on\npopulating knowledge bases mainly focus on Freebase. To automatically complete\ncommonsense knowledge bases to improve their coverage is under-explored. In\nthis paper, we propose a new task of mining commonsense facts from the raw text\nthat describes the physical world. We build an effective new model that fuses\ninformation from both sequence text and existing knowledge base resource. Then\nwe create two large annotated datasets each with approximate 200k instances for\ncommonsense knowledge base completion. Empirical results demonstrate that our\nmodel significantly outperforms baselines.", "published": "2020-02-08 12:02:45", "link": "http://arxiv.org/abs/2002.03149v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "autoNLP: NLP Feature Recommendations for Text Analytics Applications", "abstract": "While designing machine learning based text analytics applications, often,\nNLP data scientists manually determine which NLP features to use based upon\ntheir knowledge and experience with related problems. This results in increased\nefforts during feature engineering process and renders automated reuse of\nfeatures across semantically related applications inherently difficult. In this\npaper, we argue for standardization in feature specification by outlining\nstructure of a language for specifying NLP features and present an approach for\ntheir reuse across applications to increase likelihood of identifying optimal\nfeatures.", "published": "2020-02-08 00:42:21", "link": "http://arxiv.org/abs/2002.03056v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Blank Language Models", "abstract": "We propose Blank Language Model (BLM), a model that generates sequences by\ndynamically creating and filling in blanks. The blanks control which part of\nthe sequence to expand, making BLM ideal for a variety of text editing and\nrewriting tasks. The model can start from a single blank or partially completed\ntext with blanks at specified locations. It iteratively determines which word\nto place in a blank and whether to insert new blanks, and stops generating when\nno blanks are left to fill. BLM can be efficiently trained using a lower bound\nof the marginal data likelihood. On the task of filling missing text snippets,\nBLM significantly outperforms all other baselines in terms of both accuracy and\nfluency. Experiments on style transfer and damaged ancient text restoration\ndemonstrate the potential of this framework for a wide range of applications.", "published": "2020-02-08 03:41:37", "link": "http://arxiv.org/abs/2002.03079v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SPA: Verbal Interactions between Agents and Avatars in Shared Virtual\n  Environments using Propositional Planning", "abstract": "We present a novel approach for generating plausible verbal interactions\nbetween virtual human-like agents and user avatars in shared virtual\nenvironments. Sense-Plan-Ask, or SPA, extends prior work in propositional\nplanning and natural language processing to enable agents to plan with\nuncertain information, and leverage question and answer dialogue with other\nagents and avatars to obtain the needed information and complete their goals.\nThe agents are additionally able to respond to questions from the avatars and\nother agents using natural-language enabling real-time multi-agent multi-avatar\ncommunication environments.\n  Our algorithm can simulate tens of virtual agents at interactive rates\ninteracting, moving, communicating, planning, and replanning. We find that our\nalgorithm creates a small runtime cost and enables agents to complete their\ngoals more effectively than agents without the ability to leverage\nnatural-language communication. We demonstrate quantitative results on a set of\nsimulated benchmarks and detail the results of a preliminary user-study\nconducted to evaluate the plausibility of the virtual interactions generated by\nSPA. Overall, we find that participants prefer SPA to prior techniques in 84\\%\nof responses including significant benefits in terms of the plausibility of\nnatural-language interactions and the positive impact of those interactions.", "published": "2020-02-08 23:15:06", "link": "http://arxiv.org/abs/2002.03246v1", "categories": ["cs.MA", "cs.CL"], "primary_category": "cs.MA"}
{"title": "Time-aware Large Kernel Convolutions", "abstract": "To date, most state-of-the-art sequence modeling architectures use attention\nto build generative models for language based tasks. Some of these models use\nall the available sequence tokens to generate an attention distribution which\nresults in time complexity of $O(n^2)$. Alternatively, they utilize depthwise\nconvolutions with softmax normalized kernels of size $k$ acting as a\nlimited-window self-attention, resulting in time complexity of $O(k{\\cdot}n)$.\nIn this paper, we introduce Time-aware Large Kernel (TaLK) Convolutions, a\nnovel adaptive convolution operation that learns to predict the size of a\nsummation kernel instead of using a fixed-sized kernel matrix. This method\nyields a time complexity of $O(n)$, effectively making the sequence encoding\nprocess linear to the number of tokens. We evaluate the proposed method on\nlarge-scale standard machine translation, abstractive summarization and\nlanguage modeling datasets and show that TaLK Convolutions constitute an\nefficient improvement over other attention/convolution based approaches.", "published": "2020-02-08 15:30:28", "link": "http://arxiv.org/abs/2002.03184v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Time-Frequency Perspective on Audio Watermarking", "abstract": "Existing audio watermarking methods usually treat the host audio signals of a\nfunction of time or frequency individually, while considering them in the joint\ntime-frequency (TF) domain has received less attention. This paper proposes an\naudio watermarking framework from the perspective of TF analysis. The proposed\nframework treats the host audio signal in the 2-dimensional (2D) TF plane, and\nselects a series of patches within the 2D TF image. These patches correspond to\nthe TF clusters with minimum averaged energy, and are used to form the feature\nvectors for watermark embedding. Classical spread spectrum embedding schemes\nare incorporated in the framework. The feature patches that carry the\nwatermarks only occupy a few TF regions of the host audio signal, thus leading\nto improved imperceptibility property. In addition, since the feature patches\ncontain a neighborhood area of TF representation of audio samples, the\ncorrelations among the samples within a single patch could be exploited for\nimproved robustness against a series of processing attacks. Extensive\nexperiments are carried out to illustrate the effectiveness of the proposed\nsystem, as compared to its counterpart systems. The aim of this work is to shed\nsome light on the notion of audio watermarking in TF feature domain, which may\npotentially lead us to more robust watermarking solutions against malicious\nattacks.", "published": "2020-02-08 13:02:24", "link": "http://arxiv.org/abs/2002.03156v1", "categories": ["cs.MM", "eess.AS"], "primary_category": "cs.MM"}
{"title": "RL-Duet: Online Music Accompaniment Generation Using Deep Reinforcement\n  Learning", "abstract": "This paper presents a deep reinforcement learning algorithm for online\naccompaniment generation, with potential for real-time interactive\nhuman-machine duet improvisation. Different from offline music generation and\nharmonization, online music accompaniment requires the algorithm to respond to\nhuman input and generate the machine counterpart in a sequential order. We cast\nthis as a reinforcement learning problem, where the generation agent learns a\npolicy to generate a musical note (action) based on previously generated\ncontext (state). The key of this algorithm is the well-functioning reward\nmodel. Instead of defining it using music composition rules, we learn this\nmodel from monophonic and polyphonic training data. This model considers the\ncompatibility of the machine-generated note with both the machine-generated\ncontext and the human-generated context. Experiments show that this algorithm\nis able to respond to the human part and generate a melodic, harmonic and\ndiverse machine part. Subjective evaluations on preferences show that the\nproposed algorithm generates music pieces of higher quality than the baseline\nmethod.", "published": "2020-02-08 03:53:52", "link": "http://arxiv.org/abs/2002.03082v1", "categories": ["cs.LG", "cs.HC", "eess.AS"], "primary_category": "cs.LG"}
