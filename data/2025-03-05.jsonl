{"title": "Enhancing Collective Intelligence in Large Language Models Through Emotional Integration", "abstract": "This research investigates the integration of emotional diversity into Large\nLanguage Models (LLMs) to enhance collective intelligence. Inspired by the\nhuman wisdom of crowds phenomenon, where group decisions often outperform\nindividual judgments, we fine-tuned the DarkIdol-Llama-3.1-8B model using\nGoogle's GoEmotions dataset and Low-Rank Adaptation (LoRA) to simulate\nemotionally diverse responses. Evaluating the model on a distance estimation\ntask between Fargo, ND, and Seattle, WA, across 15,064 unique persona\nconfigurations, we analyzed how emotional states and social attributes\ninfluence decision-making. Our findings demonstrate that emotional integration\nshapes response patterns while maintaining acceptable prediction accuracy,\nrevealing its potential to enhance artificial collective intelligence. This\nstudy provides valuable insights into the interplay of emotional diversity and\ndecision-making in LLMs, suggesting pathways for creating emotionally aware AI\nsystems that balance emotional depth with analytical precision.", "published": "2025-03-05 23:42:48", "link": "http://arxiv.org/abs/2503.04849v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Preliminary Report: Enhancing Role Differentiation in Conversational HCI Through Chromostereopsis", "abstract": "We propose leveraging chromostereopsis, a perceptual phenomenon inducing\ndepth perception through color contrast, as a novel approach to visually\ndifferentiating conversational roles in text-based AI interfaces. This method\naims to implicitly communicate role hierarchy and add a subtle sense of\nphysical space.", "published": "2025-03-05 23:40:10", "link": "http://arxiv.org/abs/2503.03968v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "On the Acquisition of Shared Grammatical Representations in Bilingual Language Models", "abstract": "While crosslingual transfer is crucial to contemporary language models'\nmultilingual capabilities, how it occurs is not well understood. In this paper,\nwe ask what happens to a monolingual language model when it begins to be\ntrained on a second language. Specifically, we train small bilingual models for\nwhich we control the amount of data for each language and the order of language\nexposure. To find evidence of shared multilingual representations, we turn to\nstructural priming, a method used to study grammatical representations in\nhumans. We first replicate previous crosslingual structural priming results and\nfind that after controlling for training data quantity and language exposure,\nthere are asymmetrical effects across language pairs and directions. We argue\nthat this asymmetry may shape hypotheses about human structural priming\neffects. We also find that structural priming effects are less robust for less\nsimilar language pairs, highlighting potential limitations of crosslingual\ntransfer learning and shared representations for typologically diverse\nlanguages.", "published": "2025-03-05 23:27:58", "link": "http://arxiv.org/abs/2503.03962v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Performance Comparison of Large Language Models on Advanced Calculus Problems", "abstract": "This paper presents an in-depth analysis of the performance of seven\ndifferent Large Language Models (LLMs) in solving a diverse set of math\nadvanced calculus problems. The study aims to evaluate these models' accuracy,\nreliability, and problem-solving capabilities, including ChatGPT 4o, Gemini\nAdvanced with 1.5 Pro, Copilot Pro, Claude 3.5 Sonnet, Meta AI, Mistral AI, and\nPerplexity. The assessment was conducted through a series of thirty-two test\nproblems, encompassing a total of 320 points. The problems covered various\ntopics, from vector calculations and geometric interpretations to integral\nevaluations and optimization tasks. The results highlight significant trends\nand patterns in the models' performance, revealing both their strengths and\nweaknesses - for instance, models like ChatGPT 4o and Mistral AI demonstrated\nconsistent accuracy across various problem types, indicating their robustness\nand reliability in mathematical problem-solving, while models such as Gemini\nAdvanced with 1.5 Pro and Meta AI exhibited specific weaknesses, particularly\nin complex problems involving integrals and optimization, suggesting areas for\ntargeted improvements. The study also underscores the importance of\nre-prompting in achieving accurate solutions, as seen in several instances\nwhere models initially provided incorrect answers but corrected them upon\nre-prompting. Overall, this research provides valuable insights into the\ncurrent capabilities and limitations of LLMs in the domain of math calculus,\nwith the detailed analysis of each model's performance on specific problems\noffering a comprehensive understanding of their strengths and areas for\nimprovement, contributing to the ongoing development and refinement of LLM\ntechnology. The findings are particularly relevant for educators, researchers,\nand developers seeking to leverage LLMs for educational and practical\napplications in mathematics.", "published": "2025-03-05 23:26:12", "link": "http://arxiv.org/abs/2503.03960v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Three tiers of computation in transformers and in brain architectures", "abstract": "Human language and logic abilities are computationally quantified within the\nwell-studied grammar-automata hierarchy. We identify three hierarchical tiers\nand two corresponding transitions and show their correspondence to specific\nabilities in transformer-based language models (LMs). These emergent abilities\nhave often been described in terms of scaling; we show that it is the\ntransition between tiers, rather than scaled size itself, that determines a\nsystem's capabilities. Specifically, humans effortlessly process language yet\nrequire critical training to perform arithmetic or logical reasoning tasks; and\nLMs possess language abilities absent from predecessor systems, yet still\nstruggle with logical processing. We submit a novel benchmark of computational\npower, provide empirical evaluations of humans and fifteen LMs, and, most\nsignificantly, provide a theoretically grounded framework to promote careful\nthinking about these crucial topics. The resulting principled analyses provide\nexplanatory accounts of the abilities and shortfalls of LMs, and suggest\nactionable insights into the expansion of their logic abilities.", "published": "2025-03-05 22:47:09", "link": "http://arxiv.org/abs/2503.04848v2", "categories": ["cs.CL", "cs.NE", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Tec-Habilidad: Skill Classification for Bridging Education and Employment", "abstract": "Job application and assessment processes have evolved significantly in recent\nyears, largely due to advancements in technology and changes in the way\ncompanies operate. Skill extraction and classification remain an important\ncomponent of the modern hiring process as it provides a more objective way to\nevaluate candidates and automatically align their skills with the job\nrequirements. However, to effectively evaluate the skills, the skill extraction\ntools must recognize varied mentions of skills on resumes, including direct\nmentions, implications, synonyms, acronyms, phrases, and proficiency levels,\nand differentiate between hard and soft skills. While tools like LLMs (Large\nModel Models) help extract and categorize skills from job applications, there's\na lack of comprehensive datasets for evaluating the effectiveness of these\nmodels in accurately identifying and classifying skills in Spanish-language job\napplications. This gap hinders our ability to assess the reliability and\nprecision of the models, which is crucial for ensuring that the selected\ncandidates truly possess the required skills for the job. In this paper, we\ndevelop a Spanish language dataset for skill extraction and classification,\nprovide annotation methodology to distinguish between knowledge, skill, and\nabilities, and provide deep learning baselines to advance robust solutions for\nskill classification.", "published": "2025-03-05 22:05:42", "link": "http://arxiv.org/abs/2503.03932v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Personalized Federated Fine-tuning for Heterogeneous Data: An Automatic Rank Learning Approach via Two-Level LoRA", "abstract": "We study the task of personalized federated fine-tuning with heterogeneous\ndata in the context of language models, where clients collaboratively fine-tune\na language model (e.g., BERT, GPT) without sharing their local data, achieving\npersonalization simultaneously. While recent efforts have applied\nparameter-efficient fine-tuning techniques like low-rank adaptation (LoRA) in\nfederated settings, they typically use single or multiple independent low-rank\nadapters with predefined maximal and minimal ranks, which may not be optimal\nfor diverse data sources over clients.\n  To address this issue, we propose PF2LoRA, a new personalized federated\nfine-tuning algorithm built on a novel \\emph{automatic rank learning approach\nvia two-level LoRA}. Given the pretrained language model whose weight is\nfrozen, our algorithm aims to learn two levels of adaptation simultaneously:\nthe first level aims to learn a common adapter for all clients, while the\nsecond level fosters individual client personalization. A key advantage of\nPF2LoRA is its ability to adaptively determine a suitable rank based on an\nindividual client's data, rather than relying on a predefined rank that is\nagnostic to data heterogeneity. We present a synthetic example that highlights\nhow PF2LoRA automatically learns the ground-truth rank for each client,\ntailoring the adaptation to match the properties of their individual data.\nNotably, this approach introduces minimal additional memory overhead, as the\nsecond-level adaptation comprises a small number of parameters compared to the\nfirst level. Our experiments on natural language understanding and generation\ntasks demonstrate that PF2LoRA significantly outperforms existing federated\nfine-tuning methods.", "published": "2025-03-05 21:41:03", "link": "http://arxiv.org/abs/2503.03920v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LEWIS (LayEr WIse Sparsity) -- A Training Free Guided Model Merging Approach", "abstract": "As specialized large language models (LLMs) become increasingly prevalent,\nmodel merging methods are being used to combine them to create a single\nmulti-task model without requiring any additional data or training. However,\nthese approaches fall short when the objective of merging is to increase the\ndownstream model's performance on a particular task-specific benchmark. In this\nwork, we propose LEWIS (Layer Wise Sparsity), a guided model-merging framework\nthat uses activation-based layer importance to dynamically adjust layer-wise\ntask-vector sparsity required for the merge process. LEWIS uses a calibration\ndataset to prioritize critical layers during the task-vector pruning process\nrequired for model merging. This approach guides existing merging methods by\npreserving essential layer-wise task-specific knowledge while ensuring the\nmerged model performs the best at benchmarks resembling the calibration\ndataset. Our experiments demonstrate the effectiveness of LEWIS with\nperformance improvements of code instruction-following and math-solving models\ncreated through model merging up to 4 percent and 11.3 percent, respectively,\noutperforming unguided data-less model merging approaches that use\nuniform-sparsity.", "published": "2025-03-05 20:09:59", "link": "http://arxiv.org/abs/2503.03874v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Not-Just-Scaling Laws: Towards a Better Understanding of the Downstream Impact of Language Model Design Decisions", "abstract": "Improvements in language model capabilities are often attributed to\nincreasing model size or training data, but in some cases smaller models\ntrained on curated data or with different architectural decisions can\noutperform larger ones trained on more tokens. What accounts for this? To\nquantify the impact of these design choices, we meta-analyze 92 open-source\npretrained models across a wide array of scales, including state-of-the-art\nopen-weights models as well as less performant models and those with less\nconventional design decisions. We find that by incorporating features besides\nmodel size and number of training tokens, we can achieve a relative 3-28%\nincrease in ability to predict downstream performance compared with using scale\nalone. Analysis of model design decisions reveal insights into data\ncomposition, such as the trade-off between language and code tasks at 15-25\\%\ncode, as well as the better performance of some architectural decisions such as\nchoosing rotary over learned embeddings. Broadly, our framework lays a\nfoundation for more systematic investigation of how model development choices\nshape final capabilities.", "published": "2025-03-05 19:46:04", "link": "http://arxiv.org/abs/2503.03862v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Vision-Language Models Struggle to Align Entities across Modalities", "abstract": "Cross-modal entity linking refers to the ability to align entities and their\nattributes across different modalities. While cross-modal entity linking is a\nfundamental skill needed for real-world applications such as multimodal code\ngeneration, fake news detection, or scene understanding, it has not been\nthoroughly studied in the literature. In this paper, we introduce a new task\nand benchmark to address this gap. Our benchmark, MATE, consists of 5.5k\nevaluation instances featuring visual scenes aligned with their textual\nrepresentations. To evaluate cross-modal entity linking performance, we design\na question-answering task that involves retrieving one attribute of an object\nin one modality based on a unique attribute of that object in another modality.\nWe evaluate state-of-the-art Vision-Language Models (VLMs) and humans on this\ntask, and find that VLMs struggle significantly compared to humans,\nparticularly as the number of objects in the scene increases. Our analysis also\nshows that, while chain-of-thought prompting can improve VLM performance,\nmodels remain far from achieving human-level proficiency. These findings\nhighlight the need for further research in cross-modal entity linking and show\nthat MATE is a strong benchmark to support that progress.", "published": "2025-03-05 19:36:43", "link": "http://arxiv.org/abs/2503.03854v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The MASK Benchmark: Disentangling Honesty From Accuracy in AI Systems", "abstract": "As large language models (LLMs) become more capable and agentic, the\nrequirement for trust in their outputs grows significantly, yet at the same\ntime concerns have been mounting that models may learn to lie in pursuit of\ntheir goals. To address these concerns, a body of work has emerged around the\nnotion of \"honesty\" in LLMs, along with interventions aimed at mitigating\ndeceptive behaviors. However, evaluations of honesty are currently highly\nlimited, with no benchmark combining large scale and applicability to all\nmodels. Moreover, many benchmarks claiming to measure honesty in fact simply\nmeasure accuracy--the correctness of a model's beliefs--in disguise. In this\nwork, we introduce a large-scale human-collected dataset for measuring honesty\ndirectly, allowing us to disentangle accuracy from honesty for the first time.\nAcross a diverse set of LLMs, we find that while larger models obtain higher\naccuracy on our benchmark, they do not become more honest. Surprisingly, while\nmost frontier LLMs obtain high scores on truthfulness benchmarks, we find a\nsubstantial propensity in frontier LLMs to lie when pressured to do so,\nresulting in low honesty scores on our benchmark. We find that simple methods,\nsuch as representation engineering interventions, can improve honesty. These\nresults underscore the growing need for robust evaluations and effective\ninterventions to ensure LLMs remain trustworthy.", "published": "2025-03-05 18:59:23", "link": "http://arxiv.org/abs/2503.03750v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Process-based Self-Rewarding Language Models", "abstract": "Large Language Models have demonstrated outstanding performance across\nvarious downstream tasks and have been widely applied in multiple scenarios.\nHuman-annotated preference data is used for training to further improve LLMs'\nperformance, which is constrained by the upper limit of human performance.\nTherefore, Self-Rewarding method has been proposed, where LLMs generate\ntraining data by rewarding their own outputs. However, the existing\nself-rewarding paradigm is not effective in mathematical reasoning scenarios\nand may even lead to a decline in performance. In this work, we propose the\nProcess-based Self-Rewarding pipeline for language models, which introduces\nlong-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference\noptimization within the self-rewarding paradigm. Our new paradigm successfully\nenhances the performance of LLMs on multiple mathematical reasoning benchmarks\nthrough iterative Process-based Self-Rewarding, demonstrating the immense\npotential of self-rewarding to achieve LLM reasoning that may surpass human\ncapabilities.", "published": "2025-03-05 18:58:44", "link": "http://arxiv.org/abs/2503.03746v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Universal Narrative Model: an Author-centric Storytelling Framework for Generative AI", "abstract": "Generative AI promises to finally realize dynamic, personalized storytelling\ntechnologies across a range of media. To date, experimentation with generative\nAI in the field of procedural narrative generation has been quite promising\nfrom a technical perspective. However, fundamental narrative dilemmas remain,\nsuch as the balance between player agency and narrative coherence, and no\nrigorous narrative standard has been proposed to specifically leverage the\nstrengths of generative AI. In this paper, we propose the Universal Narrative\nModel (UNM), an open and extensible standard designed to place writers at the\ncenter of future narrative design workflows and enable interoperability across\nauthoring platforms. By encoding an author's intent according to an objective\nnarrative model, the UNM enables narrative portability as well as intent-based\nconstraints for generative systems.", "published": "2025-03-05 18:29:15", "link": "http://arxiv.org/abs/2503.04844v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving LLM Safety Alignment with Dual-Objective Optimization", "abstract": "Existing training-time safety alignment techniques for large language models\n(LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization\n(DPO), a widely deployed alignment method, exhibits limitations in both\nexperimental and theoretical contexts as its loss function proves suboptimal\nfor refusal learning. Through gradient-based analysis, we identify these\nshortcomings and propose an improved safety alignment that disentangles DPO\nobjectives into two components: (1) robust refusal training, which encourages\nrefusal even when partial unsafe generations are produced, and (2) targeted\nunlearning of harmful knowledge. This approach significantly increases LLM\nrobustness against a wide range of jailbreak attacks, including prefilling,\nsuffix, and multi-turn attacks across both in-distribution and\nout-of-distribution scenarios. Furthermore, we introduce a method to emphasize\ncritical refusal tokens by incorporating a reward-based token-level weighting\nmechanism for refusal learning, which further improves the robustness against\nadversarial exploits. Our research also suggests that robustness to jailbreak\nattacks is correlated with token distribution shifts in the training process\nand internal representations of refusal and harmful tokens, offering valuable\ndirections for future research in LLM safety alignment. The code is available\nat https://github.com/wicai24/DOOR-Alignment", "published": "2025-03-05 18:01:05", "link": "http://arxiv.org/abs/2503.03710v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Effective LLM Knowledge Learning via Model Generalization", "abstract": "Large language models (LLMs) are trained on enormous documents that contain\nextensive world knowledge. However, it is still not well-understood how\nknowledge is acquired via autoregressive pre-training. This lack of\nunderstanding greatly hinders effective knowledge learning, especially for\ncontinued pretraining on up-to-date information, as this evolving information\noften lacks diverse repetitions like foundational knowledge. In this paper, we\nfocus on understanding and improving LLM knowledge learning. We found and\nverified that knowledge learning for LLMs can be deemed as an implicit\nsupervised task hidden in the autoregressive pre-training objective. Our\nfindings suggest that knowledge learning for LLMs would benefit from methods\ndesigned to improve generalization ability for supervised tasks. Based on our\nanalysis, we propose the formatting-based data augmentation to grow\nin-distribution samples, which does not present the risk of altering the facts\nembedded in documents as text paraphrasing. We also introduce sharpness-aware\nminimization as an effective optimization algorithm to better improve\ngeneralization. Moreover, our analysis and method can be readily extended to\ninstruction tuning. Extensive experiment results validate our findings and\ndemonstrate our methods' effectiveness in both continued pre-training and\ninstruction tuning. This paper offers new perspectives and insights to\ninterpret and design effective strategies for LLM knowledge learning.", "published": "2025-03-05 17:56:20", "link": "http://arxiv.org/abs/2503.03705v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SoftMatcha: A Soft and Fast Pattern Matcher for Billion-Scale Corpus Searches", "abstract": "Researchers and practitioners in natural language processing and\ncomputational linguistics frequently observe and analyze the real language\nusage in large-scale corpora. For that purpose, they often employ off-the-shelf\npattern-matching tools, such as grep, and keyword-in-context concordancers,\nwhich is widely used in corpus linguistics for gathering examples. Nonetheless,\nthese existing techniques rely on surface-level string matching, and thus they\nsuffer from the major limitation of not being able to handle orthographic\nvariations and paraphrasing -- notable and common phenomena in any natural\nlanguage. In addition, existing continuous approaches such as dense vector\nsearch tend to be overly coarse, often retrieving texts that are unrelated but\nshare similar topics. Given these challenges, we propose a novel algorithm that\nachieves \\emph{soft} (or semantic) yet efficient pattern matching by relaxing a\nsurface-level matching with word embeddings. Our algorithm is highly scalable\nwith respect to the size of the corpus text utilizing inverted indexes. We have\nprepared an efficient implementation, and we provide an accessible web tool.\nOur experiments demonstrate that the proposed method (i) can execute searches\non billion-scale corpora in less than a second, which is comparable in speed to\nsurface-level string matching and dense vector search; (ii) can extract harmful\ninstances that semantically match queries from a large set of English and\nJapanese Wikipedia articles; and (iii) can be effectively applied to\ncorpus-linguistic analyses of Latin, a language with highly diverse\ninflections.", "published": "2025-03-05 17:53:11", "link": "http://arxiv.org/abs/2503.03703v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Developing and Utilizing a Large-Scale Cantonese Dataset for Multi-Tasking in Large Language Models", "abstract": "High-quality data resources play a crucial role in learning large language\nmodels (LLMs), particularly for low-resource languages like Cantonese. Despite\nhaving more than 85 million native speakers, Cantonese is still considered a\nlow-resource language in the field of natural language processing (NLP) due to\nfactors such as the dominance of Mandarin, lack of cohesion within the\nCantonese-speaking community, diversity in character encoding and input\nmethods, and the tendency of overseas Cantonese speakers to prefer using\nEnglish. In addition, rich colloquial vocabulary of Cantonese, English\nloanwords, and code-switching characteristics add to the complexity of corpus\ncollection and processing. To address these challenges, we collect Cantonese\ntexts from a variety of sources, including open source corpora, Hong\nKong-specific forums, Wikipedia, and Common Crawl data. We conduct rigorous\ndata processing through language filtering, quality filtering, content\nfiltering, and de-duplication steps, successfully constructing a high-quality\nCantonese corpus of over 2 billion tokens for training large language models.\nWe further refined the model through supervised fine-tuning (SFT) on curated\nCantonese tasks, enhancing its ability to handle specific applications. Upon\ncompletion of the training, the model achieves state-of-the-art (SOTA)\nperformance on four Cantonese benchmarks. After training on our dataset, the\nmodel also exhibits improved performance on other mainstream language tasks.", "published": "2025-03-05 17:53:07", "link": "http://arxiv.org/abs/2503.03702v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixture of Experts Made Intrinsically Interpretable", "abstract": "Neurons in large language models often exhibit \\emph{polysemanticity},\nsimultaneously encoding multiple unrelated concepts and obscuring\ninterpretability. Instead of relying on post-hoc methods, we present\n\\textbf{MoE-X}, a Mixture-of-Experts (MoE) language model designed to be\n\\emph{intrinsically} interpretable. Our approach is motivated by the\nobservation that, in language models, wider networks with sparse activations\nare more likely to capture interpretable factors. However, directly training\nsuch large sparse networks is computationally prohibitive. MoE architectures\noffer a scalable alternative by activating only a subset of experts for any\ngiven input, inherently aligning with interpretability objectives. In MoE-X, we\nestablish this connection by rewriting the MoE layer as an equivalent sparse,\nlarge MLP. This approach enables efficient scaling of the hidden size while\nmaintaining sparsity. To further enhance interpretability, we enforce sparse\nactivation within each expert and redesign the routing mechanism to prioritize\nexperts with the highest activation sparsity. These designs ensure that only\nthe most salient features are routed and processed by the experts. We evaluate\nMoE-X on chess and natural language tasks, showing that it achieves performance\ncomparable to dense models while significantly improving interpretability.\nMoE-X achieves a perplexity better than GPT-2, with interpretability surpassing\neven sparse autoencoder (SAE)-based approaches.", "published": "2025-03-05 17:40:54", "link": "http://arxiv.org/abs/2503.07639v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Replicating Human Social Perception in Generative AI: Evaluating the Valence-Dominance Model", "abstract": "As artificial intelligence (AI) continues to advance--particularly in\ngenerative models--an open question is whether these systems can replicate\nfoundational models of human social perception. A well-established framework in\nsocial cognition suggests that social judgments are organized along two primary\ndimensions: valence (e.g., trustworthiness, warmth) and dominance (e.g., power,\nassertiveness). This study examines whether multimodal generative AI systems\ncan reproduce this valence-dominance structure when evaluating facial images\nand how their representations align with those observed across world regions.\nThrough principal component analysis (PCA), we found that the extracted\ndimensions closely mirrored the theoretical structure of valence and dominance,\nwith trait loadings aligning with established definitions. However, many world\nregions and generative AI models also exhibited a third component, the nature\nand significance of which warrant further investigation. These findings\ndemonstrate that multimodal generative AI systems can replicate key aspects of\nhuman social perception, raising important questions about their implications\nfor AI-driven decision-making and human-AI interactions.", "published": "2025-03-05 17:35:18", "link": "http://arxiv.org/abs/2503.04842v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems", "abstract": "LLM-based multi-agent systems (MAS) have shown significant potential in\ntackling diverse tasks. However, to design effective MAS, existing approaches\nheavily rely on manual configurations or multiple calls of advanced LLMs,\nresulting in inadaptability and high inference costs. In this paper, we\nsimplify the process of building an MAS by reframing it as a generative\nlanguage task, where the input is a user query and the output is a\ncorresponding MAS. To address this novel task, we unify the representation of\nMAS as executable code and propose a consistency-oriented data construction\npipeline to create a high-quality dataset comprising coherent and consistent\nquery-MAS pairs. Using this dataset, we train MAS-GPT, an open-source\nmedium-sized LLM that is capable of generating query-adaptive MAS within a\nsingle LLM inference. The generated MAS can be seamlessly applied to process\nuser queries and deliver high-quality responses. Extensive experiments on 9\nbenchmarks and 5 LLMs show that the proposed MAS-GPT consistently outperforms\n10+ baseline MAS methods on diverse settings, indicating MAS-GPT's high\neffectiveness, efficiency and strong generalization ability. Code will be\navailable at https://github.com/rui-ye/MAS-GPT.", "published": "2025-03-05 17:27:59", "link": "http://arxiv.org/abs/2503.03686v1", "categories": ["cs.CL", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Quantification of Tenseness in English and Japanese Tense-Lax Vowels: A Lagrangian Model with Indicator \u03b81 and Force of Tenseness Ftense(t)", "abstract": "The concept of vowel tenseness has traditionally been examined through the\nbinary distinction of tense and lax vowels. However, no universally accepted\nquantitative definition of tenseness has been established in any language.\nPrevious studies, including those by Jakobson, Fant, and Halle (1951) and\nChomsky and Halle (1968), have explored the relationship between vowel\ntenseness and the vocal tract. Building on these foundations, Ishizaki (2019,\n2022) proposed an indirect quantification of vowel tenseness using formant\nangles {\\theta}1 and {\\theta}F1 and their first and second derivatives,\ndZ1(t)/dt = lim tan {\\theta}1(t) and d2Z1(t)/dt2 = d/dt lim tan {\\theta}1(t).\nThis study extends this approach by investigating the potential role of a\nforce-related parameter in determining vowel quality. Specifically, we\nintroduce a simplified model based on the Lagrangian equation to describe the\ndynamic interaction of the tongue and jaw within the oral cavity during the\narticulation of close vowels. This model provides a theoretical framework for\nestimating the forces involved in vowel production across different languages,\noffering new insights into the physical mechanisms underlying vowel\narticulation. The findings suggest that this force-based perspective warrants\nfurther exploration as a key factor in phonetic and phonological studies.", "published": "2025-03-05 17:22:33", "link": "http://arxiv.org/abs/2503.03681v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multi-Agent Systems Powered by Large Language Models: Applications in Swarm Intelligence", "abstract": "This work examines the integration of large language models (LLMs) into\nmulti-agent simulations by replacing the hard-coded programs of agents with\nLLM-driven prompts. The proposed approach is showcased in the context of two\nexamples of complex systems from the field of swarm intelligence: ant colony\nforaging and bird flocking. Central to this study is a toolchain that\nintegrates LLMs with the NetLogo simulation platform, leveraging its Python\nextension to enable communication with GPT-4o via the OpenAI API. This\ntoolchain facilitates prompt-driven behavior generation, allowing agents to\nrespond adaptively to environmental data. For both example applications\nmentioned above, we employ both structured, rule-based prompts and autonomous,\nknowledge-driven prompts. Our work demonstrates how this toolchain enables LLMs\nto study self-organizing processes and induce emergent behaviors within\nmulti-agent environments, paving the way for new approaches to exploring\nintelligent systems and modeling swarm intelligence inspired by natural\nphenomena. We provide the code, including simulation files and data at\nhttps://github.com/crjimene/swarm_gpt.", "published": "2025-03-05 17:13:27", "link": "http://arxiv.org/abs/2503.03800v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG", "I.6.0; I.2.7"], "primary_category": "cs.MA"}
{"title": "Attentive Reasoning Queries: A Systematic Method for Optimizing Instruction-Following in Large Language Models", "abstract": "We present Attentive Reasoning Queries (ARQs), a novel structured reasoning\napproach that significantly improves instruction-following in Large Language\nModels through domain-specialized reasoning blueprints. While LLMs demonstrate\nremarkable capabilities across diverse tasks, they often fail to maintain\nadherence to complex, use-case-specific instructions during multi-turn\nconversations, presenting challenges for business-critical applications. ARQs\naddress this limitation by guiding LLMs through systematic reasoning steps with\ntargeted queries that reinstate critical instructions and facilitate\nintermediate reasoning throughout the completion process. In extensive testing\nwithin Parlant, our framework for reliable customer-facing agents in which ARQs\nwere born out of necessity, they achieved a 90.2% success rate across 87 test\nscenarios, outperforming both Chain-of-Thought reasoning (86.1%) and direct\nresponse generation (81.5%). ARQs showed particular strength in addressing\npersistent failure modes like guideline re-application and hallucination\nprevention. Our analysis also revealed that ARQs can potentially be more\ncomputationally efficient than free-form reasoning when carefully designed.\nThese findings demonstrate that structured reasoning approaches provide\neffective mechanisms for controlling how LLMs process information and make\ndecisions in complex scenarios.", "published": "2025-03-05 17:03:48", "link": "http://arxiv.org/abs/2503.03669v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Framing the Game: How Context Shapes LLM Decision-Making", "abstract": "Large Language Models (LLMs) are increasingly deployed across diverse\ncontexts to support decision-making. While existing evaluations effectively\nprobe latent model capabilities, they often overlook the impact of context\nframing on perceived rational decision-making. In this study, we introduce a\nnovel evaluation framework that systematically varies evaluation instances\nacross key features and procedurally generates vignettes to create highly\nvaried scenarios. By analyzing decision-making patterns across different\ncontexts with the same underlying game structure, we uncover significant\ncontextual variability in LLM responses. Our findings demonstrate that this\nvariability is largely predictable yet highly sensitive to framing effects. Our\nresults underscore the need for dynamic, context-aware evaluation methodologies\nfor real-world deployments.", "published": "2025-03-05 17:03:28", "link": "http://arxiv.org/abs/2503.04840v1", "categories": ["cs.CL", "cs.AI", "cs.GT"], "primary_category": "cs.CL"}
{"title": "Analogical Reasoning Inside Large Language Models: Concept Vectors and the Limits of Abstraction", "abstract": "Analogical reasoning relies on conceptual abstractions, but it is unclear\nwhether Large Language Models (LLMs) harbor such internal representations. We\nexplore distilled representations from LLM activations and find that function\nvectors (FVs; Todd et al., 2024) - compact representations for in-context\nlearning (ICL) tasks - are not invariant to simple input changes (e.g.,\nopen-ended vs. multiple-choice), suggesting they capture more than pure\nconcepts. Using representational similarity analysis (RSA), we localize a small\nset of attention heads that encode invariant concept vectors (CVs) for verbal\nconcepts like \"antonym\". These CVs function as feature detectors that operate\nindependently of the final output - meaning that a model may form a correct\ninternal representation yet still produce an incorrect output. Furthermore, CVs\ncan be used to causally guide model behaviour. However, for more abstract\nconcepts like \"previous\" and \"next\", we do not observe invariant linear\nrepresentations, a finding we link to generalizability issues LLMs display\nwithin these domains.", "published": "2025-03-05 16:59:08", "link": "http://arxiv.org/abs/2503.03666v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Advancing Multimodal In-Context Learning in Large Vision-Language Models with Task-aware Demonstrations", "abstract": "Multimodal in-context learning (ICL) has emerged as a key capability of Large\nVision-Language Models (LVLMs), driven by their increasing scale and\napplicability. Despite its promise, effective ICL in the multimodal setting\nremains challenging due to the inherent complexity of image-text inputs and the\nhigh sensitivity of ICL performance to input configurations. In this work, we\nshed light on the core mechanism underlying multimodal ICL, identifying task\nmapping as a crucial factor in configuring robust in-context demonstration\n(ICD) sequences. Building on these insights, we propose \\textit{SabER}, a\nlightweight yet powerful decoder-only transformer equipped with task-aware\nattention, which intelligently selects and arranges ICDs from a demonstration\nlibrary in an autoregressive fashion. This design enables fine-grained feature\nextraction and cross-modal reasoning, iteratively refining task mapping to\ngenerate high-quality ICD sequences. Through extensive experiments covering\nfive LVLMs and nine benchmark datasets, SabER not only demonstrates strong\nempirical performance, but also provides deeper understanding of how task\nsemantics interact with multimodal ICDs. Our findings highlight the importance\nof principled ICD sequence configuration and open new avenues to enhance\nmultimodal ICL in a wide range of real-world scenarios.", "published": "2025-03-05 16:33:10", "link": "http://arxiv.org/abs/2503.04839v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Improving Neutral Point of View Text Generation through Parameter-Efficient Reinforcement Learning and a Small-Scale High-Quality Dataset", "abstract": "This paper describes the construction of a dataset and the evaluation of\ntraining methods to improve generative large language models' (LLMs) ability to\nanswer queries on sensitive topics with a Neutral Point of View (NPOV), i.e.,\nto provide significantly more informative, diverse and impartial answers. The\ndataset, the SHQ-NPOV dataset, comprises 300 high-quality, human-written\nquadruplets: a query on a sensitive topic, an answer, an NPOV rating, and a set\nof links to source texts elaborating the various points of view. The first key\ncontribution of this paper is a new methodology to create such datasets through\niterative rounds of human peer-critique and annotator training, which we\nrelease alongside the dataset. The second key contribution is the\nidentification of a highly effective training regime for parameter-efficient\nreinforcement learning (PE-RL) to improve NPOV generation. We compare and\nextensively evaluate PE-RL and multiple baselines-including LoRA finetuning (a\nstrong baseline), SFT and RLHF.\n  PE-RL not only improves on overall NPOV quality compared to the strongest\nbaseline ($97.06\\%\\rightarrow 99.08\\%$), but also scores much higher on\nfeatures linguists identify as key to separating good answers from the best\nanswers ($60.25\\%\\rightarrow 85.21\\%$ for presence of supportive details,\n$68.74\\%\\rightarrow 91.43\\%$ for absence of oversimplification). A qualitative\nanalysis corroborates this. Finally, our evaluation finds no statistical\ndifferences between results on topics that appear in the training dataset and\nthose on separated evaluation topics, which provides strong evidence that our\napproach to training PE-RL exhibits very effective out of topic generalization.", "published": "2025-03-05 16:32:47", "link": "http://arxiv.org/abs/2503.03654v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Token-Level Privacy in Large Language Models", "abstract": "The use of language models as remote services requires transmitting private\ninformation to external providers, raising significant privacy concerns. This\nprocess not only risks exposing sensitive data to untrusted service providers\nbut also leaves it vulnerable to interception by eavesdroppers. Existing\nprivacy-preserving methods for natural language processing (NLP) interactions\nprimarily rely on semantic similarity, overlooking the role of contextual\ninformation. In this work, we introduce dchi-stencil, a novel token-level\nprivacy-preserving mechanism that integrates contextual and semantic\ninformation while ensuring strong privacy guarantees under the dchi\ndifferential privacy framework, achieving 2epsilon-dchi-privacy. By\nincorporating both semantic and contextual nuances, dchi-stencil achieves a\nrobust balance between privacy and utility. We evaluate dchi-stencil using\nstate-of-the-art language models and diverse datasets, achieving comparable and\neven better trade-off between utility and privacy compared to existing methods.\nThis work highlights the potential of dchi-stencil to set a new standard for\nprivacy-preserving NLP in modern, high-risk applications.", "published": "2025-03-05 16:27:25", "link": "http://arxiv.org/abs/2503.03652v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Psy-Copilot: Visual Chain of Thought for Counseling", "abstract": "Large language models (LLMs) are becoming increasingly popular in the field\nof psychological counseling. However, when human therapists work with LLMs in\ntherapy sessions, it is hard to understand how the model gives the answers. To\naddress this, we have constructed Psy-COT, a graph designed to visualize the\nthought processes of LLMs during therapy sessions. The Psy-COT graph presents\nsemi-structured counseling conversations alongside step-by-step annotations\nthat capture the reasoning and insights of therapists. Moreover, we have\ndeveloped Psy-Copilot, which is a conversational AI assistant designed to\nassist human psychological therapists in their consultations. It can offer\ntraceable psycho-information based on retrieval, including response candidates,\nsimilar dialogue sessions, related strategies, and visual traces of results. We\nhave also built an interactive platform for AI-assisted counseling. It has an\ninterface that displays the relevant parts of the retrieval sub-graph. The\nPsy-Copilot is designed not to replace psychotherapists but to foster\ncollaboration between AI and human therapists, thereby promoting mental health\ndevelopment. Our code and demo are both open-sourced and available for use.", "published": "2025-03-05 16:23:15", "link": "http://arxiv.org/abs/2503.03645v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Psy-Insight: Explainable Multi-turn Bilingual Dataset for Mental Health Counseling", "abstract": "The in-context learning capabilities of large language models (LLMs) show\ngreat potential in mental health support. However, the lack of counseling\ndatasets, particularly in Chinese corpora, restricts their application in this\nfield. To address this, we constructed Psy-Insight, the first mental\nhealth-oriented explainable multi-task bilingual dataset. We collected\nface-to-face multi-turn counseling dialogues, which are annotated with\nmulti-task labels and conversation process explanations. Our annotations\ninclude psychotherapy, emotion, strategy, and topic labels, as well as\nturn-level reasoning and session-level guidance. Psy-Insight is not only\nsuitable for tasks such as label recognition but also meets the need for\ntraining LLMs to act as empathetic counselors through logical reasoning.\nExperiments show that training LLMs on Psy-Insight enables the models to not\nonly mimic the conversation style but also understand the underlying strategies\nand reasoning of counseling.", "published": "2025-03-05 15:44:21", "link": "http://arxiv.org/abs/2503.03607v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Feature-Level Insights into Artificial Text Detection with Sparse Autoencoders", "abstract": "Artificial Text Detection (ATD) is becoming increasingly important with the\nrise of advanced Large Language Models (LLMs). Despite numerous efforts, no\nsingle algorithm performs consistently well across different types of unseen\ntext or guarantees effective generalization to new LLMs. Interpretability plays\na crucial role in achieving this goal. In this study, we enhance ATD\ninterpretability by using Sparse Autoencoders (SAE) to extract features from\nGemma-2-2b residual stream. We identify both interpretable and efficient\nfeatures, analyzing their semantics and relevance through domain- and\nmodel-specific statistics, a steering approach, and manual or LLM-based\ninterpretation. Our methods offer valuable insights into how texts from various\nmodels differ from human-written content. We show that modern LLMs have a\ndistinct writing style, especially in information-dense domains, even though\nthey can produce human-like outputs with personalized prompts.", "published": "2025-03-05 15:33:52", "link": "http://arxiv.org/abs/2503.03601v1", "categories": ["cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Small but Mighty: Enhancing Time Series Forecasting with Lightweight LLMs", "abstract": "While LLMs have demonstrated remarkable potential in time series forecasting,\ntheir practical deployment remains constrained by excessive computational\ndemands and memory footprints. Existing LLM-based approaches typically suffer\nfrom three critical limitations: Inefficient parameter utilization in handling\nnumerical time series patterns; Modality misalignment between continuous\ntemporal signals and discrete text embeddings; and Inflexibility for real-time\nexpert knowledge integration. We present SMETimes, the first systematic\ninvestigation of sub-3B parameter SLMs for efficient and accurate time series\nforecasting. Our approach centers on three key innovations: A\nstatistically-enhanced prompting mechanism that bridges numerical time series\nwith textual semantics through descriptive statistical features; A adaptive\nfusion embedding architecture that aligns temporal patterns with language model\ntoken spaces through learnable parameters; And a dynamic mixture-of-experts\nframework enabled by SLMs' computational efficiency, adaptively combining base\npredictions with domain-specific models. Extensive evaluations across seven\nbenchmark datasets demonstrate that our 3B-parameter SLM achieves\nstate-of-the-art performance on five primary datasets while maintaining 3.8x\nfaster training and 5.2x lower memory consumption compared to 7B-parameter LLM\nbaselines. Notably, the proposed model exhibits better learning capabilities,\nachieving 12.3% lower MSE than conventional LLM. Ablation studies validate that\nour statistical prompting and cross-modal fusion modules respectively\ncontribute 15.7% and 18.2% error reduction in long-horizon forecasting tasks.\nBy redefining the efficiency-accuracy trade-off landscape, this work\nestablishes SLMs as viable alternatives to resource-intensive LLMs for\npractical time series forecasting. Code and models are available at\nhttps://github.com/xiyan1234567/SMETimes.", "published": "2025-03-05 15:27:36", "link": "http://arxiv.org/abs/2503.03594v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "English K_Quantization of LLMs Does Not Disproportionately Diminish Multilingual Performance", "abstract": "For consumer usage of locally deployed LLMs, the GGUF format and\nk\\_quantization are invaluable tools for maintaining the performance of the\noriginal model while reducing it to sizes deployable with consumer-grade\nhardware. The number of bits dedicated to each weight from the original model\nis reduced based on how important they are thought to be during model\ninference. This importance is arrived at through the application of an\n'importance matrix'-a relatively small text document meant to be representative\nof the LLM's standard use-cases. In the vast majority of quants available\nonline, this document is primarily written in English. It was therefore an open\nquestion whether performance on English language tasks was preserved through\nthe sacrifice of multilingual performance and whether it can be preserved with\nalternate importance matrices. This article investigates these hypotheses by\nquantizing Llama3.3 70B on importance matrices written in three languages\n(English, Norwegian, and Malayalam) and evaluating them on the MixEval dataset\nin both English and Norwegian. All experiments related to yielded\nnon-significant results indicating that current quantization practices do not\ndisproportionately harm multilingual performance.", "published": "2025-03-05 15:26:59", "link": "http://arxiv.org/abs/2503.03592v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention", "abstract": "Large Language Models (LLMs) face efficiency bottlenecks due to the quadratic\ncomplexity of the attention mechanism when processing long contexts. Sparse\nattention methods offer a promising solution, but existing approaches often\nsuffer from incomplete effective context and/or require complex implementation\nof pipeline. We present a comprehensive analysis of sparse attention for\nautoregressive LLMs from the respective of receptive field, recognize the\nsuboptimal nature of existing methods for expanding the receptive field, and\nintroduce PowerAttention, a novel sparse attention design that facilitates\neffective and complete context extension through the theoretical analysis.\nPowerAttention achieves exponential receptive field growth in $d$-layer LLMs,\nallowing each output token to attend to $2^d$ tokens, ensuring completeness and\ncontinuity of the receptive field. Experiments demonstrate that PowerAttention\noutperforms existing static sparse attention methods by $5\\sim 40\\%$,\nespecially on tasks demanding long-range dependencies like Passkey Retrieval\nand RULER, while maintaining a comparable time complexity to sliding window\nattention. Efficiency evaluations further highlight PowerAttention's superior\nspeedup in both prefilling and decoding phases compared with dynamic sparse\nattentions and full attention ($3.0\\times$ faster on 128K context), making it a\nhighly effective and user-friendly solution for processing long sequences in\nLLMs.", "published": "2025-03-05 15:24:11", "link": "http://arxiv.org/abs/2503.03588v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling Crowdsourced Election Monitoring: Construction and Evaluation of Classification Models for Multilingual and Cross-Domain Classification Settings", "abstract": "The adoption of crowdsourced election monitoring as a complementary\nalternative to traditional election monitoring is on the rise. Yet, its\nreliance on digital response volunteers to manually process incoming election\nreports poses a significant scaling bottleneck. In this paper, we address the\nchallenge of scaling crowdsourced election monitoring by advancing the task of\nautomated classification of crowdsourced election reports to multilingual and\ncross-domain classification settings. We propose a two-step classification\napproach of first identifying informative reports and then categorising them\ninto distinct information types. We conduct classification experiments using\nmultilingual transformer models such as XLM-RoBERTa and multilingual embeddings\nsuch as SBERT, augmented with linguistically motivated features. Our approach\nachieves F1-Scores of 77\\% for informativeness detection and 75\\% for\ninformation type classification. We conduct cross-domain experiments, applying\nmodels trained in a source electoral domain to a new target electoral domain in\nzero-shot and few-shot classification settings. Our results show promising\npotential for model transfer across electoral domains, with F1-Scores of 59\\%\nin zero-shot and 63\\% in few-shot settings. However, our analysis also reveals\na performance bias in detecting informative English reports over Swahili,\nlikely due to imbalances in the training data, indicating a need for caution\nwhen deploying classification models in real-world election scenarios.", "published": "2025-03-05 15:17:18", "link": "http://arxiv.org/abs/2503.03582v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Human Preferences for Constructive Interactions in Language Model Alignment", "abstract": "As large language models (LLMs) enter the mainstream, aligning them to foster\nconstructive dialogue rather than exacerbate societal divisions is critical.\nUsing an individualized and multicultural alignment dataset of over 7,500\nconversations of individuals from 74 countries engaging with 21 LLMs, we\nexamined how linguistic attributes linked to constructive interactions are\nreflected in human preference data used for training AI. We found that users\nconsistently preferred well-reasoned and nuanced responses while rejecting\nthose high in personal storytelling. However, users who believed that AI should\nreflect their values tended to place less preference on reasoning in LLM\nresponses and more on curiosity. Encouragingly, we observed that users could\nset the tone for how constructive their conversation would be, as LLMs mirrored\nlinguistic attributes, including toxicity, in user queries.", "published": "2025-03-05 15:08:41", "link": "http://arxiv.org/abs/2503.16480v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "Extrapolation Merging: Keep Improving With Extrapolation and Merging", "abstract": "Large Language Models (LLMs) require instruction fine-tuning to perform\ndifferent downstream tasks. However, the instruction fine-tuning phase still\ndemands significant computational resources and labeled data, lacking a\nparadigm that can improve model performance without additional computational\npower and data. Model merging aims to enhance performance by combining the\nparameters of different models, but the lack of a clear optimization direction\nduring the merging process does not always guarantee improved performance. In\nthis paper, we attempt to provide a clear optimization direction for model\nmerging. We first validate the effectiveness of the model extrapolation method\nduring the instruction fine-tuning phase. Then, we propose Extrapolation\nMerging, a paradigm that can continue improving model performance without\nrequiring extra computational resources or data. Using the extrapolation\nmethod, we provide a clear direction for model merging, achieving local\noptimization search, and consequently enhancing the merged model's performance.\nWe conduct experiments on seven different tasks, and the results show that our\nmethod can consistently improve the model's performance after fine-tuning.", "published": "2025-03-05 14:28:22", "link": "http://arxiv.org/abs/2503.04834v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adversarial Training for Multimodal Large Language Models against Jailbreak Attacks", "abstract": "Multimodal large language models (MLLMs) have made remarkable strides in\ncross-modal comprehension and generation tasks. However, they remain vulnerable\nto jailbreak attacks, where crafted perturbations bypass security guardrails\nand elicit harmful outputs. In this paper, we present the first adversarial\ntraining (AT) paradigm tailored to defend against jailbreak attacks during the\nMLLM training phase. Extending traditional AT to this domain poses two critical\nchallenges: efficiently tuning massive parameters and ensuring robustness\nagainst attacks across multiple modalities. To address these challenges, we\nintroduce Projection Layer Against Adversarial Training (ProEAT), an end-to-end\nAT framework. ProEAT incorporates a projector-based adversarial training\narchitecture that efficiently handles large-scale parameters while maintaining\ncomputational feasibility by focusing adversarial training on a lightweight\nprojector layer instead of the entire model; additionally, we design a dynamic\nweight adjustment mechanism that optimizes the loss function's weight\nallocation based on task demands, streamlining the tuning process. To enhance\ndefense performance, we propose a joint optimization strategy across visual and\ntextual modalities, ensuring robust resistance to jailbreak attacks originating\nfrom either modality. Extensive experiments conducted on five major jailbreak\nattack methods across three mainstream MLLMs demonstrate the effectiveness of\nour approach. ProEAT achieves state-of-the-art defense performance,\noutperforming existing baselines by an average margin of +34% across text and\nimage modalities, while incurring only a 1% reduction in clean accuracy.\nFurthermore, evaluations on real-world embodied intelligent systems highlight\nthe practical applicability of our framework, paving the way for the\ndevelopment of more secure and reliable multimodal systems.", "published": "2025-03-05 14:13:35", "link": "http://arxiv.org/abs/2503.04833v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "An Aspect Extraction Framework using Different Embedding Types, Learning Models, and Dependency Structure", "abstract": "Aspect-based sentiment analysis has gained significant attention in recent\nyears due to its ability to provide fine-grained insights for sentiment\nexpressions related to specific features of entities. An important component of\naspect-based sentiment analysis is aspect extraction, which involves\nidentifying and extracting aspect terms from text. Effective aspect extraction\nserves as the foundation for accurate sentiment analysis at the aspect level.\nIn this paper, we propose aspect extraction models that use different types of\nembeddings for words and part-of-speech tags and that combine several learning\nmodels. We also propose tree positional encoding that is based on dependency\nparsing output to capture better the aspect positions in sentences. In\naddition, a new aspect extraction dataset is built for Turkish by machine\ntranslating an English dataset in a controlled setting. The experiments\nconducted on two Turkish datasets showed that the proposed models mostly\noutperform the studies that use the same datasets, and incorporating tree\npositional encoding increases the performance of the models.", "published": "2025-03-05 13:57:48", "link": "http://arxiv.org/abs/2503.03512v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CURVALID: Geometrically-guided Adversarial Prompt Detection", "abstract": "Adversarial prompts capable of jailbreaking large language models (LLMs) and\ninducing undesirable behaviours pose a significant obstacle to their safe\ndeployment. Current mitigation strategies rely on activating built-in defence\nmechanisms or fine-tuning the LLMs, but the fundamental distinctions between\nadversarial and benign prompts are yet to be understood. In this work, we\nintroduce CurvaLID, a novel defense framework that efficiently detects\nadversarial prompts by leveraging their geometric properties. It is agnostic to\nthe type of LLM, offering a unified detection framework across diverse\nadversarial prompts and LLM architectures. CurvaLID builds on the geometric\nanalysis of text prompts to uncover their underlying differences. We\ntheoretically extend the concept of curvature via the Whewell equation into an\n$n$-dimensional word embedding space, enabling us to quantify local geometric\nproperties, including semantic shifts and curvature in the underlying\nmanifolds. Additionally, we employ Local Intrinsic Dimensionality (LID) to\ncapture geometric features of text prompts within adversarial subspaces. Our\nfindings reveal that adversarial prompts differ fundamentally from benign\nprompts in terms of their geometric characteristics. Our results demonstrate\nthat CurvaLID delivers superior detection and rejection of adversarial queries,\npaving the way for safer LLM deployment. The source code can be found at\nhttps://github.com/Cancanxxx/CurvaLID", "published": "2025-03-05 13:47:53", "link": "http://arxiv.org/abs/2503.03502v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deictic Codes, Demonstratives, and Reference: A Step Toward Solving the Grounding Problem", "abstract": "In this paper we address the issue of grounding for experiential concepts.\nGiven that perceptual demonstratives are a basic form of such concepts, we\nexamine ways of fixing the referents of such demonstratives. To avoid\n'encodingism', that is, relating representations to representations, we\npostulate that the process of reference fixing must be bottom-up and\nnonconceptual, so that it can break the circle of conceptual content and touch\nthe world. For that purpose, an appropriate causal relation between\nrepresentations and the world is needed. We claim that this relation is\nprovided by spatial and object-centered attention that leads to the formation\nof object files through the function of deictic acts. This entire causal\nprocess takes place at a pre-conceptual level, meeting the requirement for a\nsolution to the grounding problem. Finally we claim that our account captures\nfundamental insights in Putnam's and Kripke's work on \"new\" reference.", "published": "2025-03-05 13:34:49", "link": "http://arxiv.org/abs/2503.03495v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Spoken Discourse Modeling in Language Models Using Gestural Cues", "abstract": "Research in linguistics shows that non-verbal cues, such as gestures, play a\ncrucial role in spoken discourse. For example, speakers perform hand gestures\nto indicate topic shifts, helping listeners identify transitions in discourse.\nIn this work, we investigate whether the joint modeling of gestures using human\nmotion sequences and language can improve spoken discourse modeling in language\nmodels. To integrate gestures into language models, we first encode 3D human\nmotion sequences into discrete gesture tokens using a VQ-VAE. These gesture\ntoken embeddings are then aligned with text embeddings through feature\nalignment, mapping them into the text embedding space. To evaluate the\ngesture-aligned language model on spoken discourse, we construct text infilling\ntasks targeting three key discourse cues grounded in linguistic research:\ndiscourse connectives, stance markers, and quantifiers. Results show that\nincorporating gestures enhances marker prediction accuracy across the three\ntasks, highlighting the complementary information that gestures can offer in\nmodeling spoken discourse. We view this work as an initial step toward\nleveraging non-verbal cues to advance spoken language modeling in language\nmodels.", "published": "2025-03-05 13:10:07", "link": "http://arxiv.org/abs/2503.03474v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open-Source Large Language Models as Multilingual Crowdworkers: Synthesizing Open-Domain Dialogues in Several Languages With No Examples in Targets and No Machine Translation", "abstract": "The prevailing paradigm in the domain of Open-Domain Dialogue agents\npredominantly focuses on the English language, encompassing both models and\ndatasets. Furthermore, the financial and temporal investments required for\ncrowdsourcing such datasets for finetuning are substantial, particularly when\nmultiple languages are involved. Fortunately, advancements in Large Language\nModels (LLMs) have unveiled a plethora of possibilities across diverse tasks.\nSpecifically, instruction-tuning has enabled LLMs to execute tasks based on\nnatural language instructions, occasionally surpassing the performance of human\ncrowdworkers. Additionally, these models possess the capability to function in\nvarious languages within a single thread. Consequently, to generate new samples\nin different languages, we propose leveraging these capabilities to replicate\nthe data collection process. We introduce a pipeline for generating Open-Domain\nDialogue data in multiple Target Languages using LLMs, with demonstrations\nprovided in a unique Source Language. By eschewing explicit Machine Translation\nin this approach, we enhance the adherence to language-specific nuances. We\napply this methodology to the PersonaChat dataset. To enhance the openness of\ngenerated dialogues and mimic real life scenarii, we added the notion of speech\nevents corresponding to the type of conversation the speakers are involved in\nand also that of common ground which represents the premises of a conversation.", "published": "2025-03-05 12:52:14", "link": "http://arxiv.org/abs/2503.03462v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Visualising Policy-Reward Interplay to Inform Zeroth-Order Preference Optimisation of Large Language Models", "abstract": "Fine-tuning LLMs with first-order methods like back-propagation is\ncomputationally intensive. Zeroth-Order (ZO) optimisation, using function\nevaluations instead of gradients, reduces memory usage but suffers from slow\nconvergence in high-dimensional models. As a result, ZO research in LLMs has\nmostly focused on classification, overlooking more complex generative tasks. In\nthis paper, we introduce ZOPrO, a novel ZO algorithm designed for\n\\textit{Preference Optimisation} in LLMs. We begin by analysing the interplay\nbetween policy and reward models during traditional (first-order) Preference\nOptimisation, uncovering patterns in their relative updates. Guided by these\ninsights, we adapt Simultaneous Perturbation Stochastic Approximation (SPSA)\nwith a targeted sampling strategy to accelerate convergence. Through\nexperiments on summarisation, machine translation, and conversational\nassistants, we demonstrate that our method consistently enhances reward signals\nwhile achieving convergence times comparable to first-order methods. While it\nfalls short of some state-of-the-art methods, our work is the first to apply\nZeroth-Order methods to Preference Optimisation in LLMs, going beyond\nclassification tasks and paving the way for a largely unexplored research\ndirection. Code and visualisations are available at\nhttps://github.com/alessioGalatolo/VisZOPrO", "published": "2025-03-05 12:49:48", "link": "http://arxiv.org/abs/2503.03460v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unified Mind Model: Reimagining Autonomous Agents in the LLM Era", "abstract": "Large language models (LLMs) have recently demonstrated remarkable\ncapabilities across domains, tasks, and languages (e.g., ChatGPT and GPT-4),\nreviving the research of general autonomous agents with human-like cognitive\nabilities. Such human-level agents require semantic comprehension and\ninstruction-following capabilities, which exactly fall into the strengths of\nLLMs. Although there have been several initial attempts to build human-level\nagents based on LLMs, the theoretical foundation remains a challenging open\nproblem. In this paper, we propose a novel theoretical cognitive architecture,\nthe Unified Mind Model (UMM), which offers guidance to facilitate the rapid\ncreation of autonomous agents with human-level cognitive abilities.\nSpecifically, our UMM starts with the global workspace theory and further\nleverage LLMs to enable the agent with various cognitive abilities, such as\nmulti-modal perception, planning, reasoning, tool use, learning, memory,\nreflection and motivation. Building upon UMM, we then develop an agent-building\nengine, MindOS, which allows users to quickly create domain-/task-specific\nautonomous agents without any programming effort.", "published": "2025-03-05 12:49:44", "link": "http://arxiv.org/abs/2503.03459v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Taxation Perspectives from Large Language Models: A Case Study on Additional Tax Penalties", "abstract": "How capable are large language models (LLMs) in the domain of taxation?\nAlthough numerous studies have explored the legal domain in general, research\ndedicated to taxation remain scarce. Moreover, the datasets used in these\nstudies are either simplified, failing to reflect the real-world complexities,\nor unavailable as open source. To address this gap, we introduce PLAT, a new\nbenchmark designed to assess the ability of LLMs to predict the legitimacy of\nadditional tax penalties. PLAT is constructed to evaluate LLMs' understanding\nof tax law, particularly in cases where resolving the issue requires more than\njust applying related statutes. Our experiments with six LLMs reveal that their\nbaseline capabilities are limited, especially when dealing with conflicting\nissues that demand a comprehensive understanding. However, we found that\nenabling retrieval, self-reasoning, and discussion among multiple agents with\nspecific role assignments, this limitation can be mitigated.", "published": "2025-03-05 12:24:20", "link": "http://arxiv.org/abs/2503.03444v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RASD: Retrieval-Augmented Speculative Decoding", "abstract": "Speculative decoding accelerates inference in large language models (LLMs) by\ngenerating draft tokens for target model verification. Current approaches for\nobtaining draft tokens rely on lightweight draft models or additional model\nstructures to generate draft tokens and retrieve context from databases. Due to\nthe draft model's small size and limited training data, model-based speculative\ndecoding frequently becomes less effective in out-of-domain scenarios.\nAdditionally, the time cost of the drafting phase results in a low upper limit\non acceptance length during the verification step, limiting overall efficiency.\nThis paper proposes RASD (Retrieval-Augmented Speculative Decoding), which\nadopts retrieval methods to enhance model-based speculative decoding. We\nintroduce tree pruning and tree fusion to achieve this. Specifically, we\ndevelop a pruning method based on the draft model's probability distribution to\nconstruct the optimal retrieval tree. Second, we employ the longest prefix\nmatching algorithm to merge the tree generated by the draft model with the\nretrieval tree, resulting in a unified tree for verification. Experimental\nresults demonstrate that RASD achieves state-of-the-art inference acceleration\nacross tasks such as DocQA, Summary, Code, and In-Domain QA. Moreover, RASD\nexhibits strong scalability, seamlessly integrating with various speculative\ndecoding approaches, including both generation-based and retrieval-based\nmethods.", "published": "2025-03-05 12:10:14", "link": "http://arxiv.org/abs/2503.03434v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding Models Against Misinformation Edits", "abstract": "Online misinformation remains a critical challenge, and fact-checkers\nincreasingly rely on embedding-based methods to retrieve relevant fact-checks.\nYet, when debunked claims reappear in edited forms, the performance of these\nmethods is unclear. In this work, we introduce a taxonomy of six common\nreal-world misinformation edits and propose a perturbation framework that\ngenerates valid, natural claim variations. Our multi-stage retrieval evaluation\nreveals that standard embedding models struggle with user-introduced edits,\nwhile LLM-distilled embeddings offer improved robustness at a higher\ncomputational cost. Although a strong reranker helps mitigate some issues, it\ncannot fully compensate for first-stage retrieval gaps. Addressing these\nretrieval gaps, our train- and inference-time mitigation approaches enhance\nin-domain robustness by up to 17 percentage points and boost out-of-domain\ngeneralization by 10 percentage points over baseline models. Overall, our\nfindings provide practical improvements to claim-matching systems, enabling\nmore reliable fact-checking of evolving misinformation.", "published": "2025-03-05 11:47:32", "link": "http://arxiv.org/abs/2503.03417v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Serendipity of Claude AI: Case of the 13 Low-Resource National Languages of Mali", "abstract": "Recent advances in artificial intelligence (AI) and natural language\nprocessing (NLP) have improved the representation of underrepresented\nlanguages. However, most languages, including Mali's 13 official national\nlanguages, continue to be poorly supported or unsupported by automatic\ntranslation and generative AI. This situation appears to have slightly improved\nwith certain recent LLM releases. The study evaluated Claude AI's translation\nperformance on each of the 13 national languages of Mali. In addition to ChrF2\nand BLEU scores, human evaluators assessed translation accuracy, contextual\nconsistency, robustness to dialect variations, management of linguistic bias,\nadaptation to a limited corpus, and ease of understanding. The study found that\nClaude AI performs robustly for languages with very modest language resources\nand, while unable to produce understandable and coherent texts for Malian\nlanguages with minimal resources, still manages to produce results which\ndemonstrate the ability to mimic some elements of the language.", "published": "2025-03-05 10:55:47", "link": "http://arxiv.org/abs/2503.03380v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformers for molecular property prediction: Domain adaptation efficiently improves performance", "abstract": "Most of the current transformer-based chemical language models are\npre-trained on millions to billions of molecules. However, the improvement from\nsuch scaling in dataset size is not confidently linked to improved molecular\nproperty prediction. The aim of this study is to investigate and overcome some\nof the limitations of transformer models in predicting molecular properties.\nSpecifically, we examine the impact of pre-training dataset size and diversity\non the performance of transformer models and investigate the use of domain\nadaptation as a technique for improving model performance. First, our findings\nindicate that increasing pretraining dataset size beyond 400K molecules from\nthe GuacaMol dataset does not result in a significant improvement on four ADME\nendpoints, namely, solubility, permeability, microsomal stability, and plasma\nprotein binding. Second, our results demonstrate that using domain adaptation\nby further training the transformer model on a small set of domain-relevant\nmolecules, i.e., a few hundred to a few thousand, using multi-task regression\nof physicochemical properties was sufficient to significantly improve\nperformance for three out of the four investigated ADME endpoints (P-value <\n0.001). Finally, we observe that a model pre-trained on 400K molecules and\ndomain adopted on a few hundred/thousand molecules performs similarly (P-value\n> 0.05) to more complicated transformer models like MolBERT(pre-trained on 1.3M\nmolecules) and MolFormer (pre-trained on 100M molecules). A comparison to a\nrandom forest model trained on basic physicochemical properties showed similar\nperformance to the examined transformer models. We believe that current\ntransformer models can be improved through further systematic analysis of\npre-training and downstream data, pre-training objectives, and scaling laws,\nultimately leading to better and more helpful models.", "published": "2025-03-05 10:40:09", "link": "http://arxiv.org/abs/2503.03360v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "EnigmaToM: Improve LLMs' Theory-of-Mind Reasoning Capabilities with Neural Knowledge Base of Entity States", "abstract": "Theory-of-Mind (ToM), the ability to infer others' perceptions and mental\nstates, is fundamental to human interaction but remains a challenging task for\nLarge Language Models (LLMs). While existing ToM reasoning methods show promise\nwith reasoning via perceptual perspective-taking, they often rely excessively\non LLMs, reducing their efficiency and limiting their applicability to\nhigh-order ToM reasoning, which requires multi-hop reasoning about characters'\nbeliefs. To address these issues, we present EnigmaToM, a novel neuro-symbolic\nframework that enhances ToM reasoning by integrating a Neural Knowledge Base of\nentity states (Enigma) for (1) a psychology-inspired iterative masking\nmechanism that facilitates accurate perspective-taking and (2) knowledge\ninjection that elicits key entity information. Enigma generates structured\nrepresentations of entity states, which construct spatial scene graphs --\nleveraging spatial information as an inductive bias -- for belief tracking of\nvarious ToM orders and enhancing events with fine-grained entity state details.\nExperimental results on multiple benchmarks, including ToMi, HiToM, and FANToM,\nshow that EnigmaToM significantly improves ToM reasoning across LLMs of varying\nsizes, particularly excelling in high-order reasoning scenarios.", "published": "2025-03-05 10:13:05", "link": "http://arxiv.org/abs/2503.03340v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "iNews: A Multimodal Dataset for Modeling Personalized Affective Responses to News", "abstract": "Current approaches to emotion detection often overlook the inherent\nsubjectivity of affective experiences, instead relying on aggregated labels\nthat mask individual variations in emotional responses. We introduce iNews, a\nnovel large-scale dataset explicitly capturing subjective affective responses\nto news headlines. Our dataset comprises annotations from 291 demographically\ndiverse UK participants across 2,899 multimodal Facebook news posts from major\nUK outlets, with an average of 5.18 annotators per sample. For each post,\nannotators provide multifaceted labels including valence, arousal, dominance,\ndiscrete emotions, content relevance judgments, sharing likelihood, and\nmodality importance ratings (text, image, or both). Furthermore, we collect\ncomprehensive annotator persona information covering demographics, personality,\nmedia trust, and consumption patterns, which explain 15.2% of annotation\nvariance - higher than existing NLP datasets. Incorporating this information\nyields a 7% accuracy gain in zero-shot prediction and remains beneficial even\nwith 32-shot. iNews will enhance research in LLM personalization, subjectivity,\naffective computing, and individual-level behavior simulation.", "published": "2025-03-05 10:09:53", "link": "http://arxiv.org/abs/2503.03335v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "\"Only ChatGPT gets me\": An Empirical Analysis of GPT versus other Large Language Models for Emotion Detection in Text", "abstract": "This work investigates the capabilities of large language models (LLMs) in\ndetecting and understanding human emotions through text. Drawing upon emotion\nmodels from psychology, we adopt an interdisciplinary perspective that\nintegrates computational and affective sciences insights. The main goal is to\nassess how accurately they can identify emotions expressed in textual\ninteractions and compare different models on this specific task. This research\ncontributes to broader efforts to enhance human-computer interaction, making\nartificial intelligence technologies more responsive and sensitive to users'\nemotional nuances. By employing a methodology that involves comparisons with a\nstate-of-the-art model on the GoEmotions dataset, we aim to gauge LLMs'\neffectiveness as a system for emotional analysis, paving the way for potential\napplications in various fields that require a nuanced understanding of human\nlanguage.", "published": "2025-03-05 09:47:49", "link": "http://arxiv.org/abs/2503.04831v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph Foundation Models", "abstract": "Text-Attributed Graphs (TAGs), where each node is associated with text\ndescriptions, are ubiquitous in real-world scenarios. They typically exhibit\ndistinctive structure and domain-specific knowledge, motivating the development\nof a Graph Foundation Model (GFM) that generalizes across diverse graphs and\ntasks. Despite large efforts to integrate Large Language Models (LLMs) and\nGraph Neural Networks (GNNs) for TAGs, existing approaches suffer from\ndecoupled architectures with two-stage alignment, limiting their synergistic\npotential. Even worse, existing methods assign out-of-vocabulary (OOV) tokens\nto graph nodes, leading to graph-specific semantics, token explosion, and\nincompatibility with task-oriented prompt templates, which hinders cross-graph\nand cross-task transferability. To address these challenges, we propose\nPromptGFM, a versatile GFM for TAGs grounded in graph vocabulary learning.\nPromptGFM comprises two key components: (1) Graph Understanding Module, which\nexplicitly prompts LLMs to replicate the finest GNN workflow within the text\nspace, facilitating seamless GNN-LLM integration and elegant graph-text\nalignment; (2) Graph Inference Module, which establishes a language-based graph\nvocabulary ensuring expressiveness, transferability, and scalability, enabling\nreadable instructions for LLM fine-tuning. Extensive experiments demonstrate\nour superiority and transferability across diverse graphs and tasks. The code\nis available at this: https://github.com/agiresearch/PromptGFM.", "published": "2025-03-05 09:45:22", "link": "http://arxiv.org/abs/2503.03313v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The Box is in the Pen: Evaluating Commonsense Reasoning in Neural Machine Translation", "abstract": "Does neural machine translation yield translations that are congenial with\ncommon sense? In this paper, we present a test suite to evaluate the\ncommonsense reasoning capability of neural machine translation. The test suite\nconsists of three test sets, covering lexical and contextless/contextual\nsyntactic ambiguity that requires commonsense knowledge to resolve. We manually\ncreate 1,200 triples, each of which contain a source sentence and two\ncontrastive translations, involving 7 different common sense types. Language\nmodels pretrained on large-scale corpora, such as BERT, GPT-2, achieve a\ncommonsense reasoning accuracy of lower than 72% on target translations of this\ntest suite. We conduct extensive experiments on the test suite to evaluate\ncommonsense reasoning in neural machine translation and investigate factors\nthat have impact on this capability. Our experiments and analyses demonstrate\nthat neural machine translation performs poorly on commonsense reasoning of the\nthree ambiguity types in terms of both reasoning accuracy (60.1%) and reasoning\nconsistency (31%). The built commonsense test suite is available at\nhttps://github.com/tjunlp-lab/CommonMT.", "published": "2025-03-05 09:41:03", "link": "http://arxiv.org/abs/2503.03308v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SEOE: A Scalable and Reliable Semantic Evaluation Framework for Open Domain Event Detection", "abstract": "Automatic evaluation for Open Domain Event Detection (ODED) is a highly\nchallenging task, because ODED is characterized by a vast diversity of\nun-constrained output labels from various domains. Nearly all existing\nevaluation methods for ODED usually first construct evaluation benchmarks with\nlimited labels and domain coverage, and then evaluate ODED methods using\nmetrics based on token-level label matching rules. However, this kind of\nevaluation framework faces two issues: (1) The limited evaluation benchmarks\nlack representatives of the real world, making it difficult to accurately\nreflect the performance of various ODED methods in real-world scenarios; (2)\nEvaluation metrics based on token-level matching rules fail to capture semantic\nsimilarity between predictions and golden labels. To address these two problems\nabove, we propose a scalable and reliable Semantic-level Evaluation framework\nfor Open domain Event detection (SEOE) by constructing a more representative\nevaluation benchmark and introducing a semantic evaluation metric.\nSpecifically, our proposed framework first constructs a scalable evaluation\nbenchmark that currently includes 564 event types covering 7 major domains,\nwith a cost-effective supplementary annotation strategy to ensure the\nbenchmark's representativeness. The strategy also allows for the supplement of\nnew event types and domains in the future. Then, the proposed SEOE leverages\nlarge language models (LLMs) as automatic evaluation agents to compute a\nsemantic F1-score, incorporating fine-grained definitions of semantically\nsimilar labels to enhance the reliability of the evaluation. Extensive\nexperiments validate the representatives of the benchmark and the reliability\nof the semantic evaluation metric. Existing ODED methods are thoroughly\nevaluated, and the error patterns of predictions are analyzed, revealing\nseveral insightful findings.", "published": "2025-03-05 09:37:05", "link": "http://arxiv.org/abs/2503.03303v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Which books do I like?", "abstract": "Finding enjoyable fiction books can be challenging, partly because stories\nare multi-faceted and one's own literary taste might be difficult to ascertain.\nHere, we introduce the ISAAC method (Introspection-Support, AI-Annotation, and\nCuration), a pipeline which supports fiction readers in gaining awareness of\ntheir literary preferences and finding enjoyable books. ISAAC consists of four\nsteps: a user supplies book ratings, an AI agent researches and annotates the\nprovided books, patterns in book enjoyment are reviewed by the user, and the AI\nagent recommends new books. In this proof-of-concept self-study, the authors\ntest whether ISAAC can highlight idiosyncratic patterns in their book\nenjoyment, spark a deeper reflection about their literary tastes, and make\naccurate, personalized recommendations of enjoyable books and underexplored\nliterary niches. Results highlight substantial advantages of ISAAC over\nexisting methods such as an integration of automation and intuition, accurate\nand customizable annotations, and explainable book recommendations. Observed\ndisadvantages are that ISAAC's outputs can elicit false self-narratives (if\nstatistical patterns are taken at face value), that books cannot be annotated\nif their online documentation is lacking, and that people who are new to\nreading have to rely on assumed book ratings or movie ratings to power the\nISAAC pipeline. We discuss additional opportunities of ISAAC-style book\nannotations for the study of literary trends, and the scientific classification\nof books and readers.", "published": "2025-03-05 09:31:49", "link": "http://arxiv.org/abs/2503.03300v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions", "abstract": "Visual Language Models (VLMs) have demonstrated impressive capabilities in\nvisual grounding tasks. However, their effectiveness in the medical domain,\nparticularly for abnormality detection and localization within medical images,\nremains underexplored. A major challenge is the complex and abstract nature of\nmedical terminology, which makes it difficult to directly associate\npathological anomaly terms with their corresponding visual features. In this\nwork, we introduce a novel approach to enhance VLM performance in medical\nabnormality detection and localization by leveraging decomposed medical\nknowledge. Instead of directly prompting models to recognize specific\nabnormalities, we focus on breaking down medical concepts into fundamental\nattributes and common visual patterns. This strategy promotes a stronger\nalignment between textual descriptions and visual features, improving both the\nrecognition and localization of abnormalities in medical images.We evaluate our\nmethod on the 0.23B Florence-2 base model and demonstrate that it achieves\ncomparable performance in abnormality grounding to significantly larger 7B\nLLaVA-based medical VLMs, despite being trained on only 1.5% of the data used\nfor such models. Experimental results also demonstrate the effectiveness of our\napproach in both known and previously unseen abnormalities, suggesting its\nstrong generalization capabilities.", "published": "2025-03-05 09:02:33", "link": "http://arxiv.org/abs/2503.03278v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Cite Before You Speak: Enhancing Context-Response Grounding in E-commerce Conversational LLM-Agents", "abstract": "With the advancement of conversational large language models (LLMs), several\nLLM-based Conversational Shopping Agents (CSA) have been developed to help\ncustomers answer questions and smooth their shopping journey in e-commerce\ndomain. The primary objective in building a trustworthy CSA is to ensure the\nagent's responses are accurate and factually grounded, which is essential for\nbuilding customer trust and encouraging continuous engagement. However, two\nchallenges remain. First, LLMs produce hallucinated or unsupported claims. Such\ninaccuracies risk spreading misinformation and diminishing customer trust.\nSecond, without providing knowledge source attribution in CSA response,\ncustomers struggle to verify LLM-generated information. To address these\nchallenges, we present an easily productionized solution that enables a\n\"citation experience\" utilizing In-context Learning (ICL) and\nMulti-UX-Inference (MUI) to generate responses with citations to attribute its\noriginal sources without interfering other existing UX features. With proper UX\ndesign, these citation marks can be linked to the related product information\nand display the source to our customers. In this work, we also build\nauto-metrics and scalable benchmarks to holistically evaluate LLM's grounding\nand attribution capabilities. Our experiments demonstrate that incorporating\nthis citation generation paradigm can substantially enhance the grounding of\nLLM responses by 13.83% on the real-world data. As such, our solution not only\naddresses the immediate challenges of LLM grounding issues but also adds\ntransparency to conversational AI.", "published": "2025-03-05 08:58:35", "link": "http://arxiv.org/abs/2503.04830v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LexGenie: Automated Generation of Structured Reports for European Court of Human Rights Case Law", "abstract": "Analyzing large volumes of case law to uncover evolving legal principles,\nacross multiple cases, on a given topic is a demanding task for legal\nprofessionals. Structured topical reports provide an effective solution by\nsummarizing key issues, principles, and judgments, enabling comprehensive legal\nanalysis on a particular topic. While prior works have advanced query-based\nindividual case summarization, none have extended to automatically generating\nmulti-case structured reports. To address this, we introduce LexGenie, an\nautomated LLM-based pipeline designed to create structured reports using the\nentire body of case law on user-specified topics within the European Court of\nHuman Rights jurisdiction. LexGenie retrieves, clusters, and organizes relevant\npassages by topic to generate a structured outline and cohesive content for\neach section. Expert evaluation confirms LexGenie's utility in producing\nstructured reports that enhance efficient, scalable legal analysis.", "published": "2025-03-05 08:49:28", "link": "http://arxiv.org/abs/2503.03266v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Frontier LLMs Replace Annotators in Biomedical Text Mining? Analyzing Challenges and Exploring Solutions", "abstract": "Large language models (LLMs) can perform various natural language processing\n(NLP) tasks through in-context learning without relying on supervised data.\nHowever, multiple previous studies have reported suboptimal performance of LLMs\nin biological text mining. By analyzing failure patterns in these evaluations,\nwe identified three primary challenges for LLMs in biomedical corpora: (1) LLMs\nfail to learn implicit dataset-specific nuances from supervised data, (2) The\ncommon formatting requirements of discriminative tasks limit the reasoning\ncapabilities of LLMs particularly for LLMs that lack test-time compute, and (3)\nLLMs struggle to adhere to annotation guidelines and match exact schemas, which\nhinders their ability to understand detailed annotation requirements which is\nessential in biomedical annotation workflow. To address these challenges, we\nexperimented with prompt engineering techniques targeted to the above issues,\nand developed a pipeline that dynamically extracts instructions from annotation\nguidelines. Our findings show that frontier LLMs can approach or surpass the\nperformance of state-of-the-art (SOTA) BERT-based models with minimal reliance\non manually annotated data and without fine-tuning. Furthermore, we performed\nmodel distillation on a closed-source LLM, demonstrating that a BERT model\ntrained exclusively on synthetic data annotated by LLMs can also achieve a\npractical performance. Based on these results, we explored the feasibility of\npartially replacing manual annotation with LLMs in production scenarios for\nbiomedical text mining.", "published": "2025-03-05 08:37:10", "link": "http://arxiv.org/abs/2503.03261v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FANS -- Formal Answer Selection for Natural Language Math Reasoning Using Lean4", "abstract": "Large Language Models (LLMs) have displayed astonishing abilities in various\ntasks, especially in text generation, classification, question answering, etc.\nHowever, the reasoning ability of LLMs still faces many debates. The inherent\nambiguity of Natural Language (NL) limits LLMs' ability to perform verifiable\nreasoning, making its answers lack coherence and trustworthy support. To tackle\nthe above problems, we propose a novel framework named FANS: Formal ANswer\nSelection for Natural Language Math Reasoning Using Lean4. To the best of our\nknowledge, it is the first framework that utilizes Lean4 to enhance LLMs' NL\nmath reasoning ability. In particular, given an NL math question and\nLLM-generated answers, FANS first translates it into Lean4 theorem statements.\nThen it tries to prove it using a Lean4 prover and verify it by Lean4. Finally,\nit uses the FL result to assist in answer selection. It enhances LLMs' NL math\nability in providing a computer-verifiable solution for its correct answer and\nproposes an alternative method for answer selection beyond the reward model.\nExtensive experiments indicate the effectiveness of our framework. It can\nimprove the accuracy rate of reward model enhanced LLMs in the MATH-500 dataset\nby at most 1.91% and AMC-23 by at most 8.33% on strong reward-model baselines.\nIn some particular fields like number theory that Lean4 experts in, we can even\nselect all correct solutions. The qualitative analysis also shows our framework\ncan make NL results formally backed by Lean4 proofs. As a pioneering work in\nthe corresponding field, we will open-source all our models and datasets to\nfurther boost the development of the field.", "published": "2025-03-05 07:34:53", "link": "http://arxiv.org/abs/2503.03238v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Targeted Distillation for Sentiment Analysis", "abstract": "This paper presents a compact model that achieves strong sentiment analysis\ncapabilities through targeted distillation from advanced large language models\n(LLMs). Our methodology decouples the distillation target into two key\ncomponents: sentiment-related knowledge and task alignment. To transfer these\ncomponents, we propose a two-stage distillation framework. The first stage,\nknowledge-driven distillation (\\textsc{KnowDist}), transfers sentiment-related\nknowledge to enhance fundamental sentiment analysis capabilities. The second\nstage, in-context learning distillation (\\textsc{ICLDist}), transfers\ntask-specific prompt-following abilities to optimize task alignment. For\nevaluation, we introduce \\textsc{SentiBench}, a comprehensive sentiment\nanalysis benchmark comprising 3 task categories across 12 datasets. Experiments\non this benchmark demonstrate that our model effectively balances model size\nand performance, showing strong competitiveness compared to existing\nsmall-scale LLMs.", "published": "2025-03-05 06:45:25", "link": "http://arxiv.org/abs/2503.03225v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Next Word Prediction: Developing Comprehensive Evaluation Frameworks for measuring LLM performance on real world applications", "abstract": "While Large Language Models (LLMs) are fundamentally next-token prediction\nsystems, their practical applications extend far beyond this basic function.\nFrom natural language processing and text generation to conversational\nassistants and software use, LLMs have numerous use-cases, and have already\nacquired a significant degree of enterprise adoption. To evaluate such models,\nstatic evaluation datasets, consisting of a set of prompts and their\ncorresponding ground truths, are often used to benchmark the efficacy of the\nmodel for a particular task. In this paper, we provide the basis for a more\ncomprehensive evaluation framework, based upon a traditional game and\ntool-based architecture that enables a more overarching measurement of a\nmodel's capabilities. For simplicity, we provide a generalized foundation that\ncan be extended, without significant alteration, to numerous scenarios, from\nspecific use cases such as supply chain management or financial reasoning, to\nabstract measurements such as ethics or safety.", "published": "2025-03-05 06:44:38", "link": "http://arxiv.org/abs/2503.04828v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Preserving Cultural Identity with Context-Aware Translation Through Multi-Agent AI Systems", "abstract": "Language is a cornerstone of cultural identity, yet globalization and the\ndominance of major languages have placed nearly 3,000 languages at risk of\nextinction. Existing AI-driven translation models prioritize efficiency but\noften fail to capture cultural nuances, idiomatic expressions, and historical\nsignificance, leading to translations that marginalize linguistic diversity. To\naddress these challenges, we propose a multi-agent AI framework designed for\nculturally adaptive translation in underserved language communities. Our\napproach leverages specialized agents for translation, interpretation, content\nsynthesis, and bias evaluation, ensuring that linguistic accuracy and cultural\nrelevance are preserved. Using CrewAI and LangChain, our system enhances\ncontextual fidelity while mitigating biases through external validation.\nComparative analysis shows that our framework outperforms GPT-4o, producing\ncontextually rich and culturally embedded translations, a critical advancement\nfor Indigenous, regional, and low-resource languages. This research underscores\nthe potential of multi-agent AI in fostering equitable, sustainable, and\nculturally sensitive NLP technologies, aligning with the AI Governance,\nCultural NLP, and Sustainable NLP pillars of Language Models for Underserved\nCommunities. Our full experimental codebase is publicly available at:\nhttps://github.com/ciol-researchlab/Context-Aware_Translation_MAS", "published": "2025-03-05 06:43:59", "link": "http://arxiv.org/abs/2503.04827v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "primary_category": "cs.CL"}
{"title": "MA-LoT: Multi-Agent Lean-based Long Chain-of-Thought Reasoning enhances Formal Theorem Proving", "abstract": "Solving mathematical problems using computer-verifiable languages like Lean\nhas significantly impacted mathematical and computer science communities.\nState-of-the-art methods utilize single Large Language Models (LLMs) as agents\nor provers to either generate complete proof or perform tree searches. However,\nsingle-agent methods inherently lack a structured way to combine high-level\nreasoning in Natural Language (NL) with Formal Language (FL) verification\nfeedback. To solve these issues, we propose MA-LoT: Multi-Agent Lean-based Long\nChain-of-Thought framework, (to the best of our knowledge), the first\nmulti-agent framework for Lean4 theorem proving that balance high-level NL\nreasoning and FL verification in Long CoT. Using this structured interaction,\nour approach enables deeper insights and long-term coherence in proof\ngeneration, with which past methods struggle. We do this by leveraging emergent\nformal reasoning ability in Long CoT using our novel LoT-Transfer Learning\ntraining-inference pipeline. Extensive experiments show that our framework\nachieves a 61.07% accuracy rate on the Lean4 version of the MiniF2F-Test\ndataset, largely outperforming GPT-4 (22.95%), single-agent tree search\n(InternLM-Step-Prover, 50.70%), and whole-proof generation (Godel-Prover,\n55.33%) baselines. Furthermore, our findings highlight the potential of\ncombining Long CoT with formal verification for a more insightful generation in\na broader perspective.", "published": "2025-03-05 05:50:31", "link": "http://arxiv.org/abs/2503.03205v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Robust Universal Information Extraction: Benchmark, Evaluation, and Solution", "abstract": "In this paper, we aim to enhance the robustness of Universal Information\nExtraction (UIE) by introducing a new benchmark dataset, a comprehensive\nevaluation, and a feasible solution. Existing robust benchmark datasets have\ntwo key limitations: 1) They generate only a limited range of perturbations for\na single Information Extraction (IE) task, which fails to evaluate the\nrobustness of UIE models effectively; 2) They rely on small models or\nhandcrafted rules to generate perturbations, often resulting in unnatural\nadversarial examples. Considering the powerful generation capabilities of Large\nLanguage Models (LLMs), we introduce a new benchmark dataset for Robust UIE,\ncalled RUIE-Bench, which utilizes LLMs to generate more diverse and realistic\nperturbations across different IE tasks. Based on this dataset, we\ncomprehensively evaluate existing UIE models and reveal that both LLM-based\nmodels and other models suffer from significant performance drops. To improve\nrobustness and reduce training costs, we propose a data-augmentation solution\nthat dynamically selects hard samples for iterative training based on the\nmodel's inference loss. Experimental results show that training with only\n\\textbf{15\\%} of the data leads to an average \\textbf{7.5\\%} relative\nperformance improvement across three IE tasks.", "published": "2025-03-05 05:39:29", "link": "http://arxiv.org/abs/2503.03201v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sarcasm Detection as a Catalyst: Improving Stance Detection with Cross-Target Capabilities", "abstract": "Stance Detection (SD) has become a critical area of interest due to its\napplications in various contexts leading to increased research within NLP. Yet\nthe subtlety and complexity of texts sourced from online platforms often\ncontaining sarcastic language pose significant challenges for SD algorithms in\naccurately determining the authors stance. This paper addresses this by\nemploying sarcasm for SD. It also tackles the issue of insufficient annotated\ndata for training SD models on new targets by conducting Cross-Target SD\n(CTSD). The proposed approach involves fine-tuning BERT and RoBERTa models\nfollowed by concatenating additional deep learning layers. The approach is\nassessed against various State-Of-The-Art baselines for SD demonstrating\nsuperior performance using publicly available datasets. Notably our model\noutperforms the best SOTA models on both in-domain SD and CTSD tasks even\nbefore the incorporation of sarcasm-detection pre-training. The integration of\nsarcasm knowledge into the model significantly reduces misclassifications of\nsarcastic text elements in SD allowing our model to accurately predict 85% of\ntexts that were previously misclassified without sarcasm-detection pre-training\non in-domain SD. This enhancement contributes to an increase in the models\naverage macro F1-score. The CTSD task achieves performance comparable to that\nof the in-domain task despite using a zero-shot finetuning. We also reveal that\nthe success of the transfer-learning framework relies on the correlation\nbetween the lexical attributes of sarcasm detection and SD. This study\nrepresents the first exploration of sarcasm detection as an intermediate\ntransfer-learning task within the context of SD while also leveraging the\nconcatenation of BERT or RoBERTa with other deep-learning techniques. The\nproposed approach establishes a foundational baseline for future research in\nthis domain.", "published": "2025-03-05 05:27:16", "link": "http://arxiv.org/abs/2503.03787v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Structured Outputs Enable General-Purpose LLMs to be Medical Experts", "abstract": "Medical question-answering (QA) is a critical task for evaluating how\neffectively large language models (LLMs) encode clinical knowledge and\nassessing their potential applications in medicine. Despite showing promise on\nmultiple-choice tests, LLMs frequently struggle with open-ended medical\nquestions, producing responses with dangerous hallucinations or lacking\ncomprehensive coverage of critical aspects. Existing approaches attempt to\naddress these challenges through domain-specific fine-tuning, but this proves\nresource-intensive and difficult to scale across models. To improve the\ncomprehensiveness and factuality of medical responses, we propose a novel\napproach utilizing structured medical reasoning. Our method guides LLMs through\nan seven-step cognitive process inspired by clinical diagnosis, enabling more\naccurate and complete answers without additional training. Experiments on the\nMedLFQA benchmark demonstrate that our approach achieves the highest Factuality\nScore of 85.8, surpassing fine-tuned models. Notably, this improvement\ntransfers to smaller models, highlighting the method's efficiency and\nscalability. Our code and datasets are available.", "published": "2025-03-05 05:24:55", "link": "http://arxiv.org/abs/2503.03194v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Designing Speech Technologies for Australian Aboriginal English: Opportunities, Risks and Participation", "abstract": "In Australia, post-contact language varieties, including creoles and local\nvarieties of international languages, emerged as a result of forced contact\nbetween Indigenous communities and English speakers. These contact varieties\nare widely used, yet are poorly supported by language technologies. This gap\npresents barriers to participation in civil and economic society for Indigenous\ncommunities using these varieties, and reproduces minoritisation of\ncontemporary Indigenous sociolinguistic identities. This paper concerns three\nquestions regarding this context. First, can speech technologies support\nspeakers of Australian Aboriginal English, a local indigenised variety of\nEnglish? Second, what risks are inherent in such a project? Third, what\ntechnology development practices are appropriate for this context, and how can\nresearchers integrate meaningful community participation in order to mitigate\nrisks? We argue that opportunities do exist -- as well as risks -- and\ndemonstrate this through a case study exploring design practices in a\nreal-world project aiming to improve speech technologies for Australian\nAboriginal English. We discuss how we integrated culturally appropriate and\nparticipatory processes throughout the project. We call for increased support\nfor languages used by Indigenous communities, including contact varieties,\nwhich provide practical economic and socio-cultural benefits, provided that\nparticipatory and culturally safe practices are enacted.", "published": "2025-03-05 05:07:39", "link": "http://arxiv.org/abs/2503.03186v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intermediate-Task Transfer Learning: Leveraging Sarcasm Detection for Stance Detection", "abstract": "Stance Detection (SD) on social media has emerged as a prominent area of\ninterest with implications for social business and political applications\nthereby garnering escalating research attention within NLP. The inherent\nsubtlety and complexity of texts procured from online platforms pose challenges\nfor SD algorithms in accurately discerning the authors stance. Mostly the\ninclusion of sarcastic and figurative language drastically impacts the\nperformance of SD models. This paper addresses this by employing sarcasm\ndetection intermediate-task transfer learning tailored for SD. The proposed\nmethodology involves the finetuning of BERT and RoBERTa and the concatenation\nof convolutional BiLSTM and dense layers. Rigorous experiments are conducted on\npublicly available datasets to evaluate our transfer-learning framework. The\nperformance of the approach is assessed against various State-Of-The-Art\nbaselines for SD providing empirical evidence of its effectiveness. Notably our\nmodel outperforms the best SOTA models even prior to sarcasm-detection\npretraining. The integration of sarcasm knowledge into the model proves\ninstrumental in mitigating misclassifications of sarcastic textual elements in\nSD. Our model accurately predicts 85% of texts that were previously\nmisclassified by the model without sarcasm-detection pretraining thereby\namplifying the average F1-score of the model. Our experiments also revealed\nthat the success of the transfer-learning framework is contingent upon the\ncorrelation of lexical attributes between the intermediate task and the target\ntask. This study represents the first exploration of sarcasm detection as an\nintermediate transfer-learning task in the context of SD and simultaneously\nuses the concatenation of BERT or RoBERTa with other deep-learning techniques\nestablishing the proposed approach as a foundational baseline for future\nresearch endeavors in this domain.", "published": "2025-03-05 04:30:53", "link": "http://arxiv.org/abs/2503.03172v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DSVD: Dynamic Self-Verify Decoding for Faithful Generation in Large Language Models", "abstract": "The reliability of large language models remains a critical challenge,\nparticularly due to their susceptibility to hallucinations and factual\ninaccuracies during text generation. Existing solutions either underutilize\nmodels' self-correction with preemptive strategies or use costly post-hoc\nverification. To further explore the potential of real-time self-verification\nand correction, we present Dynamic Self-Verify Decoding (DSVD), a novel\ndecoding framework that enhances generation reliability through real-time\nhallucination detection and efficient error correction. DSVD integrates two key\ncomponents: (1) parallel self-verification architecture for continuous quality\nassessment, (2) dynamic rollback mechanism for targeted error recovery.\nExtensive experiments across five benchmarks demonstrate DSVD's effectiveness,\nachieving significant improvement in truthfulness (Quesetion-Answering) and\nfactual accuracy (FActScore). Results show the DSVD can be further incorporated\nwith existing faithful decoding methods to achieve stronger performance. Our\nwork establishes that real-time self-verification during generation offers a\nviable path toward more trustworthy language models without sacrificing\npractical deployability.", "published": "2025-03-05 03:45:50", "link": "http://arxiv.org/abs/2503.03149v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Understanding Multi-Round Large Language Model Reasoning: Approximability, Learnability and Generalizability", "abstract": "Recent advancements in cognitive science and multi-round reasoning techniques\nfor Large Language Models (LLMs) suggest that iterative thinking processes\nimprove problem-solving performance in complex tasks. Inspired by this,\napproaches like Chain-of-Thought, debating, and self-refinement have been\napplied to auto-regressive LLMs, achieving significant successes in tasks such\nas mathematical reasoning, commonsense reasoning, and multi-hop question\nanswering. Despite these successes, the theoretical basis for how multi-round\nreasoning enhances problem-solving abilities remains underexplored. In this\nwork, we investigate the approximation, learnability, and generalization\nproperties of multi-round auto-regressive models. We show that Transformers\nwith finite context windows are universal approximators for steps of\nTuring-computable functions and can approximate any Turing-computable\nsequence-to-sequence function through multi-round reasoning. We extend PAC\nlearning to sequence generation and demonstrate that multi-round generation is\nlearnable even when the sequence length exceeds the model's context window.\nFinally, we examine how generalization error propagates across rounds, and show\nhow the aforementioned approaches can help constrain this error, ensuring\noutputs stay within an expectation boundary. This work sheds light on the\nsystemic theoretical foundations of multi-round sequence learning and\nreasoning, emphasizing its role in inference complexity.", "published": "2025-03-05 02:50:55", "link": "http://arxiv.org/abs/2503.03128v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.AI"}
{"title": "The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models", "abstract": "Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language\nModels (LLMs) with human preferences, particularly as LLMs increasingly\ninteract with multimodal data. However, we find that MM-RMs trained on existing\ndatasets often struggle to generalize to out-of-distribution data due to their\nreliance on unimodal spurious correlations, primarily text-only shortcuts\nwithin the training distribution, which prevents them from leveraging true\nmultimodal reward functions. To address this, we introduce a Shortcut-aware\nMM-RM learning algorithm that mitigates this issue by dynamically reweighting\ntraining samples, shifting the distribution toward better multimodal\nunderstanding, and reducing dependence on unimodal spurious correlations. Our\nexperiments demonstrate significant improvements in generalization, downstream\ntask performance, and scalability, establishing a more robust framework for\nmultimodal reward modeling.", "published": "2025-03-05 02:37:41", "link": "http://arxiv.org/abs/2503.03122v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "External Reliable Information-enhanced Multimodal Contrastive Learning for Fake News Detection", "abstract": "With the rapid development of the Internet, the information dissemination\nparadigm has changed and the efficiency has been improved greatly. While this\nalso brings the quick spread of fake news and leads to negative impacts on\ncyberspace. Currently, the information presentation formats have evolved\ngradually, with the news formats shifting from texts to multimodal contents. As\na result, detecting multimodal fake news has become one of the research\nhotspots. However, multimodal fake news detection research field still faces\ntwo main challenges: the inability to fully and effectively utilize multimodal\ninformation for detection, and the low credibility or static nature of the\nintroduced external information, which limits dynamic updates. To bridge the\ngaps, we propose ERIC-FND, an external reliable information-enhanced multimodal\ncontrastive learning framework for fake news detection. ERIC-FND strengthens\nthe representation of news contents by entity-enriched external information\nenhancement method. It also enriches the multimodal news information via\nmultimodal semantic interaction method where the multimodal constrative\nlearning is employed to make different modality representations learn from each\nother. Moreover, an adaptive fusion method is taken to integrate the news\nrepresentations from different dimensions for the eventual classification.\nExperiments are done on two commonly used datasets in different languages, X\n(Twitter) and Weibo. Experiment results demonstrate that our proposed model\nERIC-FND outperforms existing state-of-the-art fake news detection methods\nunder the same settings.", "published": "2025-03-05 02:07:38", "link": "http://arxiv.org/abs/2503.03107v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HeTGB: A Comprehensive Benchmark for Heterophilic Text-Attributed Graphs", "abstract": "Graph neural networks (GNNs) have demonstrated success in modeling relational\ndata primarily under the assumption of homophily. However, many real-world\ngraphs exhibit heterophily, where linked nodes belong to different categories\nor possess diverse attributes. Additionally, nodes in many domains are\nassociated with textual descriptions, forming heterophilic text-attributed\ngraphs (TAGs). Despite their significance, the study of heterophilic TAGs\nremains underexplored due to the lack of comprehensive benchmarks. To address\nthis gap, we introduce the Heterophilic Text-attributed Graph Benchmark\n(HeTGB), a novel benchmark comprising five real-world heterophilic graph\ndatasets from diverse domains, with nodes enriched by extensive textual\ndescriptions. HeTGB enables systematic evaluation of GNNs, pre-trained language\nmodels (PLMs) and co-training methods on the node classification task. Through\nextensive benchmarking experiments, we showcase the utility of text attributes\nin heterophilic graphs, analyze the challenges posed by heterophilic TAGs and\nthe limitations of existing models, and provide insights into the interplay\nbetween graph structures and textual attributes. We have publicly released\nHeTGB with baseline implementations to facilitate further research in this\nfield.", "published": "2025-03-05 02:00:32", "link": "http://arxiv.org/abs/2503.04822v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Monitoring Decoding: Mitigating Hallucination via Evaluating the Factuality of Partial Response during Generation", "abstract": "While large language models have demonstrated exceptional performance across\na wide range of tasks, they remain susceptible to hallucinations -- generating\nplausible yet factually incorrect contents. Existing methods to mitigating such\nrisk often rely on sampling multiple full-length generations, which introduces\nsignificant response latency and becomes ineffective when the model\nconsistently produces hallucinated outputs with high confidence. To address\nthese limitations, we introduce Monitoring Decoding (MD), a novel framework\nthat dynamically monitors the generation process and selectively applies\nin-process interventions, focusing on revising crucial tokens responsible for\nhallucinations. Instead of waiting until completion of multiple full-length\ngenerations, we identify hallucination-prone tokens during generation using a\nmonitor function, and further refine these tokens through a tree-based decoding\nstrategy. This approach ensures an enhanced factual accuracy and coherence in\nthe generated output while maintaining efficiency. Experimental results\ndemonstrate that MD consistently outperforms self-consistency-based approaches\nin both effectiveness and efficiency, achieving higher factual accuracy while\nsignificantly reducing computational overhead.", "published": "2025-03-05 01:51:03", "link": "http://arxiv.org/abs/2503.03106v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-modal Causal Relation Alignment for Video Question Grounding", "abstract": "Video question grounding (VideoQG) requires models to answer the questions\nand simultaneously infer the relevant video segments to support the answers.\nHowever, existing VideoQG methods usually suffer from spurious cross-modal\ncorrelations, leading to a failure to identify the dominant visual scenes that\nalign with the intended question. Moreover, vision-language models exhibit\nunfaithful generalization performance and lack robustness on challenging\ndownstream tasks such as VideoQG. In this work, we propose a novel VideoQG\nframework named Cross-modal Causal Relation Alignment (CRA), to eliminate\nspurious correlations and improve the causal consistency between\nquestion-answering and video temporal grounding. Our CRA involves three\nessential components: i) Gaussian Smoothing Grounding (GSG) module for\nestimating the time interval via cross-modal attention, which is de-noised by\nan adaptive Gaussian filter, ii) Cross-Modal Alignment (CMA) enhances the\nperformance of weakly supervised VideoQG by leveraging bidirectional\ncontrastive learning between estimated video segments and QA features, iii)\nExplicit Causal Intervention (ECI) module for multimodal deconfounding, which\ninvolves front-door intervention for vision and back-door intervention for\nlanguage. Extensive experiments on two VideoQG datasets demonstrate the\nsuperiority of our CRA in discovering visually grounded content and achieving\nrobust question reasoning. Codes are available at\nhttps://github.com/WissingChen/CRA-GQA.", "published": "2025-03-05 01:36:32", "link": "http://arxiv.org/abs/2503.07635v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "MuCo-KGC: Multi-Context-Aware Knowledge Graph Completion", "abstract": "Knowledge graph completion (KGC) seeks to predict missing entities (e.g.,\nheads or tails) or relationships in knowledge graphs (KGs), which often contain\nincomplete data. Traditional embedding-based methods, such as TransE and\nComplEx, have improved tail entity prediction but struggle to generalize to\nunseen entities during testing. Textual-based models mitigate this issue by\nleveraging additional semantic context; however, their reliance on negative\ntriplet sampling introduces high computational overhead, semantic\ninconsistencies, and data imbalance. Recent approaches, like KG-BERT, show\npromise but depend heavily on entity descriptions, which are often unavailable\nin KGs. Critically, existing methods overlook valuable structural information\nin the KG related to the entities and relationships. To address these\nchallenges, we propose Multi-Context-Aware Knowledge Graph Completion\n(MuCo-KGC), a novel model that utilizes contextual information from linked\nentities and relations within the graph to predict tail entities. MuCo-KGC\neliminates the need for entity descriptions and negative triplet sampling,\nsignificantly reducing computational complexity while enhancing performance.\nOur experiments on standard datasets, including FB15k-237, WN18RR, CoDEx-S, and\nCoDEx-M, demonstrate that MuCo-KGC outperforms state-of-the-art methods on\nthree datasets. Notably, MuCo-KGC improves MRR on WN18RR, and CoDEx-S and\nCoDEx-M datasets by $1.63\\%$, and $3.77\\%$ and $20.15\\%$ respectively,\ndemonstrating its effectiveness for KGC tasks.", "published": "2025-03-05 01:18:11", "link": "http://arxiv.org/abs/2503.03091v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Graph Width Perspective on Partially Ordered Hamiltonian Paths", "abstract": "We consider the problem of finding a Hamiltonian path with precedence\nconstraints in the form of a partial order on the vertex set. This problem is\nknown as Partially Ordered Hamiltonian Path Problem (POHPP). Here, we study the\ncomplexity for graph width parameters for which the ordinary Hamiltonian Path\nproblem is in $\\mathsf{FPT}$. We show that POHPP is $\\mathsf{NP}$-complete for\ngraphs of pathwidth 4. We complement this result by giving polynomial-time\nalgorithms for graphs of pathwidth 3 and treewidth 2. Furthermore, we show that\nPOHPP is $\\mathsf{NP}$-hard for graphs of clique cover number 2 and\n$\\mathsf{W[1]}$-hard for some distance-to-$\\mathcal{G}$ parameters, including\ndistance to path and distance to clique. In addition, we present $\\mathsf{XP}$\nand $\\mathsf{FPT}$ algorithms for parameters such as distance to block and\nfeedback edge set number.", "published": "2025-03-05 14:41:59", "link": "http://arxiv.org/abs/2503.03553v1", "categories": ["cs.DM", "cs.CC", "cs.DS", "math.CO"], "primary_category": "cs.DM"}
{"title": "Learning to Negotiate via Voluntary Commitment", "abstract": "The partial alignment and conflict of autonomous agents lead to mixed-motive\nscenarios in many real-world applications. However, agents may fail to\ncooperate in practice even when cooperation yields a better outcome. One well\nknown reason for this failure comes from non-credible commitments. To\nfacilitate commitments among agents for better cooperation, we define Markov\nCommitment Games (MCGs), a variant of commitment games, where agents can\nvoluntarily commit to their proposed future plans. Based on MCGs, we propose a\nlearnable commitment protocol via policy gradients. We further propose\nincentive-compatible learning to accelerate convergence to equilibria with\nbetter social welfare. Experimental results in challenging mixed-motive tasks\ndemonstrate faster empirical convergence and higher returns for our method\ncompared with its counterparts. Our code is available at\nhttps://github.com/shuhui-zhu/DCL.", "published": "2025-03-05 19:55:10", "link": "http://arxiv.org/abs/2503.03866v2", "categories": ["cs.AI", "cs.GT", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "RiskAgent: Autonomous Medical AI Copilot for Generalist Risk Prediction", "abstract": "The application of Large Language Models (LLMs) to various clinical\napplications has attracted growing research attention. However, real-world\nclinical decision-making differs significantly from the standardized,\nexam-style scenarios commonly used in current efforts. In this paper, we\npresent the RiskAgent system to perform a broad range of medical risk\npredictions, covering over 387 risk scenarios across diverse complex diseases,\ne.g., cardiovascular disease and cancer. RiskAgent is designed to collaborate\nwith hundreds of clinical decision tools, i.e., risk calculators and scoring\nsystems that are supported by evidence-based medicine. To evaluate our method,\nwe have built the first benchmark MedRisk specialized for risk prediction,\nincluding 12,352 questions spanning 154 diseases, 86 symptoms, 50 specialties,\nand 24 organ systems. The results show that our RiskAgent, with 8 billion model\nparameters, achieves 76.33% accuracy, outperforming the most recent commercial\nLLMs, o1, o3-mini, and GPT-4.5, and doubling the 38.39% accuracy of GPT-4o. On\nrare diseases, e.g., Idiopathic Pulmonary Fibrosis (IPF), RiskAgent outperforms\no1 and GPT-4.5 by 27.27% and 45.46% accuracy, respectively. Finally, we further\nconduct a generalization evaluation on an external evidence-based diagnosis\nbenchmark and show that our RiskAgent achieves the best results. These\nencouraging results demonstrate the great potential of our solution for diverse\ndiagnosis domains. To improve the adaptability of our model in different\nscenarios, we have built and open-sourced a family of models ranging from 1\nbillion to 70 billion parameters. Our code, data, and models are all available\nat https://github.com/AI-in-Health/RiskAgent.", "published": "2025-03-05 18:46:51", "link": "http://arxiv.org/abs/2503.03802v1", "categories": ["cs.LG", "cs.AI", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Human Implicit Preference-Based Policy Fine-tuning for Multi-Agent Reinforcement Learning in USV Swarm", "abstract": "Multi-Agent Reinforcement Learning (MARL) has shown promise in solving\ncomplex problems involving cooperation and competition among agents, such as an\nUnmanned Surface Vehicle (USV) swarm used in search and rescue, surveillance,\nand vessel protection. However, aligning system behavior with user preferences\nis challenging due to the difficulty of encoding expert intuition into reward\nfunctions. To address the issue, we propose a Reinforcement Learning with Human\nFeedback (RLHF) approach for MARL that resolves credit-assignment challenges\nthrough an Agent-Level Feedback system categorizing feedback into intra-agent,\ninter-agent, and intra-team types. To overcome the challenges of direct human\nfeedback, we employ a Large Language Model (LLM) evaluator to validate our\napproach using feedback scenarios such as region constraints, collision\navoidance, and task allocation. Our method effectively refines USV swarm\npolicies, addressing key challenges in multi-agent systems while maintaining\nfairness and performance consistency.", "published": "2025-03-05 14:33:18", "link": "http://arxiv.org/abs/2503.03796v2", "categories": ["cs.MA", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "cs.MA"}
{"title": "Impact of Level 2/3 Automated Driving Technology on Road Work Zone Safety", "abstract": "As China's road network enters the maintenance era, work zones will become a\ncommon sight on the roads. With the development of automated driving, vehicles\nequipped with Level 2/3 automated driving capabilities will also become a\ncommon presence on the roads. When these vehicles pass through work zones,\nautomated driving may disengage, which can have complex effects on traffic\nsafety. This paper explores the impact of Level 2/3 automated driving\ntechnology on road safety in high-speed highway work zone environments. Through\nmicroscopic traffic simulation method and using full-type traffic conflict\ntechnique, factors such as market penetration rate (MPR), traffic volume level,\ndisengagement threshold, and driver takeover style are studied to understand\ntheir impact on work zone safety. The study found that the impact of automated\ndriving technology on work zone safety is complex. Disengagement of automated\nvehicles in work zones reduces the proportion of vehicles that can maintain\nautomated driving status. If takeover is not timely or adequate, it can easily\nlead to new traffic conflicts. Different factors have varying degrees of impact\non work zone safety. Increasing MPR helps reduce the occurrence of\nsingle-vehicle conflicts, but it also increases the possibility of\nmulti-vehicle conflicts. Therefore, future research and improvement directions\nshould focus on optimizing the disengagement detection and takeover mechanisms\nof automated driving systems.", "published": "2025-03-05 00:26:53", "link": "http://arxiv.org/abs/2503.07634v1", "categories": ["cs.AI", "cs.MA", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Multimodal Stock Price Prediction: A Case Study of the Russian Securities Market", "abstract": "Classical asset price forecasting methods primarily rely on numerical data,\nsuch as price time series, trading volumes, limit order book data, and\ntechnical analysis indicators. However, the news flow plays a significant role\nin price formation, making the development of multimodal approaches that\ncombine textual and numerical data for improved prediction accuracy highly\nrelevant. This paper addresses the problem of forecasting financial asset\nprices using the multimodal approach that combines candlestick time series and\ntextual news flow data. A unique dataset was collected for the study, which\nincludes time series for 176 Russian stocks traded on the Moscow Exchange and\n79,555 financial news articles in Russian. For processing textual data,\npre-trained models RuBERT and Vikhr-Qwen2.5-0.5b-Instruct (a large language\nmodel) were used, while time series and vectorized text data were processed\nusing an LSTM recurrent neural network. The experiments compared models based\non a single modality (time series only) and two modalities, as well as various\nmethods for aggregating text vector representations. Prediction quality was\nestimated using two key metrics: Accuracy (direction of price movement\nprediction: up or down) and Mean Absolute Percentage Error (MAPE), which\nmeasures the deviation of the predicted price from the true price. The\nexperiments showed that incorporating textual modality reduced the MAPE value\nby 55%. The resulting multimodal dataset holds value for the further adaptation\nof language models in the financial sector. Future research directions include\noptimizing textual modality parameters, such as the time window, sentiment, and\nchronological order of news messages.", "published": "2025-03-05 21:20:32", "link": "http://arxiv.org/abs/2503.08696v1", "categories": ["q-fin.ST", "cs.LG", "q-fin.CP"], "primary_category": "q-fin.ST"}
{"title": "Large language models in finance : what is financial sentiment?", "abstract": "Financial sentiment has become a crucial yet complex concept in finance,\nincreasingly used in market forecasting and investment strategies. Despite its\ngrowing importance, there remains a need to define and understand what\nfinancial sentiment truly represents and how it can be effectively measured. We\nexplore the nature of financial sentiment and investigate how large language\nmodels (LLMs) contribute to its estimation. We trace the evolution of sentiment\nmeasurement in finance, from market-based and lexicon-based methods to advanced\nnatural language processing techniques. The emergence of LLMs has significantly\nenhanced sentiment analysis, providing deeper contextual understanding and\ngreater accuracy in extracting sentiment from financial text. We examine how\nBERT-based models, such as RoBERTa and FinBERT, are optimized for structured\nsentiment classification, while GPT-based models, including GPT-4, OPT, and\nLLaMA, excel in financial text generation and real-time sentiment\ninterpretation. A comparative analysis of bidirectional and autoregressive\ntransformer architectures highlights their respective roles in investor\nsentiment analysis, algorithmic trading, and financial decision-making. By\nexploring what financial sentiment is and how it is estimated within LLMs, we\nprovide insights into the growing role of AI-driven sentiment analysis in\nfinance.", "published": "2025-03-05 15:51:25", "link": "http://arxiv.org/abs/2503.03612v4", "categories": ["q-fin.ST", "q-fin.CP", "q-fin.GN", "I.2.7; H.3.3; H.3.4; J.4; J.1"], "primary_category": "q-fin.ST"}
{"title": "Constructing elicitable risk measures", "abstract": "We provide a constructive way of defining new elicitable risk measures that\nare characterised by a multiplicative scoring function. We show that depending\non the choice of the scoring function's components, the resulting risk measure\npossesses properties such as monotonicity, translation invariance, convexity,\nand positive homogeneity. Our framework encompasses the majority of well-known\nelicitable risk measures including all elicitable convex and coherent risk\nmeasures. Our setting moreover allows to construct novel elicitable risk\nmeasures that are, for example, convex but not coherent. Furthermore, we\ndiscuss how higher-order elicitability, such as jointly eliciting the mean and\nvariance or different quantile levels, fall within our setting.", "published": "2025-03-05 13:08:31", "link": "http://arxiv.org/abs/2503.03471v1", "categories": ["q-fin.MF", "q-fin.RM"], "primary_category": "q-fin.MF"}
{"title": "Looking into informal currency markets as Limit Order Books: impact of market makers", "abstract": "This study pioneers the application of the market microstructure framework to\nan informal financial market. By scraping data from websites and social media\nabout the Cuban informal currency market, we model the dynamics of bid/ask\nintentions using a Limit Order Book (LOB). This approach enables us to study\nkey characteristics such as liquidity, stability and volume profiles. We\ncontinue exploiting the Avellaneda-Stoikov model to explore the impact of\nintroducing a Market Maker (MM) into this informal setting, assessing its\ninfluence on the market structure and the bid/ask dynamics. We show that the\nMarket Maker improves the quality of the market. Beyond their academic\nsignificance, we believe that our findings are relevant for policymakers\nseeking to intervene informal markets with limited resources.", "published": "2025-03-05 19:41:22", "link": "http://arxiv.org/abs/2503.03858v1", "categories": ["q-fin.TR", "91B26, 91B24"], "primary_category": "q-fin.TR"}
