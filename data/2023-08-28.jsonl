{"title": "ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large\n  Language Models", "abstract": "The unprecedented performance of large language models (LLMs) requires\ncomprehensive and accurate evaluation. We argue that for LLMs evaluation,\nbenchmarks need to be comprehensive and systematic. To this end, we propose the\nZhuJiu benchmark, which has the following strengths: (1) Multi-dimensional\nability coverage: We comprehensively evaluate LLMs across 7 ability dimensions\ncovering 51 tasks. Especially, we also propose a new benchmark that focuses on\nknowledge ability of LLMs. (2) Multi-faceted evaluation methods collaboration:\nWe use 3 different yet complementary evaluation methods to comprehensively\nevaluate LLMs, which can ensure the authority and accuracy of the evaluation\nresults. (3) Comprehensive Chinese benchmark: ZhuJiu is the pioneering\nbenchmark that fully assesses LLMs in Chinese, while also providing equally\nrobust evaluation abilities in English. (4) Avoiding potential data leakage: To\navoid data leakage, we construct evaluation data specifically for 37 tasks. We\nevaluate 10 current mainstream LLMs and conduct an in-depth discussion and\nanalysis of their results. The ZhuJiu benchmark and open-participation\nleaderboard are publicly released at http://www.zhujiu-benchmark.com/ and we\nalso provide a demo video at https://youtu.be/qypkJ89L1Ic.", "published": "2023-08-28 06:56:44", "link": "http://arxiv.org/abs/2308.14353v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GADePo: Graph-Assisted Declarative Pooling Transformers for\n  Document-Level Relation Extraction", "abstract": "Document-level relation extraction typically relies on text-based encoders\nand hand-coded pooling heuristics to aggregate information learned by the\nencoder. In this paper, we leverage the intrinsic graph processing capabilities\nof the Transformer model and propose replacing hand-coded pooling methods with\nnew tokens in the input, which are designed to aggregate information via\nexplicit graph relations in the computation of attention weights. We introduce\na joint text-graph Transformer model and a graph-assisted declarative pooling\n(GADePo) specification of the input, which provides explicit and high-level\ninstructions for information aggregation. GADePo allows the pooling process to\nbe guided by domain-specific knowledge or desired outcomes but still learned by\nthe Transformer, leading to more flexible and customisable pooling strategies.\nWe evaluate our method across diverse datasets and models and show that our\napproach yields promising results that are consistently better than those\nachieved by the hand-coded pooling functions.", "published": "2023-08-28 09:04:03", "link": "http://arxiv.org/abs/2308.14423v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of Consistency Regularization for End-to-End\n  Speech-to-Text Translation", "abstract": "Consistency regularization methods, such as R-Drop (Liang et al., 2021) and\nCrossConST (Gao et al., 2023), have achieved impressive supervised and\nzero-shot performance in the neural machine translation (NMT) field. Can we\nalso boost end-to-end (E2E) speech-to-text translation (ST) by leveraging\nconsistency regularization? In this paper, we conduct empirical studies on\nintra-modal and cross-modal consistency and propose two training strategies,\nSimRegCR and SimZeroCR, for E2E ST in regular and zero-shot scenarios.\nExperiments on the MuST-C benchmark show that our approaches achieve\nstate-of-the-art (SOTA) performance in most translation directions. The\nanalyses prove that regularization brought by the intra-modal consistency,\ninstead of modality gap, is crucial for the regular E2E ST, and the cross-modal\nconsistency could close the modality gap and boost the zero-shot E2E ST\nperformance.", "published": "2023-08-28 10:44:18", "link": "http://arxiv.org/abs/2308.14482v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Detection of Bots on X (Twitter) using Transformers", "abstract": "Although not all bots are malicious, the vast majority of them are\nresponsible for spreading misinformation and manipulating the public opinion\nabout several issues, i.e., elections and many more. Therefore, the early\ndetection of bots is crucial. Although there have been proposed methods for\ndetecting bots in social media, there are still substantial limitations. For\ninstance, existing research initiatives still extract a large number of\nfeatures and train traditional machine learning algorithms or use GloVe\nembeddings and train LSTMs. However, feature extraction is a tedious procedure\ndemanding domain expertise. Also, language models based on transformers have\nbeen proved to be better than LSTMs. Other approaches create large graphs and\ntrain graph neural networks requiring in this way many hours for training and\naccess to computational resources. To tackle these limitations, this is the\nfirst study employing only the user description field and images of three\nchannels denoting the type and content of tweets posted by the users. Firstly,\nwe create digital DNA sequences, transform them to 3d images, and apply\npretrained models of the vision domain, including EfficientNet, AlexNet, VGG16,\netc. Next, we propose a multimodal approach, where we use TwHIN-BERT for\ngetting the textual representation of the user description field and employ\nVGG16 for acquiring the visual representation for the image modality. We\npropose three different fusion methods, namely concatenation, gated multimodal\nunit, and crossmodal attention, for fusing the different modalities and compare\ntheir performances. Finally, we present a qualitative analysis of the behavior\nof our best performing model. Extensive experiments conducted on the Cresci'17\nand TwiBot-20 datasets demonstrate valuable advantages of our introduced\napproaches over state-of-the-art ones.", "published": "2023-08-28 10:51:11", "link": "http://arxiv.org/abs/2308.14484v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LongBench: A Bilingual, Multitask Benchmark for Long Context\n  Understanding", "abstract": "Although large language models (LLMs) demonstrate impressive performance for\nmany language tasks, most of them can only handle texts a few thousand tokens\nlong, limiting their applications on longer sequence inputs, such as books,\nreports, and codebases. Recent works have proposed methods to improve LLMs'\nlong context capabilities by extending context windows and more sophisticated\nmemory mechanisms. However, comprehensive benchmarks tailored for evaluating\nlong context understanding are lacking. In this paper, we introduce LongBench,\nthe first bilingual, multi-task benchmark for long context understanding,\nenabling a more rigorous evaluation of long context understanding. LongBench\ncomprises 21 datasets across 6 task categories in both English and Chinese,\nwith an average length of 6,711 words (English) and 13,386 characters\n(Chinese). These tasks cover key long-text application areas including\nsingle-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks,\nand code completion. All datasets in LongBench are standardized into a unified\nformat, allowing for effortless automatic evaluation of LLMs. Upon\ncomprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial\nmodel (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still\nstruggles on longer contexts. (2) Scaled position embedding and fine-tuning on\nlonger sequences lead to substantial improvement on long context understanding.\n(3) Context compression technique such as retrieval brings improvement for\nmodel with weak ability on long contexts, but the performance still lags behind\nmodels that have strong long context understanding capability. The code and\ndatasets are available at https://github.com/THUDM/LongBench.", "published": "2023-08-28 11:53:40", "link": "http://arxiv.org/abs/2308.14508v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multi-Task Semantic Decomposition Framework with Task-specific\n  Pre-training for Few-Shot NER", "abstract": "The objective of few-shot named entity recognition is to identify named\nentities with limited labeled instances. Previous works have primarily focused\non optimizing the traditional token-wise classification framework, while\nneglecting the exploration of information based on NER data characteristics. To\naddress this issue, we propose a Multi-Task Semantic Decomposition Framework\nvia Joint Task-specific Pre-training (MSDP) for few-shot NER. Drawing\ninspiration from demonstration-based and contrastive learning, we introduce two\nnovel pre-training tasks: Demonstration-based Masked Language Modeling (MLM)\nand Class Contrastive Discrimination. These tasks effectively incorporate\nentity boundary information and enhance entity representation in Pre-trained\nLanguage Models (PLMs). In the downstream main task, we introduce a multi-task\njoint optimization framework with the semantic decomposing method, which\nfacilitates the model to integrate two different semantic information for\nentity classification. Experimental results of two few-shot NER benchmarks\ndemonstrate that MSDP consistently outperforms strong baselines by a large\nmargin. Extensive analyses validate the effectiveness and generalization of\nMSDP.", "published": "2023-08-28 12:46:21", "link": "http://arxiv.org/abs/2308.14533v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Challenges of GPT-3-based Conversational Agents for Healthcare", "abstract": "The potential to provide patients with faster information access while\nallowing medical specialists to concentrate on critical tasks makes medical\ndomain dialog agents appealing. However, the integration of large-language\nmodels (LLMs) into these agents presents certain limitations that may result in\nserious consequences. This paper investigates the challenges and risks of using\nGPT-3-based models for medical question-answering (MedQA). We perform several\nevaluations contextualized in terms of standard medical principles. We provide\na procedure for manually designing patient queries to stress-test high-risk\nlimitations of LLMs in MedQA systems. Our analysis reveals that LLMs fail to\nrespond adequately to these queries, generating erroneous medical information,\nunsafe recommendations, and content that may be considered offensive.", "published": "2023-08-28 15:12:34", "link": "http://arxiv.org/abs/2308.14641v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MEMORY-VQ: Compression for Tractable Internet-Scale Memory", "abstract": "Retrieval augmentation is a powerful but expensive method to make language\nmodels more knowledgeable about the world. Memory-based methods like LUMEN\npre-compute token representations for retrieved passages to drastically speed\nup inference. However, memory also leads to much greater storage requirements\nfrom storing pre-computed representations.\n  We propose MEMORY-VQ, a new method to reduce storage requirements of\nmemory-augmented models without sacrificing performance. Our method uses a\nvector quantization variational autoencoder (VQ-VAE) to compress token\nrepresentations. We apply MEMORY-VQ to the LUMEN model to obtain LUMEN-VQ, a\nmemory model that achieves a 16x compression rate with comparable performance\non the KILT benchmark. LUMEN-VQ enables practical retrieval augmentation even\nfor extremely large retrieval corpora.", "published": "2023-08-28 21:11:18", "link": "http://arxiv.org/abs/2308.14903v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Cultural Psychology of Large Language Models: Is ChatGPT a Holistic\n  or Analytic Thinker?", "abstract": "The prevalent use of Large Language Models (LLMs) has necessitated studying\ntheir mental models, yielding noteworthy theoretical and practical\nimplications. Current research has demonstrated that state-of-the-art LLMs,\nsuch as ChatGPT, exhibit certain theory of mind capabilities and possess\nrelatively stable Big Five and/or MBTI personality traits. In addition,\ncognitive process features form an essential component of these mental models.\nResearch in cultural psychology indicated significant differences in the\ncognitive processes of Eastern and Western people when processing information\nand making judgments. While Westerners predominantly exhibit analytical\nthinking that isolates things from their environment to analyze their nature\nindependently, Easterners often showcase holistic thinking, emphasizing\nrelationships and adopting a global viewpoint. In our research, we probed the\ncultural cognitive traits of ChatGPT. We employed two scales that directly\nmeasure the cognitive process: the Analysis-Holism Scale (AHS) and the Triadic\nCategorization Task (TCT). Additionally, we used two scales that investigate\nthe value differences shaped by cultural thinking: the Dialectical Self Scale\n(DSS) and the Self-construal Scale (SCS). In cognitive process tests (AHS/TCT),\nChatGPT consistently tends towards Eastern holistic thinking, but regarding\nvalue judgments (DSS/SCS), ChatGPT does not significantly lean towards the East\nor the West. We suggest that the result could be attributed to both the\ntraining paradigm and the training data in LLM development. We discuss the\npotential value of this finding for AI research and directions for future\nresearch.", "published": "2023-08-28 01:05:18", "link": "http://arxiv.org/abs/2308.14242v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "SalesBot 2.0: A Human-Like Intent-Guided Chit-Chat Dataset", "abstract": "In recent research on dialogue systems and corpora, there has been a\nsignificant focus on two distinct categories: task-oriented (TOD) and\nopen-domain (chit-chat) dialogues. TOD systems aim to satisfy specific user\ngoals, such as finding a movie to watch, whereas open-domain systems primarily\nfocus on generating engaging conversations. A recent study by Chiu et al.\n(2022) introduced SalesBot, which provides simulators and a dataset with\none-turn transition from chit-chat to task-oriented dialogues. However, the\npreviously generated data solely relied on BlenderBot, which raised concerns\nabout its long-turn naturalness and consistency during a conversation. To\naddress this issue, this paper aims to build SalesBot 2.0, a revised version of\nthe published data, by leveraging the commonsense knowledge of large language\nmodels (LLMs) through proper prompting. The objective is to gradually bridge\nthe gap between chit-chat and TOD towards better naturalness and consistency.\nThe newly released large-scale dataset with detailed annotations exhibits\nsmoother transitions between topics and is more human-like in terms of\nnaturalness and consistency. It can serve as a valuable resource for both\nacademic research and commercial applications. Furthermore, our proposed\nframework can be applied to generate numerous dialogues with various target\nintents.", "published": "2023-08-28 02:48:49", "link": "http://arxiv.org/abs/2308.14266v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Goodhart's Law Applies to NLP's Explanation Benchmarks", "abstract": "Despite the rising popularity of saliency-based explanations, the research\ncommunity remains at an impasse, facing doubts concerning their purpose,\nefficacy, and tendency to contradict each other. Seeking to unite the\ncommunity's efforts around common goals, several recent works have proposed\nevaluation metrics. In this paper, we critically examine two sets of metrics:\nthe ERASER metrics (comprehensiveness and sufficiency) and the EVAL-X metrics,\nfocusing our inquiry on natural language processing. First, we show that we can\ninflate a model's comprehensiveness and sufficiency scores dramatically without\naltering its predictions or explanations on in-distribution test inputs. Our\nstrategy exploits the tendency for extracted explanations and their complements\nto be \"out-of-support\" relative to each other and in-distribution inputs. Next,\nwe demonstrate that the EVAL-X metrics can be inflated arbitrarily by a simple\nmethod that encodes the label, even though EVAL-X is precisely motivated to\naddress such exploits. Our results raise doubts about the ability of current\nmetrics to guide explainability research, underscoring the need for a broader\nreassessment of what precisely these metrics are intended to capture.", "published": "2023-08-28 03:03:03", "link": "http://arxiv.org/abs/2308.14272v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FonMTL: Towards Multitask Learning for the Fon Language", "abstract": "The Fon language, spoken by an average 2 million of people, is a truly\nlow-resourced African language, with a limited online presence, and existing\ndatasets (just to name but a few). Multitask learning is a learning paradigm\nthat aims to improve the generalization capacity of a model by sharing\nknowledge across different but related tasks: this could be prevalent in very\ndata-scarce scenarios. In this paper, we present the first explorative approach\nto multitask learning, for model capabilities enhancement in Natural Language\nProcessing for the Fon language. Specifically, we explore the tasks of Named\nEntity Recognition (NER) and Part of Speech Tagging (POS) for Fon. We leverage\ntwo language model heads as encoders to build shared representations for the\ninputs, and we use linear layers blocks for classification relative to each\ntask. Our results on the NER and POS tasks for Fon, show competitive (or\nbetter) performances compared to several multilingual pretrained language\nmodels finetuned on single tasks. Additionally, we perform a few ablation\nstudies to leverage the efficiency of two different loss combination strategies\nand find out that the equal loss weighting approach works best in our case. Our\ncode is open-sourced at https://github.com/bonaventuredossou/multitask_fon.", "published": "2023-08-28 03:26:21", "link": "http://arxiv.org/abs/2308.14280v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating the Robustness to Instructions of Large Language Models", "abstract": "Recently, Instruction fine-tuning has risen to prominence as a potential\nmethod for enhancing the zero-shot capabilities of Large Language Models (LLMs)\non novel tasks. This technique has shown an exceptional ability to boost the\nperformance of moderately sized LLMs, sometimes even reaching performance\nlevels comparable to those of much larger model variants. The focus is on the\nrobustness of instruction-tuned LLMs to seen and unseen tasks. We conducted an\nexploration of six models including Alpaca, Vicuna, WizardLM, and Traditional\nTask-oriented Models(Flan-T5-XL/XXL, T0++) using real-world relation extraction\ndatasets as case studies. We carried out a comprehensive evaluation of these\ninstruction-following LLMs which have been tuned based on open-domain\ninstructions and task-oriented instructions. The main discussion is their\nperformance and robustness towards instructions. We have observed that in most\ncases, the model's performance in dealing with unfamiliar instructions tends to\nworsen significantly, and the robustness of the model for RE instructions\ndeteriorates compared to QA. Further, we discovered that up until a certain\nparameter size threshold (3B), the performance of the FLAN-T5 model improves as\nthe parameter count increases. The robustness of different scales of FLAN-T5\nmodels to RE instruction is worse than the robustness to QA instruction.", "published": "2023-08-28 04:57:07", "link": "http://arxiv.org/abs/2308.14306v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging Medical Knowledge Graphs Into Large Language Models for\n  Diagnosis Prediction: Design and Application Study", "abstract": "Electronic Health Records (EHRs) and routine documentation practices play a\nvital role in patients' daily care, providing a holistic record of health,\ndiagnoses, and treatment. However, complex and verbose EHR narratives overload\nhealthcare providers, risking diagnostic inaccuracies. While Large Language\nModels (LLMs) have showcased their potential in diverse language tasks, their\napplication in the healthcare arena needs to ensure the minimization of\ndiagnostic errors and the prevention of patient harm. In this paper, we outline\nan innovative approach for augmenting the proficiency of LLMs in the realm of\nautomated diagnosis generation, achieved through the incorporation of a medical\nknowledge graph (KG) and a novel graph model: Dr.Knows, inspired by the\nclinical diagnostic reasoning process. We derive the KG from the National\nLibrary of Medicine's Unified Medical Language System (UMLS), a robust\nrepository of biomedical knowledge. Our method negates the need for\npre-training and instead leverages the KG as an auxiliary instrument aiding in\nthe interpretation and summarization of complex medical concepts. Using\nreal-world hospital datasets, our experimental results demonstrate that the\nproposed approach of combining LLMs with KG has the potential to improve the\naccuracy of automated diagnosis generation. More importantly, our approach\noffers an explainable diagnostic pathway, edging us closer to the realization\nof AI-augmented diagnostic decision support systems.", "published": "2023-08-28 06:05:18", "link": "http://arxiv.org/abs/2308.14321v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cognitive Effects in Large Language Models", "abstract": "Large Language Models (LLMs) such as ChatGPT have received enormous attention\nover the past year and are now used by hundreds of millions of people every\nday. The rapid adoption of this technology naturally raises questions about the\npossible biases such models might exhibit. In this work, we tested one of these\nmodels (GPT-3) on a range of cognitive effects, which are systematic patterns\nthat are usually found in human cognitive tasks. We found that LLMs are indeed\nprone to several human cognitive effects. Specifically, we show that the\npriming, distance, SNARC, and size congruity effects were presented with GPT-3,\nwhile the anchoring effect is absent. We describe our methodology, and\nspecifically the way we converted real-world experiments to text-based\nexperiments. Finally, we speculate on the possible reasons why GPT-3 exhibits\nthese effects and discuss whether they are imitated or reinvented.", "published": "2023-08-28 06:30:33", "link": "http://arxiv.org/abs/2308.14337v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "DISC-MedLLM: Bridging General Large Language Models and Real-World\n  Medical Consultation", "abstract": "We propose DISC-MedLLM, a comprehensive solution that leverages Large\nLanguage Models (LLMs) to provide accurate and truthful medical response in\nend-to-end conversational healthcare services. To construct high-quality\nSupervised Fine-Tuning (SFT) datasets, we employ three strategies: utilizing\nmedical knowledge-graphs, reconstructing real-world dialogues, and\nincorporating human-guided preference rephrasing. These datasets are\ninstrumental in training DISC-MedLLM, surpassing existing medical LLMs in both\nsingle-turn and multi-turn consultation scenarios. Extensive experimental\nresults demonstrate the effectiveness of the proposed model in bridging the gap\nbetween general language models and real-world medical consultation.\nAdditionally, we release the constructed dataset and model weights to further\ncontribute to research and development. Further details and resources can be\nfound at https://github.com/FudanDISC/DISC-MedLLM", "published": "2023-08-28 06:41:49", "link": "http://arxiv.org/abs/2308.14346v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Effect of Attention and Self-Supervised Speech Embeddings on\n  Non-Semantic Speech Tasks", "abstract": "Human emotion understanding is pivotal in making conversational technology\nmainstream. We view speech emotion understanding as a perception task which is\na more realistic setting. With varying contexts (languages, demographics, etc.)\ndifferent share of people perceive the same speech segment as a non-unanimous\nemotion. As part of the ACM Multimedia 2023 Computational Paralinguistics\nChallengE (ComParE) in the EMotion Share track, we leverage their rich dataset\nof multilingual speakers and multi-label regression target of 'emotion share'\nor perception of that emotion. We demonstrate that the training scheme of\ndifferent foundation models dictates their effectiveness for tasks beyond\nspeech recognition, especially for non-semantic speech tasks like emotion\nunderstanding. This is a very complex task due to multilingual speakers,\nvariability in the target labels, and inherent imbalance in the regression\ndataset. Our results show that HuBERT-Large with a self-attention-based\nlight-weight sequence model provides 4.6% improvement over the reported\nbaseline.", "published": "2023-08-28 07:11:27", "link": "http://arxiv.org/abs/2308.14359v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "FIRE: Food Image to REcipe generation", "abstract": "Food computing has emerged as a prominent multidisciplinary field of research\nin recent years. An ambitious goal of food computing is to develop end-to-end\nintelligent systems capable of autonomously producing recipe information for a\nfood image. Current image-to-recipe methods are retrieval-based and their\nsuccess depends heavily on the dataset size and diversity, as well as the\nquality of learned embeddings. Meanwhile, the emergence of powerful\nattention-based vision and language models presents a promising avenue for\naccurate and generalizable recipe generation, which has yet to be extensively\nexplored. This paper proposes FIRE, a novel multimodal methodology tailored to\nrecipe generation in the food computing domain, which generates the food title,\ningredients, and cooking instructions based on input food images. FIRE\nleverages the BLIP model to generate titles, utilizes a Vision Transformer with\na decoder for ingredient extraction, and employs the T5 model to generate\nrecipes incorporating titles and ingredients as inputs. We showcase two\npractical applications that can benefit from integrating FIRE with large\nlanguage model prompting: recipe customization to fit recipes to user\npreferences and recipe-to-code transformation to enable automated cooking\nprocesses. Our experimental findings validate the efficacy of our proposed\napproach, underscoring its potential for future advancements and widespread\nadoption in food computing.", "published": "2023-08-28 08:14:20", "link": "http://arxiv.org/abs/2308.14391v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Biomedical Entity Linking with Triple-aware Pre-Training", "abstract": "Linking biomedical entities is an essential aspect in biomedical natural\nlanguage processing tasks, such as text mining and question answering. However,\na difficulty of linking the biomedical entities using current large language\nmodels (LLM) trained on a general corpus is that biomedical entities are\nscarcely distributed in texts and therefore have been rarely seen during\ntraining by the LLM. At the same time, those LLMs are not aware of high level\nsemantic connection between different biomedical entities, which are useful in\nidentifying similar concepts in different textual contexts. To cope with\naforementioned problems, some recent works focused on injecting knowledge graph\ninformation into LLMs. However, former methods either ignore the relational\nknowledge of the entities or lead to catastrophic forgetting. Therefore, we\npropose a novel framework to pre-train the powerful generative LLM by a corpus\nsynthesized from a KG. In the evaluations we are unable to confirm the benefit\nof including synonym, description or relational information.", "published": "2023-08-28 09:06:28", "link": "http://arxiv.org/abs/2308.14429v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bridging the KB-Text Gap: Leveraging Structured Knowledge-aware\n  Pre-training for KBQA", "abstract": "Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions with factual information such as entities and relations in KBs.\nHowever, traditional Pre-trained Language Models (PLMs) are directly\npre-trained on large-scale natural language corpus, which poses challenges for\nthem in understanding and representing complex subgraphs in structured KBs. To\nbridge the gap between texts and structured KBs, we propose a Structured\nKnowledge-aware Pre-training method (SKP). In the pre-training stage, we\nintroduce two novel structured knowledge-aware tasks, guiding the model to\neffectively learn the implicit relationship and better representations of\ncomplex subgraphs. In downstream KBQA task, we further design an efficient\nlinearization strategy and an interval attention mechanism, which assist the\nmodel to better encode complex subgraphs and shield the interference of\nirrelevant subgraphs during reasoning respectively. Detailed experiments and\nanalyses on WebQSP verify the effectiveness of SKP, especially the significant\nimprovement in subgraph retrieval (+4.08% H@10).", "published": "2023-08-28 09:22:02", "link": "http://arxiv.org/abs/2308.14436v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Joint Multiple Intent Detection and Slot Filling with Supervised\n  Contrastive Learning and Self-Distillation", "abstract": "Multiple intent detection and slot filling are two fundamental and crucial\ntasks in spoken language understanding. Motivated by the fact that the two\ntasks are closely related, joint models that can detect intents and extract\nslots simultaneously are preferred to individual models that perform each task\nindependently. The accuracy of a joint model depends heavily on the ability of\nthe model to transfer information between the two tasks so that the result of\none task can correct the result of the other. In addition, since a joint model\nhas multiple outputs, how to train the model effectively is also challenging.\nIn this paper, we present a method for multiple intent detection and slot\nfilling by addressing these challenges. First, we propose a bidirectional joint\nmodel that explicitly employs intent information to recognize slots and slot\nfeatures to detect intents. Second, we introduce a novel method for training\nthe proposed joint model using supervised contrastive learning and\nself-distillation. Experimental results on two benchmark datasets MixATIS and\nMixSNIPS show that our method outperforms state-of-the-art models in both\ntasks. The results also demonstrate the contributions of both bidirectional\ndesign and the training method to the accuracy improvement. Our source code is\navailable at https://github.com/anhtunguyen98/BiSLU", "published": "2023-08-28 15:36:33", "link": "http://arxiv.org/abs/2308.14654v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ANER: Arabic and Arabizi Named Entity Recognition using\n  Transformer-Based Approach", "abstract": "One of the main tasks of Natural Language Processing (NLP), is Named Entity\nRecognition (NER). It is used in many applications and also can be used as an\nintermediate step for other tasks. We present ANER, a web-based named entity\nrecognizer for the Arabic, and Arabizi languages. The model is built upon BERT,\nwhich is a transformer-based encoder. It can recognize 50 different entity\nclasses, covering various fields. We trained our model on the WikiFANE\\_Gold\ndataset which consists of Wikipedia articles. We achieved an F1 score of\n88.7\\%, which beats CAMeL Tools' F1 score of 83\\% on the ANERcorp dataset,\nwhich has only 4 classes. We also got an F1 score of 77.7\\% on the\nNewsFANE\\_Gold dataset which contains out-of-domain data from News articles.\nThe system is deployed on a user-friendly web interface that accepts users'\ninputs in Arabic, or Arabizi. It allows users to explore the entities in the\ntext by highlighting them. It can also direct users to get information about\nentities through Wikipedia directly. We added the ability to do NER using our\nmodel, or CAMeL Tools' model through our website. ANER is publicly accessible\nat \\url{http://www.aner.online}. We also deployed our model on HuggingFace at\nhttps://huggingface.co/boda/ANER, to allow developers to test and use it.", "published": "2023-08-28 15:54:48", "link": "http://arxiv.org/abs/2308.14669v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Attention Visualizer Package: Revealing Word Importance for Deeper\n  Insight into Encoder-Only Transformer Models", "abstract": "This report introduces the Attention Visualizer package, which is crafted to\nvisually illustrate the significance of individual words in encoder-only\ntransformer-based models. In contrast to other methods that center on tokens\nand self-attention scores, our approach will examine the words and their impact\non the final embedding representation. Libraries like this play a crucial role\nin enhancing the interpretability and explainability of neural networks. They\noffer the opportunity to illuminate their internal mechanisms, providing a\nbetter understanding of how they operate and can be enhanced. You can access\nthe code and review examples on the following GitHub repository:\nhttps://github.com/AlaFalaki/AttentionVisualizer.", "published": "2023-08-28 19:11:52", "link": "http://arxiv.org/abs/2308.14850v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EdgeMoE: Empowering Sparse Large Language Models on Mobile Devices", "abstract": "Large language models (LLMs) such as GPTs and Mixtral-8x7B have\nrevolutionized machine intelligence due to their exceptional abilities in\ngeneric ML tasks. Transiting LLMs from datacenters to edge devices brings\nbenefits like better privacy and availability, but is challenged by their\nmassive parameter size and thus unbearable runtime costs. To this end, we\npresent EdgeMoE, an on-device inference engine for mixture-of-expert (MoE) LLMs\n-- a popular form of sparse LLM that scales its parameter size with almost\nconstant computing complexity. EdgeMoE achieves both memory- and\ncompute-efficiency by partitioning the model into the storage hierarchy:\nnon-expert weights are held in device memory; while expert weights are held on\nexternal storage and fetched to memory only when activated. This design is\nmotivated by a key observation that expert weights are bulky but infrequently\nused due to sparse activation. To further reduce the expert I/O swapping\noverhead, EdgeMoE incorporates two novel techniques: (1) expert-wise bitwidth\nadaptation that reduces the expert sizes with tolerable accuracy loss; (2)\nexpert preloading that predicts the activated experts ahead of time and\npreloads it with the compute-I/O pipeline. On popular MoE LLMs and edge\ndevices, EdgeMoE showcase significant memory savings and speedup over\ncompetitive baselines. The code is available at\nhttps://github.com/UbiquitousLearning/mllm.", "published": "2023-08-28 06:56:08", "link": "http://arxiv.org/abs/2308.14352v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "AI in the Gray: Exploring Moderation Policies in Dialogic Large Language\n  Models vs. Human Answers in Controversial Topics", "abstract": "The introduction of ChatGPT and the subsequent improvement of Large Language\nModels (LLMs) have prompted more and more individuals to turn to the use of\nChatBots, both for information and assistance with decision-making. However,\nthe information the user is after is often not formulated by these ChatBots\nobjectively enough to be provided with a definite, globally accepted answer.\n  Controversial topics, such as \"religion\", \"gender identity\", \"freedom of\nspeech\", and \"equality\", among others, can be a source of conflict as partisan\nor biased answers can reinforce preconceived notions or promote disinformation.\nBy exposing ChatGPT to such debatable questions, we aim to understand its level\nof awareness and if existing models are subject to socio-political and/or\neconomic biases. We also aim to explore how AI-generated answers compare to\nhuman ones. For exploring this, we use a dataset of a social media platform\ncreated for the purpose of debating human-generated claims on polemic subjects\namong users, dubbed Kialo.\n  Our results show that while previous versions of ChatGPT have had important\nissues with controversial topics, more recent versions of ChatGPT\n(gpt-3.5-turbo) are no longer manifesting significant explicit biases in\nseveral knowledge areas. In particular, it is well-moderated regarding economic\naspects. However, it still maintains degrees of implicit libertarian leaning\ntoward right-winged ideals which suggest the need for increased moderation from\nthe socio-political point of view. In terms of domain knowledge on\ncontroversial topics, with the exception of the \"Philosophical\" category,\nChatGPT is performing well in keeping up with the collective human level of\nknowledge. Finally, we see that sources of Bing AI have slightly more tendency\nto the center when compared to human answers. All the analyses we make are\ngeneralizable to other types of biases and domains.", "published": "2023-08-28 14:23:04", "link": "http://arxiv.org/abs/2308.14608v1", "categories": ["cs.LG", "cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Breaking the Bank with ChatGPT: Few-Shot Text Classification for Finance", "abstract": "We propose the use of conversational GPT models for easy and quick few-shot\ntext classification in the financial domain using the Banking77 dataset. Our\napproach involves in-context learning with GPT-3.5 and GPT-4, which minimizes\nthe technical expertise required and eliminates the need for expensive GPU\ncomputing while yielding quick and accurate results. Additionally, we fine-tune\nother pre-trained, masked language models with SetFit, a recent contrastive\nlearning technique, to achieve state-of-the-art results both in full-data and\nfew-shot settings. Our findings show that querying GPT-3.5 and GPT-4 can\noutperform fine-tuned, non-generative models even with fewer examples. However,\nsubscription fees associated with these solutions may be considered costly for\nsmall organizations. Lastly, we find that generative models perform better on\nthe given task when shown representative samples selected by a human expert\nrather than when shown random ones. We conclude that a) our proposed methods\noffer a practical solution for few-shot tasks in datasets with limited label\navailability, and b) our state-of-the-art results can inspire future work in\nthe area.", "published": "2023-08-28 15:04:16", "link": "http://arxiv.org/abs/2308.14634v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-fin.CP"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning Llama 2 Large Language Models for Detecting Online Sexual\n  Predatory Chats and Abusive Texts", "abstract": "Detecting online sexual predatory behaviours and abusive language on social\nmedia platforms has become a critical area of research due to the growing\nconcerns about online safety, especially for vulnerable populations such as\nchildren and adolescents. Researchers have been exploring various techniques\nand approaches to develop effective detection systems that can identify and\nmitigate these risks. Recent development of large language models (LLMs) has\nopened a new opportunity to address this problem more effectively. This paper\nproposes an approach to detection of online sexual predatory chats and abusive\nlanguage using the open-source pretrained Llama 2 7B-parameter model, recently\nreleased by Meta GenAI. We fine-tune the LLM using datasets with different\nsizes, imbalance degrees, and languages (i.e., English, Roman Urdu and Urdu).\nBased on the power of LLMs, our approach is generic and automated without a\nmanual search for a synergy between feature extraction and classifier design\nsteps like conventional methods in this domain. Experimental results show a\nstrong performance of the proposed approach, which performs proficiently and\nconsistently across three distinct datasets with five sets of experiments. This\nstudy's outcomes indicate that the proposed method can be implemented in\nreal-world applications (even with non-English languages) for flagging sexual\npredators, offensive or toxic content, hate speech, and discriminatory language\nin online discussions and comments to maintain respectful internet or digital\ncommunities. Furthermore, it can be employed for solving text classification\nproblems with other potential applications such as sentiment analysis, spam and\nphishing detection, sorting legal documents, fake news detection, language\nidentification, user intent recognition, text-based product categorization,\nmedical record analysis, and resume screening.", "published": "2023-08-28 16:18:50", "link": "http://arxiv.org/abs/2308.14683v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CommunityFish: A Poisson-based Document Scaling With Hierarchical\n  Clustering", "abstract": "Document scaling has been a key component in text-as-data applications for\nsocial scientists and a major field of interest for political researchers, who\naim at uncovering differences between speakers or parties with the help of\ndifferent probabilistic and non-probabilistic approaches. Yet, most of these\ntechniques are either built upon the agnostically bag-of-word hypothesis or use\nprior information borrowed from external sources that might embed the results\nwith a significant bias. If the corpus has long been considered as a collection\nof documents, it can also be seen as a dense network of connected words whose\nstructure could be clustered to differentiate independent groups of words,\nbased on their co-occurrences in documents, known as communities. This paper\nintroduces CommunityFish as an augmented version of Wordfish based on a\nhierarchical clustering, namely the Louvain algorithm, on the word space to\nyield communities as semantic and independent n-grams emerging from the corpus\nand use them as an input to Wordfish method, instead of considering the word\nspace. This strategy emphasizes the interpretability of the results, since\ncommunities have a non-overlapping structure, hence a crucial informative power\nin discriminating parties or speakers, in addition to allowing a faster\nexecution of the Poisson scaling model. Aside from yielding communities,\nassumed to be subtopic proxies, the application of this technique outperforms\nthe classic Wordfish model by highlighting historical developments in the U.S.\nState of the Union addresses and was found to replicate the prevailing\npolitical stance in Germany when using the corpus of parties' legislative\nmanifestos.", "published": "2023-08-28 19:52:18", "link": "http://arxiv.org/abs/2308.14873v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Multiscale Contextual Learning for Speech Emotion Recognition in\n  Emergency Call Center Conversations", "abstract": "Emotion recognition in conversations is essential for ensuring advanced\nhuman-machine interactions. However, creating robust and accurate emotion\nrecognition systems in real life is challenging, mainly due to the scarcity of\nemotion datasets collected in the wild and the inability to take into account\nthe dialogue context. The CEMO dataset, composed of conversations between\nagents and patients during emergency calls to a French call center, fills this\ngap. The nature of these interactions highlights the role of the emotional flow\nof the conversation in predicting patient emotions, as context can often make a\ndifference in understanding actual feelings. This paper presents a multi-scale\nconversational context learning approach for speech emotion recognition, which\ntakes advantage of this hypothesis. We investigated this approach on both\nspeech transcriptions and acoustic segments. Experimentally, our method uses\nthe previous or next information of the targeted segment. In the text domain,\nwe tested the context window using a wide range of tokens (from 10 to 100) and\nat the speech turns level, considering inputs from both the same and opposing\nspeakers. According to our tests, the context derived from previous tokens has\na more significant influence on accurate prediction than the following tokens.\nFurthermore, taking the last speech turn of the same speaker in the\nconversation seems useful. In the acoustic domain, we conducted an in-depth\nanalysis of the impact of the surrounding emotions on the prediction. While\nmulti-scale conversational context learning using Transformers can enhance\nperformance in the textual modality for emergency call recordings,\nincorporating acoustic context is more challenging.", "published": "2023-08-28 20:31:45", "link": "http://arxiv.org/abs/2308.14894v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Neural approaches to spoken content embedding", "abstract": "Comparing spoken segments is a central operation to speech processing.\nTraditional approaches in this area have favored frame-level dynamic\nprogramming algorithms, such as dynamic time warping, because they require no\nsupervision, but they are limited in performance and efficiency. As an\nalternative, acoustic word embeddings -- fixed-dimensional vector\nrepresentations of variable-length spoken word segments -- have begun to be\nconsidered for such tasks as well. However, the current space of such\ndiscriminative embedding models, training approaches, and their application to\nreal-world downstream tasks is limited. We start by considering ``single-view\"\ntraining losses where the goal is to learn an acoustic word embedding model\nthat separates same-word and different-word spoken segment pairs. Then, we\nconsider ``multi-view\" contrastive losses. In this setting, acoustic word\nembeddings are learned jointly with embeddings of character sequences to\ngenerate acoustically grounded embeddings of written words, or acoustically\ngrounded word embeddings.\n  In this thesis, we contribute new discriminative acoustic word embedding\n(AWE) and acoustically grounded word embedding (AGWE) approaches based on\nrecurrent neural networks (RNNs). We improve model training in terms of both\nefficiency and performance. We take these developments beyond English to\nseveral low-resource languages and show that multilingual training improves\nperformance when labeled data is limited. We apply our embedding models, both\nmonolingual and multilingual, to the downstream tasks of query-by-example\nspeech search and automatic speech recognition. Finally, we show how our\nembedding approaches compare with and complement more recent self-supervised\nspeech models.", "published": "2023-08-28 21:16:08", "link": "http://arxiv.org/abs/2308.14905v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Gender bias and stereotypes in Large Language Models", "abstract": "Large Language Models (LLMs) have made substantial progress in the past\nseveral months, shattering state-of-the-art benchmarks in many domains. This\npaper investigates LLMs' behavior with respect to gender stereotypes, a known\nissue for prior models. We use a simple paradigm to test the presence of gender\nbias, building on but differing from WinoBias, a commonly used gender bias\ndataset, which is likely to be included in the training data of current LLMs.\nWe test four recently published LLMs and demonstrate that they express biased\nassumptions about men and women's occupations. Our contributions in this paper\nare as follows: (a) LLMs are 3-6 times more likely to choose an occupation that\nstereotypically aligns with a person's gender; (b) these choices align with\npeople's perceptions better than with the ground truth as reflected in official\njob statistics; (c) LLMs in fact amplify the bias beyond what is reflected in\nperceptions or the ground truth; (d) LLMs ignore crucial ambiguities in\nsentence structure 95% of the time in our study items, but when explicitly\nprompted, they recognize the ambiguity; (e) LLMs provide explanations for their\nchoices that are factually inaccurate and likely obscure the true reason behind\ntheir predictions. That is, they provide rationalizations of their biased\nbehavior. This highlights a key property of these models: LLMs are trained on\nimbalanced datasets; as such, even with the recent successes of reinforcement\nlearning with human feedback, they tend to reflect those imbalances back at us.\nAs with other types of societal biases, we suggest that LLMs must be carefully\ntested to ensure that they treat minoritized individuals and communities\nequitably.", "published": "2023-08-28 22:32:05", "link": "http://arxiv.org/abs/2308.14921v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Spoken Language Intelligence of Large Language Models for Language\n  Learning", "abstract": "People have long hoped for a conversational system that can assist in\nreal-life situations, and recent progress on large language models (LLMs) is\nbringing this idea closer to reality. While LLMs are often impressive in\nperformance, their efficacy in real-world scenarios that demand expert\nknowledge remains unclear. LLMs are believed to hold the most potential and\nvalue in education, especially in the development of Artificial intelligence\n(AI) based virtual teachers capable of facilitating language learning. Our\nfocus is centered on evaluating the efficacy of LLMs in the realm of education,\nspecifically in the areas of spoken language learning which encompass\nphonetics, phonology, and second language acquisition. We introduce a new\nmultiple-choice question dataset to evaluate the effectiveness of LLMs in the\naforementioned scenarios, including understanding and application of spoken\nlanguage knowledge. In addition, we investigate the influence of various\nprompting techniques such as zero- and few-shot method (prepending the question\nwith question-answer exemplars), chain-of-thought (CoT, think step-by-step),\nin-domain exampler and external tools (Google, Wikipedia). We conducted\nlarge-scale evaluation on popular LLMs (20 distinct models) using these\nmethods. We achieved significant performance improvements compared to the\nzero-shot baseline in the practical questions reasoning (GPT-3.5, 49.1% ->\n63.1%; LLaMA2-70B-Chat, 42.2% -> 48.6%). We found that models of different\nsizes have good understanding of concepts in phonetics, phonology, and second\nlanguage acquisition, but show limitations in reasoning for real-world\nproblems. Additionally, we also explore preliminary findings on conversational\ncommunication.", "published": "2023-08-28 12:47:41", "link": "http://arxiv.org/abs/2308.14536v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Symbolic & Acoustic: Multi-domain Music Emotion Modeling for\n  Instrumental Music", "abstract": "Music Emotion Recognition involves the automatic identification of emotional\nelements within music tracks, and it has garnered significant attention due to\nits broad applicability in the field of Music Information Retrieval. It can\nalso be used as the upstream task of many other human-related tasks such as\nemotional music generation and music recommendation. Due to existing psychology\nresearch, music emotion is determined by multiple factors such as the Timbre,\nVelocity, and Structure of the music. Incorporating multiple factors in MER\nhelps achieve more interpretable and finer-grained methods. However, most prior\nworks were uni-domain and showed weak consistency between arousal modeling\nperformance and valence modeling performance. Based on this background, we\ndesigned a multi-domain emotion modeling method for instrumental music that\ncombines symbolic analysis and acoustic analysis. At the same time, because of\nthe rarity of music data and the difficulty of labeling, our multi-domain\napproach can make full use of limited data. Our approach was implemented and\nassessed using the publicly available piano dataset EMOPIA, resulting in a\nnotable improvement over our baseline model with a 2.4% increase in overall\naccuracy, establishing its state-of-the-art performance.", "published": "2023-08-28 05:47:57", "link": "http://arxiv.org/abs/2308.14317v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Voice Conversion with Denoising Diffusion Probabilistic GAN Models", "abstract": "Voice conversion is a method that allows for the transformation of speaking\nstyle while maintaining the integrity of linguistic information. There are many\nresearchers using deep generative models for voice conversion tasks. Generative\nAdversarial Networks (GANs) can quickly generate high-quality samples, but the\ngenerated samples lack diversity. The samples generated by the Denoising\nDiffusion Probabilistic Models (DDPMs) are better than GANs in terms of mode\ncoverage and sample diversity. But the DDPMs have high computational costs and\nthe inference speed is slower than GANs. In order to make GANs and DDPMs more\npractical we proposes DiffGAN-VC, a variant of GANs and DDPMS, to achieve\nnon-parallel many-to-many voice conversion (VC). We use large steps to achieve\ndenoising, and also introduce a multimodal conditional GANs to model the\ndenoising diffusion generative adversarial network. According to both objective\nand subjective evaluation experiments, DiffGAN-VC has been shown to achieve\nhigh voice quality on non-parallel data sets. Compared with the CycleGAN-VC\nmethod, DiffGAN-VC achieves speaker similarity, naturalness and higher sound\nquality.", "published": "2023-08-28 05:53:06", "link": "http://arxiv.org/abs/2308.14319v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Rep2wav: Noise Robust text-to-speech Using self-supervised\n  representations", "abstract": "Benefiting from the development of deep learning, text-to-speech (TTS)\ntechniques using clean speech have achieved significant performance\nimprovements. The data collected from real scenes often contains noise and\ngenerally needs to be denoised by speech enhancement models. Noise-robust TTS\nmodels are often trained using the enhanced speech, which thus suffer from\nspeech distortion and background noise that affect the quality of the\nsynthesized speech. Meanwhile, it was shown that self-supervised pre-trained\nmodels exhibit excellent noise robustness on many speech tasks, implying that\nthe learned representation has a better tolerance for noise perturbations. In\nthis work, we therefore explore pre-trained models to improve the noise\nrobustness of TTS models. Based on HiFi-GAN, we first propose a\nrepresentation-to-waveform vocoder, which aims to learn to map the\nrepresentation of pre-trained models to the waveform. We then propose a\ntext-to-representation FastSpeech2 model, which aims to learn to map text to\npre-trained model representations. Experimental results on the LJSpeech and\nLibriTTS datasets show that our method outperforms those using speech\nenhancement methods in both subjective and objective metrics. Audio samples are\navailable at: https://zqs01.github.io/rep2wav.", "published": "2023-08-28 13:13:15", "link": "http://arxiv.org/abs/2308.14553v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Time-Frequency Transformer: A Novel Time Frequency Joint Learning Method\n  for Speech Emotion Recognition", "abstract": "In this paper, we propose a novel time-frequency joint learning method for\nspeech emotion recognition, called Time-Frequency Transformer. Its advantage is\nthat the Time-Frequency Transformer can excavate global emotion patterns in the\ntime-frequency domain of speech signal while modeling the local emotional\ncorrelations in the time domain and frequency domain respectively. For the\npurpose, we first design a Time Transformer and Frequency Transformer to\ncapture the local emotion patterns between frames and inside frequency bands\nrespectively, so as to ensure the integrity of the emotion information modeling\nin both time and frequency domains. Then, a Time-Frequency Transformer is\nproposed to mine the time-frequency emotional correlations through the local\ntime-domain and frequency-domain emotion features for learning more\ndiscriminative global speech emotion representation. The whole process is a\ntime-frequency joint learning process implemented by a series of Transformer\nmodels. Experiments on IEMOCAP and CASIA databases indicate that our proposed\nmethod outdoes the state-of-the-art methods.", "published": "2023-08-28 13:34:02", "link": "http://arxiv.org/abs/2308.14568v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Data-driven 3D Room Geometry Inference with a Linear Loudspeaker Array\n  and a Single Microphone", "abstract": "Knowing the room geometry may be very beneficial for many audio applications,\nincluding sound reproduction, acoustic scene analysis, and sound source\nlocalization. Room geometry inference (RGI) deals with the problem of reflector\nlocalization (RL) based on a set of room impulse responses (RIRs). Motivated by\nthe increasing popularity of commercially available soundbars, this article\npresents a data-driven 3D RGI method using RIRs measured from a linear\nloudspeaker array to a single microphone. A convolutional recurrent neural\nnetwork (CRNN) is trained using simulated RIRs in a supervised fashion for RL.\nThe Radon transform, which is equivalent to delay-and-sum beamforming, is\napplied to multi-channel RIRs, and the resulting time-domain acoustic\nbeamforming map is fed into the CRNN. The room geometry is inferred from the\nmicrophone position and the reflector locations estimated by the network. The\nresults obtained using measured RIRs show that the proposed data-driven\napproach generalizes well to unseen RIRs and achieves an accuracy level\ncomparable to a baseline model-driven RGI method that involves intermediate\nsemi-supervised steps, thereby offering a unified and fully automated RGI\nframework.", "published": "2023-08-28 14:28:01", "link": "http://arxiv.org/abs/2308.14611v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The USTC-NERCSLIP Systems for the CHiME-7 DASR Challenge", "abstract": "This technical report details our submission system to the CHiME-7 DASR\nChallenge, which focuses on speaker diarization and speech recognition under\ncomplex multi-speaker scenarios. Additionally, it also evaluates the efficiency\nof systems in handling diverse array devices. To address these issues, we\nimplemented an end-to-end speaker diarization system and introduced a\nrectification strategy based on multi-channel spatial information. This\napproach significantly diminished the word error rates (WER). In terms of\nrecognition, we utilized publicly available pre-trained models as the\nfoundational models to train our end-to-end speech recognition models. Our\nsystem attained a Macro-averaged diarization-attributed WER (DA-WER) of 21.01%\non the CHiME-7 evaluation set, which signifies a relative improvement of 62.04%\nover the official baseline system.", "published": "2023-08-28 15:08:25", "link": "http://arxiv.org/abs/2308.14638v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Active Learning: Optimizing Labeling Cost-Effectiveness for\n  Automatic Speech Recognition", "abstract": "In recent years, speech-based self-supervised learning (SSL) has made\nsignificant progress in various tasks, including automatic speech recognition\n(ASR). An ASR model with decent performance can be realized by fine-tuning an\nSSL model with a small fraction of labeled data. Reducing the demand for\nlabeled data is always of great practical value. In this paper, we further\nextend the use of SSL to cut down labeling costs with active learning. Three\ntypes of units on different granularities are derived from speech signals in an\nunsupervised way, and their effects are compared by applying a contrastive data\nselection method. The experimental results show that our proposed data\nselection framework can effectively improve the word error rate (WER) by more\nthan 11% with the same amount of labeled data, or halve the labeling cost while\nmaintaining the same WER, compared to random selection.", "published": "2023-08-28 18:04:37", "link": "http://arxiv.org/abs/2308.14814v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "InstructME: An Instruction Guided Music Edit And Remix Framework with\n  Latent Diffusion Models", "abstract": "Music editing primarily entails the modification of instrument tracks or\nremixing in the whole, which offers a novel reinterpretation of the original\npiece through a series of operations. These music processing methods hold\nimmense potential across various applications but demand substantial expertise.\nPrior methodologies, although effective for image and audio modifications,\nfalter when directly applied to music. This is attributed to music's\ndistinctive data nature, where such methods can inadvertently compromise the\nintrinsic harmony and coherence of music. In this paper, we develop InstructME,\nan Instruction guided Music Editing and remixing framework based on latent\ndiffusion models. Our framework fortifies the U-Net with multi-scale\naggregation in order to maintain consistency before and after editing. In\naddition, we introduce chord progression matrix as condition information and\nincorporate it in the semantic space to improve melodic harmony while editing.\nFor accommodating extended musical pieces, InstructME employs a chunk\ntransformer, enabling it to discern long-term temporal dependencies within\nmusic sequences. We tested InstructME in instrument-editing, remixing, and\nmulti-round editing. Both subjective and objective evaluations indicate that\nour proposed method significantly surpasses preceding systems in music quality,\ntext relevance and harmony. Demo samples are available at\nhttps://musicedit.github.io/", "published": "2023-08-28 07:11:42", "link": "http://arxiv.org/abs/2308.14360v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TextrolSpeech: A Text Style Control Speech Corpus With Codec Language\n  Text-to-Speech Models", "abstract": "Recently, there has been a growing interest in the field of controllable\nText-to-Speech (TTS). While previous studies have relied on users providing\nspecific style factor values based on acoustic knowledge or selecting reference\nspeeches that meet certain requirements, generating speech solely from natural\ntext prompts has emerged as a new challenge for researchers. This challenge\narises due to the scarcity of high-quality speech datasets with natural text\nstyle prompt and the absence of advanced text-controllable TTS models. In light\nof this, 1) we propose TextrolSpeech, which is the first large-scale speech\nemotion dataset annotated with rich text attributes. The dataset comprises\n236,220 pairs of style prompt in natural text descriptions with five style\nfactors and corresponding speech samples. Through iterative experimentation, we\nintroduce a multi-stage prompt programming approach that effectively utilizes\nthe GPT model for generating natural style descriptions in large volumes. 2)\nFurthermore, to address the need for generating audio with greater style\ndiversity, we propose an efficient architecture called Salle. This architecture\ntreats text controllable TTS as a language model task, utilizing audio codec\ncodes as an intermediate representation to replace the conventional\nmel-spectrogram. Finally, we successfully demonstrate the ability of the\nproposed model by showing a comparable performance in the controllable TTS\ntask. Audio samples are available at https://sall-e.github.io/", "published": "2023-08-28 09:06:32", "link": "http://arxiv.org/abs/2308.14430v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Self-Supervised Representations Benchmarking: a Case for Larger\n  Probing Heads", "abstract": "Self-supervised learning (SSL) leverages large datasets of unlabeled speech\nto reach impressive performance with reduced amounts of annotated data. The\nhigh number of proposed approaches fostered the emergence of comprehensive\nbenchmarks that evaluate their performance on a set of downstream tasks\nexploring various aspects of the speech signal. However, while the number of\nconsidered tasks has been growing, most proposals rely upon a single downstream\narchitecture that maps the frozen SSL representations to the task labels. This\nstudy examines how benchmarking results are affected by changes in the probing\nhead architecture. Interestingly, we found that altering the downstream\narchitecture structure leads to significant fluctuations in the performance\nranking of the evaluated models. Against common practices in speech SSL\nbenchmarking, we evaluate larger-capacity probing heads, showing their impact\non performance, inference costs, generalization and multi-level feature\nexploitation.", "published": "2023-08-28 09:49:48", "link": "http://arxiv.org/abs/2308.14456v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "A time-causal and time-recursive analogue of the Gabor transform", "abstract": "This paper presents a time-causal analogue of the Gabor filter, as well as a\nboth time-causal and time-recursive analogue of the Gabor transform, where the\nproposed time-causal representations obey both temporal scale covariance and a\ncascade property with a simplifying kernel over temporal scales. The motivation\nbehind these constructions is to enable theoretically well-founded\ntime-frequency analysis over multiple temporal scales for real-time situations,\nor for physical or biological modelling situations, when the future cannot be\naccessed, and the non-causal access to future in Gabor filtering is therefore\nnot viable for a time-frequency analysis of the system.\n  We develop the theory for these representations, obtained by replacing the\nGaussian kernel in Gabor filtering with a time-causal kernel, referred to as\nthe time-causal limit kernel, which guarantees simplification properties from\nfiner to coarser levels of scales in a time-causal situation, similar as the\nGaussian kernel can be shown to guarantee over a non-causal temporal domain. In\nthese ways, the proposed time-frequency representations guarantee well-founded\ntreatment over multiple scales, in situations when the characteristic scales in\nthe signals, or physical or biological phenomena, to be analyzed may vary\nsubstantially, and additionally all steps in the time-frequency analysis have\nto be fully time-causal.", "published": "2023-08-28 11:58:07", "link": "http://arxiv.org/abs/2308.14512v9", "categories": ["eess.SP", "cs.SD", "eess.AS", "math.FA", "42C15, 42C40"], "primary_category": "eess.SP"}
{"title": "EEG-Derived Voice Signature for Attended Speaker Detection", "abstract": "\\textit{Objective:} Conventional EEG-based auditory attention detection (AAD)\nis achieved by comparing the time-varying speech stimuli and the elicited EEG\nsignals. However, in order to obtain reliable correlation values, these methods\nnecessitate a long decision window, resulting in a long detection latency.\nHumans have a remarkable ability to recognize and follow a known speaker,\nregardless of the spoken content. In this paper, we seek to detect the attended\nspeaker among the pre-enrolled speakers from the elicited EEG signals. In this\nmanner, we avoid relying on the speech stimuli for AAD at run-time. In doing\nso, we propose a novel EEG-based attended speaker detection (E-ASD) task.\n\\textit{Methods:} We encode a speaker's voice with a fixed dimensional vector,\nknown as speaker embedding, and project it to an audio-derived voice signature,\nwhich characterizes the speaker's unique voice regardless of the spoken\ncontent. We hypothesize that such a voice signature also exists in the\nlistener's brain that can be decoded from the elicited EEG signals, referred to\nas EEG-derived voice signature. By comparing the audio-derived voice signature\nand the EEG-derived voice signature, we are able to effectively detect the\nattended speaker in the listening brain. \\textit{Results:} Experiments show\nthat E-ASD can effectively detect the attended speaker from the 0.5s EEG\ndecision windows, achieving 99.78\\% AAD accuracy, 99.94\\% AUC, and 0.27\\% EER.\n\\textit{Conclusion:} We conclude that it is possible to derive the attended\nspeaker's voice signature from the EEG signals so as to detect the attended\nspeaker in a listening brain. \\textit{Significance:} We present the first proof\nof concept for detecting the attended speaker from the elicited EEG signals in\na cocktail party environment. The successful implementation of E-ASD marks a\nnon-trivial, but crucial step towards smart hearing aids.", "published": "2023-08-28 10:39:03", "link": "http://arxiv.org/abs/2308.14774v1", "categories": ["eess.AS", "cs.SD", "eess.SP", "q-bio.QM"], "primary_category": "eess.AS"}
{"title": "Pruning Self-Attention for Zero-Shot Multi-Speaker Text-to-Speech", "abstract": "For personalized speech generation, a neural text-to-speech (TTS) model must\nbe successfully implemented with limited data from a target speaker. To this\nend, the baseline TTS model needs to be amply generalized to out-of-domain data\n(i.e., target speaker's speech). However, approaches to address this\nout-of-domain generalization problem in TTS have yet to be thoroughly studied.\nIn this work, we propose an effective pruning method for a transformer known as\nsparse attention, to improve the TTS model's generalization abilities. In\nparticular, we prune off redundant connections from self-attention layers whose\nattention weights are below the threshold. To flexibly determine the pruning\nstrength for searching optimal degree of generalization, we also propose a new\ndifferentiable pruning method that allows the model to automatically learn the\nthresholds. Evaluations on zero-shot multi-speaker TTS verify the effectiveness\nof our method in terms of voice quality and speaker similarity.", "published": "2023-08-28 21:25:05", "link": "http://arxiv.org/abs/2308.14909v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
