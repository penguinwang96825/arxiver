{"title": "AIR-JPMC@SMM4H'22: Classifying Self-Reported Intimate Partner Violence\n  in Tweets with Multiple BERT-based Models", "abstract": "This paper presents our submission for the SMM4H 2022-Shared Task on the\nclassification of self-reported intimate partner violence on Twitter (in\nEnglish). The goal of this task was to accurately determine if the contents of\na given tweet demonstrated someone reporting their own experience with intimate\npartner violence. The submitted system is an ensemble of five RoBERTa models\neach weighted by their respective F1-scores on the validation data-set. This\nsystem performed 13% better than the baseline and was the best performing\nsystem overall for this shared task.", "published": "2022-09-22 03:43:25", "link": "http://arxiv.org/abs/2209.10763v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Learning Based Page Creation for Improving E-Commerce Organic\n  Search Traffic", "abstract": "Organic search comprises a large portion of the total traffic for e-commerce\ncompanies. One approach to expand company's exposure on organic search channel\nlies on creating landing pages having broader coverage on customer intentions.\nIn this paper, we present a transformer language model based organic channel\npage management system aiming at increasing prominence of the company's overall\nclicks on the channel. Our system successfully handles the creation and\ndeployment process of millions of new landing pages. We show and discuss the\nreal-world performances of state-of-the-art language representation learning\nmethod, and reveal how we find them as the production-optimal solutions.", "published": "2022-09-22 05:36:44", "link": "http://arxiv.org/abs/2209.10792v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantically Consistent Data Augmentation for Neural Machine Translation\n  via Conditional Masked Language Model", "abstract": "This paper introduces a new data augmentation method for neural machine\ntranslation that can enforce stronger semantic consistency both within and\nacross languages. Our method is based on Conditional Masked Language Model\n(CMLM) which is bi-directional and can be conditional on both left and right\ncontext, as well as the label. We demonstrate that CMLM is a good technique for\ngenerating context-dependent word distributions. In particular, we show that\nCMLM is capable of enforcing semantic consistency by conditioning on both\nsource and target during substitution. In addition, to enhance diversity, we\nincorporate the idea of soft word substitution for data augmentation which\nreplaces a word with a probabilistic distribution over the vocabulary.\nExperiments on four translation datasets of different scales show that the\noverall solution results in more realistic data augmentation and better\ntranslation quality. Our approach consistently achieves the best performance in\ncomparison with strong and recent works and yields improvements of up to 1.90\nBLEU points over the baseline.", "published": "2022-09-22 09:19:08", "link": "http://arxiv.org/abs/2209.10875v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Write with Coherence From Negative Examples", "abstract": "Coherence is one of the critical factors that determine the quality of\nwriting. We propose writing relevance (WR) training method for neural\nencoder-decoder natural language generation (NLG) models which improves\ncoherence of the continuation by leveraging negative examples. WR loss\nregresses the vector representation of the context and generated sentence\ntoward positive continuation by contrasting it with the negatives. We compare\nour approach with Unlikelihood (UL) training in a text continuation task on\ncommonsense natural language inference (NLI) corpora to show which method\nbetter models the coherence by avoiding unlikely continuations. The preference\nof our approach in human evaluation shows the efficacy of our method in\nimproving coherence.", "published": "2022-09-22 11:02:54", "link": "http://arxiv.org/abs/2209.10922v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptation of domain-specific transformer models with text oversampling\n  for sentiment analysis of social media posts on Covid-19 vaccines", "abstract": "Covid-19 has spread across the world and several vaccines have been developed\nto counter its surge. To identify the correct sentiments associated with the\nvaccines from social media posts, we fine-tune various state-of-the-art\npre-trained transformer models on tweets associated with Covid-19 vaccines.\nSpecifically, we use the recently introduced state-of-the-art pre-trained\ntransformer models RoBERTa, XLNet and BERT, and the domain-specific transformer\nmodels CT-BERT and BERTweet that are pre-trained on Covid-19 tweets. We further\nexplore the option of text augmentation by oversampling using Language Model\nbased Oversampling Technique (LMOTE) to improve the accuracies of these models,\nspecifically, for small sample datasets where there is an imbalanced class\ndistribution among the positive, negative and neutral sentiment classes. Our\nresults summarize our findings on the suitability of text oversampling for\nimbalanced small sample datasets that are used to fine-tune state-of-the-art\npre-trained transformer models, and the utility of domain-specific transformer\nmodels for the classification task.", "published": "2022-09-22 12:36:40", "link": "http://arxiv.org/abs/2209.10966v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Selecting Better Samples from Pre-trained LLMs: A Case Study on Question\n  Generation", "abstract": "Large Language Models (LLMs) have in recent years demonstrated impressive\nprowess in natural language generation. A common practice to improve generation\ndiversity is to sample multiple outputs from the model. However, there lacks a\nsimple and robust way of selecting the best output from these stochastic\nsamples. As a case study framed in the context of question generation, we\npropose two prompt-based approaches to selecting high-quality questions from a\nset of LLM-generated candidates. Our method works under the constraints of 1) a\nblack-box (non-modifiable) question generation model and 2) lack of access to\nhuman-annotated references -- both of which are realistic limitations for\nreal-world deployment of LLMs. With automatic as well as human evaluations, we\nempirically demonstrate that our approach can effectively select questions of\nhigher qualities than greedy generation.", "published": "2022-09-22 13:33:48", "link": "http://arxiv.org/abs/2209.11000v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Approaching English-Polish Machine Translation Quality Assessment with\n  Neural-based Methods", "abstract": "This paper presents our contribution to the PolEval 2021 Task 2: Evaluation\nof translation quality assessment metrics. We describe experiments with\npre-trained language models and state-of-the-art frameworks for translation\nquality assessment in both nonblind and blind versions of the task. Our\nsolutions ranked second in the nonblind version and third in the blind version.", "published": "2022-09-22 13:53:15", "link": "http://arxiv.org/abs/2209.11016v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MonoByte: A Pool of Monolingual Byte-level Language Models", "abstract": "The zero-shot cross-lingual ability of models pretrained on multilingual and\neven monolingual corpora has spurred many hypotheses to explain this intriguing\nempirical result. However, due to the costs of pretraining, most research uses\npublic models whose pretraining methodology, such as the choice of\ntokenization, corpus size, and computational budget, might differ drastically.\nWhen researchers pretrain their own models, they often do so under a\nconstrained budget, and the resulting models might underperform significantly\ncompared to SOTA models. These experimental differences led to various\ninconsistent conclusions about the nature of the cross-lingual ability of these\nmodels. To help further research on the topic, we released 10 monolingual\nbyte-level models rigorously pretrained under the same configuration with a\nlarge compute budget (equivalent to 420 days on a V100) and corpora that are 4\ntimes larger than the original BERT's. Because they are tokenizer-free, the\nproblem of unseen token embeddings is eliminated, thus allowing researchers to\ntry a wider range of cross-lingual experiments in languages with different\nscripts. Additionally, we release two models pretrained on non-natural language\ntexts that can be used in sanity-check experiments. Experiments on QA and NLI\ntasks show that our monolingual models achieve competitive performance to the\nmultilingual one, and hence can be served to strengthen our understanding of\ncross-lingual transferability in language models.", "published": "2022-09-22 14:32:48", "link": "http://arxiv.org/abs/2209.11035v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Few-Shot Learning Without Prompts", "abstract": "Recent few-shot methods, such as parameter-efficient fine-tuning (PEFT) and\npattern exploiting training (PET), have achieved impressive results in\nlabel-scarce settings. However, they are difficult to employ since they are\nsubject to high variability from manually crafted prompts, and typically\nrequire billion-parameter language models to achieve high accuracy. To address\nthese shortcomings, we propose SetFit (Sentence Transformer Fine-tuning), an\nefficient and prompt-free framework for few-shot fine-tuning of Sentence\nTransformers (ST). SetFit works by first fine-tuning a pretrained ST on a small\nnumber of text pairs, in a contrastive Siamese manner. The resulting model is\nthen used to generate rich text embeddings, which are used to train a\nclassification head. This simple framework requires no prompts or verbalizers,\nand achieves high accuracy with orders of magnitude less parameters than\nexisting techniques. Our experiments show that SetFit obtains comparable\nresults with PEFT and PET techniques, while being an order of magnitude faster\nto train. We also show that SetFit can be applied in multilingual settings by\nsimply switching the ST body. Our code is available at\nhttps://github.com/huggingface/setfit and our datasets at\nhttps://huggingface.co/setfit .", "published": "2022-09-22 14:48:11", "link": "http://arxiv.org/abs/2209.11055v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompting for a conversation: How to control a dialog model?", "abstract": "Dialog modelling faces a difficult trade-off. Models are trained on a large\namount of text, yet their responses need to be limited to a desired scope and\nstyle of a dialog agent. Because the datasets used to achieve the former\ncontain language that is not compatible with the latter, pre-trained dialog\nmodels are fine-tuned on smaller curated datasets. However, the fine-tuning\nprocess robs them of the ability to produce diverse responses, eventually\nreducing them to dull conversation partners. In this paper we investigate if\nprompting can mitigate the above trade-off. Specifically, we experiment with\nconditioning the prompt on the query, rather than training a single prompt for\nall queries. By following the intuition that freezing the pre-trained language\nmodel will conserve its expressivity, we find that compared to fine-tuning,\nprompting can achieve a higher BLEU score and substantially improve the\ndiversity and novelty of the responses.", "published": "2022-09-22 14:59:55", "link": "http://arxiv.org/abs/2209.11068v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scope of Pre-trained Language Models for Detecting Conflicting Health\n  Information", "abstract": "An increasing number of people now rely on online platforms to meet their\nhealth information needs. Thus identifying inconsistent or conflicting textual\nhealth information has become a safety-critical task. Health advice data poses\na unique challenge where information that is accurate in the context of one\ndiagnosis can be conflicting in the context of another. For example, people\nsuffering from diabetes and hypertension often receive conflicting health\nadvice on diet. This motivates the need for technologies which can provide\ncontextualized, user-specific health advice. A crucial step towards\ncontextualized advice is the ability to compare health advice statements and\ndetect if and how they are conflicting. This is the task of health conflict\ndetection (HCD). Given two pieces of health advice, the goal of HCD is to\ndetect and categorize the type of conflict. It is a challenging task, as (i)\nautomatically identifying and categorizing conflicts requires a deeper\nunderstanding of the semantics of the text, and (ii) the amount of available\ndata is quite limited.\n  In this study, we are the first to explore HCD in the context of pre-trained\nlanguage models. We find that DeBERTa-v3 performs best with a mean F1 score of\n0.68 across all experiments. We additionally investigate the challenges posed\nby different conflict types and how synthetic data improves a model's\nunderstanding of conflict-specific semantics. Finally, we highlight the\ndifficulty in collecting real health conflicts and propose a human-in-the-loop\nsynthetic data augmentation approach to expand existing HCD datasets. Our HCD\ntraining dataset is over 2x bigger than the existing HCD dataset and is made\npublicly available on Github.", "published": "2022-09-22 15:40:04", "link": "http://arxiv.org/abs/2209.11102v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Interpretable Latent Dialogue Actions With Less Supervision", "abstract": "We present a novel architecture for explainable modeling of task-oriented\ndialogues with discrete latent variables to represent dialogue actions. Our\nmodel is based on variational recurrent neural networks (VRNN) and requires no\nexplicit annotation of semantic information. Unlike previous works, our\napproach models the system and user turns separately and performs database\nquery modeling, which makes the model applicable to task-oriented dialogues\nwhile producing easily interpretable action latent variables. We show that our\nmodel outperforms previous approaches with less supervision in terms of\nperplexity and BLEU on three datasets, and we propose a way to measure dialogue\nsuccess without the need for expert annotation. Finally, we propose a novel way\nto explain semantics of the latent variables with respect to system actions.", "published": "2022-09-22 16:14:06", "link": "http://arxiv.org/abs/2209.11128v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XF2T: Cross-lingual Fact-to-Text Generation for Low-Resource Languages", "abstract": "Multiple business scenarios require an automated generation of descriptive\nhuman-readable text from structured input data. Hence, fact-to-text generation\nsystems have been developed for various downstream tasks like generating soccer\nreports, weather and financial reports, medical reports, person biographies,\netc. Unfortunately, previous work on fact-to-text (F2T) generation has focused\nprimarily on English mainly due to the high availability of relevant datasets.\nOnly recently, the problem of cross-lingual fact-to-text (XF2T) was proposed\nfor generation across multiple languages alongwith a dataset, XALIGN for eight\nlanguages. However, there has been no rigorous work on the actual XF2T\ngeneration problem. We extend XALIGN dataset with annotated data for four more\nlanguages: Punjabi, Malayalam, Assamese and Oriya. We conduct an extensive\nstudy using popular Transformer-based text generation models on our extended\nmulti-lingual dataset, which we call XALIGNV2. Further, we investigate the\nperformance of different text generation strategies: multiple variations of\npretraining, fact-aware embeddings and structure-aware input encoding. Our\nextensive experiments show that a multi-lingual mT5 model which uses fact-aware\nembeddings with structure-aware input encoding leads to best results on average\nacross the twelve languages. We make our code, dataset and model publicly\navailable, and hope that this will help advance further research in this\ncritical area.", "published": "2022-09-22 18:01:27", "link": "http://arxiv.org/abs/2209.11252v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Faithful Model Explanation in NLP: A Survey", "abstract": "End-to-end neural Natural Language Processing (NLP) models are notoriously\ndifficult to understand. This has given rise to numerous efforts towards model\nexplainability in recent years. One desideratum of model explanation is\nfaithfulness, i.e. an explanation should accurately represent the reasoning\nprocess behind the model's prediction. In this survey, we review over 110 model\nexplanation methods in NLP through the lens of faithfulness. We first discuss\nthe definition and evaluation of faithfulness, as well as its significance for\nexplainability. We then introduce recent advances in faithful explanation,\ngrouping existing approaches into five categories: similarity-based methods,\nanalysis of model-internal structures, backpropagation-based methods,\ncounterfactual intervention, and self-explanatory models. For each category, we\nsynthesize its representative studies, strengths, and weaknesses. Finally, we\nsummarize their common virtues and remaining challenges, and reflect on future\nwork directions towards faithful explainability in NLP.", "published": "2022-09-22 21:40:51", "link": "http://arxiv.org/abs/2209.11326v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "INFINITY: A Simple Yet Effective Unsupervised Framework for Graph-Text\n  Mutual Conversion", "abstract": "Graph-to-text (G2T) generation and text-to-graph (T2G) triple extraction are\ntwo essential tasks for constructing and applying knowledge graphs. Existing\nunsupervised approaches turn out to be suitable candidates for jointly learning\nthe two tasks due to their avoidance of using graph-text parallel data.\nHowever, they are composed of multiple modules and still require both entity\ninformation and relation type in the training process. To this end, we propose\nINFINITY, a simple yet effective unsupervised approach that does not require\nexternal annotation tools or additional parallel information. It achieves fully\nunsupervised graph-text mutual conversion for the first time. Specifically,\nINFINITY treats both G2T and T2G as a bidirectional sequence generation task by\nfine-tuning only one pretrained seq2seq model. A novel back-translation-based\nframework is then designed to automatically generate continuous synthetic\nparallel data. To obtain reasonable graph sequences with structural information\nfrom source texts, INFINITY employs reward-based training loss by leveraging\nthe advantage of reward augmented maximum likelihood. As a fully unsupervised\nframework, INFINITY is empirically verified to outperform state-of-the-art\nbaselines for G2T and T2G tasks.", "published": "2022-09-22 03:12:43", "link": "http://arxiv.org/abs/2209.10754v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Attention Matrix for Every Decision: Faithfulness-based Arbitration\n  Among Multiple Attention-Based Interpretations of Transformers in Text\n  Classification", "abstract": "Transformers are widely used in natural language processing, where they\nconsistently achieve state-of-the-art performance. This is mainly due to their\nattention-based architecture, which allows them to model rich linguistic\nrelations between (sub)words. However, transformers are difficult to interpret.\nBeing able to provide reasoning for its decisions is an important property for\na model in domains where human lives are affected. With transformers finding\nwide use in such fields, the need for interpretability techniques tailored to\nthem arises. We propose a new technique that selects the most faithful\nattention-based interpretation among the several ones that can be obtained by\ncombining different head, layer and matrix operations. In addition, two\nvariations are introduced towards (i) reducing the computational complexity,\nthus being faster and friendlier to the environment, and (ii) enhancing the\nperformance in multi-label data. We further propose a new faithfulness metric\nthat is more suitable for transformer models and exhibits high correlation with\nthe area under the precision-recall curve based on ground truth rationales. We\nvalidate the utility of our contributions with a series of quantitative and\nqualitative experiments on seven datasets.", "published": "2022-09-22 09:19:22", "link": "http://arxiv.org/abs/2209.10876v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Equivariant Transduction through Invariant Alignment", "abstract": "The ability to generalize compositionally is key to understanding the\npotentially infinite number of sentences that can be constructed in a human\nlanguage from only a finite number of words. Investigating whether NLP models\npossess this ability has been a topic of interest: SCAN (Lake and Baroni, 2018)\nis one task specifically proposed to test for this property. Previous work has\nachieved impressive empirical results using a group-equivariant neural network\nthat naturally encodes a useful inductive bias for SCAN (Gordon et al., 2020).\nInspired by this, we introduce a novel group-equivariant architecture that\nincorporates a group-invariant hard alignment mechanism. We find that our\nnetwork's structure allows it to develop stronger equivariance properties than\nexisting group-equivariant approaches. We additionally find that it outperforms\nprevious group-equivariant networks empirically on the SCAN task. Our results\nsuggest that integrating group-equivariance into a variety of neural\narchitectures is a potentially fruitful avenue of research, and demonstrate the\nvalue of careful analysis of the theoretical properties of such architectures.", "published": "2022-09-22 11:19:45", "link": "http://arxiv.org/abs/2209.10926v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Information Minimization Based Contrastive Learning Model for\n  Unsupervised Sentence Embeddings Learning", "abstract": "Unsupervised sentence embeddings learning has been recently dominated by\ncontrastive learning methods (e.g., SimCSE), which keep positive pairs similar\nand push negative pairs apart. The contrast operation aims to keep as much\ninformation as possible by maximizing the mutual information between positive\ninstances, which leads to redundant information in sentence embedding. To\naddress this problem, we present an information minimization based contrastive\nlearning (InforMin-CL) model to retain the useful information and discard the\nredundant information by maximizing the mutual information and minimizing the\ninformation entropy between positive instances meanwhile for unsupervised\nsentence representation learning. Specifically, we find that information\nminimization can be achieved by simple contrast and reconstruction objectives.\nThe reconstruction operation reconstitutes the positive instance via the other\npositive instance to minimize the information entropy between positive\ninstances. We evaluate our model on fourteen downstream tasks, including both\nsupervised and unsupervised (semantic textual similarity) tasks. Extensive\nexperimental results show that our InforMin-CL obtains a state-of-the-art\nperformance.", "published": "2022-09-22 12:07:35", "link": "http://arxiv.org/abs/2209.10951v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Case Report On The \"A.I. Locked-In Problem\": social concerns with\n  modern NLP", "abstract": "Modern NLP models are becoming better conversational agents than their\npredecessors. Recurrent Neural Networks (RNNs) and especially Long-Short Term\nMemory (LSTM) features allow the agent to better store and use information\nabout semantic content, a trend that has become even more pronounced with the\nTransformer Models. Large Language Models (LLMs) such as GPT-3 by OpenAI have\nbecome known to be able to construct and follow a narrative, which enables the\nsystem to adopt personas on the go, adapt them and play along in conversational\nstories. However, practical experimentation with GPT-3 shows that there is a\nrecurring problem with these modern NLP systems, namely that they can \"get\nstuck\" in the narrative so that further conversations, prompt executions or\ncommands become futile. This is here referred to as the \"Locked-In Problem\" and\nis exemplified with an experimental case report, followed by practical and\nsocial concerns that are accompanied with this problem.", "published": "2022-09-22 16:39:35", "link": "http://arxiv.org/abs/2209.12687v1", "categories": ["cs.CL", "cs.CY", "I.2; J.4; J.5; K.4"], "primary_category": "cs.CL"}
{"title": "Learning Disentangled Representations for Natural Language Definitions", "abstract": "Disentangling the encodings of neural models is a fundamental aspect for\nimproving interpretability, semantic control and downstream task performance in\nNatural Language Processing. Currently, most disentanglement methods are\nunsupervised or rely on synthetic datasets with known generative factors. We\nargue that recurrent syntactic and semantic regularities in textual data can be\nused to provide the models with both structural biases and generative factors.\nWe leverage the semantic structures present in a representative and\nsemantically dense category of sentence types, definitional sentences, for\ntraining a Variational Autoencoder to learn disentangled representations. Our\nexperimental results show that the proposed model outperforms unsupervised\nbaselines on several qualitative and quantitative benchmarks for\ndisentanglement, and it also improves the results in the downstream task of\ndefinition modeling.", "published": "2022-09-22 14:31:55", "link": "http://arxiv.org/abs/2210.02898v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Homophone Reveals the Truth: A Reality Check for Speech2Vec", "abstract": "Generating spoken word embeddings that possess semantic information is a\nfascinating topic. Compared with text-based embeddings, they cover both\nphonetic and semantic characteristics, which can provide richer information and\nare potentially helpful for improving ASR and speech translation systems. In\nthis paper, we review and examine the authenticity of a seminal work in this\nfield: Speech2Vec. First, a homophone-based inspection method is proposed to\ncheck the speech embeddings released by the author of Speech2Vec. There is no\nindication that these embeddings are generated by the Speech2Vec model.\nMoreover, through further analysis of the vocabulary composition, we suspect\nthat a text-based model fabricates these embeddings. Finally, we reproduce the\nSpeech2Vec model, referring to the official code and optimal settings in the\noriginal paper. Experiments showed that this model failed to learn effective\nsemantic embeddings. In word similarity benchmarks, it gets a correlation score\nof 0.08 in MEN and 0.15 in WS-353-SIM tests, which is over 0.5 lower than those\ndescribed in the original paper. Our data and code are available.", "published": "2022-09-22 05:32:09", "link": "http://arxiv.org/abs/2209.10791v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Controllable Accented Text-to-Speech Synthesis", "abstract": "Accented text-to-speech (TTS) synthesis seeks to generate speech with an\naccent (L2) as a variant of the standard version (L1). Accented TTS synthesis\nis challenging as L2 is different from L1 in both in terms of phonetic\nrendering and prosody pattern. Furthermore, there is no easy solution to the\ncontrol of the accent intensity in an utterance. In this work, we propose a\nneural TTS architecture, that allows us to control the accent and its intensity\nduring inference. This is achieved through three novel mechanisms, 1) an accent\nvariance adaptor to model the complex accent variance with three prosody\ncontrolling factors, namely pitch, energy and duration; 2) an accent intensity\nmodeling strategy to quantify the accent intensity; 3) a consistency constraint\nmodule to encourage the TTS system to render the expected accent intensity at a\nfine level. Experiments show that the proposed system attains superior\nperformance to the baseline models in terms of accent rendering and intensity\ncontrol. To our best knowledge, this is the first study of accented TTS\nsynthesis with explicit intensity control.", "published": "2022-09-22 06:13:07", "link": "http://arxiv.org/abs/2209.10804v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Multi-Stage Multi-Codebook VQ-VAE Approach to High-Performance Neural\n  TTS", "abstract": "We propose a Multi-Stage, Multi-Codebook (MSMC) approach to high-performance\nneural TTS synthesis. A vector-quantized, variational autoencoder (VQ-VAE)\nbased feature analyzer is used to encode Mel spectrograms of speech training\ndata by down-sampling progressively in multiple stages into MSMC\nRepresentations (MSMCRs) with different time resolutions, and quantizing them\nwith multiple VQ codebooks, respectively. Multi-stage predictors are trained to\nmap the input text sequence to MSMCRs progressively by minimizing a combined\nloss of the reconstruction Mean Square Error (MSE) and \"triplet loss\". In\nsynthesis, the neural vocoder converts the predicted MSMCRs into final speech\nwaveforms. The proposed approach is trained and tested with an English TTS\ndatabase of 16 hours by a female speaker. The proposed TTS achieves an MOS\nscore of 4.41, which outperforms the baseline with an MOS of 3.62. Compact\nversions of the proposed TTS with much less parameters can still preserve high\nMOS scores. Ablation studies show that both multiple stages and multiple\ncodebooks are effective for achieving high TTS performance.", "published": "2022-09-22 09:43:17", "link": "http://arxiv.org/abs/2209.10887v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CONE: An Efficient COarse-to-fiNE Alignment Framework for Long Video\n  Temporal Grounding", "abstract": "This paper tackles an emerging and challenging problem of long video temporal\ngrounding~(VTG) that localizes video moments related to a natural language (NL)\nquery. Compared with short videos, long videos are also highly demanded but\nless explored, which brings new challenges in higher inference computation cost\nand weaker multi-modal alignment. To address these challenges, we propose CONE,\nan efficient COarse-to-fiNE alignment framework. CONE is a plug-and-play\nframework on top of existing VTG models to handle long videos through a sliding\nwindow mechanism. Specifically, CONE (1) introduces a query-guided window\nselection strategy to speed up inference, and (2) proposes a coarse-to-fine\nmechanism via a novel incorporation of contrastive learning to enhance\nmulti-modal alignment for long videos. Extensive experiments on two large-scale\nlong VTG benchmarks consistently show both substantial performance gains (e.g.,\nfrom 3.13% to 6.87% on MAD) and state-of-the-art results. Analyses also reveal\nhigher efficiency as the query-guided window selection mechanism accelerates\ninference time by 2x on Ego4D-NLQ and 15x on MAD while keeping SOTA results.\nCodes have been released at https://github.com/houzhijian/CONE.", "published": "2022-09-22 10:58:42", "link": "http://arxiv.org/abs/2209.10918v2", "categories": ["cs.CV", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "Predicting pairwise preferences between TTS audio stimuli using parallel\n  ratings data and anti-symmetric twin neural networks", "abstract": "Automatically predicting the outcome of subjective listening tests is a\nchallenging task. Ratings may vary from person to person even if preferences\nare consistent across listeners. While previous work has focused on predicting\nlisteners' ratings (mean opinion scores) of individual stimuli, we focus on the\nsimpler task of predicting subjective preference given two speech stimuli for\nthe same text. We propose a model based on anti-symmetric twin neural networks,\ntrained on pairs of waveforms and their corresponding preference scores. We\nexplore both attention and recurrent neural nets to account for the fact that\nstimuli in a pair are not time aligned. To obtain a large training set we\nconvert listeners' ratings from MUSHRA tests to values that reflect how often\none stimulus in the pair was rated higher than the other. Specifically, we\nevaluate performance on data obtained from twelve MUSHRA evaluations conducted\nover five years, containing different TTS systems, built from data of different\nspeakers. Our results compare favourably to a state-of-the-art model trained to\npredict MOS scores.", "published": "2022-09-22 13:34:22", "link": "http://arxiv.org/abs/2209.11003v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language\n  Models", "abstract": "Task planning can require defining myriad domain knowledge about the world in\nwhich a robot needs to act. To ameliorate that effort, large language models\n(LLMs) can be used to score potential next actions during task planning, and\neven generate action sequences directly, given an instruction in natural\nlanguage with no additional domain information. However, such methods either\nrequire enumerating all possible next steps for scoring, or generate free-form\ntext that may contain actions not possible on a given robot in its current\ncontext. We present a programmatic LLM prompt structure that enables plan\ngeneration functional across situated environments, robot capabilities, and\ntasks. Our key insight is to prompt the LLM with program-like specifications of\nthe available actions and objects in an environment, as well as with example\nprograms that can be executed. We make concrete recommendations about prompt\nstructure and generation constraints through ablation experiments, demonstrate\nstate of the art success rates in VirtualHome household tasks, and deploy our\nmethod on a physical robot arm for tabletop tasks. Website at\nprogprompt.github.io", "published": "2022-09-22 20:29:49", "link": "http://arxiv.org/abs/2209.11302v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Isolation performance metrics for personal sound zone reproduction\n  systems", "abstract": "Two isolation performance metrics, Inter-Zone Isolation (IZI) and\nInter-Program Isolation (IPI), are introduced for evaluating Personal Sound\nZone (PSZ) systems. Compared to the commonly-used Acoustic Contrast metric, IZI\nand IPI are generalized for multichannel audio, and quantify the isolation of\nsound zones and of audio programs, respectively. The two metrics are shown to\nbe generally non-interchangeable and suitable for different scenarios, such as\ngenerating dark zones (IZI) or minimizing audio-on-audio interference (IPI).\nFurthermore, two examples with free-field simulations are presented and\ndemonstrate the applications of IZI and IPI in evaluating PSZ performance in\ndifferent rendering modes and PSZ robustness.", "published": "2022-09-22 20:16:49", "link": "http://arxiv.org/abs/2209.11296v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "EPIC TTS Models: Empirical Pruning Investigations Characterizing\n  Text-To-Speech Models", "abstract": "Neural models are known to be over-parameterized, and recent work has shown\nthat sparse text-to-speech (TTS) models can outperform dense models. Although a\nplethora of sparse methods has been proposed for other domains, such methods\nhave rarely been applied in TTS. In this work, we seek to answer the question:\nwhat are the characteristics of selected sparse techniques on the performance\nand model complexity? We compare a Tacotron2 baseline and the results of\napplying five techniques. We then evaluate the performance via the factors of\nnaturalness, intelligibility and prosody, while reporting model size and\ntraining time. Complementary to prior research, we find that pruning before or\nduring training can achieve similar performance to pruning after training and\ncan be trained much faster, while removing entire neurons degrades performance\nmuch more than removing parameters. To our best knowledge, this is the first\nwork that compares sparsity paradigms in text-to-speech synthesis.", "published": "2022-09-22 09:47:25", "link": "http://arxiv.org/abs/2209.10890v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "The SpeakIn System Description for CNSRC2022", "abstract": "This report describes our speaker verification systems for the tasks of the\nCN-Celeb Speaker Recognition Challenge 2022 (CNSRC 2022). This challenge\nincludes two tasks, namely speaker verification(SV) and speaker retrieval(SR).\nThe SV task involves two tracks: fixed track and open track. In the fixed\ntrack, we only used CN-Celeb.T as the training set. For the open track of the\nSV task and SR task, we added our open-source audio data. The ResNet-based,\nRepVGG-based, and TDNN-based architectures were developed for this challenge.\nGlobal statistic pooling structure and MQMHA pooling structure were used to\naggregate the frame-level features across time to obtain utterance-level\nrepresentation. We adopted AM-Softmax and AAM-Softmax combined with the\nSub-Center method to classify the resulting embeddings. We also used the\nLarge-Margin Fine-Tuning strategy to further improve the model performance. In\nthe backend, Sub-Mean and AS-Norm were used. In the SV task fixed track, our\nsystem was a fusion of five models, and two models were fused in the SV task\nopen track. And we used a single system in the SR task. Our approach leads to\nsuperior performance and comes the 1st place in the open track of the SV task,\nthe 2nd place in the fixed track of the SV task, and the 3rd place in the SR\ntask.", "published": "2022-09-22 08:17:47", "link": "http://arxiv.org/abs/2209.10846v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MnTTS: An Open-Source Mongolian Text-to-Speech Synthesis Dataset and\n  Accompanied Baseline", "abstract": "This paper introduces a high-quality open-source text-to-speech (TTS)\nsynthesis dataset for Mongolian, a low-resource language spoken by over 10\nmillion people worldwide. The dataset, named MnTTS, consists of about 8 hours\nof transcribed audio recordings spoken by a 22-year-old professional female\nMongolian announcer. It is the first publicly available dataset developed to\npromote Mongolian TTS applications in both academia and industry. In this\npaper, we share our experience by describing the dataset development procedures\nand faced challenges. To demonstrate the reliability of our dataset, we built a\npowerful non-autoregressive baseline system based on FastSpeech2 model and\nHiFi-GAN vocoder, and evaluated it using the subjective mean opinion score\n(MOS) and real time factor (RTF) metrics. Evaluation results show that the\npowerful baseline system trained on our dataset achieves MOS above 4 and RTF\nabout $3.30\\times10^{-1}$, which makes it applicable for practical use. The\ndataset, training recipe, and pretrained TTS models are freely available\n\\footnote{\\label{github}\\url{https://github.com/walker-hyf/MnTTS}}.", "published": "2022-09-22 08:24:43", "link": "http://arxiv.org/abs/2209.10848v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Maths, Computation and Flamenco: overview and challenges", "abstract": "Flamenco is a rich performance-oriented art music genre from Southern Spain\nwhich attracts a growing community of aficionados around the globe. Due to its\nimprovisational and expressive nature, its unique musical characteristics, and\nthe fact that the genre is largely undocumented, flamenco poses a number of\ninteresting mathematical and computational challenges. Most existing approaches\nin Musical Information Retrieval (MIR) were developed in the context of popular\nor classical music and do often not generalize well to non-Western music\ntraditions, in particular when the underlying music theoretical assumptions do\nnot hold for these genres. Over the recent decade, a number of computational\nproblems related to the automatic analysis of flamenco music have been defined\nand several methods addressing a variety of musical aspects have been proposed.\nThis paper provides an overview of the challenges which arise in the context of\ncomputational analysis of flamenco music and outlines an overview of existing\napproaches.", "published": "2022-09-22 12:43:17", "link": "http://arxiv.org/abs/2209.10970v1", "categories": ["cs.SD", "cs.CG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cross-domain Voice Activity Detection with Self-Supervised\n  Representations", "abstract": "Voice Activity Detection (VAD) aims at detecting speech segments on an audio\nsignal, which is a necessary first step for many today's speech based\napplications. Current state-of-the-art methods focus on training a neural\nnetwork exploiting features directly contained in the acoustics, such as Mel\nFilter Banks (MFBs). Such methods therefore require an extra normalisation step\nto adapt to a new domain where the acoustics is impacted, which can be simply\ndue to a change of speaker, microphone, or environment. In addition, this\nnormalisation step is usually a rather rudimentary method that has certain\nlimitations, such as being highly susceptible to the amount of data available\nfor the new domain. Here, we exploited the crowd-sourced Common Voice (CV)\ncorpus to show that representations based on Self-Supervised Learning (SSL) can\nadapt well to different domains, because they are computed with contextualised\nrepresentations of speech across multiple domains. SSL representations also\nachieve better results than systems based on hand-crafted representations\n(MFBs), and off-the-shelf VADs, with significant improvement in cross-domain\nsettings.", "published": "2022-09-22 14:53:44", "link": "http://arxiv.org/abs/2209.11061v1", "categories": ["eess.AS", "cs.HC", "cs.LG"], "primary_category": "eess.AS"}
{"title": "CMGAN: Conformer-Based Metric-GAN for Monaural Speech Enhancement", "abstract": "In this work, we further develop the conformer-based metric generative\nadversarial network (CMGAN) model for speech enhancement (SE) in the\ntime-frequency (TF) domain. This paper builds on our previous work but takes a\nmore in-depth look by conducting extensive ablation studies on model inputs and\narchitectural design choices. We rigorously tested the generalization ability\nof the model to unseen noise types and distortions. We have fortified our\nclaims through DNS-MOS measurements and listening tests. Rather than focusing\nexclusively on the speech denoising task, we extend this work to address the\ndereverberation and super-resolution tasks. This necessitated exploring various\narchitectural changes, specifically metric discriminator scores and masking\ntechniques. It is essential to highlight that this is among the earliest works\nthat attempted complex TF-domain super-resolution. Our findings show that CMGAN\noutperforms existing state-of-the-art methods in the three major speech\nenhancement tasks: denoising, dereverberation, and super-resolution. For\nexample, in the denoising task using the Voice Bank+DEMAND dataset, CMGAN\nnotably exceeded the performance of prior models, attaining a PESQ score of\n3.41 and an SSNR of 11.10 dB. Audio samples and CMGAN implementations are\navailable online.", "published": "2022-09-22 15:50:21", "link": "http://arxiv.org/abs/2209.11112v3", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
