{"title": "Quality Estimation without Human-labeled Data", "abstract": "Quality estimation aims to measure the quality of translated content without\naccess to a reference translation. This is crucial for machine translation\nsystems in real-world scenarios where high-quality translation is needed. While\nmany approaches exist for quality estimation, they are based on supervised\nmachine learning requiring costly human labelled data. As an alternative, we\npropose a technique that does not rely on examples from human-annotators and\ninstead uses synthetic training data. We train off-the-shelf architectures for\nsupervised quality estimation on our synthetic data and show that the resulting\nmodels achieve comparable performance to models trained on human-annotated\ndata, both for sentence and word-level prediction.", "published": "2021-02-08 06:25:46", "link": "http://arxiv.org/abs/2102.04020v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-Order Chart-Based Constituent Parsing", "abstract": "We propose a novel in-order chart-based model for constituent parsing.\nCompared with previous CKY-style and top-down models, our model gains\nadvantages from in-order traversal of a tree (rich features, lookahead\ninformation and high efficiency) and makes a better use of structural knowledge\nby encoding the history of decisions. Experiments on the Penn Treebank show\nthat our model outperforms previous chart-based models and achieves competitive\nperformance compared with other discriminative single models.", "published": "2021-02-08 09:03:07", "link": "http://arxiv.org/abs/2102.04065v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VeeAlign: Multifaceted Context Representation using Dual Attention for\n  Ontology Alignment", "abstract": "Ontology Alignment is an important research problem applied to various fields\nsuch as data integration, data transfer, data preparation, etc.\nState-of-the-art (SOTA) Ontology Alignment systems typically use naive\ndomain-dependent approaches with handcrafted rules or domain-specific\narchitectures, making them unscalable and inefficient. In this work, we propose\nVeeAlign, a Deep Learning based model that uses a novel dual-attention\nmechanism to compute the contextualized representation of a concept which, in\nturn, is used to discover alignments. By doing this, not only is our approach\nable to exploit both syntactic and semantic information encoded in ontologies,\nit is also, by design, flexible and scalable to different domains with minimal\neffort. We evaluate our model on four different datasets from different domains\nand languages, and establish its superiority through these results as well as\ndetailed ablation studies. The code and datasets used are available at\nhttps://github.com/Remorax/VeeAlign.", "published": "2021-02-08 09:43:05", "link": "http://arxiv.org/abs/2102.04081v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effects of Layer Freezing on Transferring a Speech Recognition System to\n  Under-resourced Languages", "abstract": "In this paper, we investigate the effect of layer freezing on the\neffectiveness of model transfer in the area of automatic speech recognition. We\nexperiment with Mozilla's DeepSpeech architecture on German and Swiss German\nspeech datasets and compare the results of either training from scratch vs.\ntransferring a pre-trained model. We compare different layer freezing schemes\nand find that even freezing only one layer already significantly improves\nresults.", "published": "2021-02-08 10:05:22", "link": "http://arxiv.org/abs/2102.04097v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Clinical Outcome Prediction from Admission Notes using Self-Supervised\n  Knowledge Integration", "abstract": "Outcome prediction from clinical text can prevent doctors from overlooking\npossible risks and help hospitals to plan capacities. We simulate patients at\nadmission time, when decision support can be especially valuable, and\ncontribute a novel admission to discharge task with four common outcome\nprediction targets: Diagnoses at discharge, procedures performed, in-hospital\nmortality and length-of-stay prediction. The ideal system should infer outcomes\nbased on symptoms, pre-conditions and risk factors of a patient. We evaluate\nthe effectiveness of language models to handle this scenario and propose\nclinical outcome pre-training to integrate knowledge about patient outcomes\nfrom multiple public sources. We further present a simple method to incorporate\nICD code hierarchy into the models. We show that our approach improves\nperformance on the outcome tasks against several baselines. A detailed analysis\nreveals further strengths of the model, including transferability, but also\nweaknesses such as handling of vital values and inconsistencies in the\nunderlying data.", "published": "2021-02-08 10:26:44", "link": "http://arxiv.org/abs/2102.04110v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Singleton Fallacy: Why Current Critiques of Language Models Miss the\n  Point", "abstract": "This paper discusses the current critique against neural network-based\nNatural Language Understanding (NLU) solutions known as language models. We\nargue that much of the current debate rests on an argumentation error that we\nwill refer to as the singleton fallacy: the assumption that language, meaning,\nand understanding are single and uniform phenomena that are unobtainable by\n(current) language models. By contrast, we will argue that there are many\ndifferent types of language use, meaning and understanding, and that (current)\nlanguage models are build with the explicit purpose of acquiring and\nrepresenting one type of structural understanding of language. We will argue\nthat such structural understanding may cover several different modalities, and\nas such can handle several different types of meaning. Our position is that we\ncurrently see no theoretical reason why such structural knowledge would be\ninsufficient to count as \"real\" understanding.", "published": "2021-02-08 16:12:36", "link": "http://arxiv.org/abs/2102.04310v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A study of text representations in Hate Speech Detection", "abstract": "The pervasiveness of the Internet and social media have enabled the rapid and\nanonymous spread of Hate Speech content on microblogging platforms such as\nTwitter. Current EU and US legislation against hateful language, in conjunction\nwith the large amount of data produced in these platforms has led to automatic\ntools being a necessary component of the Hate Speech detection task and\npipeline. In this study, we examine the performance of several, diverse text\nrepresentation techniques paired with multiple classification algorithms, on\nthe automatic Hate Speech detection and abusive language discrimination task.\nWe perform an experimental evaluation on binary and multiclass datasets, paired\nwith significance testing. Our results show that simple hate-keyword frequency\nfeatures (BoW) work best, followed by pre-trained word embeddings (GLoVe) as\nwell as N-gram graphs (NGGs): a graph-based representation which proved to\nproduce efficient, very low-dimensional but rich features for this task. A\ncombination of these representations paired with Logistic Regression or 3-layer\nneural network classifiers achieved the best detection performance, in terms of\nmicro and macro F-measure.", "published": "2021-02-08 20:39:17", "link": "http://arxiv.org/abs/2102.04521v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MirrorAlign: A Super Lightweight Unsupervised Word Alignment Model via\n  Cross-Lingual Contrastive Learning", "abstract": "Word alignment is essential for the downstream cross-lingual language\nunderstanding and generation tasks. Recently, the performance of the neural\nword alignment models has exceeded that of statistical models. However, they\nheavily rely on sophisticated translation models. In this study, we propose a\nsuper lightweight unsupervised word alignment model named MirrorAlign, in which\nbidirectional symmetric attention trained with a contrastive learning objective\nis introduced, and an agreement loss is employed to bind the attention maps,\nsuch that the alignments follow mirror-like symmetry hypothesis. Experimental\nresults on several public benchmarks demonstrate that our model achieves\ncompetitive, if not better, performance compared to the state of the art in\nword alignment while significantly reducing the training and decoding time on\naverage. Further ablation analysis and case studies show the superiority of our\nproposed MirrorAlign. Notably, we recognize our model as a pioneer attempt to\nunify bilingual word embedding and word alignments. Encouragingly, our approach\nachieves {16.4X speedup} against GIZA++, and {50X parameter compression}\ncompared with the Transformer-based alignment methods. We release our code to\nfacilitate the community: https://github.com/moore3930/MirrorAlign.", "published": "2021-02-08 05:54:11", "link": "http://arxiv.org/abs/2102.04009v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bias Out-of-the-Box: An Empirical Analysis of Intersectional\n  Occupational Biases in Popular Generative Language Models", "abstract": "The capabilities of natural language models trained on large-scale data have\nincreased immensely over the past few years. Open source libraries such as\nHuggingFace have made these models easily available and accessible. While prior\nresearch has identified biases in large language models, this paper considers\nbiases contained in the most popular versions of these models when applied\n`out-of-the-box' for downstream tasks. We focus on generative language models\nas they are well-suited for extracting biases inherited from training data.\nSpecifically, we conduct an in-depth analysis of GPT-2, which is the most\ndownloaded text generation model on HuggingFace, with over half a million\ndownloads per month. We assess biases related to occupational associations for\ndifferent protected categories by intersecting gender with religion, sexuality,\nethnicity, political affiliation, and continental name origin. Using a\ntemplate-based data collection pipeline, we collect 396K sentence completions\nmade by GPT-2 and find: (i) The machine-predicted jobs are less diverse and\nmore stereotypical for women than for men, especially for intersections; (ii)\nIntersectional interactions are highly relevant for occupational associations,\nwhich we quantify by fitting 262 logistic models; (iii) For most occupations,\nGPT-2 reflects the skewed gender and ethnicity distribution found in US Labor\nBureau data, and even pulls the societally-skewed distribution towards gender\nparity in cases where its predictions deviate from real labor market\nobservations. This raises the normative question of what language models should\nlearn - whether they should reflect or correct for existing inequalities.", "published": "2021-02-08 11:10:27", "link": "http://arxiv.org/abs/2102.04130v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Hybrid Task-Oriented Dialog System with Domain and Task Adaptive\n  Pretraining", "abstract": "This paper describes our submission for the End-to-end Multi-domain Task\nCompletion Dialog shared task at the 9th Dialog System Technology Challenge\n(DSTC-9). Participants in the shared task build an end-to-end task completion\ndialog system which is evaluated by human evaluation and a user simulator based\nautomatic evaluation. Different from traditional pipelined approaches where\nmodules are optimized individually and suffer from cascading failure, we\npropose an end-to-end dialog system that 1) uses Generative Pretraining 2\n(GPT-2) as the backbone to jointly solve Natural Language Understanding, Dialog\nState Tracking, and Natural Language Generation tasks, 2) adopts Domain and\nTask Adaptive Pretraining to tailor GPT-2 to the dialog domain before\nfinetuning, 3) utilizes heuristic pre/post-processing rules that greatly\nsimplify the prediction tasks and improve generalizability, and 4) equips a\nfault tolerance module to correct errors and inappropriate responses. Our\nproposed method significantly outperforms baselines and ties for first place in\nthe official evaluation. We make our source code publicly available.", "published": "2021-02-08 20:02:30", "link": "http://arxiv.org/abs/2102.04506v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "End-to-End Multi-Channel Transformer for Speech Recognition", "abstract": "Transformers are powerful neural architectures that allow integrating\ndifferent modalities using attention mechanisms. In this paper, we leverage the\nneural transformer architectures for multi-channel speech recognition systems,\nwhere the spectral and spatial information collected from different microphones\nare integrated using attention layers. Our multi-channel transformer network\nmainly consists of three parts: channel-wise self attention layers (CSA),\ncross-channel attention layers (CCA), and multi-channel encoder-decoder\nattention layers (EDA). The CSA and CCA layers encode the contextual\nrelationship within and between channels and across time, respectively. The\nchannel-attended outputs from CSA and CCA are then fed into the EDA layers to\nhelp decode the next token given the preceding ones. The experiments show that\nin a far-field in-house dataset, our method outperforms the baseline\nsingle-channel transformer, as well as the super-directive and neural\nbeamformers cascaded with the transformers.", "published": "2021-02-08 00:12:44", "link": "http://arxiv.org/abs/2102.03951v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Generate and Revise: Reinforcement Learning in Neural Poetry", "abstract": "Writers, poets, singers usually do not create their compositions in just one\nbreath. Text is revisited, adjusted, modified, rephrased, even multiple times,\nin order to better convey meanings, emotions and feelings that the author wants\nto express. Amongst the noble written arts, Poetry is probably the one that\nneeds to be elaborated the most, since the composition has to formally respect\npredefined meter and rhyming schemes. In this paper, we propose a framework to\ngenerate poems that are repeatedly revisited and corrected, as humans do, in\norder to improve their overall quality. We frame the problem of revising poems\nin the context of Reinforcement Learning and, in particular, using Proximal\nPolicy Optimization. Our model generates poems from scratch and it learns to\nprogressively adjust the generated text in order to match a target criterion.\nWe evaluate this approach in the case of matching a rhyming scheme, without\nhaving any information on which words are responsible of creating rhymes and on\nhow to coherently alter the poem words. The proposed framework is general and,\nwith an appropriate reward shaping, it can be applied to other text generation\nproblems.", "published": "2021-02-08 10:35:33", "link": "http://arxiv.org/abs/2102.04114v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RECAST: Enabling User Recourse and Interpretability of Toxicity\n  Detection Models with Interactive Visualization", "abstract": "With the widespread use of toxic language online, platforms are increasingly\nusing automated systems that leverage advances in natural language processing\nto automatically flag and remove toxic comments. However, most automated\nsystems -- when detecting and moderating toxic language -- do not provide\nfeedback to their users, let alone provide an avenue of recourse for these\nusers to make actionable changes. We present our work, RECAST, an interactive,\nopen-sourced web tool for visualizing these models' toxic predictions, while\nproviding alternative suggestions for flagged toxic language. Our work also\nprovides users with a new path of recourse when using these automated\nmoderation tools. RECAST highlights text responsible for classifying toxicity,\nand allows users to interactively substitute potentially toxic phrases with\nneutral alternatives. We examined the effect of RECAST via two large-scale user\nevaluations, and found that RECAST was highly effective at helping users reduce\ntoxicity as detected through the model. Users also gained a stronger\nunderstanding of the underlying toxicity criterion used by black-box models,\nenabling transparency and recourse. In addition, we found that when users focus\non optimizing language for these models instead of their own judgement (which\nis the implied incentive and goal of deploying automated models), these models\ncease to be effective classifiers of toxicity compared to human annotations.\nThis opens a discussion for how toxicity detection models work and should work,\nand their effect on the future of online discourse.", "published": "2021-02-08 18:37:50", "link": "http://arxiv.org/abs/2102.04427v2", "categories": ["cs.HC", "cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.HC"}
{"title": "Wake Word Detection with Streaming Transformers", "abstract": "Modern wake word detection systems usually rely on neural networks for\nacoustic modeling. Transformers has recently shown superior performance over\nLSTM and convolutional networks in various sequence modeling tasks with their\nbetter temporal modeling power. However it is not clear whether this advantage\nstill holds for short-range temporal modeling like wake word detection.\nBesides, the vanilla Transformer is not directly applicable to the task due to\nits non-streaming nature and the quadratic time and space complexity. In this\npaper we explore the performance of several variants of chunk-wise streaming\nTransformers tailored for wake word detection in a recently proposed LF-MMI\nsystem, including looking-ahead to the next chunk, gradient stopping, different\npositional embedding methods and adding same-layer dependency between chunks.\nOur experiments on the Mobvoi wake word dataset demonstrate that our proposed\nTransformer model outperforms the baseline convolution network by 25% on\naverage in false rejection rate at the same false alarm rate with a comparable\nmodel size, while still maintaining linear complexity w.r.t. the sequence\nlength.", "published": "2021-02-08 19:14:32", "link": "http://arxiv.org/abs/2102.04488v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "An Enhanced Corpus for Arabic Newspapers Comments", "abstract": "In this paper, we propose our enhanced approach to create a dedicated corpus\nfor Algerian Arabic newspapers comments. The developed approach has to enhance\nan existing approach by the enrichment of the available corpus and the\ninclusion of the annotation step by following the Model Annotate Train Test\nEvaluate Revise (MATTER) approach. A corpus is created by collecting comments\nfrom web sites of three well know Algerian newspapers. Three classifiers,\nsupport vector machines, na{\\\"i}ve Bayes, and k-nearest neighbors, were used\nfor classification of comments into positive and negative classes. To identify\nthe influence of the stemming in the obtained results, the classification was\ntested with and without stemming. Obtained results show that stemming does not\nenhance considerably the classification due to the nature of Algerian comments\ntied to Algerian Arabic Dialect. The promising results constitute a motivation\nfor us to improve our approach especially in dealing with non Arabic sentences,\nespecially Dialectal and French ones.", "published": "2021-02-08 10:15:44", "link": "http://arxiv.org/abs/2102.09965v1", "categories": ["cs.IR", "cs.CL", "cs.MA"], "primary_category": "cs.IR"}
{"title": "Speaker and Direction Inferred Dual-channel Speech Separation", "abstract": "Most speech separation methods, trying to separate all channel sources\nsimultaneously, are still far from having enough general- ization capabilities\nfor real scenarios where the number of input sounds is usually uncertain and\neven dynamic. In this work, we employ ideas from auditory attention with two\nears and propose a speaker and direction inferred speech separation network\n(dubbed SDNet) to solve the cocktail party problem. Specifically, our SDNet\nfirst parses out the respective perceptual representations with their speaker\nand direction characteristics from the mixture of the scene in a sequential\nmanner. Then, the perceptual representations are utilized to attend to each\ncorresponding speech. Our model gener- ates more precise perceptual\nrepresentations with the help of spatial features and successfully deals with\nthe problem of the unknown number of sources and the selection of outputs. The\nexperiments on standard fully-overlapped speech separation benchmarks, WSJ0-\n2mix, WSJ0-3mix, and WSJ0-2&3mix, show the effectiveness, and our method\nachieves SDR improvements of 25.31 dB, 17.26 dB, and 21.56 dB under anechoic\nsettings. Our codes will be released at https://github.com/aispeech-lab/SDNet.", "published": "2021-02-08 08:28:00", "link": "http://arxiv.org/abs/2102.04056v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ICASSP 2021 Deep Noise Suppression Challenge: Decoupling Magnitude and\n  Phase Optimization with a Two-Stage Deep Network", "abstract": "It remains a tough challenge to recover the speech signals contaminated by\nvarious noises under real acoustic environments. To this end, we propose a\nnovel system for denoising in the complicated applications, which is mainly\ncomprised of two pipelines, namely a two-stage network and a post-processing\nmodule. The first pipeline is proposed to decouple the optimization problem\nw:r:t: magnitude and phase, i.e., only the magnitude is estimated in the first\nstage and both of them are further refined in the second stage. The second\npipeline aims to further suppress the remaining unnatural distorted noise,\nwhich is demonstrated to sufficiently improve the subjective quality. In the\nICASSP 2021 Deep Noise Suppression (DNS) Challenge, our submitted system ranked\ntop-1 for the real-time track 1 in terms of Mean Opinion Score (MOS) with ITU-T\nP.808 framework.", "published": "2021-02-08 13:58:45", "link": "http://arxiv.org/abs/2102.04198v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Extracting the Auditory Attention in a Dual-Speaker Scenario from EEG\n  using a Joint CNN-LSTM Model", "abstract": "Human brain performs remarkably well in segregating a particular speaker from\ninterfering ones in a multi-speaker scenario. It has been recently shown that\nwe can quantitatively evaluate the segregation capability by modelling the\nrelationship between the speech signals present in an auditory scene and the\ncortical signals of the listener measured using electroencephalography (EEG).\nThis has opened up avenues to integrate neuro-feedback into hearing aids\nwhereby the device can infer user's attention and enhance the attended speaker.\nCommonly used algorithms to infer the auditory attention are based on linear\nsystems theory where the speech cues such as envelopes are mapped on to the EEG\nsignals. Here, we present a joint convolutional neural network (CNN) - long\nshort-term memory (LSTM) model to infer the auditory attention. Our joint\nCNN-LSTM model takes the EEG signals and the spectrogram of the multiple\nspeakers as inputs and classifies the attention to one of the speakers. We\nevaluated the reliability of our neural network using three different datasets\ncomprising of 61 subjects where, each subject undertook a dual-speaker\nexperiment. The three datasets analysed corresponded to speech stimuli\npresented in three different languages namely German, Danish and Dutch. Using\nthe proposed joint CNN-LSTM model, we obtained a median decoding accuracy of\n77.2% at a trial duration of three seconds. Furthermore, we evaluated the\namount of sparsity that our model can tolerate by means of magnitude pruning\nand found that the model can tolerate up to 50% sparsity without substantial\nloss of decoding accuracy.", "published": "2021-02-08 01:06:48", "link": "http://arxiv.org/abs/2102.03957v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Non-linear frequency warping using constant-Q transformation for speech\n  emotion recognition", "abstract": "In this work, we explore the constant-Q transform (CQT) for speech emotion\nrecognition (SER). The CQT-based time-frequency analysis provides variable\nspectro-temporal resolution with higher frequency resolution at lower\nfrequencies. Since lower-frequency regions of speech signal contain more\nemotion-related information than higher-frequency regions, the increased\nlow-frequency resolution of CQT makes it more promising for SER than standard\nshort-time Fourier transform (STFT). We present a comparative analysis of\nshort-term acoustic features based on STFT and CQT for SER with deep neural\nnetwork (DNN) as a back-end classifier. We optimize different parameters for\nboth features. The CQT-based features outperform the STFT-based spectral\nfeatures for SER experiments. Further experiments with cross-corpora evaluation\ndemonstrate that the CQT-based systems provide better generalization with\nout-of-domain training data.", "published": "2021-02-08 06:57:16", "link": "http://arxiv.org/abs/2102.04029v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "LightSpeech: Lightweight and Fast Text to Speech with Neural\n  Architecture Search", "abstract": "Text to speech (TTS) has been broadly used to synthesize natural and\nintelligible speech in different scenarios. Deploying TTS in various end\ndevices such as mobile phones or embedded devices requires extremely small\nmemory usage and inference latency. While non-autoregressive TTS models such as\nFastSpeech have achieved significantly faster inference speed than\nautoregressive models, their model size and inference latency are still large\nfor the deployment in resource constrained devices. In this paper, we propose\nLightSpeech, which leverages neural architecture search~(NAS) to automatically\ndesign more lightweight and efficient models based on FastSpeech. We first\nprofile the components of current FastSpeech model and carefully design a novel\nsearch space containing various lightweight and potentially effective\narchitectures. Then NAS is utilized to automatically discover well performing\narchitectures within the search space. Experiments show that the model\ndiscovered by our method achieves 15x model compression ratio and 6.5x\ninference speedup on CPU with on par voice quality. Audio demos are provided at\nhttps://speechresearch.github.io/lightspeech.", "published": "2021-02-08 07:45:06", "link": "http://arxiv.org/abs/2102.04040v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "HumanACGAN: conditional generative adversarial network with human-based\n  auxiliary classifier and its evaluation in phoneme perception", "abstract": "We propose a conditional generative adversarial network (GAN) incorporating\nhumans' perceptual evaluations. A deep neural network (DNN)-based generator of\na GAN can represent a real-data distribution accurately but can never represent\na human-acceptable distribution, which are ranges of data in which humans\naccept the naturalness regardless of whether the data are real or not. A\nHumanGAN was proposed to model the human-acceptable distribution. A DNN-based\ngenerator is trained using a human-based discriminator, i.e., humans'\nperceptual evaluations, instead of the GAN's DNN-based discriminator. However,\nthe HumanGAN cannot represent conditional distributions. This paper proposes\nthe HumanACGAN, a theoretical extension of the HumanGAN, to deal with\nconditional human-acceptable distributions. Our HumanACGAN trains a DNN-based\nconditional generator by regarding humans as not only a discriminator but also\nan auxiliary classifier. The generator is trained by deceiving the human-based\ndiscriminator that scores the unconditioned naturalness and the human-based\nclassifier that scores the class-conditioned perceptual acceptability. The\ntraining can be executed using the backpropagation algorithm involving humans'\nperceptual evaluations. Our experimental results in phoneme perception\ndemonstrate that our HumanACGAN can successfully train this conditional\ngenerator.", "published": "2021-02-08 08:25:29", "link": "http://arxiv.org/abs/2102.04051v1", "categories": ["cs.HC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "An Update on a Progressively Expanded Database for Automated Lung Sound\n  Analysis", "abstract": "Purpose: We previously established an open-access lung sound database,\nHF_Lung_V1, and developed deep learning models for inhalation, exhalation,\ncontinuous adventitious sound (CAS), and discontinuous adventitious sound (DAS)\ndetection. The amount of data used for training contributes to model accuracy.\nHerein, we collected larger quantities of data to further improve model\nperformance. Moreover, the issues of noisy labels and sound overlapping were\nexplored. Methods: HF_Lung_V1 was expanded to HF_Lung_V2 with a 1.45x increase\nin the number of audio files. Convolutional neural network-bidirectional gated\nrecurrent unit network models were trained separately using the HF_Lung_V1\n(V1_Train) and HF_Lung_V2 (V2_Train) training sets and then tested using the\nHF_Lung_V1 (V1_Test) and HF_Lung_V2 (V2_Test) test sets, respectively. Segment\nand event detection performance was evaluated using the F1 scores. Label\nquality was assessed. Moreover, the overlap ratios between inhalation,\nexhalation, CAS, and DAS labels were computed. Results: The model trained using\nV2_Train exhibited improved F1 scores in inhalation, exhalation, and CAS\ndetection on both V1_Test and V2_Test but not in DAS detection. Poor CAS\ndetection was attributed to the quality of CAS labels. DAS detection was\nstrongly influenced by the overlapping of DAS labels with inhalation and\nexhalation labels. Conclusion: Collecting greater quantities of lung sound data\nis vital for developing more accurate lung sound analysis models. To build real\nground-truth labels, the labels must be reworked; this process is ongoing.\nFurthermore, a method for addressing the sound overlapping problem in DAS\ndetection must be formulated.", "published": "2021-02-08 08:52:17", "link": "http://arxiv.org/abs/2102.04062v3", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Switching Variational Auto-Encoders for Noise-Agnostic Audio-visual\n  Speech Enhancement", "abstract": "Recently, audio-visual speech enhancement has been tackled in the\nunsupervised settings based on variational auto-encoders (VAEs), where during\ntraining only clean data is used to train a generative model for speech, which\nat test time is combined with a noise model, e.g. nonnegative matrix\nfactorization (NMF), whose parameters are learned without supervision.\nConsequently, the proposed model is agnostic to the noise type. When visual\ndata are clean, audio-visual VAE-based architectures usually outperform the\naudio-only counterpart. The opposite happens when the visual data are corrupted\nby clutter, e.g. the speaker not facing the camera. In this paper, we propose\nto find the optimal combination of these two architectures through time. More\nprecisely, we introduce the use of a latent sequential variable with Markovian\ndependencies to switch between different VAE architectures through time in an\nunsupervised manner: leading to switching variational auto-encoder (SwVAE). We\npropose a variational factorization to approximate the computationally\nintractable posterior distribution. We also derive the corresponding\nvariational expectation-maximization algorithm to estimate the parameters of\nthe model and enhance the speech signal. Our experiments demonstrate the\npromising performance of SwVAE.", "published": "2021-02-08 11:45:02", "link": "http://arxiv.org/abs/2102.04144v1", "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Federated Acoustic Modeling For Automatic Speech Recognition", "abstract": "Data privacy and protection is a crucial issue for any automatic speech\nrecognition (ASR) service provider when dealing with clients. In this paper, we\ninvestigate federated acoustic modeling using data from multiple clients. A\nclient's data is stored on a local data server and the clients communicate only\nmodel parameters with a central server, and not their data. The communication\nhappens infrequently to reduce the communication cost. To mitigate the non-iid\nissue, client adaptive federated training (CAFT) is proposed to canonicalize\ndata across clients. The experiments are carried out on 1,150 hours of speech\ndata from multiple domains. Hybrid LSTM acoustic models are trained via\nfederated learning and their performance is compared to traditional centralized\nacoustic model training. The experimental results demonstrate the effectiveness\nof the proposed federated acoustic modeling strategy. We also show that CAFT\ncan further improve the performance of the federated acoustic model.", "published": "2021-02-08 18:39:36", "link": "http://arxiv.org/abs/2102.04429v1", "categories": ["cs.SD", "cs.DC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Diagnosis of COVID-19 and Non-COVID-19 Patients by Classifying Only a\n  Single Cough Sound", "abstract": "In this study, we proposed a machine learning-based system to distinguish\npatients with COVID-19 from non-COVID-19 patients by analyzing only a single\ncough sound. Two different data sets were used, one accessible for the public\nand the other available on request. After combining the data sets, the features\nwere obtained from the cough sounds using the mel-frequency cepstral\ncoefficients (MFCCs) method, and then they were classified with seven different\nmachine learning classifiers. To determine the optimum values of\nhyperparameters for MFCCs and classifiers, the leave-one-out cross-validation\n(LOO-CV) strategy was implemented. Based on the results, the k-nearest\nneighbors classifier based on the Euclidean distance (k-NN Euclidean) with the\naccuracy rate, sensitivity of COVID-19, sensitivity of non-COVID-19, F-measure,\nand area under the ROC curve (AUC) of 0.9833, 1.0000, 0.9720, 0.9799, and\n0.9860, respectively, is more successful than other classifiers. Finally, the\nbest and most effective features were determined for each classifier using the\nsequential forward selection (SFS) method. According to the results, the\nproposed system is excellent compared with similar studies in the literature\nand can be easily used in smartphones and facilitate the diagnosis of COVID-19\npatients. In addition, since the used data set includes reflex and unconscious\ncoughs, the results showed that conscious or unconscious coughing has no effect\non the diagnosis of COVID-19 patients based on the cough sound.", "published": "2021-02-08 12:38:37", "link": "http://arxiv.org/abs/2102.04880v1", "categories": ["cs.SD", "eess.AS", "math.OC"], "primary_category": "cs.SD"}
