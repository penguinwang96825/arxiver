{"title": "EPiDA: An Easy Plug-in Data Augmentation Framework for High Performance\n  Text Classification", "abstract": "Recent works have empirically shown the effectiveness of data augmentation\n(DA) in NLP tasks, especially for those suffering from data scarcity.\nIntuitively, given the size of generated data, their diversity and quality are\ncrucial to the performance of targeted tasks. However, to the best of our\nknowledge, most existing methods consider only either the diversity or the\nquality of augmented data, thus cannot fully mine the potential of DA for NLP.\nIn this paper, we present an easy and plug-in data augmentation framework EPiDA\nto support effective text classification. EPiDA employs two mechanisms:\nrelative entropy maximization (REM) and conditional entropy minimization (CEM)\nto control data generation, where REM is designed to enhance the diversity of\naugmented data while CEM is exploited to ensure their semantic consistency.\nEPiDA can support efficient and continuous data generation for effective\nclassifier training. Extensive experiments show that EPiDA outperforms existing\nSOTA methods in most cases, though not using any agent networks or pre-trained\ngeneration networks, and it works well with various DA algorithms and\nclassification models. Code is available at\nhttps://github.com/zhaominyiz/EPiDA.", "published": "2022-04-24 06:53:48", "link": "http://arxiv.org/abs/2204.11205v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Win Lottery Tickets in BERT Transfer via Task-agnostic Mask\n  Training", "abstract": "Recent studies on the lottery ticket hypothesis (LTH) show that pre-trained\nlanguage models (PLMs) like BERT contain matching subnetworks that have similar\ntransfer learning performance as the original PLM. These subnetworks are found\nusing magnitude-based pruning. In this paper, we find that the BERT subnetworks\nhave even more potential than these studies have shown. Firstly, we discover\nthat the success of magnitude pruning can be attributed to the preserved\npre-training performance, which correlates with the downstream transferability.\nInspired by this, we propose to directly optimize the subnetwork structure\ntowards the pre-training objectives, which can better preserve the pre-training\nperformance. Specifically, we train binary masks over model weights on the\npre-training tasks, with the aim of preserving the universal transferability of\nthe subnetwork, which is agnostic to any specific downstream tasks. We then\nfine-tune the subnetworks on the GLUE benchmark and the SQuAD dataset. The\nresults show that, compared with magnitude pruning, mask training can\neffectively find BERT subnetworks with improved overall performance on\ndownstream tasks. Moreover, our method is also more efficient in searching\nsubnetworks and more advantageous when fine-tuning within a certain range of\ndata scarcity. Our code is available at https://github.com/llyx97/TAMT.", "published": "2022-04-24 08:42:47", "link": "http://arxiv.org/abs/2204.11218v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open-domain Dialogue Generation Grounded with Dynamic Multi-form\n  Knowledge Fusion", "abstract": "Open-domain multi-turn conversations normally face the challenges of how to\nenrich and expand the content of the conversation. Recently, many approaches\nbased on external knowledge are proposed to generate rich semantic and\ninformation conversation. Two types of knowledge have been studied for\nknowledge-aware open-domain dialogue generation: structured triples from\nknowledge graphs and unstructured texts from documents. To take both advantages\nof abundant unstructured latent knowledge in the documents and the information\nexpansion capabilities of the structured knowledge graph, this paper presents a\nnew dialogue generation model, Dynamic Multi-form Knowledge Fusion based\nOpen-domain Chatt-ing Machine (DMKCM).In particular, DMKCM applies an indexed\ntext (a virtual Knowledge Base) to locate relevant documents as 1st hop and\nthen expands the content of the dialogue and its 1st hop using a commonsense\nknowledge graph to get apposite triples as 2nd hop. To merge these two forms of\nknowledge into the dialogue effectively, we design a dynamic virtual knowledge\nselector and a controller that help to enrich and expand knowledge space.\nMoreover, DMKCM adopts a novel dynamic knowledge memory module that effectively\nuses historical reasoning knowledge to generate better responses. Experimental\nresults indicate the effectiveness of our method in terms of dialogue coherence\nand informativeness.", "published": "2022-04-24 10:32:48", "link": "http://arxiv.org/abs/2204.11239v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Item Response Theory Framework for Persuasion", "abstract": "In this paper, we apply Item Response Theory, popular in education and\npolitical science research, to the analysis of argument persuasiveness in\nlanguage. We empirically evaluate the model's performance on three datasets,\nincluding a novel dataset in the area of political advocacy. We show the\nadvantages of separating these components under several style and content\nrepresentations, including evaluating the ability of the speaker embeddings\ngenerated by the model to parallel real-world observations about\npersuadability.", "published": "2022-04-24 19:14:11", "link": "http://arxiv.org/abs/2204.11337v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Complexity and Avoidance", "abstract": "In this dissertation we examine the relationships between the several\nhierarchies, including the complexity, $\\mathrm{LUA}$ (Linearly Universal\nAvoidance), and shift complexity hierarchies, with an eye towards quantitative\nbounds on growth rates therein. We show that for suitable $f$ and $p$, there\nare $q$ and $g$ such that $\\mathrm{LUA}(q) \\leq_\\mathrm{s} \\mathrm{COMPLEX}(f)$\nand $\\mathrm{COMPLEX}(g) \\leq_\\mathrm{s} \\mathrm{LUA}(p)$, as well as quantify\nthe growth rates of $q$ and $g$. In the opposite direction, we show that for\ncertain sub-identical $f$ satisfying $\\lim_{n \\to \\infty}{f(n)/n}=1$ there is a\n$q$ such that $\\mathrm{COMPLEX}(f) \\leq_\\mathrm{w} \\mathrm{LUA}(q)$, and for\ncertain fast-growing $p$ there is a $g$ such that $\\mathrm{LUA}(p)\n\\leq_\\mathrm{s} \\mathrm{COMPLEX}(g)$, as well as quantify the growth rates of\n$q$ and $g$.\n  Concerning shift complexity, explicit bounds are given on how slow-growing\n$q$ must be for any member of $\\rm{LUA}(q)$ to compute $\\delta$-shift complex\nsequences. Motivated by the complexity hierarchy, we generalize the notion of\nshift complexity to consider sequences $X$ satisfying $\\operatorname{KP}(\\tau)\n\\geq f(|\\tau|) - O(1)$ for all substrings $\\tau$ of $X$ where $f$ is any order\nfunction. We show that for sufficiently slow-growing $f$, $f$-shift complex\nsequences can be uniformly computed by $g$-complex sequences, where $g$ grows\nslightly faster than $f$.\n  The structure of the $\\mathrm{LUA}$ hierarchy is examined using bushy tree\nforcing, with the main result being that for any order function $p$, there is a\nslow-growing order function $q$ such that $\\mathrm{LUA}(p)$ and\n$\\mathrm{LUA}(q)$ are weakly incomparable. Using this, we prove new results\nabout the filter of the weak degrees of deep nonempty $\\Pi^0_1$ classes and the\nconnection between the shift complexity and $\\mathrm{LUA}$ hierarchies.", "published": "2022-04-24 14:36:38", "link": "http://arxiv.org/abs/2204.11289v1", "categories": ["math.LO", "cs.CL", "03D75", "F.4.1"], "primary_category": "math.LO"}
{"title": "Entity-Conditioned Question Generation for Robust Attention Distribution\n  in Neural Information Retrieval", "abstract": "We show that supervised neural information retrieval (IR) models are prone to\nlearning sparse attention patterns over passage tokens, which can result in key\nphrases including named entities receiving low attention weights, eventually\nleading to model under-performance. Using a novel targeted synthetic data\ngeneration method that identifies poorly attended entities and conditions the\ngeneration episodes on those, we teach neural IR to attend more uniformly and\nrobustly to all entities in a given passage. On two public IR benchmarks, we\nempirically show that the proposed method helps improve both the model's\nattention patterns and retrieval performance, including in zero-shot settings.", "published": "2022-04-24 22:36:48", "link": "http://arxiv.org/abs/2204.11373v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Knowledge-aware Document Summarization: A Survey of Knowledge, Embedding\n  Methods and Architectures", "abstract": "Knowledge-aware methods have boosted a range of natural language processing\napplications over the last decades. With the gathered momentum, knowledge\nrecently has been pumped into enormous attention in document summarization, one\nof natural language processing applications. Previous works reported that\nknowledge-embedded document summarizers excel at generating superior digests,\nespecially in terms of informativeness, coherence, and fact consistency. This\npaper pursues to present the first systematic survey for the state-of-the-art\nmethodologies that embed knowledge into document summarizers. Particularly, we\npropose novel taxonomies to recapitulate knowledge and knowledge embeddings\nunder the document summarization view. We further explore how embeddings are\ngenerated in embedding learning architectures of document summarization models,\nespecially of deep learning models. At last, we discuss the challenges of this\ntopic and future directions.", "published": "2022-04-24 04:36:07", "link": "http://arxiv.org/abs/2204.11190v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving the Naturalness of Simulated Conversations for End-to-End\n  Neural Diarization", "abstract": "This paper investigates a method for simulating natural conversation in the\nmodel training of end-to-end neural diarization (EEND). Due to the lack of any\nannotated real conversational dataset, EEND is usually pretrained on a\nlarge-scale simulated conversational dataset first and then adapted to the\ntarget real dataset. Simulated datasets play an essential role in the training\nof EEND, but as yet there has been insufficient investigation into an optimal\nsimulation method. We thus propose a method to simulate natural conversational\nspeech. In contrast to conventional methods, which simply combine the speech of\nmultiple speakers, our method takes turn-taking into account. We define four\ntypes of speaker transition and sequentially arrange them to simulate natural\nconversations. The dataset simulated using our method was found to be\nstatistically similar to the real dataset in terms of the silence and overlap\nratios. The experimental results on two-speaker diarization using the CALLHOME\nand CSJ datasets showed that the simulated dataset contributes to improving the\nperformance of EEND.", "published": "2022-04-24 09:55:32", "link": "http://arxiv.org/abs/2204.11232v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improved far-field speech recognition using Joint Variational\n  Autoencoder", "abstract": "Automatic Speech Recognition (ASR) systems suffer considerably when source\nspeech is corrupted with noise or room impulse responses (RIR). Typically,\nspeech enhancement is applied in both mismatched and matched scenario training\nand testing. In matched setting, acoustic model (AM) is trained on\ndereverberated far-field features while in mismatched setting, AM is fixed. In\nrecent past, mapping speech features from far-field to close-talk using\ndenoising autoencoder (DA) has been explored. In this paper, we focus on\nmatched scenario training and show that the proposed joint VAE based mapping\nachieves a significant improvement over DA. Specifically, we observe an\nabsolute improvement of 2.5% in word error rate (WER) compared to DA based\nenhancement and 3.96% compared to AM trained directly on far-field filterbank\nfeatures.", "published": "2022-04-24 14:14:04", "link": "http://arxiv.org/abs/2204.11286v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Emotion-Aware Transformer Encoder for Empathetic Dialogue Generation", "abstract": "Modern day conversational agents are trained to emulate the manner in which\nhumans communicate. To emotionally bond with the user, these virtual agents\nneed to be aware of the affective state of the user. Transformers are the\nrecent state of the art in sequence-to-sequence learning that involves training\nan encoder-decoder model with word embeddings from utterance-response pairs. We\npropose an emotion-aware transformer encoder for capturing the emotional\nquotient in the user utterance in order to generate human-like empathetic\nresponses. The contributions of our paper are as follows: 1) An emotion\ndetector module trained on the input utterances determines the affective state\nof the user in the initial phase 2) A novel transformer encoder is proposed\nthat adds and normalizes the word embedding with emotion embedding thereby\nintegrating the semantic and affective aspects of the input utterance 3) The\nencoder and decoder stacks belong to the Transformer-XL architecture which is\nthe recent state of the art in language modeling. Experimentation on the\nbenchmark Facebook AI empathetic dialogue dataset confirms the efficacy of our\nmodel from the higher BLEU-4 scores achieved for the generated responses as\ncompared to existing methods. Emotionally intelligent virtual agents are now a\nreality and inclusion of affect as a modality in all human-machine interfaces\nis foreseen in the immediate future.", "published": "2022-04-24 17:05:36", "link": "http://arxiv.org/abs/2204.11320v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Hate Me Not: Detecting Hate Inducing Memes in Code Switched Languages", "abstract": "The rise in the number of social media users has led to an increase in the\nhateful content posted online. In countries like India, where multiple\nlanguages are spoken, these abhorrent posts are from an unusual blend of\ncode-switched languages. This hate speech is depicted with the help of images\nto form \"Memes\" which create a long-lasting impact on the human mind. In this\npaper, we take up the task of hate and offense detection from multimodal data,\ni.e. images (Memes) that contain text in code-switched languages. We firstly\npresent a novel triply annotated Indian political Memes (IPM) dataset, which\ncomprises memes from various Indian political events that have taken place\npost-independence and are classified into three distinct categories. We also\npropose a binary-channelled CNN cum LSTM based model to process the images\nusing the CNN model and text using the LSTM model to get state-of-the-art\nresults for this task.", "published": "2022-04-24 21:03:57", "link": "http://arxiv.org/abs/2204.11356v1", "categories": ["cs.LG", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Twitter-Based Gender Recognition Using Transformers", "abstract": "Social media contains useful information about people and the society that\ncould help advance research in many different areas (e.g. by applying opinion\nmining, emotion/sentiment analysis, and statistical analysis) such as business\nand finance, health, socio-economic inequality and gender vulnerability. User\ndemographics provide rich information that could help study the subject\nfurther. However, user demographics such as gender are considered private and\nare not freely available. In this study, we propose a model based on\ntransformers to predict the user's gender from their images and tweets. We\nfine-tune a model based on Vision Transformers (ViT) to stratify female and\nmale images. Next, we fine-tune another model based on Bidirectional Encoders\nRepresentations from Transformers (BERT) to recognize the user's gender by\ntheir tweets. This is highly beneficial, because not all users provide an image\nthat indicates their gender. The gender of such users could be detected form\ntheir tweets. The combination model improves the accuracy of image and text\nclassification models by 6.98% and 4.43%, respectively. This shows that the\nimage and text classification models are capable of complementing each other by\nproviding additional information to one another. We apply our method to the\nPAN-2018 dataset, and obtain an accuracy of 85.52%.", "published": "2022-04-24 19:58:42", "link": "http://arxiv.org/abs/2205.06801v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Few-Shot Speaker Identification Using Depthwise Separable Convolutional\n  Network with Channel Attention", "abstract": "Although few-shot learning has attracted much attention from the fields of\nimage and audio classification, few efforts have been made on few-shot speaker\nidentification. In the task of few-shot learning, overfitting is a tough\nproblem mainly due to the mismatch between training and testing conditions. In\nthis paper, we propose a few-shot speaker identification method which can\nalleviate the overfitting problem. In the proposed method, the model of a\ndepthwise separable convolutional network with channel attention is trained\nwith a prototypical loss function. Experimental datasets are extracted from\nthree public speech corpora: Aishell-2, VoxCeleb1 and TORGO. Experimental\nresults show that the proposed method exceeds state-of-the-art methods for\nfew-shot speaker identification in terms of accuracy and F-score.", "published": "2022-04-24 03:31:05", "link": "http://arxiv.org/abs/2204.11180v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dictionary Attacks on Speaker Verification", "abstract": "In this paper, we propose dictionary attacks against speaker verification - a\nnovel attack vector that aims to match a large fraction of speaker population\nby chance. We introduce a generic formulation of the attack that can be used\nwith various speech representations and threat models. The attacker uses\nadversarial optimization to maximize raw similarity of speaker embeddings\nbetween a seed speech sample and a proxy population. The resulting master voice\nsuccessfully matches a non-trivial fraction of people in an unknown population.\nAdversarial waveforms obtained with our approach can match on average 69% of\nfemales and 38% of males enrolled in the target system at a strict decision\nthreshold calibrated to yield false alarm rate of 1%. By using the attack with\na black-box voice cloning system, we obtain master voices that are effective in\nthe most challenging conditions and transferable between speaker encoders. We\nalso show that, combined with multiple attempts, this attack opens even more to\nserious issues on the security of these systems.", "published": "2022-04-24 15:31:41", "link": "http://arxiv.org/abs/2204.11304v2", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
