{"title": "Controlling Large Language Model Agents with Entropic Activation\n  Steering", "abstract": "The rise of large language models (LLMs) has prompted increasing interest in\ntheir use as in-context learning agents. At the core of agentic behavior is the\ncapacity for exploration, or the ability to actively gather information about\nthe environment. But how do LLM agents explore, and how can we control their\nexploratory behaviors? To answer these questions, we take a\nrepresentation-level perspective, and introduce Entropic Activation Steering\n(EAST), an activation steering method for in-context LLM agents. Firstly, we\ndemonstrate that EAST can effectively manipulate an LLM agent's exploration by\ndirectly affecting the high-level actions parsed from the outputs of the LLM,\nin contrast to token-level temperature sampling. Secondly, we reveal how\napplying this control modulates the uncertainty exhibited in the LLM's\nthoughts, guiding the agent towards more exploratory actions. Finally, we\ndemonstrate that the steering vectors obtained by EAST generalize across task\nvariants. In total, these results show that LLM agents explicitly encode\nuncertainty over their actions in their representation space. Our work paves\nthe way for a new understanding of the functioning of LLM agents and to\neffective control of their decision-making behaviors.", "published": "2024-06-01 00:25:00", "link": "http://arxiv.org/abs/2406.00244v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Large Vision Language Models up to the Challenge of Chart\n  Comprehension and Reasoning? An Extensive Investigation into the Capabilities\n  and Limitations of LVLMs", "abstract": "Natural language is a powerful complementary modality of communication for\ndata visualizations, such as bar and line charts. To facilitate chart-based\nreasoning using natural language, various downstream tasks have been introduced\nrecently such as chart question answering, chart summarization, and\nfact-checking with charts. These tasks pose a unique challenge, demanding both\nvision-language reasoning and a nuanced understanding of chart data tables,\nvisual encodings, and natural language prompts. Despite the recent success of\nLarge Language Models (LLMs) across diverse NLP tasks, their abilities and\nlimitations in the realm of data visualization remain under-explored, possibly\ndue to their lack of multi-modal capabilities. To bridge the gap, this paper\npresents the first comprehensive evaluation of the recently developed large\nvision language models (LVLMs) for chart understanding and reasoning tasks. Our\nevaluation includes a comprehensive assessment of LVLMs, including GPT-4V and\nGemini, across four major chart reasoning tasks. Furthermore, we perform a\nqualitative evaluation of LVLMs' performance on a diverse range of charts,\naiming to provide a thorough analysis of their strengths and weaknesses. Our\nfindings reveal that LVLMs demonstrate impressive abilities in generating\nfluent texts covering high-level data insights while also encountering common\nproblems like hallucinations, factual errors, and data bias. We highlight the\nkey strengths and limitations of chart comprehension tasks, offering insights\nfor future research.", "published": "2024-06-01 01:43:30", "link": "http://arxiv.org/abs/2406.00257v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Closer Look at Logical Reasoning with LLMs: The Choice of Tool Matters", "abstract": "The emergence of Large Language Models (LLMs) has demonstrated promising\nprogress in solving logical reasoning tasks effectively. Several recent\napproaches have proposed to change the role of the LLM from the reasoner into a\ntranslator between natural language statements and symbolic representations\nwhich are then sent to external symbolic solvers to resolve. This paradigm has\nestablished the current state-of-the-art result in logical reasoning (i.e.,\ndeductive reasoning). However, it remains unclear whether the variance in\nperformance of these approaches stems from the methodologies employed or the\nspecific symbolic solvers utilized. There is a lack of consistent comparison\nbetween symbolic solvers and how they influence the overall reported\nperformance. This is important, as each symbolic solver also has its own input\nsymbolic language, presenting varying degrees of challenge in the translation\nprocess. To address this gap, we perform experiments on 3 deductive reasoning\nbenchmarks with LLMs augmented with widely used symbolic solvers: Z3, Pyke, and\nProver9. The tool-executable rates of symbolic translation generated by\ndifferent LLMs exhibit a near 50% performance variation. This highlights a\nsignificant difference in performance rooted in very basic choices of tools.\nThe almost linear correlation between the executable rate of translations and\nthe accuracy of the outcomes from Prover9 highlight a strong alignment between\nLLMs ability to translate into Prover9 symbolic language, and the correctness\nof those translations.", "published": "2024-06-01 03:29:56", "link": "http://arxiv.org/abs/2406.00284v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Metrics: Evaluating LLMs' Effectiveness in Culturally Nuanced,\n  Low-Resource Real-World Scenarios", "abstract": "The deployment of Large Language Models (LLMs) in real-world applications\npresents both opportunities and challenges, particularly in multilingual and\ncode-mixed communication settings. This research evaluates the performance of\nseven leading LLMs in sentiment analysis on a dataset derived from multilingual\nand code-mixed WhatsApp chats, including Swahili, English and Sheng. Our\nevaluation includes both quantitative analysis using metrics like F1 score and\nqualitative assessment of LLMs' explanations for their predictions. We find\nthat, while Mistral-7b and Mixtral-8x7b achieved high F1 scores, they and other\nLLMs such as GPT-3.5-Turbo, Llama-2-70b, and Gemma-7b struggled with\nunderstanding linguistic and contextual nuances, as well as lack of\ntransparency in their decision-making process as observed from their\nexplanations. In contrast, GPT-4 and GPT-4-Turbo excelled in grasping diverse\nlinguistic inputs and managing various contextual information, demonstrating\nhigh consistency with human alignment and transparency in their decision-making\nprocess. The LLMs however, encountered difficulties in incorporating cultural\nnuance especially in non-English settings with GPT-4s doing so inconsistently.\nThe findings emphasize the necessity of continuous improvement of LLMs to\neffectively tackle the challenges of culturally nuanced, low-resource\nreal-world settings and the need for developing evaluation benchmarks for\ncapturing these issues.", "published": "2024-06-01 07:36:59", "link": "http://arxiv.org/abs/2406.00343v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SPAGHETTI: Open-Domain Question Answering from Heterogeneous Data\n  Sources with Retrieval and Semantic Parsing", "abstract": "We introduce SPAGHETTI: Semantic Parsing Augmented Generation for Hybrid\nEnglish information from Text Tables and Infoboxes, a hybrid question-answering\n(QA) pipeline that utilizes information from heterogeneous knowledge sources,\nincluding knowledge base, text, tables, and infoboxes. Our LLM-augmented\napproach achieves state-of-the-art performance on the Compmix dataset, the most\ncomprehensive heterogeneous open-domain QA dataset, with 56.5% exact match (EM)\nrate. More importantly, manual analysis on a sample of the dataset suggests\nthat SPAGHETTI is more than 90% accurate, indicating that EM is no longer\nsuitable for assessing the capabilities of QA systems today.", "published": "2024-06-01 21:54:09", "link": "http://arxiv.org/abs/2406.00562v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Dimensional Optimization for Text Summarization via Reinforcement\n  Learning", "abstract": "The evaluation of summary quality encompasses diverse dimensions such as\nconsistency, coherence, relevance, and fluency. However, existing summarization\nmethods often target a specific dimension, facing challenges in generating\nwell-balanced summaries across multiple dimensions. In this paper, we propose\nmulti-objective reinforcement learning tailored to generate balanced summaries\nacross all four dimensions. We introduce two multi-dimensional optimization\n(MDO) strategies for adaptive learning: 1) MDO_min, rewarding the current\nlowest dimension score, and 2) MDO_pro, optimizing multiple dimensions similar\nto multi-task learning, resolves conflicting gradients across dimensions\nthrough gradient projection. Unlike prior ROUGE-based rewards relying on\nreference summaries, we use a QA-based reward model that aligns with human\npreferences. Further, we discover the capability to regulate the length of\nsummaries by adjusting the discount factor, seeking the generation of concise\nyet informative summaries that encapsulate crucial points. Our approach\nachieved substantial performance gains compared to baseline models on\nrepresentative summarization datasets, particularly in the overlooked\ndimensions.", "published": "2024-06-01 05:15:12", "link": "http://arxiv.org/abs/2406.00303v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HonestLLM: Toward an Honest and Helpful Large Language Model", "abstract": "Large Language Models (LLMs) have achieved remarkable success across various\nindustries due to their exceptional generative capabilities. However, for safe\nand effective real-world deployments, ensuring honesty and helpfulness is\ncritical. This paper addresses the question: Can we prioritize the helpfulness\nof LLMs while preserving their honesty? To begin with, we establish exhaustive\nprinciples aimed at guaranteeing the honesty of LLM. Additionally, we introduce\na novel dataset, referred to as HoneSet, comprising 930 queries spanning six\ncategories meticulously crafted to assess an LLM's capacity for maintaining\nhonesty. Subsequently, we present two approaches to augmenting honesty and\nhelpfulness in LLMs: a training-free enhancement and a fine-tuning-based\nimprovement. The training-free approach, which is based on curiosity-driven\nprompting, empowers LLMs to articulate internal confusion and uncertainty\nregarding queries, thereby optimizing their responses. Conversely, the\nfine-tuning-based method employs a two-stage process inspired by curriculum\nlearning: initially instructing LLMs to discern between honest and dishonest\nresponses, then refining their training to enhance helpfulness. Experiments\nconducted on nine prominent LLMs demonstrate a significant improvement in\nalignment with honesty across all models through the implementation of our\nproposed enhancements. Particularly noteworthy is the 65.3% enhancement\nobserved in Llama3-8b and the remarkable 124.7% improvement in Mistral-7b, as\nmeasured by the H$^{2}$ (honest and helpful) assessment. We believe that our\nwork can pave the way for developing more trustworthy LLMs for real-world\napplications.", "published": "2024-06-01 09:36:16", "link": "http://arxiv.org/abs/2406.00380v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Gender Bias Detection in Court Decisions: A Brazilian Case Study", "abstract": "Data derived from the realm of the social sciences is often produced in\ndigital text form, which motivates its use as a source for natural language\nprocessing methods. Researchers and practitioners have developed and relied on\nartificial intelligence techniques to collect, process, and analyze documents\nin the legal field, especially for tasks such as text summarization and\nclassification. While increasing procedural efficiency is often the primary\nmotivation behind natural language processing in the field, several works have\nproposed solutions for human rights-related issues, such as assessment of\npublic policy and institutional social settings. One such issue is the presence\nof gender biases in court decisions, which has been largely studied in social\nsciences fields; biased institutional responses to gender-based violence are a\nviolation of international human rights dispositions since they prevent gender\nminorities from accessing rights and hamper their dignity. Natural language\nprocessing-based approaches can help detect these biases on a larger scale.\nStill, the development and use of such tools require researchers and\npractitioners to be mindful of legal and ethical aspects concerning data\nsharing and use, reproducibility, domain expertise, and value-charged choices.\nIn this work, we (a) present an experimental framework developed to\nautomatically detect gender biases in court decisions issued in Brazilian\nPortuguese and (b) describe and elaborate on features we identify to be\ncritical in such a technology, given its proposed use as a support tool for\nresearch and assessment of court~activity.", "published": "2024-06-01 10:34:15", "link": "http://arxiv.org/abs/2406.00393v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Prompt Chaining or Stepwise Prompt? Refinement in Text Summarization", "abstract": "Large language models (LLMs) have demonstrated the capacity to improve\nsummary quality by mirroring a human-like iterative process of critique and\nrefinement starting from the initial draft. Two strategies are designed to\nperform this iterative process: Prompt Chaining and Stepwise Prompt. Prompt\nchaining orchestrates the drafting, critiquing, and refining phases through a\nseries of three discrete prompts, while Stepwise prompt integrates these phases\nwithin a single prompt. However, the relative effectiveness of the two methods\nhas not been extensively studied. This paper is dedicated to examining and\ncomparing these two methods in the context of text summarization to ascertain\nwhich method stands out as the most effective. Experimental results show that\nthe prompt chaining method can produce a more favorable outcome. This might be\nbecause stepwise prompt might produce a simulated refinement process according\nto our various experiments. Since refinement is adaptable to diverse tasks, our\nconclusions have the potential to be extrapolated to other applications,\nthereby offering insights that may contribute to the broader development of\nLLMs.", "published": "2024-06-01 17:28:38", "link": "http://arxiv.org/abs/2406.00507v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LIDAO: Towards Limited Interventions for Debiasing (Large) Language\n  Models", "abstract": "Large language models (LLMs) have achieved impressive performance on various\nnatural language generation tasks. Nonetheless, they suffer from generating\nnegative and harmful contents that are biased against certain demographic\ngroups (e.g., female), raising severe fairness concerns. As remedies, prior\nworks intervened the generation by removing attitude or demographic\ninformation, inevitably degrading the generation quality and resulting in\nnotable \\textit{fairness-fluency} trade-offs. However, it is still\nunder-explored to what extent the fluency \\textit{has to} be affected in order\nto achieve a desired level of fairness. In this work, we conduct the first\nformal study from an information-theoretic perspective. We show that previous\napproaches are excessive for debiasing and propose LIDAO, a general framework\nto debias a (L)LM at a better fluency provably. We further robustify LIDAO in\nadversarial scenarios, where a carefully-crafted prompt may stimulate LLMs\nexhibiting instruction-following abilities to generate texts with fairness\nissue appears only when the prompt is also taken into account. Experiments on\nthree LMs ranging from 0.7B to 7B parameters demonstrate the superiority of our\nmethod.", "published": "2024-06-01 20:12:54", "link": "http://arxiv.org/abs/2406.00548v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Guiding and Diversifying LLM-Based Story Generation via Answer Set\n  Programming", "abstract": "Instruction-tuned large language models (LLMs) are capable of generating\nstories in response to open-ended user requests, but the resulting stories tend\nto be limited in their diversity. Older, symbolic approaches to story\ngeneration (such as planning) can generate substantially more diverse plot\noutlines, but are limited to producing stories that recombine a fixed set of\nhand-engineered character action templates. Can we combine the strengths of\nthese approaches while mitigating their weaknesses? We propose to do so by\nusing a higher-level and more abstract symbolic specification of high-level\nstory structure -- implemented via answer set programming (ASP) -- to guide and\ndiversify LLM-based story generation. Via semantic similarity analysis, we\ndemonstrate that our approach produces more diverse stories than an unguided\nLLM, and via code excerpts, we demonstrate the improved compactness and\nflexibility of ASP-based outline generation over full-fledged narrative\nplanning.", "published": "2024-06-01 21:14:25", "link": "http://arxiv.org/abs/2406.00554v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Phased Instruction Fine-Tuning for Large Language Models", "abstract": "Instruction Fine-Tuning enhances pre-trained language models from basic\nnext-word prediction to complex instruction-following. However, existing\nOne-off Instruction Fine-Tuning (One-off IFT) method, applied on a diverse\ninstruction, may not effectively boost models' adherence to instructions due to\nthe simultaneous handling of varying instruction complexities. To improve this,\nPhased Instruction Fine-Tuning (Phased IFT) is proposed, based on the idea that\nlearning to follow instructions is a gradual process. It assesses instruction\ndifficulty using GPT-4, divides the instruction data into subsets of increasing\ndifficulty, and uptrains the model sequentially on these subsets. Experiments\nwith Llama-2 7B/13B/70B, Llama3 8/70B and Mistral-7B models using Alpaca data\nshow that Phased IFT significantly outperforms One-off IFT, supporting the\nprogressive alignment hypothesis and providing a simple and efficient way to\nenhance large language models. Codes and datasets from our experiments are\nfreely available at https://github.com/xubuvd/PhasedSFT.", "published": "2024-06-01 04:25:26", "link": "http://arxiv.org/abs/2406.04371v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Presentation Slide Generation by LLMs with a Multi-Staged\n  End-to-End Approach", "abstract": "Generating presentation slides from a long document with multimodal elements\nsuch as text and images is an important task. This is time consuming and needs\ndomain expertise if done manually. Existing approaches for generating a rich\npresentation from a document are often semi-automatic or only put a flat\nsummary into the slides ignoring the importance of a good narrative. In this\npaper, we address this research gap by proposing a multi-staged end-to-end\nmodel which uses a combination of LLM and VLM. We have experimentally shown\nthat compared to applying LLMs directly with state-of-the-art prompting, our\nproposed multi-staged solution is better in terms of automated metrics and\nhuman evaluation.", "published": "2024-06-01 07:49:31", "link": "http://arxiv.org/abs/2406.06556v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Text Authenticity: A Novel Hybrid Approach for AI-Generated\n  Text Detection", "abstract": "The rapid advancement of Large Language Models (LLMs) has ushered in an era\nwhere AI-generated text is increasingly indistinguishable from human-generated\ncontent. Detecting AI-generated text has become imperative to combat\nmisinformation, ensure content authenticity, and safeguard against malicious\nuses of AI. In this paper, we propose a novel hybrid approach that combines\ntraditional TF-IDF techniques with advanced machine learning models, including\nBayesian classifiers, Stochastic Gradient Descent (SGD), Categorical Gradient\nBoosting (CatBoost), and 12 instances of Deberta-v3-large models. Our approach\naims to address the challenges associated with detecting AI-generated text by\nleveraging the strengths of both traditional feature extraction methods and\nstate-of-the-art deep learning models. Through extensive experiments on a\ncomprehensive dataset, we demonstrate the effectiveness of our proposed method\nin accurately distinguishing between human and AI-generated text. Our approach\nachieves superior performance compared to existing methods. This research\ncontributes to the advancement of AI-generated text detection techniques and\nlays the foundation for developing robust solutions to mitigate the challenges\nposed by AI-generated content.", "published": "2024-06-01 10:21:54", "link": "http://arxiv.org/abs/2406.06558v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Vulnerabilities and Protections in Large Language Models: A\n  Survey", "abstract": "As Large Language Models (LLMs) increasingly become key components in various\nAI applications, understanding their security vulnerabilities and the\neffectiveness of defense mechanisms is crucial. This survey examines the\nsecurity challenges of LLMs, focusing on two main areas: Prompt Hacking and\nAdversarial Attacks, each with specific types of threats. Under Prompt Hacking,\nwe explore Prompt Injection and Jailbreaking Attacks, discussing how they work,\ntheir potential impacts, and ways to mitigate them. Similarly, we analyze\nAdversarial Attacks, breaking them down into Data Poisoning Attacks and\nBackdoor Attacks. This structured examination helps us understand the\nrelationships between these vulnerabilities and the defense strategies that can\nbe implemented. The survey highlights these security challenges and discusses\nrobust defensive frameworks to protect LLMs against these threats. By detailing\nthese security issues, the survey contributes to the broader discussion on\ncreating resilient AI systems that can resist sophisticated attacks.", "published": "2024-06-01 00:11:09", "link": "http://arxiv.org/abs/2406.00240v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Towards Rationality in Language and Multimodal Agents: A Survey", "abstract": "This work discusses how to build more rational language and multimodal agents\nand what criteria define rationality in intelligent systems. Rationality is the\nquality of being guided by reason, characterized by decision-making that aligns\nwith evidence and logical principles. It plays a crucial role in reliable\nproblem-solving by ensuring well-grounded and consistent solutions. Despite\ntheir progress, large language models (LLMs) often fall short of rationality\ndue to their bounded knowledge space and inconsistent outputs. In response,\nrecent efforts have shifted toward developing multimodal and multi-agent\nsystems, as well as integrating modules like external tools, programming codes,\nsymbolic reasoners, utility function, and conformal risk controls rather than\nrelying solely on a single LLM for decision-making. This paper surveys\nstate-of-the-art advancements in language and multimodal agents, assesses their\nrole in enhancing rationality, and outlines open challenges and future research\ndirections. We maintain an open repository at\nhttps://github.com/bowen-upenn/Agent_Rationality.", "published": "2024-06-01 01:17:25", "link": "http://arxiv.org/abs/2406.00252v6", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MA"], "primary_category": "cs.AI"}
{"title": "CASE: Efficient Curricular Data Pre-training for Building Assistive\n  Psychology Expert Models", "abstract": "The limited availability of psychologists necessitates efficient\nidentification of individuals requiring urgent mental healthcare. This study\nexplores the use of Natural Language Processing (NLP) pipelines to analyze text\ndata from online mental health forums used for consultations. By analyzing\nforum posts, these pipelines can flag users who may require immediate\nprofessional attention. A crucial challenge in this domain is data privacy and\nscarcity. To address this, we propose utilizing readily available curricular\ntexts used in institutes specializing in mental health for pre-training the NLP\npipelines. This helps us mimic the training process of a psychologist. Our work\npresents CASE-BERT that flags potential mental health disorders based on forum\ntext. CASE-BERT demonstrates superior performance compared to existing methods,\nachieving an f1 score of 0.91 for Depression and 0.88 for Anxiety, two of the\nmost commonly reported mental health disorders. Our code and data are publicly\navailable.", "published": "2024-06-01 06:17:32", "link": "http://arxiv.org/abs/2406.00314v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KGLink: A column type annotation method that combines knowledge graph\n  and pre-trained language model", "abstract": "The semantic annotation of tabular data plays a crucial role in various\ndownstream tasks. Previous research has proposed knowledge graph (KG)-based and\ndeep learning-based methods, each with its inherent limitations. KG-based\nmethods encounter difficulties annotating columns when there is no match for\ncolumn cells in the KG. Moreover, KG-based methods can provide multiple\npredictions for one column, making it challenging to determine the semantic\ntype with the most suitable granularity for the dataset. This type granularity\nissue limits their scalability.\n  On the other hand, deep learning-based methods face challenges related to the\nvaluable context missing issue. This occurs when the information within the\ntable is insufficient for determining the correct column type.\n  This paper presents KGLink, a method that combines WikiData KG information\nwith a pre-trained deep learning language model for table column annotation,\neffectively addressing both type granularity and valuable context missing\nissues. Through comprehensive experiments on widely used tabular datasets\nencompassing numeric and string columns with varying type granularity, we\nshowcase the effectiveness and efficiency of KGLink. By leveraging the\nstrengths of KGLink, we successfully surmount challenges related to type\ngranularity and valuable context issues, establishing it as a robust solution\nfor the semantic annotation of tabular data.", "published": "2024-06-01 06:28:41", "link": "http://arxiv.org/abs/2406.00318v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "RoBERTa-BiLSTM: A Context-Aware Hybrid Model for Sentiment Analysis", "abstract": "Effectively analyzing the comments to uncover latent intentions holds immense\nvalue in making strategic decisions across various domains. However, several\nchallenges hinder the process of sentiment analysis including the lexical\ndiversity exhibited in comments, the presence of long dependencies within the\ntext, encountering unknown symbols and words, and dealing with imbalanced\ndatasets. Moreover, existing sentiment analysis tasks mostly leveraged\nsequential models to encode the long dependent texts and it requires longer\nexecution time as it processes the text sequentially. In contrast, the\nTransformer requires less execution time due to its parallel processing nature.\nIn this work, we introduce a novel hybrid deep learning model, RoBERTa-BiLSTM,\nwhich combines the Robustly Optimized BERT Pretraining Approach (RoBERTa) with\nBidirectional Long Short-Term Memory (BiLSTM) networks. RoBERTa is utilized to\ngenerate meaningful word embedding vectors, while BiLSTM effectively captures\nthe contextual semantics of long-dependent texts. The RoBERTa-BiLSTM hybrid\nmodel leverages the strengths of both sequential and Transformer models to\nenhance performance in sentiment analysis. We conducted experiments using\ndatasets from IMDb, Twitter US Airline, and Sentiment140 to evaluate the\nproposed model against existing state-of-the-art methods. Our experimental\nfindings demonstrate that the RoBERTa-BiLSTM model surpasses baseline models\n(e.g., BERT, RoBERTa-base, RoBERTa-GRU, and RoBERTa-LSTM), achieving accuracies\nof 80.74%, 92.36%, and 82.25% on the Twitter US Airline, IMDb, and Sentiment140\ndatasets, respectively. Additionally, the model achieves F1-scores of 80.73%,\n92.35%, and 82.25% on the same datasets, respectively.", "published": "2024-06-01 08:59:46", "link": "http://arxiv.org/abs/2406.00367v1", "categories": ["cs.CL", "cs.AI", "cs.CE"], "primary_category": "cs.CL"}
{"title": "Mix-of-Granularity: Optimize the Chunking Granularity for\n  Retrieval-Augmented Generation", "abstract": "Integrating information from various reference databases is a major challenge\nfor Retrieval-Augmented Generation (RAG) systems because each knowledge source\nadopts a unique data structure and follows different conventions. Retrieving\nfrom multiple knowledge sources with one fixed strategy usually leads to\nunder-exploitation of information. To mitigate this drawback, inspired by\nMix-of-Expert, we introduce Mix-of-Granularity (MoG), a method that dynamically\ndetermines the optimal granularity of a knowledge source based on input queries\nusing a router. The router is efficiently trained with a newly proposed loss\nfunction employing soft labels. We further extend MoG to MoG-Graph (MoGG),\nwhere reference documents are pre-processed as graphs, enabling the retrieval\nof distantly situated snippets. Experiments demonstrate that MoG and MoGG\neffectively predict optimal granularity levels, significantly enhancing the\nperformance of the RAG system in downstream tasks. The code of both MoG and\nMoGG are released in https://github.com/ZGChung/Mix-of-Granularity.", "published": "2024-06-01 14:45:03", "link": "http://arxiv.org/abs/2406.00456v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Recent Advances in End-to-End Simultaneous Speech Translation", "abstract": "Simultaneous speech translation (SimulST) is a demanding task that involves\ngenerating translations in real-time while continuously processing speech\ninput. This paper offers a comprehensive overview of the recent developments in\nSimulST research, focusing on four major challenges. Firstly, the complexities\nassociated with processing lengthy and continuous speech streams pose\nsignificant hurdles. Secondly, satisfying real-time requirements presents\ninherent difficulties due to the need for immediate translation output.\nThirdly, striking a balance between translation quality and latency constraints\nremains a critical challenge. Finally, the scarcity of annotated data adds\nanother layer of complexity to the task. Through our exploration of these\nchallenges and the proposed solutions, we aim to provide valuable insights into\nthe current landscape of SimulST research and suggest promising directions for\nfuture exploration.", "published": "2024-06-01 16:56:19", "link": "http://arxiv.org/abs/2406.00497v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Survey on Large Language Models for Code Generation", "abstract": "Large Language Models (LLMs) have garnered remarkable advancements across\ndiverse code-related tasks, known as Code LLMs, particularly in code generation\nthat generates source code with LLM from natural language descriptions. This\nburgeoning field has captured significant interest from both academic\nresearchers and industry professionals due to its practical significance in\nsoftware development, e.g., GitHub Copilot. Despite the active exploration of\nLLMs for a variety of code tasks, either from the perspective of natural\nlanguage processing (NLP) or software engineering (SE) or both, there is a\nnoticeable absence of a comprehensive and up-to-date literature review\ndedicated to LLM for code generation. In this survey, we aim to bridge this gap\nby providing a systematic literature review that serves as a valuable reference\nfor researchers investigating the cutting-edge progress in LLMs for code\ngeneration. We introduce a taxonomy to categorize and discuss the recent\ndevelopments in LLMs for code generation, covering aspects such as data\ncuration, latest advances, performance evaluation, ethical implications,\nenvironmental impact, and real-world applications. In addition, we present a\nhistorical overview of the evolution of LLMs for code generation and offer an\nempirical comparison using the HumanEval, MBPP, and BigCodeBench benchmarks\nacross various levels of difficulty and types of programming tasks to highlight\nthe progressive enhancements in LLM capabilities for code generation. We\nidentify critical challenges and promising opportunities regarding the gap\nbetween academia and practical development. Furthermore, we have established a\ndedicated resource GitHub page (https://github.com/juyongjiang/CodeLLMSurvey)\nto continuously document and disseminate the most recent advances in the field.", "published": "2024-06-01 17:48:15", "link": "http://arxiv.org/abs/2406.00515v2", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "On Overcoming Miscalibrated Conversational Priors in LLM-based Chatbots", "abstract": "We explore the use of Large Language Model (LLM-based) chatbots to power\nrecommender systems. We observe that the chatbots respond poorly when they\nencounter under-specified requests (e.g., they make incorrect assumptions,\nhedge with a long response, or refuse to answer). We conjecture that such\nmiscalibrated response tendencies (i.e., conversational priors) can be\nattributed to LLM fine-tuning using annotators -- single-turn annotations may\nnot capture multi-turn conversation utility, and the annotators' preferences\nmay not even be representative of users interacting with a recommender system.\n  We first analyze public LLM chat logs to conclude that query\nunder-specification is common. Next, we study synthetic recommendation problems\nwith configurable latent item utilities and frame them as Partially Observed\nDecision Processes (PODP). We find that pre-trained LLMs can be sub-optimal for\nPODPs and derive better policies that clarify under-specified queries when\nappropriate. Then, we re-calibrate LLMs by prompting them with learned control\nmessages to approximate the improved policy. Finally, we show empirically that\nour lightweight learning approach effectively uses logged conversation data to\nre-calibrate the response strategies of LLM-based chatbots for recommendation\ntasks.", "published": "2024-06-01 15:54:45", "link": "http://arxiv.org/abs/2406.01633v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Large Language Model Confidence Estimation via Black-Box Access", "abstract": "Estimating uncertainty or confidence in the responses of a model can be\nsignificant in evaluating trust not only in the responses, but also in the\nmodel as a whole. In this paper, we explore the problem of estimating\nconfidence for responses of large language models (LLMs) with simply black-box\nor query access to them. We propose a simple and extensible framework where, we\nengineer novel features and train a (interpretable) model (viz. logistic\nregression) on these features to estimate the confidence. We empirically\ndemonstrate that our simple framework is effective in estimating confidence of\nFlan-ul2, Llama-13b, Mistral-7b and GPT-4 on four benchmark Q\\&A tasks as well\nas of Pegasus-large and BART-large on two benchmark summarization tasks with it\nsurpassing baselines by even over $10\\%$ (on AUROC) in some cases.\nAdditionally, our interpretable approach provides insight into features that\nare predictive of confidence, leading to the interesting and useful discovery\nthat our confidence models built for one LLM generalize zero-shot across others\non a given dataset.", "published": "2024-06-01 02:08:44", "link": "http://arxiv.org/abs/2406.04370v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Evaluation Benchmark for Autoformalization in Lean4", "abstract": "Large Language Models (LLMs) hold the potential to revolutionize\nautoformalization. The introduction of Lean4, a mathematical programming\nlanguage, presents an unprecedented opportunity to rigorously assess the\nautoformalization capabilities of LLMs. This paper introduces a novel\nevaluation benchmark designed for Lean4, applying it to test the abilities of\nstate-of-the-art LLMs, including GPT-3.5, GPT-4, and Gemini Pro. Our\ncomprehensive analysis reveals that, despite recent advancements, these LLMs\nstill exhibit limitations in autoformalization, particularly in more complex\nareas of mathematics. These findings underscore the need for further\ndevelopment in LLMs to fully harness their potential in scientific research and\ndevelopment. This study not only benchmarks current LLM capabilities but also\nsets the stage for future enhancements in autoformalization.", "published": "2024-06-01 07:06:57", "link": "http://arxiv.org/abs/2406.06555v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.PL"], "primary_category": "cs.LG"}
{"title": "Unveiling Hidden Factors: Explainable AI for Feature Boosting in Speech\n  Emotion Recognition", "abstract": "Speech emotion recognition (SER) has gained significant attention due to its\nseveral application fields, such as mental health, education, and\nhuman-computer interaction. However, the accuracy of SER systems is hindered by\nhigh-dimensional feature sets that may contain irrelevant and redundant\ninformation. To overcome this challenge, this study proposes an iterative\nfeature boosting approach for SER that emphasizes feature relevance and\nexplainability to enhance machine learning model performance. Our approach\ninvolves meticulous feature selection and analysis to build efficient SER\nsystems. In addressing our main problem through model explainability, we employ\na feature evaluation loop with Shapley values to iteratively refine feature\nsets. This process strikes a balance between model performance and\ntransparency, which enables a comprehensive understanding of the model's\npredictions. The proposed approach offers several advantages, including the\nidentification and removal of irrelevant and redundant features, leading to a\nmore effective model. Additionally, it promotes explainability, facilitating\ncomprehension of the model's predictions and the identification of crucial\nfeatures for emotion determination. The effectiveness of the proposed method is\nvalidated on the SER benchmarks of the Toronto emotional speech set (TESS),\nBerlin Database of Emotional Speech (EMO-DB), Ryerson Audio-Visual Database of\nEmotional Speech and Song (RAVDESS), and Surrey Audio-Visual Expressed Emotion\n(SAVEE) datasets, outperforming state-of-the-art methods. To the best of our\nknowledge, this is the first work to incorporate model explainability into an\nSER framework. The source code of this paper is publicly available via this\nhttps://github.com/alaaNfissi/Unveiling-Hidden-Factors-Explainable-AI-for-Feature-Boosting-in-Speech-Emotion-Recognition.", "published": "2024-06-01 00:39:55", "link": "http://arxiv.org/abs/2406.01624v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD", "I.2.7; I.2.6; I.2.1; I.2.8"], "primary_category": "eess.AS"}
{"title": "AudioLCM: Text-to-Audio Generation with Latent Consistency Models", "abstract": "Recent advancements in Latent Diffusion Models (LDMs) have propelled them to\nthe forefront of various generative tasks. However, their iterative sampling\nprocess poses a significant computational burden, resulting in slow generation\nspeeds and limiting their application in text-to-audio generation deployment.\nIn this work, we introduce AudioLCM, a novel consistency-based model tailored\nfor efficient and high-quality text-to-audio generation. AudioLCM integrates\nConsistency Models into the generation process, facilitating rapid inference\nthrough a mapping from any point at any time step to the trajectory's initial\npoint. To overcome the convergence issue inherent in LDMs with reduced sample\niterations, we propose the Guided Latent Consistency Distillation with a\nmulti-step Ordinary Differential Equation (ODE) solver. This innovation\nshortens the time schedule from thousands to dozens of steps while maintaining\nsample quality, thereby achieving fast convergence and high-quality generation.\nFurthermore, to optimize the performance of transformer-based neural network\narchitectures, we integrate the advanced techniques pioneered by LLaMA into the\nfoundational framework of transformers. This architecture supports stable and\nefficient training, ensuring robust performance in text-to-audio synthesis.\nExperimental results on text-to-sound generation and text-to-music synthesis\ntasks demonstrate that AudioLCM needs only 2 iterations to synthesize\nhigh-fidelity audios, while it maintains sample quality competitive with\nstate-of-the-art models using hundreds of steps. AudioLCM enables a sampling\nspeed of 333x faster than real-time on a single NVIDIA 4090Ti GPU, making\ngenerative models practically applicable to text-to-audio generation\ndeployment. Our extensive preliminary analysis shows that each design in\nAudioLCM is effective.", "published": "2024-06-01 08:19:56", "link": "http://arxiv.org/abs/2406.00356v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Wav2Prompt: End-to-End Speech Prompt Generation and Tuning For LLM in\n  Zero and Few-shot Learning", "abstract": "Wav2Prompt is proposed which allows straightforward integration between\nspoken input and a text-based large language model (LLM). Wav2Prompt uses a\nsimple training process with only the same data used to train an automatic\nspeech recognition (ASR) model. After training, Wav2Prompt learns continuous\nrepresentations from speech and uses them as LLM prompts. To avoid task\nover-fitting issues found in prior work and preserve the emergent abilities of\nLLMs, Wav2Prompt takes LLM token embeddings as the training targets and\nutilises a continuous integrate-and-fire mechanism for explicit speech-text\nalignment. Therefore, a Wav2Prompt-LLM combination can be applied to zero-shot\nspoken language tasks such as speech translation (ST), speech understanding\n(SLU), speech question answering (SQA) and spoken-query-based QA (SQQA). It is\nshown that for these zero-shot tasks, Wav2Prompt performs similarly to an\nASR-LLM cascade and better than recent prior work. If relatively small amounts\nof task-specific paired data are available in few-shot scenarios, the\nWav2Prompt-LLM combination can be end-to-end (E2E) fine-tuned. The\nWav2Prompt-LLM combination then yields greatly improved results relative to an\nASR-LLM cascade for the above tasks. For instance, for English-French ST with\nthe BLOOMZ-7B1 LLM, a Wav2Prompt-LLM combination gave a 8.5 BLEU point increase\nover an ASR-LLM cascade.", "published": "2024-06-01 18:06:26", "link": "http://arxiv.org/abs/2406.00522v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Creative Text-to-Audio Generation via Synthesizer Programming", "abstract": "Neural audio synthesis methods now allow specifying ideas in natural\nlanguage. However, these methods produce results that cannot be easily tweaked,\nas they are based on large latent spaces and up to billions of uninterpretable\nparameters. We propose a text-to-audio generation method that leverages a\nvirtual modular sound synthesizer with only 78 parameters. Synthesizers have\nlong been used by skilled sound designers for media like music and film due to\ntheir flexibility and intuitive controls. Our method, CTAG, iteratively updates\na synthesizer's parameters to produce high-quality audio renderings of text\nprompts that can be easily inspected and tweaked. Sounds produced this way are\nalso more abstract, capturing essential conceptual features over fine-grained\nacoustic details, akin to how simple sketches can vividly convey visual\nconcepts. Our results show how CTAG produces sounds that are distinctive,\nperceived as artistic, and yet similarly identifiable to recent neural audio\nsynthesis models, positioning it as a valuable and complementary tool.", "published": "2024-06-01 04:08:31", "link": "http://arxiv.org/abs/2406.00294v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Frieren: Efficient Video-to-Audio Generation Network with Rectified Flow\n  Matching", "abstract": "Video-to-audio (V2A) generation aims to synthesize content-matching audio\nfrom silent video, and it remains challenging to build V2A models with high\ngeneration quality, efficiency, and visual-audio temporal synchrony. We propose\nFrieren, a V2A model based on rectified flow matching. Frieren regresses the\nconditional transport vector field from noise to spectrogram latent with\nstraight paths and conducts sampling by solving ODE, outperforming\nautoregressive and score-based models in terms of audio quality. By employing a\nnon-autoregressive vector field estimator based on a feed-forward transformer\nand channel-level cross-modal feature fusion with strong temporal alignment,\nour model generates audio that is highly synchronized with the input video.\nFurthermore, through reflow and one-step distillation with guided vector field,\nour model can generate decent audio in a few, or even only one sampling step.\nExperiments indicate that Frieren achieves state-of-the-art performance in both\ngeneration quality and temporal alignment on VGGSound, with alignment accuracy\nreaching 97.22%, and 6.2% improvement in inception score over the strong\ndiffusion-based baseline. Audio samples are available at\nhttp://frieren-v2a.github.io.", "published": "2024-06-01 06:40:22", "link": "http://arxiv.org/abs/2406.00320v4", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-Visual Talker Localization in Video for Spatial Sound Reproduction", "abstract": "Object-based audio production requires the positional metadata to be defined\nfor each point-source object, including the key elements in the foreground of\nthe sound scene. In many media production use cases, both cameras and\nmicrophones are employed to make recordings, and the human voice is often a key\nelement. In this research, we detect and locate the active speaker in the\nvideo, facilitating the automatic extraction of the positional metadata of the\ntalker relative to the camera's reference frame. With the integration of the\nvisual modality, this study expands upon our previous investigation focused\nsolely on audio-based active speaker detection and localization. Our\nexperiments compare conventional audio-visual approaches for active speaker\ndetection that leverage monaural audio, our previous audio-only method that\nleverages multichannel recordings from a microphone array, and a novel\naudio-visual approach integrating vision and multichannel audio. We found the\nrole of the two modalities to complement each other. Multichannel audio,\novercoming the problem of visual occlusions, provides a double-digit reduction\nin detection error compared to audio-visual methods with single-channel audio.\nThe combination of multichannel audio and vision further enhances spatial\naccuracy, leading to a four-percentage point increase in F1 score on the Tragic\nTalkers dataset. Future investigations will assess the robustness of the model\nin noisy and highly reverberant environments, as well as tackle the problem of\noff-screen speakers.", "published": "2024-06-01 16:47:07", "link": "http://arxiv.org/abs/2406.00495v1", "categories": ["eess.AS", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
