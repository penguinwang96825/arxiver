{"title": "Assessing the Benchmarking Capacity of Machine Reading Comprehension\n  Datasets", "abstract": "Existing analysis work in machine reading comprehension (MRC) is largely\nconcerned with evaluating the capabilities of systems. However, the\ncapabilities of datasets are not assessed for benchmarking language\nunderstanding precisely. We propose a semi-automated, ablation-based\nmethodology for this challenge; By checking whether questions can be solved\neven after removing features associated with a skill requisite for language\nunderstanding, we evaluate to what degree the questions do not require the\nskill. Experiments on 10 datasets (e.g., CoQA, SQuAD v2.0, and RACE) with a\nstrong baseline model show that, for example, the relative scores of a baseline\nmodel provided with content words only and with shuffled sentence words in the\ncontext are on average 89.2% and 78.5% of the original score, respectively.\nThese results suggest that most of the questions already answered correctly by\nthe model do not necessarily require grammatical and complex reasoning. For\nprecise benchmarking, MRC datasets will need to take extra care in their design\nto ensure that questions can correctly evaluate the intended skills.", "published": "2019-11-21 02:04:52", "link": "http://arxiv.org/abs/1911.09241v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to Ask Better Questions? A Large-Scale Multi-Domain Dataset for\n  Rewriting Ill-Formed Questions", "abstract": "We present a large-scale dataset for the task of rewriting an ill-formed\nnatural language question to a well-formed one. Our multi-domain question\nrewriting MQR dataset is constructed from human contributed Stack Exchange\nquestion edit histories. The dataset contains 427,719 question pairs which come\nfrom 303 domains. We provide human annotations for a subset of the dataset as a\nquality estimate. When moving from ill-formed to well-formed questions, the\nquestion quality improves by an average of 45 points across three aspects. We\ntrain sequence-to-sequence neural models on the constructed dataset and obtain\nan improvement of 13.2% in BLEU-4 over baseline methods built from other data\nresources. We release the MQR dataset to encourage research on the problem of\nquestion rewriting.", "published": "2019-11-21 02:24:21", "link": "http://arxiv.org/abs/1911.09247v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cantonese Automatic Speech Recognition Using Transfer Learning from\n  Mandarin", "abstract": "We propose a system to develop a basic automatic speech recognizer(ASR) for\nCantonese, a low-resource language, through transfer learning of Mandarin, a\nhigh-resource language. We take a time-delayed neural network trained on\nMandarin, and perform weight transfer of several layers to a newly initialized\nmodel for Cantonese. We experiment with the number of layers transferred, their\nlearning rates, and pretraining i-vectors. Key findings are that this approach\nallows for quicker training time with less data. We find that for every epoch,\nlog-probability is smaller for transfer learning models compared to a\nCantonese-only model. The transfer learning models show slight improvement in\nCER.", "published": "2019-11-21 03:48:46", "link": "http://arxiv.org/abs/1911.09271v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of Sections in Classifying Disease Outbreak Reports", "abstract": "Identifying articles that relate to infectious diseases is a necessary step\nfor any automatic bio-surveillance system that monitors news articles from the\nInternet. Unlike scientific articles which are available in a strongly\nstructured form, news articles are usually loosely structured. In this chapter,\nwe investigate the importance of each section and the effect of section\nweighting on performance of text classification. The experimental results show\nthat (1) classification models using the headline and leading sentence achieve\na high performance in terms of F-score compared to other parts of the article;\n(2) all section with bag-of-word representation (full text) achieves the\nhighest recall; and (3) section weighting information can help to improve\naccuracy.", "published": "2019-11-21 07:22:03", "link": "http://arxiv.org/abs/1911.09319v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Diverse Translation by Manipulating Multi-Head Attention", "abstract": "Transformer model has been widely used on machine translation tasks and\nobtained state-of-the-art results. In this paper, we report an interesting\nphenomenon in its encoder-decoder multi-head attention: different attention\nheads of the final decoder layer align to different word translation\ncandidates. We empirically verify this discovery and propose a method to\ngenerate diverse translations by manipulating heads. Furthermore, we make use\nof these diverse translations with the back-translation technique for better\ndata augmentation. Experiment results show that our method generates diverse\ntranslations without severe drop in translation quality. Experiments also show\nthat back-translation with these diverse translations could bring significant\nimprovement on performance on translation tasks. An auxiliary experiment of\nconversation response generation task proves the effect of diversity as well.", "published": "2019-11-21 08:22:07", "link": "http://arxiv.org/abs/1911.09333v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotion Recognition for Vietnamese Social Media Text", "abstract": "Emotion recognition or emotion prediction is a higher approach or a special\ncase of sentiment analysis. In this task, the result is not produced in terms\nof either polarity: positive or negative or in the form of rating (from 1 to 5)\nbut of a more detailed level of analysis in which the results are depicted in\nmore expressions like sadness, enjoyment, anger, disgust, fear, and surprise.\nEmotion recognition plays a critical role in measuring the brand value of a\nproduct by recognizing specific emotions of customers' comments. In this study,\nwe have achieved two targets. First and foremost, we built a standard\nVietnamese Social Media Emotion Corpus (UIT-VSMEC) with exactly 6,927\nemotion-annotated sentences, contributing to emotion recognition research in\nVietnamese which is a low-resource language in natural language processing\n(NLP). Secondly, we assessed and measured machine learning and deep neural\nnetwork models on our UIT-VSMEC corpus. As a result, the CNN model achieved the\nhighest performance with the weighted F1-score of 59.74%. Our corpus is\navailable at our research website.", "published": "2019-11-21 08:39:21", "link": "http://arxiv.org/abs/1911.09339v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chemical-protein Interaction Extraction via Gaussian Probability\n  Distribution and External Biomedical Knowledge", "abstract": "Motivation: The biomedical literature contains a wealth of chemical-protein\ninteractions (CPIs). Automatically extracting CPIs described in biomedical\nliterature is essential for drug discovery, precision medicine, as well as\nbasic biomedical research. Most existing methods focus only on the sentence\nsequence to identify these CPIs. However, the local structure of sentences and\nexternal biomedical knowledge also contain valuable information. Effective use\nof such information may improve the performance of CPI extraction. Results: In\nthis paper, we propose a novel neural network-based approach to improve CPI\nextraction. Specifically, the approach first employs BERT to generate\nhigh-quality contextual representations of the title sequence, instance\nsequence, and knowledge sequence. Then, the Gaussian probability distribution\nis introduced to capture the local structure of the instance. Meanwhile, the\nattention mechanism is applied to fuse the title information and biomedical\nknowledge, respectively. Finally, the related representations are concatenated\nand fed into the softmax function to extract CPIs. We evaluate our proposed\nmodel on the CHEMPROT corpus. Our proposed model is superior in performance as\ncompared with other state-of-the-art models. The experimental results show that\nthe Gaussian probability distribution and external knowledge are complementary\nto each other. Integrating them can effectively improve the CPI extraction\nperformance. Furthermore, the Gaussian probability distribution can effectively\nimprove the extraction performance of sentences with overlapping relations in\nbiomedical relation extraction tasks. Availability: Data and code are available\nat https://github.com/CongSun-dlut/CPI_extraction. Contact: yangzh@dlut.edu.cn,\nwangleibihami@gmail.com Supplementary information: Supplementary data are\navailable at Bioinformatics online.", "published": "2019-11-21 14:29:42", "link": "http://arxiv.org/abs/1911.09487v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An analysis of observation length requirements for machine understanding\n  of human behaviors from spoken language", "abstract": "The task of quantifying human behavior by observing interaction cues is an\nimportant and useful one across a range of domains in psychological research\nand practice. Machine learning-based approaches typically perform this task by\nfirst estimating behavior based on cues within an observation window, such as a\nfixed number of words, and then aggregating the behavior over all the windows\nin that interaction. The length of this window directly impacts the accuracy of\nestimation by controlling the amount of information being used. The exact link\nbetween window length and accuracy, however, has not been well studied,\nespecially in spoken language. In this paper, we investigate this link and\npresent an analysis framework that determines appropriate window lengths for\nthe task of behavior estimation. Our proposed framework utilizes a two-pronged\nevaluation approach: (a) extrinsic similarity between machine predictions and\nhuman expert annotations, and (b) intrinsic consistency between intra-machine\nand intra-human behavior relations. We apply our analysis to real-life\nconversations that are annotated for a large and diverse set of behavior codes\nand examine the relation between the nature of a behavior and how long it\nshould be observed. We find that behaviors describing negative and positive\naffect can be accurately estimated from short to medium-length expressions\nwhereas behaviors related to problem-solving and dysphoria require much longer\nobservations and are difficult to quantify from language alone. These findings\nare found to be generally consistent across different behavior modeling\napproaches.", "published": "2019-11-21 15:05:10", "link": "http://arxiv.org/abs/1911.09515v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Cluster Ranking Model for Full Anaphora Resolution", "abstract": "Anaphora resolution (coreference) systems designed for the CONLL 2012 dataset\ntypically cannot handle key aspects of the full anaphora resolution task such\nas the identification of singletons and of certain types of non-referring\nexpressions (e.g., expletives), as these aspects are not annotated in that\ncorpus. However, the recently released dataset for the CRAC 2018 Shared Task\ncan now be used for that purpose. In this paper, we introduce an architecture\nto simultaneously identify non-referring expressions (including expletives,\npredicative s, and other types) and build coreference chains, including\nsingletons. Our cluster-ranking system uses an attention mechanism to determine\nthe relative importance of the mentions in the same cluster. Additional\nclassifiers are used to identify singletons and non-referring markables. Our\ncontributions are as follows. First all, we report the first result on the CRAC\ndata using system mentions; our result is 5.8% better than the shared task\nbaseline system, which used gold mentions. Second, we demonstrate that the\navailability of singleton clusters and non-referring expressions can lead to\nsubstantially improved performance on non-singleton clusters as well. Third, we\nshow that despite our model not being designed specifically for the CONLL data,\nit achieves a score equivalent to that of the state-of-the-art system by Kantor\nand Globerson (2019) on that dataset.", "published": "2019-11-21 15:14:39", "link": "http://arxiv.org/abs/1911.09532v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention-Informed Mixed-Language Training for Zero-shot Cross-lingual\n  Task-oriented Dialogue Systems", "abstract": "Recently, data-driven task-oriented dialogue systems have achieved promising\nperformance in English. However, developing dialogue systems that support\nlow-resource languages remains a long-standing challenge due to the absence of\nhigh-quality data. In order to circumvent the expensive and time-consuming data\ncollection, we introduce Attention-Informed Mixed-Language Training (MLT), a\nnovel zero-shot adaptation method for cross-lingual task-oriented dialogue\nsystems. It leverages very few task-related parallel word pairs to generate\ncode-switching sentences for learning the inter-lingual semantics across\nlanguages. Instead of manually selecting the word pairs, we propose to extract\nsource words based on the scores computed by the attention layer of a trained\nEnglish task-related model and then generate word pairs using existing\nbilingual dictionaries. Furthermore, intensive experiments with different\ncross-lingual embeddings demonstrate the effectiveness of our approach.\nFinally, with very few word pairs, our model achieves significant zero-shot\nadaptation performance improvements in both cross-lingual dialogue state\ntracking and natural language understanding (i.e., intent detection and slot\nfilling) tasks compared to the current state-of-the-art approaches, which\nutilize a much larger amount of bilingual data.", "published": "2019-11-21 03:52:50", "link": "http://arxiv.org/abs/1911.09273v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic Text-based Personality Recognition on Monologues and\n  Multiparty Dialogues Using Attentive Networks and Contextual Embeddings", "abstract": "Previous works related to automatic personality recognition focus on using\ntraditional classification models with linguistic features. However, attentive\nneural networks with contextual embeddings, which have achieved huge success in\ntext classification, are rarely explored for this task. In this project, we\nhave two major contributions. First, we create the first dialogue-based\npersonality dataset, FriendsPersona, by annotating 5 personality traits of\nspeakers from Friends TV Show through crowdsourcing. Second, we present a novel\napproach to automatic personality recognition using pre-trained contextual\nembeddings (BERT and RoBERTa) and attentive neural networks. Our models largely\nimprove the state-of-art results on the monologue Essays dataset by 2.49%, and\nestablish a solid benchmark on our FriendsPersona. By comparing results in two\ndatasets, we demonstrate the challenges of modeling personality in multi-party\ndialogue.", "published": "2019-11-21 06:14:05", "link": "http://arxiv.org/abs/1911.09304v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Minimizing the Bag-of-Ngrams Difference for Non-Autoregressive Neural\n  Machine Translation", "abstract": "Non-Autoregressive Neural Machine Translation (NAT) achieves significant\ndecoding speedup through generating target words independently and\nsimultaneously. However, in the context of non-autoregressive translation, the\nword-level cross-entropy loss cannot model the target-side sequential\ndependency properly, leading to its weak correlation with the translation\nquality. As a result, NAT tends to generate influent translations with\nover-translation and under-translation errors. In this paper, we propose to\ntrain NAT to minimize the Bag-of-Ngrams (BoN) difference between the model\noutput and the reference sentence. The bag-of-ngrams training objective is\ndifferentiable and can be efficiently calculated, which encourages NAT to\ncapture the target-side sequential dependency and correlates well with the\ntranslation quality. We validate our approach on three translation tasks and\nshow that our approach largely outperforms the NAT baseline by about 5.0 BLEU\nscores on WMT14 En$\\leftrightarrow$De and about 2.5 BLEU scores on WMT16\nEn$\\leftrightarrow$Ro.", "published": "2019-11-21 07:26:05", "link": "http://arxiv.org/abs/1911.09320v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Incorporating Textual Evidence in Visual Storytelling", "abstract": "Previous work on visual storytelling mainly focused on exploring image\nsequence as evidence for storytelling and neglected textual evidence for\nguiding story generation. Motivated by human storytelling process which recalls\nstories for familiar images, we exploit textual evidence from similar images to\nhelp generate coherent and meaningful stories. To pick the images which may\nprovide textual experience, we propose a two-step ranking method based on image\nobject recognition techniques. To utilize textual information, we design an\nextended Seq2Seq model with two-channel encoder and attention. Experiments on\nthe VIST dataset show that our method outperforms state-of-the-art baseline\nmodels without heavy engineering.", "published": "2019-11-21 08:22:37", "link": "http://arxiv.org/abs/1911.09334v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What Do You Mean `Why?': Resolving Sluices in Conversations", "abstract": "In conversation, we often ask one-word questions such as `Why?' or `Who?'.\nSuch questions are typically easy for humans to answer, but can be hard for\ncomputers, because their resolution requires retrieving both the right semantic\nframes and the right arguments from context. This paper introduces the novel\nellipsis resolution task of resolving such one-word questions, referred to as\nsluices in linguistics. We present a crowd-sourced dataset containing\nannotations of sluices from over 4,000 dialogues collected from conversational\nQA datasets, as well as a series of strong baseline architectures.", "published": "2019-11-21 14:09:33", "link": "http://arxiv.org/abs/1911.09478v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Paraphrasing with Large Language Models", "abstract": "Recently, large language models such as GPT-2 have shown themselves to be\nextremely adept at text generation and have also been able to achieve\nhigh-quality results in many downstream NLP tasks such as text classification,\nsentiment analysis and question answering with the aid of fine-tuning. We\npresent a useful technique for using a large language model to perform the task\nof paraphrasing on a variety of texts and subjects. Our approach is\ndemonstrated to be capable of generating paraphrases not only at a sentence\nlevel but also for longer spans of text such as paragraphs without needing to\nbreak the text into smaller chunks.", "published": "2019-11-21 18:45:54", "link": "http://arxiv.org/abs/1911.09661v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatically Neutralizing Subjective Bias in Text", "abstract": "Texts like news, encyclopedias, and some social media strive for objectivity.\nYet bias in the form of inappropriate subjectivity - introducing attitudes via\nframing, presupposing truth, and casting doubt - remains ubiquitous. This kind\nof bias erodes our collective trust and fuels social conflict. To address this\nissue, we introduce a novel testbed for natural language generation:\nautomatically bringing inappropriately subjective text into a neutral point of\nview (\"neutralizing\" biased text). We also offer the first parallel corpus of\nbiased language. The corpus contains 180,000 sentence pairs and originates from\nWikipedia edits that removed various framings, presuppositions, and attitudes\nfrom biased sentences. Last, we propose two strong encoder-decoder baselines\nfor the task. A straightforward yet opaque CONCURRENT system uses a BERT\nencoder to identify subjective words as part of the generation process. An\ninterpretable and controllable MODULAR algorithm separates these steps, using\n(1) a BERT-based classifier to identify problematic words and (2) a novel join\nembedding through which the classifier can edit the hidden states of the\nencoder. Large-scale human evaluation across four domains (encyclopedias, news\nheadlines, books, and political speeches) suggests that these algorithms are a\nfirst step towards the automatic identification and reduction of bias.", "published": "2019-11-21 19:15:03", "link": "http://arxiv.org/abs/1911.09709v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Conditioning in Context-Aware Sequence to Sequence Models", "abstract": "Neural sequence to sequence models are well established for applications\nwhich can be cast as mapping a single input sequence into a single output\nsequence. In this work, we focus on cases where generation is conditioned on\nboth a short query and a long context, such as abstractive question answering\nor document-level translation. We modify the standard sequence-to-sequence\napproach to make better use of both the query and the context by expanding the\nconditioning mechanism to intertwine query and context attention. We also\nintroduce a simple and efficient data augmentation method for the proposed\nmodel. Experiments on three different tasks show that both changes lead to\nconsistent improvements.", "published": "2019-11-21 20:01:46", "link": "http://arxiv.org/abs/1911.09728v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reinforcing an Image Caption Generator Using Off-Line Human Feedback", "abstract": "Human ratings are currently the most accurate way to assess the quality of an\nimage captioning model, yet most often the only used outcome of an expensive\nhuman rating evaluation is a few overall statistics over the evaluation\ndataset. In this paper, we show that the signal from instance-level human\ncaption ratings can be leveraged to improve captioning models, even when the\namount of caption ratings is several orders of magnitude less than the caption\ntraining data. We employ a policy gradient method to maximize the human ratings\nas rewards in an off-policy reinforcement learning setting, where policy\ngradients are estimated by samples from a distribution that focuses on the\ncaptions in a caption ratings dataset. Our empirical evidence indicates that\nthe proposed method learns to generalize the human raters' judgments to a\npreviously unseen set of images, as judged by a different set of human judges,\nand additionally on a different, multi-dimensional side-by-side human\nevaluation procedure.", "published": "2019-11-21 21:26:28", "link": "http://arxiv.org/abs/1911.09753v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "How Do You #relax When You're #stressed? A Content Analysis and\n  Infodemiology Study of Stress-Related Tweets", "abstract": "Background: Stress is a contributing factor to many major health problems in\nthe United States, such as heart disease, depression, and autoimmune diseases.\nRelaxation is often recommended in mental health treatment as a frontline\nstrategy to reduce stress, thereby improving health conditions.\n  Objective: The objective of our study was to understand how people express\ntheir feelings of stress and relaxation through Twitter messages.\n  Methods: We first performed a qualitative content analysis of 1326 and 781\ntweets containing the keywords \"stress\" and \"relax\", respectively. We then\ninvestigated the use of machine learning algorithms to automatically classify\ntweets as stress versus non stress and relaxation versus non relaxation.\nFinally, we applied these classifiers to sample datasets drawn from 4 cities\nwith the goal of evaluating the extent of any correlation between our automatic\nclassification of tweets and results from public stress surveys.\n  Results: Content analysis showed that the most frequent topic of stress\ntweets was education, followed by work and social relationships. The most\nfrequent topic of relaxation tweets was rest and vacation, followed by nature\nand water. When we applied the classifiers to the cities dataset, the\nproportion of stress tweets in New York and San Diego was substantially higher\nthan that in Los Angeles and San Francisco.\n  Conclusions: This content analysis and infodemiology study revealed that\nTwitter, when used in conjunction with natural language processing techniques,\nis a useful data source for understanding stress and stress management\nstrategies, and can potentially supplement infrequently collected survey-based\nstress data.", "published": "2019-11-21 02:08:14", "link": "http://arxiv.org/abs/1911.09242v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Empirical Autopsy of Deep Video Captioning Frameworks", "abstract": "Contemporary deep learning based video captioning follows encoder-decoder\nframework. In encoder, visual features are extracted with 2D/3D Convolutional\nNeural Networks (CNNs) and a transformed version of those features is passed to\nthe decoder. The decoder uses word embeddings and a language model to map\nvisual features to natural language captions. Due to its composite nature, the\nencoder-decoder pipeline provides the freedom of multiple choices for each of\nits components, e.g the choices of CNNs models, feature transformations, word\nembeddings, and language models etc. Component selection can have drastic\neffects on the overall video captioning performance. However, current\nliterature is void of any systematic investigation in this regard. This article\nfills this gap by providing the first thorough empirical analysis of the role\nthat each major component plays in a contemporary video captioning pipeline. We\nperform extensive experiments by varying the constituent components of the\nvideo captioning framework, and quantify the performance gains that are\npossible by mere component selection. We use the popular MSVD dataset as the\ntest-bed, and demonstrate that substantial performance gains are possible by\ncareful selection of the constituent components without major changes to the\npipeline itself. These results are expected to provide guiding principles for\nfuture research in the fast growing direction of video captioning.", "published": "2019-11-21 08:47:36", "link": "http://arxiv.org/abs/1911.09345v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Entity Extraction with Knowledge from Web Scale Corpora", "abstract": "Entity extraction is an important task in text mining and natural language\nprocessing. A popular method for entity extraction is by comparing substrings\nfrom free text against a dictionary of entities. In this paper, we present\nseveral techniques as a post-processing step for improving the effectiveness of\nthe existing entity extraction technique. These techniques utilise models\ntrained with the web-scale corpora which makes our techniques robust and\nversatile. Experiments show that our techniques bring a notable improvement on\nefficiency and effectiveness.", "published": "2019-11-21 10:01:16", "link": "http://arxiv.org/abs/1911.09373v1", "categories": ["cs.CL", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ChartNet: Visual Reasoning over Statistical Charts using MAC-Networks", "abstract": "Despite the improvements in perception accuracies brought about via deep\nlearning, developing systems combining accurate visual perception with the\nability to reason over the visual percepts remains extremely challenging. A\nparticular application area of interest from an accessibility perspective is\nthat of reasoning over statistical charts such as bar and pie charts. To this\nend, we formulate the problem of reasoning over statistical charts as a\nclassification task using MAC-Networks to give answers from a predefined\nvocabulary of generic answers. Additionally, we enhance the capabilities of\nMAC-Networks to give chart-specific answers to open-ended questions by\nreplacing the classification layer by a regression layer to localize the\ntextual answers present over the images. We call our network ChartNet, and\ndemonstrate its efficacy on predicting both in vocabulary and out of vocabulary\nanswers. To test our methods, we generated our own dataset of statistical chart\nimages and corresponding question answer pairs. Results show that ChartNet\nconsistently outperform other state-of-the-art methods on reasoning over these\nquestions and may be a viable candidate for applications containing images of\nstatistical charts.", "published": "2019-11-21 10:03:25", "link": "http://arxiv.org/abs/1911.09375v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction", "abstract": "Knowledge graph embedding, which aims to represent entities and relations as\nlow dimensional vectors (or matrices, tensors, etc.), has been shown to be a\npowerful technique for predicting missing links in knowledge graphs. Existing\nknowledge graph embedding models mainly focus on modeling relation patterns\nsuch as symmetry/antisymmetry, inversion, and composition. However, many\nexisting approaches fail to model semantic hierarchies, which are common in\nreal-world applications. To address this challenge, we propose a novel\nknowledge graph embedding model -- namely, Hierarchy-Aware Knowledge Graph\nEmbedding (HAKE) -- which maps entities into the polar coordinate system. HAKE\nis inspired by the fact that concentric circles in the polar coordinate system\ncan naturally reflect the hierarchy. Specifically, the radial coordinate aims\nto model entities at different levels of the hierarchy, and entities with\nsmaller radii are expected to be at higher levels; the angular coordinate aims\nto distinguish entities at the same level of the hierarchy, and these entities\nare expected to have roughly the same radii but different angles. Experiments\ndemonstrate that HAKE can effectively model the semantic hierarchies in\nknowledge graphs, and significantly outperforms existing state-of-the-art\nmethods on benchmark datasets for the link prediction task.", "published": "2019-11-21 11:37:18", "link": "http://arxiv.org/abs/1911.09419v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Automatically Generating Macro Research Reports from a Piece of News", "abstract": "Automatically generating macro research reports from economic news is an\nimportant yet challenging task. As we all know, it requires the macro analysts\nto write such reports within a short period of time after the important\neconomic news are released. This motivates our work, i.e., using AI techniques\nto save manual cost. The goal of the proposed system is to generate macro\nresearch reports as the draft for macro analysts. Essentially, the core\nchallenge is the long text generation issue. To address this issue, we propose\na novel deep learning technique based approach which includes two components,\ni.e., outline generation and macro research report generation.For the model\nperformance evaluation, we first crawl a large news-to-report dataset and then\nevaluate our approach on this dataset, and the generated reports are given for\nthe subjective evaluation.", "published": "2019-11-21 16:05:02", "link": "http://arxiv.org/abs/1911.09572v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Hierarchical Discrete Linguistic Units from Visually-Grounded\n  Speech", "abstract": "In this paper, we present a method for learning discrete linguistic units by\nincorporating vector quantization layers into neural models of visually\ngrounded speech. We show that our method is capable of capturing both\nword-level and sub-word units, depending on how it is configured. What\ndifferentiates this paper from prior work on speech unit learning is the choice\nof training objective. Rather than using a reconstruction-based loss, we use a\ndiscriminative, multimodal grounding objective which forces the learned units\nto be useful for semantic image retrieval. We evaluate the sub-word units on\nthe ZeroSpeech 2019 challenge, achieving a 27.3\\% reduction in ABX error rate\nover the top-performing submission, while keeping the bitrate approximately the\nsame. We also present experiments demonstrating the noise robustness of these\nunits. Finally, we show that a model with multiple quantizers can\nsimultaneously learn phone-like detectors at a lower layer and word-like\ndetectors at a higher layer. We show that these detectors are highly accurate,\ndiscovering 279 words with an F1 score of greater than 0.5.", "published": "2019-11-21 16:54:14", "link": "http://arxiv.org/abs/1911.09602v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Prosody Transfer in Neural Text to Speech Using Global Pitch and\n  Loudness Features", "abstract": "This paper presents a simple yet effective method to achieve prosody transfer\nfrom a reference speech signal to synthesized speech. The main idea is to\nincorporate well-known acoustic correlates of prosody such as pitch and\nloudness contours of the reference speech into a modern neural text-to-speech\n(TTS) synthesizer such as Tacotron2 (TC2). More specifically, a small set of\nacoustic features are extracted from reference audio and then used to condition\na TC2 synthesizer. The trained model is evaluated using subjective listening\ntests and a novel objective evaluation of prosody transfer is proposed.\nListening tests show that the synthesized speech is rated as highly natural and\nthat prosody is successfully transferred from the reference speech signal to\nthe synthesized signal.", "published": "2019-11-21 18:04:47", "link": "http://arxiv.org/abs/1911.09645v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Temporal Reasoning via Audio Question Answering", "abstract": "Multimodal question answering tasks can be used as proxy tasks to study\nsystems that can perceive and reason about the world. Answering questions about\ndifferent types of input modalities stresses different aspects of reasoning\nsuch as visual reasoning, reading comprehension, story understanding, or\nnavigation. In this paper, we use the task of Audio Question Answering (AQA) to\nstudy the temporal reasoning abilities of machine learning models. To this end,\nwe introduce the Diagnostic Audio Question Answering (DAQA) dataset comprising\naudio sequences of natural sound events and programmatically generated\nquestions and answers that probe various aspects of temporal reasoning. We\nadapt several recent state-of-the-art methods for visual question answering to\nthe AQA task, and use DAQA to demonstrate that they perform poorly on questions\nthat require in-depth temporal reasoning. Finally, we propose a new model,\nMultiple Auxiliary Controllers for Linear Modulation (MALiMo) that extends the\nrecent Feature-wise Linear Modulation (FiLM) model and significantly improves\nits temporal reasoning capabilities. We envisage DAQA to foster research on AQA\nand temporal reasoning and MALiMo a step towards models for AQA.", "published": "2019-11-21 18:26:30", "link": "http://arxiv.org/abs/1911.09655v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Separate and Attend in Personal Email Search", "abstract": "In personal email search, user queries often impose different requirements on\ndifferent aspects of the retrieved emails. For example, the query \"my recent\nflight to the US\" requires emails to be ranked based on both textual contents\nand recency of the email documents, while other queries such as \"medical\nhistory\" do not impose any constraints on the recency of the email. Recent deep\nlearning-to-rank models for personal email search often directly concatenate\ndense numerical features (e.g., document age) with embedded sparse features\n(e.g., n-gram embeddings). In this paper, we first show with a set of\nexperiments on synthetic datasets that direct concatenation of dense and sparse\nfeatures does not lead to the optimal search performance of deep neural ranking\nmodels. To effectively incorporate both sparse and dense email features into\npersonal email search ranking, we propose a novel neural model, SepAttn.\nSepAttn first builds two separate neural models to learn from sparse and dense\nfeatures respectively, and then applies an attention mechanism at the\nprediction level to derive the final prediction from these two models. We\nconduct a comprehensive set of experiments on a large-scale email search\ndataset, and demonstrate that our SepAttn model consistently improves the\nsearch quality over the baseline models.", "published": "2019-11-21 20:19:28", "link": "http://arxiv.org/abs/1911.09732v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Global Health Monitor: A Web-based System for Detecting and Mapping\n  Infectious Diseases", "abstract": "We present the Global Health Monitor, an online Web-based system for\ndetecting and mapping infectious disease outbreaks that appear in news stories.\nThe system analyzes English news stories from news feed providers, classifies\nthem for topical relevance and plots them onto a Google map using geo-coding\ninformation, helping public health workers to monitor the spread of diseases in\na geo-temporal context. The background knowledge for the system is contained in\nthe BioCaster ontology (BCO) (Collier et al., 2007a) which includes both\ninformation on infectious diseases as well as geographical locations with their\nlatitudes/longitudes. The system consists of four main stages: topic\nclassification, named entity recognition (NER), disease/location detection and\nvisualization. Evaluation of the system shows that it achieved high accuracy on\na gold standard corpus. The system is now in practical use. Running on a\nclustercomputer, it monitors more than 1500 news feeds 24/7, updating the map\nevery hour.", "published": "2019-11-21 20:26:29", "link": "http://arxiv.org/abs/1911.09735v1", "categories": ["cs.IR", "cs.CL", "cs.CY"], "primary_category": "cs.IR"}
{"title": "Speech Sentiment Analysis via Pre-trained Features from End-to-end ASR\n  Models", "abstract": "In this paper, we propose to use pre-trained features from end-to-end ASR\nmodels to solve speech sentiment analysis as a down-stream task. We show that\nend-to-end ASR features, which integrate both acoustic and text information\nfrom speech, achieve promising results. We use RNN with self-attention as the\nsentiment classifier, which also provides an easy visualization through\nattention weights to help interpret model predictions. We use well benchmarked\nIEMOCAP dataset and a new large-scale speech sentiment dataset SWBD-sentiment\nfor evaluation. Our approach improves the-state-of-the-art accuracy on IEMOCAP\nfrom 66.6% to 71.7%, and achieves an accuracy of 70.10% on SWBD-sentiment with\nmore than 49,500 utterances.", "published": "2019-11-21 21:38:36", "link": "http://arxiv.org/abs/1911.09762v2", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LATTE: Latent Type Modeling for Biomedical Entity Linking", "abstract": "Entity linking is the task of linking mentions of named entities in natural\nlanguage text, to entities in a curated knowledge-base. This is of significant\nimportance in the biomedical domain, where it could be used to semantically\nannotate a large volume of clinical records and biomedical literature, to\nstandardized concepts described in an ontology such as Unified Medical Language\nSystem (UMLS). We observe that with precise type information, entity\ndisambiguation becomes a straightforward task. However, fine-grained type\ninformation is usually not available in biomedical domain. Thus, we propose\nLATTE, a LATent Type Entity Linking model, that improves entity linking by\nmodeling the latent fine-grained type information about mentions and entities.\nUnlike previous methods that perform entity linking directly between the\nmentions and the entities, LATTE jointly does entity disambiguation, and latent\nfine-grained type learning, without direct supervision. We evaluate our model\non two biomedical datasets: MedMentions, a large scale public dataset annotated\nwith UMLS concepts, and a de-identified corpus of dictated doctor's notes that\nhas been annotated with ICD concepts. Extensive experimental evaluation shows\nour model achieves significant performance improvements over several\nstate-of-the-art techniques.", "published": "2019-11-21 23:55:15", "link": "http://arxiv.org/abs/1911.09787v2", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Designing Virtual Soundscapes for Alzheimer's Disease Care", "abstract": "Sound environment is a prime source of conscious and unconscious information\nwhich allows listeners to place themselves, to communicate, to feel, to\nremember. The author describes the process of designing a new audio interactive\napparatus for Alzheimer's care, in the context of an active multidisciplinary\nresearch project led by the author in collaboration with a longterm care centre\n(EHPAD) in Burgundy (France), a geriatrician, a gerontologist, psychologists\nand caregivers. The apparatus, named Madeleines Sonores in reference to\nProust's madeleine, have provided virtual soundscapes sounding for a year for\n14 elderly people hosted in the dedicated Alzheimer's unit of the care centre,\n24/7. Empiric aspects of sonic interactivity are discussed in relation to\ndementia and to the activity of caring. Scientific studies are initiated to\nevaluate the benefits of such a disposal in Alzheimer's disease therapy and in\ncaring dementia.", "published": "2019-11-21 13:23:12", "link": "http://arxiv.org/abs/1911.09459v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An End-to-End Audio Classification System based on Raw Waveforms and\n  Mix-Training Strategy", "abstract": "Audio classification can distinguish different kinds of sounds, which is\nhelpful for intelligent applications in daily life. However, it remains a\nchallenging task since the sound events in an audio clip is probably multiple,\neven overlapping. This paper introduces an end-to-end audio classification\nsystem based on raw waveforms and mix-training strategy. Compared to\nhuman-designed features which have been widely used in existing research, raw\nwaveforms contain more complete information and are more appropriate for\nmulti-label classification. Taking raw waveforms as input, our network consists\nof two variants of ResNet structure which can learn a discriminative\nrepresentation. To explore the information in intermediate layers, a\nmulti-level prediction with attention structure is applied in our model.\nFurthermore, we design a mix-training strategy to break the performance\nlimitation caused by the amount of training data. Experiments show that the\nmean average precision of the proposed audio classification system on Audio Set\ndataset is 37.2%. Without using extra training data, our system exceeds the\nstate-of-the-art multi-level attention model.", "published": "2019-11-21 08:54:48", "link": "http://arxiv.org/abs/1911.09349v1", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.MM"], "primary_category": "eess.AS"}
{"title": "WildMix Dataset and Spectro-Temporal Transformer Model for Monoaural\n  Audio Source Separation", "abstract": "Monoaural audio source separation is a challenging research area in machine\nlearning. In this area, a mixture containing multiple audio sources is given,\nand a model is expected to disentangle the mixture into isolated atomic\nsources. In this paper, we first introduce a challenging new dataset for\nmonoaural source separation called WildMix. WildMix is designed with the goal\nof extending the boundaries of source separation beyond what previous datasets\nin this area would allow. It contains diverse in-the-wild recordings from 25\ndifferent sound classes, combined with each other using arbitrary composition\npolicies. Source separation often requires modeling long-range dependencies in\nboth temporal and spectral domains. To this end, we introduce a novel\ntrasnformer-based model called Spectro-Temporal Transformer (STT). STT utilizes\na specialized encoder, called Spectro-Temporal Encoder (STE). STE highlights\ntemporal and spectral components of sources within a mixture, using a\nself-attention mechanism. It subsequently disentangles them in a hierarchical\nmanner. In our experiments, STT swiftly outperforms various previous baselines\nfor monoaural source separation on the challenging WildMix dataset.", "published": "2019-11-21 23:23:02", "link": "http://arxiv.org/abs/1911.09783v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
