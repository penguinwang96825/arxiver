{"title": "Reasoning about Procedures with Natural Language Processing: A Tutorial", "abstract": "This tutorial provides a comprehensive and in-depth view of the research on\nprocedures, primarily in Natural Language Processing. A procedure is a sequence\nof steps intended to achieve some goal. Understanding procedures in natural\nlanguage has a long history, with recent breakthroughs made possible by\nadvances in technology. First, we discuss established approaches to collect\nprocedures, by human annotation or extraction from web resources. Then, we\nexamine different angles from which procedures can be reasoned about, as well\nas ways to represent them. Finally, we enumerate scenarios where procedural\nknowledge can be applied to the real world.", "published": "2022-05-16 05:42:00", "link": "http://arxiv.org/abs/2205.07455v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Directed Acyclic Transformer for Non-Autoregressive Machine Translation", "abstract": "Non-autoregressive Transformers (NATs) significantly reduce the decoding\nlatency by generating all tokens in parallel. However, such independent\npredictions prevent NATs from capturing the dependencies between the tokens for\ngenerating multiple possible translations. In this paper, we propose Directed\nAcyclic Transfomer (DA-Transformer), which represents the hidden states in a\nDirected Acyclic Graph (DAG), where each path of the DAG corresponds to a\nspecific translation. The whole DAG simultaneously captures multiple\ntranslations and facilitates fast predictions in a non-autoregressive fashion.\nExperiments on the raw training data of WMT benchmark show that DA-Transformer\nsubstantially outperforms previous NATs by about 3 BLEU on average, which is\nthe first NAT model that achieves competitive results with autoregressive\nTransformers without relying on knowledge distillation.", "published": "2022-05-16 06:02:29", "link": "http://arxiv.org/abs/2205.07459v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompting to Distill: Boosting Data-Free Knowledge Distillation via\n  Reinforced Prompt", "abstract": "Data-free knowledge distillation (DFKD) conducts knowledge distillation via\neliminating the dependence of original training data, and has recently achieved\nimpressive results in accelerating pre-trained language models. At the heart of\nDFKD is to reconstruct a synthetic dataset by inverting the parameters of the\nuncompressed model. Prior DFKD approaches, however, have largely relied on\nhand-crafted priors of the target data distribution for the reconstruction,\nwhich can be inevitably biased and often incompetent to capture the intrinsic\ndistributions. To address this problem, we propose a prompt-based method,\ntermed as PromptDFD, that allows us to take advantage of learned language\npriors, which effectively harmonizes the synthetic sentences to be semantically\nand grammatically correct. Specifically, PromptDFD leverages a pre-trained\ngenerative model to provide language priors and introduces a reinforced topic\nprompter to control data synthesis, making the generated samples thematically\nrelevant and semantically plausible, and thus friendly to downstream tasks. As\nshown in our experiments, the proposed method substantially improves the\nsynthesis quality and achieves considerable improvements on distillation\nperformance. In some cases, PromptDFD even gives rise to results on par with\nthose from the data-driven knowledge distillation with access to the original\ntraining data.", "published": "2022-05-16 08:56:53", "link": "http://arxiv.org/abs/2205.07523v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantitative Discourse Cohesion Analysis of Scientific Scholarly Texts\n  using Multilayer Networks", "abstract": "Discourse cohesion facilitates text comprehension and helps the reader form a\ncoherent narrative. In this study, we aim to computationally analyze the\ndiscourse cohesion in scientific scholarly texts using multilayer network\nrepresentation and quantify the writing quality of the document. Exploiting the\nhierarchical structure of scientific scholarly texts, we design section-level\nand document-level metrics to assess the extent of lexical cohesion in text. We\nuse a publicly available dataset along with a curated set of contrasting\nexamples to validate the proposed metrics by comparing them against select\nindices computed using existing cohesion analysis tools. We observe that the\nproposed metrics correlate as expected with the existing cohesion indices.\n  We also present an analytical framework, CHIAA (CHeck It Again, Author), to\nprovide pointers to the author for potential improvements in the manuscript\nwith the help of the section-level and document-level metrics. The proposed\nCHIAA framework furnishes a clear and precise prescription to the author for\nimproving writing by localizing regions in text with cohesion gaps. We\ndemonstrate the efficacy of CHIAA framework using succinct examples from\ncohesion-deficient text excerpts in the experimental dataset.", "published": "2022-05-16 09:10:41", "link": "http://arxiv.org/abs/2205.07532v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Heroes, Villains, and Victims, and GPT-3: Automated Extraction of\n  Character Roles Without Training Data", "abstract": "This paper shows how to use large-scale pre-trained language models to\nextract character roles from narrative texts without training data. Queried\nwith a zero-shot question-answering prompt, GPT-3 can identify the hero,\nvillain, and victim in diverse domains: newspaper articles, movie plot\nsummaries, and political speeches.", "published": "2022-05-16 10:08:11", "link": "http://arxiv.org/abs/2205.07557v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Precis of Language Models are not Models of Language", "abstract": "Natural Language Processing is one of the leading application areas in the\ncurrent resurgence of Artificial Intelligence, spearheaded by Artificial Neural\nNetworks. We show that despite their many successes at performing linguistic\ntasks, Large Neural Language Models are ill-suited as comprehensive models of\nnatural language. The wider implication is that, in spite of the often\noverbearing optimism about AI, modern neural models do not represent a\nrevolution in our understanding of cognition.", "published": "2022-05-16 12:50:58", "link": "http://arxiv.org/abs/2205.07634v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CQR-SQL: Conversational Question Reformulation Enhanced\n  Context-Dependent Text-to-SQL Parsers", "abstract": "Context-dependent text-to-SQL is the task of translating multi-turn questions\ninto database-related SQL queries. Existing methods typically focus on making\nfull use of history context or previously predicted SQL for currently SQL\nparsing, while neglecting to explicitly comprehend the schema and\nconversational dependency, such as co-reference, ellipsis and user focus\nchange. In this paper, we propose CQR-SQL, which uses auxiliary Conversational\nQuestion Reformulation (CQR) learning to explicitly exploit schema and decouple\ncontextual dependency for SQL parsing. Specifically, we first present a schema\nenhanced recursive CQR method to produce domain-relevant self-contained\nquestions. Secondly, we train CQR-SQL models to map the semantics of multi-turn\nquestions and auxiliary self-contained questions into the same latent space\nthrough schema grounding consistency task and tree-structured SQL parsing\nconsistency task, which enhances the abilities of SQL parsing by adequately\ncontextual understanding. At the time of writing, our CQR-SQL achieves new\nstate-of-the-art results on two context-dependent text-to-SQL benchmarks SParC\nand CoSQL.", "published": "2022-05-16 13:52:42", "link": "http://arxiv.org/abs/2205.07686v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Persian Abstract Meaning Representation", "abstract": "Abstract Meaning Representation (AMR) is an annotation framework representing\nthe semantic structure of a sentence as a whole. From the beginning, AMR was\nnot intended to act as an interlingua; however, it has made progress towards\nthe idea of designing a universal meaning representation framework.\nAccordingly, developing AMR annotation guidelines for different languages,\nbased on language divergences, is of significant importance. In this paper, we\nelaborate on Persian Abstract Meaning Representation (PAMR) annotation\nspecifications, based on which we annotated the Persian translation of \"The\nLittle Prince\" as the first gold standard for Persian AMR. Moreover, we\ndescribe how some Persian-specific syntactic constructions would result in\ndifferent AMR annotations.", "published": "2022-05-16 14:25:43", "link": "http://arxiv.org/abs/2205.07712v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Referring Expressions with Rational Speech Act Framework: A\n  Probabilistic Approach", "abstract": "This paper focuses on a referring expression generation (REG) task in which\nthe aim is to pick out an object in a complex visual scene. One common\ntheoretical approach to this problem is to model the task as a two-agent\ncooperative scheme in which a `speaker' agent would generate the expression\nthat best describes a targeted area and a `listener' agent would identify the\ntarget. Several recent REG systems have used deep learning approaches to\nrepresent the speaker/listener agents. The Rational Speech Act framework (RSA),\na Bayesian approach to pragmatics that can predict human linguistic behavior\nquite accurately, has been shown to generate high quality and explainable\nexpressions on toy datasets involving simple visual scenes. Its application to\nlarge scale problems, however, remains largely unexplored. This paper applies a\ncombination of the probabilistic RSA framework and deep learning approaches to\nlarger datasets involving complex visual scenes in a multi-step process with\nthe aim of generating better-explained expressions. We carry out experiments on\nthe RefCOCO and RefCOCO+ datasets and compare our approach with other\nend-to-end deep learning approaches as well as a variation of RSA to highlight\nour key contribution. Experimental results show that while achieving lower\naccuracy than SOTA deep learning methods, our approach outperforms similar RSA\napproach in human comprehension and has an advantage over end-to-end deep\nlearning under limited data scenario. Lastly, we provide a detailed analysis on\nthe expression generation process with concrete examples, thus providing a\nsystematic view on error types and deficiencies in the generation process and\nidentifying possible areas for future improvements.", "published": "2022-05-16 16:37:50", "link": "http://arxiv.org/abs/2205.07795v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Meta AI at Arabic Hate Speech 2022: MultiTask Learning with\n  Self-Correction for Hate Speech Classification", "abstract": "In this paper, we tackle the Arabic Fine-Grained Hate Speech Detection shared\ntask and demonstrate significant improvements over reported baselines for its\nthree subtasks. The tasks are to predict if a tweet contains (1) Offensive\nlanguage; and whether it is considered (2) Hate Speech or not and if so, then\npredict the (3) Fine-Grained Hate Speech label from one of six categories. Our\nfinal solution is an ensemble of models that employs multitask learning and a\nself-consistency correction method yielding 82.7% on the hate speech subtask --\nreflecting a 3.4% relative improvement compared to previous work.", "published": "2022-05-16 19:53:16", "link": "http://arxiv.org/abs/2205.07960v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Debiasing Translation Artifacts", "abstract": "Cross-lingual natural language processing relies on translation, either by\nhumans or machines, at different levels, from translating training data to\ntranslating test sets. However, compared to original texts in the same\nlanguage, translations possess distinct qualities referred to as\ntranslationese. Previous research has shown that these translation artifacts\ninfluence the performance of a variety of cross-lingual tasks. In this work, we\npropose a novel approach to reducing translationese by extending an established\nbias-removal technique. We use the Iterative Null-space Projection (INLP)\nalgorithm, and show by measuring classification accuracy before and after\ndebiasing, that translationese is reduced at both sentence and word level. We\nevaluate the utility of debiasing translationese on a natural language\ninference (NLI) task, and show that by reducing this bias, NLI accuracy\nimproves. To the best of our knowledge, this is the first study to debias\ntranslationese as represented in latent embedding space.", "published": "2022-05-16 21:46:51", "link": "http://arxiv.org/abs/2205.08001v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Harnessing Multilingual Resources to Question Answering in Arabic", "abstract": "The goal of the paper is to predict answers to questions given a passage of\nQur'an. The answers are always found in the passage, so the task of the model\nis to predict where an answer starts and where it ends. As the initial data set\nis rather small for training, we make use of multilingual BERT so that we can\naugment the training data by using data available for languages other than\nArabic. Furthermore, we crawl a large Arabic corpus that is domain specific to\nreligious discourse. Our approach consists of two steps, first we train a BERT\nmodel to predict a set of possible answers in a passage. Finally, we use\nanother BERT based model to rank the candidate answers produced by the first\nBERT model.", "published": "2022-05-16 23:28:01", "link": "http://arxiv.org/abs/2205.08024v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What GPT Knows About Who is Who", "abstract": "Coreference resolution -- which is a crucial task for understanding discourse\nand language at large -- has yet to witness widespread benefits from large\nlanguage models (LLMs). Moreover, coreference resolution systems largely rely\non supervised labels, which are highly expensive and difficult to annotate,\nthus making it ripe for prompt engineering. In this paper, we introduce a\nQA-based prompt-engineering method and discern \\textit{generative}, pre-trained\nLLMs' abilities and limitations toward the task of coreference resolution. Our\nexperiments show that GPT-2 and GPT-Neo can return valid answers, but that\ntheir capabilities to identify coreferent mentions are limited and\nprompt-sensitive, leading to inconsistent results.", "published": "2022-05-16 00:59:37", "link": "http://arxiv.org/abs/2205.07407v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The AI Teacher Test: Measuring the Pedagogical Ability of Blender and\n  GPT-3 in Educational Dialogues", "abstract": "How can we test whether state-of-the-art generative models, such as Blender\nand GPT-3, are good AI teachers, capable of replying to a student in an\neducational dialogue? Designing an AI teacher test is challenging: although\nevaluation methods are much-needed, there is no off-the-shelf solution to\nmeasuring pedagogical ability. This paper reports on a first attempt at an AI\nteacher test. We built a solution around the insight that you can run\nconversational agents in parallel to human teachers in real-world dialogues,\nsimulate how different agents would respond to a student, and compare these\ncounterpart responses in terms of three abilities: speak like a teacher,\nunderstand a student, help a student. Our method builds on the reliability of\ncomparative judgments in education and uses a probabilistic model and Bayesian\nsampling to infer estimates of pedagogical ability. We find that, even though\nconversational agents (Blender in particular) perform well on conversational\nuptake, they are quantifiably worse than real teachers on several pedagogical\ndimensions, especially with regard to helpfulness (Blender: {\\Delta} ability =\n-0.75; GPT-3: {\\Delta} ability = -0.93).", "published": "2022-05-16 09:36:30", "link": "http://arxiv.org/abs/2205.07540v1", "categories": ["cs.CL", "cs.AI", "I.2.7; K.3"], "primary_category": "cs.CL"}
{"title": "Assessing the Limits of the Distributional Hypothesis in Semantic\n  Spaces: Trait-based Relational Knowledge and the Impact of Co-occurrences", "abstract": "The increase in performance in NLP due to the prevalence of distributional\nmodels and deep learning has brought with it a reciprocal decrease in\ninterpretability. This has spurred a focus on what neural networks learn about\nnatural language with less of a focus on how. Some work has focused on the data\nused to develop data-driven models, but typically this line of work aims to\nhighlight issues with the data, e.g. highlighting and offsetting harmful\nbiases. This work contributes to the relatively untrodden path of what is\nrequired in data for models to capture meaningful representations of natural\nlanguage. This entails evaluating how well English and Spanish semantic spaces\ncapture a particular type of relational knowledge, namely the traits associated\nwith concepts (e.g. bananas-yellow), and exploring the role of co-occurrences\nin this context.", "published": "2022-05-16 12:09:40", "link": "http://arxiv.org/abs/2205.07603v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Strong Equivalence of TAG and CCG", "abstract": "Tree-adjoining grammar (TAG) and combinatory categorial grammar (CCG) are two\nwell-established mildly context-sensitive grammar formalisms that are known to\nhave the same expressive power on strings (i.e., generate the same class of\nstring languages). It is demonstrated that their expressive power on trees also\nessentially coincides. In fact, CCG without lexicon entries for the empty\nstring and only first-order rules of degree at most 2 are sufficient for its\nfull expressive power.", "published": "2022-05-16 15:12:03", "link": "http://arxiv.org/abs/2205.07743v1", "categories": ["cs.FL", "cs.CL", "68Q45 (Primary) 68Q42, 68T50, 03B65, 91F20 (Secondary)", "F.4.3; F.4.2; F.1.1"], "primary_category": "cs.FL"}
{"title": "Natural Language Specifications in Proof Assistants", "abstract": "Interactive proof assistants are computer programs carefully constructed to\ncheck a human-designed proof of a mathematical claim with high confidence in\nthe implementation. However, this only validates truth of a formal claim, which\nmay have been mistranslated from a claim made in natural language. This is\nespecially problematic when using proof assistants to formally verify the\ncorrectness of software with respect to a natural language specification. The\ntranslation from informal to formal remains a challenging, time-consuming\nprocess that is difficult to audit for correctness. This paper argues that it\nis possible to build support for natural language specifications within\nexisting proof assistants, in a way that complements the principles used to\nestablish trust and auditability in proof assistants themselves.", "published": "2022-05-16 17:05:45", "link": "http://arxiv.org/abs/2205.07811v1", "categories": ["cs.PL", "cs.CL"], "primary_category": "cs.PL"}
{"title": "Miutsu: NTU's TaskBot for the Alexa Prize", "abstract": "This paper introduces Miutsu, National Taiwan University's Alexa Prize\nTaskBot, which is designed to assist users in completing tasks requiring\nmultiple steps and decisions in two different domains -- home improvement and\ncooking. We overview our system design and architectural goals, and detail the\nproposed core elements, including question answering, task retrieval, social\nchatting, and various conversational modules. A dialogue flow is proposed to\nprovide a robust and engaging conversation when handling complex tasks. We\ndiscuss the faced challenges during the competition and potential future work.", "published": "2022-05-16 04:56:55", "link": "http://arxiv.org/abs/2205.07446v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Taming Continuous Posteriors for Latent Variational Dialogue Policies", "abstract": "Utilizing amortized variational inference for latent-action reinforcement\nlearning (RL) has been shown to be an effective approach in Task-oriented\nDialogue (ToD) systems for optimizing dialogue success. Until now, categorical\nposteriors have been argued to be one of the main drivers of performance. In\nthis work we revisit Gaussian variational posteriors for latent-action RL and\nshow that they can yield even better performance than categoricals. We achieve\nthis by simplifying the training procedure and propose ways to regularize the\nlatent dialogue policy to retain good response coherence. Using continuous\nlatent representations our model achieves state of the art dialogue success\nrate on the MultiWOZ benchmark, and also compares well to categorical latent\nmethods in response coherence.", "published": "2022-05-16 12:50:32", "link": "http://arxiv.org/abs/2205.07633v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Fast Attention Network for Joint Intent Detection and Slot Filling on\n  Edge Devices", "abstract": "Intent detection and slot filling are two main tasks in natural language\nunderstanding and play an essential role in task-oriented dialogue systems. The\njoint learning of both tasks can improve inference accuracy and is popular in\nrecent works. However, most joint models ignore the inference latency and\ncannot meet the need to deploy dialogue systems at the edge. In this paper, we\npropose a Fast Attention Network (FAN) for joint intent detection and slot\nfilling tasks, guaranteeing both accuracy and latency. Specifically, we\nintroduce a clean and parameter-refined attention module to enhance the\ninformation exchange between intent and slot, improving semantic accuracy by\nmore than 2%. FAN can be implemented on different encoders and delivers more\naccurate models at every speed level. Our experiments on the Jetson Nano\nplatform show that FAN inferences fifteen utterances per second with a small\naccuracy drop, showing its effectiveness and efficiency on edge devices.", "published": "2022-05-16 13:06:51", "link": "http://arxiv.org/abs/2205.07646v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "What company do words keep? Revisiting the distributional semantics of\n  J.R. Firth & Zellig Harris", "abstract": "The power of word embeddings is attributed to the linguistic theory that\nsimilar words will appear in similar contexts. This idea is specifically\ninvoked by noting that \"you shall know a word by the company it keeps,\" a quote\nfrom British linguist J.R. Firth who, along with his American colleague Zellig\nHarris, is often credited with the invention of \"distributional semantics.\"\nWhile both Firth and Harris are cited in all major NLP textbooks and many\nfoundational papers, the content and differences between their theories is\nseldom discussed. Engaging in a close reading of their work, we discover two\ndistinct and in many ways divergent theories of meaning. One focuses\nexclusively on the internal workings of linguistic forms, while the other\ninvites us to consider words in new company - not just with other linguistic\nelements, but also in a broader cultural and situational context. Contrasting\nthese theories from the perspective of current debates in NLP, we discover in\nFirth a figure who could guide the field towards a more culturally grounded\nnotion of semantics. We consider how an expanded notion of \"context\" might be\nmodeled in practice through two different strategies: comparative\nstratification and syntagmatic extension", "published": "2022-05-16 15:24:30", "link": "http://arxiv.org/abs/2205.07750v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "FactPEGASUS: Factuality-Aware Pre-training and Fine-tuning for\n  Abstractive Summarization", "abstract": "We present FactPEGASUS, an abstractive summarization model that addresses the\nproblem of factuality during pre-training and fine-tuning: (1) We augment the\nsentence selection strategy of PEGASUS's (Zhang et al., 2020) pre-training\nobjective to create pseudo-summaries that are both important and factual; (2)\nWe introduce three complementary components for fine-tuning. The corrector\nremoves hallucinations present in the reference summary, the contrastor uses\ncontrastive learning to better differentiate nonfactual summaries from factual\nones, and the connector bridges the gap between the pre-training and\nfine-tuning for better transfer of knowledge. Experiments on three downstream\ntasks demonstrate that FactPEGASUS substantially improves factuality evaluated\nby multiple automatic metrics and humans. Our thorough analysis suggests that\nFactPEGASUS is more factual than using the original pre-training objective in\nzero-shot and few-shot settings, retains factual behavior more robustly than\nstrong baselines, and does not rely entirely on becoming more extractive to\nimprove factuality. Our code and data are publicly available at:\nhttps://github.com/meetdavidwan/factpegasus", "published": "2022-05-16 17:39:14", "link": "http://arxiv.org/abs/2205.07830v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Diversity of Argument-Making in the Wild: from Assumptions and\n  Definitions to Causation and Anecdote in Reddit's \"Change My View\"", "abstract": "What kinds of arguments do people make, and what effect do they have on\nothers? Normative constraints on argument-making are as old as philosophy\nitself, but little is known about the diversity of arguments made in practice.\nWe use NLP tools to extract patterns of argument-making from the Reddit site\n\"Change My View\" (r/CMV). This reveals six distinct argument patterns: not just\nthe familiar deductive and inductive forms, but also arguments about\ndefinitions, relevance, possibility and cause, and personal experience. Data\nfrom r/CMV also reveal differences in efficacy: personal experience and, to a\nlesser extent, arguments about causation and examples, are most likely to shift\na person's view, while arguments about relevance are the least. Finally, our\nmethods reveal a gradient of argument-making preferences among users: a\ntwo-axis model, of \"personal--impersonal\" and \"concrete--abstract\", can account\nfor nearly 80% of the strategy variance between individuals.", "published": "2022-05-16 18:39:21", "link": "http://arxiv.org/abs/2205.07938v1", "categories": ["cs.CL", "cs.SI", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Budge: a programming language and a theorem prover", "abstract": "We present a simple programming language based on G\\\"odel numbering and prime\nfactorization, enhanced with explicit, scoped loops, allowing for easy program\ncomposition. Further, we will present a theorem prover that allows expressing\nand working with formal systems. The theorem prover is simple as it relies\nmerely on a substitution rule and set equality to derive theorems. Finally, we\nwill represent the programming language in the theorem prover. We will show the\nsyntax and semantics of both, and then provide a few example programs and their\nevaluation.", "published": "2022-05-16 20:35:25", "link": "http://arxiv.org/abs/2205.07979v6", "categories": ["cs.PL", "cs.CL", "cs.LO"], "primary_category": "cs.PL"}
{"title": "CascadER: Cross-Modal Cascading for Knowledge Graph Link Prediction", "abstract": "Knowledge graph (KG) link prediction is a fundamental task in artificial\nintelligence, with applications in natural language processing, information\nretrieval, and biomedicine. Recently, promising results have been achieved by\nleveraging cross-modal information in KGs, using ensembles that combine\nknowledge graph embeddings (KGEs) and contextual language models (LMs).\nHowever, existing ensembles are either (1) not consistently effective in terms\nof ranking accuracy gains or (2) impractically inefficient on larger datasets\ndue to the combinatorial explosion problem of pairwise ranking with deep\nlanguage models. In this paper, we propose a novel tiered ranking architecture\nCascadER to maintain the ranking accuracy of full ensembling while improving\nefficiency considerably. CascadER uses LMs to rerank the outputs of more\nefficient base KGEs, relying on an adaptive subset selection scheme aimed at\ninvoking the LMs minimally while maximizing accuracy gain over the KGE.\nExtensive experiments demonstrate that CascadER improves MRR by up to 9 points\nover KGE baselines, setting new state-of-the-art performance on four benchmarks\nwhile improving efficiency by one or more orders of magnitude over competitive\ncross-modal baselines. Our empirical analyses reveal that diversity of models\nacross modalities and preservation of individual models' confidence signals\nhelp explain the effectiveness of CascadER, and suggest promising directions\nfor cross-modal cascaded architectures. Code and pretrained models are\navailable at https://github.com/tsafavi/cascader.", "published": "2022-05-16 22:55:45", "link": "http://arxiv.org/abs/2205.08012v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PRISM: Pre-trained Indeterminate Speaker Representation Model for\n  Speaker Diarization and Speaker Verification", "abstract": "Speaker embedding has been a fundamental feature for speaker-related tasks\nsuch as verification, clustering, and diarization. Traditionally, speaker\nembeddings are represented as fixed vectors in high-dimensional space. This\ncould lead to biased estimations, especially when handling shorter utterances.\nIn this paper we propose to represent a speaker utterance as \"floating\" vector\nwhose state is indeterminate without knowing the context. The state of a\nspeaker representation is jointly determined by itself, other speech from the\nsame speaker, as well as other speakers it is being compared to. The content of\nthe speech also contributes to determining the final state of a speaker\nrepresentation. We pre-train an indeterminate speaker representation model that\nestimates the state of an utterance based on the context. The pre-trained model\ncan be fine-tuned for downstream tasks such as speaker verification, speaker\nclustering, and speaker diarization. Substantial improvements are observed\nacross all downstream tasks.", "published": "2022-05-16 05:21:11", "link": "http://arxiv.org/abs/2205.07450v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Accented Speech Recognition: Benchmarking, Pre-training, and Diverse\n  Data", "abstract": "Building inclusive speech recognition systems is a crucial step towards\ndeveloping technologies that speakers of all language varieties can use.\nTherefore, ASR systems must work for everybody independently of the way they\nspeak. To accomplish this goal, there should be available data sets\nrepresenting language varieties, and also an understanding of model\nconfiguration that is the most helpful in achieving robust understanding of all\ntypes of speech. However, there are not enough data sets for accented speech,\nand for the ones that are already available, more training approaches need to\nbe explored to improve the quality of accented speech recognition. In this\npaper, we discuss recent progress towards developing more inclusive ASR\nsystems, namely, the importance of building new data sets representing\nlinguistic diversity, and exploring novel training approaches to improve\nperformance for all users. We address recent directions within benchmarking ASR\nsystems for accented speech, measure the effects of wav2vec 2.0 pre-training on\naccented speech recognition, and highlight corpora relevant for diverse ASR\nevaluations.", "published": "2022-05-16 23:01:17", "link": "http://arxiv.org/abs/2205.08014v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "L3-Net Deep Audio Embeddings to Improve COVID-19 Detection from\n  Smartphone Data", "abstract": "Smartphones and wearable devices, along with Artificial Intelligence, can\nrepresent a game-changer in the pandemic control, by implementing low-cost and\npervasive solutions to recognize the development of new diseases at their early\nstages and by potentially avoiding the rise of new outbreaks. Some recent works\nshow promise in detecting diagnostic signals of COVID-19 from voice and coughs\nby using machine learning and hand-crafted acoustic features. In this paper, we\ndecided to investigate the capabilities of the recently proposed deep embedding\nmodel L3-Net to automatically extract meaningful features from raw respiratory\naudio recordings in order to improve the performances of standard machine\nlearning classifiers in discriminating between COVID-19 positive and negative\nsubjects from smartphone data. We evaluated the proposed model on 3 datasets,\ncomparing the obtained results with those of two reference works. Results show\nthat the combination of L3-Net with hand-crafted features overcomes the\nperformance of the other works of 28.57% in terms of AUC in a set of\nsubject-independent experiments. This result paves the way to further\ninvestigation on different deep audio embeddings, also for the automatic\ndetection of different diseases.", "published": "2022-05-16 13:50:22", "link": "http://arxiv.org/abs/2205.07682v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Transferability of Adversarial Attacks on Synthetic Speech Detection", "abstract": "Synthetic speech detection is one of the most important research problems in\naudio security. Meanwhile, deep neural networks are vulnerable to adversarial\nattacks. Therefore, we establish a comprehensive benchmark to evaluate the\ntransferability of adversarial attacks on the synthetic speech detection task.\nSpecifically, we attempt to investigate: 1) The transferability of adversarial\nattacks between different features. 2) The influence of varying extraction\nhyperparameters of features on the transferability of adversarial attacks. 3)\nThe effect of clipping or self-padding operation on the transferability of\nadversarial attacks. By performing these analyses, we summarise the weaknesses\nof synthetic speech detectors and the transferability behaviours of adversarial\nattacks, which provide insights for future research. More details can be found\nat https://gitee.com/djc_QRICK/Attack-Transferability-On-Synthetic-Detection.", "published": "2022-05-16 14:24:56", "link": "http://arxiv.org/abs/2205.07711v1", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Perceptual Evaluation on Audio-visual Dataset of 360 Content", "abstract": "To open up new possibilities to assess the multimodal perceptual quality of\nomnidirectional media formats, we proposed a novel open source 360 audiovisual\n(AV) quality dataset. The dataset consists of high-quality 360 video clips in\nequirectangular (ERP) format and higher-order ambisonic (4th order) along with\nthe subjective scores. Three subjective quality experiments were conducted for\naudio, video, and AV with the procedures detailed in this paper. Using the data\nfrom subjective tests, we demonstrated that this dataset can be used to\nquantify perceived audio, video, and audiovisual quality. The diversity and\ndiscriminability of subjective scores were also analyzed. Finally, we\ninvestigated how our dataset correlates with various objective quality metrics\nof audio and video. Evidence from the results of this study implies that the\nproposed dataset can benefit future studies on multimodal quality evaluation of\n360 content.", "published": "2022-05-16 22:31:29", "link": "http://arxiv.org/abs/2205.08007v1", "categories": ["cs.MM", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.MM"}
