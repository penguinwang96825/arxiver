{"title": "Bipartite-play Dialogue Collection for Practical Automatic Evaluation of\n  Dialogue Systems", "abstract": "Automation of dialogue system evaluation is a driving force for the efficient\ndevelopment of dialogue systems. This paper introduces the bipartite-play\nmethod, a dialogue collection method for automating dialogue system evaluation.\nIt addresses the limitations of existing dialogue collection methods: (i)\ninability to compare with systems that are not publicly available, and (ii)\nvulnerability to cheating by intentionally selecting systems to be compared.\nExperimental results show that the automatic evaluation using the\nbipartite-play method mitigates these two drawbacks and correlates as strongly\nwith human subjectivity as existing methods.", "published": "2022-11-19 06:12:50", "link": "http://arxiv.org/abs/2211.10596v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pairwise Instance Relation Augmentation for Long-tailed Multi-label Text\n  Classification", "abstract": "Multi-label text classification (MLTC) is one of the key tasks in natural\nlanguage processing. It aims to assign multiple target labels to one document.\nDue to the uneven popularity of labels, the number of documents per label\nfollows a long-tailed distribution in most cases. It is much more challenging\nto learn classifiers for data-scarce tail labels than for data-rich head\nlabels. The main reason is that head labels usually have sufficient\ninformation, e.g., a large intra-class diversity, while tail labels do not. In\nresponse, we propose a Pairwise Instance Relation Augmentation Network (PIRAN)\nto augment tailed-label documents for balancing tail labels and head labels.\nPIRAN consists of a relation collector and an instance generator. The former\naims to extract the document pairwise relations from head labels. Taking these\nrelations as perturbations, the latter tries to generate new document instances\nin high-level feature space around the limited given tailed-label instances.\nMeanwhile, two regularizers (diversity and consistency) are designed to\nconstrain the generation process. The consistency-regularizer encourages the\nvariance of tail labels to be close to head labels and further balances the\nwhole datasets. And diversity-regularizer makes sure the generated instances\nhave diversity and avoids generating redundant instances. Extensive\nexperimental results on three benchmark datasets demonstrate that PIRAN\nconsistently outperforms the SOTA methods, and dramatically improves the\nperformance of tail labels.", "published": "2022-11-19 12:45:54", "link": "http://arxiv.org/abs/2211.10685v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study On Contrastive Search And Contrastive Decoding For\n  Open-ended Text Generation", "abstract": "In the study, we empirically compare the two recently proposed decoding\nmethods, i.e. Contrastive Search (CS) and Contrastive Decoding (CD), for\nopen-ended text generation. The automatic evaluation results suggest that,\nwhile CS performs worse than CD on the MAUVE metric, it substantially surpasses\nCD on the diversity and coherence metrics. More notably, extensive human\nevaluations across three different domains demonstrate that human annotators\nare universally more in favor of CS over CD with substantial margins.\n  The contradicted results between MAUVE and human evaluations reveal that\nMAUVE does not accurately reflect human preferences. Therefore, we call upon\nthe research community to develop better evaluation metrics for open-ended text\ngeneration. To ensure the reproducibility of our work, we have open-sourced all\nour code, evaluation results, as well as human annotations at\nhttps://github.com/yxuansu/Contrastive_Search_versus_Contrastive_Decoding.", "published": "2022-11-19 20:50:26", "link": "http://arxiv.org/abs/2211.10797v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining State-of-the-Art Models with Maximal Marginal Relevance for\n  Few-Shot and Zero-Shot Multi-Document Summarization", "abstract": "In Natural Language Processing, multi-document summarization (MDS) poses many\nchallenges to researchers above those posed by single-document summarization\n(SDS). These challenges include the increased search space and greater\npotential for the inclusion of redundant information. While advancements in\ndeep learning approaches have led to the development of several advanced\nlanguage models capable of summarization, the variety of training data specific\nto the problem of MDS remains relatively limited. Therefore, MDS approaches\nwhich require little to no pretraining, known as few-shot or zero-shot\napplications, respectively, could be beneficial additions to the current set of\ntools available in summarization. To explore one possible approach, we devise a\nstrategy for combining state-of-the-art models' outputs using maximal marginal\nrelevance (MMR) with a focus on query relevance rather than document diversity.\nOur MMR-based approach shows improvement over some aspects of the current\nstate-of-the-art results in both few-shot and zero-shot MDS applications while\nmaintaining a state-of-the-art standard of output by all available metrics.", "published": "2022-11-19 21:46:31", "link": "http://arxiv.org/abs/2211.10808v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ReInform: Selecting paths with reinforcement learning for contextualized\n  link prediction", "abstract": "We propose to use reinforcement learning to inform transformer-based\ncontextualized link prediction models by providing paths that are most useful\nfor predicting the correct answer. This is in contrast to previous approaches,\nthat either used reinforcement learning (RL) to directly search for the answer,\nor based their prediction on limited or randomly selected context. Our\nexperiments on WN18RR and FB15k-237 show that contextualized link prediction\nmodels consistently outperform RL-based answer search, and that additional\nimprovements (of up to 13.5% MRR) can be gained by combining RL with a link\nprediction model. The PyTorch implementation of the RL agent is available at\nhttps://github.com/marina-sp/reinform", "published": "2022-11-19 13:04:53", "link": "http://arxiv.org/abs/2211.10688v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Metaphorical Language Change Is Self-Organized Criticality", "abstract": "One way to resolve the actuation problem of metaphorical language change is\nto provide a statistical profile of metaphorical constructions and generative\nrules with antecedent conditions. Based on arguments from the view of language\nas complex systems and the dynamic view of metaphor, this paper argues that\nmetaphorical language change qualifies as a self-organized criticality state\nand the linguistic expressions of a metaphor can be profiled as a fractal with\nspatio-temporal correlations. Synchronously, these metaphorical expressions\nself-organize into a self-similar, scale-invariant fractal that follows a\npower-law distribution; temporally, long range inter-dependence constrains the\nself-organization process by the way of transformation rules that are intrinsic\nof a language system. This argument is verified in the paper with statistical\nanalyses of twelve randomly selected Chinese verb metaphors in a large-scale\ndiachronic corpus.", "published": "2022-11-19 14:38:38", "link": "http://arxiv.org/abs/2211.10709v1", "categories": ["cs.CL", "nlin.AO"], "primary_category": "cs.CL"}
{"title": "Machine Learning Approaches for Principle Prediction in Naturally\n  Occurring Stories", "abstract": "Value alignment is the task of creating autonomous systems whose values align\nwith those of humans. Past work has shown that stories are a potentially rich\nsource of information on human values; however, past work has been limited to\nconsidering values in a binary sense. In this work, we explore the use of\nmachine learning models for the task of normative principle prediction on\nnaturally occurring story data. To do this, we extend a dataset that has been\npreviously used to train a binary normative classifier with annotations of\nmoral principles. We then use this dataset to train a variety of machine\nlearning models, evaluate these models and compare their results against humans\nwho were asked to perform the same task. We show that while individual\nprinciples can be classified, the ambiguity of what \"moral principles\"\nrepresent, poses a challenge for both human participants and autonomous systems\nwhich are faced with the same task.", "published": "2022-11-19 03:14:23", "link": "http://arxiv.org/abs/2212.06048v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Entity-Assisted Language Models for Identifying Check-worthy Sentences", "abstract": "We propose a new uniform framework for text classification and ranking that\ncan automate the process of identifying check-worthy sentences in political\ndebates and speech transcripts. Our framework combines the semantic analysis of\nthe sentences, with additional entity embeddings obtained through the\nidentified entities within the sentences. In particular, we analyse the\nsemantic meaning of each sentence using state-of-the-art neural language models\nsuch as BERT, ALBERT, and RoBERTa, while embeddings for entities are obtained\nfrom knowledge graph (KG) embedding models. Specifically, we instantiate our\nframework using five different language models, entity embeddings obtained from\nsix different KG embedding models, as well as two combination methods leading\nto several Entity-Assisted neural language models. We extensively evaluate the\neffectiveness of our framework using two publicly available datasets from the\nCLEF' 2019 & 2020 CheckThat! Labs. Our results show that the neural language\nmodels significantly outperform traditional TF.IDF and LSTM methods. In\naddition, we show that the ALBERT model is consistently the most effective\nmodel among all the tested neural language models. Our entity embeddings\nsignificantly outperform other existing approaches from the literature that are\nbased on similarity and relatedness scores between the entities in a sentence,\nwhen used alongside a KG embedding.", "published": "2022-11-19 12:03:30", "link": "http://arxiv.org/abs/2211.10678v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Suffering from Vaccines or from Government? : Partisan Bias in COVID-19\n  Vaccine Adverse Events Coverage", "abstract": "Vaccine adverse events have been presumed to be a relatively objective\nmeasure that is immune to political polarization. The real-world data, however,\nshows the correlation between presidential disapproval ratings and the\nsubjective severity of adverse events. This paper investigates the partisan\nbias in COVID vaccine adverse events coverage with language models that can\nclassify the topic of vaccine-related articles and the political disposition of\nnews comments. Based on 90K news articles from 52 major newspaper companies, we\nfound that conservative media are inclined to report adverse events more\nfrequently than their liberal counterparts, while the coverage itself was\nstatistically uncorrelated with the severity of real-world adverse events. The\nusers who support the conservative opposing party were more likely to write the\npopular comments from 2.3K random sampled articles on news platforms. This\nresearch implies that bipartisanship can still play a significant role in\nforming public opinion on the COVID vaccine even after the majority of the\npopulation's vaccination", "published": "2022-11-19 14:17:07", "link": "http://arxiv.org/abs/2211.10707v1", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "ArtELingo: A Million Emotion Annotations of WikiArt with Emphasis on\n  Diversity over Language and Culture", "abstract": "This paper introduces ArtELingo, a new benchmark and dataset, designed to\nencourage work on diversity across languages and cultures. Following ArtEmis, a\ncollection of 80k artworks from WikiArt with 0.45M emotion labels and\nEnglish-only captions, ArtELingo adds another 0.79M annotations in Arabic and\nChinese, plus 4.8K in Spanish to evaluate \"cultural-transfer\" performance. More\nthan 51K artworks have 5 annotations or more in 3 languages. This diversity\nmakes it possible to study similarities and differences across languages and\ncultures. Further, we investigate captioning tasks, and find diversity improves\nthe performance of baseline models. ArtELingo is publicly available at\nhttps://www.artelingo.org/ with standard splits and baseline models. We hope\nour work will help ease future research on multilinguality and culturally-aware\nAI.", "published": "2022-11-19 19:34:18", "link": "http://arxiv.org/abs/2211.10780v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Operationalizing Specifications, In Addition to Test Sets for Evaluating\n  Constrained Generative Models", "abstract": "In this work, we present some recommendations on the evaluation of\nstate-of-the-art generative models for constrained generation tasks. The\nprogress on generative models has been rapid in recent years. These large-scale\nmodels have had three impacts: firstly, the fluency of generation in both\nlanguage and vision modalities has rendered common average-case evaluation\nmetrics much less useful in diagnosing system errors. Secondly, the same\nsubstrate models now form the basis of a number of applications, driven both by\nthe utility of their representations as well as phenomena such as in-context\nlearning, which raise the abstraction level of interacting with such models.\nThirdly, the user expectations around these models and their feted public\nreleases have made the technical challenge of out of domain generalization much\nless excusable in practice. Subsequently, our evaluation methodologies haven't\nadapted to these changes. More concretely, while the associated utility and\nmethods of interacting with generative models have expanded, a similar\nexpansion has not been observed in their evaluation practices. In this paper,\nwe argue that the scale of generative models could be exploited to raise the\nabstraction level at which evaluation itself is conducted and provide\nrecommendations for the same. Our recommendations are based on leveraging\nspecifications as a powerful instrument to evaluate generation quality and are\nreadily applicable to a variety of tasks.", "published": "2022-11-19 06:39:43", "link": "http://arxiv.org/abs/2212.00006v1", "categories": ["cs.HC", "cs.CL", "cs.CV", "cs.CY"], "primary_category": "cs.HC"}
{"title": "Multi-Speaker Expressive Speech Synthesis via Multiple Factors\n  Decoupling", "abstract": "This paper aims to synthesize the target speaker's speech with desired\nspeaking style and emotion by transferring the style and emotion from reference\nspeech recorded by other speakers. We address this challenging problem with a\ntwo-stage framework composed of a text-to-style-and-emotion (Text2SE) module\nand a style-and-emotion-to-wave (SE2Wave) module, bridging by neural bottleneck\n(BN) features. To further solve the multi-factor (speaker timbre, speaking\nstyle and emotion) decoupling problem, we adopt the multi-label binary vector\n(MBV) and mutual information (MI) minimization to respectively discretize the\nextracted embeddings and disentangle these highly entangled factors in both\nText2SE and SE2Wave modules. Moreover, we introduce a semi-supervised training\nstrategy to leverage data from multiple speakers, including emotion-labeled\ndata, style-labeled data, and unlabeled data. To better transfer the\nfine-grained expression from references to the target speaker in non-parallel\ntransfer, we introduce a reference-candidate pool and propose an\nattention-based reference selection approach. Extensive experiments demonstrate\nthe good design of our model.", "published": "2022-11-19 02:44:51", "link": "http://arxiv.org/abs/2211.10568v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Filterbank Learning for Noise-Robust Small-Footprint Keyword Spotting", "abstract": "In the context of keyword spotting (KWS), the replacement of handcrafted\nspeech features by learnable features has not yielded superior KWS performance.\nIn this study, we demonstrate that filterbank learning outperforms handcrafted\nspeech features for KWS whenever the number of filterbank channels is severely\ndecreased. Reducing the number of channels might yield certain KWS performance\ndrop, but also a substantial energy consumption reduction, which is key when\ndeploying common always-on KWS on low-resource devices. Experimental results on\na noisy version of the Google Speech Commands Dataset show that filterbank\nlearning adapts to noise characteristics to provide a higher degree of\nrobustness to noise, especially when dropout is integrated. Thus, switching\nfrom typically used 40-channel log-Mel features to 8-channel learned features\nleads to a relative KWS accuracy loss of only 3.5% while simultaneously\nachieving a 6.3x energy consumption reduction.", "published": "2022-11-19 02:20:14", "link": "http://arxiv.org/abs/2211.10565v2", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "EDGE: Editable Dance Generation From Music", "abstract": "Dance is an important human art form, but creating new dances can be\ndifficult and time-consuming. In this work, we introduce Editable Dance\nGEneration (EDGE), a state-of-the-art method for editable dance generation that\nis capable of creating realistic, physically-plausible dances while remaining\nfaithful to the input music. EDGE uses a transformer-based diffusion model\npaired with Jukebox, a strong music feature extractor, and confers powerful\nediting capabilities well-suited to dance, including joint-wise conditioning,\nand in-betweening. We introduce a new metric for physical plausibility, and\nevaluate dance quality generated by our method extensively through (1) multiple\nquantitative metrics on physical plausibility, beat alignment, and diversity\nbenchmarks, and more importantly, (2) a large-scale user study, demonstrating a\nsignificant improvement over previous state-of-the-art methods. Qualitative\nsamples from our model can be found at our website.", "published": "2022-11-19 10:41:38", "link": "http://arxiv.org/abs/2211.10658v2", "categories": ["cs.SD", "cs.CV", "cs.GR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Phonemic Adversarial Attack against Audio Recognition in Real World", "abstract": "Recently, adversarial attacks for audio recognition have attracted much\nattention. However, most of the existing studies mainly rely on the\ncoarse-grain audio features at the instance level to generate adversarial\nnoises, which leads to expensive generation time costs and weak universal\nattacking ability. Motivated by the observations that all audio speech consists\nof fundamental phonemes, this paper proposes a phonemic adversarial tack (PAT)\nparadigm, which attacks the fine-grain audio features at the phoneme level\ncommonly shared across audio instances, to generate phonemic adversarial\nnoises, enjoying the more general attacking ability with fast generation speed.\nSpecifically, for accelerating the generation, a phoneme density balanced\nsampling strategy is introduced to sample quantity less but phonemic features\nabundant audio instances as the training data via estimating the phoneme\ndensity, which substantially alleviates the heavy dependency on the large\ntraining dataset. Moreover, for promoting universal attacking ability, the\nphonemic noise is optimized in an asynchronous way with a sliding window, which\nenhances the phoneme diversity and thus well captures the critical fundamental\nphonemic patterns. By conducting extensive experiments, we comprehensively\ninvestigate the proposed PAT framework and demonstrate that it outperforms the\nSOTA baselines by large margins (i.e., at least 11X speed up and 78% attacking\nability improvement).", "published": "2022-11-19 11:01:21", "link": "http://arxiv.org/abs/2211.10661v1", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VarietySound: Timbre-Controllable Video to Sound Generation via\n  Unsupervised Information Disentanglement", "abstract": "Video to sound generation aims to generate realistic and natural sound given\na video input. However, previous video-to-sound generation methods can only\ngenerate a random or average timbre without any controls or specializations of\nthe generated sound timbre, leading to the problem that people cannot obtain\nthe desired timbre under these methods sometimes. In this paper, we pose the\ntask of generating sound with a specific timbre given a video input and a\nreference audio sample. To solve this task, we disentangle each target sound\naudio into three components: temporal information, acoustic information, and\nbackground information. We first use three encoders to encode these components\nrespectively: 1) a temporal encoder to encode temporal information, which is\nfed with video frames since the input video shares the same temporal\ninformation as the original audio; 2) an acoustic encoder to encode timbre\ninformation, which takes the original audio as input and discards its temporal\ninformation by a temporal-corrupting operation; and 3) a background encoder to\nencode the residual or background sound, which uses the background part of the\noriginal audio as input. To make the generated result achieve better quality\nand temporal alignment, we also adopt a mel discriminator and a temporal\ndiscriminator for the adversarial training. Our experimental results on the VAS\ndataset demonstrate that our method can generate high-quality audio samples\nwith good synchronization with events in video and high timbre similarity with\nthe reference audio.", "published": "2022-11-19 11:12:01", "link": "http://arxiv.org/abs/2211.10666v1", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
