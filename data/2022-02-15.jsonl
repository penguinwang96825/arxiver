{"title": "Saving Dense Retriever from Shortcut Dependency in Conversational Search", "abstract": "Conversational search (CS) needs a holistic understanding of conversational\ninputs to retrieve relevant passages. In this paper, we demonstrate the\nexistence of a retrieval shortcut in CS, which causes models to retrieve\npassages solely relying on partial history while disregarding the latest\nquestion. With in-depth analysis, we first show that naively trained dense\nretrievers heavily exploit the shortcut and hence perform poorly when asked to\nanswer history-independent questions. To build more robust models against\nshortcut dependency, we explore various hard negative mining strategies.\nExperimental results show that training with the model-based hard negatives\neffectively mitigates the dependency on the shortcut, significantly improving\ndense retrievers on recent CS benchmarks. In particular, our retriever\noutperforms the previous state-of-the-art model by 11.0 in Recall@10 on QReCC.", "published": "2022-02-15 09:53:35", "link": "http://arxiv.org/abs/2202.07280v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Effective Multi-Task Interaction for Entity-Relation Extraction:\n  A Unified Framework with Selection Recurrent Network", "abstract": "Entity-relation extraction aims to jointly solve named entity recognition\n(NER) and relation extraction (RE). Recent approaches use either one-way\nsequential information propagation in a pipeline manner or two-way implicit\ninteraction with a shared encoder. However, they still suffer from poor\ninformation interaction due to the gap between the different task forms of NER\nand RE, raising a controversial question whether RE is really beneficial to\nNER. Motivated by this, we propose a novel and unified cascade framework that\ncombines the advantages of both sequential information propagation and implicit\ninteraction. Meanwhile, it eliminates the gap between the two tasks by\nreformulating entity-relation extraction as unified span-extraction tasks.\nSpecifically, we propose a selection recurrent network as a shared encoder to\nencode task-specific independent and shared representations and design two\nsequential information propagation strategies to realize the sequential\ninformation flow between NER and RE. Extensive experiments demonstrate that our\napproaches can achieve state-of-the-art results on two common benchmarks, ACE05\nand SciERC, and effectively model the multi-task interaction, which realizes\nsignificant mutual benefits of NER and RE.", "published": "2022-02-15 09:54:33", "link": "http://arxiv.org/abs/2202.07281v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BLUE at Memotion 2.0 2022: You have my Image, my Text and my Transformer", "abstract": "Memes are prevalent on the internet and continue to grow and evolve alongside\nour culture. An automatic understanding of memes propagating on the internet\ncan shed light on the general sentiment and cultural attitudes of people. In\nthis work, we present team BLUE's solution for the second edition of the\nMEMOTION shared task. We showcase two approaches for meme classification (i.e.\nsentiment, humour, offensive, sarcasm and motivation levels) using a text-only\nmethod using BERT, and a Multi-Modal-Multi-Task transformer network that\noperates on both the meme image and its caption to output the final scores. In\nboth approaches, we leverage state-of-the-art pretrained models for text (BERT,\nSentence Transformer) and image processing (EfficientNetV4, CLIP). Through our\nefforts, we obtain first place in task A, second place in task B and third\nplace in task C. In addition, our team obtained the highest average score for\nall three tasks.", "published": "2022-02-15 16:25:02", "link": "http://arxiv.org/abs/2202.07543v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "P4E: Few-Shot Event Detection as Prompt-Guided Identification and\n  Localization", "abstract": "We propose P4E, an identify-and-localize event detection framework that\nintegrates the best of few-shot prompting and structured prediction. Our\nframework decomposes event detection into an identification task and a\nlocalization task. For the identification task, which we formulate as\nmulti-label classification, we leverage cloze-based prompting to align our\nobjective with the pre-training task of language models, allowing our model to\nquickly adapt to new event types. We then employ an event type-agnostic\nsequence labeling model to localize the event trigger conditioned on the\nidentification output. This heterogeneous model design allows P4E to quickly\nlearn new event types without sacrificing the ability to make structured\npredictions. Our experiments demonstrate the effectiveness of our proposed\ndesign, and P4E shows superior performance for few-shot event detection on\nbenchmark datasets FewEvent and MAVEN and comparable performance to SOTA for\nfully-supervised event detection on ACE.", "published": "2022-02-15 18:01:39", "link": "http://arxiv.org/abs/2202.07615v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Delving Deeper into Cross-lingual Visual Question Answering", "abstract": "Visual question answering (VQA) is one of the crucial vision-and-language\ntasks. Yet, existing VQA research has mostly focused on the English language,\ndue to a lack of suitable evaluation resources. Previous work on cross-lingual\nVQA has reported poor zero-shot transfer performance of current multilingual\nmultimodal Transformers with large gaps to monolingual performance, without any\ndeeper analysis. In this work, we delve deeper into the different aspects of\ncross-lingual VQA, aiming to understand the impact of 1) modeling methods and\nchoices, including architecture, inductive bias, fine-tuning; 2) learning\nbiases: including question types and modality biases in cross-lingual setups.\nThe key results of our analysis are: 1) We show that simple modifications to\nthe standard training setup can substantially reduce the transfer gap to\nmonolingual English performance, yielding +10 accuracy points over existing\nmethods. 2) We analyze cross-lingual VQA across different question types of\nvarying complexity for different multilingual multimodal Transformers, and\nidentify question types that are the most difficult to improve on. 3) We\nprovide an analysis of modality biases present in training data and models,\nrevealing why zero-shot performance gaps remain for certain question types and\nlanguages.", "published": "2022-02-15 18:22:18", "link": "http://arxiv.org/abs/2202.07630v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One Configuration to Rule Them All? Towards Hyperparameter Transfer in\n  Topic Models using Multi-Objective Bayesian Optimization", "abstract": "Topic models are statistical methods that extract underlying topics from\ndocument collections. When performing topic modeling, a user usually desires\ntopics that are coherent, diverse between each other, and that constitute good\ndocument representations for downstream tasks (e.g. document classification).\nIn this paper, we conduct a multi-objective hyperparameter optimization of\nthree well-known topic models. The obtained results reveal the conflicting\nnature of different objectives and that the training corpus characteristics are\ncrucial for the hyperparameter selection, suggesting that it is possible to\ntransfer the optimal hyperparameter configurations between datasets.", "published": "2022-02-15 18:26:02", "link": "http://arxiv.org/abs/2202.07631v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Integrating AI Planning with Natural Language Processing: A Combination\n  of Explicit and Tacit Knowledge", "abstract": "Natural language processing (NLP) aims at investigating the interactions\nbetween agents and humans, processing and analyzing large amounts of natural\nlanguage data. Large-scale language models play an important role in current\nnatural language processing. However, the challenges of explainability and\ncomplexity come along with the developments of language models. One way is to\nintroduce logical relations and rules into natural language processing models,\nsuch as making use of Automated Planning. Automated planning (AI planning)\nfocuses on building symbolic domain models and synthesizing plans to transit\ninitial states to goals based on domain models. Recently, there have been\nplenty of works related to these two fields, which have the abilities to\ngenerate explicit knowledge, e.g., preconditions and effects of action models,\nand learn from tacit knowledge, e.g., neural models, respectively. Integrating\nAI planning and natural language processing effectively improves the\ncommunication between human and intelligent agents. This paper outlines the\ncommons and relations between AI planning and natural language processing,\nargues that each of them can effectively impact on the other one by five areas:\n(1) planning-based text understanding, (2) planning-based natural language\nprocessing, (3) planning-based explainability, (4) text-based human-robot\ninteraction, and (5) applications. We also explore some potential future issues\nbetween AI planning and natural language processing. To the best of our\nknowledge, this survey is the first work that addresses the deep connections\nbetween AI planning and Natural language processing.", "published": "2022-02-15 02:19:09", "link": "http://arxiv.org/abs/2202.07138v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "NewsPod: Automatic and Interactive News Podcasts", "abstract": "News podcasts are a popular medium to stay informed and dive deep into news\ntopics. Today, most podcasts are handcrafted by professionals. In this work, we\nadvance the state-of-the-art in automatically generated podcasts, making use of\nrecent advances in natural language processing and text-to-speech technology.\nWe present NewsPod, an automatically generated, interactive news podcast. The\npodcast is divided into segments, each centered on a news event, with each\nsegment structured as a Question and Answer conversation, whose goal is to\nengage the listener. A key aspect of the design is the use of distinct voices\nfor each role (questioner, responder), to better simulate a conversation.\nAnother novel aspect of NewsPod allows listeners to interact with the podcast\nby asking their own questions and receiving automatically generated answers. We\nvalidate the soundness of this system design through two usability studies,\nfocused on evaluating the narrative style and interactions with the podcast,\nrespectively. We find that NewsPod is preferred over a baseline by\nparticipants, with 80% claiming they would use the system in the future.", "published": "2022-02-15 02:37:04", "link": "http://arxiv.org/abs/2202.07146v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "On Tracking Dialogue State by Inheriting Slot Values in Mentioned Slot\n  Pools", "abstract": "Dialogue state tracking (DST) is a component of the task-oriented dialogue\nsystem. It is responsible for extracting and managing slot values according to\ndialogue utterances, where each slot represents an essential part of the\ninformation to accomplish a task, and slot value is updated recurrently in each\ndialogue turn. However, many DST models cannot update slot values\nappropriately. These models may repeatedly inherit wrong slot values extracted\nin previous turns, resulting in the fail of the entire DST task. They cannot\nupdate indirectly mentioned slots well, either. This study designed a model\nwith a mentioned slot pool (MSP) to tackle the update problem. The MSP is a\nslot-specific memory that records all mentioned slot values that may be\ninherited, and our model updates slot values according to the MSP and the\ndialogue context. Our model rejects inheriting the previous slot value when it\npredicates the value is wrong. Then, it re-extracts the slot value from the\ncurrent dialogue context. As the contextual information accumulates with the\ndialogue progress, the new value is more likely to be correct. It also can\ntrack the indirectly mentioned slot by picking a value from the MSP.\nExperimental results showed our model reached state-of-the-art DST performance\non MultiWOZ 2.1 and 2.2 datasets.", "published": "2022-02-15 03:03:38", "link": "http://arxiv.org/abs/2202.07156v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Impact of Pretraining Term Frequencies on Few-Shot Reasoning", "abstract": "Pretrained Language Models (LMs) have demonstrated ability to perform\nnumerical reasoning by extrapolating from a few examples in few-shot settings.\nHowever, the extent to which this extrapolation relies on robust reasoning is\nunclear. In this paper, we investigate how well these models reason with terms\nthat are less frequent in the pretraining data. In particular, we examine the\ncorrelations between the model performance on test instances and the frequency\nof terms from those instances in the pretraining data. We measure the strength\nof this correlation for a number of GPT-based language models (pretrained on\nthe Pile dataset) on various numerical deduction tasks (e.g., arithmetic and\nunit conversion). Our results consistently demonstrate that models are more\naccurate on instances whose terms are more prevalent, in some cases above\n$70\\%$ (absolute) more accurate on the top 10\\% frequent terms in comparison to\nthe bottom 10\\%. Overall, although LMs exhibit strong performance at few-shot\nnumerical reasoning tasks, our results raise the question of how much models\nactually generalize beyond pretraining data, and we encourage researchers to\ntake the pretraining data into account when interpreting evaluation results.", "published": "2022-02-15 05:43:54", "link": "http://arxiv.org/abs/2202.07206v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Cross-lingual Prompting with Dual Prompt Augmentation", "abstract": "Prompting shows promising results in few-shot scenarios. However, its\nstrength for multilingual/cross-lingual problems has not been fully exploited.\nZhao and Sch\\\"utze (2021) made initial explorations in this direction by\npresenting that cross-lingual prompting outperforms cross-lingual finetuning.\nIn this paper, we conduct an empirical exploration on the effect of each\ncomponent in cross-lingual prompting and derive language-agnostic Universal\nPrompting, which helps alleviate the discrepancies between source-language\ntraining and target-language inference. Based on this, we propose DPA, a dual\nprompt augmentation framework, aiming at relieving the data scarcity issue in\nfew-shot cross-lingual prompting. Notably, for XNLI, our method achieves 46.54%\nwith only 16 English training examples per class, significantly better than\n34.99% of finetuning. Our code is available at\nhttps://github.com/DAMO-NLP-SG/DPA.", "published": "2022-02-15 09:04:14", "link": "http://arxiv.org/abs/2202.07255v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ViNTER: Image Narrative Generation with Emotion-Arc-Aware Transformer", "abstract": "Image narrative generation is a task to create a story from an image with a\nsubjective viewpoint. Given the importance of the subjective feelings of\nwriters, readers, and characters in storytelling, an image narrative generation\nmethod should consider human emotion. In this study, we propose a novel method\nof image narrative generation called ViNTER (Visual Narrative Transformer with\nEmotion arc Representation), which takes \"emotion arc\" as input to capture a\nsequence of emotional changes. Since emotion arcs represent the trajectory of\nemotional change, it is expected that we can include detailed information about\nthe emotional changes in the story to the model. We present experimental\nresults of both automatic and manual evaluations on the Image Narrative dataset\nand demonstrate the effectiveness of the proposed approach.", "published": "2022-02-15 10:53:08", "link": "http://arxiv.org/abs/2202.07305v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MuLD: The Multitask Long Document Benchmark", "abstract": "The impressive progress in NLP techniques has been driven by the development\nof multi-task benchmarks such as GLUE and SuperGLUE. While these benchmarks\nfocus on tasks for one or two input sentences, there has been exciting work in\ndesigning efficient techniques for processing much longer inputs. In this\npaper, we present MuLD: a new long document benchmark consisting of only\ndocuments over 10,000 tokens. By modifying existing NLP tasks, we create a\ndiverse benchmark which requires models to successfully model long-term\ndependencies in the text. We evaluate how existing models perform, and find\nthat our benchmark is much more challenging than their `short document'\nequivalents. Furthermore, by evaluating both regular and efficient\ntransformers, we show that models with increased context length are better able\nto solve the tasks presented, suggesting that future improvements in these\nmodels are vital for solving similar long document problems. We release the\ndata and code for baselines to encourage further research on efficient NLP\nmodels.", "published": "2022-02-15 12:42:55", "link": "http://arxiv.org/abs/2202.07362v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Shifting Trends of COVID-19 Tweet Sentiment with Respect to Voting\n  Preferences in the 2020 Election Year of the United States", "abstract": "COVID-19 related policies were extensively politicized during the 2020\nelection year of the United States, resulting in polarizing viewpoints. Twitter\nusers were particularly engaged during the 2020 election year. Here we\ninvestigated whether COVID-19 related tweets were associated with the overall\nelection results at the state level during the period leading up to the\nelection day. We observed weak correlations between the average sentiment of\nCOVID-19 related tweets and popular votes in two-week intervals, and the trends\ngradually become opposite. We then compared the average sentiments of COVID-19\nrelated tweets between states called in favor of Republican (red states) or\nDemocratic parties (blue states). We found that at the beginning of lockdowns\nsentiments in the blue states were much more positive than those in the red\nstates. However, sentiments in the red states gradually become more positive\nduring the summer of 2020 and persisted until the election day.", "published": "2022-02-15 17:22:22", "link": "http://arxiv.org/abs/2202.07587v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Quantifying Memorization Across Neural Language Models", "abstract": "Large language models (LMs) have been shown to memorize parts of their\ntraining data, and when prompted appropriately, they will emit the memorized\ntraining data verbatim. This is undesirable because memorization violates\nprivacy (exposing user data), degrades utility (repeated easy-to-memorize text\nis often low quality), and hurts fairness (some texts are memorized over\nothers).\n  We describe three log-linear relationships that quantify the degree to which\nLMs emit memorized training data. Memorization significantly grows as we\nincrease (1) the capacity of a model, (2) the number of times an example has\nbeen duplicated, and (3) the number of tokens of context used to prompt the\nmodel. Surprisingly, we find the situation becomes more complicated when\ngeneralizing these results across model families. On the whole, we find that\nmemorization in LMs is more prevalent than previously believed and will likely\nget worse as models continues to scale, at least without active mitigations.", "published": "2022-02-15 18:48:31", "link": "http://arxiv.org/abs/2202.07646v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question\n  Answering Evaluation", "abstract": "The predictions of question answering (QA)systems are typically evaluated\nagainst manually annotated finite sets of one or more answers. This leads to a\ncoverage limitation that results in underestimating the true performance of\nsystems, and is typically addressed by extending over exact match (EM) with\npre-defined rules or with the token-level F1 measure. In this paper, we present\nthe first systematic conceptual and data-driven analysis to examine the\nshortcomings of token-level equivalence measures.\n  To this end, we define the asymmetric notion of answer equivalence (AE),\naccepting answers that are equivalent to or improve over the reference, and\npublish over 23k human judgments for candidates produced by multiple QA systems\non SQuAD. Through a careful analysis of this data, we reveal and quantify\nseveral concrete limitations of the F1 measure, such as a false impression of\ngraduality, or missing dependence on the question.\n  Since collecting AE annotations for each evaluated model is expensive, we\nlearn a BERT matching (BEM) measure to approximate this task. Being a simpler\ntask than QA, we find BEM to provide significantly better AE approximations\nthan F1, and to more accurately reflect the performance of systems.\n  Finally, we demonstrate the practical utility of AE and BEM on the concrete\napplication of minimal accurate prediction sets, reducing the number of\nrequired answers by up to x2.6.", "published": "2022-02-15 18:53:58", "link": "http://arxiv.org/abs/2202.07654v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Russian SuperGLUE 1.1: Revising the Lessons not Learned by Russian NLP\n  models", "abstract": "In the last year, new neural architectures and multilingual pre-trained\nmodels have been released for Russian, which led to performance evaluation\nproblems across a range of language understanding tasks.\n  This paper presents Russian SuperGLUE 1.1, an updated benchmark styled after\nGLUE for Russian NLP models. The new version includes a number of technical,\nuser experience and methodological improvements, including fixes of the\nbenchmark vulnerabilities unresolved in the previous version: novel and\nimproved tests for understanding the meaning of a word in context (RUSSE) along\nwith reading comprehension and common sense reasoning (DaNetQA, RuCoS, MuSeRC).\nTogether with the release of the updated datasets, we improve the benchmark\ntoolkit based on \\texttt{jiant} framework for consistent training and\nevaluation of NLP-models of various architectures which now supports the most\nrecent models for Russian. Finally, we provide the integration of Russian\nSuperGLUE with a framework for industrial evaluation of the open-source models,\nMOROCCO (MOdel ResOurCe COmparison), in which the models are evaluated\naccording to the weighted average metric over all tasks, the inference speed,\nand the occupied amount of RAM. Russian SuperGLUE is publicly available at\nhttps://russiansuperglue.com/.", "published": "2022-02-15 23:45:30", "link": "http://arxiv.org/abs/2202.07791v1", "categories": ["cs.CL", "cs.AI", "68-06, 68T50, 68T01", "G.3; I.2.7"], "primary_category": "cs.CL"}
{"title": "Toxic Comments Hunter : Score Severity of Toxic Comments", "abstract": "The detection and identification of toxic comments are conducive to creating\na civilized and harmonious Internet environment. In this experiment, we\ncollected various data sets related to toxic comments. Because of the\ncharacteristics of comment data, we perform data cleaning and feature\nextraction operations on it from different angles to obtain different toxic\ncomment training sets. In terms of model construction, we used the training set\nto train the models based on TFIDF and finetuned the Bert model separately.\nFinally, we encapsulated the code into software to score toxic comments in\nreal-time.", "published": "2022-02-15 07:35:52", "link": "http://arxiv.org/abs/2203.03548v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey on Dynamic Neural Networks for Natural Language Processing", "abstract": "Effectively scaling large Transformer models is a main driver of recent\nadvances in natural language processing. Dynamic neural networks, as an\nemerging research direction, are capable of scaling up neural networks with\nsub-linear increases in computation and time by dynamically adjusting their\ncomputational path based on the input. Dynamic neural networks could be a\npromising solution to the growing parameter numbers of pretrained language\nmodels, allowing both model pretraining with trillions of parameters and faster\ninference on mobile devices. In this survey, we summarize progress of three\ntypes of dynamic neural networks in NLP: skimming, mixture of experts, and\nearly exit. We also highlight current challenges in dynamic neural networks and\ndirections for future research.", "published": "2022-02-15 00:13:05", "link": "http://arxiv.org/abs/2202.07101v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey on Model Compression and Acceleration for Pretrained Language\n  Models", "abstract": "Despite achieving state-of-the-art performance on many NLP tasks, the high\nenergy cost and long inference delay prevent Transformer-based pretrained\nlanguage models (PLMs) from seeing broader adoption including for edge and\nmobile computing. Efficient NLP research aims to comprehensively consider\ncomputation, time and carbon emission for the entire life-cycle of NLP,\nincluding data preparation, model training and inference. In this survey, we\nfocus on the inference stage and review the current state of model compression\nand acceleration for pretrained language models, including benchmarks, metrics\nand methodology.", "published": "2022-02-15 00:18:47", "link": "http://arxiv.org/abs/2202.07105v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Media Slant is Contagious", "abstract": "This paper examines the diffusion of media slant. We document the influence\nof Fox News Channel (FNC) on the partisan slant of local newspapers in the U.S.\nover the years 1995-2008. We measure the political slant of local newspapers by\nscaling the news article texts to Republicans' and Democrats' speeches in\nCongress. Using channel positioning as an instrument for viewership, we find\nthat higher FNC viewership causes local newspapers to adopt more right-wing\nslant. The effect emerges gradually, only several years after FNC's\nintroduction, mirroring the channel's growing influence on voting behavior. A\nmain driver of the shift in newspaper slant appears to be a change in local\npolitical preferences.", "published": "2022-02-15 09:25:02", "link": "http://arxiv.org/abs/2202.07269v4", "categories": ["econ.GN", "cs.CL", "cs.CY", "q-fin.EC"], "primary_category": "econ.GN"}
{"title": "textless-lib: a Library for Textless Spoken Language Processing", "abstract": "Textless spoken language processing research aims to extend the applicability\nof standard NLP toolset onto spoken language and languages with few or no\ntextual resources. In this paper, we introduce textless-lib, a PyTorch-based\nlibrary aimed to facilitate research in this research area. We describe the\nbuilding blocks that the library provides and demonstrate its usability by\ndiscuss three different use-case examples: (i) speaker probing, (ii) speech\nresynthesis and compression, and (iii) speech continuation. We believe that\ntextless-lib substantially simplifies research the textless setting and will be\nhandful not only for speech researchers but also for the NLP community at\nlarge. The code, documentation, and pre-trained models are available at\nhttps://github.com/facebookresearch/textlesslib/ .", "published": "2022-02-15 12:39:42", "link": "http://arxiv.org/abs/2202.07359v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Personalized Prompt Learning for Explainable Recommendation", "abstract": "Providing user-understandable explanations to justify recommendations could\nhelp users better understand the recommended items, increase the system's ease\nof use, and gain users' trust. A typical approach to realize it is natural\nlanguage generation. However, previous works mostly adopt recurrent neural\nnetworks to meet the ends, leaving the potentially more effective pre-trained\nTransformer models under-explored. In fact, user and item IDs, as important\nidentifiers in recommender systems, are inherently in different semantic space\nas words that pre-trained models were already trained on. Thus, how to\neffectively fuse IDs into such models becomes a critical issue. Inspired by\nrecent advancement in prompt learning, we come up with two solutions: find\nalternative words to represent IDs (called discrete prompt learning), and\ndirectly input ID vectors to a pre-trained model (termed continuous prompt\nlearning). In the latter case, ID vectors are randomly initialized but the\nmodel is trained in advance on large corpora, so they are actually in different\nlearning stages. To bridge the gap, we further propose two training strategies:\nsequential tuning and recommendation as regularization. Extensive experiments\nshow that our continuous prompt learning approach equipped with the training\nstrategies consistently outperforms strong baselines on three datasets of\nexplainable recommendation.", "published": "2022-02-15 12:53:52", "link": "http://arxiv.org/abs/2202.07371v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Nonverbal Sound Detection for Disordered Speech", "abstract": "Voice assistants have become an essential tool for people with various\ndisabilities because they enable complex phone- or tablet-based interactions\nwithout the need for fine-grained motor control, such as with touchscreens.\nHowever, these systems are not tuned for the unique characteristics of\nindividuals with speech disorders, including many of those who have a\nmotor-speech disorder, are deaf or hard of hearing, have a severe stutter, or\nare minimally verbal. We introduce an alternative voice-based input system\nwhich relies on sound event detection using fifteen nonverbal mouth sounds like\n\"pop,\" \"click,\" or \"eh.\" This system was designed to work regardless of ones'\nspeech abilities and allows full access to existing technology. In this paper,\nwe describe the design of a dataset, model considerations for real-world\ndeployment, and efforts towards model personalization. Our fully-supervised\nmodel achieves segment-level precision and recall of 88.6% and 88.4% on an\ninternal dataset of 710 adults, while achieving 0.31 false positives per hour\non aggressors such as speech. Five-shot personalization enables satisfactory\nperformance in 84.5% of cases where the generic model fails.", "published": "2022-02-15 22:02:58", "link": "http://arxiv.org/abs/2202.07750v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Text-Based Action-Model Acquisition for Planning", "abstract": "Although there have been approaches that are capable of learning action\nmodels from plan traces, there is no work on learning action models from\ntextual observations, which is pervasive and much easier to collect from\nreal-world applications compared to plan traces. In this paper we propose a\nnovel approach to learning action models from natural language texts by\nintegrating Constraint Satisfaction and Natural Language Processing techniques.\nSpecifically, we first build a novel language model to extract plan traces from\ntexts, and then build a set of constraints to generate action models based on\nthe extracted plan traces. After that, we iteratively improve the language\nmodel and constraints until we achieve the convergent language model and action\nmodels. We empirically exhibit that our approach is both effective and\nefficient.", "published": "2022-02-15 02:23:31", "link": "http://arxiv.org/abs/2202.08373v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CommerceMM: Large-Scale Commerce MultiModal Representation Learning with\n  Omni Retrieval", "abstract": "We introduce CommerceMM - a multimodal model capable of providing a diverse\nand granular understanding of commerce topics associated to the given piece of\ncontent (image, text, image+text), and having the capability to generalize to a\nwide range of tasks, including Multimodal Categorization, Image-Text Retrieval,\nQuery-to-Product Retrieval, Image-to-Product Retrieval, etc. We follow the\npre-training + fine-tuning training regime and present 5 effective pre-training\ntasks on image-text pairs. To embrace more common and diverse commerce data\nwith text-to-multimodal, image-to-multimodal, and multimodal-to-multimodal\nmapping, we propose another 9 novel cross-modal and cross-pair retrieval tasks,\ncalled Omni-Retrieval pre-training. The pre-training is conducted in an\nefficient manner with only two forward/backward updates for the combined 14\ntasks. Extensive experiments and analysis show the effectiveness of each task.\nWhen combining all pre-training tasks, our model achieves state-of-the-art\nperformance on 7 commerce-related downstream tasks after fine-tuning.\nAdditionally, we propose a novel approach of modality randomization to\ndynamically adjust our model under different efficiency constraints.", "published": "2022-02-15 08:23:59", "link": "http://arxiv.org/abs/2202.07247v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM", "cs.SI"], "primary_category": "cs.CV"}
{"title": "Audio Inpainting via $\\ell_1$-Minimization and Dictionary Learning", "abstract": "Audio inpainting refers to signal processing techniques that aim at restoring\nmissing or corrupted consecutive samples in audio signals. Prior works have\nshown that $\\ell_1$- minimization with appropriate weighting is capable of\nsolving audio inpainting problems, both for the analysis and the synthesis\nmodels. These models assume that audio signals are sparse with respect to some\nredundant dictionary and exploit that sparsity for inpainting purposes.\nRemaining within the sparsity framework, we utilize dictionary learning to\nfurther increase the sparsity and combine it with weighted\n$\\ell_1$-minimization adapted for audio inpainting to compensate for the loss\nof energy within the gap after restoration. Our experiments demonstrate that\nour approach is superior in terms of signal-to-distortion ratio (SDR) and\nobjective difference grade (ODG) compared with its original counterpart.", "published": "2022-02-15 14:47:07", "link": "http://arxiv.org/abs/2202.07479v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SpaIn-Net: Spatially-Informed Stereophonic Music Source Separation", "abstract": "With the recent advancements of data driven approaches using deep neural\nnetworks, music source separation has been formulated as an instrument-specific\nsupervised problem. While existing deep learning models implicitly absorb the\nspatial information conveyed by the multi-channel input signals, we argue that\na more explicit and active use of spatial information could not only improve\nthe separation process but also provide an entry-point for many\nuser-interaction based tools. To this end, we introduce a control method based\non the stereophonic location of the sources of interest, expressed as the\npanning angle. We present various conditioning mechanisms, including the use of\nraw angle and its derived feature representations, and show that spatial\ninformation helps. Our proposed approaches improve the separation performance\ncompared to location agnostic architectures by 1.8 dB SI-SDR in our Slakh-based\nsimulated experiments. Furthermore, the proposed methods allow for the\ndisentanglement of same-class instruments, for example, in mixtures containing\ntwo guitar tracks. Finally, we also demonstrate that our approach is robust to\nincorrect source panning information, which can be incurred by our proposed\nuser interaction.", "published": "2022-02-15 15:49:47", "link": "http://arxiv.org/abs/2202.07523v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised word-level prosody tagging for controllable speech\n  synthesis", "abstract": "Although word-level prosody modeling in neural text-to-speech (TTS) has been\ninvestigated in recent research for diverse speech synthesis, it is still\nchallenging to control speech synthesis manually without a specific reference.\nThis is largely due to lack of word-level prosody tags. In this work, we\npropose a novel approach for unsupervised word-level prosody tagging with two\nstages, where we first group the words into different types with a decision\ntree according to their phonetic content and then cluster the prosodies using\nGMM within each type of words separately. This design is based on the\nassumption that the prosodies of different type of words, such as long or short\nwords, should be tagged with different label sets. Furthermore, a TTS system\nwith the derived word-level prosody tags is trained for controllable speech\nsynthesis. Experiments on LJSpeech show that the TTS model trained with\nword-level prosody tags not only achieves better naturalness than a typical\nFastSpeech2 model, but also gains the ability to manipulate word-level prosody.", "published": "2022-02-15 05:28:23", "link": "http://arxiv.org/abs/2202.07200v2", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-style Training for South African Call Centre Audio", "abstract": "Mismatched data is a challenging problem for automatic speech recognition\n(ASR) systems. One of the most common techniques used to address mismatched\ndata is multi-style training (MTR), a form of data augmentation that attempts\nto transform the training data to be more representative of the testing data;\nand to learn robust representations applicable to different conditions. This\ntask can be very challenging if the test conditions are unknown. We explore the\nimpact of different MTR styles on system performance when testing conditions\nare different from training conditions in the context of deep neural network\nhidden Markov model (DNN-HMM) ASR systems. A controlled environment is created\nusing the LibriSpeech corpus, where we isolate the effect of different MTR\nstyles on final system performance. We evaluate our findings on a South African\ncall centre dataset that contains noisy, WAV49-encoded audio.", "published": "2022-02-15 06:22:11", "link": "http://arxiv.org/abs/2202.07219v1", "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SpeechPainter: Text-conditioned Speech Inpainting", "abstract": "We propose SpeechPainter, a model for filling in gaps of up to one second in\nspeech samples by leveraging an auxiliary textual input. We demonstrate that\nthe model performs speech inpainting with the appropriate content, while\nmaintaining speaker identity, prosody and recording environment conditions, and\ngeneralizing to unseen speakers. Our approach significantly outperforms\nbaselines constructed using adaptive TTS, as judged by human raters in\nside-by-side preference and MOS tests.", "published": "2022-02-15 09:33:30", "link": "http://arxiv.org/abs/2202.07273v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Phase Vocoder Done Right", "abstract": "The phase vocoder (PV) is a widely spread technique for processing audio\nsignals. It employs a short-time Fourier transform (STFT)\nanalysis-modify-synthesis loop and is typically used for time-scaling of\nsignals by means of using different time steps for STFT analysis and synthesis.\nThe main challenge of PV used for that purpose is the correction of the STFT\nphase. In this paper, we introduce a novel method for phase correction based on\nphase gradient estimation and its integration. The method does not require\nexplicit peak picking and tracking nor does it require detection of transients\nand their separate treatment. Yet, the method does not suffer from the typical\nphase vocoder artifacts even for extreme time stretching factors.", "published": "2022-02-15 13:20:14", "link": "http://arxiv.org/abs/2202.07382v1", "categories": ["cs.SD", "cs.MS", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning Contextually Fused Audio-visual Representations for\n  Audio-visual Speech Recognition", "abstract": "With the advance in self-supervised learning for audio and visual modalities,\nit has become possible to learn a robust audio-visual speech representation.\nThis would be beneficial for improving the audio-visual speech recognition\n(AVSR) performance, as the multi-modal inputs contain more fruitful information\nin principle. In this paper, based on existing self-supervised representation\nlearning methods for audio modality, we therefore propose an audio-visual\nrepresentation learning approach. The proposed approach explores both the\ncomplementarity of audio-visual modalities and long-term context dependency\nusing a transformer-based fusion module and a flexible masking strategy. After\npre-training, the model is able to extract fused representations required by\nAVSR. Without loss of generality, it can be applied to single-modal tasks, e.g.\naudio/visual speech recognition by simply masking out one modality in the\nfusion module. The proposed pre-trained model is evaluated on speech\nrecognition and lipreading tasks using one or two modalities, where the\nsuperiority is revealed.", "published": "2022-02-15 14:15:58", "link": "http://arxiv.org/abs/2202.07428v2", "categories": ["eess.IV", "cs.SD", "eess.AS"], "primary_category": "eess.IV"}
{"title": "Phase-Based Signal Representations for Scattering", "abstract": "The scattering transform is a non-linear signal representation method based\non cascaded wavelet transform magnitudes. In this paper we introduce phase\nscattering, a novel approach where we use phase derivatives in a scattering\nprocedure. We first revisit phase-related concepts for representing\ntime-frequency information of audio signals, in particular, the partial\nderivatives of the phase in the time-frequency domain. By putting analytical\nand numerical results in a new light, we set the basis to extend the\nphase-based representations to higher orders by means of a scattering\ntransform, which leads to well localized signal representations of large-scale\nstructures. All the ideas are introduced in a general way and then applied\nusing the STFT.", "published": "2022-02-15 14:51:58", "link": "http://arxiv.org/abs/2202.07484v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Non-iterative Filter Bank Phase (Re)Construction", "abstract": "Signal reconstruction from magnitude-only measurements presents a\nlong-standing problem in signal processing. In this contribution, we propose a\nphase (re)construction method for filter banks with uniform decimation and\ncontrolled frequency variation. The suggested procedure extends the recently\nintroduced phase-gradient heap integration and relies on a phase-magnitude\nrelationship for filter bank coefficients obtained from Gaussian filters.\nAdmissible filter banks are modeled as the discretization of certain\ngeneralized translation-invariant systems, for which we derive the\nphase-magnitude relationship explicitly. The implementation for discrete\nsignals is described and the performance of the algorithm is evaluated on a\nrange of real and synthetic signals.", "published": "2022-02-15 15:11:54", "link": "http://arxiv.org/abs/2202.07498v1", "categories": ["cs.SD", "cs.MS", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Speech Denoising in the Waveform Domain with Self-Attention", "abstract": "In this work, we present CleanUNet, a causal speech denoising model on the\nraw waveform. The proposed model is based on an encoder-decoder architecture\ncombined with several self-attention blocks to refine its bottleneck\nrepresentations, which is crucial to obtain good results. The model is\noptimized through a set of losses defined over both waveform and\nmulti-resolution spectrograms. The proposed method outperforms the\nstate-of-the-art models in terms of denoised speech quality from various\nobjective and subjective evaluation metrics. We release our code and models at\nhttps://github.com/nvidia/cleanunet.", "published": "2022-02-15 23:44:02", "link": "http://arxiv.org/abs/2202.07790v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automatic Depression Detection: An Emotional Audio-Textual Corpus and a\n  GRU/BiLSTM-based Model", "abstract": "Depression is a global mental health problem, the worst case of which can\nlead to suicide. An automatic depression detection system provides great help\nin facilitating depression self-assessment and improving diagnostic accuracy.\nIn this work, we propose a novel depression detection approach utilizing speech\ncharacteristics and linguistic contents from participants' interviews. In\naddition, we establish an Emotional Audio-Textual Depression Corpus\n(EATD-Corpus) which contains audios and extracted transcripts of responses from\ndepressed and non-depressed volunteers. To the best of our knowledge,\nEATD-Corpus is the first and only public depression dataset that contains audio\nand text data in Chinese. Evaluated on two depression datasets, the proposed\nmethod achieves the state-of-the-art performances. The outperforming results\ndemonstrate the effectiveness and generalization ability of the proposed\nmethod. The source code and EATD-Corpus are available at\nhttps://github.com/speechandlanguageprocessing/ICASSP2022-Depression.", "published": "2022-02-15 03:29:39", "link": "http://arxiv.org/abs/2202.08210v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "q-bio.QM"], "primary_category": "eess.AS"}
{"title": "General-purpose, long-context autoregressive modeling with Perceiver AR", "abstract": "Real-world data is high-dimensional: a book, image, or musical performance\ncan easily contain hundreds of thousands of elements even after compression.\nHowever, the most commonly used autoregressive models, Transformers, are\nprohibitively expensive to scale to the number of inputs and layers needed to\ncapture this long-range structure. We develop Perceiver AR, an autoregressive,\nmodality-agnostic architecture which uses cross-attention to map long-range\ninputs to a small number of latents while also maintaining end-to-end causal\nmasking. Perceiver AR can directly attend to over a hundred thousand tokens,\nenabling practical long-context density estimation without the need for\nhand-crafted sparsity patterns or memory mechanisms. When trained on images or\nmusic, Perceiver AR generates outputs with clear long-term coherence and\nstructure. Our architecture also obtains state-of-the-art likelihood on\nlong-sequence benchmarks, including 64 x 64 ImageNet images and PG-19 books.", "published": "2022-02-15 22:31:42", "link": "http://arxiv.org/abs/2202.07765v2", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
