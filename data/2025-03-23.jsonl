{"title": "A unified convention for achievement positional games", "abstract": "We introduce achievement positional games, a convention for positional games\nwhich encompasses the Maker-Maker and Maker-Breaker conventions. We consider\ntwo hypergraphs, one red and one blue, on the same vertex set. Two players,\nLeft and Right, take turns picking a previously unpicked vertex. Whoever first\nfills an edge of their color, blue for Left or red for Right, wins the game\n(draws are possible). We establish general properties of such games. In\nparticular, we show that a lot of principles which hold for Maker-Maker games\ngeneralize to achievement positional games. We also study the algorithmic\ncomplexity of deciding whether Left has a winning strategy as first player when\nall blue edges have size at mot $p$ and all red edges have size at most $q$.\nThis problem is in P for $p,q \\leq 2$, but it is NP-hard for $p \\geq 3$ and\n$q=2$, coNP-complete for $p=2$ and $q \\geq 3$, and PSPACE-complete for $p,q\n\\geq 3$. A consequence of this last result is that, in the Maker-Maker\nconvention, deciding whether the first player has a winning strategy on a\nhypergraph of rank 4 after one round of (non-optimal) play is PSPACE-complete.", "published": "2025-03-23 18:27:13", "link": "http://arxiv.org/abs/2503.18163v1", "categories": ["cs.DM", "math.CO"], "primary_category": "cs.DM"}
{"title": "Linear, nested, and quadratic ordered measures: Computation and incorporation into optimization problems", "abstract": "In this paper we address a unified mathematical optimization framework to\ncompute a wide range of measures used in most operations research and data\nscience contexts. The goal is to embed such metrics within general optimization\nmodels allowing their efficient computation. We assess the usefulness of this\napproach applying it to three different families of measures, namely linear,\nnested, and quadratic ordered measures. Computational results are reported\nshowing the efficiency and accuracy of our methods as compared with standard\nimplementations in numerical software packages. Finally, we illustrate this\nmethodology by computing a number of optimal solutions with respect to\ndifferent metrics on three well-known linear and combinatorial optimization\nproblems: scenario analysis in linear programming, the traveling salesman and\nthe weighted multicover set problem.", "published": "2025-03-23 15:04:38", "link": "http://arxiv.org/abs/2503.18097v1", "categories": ["math.OC", "cs.DM", "stat.CO", "90C", "G.2.0"], "primary_category": "math.OC"}
{"title": "Unleashing the power of text for credit default prediction: Comparing human-written and generative AI-refined texts", "abstract": "This study explores the integration of a representative large language model,\nChatGPT, into lending decision-making with a focus on credit default\nprediction. Specifically, we use ChatGPT to analyse and interpret loan\nassessments written by loan officers and generate refined versions of these\ntexts. Our comparative analysis reveals significant differences between\ngenerative artificial intelligence (AI)-refined and human-written texts in\nterms of text length, semantic similarity, and linguistic representations.\nUsing deep learning techniques, we show that incorporating unstructured text\ndata, particularly ChatGPT-refined texts, alongside conventional structured\ndata significantly enhances credit default predictions. Furthermore, we\ndemonstrate how the contents of both human-written and ChatGPT-refined\nassessments contribute to the models' prediction and show that the effect of\nessential words is highly context-dependent. Moreover, we find that ChatGPT's\nanalysis of borrower delinquency contributes the most to improving predictive\naccuracy. We also evaluate the business impact of the models based on\nhuman-written and ChatGPT-refined texts, and find that, in most cases, the\nlatter yields higher profitability than the former. This study provides\nvaluable insights into the transformative potential of generative AI in\nfinancial services.", "published": "2025-03-23 11:07:24", "link": "http://arxiv.org/abs/2503.18029v1", "categories": ["q-fin.RM", "q-fin.CP"], "primary_category": "q-fin.RM"}
{"title": "Financial Wind Tunnel: A Retrieval-Augmented Market Simulator", "abstract": "Market simulator tries to create high-quality synthetic financial data that\nmimics real-world market dynamics, which is crucial for model development and\nrobust assessment. Despite continuous advancements in simulation methodologies,\nmarket fluctuations vary in terms of scale and sources, but existing frameworks\noften excel in only specific tasks. To address this challenge, we propose\nFinancial Wind Tunnel (FWT), a retrieval-augmented market simulator designed to\ngenerate controllable, reasonable, and adaptable market dynamics for model\ntesting. FWT offers a more comprehensive and systematic generative capability\nacross different data frequencies. By leveraging a retrieval method to discover\ncross-sectional information as the augmented condition, our diffusion-based\nsimulator seamlessly integrates both macro- and micro-level market patterns.\nFurthermore, our framework allows the simulation to be controlled with wide\napplicability, including causal generation through \"what-if\" prompts or\nunprecedented cross-market trend synthesis. Additionally, we develop an\nautomated optimizer for downstream quantitative models, using stress testing of\nsimulated scenarios via FWT to enhance returns while controlling risks.\nExperimental results demonstrate that our approach enables the generalizable\nand reliable market simulation, significantly improve the performance and\nadaptability of downstream models, particularly in highly complex and volatile\nmarket conditions. Our code and data sample is available at\nhttps://anonymous.4open.science/r/fwt_-E852", "published": "2025-03-23 03:10:13", "link": "http://arxiv.org/abs/2503.17909v1", "categories": ["cs.CE", "cs.LG", "q-fin.CP"], "primary_category": "cs.CE"}
{"title": "Agent-Based Models for Two Stocks with Superhedging", "abstract": "An agent-based modelling methodology for the joint price evolution of two\nstocks is put forward. The method models future multidimensional price\ntrajectories reflecting how a class of agents rebalance their portfolios in an\noperational way by reacting to how stocks' charts unfold. Prices are expressed\nin units of a third stock that acts as numeraire. The methodology is robust, in\nparticular, it does not depend on any prior probability or analytical\nassumptions and it is based on constructing scenarios/trajectories. A main\ningredient is a superhedging interpretation that provides relative superhedging\nprices between the two modelled stocks. The operational nature of the\nmethodology gives objective conditions for the validity of the model and so\nimplies realistic risk-rewards profiles for the agent's operations.\nSuperhedging computations are performed with a dynamic programming algorithm\ndeployed on a graph data structure. Null subsets of the trajectory space are\ndirectly related to arbitrage opportunities (i.e. there is no need for\nprobabilistic considerations) that may emerge during the trajectory set\nconstruction. It follows that the superhedging algorithm handles null sets in a\nrigorous and intuitive way. Superhedging and underhedging bounds are kept\nrelevant to the investor by means of a worst case pruning method and, as an\nalternative, a theory supported pruning that relies on a new notion of small\narbitrage.", "published": "2025-03-23 18:33:59", "link": "http://arxiv.org/abs/2503.18165v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Generating realistic metaorders from public data", "abstract": "This paper introduces a novel algorithm for generating realistic metaorders\nfrom public trade data, addressing a longstanding challenge in price impact\nresearch that has traditionally relied on proprietary datasets. Our method\neffectively recovers all established stylized facts of metaorders impact, such\nas the Square Root Law, the concave profile during metaorder execution, and the\npost-execution decay. This algorithm not only overcomes the dependence on\nproprietary data, a major barrier to research reproducibility, but also enables\nthe creation of larger and more robust datasets that may increase the quality\nof empirical studies. Our findings strongly suggest that average realized\nshort-term price impact is not due to information revelation (as in the Kyle\nframework) but has a mechanical origin which could explain the universality of\nthe Square Root Law.", "published": "2025-03-23 20:47:24", "link": "http://arxiv.org/abs/2503.18199v2", "categories": ["q-fin.TR", "q-fin.PM"], "primary_category": "q-fin.TR"}
{"title": "Informer in Algorithmic Investment Strategies on High Frequency Bitcoin Data", "abstract": "The article investigates the usage of Informer architecture for building\nautomated trading strategies for high frequency Bitcoin data. Three strategies\nusing Informer model with different loss functions: Root Mean Squared Error\n(RMSE), Generalized Mean Absolute Directional Loss (GMADL) and Quantile loss,\nare proposed and evaluated against the Buy and Hold benchmark and two benchmark\nstrategies based on technical indicators. The evaluation is conducted using\ndata of various frequencies: 5 minute, 15 minute, and 30 minute intervals, over\nthe 6 different periods. Although the Informer-based model with Quantile loss\ndid not outperform the benchmark, two other models achieved better results. The\nperformance of the model using RMSE loss worsens when used with higher\nfrequency data while the model that uses novel GMADL loss function is\nbenefiting from higher frequency data and when trained on 5 minute interval it\nbeat all the other strategies on most of the testing periods. The primary\ncontribution of this study is the application and assessment of the RMSE,\nGMADL, and Quantile loss functions with the Informer model to forecast future\nreturns, subsequently using these forecasts to develop automated trading\nstrategies. The research provides evidence that employing an Informer model\ntrained with the GMADL loss function can result in superior trading outcomes\ncompared to the buy-and-hold approach.", "published": "2025-03-23 15:00:13", "link": "http://arxiv.org/abs/2503.18096v1", "categories": ["q-fin.TR", "cs.LG", "cs.NE", "q-fin.PM"], "primary_category": "q-fin.TR"}
{"title": "A Simple Strategy to Deal with Toxic Flow", "abstract": "We model the trading activity between a broker and her clients (informed and\nuninformed traders) as an infinite-horizon stochastic control problem. We\nderive the broker's optimal dealing strategy in closed form and use this to\nintroduce an algorithm that bypasses the need to calibrate individual\nparameters, so the dealing strategy can be executed in real-world trading\nenvironments. Finally, we characterise the discount in the price of liquidity a\nbroker offers clients. The discount strikes the optimal balance between\nmaximising the order flow from the broker's clients and minimising adverse\nselection losses to the informed traders.", "published": "2025-03-23 09:39:18", "link": "http://arxiv.org/abs/2503.18005v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
