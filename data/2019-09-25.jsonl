{"title": "Independent Generative Adversarial Self-Imitation Learning in Cooperative Multiagent Systems", "abstract": "Many tasks in practice require the collaboration of multiple agents through reinforcement learning. In general, cooperative multiagent reinforcement learning algorithms can be classified into two paradigms: Joint Action Learners (JALs) and Independent Learners (ILs). In many practical applications, agents are unable to observe other agents' actions and rewards, making JALs inapplicable. In this work, we focus on independent learning paradigm in which each agent makes decisions based on its local observations only. However, learning is challenging in independent settings due to the local viewpoints of all agents, which perceive the world as a non-stationary environment due to the concurrently exploring teammates. In this paper, we propose a novel framework called Independent Generative Adversarial Self-Imitation Learning (IGASIL) to address the coordination problems in fully cooperative multiagent environments. To the best of our knowledge, we are the first to combine self-imitation learning with generative adversarial imitation learning (GAIL) and apply it to cooperative multiagent systems. Besides, we put forward a Sub-Curriculum Experience Replay mechanism to pick out the past beneficial experiences as much as possible and accelerate the self-imitation learning process. Evaluations conducted in the testbed of StarCraft unit micromanagement and a commonly adopted benchmark show that our IGASIL produces state-of-the-art results and even outperforms JALs in terms of both convergence speed and final performance.", "published": "2019-09-25 13:11:33", "link": "http://arxiv.org/abs/1909.11468v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "$\u03b1^\u03b1$-Rank: Practically Scaling $\u03b1$-Rank through Stochastic Optimisation", "abstract": "Recently, $\u03b1$-Rank, a graph-based algorithm, has been proposed as a solution to ranking joint policy profiles in large scale multi-agent systems. $\u03b1$-Rank claimed tractability through a polynomial time implementation with respect to the total number of pure strategy profiles. Here, we note that inputs to the algorithm were not clearly specified in the original presentation; as such, we deem complexity claims as not grounded, and conjecture solving $\u03b1$-Rank is NP-hard. The authors of $\u03b1$-Rank suggested that the input to $\u03b1$-Rank can be an exponentially-sized payoff matrix; a claim promised to be clarified in subsequent manuscripts. Even though $\u03b1$-Rank exhibits a polynomial-time solution with respect to such an input, we further reflect additional critical problems. We demonstrate that due to the need of constructing an exponentially large Markov chain, $\u03b1$-Rank is infeasible beyond a small finite number of agents. We ground these claims by adopting amount of dollars spent as a non-refutable evaluation metric. Realising such scalability issue, we present a stochastic implementation of $\u03b1$-Rank with a double oracle mechanism allowing for reductions in joint strategy spaces. Our method, $\u03b1^\u03b1$-Rank, does not need to save exponentially-large transition matrix, and can terminate early under required precision. Although theoretically our method exhibits similar worst-case complexity guarantees compared to $\u03b1$-Rank, it allows us, for the first time, to practically conduct large-scale multi-agent evaluations. On $10^4 \\times 10^4$ random matrices, we achieve $1000x$ speed reduction. Furthermore, we also show successful results on large joint strategy profiles with a maximum size in the order of $\\mathcal{O}(2^{25})$ ($\\approx 33$ million joint strategies) -- a setting not evaluable using $\u03b1$-Rank with reasonable computational budget.", "published": "2019-09-25 17:21:45", "link": "http://arxiv.org/abs/1909.11628v6", "categories": ["cs.MA", "cs.LG"], "primary_category": "cs.MA"}
