{"title": "Question Answering is a Format; When is it Useful?", "abstract": "Recent years have seen a dramatic expansion of tasks and datasets posed as\nquestion answering, from reading comprehension, semantic role labeling, and\neven machine translation, to image and video understanding. With this\nexpansion, there are many differing views on the utility and definition of\n\"question answering\" itself. Some argue that its scope should be narrow, or\nbroad, or that it is overused in datasets today. In this opinion piece, we\nargue that question answering should be considered a format which is sometimes\nuseful for studying particular phenomena, not a phenomenon or task in itself.\nWe discuss when a task is correctly described as question answering, and when a\ntask is usefully posed as question answering, instead of using some other\nformat.", "published": "2019-09-25 05:16:15", "link": "http://arxiv.org/abs/1909.11291v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Detect Opinion Snippet for Aspect-Based Sentiment Analysis", "abstract": "Aspect-based sentiment analysis (ABSA) is to predict the sentiment polarity\ntowards a particular aspect in a sentence. Recently, this task has been widely\naddressed by the neural attention mechanism, which computes attention weights\nto softly select words for generating aspect-specific sentence representations.\nThe attention is expected to concentrate on opinion words for accurate\nsentiment prediction. However, attention is prone to be distracted by noisy or\nmisleading words, or opinion words from other aspects. In this paper, we\npropose an alternative hard-selection approach, which determines the start and\nend positions of the opinion snippet, and selects the words between these two\npositions for sentiment prediction. Specifically, we learn deep associations\nbetween the sentence and aspect, and the long-term dependencies within the\nsentence by leveraging the pre-trained BERT model. We further detect the\nopinion snippet by self-critical reinforcement learning. Especially,\nexperimental results demonstrate the effectiveness of our method and prove that\nour hard-selection approach outperforms soft-selection approaches when handling\nmulti-aspect sentences.", "published": "2019-09-25 05:43:28", "link": "http://arxiv.org/abs/1909.11297v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tackling Long-Tailed Relations and Uncommon Entities in Knowledge Graph\n  Completion", "abstract": "For large-scale knowledge graphs (KGs), recent research has been focusing on\nthe large proportion of infrequent relations which have been ignored by\nprevious studies. For example few-shot learning paradigm for relations has been\ninvestigated. In this work, we further advocate that handling uncommon entities\nis inevitable when dealing with infrequent relations. Therefore, we propose a\nmeta-learning framework that aims at handling infrequent relations with\nfew-shot learning and uncommon entities by using textual descriptions. We\ndesign a novel model to better extract key information from textual\ndescriptions. Besides, we also develop a novel generative model in our\nframework to enhance the performance by generating extra triplets during the\ntraining stage. Experiments are conducted on two datasets from real-world KGs,\nand the results show that our framework outperforms previous methods when\ndealing with infrequent relations and their accompanying uncommon entities.", "published": "2019-09-25 09:20:39", "link": "http://arxiv.org/abs/1909.11359v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Developing a Fine-Grained Corpus for a Less-resourced Language: the case\n  of Kurdish", "abstract": "Kurdish is a less-resourced language consisting of different dialects written\nin various scripts. Approximately 30 million people in different countries\nspeak the language. The lack of corpora is one of the main obstacles in Kurdish\nlanguage processing. In this paper, we present KTC-the Kurdish Textbooks\nCorpus, which is composed of 31 K-12 textbooks in Sorani dialect. The corpus is\nnormalized and categorized into 12 educational subjects containing 693,800\ntokens (110,297 types). Our resource is publicly available for non-commercial\nuse under the CC BY-NC-SA 4.0 license.", "published": "2019-09-25 13:10:15", "link": "http://arxiv.org/abs/1909.11467v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-supervised Text Style Transfer: Cross Projection in Latent Space", "abstract": "Text style transfer task requires the model to transfer a sentence of one\nstyle to another style while retaining its original content meaning, which is a\nchallenging problem that has long suffered from the shortage of parallel data.\nIn this paper, we first propose a semi-supervised text style transfer model\nthat combines the small-scale parallel data with the large-scale nonparallel\ndata. With these two types of training data, we introduce a projection function\nbetween the latent space of different styles and design two constraints to\ntrain it. We also introduce two other simple but effective semi-supervised\nmethods to compare with. To evaluate the performance of the proposed methods,\nwe build and release a novel style transfer dataset that alters sentences\nbetween the style of ancient Chinese poem and the modern Chinese.", "published": "2019-09-25 13:46:29", "link": "http://arxiv.org/abs/1909.11493v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning A Unified Named Entity Tagger From Multiple Partially Annotated\n  Corpora For Efficient Adaptation", "abstract": "Named entity recognition (NER) identifies typed entity mentions in raw text.\nWhile the task is well-established, there is no universally used tagset: often,\ndatasets are annotated for use in downstream applications and accordingly only\ncover a small set of entity types relevant to a particular task. For instance,\nin the biomedical domain, one corpus might annotate genes, another chemicals,\nand another diseases---despite the texts in each corpus containing references\nto all three types of entities. In this paper, we propose a deep structured\nmodel to integrate these \"partially annotated\" datasets to jointly identify all\nentity types appearing in the training corpora. By leveraging multiple\ndatasets, the model can learn robust input representations; by building a joint\nstructured model, it avoids potential conflicts caused by combining several\nmodels' predictions at test time. Experiments show that the proposed model\nsignificantly outperforms strong multi-task learning baselines when training on\nmultiple, partially annotated datasets and testing on datasets that contain\ntags from more than one of the training corpora.", "published": "2019-09-25 14:53:57", "link": "http://arxiv.org/abs/1909.11535v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extremely Small BERT Models from Mixed-Vocabulary Training", "abstract": "Pretrained language models like BERT have achieved good results on NLP tasks,\nbut are impractical on resource-limited devices due to memory footprint. A\nlarge fraction of this footprint comes from the input embeddings with large\ninput vocabulary and embedding dimensions. Existing knowledge distillation\nmethods used for model compression cannot be directly applied to train student\nmodels with reduced vocabulary sizes. To this end, we propose a distillation\nmethod to align the teacher and student embeddings via mixed-vocabulary\ntraining. Our method compresses BERT-LARGE to a task-agnostic model with\nsmaller vocabulary and hidden dimensions, which is an order of magnitude\nsmaller than other distilled BERT models and offers a better size-accuracy\ntrade-off on language understanding benchmarks as well as a practical dialogue\ntask.", "published": "2019-09-25 18:07:35", "link": "http://arxiv.org/abs/1909.11687v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Atalaya at TASS 2019: Data Augmentation and Robust Embeddings for\n  Sentiment Analysis", "abstract": "In this article we describe our participation in TASS 2019, a shared task\naimed at the detection of sentiment polarity of Spanish tweets. We combined\ndifferent representations such as bag-of-words, bag-of-characters, and tweet\nembeddings. In particular, we trained robust subword-aware word embeddings and\ncomputed tweet representations using a weighted-averaging strategy. We also\nused two data augmentation techniques to deal with data scarcity: two-way\ntranslation augmentation, and instance crossover augmentation, a novel\ntechnique that generates new instances by combining halves of tweets. In\nexperiments, we trained linear classifiers and ensemble models, obtaining\nhighly competitive results despite the simplicity of our approaches.", "published": "2019-09-25 00:28:50", "link": "http://arxiv.org/abs/1909.11241v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TalkDown: A Corpus for Condescension Detection in Context", "abstract": "Condescending language use is caustic; it can bring dialogues to an end and\nbifurcate communities. Thus, systems for condescension detection could have a\nlarge positive impact. A challenge here is that condescension is often\nimpossible to detect from isolated utterances, as it depends on the discourse\nand social context. To address this, we present TalkDown, a new labeled dataset\nof condescending linguistic acts in context. We show that extending a\nlanguage-only model with representations of the discourse improves performance,\nand we motivate techniques for dealing with the low rates of condescension\noverall. We also use our model to estimate condescension rates in various\nonline communities and relate these differences to differing community norms.", "published": "2019-09-25 03:39:00", "link": "http://arxiv.org/abs/1909.11272v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Task-Oriented Conversation Generation Using Heterogeneous Memory\n  Networks", "abstract": "How to incorporate external knowledge into a neural dialogue model is\ncritically important for dialogue systems to behave like real humans. To handle\nthis problem, memory networks are usually a great choice and a promising way.\nHowever, existing memory networks do not perform well when leveraging\nheterogeneous information from different sources. In this paper, we propose a\nnovel and versatile external memory networks called Heterogeneous Memory\nNetworks (HMNs), to simultaneously utilize user utterances, dialogue history\nand background knowledge tuples. In our method, historical sequential dialogues\nare encoded and stored into the context-aware memory enhanced by gating\nmechanism while grounding knowledge tuples are encoded and stored into the\ncontext-free memory. During decoding, the decoder augmented with HMNs\nrecurrently selects each word in one response utterance from these two memories\nand a general vocabulary. Experimental results on multiple real-world datasets\nshow that HMNs significantly outperform the state-of-the-art data-driven\ntask-oriented dialogue models in most domains.", "published": "2019-09-25 04:40:27", "link": "http://arxiv.org/abs/1909.11287v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Dimensional Explanation of Target Variables from Documents", "abstract": "Automated predictions require explanations to be interpretable by humans.\nPast work used attention and rationale mechanisms to find words that predict\nthe target variable of a document. Often though, they result in a tradeoff\nbetween noisy explanations or a drop in accuracy. Furthermore, rationale\nmethods cannot capture the multi-faceted nature of justifications for multiple\ntargets, because of the non-probabilistic nature of the mask. In this paper, we\npropose the Multi-Target Masker (MTM) to address these shortcomings. The\nnovelty lies in the soft multi-dimensional mask that models a relevance\nprobability distribution over the set of target variables to handle\nambiguities. Additionally, two regularizers guide MTM to induce long,\nmeaningful explanations. We evaluate MTM on two datasets and show, using\nstandard metrics and human annotations, that the resulting masks are more\naccurate and coherent than those generated by the state-of-the-art methods.\nMoreover, MTM is the first to also achieve the highest F1 scores for all the\ntarget variables simultaneously.", "published": "2019-09-25 10:26:36", "link": "http://arxiv.org/abs/1909.11386v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Breaking the Data Barrier: Towards Robust Speech Translation via\n  Adversarial Stability Training", "abstract": "In a pipeline speech translation system, automatic speech recognition (ASR)\nsystem will transmit errors in recognition to the downstream machine\ntranslation (MT) system. A standard machine translation system is usually\ntrained on parallel corpus composed of clean text and will perform poorly on\ntext with recognition noise, a gap well known in speech translation community.\nIn this paper, we propose a training architecture which aims at making a neural\nmachine translation model more robust against speech recognition errors. Our\napproach addresses the encoder and the decoder simultaneously using adversarial\nlearning and data augmentation, respectively. Experimental results on IWSLT2018\nspeech translation task show that our approach can bridge the gap between the\nASR output and the MT input, outperforms the baseline by up to 2.83 BLEU on\nnoisy ASR output, while maintaining close performance on clean text.", "published": "2019-09-25 12:13:43", "link": "http://arxiv.org/abs/1909.11430v3", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "FreeLB: Enhanced Adversarial Training for Natural Language Understanding", "abstract": "Adversarial training, which minimizes the maximal risk for label-preserving\ninput perturbations, has proved to be effective for improving the\ngeneralization of language models. In this work, we propose a novel adversarial\ntraining algorithm, FreeLB, that promotes higher invariance in the embedding\nspace, by adding adversarial perturbations to word embeddings and minimizing\nthe resultant adversarial risk inside different regions around input samples.\nTo validate the effectiveness of the proposed approach, we apply it to\nTransformer-based models for natural language understanding and commonsense\nreasoning tasks. Experiments on the GLUE benchmark show that when applied only\nto the finetuning stage, it is able to improve the overall test scores of\nBERT-base model from 78.3 to 79.4, and RoBERTa-large model from 88.5 to 88.8.\nIn addition, the proposed approach achieves state-of-the-art single-model test\naccuracies of 85.44\\% and 67.75\\% on ARC-Easy and ARC-Challenge. Experiments on\nCommonsenseQA benchmark further demonstrate that FreeLB can be generalized and\nboost the performance of RoBERTa-large model on other tasks as well. Code is\navailable at \\url{https://github.com/zhuchen03/FreeLB .", "published": "2019-09-25 20:50:32", "link": "http://arxiv.org/abs/1909.11764v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PaRe: A Paper-Reviewer Matching Approach Using a Common Topic Space", "abstract": "Finding the right reviewers to assess the quality of conference submissions\nis a time consuming process for conference organizers. Given the importance of\nthis step, various automated reviewer-paper matching solutions have been\nproposed to alleviate the burden. Prior approaches, including bag-of-words\nmodels and probabilistic topic models have been inadequate to deal with the\nvocabulary mismatch and partial topic overlap between a paper submission and\nthe reviewer's expertise. Our approach, the common topic model, jointly models\nthe topics common to the submission and the reviewer's profile while relying on\nabstract topic vectors. Experiments and insightful evaluations on two datasets\ndemonstrate that the proposed method achieves consistent improvements compared\nto available state-of-the-art implementations of paper-reviewer matching.", "published": "2019-09-25 02:25:23", "link": "http://arxiv.org/abs/1909.11258v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Annotated Guidelines and Building Reference Corpus for Myanmar-English\n  Word Alignment", "abstract": "Reference corpus for word alignment is an important resource for developing\nand evaluating word alignment methods. For Myanmar-English language pairs,\nthere is no reference corpus to evaluate the word alignment tasks. Therefore,\nwe created the guidelines for Myanmar-English word alignment annotation between\ntwo languages over contrastive learning and built the Myanmar-English reference\ncorpus consisting of verified alignments from Myanmar ALT of the Asian Language\nTreebank (ALT). This reference corpus contains confident labels sure (S) and\npossible (P) for word alignments which are used to test for the purpose of\nevaluation of the word alignments tasks. We discuss the most linking\nambiguities to define consistent and systematic instructions to align manual\nwords. We evaluated the results of annotators agreement using our reference\ncorpus in terms of alignment error rate (AER) in word alignment tasks and\ndiscuss the words relationships in terms of BLEU scores.", "published": "2019-09-25 04:47:49", "link": "http://arxiv.org/abs/1909.11288v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Reducing Transformer Depth on Demand with Structured Dropout", "abstract": "Overparameterized transformer networks have obtained state of the art results\nin various natural language processing tasks, such as machine translation,\nlanguage modeling, and question answering. These models contain hundreds of\nmillions of parameters, necessitating a large amount of computation and making\nthem prone to overfitting. In this work, we explore LayerDrop, a form of\nstructured dropout, which has a regularization effect during training and\nallows for efficient pruning at inference time. In particular, we show that it\nis possible to select sub-networks of any depth from one large network without\nhaving to finetune them and with limited impact on performance. We demonstrate\nthe effectiveness of our approach by improving the state of the art on machine\ntranslation, language modeling, summarization, question answering, and language\nunderstanding benchmarks. Moreover, we show that our approach leads to small\nBERT-like models of higher quality compared to training from scratch or using\ndistillation.", "published": "2019-09-25 15:35:03", "link": "http://arxiv.org/abs/1909.11556v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Speech Recognition with Augmented Synthesized Speech", "abstract": "Recent success of the Tacotron speech synthesis architecture and its variants\nin producing natural sounding multi-speaker synthesized speech has raised the\nexciting possibility of replacing expensive, manually transcribed,\ndomain-specific, human speech that is used to train speech recognizers. The\nmulti-speaker speech synthesis architecture can learn latent embedding spaces\nof prosody, speaker and style variations derived from input acoustic\nrepresentations thereby allowing for manipulation of the synthesized speech. In\nthis paper, we evaluate the feasibility of enhancing speech recognition\nperformance using speech synthesis using two corpora from different domains. We\nexplore algorithms to provide the necessary acoustic and lexical diversity\nneeded for robust speech recognition. Finally, we demonstrate the feasibility\nof this approach as a data augmentation strategy for domain-transfer.\n  We find that improvements to speech recognition performance is achievable by\naugmenting training data with synthesized material. However, there remains a\nsubstantial gap in performance between recognizers trained on human speech\nthose trained on synthesized speech.", "published": "2019-09-25 18:32:50", "link": "http://arxiv.org/abs/1909.11699v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "The Power of Communities: A Text Classification Model with Automated\n  Labeling Process Using Network Community Detection", "abstract": "Text classification is one of the most critical areas in machine learning and\nartificial intelligence research. It has been actively adopted in many business\napplications such as conversational intelligence systems, news articles\ncategorizations, sentiment analysis, emotion detection systems, and many other\nrecommendation systems in our daily life. One of the problems in supervised\ntext classification models is that the models' performance depends heavily on\nthe quality of data labeling that is typically done by humans. In this study,\nwe propose a new network community detection-based approach to automatically\nlabel and classify text data into multiclass value spaces. Specifically, we\nbuild networks with sentences as the network nodes and pairwise cosine\nsimilarities between the Term Frequency-Inversed Document Frequency (TFIDF)\nvector representations of the sentences as the network link weights. We use the\nLouvain method to detect the communities in the sentence networks. We train and\ntest the Support Vector Machine and the Random Forest models on both the\nhuman-labeled data and network community detection labeled data. Results showed\nthat models with the data labeled by the network community detection\noutperformed the models with the human-labeled data by 2.68-3.75% of\nclassification accuracy. Our method may help developments of more accurate\nconversational intelligence and other text classification systems.", "published": "2019-09-25 18:43:22", "link": "http://arxiv.org/abs/1909.11706v3", "categories": ["cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "UNITER: UNiversal Image-TExt Representation Learning", "abstract": "Joint image-text embedding is the bedrock for most Vision-and-Language (V+L)\ntasks, where multimodality inputs are simultaneously processed for joint visual\nand textual understanding. In this paper, we introduce UNITER, a UNiversal\nImage-TExt Representation, learned through large-scale pre-training over four\nimage-text datasets (COCO, Visual Genome, Conceptual Captions, and SBU\nCaptions), which can power heterogeneous downstream V+L tasks with joint\nmultimodal embeddings. We design four pre-training tasks: Masked Language\nModeling (MLM), Masked Region Modeling (MRM, with three variants), Image-Text\nMatching (ITM), and Word-Region Alignment (WRA). Different from previous work\nthat applies joint random masking to both modalities, we use conditional\nmasking on pre-training tasks (i.e., masked language/region modeling is\nconditioned on full observation of image/text). In addition to ITM for global\nimage-text alignment, we also propose WRA via the use of Optimal Transport (OT)\nto explicitly encourage fine-grained alignment between words and image regions\nduring pre-training. Comprehensive analysis shows that both conditional masking\nand OT-based WRA contribute to better pre-training. We also conduct a thorough\nablation study to find an optimal combination of pre-training tasks. Extensive\nexperiments show that UNITER achieves new state of the art across six V+L tasks\n(over nine datasets), including Visual Question Answering, Image-Text\nRetrieval, Referring Expression Comprehension, Visual Commonsense Reasoning,\nVisual Entailment, and NLVR$^2$. Code is available at\nhttps://github.com/ChenRocks/UNITER.", "published": "2019-09-25 20:02:54", "link": "http://arxiv.org/abs/1909.11740v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "MPEG-H Audio for Improving Accessibility in Broadcasting and Streaming", "abstract": "Broadcasting and streaming services still suffer from various levels of\naccessibility barriers for a significant portion of the population, limiting\nthe access to information and culture, and in the most severe cases limiting\nthe empowerment of people. This paper provides a brief overview of some of the\nmost common accessibility barriers encountered. It then gives a short\nintroduction to object-based audio (OBA) production and transport, focusing on\nthe aspects relevant for lowering accessibility barriers. MPEG-H Audio is used\nas a concrete example of an OBA system already deployed. Two example cases\n(dialog enhancement and audio description) are used to demonstrate in detail\nthe simplicity of producing MPEG-H Audio content providing improved\naccessibility. Several other possibilities are outlined briefly. We show that\nusing OBA for broadcasting and streaming content allows offering several\naccessibility features in a flexible manner, requiring only small changes to\nthe existing production workflow, assuming the receiver supports the\nfunctionality.", "published": "2019-09-25 15:22:20", "link": "http://arxiv.org/abs/1909.11549v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "HumanGAN: generative adversarial network with human-based discriminator\n  and its evaluation in speech perception modeling", "abstract": "We propose the HumanGAN, a generative adversarial network (GAN) incorporating\nhuman perception as a discriminator. A basic GAN trains a generator to\nrepresent a real-data distribution by fooling the discriminator that\ndistinguishes real and generated data. Therefore, the basic GAN cannot\nrepresent the outside of a real-data distribution. In the case of speech\nperception, humans can recognize not only human voices but also processed\n(i.e., a non-existent human) voices as human voice. Such a human-acceptable\ndistribution is typically wider than a real-data one and cannot be modeled by\nthe basic GAN. To model the human-acceptable distribution, we formulate a\nbackpropagation-based generator training algorithm by regarding human\nperception as a black-boxed discriminator. The training efficiently iterates\ngenerator training by using a computer and discrimination by crowdsourcing. We\nevaluate our HumanGAN in speech naturalness modeling and demonstrate that it\ncan represent a human-acceptable distribution that is wider than a real-data\ndistribution.", "published": "2019-09-25 10:32:41", "link": "http://arxiv.org/abs/1909.11391v1", "categories": ["cs.SD", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
{"title": "High Fidelity Speech Synthesis with Adversarial Networks", "abstract": "Generative adversarial networks have seen rapid development in recent years\nand have led to remarkable improvements in generative modelling of images.\nHowever, their application in the audio domain has received limited attention,\nand autoregressive models, such as WaveNet, remain the state of the art in\ngenerative modelling of audio signals such as human speech. To address this\npaucity, we introduce GAN-TTS, a Generative Adversarial Network for\nText-to-Speech. Our architecture is composed of a conditional feed-forward\ngenerator producing raw speech audio, and an ensemble of discriminators which\noperate on random windows of different sizes. The discriminators analyse the\naudio both in terms of general realism, as well as how well the audio\ncorresponds to the utterance that should be pronounced. To measure the\nperformance of GAN-TTS, we employ both subjective human evaluation (MOS - Mean\nOpinion Score), as well as novel quantitative metrics (Fr\\'echet DeepSpeech\nDistance and Kernel DeepSpeech Distance), which we find to be well correlated\nwith MOS. We show that GAN-TTS is capable of generating high-fidelity speech\nwith naturalness comparable to the state-of-the-art models, and unlike\nautoregressive models, it is highly parallelisable thanks to an efficient\nfeed-forward generator. Listen to GAN-TTS reading this abstract at\nhttps://storage.googleapis.com/deepmind-media/research/abstract.wav.", "published": "2019-09-25 17:47:49", "link": "http://arxiv.org/abs/1909.11646v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Disentangling Speech and Non-Speech Components for Building Robust\n  Acoustic Models from Found Data", "abstract": "In order to build language technologies for majority of the languages, it is\nimportant to leverage the resources available in public domain on the internet\n- commonly referred to as `Found Data'. However, such data is characterized by\nthe presence of non-standard, non-trivial variations. For instance, speech\nresources found on the internet have non-speech content, such as music.\nTherefore, speech recognition and speech synthesis models need to be robust to\nsuch variations. In this work, we present an analysis to show that it is\nimportant to disentangle the latent causal factors of variation in the original\ndata to accomplish these tasks. Based on this, we present approaches to\ndisentangle such variations from the data using Latent Stochastic Models.\nSpecifically, we present a method to split the latent prior space into\ncontinuous representations of dominant speech modes present in the magnitude\nspectra of audio signals. We propose a completely unsupervised approach using\nmultinode latent space variational autoencoders (VAE). We show that the\nconstraints on the latent space of a VAE can be in-fact used to separate speech\nand music, independent of the language of the speech. This paper also\nanalytically presents the requirement on the number of latent variables for the\ntask based on distribution of the speech data.", "published": "2019-09-25 19:37:47", "link": "http://arxiv.org/abs/1909.11727v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
