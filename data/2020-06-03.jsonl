{"title": "Towards Large-Scale Data Mining for Data-Driven Analysis of Sign\n  Languages", "abstract": "Access to sign language data is far from adequate. We show that it is\npossible to collect the data from social networking services such as TikTok,\nInstagram, and YouTube by applying data filtering to enforce quality standards\nand by discovering patterns in the filtered data, making it easier to analyse\nand model. Using our data collection pipeline, we collect and examine the\ninterpretation of songs in both the American Sign Language (ASL) and the\nBrazilian Sign Language (Libras). We explore their differences and similarities\nby looking at the co-dependence of the orientation and location phonological\nparameters", "published": "2020-06-03 09:28:17", "link": "http://arxiv.org/abs/2006.02120v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer Learning for British Sign Language Modelling", "abstract": "Automatic speech recognition and spoken dialogue systems have made great\nadvances through the use of deep machine learning methods. This is partly due\nto greater computing power but also through the large amount of data available\nin common languages, such as English. Conversely, research in minority\nlanguages, including sign languages, is hampered by the severe lack of data.\nThis has led to work on transfer learning methods, whereby a model developed\nfor one language is reused as the starting point for a model on a second\nlanguage, which is less resourced. In this paper, we examine two transfer\nlearning techniques of fine-tuning and layer substitution for language\nmodelling of British Sign Language. Our results show improvement in perplexity\nwhen using transfer learning with standard stacked LSTM models, trained\ninitially using a large corpus for standard English from the Penn Treebank\ncorpus", "published": "2020-06-03 10:13:29", "link": "http://arxiv.org/abs/2006.02144v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Text Summarization of COVID-19 Medical Research Articles using\n  BERT and GPT-2", "abstract": "With the COVID-19 pandemic, there is a growing urgency for medical community\nto keep up with the accelerating growth in the new coronavirus-related\nliterature. As a result, the COVID-19 Open Research Dataset Challenge has\nreleased a corpus of scholarly articles and is calling for machine learning\napproaches to help bridging the gap between the researchers and the rapidly\ngrowing publications. Here, we take advantage of the recent advances in\npre-trained NLP models, BERT and OpenAI GPT-2, to solve this challenge by\nperforming text summarization on this dataset. We evaluate the results using\nROUGE scores and visual inspection. Our model provides abstractive and\ncomprehensive information based on keywords extracted from the original\narticles. Our work can help the the medical community, by providing succinct\nsummaries of articles for which the abstract are not already available.", "published": "2020-06-03 00:54:44", "link": "http://arxiv.org/abs/2006.01997v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Norm-Based Curriculum Learning for Neural Machine Translation", "abstract": "A neural machine translation (NMT) system is expensive to train, especially\nwith high-resource settings. As the NMT architectures become deeper and wider,\nthis issue gets worse and worse. In this paper, we aim to improve the\nefficiency of training an NMT by introducing a novel norm-based curriculum\nlearning method. We use the norm (aka length or module) of a word embedding as\na measure of 1) the difficulty of the sentence, 2) the competence of the model,\nand 3) the weight of the sentence. The norm-based sentence difficulty takes the\nadvantages of both linguistically motivated and model-based sentence\ndifficulties. It is easy to determine and contains learning-dependent features.\nThe norm-based model competence makes NMT learn the curriculum in a fully\nautomated way, while the norm-based sentence weight further enhances the\nlearning of the vector representation of the NMT. Experimental results for the\nWMT'14 English-German and WMT'17 Chinese-English translation tasks demonstrate\nthat the proposed method outperforms strong baselines in terms of BLEU score\n(+1.17/+1.56) and training speedup (2.22x/3.33x).", "published": "2020-06-03 02:22:00", "link": "http://arxiv.org/abs/2006.02014v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploiting Class Labels to Boost Performance on Embedding-based Text\n  Classification", "abstract": "Text classification is one of the most frequent tasks for processing textual\ndata, facilitating among others research from large-scale datasets. Embeddings\nof different kinds have recently become the de facto standard as features used\nfor text classification. These embeddings have the capacity to capture meanings\nof words inferred from occurrences in large external collections. While they\nare built out of external collections, they are unaware of the distributional\ncharacteristics of words in the classification dataset at hand, including most\nimportantly the distribution of words across classes in training data. To make\nthe most of these embeddings as features and to boost the performance of\nclassifiers using them, we introduce a weighting scheme, Term\nFrequency-Category Ratio (TF-CR), which can weight high-frequency,\ncategory-exclusive words higher when computing word embeddings. Our experiments\non eight datasets show the effectiveness of TF-CR, leading to improved\nperformance scores over the well-known weighting schemes TF-IDF and KLD as well\nas over the absence of a weighting scheme in most cases.", "published": "2020-06-03 08:53:40", "link": "http://arxiv.org/abs/2006.02104v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Cross-model Back-translated Distillation for Unsupervised Machine\n  Translation", "abstract": "Recent unsupervised machine translation (UMT) systems usually employ three\nmain principles: initialization, language modeling and iterative\nback-translation, though they may apply them differently. Crucially, iterative\nback-translation and denoising auto-encoding for language modeling provide data\ndiversity to train the UMT systems. However, the gains from these\ndiversification processes has seemed to plateau. We introduce a novel component\nto the standard UMT framework called Cross-model Back-translated Distillation\n(CBD), that is aimed to induce another level of data diversification that\nexisting principles lack. CBD is applicable to all previous UMT approaches. In\nour experiments, CBD achieves the state of the art in the WMT'14\nEnglish-French, WMT'16 English-German and English-Romanian bilingual\nunsupervised translation tasks, with 38.2, 30.1, and 36.3 BLEU respectively. It\nalso yields 1.5-3.3 BLEU improvements in IWSLT English-French and\nEnglish-German tasks. Through extensive experimental analyses, we show that CBD\nis effective because it embraces data diversity while other similar variants do\nnot.", "published": "2020-06-03 10:57:21", "link": "http://arxiv.org/abs/2006.02163v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Emergent Multi-Agent Communication in the Deep Learning Era", "abstract": "The ability to cooperate through language is a defining feature of humans. As\nthe perceptual, motory and planning capabilities of deep artificial networks\nincrease, researchers are studying whether they also can develop a shared\nlanguage to interact. From a scientific perspective, understanding the\nconditions under which language evolves in communities of deep agents and its\nemergent features can shed light on human language evolution. From an applied\nperspective, endowing deep networks with the ability to solve problems\ninteractively by communicating with each other and with us should make them\nmore flexible and useful in everyday life.\n  This article surveys representative recent language emergence studies from\n  both of these two angles.", "published": "2020-06-03 17:50:16", "link": "http://arxiv.org/abs/2006.02419v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Extracting a Knowledge Base of COVID-19 Events from Social Media", "abstract": "In this paper, we present a manually annotated corpus of 10,000 tweets\ncontaining public reports of five COVID-19 events, including positive and\nnegative tests, deaths, denied access to testing, claimed cures and\npreventions. We designed slot-filling questions for each event type and\nannotated a total of 31 fine-grained slots, such as the location of events,\nrecent travel, and close contacts. We show that our corpus can support\nfine-tuning BERT-based classifiers to automatically extract publicly reported\nevents and help track the spread of a new disease. We also demonstrate that, by\naggregating events extracted from millions of tweets, we achieve surprisingly\nhigh precision when answering complex queries, such as \"Which organizations\nhave employees that tested positive in Philadelphia?\" We will release our\ncorpus (with user-information removed), automatic extraction models, and the\ncorresponding knowledge base to the research community.", "published": "2020-06-03 22:39:24", "link": "http://arxiv.org/abs/2006.02567v4", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Meta Dialogue Policy Learning", "abstract": "Dialog policy determines the next-step actions for agents and hence is\ncentral to a dialogue system. However, when migrated to novel domains with\nlittle data, a policy model can fail to adapt due to insufficient interactions\nwith the new environment. We propose Deep Transferable Q-Network (DTQN) to\nutilize shareable low-level signals between domains, such as dialogue acts and\nslots. We decompose the state and action representation space into feature\nsubspaces corresponding to these low-level components to facilitate\ncross-domain knowledge transfer. Furthermore, we embed DTQN in a meta-learning\nframework and introduce Meta-DTQN with a dual-replay mechanism to enable\neffective off-policy training and adaptation. In experiments, our model\noutperforms baseline models in terms of both success rate and dialogue\nefficiency on the multi-domain dialogue dataset MultiWOZ 2.0.", "published": "2020-06-03 23:53:06", "link": "http://arxiv.org/abs/2006.02588v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CompGuessWhat?!: A Multi-task Evaluation Framework for Grounded Language\n  Learning", "abstract": "Approaches to Grounded Language Learning typically focus on a single\ntask-based final performance measure that may not depend on desirable\nproperties of the learned hidden representations, such as their ability to\npredict salient attributes or to generalise to unseen situations. To remedy\nthis, we present GROLLA, an evaluation framework for Grounded Language Learning\nwith Attributes with three sub-tasks: 1) Goal-oriented evaluation; 2) Object\nattribute prediction evaluation; and 3) Zero-shot evaluation. We also propose a\nnew dataset CompGuessWhat?! as an instance of this framework for evaluating the\nquality of learned neural representations, in particular concerning attribute\ngrounding. To this end, we extend the original GuessWhat?! dataset by including\na semantic layer on top of the perceptual one. Specifically, we enrich the\nVisualGenome scene graphs associated with the GuessWhat?! images with abstract\nand situated attributes. By using diagnostic classifiers, we show that current\nmodels learn representations that are not expressive enough to encode object\nattributes (average F1 of 44.27). In addition, they do not learn strategies nor\nrepresentations that are robust enough to perform well when novel scenes or\nobjects are involved in gameplay (zero-shot best accuracy 50.06%).", "published": "2020-06-03 11:21:42", "link": "http://arxiv.org/abs/2006.02174v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Training for End-to-End Speech Translation", "abstract": "One of the main challenges for end-to-end speech translation is data\nscarcity. We leverage pseudo-labels generated from unlabeled audio by a cascade\nand an end-to-end speech translation model. This provides 8.3 and 5.7 BLEU\ngains over a strong semi-supervised baseline on the MuST-C English-French and\nEnglish-German datasets, reaching state-of-the art performance. The effect of\nthe quality of the pseudo-labels is investigated. Our approach is shown to be\nmore effective than simply pre-training the encoder on the speech recognition\ntask. Finally, we demonstrate the effectiveness of self-training by directly\ngenerating pseudo-labels with an end-to-end model instead of a cascade model.", "published": "2020-06-03 19:28:36", "link": "http://arxiv.org/abs/2006.02490v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Convolutional Deep Markov Model for Unsupervised Speech Representation\n  Learning", "abstract": "Probabilistic Latent Variable Models (LVMs) provide an alternative to\nself-supervised learning approaches for linguistic representation learning from\nspeech. LVMs admit an intuitive probabilistic interpretation where the latent\nstructure shapes the information extracted from the signal. Even though LVMs\nhave recently seen a renewed interest due to the introduction of Variational\nAutoencoders (VAEs), their use for speech representation learning remains\nlargely unexplored. In this work, we propose Convolutional Deep Markov Model\n(ConvDMM), a Gaussian state-space model with non-linear emission and transition\nfunctions modelled by deep neural networks. This unsupervised model is trained\nusing black box variational inference. A deep convolutional neural network is\nused as an inference network for structured variational approximation. When\ntrained on a large scale speech dataset (LibriSpeech), ConvDMM produces\nfeatures that significantly outperform multiple self-supervised feature\nextracting methods on linear phone classification and recognition on the Wall\nStreet Journal dataset. Furthermore, we found that ConvDMM complements\nself-supervised methods like Wav2Vec and PASE, improving on the results\nachieved with any of the methods alone. Lastly, we find that ConvDMM features\nenable learning better phone recognizers than any other features in an extreme\nlow-resource regime with few labeled training examples.", "published": "2020-06-03 21:50:20", "link": "http://arxiv.org/abs/2006.02547v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "M2P2: Multimodal Persuasion Prediction using Adaptive Fusion", "abstract": "Identifying persuasive speakers in an adversarial environment is a critical\ntask. In a national election, politicians would like to have persuasive\nspeakers campaign on their behalf. When a company faces adverse publicity, they\nwould like to engage persuasive advocates for their position in the presence of\nadversaries who are critical of them. Debates represent a common platform for\nthese forms of adversarial persuasion. This paper solves two problems: the\nDebate Outcome Prediction (DOP) problem predicts who wins a debate while the\nIntensity of Persuasion Prediction (IPP) problem predicts the change in the\nnumber of votes before and after a speaker speaks. Though DOP has been\npreviously studied, we are the first to study IPP. Past studies on DOP fail to\nleverage two important aspects of multimodal data: 1) multiple modalities are\noften semantically aligned, and 2) different modalities may provide diverse\ninformation for prediction. Our M2P2 (Multimodal Persuasion Prediction)\nframework is the first to use multimodal (acoustic, visual, language) data to\nsolve the IPP problem. To leverage the alignment of different modalities while\nmaintaining the diversity of the cues they provide, M2P2 devises a novel\nadaptive fusion learning framework which fuses embeddings obtained from two\nmodules -- an alignment module that extracts shared information between\nmodalities and a heterogeneity module that learns the weights of different\nmodalities with guidance from three separately trained unimodal reference\nmodels. We test M2P2 on the popular IQ2US dataset designed for DOP. We also\nintroduce a new dataset called QPS (from Qipashuo, a popular Chinese debate TV\nshow ) for IPP. M2P2 significantly outperforms 4 recent baselines on both\ndatasets.", "published": "2020-06-03 18:47:24", "link": "http://arxiv.org/abs/2006.11405v2", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Time Domain Velocity Vector for Retracing the Multipath Propagation", "abstract": "We propose a conceptually and computationally simple form of sound velocity\nthat offers a readable view of the interference between direct and indirect\nsound waves. Unlike most approaches in the literature, it jointly exploits both\nactive and reactive sound intensity measurements, as typically derived from a\nfirst order ambisonics recording. This representation has a potential both as a\nvaluable tool for directly analyzing sound multipath propagation, as well as\nbeing a new spatial feature format for machine learning algorithms in audio and\nacoustics. As a showcase, we demonstrate that the Direction-Of-Arrival and the\nrange of a sound source can be estimated as a development of this approach. To\nthe best knowledge of the authors, this is the first time that range is\nestimated from an ambisonics recording.", "published": "2020-06-03 08:30:06", "link": "http://arxiv.org/abs/2006.02099v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Graph2Speak: Improving Speaker Identification using Network Knowledge in\n  Criminal Conversational Data", "abstract": "Criminal investigations mostly rely on the collection of speech\nconversational data in order to identify speakers and build or enrich an\nexisting criminal network. Social network analysis tools are then applied to\nidentify the most central characters and the different communities within the\nnetwork. We introduce two candidate datasets for criminal conversational data,\nCrime Scene Investigation (CSI), a television show, and the ROXANNE simulated\ndata. We also introduce the metric of conversation accuracy in the context of\ncriminal investigations. By re-ranking candidate speakers based on the\nfrequency of previous interactions, we improve the speaker identification\nbaseline by 1.2% absolute (1.3% relative), and the conversation accuracy by\n2.6% absolute (3.4% relative) on CSI data, and by 1.1% absolute (1.2%\nrelative), and 2% absolute (2.5% relative) respectively on the ROXANNE\nsimulated data.", "published": "2020-06-03 08:08:42", "link": "http://arxiv.org/abs/2006.02093v4", "categories": ["cs.SI", "cs.SD", "eess.AS"], "primary_category": "cs.SI"}
