{"title": "Emotion Detection on TV Show Transcripts with Sequence-based\n  Convolutional Neural Networks", "abstract": "While there have been significant advances in detecting emotions from speech\nand image recognition, emotion detection on text is still under-explored and\nremained as an active research field. This paper introduces a corpus for\ntext-based emotion detection on multiparty dialogue as well as deep neural\nmodels that outperform the existing approaches for document classification. We\nfirst present a new corpus that provides annotation of seven emotions on\nconsecutive utterances in dialogues extracted from the show, Friends. We then\nsuggest four types of sequence-based convolutional neural network models with\nattention that leverage the sequence information encapsulated in dialogue. Our\nbest model shows the accuracies of 37.9% and 54% for fine- and coarse-grained\nemotions, respectively. Given the difficulty of this task, this is promising.", "published": "2017-08-14 20:01:44", "link": "http://arxiv.org/abs/1708.04299v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Sets: Word Embeddings Learned from Tweets and General Data", "abstract": "A word embedding is a low-dimensional, dense and real- valued vector\nrepresentation of a word. Word embeddings have been used in many NLP tasks.\nThey are usually gener- ated from a large text corpus. The embedding of a word\ncap- tures both its syntactic and semantic aspects. Tweets are short, noisy and\nhave unique lexical and semantic features that are different from other types\nof text. Therefore, it is necessary to have word embeddings learned\nspecifically from tweets. In this paper, we present ten word embedding data\nsets. In addition to the data sets learned from just tweet data, we also built\nembedding sets from the general data and the combination of tweets with the\ngeneral data. The general data consist of news articles, Wikipedia data and\nother web data. These ten embedding models were learned from about 400 million\ntweets and 7 billion words from the general text. In this paper, we also\npresent two experiments demonstrating how to use the data sets in some NLP\ntasks, such as tweet sentiment analysis and tweet topic classification tasks.", "published": "2017-08-14 02:34:17", "link": "http://arxiv.org/abs/1708.03994v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Learning spectro-temporal features with 3D CNNs for speech emotion\n  recognition", "abstract": "In this paper, we propose to use deep 3-dimensional convolutional networks\n(3D CNNs) in order to address the challenge of modelling spectro-temporal\ndynamics for speech emotion recognition (SER). Compared to a hybrid of\nConvolutional Neural Network and Long-Short-Term-Memory (CNN-LSTM), our\nproposed 3D CNNs simultaneously extract short-term and long-term spectral\nfeatures with a moderate number of parameters. We evaluated our proposed and\nother state-of-the-art methods in a speaker-independent manner using aggregated\ncorpora that give a large and diverse set of speakers. We found that 1) shallow\ntemporal and moderately deep spectral kernels of a homogeneous architecture are\noptimal for the task; and 2) our 3D CNNs are more effective for\nspectro-temporal feature learning compared to other methods. Finally, we\nvisualised the feature space obtained with our proposed method using\nt-distributed stochastic neighbour embedding (T-SNE) and could observe distinct\nclusters of emotions.", "published": "2017-08-14 17:32:06", "link": "http://arxiv.org/abs/1708.05071v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Sentiment Analysis by Joint Learning of Word Embeddings and Classifier", "abstract": "Word embeddings are representations of individual words of a text document in\na vector space and they are often use- ful for performing natural language pro-\ncessing tasks. Current state of the art al- gorithms for learning word\nembeddings learn vector representations from large corpora of text documents in\nan unsu- pervised fashion. This paper introduces SWESA (Supervised Word\nEmbeddings for Sentiment Analysis), an algorithm for sentiment analysis via\nword embeddings. SWESA leverages document label infor- mation to learn vector\nrepresentations of words from a modest corpus of text doc- uments by solving an\noptimization prob- lem that minimizes a cost function with respect to both word\nembeddings as well as classification accuracy. Analysis re- veals that SWESA\nprovides an efficient way of estimating the dimension of the word embeddings\nthat are to be learned. Experiments on several real world data sets show that\nSWESA has superior per- formance when compared to previously suggested\napproaches to word embeddings and sentiment analysis tasks.", "published": "2017-08-14 02:40:20", "link": "http://arxiv.org/abs/1708.03995v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Continuous Representation of Location for Geolocation and Lexical\n  Dialectology using Mixture Density Networks", "abstract": "We propose a method for embedding two-dimensional locations in a continuous\nvector space using a neural network-based model incorporating mixtures of\nGaussian distributions, presenting two model variants for text-based\ngeolocation and lexical dialectology. Evaluated over Twitter data, the proposed\nmodel outperforms conventional regression-based geolocation and provides a\nbetter estimate of uncertainty. We also show the effectiveness of the\nrepresentation for predicting words from location in lexical dialectology, and\nevaluate it using the DARE dataset.", "published": "2017-08-14 23:52:02", "link": "http://arxiv.org/abs/1708.04358v1", "categories": ["cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
