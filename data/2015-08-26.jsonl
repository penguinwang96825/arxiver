{"title": "Alignment-based compositional semantics for instruction following", "abstract": "This paper describes an alignment-based model for interpreting natural\nlanguage instructions in context. We approach instruction following as a search\nover plans, scoring sequences of actions conditioned on structured observations\nof text and the environment. By explicitly modeling both the low-level\ncompositional structure of individual actions and the high-level structure of\nfull plans, we are able to learn both grounded representations of sentence\nmeaning and pragmatic constraints on interpretation. To demonstrate the model's\nflexibility, we apply it to a diverse set of benchmark tasks. On every task, we\noutperform strong task-specific baselines, and achieve several new\nstate-of-the-art results.", "published": "2015-08-26 13:44:54", "link": "http://arxiv.org/abs/1508.06491v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Component-Enhanced Chinese Character Embeddings", "abstract": "Distributed word representations are very useful for capturing semantic\ninformation and have been successfully applied in a variety of NLP tasks,\nespecially on English. In this work, we innovatively develop two\ncomponent-enhanced Chinese character embedding models and their bigram\nextensions. Distinguished from English word embeddings, our models explore the\ncompositions of Chinese characters, which often serve as semantic indictors\ninherently. The evaluations on both word similarity and text classification\ndemonstrate the effectiveness of our models.", "published": "2015-08-26 21:25:25", "link": "http://arxiv.org/abs/1508.06669v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A fully data-driven method to identify (correlated) changes in\n  diachronic corpora", "abstract": "In this paper, a method for measuring synchronic corpus (dis-)similarity put\nforward by Kilgarriff (2001) is adapted and extended to identify trends and\ncorrelated changes in diachronic text data, using the Corpus of Historical\nAmerican English (Davies 2010a) and the Google Ngram Corpora (Michel et al.\n2010a). This paper shows that this fully data-driven method, which extracts\nword types that have undergone the most pronounced change in frequency in a\ngiven period of time, is computationally very cheap and that it allows\ninterpretations of diachronic trends that are both intuitively plausible and\nmotivated from the perspective of information theory. Furthermore, it\ndemonstrates that the method is able to identify correlated linguistic changes\nand diachronic shifts that can be linked to historical events. Finally, it can\nhelp to improve diachronic POS tagging and complement existing NLP approaches.\nThis indicates that the approach can facilitate an improved understanding of\ndiachronic processes in language change.", "published": "2015-08-26 06:18:51", "link": "http://arxiv.org/abs/1508.06374v2", "categories": ["cs.CL", "cs.IR", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Crossings as a side effect of dependency lengths", "abstract": "The syntactic structure of sentences exhibits a striking regularity:\ndependencies tend to not cross when drawn above the sentence. We investigate\ntwo competing explanations. The traditional hypothesis is that this trend\narises from an independent principle of syntax that reduces crossings\npractically to zero. An alternative to this view is the hypothesis that\ncrossings are a side effect of dependency lengths, i.e. sentences with shorter\ndependency lengths should tend to have fewer crossings. We are able to reject\nthe traditional view in the majority of languages considered. The alternative\nhypothesis can lead to a more parsimonious theory of language.", "published": "2015-08-26 11:39:18", "link": "http://arxiv.org/abs/1508.06451v2", "categories": ["cs.CL", "cs.SI", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Character-Aware Neural Language Models", "abstract": "We describe a simple neural language model that relies only on\ncharacter-level inputs. Predictions are still made at the word-level. Our model\nemploys a convolutional neural network (CNN) and a highway network over\ncharacters, whose output is given to a long short-term memory (LSTM) recurrent\nneural network language model (RNN-LM). On the English Penn Treebank the model\nis on par with the existing state-of-the-art despite having 60% fewer\nparameters. On languages with rich morphology (Arabic, Czech, French, German,\nSpanish, Russian), the model outperforms word-level/morpheme-level LSTM\nbaselines, again with fewer parameters. The results suggest that on many\nlanguages, character inputs are sufficient for language modeling. Analysis of\nword representations obtained from the character composition part of the model\nreveals that the model is able to encode, from characters only, both semantic\nand orthographic information.", "published": "2015-08-26 19:25:34", "link": "http://arxiv.org/abs/1508.06615v4", "categories": ["cs.CL", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
