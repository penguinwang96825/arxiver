{"title": "Unified Approach for Hedging Impermanent Loss of Liquidity Provision", "abstract": "We develop static and dynamic approaches for hedging of the impermanent loss\n(IL) of liquidity provision (LP) staked at Decentralised Exchanges (DEXes)\nwhich employ Uniswap V2 and V3 protocols. We provide detailed definitions and\nformulas for computing the IL to unify different definitions occurring in the\nexisting literature. We show that the IL can be seen a contingent claim with a\nnon-linear payoff for a fixed maturity date. Thus, we introduce the contingent\nclaim termed as IL protection claim which delivers the negative of IL payoff at\nthe maturity date. We apply arbitrage-based methods for valuation and risk\nmanagement of this claim. First, we develop the static model-independent\nreplication method for the valuation of IL protection claim using traded\nEuropean vanilla call and put options. We extend and generalize an existing\nmethod to show that the IL protection claim can be hedged perfectly with\noptions if there is a liquid options market. Second, we develop the dynamic\nmodel-based approach for the valuation and hedging of IL protection claims\nunder a risk-neutral measure. We derive analytic valuation formulas using a\nwide class of price dynamics for which the characteristic function is available\nunder the risk-neutral measure. As base cases, we derive analytic valuation\nformulas for IL protection claim under the Black-Scholes-Merton model and the\nlog-normal stochastic volatility model. We finally discuss estimation of\nrisk-reward of LP staking using our results.", "published": "2024-07-06 18:12:11", "link": "http://arxiv.org/abs/2407.05146v1", "categories": ["q-fin.MF", "q-fin.TR", "91-10"], "primary_category": "q-fin.MF"}
{"title": "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM\n  Compression", "abstract": "Increasingly, model compression techniques enable large language models\n(LLMs) to be deployed in real-world applications. As a result of this momentum\ntowards local deployment, compressed LLMs will interact with a large\npopulation. Prior work on compression typically prioritize preserving\nperplexity, which is directly analogous to training loss. The impact of\ncompression method on other critical aspects of model behavior\\, --\n\\,particularly safety\\, -- \\,requires systematic assessment. To this end, we\ninvestigate the impact of model compression along four dimensions: (1)\ndegeneration harm, i.e., bias and toxicity in generation; (2) representational\nharm, i.e., biases in discriminative tasks; (3) dialect bias; and(4) language\nmodeling and downstream task performance. We examine a wide spectrum of LLM\ncompression techniques, including unstructured pruning, semi-structured\npruning, and quantization. Our analysis reveals that compression can lead to\nunexpected consequences. Although compression may unintentionally alleviate\nLLMs' degeneration harm, it can still exacerbate representational harm.\nFurthermore, increasing compression produces a divergent impact on different\nprotected groups. Finally, different compression methods have drastically\ndifferent safety impacts: for example, quantization mostly preserves bias while\npruning degrades quickly. Our findings underscore the importance of integrating\nsafety assessments into the development of compressed LLMs to ensure their\nreliability across real-world applications.\\footnote{Our implementation and\nresults are available here:\n\\url{https://github.com/zhichaoxu-shufe/Beyond-Perplexity-Compression-Safety-Eval}}", "published": "2024-07-06 05:56:22", "link": "http://arxiv.org/abs/2407.04965v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EVA-Score: Evaluating Abstractive Long-form Summarization on\n  Informativeness through Extraction and Validation", "abstract": "Since LLMs emerged, more attention has been paid to abstractive long-form\nsummarization, where longer input sequences indicate more information\ncontained. Nevertheless, the automatic evaluation of such summaries remains\nunderexplored. The current evaluation metrics for long-form summarization\neither use similarity-based metrics like ROUGE and BERTScore or LLM-based\nmetrics using appropriate prompts or pre-defined schema. We argue that the\nformer only relies on similarity and fails to consider informativeness while\nthe latter lacks quantitative analysis of informative richness, and is rather\nsubjective and hard to explain. Current evaluation metrics either use\ntraditional metrics like ROUGE and BERTScore, which rely on surface-level\nsimilarity and fail to consider informativeness, or simple LLM-based metrics,\nwhich are not robust and easily overwhelmed by the long contexts. In this\npaper, we propose a new evaluation metric called EVA-Score to extract all\ninformation from the given summaries, identify overlapped information based on\nreference, and calculate the information score. We test EVA-Score on several\ndatasets and the experimental results reveal that EVA-Score shows the highest\ncorrelation with humans. We also re-evaluate the performance of LLMs on\nlong-form summarization from the information perspective. The results indicate\nthat responses of LLMs still have a gap with the human-written answers.\nMoreover, we provide a detailed analysis of the effectiveness of EVA-Score,\nforecasting future ways to automatically evaluate abstractive long-form\nsummarization.", "published": "2024-07-06 06:02:38", "link": "http://arxiv.org/abs/2407.04969v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recent Advancements and Challenges of Turkic Central Asian Language\n  Processing", "abstract": "Research in NLP for Central Asian Turkic languages - Kazakh, Uzbek, Kyrgyz,\nand Turkmen - faces typical low-resource language challenges like data\nscarcity, limited linguistic resources and technology development. However,\nrecent advancements have included the collection of language-specific datasets\nand the development of models for downstream tasks. Thus, this paper aims to\nsummarize recent progress and identify future research directions. It provides\na high-level overview of each language's linguistic features, the current\ntechnology landscape, the application of transfer learning from higher-resource\nlanguages, and the availability of labeled and unlabeled data. By outlining the\ncurrent state, we hope to inspire and facilitate future research.", "published": "2024-07-06 08:58:26", "link": "http://arxiv.org/abs/2407.05006v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Progress or Regress? Self-Improvement Reversal in Post-training", "abstract": "Self-improvement through post-training methods such as iterative preference\nlearning has been acclaimed for enhancing the problem-solving capabilities\n(e.g., mathematical reasoning) of Large Language Models (LLMs) without human\nintervention. However, as exploration deepens, it becomes crucial to assess\nwhether these improvements genuinely signify progress in solving more\nchallenging problems or if they could lead to unintended regressions. To\naddress this, we propose a comprehensive evaluative framework that goes beyond\nthe superficial pass@1 metric to scrutinize the underlying enhancements of\npost-training paradigms for self-improvement. Through rigorous experimentation\nand analysis across diverse problem-solving tasks, the empirical results point\nout the phenomenon of \\emph{self-improvement reversal}, where models showing\nimproved performance across benchmarks will paradoxically exhibit declines in\nbroader, essential capabilities, like output diversity and out-of-distribution\n(OOD) generalization. These findings indicate that current self-improvement\npractices through post-training are inadequate for equipping models to tackle\nmore complex problems. Furthermore, they underscore the necessity of our\ncritical evaluation metrics in discerning the \\emph{progress or regress}\ndichotomy for self-improving LLMs.", "published": "2024-07-06 09:07:11", "link": "http://arxiv.org/abs/2407.05013v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Principled Framework for Evaluating on Typologically Diverse Languages", "abstract": "Beyond individual languages, multilingual natural language processing (NLP)\nresearch increasingly aims to develop models that perform well across languages\ngenerally. However, evaluating these systems on all the world's languages is\npractically infeasible. To attain generalizability, representative language\nsampling is essential. Previous work argues that generalizable multilingual\nevaluation sets should contain languages with diverse typological properties.\nHowever, 'typologically diverse' language samples have been found to vary\nconsiderably in this regard, and popular sampling methods are flawed and\ninconsistent. We present a language sampling framework for selecting highly\ntypologically diverse languages given a sampling frame, informed by language\ntypology. We compare sampling methods with a range of metrics and find that our\nsystematic methods consistently retrieve more typologically diverse language\nselections than previous methods in NLP. Moreover, we provide evidence that\nthis affects generalizability in multilingual model evaluation, emphasizing the\nimportance of diverse language sampling in NLP evaluation.", "published": "2024-07-06 09:31:02", "link": "http://arxiv.org/abs/2407.05022v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Word Alignment for ASEAN Languages with Contrastive\n  Learning", "abstract": "Cross-lingual word alignment plays a crucial role in various natural language\nprocessing tasks, particularly for low-resource languages. Recent study\nproposes a BiLSTM-based encoder-decoder model that outperforms pre-trained\nlanguage models in low-resource settings. However, their model only considers\nthe similarity of word embedding spaces and does not explicitly model the\ndifferences between word embeddings. To address this limitation, we propose\nincorporating contrastive learning into the BiLSTM-based encoder-decoder\nframework. Our approach introduces a multi-view negative sampling strategy to\nlearn the differences between word pairs in the shared cross-lingual embedding\nspace. We evaluate our model on five bilingual aligned datasets spanning four\nASEAN languages: Lao, Vietnamese, Thai, and Indonesian. Experimental results\ndemonstrate that integrating contrastive learning consistently improves word\nalignment accuracy across all datasets, confirming the effectiveness of the\nproposed method in low-resource scenarios. We will release our data set and\ncode to support future research on ASEAN or more low-resource word alignment.", "published": "2024-07-06 11:56:41", "link": "http://arxiv.org/abs/2407.05054v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Sound Change Over Time: A Review of Computational and Human\n  Perception", "abstract": "Computational and human perception are often considered separate approaches\nfor studying sound changes over time; few works have touched on the\nintersection of both. To fill this research gap, we provide a pioneering review\ncontrasting computational with human perception from the perspectives of\nmethods and tasks. Overall, computational approaches rely on computer-driven\nmodels to perceive historical sound changes on etymological datasets, while\nhuman approaches use listener-driven models to perceive ongoing sound changes\non recording corpora. Despite their differences, both approaches complement\neach other on phonetic and acoustic levels, showing the potential to achieve a\nmore comprehensive perception of sound change. Moreover, we call for a\ncomparative study on the datasets used by both approaches to investigate the\ninfluence of historical sound changes on ongoing changes. Lastly, we discuss\nthe applications of sound change in computational linguistics, and point out\nthat perceiving sound change alone is insufficient, as many processes of\nlanguage change are complex, with entangled changes at syntactic, semantic, and\nphonetic levels.", "published": "2024-07-06 14:44:59", "link": "http://arxiv.org/abs/2407.05092v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Language Learning through Technology: Introducing a New\n  English-Azerbaijani (Arabic Script) Parallel Corpus", "abstract": "This paper introduces a pioneering English-Azerbaijani (Arabic Script)\nparallel corpus, designed to bridge the technological gap in language learning\nand machine translation (MT) for under-resourced languages. Consisting of\n548,000 parallel sentences and approximately 9 million words per language, this\ndataset is derived from diverse sources such as news articles and holy texts,\naiming to enhance natural language processing (NLP) applications and language\neducation technology. This corpus marks a significant step forward in the realm\nof linguistic resources, particularly for Turkic languages, which have lagged\nin the neural machine translation (NMT) revolution. By presenting the first\ncomprehensive case study for the English-Azerbaijani (Arabic Script) language\npair, this work underscores the transformative potential of NMT in low-resource\ncontexts. The development and utilization of this corpus not only facilitate\nthe advancement of machine translation systems tailored for specific linguistic\nneeds but also promote inclusive language learning through technology. The\nfindings demonstrate the corpus's effectiveness in training deep learning MT\nsystems and underscore its role as an essential asset for researchers and\neducators aiming to foster bilingual education and multilingual communication.\nThis research covers the way for future explorations into NMT applications for\nlanguages lacking substantial digital resources, thereby enhancing global\nlanguage education frameworks. The Python package of our code is available at\nhttps://pypi.org/project/chevir-kartalol/, and we also have a website\naccessible at https://translate.kartalol.com/.", "published": "2024-07-06 21:23:20", "link": "http://arxiv.org/abs/2407.05189v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NADI 2024: The Fifth Nuanced Arabic Dialect Identification Shared Task", "abstract": "We describe the findings of the fifth Nuanced Arabic Dialect Identification\nShared Task (NADI 2024). NADI's objective is to help advance SoTA Arabic NLP by\nproviding guidance, datasets, modeling opportunities, and standardized\nevaluation conditions that allow researchers to collaboratively compete on\npre-specified tasks. NADI 2024 targeted both dialect identification cast as a\nmulti-label task (Subtask~1), identification of the Arabic level of dialectness\n(Subtask~2), and dialect-to-MSA machine translation (Subtask~3). A total of 51\nunique teams registered for the shared task, of whom 12 teams have participated\n(with 76 valid submissions during the test phase). Among these, three teams\nparticipated in Subtask~1, three in Subtask~2, and eight in Subtask~3. The\nwinning teams achieved 50.57 F\\textsubscript{1} on Subtask~1, 0.1403 RMSE for\nSubtask~2, and 20.44 BLEU in Subtask~3, respectively. Results show that Arabic\ndialect processing tasks such as dialect identification and machine translation\nremain challenging. We describe the methods employed by the participating teams\nand briefly offer an outlook for NADI.", "published": "2024-07-06 01:18:58", "link": "http://arxiv.org/abs/2407.04910v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OmChat: A Recipe to Train Multimodal Language Models with Strong Long\n  Context and Video Understanding", "abstract": "We introduce OmChat, a model designed to excel in handling long contexts and\nvideo understanding tasks. OmChat's new architecture standardizes how different\nvisual inputs are processed, making it more efficient and adaptable. It uses a\ndynamic vision encoding process to effectively handle images of various\nresolutions, capturing fine details across a range of image qualities. OmChat\nutilizes an active progressive multimodal pretraining strategy, which gradually\nincreases the model's capacity for long contexts and enhances its overall\nabilities. By selecting high-quality data during training, OmChat learns from\nthe most relevant and informative data points. With support for a context\nlength of up to 512K, OmChat demonstrates promising performance in tasks\ninvolving multiple images and videos, outperforming most open-source models in\nthese benchmarks. Additionally, OmChat proposes a prompting strategy for\nunifying complex multimodal inputs including single image text, multi-image\ntext and videos, and achieving competitive performance on single-image\nbenchmarks. To further evaluate the model's capabilities, we proposed a\nbenchmark dataset named Temporal Visual Needle in a Haystack. This dataset\nassesses OmChat's ability to comprehend temporal visual details within long\nvideos. Our analysis highlights several key factors contributing to OmChat's\nsuccess: support for any-aspect high image resolution, the active progressive\npretraining strategy, and high-quality supervised fine-tuning datasets. This\nreport provides a detailed overview of OmChat's capabilities and the strategies\nthat enhance its performance in visual understanding.", "published": "2024-07-06 02:16:10", "link": "http://arxiv.org/abs/2407.04923v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Granular Privacy Control for Geolocation with Vision Language Models", "abstract": "Vision Language Models (VLMs) are rapidly advancing in their capability to\nanswer information-seeking questions. As these models are widely deployed in\nconsumer applications, they could lead to new privacy risks due to emergent\nabilities to identify people in photos, geolocate images, etc. As we\ndemonstrate, somewhat surprisingly, current open-source and proprietary VLMs\nare very capable image geolocators, making widespread geolocation with VLMs an\nimmediate privacy risk, rather than merely a theoretical future concern. As a\nfirst step to address this challenge, we develop a new benchmark, GPTGeoChat,\nto test the ability of VLMs to moderate geolocation dialogues with users. We\ncollect a set of 1,000 image geolocation conversations between in-house\nannotators and GPT-4v, which are annotated with the granularity of location\ninformation revealed at each turn. Using this new dataset, we evaluate the\nability of various VLMs to moderate GPT-4v geolocation conversations by\ndetermining when too much location information has been revealed. We find that\ncustom fine-tuned models perform on par with prompted API-based models when\nidentifying leaked location information at the country or city level; however,\nfine-tuning on supervised data appears to be needed to accurately moderate\nfiner granularities, such as the name of a restaurant or building.", "published": "2024-07-06 04:06:55", "link": "http://arxiv.org/abs/2407.04952v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "TRACE: TRansformer-based Attribution using Contrastive Embeddings in\n  LLMs", "abstract": "The rapid evolution of large language models (LLMs) represents a substantial\nleap forward in natural language understanding and generation. However,\nalongside these advancements come significant challenges related to the\naccountability and transparency of LLM responses. Reliable source attribution\nis essential to adhering to stringent legal and regulatory standards, including\nthose set forth by the General Data Protection Regulation. Despite the\nwell-established methods in source attribution within the computer vision\ndomain, the application of robust attribution frameworks to natural language\nprocessing remains underexplored. To bridge this gap, we propose a novel and\nversatile TRansformer-based Attribution framework using Contrastive Embeddings\ncalled TRACE that, in particular, exploits contrastive learning for source\nattribution. We perform an extensive empirical evaluation to demonstrate the\nperformance and efficiency of TRACE in various settings and show that TRACE\nsignificantly improves the ability to attribute sources accurately, making it a\nvaluable tool for enhancing the reliability and trustworthiness of LLMs.", "published": "2024-07-06 07:19:30", "link": "http://arxiv.org/abs/2407.04981v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Conditional Semi-Supervised Data Augmentation for Spam Message Detection\n  with Low Resource Data", "abstract": "Several machine learning schemes have attempted to perform the detection of\nspam messages. However, those schemes mostly require a huge amount of labeled\ndata. The existing techniques addressing the lack of data availability have\nissues with effectiveness and robustness. Therefore, this paper proposes a\nconditional semi-supervised data augmentation (CSSDA) for a spam detection\nmodel lacking the availability of data. The main architecture of CSSDA\ncomprises feature extraction and enhanced generative network. Here, we exploit\nunlabeled data for data augmentation to extend training data. The enhanced\ngenerative in our proposed scheme produces latent variables as fake samples\nfrom unlabeled data through a conditional scheme. Latent variables can come\nfrom labeled and unlabeled data as the input for the final classifier in our\nspam detection model. The experimental results indicate that our proposed CSSDA\nachieves excellent results compared to several related methods both exploiting\nunlabeled data and not. In the experiment stage with various amounts of\nunlabeled data, CSSDA is the only robust model that obtains a balanced accuracy\nof about 85% when the availability of labeled data is large. We also conduct\nseveral ablation studies to investigate our proposed scheme in detail. The\nresult also shows that several ablation studies strengthen our proposed\ninnovations. These experiments indicate that unlabeled data has a significant\ncontribution to data augmentation using the conditional semi-supervised scheme\nfor spam detection.", "published": "2024-07-06 07:51:24", "link": "http://arxiv.org/abs/2407.04990v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "The Solution for the AIGC Inference Performance Optimization Competition", "abstract": "In recent years, the rapid advancement of large-scale pre-trained language\nmodels based on transformer architectures has revolutionized natural language\nprocessing tasks. Among these, ChatGPT has gained widespread popularity,\ndemonstrating human-level conversational abilities and attracting over 100\nmillion monthly users by late 2022. Concurrently, Baidu's commercial deployment\nof the Ernie Wenxin model has significantly enhanced marketing effectiveness\nthrough AI-driven technologies. This paper focuses on optimizing\nhigh-performance inference for Ernie models, emphasizing GPU acceleration and\nleveraging the Paddle inference framework. We employ techniques such as Faster\nTransformer for efficient model processing, embedding layer pruning to reduce\ncomputational overhead, and FP16 half-precision inference for enhanced\ncomputational efficiency. Additionally, our approach integrates efficient data\nhandling strategies using multi-process parallel processing to minimize\nlatency. Experimental results demonstrate that our optimized solution achieves\nup to an 8.96x improvement in inference speed compared to standard methods,\nwhile maintaining competitive performance.", "published": "2024-07-06 07:54:45", "link": "http://arxiv.org/abs/2407.04991v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LoRA-GA: Low-Rank Adaptation with Gradient Approximation", "abstract": "Fine-tuning large-scale pretrained models is prohibitively expensive in terms\nof computational and memory costs. LoRA, as one of the most popular\nParameter-Efficient Fine-Tuning (PEFT) methods, offers a cost-effective\nalternative by fine-tuning an auxiliary low-rank model that has significantly\nfewer parameters. Although LoRA reduces the computational and memory\nrequirements significantly at each iteration, extensive empirical evidence\nindicates that it converges at a considerably slower rate compared to full\nfine-tuning, ultimately leading to increased overall compute and often worse\ntest performance. In our paper, we perform an in-depth investigation of the\ninitialization method of LoRA and show that careful initialization (without any\nchange of the architecture and the training algorithm) can significantly\nenhance both efficiency and performance. In particular, we introduce a novel\ninitialization method, LoRA-GA (Low Rank Adaptation with Gradient\nApproximation), which aligns the gradients of low-rank matrix product with\nthose of full fine-tuning at the first step. Our extensive experiments\ndemonstrate that LoRA-GA achieves a convergence rate comparable to that of full\nfine-tuning (hence being significantly faster than vanilla LoRA as well as\nvarious recent improvements) while simultaneously attaining comparable or even\nbetter performance. For example, on the subset of the GLUE dataset with\nT5-Base, LoRA-GA outperforms LoRA by 5.69% on average. On larger models such as\nLlama 2-7B, LoRA-GA shows performance improvements of 0.34, 11.52%, and 5.05%\non MT-bench, GSM8K, and Human-eval, respectively. Additionally, we observe up\nto 2-4 times convergence speed improvement compared to vanilla LoRA, validating\nits effectiveness in accelerating convergence and enhancing model performance.\nCode is available at https://github.com/Outsider565/LoRA-GA.", "published": "2024-07-06 08:37:21", "link": "http://arxiv.org/abs/2407.05000v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "How do you know that? Teaching Generative Language Models to Reference\n  Answers to Biomedical Questions", "abstract": "Large language models (LLMs) have recently become the leading source of\nanswers for users' questions online. Despite their ability to offer eloquent\nanswers, their accuracy and reliability can pose a significant challenge. This\nis especially true for sensitive domains such as biomedicine, where there is a\nhigher need for factually correct answers. This paper introduces a biomedical\nretrieval-augmented generation (RAG) system designed to enhance the reliability\nof generated responses. The system is based on a fine-tuned LLM for the\nreferenced question-answering, where retrieved relevant abstracts from PubMed\nare passed to LLM's context as input through a prompt. Its output is an answer\nbased on PubMed abstracts, where each statement is referenced accordingly,\nallowing the users to verify the answer. Our retrieval system achieves an\nabsolute improvement of 23% compared to the PubMed search engine. Based on the\nmanual evaluation on a small sample, our fine-tuned LLM component achieves\ncomparable results to GPT-4 Turbo in referencing relevant abstracts. We make\nthe dataset used to fine-tune the models and the fine-tuned models based on\nMistral-7B-instruct-v0.1 and v0.2 publicly available.", "published": "2024-07-06 09:10:05", "link": "http://arxiv.org/abs/2407.05015v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhance the Robustness of Text-Centric Multimodal Alignments", "abstract": "Converting different modalities into general text, serving as input prompts\nfor large language models (LLMs), is a common method to align multimodal models\nwhen there is limited pairwise data. This text-centric approach leverages the\nunique properties of text as a modality space, transforming diverse inputs into\na unified textual representation. This enables downstream models to effectively\ninterpret various modal inputs. This study assesses the quality and robustness\nof multimodal representations in the presence of missing entries, noise, or\nabsent modalities, revealing that current text-centric alignment methods\ncompromise downstream robustness. To address this issue, we propose a new\ntext-centric approach that achieves superior robustness compared to previous\nmethods across various modalities in different settings. Our findings highlight\nthe potential of this approach to enhance the robustness and adaptability of\nmultimodal representations, offering a promising solution for dynamic and\nreal-world applications.", "published": "2024-07-06 10:12:29", "link": "http://arxiv.org/abs/2407.05036v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic Prediction of the Performance of Every Parser", "abstract": "We present a new parser performance prediction (PPP) model using machine\ntranslation performance prediction system (MTPPS), statistically independent of\nany language or parser, relying only on extrinsic and novel features based on\ntextual, link structural, and bracketing tree structural information. This new\nsystem, MTPPS-PPP, can predict the performance of any parser in any language\nand can be useful for estimating the grammatical difficulty when understanding\na given text, for setting expectations from parsing output, for parser\nselection for a specific domain, and for parser combination systems. We obtain\nSoA results in PPP of bracketing $F_1$ with better results over textual\nfeatures and similar performance with previous results that use parser and\nlinguistic label specific information. Our results show the contribution of\ndifferent types of features as well as rankings of individual features in\ndifferent experimental settings (cased vs. uncased), in different learning\ntasks (in-domain vs. out-of-domain), with different training sets, with\ndifferent learning algorithms, and with different dimensionality reduction\ntechniques. We achieve $0.0678$ MAE and $0.85$ RAE in setting +Link, which\ncorresponds to about $7.4\\%$ error when predicting the bracketing $F_1$ score\nfor the Charniak and Johnson parser on the WSJ23 test set. MTPPS-PPP system can\npredict without parsing using only the text, without a supervised parser using\nonly an unsupervised parser, without any parser or language dependent\ninformation, without using a reference parser output, and can be used to\npredict the performance of any parser in any language.", "published": "2024-07-06 15:49:24", "link": "http://arxiv.org/abs/2407.05116v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Identifying Intensity of the Structure and Content in Tweets and the\n  Discriminative Power of Attributes in Context with Referential Translation\n  Machines", "abstract": "We use referential translation machines (RTMs) to identify the similarity\nbetween an attribute and two words in English by casting the task as machine\ntranslation performance prediction (MTPP) between the words and the attribute\nword and the distance between their similarities for Task 10 with stacked RTM\nmodels. RTMs are also used to predict the intensity of the structure and\ncontent in tweets in English, Arabic, and Spanish in Task 1 where MTPP is\nbetween the tweets and the set of words for the emotion selected from WordNet\naffect emotion lists. Stacked RTM models obtain encouraging results in both.", "published": "2024-07-06 18:58:10", "link": "http://arxiv.org/abs/2407.05154v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "BadCLM: Backdoor Attack in Clinical Language Models for Electronic\n  Health Records", "abstract": "The advent of clinical language models integrated into electronic health\nrecords (EHR) for clinical decision support has marked a significant\nadvancement, leveraging the depth of clinical notes for improved\ndecision-making. Despite their success, the potential vulnerabilities of these\nmodels remain largely unexplored. This paper delves into the realm of backdoor\nattacks on clinical language models, introducing an innovative attention-based\nbackdoor attack method, BadCLM (Bad Clinical Language Models). This technique\nclandestinely embeds a backdoor within the models, causing them to produce\nincorrect predictions when a pre-defined trigger is present in inputs, while\nfunctioning accurately otherwise. We demonstrate the efficacy of BadCLM through\nan in-hospital mortality prediction task with MIMIC III dataset, showcasing its\npotential to compromise model integrity. Our findings illuminate a significant\nsecurity risk in clinical decision support systems and pave the way for future\nendeavors in fortifying clinical language models against such vulnerabilities.", "published": "2024-07-06 23:56:43", "link": "http://arxiv.org/abs/2407.05213v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Applicability of Large Language Models and Generative Models for Legal\n  Case Judgement Summarization", "abstract": "Automatic summarization of legal case judgements, which are known to be long\nand complex, has traditionally been tried via extractive summarization models.\nIn recent years, generative models including abstractive summarization models\nand Large language models (LLMs) have gained huge popularity. In this paper, we\nexplore the applicability of such models for legal case judgement\nsummarization. We applied various domain specific abstractive summarization\nmodels and general domain LLMs as well as extractive summarization models over\ntwo sets of legal case judgements from the United Kingdom (UK) Supreme Court\nand the Indian (IN) Supreme Court and evaluated the quality of the generated\nsummaries. We also perform experiments on a third dataset of legal documents of\na different type, Government reports from the United States (US). Results show\nthat abstractive summarization models and LLMs generally perform better than\nthe extractive methods as per traditional metrics for evaluating summary\nquality. However, detailed investigation shows the presence of inconsistencies\nand hallucinations in the outputs of the generative models, and we explore ways\nto reduce the hallucinations and inconsistencies in the summaries. Overall, the\ninvestigation suggests that further improvements are needed to enhance the\nreliability of abstractive models and LLMs for legal case judgement\nsummarization. At present, a human-in-the-loop technique is more suitable for\nperforming manual checks to identify inconsistencies in the generated\nsummaries.", "published": "2024-07-06 04:49:40", "link": "http://arxiv.org/abs/2407.12848v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large language models are good medical coders, if provided with tools", "abstract": "This study presents a novel two-stage Retrieve-Rank system for automated\nICD-10-CM medical coding, comparing its performance against a Vanilla Large\nLanguage Model (LLM) approach. Evaluating both systems on a dataset of 100\nsingle-term medical conditions, the Retrieve-Rank system achieved 100% accuracy\nin predicting correct ICD-10-CM codes, significantly outperforming the Vanilla\nLLM (GPT-3.5-turbo), which achieved only 6% accuracy. Our analysis demonstrates\nthe Retrieve-Rank system's superior precision in handling various medical terms\nacross different specialties. While these results are promising, we acknowledge\nthe limitations of using simplified inputs and the need for further testing on\nmore complex, realistic medical cases. This research contributes to the ongoing\neffort to improve the efficiency and accuracy of medical coding, highlighting\nthe importance of retrieval-based approaches.", "published": "2024-07-06 06:58:51", "link": "http://arxiv.org/abs/2407.12849v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "AI Safety in Generative AI Large Language Models: A Survey", "abstract": "Large Language Model (LLMs) such as ChatGPT that exhibit generative AI\ncapabilities are facing accelerated adoption and innovation. The increased\npresence of Generative AI (GAI) inevitably raises concerns about the risks and\nsafety associated with these models. This article provides an up-to-date survey\nof recent trends in AI safety research of GAI-LLMs from a computer scientist's\nperspective: specific and technical. In this survey, we explore the background\nand motivation for the identified harms and risks in the context of LLMs being\ngenerative language models; our survey differentiates by emphasising the need\nfor unified theories of the distinct safety challenges in the research\ndevelopment and applications of LLMs. We start our discussion with a concise\nintroduction to the workings of LLMs, supported by relevant literature. Then we\ndiscuss earlier research that has pointed out the fundamental constraints of\ngenerative models, or lack of understanding thereof (e.g., performance and\nsafety trade-offs as LLMs scale in number of parameters). We provide a\nsufficient coverage of LLM alignment -- delving into various approaches,\ncontending methods and present challenges associated with aligning LLMs with\nhuman preferences. By highlighting the gaps in the literature and possible\nimplementation oversights, our aim is to create a comprehensive analysis that\nprovides insights for addressing AI safety in LLMs and encourages the\ndevelopment of aligned and secure models. We conclude our survey by discussing\nfuture directions of LLMs for AI safety, offering insights into ongoing\nresearch in this critical area.", "published": "2024-07-06 09:00:18", "link": "http://arxiv.org/abs/2407.18369v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Algorithmic Language Models with Neurally Compiled Libraries", "abstract": "Important tasks such as reasoning and planning are fundamentally algorithmic,\nmeaning that solving them robustly requires acquiring true reasoning or\nplanning algorithms, rather than shortcuts. Large Language Models lack true\nalgorithmic ability primarily because of the limitations of neural network\noptimization algorithms, their optimization data and optimization objective,\nbut also due to architectural inexpressivity. To solve this, our paper proposes\naugmenting LLMs with a library of fundamental operations and sophisticated\ndifferentiable programs, so that common algorithms do not need to be learned\nfrom scratch. We add memory, registers, basic operations, and adaptive\nrecurrence to a transformer architecture built on LLaMA3. Then, we define a\nmethod for directly compiling algorithms into a differentiable starting\nlibrary, which is used natively and propagates gradients for optimization. In\nthis preliminary study, we explore the feasability of augmenting LLaMA3 with a\ndifferentiable computer, for instance by fine-tuning small transformers on\nsimple algorithmic tasks with variable computational depth.", "published": "2024-07-06 00:27:05", "link": "http://arxiv.org/abs/2407.04899v1", "categories": ["cs.AI", "cs.CL", "cs.PL"], "primary_category": "cs.AI"}
{"title": "MMSci: A Dataset for Graduate-Level Multi-Discipline Multimodal\n  Scientific Understanding", "abstract": "Scientific figure interpretation is a crucial capability for AI-driven\nscientific assistants built on advanced Large Vision Language Models. However,\ncurrent datasets and benchmarks primarily focus on simple charts or other\nrelatively straightforward figures from limited science domains. To address\nthis gap, we present a comprehensive dataset compiled from peer-reviewed Nature\nCommunications articles covering 72 scientific fields, encompassing complex\nvisualizations such as schematic diagrams, microscopic images, and experimental\ndata which require graduate-level expertise to interpret. We evaluated 19\nproprietary and open-source models on two benchmark tasks, figure captioning\nand multiple-choice, and conducted human expert annotation. Our analysis\nrevealed significant task challenges and performance gaps among models. Beyond\nserving as a benchmark, this dataset serves as a valuable resource for\nlarge-scale training. Fine-tuning Qwen2-VL-7B with our task-specific data\nachieved better performance than GPT-4o and even human experts in\nmultiple-choice evaluations. Furthermore, continuous pre-training on our\ninterleaved article and figure data substantially enhanced the model's\ndownstream task performance in materials science. We have released our dataset\nto support further research.", "published": "2024-07-06 00:40:53", "link": "http://arxiv.org/abs/2407.04903v3", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "LogicVista: Multimodal LLM Logical Reasoning Benchmark in Visual\n  Contexts", "abstract": "We propose LogicVista, an evaluation benchmark that assesses the integrated\nlogical reasoning capabilities of multimodal large language models (MLLMs) in\nVisual contexts. Recent advancements in MLLMs have demonstrated various\nfascinating abilities, from crafting poetry based on an image to performing\nmathematical reasoning. However, there is still a lack of systematic evaluation\nof MLLMs' proficiency in logical reasoning tasks, which are essential for\nactivities like navigation and puzzle-solving. Thus we evaluate general logical\ncognition abilities across 5 logical reasoning tasks encompassing 9 different\ncapabilities, using a sample of 448 multiple-choice questions. Each question is\nannotated with the correct answer and the human-written reasoning behind the\nselection, enabling both open-ended and multiple-choice evaluation. A total of\n8 MLLMs are comprehensively evaluated using LogicVista. Code and Data Available\nat https://github.com/Yijia-Xiao/LogicVista.", "published": "2024-07-06 06:48:16", "link": "http://arxiv.org/abs/2407.04973v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "The Solution for the 5th GCAIAC Zero-shot Referring Expression\n  Comprehension Challenge", "abstract": "This report presents a solution for the zero-shot referring expression\ncomprehension task. Visual-language multimodal base models (such as CLIP, SAM)\nhave gained significant attention in recent years as a cornerstone of\nmainstream research. One of the key applications of multimodal base models lies\nin their ability to generalize to zero-shot downstream tasks. Unlike\ntraditional referring expression comprehension, zero-shot referring expression\ncomprehension aims to apply pre-trained visual-language models directly to the\ntask without specific training. Recent studies have enhanced the zero-shot\nperformance of multimodal base models in referring expression comprehension\ntasks by introducing visual prompts. To address the zero-shot referring\nexpression comprehension challenge, we introduced a combination of visual\nprompts and considered the influence of textual prompts, employing joint\nprediction tailored to the data characteristics. Ultimately, our approach\nachieved accuracy rates of 84.825 on the A leaderboard and 71.460 on the B\nleaderboard, securing the first position.", "published": "2024-07-06 08:31:33", "link": "http://arxiv.org/abs/2407.04998v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Ask Questions with Double Hints: Visual Question Generation with\n  Answer-awareness and Region-reference", "abstract": "The visual question generation (VQG) task aims to generate human-like\nquestions from an image and potentially other side information (e.g. answer\ntype). Previous works on VQG fall in two aspects: i) They suffer from one image\nto many questions mapping problem, which leads to the failure of generating\nreferential and meaningful questions from an image. ii) They fail to model\ncomplex implicit relations among the visual objects in an image and also\noverlook potential interactions between the side information and image. To\naddress these limitations, we first propose a novel learning paradigm to\ngenerate visual questions with answer-awareness and region-reference.\nConcretely, we aim to ask the right visual questions with Double Hints -\ntextual answers and visual regions of interests, which could effectively\nmitigate the existing one-to-many mapping issue. Particularly, we develop a\nsimple methodology to self-learn the visual hints without introducing any\nadditional human annotations. Furthermore, to capture these sophisticated\nrelationships, we propose a new double-hints guided Graph-to-Sequence learning\nframework, which first models them as a dynamic graph and learns the implicit\ntopology end-to-end, and then utilizes a graph-to-sequence model to generate\nthe questions with double hints. Experimental results demonstrate the priority\nof our proposed method.", "published": "2024-07-06 15:07:32", "link": "http://arxiv.org/abs/2407.05100v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Solving for X and Beyond: Can Large Language Models Solve Complex Math\n  Problems with More-Than-Two Unknowns?", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in\nsolving math problems, a hallmark of human intelligence. Despite high success\nrates on current benchmarks; however, these often feature simple problems with\nonly one or two unknowns, which do not sufficiently challenge their reasoning\ncapacities. This paper introduces a novel benchmark, BeyondX, designed to\naddress these limitations by incorporating problems with multiple unknowns.\nRecognizing the challenges in proposing multi-unknown problems from scratch, we\ndeveloped BeyondX using an innovative automated pipeline that progressively\nincreases complexity by expanding the number of unknowns in simpler problems.\nEmpirical study on BeyondX reveals that the performance of existing LLMs, even\nthose fine-tuned specifically on math tasks, significantly decreases as the\nnumber of unknowns increases - with a performance drop of up to 70\\% observed\nin GPT-4. To tackle these challenges, we propose the Formulate-and-Solve\nstrategy, a generalized prompting approach that effectively handles problems\nwith an arbitrary number of unknowns. Our findings reveal that this strategy\nnot only enhances LLM performance on the BeyondX benchmark but also provides\ndeeper insights into the computational limits of LLMs when faced with more\ncomplex mathematical challenges.", "published": "2024-07-06 17:01:04", "link": "http://arxiv.org/abs/2407.05134v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "LLMCloudHunter: Harnessing LLMs for Automated Extraction of Detection\n  Rules from Cloud-Based CTI", "abstract": "As the number and sophistication of cyber attacks have increased, threat\nhunting has become a critical aspect of active security, enabling proactive\ndetection and mitigation of threats before they cause significant harm.\nOpen-source cyber threat intelligence (OS-CTI) is a valuable resource for\nthreat hunters, however, it often comes in unstructured formats that require\nfurther manual analysis. Previous studies aimed at automating OSCTI analysis\nare limited since (1) they failed to provide actionable outputs, (2) they did\nnot take advantage of images present in OSCTI sources, and (3) they focused on\non-premises environments, overlooking the growing importance of cloud\nenvironments. To address these gaps, we propose LLMCloudHunter, a novel\nframework that leverages large language models (LLMs) to automatically generate\ngeneric-signature detection rule candidates from textual and visual OSCTI data.\nWe evaluated the quality of the rules generated by the proposed framework using\n12 annotated real-world cloud threat reports. The results show that our\nframework achieved a precision of 92% and recall of 98% for the task of\naccurately extracting API calls made by the threat actor and a precision of 99%\nwith a recall of 98% for IoCs. Additionally, 99.18% of the generated detection\nrule candidates were successfully compiled and converted into Splunk queries.", "published": "2024-07-06 21:43:35", "link": "http://arxiv.org/abs/2407.05194v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language\n  Models", "abstract": "The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has\nenhanced medical diagnosis. However, current Med-LVLMs frequently encounter\nfactual issues, often generating responses that do not align with established\nmedical facts. Retrieval-Augmented Generation (RAG), which utilizes external\nknowledge, can improve the factual accuracy of these models but introduces two\nmajor challenges. First, limited retrieved contexts might not cover all\nnecessary information, while excessive retrieval can introduce irrelevant and\ninaccurate references, interfering with the model's generation. Second, in\ncases where the model originally responds correctly, applying RAG can lead to\nan over-reliance on retrieved contexts, resulting in incorrect answers. To\naddress these issues, we propose RULE, which consists of two components. First,\nwe introduce a provably effective strategy for controlling factuality risk\nthrough the calibrated selection of the number of retrieved contexts. Second,\nbased on samples where over-reliance on retrieved contexts led to errors, we\ncurate a preference dataset to fine-tune the model, balancing its dependence on\ninherent knowledge and retrieved contexts for generation. We demonstrate the\neffectiveness of RULE on medical VQA and report generation tasks across three\ndatasets, achieving an average improvement of 47.4% in factual accuracy. We\npublicly release our benchmark and code in\nhttps://github.com/richard-peng-xia/RULE.", "published": "2024-07-06 16:45:07", "link": "http://arxiv.org/abs/2407.05131v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.CY"], "primary_category": "cs.LG"}
{"title": "A Reference-free Metric for Language-Queried Audio Source Separation\n  using Contrastive Language-Audio Pretraining", "abstract": "Language-queried audio source separation (LASS) aims to separate an audio\nsource guided by a text query, with the signal-to-distortion ratio (SDR)-based\nmetrics being commonly used to objectively measure the quality of the separated\naudio. However, the SDR-based metrics require a reference signal, which is\noften difficult to obtain in real-world scenarios. In addition, with the\nSDR-based metrics, the content information of the text query is not considered\neffectively in LASS. This paper introduces a reference-free evaluation metric\nusing a contrastive language-audio pretraining (CLAP) module, termed CLAPScore,\nwhich measures the semantic similarity between the separated audio and the text\nquery. Unlike SDR, the proposed CLAPScore metric evaluates the quality of the\nseparated audio based on the content information of the text query, without\nneeding a reference signal. Experiments show that the CLAPScore provides an\neffective evaluation of the semantic relevance of the separated audio to the\ntext query, as compared to the SDR metric, offering an alternative for the\nperformance evaluation of LASS systems. The code for evaluation is publicly\navailable.", "published": "2024-07-06 02:55:28", "link": "http://arxiv.org/abs/2407.04936v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Layer-Anchoring Strategy for Enhancing Cross-Lingual Speech Emotion\n  Recognition", "abstract": "Cross-lingual speech emotion recognition (SER) is important for a wide range\nof everyday applications. While recent SER research relies heavily on large\npretrained models for emotion training, existing studies often concentrate\nsolely on the final transformer layer of these models. However, given the\ntask-specific nature and hierarchical architecture of these models, each\ntransformer layer encapsulates different levels of information. Leveraging this\nhierarchical structure, our study focuses on the information embedded across\ndifferent layers. Through an examination of layer feature similarity across\ndifferent languages, we propose a novel strategy called a layer-anchoring\nmechanism to facilitate emotion transfer in cross-lingual SER tasks. Our\napproach is evaluated using two distinct language affective corpora\n(MSP-Podcast and BIIC-Podcast), achieving a best UAR performance of 60.21% on\nthe BIIC-podcast corpus. The analysis uncovers interesting insights into the\nbehavior of popular pretrained models.", "published": "2024-07-06 05:56:55", "link": "http://arxiv.org/abs/2407.04966v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
