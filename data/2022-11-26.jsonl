{"title": "SKDBERT: Compressing BERT via Stochastic Knowledge Distillation", "abstract": "In this paper, we propose Stochastic Knowledge Distillation (SKD) to obtain\ncompact BERT-style language model dubbed SKDBERT. In each iteration, SKD\nsamples a teacher model from a pre-defined teacher ensemble, which consists of\nmultiple teacher models with multi-level capacities, to transfer knowledge into\nstudent model in an one-to-one manner. Sampling distribution plays an important\nrole in SKD. We heuristically present three types of sampling distributions to\nassign appropriate probabilities for multi-level teacher models. SKD has two\nadvantages: 1) it can preserve the diversities of multi-level teacher models\nvia stochastically sampling single teacher model in each iteration, and 2) it\ncan also improve the efficacy of knowledge distillation via multi-level teacher\nmodels when large capacity gap exists between the teacher model and the student\nmodel. Experimental results on GLUE benchmark show that SKDBERT reduces the\nsize of a BERT$_{\\rm BASE}$ model by 40% while retaining 99.5% performances of\nlanguage understanding and being 100% faster.", "published": "2022-11-26 03:18:55", "link": "http://arxiv.org/abs/2211.14466v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Better Document-level Relation Extraction via Iterative\n  Inference", "abstract": "Document-level relation extraction (RE) aims to extract the relations between\nentities from the input document that usually containing many\ndifficultly-predicted entity pairs whose relations can only be predicted\nthrough relational inference. Existing methods usually directly predict the\nrelations of all entity pairs of input document in a one-pass manner, ignoring\nthe fact that predictions of some entity pairs heavily depend on the predicted\nresults of other pairs. To deal with this issue, in this paper, we propose a\nnovel document-level RE model with iterative inference. Our model is mainly\ncomposed of two modules: 1) a base module expected to provide preliminary\nrelation predictions on entity pairs; 2) an inference module introduced to\nrefine these preliminary predictions by iteratively dealing with\ndifficultly-predicted entity pairs depending on other pairs in an easy-to-hard\nmanner. Unlike previous methods which only consider feature information of\nentity pairs, our inference module is equipped with two Extended Cross\nAttention units, allowing it to exploit both feature information and previous\npredictions of entity pairs during relational inference. Furthermore, we adopt\na two-stage strategy to train our model. At the first stage, we only train our\nbase module. During the second stage, we train the whole model, where\ncontrastive learning is introduced to enhance the training of inference module.\nExperimental results on three commonly-used datasets show that our model\nconsistently outperforms other competitive baselines.", "published": "2022-11-26 03:57:34", "link": "http://arxiv.org/abs/2211.14470v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Automatic SOAP Classification System Using Weakly Supervision And\n  Transfer Learning", "abstract": "In this paper, we introduce a comprehensive framework for developing a\nmachine learning-based SOAP (Subjective, Objective, Assessment, and Plan)\nclassification system without manually SOAP annotated training data or with\nless manually SOAP annotated training data. The system is composed of the\nfollowing two parts: 1) Data construction, 2) A neural network-based SOAP\nclassifier, and 3) Transfer learning framework. In data construction, since a\nmanual construction of a large size training dataset is expensive, we propose a\nrule-based weak labeling method utilizing the structured information of an EHR\nnote. Then, we present a SOAP classifier composed of a pre-trained language\nmodel and bi-directional long-short term memory with conditional random field\n(Bi-LSTM-CRF). Finally, we propose a transfer learning framework that re-uses\nthe trained parameters of the SOAP classifier trained with the weakly labeled\ndataset for datasets collected from another hospital. The proposed weakly\nlabel-based learning model successfully performed SOAP classification (89.99\nF1-score) on the notes collected from the target hospital. Otherwise, in the\nnotes collected from other hospitals and departments, the performance\ndramatically decreased. Meanwhile, we verified that the transfer learning\nframework is advantageous for inter-hospital adaptation of the model increasing\nthe models' performance in every cases. In particular, the transfer learning\napproach was more efficient when the manually annotated data size was smaller.\nWe showed that SOAP classification models trained with our weakly labeling\nalgorithm can perform SOAP classification without manually annotated data on\nthe EHR notes from the same hospital. The transfer learning framework helps\nSOAP classification model's inter-hospital migration with a minimal size of the\nmanually annotated dataset.", "published": "2022-11-26 10:58:18", "link": "http://arxiv.org/abs/2211.14539v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lexical Complexity Controlled Sentence Generation", "abstract": "Text generation rarely considers the control of lexical complexity, which\nlimits its more comprehensive practical application. We introduce a novel task\nof lexical complexity controlled sentence generation, which aims at keywords to\nsentence generation with desired complexity levels. It has enormous potential\nin domains such as grade reading, language teaching and acquisition. The\nchallenge of this task is to generate fluent sentences only using the words of\ngiven complexity levels. We propose a simple but effective approach for this\ntask based on complexity embedding. Compared with potential solutions, our\napproach fuses the representations of the word complexity levels into the model\nto get better control of lexical complexity. And we demonstrate the feasibility\nof the approach for both training models from scratch and fine-tuning the\npre-trained models. To facilitate the research, we develop two datasets in\nEnglish and Chinese respectively, on which extensive experiments are conducted.\nResults show that our approach better controls lexical complexity and generates\nhigher quality sentences than baseline methods.", "published": "2022-11-26 11:03:56", "link": "http://arxiv.org/abs/2211.14540v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The distribution of syntactic dependency distances", "abstract": "The syntactic structure of a sentence can be represented as a graph where\nvertices are words and edges indicate syntactic dependencies between them. In\nthis setting, the distance between two syntactically linked words can be\ndefined as the difference between their positions. Here we want to contribute\nto the characterization of the actual distribution of syntactic dependency\ndistances, and unveil its relationship with short-term memory limitations. We\npropose a new double-exponential model in which decay in probability is allowed\nto change after a break-point. This transition could mirror the transition from\nthe processing of words chunks to higher-level structures. We find that a\ntwo-regime model -- where the first regime follows either an exponential or a\npower-law decay -- is the most likely one in all 20 languages we considered,\nindependently of sentence length and annotation style. Moreover, the\nbreak-point is fairly stable across languages and averages values of 4-5 words,\nsuggesting that the amount of words that can be simultaneously processed\nabstracts from the specific language to a high degree. Finally, we give an\naccount of the relation between the best estimated model and the closeness of\nsyntactic dependencies, as measured by a recently introduced optimality score.", "published": "2022-11-26 17:31:25", "link": "http://arxiv.org/abs/2211.14620v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gender Biases Unexpectedly Fluctuate in the Pre-training Stage of Masked\n  Language Models", "abstract": "Masked language models pick up gender biases during pre-training. Such biases\nare usually attributed to a certain model architecture and its pre-training\ncorpora, with the implicit assumption that other variations in the pre-training\nprocess, such as the choices of the random seed or the stopping point, have no\neffect on the biases measured. However, we show that severe fluctuations exist\nat the fundamental level of individual templates, invalidating the assumption.\nFurther against the intuition of how humans acquire biases, these fluctuations\nare not correlated with the certainty of the predicted pronouns or the\nprofession frequencies in pre-training corpora. We release our code and data to\nbenefit future research.", "published": "2022-11-26 18:43:05", "link": "http://arxiv.org/abs/2211.14639v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Emotion-guided Approach to Domain Adaptive Fake News Detection using\n  Adversarial Learning", "abstract": "Recent works on fake news detection have shown the efficacy of using emotions\nas a feature for improved performance. However, the cross-domain impact of\nemotion-guided features for fake news detection still remains an open problem.\nIn this work, we propose an emotion-guided, domain-adaptive, multi-task\napproach for cross-domain fake news detection, proving the efficacy of\nemotion-guided models in cross-domain settings for various datasets.", "published": "2022-11-26 05:12:07", "link": "http://arxiv.org/abs/2211.17108v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer-based Model for Word Level Language Identification in\n  Code-mixed Kannada-English Texts", "abstract": "Using code-mixed data in natural language processing (NLP) research currently\ngets a lot of attention. Language identification of social media code-mixed\ntext has been an interesting problem of study in recent years due to the\nadvancement and influences of social media in communication. This paper\npresents the Instituto Polit\\'ecnico Nacional, Centro de Investigaci\\'on en\nComputaci\\'on (CIC) team's system description paper for the CoLI-Kanglish\nshared task at ICON2022. In this paper, we propose the use of a Transformer\nbased model for word-level language identification in code-mixed Kannada\nEnglish texts. The proposed model on the CoLI-Kenglish dataset achieves a\nweighted F1-score of 0.84 and a macro F1-score of 0.61.", "published": "2022-11-26 02:39:19", "link": "http://arxiv.org/abs/2211.14459v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PCRED: Zero-shot Relation Triplet Extraction with Potential Candidate\n  Relation Selection and Entity Boundary Detection", "abstract": "Zero-shot relation triplet extraction (ZeroRTE) aims to extract relation\ntriplets from unstructured texts under the zero-shot setting, where the\nrelation sets at the training and testing stages are disjoint. Previous\nstate-of-the-art method handles this challenging task by leveraging pretrained\nlanguage models to generate data as additional training samples, which\nincreases the training cost and severely constrains the model performance. To\naddress the above issues, we propose a novel method named PCRED for ZeroRTE\nwith Potential Candidate Relation Selection and Entity Boundary Detection. The\nremarkable characteristic of PCRED is that it does not rely on additional data\nand still achieves promising performance. The model adopts a relation-first\nparadigm, recognizing unseen relations through candidate relation selection.\nWith this approach, the semantics of relations are naturally infused in the\ncontext. Entities are extracted based on the context and the semantics of\nrelations subsequently. We evaluate our model on two ZeroRTE datasets. The\nexperiment results show that our method consistently outperforms previous\nworks. Our code will be available at https://anonymous.4open.science/r/PCRED.", "published": "2022-11-26 04:27:31", "link": "http://arxiv.org/abs/2211.14477v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Predictive linguistic cues for fake news: a societal artificial\n  intelligence problem", "abstract": "Media news are making a large part of public opinion and, therefore, must not\nbe fake. News on web sites, blogs, and social media must be analyzed before\nbeing published. In this paper, we present linguistic characteristics of media\nnews items to differentiate between fake news and real news using machine\nlearning algorithms. Neural fake news generation, headlines created by\nmachines, semantic incongruities in text and image captions generated by\nmachine are other types of fake news problems. These problems use neural\nnetworks which mainly control distributional features rather than evidence. We\npropose applying correlation between features set and class, and correlation\namong the features to compute correlation attribute evaluation metric and\ncovariance metric to compute variance of attributes over the news items.\nFeatures unique, negative, positive, and cardinal numbers with high values on\nthe metrics are observed to provide a high area under the curve (AUC) and\nF1-score.", "published": "2022-11-26 07:50:01", "link": "http://arxiv.org/abs/2211.14505v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lexicon-injected Semantic Parsing for Task-Oriented Dialog", "abstract": "Recently, semantic parsing using hierarchical representations for dialog\nsystems has captured substantial attention. Task-Oriented Parse (TOP), a tree\nrepresentation with intents and slots as labels of nested tree nodes, has been\nproposed for parsing user utterances. Previous TOP parsing methods are limited\non tackling unseen dynamic slot values (e.g., new songs and locations added),\nwhich is an urgent matter for real dialog systems. To mitigate this issue, we\nfirst propose a novel span-splitting representation for span-based parser that\noutperforms existing methods. Then we present a novel lexicon-injected semantic\nparser, which collects slot labels of tree representation as a lexicon, and\ninjects lexical features to the span representation of parser. An additional\nslot disambiguation technique is involved to remove inappropriate span match\noccurrences from the lexicon. Our best parser produces a new state-of-the-art\nresult (87.62%) on the TOP dataset, and demonstrates its adaptability to\nfrequently updated slot lexicon entries in real task-oriented dialog, with no\nneed of retraining.", "published": "2022-11-26 07:59:20", "link": "http://arxiv.org/abs/2211.14508v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Who are you referring to? Coreference resolution in image narrations", "abstract": "Coreference resolution aims to identify words and phrases which refer to same\nentity in a text, a core task in natural language processing. In this paper, we\nextend this task to resolving coreferences in long-form narrations of visual\nscenes. First we introduce a new dataset with annotated coreference chains and\ntheir bounding boxes, as most existing image-text datasets only contain short\nsentences without coreferring expressions or labeled chains. We propose a new\ntechnique that learns to identify coreference chains using weak supervision,\nonly from image-text pairs and a regularization using prior linguistic\nknowledge. Our model yields large performance gains over several strong\nbaselines in resolving coreferences. We also show that coreference resolution\nhelps improving grounding narratives in images.", "published": "2022-11-26 13:33:42", "link": "http://arxiv.org/abs/2211.14563v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Contextual Expressive Text-to-Speech", "abstract": "The goal of expressive Text-to-speech (TTS) is to synthesize natural speech\nwith desired content, prosody, emotion, or timbre, in high expressiveness. Most\nof previous studies attempt to generate speech from given labels of styles and\nemotions, which over-simplifies the problem by classifying styles and emotions\ninto a fixed number of pre-defined categories. In this paper, we introduce a\nnew task setting, Contextual TTS (CTTS). The main idea of CTTS is that how a\nperson speaks depends on the particular context she is in, where the context\ncan typically be represented as text. Thus, in the CTTS task, we propose to\nutilize such context to guide the speech synthesis process instead of relying\non explicit labels of styles and emotions. To achieve this task, we construct a\nsynthetic dataset and develop an effective framework. Experiments show that our\nframework can generate high-quality expressive speech based on the given\ncontext both in synthetic datasets and real-world scenarios.", "published": "2022-11-26 12:06:21", "link": "http://arxiv.org/abs/2211.14548v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "eess.AS"}
{"title": "A Survey of Text Representation Methods and Their Genealogy", "abstract": "In recent years, with the advent of highly scalable\nartificial-neural-network-based text representation methods the field of\nnatural language processing has seen unprecedented growth and sophistication.\nIt has become possible to distill complex linguistic information of text into\nmultidimensional dense numeric vectors with the use of the distributional\nhypothesis. As a consequence, text representation methods have been evolving at\nsuch a quick pace that the research community is struggling to retain knowledge\nof the methods and their interrelations. We contribute threefold to this lack\nof compilation, composition, and systematization by providing a survey of\ncurrent approaches, by arranging them in a genealogy, and by conceptualizing a\ntaxonomy of text representation methods to examine and explain the\nstate-of-the-art. Our research is a valuable guide and reference for artificial\nintelligence researchers and practitioners interested in natural language\nprocessing applications such as recommender systems, chatbots, and sentiment\nanalysis.", "published": "2022-11-26 15:22:01", "link": "http://arxiv.org/abs/2211.14591v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Searching for Discriminative Words in Multidimensional Continuous\n  Feature Space", "abstract": "Word feature vectors have been proven to improve many NLP tasks. With recent\nadvances in unsupervised learning of these feature vectors, it became possible\nto train it with much more data, which also resulted in better quality of\nlearned features. Since it learns joint probability of latent features of\nwords, it has the advantage that we can train it without any prior knowledge\nabout the goal task we want to solve. We aim to evaluate the universal\napplicability property of feature vectors, which has been already proven to\nhold for many standard NLP tasks like part-of-speech tagging or syntactic\nparsing. In our case, we want to understand the topical focus of text documents\nand design an efficient representation suitable for discriminating different\ntopics. The discriminativeness can be evaluated adequately on text\ncategorisation task. We propose a novel method to extract discriminative\nkeywords from documents. We utilise word feature vectors to understand the\nrelations between words better and also understand the latent topics which are\ndiscussed in the text and not mentioned directly but inferred logically. We\nalso present a simple way to calculate document feature vectors out of\nextracted discriminative words. We evaluate our method on the four most popular\ndatasets for text categorisation. We show how different discriminative metrics\ninfluence the overall results. We demonstrate the effectiveness of our approach\nby achieving state-of-the-art results on text categorisation task using just a\nsmall number of extracted keywords. We prove that word feature vectors can\nsubstantially improve the topical inference of documents' meaning. We conclude\nthat distributed representation of words can be used to build higher levels of\nabstraction as we demonstrate and build feature vectors of documents.", "published": "2022-11-26 18:05:11", "link": "http://arxiv.org/abs/2211.14631v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.NE", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Toward Universal Text-to-Music Retrieval", "abstract": "This paper introduces effective design choices for text-to-music retrieval\nsystems. An ideal text-based retrieval system would support various input\nqueries such as pre-defined tags, unseen tags, and sentence-level descriptions.\nIn reality, most previous works mainly focused on a single query type (tag or\nsentence) which may not generalize to another input type. Hence, we review\nrecent text-based music retrieval systems using our proposed benchmark in two\nmain aspects: input text representation and training objectives. Our findings\nenable a universal text-to-music retrieval system that achieves comparable\nretrieval performances in both tag- and sentence-level inputs. Furthermore, the\nproposed multimodal representation generalizes to 9 different downstream music\nclassification tasks. We present the code and demo online.", "published": "2022-11-26 13:07:26", "link": "http://arxiv.org/abs/2211.14558v1", "categories": ["cs.IR", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
