{"title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for\n  Scientific Research", "abstract": "Recently, there has been growing interest in using Large Language Models\n(LLMs) for scientific research. Numerous benchmarks have been proposed to\nevaluate the ability of LLMs for scientific research. However, current\nbenchmarks are mostly based on pre-collected objective questions. This design\nsuffers from data leakage problem and lacks the evaluation of subjective Q/A\nability. In this paper, we propose SciEval, a comprehensive and\nmulti-disciplinary evaluation benchmark to address these issues. Based on\nBloom's taxonomy, SciEval covers four dimensions to systematically evaluate\nscientific research ability. In particular, we design a \"dynamic\" subset based\non scientific principles to prevent evaluation from potential data leakage.\nBoth objective and subjective questions are included in SciEval. These\ncharacteristics make SciEval a more effective benchmark for scientific research\nability evaluation of LLMs. Comprehensive experiments on most advanced LLMs\nshow that, although GPT-4 achieves SOTA performance compared to other LLMs,\nthere is still substantial room for improvement, especially for dynamic\nquestions. The codes and data are publicly available on\nhttps://github.com/OpenDFM/SciEval.", "published": "2023-08-25 03:05:33", "link": "http://arxiv.org/abs/2308.13149v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Spurious Correlation in Classification: 'Clever Hans' in\n  Translationese", "abstract": "Recent work has shown evidence of 'Clever Hans' behavior in high-performance\nneural translationese classifiers, where BERT-based classifiers capitalize on\nspurious correlations, in particular topic information, between data and target\nclassification labels, rather than genuine translationese signals.\nTranslationese signals are subtle (especially for professional translation) and\ncompete with many other signals in the data such as genre, style, author, and,\nin particular, topic. This raises the general question of how much of the\nperformance of a classifier is really due to spurious correlations in the data\nversus the signals actually targeted for by the classifier, especially for\nsubtle target signals and in challenging (low resource) data settings. We focus\non topic-based spurious correlation and approach the question from two\ndirections: (i) where we have no knowledge about spurious topic information and\nits distribution in the data, (ii) where we have some indication about the\nnature of spurious topic correlations. For (i) we develop a measure from first\nprinciples capturing alignment of unsupervised topics with target\nclassification labels as an indication of spurious topic information in the\ndata. We show that our measure is the same as purity in clustering and propose\na 'topic floor' (as in a 'noise floor') for classification. For (ii) we\ninvestigate masking of known spurious topic carriers in classification. Both\n(i) and (ii) contribute to quantifying and (ii) to mitigating spurious\ncorrelations.", "published": "2023-08-25 04:19:58", "link": "http://arxiv.org/abs/2308.13170v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Journey to the Center of the Knowledge Neurons: Discoveries of\n  Language-Independent Knowledge Neurons and Degenerate Knowledge Neurons", "abstract": "Pre-trained language models (PLMs) contain vast amounts of factual knowledge,\nbut how the knowledge is stored in the parameters remains unclear. This paper\ndelves into the complex task of understanding how factual knowledge is stored\nin multilingual PLMs, and introduces the Architecture-adapted Multilingual\nIntegrated Gradients method, which successfully localizes knowledge neurons\nmore precisely compared to current methods, and is more universal across\nvarious architectures and languages. Moreover, we conduct an in-depth\nexploration of knowledge neurons, leading to the following two important\ndiscoveries: (1) The discovery of Language-Independent Knowledge Neurons, which\nstore factual knowledge in a form that transcends language. We design\ncross-lingual knowledge editing experiments, demonstrating that the PLMs can\naccomplish this task based on language-independent neurons; (2) The discovery\nof Degenerate Knowledge Neurons, a novel type of neuron showing that different\nknowledge neurons can store the same fact. Its property of functional overlap\nendows the PLMs with a robust mastery of factual knowledge. We design\nfact-checking experiments, proving that the degenerate knowledge neurons can\nhelp the PLMs to detect wrong facts. Experiments corroborate these findings,\nshedding light on the mechanisms of factual knowledge storage in multilingual\nPLMs, and contribute valuable insights to the field. The code is available at\nhttps://github.com/heng840/AMIG.", "published": "2023-08-25 06:26:05", "link": "http://arxiv.org/abs/2308.13198v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM2KB: Constructing Knowledge Bases using instruction tuned context\n  aware Large Language Models", "abstract": "The advent of Large Language Models (LLM) has revolutionized the field of\nnatural language processing, enabling significant progress in various\napplications. One key area of interest is the construction of Knowledge Bases\n(KB) using these powerful models. Knowledge bases serve as repositories of\nstructured information, facilitating information retrieval and inference tasks.\nOur paper proposes LLM2KB, a system for constructing knowledge bases using\nlarge language models, with a focus on the Llama 2 architecture and the\nWikipedia dataset. We perform parameter efficient instruction tuning for\nLlama-2-13b-chat and StableBeluga-13B by training small injection models that\nhave only 0.05 % of the parameters of the base models using the Low Rank\nAdaptation (LoRA) technique. These injection models have been trained with\nprompts that are engineered to utilize Wikipedia page contexts of subject\nentities fetched using a Dense Passage Retrieval (DPR) algorithm, to answer\nrelevant object entities for a given subject entity and relation. Our best\nperforming model achieved an average F1 score of 0.6185 across 21 relations in\nthe LM-KBC challenge held at the ISWC 2023 conference.", "published": "2023-08-25 07:04:16", "link": "http://arxiv.org/abs/2308.13207v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Construction Grammar and Language Models", "abstract": "Recent progress in deep learning and natural language processing has given\nrise to powerful models that are primarily trained on a cloze-like task and\nshow some evidence of having access to substantial linguistic information,\nincluding some constructional knowledge. This groundbreaking discovery presents\nan exciting opportunity for a synergistic relationship between computational\nmethods and Construction Grammar research. In this chapter, we explore three\ndistinct approaches to the interplay between computational methods and\nConstruction Grammar: (i) computational methods for text analysis, (ii)\ncomputational Construction Grammar, and (iii) deep learning models, with a\nparticular focus on language models. We touch upon the first two approaches as\na contextual foundation for the use of computational methods before providing\nan accessible, yet comprehensive overview of deep learning models, which also\naddresses reservations construction grammarians may have. Additionally, we\ndelve into experiments that explore the emergence of constructionally relevant\ninformation within these models while also examining the aspects of\nConstruction Grammar that may pose challenges for these models. This chapter\naims to foster collaboration between researchers in the fields of natural\nlanguage processing and Construction Grammar. By doing so, we hope to pave the\nway for new insights and advancements in both these fields.", "published": "2023-08-25 11:37:56", "link": "http://arxiv.org/abs/2308.13315v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs", "abstract": "With the rapid evolution of large language models (LLMs), new and\nhard-to-predict harmful capabilities are emerging. This requires developers to\nbe able to identify risks through the evaluation of \"dangerous capabilities\" in\norder to responsibly deploy LLMs. In this work, we collect the first\nopen-source dataset to evaluate safeguards in LLMs, and deploy safer\nopen-source LLMs at a low cost. Our dataset is curated and filtered to consist\nonly of instructions that responsible language models should not follow. We\nannotate and assess the responses of six popular LLMs to these instructions.\nBased on our annotation, we proceed to train several BERT-like classifiers, and\nfind that these small classifiers can achieve results that are comparable with\nGPT-4 on automatic safety evaluation. Warning: this paper contains example data\nthat may be offensive, harmful, or biased.", "published": "2023-08-25 14:02:12", "link": "http://arxiv.org/abs/2308.13387v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Poison of Alignment", "abstract": "From the perspective of content safety issues, alignment has shown to limit\nlarge language models' (LLMs) harmful content generation. This intentional\nmethod of reinforcing models to not respond to certain user inputs seem to be\npresent in many modern open-source instruction tuning datasets such as\nOpenAssistant or Guanaco. We introduce a novel insight to an instruction-tuned\nmodel's performance affected by the presence of alignment in supervised\nfine-tuning dataset. To be specific, we noticed that alignment acts as if it is\npoisoning the instruction dataset. Experimentally, we demonstrate that aligned\nanswers significantly worsen the performance of the resulting fine-tuned\nmodel's on various reasoning benchmarks such as Big Bench (BBH), Massive\nMultitask Language Understanding (MMLU), Human Eval, and Discrete Reasoning\nOver Paragraphs (DROP), performing worse than the counterpart tuned without\nalignment by 4-33%.", "published": "2023-08-25 15:51:15", "link": "http://arxiv.org/abs/2308.13449v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ARTIST: ARTificial Intelligence for Simplified Text", "abstract": "Complex text is a major barrier for many citizens when accessing public\ninformation and knowledge. While often done manually, Text Simplification is a\nkey Natural Language Processing task that aims for reducing the linguistic\ncomplexity of a text while preserving the original meaning. Recent advances in\nGenerative Artificial Intelligence (AI) have enabled automatic text\nsimplification both on the lexical and syntactical levels. However, as\napplications often focus on English, little is understood about the\neffectiveness of Generative AI techniques on low-resource languages such as\nDutch. For this reason, we carry out empirical studies to understand the\nbenefits and limitations of applying generative technologies for text\nsimplification and provide the following outcomes: 1) the design and\nimplementation for a configurable text simplification pipeline that\norchestrates state-of-the-art generative text simplification models, domain and\nreader adaptation, and visualisation modules; 2) insights and lessons learned,\nshowing the strengths of automatic text simplification while exposing the\nchallenges in handling cultural and commonsense knowledge. These outcomes\nrepresent a first step in the exploration of Dutch text simplification and shed\nlight on future endeavours both for research and practice.", "published": "2023-08-25 16:06:06", "link": "http://arxiv.org/abs/2308.13458v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training and Meta-Evaluating Machine Translation Evaluation Metrics at\n  the Paragraph Level", "abstract": "As research on machine translation moves to translating text beyond the\nsentence level, it remains unclear how effective automatic evaluation metrics\nare at scoring longer translations. In this work, we first propose a method for\ncreating paragraph-level data for training and meta-evaluating metrics from\nexisting sentence-level data. Then, we use these new datasets to benchmark\nexisting sentence-level metrics as well as train learned metrics at the\nparagraph level. Interestingly, our experimental results demonstrate that using\nsentence-level metrics to score entire paragraphs is equally as effective as\nusing a metric designed to work at the paragraph level. We speculate this\nresult can be attributed to properties of the task of reference-based\nevaluation as well as limitations of our datasets with respect to capturing all\ntypes of phenomena that occur in paragraph-level translations.", "published": "2023-08-25 17:31:46", "link": "http://arxiv.org/abs/2308.13506v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Ensemble Approach to Personalized Real Time Predictive Writing for\n  Experts", "abstract": "Completing a sentence, phrase or word after typing few words / characters is\nvery helpful for Intuit financial experts, while taking notes or having a live\nchat with users, since they need to write complex financial concepts more\nefficiently and accurately many times in a day. In this paper, we tie together\ndifferent approaches like large language models, traditional Markov Models and\nchar level models to create an end-to-end system to provide personalised\nsentence/word auto-complete suggestions to experts, under strict latency\nconstraints. Proposed system can auto-complete sentences, phrases or words\nwhile writing with personalisation and can be trained with very less data and\nresources with good efficiency. Our proposed system is not only efficient and\npersonalized but also robust as it leverages multiple machine learning\ntechniques along with transfer learning approach to fine tune large language\nmodel with Intuit specific data. This ensures that even in cases of rare or\nunusual phrases, the system can provide relevant auto-complete suggestions in\nnear real time. Survey has showed that this system saves expert note-taking\ntime and boosts expert confidence in their communication with teammates and\nclients. Since enabling this predictive writing feature for QBLive experts,\nmore than a million keystrokes have been saved based on these suggestions. We\nhave done comparative study for our ensemble choice. Moreover this feature can\nbe integrated with any product which has writing facility within a very short\nperiod of time.", "published": "2023-08-25 12:45:46", "link": "http://arxiv.org/abs/2308.13576v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OmniQuant: Omnidirectionally Calibrated Quantization for Large Language\n  Models", "abstract": "Large language models (LLMs) have revolutionized natural language processing\ntasks. However, their practical deployment is hindered by their immense memory\nand computation requirements. Although recent post-training quantization (PTQ)\nmethods are effective in reducing memory footprint and improving the\ncomputational efficiency of LLM, they hand-craft quantization parameters,\nleading to low performance, especially in extremely low-bit quantization. To\ntackle this issue, we introduce an Omnidirectionally calibrated Quantization\n(\\textbf{OmniQuant}) technique for LLMs, which achieves good performance in\ndiverse quantization settings while maintaining the computational efficiency of\nPTQ by efficiently optimizing various quantization parameters. OmniQuant\ncomprises two innovative components including Learnable Weight Clipping (LWC)\nand Learnable Equivalent Transformation (LET). LWC modulates the extreme values\nof weights by optimizing the clipping threshold. Meanwhile, LET tackles\nactivation outliers by shifting the challenge of quantization from activations\nto weights. Operating within a differentiable framework using block-wise error\nminimization, OmniQuant can optimize the quantization process efficiently for\nboth weight-only and weight-activation quantization. For instance, the LLaMA-2\nmodel family size 7-70B can be processed with OmniQuant on a single A100-40G\nGPU within 1-16 hours using 128 samples. Extensive experiments validate\nOmniQuant's superior performance across diverse quantization configurations\nsuch as W4A4 (4-bit weight, 4-bit activation), W6A6, W4A16, W3A16, and W2A16.\nAdditionally, OmniQuant demonstrates effectiveness in instruction-tuned models\nand delivers notable improvements in inference speed and memory reduction on\nreal devices. Codes are available at\n\\url{https://github.com/OpenGVLab/OmniQuant}.", "published": "2023-08-25 02:28:35", "link": "http://arxiv.org/abs/2308.13137v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MatchXML: An Efficient Text-label Matching Framework for Extreme\n  Multi-label Text Classification", "abstract": "The eXtreme Multi-label text Classification(XMC) refers to training a\nclassifier that assigns a text sample with relevant labels from an extremely\nlarge-scale label set (e.g., millions of labels). We propose MatchXML, an\nefficient text-label matching framework for XMC. We observe that the label\nembeddings generated from the sparse Term Frequency-Inverse Document\nFrequency(TF-IDF) features have several limitations. We thus propose label2vec\nto effectively train the semantic dense label embeddings by the Skip-gram\nmodel. The dense label embeddings are then used to build a Hierarchical Label\nTree by clustering. In fine-tuning the pre-trained encoder Transformer, we\nformulate the multi-label text classification as a text-label matching problem\nin a bipartite graph. We then extract the dense text representations from the\nfine-tuned Transformer. Besides the fine-tuned dense text embeddings, we also\nextract the static dense sentence embeddings from a pre-trained Sentence\nTransformer. Finally, a linear ranker is trained by utilizing the sparse TF-IDF\nfeatures, the fine-tuned dense text representations and static dense sentence\nfeatures. Experimental results demonstrate that MatchXML achieves\nstate-of-the-art accuracy on five out of six datasets. As for the speed,\nMatchXML outperforms the competing methods on all the six datasets. Our source\ncode is publicly available at https://github.com/huiyegit/MatchXML.", "published": "2023-08-25 02:32:36", "link": "http://arxiv.org/abs/2308.13139v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DISGO: Automatic End-to-End Evaluation for Scene Text OCR", "abstract": "This paper discusses the challenges of optical character recognition (OCR) on\nnatural scenes, which is harder than OCR on documents due to the wild content\nand various image backgrounds. We propose to uniformly use word error rates\n(WER) as a new measurement for evaluating scene-text OCR, both end-to-end (e2e)\nperformance and individual system component performances. Particularly for the\ne2e metric, we name it DISGO WER as it considers Deletion, Insertion,\nSubstitution, and Grouping/Ordering errors. Finally we propose to utilize the\nconcept of super blocks to automatically compute BLEU scores for e2e OCR\nmachine translation. The small SCUT public test set is used to demonstrate WER\nperformance by a modularized OCR system.", "published": "2023-08-25 04:45:37", "link": "http://arxiv.org/abs/2308.13173v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "How to Evaluate the Generalization of Detection? A Benchmark for\n  Comprehensive Open-Vocabulary Detection", "abstract": "Object detection (OD) in computer vision has made significant progress in\nrecent years, transitioning from closed-set labels to open-vocabulary detection\n(OVD) based on large-scale vision-language pre-training (VLP). However, current\nevaluation methods and datasets are limited to testing generalization over\nobject types and referral expressions, which do not provide a systematic,\nfine-grained, and accurate benchmark of OVD models' abilities. In this paper,\nwe propose a new benchmark named OVDEval, which includes 9 sub-tasks and\nintroduces evaluations on commonsense knowledge, attribute understanding,\nposition understanding, object relation comprehension, and more. The dataset is\nmeticulously created to provide hard negatives that challenge models' true\nunderstanding of visual and linguistic input. Additionally, we identify a\nproblem with the popular Average Precision (AP) metric when benchmarking models\non these fine-grained label datasets and propose a new metric called\nNon-Maximum Suppression Average Precision (NMS-AP) to address this issue.\nExtensive experimental results show that existing top OVD models all fail on\nthe new tasks except for simple object types, demonstrating the value of the\nproposed dataset in pinpointing the weakness of current OVD models and guiding\nfuture research. Furthermore, the proposed NMS-AP metric is verified by\nexperiments to provide a much more truthful evaluation of OVD models, whereas\ntraditional AP metrics yield deceptive results. Data is available at\n\\url{https://github.com/om-ai-lab/OVDEval}", "published": "2023-08-25 04:54:32", "link": "http://arxiv.org/abs/2308.13177v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Chunk, Align, Select: A Simple Long-sequence Processing Method for\n  Transformers", "abstract": "Although dominant in natural language processing, transformer-based models\nremain challenged by the task of long-sequence processing, because the\ncomputational cost of self-attention operations in transformers swells\nquadratically with the input sequence length. To alleviate the complexity of\nlong-sequence processing, we propose a simple framework to enable the\noffthe-shelf pre-trained transformers to process much longer sequences, while\nthe computation and memory costs remain growing linearly with the input\nsequence lengths. More specifically, our method divides each long-sequence\ninput into a batch of chunks, then aligns the interchunk information during the\nencoding steps, and finally selects the most representative hidden states from\nthe encoder for the decoding process. To extract inter-chunk semantic\ninformation, we align the start and end token embeddings among chunks in each\nencoding transformer block. To learn an effective hidden selection policy, we\ndesign a dual updating scheme inspired by reinforcement learning, which regards\nthe decoders of transformers as environments, and the downstream performance\nmetrics as the rewards to evaluate the hidden selection actions. Our empirical\nresults on real-world long-text summarization and reading comprehension tasks\ndemonstrate effective improvements compared to prior longsequence processing\nbaselines.", "published": "2023-08-25 05:52:05", "link": "http://arxiv.org/abs/2308.13191v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for\n  Knowledge-intensive Question Answering", "abstract": "Equipped with Chain-of-Thought (CoT), Large language models (LLMs) have shown\nimpressive reasoning ability in various downstream tasks. Even so, suffering\nfrom hallucinations and the inability to access external knowledge, LLMs often\ncome with incorrect or unfaithful intermediate reasoning steps, especially in\nthe context of answering knowledge-intensive tasks such as KBQA. To alleviate\nthis issue, we propose a framework called Knowledge-Driven Chain-of-Thought\n(KD-CoT) to verify and modify reasoning traces in CoT via interaction with\nexternal knowledge, and thus overcome the hallucinations and error propagation.\nConcretely, we formulate the CoT rationale process of LLMs into a structured\nmulti-round QA format. In each round, LLMs interact with a QA system that\nretrieves external knowledge and produce faithful reasoning traces based on\nretrieved precise answers. The structured CoT reasoning of LLMs is facilitated\nby our developed KBQA CoT collection, which serves as in-context learning\ndemonstrations and can also be utilized as feedback augmentation to train a\nrobust retriever. Extensive experiments on WebQSP and ComplexWebQuestion\ndatasets demonstrate the effectiveness of proposed KD-CoT in task-solving\nreasoning generation, which outperforms the vanilla CoT ICL with an absolute\nsuccess rate of 8.0% and 5.1%. Furthermore, our proposed feedback-augmented\nretriever outperforms the state-of-the-art baselines for retrieving knowledge,\nachieving significant improvement in Hit and recall performance. Our code and\ndata are released on https://github.com/AdelWang/KD-CoT/tree/main.", "published": "2023-08-25 09:23:55", "link": "http://arxiv.org/abs/2308.13259v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Assessing Keyness using Permutation Tests", "abstract": "We propose a resampling-based approach for assessing keyness in corpus\nlinguistics based on suggestions by Gries (2006, 2022). Traditional approaches\nbased on hypothesis tests (e.g. Likelihood Ratio) model the copora as\nindependent identically distributed samples of tokens. This model does not\naccount for the often observed uneven distribution of occurences of a word\nacross a corpus. When occurences of a word are concentrated in few documents,\nlarge values of LLR and similar scores are in fact much more likely than\naccounted for by the token-by-token sampling model, leading to false positives.\n  We replace the token-by-token sampling model by a model where corpora are\nsamples of documents rather than tokens, which is much closer to the way\ncorpora are actually assembled. We then use a permutation approach to\napproximate the distribution of a given keyness score under the null hypothesis\nof equal frequencies and obtain p-values for assessing significance. We do not\nneed any assumption on how the tokens are organized within or across documents,\nand the approach works with basically *any* keyness score. Hence, appart from\nobtaining more accurate p-values for scores like LLR, we can also assess\nsignificance for e.g. the logratio which has been proposed as a measure of\neffect size.\n  An efficient implementation of the proposed approach is provided in the `R`\npackage `keyperm` available from github.", "published": "2023-08-25 13:52:57", "link": "http://arxiv.org/abs/2308.13383v1", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Prompting a Large Language Model to Generate Diverse Motivational\n  Messages: A Comparison with Human-Written Messages", "abstract": "Large language models (LLMs) are increasingly capable and prevalent, and can\nbe used to produce creative content. The quality of content is influenced by\nthe prompt used, with more specific prompts that incorporate examples generally\nproducing better results. On from this, it could be seen that using\ninstructions written for crowdsourcing tasks (that are specific and include\nexamples to guide workers) could prove effective LLM prompts. To explore this,\nwe used a previous crowdsourcing pipeline that gave examples to people to help\nthem generate a collectively diverse corpus of motivational messages. We then\nused this same pipeline to generate messages using GPT-4, and compared the\ncollective diversity of messages from: (1) crowd-writers, (2) GPT-4 using the\npipeline, and (3 & 4) two baseline GPT-4 prompts. We found that the LLM prompts\nusing the crowdsourcing pipeline caused GPT-4 to produce more diverse messages\nthan the two baseline prompts. We also discuss implications from messages\ngenerated by both human writers and LLMs.", "published": "2023-08-25 16:35:06", "link": "http://arxiv.org/abs/2308.13479v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Ngambay-French Neural Machine Translation (sba-Fr)", "abstract": "In Africa, and the world at large, there is an increasing focus on developing\nNeural Machine Translation (NMT) systems to overcome language barriers. NMT for\nLow-resource language is particularly compelling as it involves learning with\nlimited labelled data. However, obtaining a well-aligned parallel corpus for\nlow-resource languages can be challenging. The disparity between the\ntechnological advancement of a few global languages and the lack of research on\nNMT for local languages in Chad is striking. End-to-end NMT trials on\nlow-resource Chad languages have not been attempted. Additionally, there is a\ndearth of online and well-structured data gathering for research in Natural\nLanguage Processing, unlike some African languages. However, a guided approach\nfor data gathering can produce bitext data for many Chadian language\ntranslation pairs with well-known languages that have ample data. In this\nproject, we created the first sba-Fr Dataset, which is a corpus of\nNgambay-to-French translations, and fine-tuned three pre-trained models using\nthis dataset. Our experiments show that the M2M100 model outperforms other\nmodels with high BLEU scores on both original and original+synthetic data. The\npublicly available bitext dataset can be used for research purposes.", "published": "2023-08-25 17:13:20", "link": "http://arxiv.org/abs/2308.13497v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ChatGPT as Data Augmentation for Compositional Generalization: A Case\n  Study in Open Intent Detection", "abstract": "Open intent detection, a crucial aspect of natural language understanding,\ninvolves the identification of previously unseen intents in user-generated\ntext. Despite the progress made in this field, challenges persist in handling\nnew combinations of language components, which is essential for compositional\ngeneralization. In this paper, we present a case study exploring the use of\nChatGPT as a data augmentation technique to enhance compositional\ngeneralization in open intent detection tasks. We begin by discussing the\nlimitations of existing benchmarks in evaluating this problem, highlighting the\nneed for constructing datasets for addressing compositional generalization in\nopen intent detection tasks. By incorporating synthetic data generated by\nChatGPT into the training process, we demonstrate that our approach can\neffectively improve model performance. Rigorous evaluation of multiple\nbenchmarks reveals that our method outperforms existing techniques and\nsignificantly enhances open intent detection capabilities. Our findings\nunderscore the potential of large language models like ChatGPT for data\naugmentation in natural language understanding tasks.", "published": "2023-08-25 17:51:23", "link": "http://arxiv.org/abs/2308.13517v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Discovering Mental Health Research Topics with Topic Modeling", "abstract": "Mental health significantly influences various aspects of our daily lives,\nand its importance has been increasingly recognized by the research community\nand the general public, particularly in the wake of the COVID-19 pandemic. This\nheightened interest is evident in the growing number of publications dedicated\nto mental health in the past decade. In this study, our goal is to identify\ngeneral trends in the field and pinpoint high-impact research topics by\nanalyzing a large dataset of mental health research papers. To accomplish this,\nwe collected abstracts from various databases and trained a customized\nSentence-BERT based embedding model leveraging the BERTopic framework. Our\ndataset comprises 96,676 research papers pertaining to mental health, enabling\nus to examine the relationships between different topics using their abstracts.\nTo evaluate the effectiveness of the model, we compared it against two other\nstate-of-the-art methods: Top2Vec model and LDA-BERT model. The model\ndemonstrated superior performance in metrics that measure topic diversity and\ncoherence. To enhance our analysis, we also generated word clouds to provide a\ncomprehensive overview of the machine learning models applied in mental health\nresearch, shedding light on commonly utilized techniques and emerging trends.\nFurthermore, we provide a GitHub link* to the dataset used in this paper,\nensuring its accessibility for further research endeavors.", "published": "2023-08-25 05:25:05", "link": "http://arxiv.org/abs/2308.13569v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Text Style Transfer Evaluation Using Large Language Models", "abstract": "Evaluating Text Style Transfer (TST) is a complex task due to its\nmultifaceted nature. The quality of the generated text is measured based on\nchallenging factors, such as style transfer accuracy, content preservation, and\noverall fluency. While human evaluation is considered to be the gold standard\nin TST assessment, it is costly and often hard to reproduce. Therefore,\nautomated metrics are prevalent in these domains. Nevertheless, it remains\nunclear whether these automated metrics correlate with human evaluations.\nRecent strides in Large Language Models (LLMs) have showcased their capacity to\nmatch and even exceed average human performance across diverse, unseen tasks.\nThis suggests that LLMs could be a feasible alternative to human evaluation and\nother automated metrics in TST evaluation. We compare the results of different\nLLMs in TST using multiple input prompts. Our findings highlight a strong\ncorrelation between (even zero-shot) prompting and human evaluation, showing\nthat LLMs often outperform traditional automated metrics. Furthermore, we\nintroduce the concept of prompt ensembling, demonstrating its ability to\nenhance the robustness of TST evaluation. This research contributes to the\nongoing evaluation of LLMs in diverse tasks, offering insights into successful\noutcomes and areas of limitation.", "published": "2023-08-25 13:07:33", "link": "http://arxiv.org/abs/2308.13577v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LSTM-based QoE Evaluation for Web Microservices' Reputation Scoring", "abstract": "Sentiment analysis is the task of mining the authors' opinions about specific\nentities. It allows organizations to monitor different services in real time\nand act accordingly. Reputation is what is generally said or believed about\npeople or things. Informally, reputation combines the measure of reliability\nderived from feedback, reviews, and ratings gathered from users, which reflect\ntheir quality of experience (QoE) and can either increase or harm the\nreputation of the provided services. In this study, we propose to perform\nsentiment analysis on web microservices reviews to exploit the provided\ninformation to assess and score the microservices' reputation. Our proposed\napproach uses the Long Short-Term Memory (LSTM) model to perform sentiment\nanalysis and the Net Brand Reputation (NBR) algorithm to assess reputation\nscores for microservices. This approach is tested on a set of more than 10,000\nreviews related to 15 Amazon Web microservices, and the experimental results\nhave shown that our approach is more accurate than existing approaches, with an\naccuracy and precision of 93% obtained after applying an oversampling strategy\nand a resulting reputation score of the considered microservices community of\n89%.", "published": "2023-08-25 17:23:12", "link": "http://arxiv.org/abs/2308.13590v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "1.5 million materials narratives generated by chatbots", "abstract": "The advent of artificial intelligence (AI) has enabled a comprehensive\nexploration of materials for various applications. However, AI models often\nprioritize frequently encountered materials in the scientific literature,\nlimiting the selection of suitable candidates based on inherent physical and\nchemical properties. To address this imbalance, we have generated a dataset of\n1,494,017 natural language-material paragraphs based on combined OQMD,\nMaterials Project, JARVIS, COD and AFLOW2 databases, which are dominated by ab\ninitio calculations and tend to be much more evenly distributed on the periodic\ntable. The generated text narratives were then polled and scored by both human\nexperts and ChatGPT-4, based on three rubrics: technical accuracy, language and\nstructure, and relevance and depth of content, showing similar scores but with\nhuman-scored depth of content being the most lagging. The merger of\nmulti-modality data sources and large language model (LLM) holds immense\npotential for AI frameworks to help the exploration and discovery of\nsolid-state materials for specific applications.", "published": "2023-08-25 22:00:53", "link": "http://arxiv.org/abs/2308.13687v1", "categories": ["cond-mat.mtrl-sci", "cs.CL"], "primary_category": "cond-mat.mtrl-sci"}
{"title": "On the Depth between Beam Search and Exhaustive Search for Text\n  Generation", "abstract": "Beam search and exhaustive search are two extreme ends of text decoding\nalgorithms with respect to the search depth. Beam search is limited in both\nsearch width and depth, whereas exhaustive search is a global search that has\nno such limitations. Surprisingly, beam search is not only computationally\ncheaper but also performs better than exhaustive search despite its higher\nsearch error. Plenty of research has investigated a range of beam widths, from\nsmall to large, and reported that a beam width that is neither too large nor\ntoo small is desirable. However, in terms of search depth, only the two extreme\nends, beam search and exhaustive search are studied intensively. In this paper,\nwe examine a range of search depths between the two extremes to discover the\ndesirable search depth. To this end, we introduce Lookahead Beam Search (LBS),\na multi-step lookahead search that optimizes the objective considering a fixed\nnumber of future steps. Beam search and exhaustive search are special cases of\nLBS where the lookahead depth is set to $0$ and $\\infty$, respectively. We\nempirically evaluate the performance of LBS and find that it outperforms beam\nsearch overall on machine translation tasks. The result suggests there is room\nfor improvement in beam search by searching deeper. Inspired by the analysis,\nwe propose Lookbehind Heuristic Beam Search, a computationally feasible search\nalgorithm that heuristically simulates LBS with 1-step lookahead. The empirical\nresults show that the proposed method outperforms vanilla beam search on\nmachine translation and text summarization tasks.", "published": "2023-08-25 22:57:53", "link": "http://arxiv.org/abs/2308.13696v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WellXplain: Wellness Concept Extraction and Classification in Reddit\n  Posts for Mental Health Analysis", "abstract": "During the current mental health crisis, the importance of identifying\npotential indicators of mental issues from social media content has surged.\nOverlooking the multifaceted nature of mental and social well-being can have\ndetrimental effects on one's mental state. In traditional therapy sessions,\nprofessionals manually pinpoint the origins and outcomes of underlying mental\nchallenges, a process both detailed and time-intensive. We introduce an\napproach to this intricate mental health analysis by framing the identification\nof wellness dimensions in Reddit content as a wellness concept extraction and\ncategorization challenge. We've curated a unique dataset named WELLXPLAIN,\ncomprising 3,092 entries and totaling 72,813 words. Drawing from Halbert L.\nDunn's well-regarded wellness theory, our team formulated an annotation\nframework along with guidelines. This dataset also includes human-marked\ntextual segments, offering clear reasoning for decisions made in the wellness\nconcept categorization process. Our aim in publishing this dataset and\nanalyzing initial benchmarks is to spearhead the creation of advanced language\nmodels tailored for healthcare-focused concept extraction and categorization.", "published": "2023-08-25 23:50:05", "link": "http://arxiv.org/abs/2308.13710v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Misinformation Concierge: A Proof-of-Concept with Curated Twitter\n  Dataset on COVID-19 Vaccination", "abstract": "We demonstrate the Misinformation Concierge, a proof-of-concept that provides\nactionable intelligence on misinformation prevalent in social media.\nSpecifically, it uses language processing and machine learning tools to\nidentify subtopics of discourse and discern non/misleading posts; presents\nstatistical reports for policy-makers to understand the big picture of\nprevalent misinformation in a timely manner; and recommends rebuttal messages\nfor specific pieces of misinformation, identified from within the corpus of\ndata - providing means to intervene and counter misinformation promptly. The\nMisinformation Concierge proof-of-concept using a curated dataset is accessible\nat: https://demo-frontend-uy34.onrender.com/", "published": "2023-08-25 10:06:05", "link": "http://arxiv.org/abs/2309.00639v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Media of Langue: The Interface for Exploring Word Translation\n  Network/Space", "abstract": "In the human activity of word translation, two languages face each other,\nmutually searching their own language system for the semantic place of words in\nthe other language. We discover the huge network formed by the chain of these\nmutual translations as Word Translation Network, a network where words are\nnodes, and translation volume is represented as edges, and propose Media of\nLangue, a novel interface for exploring this network. Media of Langue points to\nthe semantic configurations of many words in multiple languages at once,\ncontaining the information of existing dictionaries such as bilingual and\nsynonym dictionaries. We have also implemented and published this interface as\na web application, focusing on seven language pairs. This paper first defines\nthe Word Translation Network and describes how to actually construct the\nnetwork from bilingual corpora, followed by an analysis of the properties of\nthe network. Next, we explain how to design a Media of Langue using the Word\nTranslation Network, and finally, we analyze the features of the Media of\nLangue as a dictionary. Our website is https://www.media-of-langue.org .", "published": "2023-08-25 03:54:20", "link": "http://arxiv.org/abs/2309.08609v4", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Formalising Natural Language Quantifiers for Human-Robot Interactions", "abstract": "We present a method for formalising quantifiers in natural language in the\ncontext of human-robot interactions. The solution is based on first-order logic\nextended with capabilities to represent the cardinality of variables, operating\nsimilarly to generalised quantifiers. To demonstrate the method, we designed an\nend-to-end system able to receive input as natural language, convert it into a\nformal logical representation, evaluate it, and return a result or send a\ncommand to a simulated robot.", "published": "2023-08-25 06:05:57", "link": "http://arxiv.org/abs/2308.13192v1", "categories": ["cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Transforming the Output of Generative Pre-trained Transformer: The\n  Influence of the PGI Framework on Attention Dynamics", "abstract": "This paper presents a novel approach named Persona-Grouping-Intelligence\n(PGI), which has been crafted to tackle the challenges posed by GPT models when\napplied to real-world business issues. PGI leverages the inherent capabilities\nof the GPT model to comprehend intricate language structures and generate\nresponses that are contextually relevant. The experiment occurred in a business\nscenario where human intelligence was being underutilized due to less optimized\nbusiness processes. The primary objective of this approach is to leverage GPT\nmodels to reduce the workload on humans in tasks that are extensive,\nmonotonous, and repetitive. Instead, the focus is redirected toward\ndecision-making activities. Remarkably, the experiment yielded an accuracy rate\nof 93.81% in validating 4,000 responses generated by the model, underscoring\nthe effectiveness of the PGI strategies. Effectively addressing the issue of\nunderutilized human intelligence, this paradigm shift aligns business\nenvironments with dynamic machine intelligence, enabling them to navigate the\nintricacies of real-world challenges. This approach facilitates the practical\nutilization of these models to tackle actual problems. The methodology offers\nan opportunity to reshape the fundamental structure of business processes by\nseamlessly integrating human decision-making with adaptable machine\nintelligence. Consequently, this optimization enhances operational efficiency\nand elevates strategic decision-making across diverse business contexts.", "published": "2023-08-25 11:41:05", "link": "http://arxiv.org/abs/2308.13317v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Decoupled Structure for Improved Adaptability of End-to-End Models", "abstract": "Although end-to-end (E2E) trainable automatic speech recognition (ASR) has\nshown great success by jointly learning acoustic and linguistic information, it\nstill suffers from the effect of domain shifts, thus limiting potential\napplications. The E2E ASR model implicitly learns an internal language model\n(LM) which characterises the training distribution of the source domain, and\nthe E2E trainable nature makes the internal LM difficult to adapt to the target\ndomain with text-only data To solve this problem, this paper proposes decoupled\nstructures for attention-based encoder-decoder (Decoupled-AED) and neural\ntransducer (Decoupled-Transducer) models, which can achieve flexible domain\nadaptation in both offline and online scenarios while maintaining robust\nintra-domain performance. To this end, the acoustic and linguistic parts of the\nE2E model decoder (or prediction network) are decoupled, making the linguistic\ncomponent (i.e. internal LM) replaceable. When encountering a domain shift, the\ninternal LM can be directly replaced during inference by a target-domain LM,\nwithout re-training or using domain-specific paired speech-text data.\nExperiments for E2E ASR models trained on the LibriSpeech-100h corpus showed\nthat the proposed decoupled structure gave 15.1% and 17.2% relative word error\nrate reductions on the TED-LIUM 2 and AESRC2020 corpora while still maintaining\nperformance on intra-domain data.", "published": "2023-08-25 12:31:12", "link": "http://arxiv.org/abs/2308.13345v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "EntropyRank: Unsupervised Keyphrase Extraction via Side-Information\n  Optimization for Language Model-based Text Compression", "abstract": "We propose an unsupervised method to extract keywords and keyphrases from\ntexts based on a pre-trained language model (LM) and Shannon's information\nmaximization. Specifically, our method extracts phrases having the highest\nconditional entropy under the LM. The resulting set of keyphrases turns out to\nsolve a relevant information-theoretic problem: if provided as side\ninformation, it leads to the expected minimal binary code length in compressing\nthe text using the LM and an entropy encoder. Alternately, the resulting set is\nan approximation via a causal LM to the set of phrases that minimize the\nentropy of the text when conditioned upon it. Empirically, the method provides\nresults comparable to the most commonly used methods in various keyphrase\nextraction benchmark challenges.", "published": "2023-08-25 14:23:40", "link": "http://arxiv.org/abs/2308.13399v2", "categories": ["cs.CL", "cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.CL"}
{"title": "Leveraging Knowledge and Reinforcement Learning for Enhanced Reliability\n  of Language Models", "abstract": "The Natural Language Processing(NLP) community has been using crowd sourcing\ntechniques to create benchmark datasets such as General Language Understanding\nand Evaluation(GLUE) for training modern Language Models such as BERT. GLUE\ntasks measure the reliability scores using inter annotator metrics i.e. Cohens\nKappa. However, the reliability aspect of LMs has often been overlooked. To\ncounter this problem, we explore a knowledge-guided LM ensembling approach that\nleverages reinforcement learning to integrate knowledge from ConceptNet and\nWikipedia as knowledge graph embeddings. This approach mimics human annotators\nresorting to external knowledge to compensate for information deficits in the\ndatasets. Across nine GLUE datasets, our research shows that ensembling\nstrengthens reliability and accuracy scores, outperforming state of the art.", "published": "2023-08-25 16:11:08", "link": "http://arxiv.org/abs/2308.13467v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Large Language Models in Analyzing Crash Narratives -- A Comparative\n  Study of ChatGPT, BARD and GPT-4", "abstract": "In traffic safety research, extracting information from crash narratives\nusing text analysis is a common practice. With recent advancements of large\nlanguage models (LLM), it would be useful to know how the popular LLM\ninterfaces perform in classifying or extracting information from crash\nnarratives. To explore this, our study has used the three most popular publicly\navailable LLM interfaces- ChatGPT, BARD and GPT4. This study investigated their\nusefulness and boundaries in extracting information and answering queries\nrelated to accidents from 100 crash narratives from Iowa and Kansas. During the\ninvestigation, their capabilities and limitations were assessed and their\nresponses to the queries were compared. Five questions were asked related to\nthe narratives: 1) Who is at-fault? 2) What is the manner of collision? 3) Has\nthe crash occurred in a work-zone? 4) Did the crash involve pedestrians? and 5)\nWhat are the sequence of harmful events in the crash? For questions 1 through\n4, the overall similarity among the LLMs were 70%, 35%, 96% and 89%,\nrespectively. The similarities were higher while answering direct questions\nrequiring binary responses and significantly lower for complex questions. To\ncompare the responses to question 5, network diagram and centrality measures\nwere analyzed. The network diagram from the three LLMs were not always similar\nalthough they sometimes have the same influencing events with high in-degree,\nout-degree and betweenness centrality. This study suggests using multiple\nmodels to extract viable information from narratives. Also, caution must be\npracticed while using these interfaces to obtain crucial safety related\ninformation.", "published": "2023-08-25 00:09:16", "link": "http://arxiv.org/abs/2308.13563v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DARWIN Series: Domain Specific Large Language Models for Natural Science", "abstract": "Emerging tools bring forth fresh approaches to work, and the field of natural\nscience is no different. In natural science, traditional manual, serial, and\nlabour-intensive work is being augmented by automated, parallel, and iterative\nprocesses driven by artificial intelligence-based experimental automation and\nmore. To add new capabilities in natural science, enabling the acceleration and\nenrichment of automation of the discovery process, we present DARWIN, a series\nof tailored LLMs for natural science, mainly in physics, chemistry, and\nmaterial science. This series relies on open-source LLM, incorporating\nstructured and unstructured scientific knowledge from public datasets and\nliterature. We fine-tuned the models using over 60,000 instruction data points,\nemphasizing factual correctness. During the fine-tuning, we introduce the\nScientific Instruction Generation (SIG) model, automating instruction\ngeneration from scientific texts. This eliminates the need for manual\nextraction or domain-specific knowledge graphs and efficiently injects\nscientific knowledge into the model. We also explore multi-task training\nstrategies, revealing interconnections between scientific tasks. DARWIN series\nnot only achieves state-of-the-art results on various scientific tasks but also\ndiminishes reliance on closed-source AI models. Our research showcases the\nability of LLM in the scientific domain, with the overarching goal of fostering\nprosperity within the broader AI for science community.", "published": "2023-08-25 01:40:48", "link": "http://arxiv.org/abs/2308.13565v1", "categories": ["cs.CL", "cond-mat.mtrl-sci", "physics.app-ph"], "primary_category": "cs.CL"}
{"title": "MLLM-DataEngine: An Iterative Refinement Approach for MLLM", "abstract": "Despite the great advance of Multimodal Large Language Models (MLLMs) in both\ninstruction dataset building and benchmarking, the independence of training and\nevaluation makes current MLLMs hard to further improve their capability under\nthe guidance of evaluation results with a relatively low human cost. In this\npaper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data\ngeneration, model training, and evaluation. Within each loop iteration, the\nMLLM-DataEngine first analyze the weakness of the model based on the evaluation\nresults, then generate a proper incremental dataset for the next training\niteration and enhance the model capability iteratively. Compared with previous\ndata collection methods which are separate from the benchmarking, the data\ngenerated by MLLM-DataEngine shows better targeting, quality, and correctness.\nFor targeting, we propose an Adaptive Bad-case Sampling module, which adjusts\nthe ratio of different types of data within each incremental dataset based on\nthe benchmarking results. For quality, we resort to GPT-4 to generate\nhigh-quality data with each given data type. For correctness, prompt design is\ncritical for the data generation results. Rather than previous hand-crafted\nprompt, we propose an Interactive Prompt Optimization strategy, which optimizes\nthe prompt with the multi-round interaction between human and GPT, and improve\nthe correctness of generated data greatly. Through extensive experiments, we\nfind our MLLM-DataEngine could boost the MLLM capability in a targeted and\nautomatic manner, with only a few human participation. We hope it could be a\ngeneral solution for the following MLLMs building. The MLLM-DataEngine has been\nopen-sourced and is now available at\nhttps://github.com/opendatalab/MLLM-DataEngine.", "published": "2023-08-25 01:41:04", "link": "http://arxiv.org/abs/2308.13566v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "GRASP: A Rehearsal Policy for Efficient Online Continual Learning", "abstract": "Continual learning (CL) in deep neural networks (DNNs) involves incrementally\naccumulating knowledge in a DNN from a growing data stream. A major challenge\nin CL is that non-stationary data streams cause catastrophic forgetting of\npreviously learned abilities. A popular solution is rehearsal: storing past\nobservations in a buffer and then sampling the buffer to update the DNN.\nUniform sampling in a class-balanced manner is highly effective, and better\nsample selection policies have been elusive. Here, we propose a new sample\nselection policy called GRASP that selects the most prototypical (easy) samples\nfirst and then gradually selects less prototypical (harder) examples. GRASP has\nlittle additional compute or memory overhead compared to uniform selection,\nenabling it to scale to large datasets. Compared to 17 other rehearsal\npolicies, GRASP achieves higher accuracy in CL experiments on ImageNet.\nCompared to uniform balanced sampling, GRASP achieves the same performance with\n40% fewer updates. We also show that GRASP is effective for CL on five text\nclassification datasets.", "published": "2023-08-25 19:34:21", "link": "http://arxiv.org/abs/2308.13646v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Rethinking Language Models as Symbolic Knowledge Graphs", "abstract": "Symbolic knowledge graphs (KGs) play a pivotal role in knowledge-centric\napplications such as search, question answering and recommendation. As\ncontemporary language models (LMs) trained on extensive textual data have\ngained prominence, researchers have extensively explored whether the parametric\nknowledge within these models can match up to that present in knowledge graphs.\nVarious methodologies have indicated that enhancing the size of the model or\nthe volume of training data enhances its capacity to retrieve symbolic\nknowledge, often with minimal or no human supervision. Despite these\nadvancements, there is a void in comprehensively evaluating whether LMs can\nencompass the intricate topological and semantic attributes of KGs, attributes\ncrucial for reasoning processes. In this work, we provide an exhaustive\nevaluation of language models of varying sizes and capabilities. We construct\nnine qualitative benchmarks that encompass a spectrum of attributes including\nsymmetry, asymmetry, hierarchy, bidirectionality, compositionality, paths,\nentity-centricity, bias and ambiguity. Additionally, we propose novel\nevaluation metrics tailored for each of these attributes. Our extensive\nevaluation of various LMs shows that while these models exhibit considerable\npotential in recalling factual information, their ability to capture intricate\ntopological and semantic traits of KGs remains significantly constrained. We\nnote that our proposed evaluation metrics are more reliable in evaluating these\nabilities than the existing metrics. Lastly, some of our benchmarks challenge\nthe common notion that larger LMs (e.g., GPT-4) universally outshine their\nsmaller counterparts (e.g., BERT).", "published": "2023-08-25 21:25:08", "link": "http://arxiv.org/abs/2308.13676v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cultural Alignment in Large Language Models: An Explanatory Analysis\n  Based on Hofstede's Cultural Dimensions", "abstract": "The deployment of large language models (LLMs) raises concerns regarding\ntheir cultural misalignment and potential ramifications on individuals and\nsocieties with diverse cultural backgrounds. While the discourse has focused\nmainly on political and social biases, our research proposes a Cultural\nAlignment Test (Hoftede's CAT) to quantify cultural alignment using Hofstede's\ncultural dimension framework, which offers an explanatory cross-cultural\ncomparison through the latent variable analysis. We apply our approach to\nquantitatively evaluate LLMs, namely Llama 2, GPT-3.5, and GPT-4, against the\ncultural dimensions of regions like the United States, China, and Arab\ncountries, using different prompting styles and exploring the effects of\nlanguage-specific fine-tuning on the models' behavioural tendencies and\ncultural values. Our results quantify the cultural alignment of LLMs and reveal\nthe difference between LLMs in explanatory cultural dimensions. Our study\ndemonstrates that while all LLMs struggle to grasp cultural values, GPT-4 shows\na unique capability to adapt to cultural nuances, particularly in Chinese\nsettings. However, it faces challenges with American and Arab cultures. The\nresearch also highlights that fine-tuning LLama 2 models with different\nlanguages changes their responses to cultural questions, emphasizing the need\nfor culturally diverse development in AI for worldwide acceptance and ethical\nuse. For more details or to contribute to this research, visit our GitHub page\nhttps://github.com/reemim/Hofstedes_CAT/", "published": "2023-08-25 14:50:13", "link": "http://arxiv.org/abs/2309.12342v2", "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "On the Impact of Language Selection for Training and Evaluating\n  Programming Language Models", "abstract": "The recent advancements in Transformer-based Language Models have\ndemonstrated significant potential in enhancing the multilingual capabilities\nof these models. The remarkable progress made in this domain not only applies\nto natural language tasks but also extends to the domain of programming\nlanguages. Despite the ability of these models to learn from multiple\nlanguages, evaluations typically focus on particular combinations of the same\nlanguages. In this study, we evaluate the similarity of programming languages\nby analyzing their representations using a CodeBERT-based model. Our\nexperiments reveal that token representation in languages such as C++, Python,\nand Java exhibit proximity to one another, whereas the same tokens in languages\nsuch as Mathematica and R display significant dissimilarity. Our findings\nsuggest that this phenomenon can potentially result in performance challenges\nwhen dealing with diverse languages. Thus, we recommend using our similarity\nmeasure to select a diverse set of programming languages when training and\nevaluating future models.", "published": "2023-08-25 12:57:59", "link": "http://arxiv.org/abs/2308.13354v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG", "cs.PL"], "primary_category": "cs.SE"}
{"title": "Expressive paragraph text-to-speech synthesis with multi-step\n  variational autoencoder", "abstract": "Neural networks have been able to generate high-quality single-sentence\nspeech. However, it remains a challenge concerning audio-book speech synthesis\ndue to the intra-paragraph correlation of semantic and acoustic features as\nwell as variable styles. In this paper, we propose a highly expressive\nparagraph speech synthesis system with a multi-step variational autoencoder,\ncalled EP-MSTTS. EP-MSTTS is the first VITS-based paragraph speech synthesis\nmodel and models the variable style of paragraph speech at five levels: frame,\nphoneme, word, sentence, and paragraph. We also propose a series of\nimprovements to enhance the performance of this hierarchical model. In\naddition, we directly train EP-MSTTS on speech sliced by paragraph rather than\nsentence. Experiment results on the single-speaker French audiobook corpus\nreleased at Blizzard Challenge 2023 show EP-MSTTS obtains better performance\nthan baseline models.", "published": "2023-08-25 13:22:42", "link": "http://arxiv.org/abs/2308.13365v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Active Audio Feature Learning in Resource-Constrained Environments", "abstract": "The scarcity of labelled data makes training Deep Neural Network (DNN) models\nin bioacoustic applications challenging. In typical bioacoustics applications,\nmanually labelling the required amount of data can be prohibitively expensive.\nTo effectively identify both new and current classes, DNN models must continue\nto learn new features from a modest amount of fresh data. Active Learning (AL)\nis an approach that can help with this learning while requiring little\nlabelling effort. Nevertheless, the use of fixed feature extraction approaches\nlimits feature quality, resulting in underutilization of the benefits of AL. We\ndescribe an AL framework that addresses this issue by incorporating feature\nextraction into the AL loop and refining the feature extractor after each round\nof manual annotation. In addition, we use raw audio processing rather than\nspectrograms, which is a novel approach. Experiments reveal that the proposed\nAL framework requires 14.3%, 66.7%, and 47.4% less labelling effort on\nbenchmark audio datasets ESC-50, UrbanSound8k, and InsectWingBeat,\nrespectively, for a large DNN model and similar savings on a\nmicrocontroller-based counterpart. Furthermore, we showcase the practical\nrelevance of our study by incorporating data from conservation biology\nprojects. All codes are publicly available on GitHub.", "published": "2023-08-25 06:45:02", "link": "http://arxiv.org/abs/2308.13201v2", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploiting Diverse Feature for Multimodal Sentiment Analysis", "abstract": "In this paper, we present our solution to the MuSe-Personalisation\nsub-challenge in the MuSe 2023 Multimodal Sentiment Analysis Challenge. The\ntask of MuSe-Personalisation aims to predict the continuous arousal and valence\nvalues of a participant based on their audio-visual, language, and\nphysiological signal modalities data. Considering different people have\npersonal characteristics, the main challenge of this task is how to build\nrobustness feature presentation for sentiment prediction. To address this\nissue, we propose exploiting diverse features. Specifically, we proposed a\nseries of feature extraction methods to build a robust representation and model\nensemble. We empirically evaluate the performance of the utilized method on the\nofficially provided dataset. \\textbf{As a result, we achieved 3rd place in the\nMuSe-Personalisation sub-challenge.} Specifically, we achieve the results of\n0.8492 and 0.8439 for MuSe-Personalisation in terms of arousal and valence CCC.", "published": "2023-08-25 15:06:14", "link": "http://arxiv.org/abs/2308.13421v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
