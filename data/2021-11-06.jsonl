{"title": "Finnish Dialect Identification: The Effect of Audio and Text", "abstract": "Finnish is a language with multiple dialects that not only differ from each\nother in terms of accent (pronunciation) but also in terms of morphological\nforms and lexical choice. We present the first approach to automatically detect\nthe dialect of a speaker based on a dialect transcript and transcript with\naudio recording in a dataset consisting of 23 different dialects. Our results\nshow that the best accuracy is received by combining both of the modalities, as\ntext only reaches to an overall accuracy of 57\\%, where as text and audio reach\nto 85\\%. Our code, models and data have been released openly on Github and\nZenodo.", "published": "2021-11-06 04:25:53", "link": "http://arxiv.org/abs/2111.03800v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Architectures for Neural Machine Translation Using Low\n  Computational Resources", "abstract": "With the recent developments in the field of Natural Language Processing,\nthere has been a rise in the use of different architectures for Neural Machine\nTranslation. Transformer architectures are used to achieve state-of-the-art\naccuracy, but they are very computationally expensive to train. Everyone cannot\nhave such setups consisting of high-end GPUs and other resources. We train our\nmodels on low computational resources and investigate the results. As expected,\ntransformers outperformed other architectures, but there were some surprising\nresults. Transformers consisting of more encoders and decoders took more time\nto train but had fewer BLEU scores. LSTM performed well in the experiment and\ntook comparatively less time to train than transformers, making it suitable to\nuse in situations having time constraints.", "published": "2021-11-06 06:25:46", "link": "http://arxiv.org/abs/2111.03813v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistic Cues of Deception in a Multilingual April Fools' Day Context", "abstract": "In this work we consider the collection of deceptive April Fools' Day(AFD)\nnews articles as a useful addition in existing datasets for deception detection\ntasks. Such collections have an established ground truth and are relatively\neasy to construct across languages. As a result, we introduce a corpus that\nincludes diachronic AFD and normal articles from Greek newspapers and news\nwebsites. On top of that, we build a rich linguistic feature set, and analyze\nand compare its deception cues with the only AFD collection currently\navailable, which is in English. Following a current research thread, we also\ndiscuss the individualism/collectivism dimension in deception with respect to\nthese two datasets. Lastly, we build classifiers by testing various monolingual\nand crosslingual settings. The results showcase that AFD datasets can be\nhelpful in deception detection studies, and are in alignment with the\nobservations of other deception detection works.", "published": "2021-11-06 16:28:12", "link": "http://arxiv.org/abs/2111.03913v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distinguishing Commercial from Editorial Content in News", "abstract": "How can we distinguish commercial from editorial content in news, or more\nspecifically, differentiate between advertorials and regular news articles? An\nadvertorial is a commercial message written and formatted as an article, making\nit harder for readers to recognize these as advertising, despite the use of\ndisclaimers. In our research we aim to differentiate the two using a machine\nlearning model, and a lexicon derived from it. This was accomplished by\nscraping 1.000 articles and 1.000 advertorials from four different Dutch news\nsources and classifying these based on textual features. With this setup our\nmost successful machine learning model had an accuracy of just over $90\\%$. To\ngenerate additional insights into differences between news and advertorial\nlanguage, we also analyzed model coefficients and explored the corpus through\nco-occurrence networks and t-SNE graphs.", "published": "2021-11-06 16:45:48", "link": "http://arxiv.org/abs/2111.03916v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Trend and Thoughts: Understanding Climate Change Concern using Machine\n  Learning and Social Media Data", "abstract": "Nowadays social media platforms such as Twitter provide a great opportunity\nto understand public opinion of climate change compared to traditional survey\nmethods. In this paper, we constructed a massive climate change Twitter dataset\nand conducted comprehensive analysis using machine learning. By conducting\ntopic modeling and natural language processing, we show the relationship\nbetween the number of tweets about climate change and major climate events; the\ncommon topics people discuss climate change; and the trend of sentiment. Our\ndataset was published on Kaggle\n(\\url{https://www.kaggle.com/leonshangguan/climate-change-tweets-ids-until-aug-2021})\nand can be used in further research.", "published": "2021-11-06 19:59:03", "link": "http://arxiv.org/abs/2111.14929v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language\n  Modeling", "abstract": "Contrastive Vision-Language Pre-training, known as CLIP, has provided a new\nparadigm for learning visual representations by using large-scale contrastive\nimage-text pairs. It shows impressive performance on zero-shot knowledge\ntransfer to downstream tasks. To further enhance CLIP's few-shot capability,\nCLIP-Adapter proposed to fine-tune a lightweight residual feature adapter and\nsignificantly improves the performance for few-shot classification. However,\nsuch a process still needs extra training and computational resources. In this\npaper, we propose \\textbf{T}raining-Free CL\\textbf{IP}-\\textbf{Adapter}\n(\\textbf{Tip-Adapter}), which not only inherits CLIP's training-free advantage\nbut also performs comparably or even better than CLIP-Adapter. Tip-Adapter does\nnot require any back propagation for training the adapter, but creates the\nweights by a key-value cache model constructed from the few-shot training set.\nIn this non-parametric manner, Tip-Adapter acquires well-performed adapter\nweights without any training, which is both efficient and effective. Moreover,\nthe performance of Tip-Adapter can be further boosted by fine-tuning such\nproperly initialized adapter for only a few epochs with super-fast convergence\nspeed. We conduct extensive experiments of few-shot classification on ImageNet\nand other 10 datasets to demonstrate the superiority of proposed Tip-Adapter.\nThe code will be released at \\url{https://github.com/gaopengcuhk/Tip-Adapter}.", "published": "2021-11-06 18:09:22", "link": "http://arxiv.org/abs/2111.03930v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Transformer Based Bengali Chatbot Using General Knowledge Dataset", "abstract": "An AI chatbot provides an impressive response after learning from the trained\ndataset. In this decade, most of the research work demonstrates that deep\nneural models superior to any other model. RNN model regularly used for\ndetermining the sequence-related problem like a question and it answers. This\napproach acquainted with everyone as seq2seq learning. In a seq2seq model\nmechanism, it has encoder and decoder. The encoder embedded any input sequence,\nand the decoder embedded output sequence. For reinforcing the seq2seq model\nperformance, attention mechanism added into the encoder and decoder. After\nthat, the transformer model has introduced itself as a high-performance model\nwith multiple attention mechanism for solving the sequence-related dilemma.\nThis model reduces training time compared with RNN based model and also\nachieved state-of-the-art performance for sequence transduction. In this\nresearch, we applied the transformer model for Bengali general knowledge\nchatbot based on the Bengali general knowledge Question Answer (QA) dataset. It\nscores 85.0 BLEU on the applied QA data. To check the comparison of the\ntransformer model performance, we trained the seq2seq model with attention on\nour dataset that scores 23.5 BLEU.", "published": "2021-11-06 18:33:20", "link": "http://arxiv.org/abs/2111.03937v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Profitable Trade-Off Between Memory and Performance In Multi-Domain\n  Chatbot Architectures", "abstract": "Text classification problem is a very broad field of study in the field of\nnatural language processing. In short, the text classification problem is to\ndetermine which of the previously determined classes the given text belongs to.\nSuccessful studies have been carried out in this field in the past studies. In\nthe study, Bidirectional Encoder Representations for Transformers (BERT), which\nis a frequently preferred method for solving the classification problem in the\nfield of natural language processing, is used. By solving classification\nproblems through a single model to be used in a chatbot architecture, it is\naimed to alleviate the load on the server that will be created by more than one\nmodel used for solving more than one classification problem. At this point,\nwith the masking method applied during the estimation of a single BERT model,\nwhich was created for classification in more than one subject, the estimation\nof the model was provided on a problem-based basis. Three separate data sets\ncovering different fields from each other are divided by various methods in\norder to complicate the problem, and classification problems that are very\nclose to each other in terms of field are also included in this way. The\ndataset used in this way consists of five classification problems with 154\nclasses. A BERT model containing all classification problems and other BERT\nmodels trained specifically for the problems were compared with each other in\nterms of performance and the space they occupied on the server.", "published": "2021-11-06 20:45:17", "link": "http://arxiv.org/abs/2111.03963v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Privacy attacks for automatic speech recognition acoustic models in a\n  federated learning framework", "abstract": "This paper investigates methods to effectively retrieve speaker information\nfrom the personalized speaker adapted neural network acoustic models (AMs) in\nautomatic speech recognition (ASR). This problem is especially important in the\ncontext of federated learning of ASR acoustic models where a global model is\nlearnt on the server based on the updates received from multiple clients. We\npropose an approach to analyze information in neural network AMs based on a\nneural network footprint on the so-called Indicator dataset. Using this method,\nwe develop two attack models that aim to infer speaker identity from the\nupdated personalized models without access to the actual users' speech data.\nExperiments on the TED-LIUM 3 corpus demonstrate that the proposed approaches\nare very effective and can provide equal error rate (EER) of 1-2%.", "published": "2021-11-06 02:08:13", "link": "http://arxiv.org/abs/2111.03777v2", "categories": ["cs.CL", "cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Focusing on Potential Named Entities During Active Label Acquisition", "abstract": "Named entity recognition (NER) aims to identify mentions of named entities in\nan unstructured text and classify them into predefined named entity classes.\nWhile deep learning-based pre-trained language models help to achieve good\npredictive performances in NER, many domain-specific NER applications still\ncall for a substantial amount of labeled data. Active learning (AL), a general\nframework for the label acquisition problem, has been used for NER tasks to\nminimize the annotation cost without sacrificing model performance. However,\nthe heavily imbalanced class distribution of tokens introduces challenges in\ndesigning effective AL querying methods for NER. We propose several AL sentence\nquery evaluation functions that pay more attention to potential positive\ntokens, and evaluate these proposed functions with both sentence-based and\ntoken-based cost evaluation strategies. We also propose a better data-driven\nnormalization approach to penalize sentences that are too long or too short.\nOur experiments on three datasets from different domains reveal that the\nproposed approach reduces the number of annotated tokens while achieving better\nor comparable prediction performance with conventional methods.", "published": "2021-11-06 09:04:16", "link": "http://arxiv.org/abs/2111.03837v3", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Building ASR Systems for the Next Billion Users", "abstract": "Recent methods in speech and language technology pretrain very LARGE models\nwhich are fine-tuned for specific tasks. However, the benefits of such LARGE\nmodels are often limited to a few resource rich languages of the world. In this\nwork, we make multiple contributions towards building ASR systems for low\nresource languages from the Indian subcontinent. First, we curate 17,000 hours\nof raw speech data for 40 Indian languages from a wide variety of domains\nincluding education, news, technology, and finance. Second, using this raw\nspeech data we pretrain several variants of wav2vec style models for 40 Indian\nlanguages. Third, we analyze the pretrained models to find key features:\ncodebook vectors of similar sounding phonemes are shared across languages,\nrepresentations across layers are discriminative of the language family, and\nattention heads often pay attention within small local windows. Fourth, we\nfine-tune this model for downstream ASR for 9 languages and obtain\nstate-of-the-art results on 3 public datasets, including on very low-resource\nlanguages such as Sinhala and Nepali. Our work establishes that multilingual\npretraining is an effective strategy for building ASR systems for the\nlinguistically diverse speakers of the Indian subcontinent. Our code, data and\nmodels are available publicly at https://indicnlp.ai4bharat.org/indicwav2vec/\nand we hope they will help advance research in ASR for Indic languages.", "published": "2021-11-06 19:34:33", "link": "http://arxiv.org/abs/2111.03945v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Patent Sentiment Analysis to Highlight Patent Paragraphs", "abstract": "Given a patent document, identifying distinct semantic annotations is an\ninteresting research aspect. Text annotation helps the patent practitioners\nsuch as examiners and patent attorneys to quickly identify the key arguments of\nany invention, successively providing a timely marking of a patent text. In the\nprocess of manual patent analysis, to attain better readability, recognising\nthe semantic information by marking paragraphs is in practice. This semantic\nannotation process is laborious and time-consuming. To alleviate such a\nproblem, we proposed a novel dataset to train Machine Learning algorithms to\nautomate the highlighting process. The contributions of this work are: i) we\ndeveloped a multi-class, novel dataset of size 150k samples by traversing USPTO\npatents over a decade, ii) articulated statistics and distributions of data\nusing imperative exploratory data analysis, iii) baseline Machine Learning\nmodels are developed to utilize the dataset to address patent paragraph\nhighlighting task, iv) dataset and codes relating to this task are open-sourced\nthrough a dedicated GIT web page:\nhttps://github.com/Renuk9390/Patent_Sentiment_Analysis and v) future path to\nextend this work using Deep Learning and domain specific pre-trained language\nmodels to develop a tool to highlight is provided. This work assist patent\npractitioners in highlighting semantic information automatically and aid to\ncreate a sustainable and efficient patent analysis using the aptitude of\nMachine Learning.", "published": "2021-11-06 13:28:29", "link": "http://arxiv.org/abs/2111.09741v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Deep Noise Suppression Maximizing Non-Differentiable PESQ Mediated by a\n  Non-Intrusive PESQNet", "abstract": "Speech enhancement employing deep neural networks (DNNs) for denoising are\ncalled deep noise suppression (DNS). During training, DNS methods are typically\ntrained with mean squared error (MSE) type loss functions, which do not\nguarantee good perceptual quality. Perceptual evaluation of speech quality\n(PESQ) is a widely used metric for evaluating speech quality. However, the\noriginal PESQ algorithm is non-differentiable, and therefore cannot directly be\nused as optimization criterion for gradient-based learning. In this work, we\npropose an end-to-end non-intrusive PESQNet DNN to estimate the PESQ scores of\nthe enhanced speech signal. Thus, by providing a reference-free perceptual\nloss, it serves as a mediator towards the DNS training, allowing to maximize\nthe PESQ score of the enhanced speech signal. We illustrate the potential of\nour proposed PESQNet-mediated training on the basis of an already strong\nbaseline DNS. As further novelty, we propose to train the DNS and the PESQNet\nalternatingly to keep the PESQNet up-to-date and perform well specifically for\nthe DNS under training. Our proposed method is compared to the same DNS trained\nwith MSE-based loss for joint denoising and dereverberation, and the\nInterspeech 2021 DNS Challenge baseline. Detailed analysis shows that the\nPESQNet mediation can further increase the DNS performance by about 0.1 PESQ\npoints on synthetic test data and by 0.03 DNSMOS points on real test data,\ncompared to training with the MSE-based loss. Our proposed method also\noutperforms the Challenge baseline by 0.2 PESQ points on synthetic test data\nand 0.1 DNSMOS points on real test data.", "published": "2021-11-06 10:19:24", "link": "http://arxiv.org/abs/2111.03847v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Digital Audio Processing Tools for Music Corpus Studies", "abstract": "Digital audio processing tools offer music researchers the opportunity to\nexamine both non-notated music and music as performance. This chapter\nsummarises the types of information that can be extracted from audio as well as\ncurrently available audio tools for music corpus studies. The survey of\nextraction methods includes both a primer on signal processing and background\ntheory on audio feature extraction. The survey of audio tools focuses on widely\nused tools, including both those with a graphical user interface, namely\nAudacity and Sonic Visualiser, and code-based tools written in the C/C++, Java,\nMATLAB, and Python computer programming languages.", "published": "2021-11-06 14:40:09", "link": "http://arxiv.org/abs/2111.03895v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SIG-VC: A Speaker Information Guided Zero-shot Voice Conversion System\n  for Both Human Beings and Machines", "abstract": "Nowadays, as more and more systems achieve good performance in traditional\nvoice conversion (VC) tasks, people's attention gradually turns to VC tasks\nunder extreme conditions. In this paper, we propose a novel method for\nzero-shot voice conversion. We aim to obtain intermediate representations for\nspeaker-content disentanglement of speech to better remove speaker information\nand get pure content information. Accordingly, our proposed framework contains\na module that removes the speaker information from the acoustic feature of the\nsource speaker. Moreover, speaker information control is added to our system to\nmaintain the voice cloning performance. The proposed system is evaluated by\nsubjective and objective metrics. Results show that our proposed system\nsignificantly reduces the trade-off problem in zero-shot voice conversion,\nwhile it also manages to have high spoofing power to the speaker verification\nsystem.", "published": "2021-11-06 06:22:45", "link": "http://arxiv.org/abs/2111.03811v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Class Token and Knowledge Distillation for Multi-head Self-Attention\n  Speaker Verification Systems", "abstract": "This paper explores three novel approaches to improve the performance of\nspeaker verification (SV) systems based on deep neural networks (DNN) using\nMulti-head Self-Attention (MSA) mechanisms and memory layers. Firstly, we\npropose the use of a learnable vector called Class token to replace the average\nglobal pooling mechanism to extract the embeddings. Unlike global average\npooling, our proposal takes into account the temporal structure of the input\nwhat is relevant for the text-dependent SV task. The class token is\nconcatenated to the input before the first MSA layer, and its state at the\noutput is used to predict the classes. To gain additional robustness, we\nintroduce two approaches. First, we have developed a Bayesian estimation of the\nclass token. Second, we have added a distilled representation token for\ntraining a teacher-student pair of networks using the Knowledge Distillation\n(KD) philosophy, which is combined with the class token. This distillation\ntoken is trained to mimic the predictions from the teacher network, while the\nclass token replicates the true label. All the strategies have been tested on\nthe RSR2015-Part II and DeepMine-Part 1 databases for text-dependent SV,\nproviding competitive results compared to the same architecture using the\naverage pooling mechanism to extract average embeddings.", "published": "2021-11-06 09:47:05", "link": "http://arxiv.org/abs/2111.03842v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards noise robust trigger-word detection with contrastive learning\n  pre-task for fast on-boarding of new trigger-words", "abstract": "Trigger-word detection plays an important role as the entry point of user's\ncommunication with voice assistants. But supporting a particular word as a\ntrigger-word involves huge amount of data collection, augmentation and\nlabelling for that word. This makes supporting new trigger-words a tedious and\ntime consuming process. To combat this, we explore the use of contrastive\nlearning as a pre-training task that helps the detection model to generalize to\ndifferent words and noise conditions. We explore supervised contrastive\ntechniques and also propose a novel self-supervised training technique using\nchunked words from long sentence audios. We show that both supervised and the\nnew self-supervised contrastive pre-training techniques have comparable results\nto a traditional classification pre-training on new trigger words with less\ndata availability.", "published": "2021-11-06 22:39:05", "link": "http://arxiv.org/abs/2111.03971v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
