{"title": "Adaptation Approaches for Nearest Neighbor Language Models", "abstract": "Semi-parametric Nearest Neighbor Language Models ($k$NN-LMs) have produced\nimpressive gains over purely parametric LMs, by leveraging large-scale\nneighborhood retrieval over external memory datastores. However, there has been\nlittle investigation into adapting such models for new domains. This work\nattempts to fill that gap and suggests the following approaches for adapting\n$k$NN-LMs -- 1) adapting the underlying LM (using Adapters), 2) expanding\nneighborhood retrieval over an additional adaptation datastore, and 3) adapting\nthe weights (scores) of retrieved neighbors using a learned Rescorer module. We\nstudy each adaptation strategy separately, as well as the combined performance\nimprovement through ablation experiments and an extensive set of evaluations\nrun over seven adaptation domains. Our combined adaptation approach\nconsistently outperforms purely parametric adaptation and zero-shot ($k$NN-LM)\nbaselines that construct datastores from the adaptation data. On average, we\nsee perplexity improvements of 17.1% and 16% for these respective baselines,\nacross domains.", "published": "2022-11-15 01:10:52", "link": "http://arxiv.org/abs/2211.07828v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompting Language Models for Linguistic Structure", "abstract": "Although pretrained language models (PLMs) can be prompted to perform a wide\nrange of language tasks, it remains an open question how much this ability\ncomes from generalizable linguistic understanding versus surface-level lexical\npatterns. To test this, we present a structured prompting approach for\nlinguistic structured prediction tasks, allowing us to perform zero- and\nfew-shot sequence tagging with autoregressive PLMs. We evaluate this approach\non part-of-speech tagging, named entity recognition, and sentence chunking,\ndemonstrating strong few-shot performance in all cases. We also find that while\nPLMs contain significant prior knowledge of task labels due to task leakage\ninto the pretraining corpus, structured prompting can also retrieve linguistic\nstructure with arbitrary labels. These findings indicate that the in-context\nlearning ability and linguistic knowledge of PLMs generalizes beyond\nmemorization of their training data.", "published": "2022-11-15 01:13:39", "link": "http://arxiv.org/abs/2211.07830v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Error-Robust Retrieval for Chinese Spelling Check", "abstract": "Chinese Spelling Check (CSC) aims to detect and correct error tokens in\nChinese contexts, which has a wide range of applications. However, it is\nconfronted with the challenges of insufficient annotated data and the issue\nthat previous methods may actually not fully leverage the existing datasets. In\nthis paper, we introduce our plug-and-play retrieval method with error-robust\ninformation for Chinese Spelling Check (RERIC), which can be directly applied\nto existing CSC models. The datastore for retrieval is built completely based\non the training data, with elaborate designs according to the characteristics\nof CSC. Specifically, we employ multimodal representations that fuse phonetic,\nmorphologic, and contextual information in the calculation of query and key\nduring retrieval to enhance robustness against potential errors. Furthermore,\nin order to better judge the retrieved candidates, the n-gram surrounding the\ntoken to be checked is regarded as the value and utilized for specific\nreranking. The experiment results on the SIGHAN benchmarks demonstrate that our\nproposed method achieves substantial improvements over existing work.", "published": "2022-11-15 01:55:34", "link": "http://arxiv.org/abs/2211.07843v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey for Efficient Open Domain Question Answering", "abstract": "Open domain question answering (ODQA) is a longstanding task aimed at\nanswering factual questions from a large knowledge corpus without any explicit\nevidence in natural language processing (NLP). Recent works have predominantly\nfocused on improving the answering accuracy and achieved promising progress.\nHowever, higher accuracy often comes with more memory consumption and inference\nlatency, which might not necessarily be efficient enough for direct deployment\nin the real world. Thus, a trade-off between accuracy, memory consumption and\nprocessing speed is pursued. In this paper, we provide a survey of recent\nadvances in the efficiency of ODQA models. We walk through the ODQA models and\nconclude the core techniques on efficiency. Quantitative analysis on memory\ncost, processing speed, accuracy and overall comparison are given. We hope that\nthis work would keep interested scholars informed of the advances and open\nchallenges in ODQA efficiency research, and thus contribute to the further\ndevelopment of ODQA efficiency.", "published": "2022-11-15 04:18:53", "link": "http://arxiv.org/abs/2211.07886v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Phrase-based Sequence-to-Sequence Learning", "abstract": "We describe a neural transducer that maintains the flexibility of standard\nsequence-to-sequence (seq2seq) models while incorporating hierarchical phrases\nas a source of inductive bias during training and as explicit constraints\nduring inference. Our approach trains two models: a discriminative parser based\non a bracketing transduction grammar whose derivation tree hierarchically\naligns source and target phrases, and a neural seq2seq model that learns to\ntranslate the aligned phrases one-by-one. We use the same seq2seq model to\ntranslate at all phrase scales, which results in two inference modes: one mode\nin which the parser is discarded and only the seq2seq component is used at the\nsequence-level, and another in which the parser is combined with the seq2seq\nmodel. Decoding in the latter mode is done with the cube-pruned CKY algorithm,\nwhich is more involved but can make use of new translation rules during\ninference. We formalize our model as a source-conditioned synchronous grammar\nand develop an efficient variational inference algorithm for training. When\napplied on top of both randomly initialized and pretrained seq2seq models, we\nfind that both inference modes performs well compared to baselines on small\nscale machine translation benchmarks.", "published": "2022-11-15 05:22:40", "link": "http://arxiv.org/abs/2211.07906v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Open-Ended Stressor Responses to Predict Depressive Symptoms\n  across Demographics", "abstract": "Stressors are related to depression, but this relationship is complex. We\ninvestigate the relationship between open-ended text responses about stressors\nand depressive symptoms across gender and racial/ethnic groups. First, we use\ntopic models and other NLP tools to find thematic and vocabulary differences\nwhen reporting stressors across demographic groups. We train language models\nusing self-reported stressors to predict depressive symptoms, finding a\nrelationship between stressors and depression. Finally, we find that\ndifferences in stressors translate to downstream performance differences across\ndemographic groups.", "published": "2022-11-15 06:34:58", "link": "http://arxiv.org/abs/2211.07932v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Breakpoint Transformers for Modeling and Tracking Intermediate Beliefs", "abstract": "Can we teach natural language understanding models to track their beliefs\nthrough intermediate points in text? We propose a representation learning\nframework called breakpoint modeling that allows for learning of this type.\nGiven any text encoder and data marked with intermediate states (breakpoints)\nalong with corresponding textual queries viewed as true/false propositions\n(i.e., the candidate beliefs of a model, consisting of information changing\nthrough time) our approach trains models in an efficient and end-to-end fashion\nto build intermediate representations that facilitate teaching and direct\nquerying of beliefs at arbitrary points alongside solving other end tasks. To\nshow the benefit of our approach, we experiment with a diverse set of NLU tasks\nincluding relational reasoning on CLUTRR and narrative understanding on bAbI.\nUsing novel belief prediction tasks for both tasks, we show the benefit of our\nmain breakpoint transformer, based on T5, over conventional representation\nlearning approaches in terms of processing efficiency, prediction accuracy and\nprediction consistency, all with minimal to no effect on corresponding QA end\ntasks. To show the feasibility of incorporating our belief tracker into more\ncomplex reasoning pipelines, we also obtain SOTA performance on the\nthree-tiered reasoning challenge for the TRIP benchmark (around 23-32% absolute\nimprovement on Tasks 2-3).", "published": "2022-11-15 07:28:14", "link": "http://arxiv.org/abs/2211.07950v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Overview on Controllable Text Generation via Variational\n  Auto-Encoders", "abstract": "Recent advances in neural-based generative modeling have reignited the hopes\nof having computer systems capable of conversing with humans and able to\nunderstand natural language. The employment of deep neural architectures has\nbeen largely explored in a multitude of context and tasks to fulfill various\nuser needs. On one hand, producing textual content that meets specific\nrequirements is of priority for a model to seamlessly conduct conversations\nwith different groups of people. On the other hand, latent variable models\n(LVM) such as variational auto-encoders (VAEs) as one of the most popular\ngenres of generative models are designed to characterize the distributional\npattern of textual data. Thus they are inherently capable of learning the\nintegral textual features that are worth exploring for controllable pursuits.\n  \\noindent This overview gives an introduction to existing generation schemes,\nproblems associated with text variational auto-encoders, and a review of\nseveral applications about the controllable generation that are instantiations\nof these general formulations,\\footnote{A detailed paper list is available at\n\\url{https://github.com/ImKeTT/CTG-latentAEs}} as well as related datasets,\nmetrics and discussions for future researches. Hopefully, this overview will\nprovide an overview of living questions, popular methodologies and raw thoughts\nfor controllable language generation under the scope of variational\nauto-encoder.", "published": "2022-11-15 07:36:11", "link": "http://arxiv.org/abs/2211.07954v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discourse and conversation impairments in patients with dementia", "abstract": "Neurodegeneration characterizes individuals with different dementia subtypes\n(e.g., individuals with Alzheimer's Disease, Primary Progressive Aphasia, and\nParkinson's Disease), leading to progressive decline in cognitive, linguistic,\nand social functioning. Speech and language impairments are early symptoms in\nindividuals with focal forms of neurodegenerative conditions, coupled with\ndeficits in cognitive, social, and behavioral domains. This paper reviews the\nfindings on language and communication deficits and identifies the effects of\ndementia on the production and perception of discourse. It discusses findings\nconcerning (i) language function, cognitive representation, and impairment,\n(ii) communicative competence, emotions, empathy, and theory-of-mind, and (iii)\nspeech-in-interaction. It argues that clinical discourse analysis can provide a\ncomprehensive assessment of language and communication skills in individuals,\nwhich complements the existing neurolinguistic evaluation for (differential)\ndiagnosis, prognosis, and treatment efficacy evaluation.", "published": "2022-11-15 08:18:30", "link": "http://arxiv.org/abs/2211.07971v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Benchmark and Dataset for Post-OCR text correction in Sanskrit", "abstract": "Sanskrit is a classical language with about 30 million extant manuscripts fit\nfor digitisation, available in written, printed or scannedimage forms. However,\nit is still considered to be a low-resource language when it comes to available\ndigital resources. In this work, we release a post-OCR text correction dataset\ncontaining around 218,000 sentences, with 1.5 million words, from 30 different\nbooks. Texts in Sanskrit are known to be diverse in terms of their linguistic\nand stylistic usage since Sanskrit was the 'lingua franca' for discourse in the\nIndian subcontinent for about 3 millennia. Keeping this in mind, we release a\nmulti-domain dataset, from areas as diverse as astronomy, medicine and\nmathematics, with some of them as old as 18 centuries. Further, we release\nmultiple strong baselines as benchmarks for the task, based on pre-trained\nSeq2Seq language models. We find that our best-performing model, consisting of\nbyte level tokenization in conjunction with phonetic encoding (Byt5+SLP1),\nyields a 23% point increase over the OCR output in terms of word and character\nerror rates. Moreover, we perform extensive experiments in evaluating these\nmodels on their performance and analyse common causes of mispredictions both at\nthe graphemic and lexical levels. Our code and dataset is publicly available at\nhttps://github.com/ayushbits/pe-ocr-sanskrit.", "published": "2022-11-15 08:32:18", "link": "http://arxiv.org/abs/2211.07980v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Persian Emotion Detection using ParsBERT and Imbalanced Data Handling\n  Approaches", "abstract": "Emotion recognition is one of the machine learning applications which can be\ndone using text, speech, or image data gathered from social media spaces.\nDetecting emotion can help us in different fields, including opinion mining.\nWith the spread of social media, different platforms like Twitter have become\ndata sources, and the language used in these platforms is informal, making the\nemotion detection task difficult. EmoPars and ArmanEmo are two new\nhuman-labeled emotion datasets for the Persian language. These datasets,\nespecially EmoPars, are suffering from inequality between several samples\nbetween two classes. In this paper, we evaluate EmoPars and compare them with\nArmanEmo. Throughout this analysis, we use data augmentation techniques, data\nre-sampling, and class-weights with Transformer-based Pretrained Language\nModels(PLMs) to handle the imbalance problem of these datasets. Moreover,\nfeature selection is used to enhance the models' performance by emphasizing the\ntext's specific features. In addition, we provide a new policy for selecting\ndata from EmoPars, which selects the high-confidence samples; as a result, the\nmodel does not see samples that do not have specific emotion during training.\nOur model reaches a Macro-averaged F1-score of 0.81 and 0.76 on ArmanEmo and\nEmoPars, respectively, which are new state-of-the-art results in these\nbenchmarks.", "published": "2022-11-15 10:22:49", "link": "http://arxiv.org/abs/2211.08029v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Universal Discriminator for Zero-Shot Generalization", "abstract": "Generative modeling has been the dominant approach for large-scale\npretraining and zero-shot generalization. In this work, we challenge this\nconvention by showing that discriminative approaches perform substantially\nbetter than generative ones on a large number of NLP tasks. Technically, we\ntrain a single discriminator to predict whether a text sample comes from the\ntrue data distribution, similar to GANs. Since many NLP tasks can be formulated\nas selecting from a few options, we use this discriminator to predict the\nconcatenation of input and which option has the highest probability of coming\nfrom the true data distribution. This simple formulation achieves\nstate-of-the-art zero-shot results on the T0 benchmark, outperforming T0 by\n16.0\\%, 7.8\\%, and 11.5\\% respectively on different scales. In the finetuning\nsetting, our approach also achieves new state-of-the-art results on a wide\nrange of NLP tasks, with only 1/4 parameters of previous methods. Meanwhile,\nour approach requires minimal prompting efforts, which largely improves\nrobustness and is essential for real-world applications. Furthermore, we also\njointly train a generalized UD in combination with generative tasks, which\nmaintains its advantage on discriminative tasks and simultaneously works on\ngenerative tasks.", "published": "2022-11-15 12:33:31", "link": "http://arxiv.org/abs/2211.08099v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DualNER: A Dual-Teaching framework for Zero-shot Cross-lingual Named\n  Entity Recognition", "abstract": "We present DualNER, a simple and effective framework to make full use of both\nannotated source language corpus and unlabeled target language text for\nzero-shot cross-lingual named entity recognition (NER). In particular, we\ncombine two complementary learning paradigms of NER, i.e., sequence labeling\nand span prediction, into a unified multi-task framework. After obtaining a\nsufficient NER model trained on the source data, we further train it on the\ntarget data in a {\\it dual-teaching} manner, in which the pseudo-labels for one\ntask are constructed from the prediction of the other task. Moreover, based on\nthe span prediction, an entity-aware regularization is proposed to enhance the\nintrinsic cross-lingual alignment between the same entities in different\nlanguages. Experiments and analysis demonstrate the effectiveness of our\nDualNER. Code is available at https://github.com/lemon0830/dualNER.", "published": "2022-11-15 12:50:59", "link": "http://arxiv.org/abs/2211.08104v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Efficient Active Learning Pipeline for Legal Text Classification", "abstract": "Active Learning (AL) is a powerful tool for learning with less labeled data,\nin particular, for specialized domains, like legal documents, where unlabeled\ndata is abundant, but the annotation requires domain expertise and is thus\nexpensive. Recent works have shown the effectiveness of AL strategies for\npre-trained language models. However, most AL strategies require a set of\nlabeled samples to start with, which is expensive to acquire. In addition,\npre-trained language models have been shown unstable during fine-tuning with\nsmall datasets, and their embeddings are not semantically meaningful. In this\nwork, we propose a pipeline for effectively using active learning with\npre-trained language models in the legal domain. To this end, we leverage the\navailable unlabeled data in three phases. First, we continue pre-training the\nmodel to adapt it to the downstream task. Second, we use knowledge distillation\nto guide the model's embeddings to a semantically meaningful space. Finally, we\npropose a simple, yet effective, strategy to find the initial set of labeled\nsamples with fewer actions compared to existing methods. Our experiments on\nContract-NLI, adapted to the classification task, and LEDGAR benchmarks show\nthat our approach outperforms standard AL strategies, and is more efficient.\nFurthermore, our pipeline reaches comparable results to the fully-supervised\napproach with a small performance gap, and dramatically reduced annotation\ncost. Code and the adapted data will be made available.", "published": "2022-11-15 13:07:02", "link": "http://arxiv.org/abs/2211.08112v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CSynGEC: Incorporating Constituent-based Syntax for Grammatical Error\n  Correction with a Tailored GEC-Oriented Parser", "abstract": "Recently, Zhang et al. (2022) propose a syntax-aware grammatical error\ncorrection (GEC) approach, named SynGEC, showing that incorporating tailored\ndependency-based syntax of the input sentence is quite beneficial to GEC. This\nwork considers another mainstream syntax formalism, i.e., constituent-based\nsyntax. By drawing on the successful experience of SynGEC, we first propose an\nextended constituent-based syntax scheme to accommodate errors in ungrammatical\nsentences. Then, we automatically obtain constituency trees of ungrammatical\nsentences to train a GEC-oriented constituency parser by using parallel GEC\ndata as a pivot. For syntax encoding, we employ the graph convolutional network\n(GCN). Experimental results show that our method, named CSynGEC, yields\nsubstantial improvements over strong baselines. Moreover, we investigate the\nintegration of constituent-based and dependency-based syntax for GEC in two\nways: 1) intra-model combination, which means using separate GCNs to encode\nboth kinds of syntax for decoding in a single model; 2)inter-model combination,\nwhich means gathering and selecting edits predicted by different models to\nachieve final corrections. We find that the former method improves recall over\nusing one standalone syntax formalism while the latter improves precision, and\nboth lead to better F0.5 values.", "published": "2022-11-15 14:11:39", "link": "http://arxiv.org/abs/2211.08158v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Type Information Utilized Event Detection via Multi-Channel GNNs in\n  Electrical Power Systems", "abstract": "Event detection in power systems aims to identify triggers and event types,\nwhich helps relevant personnel respond to emergencies promptly and facilitates\nthe optimization of power supply strategies. However, the limited length of\nshort electrical record texts causes severe information sparsity, and numerous\ndomain-specific terminologies of power systems makes it difficult to transfer\nknowledge from language models pre-trained on general-domain texts. Traditional\nevent detection approaches primarily focus on the general domain and ignore\nthese two problems in the power system domain. To address the above issues, we\npropose a Multi-Channel graph neural network utilizing Type information for\nEvent Detection in power systems, named MC-TED, leveraging a semantic channel\nand a topological channel to enrich information interaction from short texts.\nConcretely, the semantic channel refines textual representations with semantic\nsimilarity, building the semantic information interaction among potential\nevent-related words. The topological channel generates a relation-type-aware\ngraph modeling word dependencies, and a word-type-aware graph integrating\npart-of-speech tags. To further reduce errors worsened by professional\nterminologies in type analysis, a type learning mechanism is designed for\nupdating the representations of both the word type and relation type in the\ntopological channel. In this way, the information sparsity and professional\nterm occurrence problems can be alleviated by enabling interaction between\ntopological and semantic information. Furthermore, to address the lack of\nlabeled data in power systems, we built a Chinese event detection dataset based\non electrical Power Event texts, named PoE. In experiments, our model achieves\ncompelling results not only on the PoE dataset, but on general-domain event\ndetection datasets including ACE 2005 and MAVEN.", "published": "2022-11-15 14:22:27", "link": "http://arxiv.org/abs/2211.08168v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating the Frequency Distortion of Word Embeddings and Its Impact\n  on Bias Metrics", "abstract": "Recent research has shown that static word embeddings can encode word\nfrequency information. However, little has been studied about this phenomenon\nand its effects on downstream tasks. In the present work, we systematically\nstudy the association between frequency and semantic similarity in several\nstatic word embeddings. We find that Skip-gram, GloVe and FastText embeddings\ntend to produce higher semantic similarity between high-frequency words than\nbetween other frequency combinations. We show that the association between\nfrequency and similarity also appears when words are randomly shuffled. This\nproves that the patterns found are not due to real semantic associations\npresent in the texts, but are an artifact produced by the word embeddings.\nFinally, we provide an example of how word frequency can strongly impact the\nmeasurement of gender bias with embedding-based metrics. In particular, we\ncarry out a controlled experiment that shows that biases can even change sign\nor reverse their order by manipulating word frequencies.", "published": "2022-11-15 15:11:06", "link": "http://arxiv.org/abs/2211.08203v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When to Use What: An In-Depth Comparative Empirical Analysis of OpenIE\n  Systems for Downstream Applications", "abstract": "Open Information Extraction (OpenIE) has been used in the pipelines of\nvarious NLP tasks. Unfortunately, there is no clear consensus on which models\nto use in which tasks. Muddying things further is the lack of comparisons that\ntake differing training sets into account. In this paper, we present an\napplication-focused empirical survey of neural OpenIE models, training sets,\nand benchmarks in an effort to help users choose the most suitable OpenIE\nsystems for their applications. We find that the different assumptions made by\ndifferent models and datasets have a statistically significant effect on\nperformance, making it important to choose the most appropriate model for one's\napplications. We demonstrate the applicability of our recommendations on a\ndownstream Complex QA application.", "published": "2022-11-15 15:48:27", "link": "http://arxiv.org/abs/2211.08228v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QAmeleon: Multilingual QA with Only 5 Examples", "abstract": "The availability of large, high-quality datasets has been one of the main\ndrivers of recent progress in question answering (QA). Such annotated datasets\nhowever are difficult and costly to collect, and rarely exist in languages\nother than English, rendering QA technology inaccessible to underrepresented\nlanguages. An alternative to building large monolingual training datasets is to\nleverage pre-trained language models (PLMs) under a few-shot learning setting.\nOur approach, QAmeleon, uses a PLM to automatically generate multilingual data\nupon which QA models are trained, thus avoiding costly annotation. Prompt\ntuning the PLM for data synthesis with only five examples per language delivers\naccuracy superior to translation-based baselines, bridges nearly 60% of the gap\nbetween an English-only baseline and a fully supervised upper bound trained on\nalmost 50,000 hand labeled examples, and always leads to substantial\nimprovements compared to fine-tuning a QA model directly on labeled examples in\nlow resource settings. Experiments on the TyDiQA-GoldP and MLQA benchmarks show\nthat few-shot prompt tuning for data synthesis scales across languages and is a\nviable alternative to large-scale annotation.", "published": "2022-11-15 16:14:39", "link": "http://arxiv.org/abs/2211.08264v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FolkScope: Intention Knowledge Graph Construction for E-commerce\n  Commonsense Discovery", "abstract": "Understanding users' intentions in e-commerce platforms requires commonsense\nknowledge. In this paper, we present FolkScope, an intention knowledge graph\nconstruction framework to reveal the structure of humans' minds about\npurchasing items. As commonsense knowledge is usually ineffable and not\nexpressed explicitly, it is challenging to perform information extraction.\nThus, we propose a new approach that leverages the generation power of large\nlanguage models~(LLMs) and human-in-the-loop annotation to semi-automatically\nconstruct the knowledge graph. LLMs first generate intention assertions via\ne-commerce-specific prompts to explain shopping behaviors, where the intention\ncan be an open reason or a predicate falling into one of 18 categories aligning\nwith ConceptNet, e.g., IsA, MadeOf, UsedFor, etc. Then we annotate plausibility\nand typicality labels of sampled intentions as training data in order to\npopulate human judgments to all automatic generations. Last, to structurize the\nassertions, we propose pattern mining and conceptualization to form more\ncondensed and abstract knowledge. Extensive evaluations and studies demonstrate\nthat our constructed knowledge graph can well model e-commerce knowledge and\nhave many potential applications.", "published": "2022-11-15 17:20:40", "link": "http://arxiv.org/abs/2211.08316v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MEAL: Stable and Active Learning for Few-Shot Prompting", "abstract": "Few-shot classification has made great strides due to foundation models that,\nthrough priming and prompting, are highly effective few-shot learners. However,\nthis approach has high variance both across different sets of few shots (data\nselection) and across different finetuning runs (run variability). This is\nproblematic not only because it impedes the fair comparison of different\napproaches, but especially because it makes few-shot learning too unreliable\nfor many real-world applications. To alleviate these issues, we make two\ncontributions for more stable and effective few-shot learning: First, we\npropose novel ensembling methods and show that they substantially reduce run\nvariability. Second, we introduce a new active learning (AL) criterion for data\nselection and present the first AL-based approach specifically tailored towards\nprompt-based learning. In our experiments, we show that our combined method,\nMEAL (Multiprompt finetuning and prediction Ensembling with Active Learning),\nimproves overall performance of prompt-based finetuning by 2.3 points on five\ndiverse tasks. We publicly share our code and data splits in\nhttps://github.com/akoksal/MEAL.", "published": "2022-11-15 18:06:53", "link": "http://arxiv.org/abs/2211.08358v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Easy to Decide, Hard to Agree: Reducing Disagreements Between Saliency\n  Methods", "abstract": "A popular approach to unveiling the black box of neural NLP models is to\nleverage saliency methods, which assign scalar importance scores to each input\ncomponent. A common practice for evaluating whether an interpretability method\nis faithful has been to use evaluation-by-agreement -- if multiple methods\nagree on an explanation, its credibility increases. However, recent work has\nfound that saliency methods exhibit weak rank correlations even when applied to\nthe same model instance and advocated for the use of alternative diagnostic\nmethods. In our work, we demonstrate that rank correlation is not a good fit\nfor evaluating agreement and argue that Pearson-$r$ is a better-suited\nalternative. We further show that regularization techniques that increase\nfaithfulness of attention explanations also increase agreement between saliency\nmethods. By connecting our findings to instance categories based on training\ndynamics, we show that the agreement of saliency method explanations is very\nlow for easy-to-learn instances. Finally, we connect the improvement in\nagreement across instance categories to local representation space statistics\nof instances, paving the way for work on analyzing which intrinsic model\nproperties improve their predisposition to interpretability methods.", "published": "2022-11-15 18:18:34", "link": "http://arxiv.org/abs/2211.08369v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pragmatics in Language Grounding: Phenomena, Tasks, and Modeling\n  Approaches", "abstract": "People rely heavily on context to enrich meaning beyond what is literally\nsaid, enabling concise but effective communication. To interact successfully\nand naturally with people, user-facing artificial intelligence systems will\nrequire similar skills in pragmatics: relying on various types of context --\nfrom shared linguistic goals and conventions, to the visual and embodied world\n-- to use language effectively. We survey existing grounded settings and\npragmatic modeling approaches and analyze how the task goals, environmental\ncontexts, and communicative affordances in each work enrich linguistic meaning.\nWe present recommendations for future grounded task design to naturally elicit\npragmatic phenomena, and suggest directions that focus on a broader range of\ncommunicative contexts and affordances.", "published": "2022-11-15 18:21:46", "link": "http://arxiv.org/abs/2211.08371v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AutoTemplate: A Simple Recipe for Lexically Constrained Text Generation", "abstract": "Lexically constrained text generation is one of the constrained text\ngeneration tasks, which aims to generate text that covers all the given\nconstraint lexicons. While the existing approaches tackle this problem using a\nlexically constrained beam search algorithm or dedicated model using\nnon-autoregressive decoding, there is a trade-off between the generated text\nquality and the hard constraint satisfaction. We introduce AutoTemplate, a\nsimple yet effective lexically constrained text generation framework divided\ninto template generation and lexicalization tasks. The template generation is\nto generate the text with the placeholders, and lexicalization replaces them\ninto the constraint lexicons to perform lexically constrained text generation.\nWe conducted the experiments on two tasks: keywords-to-sentence generations and\nentity-guided summarization. Experimental results show that the AutoTemplate\noutperforms the competitive baselines on both tasks while satisfying the hard\nlexical constraints. The code is available at\nhttps://github.com/megagonlabs/autotemplate", "published": "2022-11-15 18:36:18", "link": "http://arxiv.org/abs/2211.08387v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Factual Consistency of Large Language Models Through News\n  Summarization", "abstract": "While large language models (LLMs) have proven to be effective on a large\nvariety of tasks, they are also known to hallucinate information. To measure\nwhether an LLM prefers factually consistent continuations of its input, we\npropose a new benchmark called FIB(Factual Inconsistency Benchmark) that\nfocuses on the task of summarization. Specifically, our benchmark involves\ncomparing the scores an LLM assigns to a factually consistent versus a\nfactually inconsistent summary for an input news article. For factually\nconsistent summaries, we use human-written reference summaries that we manually\nverify as factually consistent. To generate summaries that are factually\ninconsistent, we generate summaries from a suite of summarization models that\nwe have manually annotated as factually inconsistent. A model's factual\nconsistency is then measured according to its accuracy, i.e.\\ the proportion of\ndocuments where it assigns a higher score to the factually consistent summary.\nTo validate the usefulness of FIB, we evaluate 23 large language models ranging\nfrom 1B to 176B parameters from six different model families including BLOOM\nand OPT. We find that existing LLMs generally assign a higher score to\nfactually consistent summaries than to factually inconsistent summaries.\nHowever, if the factually inconsistent summaries occur verbatim in the\ndocument, then LLMs assign a higher score to these factually inconsistent\nsummaries than factually consistent summaries. We validate design choices in\nour benchmark including the scoring method and source of distractor summaries.\nOur code and benchmark data can be found at https://github.com/r-three/fib.", "published": "2022-11-15 18:50:34", "link": "http://arxiv.org/abs/2211.08412v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "kogito: A Commonsense Knowledge Inference Toolkit", "abstract": "In this paper, we present kogito, an open-source tool for generating\ncommonsense inferences about situations described in text. kogito provides an\nintuitive and extensible interface to interact with natural language generation\nmodels that can be used for hypothesizing commonsense knowledge inference from\na textual input. In particular, kogito offers several features for targeted,\nmulti-granularity knowledge generation. These include a standardized API for\ntraining and evaluating knowledge models, and generating and filtering\ninferences from them. We also include helper functions for converting natural\nlanguage texts into a format ingestible by knowledge models - intermediate\npipeline stages such as knowledge head extraction from text, heuristic and\nmodel-based knowledge head-relation matching, and an ability to define and use\ncustom knowledge relations. We make the code for kogito available at\nhttps://github.com/epfl-nlp/kogito along with thorough documentation at\nhttps://kogito.readthedocs.io.", "published": "2022-11-15 19:04:13", "link": "http://arxiv.org/abs/2211.08451v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Navigating Connected Memories with a Task-oriented Dialog System", "abstract": "Recent years have seen an increasing trend in the volume of personal media\ncaptured by users, thanks to the advent of smartphones and smart glasses,\nresulting in large media collections. Despite conversation being an intuitive\nhuman-computer interface, current efforts focus mostly on single-shot natural\nlanguage based media retrieval to aid users query their media and re-live their\nmemories. This severely limits the search functionality as users can neither\nask follow-up queries nor obtain information without first formulating a\nsingle-turn query.\n  In this work, we propose dialogs for connected memories as a powerful tool to\nempower users to search their media collection through a multi-turn,\ninteractive conversation. Towards this, we collect a new task-oriented dialog\ndataset COMET, which contains $11.5k$ user<->assistant dialogs (totaling $103k$\nutterances), grounded in simulated personal memory graphs. We employ a\nresource-efficient, two-phase data collection pipeline that uses: (1) a novel\nmultimodal dialog simulator that generates synthetic dialog flows grounded in\nmemory graphs, and, (2) manual paraphrasing to obtain natural language\nutterances. We analyze COMET, formulate four main tasks to benchmark meaningful\nprogress, and adopt state-of-the-art language models as strong baselines, in\norder to highlight the multimodal challenges captured by our dataset.", "published": "2022-11-15 19:31:57", "link": "http://arxiv.org/abs/2211.08462v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ED-FAITH: Evaluating Dialogue Summarization on Faithfulness", "abstract": "Abstractive summarization models typically generate content unfaithful to the\ninput, thus highlighting the significance of evaluating the faithfulness of\ngenerated summaries. Most faithfulness metrics are only evaluated on news\ndomain, can they be transferred to other summarization tasks? In this work, we\nfirst present a systematic study of faithfulness metrics for dialogue\nsummarization. We evaluate common faithfulness metrics on dialogue datasets and\nobserve that most metrics correlate poorly with human judgements despite\nperforming well on news datasets. Given these findings, to improve existing\nmetrics' performance on dialogue summarization, we first finetune on in-domain\ndataset, then apply unlikelihood training on negative samples, and show that\nthey can successfully improve metric performance on dialogue data. Inspired by\nthe strong zero-shot performance of the T0 language model, we further propose\nT0-Score -- a new metric for faithfulness evaluation, which shows consistent\nimprovement against baseline metrics across multiple domains.", "published": "2022-11-15 19:33:50", "link": "http://arxiv.org/abs/2211.08464v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reasoning Circuits: Few-shot Multihop Question Generation with\n  Structured Rationales", "abstract": "Multi-hop Question Generation is the task of generating questions which\nrequire the reader to reason over and combine information spread across\nmultiple passages using several reasoning steps. Chain-of-thought rationale\ngeneration has been shown to improve performance on multi-step reasoning tasks\nand make model predictions more interpretable. However, few-shot performance\ngains from including rationales have been largely observed only in +100B\nlanguage models, and otherwise require large scale manual rationale annotation.\nIn this work, we introduce a new framework for applying chain-of-thought\ninspired structured rationale generation to multi-hop question generation under\na very low supervision regime (8- to 128-shot). We propose to annotate a small\nnumber of examples following our proposed multi-step rationale schema, treating\neach reasoning step as a separate task to be performed by a generative language\nmodel. We show that our framework leads to improved control over the difficulty\nof the generated questions and better performance compared to baselines trained\nwithout rationales, both on automatic evaluation metrics and in human\nevaluation. Importantly, we show that this is achievable with a modest model\nsize.", "published": "2022-11-15 19:36:06", "link": "http://arxiv.org/abs/2211.08466v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Relationship of the language distance to English ability of a country", "abstract": "Language difference is one of the factors that hinder the acquisition of\nsecond language skills. In this article, we introduce a novel solution that\nleverages the strength of deep neural networks to measure the semantic\ndissimilarity between languages based on their word distributions in the\nembedding space of the multilingual pre-trained language model (e.g.,BERT).\nThen, we empirically examine the effectiveness of the proposed semantic\nlanguage distance (SLD) in explaining the consistent variation in English\nability of countries, which is proxied by their performance in the\nInternet-Based Test of English as Foreign Language (TOEFL iBT). The\nexperimental results show that the language distance demonstrates negative\ninfluence on a country's average English ability. Interestingly, the effect is\nmore significant on speaking and writing subskills, which pertain to the\nproductive aspects of language learning. Besides, we provide specific\nrecommendations for future research directions.", "published": "2022-11-15 02:40:00", "link": "http://arxiv.org/abs/2211.07855v1", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Multilingual and Multimodal Topic Modelling with Pretrained Embeddings", "abstract": "This paper presents M3L-Contrast -- a novel multimodal multilingual (M3L)\nneural topic model for comparable data that maps texts from multiple languages\nand images into a shared topic space. Our model is trained jointly on texts and\nimages and takes advantage of pretrained document and image embeddings to\nabstract the complexities between different languages and modalities. As a\nmultilingual topic model, it produces aligned language-specific topics and as\nmultimodal model, it infers textual representations of semantic concepts in\nimages. We demonstrate that our model is competitive with a zero-shot topic\nmodel in predicting topic distributions for comparable multilingual data and\nsignificantly outperforms a zero-shot model in predicting topic distributions\nfor comparable texts and images. We also show that our model performs almost as\nwell on unaligned embeddings as it does on aligned embeddings.", "published": "2022-11-15 11:15:50", "link": "http://arxiv.org/abs/2211.08057v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "RobBERT-2022: Updating a Dutch Language Model to Account for Evolving\n  Language Use", "abstract": "Large transformer-based language models, e.g. BERT and GPT-3, outperform\nprevious architectures on most natural language processing tasks. Such language\nmodels are first pre-trained on gigantic corpora of text and later used as\nbase-model for finetuning on a particular task. Since the pre-training step is\nusually not repeated, base models are not up-to-date with the latest\ninformation. In this paper, we update RobBERT, a RoBERTa-based state-of-the-art\nDutch language model, which was trained in 2019. First, the tokenizer of\nRobBERT is updated to include new high-frequent tokens present in the latest\nDutch OSCAR corpus, e.g. corona-related words. Then we further pre-train the\nRobBERT model using this dataset. To evaluate if our new model is a plug-in\nreplacement for RobBERT, we introduce two additional criteria based on concept\ndrift of existing tokens and alignment for novel tokens.We found that for\ncertain language tasks this update results in a significant performance\nincrease. These results highlight the benefit of continually updating a\nlanguage model to account for evolving language use.", "published": "2022-11-15 14:55:53", "link": "http://arxiv.org/abs/2211.08192v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploiting Contrastive Learning and Numerical Evidence for Confusing\n  Legal Judgment Prediction", "abstract": "Given the fact description text of a legal case, legal judgment prediction\n(LJP) aims to predict the case's charge, law article and penalty term. A core\nproblem of LJP is how to distinguish confusing legal cases, where only subtle\ntext differences exist. Previous studies fail to distinguish different\nclassification errors with a standard cross-entropy classification loss, and\nignore the numbers in the fact description for predicting the term of penalty.\nTo tackle these issues, in this work, first, we propose a moco-based supervised\ncontrastive learning to learn distinguishable representations, and explore the\nbest strategy to construct positive example pairs to benefit all three subtasks\nof LJP simultaneously. Second, in order to exploit the numbers in legal cases\nfor predicting the penalty terms of certain cases, we further enhance the\nrepresentation of the fact description with extracted crime amounts which are\nencoded by a pre-trained numeracy model. Extensive experiments on public\nbenchmarks show that the proposed method achieves new state-of-the-art results,\nespecially on confusing legal cases. Ablation studies also demonstrate the\neffectiveness of each component.", "published": "2022-11-15 15:53:56", "link": "http://arxiv.org/abs/2211.08238v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Classifying text using machine learning models and determining\n  conversation drift", "abstract": "Text classification helps analyse texts for semantic meaning and relevance,\nby mapping the words against this hierarchy. An analysis of various types of\ntexts is invaluable to understanding both their semantic meaning, as well as\ntheir relevance. Text classification is a method of categorising documents. It\ncombines computer text classification and natural language processing to\nanalyse text in aggregate. This method provides a descriptive categorization of\nthe text, with features like content type, object field, lexical\ncharacteristics, and style traits. In this research, the authors aim to use\nnatural language feature extraction methods in machine learning which are then\nused to train some of the basic machine learning models like Naive Bayes,\nLogistic Regression, and Support Vector Machine. These models are used to\ndetect when a teacher must get involved in a discussion when the lines go\noff-topic.", "published": "2022-11-15 18:09:45", "link": "http://arxiv.org/abs/2211.08365v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Empowering Language Models with Knowledge Graph Reasoning for Question\n  Answering", "abstract": "Answering open-domain questions requires world knowledge about in-context\nentities. As pre-trained Language Models (LMs) lack the power to store all\nrequired knowledge, external knowledge sources, such as knowledge graphs, are\noften used to augment LMs. In this work, we propose knOwledge REasOning\nempowered Language Model (OREO-LM), which consists of a novel Knowledge\nInteraction Layer that can be flexibly plugged into existing Transformer-based\nLMs to interact with a differentiable Knowledge Graph Reasoning module\ncollaboratively. In this way, LM guides KG to walk towards the desired answer,\nwhile the retrieved knowledge improves LM. By adopting OREO-LM to RoBERTa and\nT5, we show significant performance gain, achieving state-of-art results in the\nClosed-Book setting. The performance enhancement is mainly from the KG\nreasoning's capacity to infer missing relational facts. In addition, OREO-LM\nprovides reasoning paths as rationales to interpret the model's decision.", "published": "2022-11-15 18:26:26", "link": "http://arxiv.org/abs/2211.08380v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generative Long-form Question Answering: Relevance, Faithfulness and\n  Succinctness", "abstract": "In this thesis, we investigated the relevance, faithfulness, and succinctness\naspects of Long Form Question Answering (LFQA). LFQA aims to generate an\nin-depth, paragraph-length answer for a given question, to help bridge the gap\nbetween real scenarios and the existing open-domain QA models which can only\nextract short-span answers. LFQA is quite challenging and under-explored. Few\nworks have been done to build an effective LFQA system. It is even more\nchallenging to generate a good-quality long-form answer relevant to the query\nand faithful to facts, since a considerable amount of redundant, complementary,\nor contradictory information will be contained in the retrieved documents.\nMoreover, no prior work has been investigated to generate succinct answers. We\nare among the first to research the LFQA task. We pioneered the research\ndirection to improve the answer quality in terms of 1) query-relevance, 2)\nanswer faithfulness, and 3) answer succinctness.", "published": "2022-11-15 18:36:01", "link": "http://arxiv.org/abs/2211.08386v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models Struggle to Learn Long-Tail Knowledge", "abstract": "The Internet contains a wealth of knowledge -- from the birthdays of\nhistorical figures to tutorials on how to code -- all of which may be learned\nby language models. However, while certain pieces of information are ubiquitous\non the web, others appear extremely rarely. In this paper, we study the\nrelationship between the knowledge memorized by large language models and the\ninformation in pre-training datasets scraped from the web. In particular, we\nshow that a language model's ability to answer a fact-based question relates to\nhow many documents associated with that question were seen during pre-training.\nWe identify these relevant documents by entity linking pre-training datasets\nand counting documents that contain the same entities as a given\nquestion-answer pair. Our results demonstrate strong correlational and causal\nrelationships between accuracy and relevant document count for numerous\nquestion answering datasets (e.g., TriviaQA), pre-training corpora (e.g.,\nROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models\nare better at learning long-tail knowledge, we estimate that today's models\nmust be scaled by many orders of magnitude to reach competitive QA performance\non questions with little support in the pre-training data. Finally, we show\nthat retrieval-augmentation can reduce the dependence on relevant pre-training\ninformation, presenting a promising approach for capturing the long-tail.", "published": "2022-11-15 18:49:27", "link": "http://arxiv.org/abs/2211.08411v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SexWEs: Domain-Aware Word Embeddings via Cross-lingual Semantic\n  Specialisation for Chinese Sexism Detection in Social Media", "abstract": "The goal of sexism detection is to mitigate negative online content targeting\ncertain gender groups of people. However, the limited availability of labeled\nsexism-related datasets makes it problematic to identify online sexism for\nlow-resource languages. In this paper, we address the task of automatic sexism\ndetection in social media for one low-resource language -- Chinese. Rather than\ncollecting new sexism data or building cross-lingual transfer learning models,\nwe develop a cross-lingual domain-aware semantic specialisation system in order\nto make the most of existing data. Semantic specialisation is a technique for\nretrofitting pre-trained distributional word vectors by integrating external\nlinguistic knowledge (such as lexico-semantic relations) into the specialised\nfeature space. To do this, we leverage semantic resources for sexism from a\nhigh-resource language (English) to specialise pre-trained word vectors in the\ntarget language (Chinese) to inject domain knowledge. We demonstrate the\nbenefit of our sexist word embeddings (SexWEs) specialised by our framework via\nintrinsic evaluation of word similarity and extrinsic evaluation of sexism\ndetection. Compared with other specialisation approaches and Chinese baseline\nword vectors, our SexWEs shows an average score improvement of 0.033 and 0.064\nin both intrinsic and extrinsic evaluations, respectively. The ablative results\nand visualisation of SexWEs also prove the effectiveness of our framework on\nretrofitting word vectors in low-resource languages.", "published": "2022-11-15 19:00:20", "link": "http://arxiv.org/abs/2211.08447v3", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Mind Your Bias: A Critical Review of Bias Detection Methods for\n  Contextual Language Models", "abstract": "The awareness and mitigation of biases are of fundamental importance for the\nfair and transparent use of contextual language models, yet they crucially\ndepend on the accurate detection of biases as a precursor. Consequently,\nnumerous bias detection methods have been proposed, which vary in their\napproach, the considered type of bias, and the data used for evaluation.\nHowever, while most detection methods are derived from the word embedding\nassociation test for static word embeddings, the reported results are\nheterogeneous, inconsistent, and ultimately inconclusive. To address this\nissue, we conduct a rigorous analysis and comparison of bias detection methods\nfor contextual language models. Our results show that minor design and\nimplementation decisions (or errors) have a substantial and often significant\nimpact on the derived bias scores. Overall, we find the state of the field to\nbe both worse than previously acknowledged due to systematic and propagated\nerrors in implementations, yet better than anticipated since divergent results\nin the literature homogenize after accounting for implementation errors. Based\non our findings, we conclude with a discussion of paths towards more robust and\nconsistent bias detection methods.", "published": "2022-11-15 19:27:54", "link": "http://arxiv.org/abs/2211.08461v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "On the Compositional Generalization Gap of In-Context Learning", "abstract": "Pretrained large generative language models have shown great performance on\nmany tasks, but exhibit low compositional generalization abilities. Scaling\nsuch models has been shown to improve their performance on various NLP tasks\neven just by conditioning them on a few examples to solve the task without any\nfine-tuning (also known as in-context learning). In this work, we look at the\ngap between the in-distribution (ID) and out-of-distribution (OOD) performance\nof such models in semantic parsing tasks with in-context learning. In the ID\nsettings, the demonstrations are from the same split (test or train) that the\nmodel is being evaluated on, and in the OOD settings, they are from the other\nsplit. We look at how the relative generalization gap of in-context learning\nevolves as models are scaled up. We evaluate four model families, OPT, BLOOM,\nCodeGen and Codex on three semantic parsing datasets, CFQ, SCAN and GeoQuery\nwith different number of exemplars, and observe a trend of decreasing relative\ngeneralization gap as models are scaled up.", "published": "2022-11-15 19:56:37", "link": "http://arxiv.org/abs/2211.08473v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Searching for Carriers of the Diffuse Interstellar Bands Across\n  Disciplines, using Natural Language Processing", "abstract": "The explosion of scientific publications overloads researchers with\ninformation. This is even more dramatic for interdisciplinary studies, where\nseveral fields need to be explored. A tool to help researchers overcome this is\nNatural Language Processing (NLP): a machine-learning (ML) technique that\nallows scientists to automatically synthesize information from many articles.\nAs a practical example, we have used NLP to conduct an interdisciplinary search\nfor compounds that could be carriers for Diffuse Interstellar Bands (DIBs), a\nlong-standing open question in astrophysics. We have trained a NLP model on a\ncorpus of 1.5 million cross-domain articles in open access, and fine-tuned this\nmodel with a corpus of astrophysical publications about DIBs. Our analysis\npoints us toward several molecules, studied primarily in biology, having\ntransitions at the wavelengths of several DIBs and composed of abundant\ninterstellar atoms. Several of these molecules contain chromophores, small\nmolecular groups responsible for the molecule's colour, that could be promising\ncandidate carriers. Identifying viable carriers demonstrates the value of using\nNLP to tackle open scientific questions, in an interdisciplinary manner.", "published": "2022-11-15 21:16:59", "link": "http://arxiv.org/abs/2211.08513v2", "categories": ["cs.CL", "astro-ph.GA"], "primary_category": "cs.CL"}
{"title": "MapQA: A Dataset for Question Answering on Choropleth Maps", "abstract": "Choropleth maps are a common visual representation for region-specific\ntabular data and are used in a number of different venues (newspapers,\narticles, etc). These maps are human-readable but are often challenging to deal\nwith when trying to extract data for screen readers, analyses, or other related\ntasks. Recent research into Visual-Question Answering (VQA) has studied\nquestion answering on human-generated charts (ChartQA), such as bar, line, and\npie charts. However, little work has paid attention to understanding maps;\ngeneral VQA models, and ChartQA models, suffer when asked to perform this task.\nTo facilitate and encourage research in this area, we present MapQA, a\nlarge-scale dataset of ~800K question-answer pairs over ~60K map images. Our\ntask tests various levels of map understanding, from surface questions about\nmap styles to complex questions that require reasoning on the underlying data.\nWe present the unique challenges of MapQA that frustrate most strong baseline\nalgorithms designed for ChartQA and general VQA tasks. We also present a novel\nalgorithm, Visual Multi-Output Data Extraction based QA (V-MODEQA) for MapQA.\nV-MODEQA extracts the underlying structured data from a map image with a\nmulti-output model and then performs reasoning on the extracted data. Our\nexperimental results show that V-MODEQA has better overall performance and\nrobustness on MapQA than the state-of-the-art ChartQA and VQA algorithms by\ncapturing the unique properties in map question answering.", "published": "2022-11-15 22:31:38", "link": "http://arxiv.org/abs/2211.08545v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Toward expanding the scope of radiology report summarization to multiple\n  anatomies and modalities", "abstract": "Radiology report summarization (RRS) is a growing area of research. Given the\nFindings section of a radiology report, the goal is to generate a summary\n(called an Impression section) that highlights the key observations and\nconclusions of the radiology study. However, RRS currently faces essential\nlimitations.First, many prior studies conduct experiments on private datasets,\npreventing reproduction of results and fair comparisons across different\nsystems and solutions. Second, most prior approaches are evaluated solely on\nchest X-rays. To address these limitations, we propose a dataset (MIMIC-RRS)\ninvolving three new modalities and seven new anatomies based on the MIMIC-III\nand MIMIC-CXR datasets. We then conduct extensive experiments to evaluate the\nperformance of models both within and across modality-anatomy pairs in\nMIMIC-RRS. In addition, we evaluate their clinical efficacy via RadGraph, a\nfactual correctness metric.", "published": "2022-11-15 23:57:34", "link": "http://arxiv.org/abs/2211.08584v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PromptCap: Prompt-Guided Task-Aware Image Captioning", "abstract": "Knowledge-based visual question answering (VQA) involves questions that\nrequire world knowledge beyond the image to yield the correct answer. Large\nlanguage models (LMs) like GPT-3 are particularly helpful for this task because\nof their strong knowledge retrieval and reasoning capabilities. To enable LM to\nunderstand images, prior work uses a captioning model to convert images into\ntext. However, when summarizing an image in a single caption sentence, which\nvisual entities to describe are often underspecified. Generic image captions\noften miss visual details essential for the LM to answer visual questions\ncorrectly. To address this challenge, we propose PromptCap (Prompt-guided image\nCaptioning), a captioning model designed to serve as a better connector between\nimages and black-box LMs. Different from generic captions, PromptCap takes a\nnatural-language prompt to control the visual entities to describe in the\ngenerated caption. The prompt contains a question that the caption should aid\nin answering. To avoid extra annotation, PromptCap is trained by examples\nsynthesized with GPT-3 and existing datasets. We demonstrate PromptCap's\neffectiveness on an existing pipeline in which GPT-3 is prompted with image\ncaptions to carry out VQA. PromptCap outperforms generic captions by a large\nmargin and achieves state-of-the-art accuracy on knowledge-based VQA tasks\n(60.4% on OK-VQA and 59.6% on A-OKVQA). Zero-shot results on WebQA show that\nPromptCap generalizes well to unseen domains.", "published": "2022-11-15 19:07:53", "link": "http://arxiv.org/abs/2211.09699v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "VRDU: A Benchmark for Visually-rich Document Understanding", "abstract": "Understanding visually-rich business documents to extract structured data and\nautomate business workflows has been receiving attention both in academia and\nindustry. Although recent multi-modal language models have achieved impressive\nresults, we find that existing benchmarks do not reflect the complexity of real\ndocuments seen in industry. In this work, we identify the desiderata for a more\ncomprehensive benchmark and propose one we call Visually Rich Document\nUnderstanding (VRDU). VRDU contains two datasets that represent several\nchallenges: rich schema including diverse data types as well as hierarchical\nentities, complex templates including tables and multi-column layouts, and\ndiversity of different layouts (templates) within a single document type. We\ndesign few-shot and conventional experiment settings along with a carefully\ndesigned matching algorithm to evaluate extraction results. We report the\nperformance of strong baselines and offer three observations: (1) generalizing\nto new document templates is still very challenging, (2) few-shot performance\nhas a lot of headroom, and (3) models struggle with hierarchical fields such as\nline-items in an invoice. We plan to open source the benchmark and the\nevaluation toolkit. We hope this helps the community make progress on these\nchallenging tasks in extracting structured data from visually rich documents.", "published": "2022-11-15 03:17:07", "link": "http://arxiv.org/abs/2211.15421v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cheater's Bowl: Human vs. Computer Search Strategies for Open-Domain\n  Question Answering", "abstract": "For humans and computers, the first step in answering an open-domain question\nis retrieving a set of relevant documents from a large corpus. However, the\nstrategies that computers use fundamentally differ from those of humans. To\nbetter understand these differences, we design a gamified interface for data\ncollection -- Cheater's Bowl -- where a human answers complex questions with\naccess to both traditional and modern search tools. We collect a dataset of\nhuman search sessions, analyze human search strategies, and compare them to\nstate-of-the-art multi-hop QA models. Humans query logically, apply dynamic\nsearch chains, and use world knowledge to boost searching. We demonstrate how\nhuman queries can improve the accuracy of existing systems and propose\nimproving the future design of QA models.", "published": "2022-11-15 10:31:09", "link": "http://arxiv.org/abs/2212.03296v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating How Fine-tuning on Bimodal Data Effects Code Generation", "abstract": "Despite the increase in popularity of language models for code generation, it\nis still unknown how training on bimodal coding forums affects a model's code\ngeneration performance and reliability. We, therefore, collect a dataset of\nover 2.2M StackOverflow questions with answers for finetuning. These fine-tuned\nmodels have average $pass@k$ improvements of 54.64% and 85.35% on the HumanEval\n(Chen et al., 2021) and Mostly Basic Program Problems (Austin et al., 2021)\ntasks, respectively. This regime further decreases the number of generated\nprograms with both syntax and runtime errors. However, we find that at higher\ntemperatures, there are significant decreases to the model's ability to\ngenerate runnable programs despite higher $pass@k$ scores, underscoring the\nneed for better methods of incorporating such data that mitigate these side\neffects. The code can be found\nhttps://github.com/gabeorlanski/bimodalcode-generation", "published": "2022-11-15 01:53:20", "link": "http://arxiv.org/abs/2211.07842v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "FedTune: A Deep Dive into Efficient Federated Fine-Tuning with\n  Pre-trained Transformers", "abstract": "Federated Learning (FL) is an emerging paradigm that enables distributed\nusers to collaboratively and iteratively train machine learning models without\nsharing their private data. Motivated by the effectiveness and robustness of\nself-attention-based architectures, researchers are turning to using\npre-trained Transformers (i.e., foundation models) instead of traditional\nconvolutional neural networks in FL to leverage their excellent transfer\nlearning capabilities. Despite recent progress, how pre-trained Transformer\nmodels play a role in FL remains obscure, that is, how to efficiently fine-tune\nthese pre-trained models in FL and how FL users could benefit from this new\nparadigm. In this paper, we explore this issue and demonstrate that the\nfine-tuned Transformers achieve extraordinary performance on FL, and that the\nlightweight fine-tuning method facilitates a fast convergence rate and low\ncommunication costs. Concretely, we conduct a rigorous empirical study of three\ntuning methods (i.e., modifying the input, adding extra modules, and adjusting\nthe backbone) using two types of pre-trained models (i.e., vision-language\nmodels and vision models) for FL. Our experiments show that 1) Fine-tuning the\nbias term of the backbone performs best when relying on a strong pre-trained\nmodel; 2) The vision-language model (e.g., CLIP) outperforms the pure vision\nmodel (e.g., ViT) and is more robust to the few-shot settings; 3) Compared to\npure local training, FL with pre-trained models has a higher accuracy because\nit alleviates the problem of over-fitting. We will release our code and\nencourage further exploration of pre-trained Transformers and FL.", "published": "2022-11-15 10:16:13", "link": "http://arxiv.org/abs/2211.08025v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "GLUE-X: Evaluating Natural Language Understanding Models from an\n  Out-of-distribution Generalization Perspective", "abstract": "Pre-trained language models (PLMs) are known to improve the generalization\nperformance of natural language understanding models by leveraging large\namounts of data during the pre-training phase. However, the out-of-distribution\n(OOD) generalization problem remains a challenge in many NLP tasks, limiting\nthe real-world deployment of these methods. This paper presents the first\nattempt at creating a unified benchmark named GLUE-X for evaluating OOD\nrobustness in NLP models, highlighting the importance of OOD robustness and\nproviding insights on how to measure the robustness of a model and how to\nimprove it. The benchmark includes 13 publicly available datasets for OOD\ntesting, and evaluations are conducted on 8 classic NLP tasks over 21 popularly\nused PLMs, including GPT-3 and GPT-3.5. Our findings confirm the need for\nimproved OOD accuracy in NLP tasks, as significant performance degradation was\nobserved in all settings compared to in-distribution (ID) accuracy.", "published": "2022-11-15 11:53:55", "link": "http://arxiv.org/abs/2211.08073v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "primary_category": "cs.CL"}
{"title": "Hierarchical Pronunciation Assessment with Multi-Aspect Attention", "abstract": "Automatic pronunciation assessment is a major component of a\ncomputer-assisted pronunciation training system. To provide in-depth feedback,\nscoring pronunciation at various levels of granularity such as phoneme, word,\nand utterance, with diverse aspects such as accuracy, fluency, and\ncompleteness, is essential. However, existing multi-aspect multi-granularity\nmethods simultaneously predict all aspects at all granularity levels;\ntherefore, they have difficulty in capturing the linguistic hierarchy of\nphoneme, word, and utterance. This limitation further leads to neglecting\nintimate cross-aspect relations at the same linguistic unit. In this paper, we\npropose a Hierarchical Pronunciation Assessment with Multi-aspect Attention\n(HiPAMA) model, which hierarchically represents the granularity levels to\ndirectly capture their linguistic structures and introduces multi-aspect\nattention that reflects associations across aspects at the same level to create\nmore connotative representations. By obtaining relational information from both\nthe granularity- and aspect-side, HiPAMA can take full advantage of multi-task\nlearning. Remarkable improvements in the experimental results on the\nspeachocean762 datasets demonstrate the robustness of HiPAMA, particularly in\nthe difficult-to-assess aspects.", "published": "2022-11-15 12:49:35", "link": "http://arxiv.org/abs/2211.08102v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Comparative Study of Question Answering over Knowledge Bases", "abstract": "Question answering over knowledge bases (KBQA) has become a popular approach\nto help users extract information from knowledge bases. Although several\nsystems exist, choosing one suitable for a particular application scenario is\ndifficult. In this article, we provide a comparative study of six\nrepresentative KBQA systems on eight benchmark datasets. In that, we study\nvarious question types, properties, languages, and domains to provide insights\non where existing systems struggle. On top of that, we propose an advanced\nmapping algorithm to aid existing models in achieving superior results.\nMoreover, we also develop a multilingual corpus COVID-KGQA, which encourages\nCOVID-19 research and multilingualism for the diversity of future AI. Finally,\nwe discuss the key findings and their implications as well as performance\nguidelines and some future improvements. Our source code is available at\n\\url{https://github.com/tamlhp/kbqa}.", "published": "2022-11-15 14:23:47", "link": "http://arxiv.org/abs/2211.08170v1", "categories": ["cs.CL", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An FNet based Auto Encoder for Long Sequence News Story Generation", "abstract": "In this paper, we design an auto encoder based off of Google's FNet\nArchitecture in order to generate text from a subset of news stories contained\nin Google's C4 dataset. We discuss previous attempts and methods to generate\ntext from autoencoders and non LLM Models. FNET poses multiple advantages to\nBERT based encoders in the realm of efficiency which train 80% faster on GPUs\nand 70% faster on TPUs. We then compare outputs of how this autencoder perfroms\non different epochs. Finally, we analyze what outputs the encoder produces with\ndifferent seed text.", "published": "2022-11-15 16:48:09", "link": "http://arxiv.org/abs/2211.08295v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07, 68T50", "I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Introducing Semantics into Speech Encoders", "abstract": "Recent studies find existing self-supervised speech encoders contain\nprimarily acoustic rather than semantic information. As a result, pipelined\nsupervised automatic speech recognition (ASR) to large language model (LLM)\nsystems achieve state-of-the-art results on semantic spoken language tasks by\nutilizing rich semantic representations from the LLM. These systems come at the\ncost of labeled audio transcriptions, which is expensive and time-consuming to\nobtain. We propose a task-agnostic unsupervised way of incorporating semantic\ninformation from LLMs into self-supervised speech encoders without labeled\naudio transcriptions. By introducing semantics, we improve existing speech\nencoder spoken language understanding performance by over 10\\% on intent\nclassification, with modest gains in named entity resolution and slot filling,\nand spoken question answering FF1 score by over 2\\%. Our unsupervised approach\nachieves similar performance as supervised methods trained on over 100 hours of\nlabeled audio transcripts, demonstrating the feasibility of unsupervised\nsemantic augmentations to existing speech encoders.", "published": "2022-11-15 18:44:28", "link": "http://arxiv.org/abs/2211.08402v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ALIGN-MLM: Word Embedding Alignment is Crucial for Multilingual\n  Pre-training", "abstract": "Multilingual pre-trained models exhibit zero-shot cross-lingual transfer,\nwhere a model fine-tuned on a source language achieves surprisingly good\nperformance on a target language. While studies have attempted to understand\ntransfer, they focus only on MLM, and the large number of differences between\nnatural languages makes it hard to disentangle the importance of different\nproperties. In this work, we specifically highlight the importance of word\nembedding alignment by proposing a pre-training objective (ALIGN-MLM) whose\nauxiliary loss guides similar words in different languages to have similar word\nembeddings. ALIGN-MLM either outperforms or matches three widely adopted\nobjectives (MLM, XLM, DICT-MLM) when we evaluate transfer between pairs of\nnatural languages and their counterparts created by systematically modifying\nspecific properties like the script. In particular, ALIGN-MLM outperforms XLM\nand MLM by 35 and 30 F1 points on POS-tagging for transfer between languages\nthat differ both in their script and word order (left-to-right v.s.\nright-to-left). We also show a strong correlation between alignment and\ntransfer for all objectives (e.g., rho=0.727 for XNLI), which together with\nALIGN-MLM's strong performance calls for explicitly aligning word embeddings\nfor multilingual models.", "published": "2022-11-15 22:37:27", "link": "http://arxiv.org/abs/2211.08547v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Teaching Algorithmic Reasoning via In-context Learning", "abstract": "Large language models (LLMs) have shown increasing in-context learning\ncapabilities through scaling up model and data size. Despite this progress,\nLLMs are still unable to solve algorithmic reasoning problems. While providing\na rationale with the final answer has led to further improvements in multi-step\nreasoning problems, Anil et al. 2022 showed that even simple algorithmic\nreasoning tasks such as parity are far from solved. In this work, we identify\nand study four key stages for successfully teaching algorithmic reasoning to\nLLMs: (1) formulating algorithms as skills, (2) teaching multiple skills\nsimultaneously (skill accumulation), (3) teaching how to combine skills (skill\ncomposition) and (4) teaching how to use skills as tools. We show that it is\npossible to teach algorithmic reasoning to LLMs via in-context learning, which\nwe refer to as algorithmic prompting. We evaluate our approach on a variety of\narithmetic and quantitative reasoning tasks, and demonstrate significant boosts\nin performance over existing prompting techniques. In particular, for long\nparity, addition, multiplication and subtraction, we achieve an error reduction\nof approximately 10x, 9x, 5x and 2x respectively compared to the best available\nbaselines.", "published": "2022-11-15 06:12:28", "link": "http://arxiv.org/abs/2211.09066v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Analyse der Entwicklungstreiber milit\u00e4rischer Schwarmdrohnen durch\n  Natural Language Processing", "abstract": "Military drones are taking an increasingly prominent role in armed conflict,\nand the use of multiple drones in a swarm can be useful. Who the drivers of the\nresearch are and what sub-domains exist is analyzed and visually presented in\nthis research using NLP techniques based on 946 studies. Most research is\nconducted in the Western world, led by the United States, the United Kingdom,\nand Germany. Through Tf-idf scoring, it is shown that countries have\nsignificant differences in the subdomains studied. Overall, 2019 and 2020 saw\nthe most works published, with significant interest in military swarm drones as\nearly as 2008. This study provides a first glimpse into research in this area\nand prompts further investigation.", "published": "2022-11-15 20:22:33", "link": "http://arxiv.org/abs/2211.09680v1", "categories": ["cs.CL", "cs.LG", "cs.RO", "68U15", "I.2.7"], "primary_category": "cs.CL"}
{"title": "DeepParliament: A Legal domain Benchmark & Dataset for Parliament Bills\n  Prediction", "abstract": "This paper introduces DeepParliament, a legal domain Benchmark Dataset that\ngathers bill documents and metadata and performs various bill status\nclassification tasks. The proposed dataset text covers a broad range of bills\nfrom 1986 to the present and contains richer information on parliament bill\ncontent. Data collection, detailed statistics and analyses are provided in the\npaper. Moreover, we experimented with different types of models ranging from\nRNN to pretrained and reported the results. We are proposing two new\nbenchmarks: Binary and Multi-Class Bill Status classification. Models developed\nfor bill documents and relevant supportive tasks may assist Members of\nParliament (MPs), presidents, and other legal practitioners. It will help\nreview or prioritise bills, thus speeding up the billing process, improving the\nquality of decisions and reducing the time consumption in both houses.\nConsidering that the foundation of the country's democracy is Parliament and\nstate legislatures, we anticipate that our research will be an essential\naddition to the Legal NLP community. This work will be the first to present a\nParliament bill prediction task. In order to improve the accessibility of legal\nAI resources and promote reproducibility, we have made our code and dataset\npublicly accessible at github.com/monk1337/DeepParliament", "published": "2022-11-15 04:55:32", "link": "http://arxiv.org/abs/2211.15424v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Visually Grounded VQA by Lattice-based Retrieval", "abstract": "Visual Grounding (VG) in Visual Question Answering (VQA) systems describes\nhow well a system manages to tie a question and its answer to relevant image\nregions. Systems with strong VG are considered intuitively interpretable and\nsuggest an improved scene understanding. While VQA accuracy performances have\nseen impressive gains over the past few years, explicit improvements to VG\nperformance and evaluation thereof have often taken a back seat on the road to\noverall accuracy improvements. A cause of this originates in the predominant\nchoice of learning paradigm for VQA systems, which consists of training a\ndiscriminative classifier over a predetermined set of answer options.\n  In this work, we break with the dominant VQA modeling paradigm of\nclassification and investigate VQA from the standpoint of an information\nretrieval task. As such, the developed system directly ties VG into its core\nsearch procedure. Our system operates over a weighted, directed, acyclic graph,\na.k.a. \"lattice\", which is derived from the scene graph of a given image in\nconjunction with region-referring expressions extracted from the question.\n  We give a detailed analysis of our approach and discuss its distinctive\nproperties and limitations. Our approach achieves the strongest VG performance\namong examined systems and exhibits exceptional generalization capabilities in\na number of scenarios.", "published": "2022-11-15 12:12:08", "link": "http://arxiv.org/abs/2211.08086v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CV"}
{"title": "PARTNR: Pick and place Ambiguity Resolving by Trustworthy iNteractive\n  leaRning", "abstract": "Several recent works show impressive results in mapping language-based human\ncommands and image scene observations to direct robot executable policies\n(e.g., pick and place poses). However, these approaches do not consider the\nuncertainty of the trained policy and simply always execute actions suggested\nby the current policy as the most probable ones. This makes them vulnerable to\ndomain shift and inefficient in the number of required demonstrations. We\nextend previous works and present the PARTNR algorithm that can detect\nambiguities in the trained policy by analyzing multiple modalities in the pick\nand place poses using topological analysis. PARTNR employs an adaptive,\nsensitivity-based, gating function that decides if additional user\ndemonstrations are required. User demonstrations are aggregated to the dataset\nand used for subsequent training. In this way, the policy can adapt promptly to\ndomain shift and it can minimize the number of required demonstrations for a\nwell-trained policy. The adaptive threshold enables to achieve the\nuser-acceptable level of ambiguity to execute the policy autonomously and in\nturn, increase the trustworthiness of our system. We demonstrate the\nperformance of PARTNR in a table-top pick and place task.", "published": "2022-11-15 17:07:40", "link": "http://arxiv.org/abs/2211.08304v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.HC", "cs.LG", "68T05, 68T07, 68T40, 68T45, 68T50", "I.2.6; I.2.7; I.2.9; I.2.10"], "primary_category": "cs.RO"}
{"title": "Music Similarity Calculation of Individual Instrumental Sounds Using\n  Metric Learning", "abstract": "The criteria for measuring music similarity are important for developing a\nflexible music recommendation system. Some data-driven methods have been\nproposed to calculate music similarity from only music signals, such as metric\nlearning based on a triplet loss using tag information on each musical piece.\nHowever, the resulting music similarity metric usually captures the entire\npiece of music, i.e., the mixing of various instrumental sound sources,\nlimiting the capability of the music recommendation system, e.g., it is\ndifficult to search for a musical piece containing similar drum sounds. Towards\nthe development of a more flexible music recommendation system, we propose a\nmusic similarity calculation method that focuses on individual instrumental\nsound sources in a musical piece. By fully exploiting the potential of\ndata-driven methods for our proposed method, we employ weakly supervised metric\nlearning to individual instrumental sound source signals without using any tag\ninformation, where positive and negative samples in a triplet loss are defined\nby whether or not they are from the same musical piece. Furthermore, assuming\nthat each instrumental sound source is not always available in practice, we\nalso investigate the effects of using instrumental sound source separation to\nobtain each source in the proposed method. Experimental results have shown that\n(1) unique similarity metrics can be learned for individual instrumental sound\nsources, (2) similarity metrics learned using some instrumental sound sources\nare possible to lead to more accurate results than that learned using the\nentire musical piece, (3) the performance degraded when learning with the\nseparated instrumental sounds, and (4) similarity metrics learned by the\nproposed method well produced results that correspond to perception by human\nsenses.", "published": "2022-11-15 03:03:22", "link": "http://arxiv.org/abs/2211.07863v1", "categories": ["cs.SD", "eess.AS", "68T99"], "primary_category": "cs.SD"}
{"title": "An Investigation of the Combination of Rehearsal and Knowledge\n  Distillation in Continual Learning for Spoken Language Understanding", "abstract": "Continual learning refers to a dynamical framework in which a model receives\na stream of non-stationary data over time and must adapt to new data while\npreserving previously acquired knowledge. Unluckily, neural networks fail to\nmeet these two desiderata, incurring the so-called catastrophic forgetting\nphenomenon. Whereas a vast array of strategies have been proposed to attenuate\nforgetting in the computer vision domain, for speech-related tasks, on the\nother hand, there is a dearth of works. In this paper, we consider the joint\nuse of rehearsal and knowledge distillation (KD) approaches for spoken language\nunderstanding under a class-incremental learning scenario. We report on\nmultiple KD combinations at different levels in the network, showing that\ncombining feature-level and predictions-level KDs leads to the best results.\nFinally, we provide an ablation study on the effect of the size of the\nrehearsal memory that corroborates the efficacy of our approach for\nlow-resource devices.", "published": "2022-11-15 14:15:22", "link": "http://arxiv.org/abs/2211.08161v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Improved disentangled speech representations using contrastive learning\n  in factorized hierarchical variational autoencoder", "abstract": "Leveraging the fact that speaker identity and content vary on different time\nscales, \\acrlong{fhvae} (\\acrshort{fhvae}) uses different latent variables to\nsymbolize these two attributes. Disentanglement of these attributes is carried\nout by different prior settings of the corresponding latent variables. For the\nprior of speaker identity variable, \\acrshort{fhvae} assumes it is a Gaussian\ndistribution with an utterance-scale varying mean and a fixed variance. By\nsetting a small fixed variance, the training process promotes identity\nvariables within one utterance gathering close to the mean of their prior.\nHowever, this constraint is relatively weak, as the mean of the prior changes\nbetween utterances. Therefore, we introduce contrastive learning into the\n\\acrshort{fhvae} framework, to make the speaker identity variables gathering\nwhen representing the same speaker, while distancing themselves as far as\npossible from those of other speakers. The model structure has not been changed\nin this work but only the training process, thus no additional cost is needed\nduring testing. Voice conversion has been chosen as the application in this\npaper. Latent variable evaluations include speaker verification and\nidentification for the speaker identity variable, and speech recognition for\nthe content variable. Furthermore, assessments of voice conversion performance\nare on the grounds of fake speech detection experiments. Results show that the\nproposed method improves both speaker identity and content feature extraction\ncompared to \\acrshort{fhvae}, and has better performance than baseline on\nconversion.", "published": "2022-11-15 14:55:28", "link": "http://arxiv.org/abs/2211.08191v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Is Style All You Need? Dependencies Between Emotion and GST-based\n  Speaker Recognition", "abstract": "In this work, we study the hypothesis that speaker identity embeddings\nextracted from speech samples may be used for detection and classification of\nemotion. In particular, we show that emotions can be effectively identified by\nlearning speaker identities by use of a 1-D Triplet Convolutional Neural\nNetwork (CNN) & Global Style Token (GST) scheme (e.g., DeepTalk Network) and\nreusing the trained speaker recognition model weights to generate features in\nthe emotion classification domain. The automatic speaker recognition (ASR)\nnetwork is trained with VoxCeleb1, VoxCeleb2, and Librispeech datasets with a\ntriplet training loss function using speaker identity labels. Using an Support\nVector Machine (SVM) classifier, we map speaker identity embeddings into\ndiscrete emotion categories from the CREMA-D, IEMOCAP, and MSP-Podcast\ndatasets. On the task of speech emotion detection, we obtain 80.8% ACC with\nacted emotion samples from CREMA-D, 81.2% ACC with semi-natural emotion samples\nin IEMOCAP, and 66.9% ACC with natural emotion samples in MSP-Podcast. We also\npropose a novel two-stage hierarchical classifier (HC) approach which\ndemonstrates +2% ACC improvement on CREMA-D emotion samples. Through this work,\nwe seek to convey the importance of holistically modeling intra-user variation\nwithin audio samples", "published": "2022-11-15 15:29:47", "link": "http://arxiv.org/abs/2211.08213v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Hybrid Transformers for Music Source Separation", "abstract": "A natural question arising in Music Source Separation (MSS) is whether long\nrange contextual information is useful, or whether local acoustic features are\nsufficient. In other fields, attention based Transformers have shown their\nability to integrate information over long sequences. In this work, we\nintroduce Hybrid Transformer Demucs (HT Demucs), an hybrid temporal/spectral\nbi-U-Net based on Hybrid Demucs, where the innermost layers are replaced by a\ncross-domain Transformer Encoder, using self-attention within one domain, and\ncross-attention across domains. While it performs poorly when trained only on\nMUSDB, we show that it outperforms Hybrid Demucs (trained on the same data) by\n0.45 dB of SDR when using 800 extra training songs. Using sparse attention\nkernels to extend its receptive field, and per source fine-tuning, we achieve\nstate-of-the-art results on MUSDB with extra training data, with 9.20 dB of\nSDR.", "published": "2022-11-15 22:48:16", "link": "http://arxiv.org/abs/2211.08553v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Show Me the Instruments: Musical Instrument Retrieval from Mixture Audio", "abstract": "As digital music production has become mainstream, the selection of\nappropriate virtual instruments plays a crucial role in determining the quality\nof music. To search the musical instrument samples or virtual instruments that\nmake one's desired sound, music producers use their ears to listen and compare\neach instrument sample in their collection, which is time-consuming and\ninefficient. In this paper, we call this task as Musical Instrument Retrieval\nand propose a method for retrieving desired musical instruments using reference\nmusic mixture as a query. The proposed model consists of the Single-Instrument\nEncoder and the Multi-Instrument Encoder, both based on convolutional neural\nnetworks. The Single-Instrument Encoder is trained to classify the instruments\nused in single-track audio, and we take its penultimate layer's activation as\nthe instrument embedding. The Multi-Instrument Encoder is trained to estimate\nmultiple instrument embeddings using the instrument embeddings computed by the\nSingle-Instrument Encoder as a set of target embeddings. For more generalized\ntraining and realistic evaluation, we also propose a new dataset called Nlakh.\nExperimental results showed that the Single-Instrument Encoder was able to\nlearn the mapping from the audio signal of unseen instruments to the instrument\nembedding space and the Multi-Instrument Encoder was able to extract multiple\nembeddings from the mixture of music and retrieve the desired instruments\nsuccessfully. The code used for the experiment and audio samples are available\nat: https://github.com/minju0821/musical_instrument_retrieval", "published": "2022-11-15 07:32:39", "link": "http://arxiv.org/abs/2211.07951v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SSM-Net: feature learning for Music Structure Analysis using a\n  Self-Similarity-Matrix based loss", "abstract": "In this paper, we propose a new paradigm to learn audio features for Music\nStructure Analysis (MSA). We train a deep encoder to learn features such that\nthe Self-Similarity-Matrix (SSM) resulting from those approximates a\nground-truth SSM. This is done by minimizing a loss between both SSMs. Since\nthis loss is differentiable w.r.t. its input features we can train the encoder\nin a straightforward way. We successfully demonstrate the use of this training\nparadigm using the Area Under the Curve ROC (AUC) on the RWC-Pop dataset.", "published": "2022-11-15 13:48:11", "link": "http://arxiv.org/abs/2211.08141v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FlowGrad: Using Motion for Visual Sound Source Localization", "abstract": "Most recent work in visual sound source localization relies on semantic\naudio-visual representations learned in a self-supervised manner, and by design\nexcludes temporal information present in videos. While it proves to be\neffective for widely used benchmark datasets, the method falls short for\nchallenging scenarios like urban traffic. This work introduces temporal context\ninto the state-of-the-art methods for sound source localization in urban scenes\nusing optical flow as a means to encode motion information. An analysis of the\nstrengths and weaknesses of our methods helps us better understand the problem\nof visual sound source localization and sheds light on open challenges for\naudio-visual scene understanding.", "published": "2022-11-15 18:12:10", "link": "http://arxiv.org/abs/2211.08367v2", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Music Instrument Classification Reprogrammed", "abstract": "The performance of approaches to Music Instrument Classification, a popular\ntask in Music Information Retrieval, is often impacted and limited by the lack\nof availability of annotated data for training. We propose to address this\nissue with \"reprogramming,\" a technique that utilizes pre-trained deep and\ncomplex neural networks originally targeting a different task by modifying and\nmapping both the input and output of the pre-trained model. We demonstrate that\nreprogramming can effectively leverage the power of the representation learned\nfor a different task and that the resulting reprogrammed system can perform on\npar or even outperform state-of-the-art systems at a fraction of training\nparameters. Our results, therefore, indicate that reprogramming is a promising\ntechnique potentially applicable to other tasks impeded by data scarcity.", "published": "2022-11-15 18:26:01", "link": "http://arxiv.org/abs/2211.08379v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Rapid Connectionist Speaker Adaptation", "abstract": "We present SVCnet, a system for modelling speaker variability. Encoder Neural\nNetworks specialized for each speech sound produce low dimensionality models of\nacoustical variation, and these models are further combined into an overall\nmodel of voice variability. A training procedure is described which minimizes\nthe dependence of this model on which sounds have been uttered. Using the\ntrained model (SVCnet) and a brief, unconstrained sample of a new speaker's\nvoice, the system produces a Speaker Voice Code that can be used to adapt a\nrecognition system to the new speaker without retraining. A system which\ncombines SVCnet with an MS-TDNN recognizer is described", "published": "2022-11-15 00:15:11", "link": "http://arxiv.org/abs/2211.08978v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Reverberation as Supervision for Speech Separation", "abstract": "This paper proposes reverberation as supervision (RAS), a novel unsupervised\nloss function for single-channel reverberant speech separation. Prior methods\nfor unsupervised separation required the synthesis of mixtures of mixtures or\nassumed the existence of a teacher model, making them difficult to consider as\npotential methods explaining the emergence of separation abilities in an\nanimal's auditory system. We assume the availability of two-channel mixtures at\ntraining time, and train a neural network to separate the sources given one of\nthe channels as input such that the other channel may be predicted from the\nseparated sources. As the relationship between the room impulse responses\n(RIRs) of each channel depends on the locations of the sources, which are\nunknown to the network, the network cannot rely on learning that relationship.\nInstead, our proposed loss function fits each of the separated sources to the\nmixture in the target channel via Wiener filtering, and compares the resulting\nmixture to the ground-truth one. We show that minimizing the scale-invariant\nsignal-to-distortion ratio (SI-SDR) of the predicted right-channel mixture with\nrespect to the ground truth implicitly guides the network towards separating\nthe left-channel sources. On a semi-supervised reverberant speech separation\ntask based on the WHAMR! dataset, using training data where just 5% (resp.,\n10%) of the mixtures are labeled with associated isolated sources, we achieve\n70% (resp., 78%) of the SI-SDR improvement obtained when training with\nsupervision on the full training set, while a model trained only on the labeled\ndata obtains 43% (resp., 45%).", "published": "2022-11-15 17:06:50", "link": "http://arxiv.org/abs/2211.08303v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
