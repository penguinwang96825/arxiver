{"title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling", "abstract": "Recent work has demonstrated that increased training dataset diversity\nimproves general cross-domain knowledge and downstream generalization\ncapability for large-scale language models. With this in mind, we present\n\\textit{the Pile}: an 825 GiB English text corpus targeted at training\nlarge-scale language models. The Pile is constructed from 22 diverse\nhigh-quality subsets -- both existing and newly constructed -- many of which\nderive from academic or professional sources. Our evaluation of the untuned\nperformance of GPT-2 and GPT-3 on the Pile shows that these models struggle on\nmany of its components, such as academic writing. Conversely, models trained on\nthe Pile improve significantly over both Raw CC and CC-100 on all components of\nthe Pile, while improving performance on downstream evaluations. Through an\nin-depth exploratory analysis, we document potentially concerning aspects of\nthe data for prospective users. We make publicly available the code used in its\nconstruction.", "published": "2020-12-31 19:00:10", "link": "http://arxiv.org/abs/2101.00027v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KART: Parameterization of Privacy Leakage Scenarios from Pre-trained\n  Language Models", "abstract": "For the safe sharing pre-trained language models, no guidelines exist at\npresent owing to the difficulty in estimating the upper bound of the risk of\nprivacy leakage. One problem is that previous studies have assessed the risk\nfor different real-world privacy leakage scenarios and attack methods, which\nreduces the portability of the findings. To tackle this problem, we represent\ncomplex real-world privacy leakage scenarios under a universal\nparameterization, \\textit{Knowledge, Anonymization, Resource, and Target}\n(KART). KART parameterization has two merits: (i) it clarifies the definition\nof privacy leakage in each experiment and (ii) it improves the comparability of\nthe findings of risk assessments. We show that previous studies can be simply\nreviewed by parameterizing the scenarios with KART. We also demonstrate privacy\nrisk assessments in different scenarios under the same attack method, which\nsuggests that KART helps approximate the upper bound of risk under a specific\nattack or scenario. We believe that KART helps integrate past and future\nfindings on privacy risk and will contribute to a standard for sharing language\nmodels.", "published": "2020-12-31 19:06:18", "link": "http://arxiv.org/abs/2101.00036v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Modelling Coherence in Spoken Discourse", "abstract": "While there has been significant progress towards modelling coherence in\nwritten discourse, the work in modelling spoken discourse coherence has been\nquite limited. Unlike the coherence in text, coherence in spoken discourse is\nalso dependent on the prosodic and acoustic patterns in speech. In this paper,\nwe model coherence in spoken discourse with audio-based coherence models. We\nperform experiments with four coherence-related tasks with spoken discourses.\nIn our experiments, we evaluate machine-generated speech against the speech\ndelivered by expert human speakers. We also compare the spoken discourses\ngenerated by human language learners of varying language proficiency levels.\nOur results show that incorporating the audio modality along with the text\nbenefits the coherence models in performing downstream coherence related tasks\nwith spoken discourses.", "published": "2020-12-31 20:18:29", "link": "http://arxiv.org/abs/2101.00056v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controlled Analyses of Social Biases in Wikipedia Bios", "abstract": "Social biases on Wikipedia, a widely-read global platform, could greatly\ninfluence public opinion. While prior research has examined man/woman gender\nbias in biography articles, possible influences of other demographic attributes\nlimit conclusions. In this work, we present a methodology for analyzing\nWikipedia pages about people that isolates dimensions of interest (e.g.,\ngender), from other attributes (e.g., occupation). Given a target corpus for\nanalysis (e.g.~biographies about women), we present a method for constructing a\ncomparison corpus that matches the target corpus in as many attributes as\npossible, except the target one. We develop evaluation metrics to measure how\nwell the comparison corpus aligns with the target corpus and then examine how\narticles about gender and racial minorities (cis. women, non-binary people,\ntransgender women, and transgender men; African American, Asian American, and\nHispanic/Latinx American people) differ from other articles. In addition to\nidentifying suspect social biases, our results show that failing to control for\ncovariates can result in different conclusions and veil biases. Our\ncontributions include methodology that facilitates further analyses of bias in\nWikipedia articles, findings that can aid Wikipedia editors in reducing biases,\nand a framework and evaluation metrics to guide future work in this area.", "published": "2020-12-31 21:27:12", "link": "http://arxiv.org/abs/2101.00078v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UNIMO: Towards Unified-Modal Understanding and Generation via\n  Cross-Modal Contrastive Learning", "abstract": "Existed pre-training methods either focus on single-modal tasks or\nmulti-modal tasks, and cannot effectively adapt to each other. They can only\nutilize single-modal data (i.e. text or image) or limited multi-modal data\n(i.e. image-text pairs). In this work, we propose a unified-modal pre-training\narchitecture, namely UNIMO, which can effectively adapt to both single-modal\nand multi-modal understanding and generation tasks. Large scale of free text\ncorpus and image collections can be utilized to improve the capability of\nvisual and textual understanding, and cross-modal contrastive learning (CMCL)\nis leveraged to align the textual and visual information into a unified\nsemantic space over a corpus of image-text pairs. As the non-paired\nsingle-modal data is very rich, our model can utilize much larger scale of data\nto learn more generalizable representations. Moreover, the textual knowledge\nand visual knowledge can enhance each other in the unified semantic space. The\nexperimental results show that UNIMO significantly improves the performance of\nseveral single-modal and multi-modal downstream tasks. Our code and pre-trained\nmodels are public at the UNIMO project page https://unimo-ptm.github.io/", "published": "2020-12-31 02:46:47", "link": "http://arxiv.org/abs/2012.15409v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Verb Knowledge Injection for Multilingual Event Processing", "abstract": "In parallel to their overwhelming success across NLP tasks, language ability\nof deep Transformer networks, pretrained via language modeling (LM) objectives\nhas undergone extensive scrutiny. While probing revealed that these models\nencode a range of syntactic and semantic properties of a language, they are\nstill prone to fall back on superficial cues and simple heuristics to solve\ndownstream tasks, rather than leverage deeper linguistic knowledge. In this\npaper, we target one such area of their deficiency, verbal reasoning. We\ninvestigate whether injecting explicit information on verbs' semantic-syntactic\nbehaviour improves the performance of LM-pretrained Transformers in event\nextraction tasks -- downstream tasks for which accurate verb processing is\nparamount. Concretely, we impart the verb knowledge from curated lexical\nresources into dedicated adapter modules (dubbed verb adapters), allowing it to\ncomplement, in downstream tasks, the language knowledge obtained during\nLM-pretraining. We first demonstrate that injecting verb knowledge leads to\nperformance gains in English event extraction. We then explore the utility of\nverb adapters for event extraction in other languages: we investigate (1)\nzero-shot language transfer with multilingual Transformers as well as (2)\ntransfer via (noisy automatic) translation of English verb-based lexical\nconstraints. Our results show that the benefits of verb knowledge injection\nindeed extend to other languages, even when verb adapters are trained on\nnoisily translated constraints.", "published": "2020-12-31 03:24:34", "link": "http://arxiv.org/abs/2012.15421v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The jsRealB Text Realizer: Organization and Use Cases -- Revised version", "abstract": "This paper describes the design principles behind jsRealB (Version 4.0), a\nsurface realizer written JavaScript for English or French sentences from a\nspecification inspired by the constituent syntax formalism but for which a\ndependency-based input notation is also available. jsRealB can be used either\nwithin a web page or as a node.js module. We show that the seemingly simple\nprocess of text realization involves many interesting implementation challenges\nin order to take into account the specifics of each language. jsRealB has a\nlarge coverage of English and French and has been used to develop realistic\ndata-to-text applications and to reproduce existing literary texts and\nsentences from Universal Dependency annotations. Its source code and that of\nits applications are available on GitHub. The port of this approach to Python\n(pyrealb) is also presented.", "published": "2020-12-31 03:32:58", "link": "http://arxiv.org/abs/2012.15425v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Fully Synthetic Data Improves Neural Machine Translation with Knowledge\n  Distillation", "abstract": "This paper explores augmenting monolingual data for knowledge distillation in\nneural machine translation. Source language monolingual text can be\nincorporated as a forward translation. Interestingly, we find the best way to\nincorporate target language monolingual text is to translate it to the source\nlanguage and round-trip translate it back to the target language, resulting in\na fully synthetic corpus. We find that combining monolingual data from both\nsource and target languages yields better performance than a corpus twice as\nlarge only in one language. Moreover, experiments reveal that the improvement\ndepends upon the provenance of the test set. If the test set was originally in\nthe source language (with the target side written by translators), then forward\ntranslating source monolingual data matters. If the test set was originally in\nthe target language (with the source written by translators), then\nincorporating target monolingual data matters.", "published": "2020-12-31 05:28:42", "link": "http://arxiv.org/abs/2012.15455v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CLEAR: Contrastive Learning for Sentence Representation", "abstract": "Pre-trained language models have proven their unique powers in capturing\nimplicit language features. However, most pre-training approaches focus on the\nword-level training objective, while sentence-level objectives are rarely\nstudied. In this paper, we propose Contrastive LEArning for sentence\nRepresentation (CLEAR), which employs multiple sentence-level augmentation\nstrategies in order to learn a noise-invariant sentence representation. These\naugmentations include word and span deletion, reordering, and substitution.\nFurthermore, we investigate the key reasons that make contrastive learning\neffective through numerous experiments. We observe that different sentence\naugmentations during pre-training lead to different performance improvements on\nvarious downstream tasks. Our approach is shown to outperform multiple existing\nmethods on both SentEval and GLUE benchmarks.", "published": "2020-12-31 06:40:13", "link": "http://arxiv.org/abs/2012.15466v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FiD-Ex: Improving Sequence-to-Sequence Models for Extractive Rationale\n  Generation", "abstract": "Natural language (NL) explanations of model predictions are gaining\npopularity as a means to understand and verify decisions made by large\nblack-box pre-trained models, for NLP tasks such as Question Answering (QA) and\nFact Verification. Recently, pre-trained sequence to sequence (seq2seq) models\nhave proven to be very effective in jointly making predictions, as well as\ngenerating NL explanations. However, these models have many shortcomings; they\ncan fabricate explanations even for incorrect predictions, they are difficult\nto adapt to long input documents, and their training requires a large amount of\nlabeled data. In this paper, we develop FiD-Ex, which addresses these\nshortcomings for seq2seq models by: 1) introducing sentence markers to\neliminate explanation fabrication by encouraging extractive generation, 2)\nusing the fusion-in-decoder architecture to handle long input contexts, and 3)\nintermediate fine-tuning on re-structured open domain QA datasets to improve\nfew-shot performance. FiD-Ex significantly improves over prior work in terms of\nexplanation metrics and task accuracy, on multiple tasks from the ERASER\nexplainability benchmark, both in the fully supervised and in the few-shot\nsettings.", "published": "2020-12-31 07:22:15", "link": "http://arxiv.org/abs/2012.15482v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation: A Review of Methods, Resources, and Tools", "abstract": "Machine translation (MT) is an important sub-field of natural language\nprocessing that aims to translate natural languages using computers. In recent\nyears, end-to-end neural machine translation (NMT) has achieved great success\nand has become the new mainstream method in practical MT systems. In this\narticle, we first provide a broad review of the methods for NMT and focus on\nmethods relating to architectures, decoding, and data augmentation. Then we\nsummarize the resources and tools that are useful for researchers. Finally, we\nconclude with a discussion of possible future research directions.", "published": "2020-12-31 09:35:27", "link": "http://arxiv.org/abs/2012.15515v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AraELECTRA: Pre-Training Text Discriminators for Arabic Language\n  Understanding", "abstract": "Advances in English language representation enabled a more sample-efficient\npre-training task by Efficiently Learning an Encoder that Classifies Token\nReplacements Accurately (ELECTRA). Which, instead of training a model to\nrecover masked tokens, it trains a discriminator model to distinguish true\ninput tokens from corrupted tokens that were replaced by a generator network.\nOn the other hand, current Arabic language representation approaches rely only\non pretraining via masked language modeling. In this paper, we develop an\nArabic language representation model, which we name AraELECTRA. Our model is\npretrained using the replaced token detection objective on large Arabic text\ncorpora. We evaluate our model on multiple Arabic NLP tasks, including reading\ncomprehension, sentiment analysis, and named-entity recognition and we show\nthat AraELECTRA outperforms current state-of-the-art Arabic language\nrepresentation models, given the same pretraining data and with even a smaller\nmodel size.", "published": "2020-12-31 09:35:39", "link": "http://arxiv.org/abs/2012.15516v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AraGPT2: Pre-Trained Transformer for Arabic Language Generation", "abstract": "Recently, pre-trained transformer-based architectures have proven to be very\nefficient at language modeling and understanding, given that they are trained\non a large enough corpus. Applications in language generation for Arabic are\nstill lagging in comparison to other NLP advances primarily due to the lack of\nadvanced Arabic language generation models. In this paper, we develop the first\nadvanced Arabic language generation model, AraGPT2, trained from scratch on a\nlarge Arabic corpus of internet text and news articles. Our largest model,\nAraGPT2-mega, has 1.46 billion parameters, which makes it the largest Arabic\nlanguage model available. The Mega model was evaluated and showed success on\ndifferent tasks including synthetic news generation, and zero-shot question\nanswering. For text generation, our best model achieves a perplexity of 29.8 on\nheld-out Wikipedia articles. A study conducted with human evaluators showed the\nsignificant success of AraGPT2-mega in generating news articles that are\ndifficult to distinguish from articles written by humans. We thus develop and\nrelease an automatic discriminator model with a 98% percent accuracy in\ndetecting model-generated text. The models are also publicly available, hoping\nto encourage new research directions and applications for Arabic NLP.", "published": "2020-12-31 09:48:05", "link": "http://arxiv.org/abs/2012.15520v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fast WordPiece Tokenization", "abstract": "Tokenization is a fundamental preprocessing step for almost all NLP tasks. In\nthis paper, we propose efficient algorithms for the WordPiece tokenization used\nin BERT, from single-word tokenization to general text (e.g., sentence)\ntokenization. When tokenizing a single word, WordPiece uses a\nlongest-match-first strategy, known as maximum matching. The best known\nalgorithms so far are O(n^2) (where n is the input length) or O(nm) (where m is\nthe maximum vocabulary token length). We propose a novel algorithm whose\ntokenization complexity is strictly O(n). Our method is inspired by the\nAho-Corasick algorithm. We introduce additional linkages on top of the trie\nbuilt from the vocabulary, allowing smart transitions when the trie matching\ncannot continue. For general text, we further propose an algorithm that\ncombines pre-tokenization (splitting the text into words) and our linear-time\nWordPiece method into a single pass. Experimental results show that our method\nis 8.2x faster than HuggingFace Tokenizers and 5.1x faster than TensorFlow Text\non average for general text tokenization.", "published": "2020-12-31 10:01:29", "link": "http://arxiv.org/abs/2012.15524v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BANG: Bridging Autoregressive and Non-autoregressive Generation with\n  Large Scale Pretraining", "abstract": "In this paper, we propose BANG, a new pretraining model to Bridge the gap\nbetween Autoregressive (AR) and Non-autoregressive (NAR) Generation. AR and NAR\ngeneration can be uniformly regarded as to what extent previous tokens can be\nattended, and BANG bridges AR and NAR generation by designing a novel model\nstructure for large-scale pretraining. The pretrained BANG model can\nsimultaneously support AR, NAR and semi-NAR generation to meet different\nrequirements. Experiments on question generation (SQuAD 1.1), summarization\n(XSum) and dialogue generation (PersonaChat) show that BANG improves NAR and\nsemi-NAR performance significantly as well as attaining comparable performance\nwith strong AR pretrained models. Compared with the semi-NAR strong baselines,\nBANG achieves absolute improvements of 14.01 and 5.24 in the overall scores of\nSQuAD 1.1 and XSum, respectively. In addition, BANG achieves absolute\nimprovements of 10.73, 6.39 and 5.90 in the overall scores of SQuAD, XSUM and\nPersonaChat respectively compared with the strong NAR baselines.", "published": "2020-12-31 10:09:29", "link": "http://arxiv.org/abs/2012.15525v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HopRetriever: Retrieve Hops over Wikipedia to Answer Complex Questions", "abstract": "Collecting supporting evidence from large corpora of text (e.g., Wikipedia)\nis of great challenge for open-domain Question Answering (QA). Especially, for\nmulti-hop open-domain QA, scattered evidence pieces are required to be gathered\ntogether to support the answer extraction. In this paper, we propose a new\nretrieval target, hop, to collect the hidden reasoning evidence from Wikipedia\nfor complex question answering. Specifically, the hop in this paper is defined\nas the combination of a hyperlink and the corresponding outbound link document.\nThe hyperlink is encoded as the mention embedding which models the structured\nknowledge of how the outbound link entity is mentioned in the textual context,\nand the corresponding outbound link document is encoded as the document\nembedding representing the unstructured knowledge within it. Accordingly, we\nbuild HopRetriever which retrieves hops over Wikipedia to answer complex\nquestions. Experiments on the HotpotQA dataset demonstrate that HopRetriever\noutperforms previously published evidence retrieval methods by large margins.\nMoreover, our approach also yields quantifiable interpretations of the evidence\ncollection process.", "published": "2020-12-31 10:36:01", "link": "http://arxiv.org/abs/2012.15534v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XLM-T: Scaling up Multilingual Machine Translation with Pretrained\n  Cross-lingual Transformer Encoders", "abstract": "Multilingual machine translation enables a single model to translate between\ndifferent languages. Most existing multilingual machine translation systems\nadopt a randomly initialized Transformer backbone. In this work, inspired by\nthe recent success of language model pre-training, we present XLM-T, which\ninitializes the model with an off-the-shelf pretrained cross-lingual\nTransformer encoder and fine-tunes it with multilingual parallel data. This\nsimple method achieves significant improvements on a WMT dataset with 10\nlanguage pairs and the OPUS-100 corpus with 94 pairs. Surprisingly, the method\nis also effective even upon the strong baseline with back-translation.\nMoreover, extensive analysis of XLM-T on unsupervised syntactic parsing, word\nalignment, and multilingual classification explains its effectiveness for\nmachine translation. The code will be at https://aka.ms/xlm-t.", "published": "2020-12-31 11:16:51", "link": "http://arxiv.org/abs/2012.15547v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UNKs Everywhere: Adapting Multilingual Language Models to New Scripts", "abstract": "Massively multilingual language models such as multilingual BERT offer\nstate-of-the-art cross-lingual transfer performance on a range of NLP tasks.\nHowever, due to limited capacity and large differences in pretraining data\nsizes, there is a profound performance gap between resource-rich and\nresource-poor target languages. The ultimate challenge is dealing with\nunder-resourced languages not covered at all by the models and written in\nscripts unseen during pretraining. In this work, we propose a series of novel\ndata-efficient methods that enable quick and effective adaptation of pretrained\nmultilingual models to such low-resource languages and unseen scripts. Relying\non matrix factorization, our methods capitalize on the existing latent\nknowledge about multiple languages already available in the pretrained model's\nembedding matrix. Furthermore, we show that learning of the new dedicated\nembedding matrix in the target language can be improved by leveraging a small\nnumber of vocabulary items (i.e., the so-called lexically overlapping tokens)\nshared between mBERT's and target language vocabulary. Our adaptation\ntechniques offer substantial performance gains for languages with unseen\nscripts. We also demonstrate that they can yield improvements for low-resource\nlanguages written in scripts covered by the pretrained model.", "published": "2020-12-31 11:37:28", "link": "http://arxiv.org/abs/2012.15562v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coreference Reasoning in Machine Reading Comprehension", "abstract": "Coreference resolution is essential for natural language understanding and\nhas been long studied in NLP. In recent years, as the format of Question\nAnswering (QA) became a standard for machine reading comprehension (MRC), there\nhave been data collection efforts, e.g., Dasigi et al. (2019), that attempt to\nevaluate the ability of MRC models to reason about coreference. However, as we\nshow, coreference reasoning in MRC is a greater challenge than earlier thought;\nMRC datasets do not reflect the natural distribution and, consequently, the\nchallenges of coreference reasoning. Specifically, success on these datasets\ndoes not reflect a model's proficiency in coreference reasoning. We propose a\nmethodology for creating MRC datasets that better reflect the challenges of\ncoreference reasoning and use it to create a sample evaluation set. The results\non our dataset show that state-of-the-art models still struggle with these\nphenomena. Furthermore, we develop an effective way to use naturally occurring\ncoreference phenomena from existing coreference resolution datasets when\ntraining MRC models. This allows us to show an improvement in the coreference\nreasoning abilities of state-of-the-art models. The code and the resulting\ndataset are available at https://github.com/UKPLab/coref-reasoning-in-qa.", "published": "2020-12-31 12:18:41", "link": "http://arxiv.org/abs/2012.15573v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HateCheck: Functional Tests for Hate Speech Detection Models", "abstract": "Detecting online hate is a difficult task that even state-of-the-art models\nstruggle with. Typically, hate speech detection models are evaluated by\nmeasuring their performance on held-out test data using metrics such as\naccuracy and F1 score. However, this approach makes it difficult to identify\nspecific model weak points. It also risks overestimating generalisable model\nperformance due to increasingly well-evidenced systematic gaps and biases in\nhate speech datasets. To enable more targeted diagnostic insights, we introduce\nHateCheck, a suite of functional tests for hate speech detection models. We\nspecify 29 model functionalities motivated by a review of previous research and\na series of interviews with civil society stakeholders. We craft test cases for\neach functionality and validate their quality through a structured annotation\nprocess. To illustrate HateCheck's utility, we test near-state-of-the-art\ntransformer models as well as two popular commercial models, revealing critical\nmodel weaknesses.", "published": "2020-12-31 13:44:56", "link": "http://arxiv.org/abs/2012.15606v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Good is Your Tokenizer? On the Monolingual Performance of\n  Multilingual Language Models", "abstract": "In this work, we provide a systematic and comprehensive empirical comparison\nof pretrained multilingual language models versus their monolingual\ncounterparts with regard to their monolingual task performance. We study a set\nof nine typologically diverse languages with readily available pretrained\nmonolingual models on a set of five diverse monolingual downstream tasks. We\nfirst aim to establish, via fair and controlled comparisons, if a gap between\nthe multilingual and the corresponding monolingual representation of that\nlanguage exists, and subsequently investigate the reason for any performance\ndifference. To disentangle conflating factors, we train new monolingual models\non the same data, with monolingually and multilingually trained tokenizers. We\nfind that while the pretraining data size is an important factor, a designated\nmonolingual tokenizer plays an equally important role in the downstream\nperformance. Our results show that languages that are adequately represented in\nthe multilingual model's vocabulary exhibit negligible performance decreases\nover their monolingual counterparts. We further find that replacing the\noriginal multilingual tokenizer with the specialized monolingual tokenizer\nimproves the downstream performance of the multilingual model for almost every\ntask and language.", "published": "2020-12-31 14:11:00", "link": "http://arxiv.org/abs/2012.15613v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open Korean Corpora: A Practical Report", "abstract": "Korean is often referred to as a low-resource language in the research\ncommunity. While this claim is partially true, it is also because the\navailability of resources is inadequately advertised and curated. This work\ncurates and reviews a list of Korean corpora, first describing\ninstitution-level resource development, then further iterate through a list of\ncurrent open datasets for different types of tasks. We then propose a direction\non how open-source dataset construction and releases should be done for\nless-resourced languages to promote research.", "published": "2020-12-31 14:23:55", "link": "http://arxiv.org/abs/2012.15621v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TexSmart: A Text Understanding System for Fine-Grained NER and Enhanced\n  Semantic Analysis", "abstract": "This technique report introduces TexSmart, a text understanding system that\nsupports fine-grained named entity recognition (NER) and enhanced semantic\nanalysis functionalities. Compared to most previous publicly available text\nunderstanding systems and tools, TexSmart holds some unique features. First,\nthe NER function of TexSmart supports over 1,000 entity types, while most other\npublic tools typically support several to (at most) dozens of entity types.\nSecond, TexSmart introduces new semantic analysis functions like semantic\nexpansion and deep semantic representation, that are absent in most previous\nsystems. Third, a spectrum of algorithms (from very fast algorithms to those\nthat are relatively slow but more accurate) are implemented for one function in\nTexSmart, to fulfill the requirements of different academic and industrial\napplications. The adoption of unsupervised or weakly-supervised algorithms is\nespecially emphasized, with the goal of easily updating our models to include\nfresh data with less human annotation efforts.\n  The main contents of this report include major functions of TexSmart,\nalgorithms for achieving these functions, how to use the TexSmart toolkit and\nWeb APIs, and evaluation results of some key algorithms.", "published": "2020-12-31 14:58:01", "link": "http://arxiv.org/abs/2012.15639v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoCoLM: COmplex COmmonsense Enhanced Language Model with Discourse\n  Relations", "abstract": "Large-scale pre-trained language models have demonstrated strong knowledge\nrepresentation ability. However, recent studies suggest that even though these\ngiant models contains rich simple commonsense knowledge (e.g., bird can fly and\nfish can swim.), they often struggle with the complex commonsense knowledge\nthat involves multiple eventualities (verb-centric phrases, e.g., identifying\nthe relationship between ``Jim yells at Bob'' and ``Bob is upset'').To address\nthis problem, in this paper, we propose to help pre-trained language models\nbetter incorporate complex commonsense knowledge. Different from existing\nfine-tuning approaches, we do not focus on a specific task and propose a\ngeneral language model named CoCoLM. Through the careful training over a\nlarge-scale eventuality knowledge graphs ASER, we successfully teach\npre-trained language models (i.e., BERT and RoBERTa) rich complex commonsense\nknowledge among eventualities. Experiments on multiple downstream commonsense\ntasks that requires the correct understanding of eventualities demonstrate the\neffectiveness of CoCoLM.", "published": "2020-12-31 15:05:36", "link": "http://arxiv.org/abs/2012.15643v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Vocabulary Learning via Optimal Transport for Neural Machine Translation", "abstract": "The choice of token vocabulary affects the performance of machine\ntranslation. This paper aims to figure out what is a good vocabulary and\nwhether one can find the optimal vocabulary without trial training. To answer\nthese questions, we first provide an alternative understanding of the role of\nvocabulary from the perspective of information theory. Motivated by this, we\nformulate the quest of vocabularization -- finding the best token dictionary\nwith a proper size -- as an optimal transport (OT) problem. We propose VOLT, a\nsimple and efficient solution without trial training. Empirical results show\nthat VOLT outperforms widely-used vocabularies in diverse scenarios, including\nWMT-14 English-German and TED's 52 translation directions. For example, VOLT\nachieves almost 70% vocabulary size reduction and 0.5 BLEU gain on\nEnglish-German translation. Also, compared to BPE-search, VOLT reduces the\nsearch time from 384 GPU hours to 30 GPU hours on English-German translation.\nCodes are available at https://github.com/Jingjing-NLP/VOLT .", "published": "2020-12-31 15:49:49", "link": "http://arxiv.org/abs/2012.15671v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual\n  Semantics with Monolingual Corpora", "abstract": "Recent studies have demonstrated that pre-trained cross-lingual models\nachieve impressive performance in downstream cross-lingual tasks. This\nimprovement benefits from learning a large amount of monolingual and parallel\ncorpora. Although it is generally acknowledged that parallel corpora are\ncritical for improving the model performance, existing methods are often\nconstrained by the size of parallel corpora, especially for low-resource\nlanguages. In this paper, we propose ERNIE-M, a new training method that\nencourages the model to align the representation of multiple languages with\nmonolingual corpora, to overcome the constraint that the parallel corpus size\nplaces on the model performance. Our key insight is to integrate\nback-translation into the pre-training process. We generate pseudo-parallel\nsentence pairs on a monolingual corpus to enable the learning of semantic\nalignments between different languages, thereby enhancing the semantic modeling\nof cross-lingual models. Experimental results show that ERNIE-M outperforms\nexisting cross-lingual models and delivers new state-of-the-art results in\nvarious cross-lingual downstream tasks.", "published": "2020-12-31 15:52:27", "link": "http://arxiv.org/abs/2012.15674v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots\n  Matters", "abstract": "Few-shot crosslingual transfer has been shown to outperform its zero-shot\ncounterpart with pretrained encoders like multilingual BERT. Despite its\ngrowing popularity, little to no attention has been paid to standardizing and\nanalyzing the design of few-shot experiments. In this work, we highlight a\nfundamental risk posed by this shortcoming, illustrating that the model\nexhibits a high degree of sensitivity to the selection of few shots. We conduct\na large-scale experimental study on 40 sets of sampled few shots for six\ndiverse NLP tasks across up to 40 languages. We provide an analysis of success\nand failure cases of few-shot transfer, which highlights the role of lexical\nfeatures. Additionally, we show that a straightforward full model finetuning\napproach is quite effective for few-shot transfer, outperforming several\nstate-of-the-art few-shot approaches. As a step towards standardizing few-shot\ncrosslingual experimental designs, we make our sampled few shots publicly\navailable.", "published": "2020-12-31 16:03:48", "link": "http://arxiv.org/abs/2012.15682v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer", "abstract": "Transformers are not suited for processing long documents, due to their\nquadratically increasing memory and time consumption. Simply truncating a long\ndocument or applying the sparse attention mechanism will incur the context\nfragmentation problem or lead to an inferior modeling capability against\ncomparable model sizes. In this paper, we propose ERNIE-Doc, a document-level\nlanguage pretraining model based on Recurrence Transformers. Two well-designed\ntechniques, namely the retrospective feed mechanism and the enhanced recurrence\nmechanism, enable ERNIE-Doc, which has a much longer effective context length,\nto capture the contextual information of a complete document. We pretrain\nERNIE-Doc to explicitly learn the relationships among segments with an\nadditional document-aware segment-reordering objective. Various experiments\nwere conducted on both English and Chinese document-level tasks. ERNIE-Doc\nimproved the state-of-the-art language modeling result of perplexity to 16.8 on\nWikiText-103. Moreover, it outperformed competitive pretraining models by a\nlarge margin on most language understanding tasks, such as text classification\nand question answering.", "published": "2020-12-31 16:12:48", "link": "http://arxiv.org/abs/2012.15688v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Better Robustness by More Coverage: Adversarial Training with Mixup\n  Augmentation for Robust Fine-tuning", "abstract": "Pretrained language models (PLMs) perform poorly under adversarial attacks.\nTo improve the adversarial robustness, adversarial data augmentation (ADA) has\nbeen widely adopted to cover more search space of adversarial attacks by adding\ntextual adversarial examples during training. However, the number of\nadversarial examples for text augmentation is still extremely insufficient due\nto the exponentially large attack search space. In this work, we propose a\nsimple and effective method to cover a much larger proportion of the attack\nsearch space, called Adversarial and Mixup Data Augmentation (AMDA).\nSpecifically, AMDA linearly interpolates the representations of pairs of\ntraining samples to form new virtual samples, which are more abundant and\ndiverse than the discrete text adversarial examples in conventional ADA.\nMoreover, to fairly evaluate the robustness of different models, we adopt a\nchallenging evaluation setup, which generates a new set of adversarial examples\ntargeting each model. In text classification experiments of BERT and RoBERTa,\nAMDA achieves significant robustness gains under two strong adversarial attacks\nand alleviates the performance degradation of ADA on the clean data. Our code\nis available at: https://github.com/thunlp/MixADA .", "published": "2020-12-31 16:28:07", "link": "http://arxiv.org/abs/2012.15699v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BinaryBERT: Pushing the Limit of BERT Quantization", "abstract": "The rapid development of large pre-trained language models has greatly\nincreased the demand for model compression techniques, among which quantization\nis a popular solution. In this paper, we propose BinaryBERT, which pushes BERT\nquantization to the limit by weight binarization. We find that a binary BERT is\nhard to be trained directly than a ternary counterpart due to its complex and\nirregular loss landscape. Therefore, we propose ternary weight splitting, which\ninitializes BinaryBERT by equivalently splitting from a half-sized ternary\nnetwork. The binary model thus inherits the good performance of the ternary\none, and can be further enhanced by fine-tuning the new architecture after\nsplitting. Empirical results show that our BinaryBERT has only a slight\nperformance drop compared with the full-precision model while being 24x\nsmaller, achieving the state-of-the-art compression results on the GLUE and\nSQuAD benchmarks.", "published": "2020-12-31 16:34:54", "link": "http://arxiv.org/abs/2012.15701v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Robust Neural Machine Translation: A Transformer Case Study", "abstract": "Transformers (Vaswani et al., 2017) have brought a remarkable improvement in\nthe performance of neural machine translation (NMT) systems but they could be\nsurprisingly vulnerable to noise. In this work, we try to investigate how noise\nbreaks Transformers and if there exist solutions to deal with such issues.\nThere is a large body of work in the NMT literature on analyzing the behavior\nof conventional models for the problem of noise but Transformers are relatively\nunderstudied in this context. Motivated by this, we introduce a novel\ndata-driven technique called Target Augmented Fine-tuning (TAFT) to incorporate\nnoise during training. This idea is comparable to the well-known fine-tuning\nstrategy. Moreover, we propose two other novel extensions to the original\nTransformer: Controlled Denoising (CD) and Dual-Channel Decoding (DCD), that\nmodify the neural architecture as well as the training process to handle noise.\nOne important characteristic of our techniques is that they only impact the\ntraining phase and do not impose any overhead at inference time. We evaluated\nour techniques to translate the English--German pair in both directions and\nobserved that our models have a higher tolerance to noise. More specifically,\nthey perform with no deterioration where up to 10% of entire test words are\ninfected by noise.", "published": "2020-12-31 16:55:05", "link": "http://arxiv.org/abs/2012.15710v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FGraDA: A Dataset and Benchmark for Fine-Grained Domain Adaptation in\n  Machine Translation", "abstract": "Previous research for adapting a general neural machine translation (NMT)\nmodel into a specific domain usually neglects the diversity in translation\nwithin the same domain, which is a core problem for domain adaptation in\nreal-world scenarios. One representative of such challenging scenarios is to\ndeploy a translation system for a conference with a specific topic, e.g.,\nglobal warming or coronavirus, where there are usually extremely less resources\ndue to the limited schedule. To motivate wider investigation in such a\nscenario, we present a real-world fine-grained domain adaptation task in\nmachine translation (FGraDA). The FGraDA dataset consists of Chinese-English\ntranslation task for four sub-domains of information technology: autonomous\nvehicles, AI education, real-time networks, and smart phone. Each sub-domain is\nequipped with a development set and test set for evaluation purposes. To be\ncloser to reality, FGraDA does not employ any in-domain bilingual training data\nbut provides bilingual dictionaries and wiki knowledge base, which can be\neasier obtained within a short time. We benchmark the fine-grained domain\nadaptation task and present in-depth analyses showing that there are still\nchallenging problems to further improve the performance with heterogeneous\nresources.", "published": "2020-12-31 17:15:09", "link": "http://arxiv.org/abs/2012.15717v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Politics via Contextualized Discourse Processing", "abstract": "Politicians often have underlying agendas when reacting to events. Arguments\nin contexts of various events reflect a fairly consistent set of agendas for a\ngiven entity. In spite of recent advances in Pretrained Language Models (PLMs),\nthose text representations are not designed to capture such nuanced patterns.\nIn this paper, we propose a Compositional Reader model consisting of encoder\nand composer modules, that attempts to capture and leverage such information to\ngenerate more effective representations for entities, issues, and events. These\nrepresentations are contextualized by tweets, press releases, issues, news\narticles, and participating entities. Our model can process several documents\nat once and generate composed representations for multiple entities over\nseveral issues or events. Via qualitative and quantitative empirical analysis,\nwe show that these representations are meaningful and effective.", "published": "2020-12-31 18:07:07", "link": "http://arxiv.org/abs/2012.15784v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conditional Generation of Temporally-ordered Event Sequences", "abstract": "Models of narrative schema knowledge have proven useful for a range of\nevent-related tasks, but they typically do not capture the temporal\nrelationships between events. We propose a single model that addresses both\ntemporal ordering, sorting given events into the order they occurred, and event\ninfilling, predicting new events which fit into an existing temporally-ordered\nsequence. We use a BART-based conditional generation model that can capture\nboth temporality and common event co-occurrence, meaning it can be flexibly\napplied to different tasks in this space. Our model is trained as a denoising\nautoencoder: we take temporally-ordered event sequences, shuffle them, delete\nsome events, and then attempt to recover the original event sequence. This task\nteaches the model to make inferences given incomplete knowledge about the\nevents in an underlying scenario. On the temporal ordering task, we show that\nour model is able to unscramble event sequences from existing datasets without\naccess to explicitly labeled temporal training data, outperforming both a\nBERT-based pairwise model and a BERT-based pointer network. On event infilling,\nhuman evaluation shows that our model is able to generate events that fit\nbetter temporally into the input events when compared to GPT-2 story completion\nmodels.", "published": "2020-12-31 18:10:18", "link": "http://arxiv.org/abs/2012.15786v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Promoting Graph Awareness in Linearized Graph-to-Text Generation", "abstract": "Generating text from structured inputs, such as meaning representations or\nRDF triples, has often involved the use of specialized graph-encoding neural\nnetworks. However, recent applications of pretrained transformers to\nlinearizations of graph inputs have yielded state-of-the-art generation results\non graph-to-text tasks. Here, we explore the ability of these linearized models\nto encode local graph structures, in particular their invariance to the graph\nlinearization strategy and their ability to reconstruct corrupted inputs. Our\nfindings motivate solutions to enrich the quality of models' implicit graph\nencodings via scaffolding. Namely, we use graph-denoising objectives\nimplemented in a multi-task text-to-text framework. We find that these\ndenoising scaffolds lead to substantial improvements in downstream generation\nin low-resource settings.", "published": "2020-12-31 18:17:57", "link": "http://arxiv.org/abs/2012.15793v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UCCA's Foundational Layer: Annotation Guidelines v2.1", "abstract": "This is the annotation manual for Universal Conceptual Cognitive Annotation\n(UCCA; Abend and Rappoport, 2013), specifically the Foundational Layer. UCCA is\na graph-based semantic annotation scheme based on typological linguistic\nprinciples. It has been applied to several languages; for ease of exposition\nthese guidelines give examples mainly in English. New annotators may wish to\nstart with the tutorial on the UCCA framework (Abend et al., 2020). Further\nresources are available at the project homepage:\nhttps://universalconceptualcognitiveannotation.github.io", "published": "2020-12-31 18:34:29", "link": "http://arxiv.org/abs/2012.15810v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MiniLMv2: Multi-Head Self-Attention Relation Distillation for\n  Compressing Pretrained Transformers", "abstract": "We generalize deep self-attention distillation in MiniLM (Wang et al., 2020)\nby only using self-attention relation distillation for task-agnostic\ncompression of pretrained Transformers. In particular, we define multi-head\nself-attention relations as scaled dot-product between the pairs of query, key,\nand value vectors within each self-attention module. Then we employ the above\nrelational knowledge to train the student model. Besides its simplicity and\nunified principle, more favorably, there is no restriction in terms of the\nnumber of student's attention heads, while most previous work has to guarantee\nthe same head number between teacher and student. Moreover, the fine-grained\nself-attention relations tend to fully exploit the interaction knowledge\nlearned by Transformer. In addition, we thoroughly examine the layer selection\nstrategy for teacher models, rather than just relying on the last layer as in\nMiniLM. We conduct extensive experiments on compressing both monolingual and\nmultilingual pretrained models. Experimental results demonstrate that our\nmodels distilled from base-size and large-size teachers (BERT, RoBERTa and\nXLM-R) outperform the state-of-the-art.", "published": "2020-12-31 18:51:26", "link": "http://arxiv.org/abs/2012.15828v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shortformer: Better Language Modeling using Shorter Inputs", "abstract": "Increasing the input length has been a driver of progress in language\nmodeling with transformers. We identify conditions where shorter inputs are not\nharmful, and achieve perplexity and efficiency improvements through two new\nmethods that decrease input length. First, we show that initially training a\nmodel on short subsequences before moving on to longer ones both reduces\noverall training time and, surprisingly, substantially improves perplexity.\nSecond, we show how to improve the efficiency of recurrence methods in\ntransformers, which let models condition on previously processed tokens when\ngenerating sequences that exceed the maximal length the transformer can handle\nat once. Existing methods require computationally expensive relative position\nembeddings; we introduce a simple alternative of adding absolute position\nembeddings to queries and keys instead of to word embeddings, which efficiently\nproduces superior results. We show that these recurrent models also benefit\nfrom short input lengths. Combining these techniques speeds up training by a\nfactor of 1.65, reduces memory usage, and substantially improves perplexity on\nWikiText-103, without adding any parameters.", "published": "2020-12-31 18:52:59", "link": "http://arxiv.org/abs/2012.15832v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fully Non-autoregressive Neural Machine Translation: Tricks of the Trade", "abstract": "Fully non-autoregressive neural machine translation (NAT) is proposed to\nsimultaneously predict tokens with single forward of neural networks, which\nsignificantly reduces the inference latency at the expense of quality drop\ncompared to the Transformer baseline. In this work, we target on closing the\nperformance gap while maintaining the latency advantage. We first inspect the\nfundamental issues of fully NAT models, and adopt dependency reduction in the\nlearning space of output tokens as the basic guidance. Then, we revisit methods\nin four different aspects that have been proven effective for improving NAT\nmodels, and carefully combine these techniques with necessary modifications.\nOur extensive experiments on three translation benchmarks show that the\nproposed system achieves the new state-of-the-art results for fully NAT models,\nand obtains comparable performance with the autoregressive and iterative NAT\nsystems. For instance, one of the proposed models achieves 27.49 BLEU points on\nWMT14 En-De with approximately 16.5X speed up at inference time.", "published": "2020-12-31 18:52:59", "link": "http://arxiv.org/abs/2012.15833v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Natural Language Relations between Answer Choices for Machine\n  Comprehension", "abstract": "When evaluating an answer choice for Reading Comprehension task, other answer\nchoices available for the question and the answers of related questions about\nthe same paragraph often provide valuable information. In this paper, we\npropose a method to leverage the natural language relations between the answer\nchoices, such as entailment and contradiction, to improve the performance of\nmachine comprehension. We use a stand-alone question answering (QA) system to\nperform QA task and a Natural Language Inference (NLI) system to identify the\nrelations between the choice pairs. Then we perform inference using an Integer\nLinear Programming (ILP)-based relational framework to re-evaluate the\ndecisions made by the standalone QA system in light of the relations identified\nby the NLI system. We also propose a multitask learning model that learns both\nthe tasks jointly.", "published": "2020-12-31 18:55:30", "link": "http://arxiv.org/abs/2012.15837v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intrinsic Bias Metrics Do Not Correlate with Application Bias", "abstract": "Natural Language Processing (NLP) systems learn harmful societal biases that\ncause them to amplify inequality as they are deployed in more and more\nsituations. To guide efforts at debiasing these systems, the NLP community\nrelies on a variety of metrics that quantify bias in models. Some of these\nmetrics are intrinsic, measuring bias in word embedding spaces, and some are\nextrinsic, measuring bias in downstream tasks that the word embeddings enable.\nDo these intrinsic and extrinsic metrics correlate with each other? We compare\nintrinsic and extrinsic metrics across hundreds of trained models covering\ndifferent tasks and experimental conditions. Our results show no reliable\ncorrelation between these metrics that holds in all scenarios across tasks and\nlanguages. We urge researchers working on debiasing to focus on extrinsic\nmeasures of bias, and to make using these measures more feasible via creation\nof new challenge sets and annotated test data. To aid this effort, we release\ncode, a new intrinsic metric, and an annotated test set focused on gender bias\nin hate speech.", "published": "2020-12-31 18:59:44", "link": "http://arxiv.org/abs/2012.15859v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets", "abstract": "Heavily overparameterized language models such as BERT, XLNet and T5 have\nachieved impressive success in many NLP tasks. However, their high model\ncomplexity requires enormous computation resources and extremely long training\ntime for both pre-training and fine-tuning. Many works have studied model\ncompression on large NLP models, but only focusing on reducing inference time\nwhile still requiring an expensive training process. Other works use extremely\nlarge batch sizes to shorten the pre-training time, at the expense of higher\ncomputational resource demands. In this paper, inspired by the Early-Bird\nLottery Tickets recently studied for computer vision tasks, we propose\nEarlyBERT, a general computationally-efficient training algorithm applicable to\nboth pre-training and fine-tuning of large-scale language models. By slimming\nthe self-attention and fully-connected sub-layers inside a transformer, we are\nthe first to identify structured winning tickets in the early stage of BERT\ntraining. We apply those tickets towards efficient BERT training, and conduct\ncomprehensive pre-training and fine-tuning experiments on GLUE and SQuAD\ndownstream tasks. Our results show that EarlyBERT achieves comparable\nperformance to standard BERT, with 35~45% less training time. Code is available\nat https://github.com/VITA-Group/EarlyBERT.", "published": "2020-12-31 20:38:20", "link": "http://arxiv.org/abs/2101.00063v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Refine and Imitate: Reducing Repetition and Inconsistency in Persuasion\n  Dialogues via Reinforcement Learning and Human Demonstration", "abstract": "Persuasion dialogue systems reflect the machine's ability to make strategic\nmoves beyond verbal communication, and therefore differentiate themselves from\ntask-oriented or open-domain dialogue systems and have their own unique values.\nHowever, the repetition and inconsistency problems still persist in dialogue\nresponse generation and could substantially impact user experience and impede\nthe persuasion outcome. Besides, although reinforcement learning (RL)\napproaches have achieved big success in strategic tasks such as games, they\nrequire a sophisticated user simulator to provide real-time feedback to the\ndialogue system, which limits the application of RL on persuasion dialogues. To\naddress these issues towards a better persuasion dialogue system, we apply RL\nto refine a language model baseline without user simulators, and distill\nsentence-level information about repetition, inconsistency, and task relevance\nthrough rewards. Moreover, to better accomplish the persuasion task, the model\nlearns from human demonstration to imitate human persuasion behavior and\nselects the most persuasive responses. Experiments show that our model\noutperforms previous state-of-the-art dialogue models on both automatic metrics\nand human evaluation results on a donation persuasion task, and generates more\ndiverse, consistent and persuasive conversations according to the user\nfeedback.", "published": "2020-12-31 00:02:51", "link": "http://arxiv.org/abs/2012.15375v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unified Mandarin TTS Front-end Based on Distilled BERT Model", "abstract": "The front-end module in a typical Mandarin text-to-speech system (TTS) is\ncomposed of a long pipeline of text processing components, which requires\nextensive efforts to build and is prone to large accumulative model size and\ncascade errors. In this paper, a pre-trained language model (PLM) based model\nis proposed to simultaneously tackle the two most important tasks in TTS\nfront-end, i.e., prosodic structure prediction (PSP) and grapheme-to-phoneme\n(G2P) conversion. We use a pre-trained Chinese BERT[1] as the text encoder and\nemploy multi-task learning technique to adapt it to the two TTS front-end\ntasks. Then, the BERT encoder is distilled into a smaller model by employing a\nknowledge distillation technique called TinyBERT[2], making the whole model\nsize 25% of that of benchmark pipeline models while maintaining competitive\nperformance on both tasks. With the proposed the methods, we are able to run\nthe whole TTS front-end module in a light and unified manner, which is more\nfriendly to deployment on mobile devices.", "published": "2020-12-31 02:34:57", "link": "http://arxiv.org/abs/2012.15404v1", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
{"title": "An Experimental Evaluation of Transformer-based Language Models in the\n  Biomedical Domain", "abstract": "With the growing amount of text in health data, there have been rapid\nadvances in large pre-trained models that can be applied to a wide variety of\nbiomedical tasks with minimal task-specific modifications. Emphasizing the cost\nof these models, which renders technical replication challenging, this paper\nsummarizes experiments conducted in replicating BioBERT and further\npre-training and careful fine-tuning in the biomedical domain. We also\ninvestigate the effectiveness of domain-specific and domain-agnostic\npre-trained models across downstream biomedical NLP tasks. Our finding confirms\nthat pre-trained models can be impactful in some downstream NLP tasks (QA and\nNER) in the biomedical domain; however, this improvement may not justify the\nhigh cost of domain-specific pre-training.", "published": "2020-12-31 03:09:38", "link": "http://arxiv.org/abs/2012.15419v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Seeing is Knowing! Fact-based Visual Question Answering using Knowledge\n  Graph Embeddings", "abstract": "Fact-based Visual Question Answering (FVQA), a challenging variant of VQA,\nrequires a QA-system to include facts from a diverse knowledge graph (KG) in\nits reasoning process to produce an answer. Large KGs, especially common-sense\nKGs, are known to be incomplete, i.e., not all non-existent facts are always\nincorrect. Therefore, being able to reason over incomplete KGs for QA is a\ncritical requirement in real-world applications that has not been addressed\nextensively in the literature. We develop a novel QA architecture that allows\nus to reason over incomplete KGs, something current FVQA state-of-the-art\n(SOTA) approaches lack due to their critical reliance on fact retrieval. We use\nKG Embeddings, a technique widely used for KG completion, for the downstream\ntask of FVQA. We also employ a new image representation technique we call\n'Image-as-Knowledge' to enable this capability, alongside a simple one-step\nCoAttention mechanism to attend to text and image during QA. Our FVQA\narchitecture is faster during inference time, being O(m), as opposed to\nexisting FVQA SOTA methods which are O(N log N), where m = number of vertices,\nN = number of edges = O(m^2). KG embeddings are shown to hold complementary\ninformation to word embeddings: a combination of both metrics permits\nperformance comparable to SOTA methods in the standard answer retrieval task,\nand significantly better (26% absolute) in the proposed missing-edge reasoning\ntask.", "published": "2020-12-31 07:24:55", "link": "http://arxiv.org/abs/2012.15484v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Zero-Shot Knowledge Distillation for Natural Language Processing", "abstract": "Knowledge Distillation (KD) is a common knowledge transfer algorithm used for\nmodel compression across a variety of deep learning based natural language\nprocessing (NLP) solutions. In its regular manifestations, KD requires access\nto the teacher's training data for knowledge transfer to the student network.\nHowever, privacy concerns, data regulations and proprietary reasons may prevent\naccess to such data. We present, to the best of our knowledge, the first work\non Zero-Shot Knowledge Distillation for NLP, where the student learns from the\nmuch larger teacher without any task specific data. Our solution combines out\nof domain data and adversarial training to learn the teacher's output\ndistribution. We investigate six tasks from the GLUE benchmark and demonstrate\nthat we can achieve between 75% and 92% of the teacher's classification score\n(accuracy or F1) while compressing the model 30 times.", "published": "2020-12-31 08:16:29", "link": "http://arxiv.org/abs/2012.15495v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Continual Learning in Task-Oriented Dialogue Systems", "abstract": "Continual learning in task-oriented dialogue systems can allow us to add new\ndomains and functionalities through time without incurring the high cost of a\nwhole system retraining. In this paper, we propose a continual learning\nbenchmark for task-oriented dialogue systems with 37 domains to be learned\ncontinuously in four settings, such as intent recognition, state tracking,\nnatural language generation, and end-to-end. Moreover, we implement and compare\nmultiple existing continual learning baselines, and we propose a simple yet\neffective architectural method based on residual adapters. Our experiments\ndemonstrate that the proposed architectural method and a simple replay-based\nstrategy perform comparably well but they both achieve inferior performance to\nthe multi-task learning baseline, in where all the data are shown at once,\nshowing that continual learning in task-oriented dialogue systems is a\nchallenging task. Furthermore, we reveal several trade-offs between different\ncontinual learning methods in term of parameter usage and memory size, which\nare important in the design of a task-oriented dialogue system. The proposed\nbenchmark is released together with several baselines to promote more research\nin this direction.", "published": "2020-12-31 08:44:25", "link": "http://arxiv.org/abs/2012.15504v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Discovering Dialog Structure Graph for Open-Domain Dialog Generation", "abstract": "Learning interpretable dialog structure from human-human dialogs yields basic\ninsights into the structure of conversation, and also provides background\nknowledge to facilitate dialog generation. In this paper, we conduct\nunsupervised discovery of dialog structure from chitchat corpora, and then\nleverage it to facilitate dialog generation in downstream systems. To this end,\nwe present a Discrete Variational Auto-Encoder with Graph Neural Network\n(DVAE-GNN), to discover a unified human-readable dialog structure. The\nstructure is a two-layer directed graph that contains session-level semantics\nin the upper-layer vertices, utterance-level semantics in the lower-layer\nvertices, and edges among these semantic vertices. In particular, we integrate\nGNN into DVAE to fine-tune utterance-level semantics for more effective\nrecognition of session-level semantic vertex. Furthermore, to alleviate the\ndifficulty of discovering a large number of utterance-level semantics, we\ndesign a coupling mechanism that binds each utterance-level semantic vertex\nwith a distinct phrase to provide prior semantics. Experimental results on two\nbenchmark corpora confirm that DVAE-GNN can discover meaningful dialog\nstructure, and the use of dialog structure graph as background knowledge can\nfacilitate a graph grounded conversational system to conduct coherent\nmulti-turn dialog generation.", "published": "2020-12-31 10:58:37", "link": "http://arxiv.org/abs/2012.15543v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Making Pre-trained Language Models Better Few-shot Learners", "abstract": "The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot\nperformance solely by leveraging a natural-language prompt and a few task\ndemonstrations as input context. Inspired by their findings, we study few-shot\nlearning in a more practical scenario, where we use smaller language models for\nwhich fine-tuning is computationally efficient. We present LM-BFF--better\nfew-shot fine-tuning of language models--a suite of simple and complementary\ntechniques for fine-tuning language models on a small number of annotated\nexamples. Our approach includes (1) prompt-based fine-tuning together with a\nnovel pipeline for automating prompt generation; and (2) a refined strategy for\ndynamically and selectively incorporating demonstrations into each context.\nFinally, we present a systematic evaluation for analyzing few-shot performance\non a range of NLP tasks, including classification and regression. Our\nexperiments demonstrate that our methods combine to dramatically outperform\nstandard fine-tuning procedures in this low resource setting, achieving up to\n30% absolute improvement, and 11% on average across all tasks. Our approach\nmakes minimal assumptions on task resources and domain expertise, and hence\nconstitutes a strong task-agnostic method for few-shot learning.", "published": "2020-12-31 17:21:26", "link": "http://arxiv.org/abs/2012.15723v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Moral Stories: Situated Reasoning about Norms, Intents, Actions, and\n  their Consequences", "abstract": "In social settings, much of human behavior is governed by unspoken rules of\nconduct. For artificial systems to be fully integrated into social\nenvironments, adherence to such norms is a central prerequisite. We investigate\nwhether contemporary NLG models can function as behavioral priors for systems\ndeployed in social settings by generating action hypotheses that achieve\npredefined goals under moral constraints. Moreover, we examine if models can\nanticipate likely consequences of (im)moral actions, or explain why certain\nactions are preferable by generating relevant norms. For this purpose, we\nintroduce 'Moral Stories', a crowd-sourced dataset of structured, branching\nnarratives for the study of grounded, goal-oriented social reasoning. Finally,\nwe propose decoding strategies that effectively combine multiple expert models\nto significantly improve the quality of generated actions, consequences, and\nnorms compared to strong baselines, e.g. though abductive reasoning.", "published": "2020-12-31 17:28:01", "link": "http://arxiv.org/abs/2012.15738v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning from the Worst: Dynamically Generated Datasets to Improve\n  Online Hate Detection", "abstract": "We present a human-and-model-in-the-loop process for dynamically generating\ndatasets and training better performing and more robust hate detection models.\nWe provide a new dataset of ~40,000 entries, generated and labelled by trained\nannotators over four rounds of dynamic data creation. It includes ~15,000\nchallenging perturbations and each hateful entry has fine-grained labels for\nthe type and target of hate. Hateful entries make up 54% of the dataset, which\nis substantially higher than comparable datasets. We show that model\nperformance is substantially improved using this approach. Models trained on\nlater rounds of data collection perform better on test sets and are harder for\nannotators to trick. They also perform better on HateCheck, a suite of\nfunctional tests for online hate detection. We provide the code, dataset and\nannotation guidelines for other researchers to use. Accepted at ACL 2021.", "published": "2020-12-31 17:36:48", "link": "http://arxiv.org/abs/2012.15761v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evidence-based Factual Error Correction", "abstract": "This paper introduces the task of factual error correction: performing edits\nto a claim so that the generated rewrite is better supported by evidence. This\nextends the well-studied task of fact verification by providing a mechanism to\ncorrect written texts that are refuted or only partially supported by evidence.\nWe demonstrate that it is feasible to train factual error correction systems\nfrom existing fact checking datasets which only contain labeled claims\naccompanied by evidence, but not the correction. We achieve this by employing a\ntwo-stage distant supervision approach that incorporates evidence into masked\nclaims when generating corrections. Our approach, based on the T5 transformer\nand using retrieved evidence, achieved better results than existing work which\nused a pointer copy network and gold evidence, producing accurate factual error\ncorrections for 5x more instances in human evaluation and a .125 increase in\nSARI score. The evaluation is conducted on a dataset of 65,000 instances based\non a recent fact verification shared task and we release it to enable further\nwork on the task.", "published": "2020-12-31 18:11:26", "link": "http://arxiv.org/abs/2012.15788v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Studying Strategically: Learning to Mask for Closed-book QA", "abstract": "Closed-book question-answering (QA) is a challenging task that requires a\nmodel to directly answer questions without access to external knowledge. It has\nbeen shown that directly fine-tuning pre-trained language models with\n(question, answer) examples yields surprisingly competitive performance, which\nis further improved upon through adding an intermediate pre-training stage\nbetween general pre-training and fine-tuning. Prior work used a heuristic\nduring this intermediate stage, whereby named entities and dates are masked,\nand the model is trained to recover these tokens. In this paper, we aim to\nlearn the optimal masking strategy for the intermediate pre-training stage. We\nfirst train our masking policy to extract spans that are likely to be tested,\nusing supervision from the downstream task itself, then deploy the learned\npolicy during intermediate pre-training. Thus, our policy packs task-relevant\nknowledge into the parameters of a language model. Our approach is particularly\neffective on TriviaQA, outperforming strong heuristics when used to pre-train\nBART.", "published": "2020-12-31 18:59:08", "link": "http://arxiv.org/abs/2012.15856v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Directed Beam Search: Plug-and-Play Lexically Constrained Language\n  Generation", "abstract": "Large pre-trained language models are capable of generating realistic text.\nHowever, controlling these models so that the generated text satisfies lexical\nconstraints, i.e., contains specific words, is a challenging problem. Given\nthat state-of-the-art language models are too large to be trained from scratch\nin a manageable time, it is desirable to control these models without\nre-training them. Methods capable of doing this are called plug-and-play.\nRecent plug-and-play methods have been successful in constraining small\nbidirectional language models as well as forward models in tasks with a\nrestricted search space, e.g., machine translation. However, controlling large\ntransformer-based models to meet lexical constraints without re-training them\nremains a challenge. In this work, we propose Directed Beam Search (DBS), a\nplug-and-play method for lexically constrained language generation. Our method\ncan be applied to any language model, is easy to implement and can be used for\ngeneral language generation. In our experiments we use DBS to control GPT-2. We\ndemonstrate its performance on keyword-to-phrase generation and we obtain\ncomparable results as a state-of-the-art non-plug-and-play model for lexically\nconstrained story generation.", "published": "2020-12-31 03:05:44", "link": "http://arxiv.org/abs/2012.15416v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EfficientNet-Absolute Zero for Continuous Speech Keyword Spotting", "abstract": "Keyword spotting is a process of finding some specific words or phrases in\nrecorded speeches by computers. Deep neural network algorithms, as a powerful\nengine, can handle this problem if they are trained over an appropriate\ndataset. To this end, the football keyword dataset (FKD), as a new keyword\nspotting dataset in Persian, is collected with crowdsourcing. This dataset\ncontains nearly 31000 samples in 18 classes. The continuous speech synthesis\nmethod proposed to made FKD usable in the practical application which works\nwith continuous speeches. Besides, we proposed a lightweight architecture\ncalled EfficientNet-A0 (absolute zero) by applying the compound scaling method\non EfficientNet-B0 for keyword spotting task. Finally, the proposed\narchitecture is evaluated with various models. It is realized that\nEfficientNet-A0 and Resnet models outperform other models on this dataset.", "published": "2020-12-31 16:21:27", "link": "http://arxiv.org/abs/2012.15695v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Beyond Offline Mapping: Learning Cross Lingual Word Embeddings through\n  Context Anchoring", "abstract": "Recent research on cross-lingual word embeddings has been dominated by\nunsupervised mapping approaches that align monolingual embeddings. Such methods\ncritically rely on those embeddings having a similar structure, but it was\nrecently shown that the separate training in different languages causes\ndepartures from this assumption. In this paper, we propose an alternative\napproach that does not have this limitation, while requiring a weak seed\ndictionary (e.g., a list of identical words) as the only form of supervision.\nRather than aligning two fixed embedding spaces, our method works by fixing the\ntarget language embeddings, and learning a new set of embeddings for the source\nlanguage that are aligned with them. To that end, we use an extension of\nskip-gram that leverages translated context words as anchor points, and\nincorporates self-learning and iterative restarts to reduce the dependency on\nthe initial dictionary. Our approach outperforms conventional mapping methods\non bilingual lexicon induction, and obtains competitive results in the\ndownstream XNLI task.", "published": "2020-12-31 17:10:14", "link": "http://arxiv.org/abs/2012.15715v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FastIF: Scalable Influence Functions for Efficient Model Interpretation\n  and Debugging", "abstract": "Influence functions approximate the \"influences\" of training data-points for\ntest predictions and have a wide variety of applications. Despite the\npopularity, their computational cost does not scale well with model and\ntraining data size. We present FastIF, a set of simple modifications to\ninfluence functions that significantly improves their run-time. We use\nk-Nearest Neighbors (kNN) to narrow the search space down to a subset of good\ncandidate data points, identify the configurations that best balance the\nspeed-quality trade-off in estimating the inverse Hessian-vector product, and\nintroduce a fast parallel variant. Our proposed method achieves about 80X\nspeedup while being highly correlated with the original influence values. With\nthe availability of the fast influence functions, we demonstrate their\nusefulness in four applications. First, we examine whether influential\ndata-points can \"explain\" test time behavior using the framework of\nsimulatability. Second, we visualize the influence interactions between\ntraining and test data-points. Third, we show that we can correct model errors\nby additional fine-tuning on certain influential data-points, improving the\naccuracy of a trained MultiNLI model by 2.5% on the HANS dataset. Finally, we\nexperiment with a similar setup but fine-tuning on datapoints not seen during\ntraining, improving the model accuracy by 2.8% and 1.7% on HANS and ANLI\ndatasets respectively. Overall, our fast influence functions can be efficiently\napplied to large models and datasets, and our experiments demonstrate the\npotential of influence functions in model interpretation and correcting model\nerrors. Code is available at\nhttps://github.com/salesforce/fast-influence-functions", "published": "2020-12-31 18:02:34", "link": "http://arxiv.org/abs/2012.15781v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Language-Mediated, Object-Centric Representation Learning", "abstract": "We present Language-mediated, Object-centric Representation Learning (LORL),\na paradigm for learning disentangled, object-centric scene representations from\nvision and language. LORL builds upon recent advances in unsupervised object\ndiscovery and segmentation, notably MONet and Slot Attention. While these\nalgorithms learn an object-centric representation just by reconstructing the\ninput image, LORL enables them to further learn to associate the learned\nrepresentations to concepts, i.e., words for object categories, properties, and\nspatial relationships, from language input. These object-centric concepts\nderived from language facilitate the learning of object-centric\nrepresentations. LORL can be integrated with various unsupervised object\ndiscovery algorithms that are language-agnostic. Experiments show that the\nintegration of LORL consistently improves the performance of unsupervised\nobject discovery methods on two datasets via the help of language. We also show\nthat concepts learned by LORL, in conjunction with object discovery methods,\naid downstream tasks such as referring expression comprehension.", "published": "2020-12-31 18:36:07", "link": "http://arxiv.org/abs/2012.15814v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Text-Free Image-to-Speech Synthesis Using Learned Segmental Units", "abstract": "In this paper we present the first model for directly synthesizing fluent,\nnatural-sounding spoken audio captions for images that does not require natural\nlanguage text as an intermediate representation or source of supervision.\nInstead, we connect the image captioning module and the speech synthesis module\nwith a set of discrete, sub-word speech units that are discovered with a\nself-supervised visual grounding task. We conduct experiments on the Flickr8k\nspoken caption dataset in addition to a novel corpus of spoken audio captions\ncollected for the popular MSCOCO dataset, demonstrating that our generated\ncaptions also capture diverse visual semantics of the images they describe. We\ninvestigate several different intermediate speech representations, and\nempirically find that the representation must satisfy several important\nproperties to serve as drop-in replacements for text.", "published": "2020-12-31 05:28:38", "link": "http://arxiv.org/abs/2012.15454v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Psychoacoustic Calibration of Loss Functions for Efficient End-to-End\n  Neural Audio Coding", "abstract": "Conventional audio coding technologies commonly leverage human perception of\nsound, or psychoacoustics, to reduce the bitrate while preserving the\nperceptual quality of the decoded audio signals. For neural audio codecs,\nhowever, the objective nature of the loss function usually leads to suboptimal\nsound quality as well as high run-time complexity due to the large model size.\nIn this work, we present a psychoacoustic calibration scheme to re-define the\nloss functions of neural audio coding systems so that it can decode signals\nmore perceptually similar to the reference, yet with a much lower model\ncomplexity. The proposed loss function incorporates the global masking\nthreshold, allowing the reconstruction error that corresponds to inaudible\nartifacts. Experimental results show that the proposed model outperforms the\nbaseline neural codec twice as large and consuming 23.4% more bits per second.\nWith the proposed method, a lightweight neural codec, with only 0.9 million\nparameters, performs near-transparent audio coding comparable with the\ncommercial MPEG-1 Audio Layer III codec at 112 kbps.", "published": "2020-12-31 19:46:46", "link": "http://arxiv.org/abs/2101.00054v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generalized Operating Procedure for Deep Learning: an Unconstrained\n  Optimal Design Perspective", "abstract": "Deep learning (DL) has brought about remarkable breakthrough in processing\nimages, video and speech due to its efficacy in extracting highly abstract\nrepresentation and learning very complex functions. However, there is seldom\noperating procedure reported on how to make it for real use cases. In this\npaper, we intend to address this problem by presenting a generalized operating\nprocedure for DL from the perspective of unconstrained optimal design, which is\nmotivated by a simple intension to remove the barrier of using DL, especially\nfor those scientists or engineers who are new but eager to use it. Our proposed\nprocedure contains seven steps, which are project/problem statement, data\ncollection, architecture design, initialization of parameters, defining loss\nfunction, computing optimal parameters, and inference, respectively. Following\nthis procedure, we build a multi-stream end-to-end speaker verification system,\nin which the input speech utterance is processed by multiple parallel streams\nwithin different frequency range, so that the acoustic modeling can be more\nrobust resulting from the diversity of features. Trained with VoxCeleb dataset,\nour experimental results verify the effectiveness of our proposed operating\nprocedure, and also show that our multi-stream framework outperforms\nsingle-stream baseline with 20 % relative reduction in minimum decision cost\nfunction (minDCF).", "published": "2020-12-31 01:37:56", "link": "http://arxiv.org/abs/2012.15391v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
