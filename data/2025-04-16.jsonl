{"title": "Universal portfolios in continuous time: a model-free approach", "abstract": "We provide a simple and straightforward approach to a continuous-time version\nof Cover's universal portfolio strategies within the model-free context of\nF\\\"ollmer's pathwise It\\^o calculus. We establish the existence of the\nuniversal portfolio strategy and prove that its portfolio value process is the\naverage of all values of constant rebalanced strategies. This result relies on\na systematic comparison between two alternative descriptions of self-financing\ntrading strategies within pathwise It\\^o calculus. We moreover provide a\ncomparison result for the performance and the realized volatility and variance\nof constant rebalanced portfolio strategies.", "published": "2025-04-16 09:05:53", "link": "http://arxiv.org/abs/2504.11881v1", "categories": ["q-fin.MF", "q-fin.PM"], "primary_category": "q-fin.MF"}
{"title": "Benchmarking Audio Deepfake Detection Robustness in Real-world Communication Scenarios", "abstract": "Existing Audio Deepfake Detection (ADD) systems often struggle to generalise\neffectively due to the significantly degraded audio quality caused by audio\ncodec compression and channel transmission effects in real-world communication\nscenarios. To address this challenge, we developed a rigorous benchmark to\nevaluate ADD system performance under such scenarios. We introduced ADD-C, a\nnew test dataset to evaluate the robustness of ADD systems under diverse\ncommunication conditions, including different combinations of audio codecs for\ncompression and Packet Loss Rates (PLR). Benchmarking on three baseline ADD\nmodels with the ADD-C dataset demonstrated a significant decline in robustness\nunder such conditions. A novel data augmentation strategy was proposed to\nimprove the robustness of ADD systems. Experimental results demonstrated that\nthe proposed approach increases the performance of ADD systems significantly\nwith the proposed ADD-C dataset. Our benchmark can assist future efforts\ntowards building practical and robustly generalisable ADD systems.", "published": "2025-04-16 18:44:05", "link": "http://arxiv.org/abs/2504.12423v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "An accurate measurement of parametric array using a spurious sound filter topologically equivalent to a half-wavelength resonator", "abstract": "Parametric arrays (PA) offer exceptional directivity and compactness compared\nto conventional loudspeakers, facilitating various acoustic applications.\nHowever, accurate measurement of audio signals generated by PA remains\nchallenging due to spurious ultrasonic sounds arising from microphone\nnonlinearities. Existing filtering methods, including Helmholtz resonators,\nphononic crystals, polymer films, and grazing incidence techniques, exhibit\npractical constraints such as size limitations, fabrication complexity, or\ninsufficient attenuation. To address these issues, we propose and demonstrate a\nnovel acoustic filter based on the design of a half-wavelength resonator. The\ndeveloped filter exploits the nodal plane in acoustic pressure distribution,\neffectively minimizing microphone exposure to targeted ultrasonic frequencies.\nFabrication via stereolithography (SLA) 3D printing ensures high dimensional\naccuracy, which is crucial for high-frequency acoustic filters. Finite element\nmethod (FEM) simulations guided filter optimization for suppression frequencies\nat 40 kHz and 60 kHz, achieving high transmission loss (TL) around 60 dB.\nExperimental validations confirm the filter's superior performance in\nsignificantly reducing spurious acoustic signals, as reflected in frequency\nresponse, beam pattern, and propagation curve measurements. The proposed filter\nensures stable and precise acoustic characterization, independent of\nmeasurement distances and incidence angles. This new approach not only improves\nmeasurement accuracy but also enhances reliability and reproducibility in\nparametric array research and development.", "published": "2025-04-16 18:04:26", "link": "http://arxiv.org/abs/2504.12398v1", "categories": ["cs.SD", "eess.AS", "physics.app-ph"], "primary_category": "cs.SD"}
{"title": "Dysarthria Normalization via Local Lie Group Transformations for Robust ASR", "abstract": "We present a geometry-driven method for normalizing dysarthric speech using\nlocal Lie group transformations of spectrograms. Time, frequency, and amplitude\ndistortions are modeled as smooth, invertible deformations, parameterized by\nscalar fields and applied via exponential maps. A neural network is trained to\ninfer these fields from synthetic distortions of typical speech-without using\nany pathological data. At test time, the model applies an approximate inverse\nto real dysarthric inputs. Despite zero-shot generalization, we observe\nsubstantial ASR gains, including up to 16 percentage points WER reduction on\nchallenging TORGO samples, with no degradation on clean speech. This work\nintroduces a principled, interpretable approach for robust speech recognition\nunder motor speech disorders", "published": "2025-04-16 17:41:19", "link": "http://arxiv.org/abs/2504.12279v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Voice Conversion with Diverse Intonation using Conditional Variational Auto-Encoder", "abstract": "Voice conversion is a task of synthesizing an utterance with target speaker's\nvoice while maintaining linguistic information of the source utterance. While a\nspeaker can produce varying utterances from a single script with different\nintonations, conventional voice conversion models were limited to producing\nonly one result per source input. To overcome this limitation, we propose a\nnovel approach for voice conversion with diverse intonations using conditional\nvariational autoencoder (CVAE). Experiments have shown that the speaker's style\nfeature can be mapped into a latent space with Gaussian distribution. We have\nalso been able to convert voices with more diverse intonation by making the\nposterior of the latent space more complex with inverse autoregressive flow\n(IAF). As a result, the converted voice not only has a diversity of\nintonations, but also has better sound quality than the model without CVAE.", "published": "2025-04-16 11:59:56", "link": "http://arxiv.org/abs/2504.12005v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
