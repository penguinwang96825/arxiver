{"title": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context Language Models", "abstract": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency.", "published": "2025-04-16 23:15:09", "link": "http://arxiv.org/abs/2504.12526v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Memorization vs. Reasoning: Updating LLMs with New Knowledge", "abstract": "Large language models (LLMs) encode vast amounts of pre-trained knowledge in\ntheir parameters, but updating them as real-world information evolves remains a\nchallenge. Existing methodologies and benchmarks primarily target entity\nsubstitutions, failing to capture the full breadth of complex real-world\ndynamics. In this paper, we introduce Knowledge Update Playground (KUP), an\nautomatic pipeline for simulating realistic knowledge updates reflected in an\nevidence corpora. KUP's evaluation framework includes direct and indirect\nprobes to both test memorization of updated facts and reasoning over them, for\nany update learning methods. Next, we present a lightweight method called\nmemory conditioned training (MCT), which conditions tokens in the update corpus\non self-generated \"memory\" tokens during training. Our strategy encourages LLMs\nto surface and reason over newly memorized knowledge at inference. Our results\non two strong LLMs show that (1) KUP benchmark is highly challenging, with the\nbest CPT models achieving $<2\\%$ in indirect probing setting (reasoning) and\n(2) MCT training significantly outperforms prior continued pre-training (CPT)\nbaselines, improving direct probing (memorization) results by up to $25.4\\%$.", "published": "2025-04-16 23:03:40", "link": "http://arxiv.org/abs/2504.12523v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating the Diversity and Quality of LLM Generated Content", "abstract": "Recent work suggests that preference-tuning techniques--including\nReinforcement Learning from Human Preferences (RLHF) methods like PPO and GRPO,\nas well as alternatives like DPO--reduce diversity, creating a dilemma given\nthat such models are widely deployed in applications requiring diverse outputs.\nTo address this, we introduce a framework for measuring effective semantic\ndiversity--diversity among outputs that meet quality thresholds--which better\nreflects the practical utility of large language models (LLMs). Using\nopen-ended tasks that require no human intervention, we find counterintuitive\nresults: although preference-tuned models--especially those trained via\nRL--exhibit reduced lexical and syntactic diversity, they produce greater\neffective semantic diversity than SFT or base models, not from increasing\ndiversity among high-quality outputs, but from generating more high-quality\noutputs overall. We discover that preference tuning reduces syntactic diversity\nwhile preserving semantic diversity--revealing a distinction between diversity\nin form and diversity in content that traditional metrics often overlook. Our\nanalysis further shows that smaller models are consistently more\nparameter-efficient at generating unique content within a fixed sampling\nbudget, offering insights into the relationship between model scaling and\ndiversity. These findings have important implications for applications that\nrequire diverse yet high-quality outputs, from creative assistance to synthetic\ndata generation.", "published": "2025-04-16 23:02:23", "link": "http://arxiv.org/abs/2504.12522v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents", "abstract": "We present BrowseComp, a simple yet challenging benchmark for measuring the\nability for agents to browse the web. BrowseComp comprises 1,266 questions that\nrequire persistently navigating the internet in search of hard-to-find,\nentangled information. Despite the difficulty of the questions, BrowseComp is\nsimple and easy-to-use, as predicted answers are short and easily verifiable\nagainst reference answers. BrowseComp for browsing agents can be seen as\nanalogous to how programming competitions are an incomplete but useful\nbenchmark for coding agents. While BrowseComp sidesteps challenges of a true\nuser query distribution, like generating long answers or resolving ambiguity,\nit measures the important core capability of exercising persistence and\ncreativity in finding information. BrowseComp can be found at\nhttps://github.com/openai/simple-evals.", "published": "2025-04-16 22:27:45", "link": "http://arxiv.org/abs/2504.12516v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Text: Characterizing Domain Expert Needs in Document Research", "abstract": "Working with documents is a key part of almost any knowledge work, from\ncontextualizing research in a literature review to reviewing legal precedent.\nRecently, as their capabilities have expanded, primarily text-based NLP systems\nhave often been billed as able to assist or even automate this kind of work.\nBut to what extent are these systems able to model these tasks as experts\nconceptualize and perform them now? In this study, we interview sixteen domain\nexperts across two domains to understand their processes of document research,\nand compare it to the current state of NLP systems. We find that our\nparticipants processes are idiosyncratic, iterative, and rely extensively on\nthe social context of a document in addition its content; existing approaches\nin NLP and adjacent fields that explicitly center the document as an object,\nrather than as merely a container for text, tend to better reflect our\nparticipants' priorities, though they are often less accessible outside their\nresearch communities. We call on the NLP community to more carefully consider\nthe role of the document in building useful tools that are accessible,\npersonalizable, iterative, and socially aware.", "published": "2025-04-16 21:24:41", "link": "http://arxiv.org/abs/2504.12495v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Accelerating Clinical NLP at Scale with a Hybrid Framework with Reduced GPU Demands: A Case Study in Dementia Identification", "abstract": "Clinical natural language processing (NLP) is increasingly in demand in both\nclinical research and operational practice. However, most of the\nstate-of-the-art solutions are transformers-based and require high\ncomputational resources, limiting their accessibility. We propose a hybrid NLP\nframework that integrates rule-based filtering, a Support Vector Machine (SVM)\nclassifier, and a BERT-based model to improve efficiency while maintaining\naccuracy. We applied this framework in a dementia identification case study\ninvolving 4.9 million veterans with incident hypertension, analyzing 2.1\nbillion clinical notes. At the patient level, our method achieved a precision\nof 0.90, a recall of 0.84, and an F1-score of 0.87. Additionally, this NLP\napproach identified over three times as many dementia cases as structured data\nmethods. All processing was completed in approximately two weeks using a single\nmachine with dual A40 GPUs. This study demonstrates the feasibility of hybrid\nNLP solutions for large-scale clinical text analysis, making state-of-the-art\nmethods more accessible to healthcare organizations with limited computational\nresources.", "published": "2025-04-16 21:24:38", "link": "http://arxiv.org/abs/2504.12494v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Pre-training Indicators Reliably Predict Fine-tuning Outcomes of LLMs?", "abstract": "While metrics available during pre-training, such as perplexity, correlate\nwell with model performance at scaling-laws studies, their predictive\ncapacities at a fixed model size remain unclear, hindering effective model\nselection and development. To address this gap, we formulate the task of\nselecting pre-training checkpoints to maximize downstream fine-tuning\nperformance as a pairwise classification problem: predicting which of two LLMs,\ndiffering in their pre-training, will perform better after supervised\nfine-tuning (SFT). We construct a dataset using 50 1B parameter LLM variants\nwith systematically varied pre-training configurations, e.g., objectives or\ndata, and evaluate them on diverse downstream tasks after SFT. We first conduct\na study and demonstrate that the conventional perplexity is a misleading\nindicator. As such, we introduce novel unsupervised and supervised proxy\nmetrics derived from pre-training that successfully reduce the relative\nperformance prediction error rate by over 50%. Despite the inherent complexity\nof this task, we demonstrate the practical utility of our proposed proxies in\nspecific scenarios, paving the way for more efficient design of pre-training\nschemes optimized for various downstream tasks.", "published": "2025-04-16 21:19:09", "link": "http://arxiv.org/abs/2504.12491v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Conversational AI for Human-Machine Collaborative MLOps", "abstract": "This paper presents a Large Language Model (LLM) based conversational agent\nsystem designed to enhance human-machine collaboration in Machine Learning\nOperations (MLOps). We introduce the Swarm Agent, an extensible architecture\nthat integrates specialized agents to create and manage ML workflows through\nnatural language interactions. The system leverages a hierarchical, modular\ndesign incorporating a KubeFlow Pipelines (KFP) Agent for ML pipeline\norchestration, a MinIO Agent for data management, and a Retrieval-Augmented\nGeneration (RAG) Agent for domain-specific knowledge integration. Through\niterative reasoning loops and context-aware processing, the system enables\nusers with varying technical backgrounds to discover, execute, and monitor ML\npipelines; manage datasets and artifacts; and access relevant documentation,\nall via intuitive conversational interfaces. Our approach addresses the\naccessibility gap in complex MLOps platforms like Kubeflow, making advanced ML\ntools broadly accessible while maintaining the flexibility to extend to other\nplatforms. The paper describes the architecture, implementation details, and\ndemonstrates how this conversational MLOps assistant reduces complexity and\nlowers barriers to entry for users across diverse technical skill levels.", "published": "2025-04-16 20:28:50", "link": "http://arxiv.org/abs/2504.12477v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "68T50, 68T99, 68U35, 68N19", "I.2.1; H.5.2; D.2.11; I.2.7"], "primary_category": "cs.AI"}
{"title": "Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex", "abstract": "Text-attributed graphs (TAGs) present unique challenges in representation\nlearning by requiring models to capture both the semantic richness of\nnode-associated texts and the structural dependencies of the graph. While graph\nneural networks (GNNs) excel at modeling topological information, they lack the\ncapacity to process unstructured text. Conversely, large language models (LLMs)\nare proficient in text understanding but are typically unaware of graph\nstructure. In this work, we propose BiGTex (Bidirectional Graph Text), a novel\narchitecture that tightly integrates GNNs and LLMs through stacked Graph-Text\nFusion Units. Each unit allows for mutual attention between textual and\nstructural representations, enabling information to flow in both directions,\ntext influencing structure and structure guiding textual interpretation. The\nproposed architecture is trained using parameter-efficient fine-tuning (LoRA),\nkeeping the LLM frozen while adapting to task-specific signals. Extensive\nexperiments on five benchmark datasets demonstrate that BiGTex achieves\nstate-of-the-art performance in node classification and generalizes effectively\nto link prediction. An ablation study further highlights the importance of soft\nprompting and bi-directional attention in the model's success.", "published": "2025-04-16 20:25:11", "link": "http://arxiv.org/abs/2504.12474v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SLURG: Investigating the Feasibility of Generating Synthetic Online Fallacious Discourse", "abstract": "In our paper we explore the definition, and extrapolation of fallacies as\nthey pertain to the automatic detection of manipulation on social media. In\nparticular we explore how these logical fallacies might appear in the real\nworld i.e internet forums. We discovered a prevalence of misinformation /\nmisguided intention in discussion boards specifically centered around the\nUkrainian Russian Conflict which serves to narrow the domain of our task.\nAlthough automatic fallacy detection has gained attention recently, most\ndatasets use unregulated fallacy taxonomies or are limited to formal linguistic\ndomains like political debates or news reports. Online discourse, however,\noften features non-standardized and diverse language not captured in these\ndomains. We present Shady Linguistic Utterance Replication-Generation (SLURG)\nto address these limitations, exploring the feasibility of generating synthetic\nfallacious forum-style comments using large language models (LLMs),\nspecifically DeepHermes-3-Mistral-24B. Our findings indicate that LLMs can\nreplicate the syntactic patterns of real data} and that high-quality few-shot\nprompts enhance LLMs' ability to mimic the vocabulary diversity of online\nforums.", "published": "2025-04-16 20:03:47", "link": "http://arxiv.org/abs/2504.12466v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Linear Representations and Pretraining Data Frequency in Language Models", "abstract": "Pretraining data has a direct impact on the behaviors and quality of language\nmodels (LMs), but we only understand the most basic principles of this\nrelationship. While most work focuses on pretraining data's effect on\ndownstream task behavior, we investigate its relationship to LM\nrepresentations. Previous work has discovered that, in language models, some\nconcepts are encoded `linearly' in the representations, but what factors cause\nthese representations to form? We study the connection between pretraining data\nfrequency and models' linear representations of factual relations. We find\nevidence that the formation of linear representations is strongly connected to\npretraining term frequencies; specifically for subject-relation-object fact\ntriplets, both subject-object co-occurrence frequency and in-context learning\naccuracy for the relation are highly correlated with linear representations.\nThis is the case across all phases of pretraining. In OLMo-7B and GPT-J, we\ndiscover that a linear representation consistently (but not exclusively) forms\nwhen the subjects and objects within a relation co-occur at least 1k and 2k\ntimes, respectively, regardless of when these occurrences happen during\npretraining. Finally, we train a regression model on measurements of linear\nrepresentation quality in fully-trained LMs that can predict how often a term\nwas seen in pretraining. Our model achieves low error even on inputs from a\ndifferent model with a different pretraining dataset, providing a new method\nfor estimating properties of the otherwise-unknown training data of closed-data\nmodels. We conclude that the strength of linear representations in LMs contains\nsignal about the models' pretraining corpora that may provide new avenues for\ncontrolling and improving model behavior: particularly, manipulating the\nmodels' training data to meet specific frequency thresholds.", "published": "2025-04-16 19:50:03", "link": "http://arxiv.org/abs/2504.12459v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Position: The Most Expensive Part of an LLM should be its Training Data", "abstract": "Training a state-of-the-art Large Language Model (LLM) is an increasingly\nexpensive endeavor due to growing computational, hardware, energy, and\nengineering demands. Yet, an often-overlooked (and seldom paid) expense is the\nhuman labor behind these models' training data. Every LLM is built on an\nunfathomable amount of human effort: trillions of carefully written words\nsourced from books, academic papers, codebases, social media, and more. This\nposition paper aims to assign a monetary value to this labor and argues that\nthe most expensive part of producing an LLM should be the compensation provided\nto training data producers for their work. To support this position, we study\n64 LLMs released between 2016 and 2024, estimating what it would cost to pay\npeople to produce their training datasets from scratch. Even under highly\nconservative estimates of wage rates, the costs of these models' training\ndatasets are 10-1000 times larger than the costs to train the models\nthemselves, representing a significant financial liability for LLM providers.\nIn the face of the massive gap between the value of training data and the lack\nof compensation for its creation, we highlight and discuss research directions\nthat could enable fairer practices in the future.", "published": "2025-04-16 18:56:14", "link": "http://arxiv.org/abs/2504.12427v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Human-AI Comparative Analysis of Prompt Sensitivity in LLM-Based Relevance Judgment", "abstract": "Large Language Models (LLMs) are increasingly used to automate relevance\njudgments for information retrieval (IR) tasks, often demonstrating agreement\nwith human labels that approaches inter-human agreement. To assess the\nrobustness and reliability of LLM-based relevance judgments, we systematically\ninvestigate impact of prompt sensitivity on the task. We collected prompts for\nrelevance assessment from 15 human experts and 15 LLMs across three tasks~ --\n~binary, graded, and pairwise~ -- ~yielding 90 prompts in total. After\nfiltering out unusable prompts from three humans and three LLMs, we employed\nthe remaining 72 prompts with three different LLMs as judges to label\ndocument/query pairs from two TREC Deep Learning Datasets (2020 and 2021). We\ncompare LLM-generated labels with TREC official human labels using Cohen's\n$\\kappa$ and pairwise agreement measures. In addition to investigating the\nimpact of prompt variations on agreement with human labels, we compare human-\nand LLM-generated prompts and analyze differences among different LLMs as\njudges. We also compare human- and LLM-generated prompts with the standard\nUMBRELA prompt used for relevance assessment by Bing and TREC 2024 Retrieval\nAugmented Generation (RAG) Track. To support future research in LLM-based\nevaluation, we release all data and prompts at\nhttps://github.com/Narabzad/prompt-sensitivity-relevance-judgements/.", "published": "2025-04-16 18:17:19", "link": "http://arxiv.org/abs/2504.12408v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "BitNet b1.58 2B4T Technical Report", "abstract": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures.", "published": "2025-04-16 17:51:43", "link": "http://arxiv.org/abs/2504.12285v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dysarthria Normalization via Local Lie Group Transformations for Robust ASR", "abstract": "We present a geometry-driven method for normalizing dysarthric speech using\nlocal Lie group transformations of spectrograms. Time, frequency, and amplitude\ndistortions are modeled as smooth, invertible deformations, parameterized by\nscalar fields and applied via exponential maps. A neural network is trained to\ninfer these fields from synthetic distortions of typical speech-without using\nany pathological data. At test time, the model applies an approximate inverse\nto real dysarthric inputs. Despite zero-shot generalization, we observe\nsubstantial ASR gains, including up to 16 percentage points WER reduction on\nchallenging TORGO samples, with no degradation on clean speech. This work\nintroduces a principled, interpretable approach for robust speech recognition\nunder motor speech disorders", "published": "2025-04-16 17:41:19", "link": "http://arxiv.org/abs/2504.12279v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Advancing Arabic Speech Recognition Through Large-Scale Weakly Supervised Learning", "abstract": "Automatic speech recognition (ASR) is crucial for human-machine interaction\nin diverse applications like conversational agents, industrial robotics, call\ncenter automation, and automated subtitling. However, developing\nhigh-performance ASR models remains challenging, particularly for low-resource\nlanguages like Arabic, due to the scarcity of large, labeled speech datasets,\nwhich are costly and labor-intensive to produce. In this work, we employ weakly\nsupervised learning to train an Arabic ASR model using the Conformer\narchitecture. Our model is trained from scratch on 15,000 hours of weakly\nannotated speech data covering both Modern Standard Arabic (MSA) and Dialectal\nArabic (DA), eliminating the need for costly manual transcriptions. Despite the\nabsence of human-verified labels, our approach attains state-of-the-art (SOTA)\nperformance, exceeding all previous efforts in the field of Arabic ASR on the\nstandard benchmarks. By demonstrating the effectiveness of weak supervision as\na scalable, cost-efficient alternative to traditional supervised approaches,\npaving the way for improved ASR systems in low resource settings.", "published": "2025-04-16 17:05:14", "link": "http://arxiv.org/abs/2504.12254v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Watermarking Needs Input Repetition Masking", "abstract": "Recent advancements in Large Language Models (LLMs) raised concerns over\npotential misuse, such as for spreading misinformation. In response two counter\nmeasures emerged: machine learning-based detectors that predict if text is\nsynthetic, and LLM watermarking, which subtly marks generated text for\nidentification and attribution. Meanwhile, humans are known to adjust language\nto their conversational partners both syntactically and lexically. By\nimplication, it is possible that humans or unwatermarked LLMs could\nunintentionally mimic properties of LLM generated text, making counter measures\nunreliable. In this work we investigate the extent to which such conversational\nadaptation happens. We call the concept $\\textit{mimicry}$ and demonstrate that\nboth humans and LLMs end up mimicking, including the watermarking signal even\nin seemingly improbable settings. This challenges current academic assumptions\nand suggests that for long-term watermarking to be reliable, the likelihood of\nfalse positives needs to be significantly lower, while longer word sequences\nshould be used for seeding watermarking mechanisms.", "published": "2025-04-16 16:25:26", "link": "http://arxiv.org/abs/2504.12229v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning", "abstract": "Recent large language models (LLMs) have demonstrated strong reasoning\ncapabilities that benefits from online reinforcement learning (RL). These\ncapabilities have primarily been demonstrated within the left-to-right\nautoregressive (AR) generation paradigm. In contrast, non-autoregressive\nparadigms based on diffusion generate text in a coarse-to-fine manner. Although\nrecent diffusion-based large language models (dLLMs) have achieved competitive\nlanguage modeling performance compared to their AR counterparts, it remains\nunclear if dLLMs can also leverage recent advances in LLM reasoning. To this\nend, we propose d1, a framework to adapt pre-trained masked dLLMs into\nreasoning models via a combination of supervised finetuning (SFT) and RL.\nSpecifically, we develop and extend techniques to improve reasoning in\npretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge\nand instill self-improvement behavior directly from existing datasets, and (b)\nwe introduce a novel critic-free, policy-gradient based RL algorithm called\ndiffu-GRPO. Through empirical studies, we investigate the performance of\ndifferent post-training recipes on multiple mathematical and logical reasoning\nbenchmarks. We find that d1 yields the best performance and significantly\nimproves performance of a state-of-the-art dLLM.", "published": "2025-04-16 16:08:45", "link": "http://arxiv.org/abs/2504.12216v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure", "abstract": "It is sometimes assumed that Large Language Models (LLMs) know language, or\nfor example that they know that Paris is the capital of France. But what -- if\nanything -- do LLMs actually know? In this paper, I argue that LLMs can acquire\ntacit knowledge as defined by Martin Davies (1990). Whereas Davies himself\ndenies that neural networks can acquire tacit knowledge, I demonstrate that\ncertain architectural features of LLMs satisfy the constraints of semantic\ndescription, syntactic structure, and causal systematicity. Thus, tacit\nknowledge may serve as a conceptual framework for describing, explaining, and\nintervening on LLMs and their behavior.", "published": "2025-04-16 15:42:33", "link": "http://arxiv.org/abs/2504.12187v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SALAD: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and LLM-Driven Augmented Data", "abstract": "In various natural language processing (NLP) tasks, fine-tuning Pre-trained\nLanguage Models (PLMs) often leads to the issue of spurious correlations, which\nnegatively impacts performance, particularly when dealing with\nout-of-distribution data. To address this problem, we propose SALAD}(Structure\nAware and LLM-driven Augmented Data), a novel approach designed to enhance\nmodel robustness and generalization by generating structure-aware and\ncounterfactually augmented data for contrastive learning. Our method leverages\na tagging-based approach to generate structure-aware positive samples and\nutilizes large language models (LLMs) to generate counterfactual negative\nsamples with diverse sentence patterns. By applying contrastive learning, SALAD\nenables the model to focus on learning the structural relationships between key\nsentence components while minimizing reliance on spurious correlations. We\nvalidate our approach through experiments on three tasks: Sentiment\nClassification, Sexism Detection, and Natural Language Inference. The results\ndemonstrate that SALAD not only improves model robustness and performance\nacross different environments but also enhances generalization to\nout-of-distribution datasets and cross-domain scenarios.", "published": "2025-04-16 15:40:10", "link": "http://arxiv.org/abs/2504.12185v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Trusting CHATGPT: how minor tweaks in the prompts lead to major differences in sentiment classification", "abstract": "One fundamental question for the social sciences today is: how much can we\ntrust highly complex predictive models like ChatGPT? This study tests the\nhypothesis that subtle changes in the structure of prompts do not produce\nsignificant variations in the classification results of sentiment polarity\nanalysis generated by the Large Language Model GPT-4o mini. Using a dataset of\n100.000 comments in Spanish on four Latin American presidents, the model\nclassified the comments as positive, negative, or neutral on 10 occasions,\nvarying the prompts slightly each time. The experimental methodology included\nexploratory and confirmatory analyses to identify significant discrepancies\namong classifications.\n  The results reveal that even minor modifications to prompts such as lexical,\nsyntactic, or modal changes, or even their lack of structure impact the\nclassifications. In certain cases, the model produced inconsistent responses,\nsuch as mixing categories, providing unsolicited explanations, or using\nlanguages other than Spanish. Statistical analysis using Chi-square tests\nconfirmed significant differences in most comparisons between prompts, except\nin one case where linguistic structures were highly similar.\n  These findings challenge the robustness and trust of Large Language Models\nfor classification tasks, highlighting their vulnerability to variations in\ninstructions. Moreover, it was evident that the lack of structured grammar in\nprompts increases the frequency of hallucinations. The discussion underscores\nthat trust in Large Language Models is based not only on technical performance\nbut also on the social and institutional relationships underpinning their use.", "published": "2025-04-16 15:37:09", "link": "http://arxiv.org/abs/2504.12180v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mapping Controversies Using Artificial Intelligence: An Analysis of the Hamas-Israel Conflict on YouTube", "abstract": "This article analyzes the Hamas-Israel controversy through 253,925\nSpanish-language YouTube comments posted between October 2023 and January 2024,\nfollowing the October 7 attack that escalated the conflict. Adopting an\ninterdisciplinary approach, the study combines the analysis of controversies\nfrom Science and Technology Studies (STS) with advanced computational\nmethodologies, specifically Natural Language Processing (NLP) using the BERT\n(Bidirectional Encoder Representations from Transformers) model. Using this\napproach, the comments were automatically classified into seven categories,\nreflecting pro-Palestinian, pro-Israeli, anti- Palestinian, anti-Israeli\npositions, among others. The results show a predominance of pro- Palestinian\ncomments, although pro-Israeli and anti-Palestinian comments received more\n\"likes.\" This study also applies the agenda-setting theory to demonstrate how\nmedia coverage significantly influences public perception, observing a notable\nshift in public opinion, transitioning from a pro- Palestinian stance to a more\ncritical position towards Israel. This work highlights the importance of\ncombining social science perspectives with technological tools in the analysis\nof controversies, presenting a methodological innovation by integrating\ncomputational analysis with critical social theories to address complex public\nopinion phenomena and media narratives.", "published": "2025-04-16 15:27:57", "link": "http://arxiv.org/abs/2504.12177v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Poem Meter Classification of Recited Arabic Poetry: Integrating High-Resource Systems for a Low-Resource Task", "abstract": "Arabic poetry is an essential and integral part of Arabic language and\nculture. It has been used by the Arabs to spot lights on their major events\nsuch as depicting brutal battles and conflicts. They also used it, as in many\nother languages, for various purposes such as romance, pride, lamentation, etc.\nArabic poetry has received major attention from linguistics over the decades.\nOne of the main characteristics of Arabic poetry is its special rhythmic\nstructure as opposed to prose. This structure is referred to as a meter.\nMeters, along with other poetic characteristics, are intensively studied in an\nArabic linguistic field called \"\\textit{Aroud}\". Identifying these meters for a\nverse is a lengthy and complicated process. It also requires technical\nknowledge in \\textit{Aruod}. For recited poetry, it adds an extra layer of\nprocessing. Developing systems for automatic identification of poem meters for\nrecited poems need large amounts of labelled data. In this study, we propose a\nstate-of-the-art framework to identify the poem meters of recited Arabic\npoetry, where we integrate two separate high-resource systems to perform the\nlow-resource task. To ensure generalization of our proposed architecture, we\npublish a benchmark for this task for future research.", "published": "2025-04-16 15:25:45", "link": "http://arxiv.org/abs/2504.12172v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multilingual Contextualization of Large Language Models for Document-Level Machine Translation", "abstract": "Large language models (LLMs) have demonstrated strong performance in\nsentence-level machine translation, but scaling to document-level translation\nremains challenging, particularly in modeling long-range dependencies and\ndiscourse phenomena across sentences and paragraphs. In this work, we propose a\nmethod to improve LLM-based long-document translation through targeted\nfine-tuning on high-quality document-level data, which we curate and introduce\nas DocBlocks. Our approach supports multiple translation paradigms, including\ndirect document-to-document and chunk-level translation, by integrating\ninstructions both with and without surrounding context. This enables models to\nbetter capture cross-sentence dependencies while maintaining strong\nsentence-level translation performance. Experimental results show that\nincorporating multiple translation paradigms improves document-level\ntranslation quality and inference speed compared to prompting and agent-based\nmethods.", "published": "2025-04-16 14:52:22", "link": "http://arxiv.org/abs/2504.12140v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Contrastive Decoding with Probabilistic Hallucination Detection - Mitigating Hallucinations in Large Vision Language Models -", "abstract": "Despite recent advances in Large Vision Language Models (LVLMs), these models\nstill suffer from generating hallucinatory responses that do not align with the\nvisual input provided. To mitigate such hallucinations, we introduce Efficient\nContrastive Decoding (ECD), a simple method that leverages probabilistic\nhallucination detection to shift the output distribution towards contextually\naccurate answers at inference time. By contrasting token probabilities and\nhallucination scores, ECD subtracts hallucinated concepts from the original\ndistribution, effectively suppressing hallucinations. Notably, our proposed\nmethod can be applied to any open-source LVLM and does not require additional\nLVLM training. We evaluate our method on several benchmark datasets and across\ndifferent LVLMs. Our experiments show that ECD effectively mitigates\nhallucinations, outperforming state-of-the-art methods with respect to\nperformance on LVLM benchmarks and computation time.", "published": "2025-04-16 14:50:25", "link": "http://arxiv.org/abs/2504.12137v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust and Traceable Text Generation", "abstract": "The rapid development of Large Language Models (LLMs) has intensified\nconcerns about content traceability and potential misuse. Existing watermarking\nschemes for sampled text often face trade-offs between maintaining text quality\nand ensuring robust detection against various attacks. To address these issues,\nwe propose a novel watermarking scheme that improves both detectability and\ntext quality by introducing a cumulative watermark entropy threshold. Our\napproach is compatible with and generalizes existing sampling functions,\nenhancing adaptability. Experimental results across multiple LLMs show that our\nscheme significantly outperforms existing methods, achieving over 80\\%\nimprovements on widely-used datasets, e.g., MATH and GSM8K, while maintaining\nhigh detection accuracy.", "published": "2025-04-16 14:16:38", "link": "http://arxiv.org/abs/2504.12108v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gauging Overprecision in LLMs: An Empirical Study", "abstract": "Recently, overconfidence in large language models (LLMs) has garnered\nconsiderable attention due to its fundamental importance in quantifying the\ntrustworthiness of LLM generation. However, existing approaches prompt the\n\\textit{black box LLMs} to produce their confidence (\\textit{verbalized\nconfidence}), which can be subject to many biases and hallucinations. Inspired\nby a different aspect of overconfidence in cognitive science called\n\\textit{overprecision}, we designed a framework for its study in black box\nLLMs. This framework contains three main phases: 1) generation, 2) refinement\nand 3) evaluation. In the generation phase we prompt the LLM to generate\nanswers to numerical questions in the form of intervals with a certain level of\nconfidence. This confidence level is imposed in the prompt and not required for\nthe LLM to generate as in previous approaches. We use various prompting\ntechniques and use the same prompt multiple times to gauge the effects of\nrandomness in the generation process. In the refinement phase, answers from the\nprevious phase are refined to generate better answers. The LLM answers are\nevaluated and studied in the evaluation phase to understand its internal\nworkings. This study allowed us to gain various insights into LLM\noverprecision: 1) LLMs are highly uncalibrated for numerical tasks 2)\n{\\color{blue}there is no correlation between the length of the interval and the\nimposed confidence level, which can be symptomatic of a a) lack of\nunderstanding of the concept of confidence or b) inability to adjust\nself-confidence by following instructions}, {\\color{blue}3)} LLM numerical\nprecision differs depending on the task, scale of answer and prompting\ntechnique {\\color{blue}4) Refinement of answers doesn't improve precision in\nmost cases}. We believe this study offers new perspectives on LLM\noverconfidence and serves as a strong baseline for overprecision in LLMs.", "published": "2025-04-16 14:02:21", "link": "http://arxiv.org/abs/2504.12098v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Selective Demonstration Retrieval for Improved Implicit Hate Speech Detection", "abstract": "Hate speech detection is a crucial area of research in natural language\nprocessing, essential for ensuring online community safety. However, detecting\nimplicit hate speech, where harmful intent is conveyed in subtle or indirect\nways, remains a major challenge. Unlike explicit hate speech, implicit\nexpressions often depend on context, cultural subtleties, and hidden biases,\nmaking them more challenging to identify consistently. Additionally, the\ninterpretation of such speech is influenced by external knowledge and\ndemographic biases, resulting in varied detection results across different\nlanguage models. Furthermore, Large Language Models often show heightened\nsensitivity to toxic language and references to vulnerable groups, which can\nlead to misclassifications. This over-sensitivity results in false positives\n(incorrectly identifying harmless statements as hateful) and false negatives\n(failing to detect genuinely harmful content). Addressing these issues requires\nmethods that not only improve detection precision but also reduce model biases\nand enhance robustness. To address these challenges, we propose a novel method,\nwhich utilizes in-context learning without requiring model fine-tuning. By\nadaptively retrieving demonstrations that focus on similar groups or those with\nthe highest similarity scores, our approach enhances contextual comprehension.\nExperimental results show that our method outperforms current state-of-the-art\ntechniques. Implementation details and code are available at TBD.", "published": "2025-04-16 13:43:23", "link": "http://arxiv.org/abs/2504.12082v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bayesian dynamic borrowing considering semantic similarity between outcomes for disproportionality analysis in FAERS", "abstract": "We present a Bayesian dynamic borrowing (BDB) approach to enhance the\nquantitative identification of adverse events (AEs) in spontaneous reporting\nsystems (SRSs). The method embeds a robust meta-analytic predictive (MAP) prior\nwithin a Bayesian hierarchical model and incorporates semantic similarity\nmeasures (SSMs) to enable weighted information sharing from MedDRA Preferred\nTerms (PTs) that are clinically similar to the target PT. This continuous\nsimilarity-based borrowing addresses limitation of rigid hierarchical grouping\nin current disproportionality analysis (DPA).\n  Using data from the FDA Adverse Event Reporting System (FAERS) between 2015\nand 2019, we evaluate this approach - termed IC SSM - against standard\nInformation Component (IC) analysis and IC with borrowing at the MedDRA\nhigh-level group term (HLGT) level. A novel references set (PVLens), derived\nfrom FDA product label updates, enabled prospective evaluation of method\nperformance in identifying AEs prior to official labeling.\n  The IC SSM approach demonstrated improved sensitivity compared to both\ntraditional IC and HLGT-based borrowing, with minor trade-offs in F1 scores and\nYouden's index. IC SSM consistently identified more true positives and detected\nsignals over 5 months sooner than traditional IC. Despite a marginally lower\naggregate Youden's index, IC SSM showed higher performance in the early\npost-marketing period, providing more stable and relevant estimates than\nHLGT-based borrowing and traditional IC.\n  These findings support the use of SSM-informed Bayesian borrowing as a\nscalable and context-aware enhancement to traditional DPA methods. Future\nresearch should validate this approach across other datasets and explore\nadditional similarity metrics and Bayesian inference strategies using\ncase-level data.", "published": "2025-04-16 13:06:24", "link": "http://arxiv.org/abs/2504.12052v2", "categories": ["cs.CL", "I.2.4; G.3; H.3.3"], "primary_category": "cs.CL"}
{"title": "Language Models as Quasi-Crystalline Thought: Structure, Constraint, and Emergence in Generative Systems", "abstract": "This essay proposes an analogy between large language models (LLMs) and\nquasicrystals: systems that exhibit global coherence without periodic\nrepetition and that are generated through local constraints. While LLMs are\noften evaluated in terms of predictive accuracy, factuality, or alignment, this\nstructural perspective suggests that their most characteristic behavior is the\nproduction of internally resonant linguistic patterns. Just as quasicrystals\nforced a redefinition of order in physical systems, viewing LLMs as generators\nof quasi-structured language opens new paths for evaluation and design:\nprivileging propagation of constraint over token-level accuracy, and coherence\nof form over fixed meaning. LLM outputs should be read not only for what they\nsay, but for the patterns of constraint and coherence that organize them. This\nshift reframes generative language as a space of emergent patterning: LLMs are\nneither fully random nor strictly rule-based, but defined by a logic of\nconstraint, resonance, and structural depth.", "published": "2025-04-16 11:27:47", "link": "http://arxiv.org/abs/2504.11986v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on Hallucinations and Related Observable Overgeneration Mistakes", "abstract": "We present the Mu-SHROOM shared task which is focused on detecting\nhallucinations and other overgeneration mistakes in the output of\ninstruction-tuned large language models (LLMs). Mu-SHROOM addresses\ngeneral-purpose LLMs in 14 languages, and frames the hallucination detection\nproblem as a span-labeling task. We received 2,618 submissions from 43\nparticipating teams employing diverse methodologies. The large number of\nsubmissions underscores the interest of the community in hallucination\ndetection. We present the results of the participating systems and conduct an\nempirical analysis to identify key factors contributing to strong performance\nin this task. We also emphasize relevant current challenges, notably the\nvarying degree of hallucinations across languages and the high annotator\ndisagreement when labeling hallucination spans.", "published": "2025-04-16 11:15:26", "link": "http://arxiv.org/abs/2504.11975v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA", "abstract": "Extractive reading comprehension question answering (QA) datasets are\ntypically evaluated using Exact Match (EM) and F1-score, but these metrics\noften fail to fully capture model performance. With the success of large\nlanguage models (LLMs), they have been employed in various tasks, including\nserving as judges (LLM-as-a-judge). In this paper, we reassess the performance\nof QA models using LLM-as-a-judge across four reading comprehension QA\ndatasets. We examine different families of LLMs and various answer types to\nevaluate the effectiveness of LLM-as-a-judge in these tasks. Our results show\nthat LLM-as-a-judge is highly correlated with human judgments and can replace\ntraditional EM/F1 metrics. By using LLM-as-a-judge, the correlation with human\njudgments improves significantly, from 0.17 (EM) and 0.36 (F1-score) to 0.85.\nThese findings confirm that EM and F1 metrics underestimate the true\nperformance of the QA models. While LLM-as-a-judge is not perfect for more\ndifficult answer types (e.g., job), it still outperforms EM/F1, and we observe\nno bias issues, such as self-preference, when the same model is used for both\nthe QA and judgment tasks.", "published": "2025-04-16 11:08:46", "link": "http://arxiv.org/abs/2504.11972v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust and Fine-Grained Detection of AI Generated Texts", "abstract": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts.", "published": "2025-04-16 10:29:30", "link": "http://arxiv.org/abs/2504.11952v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ADAT: Time-Series-Aware Adaptive Transformer Architecture for Sign Language Translation", "abstract": "Current sign language machine translation systems rely on recognizing hand\nmovements, facial expressions and body postures, and natural language\nprocessing, to convert signs into text. Recent approaches use Transformer\narchitectures to model long-range dependencies via positional encoding.\nHowever, they lack accuracy in recognizing fine-grained, short-range temporal\ndependencies between gestures captured at high frame rates. Moreover, their\nhigh computational complexity leads to inefficient training. To mitigate these\nissues, we propose an Adaptive Transformer (ADAT), which incorporates\ncomponents for enhanced feature extraction and adaptive feature weighting\nthrough a gating mechanism to emphasize contextually relevant features while\nreducing training overhead and maintaining translation accuracy. To evaluate\nADAT, we introduce MedASL, the first public medical American Sign Language\ndataset. In sign-to-gloss-to-text experiments, ADAT outperforms the\nencoder-decoder transformer, improving BLEU-4 accuracy by 0.1% while reducing\ntraining time by 14.33% on PHOENIX14T and 3.24% on MedASL. In sign-to-text\nexperiments, it improves accuracy by 8.7% and reduces training time by 2.8% on\nPHOENIX14T and achieves 4.7% higher accuracy and 7.17% faster training on\nMedASL. Compared to encoder-only and decoder-only baselines in sign-to-text,\nADAT is at least 6.8% more accurate despite being up to 12.1% slower due to its\ndual-stream structure.", "published": "2025-04-16 10:20:11", "link": "http://arxiv.org/abs/2504.11942v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "I.2.6; I.2.7; I.2.10; I.4.8; I.4.9; I.4.10"], "primary_category": "cs.AI"}
{"title": "An LLM-as-a-judge Approach for Scalable Gender-Neutral Translation Evaluation", "abstract": "Gender-neutral translation (GNT) aims to avoid expressing the gender of human\nreferents when the source text lacks explicit cues about the gender of those\nreferents. Evaluating GNT automatically is particularly challenging, with\ncurrent solutions being limited to monolingual classifiers. Such solutions are\nnot ideal because they do not factor in the source sentence and require\ndedicated data and fine-tuning to scale to new languages. In this work, we\naddress such limitations by investigating the use of large language models\n(LLMs) as evaluators of GNT. Specifically, we explore two prompting approaches:\none in which LLMs generate sentence-level assessments only, and another, akin\nto a chain-of-thought approach, where they first produce detailed phrase-level\nannotations before a sentence-level judgment. Through extensive experiments on\nmultiple languages with five models, both open and proprietary, we show that\nLLMs can serve as evaluators of GNT. Moreover, we find that prompting for\nphrase-level annotations before sentence-level assessments consistently\nimproves the accuracy of all models, providing a better and more scalable\nalternative to current solutions.", "published": "2025-04-16 10:14:27", "link": "http://arxiv.org/abs/2504.11934v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models via Plot Hole Detection", "abstract": "Stories are a fundamental aspect of human experience. Engaging deeply with\nstories and spotting plot holes -- inconsistencies in a storyline that break\nthe internal logic or rules of a story's world -- requires nuanced reasoning\nskills, including tracking entities and events and their interplay, abstract\nthinking, pragmatic narrative understanding, commonsense and social reasoning,\nand theory of mind. As Large Language Models (LLMs) increasingly generate,\ninterpret, and modify text, rigorously assessing their narrative consistency\nand deeper language understanding becomes critical. However, existing\nbenchmarks focus mainly on surface-level comprehension. In this work, we\npropose plot hole detection in stories as a proxy to evaluate language\nunderstanding and reasoning in LLMs. We introduce FlawedFictionsMaker, a novel\nalgorithm to controllably and carefully synthesize plot holes in human-written\nstories. Using this algorithm, we construct a benchmark to evaluate LLMs' plot\nhole detection abilities in stories -- FlawedFictions -- , which is robust to\ncontamination, with human filtering ensuring high quality. We find that\nstate-of-the-art LLMs struggle in accurately solving FlawedFictions regardless\nof the reasoning effort allowed, with performance significantly degrading as\nstory length increases. Finally, we show that LLM-based story summarization and\nstory generation are prone to introducing plot holes, with more than 50% and\n100% increases in plot hole detection rates with respect to human-written\noriginals.", "published": "2025-04-16 09:25:54", "link": "http://arxiv.org/abs/2504.11900v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking LLM-Based Recommendations: A Query Generation-Based, Training-Free Approach", "abstract": "Existing large language model LLM-based recommendation methods face several\nchallenges, including inefficiency in handling large candidate pools,\nsensitivity to item order within prompts (\"lost in the middle\" phenomenon) poor\nscalability, and unrealistic evaluation due to random negative sampling. To\naddress these issues, we propose a Query-to-Recommendation approach that\nleverages LLMs to generate personalized queries for retrieving relevant items\nfrom the entire candidate pool, eliminating the need for candidate\npre-selection. This method can be integrated into an ID-based recommendation\nsystem without additional training, enhances recommendation performance and\ndiversity through LLMs' world knowledge, and performs well even for less\npopular item groups. Experiments on three datasets show up to 57 percent\nimprovement, with an average gain of 31 percent, demonstrating strong zero-shot\nperformance and further gains when ensembled with existing models.", "published": "2025-04-16 09:17:45", "link": "http://arxiv.org/abs/2504.11889v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Evaluating the Goal-Directedness of Large Language Models", "abstract": "To what extent do LLMs use their capabilities towards their given goal? We\ntake this as a measure of their goal-directedness. We evaluate\ngoal-directedness on tasks that require information gathering, cognitive\neffort, and plan execution, where we use subtasks to infer each model's\nrelevant capabilities. Our evaluations of LLMs from Google DeepMind, OpenAI,\nand Anthropic show that goal-directedness is relatively consistent across\ntasks, differs from task performance, and is only moderately sensitive to\nmotivational prompts. Notably, most models are not fully goal-directed. We hope\nour goal-directedness evaluations will enable better monitoring of LLM\nprogress, and enable more deliberate design choices of agentic properties in\nLLMs.", "published": "2025-04-16 08:07:08", "link": "http://arxiv.org/abs/2504.11844v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "FiSMiness: A Finite State Machine Based Paradigm for Emotional Support Conversations", "abstract": "Emotional support conversation (ESC) aims to alleviate the emotional distress\nof individuals through effective conversations. Although large language models\n(LLMs) have obtained remarkable progress on ESC, most of these studies might\nnot define the diagram from the state model perspective, therefore providing a\nsuboptimal solution for long-term satisfaction. To address such an issue, we\nleverage the Finite State Machine (FSM) on LLMs, and propose a framework called\nFiSMiness. Our framework allows a single LLM to bootstrap the planning during\nESC, and self-reason the seeker's emotion, support strategy and the final\nresponse upon each conversational turn. Substantial experiments on ESC datasets\nsuggest that FiSMiness outperforms many baselines, including direct inference,\nself-refine, chain of thought, finetuning, and external-assisted methods, even\nthose with many more parameters.", "published": "2025-04-16 07:52:06", "link": "http://arxiv.org/abs/2504.11837v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Could Thinking Multilingually Empower LLM Reasoning?", "abstract": "Previous work indicates that large language models exhibit a significant\n\"English bias\", i.e. they often perform better when tasks are presented in\nEnglish. Interestingly, we have observed that using certain other languages in\nreasoning tasks can yield better performance than English. However, this\nphenomenon remains under-explored. In this paper, we explore the upper bound of\nharnessing multilingualism in reasoning tasks, suggesting that multilingual\nreasoning promises significantly (by nearly 10 Acc@$k$ points) and robustly\n(tolerance for variations in translation quality and language choice) higher\nupper bounds than English-only reasoning. Besides analyzing the reason behind\nthe upper bound and challenges in reaching it, we also find that common answer\nselection methods cannot achieve this upper bound, due to their limitations and\nbiases. These insights could pave the way for future research aimed at fully\nharnessing the potential of multilingual reasoning in LLMs.", "published": "2025-04-16 07:45:10", "link": "http://arxiv.org/abs/2504.11833v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decision-based AI Visual Navigation for Cardiac Ultrasounds", "abstract": "Ultrasound imaging of the heart (echocardiography) is widely used to diagnose\ncardiac diseases. However, obtaining an echocardiogram requires an expert\nsonographer and a high-quality ultrasound imaging device, which are generally\nonly available in hospitals. Recently, AI-based navigation models and\nalgorithms have been used to aid novice sonographers in acquiring the\nstandardized cardiac views necessary to visualize potential disease\npathologies. These navigation systems typically rely on directional guidance to\npredict the necessary rotation of the ultrasound probe. This paper demonstrates\na novel AI navigation system that builds on a decision model for identifying\nthe inferior vena cava (IVC) of the heart. The decision model is trained\noffline using cardiac ultrasound videos and employs binary classification to\ndetermine whether the IVC is present in a given ultrasound video. The\nunderlying model integrates a novel localization algorithm that leverages the\nlearned feature representations to annotate the spatial location of the IVC in\nreal-time. Our model demonstrates strong localization performance on\ntraditional high-quality hospital ultrasound videos, as well as impressive\nzero-shot performance on lower-quality ultrasound videos from a more affordable\nButterfly iQ handheld ultrasound machine. This capability facilitates the\nexpansion of ultrasound diagnostics beyond hospital settings. Currently, the\nguidance system is undergoing clinical trials and is available on the Butterfly\niQ app.", "published": "2025-04-16 23:54:46", "link": "http://arxiv.org/abs/2504.12535v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Generalization through variance: how noise shapes inductive biases in diffusion models", "abstract": "How diffusion models generalize beyond their training set is not known, and\nis somewhat mysterious given two facts: the optimum of the denoising score\nmatching (DSM) objective usually used to train diffusion models is the score\nfunction of the training distribution; and the networks usually used to learn\nthe score function are expressive enough to learn this score to high accuracy.\nWe claim that a certain feature of the DSM objective -- the fact that its\ntarget is not the training distribution's score, but a noisy quantity only\nequal to it in expectation -- strongly impacts whether and to what extent\ndiffusion models generalize. In this paper, we develop a mathematical theory\nthat partly explains this 'generalization through variance' phenomenon. Our\ntheoretical analysis exploits a physics-inspired path integral approach to\ncompute the distributions typically learned by a few paradigmatic under- and\noverparameterized diffusion models. We find that the distributions diffusion\nmodels effectively learn to sample from resemble their training distributions,\nbut with 'gaps' filled in, and that this inductive bias is due to the\ncovariance structure of the noisy target used during training. We also\ncharacterize how this inductive bias interacts with feature-related inductive\nbiases.", "published": "2025-04-16 23:41:10", "link": "http://arxiv.org/abs/2504.12532v1", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Is Trust Correlated With Explainability in AI? A Meta-Analysis", "abstract": "This study critically examines the commonly held assumption that\nexplicability in artificial intelligence (AI) systems inherently boosts user\ntrust. Utilizing a meta-analytical approach, we conducted a comprehensive\nexamination of the existing literature to explore the relationship between AI\nexplainability and trust. Our analysis, incorporating data from 90 studies,\nreveals a statistically significant but moderate positive correlation between\nthe explainability of AI systems and the trust they engender among users. This\nindicates that while explainability contributes to building trust, it is not\nthe sole or predominant factor in this equation. In addition to academic\ncontributions to the field of Explainable AI (XAI), this research highlights\nits broader socio-technical implications, particularly in promoting\naccountability and fostering user trust in critical domains such as healthcare\nand justice. By addressing challenges like algorithmic bias and ethical\ntransparency, the study underscores the need for equitable and sustainable AI\nadoption. Rather than focusing solely on immediate trust, we emphasize the\nnormative importance of fostering authentic and enduring trustworthiness in AI\nsystems.", "published": "2025-04-16 23:30:55", "link": "http://arxiv.org/abs/2504.12529v1", "categories": ["cs.AI", "cs.CY"], "primary_category": "cs.AI"}
{"title": "AdaVid: Adaptive Video-Language Pretraining", "abstract": "Contrastive video-language pretraining has demonstrated great success in\nlearning rich and robust video representations. However, deploying such video\nencoders on compute-constrained edge devices remains challenging due to their\nhigh computational demands. Additionally, existing models are typically trained\nto process only short video clips, often limited to 4 to 64 frames. In this\npaper, we introduce AdaVid, a flexible architectural framework designed to\nlearn efficient video encoders that can dynamically adapt their computational\nfootprint based on available resources. At the heart of AdaVid is an adaptive\ntransformer block, inspired by Matryoshka Representation Learning, which allows\nthe model to adjust its hidden embedding dimension at inference time. We show\nthat AdaVid-EgoVLP, trained on video-narration pairs from the large-scale Ego4D\ndataset, matches the performance of the standard EgoVLP on short video-language\nbenchmarks using only half the compute, and even outperforms EgoVLP when given\nequal computational resources. We further explore the trade-off between frame\ncount and compute on the challenging Diving48 classification benchmark, showing\nthat AdaVid enables the use of more frames without exceeding computational\nlimits. To handle longer videos, we also propose a lightweight hierarchical\nnetwork that aggregates short clip features, achieving a strong balance between\ncompute efficiency and accuracy across several long video benchmarks.", "published": "2025-04-16 22:19:50", "link": "http://arxiv.org/abs/2504.12513v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Multimodal LLM Augmented Reasoning for Interpretable Visual Perception Analysis", "abstract": "In this paper, we advance the study of AI-augmented reasoning in the context\nof Human-Computer Interaction (HCI), psychology and cognitive science, focusing\non the critical task of visual perception. Specifically, we investigate the\napplicability of Multimodal Large Language Models (MLLMs) in this domain. To\nthis end, we leverage established principles and explanations from psychology\nand cognitive science related to complexity in human visual perception. We use\nthem as guiding principles for the MLLMs to compare and interprete visual\ncontent. Our study aims to benchmark MLLMs across various explainability\nprinciples relevant to visual perception. Unlike recent approaches that\nprimarily employ advanced deep learning models to predict complexity metrics\nfrom visual content, our work does not seek to develop a mere new predictive\nmodel. Instead, we propose a novel annotation-free analytical framework to\nassess utility of MLLMs as cognitive assistants for HCI tasks, using visual\nperception as a case study. The primary goal is to pave the way for principled\nstudy in quantifying and evaluating the interpretability of MLLMs for\napplications in improving human reasoning capability and uncovering biases in\nexisting perception datasets annotated by humans.", "published": "2025-04-16 22:14:27", "link": "http://arxiv.org/abs/2504.12511v1", "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Continual Learning Strategies for 3D Engineering Regression Problems: A Benchmarking Study", "abstract": "Engineering problems that apply machine learning often involve\ncomputationally intensive methods but rely on limited datasets. As engineering\ndata evolves with new designs and constraints, models must incorporate new\nknowledge over time. However, high computational costs make retraining models\nfrom scratch infeasible. Continual learning (CL) offers a promising solution by\nenabling models to learn from sequential data while mitigating catastrophic\nforgetting, where a model forgets previously learned mappings. This work\nintroduces CL to engineering design by benchmarking several CL methods on\nrepresentative regression tasks. We apply these strategies to five engineering\ndatasets and construct nine new engineering CL benchmarks to evaluate their\nability to address forgetting and improve generalization. Preliminary results\nshow that applying existing CL methods to these tasks improves performance over\nnaive baselines. In particular, the Replay strategy achieved performance\ncomparable to retraining in several benchmarks while reducing training time by\nnearly half, demonstrating its potential for real-world engineering workflows.\nThe code and datasets used in this work will be available at:\nhttps://github.com/kmsamuel/cl-for-engineering-release.", "published": "2025-04-16 21:40:03", "link": "http://arxiv.org/abs/2504.12503v1", "categories": ["cs.LG", "cs.AI", "cs.CE"], "primary_category": "cs.LG"}
{"title": "Heuristic Recognition and Rapid Response to Unfamiliar Events Outside of Agent Design Scope", "abstract": "Regardless of past learning, an agent in an open world will face unfamiliar\nsituations and events outside of prior experience, existing models, or\npolicies. Further, the agent will sometimes lack relevant knowledge and/or\nsufficient time to assess the situation, generate and evaluate options, and\npursue a robustly considered course of action. How can an agent respond\nreasonably to situations that are outside of its original design scope? How can\nit recognize such situations sufficiently quickly and reliably to determine\nreasonable, adaptive courses of action? We identify key characteristics needed\nfor solutions, evaluate the state-of-the-art by these requirements, and outline\na proposed, novel approach that combines domain-general meta-knowledge (in the\nform of appraisals inspired by human cognition) and metareasoning. It has the\npotential to provide fast, adaptive responses to unfamiliar situations, more\nfully meeting the performance characteristics required for open-world, general\nagents.", "published": "2025-04-16 21:26:12", "link": "http://arxiv.org/abs/2504.12497v1", "categories": ["cs.AI", "I.2.8"], "primary_category": "cs.AI"}
{"title": "Co-Writing with AI, on Human Terms: Aligning Research with User Demands Across the Writing Process", "abstract": "As generative AI tools like ChatGPT become integral to everyday writing,\ncritical questions arise about how to preserve writers' sense of agency and\nownership when using these tools. Yet, a systematic understanding of how AI\nassistance affects different aspects of the writing process - and how this\nshapes writers' agency - remains underexplored. To address this gap, we\nconducted a systematic review of 109 HCI papers using the PRISMA approach. From\nthis literature, we identify four overarching design strategies for AI writing\nsupport: structured guidance, guided exploration, active co-writing, and\ncritical feedback - mapped across the four key cognitive processes in writing:\nplanning, translating, reviewing, and monitoring. We complement this analysis\nwith interviews of 15 writers across diverse domains. Our findings reveal that\nwriters' desired levels of AI intervention vary across the writing process:\ncontent-focused writers (e.g., academics) prioritize ownership during planning,\nwhile form-focused writers (e.g., creatives) value control over translating and\nreviewing. Writers' preferences are also shaped by contextual goals, values,\nand notions of originality and authorship. By examining when ownership matters,\nwhat writers want to own, and how AI interactions shape agency, we surface both\nalignment and gaps between research and user needs. Our findings offer\nactionable design guidance for developing human-centered writing tools for\nco-writing with AI, on human terms.", "published": "2025-04-16 21:05:46", "link": "http://arxiv.org/abs/2504.12488v1", "categories": ["cs.HC", "cs.AI", "H.5.2; I.2.7; I.2.6; I.7.2"], "primary_category": "cs.HC"}
{"title": "Agentic AI Optimisation (AAIO): what it is, how it works, why it matters, and how to deal with it", "abstract": "The emergence of Agentic Artificial Intelligence (AAI) systems capable of\nindependently initiating digital interactions necessitates a new optimisation\nparadigm designed explicitly for seamless agent-platform interactions. This\narticle introduces Agentic AI Optimisation (AAIO) as an essential methodology\nfor ensuring effective integration between websites and agentic AI systems.\nLike how Search Engine Optimisation (SEO) has shaped digital content\ndiscoverability, AAIO can define interactions between autonomous AI agents and\nonline platforms. By examining the mutual interdependency between website\noptimisation and agentic AI success, the article highlights the virtuous cycle\nthat AAIO can create. It further explores the governance, ethical, legal, and\nsocial implications (GELSI) of AAIO, emphasising the necessity of proactive\nregulatory frameworks to mitigate potential negative impacts. The article\nconcludes by affirming AAIO's essential role as part of a fundamental digital\ninfrastructure in the era of autonomous digital agents, advocating for\nequitable and inclusive access to its benefits.", "published": "2025-04-16 20:38:09", "link": "http://arxiv.org/abs/2504.12482v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "What do people expect from Artificial Intelligence? Public opinion on alignment in AI moderation from Germany and the United States", "abstract": "Recent advances in generative Artificial Intelligence have raised public\nawareness, shaping expectations and concerns about their societal implications.\nCentral to these debates is the question of AI alignment -- how well AI systems\nmeet public expectations regarding safety, fairness, and social values.\nHowever, little is known about what people expect from AI-enabled systems and\nhow these expectations differ across national contexts. We present evidence\nfrom two surveys of public preferences for key functional features of\nAI-enabled systems in Germany (n = 1800) and the United States (n = 1756). We\nexamine support for four types of alignment in AI moderation: accuracy and\nreliability, safety, bias mitigation, and the promotion of aspirational\nimaginaries. U.S. respondents report significantly higher AI use and\nconsistently greater support for all alignment features, reflecting broader\ntechnological openness and higher societal involvement with AI. In both\ncountries, accuracy and safety enjoy the strongest support, while more\nnormatively charged goals -- like fairness and aspirational imaginaries --\nreceive more cautious backing, particularly in Germany. We also explore how\nindividual experience with AI, attitudes toward free speech, political\nideology, partisan affiliation, and gender shape these preferences. AI use and\nfree speech support explain more variation in Germany. In contrast, U.S.\nresponses show greater attitudinal uniformity, suggesting that higher exposure\nto AI may consolidate public expectations. These findings contribute to debates\non AI governance and cross-national variation in public preferences. More\nbroadly, our study demonstrates the value of empirically grounding AI alignment\ndebates in public attitudes and of explicitly developing normatively grounded\nexpectations into theoretical and policy discussions on the governance of\nAI-generated content.", "published": "2025-04-16 20:27:03", "link": "http://arxiv.org/abs/2504.12476v1", "categories": ["cs.CY", "cs.AI"], "primary_category": "cs.CY"}
{"title": "Dense Backpropagation Improves Training for Sparse Mixture-of-Experts", "abstract": "Mixture of Experts (MoE) pretraining is more scalable than dense Transformer\npretraining, because MoEs learn to route inputs to a sparse set of their\nfeedforward parameters. However, this means that MoEs only receive a sparse\nbackward update, leading to training instability and suboptimal performance. We\npresent a lightweight approximation method that gives the MoE router a dense\ngradient update while continuing to sparsely activate its parameters. Our\nmethod, which we refer to as Default MoE, substitutes missing expert\nactivations with default outputs consisting of an exponential moving average of\nexpert outputs previously seen over the course of training. This allows the\nrouter to receive signals from every expert for each token, leading to\nsignificant improvements in training performance. Our Default MoE outperforms\nstandard TopK routing in a variety of settings without requiring significant\ncomputational overhead. Code: https://github.com/vatsal0/default-moe.", "published": "2025-04-16 19:55:36", "link": "http://arxiv.org/abs/2504.12463v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Set families: restricted distances via restricted intersections", "abstract": "Denote by $f_D(n)$ the maximum size of a set family $\\mathcal{F}$ on $[n] =\n\\{1, \\dots, n\\}$ with distance set $D$. That is, $|A \\bigtriangleup B| \\in D$\nholds for every pair of distinct sets $A, B \\in \\mathcal{F}$. Kleitman's\ncelebrated discrete isodiametric inequality states that $f_D(n)$ is maximized\nat Hamming balls of radius $d/2$ when $D = \\{1, \\dots, d\\}$. We study the\ngeneralization where $D$ is a set of arithmetic progression and determine\n$f_D(n)$ asymptotically for all homogeneous $D$. In the special case when $D$\nis an interval, our result confirms a conjecture of Huang, Klurman, and\nPohoata. Moreover, we demonstrate a dichotomy in the growth of $f_D(n)$,\nshowing linear growth in $n$ when $D$ is a non-homogeneous arithmetic\nprogression. Different from previous combinatorial and spectral approaches, we\ndeduce our results by converting the restricted distance problems to restricted\nintersection problems.\n  Our proof ideas can be adapted to prove upper bounds on $t$-distance sets in\nHamming cubes (also known as binary $t$-codes), which has been extensively\nstudied by algebraic combinatorialists community, improving previous bounds\nfrom polynomial methods and optimization approaches.", "published": "2025-04-16 17:56:56", "link": "http://arxiv.org/abs/2504.12296v1", "categories": ["math.CO", "cs.DM", "05D05 (primary), 94B25, 05C35, 52C10"], "primary_category": "math.CO"}
{"title": "The Gray graph is pseudo 2-factor isomorphic", "abstract": "A graph is pseudo 2-factor isomorphic if all of its 2-factors have the same\nparity of number of cycles. Abreu et al. [J. Comb. Theory, Ser. B. 98 (2008)\n432--442] conjectured that $K_{3,3}$, the Heawood graph and the Pappus graph\nare the only essentially 4-edge-connected pseudo 2-factor isomorphic cubic\nbipartite graphs. This conjecture was disproved by Goedgebeur [Discr. Appl.\nMath. 193 (2015) 57--60] who constructed a counterexample $\\mathcal{G}$ (of\ngirth 6) on 30 vertices. Using a computer search, he also showed that this is\nthe only counterexample up to at least 40 vertices and that there are no\ncounterexamples of girth greater than 6 up to at least 48 vertices.\n  In this manuscript, we show that the Gray graph -- which has 54 vertices and\ngirth 8 -- is also a counterexample to the pseudo 2-factor isomorphic graph\nconjecture. Next to the graph $\\mathcal{G}$, this is the only other known\ncounterexample. Using a computer search, we show that there are no smaller\ncounterexamples of girth 8 and show that there are no other counterexamples up\nto at least 42 vertices of any girth. Moreover, we also verified that there are\nno further counterexamples among the known censuses of symmetrical graphs.\n  Recall that a graph is 2-factor Hamiltonian if all of its 2-factors are\nHamiltonian cycles. As a by-product of the computer searches performed for this\npaper, we have verified that the $2$-factor Hamiltonian conjecture of Funk et\nal. [J. Comb. Theory, Ser. B. 87(1) (2003) 138--144], which is still open,\nholds for cubic bipartite graphs of girth at least 8 up to 52 vertices, and up\nto 42 vertices for any girth.", "published": "2025-04-16 13:59:43", "link": "http://arxiv.org/abs/2504.12095v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "On the Regularity of Random 2-SAT and 3-SAT", "abstract": "We consider the random $k$-SAT problem with $n$ variables, $m=m(n)$ clauses,\nand clause density $\\alpha=\\lim_{n\\to\\infty}m/n$ for $k=2,3$. It is known that\nif $\\alpha$ is small enough, then the random $k$-SAT problem admits a solution\nwith high probability, which we interpret as the problem being\nunder-constrained. In this paper, we quantify exactly how under-constrained the\nrandom $k$-SAT problems are by determining their degrees of freedom, which we\ndefine as the threshold for the number of variables we can fix to an arbitrary\nvalue before the problem no longer is solvable with high probability. We show\nthat the random $2$-SAT and $3$-SAT problems have $n/m^{1/2}$ and $n/m^{1/3}$\ndegrees of freedom, respectively. Our main result is an explicit computation of\nthe corresponding threshold functions. Our result shows that the threshold\nfunction for the random $2$-SAT problem is regular, while it is non-regular for\nthe random $3$-SAT problem. By regular, we mean continuous and analytic on the\ninterior of its support. This result shows that the random $3$-SAT problem is\nmore sensitive to small changes in the clause density $\\alpha$ than the random\n$2$-SAT problem.", "published": "2025-04-16 11:17:56", "link": "http://arxiv.org/abs/2504.11979v1", "categories": ["math.PR", "cs.DM", "60K35 (Primary) 82B26, 68R07 (Secondary)"], "primary_category": "math.PR"}
{"title": "Dividing sums of cycles in the semiring of functional digraphs", "abstract": "Functional digraphs are unlabelled finite digraphs where each vertex has\nexactly one out-neighbor. They are isomorphic classes of finite discrete-time\ndynamical systems. Endowed with the direct sum and product, functional digraphs\nform a semiring with an interesting multiplicative structure. For instance, we\ndo not know if the following division problem can be solved in polynomial time:\ngiven two functional digraphs $A$ and $B$, does $A$ divide $B$? That $A$\ndivides $B$ means that there exists a functional digraph $X$ such that $AX$ is\nisomorphic to $B$, and many such $X$ can exist. We can thus ask for the number\nof solutions $X$. In this paper, we focus on the case where $B$ is a sum of\ncycles (a disjoint union of cycles, corresponding to the limit behavior of\nfinite discrete-time dynamical systems). There is then a na\\\"ive\nsub-exponential algorithm to compute the non-isomorphic solutions $X$, and our\nmain result is an improvement of this algorithm which has the property to be\npolynomial when $A$ is fixed. It uses a divide-and-conquer technique that\nshould be useful for further developments on the division problem.", "published": "2025-04-16 10:22:56", "link": "http://arxiv.org/abs/2504.11943v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "Enumeration of Bases in Matroid with Exponentially Large Ground Set", "abstract": "When we deal with a matroid ${\\mathcal M}=(U,{\\mathcal I})$, we usually\nassume that it is implicitly given by means of the membership (MEM) oracle.\nMany existing efficient algorithms run in polynomial time with respect to $|U|$\nand the running time of the MEM-oracle. However, they are not efficient any\nmore when $U$ is exponentially large in some context. In this paper, we study\ntwo problems of enumerating bases in such matroids. First, we present an\nincremental-polynomial algorithm that enumerates all minimum-weighted bases,\nwhere the bounding polynomial does not depend on $|U|$. To design the\nalgorithm, we assume two oracles other than the MEM-oracle: the MinB-oracle\nthat returns a minimum basis and the REL-oracle that returns a relevant element\none by one in non-decreasing order of weight. The proposed algorithm is\napplicable to enumeration of minimum bases of binary matroids from cycle space,\npath space and cut space, all of which have exponentially large $U$ with\nrespect to a given graph. The highlight in this context is that, to design the\nREL-oracle for cut space, we develop the first polynomial-delay algorithm that\nenumerates all relevant cuts of a given graph in non-decreasing order of\nweight. Finally, we present a polynomial-delay algorithm that enumerates all\nsets of linearly independent $r$-dimensional $r$ vectors over $\\mathit{GF}(2)$.\nUsing the algorithm, we can enumerate all unweighted bases of a binary matroid\nsuch that elements are closed under addition, in polynomial-delay with respect\nto the matroid rank $r$.", "published": "2025-04-16 03:05:39", "link": "http://arxiv.org/abs/2504.11728v1", "categories": ["cs.DS", "cs.DM", "G.2.2"], "primary_category": "cs.DS"}
{"title": "Clarifying Ambiguities: on the Role of Ambiguity Types in Prompting Methods for Clarification Generation", "abstract": "In information retrieval (IR), providing appropriate clarifications to better\nunderstand users' information needs is crucial for building a proactive\nsearch-oriented dialogue system. Due to the strong in-context learning ability\nof large language models (LLMs), recent studies investigate prompting methods\nto generate clarifications using few-shot or Chain of Thought (CoT) prompts.\nHowever, vanilla CoT prompting does not distinguish the characteristics of\ndifferent information needs, making it difficult to understand how LLMs resolve\nambiguities in user queries. In this work, we focus on the concept of ambiguity\nfor clarification, seeking to model and integrate ambiguities in the\nclarification process. To this end, we comprehensively study the impact of\nprompting schemes based on reasoning and ambiguity for clarification. The idea\nis to enhance the reasoning abilities of LLMs by limiting CoT to predict first\nambiguity types that can be interpreted as instructions to clarify, then\ncorrespondingly generate clarifications. We name this new prompting scheme\nAmbiguity Type-Chain of Thought (AT-CoT). Experiments are conducted on various\ndatasets containing human-annotated clarifying questions to compare AT-CoT with\nmultiple baselines. We also perform user simulations to implicitly measure the\nquality of generated clarifications under various IR scenarios.", "published": "2025-04-16 14:21:02", "link": "http://arxiv.org/abs/2504.12113v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Optimizing Compound Retrieval Systems", "abstract": "Modern retrieval systems do not rely on a single ranking model to construct\ntheir rankings. Instead, they generally take a cascading approach where a\nsequence of ranking models are applied in multiple re-ranking stages. Thereby,\nthey balance the quality of the top-K ranking with computational costs by\nlimiting the number of documents each model re-ranks. However, the cascading\napproach is not the only way models can interact to form a retrieval system.\n  We propose the concept of compound retrieval systems as a broader class of\nretrieval systems that apply multiple prediction models. This encapsulates\ncascading models but also allows other types of interactions than top-K\nre-ranking. In particular, we enable interactions with large language models\n(LLMs) which can provide relative relevance comparisons. We focus on the\noptimization of compound retrieval system design which uniquely involves\nlearning where to apply the component models and how to aggregate their\npredictions into a final ranking. This work shows how our compound approach can\ncombine the classic BM25 retrieval model with state-of-the-art (pairwise) LLM\nrelevance predictions, while optimizing a given ranking metric and efficiency\ntarget. Our experimental results show optimized compound retrieval systems\nprovide better trade-offs between effectiveness and efficiency than cascading\napproaches, even when applied in a self-supervised manner.\n  With the introduction of compound retrieval systems, we hope to inspire the\ninformation retrieval field to more out-of-the-box thinking on how prediction\nmodels can interact to form rankings.", "published": "2025-04-16 13:18:16", "link": "http://arxiv.org/abs/2504.12063v1", "categories": ["cs.IR", "cs.AI", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Generative Recommendation with Continuous-Token Diffusion", "abstract": "In recent years, there has been a significant trend toward using large\nlanguage model (LLM)-based recommender systems (RecSys). Current research\nprimarily focuses on representing complex user-item interactions within a\ndiscrete space to align with the inherent discrete nature of language models.\nHowever, this approach faces limitations due to its discrete nature: (i)\ninformation is often compressed during discretization; (ii) the tokenization\nand generation for the vast number of users and items in real-world scenarios\nare constrained by a limited vocabulary. Embracing continuous data presents a\npromising alternative to enhance expressive capabilities, though this approach\nis still in its early stages. To address this gap, we propose a novel\nframework, DeftRec, which incorporates \\textbf{de}noising di\\textbf{f}fusion\nmodels to enable LLM-based RecSys to seamlessly support continuous\n\\textbf{t}oken as input and target. First, we introduce a robust tokenizer with\na masking operation and an additive K-way architecture to index users and\nitems, capturing their complex collaborative relationships into continuous\ntokens. Crucially, we develop a denoising diffusion model to process user\npreferences within continuous domains by conditioning on reasoning content from\npre-trained large language model. During the denoising process, we reformulate\nthe objective to include negative interactions, building a comprehensive\nunderstanding of user preferences for effective and accurate recommendation\ngeneration. Finally, given a continuous token as output, recommendations can be\neasily generated through score-based retrieval. Extensive experiments\ndemonstrate the effectiveness of the proposed methods, showing that DeftRec\nsurpasses competitive benchmarks, including both traditional and emerging\nLLM-based RecSys.", "published": "2025-04-16 12:01:03", "link": "http://arxiv.org/abs/2504.12007v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "R\u00e9sum\u00e9 abstractif \u00e0 partir d'une transcription audio", "abstract": "Currently, large language models are gaining popularity, their achievements\nare used in many areas, ranging from text translation to generating answers to\nqueries. However, the main problem with these new machine learning algorithms\nis that training such models requires large computing resources that only large\nIT companies have. To avoid this problem, a number of methods (LoRA,\nquantization) have been proposed so that existing models can be effectively\nfine-tuned for specific tasks. In this paper, we propose an E2E (end to end)\naudio summarization model using these techniques. In addition, this paper\nexamines the effectiveness of these approaches to the problem under\nconsideration and draws conclusions about the applicability of these methods.", "published": "2025-04-16 06:24:49", "link": "http://arxiv.org/abs/2504.11803v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "A New Paradigm of User-Centric Wireless Communication Driven by Large Language Models", "abstract": "The next generation of wireless communications seeks to deeply integrate\nartificial intelligence (AI) with user-centric communication networks, with the\ngoal of developing AI-native networks that more accurately address user\nrequirements. The rapid development of large language models (LLMs) offers\nsignificant potential in realizing these goals. However, existing efforts that\nleverage LLMs for wireless communication often overlook the considerable gap\nbetween human natural language and the intricacies of real-world communication\nsystems, thus failing to fully exploit the capabilities of LLMs. To address\nthis gap, we propose a novel LLM-driven paradigm for wireless communication\nthat innovatively incorporates the nature language to structured query language\n(NL2SQL) tool. Specifically, in this paradigm, user personal requirements is\nthe primary focus. Upon receiving a user request, LLMs first analyze the user\nintent in terms of relevant communication metrics and system parameters.\nSubsequently, a structured query language (SQL) statement is generated to\nretrieve the specific parameter values from a high-performance real-time\ndatabase. We further utilize LLMs to formulate and solve an optimization\nproblem based on the user request and the retrieved parameters. The solution to\nthis optimization problem then drives adjustments in the communication system\nto fulfill the user's requirements. To validate the feasibility of the proposed\nparadigm, we present a prototype system. In this prototype, we consider\nuser-request centric semantic communication (URC-SC) system in which a dynamic\nsemantic representation network at the physical layer adapts its encoding depth\nto meet user requirements. Additionally, two LLMs are employed to analyze user\nrequests and generate SQL statements, respectively. Simulation results\ndemonstrate the effectiveness.", "published": "2025-04-16 01:43:36", "link": "http://arxiv.org/abs/2504.11696v1", "categories": ["cs.NI", "cs.IR", "cs.SY", "eess.SY"], "primary_category": "cs.NI"}
{"title": "Kernels for Storage Capacity and Dual Index Coding", "abstract": "The storage capacity of a graph measures the maximum amount of information\nthat can be stored across its vertices, such that the information at any vertex\ncan be recovered from the information stored at its neighborhood. The study of\nthis graph quantity is motivated by applications in distributed storage and by\nits intimate relations to the index coding problem from the area of network\ninformation theory. In the latter, one wishes to minimize the amount of\ninformation that has to be transmitted to a collection of receivers, in a way\nthat enables each of them to discover its required data using some prior side\ninformation.\n  In this paper, we initiate the study of the Storage Capacity and Index Coding\nproblems from the perspective of parameterized complexity. We prove that the\nStorage Capacity problem parameterized by the solution size admits a\nkernelization algorithm producing kernels of linear size. We also provide such\na result for the Index Coding problem, in the linear and non-linear settings,\nwhere it is parameterized by the dual value of the solution, i.e., the length\nof the transmission that can be saved using the side information. A key\ningredient in the proofs is the crown decomposition technique due to Chor,\nFellows, and Juedes (WG 2003, WG 2004). As an application, we significantly\nextend an algorithmic result of Dau, Skachek, and Chee (IEEE Trans. Inform.\nTheory, 2014).", "published": "2025-04-16 17:34:58", "link": "http://arxiv.org/abs/2504.12274v1", "categories": ["cs.DS", "cs.IT", "math.IT"], "primary_category": "cs.DS"}
{"title": "The Optimal Condition Number for ReLU Function", "abstract": "ReLU is a widely used activation function in deep neural networks. This paper\nexplores the stability properties of the ReLU map. For any weight matrix\n$\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n}$ and bias vector $\\boldsymbol{b}\n\\in \\mathbb{R}^{m}$ at a given layer, we define the condition number\n$\\beta_{\\boldsymbol{A},\\boldsymbol{b}}$ as\n$\\beta_{\\boldsymbol{A},\\boldsymbol{b}} =\n\\frac{\\mathcal{U}_{\\boldsymbol{A},\\boldsymbol{b}}}{\\mathcal{L}_{\\boldsymbol{A},\\boldsymbol{b}}}$,\nwhere $\\mathcal{U}_{\\boldsymbol{A},\\boldsymbol{b}}$\n  and $\\mathcal{L}_{\\boldsymbol{A},\\boldsymbol{b}}$ are the upper and lower\nLipschitz constants, respectively. We first demonstrate that for any given\n$\\boldsymbol{A}$ and $\\boldsymbol{b}$, the condition number satisfies\n$\\beta_{\\boldsymbol{A},\\boldsymbol{b}} \\geq \\sqrt{2}$. Moreover, when the\nweights of the network at a given layer are initialized as random i.i.d.\nGaussian variables and the bias term is set to zero, the condition number\nasymptotically approaches this lower bound. This theoretical finding suggests\nthat Gaussian weight initialization is optimal for preserving distances in the\ncontext of random deep neural network weights.", "published": "2025-04-16 15:47:38", "link": "http://arxiv.org/abs/2504.12194v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Battery-aware Cyclic Scheduling in Energy-harvesting Federated Learning", "abstract": "Federated Learning (FL) has emerged as a promising framework for distributed\nlearning, but its growing complexity has led to significant energy consumption,\nparticularly from computations on the client side. This challenge is especially\ncritical in energy-harvesting FL (EHFL) systems, where device availability\nfluctuates due to limited and time-varying energy resources. We propose\nFedBacys, a battery-aware FL framework that introduces cyclic client\nparticipation based on users' battery levels to cope with these issues.\nFedBacys enables clients to save energy and strategically perform local\ntraining just before their designated transmission time by clustering clients\nand scheduling their involvement sequentially. This design minimizes redundant\ncomputation, reduces system-wide energy usage, and improves learning stability.\nOur experiments demonstrate that FedBacys outperforms existing approaches in\nterms of energy efficiency and performance consistency, exhibiting robustness\neven under non-i.i.d. training data distributions and with very infrequent\nbattery charging. This work presents the first comprehensive evaluation of\ncyclic client participation in EHFL, incorporating both communication and\ncomputation costs into a unified, resource-aware scheduling strategy.", "published": "2025-04-16 15:38:38", "link": "http://arxiv.org/abs/2504.12181v1", "categories": ["cs.LG", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "Improvement of the square-root low bounds on the minimum distances of BCH codes and Matrix-product codes", "abstract": "The task of constructing infinite families of self-dual codes with unbounded\nlengths and minimum distances exhibiting square-root lower bounds is extremely\nchallenging, especially when it comes to cyclic codes. Recently, the first\ninfinite family of Euclidean self-dual binary and nonbinary cyclic codes, whose\nminimum distances have a square-root lower bound and have a lower bound better\nthan square-root lower bounds are constructed in \\cite{Chen23} for the lengths\nof these codes being unbounded. Let $q$ be a power of a prime number and\n$Q=q^2$. In this paper, we first improve the lower bounds on the minimum\ndistances of Euclidean and Hermitian duals of BCH codes with length\n$\\frac{q^m-1}{q^s-1}$ over $\\mathbb{F}_q$ and $\\frac{Q^m-1}{Q-1}$ over\n$\\mathbb{F}_Q$ in \\cite{Fan23,GDL21,Wang24} for the designed distances in some\nranges, respectively, where $\\frac{m}{s}\\geq 3$. Then based on matrix-product\nconstruction and some lower bounds on the minimum distances of BCH codes and\ntheir duals, we obtain several classes of Euclidean and Hermitian self-dual\ncodes, whose minimum distances have square-root lower bounds or a\nsquare-root-like lower bounds. Our lower bounds on the minimum distances of\nEuclidean and Hermitian self-dual cyclic codes improved many results in\n\\cite{Chen23}. In addition, our lower bounds on the minimum distances of the\nduals of BCH codes are almost $q^s-1$ or $q$ times that of the existing lower\nbounds.", "published": "2025-04-16 14:26:51", "link": "http://arxiv.org/abs/2504.12116v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Successive-Cancellation Flip and Perturbation Decoder of Polar Codes", "abstract": "In this paper, two decoding algorithms based on Successive Cancellation (SC)\nare proposed to improve the error-correction performance of cyclic redundancy\ncheck (CRC)-aided polar codes while aiming for a low-complexity implementation.\nComparisons with Dynamic SC Flip (DSCF) and SC Perturbation (SCP) are carried\nout since the proposed DSCF and Perturbation (DSCFP) and Perturbed DSCF (PDSCF)\nalgorithms combine both methods. The analysis includes comparisons with several\ncode lengths $N$ and various number of decoding attempts $T_{max}$. For\n$N=1024$ and the coding rate $R=\\frac{1}{2}$, the DSCFP and the SCP algorithms\nwith $T_{max}=17$ are bested by approximately $0.1$\\,dB at block error rate\n(BLER) of $0.001$. At $\\text{BLER}=10^{-6}$ and for $T_{max}=64$, the gain is\nof $0.375$ dB and $>0.5$ dB with respect to DSCF and SCP, respectively. At high\nsignal-to-noise ratio, the average computational complexity of the proposed\nalgorithms is virtually equivalent to that of SC.", "published": "2025-04-16 14:08:25", "link": "http://arxiv.org/abs/2504.12102v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Generalized Restart Mechanism for Successive-Cancellation Flip Decoding of Polar Codes", "abstract": "Polar codes are a class of linear error-correction codes that have received a\nlot of attention due to their ability to achieve channel capacity in an\narbitrary binary discrete memoryless channel (B-DMC) with low-complexity\nsuccessive-cancellation (SC) decoding. However, practical implementations often\nrequire better error-correction performance than what SC decoding provides,\nparticularly at short to moderate code lengths. Successive-cancellation flip\n(SCF) decoding algorithm was proposed to improve error-correction performance\nwith an aim to detect and correct the first wrongly estimated bit in a codeword\nbefore resuming SC decoding. At each additional SC decoding trial, i.e.,\ndecoding attempt beyond the initial unsuccessful trial, one bit estimated as\nthe least reliable is flipped. Dynamic SCF (DSCF) is a variation of SCF, where\nmultiple bits may be flipped simultaneously per trial. Despite the improved\nerror-correction performance compared to the SC decoder, SCF-based decoders\nhave variable execution time, which leads to high average execution time and\nlatency. In this work, we propose the generalized restart mechanism (GRM) that\nallows to skip decoding computations that are identical between the initial\ntrial and any additional trial. Under DSCF decoding with up to 3-bit flips per\ndecoding trial, our proposed GRM is shown to reduce the average execution time\nby 25% to 60% without any negative effect on error-correction performance. The\nproposed mechanism is adaptable to state-of-the-art latency-reduction\ntechniques. When applied to Fast-DSCF-3 decoding, the additional reduction\nbrought by the GRM is 15% to 22%. For the DSCF-3 decoder, the proposed\nmechanism requires approximately 4% additional memory.", "published": "2025-04-16 13:26:30", "link": "http://arxiv.org/abs/2504.12071v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Network-Centric Countermeasures Against Integrated Sensing Enabled Jamming Adversaries", "abstract": "Recent developments in Integrated Sensing and Communication have led to new\nadversarial models in wireless security through Integrated Sensing and Jamming\n(ISAJ) adversaries. ISAJ adversaries, owing to their sensing capabilities, are\nknown to inject jamming energy over the victim's frequency band, and also use\ngeneralized energy measurements on various network frequencies to detect the\npresence of countermeasures. Existing countermeasures against such ISAJ\nadversaries are laid under the assumption that the adversary does not have the\nknowledge of the countermeasure. However, according to Kerchoffs' principle in\ncryptography, security of a countermeasure should only rely on the secret-keys,\nnot on the obfuscation of the countermeasure. On testing the security of\nexisting countermeasures, we observe that they violate Kerchoffs' principle,\nthus motivating the need for new countermeasures. In this regard, we propose a\nnovel network-centric countermeasure against ISAJ adversaries, wherein a group\nof users in the network assist the victim to reliably communicate her messages\nin a covert manner. Firstly, we analyse the error performance of the proposed\ncountermeasure, and study its behavior on the number of assisting users in the\nnetwork. Subsequently, to validate its security against Kerchoffs' principle,\nwe study the Shannon's entropy associated with the presence of the victim's\nmessages in the network and analyse its behaviour as a function of the number\nof assisting users. Finally, to study the interplay between reliability and\ncovertness, we pose interesting optimization problems and solve them to choose\nthe underlying parameters of the countermeasure and the number of assisting\nusers.", "published": "2025-04-16 12:05:30", "link": "http://arxiv.org/abs/2504.12009v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "On the Intersection and Composition properties of conditional independence", "abstract": "Compositional graphoids are fundamental discrete structures which appear in\nprobabilistic reasoning, particularly in the area of graphical models. They are\nsemigraphoids which satisfy the Intersection and Composition properties. These\nimportant properties, however, are not enjoyed by general probability\ndistributions. We survey what is known in terms of sufficient conditions for\nIntersection and Composition and derive a set of new sufficient conditions in\nthe context of discrete random variables based on conditional information\ninequalities for Shannon entropies.", "published": "2025-04-16 11:17:41", "link": "http://arxiv.org/abs/2504.11978v1", "categories": ["cs.IT", "math.IT", "math.ST", "stat.TH", "94A15 (primary) 62R01, 94A17 (secondary)"], "primary_category": "cs.IT"}
{"title": "On Codes from Split Metacyclic Groups", "abstract": "The paper presents a comprehensive study of group codes from non-abelian\nsplit metacyclic group algebras. We derive an explicit Wedderburn-like\ndecomposition of finite split metacyclic group algebras over fields with\ncharacteristic coprime to the group order. Utilizing this decomposition, we\ndevelop a systematic theory of metacyclic codes, providing their algebraic\ndescription and proving that they can be viewed as generalized concatenated\ncodes with cyclic inner codes and skew quasi-cyclic outer codes. We establish\nbounds on the minimum distance of metacyclic codes and investigate the class of\ninduced codes. Furthermore, we show the feasibility of constructing a partial\nkey-recovery attack against certain McEliece-type cryptosystems based on\nmetacyclic codes by exploiting their generalized concatenated structure.", "published": "2025-04-16 10:43:23", "link": "http://arxiv.org/abs/2504.11960v1", "categories": ["cs.IT", "math.IT", "math.RA", "16S34 (Primary), 94B05 (Secondary)"], "primary_category": "cs.IT"}
{"title": "DALC: Distributed Arithmetic Coding Aided by Linear Codes", "abstract": "Distributed Arithmetic Coding (DAC) has emerged as a feasible solution to the\nSlepian-Wolf problem, particularly in scenarios with non-stationary sources and\nfor data sequences with lengths ranging from small to medium. Due to the\ninherent decoding ambiguity in DAC, the number of candidate paths grows\nexponentially with the increase in source length. To select the correct\ndecoding path from the set of candidates, DAC decoders utilize the Maximum A\nPosteriori (MAP) metric to rank the decoding sequences, outputting the path\nwith the highest MAP metric as the decoding result of the decoder. However,\nthis method may still inadvertently output incorrect paths that have a MAP\nmetric higher than the correct decoding path, despite not being the correct\ndecoding path. To address the issue, we propose Distributed Arithmetic Coding\nAided by Linear Codes (DALC), which employs linear codes to constrain the\ndecoding process, thereby eliminating some incorrect paths and preserving the\ncorrect one. During the encoding phase, DALC generates the parity bits of the\nlinear code for encoding the source data. In the decoding phase, each path in\nthe set of candidate paths is verified in descending order according to the MAP\nmetric until a path that meets the verification criteria is encountered, which\nis then outputted as the decoding result. DALC enhances the decoding\nperformance of DAC by excluding candidate paths that do not meet the\nconstraints imposed by linear codes. Our experimental results demonstrate that\nDALC reduces the Bit Error Rate(BER), with especially improvements in skewed\nsource data scenarios.", "published": "2025-04-16 05:36:52", "link": "http://arxiv.org/abs/2504.11784v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Sliding Block Martingale based Multi-hop Delay QoS Analysis", "abstract": "With the growing density of wireless networks and demand for multi-hop\ntransmissions, precise delay Quality of Service (QoS) analysis has become a\ncritical challenge. This paper introduces a multi-hop delay QoS analysis\nframework based on the sliding block martingale, addressing the loose boundary\nissue of prior methods that rely on service process martingales and min-plus\ntransformations. By constructing a sliding block martingale with a window, we\ncapture both long-term trends and short-term fluctuations in the backlog,\neliminating the reliance on the generalized incremental property. The framework\nredefines delay unreliability events using cascading attributes, deriving a\nmore compact Delay Unreliability Probability Boundary (DUPB). To improve the\nefficiency of solving the key parameter $\\theta$, we propose a Micrometric\nIntervals based Supermartingale Upcrossing Estimate Theorem, quantifying the\nupper bound of event occurrence frequency to constrain the solution space of\n$\\theta$. Simulations based on the 3GPP UMa/UMi channel model validate the\nframework's effectiveness. Results show that in 2-7 hop scenarios, the maximum\ndeviation between theoretical boundaries and Monte Carlo simulations is $4.116\n\\times 10^{-5}$, with a lower RMSE than existing methods. Iteration count and\nCPU time for solving $\\theta$ are reduced by $59\\%-72\\%$ and $60.6\\%-70.5\\%$,\nrespectively, improving analysis efficiency. Furthermore, the derived minimum\nservice rate for multi-hop queues offers a valuable reference for resource\nallocation. The framework demonstrates high accuracy, scalability, and\npracticality in complex multi-hop networks.", "published": "2025-04-16 05:13:53", "link": "http://arxiv.org/abs/2504.11769v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Beyond ISAC: Toward Integrated Heterogeneous Service Provisioning via Elastic Multi-Dimensional Multiple Access", "abstract": "Integrated heterogeneous service provisioning (IHSP) is a promising paradigm\nthat is designed to concurrently support a variety of heterogeneous services,\nextending beyond sensing and communication to meet the diverse needs of\nemerging applications. However, a primary challenge of IHSP is addressing the\nconflicts between multiple competing service demands under constrained\nresources. In this paper, we overcome this challenge by the joint use of two\nnovel elastic design strategies: compromised service value assessment and\nflexible multi-dimensional resource multiplexing. Consequently, we propose a\nvalue-prioritized elastic multi-dimensional multiple access (MDMA) mechanism\nfor IHSP systems. First, we modify the Value-of-Service (VoS) metric by\nincorporating elastic parameters to characterize user-specific tolerance and\ncompromise in response to various performance degradations under constrained\nresources. This VoS metric serves as the foundation for prioritizing services\nand enabling effective fairness service scheduling among concurrent competing\ndemands. Next, we adapt the MDMA to elastically multiplex services using\nappropriate multiple access schemes across different resource domains. This\nprotocol leverages user-specific interference tolerances and cancellation\ncapabilities across different domains to reduce resource-demanding conflicts\nand co-channel interference within the same domain. Then, we maximize the\nsystem's VoS by jointly optimizing MDMA design and power allocation. Since this\nproblem is non-convex, we propose a monotonic optimization-assisted dynamic\nprogramming (MODP) algorithm to obtain its optimal solution. Additionally, we\ndevelop the VoS-prioritized successive convex approximation (SCA) algorithm to\nefficiently find its suboptimal solution. Finally, simulations are presented to\nvalidate the effectiveness of the proposed designs.", "published": "2025-04-16 01:21:56", "link": "http://arxiv.org/abs/2504.11692v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Robust and Scalable Variational Bayes", "abstract": "We propose a robust and scalable framework for variational Bayes (VB) that\neffectively handles outliers and contamination of arbitrary nature in large\ndatasets. Our approach divides the dataset into disjoint subsets, computes the\nposterior for each subset, and applies VB approximation independently to these\nposteriors. The resulting variational posteriors with respect to the subsets\nare then aggregated using the geometric median of probability measures,\ncomputed with respect to the Wasserstein distance. This novel aggregation\nmethod yields the Variational Median Posterior (VM-Posterior) distribution. We\nrigorously demonstrate that the VM-Posterior preserves contraction properties\nakin to those of the true posterior, while accounting for approximation errors\nor the variational gap inherent in VB methods. We also provide provable\nrobustness guarantee of the VM-Posterior. Furthermore, we establish a\nvariational Bernstein-von Mises theorem for both multivariate Gaussian\ndistributions with general covariance structures and the mean-field variational\nfamily. To facilitate practical implementation, we adapt existing algorithms\nfor computing the VM-Posterior and evaluate its performance through extensive\nnumerical experiments. The results highlight its robustness and scalability,\nmaking it a reliable tool for Bayesian inference in the presence of complex,\ncontaminated datasets.", "published": "2025-04-16 23:20:43", "link": "http://arxiv.org/abs/2504.12528v1", "categories": ["stat.ML", "cs.LG", "stat.ME"], "primary_category": "stat.ML"}
{"title": "Corner Gradient Descent", "abstract": "We consider SGD-type optimization on infinite-dimensional quadratic problems\nwith power law spectral conditions. It is well-known that on such problems\ndeterministic GD has loss convergence rates $L_t=O(t^{-\\zeta})$, which can be\nimproved to $L_t=O(t^{-2\\zeta})$ by using Heavy Ball with a non-stationary\nJacobi-based schedule (and the latter rate is optimal among fixed schedules).\nHowever, in the mini-batch Stochastic GD setting, the sampling noise causes the\nJacobi HB to diverge; accordingly no $O(t^{-2\\zeta})$ algorithm is known. In\nthis paper we show that rates up to $O(t^{-2\\zeta})$ can be achieved by a\ngeneralized stationary SGD with infinite memory. We start by identifying\ngeneralized (S)GD algorithms with contours in the complex plane. We then show\nthat contours that have a corner with external angle $\\theta\\pi$ accelerate the\nplain GD rate $O(t^{-\\zeta})$ to $O(t^{-\\theta\\zeta})$. For deterministic GD,\nincreasing $\\theta$ allows to achieve rates arbitrarily close to\n$O(t^{-2\\zeta})$. However, in Stochastic GD, increasing $\\theta$ also amplifies\nthe sampling noise, so in general $\\theta$ needs to be optimized by balancing\nthe acceleration and noise effects. We prove that the optimal rate is given by\n$\\theta_{\\max}=\\min(2,\\nu,\\tfrac{2}{\\zeta+1/\\nu})$, where $\\nu,\\zeta$ are the\nexponents appearing in the capacity and source spectral conditions.\nFurthermore, using fast rational approximations of the power functions, we show\nthat ideal corner algorithms can be efficiently approximated by finite-memory\nalgorithms, and demonstrate their practical efficiency on a synthetic problem\nand MNIST.", "published": "2025-04-16 22:39:41", "link": "http://arxiv.org/abs/2504.12519v1", "categories": ["math.OC", "cs.LG"], "primary_category": "math.OC"}
{"title": "Reinforcement Learning from Human Feedback", "abstract": "Reinforcement learning from human feedback (RLHF) has become an important\ntechnical and storytelling tool to deploy the latest machine learning systems.\nIn this book, we hope to give a gentle introduction to the core methods for\npeople with some level of quantitative background. The book starts with the\norigins of RLHF -- both in recent literature and in a convergence of disparate\nfields of science in economics, philosophy, and optimal control. We then set\nthe stage with definitions, problem formulation, data collection, and other\ncommon math used in the literature. The core of the book details every\noptimization stage in using RLHF, from starting with instruction tuning to\ntraining a reward model and finally all of rejection sampling, reinforcement\nlearning, and direct alignment algorithms. The book concludes with advanced\ntopics -- understudied research questions in synthetic data and evaluation --\nand open questions for the field.", "published": "2025-04-16 21:36:46", "link": "http://arxiv.org/abs/2504.12501v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Boosting Reservoir Computing with Brain-inspired Adaptive Dynamics", "abstract": "Reservoir computers (RCs) provide a computationally efficient alternative to\ndeep learning while also offering a framework for incorporating brain-inspired\ncomputational principles. By using an internal neural network with random,\nfixed connections$-$the 'reservoir'$-$and training only the output weights, RCs\nsimplify the training process but remain sensitive to the choice of\nhyperparameters that govern activation functions and network architecture.\nMoreover, typical RC implementations overlook a critical aspect of neuronal\ndynamics: the balance between excitatory and inhibitory (E-I) signals, which is\nessential for robust brain function. We show that RCs characteristically\nperform best in balanced or slightly over-inhibited regimes, outperforming\nexcitation-dominated ones. To reduce the need for precise hyperparameter\ntuning, we introduce a self-adapting mechanism that locally adjusts E/I balance\nto achieve target neuronal firing rates, improving performance by up to 130% in\ntasks like memory capacity and time series prediction compared with globally\ntuned RCs. Incorporating brain-inspired heterogeneity in target neuronal firing\nrates further reduces the need for fine-tuning hyperparameters and enables RCs\nto excel across linear and non-linear tasks. These results support a shift from\nstatic optimization to dynamic adaptation in reservoir design, demonstrating\nhow brain-inspired mechanisms improve RC performance and robustness while\ndeepening our understanding of neural computation.", "published": "2025-04-16 20:36:08", "link": "http://arxiv.org/abs/2504.12480v1", "categories": ["cs.NE", "cs.LG", "q-bio.NC"], "primary_category": "cs.NE"}
{"title": "You Don't Need All Attentions: Distributed Dynamic Fine-Tuning for Foundation Models", "abstract": "Fine-tuning plays a crucial role in adapting models to downstream tasks with\nminimal training efforts. However, the rapidly increasing size of foundation\nmodels poses a daunting challenge for accommodating foundation model\nfine-tuning in most commercial devices, which often have limited memory\nbandwidth. Techniques like model sharding and tensor parallelism address this\nissue by distributing computation across multiple devices to meet memory\nrequirements. Nevertheless, these methods do not fully leverage their\nfoundation nature in facilitating the fine-tuning process, resulting in high\ncomputational costs and imbalanced workloads. We introduce a novel Distributed\nDynamic Fine-Tuning (D2FT) framework that strategically orchestrates operations\nacross attention modules based on our observation that not all attention\nmodules are necessary for forward and backward propagation in fine-tuning\nfoundation models. Through three innovative selection strategies, D2FT\nsignificantly reduces the computational workload required for fine-tuning\nfoundation models. Furthermore, D2FT addresses workload imbalances in\ndistributed computing environments by optimizing these selection strategies via\nmultiple knapsack optimization. Our experimental results demonstrate that the\nproposed D2FT framework reduces the training computational costs by 40% and\ntraining communication costs by 50% with only 1% to 2% accuracy drops on the\nCIFAR-10, CIFAR-100, and Stanford Cars datasets. Moreover, the results show\nthat D2FT can be effectively extended to recent LoRA, a state-of-the-art\nparameter-efficient fine-tuning technique. By reducing 40% computational cost\nor 50% communication cost, D2FT LoRA top-1 accuracy only drops 4% to 6% on\nStanford Cars dataset.", "published": "2025-04-16 20:18:15", "link": "http://arxiv.org/abs/2504.12471v1", "categories": ["cs.LG", "cs.DC", "cs.PF"], "primary_category": "cs.LG"}
{"title": "Geometric Generality of Transformer-Based Gr\u00f6bner Basis Computation", "abstract": "The intersection of deep learning and symbolic mathematics has seen rapid\nprogress in recent years, exemplified by the work of Lample and Charton. They\ndemonstrated that effective training of machine learning models for solving\nmathematical problems critically depends on high-quality, domain-specific\ndatasets. In this paper, we address the computation of Gr\\\"obner basis using\nTransformers. While a dataset generation method tailored to Transformer-based\nGr\\\"obner basis computation has previously been proposed, it lacked theoretical\nguarantees regarding the generality or quality of the generated datasets. In\nthis work, we prove that datasets generated by the previously proposed\nalgorithm are sufficiently general, enabling one to ensure that Transformers\ncan learn a sufficiently diverse range of Gr\\\"obner bases. Moreover, we propose\nan extended and generalized algorithm to systematically construct datasets of\nideal generators, further enhancing the training effectiveness of Transformer.\nOur results provide a rigorous geometric foundation for Transformers to address\na mathematical problem, which is an answer to Lample and Charton's idea of\ntraining on diverse or representative inputs.", "published": "2025-04-16 20:01:00", "link": "http://arxiv.org/abs/2504.12465v1", "categories": ["cs.LG", "cs.SC", "math.AG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "M$^2$FGB: A Min-Max Gradient Boosting Framework for Subgroup Fairness", "abstract": "In recent years, fairness in machine learning has emerged as a critical\nconcern to ensure that developed and deployed predictive models do not have\ndisadvantageous predictions for marginalized groups. It is essential to\nmitigate discrimination against individuals based on protected attributes such\nas gender and race. In this work, we consider applying subgroup justice\nconcepts to gradient-boosting machines designed for supervised learning\nproblems. Our approach expanded gradient-boosting methodologies to explore a\nbroader range of objective functions, which combines conventional losses such\nas the ones from classification and regression and a min-max fairness term. We\nstudy relevant theoretical properties of the solution of the min-max\noptimization problem. The optimization process explored the primal-dual\nproblems at each boosting round. This generic framework can be adapted to\ndiverse fairness concepts. The proposed min-max primal-dual gradient boosting\nalgorithm was theoretically shown to converge under mild conditions and\nempirically shown to be a powerful and flexible approach to address binary and\nsubgroup fairness.", "published": "2025-04-16 19:47:53", "link": "http://arxiv.org/abs/2504.12458v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Can Moran Eigenvectors Improve Machine Learning of Spatial Data? Insights from Synthetic Data Validation", "abstract": "Moran Eigenvector Spatial Filtering (ESF) approaches have shown promise in\naccounting for spatial effects in statistical models. Can this extend to\nmachine learning? This paper examines the effectiveness of using Moran\nEigenvectors as additional spatial features in machine learning models. We\ngenerate synthetic datasets with known processes involving spatially varying\nand nonlinear effects across two different geometries. Moran Eigenvectors\ncalculated from different spatial weights matrices, with and without a priori\neigenvector selection, are tested. We assess the performance of popular machine\nlearning models, including Random Forests, LightGBM, XGBoost, and TabNet, and\nbenchmark their accuracies in terms of cross-validated R2 values against models\nthat use only coordinates as features. We also extract coefficients and\nfunctions from the models using GeoShapley and compare them with the true\nprocesses. Results show that machine learning models using only location\ncoordinates achieve better accuracies than eigenvector-based approaches across\nvarious experiments and datasets. Furthermore, we discuss that while these\nfindings are relevant for spatial processes that exhibit positive spatial\nautocorrelation, they do not necessarily apply when modeling network\nautocorrelation and cases with negative spatial autocorrelation, where Moran\nEigenvectors would still be useful.", "published": "2025-04-16 19:31:42", "link": "http://arxiv.org/abs/2504.12450v1", "categories": ["cs.LG", "econ.EM", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Traveling wave profiles for a semi-discrete Burgers equation", "abstract": "We look for traveling waves of the semi-discrete conservation law $4\\dot u_j\n+u_{j+1}^2-u_{j-1}^2 = 0$, using variational principles related to concepts of\n``hidden convexity'' appearing in recent studies of various PDE (partial\ndifferential equations). We analyze and numerically compute with two\nvariational formulations related to dual convex optimization problems\nconstrained by either the differential-difference equation (DDE) or nonlinear\nintegral equation (NIE) that wave profiles should satisfy. We prove existence\ntheorems conditional on the existence of extrema that satisfy a strict\nconvexity criterion, and numerically exhibit a variety of localized, periodic\nand non-periodic wave phenomena.", "published": "2025-04-16 15:23:43", "link": "http://arxiv.org/abs/2504.12171v1", "categories": ["math.AP", "cs.NA", "math.NA", "nlin.PS", "Primary: 49J35, 49M29 Secondary: 34K31, 70G75, 35Q70"], "primary_category": "math.AP"}
{"title": "Central-Upwind Scheme for the Phase-Transition Traffic Flow Model", "abstract": "Phase-transition models are an important family of non-equilibrium continuum\ntraffic flow models, offering properties like replicating complex traffic\nphenomena, maintaining anisotropy, and promising potentials for accommodating\nautomated vehicles. However, their complex mathematical characteristics such as\ndiscontinuous solution domains, pose numerical challenges and limit their\nexploration in traffic flow theory. This paper focuses on developing a robust\nand accurate numerical method for phase-transition traffic flow models: We\npropose a second-order semi-discrete central-upwind scheme specifically\ndesigned for discontinuous phase-transition models. This novel scheme\nincorporates the projection onto appropriate flow domains, ensuring enhanced\nhandling of discontinuities and maintaining physical consistency and accuracy.\nWe demonstrate the efficacy of the proposed scheme through extensive and\nchallenging numerical tests, showcasing their potential to facilitate further\nresearch and application in phase-transition traffic flow modeling. The ability\nof phase-transition models to embed the ``time-gap'' -- a crucial element in\nautomated traffic control -- as a conserved variable aligns seamlessly with the\ncontrol logic of automated vehicles, presenting significant potential for\nfuture applications, and the proposed numerical scheme now substantially\nfacilitates exploring such potentials.", "published": "2025-04-16 15:02:14", "link": "http://arxiv.org/abs/2504.12153v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Inclusion of an Inverse Magnetic Hysteresis Model into the Space-Time Finite Element Method for Magnetoquasistatics", "abstract": "In this note we discuss the numerical solution of the eddy current\napproximation of the Maxwell equations using the simple Pragmatic Algebraic\nModel to include hysteresis effects. In addition to the more standard\ntime-stepping approach we propose a space-time finite element method which\nallows both for parallelization and adaptivity simultaneously in space and\ntime. Numerical experiments confirm both approaches yield the same numerical\nresults.", "published": "2025-04-16 11:56:42", "link": "http://arxiv.org/abs/2504.12003v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Epstein zeta method for many-body lattice sums", "abstract": "Many-body interactions arise naturally in the perturbative treatment of\nclassical and quantum many-body systems and play a crucial role in the\ndescription of condensed matter systems. In the case of three-body\ninteractions, the Axilrod-Teller-Muto (ATM) potential is highly relevant for\nthe quantitative prediction of material properties. The computation of the\nresulting energies in d-dimensional lattice systems is challenging, as a\nhigh-dimensional lattice sum needs to be evaluated to high precision. This work\nsolves this long-standing issue. We present an efficiently computable\nrepresentation of many-body lattice sums in terms of singular integrals over\nproducts of Epstein zeta functions. For three-body interactions in 3D, this\napproach reduces the runtime for computing the ATM lattice sum from weeks to\nminutes. Our approach further extends to a broad class of n-body lattice sums.\nWe demonstrate that the computational cost of our method only increases\nlinearly with n, evading the exponential increase in complexity of direct\nsummation. The evaluation of 51-body interactions on a two-dimensional lattice,\ncorresponding to a 100-dimensional sum, can be performed within seconds on a\nlaptop. We discuss techniques for computing the arising singular integrals and\ncompare the accuracy of our results against computable benchmarks, achieving\nfull precision for exponents greater than the system dimension. Finally, we\napply our method to study the stability of a three-dimensional lattice system\nwith Lennard-Jones two-body interactions under the inclusion of an ATM\nthree-body term at finite pressure, finding a transition from the\nface-centered-cubic to the body-centered-cubic lattice structure with\nincreasing ATM coupling strength. This work establishes the mathematical\nfoundation for an ongoing investigation into the influence of many-body\ninteractions on the stability of matter.", "published": "2025-04-16 11:30:11", "link": "http://arxiv.org/abs/2504.11989v1", "categories": ["math.NA", "cond-mat.mtrl-sci", "cond-mat.str-el", "cs.NA"], "primary_category": "math.NA"}
{"title": "Strong Convergence Rates for Euler Schemes of Levy-Driven SDE using Dynamic Cutting", "abstract": "We derive strong Lp convergence rates for the Euler-Maruyama schemes of\nLevy-driven SDE using a new dynamic cutting (DC) method with a time-dependent\njump threshold. In addition, we present results from numerical simulations\ncomparing the DC and Asmussen-Rosinski (AR) approaches. These simulations\ndemonstrate the superior accuracy achieved by the DC method.", "published": "2025-04-16 11:29:14", "link": "http://arxiv.org/abs/2504.11988v1", "categories": ["math.PR", "cs.NA", "math.NA"], "primary_category": "math.PR"}
{"title": "Stochastic Quadrature Rules for Solving PDEs using Neural Networks", "abstract": "In this article, we consider issues surrounding integration when using Neural\nNetworks to solve Partial Differential Equations. We focus on the Deep Ritz\nMethod as it is of practical interest and sensitive to integration errors. We\nshow how both deterministic integration rules as well as biased, stochastic\nquadrature can lead to erroneous results, whilst high order, unbiased\nstochastic quadrature rules on integration meshes can significantly improve\nconvergence at an equivalent computational cost. Furthermore, we propose novel\nstochastic quadrature rules for triangular and tetrahedral elements, offering\ngreater flexibility when designing integration meshes in more complex\ngeometries. We highlight how the variance in the stochastic gradient limits\nconvergence, whilst quadrature rules designed to give similar errors when\nintegrating the loss function may lead to disparate results when employed in a\ngradient-based optimiser.", "published": "2025-04-16 11:15:38", "link": "http://arxiv.org/abs/2504.11976v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Dynamical reweighting for estimation of fluctuation formulas", "abstract": "We propose a variance reduction method for calculating transport coefficients\nin molecular dynamics using an importance sampling method via Girsanov's\ntheorem applied to Green--Kubo's formula. We optimize the magnitude of the\nperturbation applied to the reference dynamics by means of a scalar\nparameter~$\\alpha$ and propose an asymptotic analysis to fully characterize the\nlong-time behavior in order to evaluate the possible variance reduction.\nTheoretical results corroborated by numerical results show that this method\nallows for some reduction in variance, although rather modest in most\nsituations.", "published": "2025-04-16 11:01:20", "link": "http://arxiv.org/abs/2504.11968v2", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Convergence of finite elements for the Eyles-King-Styles model of tumour growth", "abstract": "This paper presents a convergence analysis of evolving surface finite element\nmethods (ESFEM) applied to the original Eyles-King-Styles model of tumour\ngrowth. The model consists of a Poisson equation in the bulk, a forced mean\ncurvature flow on the surface, and a coupled velocity law between bulk and\nsurface. Due to the non-trivial bulk-surface coupling, all previous analyses --\nwhich exclusively relied on energy-estimate based approaches -- required an\nadditional regularization term. By adopting the $\\widehat{H}^{3/2}$ theory and\nthe multilinear forms, we develop an essentially new theoretical framework that\nenables the application of PDE regularity theory to stability analysis. Based\non this framework, we provide the first rigorous convergence proof for the\noriginal model without regularization.", "published": "2025-04-16 10:00:02", "link": "http://arxiv.org/abs/2504.11926v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Lagrangian finite elements in Sobolev-like spaces of order $3/2$", "abstract": "This paper introduces a Sobolev-like space of order $3/2$, denoted as\n$\\widehat{H}^{3/2}$, for Lagrangian finite elements, especially for $C^0$\nelements. It is motivated by the limitations of current stability analysis of\nthe evolving surface finite element method (ESFEM), which relies exclusively on\nan energy estimate framework. To establish a PDE-based analysis framework for\nESFEM, we encounter a fundamental regularity mismatch: the ESFEM adopts the\n$C^0$ elements, while the PDE regularity theory requires $H^{3/2}$ regularity\nfor solutions. To overcome this difficulty, we first examine the properties of\nthe continuous $H^{3/2}$ space, then introduce a Dirichlet lift and Scott-Zhang\ntype interpolation operators to bridge to the discrete $\\widehat{H}^{3/2}$\nspace. Our new $\\widehat{H}^{3/2}$ space is shown to be compatible with the\nelliptic PDE regularity theory, the trace inequality, and the inverse\ninequality. Notably, we extend the critical domain deformation estimate in\nESFEM to the $\\widehat{H}^{3/2}$ setting. The $\\widehat{H}^{3/2}$ theory\nprovides a foundation for establishing a PDE-based convergence analysis\nframework of ESFEM.", "published": "2025-04-16 09:56:43", "link": "http://arxiv.org/abs/2504.11920v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "A structure-preserving numerical method for quasi-incompressible Navier-Stokes-Maxwell-Stefan systems", "abstract": "A conforming finite element scheme with mixed explicit-implicit time\ndiscretization for quasi-incompressible Navier-Stokes-Maxwell-Stefan systems in\na bounded domain with periodic boundary conditions is presented. The system\nconsists of the Navier-Stokes equations, together with a\nquasi-incompressibility constraint, coupled with the cross-diffusion\nMaxwell-Stefan equations. The numerical scheme preserves the partial masses and\nthe quasi-incompressibility constraint and dissipates the discrete energy.\nNumerical experiments in two space dimensions illustrate the convergence of the\nscheme and the structure-preserving properties.", "published": "2025-04-16 09:19:39", "link": "http://arxiv.org/abs/2504.11892v1", "categories": ["math.NA", "cs.NA", "65M60, 65N30, 76T30, 80M10"], "primary_category": "math.NA"}
{"title": "On projection mappings and the gradient projection method on hyperbolic space forms", "abstract": "This paper presents several new properties of the intrinsic\n$\\kappa$-projection into $\\kappa$-hyperbolically convex sets of\n$\\kappa$-hyperbolic space forms, along with closed-form formulas for the\nintrinsic $\\kappa$-projection into specific $\\kappa$-hyperbolically convex\nsets. It also discusses the relationship between the intrinsic\n$\\kappa$-projection, the Euclidean orthogonal projection, and the Lorentz\nprojection. These properties lay the groundwork for analyzing the gradient\nprojection method and hold importance in their own right. Additionally, new\nproperties of the gradient projection method to solve constrained optimization\nproblems in $\\kappa$-hyperbolic space forms are established, considering both\nconstant and backtracking step sizes in the analysis. It is shown that every\naccumulation point of the sequence generated by the method for both step sizes\nis a stationary point for the given problem. Additionally, an iteration\ncomplexity bound is provided that upper bounds the number of iterations needed\nto achieve a suitable measure of stationarity for both step sizes. Finally, the\nproperties of the constrained Fermat-Weber problem are explored, demonstrating\nthat the sequence generated by the gradient projection method converges to its\nunique solution. Numerical experiments on solving the Fermat-Weber problem are\npresented, illustrating the theoretical findings and demonstrating the\neffectiveness of the proposed methods.", "published": "2025-04-16 07:01:00", "link": "http://arxiv.org/abs/2504.11815v1", "categories": ["math.OC", "cs.NA", "math.DG", "math.NA"], "primary_category": "math.OC"}
{"title": "A Technical Survey of Sparse Linear Solvers in Electronic Design Automation", "abstract": "Sparse linear system solvers ($Ax=b$) are critical computational kernels in\nElectronic Design Automation (EDA), underpinning vital simulations for modern\nIC and system design. Applications like power integrity verification and\nelectrothermal analysis fundamentally solve large-scale, sparse algebraic\nsystems from Modified Nodal Analysis (MNA) or Finite Element/Volume Method\n(FEM/FVM) discretizations of PDEs. Problem dimensions routinely reach\n$10^6-10^9$ unknowns, escalating towards $10^{10}$+ for full-chip power grids\n\\cite{Tsinghua21}, demanding stringent solver scalability, low memory\nfootprint, and efficiency. This paper surveys predominant sparse solver\nparadigms in EDA: direct factorization methods (LU, Cholesky), iterative Krylov\nsubspace methods (CG, GMRES, BiCGSTAB), and multilevel multigrid techniques. We\nexamine their mathematical foundations, convergence, conditioning sensitivity,\nimplementation aspects (storage formats CSR/CSC, fill-in mitigation via\nreordering), the critical role of preconditioning for ill-conditioned systems\n\\cite{SaadIterative, ComparisonSolversArxiv}, and multigrid's potential optimal\n$O(N)$ complexity \\cite{TrottenbergMG}. Solver choice critically depends on the\nperformance impact of frequent matrix updates (e.g., transient/non-linear),\nwhere iterative/multigrid methods often amortize costs better than direct\nmethods needing repeated factorization \\cite{SaadIterative}. We analyze\ntrade-offs in runtime complexity, memory needs, numerical robustness, parallel\nscalability (MPI, OpenMP, GPU), and precision (FP32/FP64). Integration into EDA\ntools for system-level multiphysics is discussed, with pseudocode\nillustrations. The survey concludes by emphasizing the indispensable nature and\nongoing evolution of sparse solvers for designing and verifying complex\nelectronic systems.", "published": "2025-04-16 02:34:21", "link": "http://arxiv.org/abs/2504.11716v1", "categories": ["math.NA", "cs.NA", "cs.PF"], "primary_category": "math.NA"}
{"title": "Fast Mixed-Precision Real Evaluation", "abstract": "Evaluating real-valued expressions to high precision is a key building block\nin computational mathematics, physics, and numerics. A typical implementation\nevaluates the whole expression in a uniform precision, doubling that precision\nuntil a sufficiently-accurate result is achieved. This is wasteful: usually\nonly a few operations really need to be performed at high precision, and the\nbulk of the expression could be computed much faster. However, such non-uniform\nprecision assignments have, to date, been impractical to compute. We propose a\nfast new algorithm for deriving such precision assignments. The algorithm\nleverages results computed at lower precisions to analytically determine a\nmixed-precision assignment that will result in a sufficiently-accurate result.\nOur implementation, Reval, achieves an average speed-up of 1.72x compared to\nthe state-of-the-art Sollya tool, with the speed-up increasing to 5.21x on the\nmost difficult input points. An examination of the precisions used with and\nwithout precision tuning shows that the speed-up results from assigning lower\nprecisions for the majority of operations, though additional optimizations\nenabled by the non-uniform precision assignments also play a role.", "published": "2025-04-16 02:12:20", "link": "http://arxiv.org/abs/2504.11708v1", "categories": ["math.NA", "cs.MS", "cs.NA"], "primary_category": "math.NA"}
{"title": "A method for bounding high-order finite element functions: Applications to mesh validity and bounds-preserving limiters", "abstract": "We introduce a novel method for bounding high-order multi-dimensional\npolynomials in finite element approximations. The method involves precomputing\noptimal piecewise-linear bounding boxes for polynomial basis functions, which\ncan then be used to locally bound any combination of these basis functions.\nThis approach can be applied to any element/basis type at any approximation\norder, can provide local (i.e., subcell) extremum bounds to a desired level of\naccuracy, and can be evaluated efficiently on-the-fly in simulations.\nFurthermore, we show that this approach generally yields more accurate bounds\nin comparison to traditional methods based on convex hull properties (e.g.,\nBernstein polynomials). The efficacy of this technique is shown in applications\nsuch as mesh validity checks and optimization for high-order curved meshes,\nwhere positivity of the element Jacobian determinant can be ensured throughout\nthe entire element, and continuously bounds-preserving limiters for hyperbolic\nsystems, which can enforce maximum principle bounds across the entire solution\npolynomial.", "published": "2025-04-16 01:06:48", "link": "http://arxiv.org/abs/2504.11688v1", "categories": ["math.NA", "cs.NA", "physics.flu-dyn"], "primary_category": "math.NA"}
{"title": "FEM-DtN-SIM Method for Computing Resonances of Schr\u00f6dinger Operators", "abstract": "The study of resonances of the Schr\\\"{o}dinger operator has a long-standing\ntradition in mathematical physics. Extensive theoretical investigations have\nexplored the proximity of resonances to the real axis, their distribution, and\nbounds on the counting functions. However, computational results beyond one\ndimension remain scarce due to the nonlinearity of the problem and the\nunbounded nature of the domain. We propose a novel approach that integrates\nfinite elements, Dirichlet-to-Neumann (DtN) mapping, and the spectral indicator\nmethod. The DtN mapping, imposed on the boundary of a truncated computational\ndomain, enforces the outgoing condition. Finite elements allow for the\nefficient handling of complicated potential functions. The spectral indicator\nmethod effectively computes (complex) eigenvalues of the resulting nonlinear\nalgebraic system without introducing spectral pollution. The viability of this\napproach is demonstrated through a range of numerical examples.", "published": "2025-04-16 00:34:11", "link": "http://arxiv.org/abs/2504.11680v1", "categories": ["math.NA", "cs.NA", "78M10, 65H17, 65N25"], "primary_category": "math.NA"}
{"title": "Maximum bound principle for Q-tensor gradient flow with low regularity integrators", "abstract": "We investigate low-regularity integrator (LRI) methods for the Q-tensor model\ngoverning nematic liquid-crystalline semilinear parabolic equation. First- and\nsecond-order temporal discretizations are developed using Duhamel's formula,\nand we rigorously prove that both schemes preserve the maximum bound principle\n(MBP) and energy dissipation under minimal regularity requirements. Optimal\nconvergence rates are established for the proposed methods. Numerical\nexperiments validate the theoretical findings, demonstrating that the\neigenvalues of Q remain strictly confined within the physical range\n(-1/3},2/3).", "published": "2025-04-16 00:22:05", "link": "http://arxiv.org/abs/2504.11676v1", "categories": ["math.NA", "cs.NA", "65M06", "G.1.8"], "primary_category": "math.NA"}
{"title": "Universal portfolios in continuous time: a model-free approach", "abstract": "We provide a simple and straightforward approach to a continuous-time version\nof Cover's universal portfolio strategies within the model-free context of\nF\\\"ollmer's pathwise It\\^o calculus. We establish the existence of the\nuniversal portfolio strategy and prove that its portfolio value process is the\naverage of all values of constant rebalanced strategies. This result relies on\na systematic comparison between two alternative descriptions of self-financing\ntrading strategies within pathwise It\\^o calculus. We moreover provide a\ncomparison result for the performance and the realized volatility and variance\nof constant rebalanced portfolio strategies.", "published": "2025-04-16 09:05:53", "link": "http://arxiv.org/abs/2504.11881v1", "categories": ["q-fin.MF", "q-fin.PM"], "primary_category": "q-fin.MF"}
{"title": "A Survey on Archetypal Analysis", "abstract": "Archetypal analysis (AA) was originally proposed in 1994 by Adele Cutler and\nLeo Breiman as a computational procedure to extract the distinct aspects called\narchetypes in observations with each observational record approximated as a\nmixture (i.e., convex combination) of these archetypes. AA thereby provides\nstraightforward, interpretable, and explainable representations for feature\nextraction and dimensionality reduction, facilitating the understanding of the\nstructure of high-dimensional data with wide applications throughout the\nsciences. However, AA also faces challenges, particularly as the associated\noptimization problem is non-convex. This survey provides researchers and data\nmining practitioners an overview of methodologies and opportunities that AA has\nto offer surveying the many applications of AA across disparate fields of\nscience, as well as best practices for modeling data using AA and limitations.\nThe survey concludes by explaining important future research directions\nconcerning AA.", "published": "2025-04-16 18:01:05", "link": "http://arxiv.org/abs/2504.12392v1", "categories": ["stat.ME", "cs.LG", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Resonances in reflective Hamiltonian Monte Carlo", "abstract": "In high dimensions, reflective Hamiltonian Monte Carlo with inexact\nreflections exhibits slow mixing when the particle ensemble is initialised from\na Dirac delta distribution and the uniform distribution is targeted. By\nquantifying the instantaneous non-uniformity of the distribution with the\nSinkhorn divergence, we elucidate the principal mechanisms underlying the\nmixing problems. In spheres and cubes, we show that the collective motion\ntransitions between fluid-like and discretisation-dominated behaviour, with the\ncritical step size scaling as a power law in the dimension. In both regimes,\nthe particles can spontaneously unmix, leading to resonances in the particle\ndensity and the aforementioned problems. Additionally, low-dimensional toy\nmodels of the dynamics are constructed which reproduce the dominant features of\nthe high-dimensional problem. Finally, the dynamics is contrasted with the\nexact Hamiltonian particle flow and tuning practices are discussed.", "published": "2025-04-16 18:00:00", "link": "http://arxiv.org/abs/2504.12374v1", "categories": ["stat.ML", "cond-mat.stat-mech", "cs.LG", "math.DS"], "primary_category": "stat.ML"}
{"title": "Trend Filtered Mixture of Experts for Automated Gating of High-Frequency Flow Cytometry Data", "abstract": "Ocean microbes are critical to both ocean ecosystems and the global climate.\nFlow cytometry, which measures cell optical properties in fluid samples, is\nroutinely used in oceanographic research. Despite decades of accumulated data,\nidentifying key microbial populations (a process known as ``gating'') remains a\nsignificant analytical challenge. To address this, we focus on gating\nmultidimensional, high-frequency flow cytometry data collected {\\it\ncontinuously} on board oceanographic research vessels, capturing time- and\nspace-wise variations in the dynamic ocean. Our paper proposes a novel\nmixture-of-experts model in which both the gating function and the experts are\ngiven by trend filtering. The model leverages two key assumptions: (1) Each\nsnapshot of flow cytometry data is a mixture of multivariate Gaussians and (2)\nthe parameters of these Gaussians vary smoothly over time. Our method uses\nregularization and a constraint to ensure smoothness and that cluster means\nmatch biologically distinct microbe types. We demonstrate, using flow cytometry\ndata from the North Pacific Ocean, that our proposed model accurately matches\nhuman-annotated gating and corrects significant errors.", "published": "2025-04-16 17:51:59", "link": "http://arxiv.org/abs/2504.12287v1", "categories": ["stat.ME", "stat.AP", "stat.ML", "62H30 (Primary) 62G08, 92B10, 62J07 (Secondary)"], "primary_category": "stat.ME"}
{"title": "Leave-One-Out Stable Conformal Prediction", "abstract": "Conformal prediction (CP) is an important tool for distribution-free\npredictive uncertainty quantification. Yet, a major challenge is to balance\ncomputational efficiency and prediction accuracy, particularly for multiple\npredictions. We propose Leave-One-Out Stable Conformal Prediction (LOO-StabCP),\na novel method to speed up full conformal using algorithmic stability without\nsample splitting. By leveraging leave-one-out stability, our method is much\nfaster in handling a large number of prediction requests compared to existing\nmethod RO-StabCP based on replace-one stability. We derived stability bounds\nfor several popular machine learning tools: regularized loss minimization (RLM)\nand stochastic gradient descent (SGD), as well as kernel method, neural\nnetworks and bagging. Our method is theoretically justified and demonstrates\nsuperior numerical performance on synthetic and real-world data. We applied our\nmethod to a screening problem, where its effective exploitation of training\ndata led to improved test power compared to state-of-the-art method based on\nsplit conformal.", "published": "2025-04-16 15:44:24", "link": "http://arxiv.org/abs/2504.12189v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Approximation Bounds for Transformer Networks with Application to Regression", "abstract": "We explore the approximation capabilities of Transformer networks for\nH\\\"older and Sobolev functions, and apply these results to address\nnonparametric regression estimation with dependent observations. First, we\nestablish novel upper bounds for standard Transformer networks approximating\nsequence-to-sequence mappings whose component functions are H\\\"older continuous\nwith smoothness index $\\gamma \\in (0,1]$. To achieve an approximation error\n$\\varepsilon$ under the $L^p$-norm for $p \\in [1, \\infty]$, it suffices to use\na fixed-depth Transformer network whose total number of parameters scales as\n$\\varepsilon^{-d_x n / \\gamma}$. This result not only extends existing findings\nto include the case $p = \\infty$, but also matches the best known upper bounds\non number of parameters previously obtained for fixed-depth FNNs and RNNs.\nSimilar bounds are also derived for Sobolev functions. Second, we derive\nexplicit convergence rates for the nonparametric regression problem under\nvarious $\\beta$-mixing data assumptions, which allow the dependence between\nobservations to weaken over time. Our bounds on the sample complexity impose no\nconstraints on weight magnitudes. Lastly, we propose a novel proof strategy to\nestablish approximation bounds, inspired by the Kolmogorov-Arnold\nrepresentation theorem. We show that if the self-attention layer in a\nTransformer can perform column averaging, the network can approximate\nsequence-to-sequence H\\\"older functions, offering new insights into the\ninterpretability of self-attention mechanisms.", "published": "2025-04-16 15:25:58", "link": "http://arxiv.org/abs/2504.12175v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Predictive Multiplicity in Survival Models: A Method for Quantifying Model Uncertainty in Predictive Maintenance Applications", "abstract": "In many applications, especially those involving prediction, models may yield\nnear-optimal performance yet significantly disagree on individual-level\noutcomes. This phenomenon, known as predictive multiplicity, has been formally\ndefined in binary, probabilistic, and multi-target classification, and\nundermines the reliability of predictive systems. However, its implications\nremain unexplored in the context of survival analysis, which involves\nestimating the time until a failure or similar event while properly handling\ncensored data. We frame predictive multiplicity as a critical concern in\nsurvival-based models and introduce formal measures -- ambiguity, discrepancy,\nand obscurity -- to quantify it. This is particularly relevant for downstream\ntasks such as maintenance scheduling, where precise individual risk estimates\nare essential. Understanding and reporting predictive multiplicity helps build\ntrust in models deployed in high-stakes environments. We apply our methodology\nto benchmark datasets from predictive maintenance, extending the notion of\nmultiplicity to survival models. Our findings show that ambiguity steadily\nincreases, reaching up to 40-45% of observations; discrepancy is lower but\nexhibits a similar trend; and obscurity remains mild and concentrated in a few\nmodels. These results demonstrate that multiple accurate survival models may\nyield conflicting estimations of failure risk and degradation progression for\nthe same equipment. This highlights the need to explicitly measure and\ncommunicate predictive multiplicity to ensure reliable decision-making in\nprocess health management.", "published": "2025-04-16 15:04:00", "link": "http://arxiv.org/abs/2504.12156v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Proximal Inference on Population Intervention Indirect Effect", "abstract": "The population intervention indirect effect (PIIE) is a novel mediation\neffect representing the indirect component of the population intervention\neffect. Unlike traditional mediation measures, such as the natural indirect\neffect, the PIIE holds particular relevance in observational studies involving\nunethical exposures, when hypothetical interventions that impose harmful\nexposures are inappropriate. Although prior research has identified PIIE under\nunmeasured confounders between exposure and outcome, it has not fully addressed\nthe confounding that affects the mediator. This study extends the PIIE\nidentification to settings where unmeasured confounders influence\nexposure-outcome, exposure-mediator, and mediator-outcome relationships.\nSpecifically, we leverage observed covariates as proxy variables for unmeasured\nconfounders, constructing three proximal identification frameworks.\nAdditionally, we characterize the semiparametric efficiency bound and develop\nmultiply robust and locally efficient estimators. To handle high-dimensional\nnuisance parameters, we propose a debiased machine learning approach that\nachieves $\\sqrt{n}$-consistency and asymptotic normality to estimate the true\nPIIE values, even when the machine learning estimators for the nuisance\nfunctions do not converge at $\\sqrt{n}$-rate. In simulations, our estimators\ndemonstrate higher confidence interval coverage rates than conventional methods\nacross various model misspecifications. In a real data application, our\napproaches reveal an indirect effect of alcohol consumption on depression risk\nmediated by depersonalization symptoms.", "published": "2025-04-16 08:14:55", "link": "http://arxiv.org/abs/2504.11848v1", "categories": ["stat.ME", "math.ST", "stat.ML", "stat.TH"], "primary_category": "stat.ME"}
{"title": "Support is All You Need for Certified VAE Training", "abstract": "Variational Autoencoders (VAEs) have become increasingly popular and deployed\nin safety-critical applications. In such applications, we want to give\ncertified probabilistic guarantees on performance under adversarial attacks. We\npropose a novel method, CIVET, for certified training of VAEs. CIVET depends on\nthe key insight that we can bound worst-case VAE error by bounding the error on\ncarefully chosen support sets at the latent layer. We show this point\nmathematically and present a novel training algorithm utilizing this insight.\nWe show in an extensive evaluation across different datasets (in both the\nwireless and vision application areas), architectures, and perturbation\nmagnitudes that our method outperforms SOTA methods achieving good standard\nperformance with strong robustness guarantees.", "published": "2025-04-16 07:41:40", "link": "http://arxiv.org/abs/2504.11831v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Discrimination-free Insurance Pricing with Privatized Sensitive Attributes", "abstract": "Fairness has emerged as a critical consideration in the landscape of machine\nlearning algorithms, particularly as AI continues to transform decision-making\nacross societal domains. To ensure that these algorithms are free from bias and\ndo not discriminate against individuals based on sensitive attributes such as\ngender and race, the field of algorithmic bias has introduced various fairness\nconcepts, along with methodologies to achieve these notions in different\ncontexts. Despite the rapid advancement, not all sectors have embraced these\nfairness principles to the same extent. One specific sector that merits\nattention in this regard is insurance. Within the realm of insurance pricing,\nfairness is defined through a distinct and specialized framework. Consequently,\nachieving fairness according to established notions does not automatically\nensure fair pricing in insurance. In particular, regulators are increasingly\nemphasizing transparency in pricing algorithms and imposing constraints on\ninsurance companies on the collection and utilization of sensitive consumer\nattributes. These factors present additional challenges in the implementation\nof fairness in pricing algorithms. To address these complexities and comply\nwith regulatory demands, we propose an efficient method for constructing fair\nmodels that are tailored to the insurance domain, using only privatized\nsensitive attributes. Notably, our approach ensures statistical guarantees,\ndoes not require direct access to sensitive attributes, and adapts to varying\ntransparency requirements, addressing regulatory demands while ensuring\nfairness in insurance pricing.", "published": "2025-04-16 05:29:11", "link": "http://arxiv.org/abs/2504.11775v1", "categories": ["stat.ML", "cs.CY", "cs.LG", "q-fin.RM"], "primary_category": "stat.ML"}
{"title": "Benchmarking Audio Deepfake Detection Robustness in Real-world Communication Scenarios", "abstract": "Existing Audio Deepfake Detection (ADD) systems often struggle to generalise\neffectively due to the significantly degraded audio quality caused by audio\ncodec compression and channel transmission effects in real-world communication\nscenarios. To address this challenge, we developed a rigorous benchmark to\nevaluate ADD system performance under such scenarios. We introduced ADD-C, a\nnew test dataset to evaluate the robustness of ADD systems under diverse\ncommunication conditions, including different combinations of audio codecs for\ncompression and Packet Loss Rates (PLR). Benchmarking on three baseline ADD\nmodels with the ADD-C dataset demonstrated a significant decline in robustness\nunder such conditions. A novel data augmentation strategy was proposed to\nimprove the robustness of ADD systems. Experimental results demonstrated that\nthe proposed approach increases the performance of ADD systems significantly\nwith the proposed ADD-C dataset. Our benchmark can assist future efforts\ntowards building practical and robustly generalisable ADD systems.", "published": "2025-04-16 18:44:05", "link": "http://arxiv.org/abs/2504.12423v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "An accurate measurement of parametric array using a spurious sound filter topologically equivalent to a half-wavelength resonator", "abstract": "Parametric arrays (PA) offer exceptional directivity and compactness compared\nto conventional loudspeakers, facilitating various acoustic applications.\nHowever, accurate measurement of audio signals generated by PA remains\nchallenging due to spurious ultrasonic sounds arising from microphone\nnonlinearities. Existing filtering methods, including Helmholtz resonators,\nphononic crystals, polymer films, and grazing incidence techniques, exhibit\npractical constraints such as size limitations, fabrication complexity, or\ninsufficient attenuation. To address these issues, we propose and demonstrate a\nnovel acoustic filter based on the design of a half-wavelength resonator. The\ndeveloped filter exploits the nodal plane in acoustic pressure distribution,\neffectively minimizing microphone exposure to targeted ultrasonic frequencies.\nFabrication via stereolithography (SLA) 3D printing ensures high dimensional\naccuracy, which is crucial for high-frequency acoustic filters. Finite element\nmethod (FEM) simulations guided filter optimization for suppression frequencies\nat 40 kHz and 60 kHz, achieving high transmission loss (TL) around 60 dB.\nExperimental validations confirm the filter's superior performance in\nsignificantly reducing spurious acoustic signals, as reflected in frequency\nresponse, beam pattern, and propagation curve measurements. The proposed filter\nensures stable and precise acoustic characterization, independent of\nmeasurement distances and incidence angles. This new approach not only improves\nmeasurement accuracy but also enhances reliability and reproducibility in\nparametric array research and development.", "published": "2025-04-16 18:04:26", "link": "http://arxiv.org/abs/2504.12398v1", "categories": ["cs.SD", "eess.AS", "physics.app-ph"], "primary_category": "cs.SD"}
{"title": "Voice Conversion with Diverse Intonation using Conditional Variational Auto-Encoder", "abstract": "Voice conversion is a task of synthesizing an utterance with target speaker's\nvoice while maintaining linguistic information of the source utterance. While a\nspeaker can produce varying utterances from a single script with different\nintonations, conventional voice conversion models were limited to producing\nonly one result per source input. To overcome this limitation, we propose a\nnovel approach for voice conversion with diverse intonations using conditional\nvariational autoencoder (CVAE). Experiments have shown that the speaker's style\nfeature can be mapped into a latent space with Gaussian distribution. We have\nalso been able to convert voices with more diverse intonation by making the\nposterior of the latent space more complex with inverse autoregressive flow\n(IAF). As a result, the converted voice not only has a diversity of\nintonations, but also has better sound quality than the model without CVAE.", "published": "2025-04-16 11:59:56", "link": "http://arxiv.org/abs/2504.12005v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Attention-Infused Autoencoder for Massive MIMO CSI Compression", "abstract": "As the number of multiple-input multiple-output (MIMO) antennas increases\ndrastically with the development towards 6G systems, channel state information\n(CSI) compression becomes crucial for mitigating feedback overhead. In recent\nyears, learning models such as autoencoders (AE) have been studied for CSI\ncompression, aiming to eliminate model assumptions and reduce compression loss.\nHowever, current learning methods are often designed and trained mainly for\nindividual channel scenarios, with limited generalizability across different\nscenarios, of which the channel characteristics are prominently discrepant.\nMotivated by this, we propose a novel AE-based learning method named\nattention-infused autoencoder network (AiANet), which can parallelly and\nadaptively extract channel-wise and spatial features of CSI with an attention\nfusion mechanism. In addition, a locally-aware self-attention mechanism is\ndeveloped to extract both global and local spatial patterns, to better capture\nthe unique CSI features of different scenarios. Moreover, a mixed-training\nscheme is introduced to enable the proposed AiANet to gain generalizability\nacross indoor and outdoor scenarios. Results show that when trained and tested\nin the same scenario, AiANet can substantially outperform the existing AE-based\nmethods such as ACRNet, with an improvement of up to 3.42 dB in terms of\nnormalized mean squared error (NMSE). With the mixed-training scheme, AiANet\nexhibits superior cross-scenario generalizability compared to the benchmark\nmethods which are trained in one scenario and misused in another.", "published": "2025-04-16 19:13:58", "link": "http://arxiv.org/abs/2504.12440v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Experimental Analysis of Multipath Characteristics in Indoor Distributed Massive MIMO Channels", "abstract": "Distributed massive multiple-input multiple-output (MIMO), also known as\ncell-free massive MIMO, has emerged as a promising technology for\nsixth-generation (6G) wireless networks. This letter introduces an indoor\nchannel measurement campaign designed to explore the behavior of multipath\ncomponents (MPCs) in distributed massive MIMO channels. Fully coherent channels\nwere measured between eight distributed uniform planar arrays (128 elements in\ntotal) and a 12-meter user equipment route. Furthermore, a method is introduced\nto determine the order (single- or multi-bounce) of MPC interaction by\nleveraging map information and MPC parameters. In addition, a Kalman\nfilter-based framework is used for identifying the MPC interaction mechanisms\n(reflection or scattering/diffraction/mixed). Finally, a comprehensive\nMPC-level characterization is performed based on the measured channels,\nincluding the significance of the single-bounce MPCs, the spherical wavefront\nfeatures, the birth-and-death processes of the MPCs, and the spatial\ndistribution of reflections. The findings serve as a valuable reference for\nunderstanding MPC propagation behavior, which is necessary for accurate\nmodeling of indoor distributed massive MIMO channels.", "published": "2025-04-16 17:08:12", "link": "http://arxiv.org/abs/2504.12258v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Mobile Distributed MIMO (MD-MIMO) for NextG: Mobility Meets Cooperation in Distributed Arrays", "abstract": "Distributed multiple-input multiple-output (D\\mbox{-}MIMO) is a promising\ntechnology to realize the promise of massive MIMO gains by fiber-connecting the\ndistributed antenna arrays, thereby overcoming the form factor limitations of\nco-located MIMO. In this paper, we introduce the concept of mobile D-MIMO\n(MD-MIMO) network, a further extension of the D-MIMO technology where\ndistributed antenna arrays are connected to the base station with a wireless\nlink allowing all radio network nodes to be mobile. This approach significantly\nimproves deployment flexibility and reduces operating costs, enabling the\nnetwork to adapt to the highly dynamic nature of next-generation (NextG)\nnetworks. We discuss use cases, system design, network architecture, and the\nkey enabling technologies for MD-MIMO. Furthermore, we investigate a case study\nof MD-MIMO for vehicular networks, presenting detailed performance evaluations\nfor both downlink and uplink. The results show that an MD-MIMO network can\nprovide substantial improvements in network throughput and reliability.", "published": "2025-04-16 16:48:29", "link": "http://arxiv.org/abs/2504.12244v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Interacting Object-Enabled Clustering and Characterization of Distributed MIMO Channels", "abstract": "Distributed multiple-input multiple-output (MIMO), also known as cell-free\nmassive MIMO, emerges as a promising technology for sixth-generation (6G)\nsystems to support uniform coverage and reliable communication. For the design\nand optimization of such systems, measurement-based investigations of\nreal-world distributed MIMO channels are essential. In this paper, we present\nan indoor channel measurement campaign, featuring eight distributed antenna\narrays with 128 elements in total. Multi-link channels are measured at 50\npositions along a 12-meter user route. A clustering algorithm enabled by\ninteracting objects is proposed to identify clusters in the measured channels.\nThe algorithm jointly clusters the multipath components for all links,\neffectively capturing the dynamic contributions of common clusters to different\nlinks. In addition, a Kalman filter-based tracking framework is introduced for\ncluster prediction, tracking, and updating along the user movement. Using the\nclustering and tracking results, cluster-level characterization of the measured\nchannels is performed. First, the number of clusters and their visibility at\nboth link ends are analyzed. Next, a maximum-likelihood estimator is utilized\nto determine the entire cluster visibility region length. Finally, key\ncluster-level properties, including the common cluster ratio, cluster power,\nshadowing, spread, among others, are statistically investigated. The results\nprovide valuable insights into cluster behavior in typical multi-link channels,\nnecessary for accurate modeling of distributed MIMO channels.", "published": "2025-04-16 16:13:33", "link": "http://arxiv.org/abs/2504.12220v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Deep Generative Models for Bayesian Inference on High-Rate Sensor Data: Applications in Automotive Radar and Medical Imaging", "abstract": "Deep generative models have been studied and developed primarily in the\ncontext of natural images and computer vision. This has spurred the development\nof (Bayesian) methods that use these generative models for inverse problems in\nimage restoration, such as denoising, inpainting, and super-resolution. In\nrecent years, generative modeling for Bayesian inference on sensory data has\nalso gained traction. Nevertheless, the direct application of generative\nmodeling techniques initially designed for natural images on raw sensory data\nis not straightforward, requiring solutions that deal with high dynamic range\nsignals acquired from multiple sensors or arrays of sensors that interfere with\neach other, and that typically acquire data at a very high rate. Moreover, the\nexact physical data-generating process is often complex or unknown. As a\nconsequence, approximate models are used, resulting in discrepancies between\nmodel predictions and the observations that are non-Gaussian, in turn\ncomplicating the Bayesian inverse problem. Finally, sensor data is often used\nin real-time processing or decision-making systems, imposing stringent\nrequirements on, e.g., latency and throughput. In this paper, we will discuss\nsome of these challenges and offer approaches to address them, all in the\ncontext of high-rate real-time sensing applications in automotive radar and\nmedical imaging.", "published": "2025-04-16 15:03:01", "link": "http://arxiv.org/abs/2504.12154v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "The CAM Model: An in vivo Testbed for Molecular Communication Systems", "abstract": "Molecular communication (MC) research increasingly focuses on biomedical\napplications like health monitoring and drug delivery, demanding testing in\nrealistic living environments. Elevating MC research requires developing\nadvanced in vivo testbeds. We introduce the chorioallantoic membrane (CAM)\nmodel as the first versatile 3D in vivo MC platform. The CAM, a highly\nvascularized membrane in fertilized chicken eggs, is established in\nbioengineering, cancer research, and drug development. Its biological realism,\nreproducibility, and versatility make it ideal for next-generation MC testbeds,\nbridging proof-of-concept systems and practical applications. We\ncomprehensively characterize the CAM model's properties and MC system\nrelevance. Through experimental studies, we investigate fluorescent molecule\ndistribution in the CAM's closed-loop vascular system. We derive an analytical\nmodel using the wrapped normal distribution to describe particle propagation in\ndispersive closed-loop systems dominated by diffusion and flow. Parametric\nmodels are developed to approximate particle dynamics in the CAM, with\nparameters estimated via nonlinear least squares curve fitting. A dataset of 69\nregions from 25 eggs validates our models. We analyze parameter relationships\nand biological plausibility. Finally, we develop a parametric model for\nlong-term particle behavior and liver accumulation in chick embryos.", "published": "2025-04-16 14:35:53", "link": "http://arxiv.org/abs/2504.12123v1", "categories": ["eess.SY", "cs.SY", "eess.SP"], "primary_category": "eess.SY"}
{"title": "Mind2Matter: Creating 3D Models from EEG Signals", "abstract": "The reconstruction of 3D objects from brain signals has gained significant\nattention in brain-computer interface (BCI) research. Current research\npredominantly utilizes functional magnetic resonance imaging (fMRI) for 3D\nreconstruction tasks due to its excellent spatial resolution. Nevertheless, the\nclinical utility of fMRI is limited by its prohibitive costs and inability to\nsupport real-time operations. In comparison, electroencephalography (EEG)\npresents distinct advantages as an affordable, non-invasive, and mobile\nsolution for real-time brain-computer interaction systems. While recent\nadvances in deep learning have enabled remarkable progress in image generation\nfrom neural data, decoding EEG signals into structured 3D representations\nremains largely unexplored. In this paper, we propose a novel framework that\ntranslates EEG recordings into 3D object reconstructions by leveraging neural\ndecoding techniques and generative models. Our approach involves training an\nEEG encoder to extract spatiotemporal visual features, fine-tuning a large\nlanguage model to interpret these features into descriptive multimodal outputs,\nand leveraging generative 3D Gaussians with layout-guided control to synthesize\nthe final 3D structures. Experiments demonstrate that our model captures\nsalient geometric and semantic features, paving the way for applications in\nbrain-computer interfaces (BCIs), virtual reality, and neuroprosthetics.Our\ncode is available in https://github.com/sddwwww/Mind2Matter.", "published": "2025-04-16 10:16:03", "link": "http://arxiv.org/abs/2504.11936v1", "categories": ["cs.GR", "cs.HC", "eess.SP"], "primary_category": "cs.GR"}
{"title": "Super-LoRa: Enhancing LoRa Throughput via Payload Superposition", "abstract": "This paper presents Super-LoRa, a novel approach to enhancing the throughput\nof LoRa networks by leveraging the inherent robustness of LoRa modulation\nagainst interference. By superimposing multiple payload symbols, Super-LoRa\nsignificantly increases the data rate while maintaining lower transmitter and\nreceiver complexity. Our solution is evaluated through both simulations and\nreal-world experiments, showing a potential throughput improvement of up to 5x\ncompared to standard LoRa. This advancement positions Super-LoRa as a viable\nsolution for data-intensive IoT applications such as smart cities and precision\nagriculture, which demand higher data transmission rates.", "published": "2025-04-16 10:02:38", "link": "http://arxiv.org/abs/2504.11927v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "A Novel Splitter Design for RSMA Networks", "abstract": "Rate splitting multiple access (RSMA) has firmly established itself as a\npowerful methodology for multiple access, interference management, and\nmulti-user strategy for next-generation communication systems. In this paper,\nwe propose a novel channel-dependent splitter design for multi-carrier RSMA\nsystems, aimed at improving reliability performance. Specifically, the proposed\nsplitter leverages channel state information and the inherent structure of RSMA\nto intelligently replicate segments of the private stream data that are likely\nto encounter deep-faded subchannels into the common stream. Thus, the\nreliability is enhanced within the same transmission slot, minimizing the need\nfor frequent retransmissions and thereby reducing latency. To assess the\neffectiveness of our approach, we conduct comprehensive evaluations using key\nperformance metrics, including achievable sum rate, average packet delay, and\nbit error rate (BER), under both perfect and imperfect channel estimation\nscenarios.", "published": "2025-04-16 09:30:02", "link": "http://arxiv.org/abs/2504.11905v1", "categories": ["eess.SP", "cs.SY", "eess.SY"], "primary_category": "eess.SP"}
{"title": "A Novel Approach to Secure RSMA Networks", "abstract": "This letter introduces a novel data-dependent interleaving technique designed\nto enhance the security of rate-splitting multiple access (RSMA) networks by\nprotecting the common stream from eavesdropping threats. Specifically, we\nexploit the RSMA structure by interleaving the common bits of each user based\non a sequence derived from their private bits. By decoding its private stream,\nthe legitimate receiver reconstructs the interleaving sequence set by the\ntransmitter and successfully de-interleaves the common stream. Therefore, the\ncommon part is successfully prevented from being intercepted by an eavesdropper\nwho is unable to deduce the dynamic changing interleaving permutations. To\nensure dynamic interleaving sequences, a private bit selection approach that\nbalances the trade-off between security and system efficiency is proposed.\nSimulation findings confirm the effectiveness of the suggested method, showing\nnotable security improvements while maintaining robust overall system\nreliability.", "published": "2025-04-16 08:59:10", "link": "http://arxiv.org/abs/2504.11878v1", "categories": ["eess.SP", "cs.SY", "eess.SY"], "primary_category": "eess.SP"}
{"title": "Channel-Adaptive Robust Resource Allocation for Highly Reliable IRS-Assisted V2X Communications", "abstract": "This paper addresses the challenges of resource allocation in vehicular\nnetworks enhanced by Intelligent Reflecting Surfaces (IRS), considering the\nuncertain Channel State Information (CSI) typical of vehicular environments due\nto the Doppler shift. Leveraging the 3GPP's Mode 1 cellular V2X architecture,\nour system model facilitates efficient subcarrier usage and interference\nreduction through coordinated V2I and V2V communications. Each Cellular User\nEquipment (CUE) shares its spectrum with at most one Vehicular User Equipment\n(VUE) in a one-to-one reuse pattern. We formulate a joint optimization problem\nfor vehicular transmit power, Multi-User Detection (MUD) matrices, V2V link\nspectrum reuse, and IRS reflection coefficients in IRS-aided V2X communication\nwith imperfect CSI. To tackle this, a novel robust resource allocation\nalgorithm is developed by first decomposing the problem into manageable\nsub-problems such as power allocation, MUD matrices optimization and IRS phase\nshifts, and then using the Block Coordinate Descent (BCD) method to alternately\noptimize these subproblems for optimal resource allocation. Our contributions\ninclude efficient approaches for self-learning based power allocation and phase\nshift optimization that adapt to CSI uncertainties, significantly enhancing the\nreliability and efficiency of vehicular communications. Simulation results\nvalidate the effectiveness of the proposed solutions in improving the Quality\nof Service (QoS) and managing the complex interference inherent in dense\nvehicular networks.", "published": "2025-04-16 08:48:53", "link": "http://arxiv.org/abs/2504.11871v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Scalable Multi-task Edge Sensing via Task-oriented Joint Information Gathering and Broadcast", "abstract": "The recent advance of edge computing technology enables significant sensing\nperformance improvement of Internet of Things (IoT) networks. In particular, an\nedge server (ES) is responsible for gathering sensing data from distributed\nsensing devices, and immediately executing different sensing tasks to\naccommodate the heterogeneous service demands of mobile users. However, as the\nnumber of users surges and the sensing tasks become increasingly\ncompute-intensive, the huge amount of computation workloads and data\ntransmissions may overwhelm the edge system of limited resources. Accordingly,\nwe propose in this paper a scalable edge sensing framework for multi-task\nexecution, in the sense that the computation workload and communication\noverhead of the ES do not increase with the number of downstream users or\ntasks. By exploiting the task-relevant correlations, the proposed scheme\nimplements a unified encoder at the ES, which produces a common low-dimensional\nmessage from the sensing data and broadcasts it to all users to execute their\nindividual tasks. To achieve high sensing accuracy, we extend the well-known\ninformation bottleneck theory to a multi-task scenario to jointly optimize the\ninformation gathering and broadcast processes. We also develop an efficient\ntwo-step training procedure to optimize the parameters of the neural\nnetwork-based codecs deployed in the edge sensing system. Experiment results\nshow that the proposed scheme significantly outperforms the considered\nrepresentative benchmark methods in multi-task inference accuracy. Besides, the\nproposed scheme is scalable to the network size, which maintains almost\nconstant computation delay with less than 1% degradation of inference\nperformance when the user number increases by four times.", "published": "2025-04-16 08:06:46", "link": "http://arxiv.org/abs/2504.11843v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Properties of Near Field Focusing for Cylindrical Dipole Arrays in Enclosed Array Volume", "abstract": "Motivated by large intelligent surface applications, the electric field\nproperties of near field focusing using phase conjugation method are analyzed\nfor cylindrical dipole arrays. Firstly, for the transmitting antennas featuring\nvertical polarization, the polarization characteristic is decomposed along the\nx, y, and z directions. Three typical cases are studied when the focal points\nare at (xf , 0, 0), (0, yf , 0), and (0, 0, zf ). When the length of the\ncylindrical dipole array is significantly larger compared to its radius,\ncertain unique insights emerge. When the focal point is positioned along (0, 0,\nzf ), apart from the region on both sides, the ratio between Ez and Ex/Ey\nremains {\\pi}/2. Additionally, When the focal point is located within the\ncylinder, the electric field of each polarization is approximately the same\neverywhere. In other words, beam focusing does not incur losses due to\ndifferent positions. The focusing resolution of Ez is the same in the\ntransverse and longitudinal directions. Different from the situation where the\n3 - dB focal beam depth is much smaller than the focal beam width for the most\nof arrays, the resolution in the longitudinal can be improved, respectively.\nThrough a comprehensive grasp of these design principles, we can gain a deeper\nunderstanding of the specific areas with significant potential for practical\napplications.", "published": "2025-04-16 07:16:33", "link": "http://arxiv.org/abs/2504.11822v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Virtual VNA 3.1: Non-Coherent-Detection-Based Non-Reciprocal Scattering Matrix Estimation Leveraging a Tunable Load Network", "abstract": "We refine the recently introduced \"Virtual VNA 3.0\" technique to remove the\nneed for coherent detection. The resulting \"Virtual VNA 3.1\" technique can\nunambiguously estimate the full scattering matrix of a non-reciprocal, linear,\npassive, time-invariant device under test (DUT) with $N$ monomodal ports using\nan $N_\\mathrm{A}$-channel coherent wavefront generator and an\n$N_\\mathrm{A}$-channel non-coherent detector, where $N_\\mathrm{A}<N$. Waves are\ninjected and received only via a fixed set of $N_\\mathrm{A}$ \"accessible\" DUT\nports while the remaining $N_\\mathrm{S}$ \"not-directly-accessible\" DUT ports\nare terminated by a specific tunable load network. To resolve all ambiguities,\nan additional modified setup is required in which waves are injected and\nreceived via a known $2N_\\mathrm{A}$-port system connected to the DUT's\naccessible ports. We experimentally validate our method for\n$N_\\mathrm{A}=N_\\mathrm{S}=4$ considering a non-reciprocal eight-port circuit\nas DUT. By eliminating the need for coherent detection, our work reduces the\nhardware complexity which may facilitate applications to large-scale or\nhigher-frequency systems. Additionally, our work provides fundamental insights\ninto the minimal requirements to fully and unambiguously characterize a\nnon-reciprocal DUT.", "published": "2025-04-16 05:42:51", "link": "http://arxiv.org/abs/2504.11790v1", "categories": ["physics.app-ph", "eess.SP"], "primary_category": "physics.app-ph"}
{"title": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G Networks", "abstract": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments.", "published": "2025-04-16 03:07:07", "link": "http://arxiv.org/abs/2504.11729v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "ESC-MVQ: End-to-End Semantic Communication With Multi-Codebook Vector Quantization", "abstract": "This paper proposes a novel end-to-end digital semantic communication\nframework based on multi-codebook vector quantization (VQ), referred to as\nESC-MVQ. Unlike prior approaches that rely on end-to-end training with a\nspecific power or modulation scheme, often under a particular channel\ncondition, ESC-MVQ models a channel transfer function as parallel binary\nsymmetric channels (BSCs) with trainable bit-flip probabilities. Building on\nthis model, ESC-MVQ jointly trains multiple VQ codebooks and their associated\nbit-flip probabilities with a single encoder-decoder pair. To maximize\ninference performance when deploying ESC-MVQ in digital communication systems,\nwe devise an optimal communication strategy that jointly optimizes codebook\nassignment, adaptive modulation, and power allocation. To this end, we develop\nan iterative algorithm that selects the most suitable VQ codebook for semantic\nfeatures and flexibly allocates power and modulation schemes across the\ntransmitted symbols. Simulation results demonstrate that ESC-MVQ, using a\nsingle encoder-decoder pair, outperforms existing digital semantic\ncommunication methods in both performance and memory efficiency, offering a\nscalable and adaptive solution for realizing digital semantic communication in\ndiverse channel conditions.", "published": "2025-04-16 02:12:57", "link": "http://arxiv.org/abs/2504.11709v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models via Plot Hole Detection", "abstract": "Stories are a fundamental aspect of human experience. Engaging deeply with\nstories and spotting plot holes -- inconsistencies in a storyline that break\nthe internal logic or rules of a story's world -- requires nuanced reasoning\nskills, including tracking entities and events and their interplay, abstract\nthinking, pragmatic narrative understanding, commonsense and social reasoning,\nand theory of mind. As Large Language Models (LLMs) increasingly generate,\ninterpret, and modify text, rigorously assessing their narrative consistency\nand deeper language understanding becomes critical. However, existing\nbenchmarks focus mainly on surface-level comprehension. In this work, we\npropose plot hole detection in stories as a proxy to evaluate language\nunderstanding and reasoning in LLMs. We introduce FlawedFictionsMaker, a novel\nalgorithm to controllably and carefully synthesize plot holes in human-written\nstories. Using this algorithm, we construct a benchmark to evaluate LLMs' plot\nhole detection abilities in stories -- FlawedFictions -- , which is robust to\ncontamination, with human filtering ensuring high quality. We find that\nstate-of-the-art LLMs struggle in accurately solving FlawedFictions regardless\nof the reasoning effort allowed, with performance significantly degrading as\nstory length increases. Finally, we show that LLM-based story summarization and\nstory generation are prone to introducing plot holes, with more than 50% and\n100% increases in plot hole detection rates with respect to human-written\noriginals.", "published": "2025-04-16 09:25:54", "link": "http://arxiv.org/abs/2504.11900v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dense Backpropagation Improves Training for Sparse Mixture-of-Experts", "abstract": "Mixture of Experts (MoE) pretraining is more scalable than dense Transformer\npretraining, because MoEs learn to route inputs to a sparse set of their\nfeedforward parameters. However, this means that MoEs only receive a sparse\nbackward update, leading to training instability and suboptimal performance. We\npresent a lightweight approximation method that gives the MoE router a dense\ngradient update while continuing to sparsely activate its parameters. Our\nmethod, which we refer to as Default MoE, substitutes missing expert\nactivations with default outputs consisting of an exponential moving average of\nexpert outputs previously seen over the course of training. This allows the\nrouter to receive signals from every expert for each token, leading to\nsignificant improvements in training performance. Our Default MoE outperforms\nstandard TopK routing in a variety of settings without requiring significant\ncomputational overhead. Code: https://github.com/vatsal0/default-moe.", "published": "2025-04-16 19:55:36", "link": "http://arxiv.org/abs/2504.12463v2", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Enumeration of Bases in Matroid with Exponentially Large Ground Set", "abstract": "When we deal with a matroid ${\\mathcal M}=(U,{\\mathcal I})$, we usually\nassume that it is implicitly given by means of the membership (MEM) oracle.\nTime complexity of many existing algorithms is polynomially bounded with\nrespect to $|U|$ and the running time of the MEM-oracle. However, they are not\nefficient any more when $U$ is exponentially large in some context. In this\npaper, we propose two algorithms for enumerating matroid bases such that the\ntime complexity does not depend on $|U|$. The first algorithm enumerates all\nminimum-weighted bases in incremental-polynomial time. To design the algorithm,\nwe assume two oracles other than the MEM-oracle: the MinB-oracle that returns a\nminimum basis and the REL-oracle that returns a relevant element one by one in\nnon-decreasing order of weight. The proposed algorithm is applicable to\nenumeration of minimum bases of binary matroids from cycle space and cut space,\nall of which have exponentially large $U$ with respect to a given graph. The\nhighlight in this context is that, to design the REL-oracle for cut space, we\ndevelop the first polynomial-delay algorithm that enumerates all relevant cuts\nof a given graph in non-decreasing order of weight. The second algorithm\nenumerates all sets of linearly independent $r$-dimensional $r$ vectors over\n$\\mathit{GF}(2)$ in polynomial-delay, which immediately yields a\npolynomial-delay algorithm with respect to the matroid rank $r$ that enumerates\nall unweighted bases of a binary matroid such that elements are closed under\naddition.", "published": "2025-04-16 03:05:39", "link": "http://arxiv.org/abs/2504.11728v2", "categories": ["cs.DS", "cs.DM", "G.2.2"], "primary_category": "cs.DS"}
{"title": "Fast Mixed-Precision Real Evaluation", "abstract": "Evaluating real-valued expressions to high precision is a key building block\nin computational mathematics, physics, and numerics. A typical implementation\nevaluates the whole expression in a uniform precision, doubling that precision\nuntil a sufficiently-accurate result is achieved. This is wasteful: usually\nonly a few operations really need to be performed at high precision, and the\nbulk of the expression could be computed much faster. However, such non-uniform\nprecision assignments have, to date, been impractical to compute. We propose a\nfast new algorithm for deriving such precision assignments. The algorithm\nleverages results computed at lower precisions to analytically determine a\nmixed-precision assignment that will result in a sufficiently-accurate result.\nOur implementation, Reval, achieves an average speed-up of 1.72x compared to\nthe state-of-the-art Sollya tool, with the speed-up increasing to 5.21x on the\nmost difficult input points. An examination of the precisions used with and\nwithout precision tuning shows that the speed-up results from assigning lower\nprecisions for the majority of operations, though additional optimizations\nenabled by the non-uniform precision assignments also play a role.", "published": "2025-04-16 02:12:20", "link": "http://arxiv.org/abs/2504.11708v2", "categories": ["math.NA", "cs.MS", "cs.NA"], "primary_category": "math.NA"}
{"title": "Universal portfolios in continuous time: an approach in pathwise It\u00f4 calculus", "abstract": "We provide a simple and straightforward approach to a continuous-time version\nof Cover's universal portfolio strategies within the model-free context of\nF\\\"ollmer's pathwise It\\^o calculus. We establish the existence of the\nuniversal portfolio strategy and prove that its portfolio value process is the\naverage of all values of constant rebalanced strategies. This result relies on\na systematic comparison between two alternative descriptions of self-financing\ntrading strategies within pathwise It\\^o calculus. We moreover provide a\ncomparison result for the performance and the realized volatility and variance\nof constant rebalanced portfolio strategies", "published": "2025-04-16 09:05:53", "link": "http://arxiv.org/abs/2504.11881v2", "categories": ["q-fin.MF", "q-fin.PM"], "primary_category": "q-fin.MF"}
{"title": "Mind2Matter: Creating 3D Models from EEG Signals", "abstract": "The reconstruction of 3D objects from brain signals has gained significant\nattention in brain-computer interface (BCI) research. Current research\npredominantly utilizes functional magnetic resonance imaging (fMRI) for 3D\nreconstruction tasks due to its excellent spatial resolution. Nevertheless, the\nclinical utility of fMRI is limited by its prohibitive costs and inability to\nsupport real-time operations. In comparison, electroencephalography (EEG)\npresents distinct advantages as an affordable, non-invasive, and mobile\nsolution for real-time brain-computer interaction systems. While recent\nadvances in deep learning have enabled remarkable progress in image generation\nfrom neural data, decoding EEG signals into structured 3D representations\nremains largely unexplored. In this paper, we propose a novel framework that\ntranslates EEG recordings into 3D object reconstructions by leveraging neural\ndecoding techniques and generative models. Our approach involves training an\nEEG encoder to extract spatiotemporal visual features, fine-tuning a large\nlanguage model to interpret these features into descriptive multimodal outputs,\nand leveraging generative 3D Gaussians with layout-guided control to synthesize\nthe final 3D structures. Experiments demonstrate that our model captures\nsalient geometric and semantic features, paving the way for applications in\nbrain-computer interfaces (BCIs), virtual reality, and neuroprosthetics. Our\ncode is available in https://github.com/sddwwww/Mind2Matter.", "published": "2025-04-16 10:16:03", "link": "http://arxiv.org/abs/2504.11936v2", "categories": ["cs.GR", "cs.HC", "eess.SP"], "primary_category": "cs.GR"}
{"title": "Maximum bound principle for Q-tensor gradient flow with low regularity integrators", "abstract": "We investigate low-regularity integrator (LRI) methods for the Q-tensor model\ngoverning nematic liquid-crystalline semilinear parabolic equation. First- and\nsecond-order temporal discretizations are developed using Duhamel's formula,\nand we rigorously prove that both schemes preserve the maximum bound principle\n(MBP) and energy dissipation under minimal regularity requirements. Optimal\nconvergence rates are established for the proposed methods. Numerical\nexperiments validate the theoretical findings, demonstrating that the\neigenvalues of Q remain strictly confined within the physical range\n(-1/3},2/3).", "published": "2025-04-16 00:22:05", "link": "http://arxiv.org/abs/2504.11676v2", "categories": ["math.NA", "cs.NA", "65M06", "G.1.8"], "primary_category": "math.NA"}
{"title": "Lagrangian finite elements in Sobolev-like spaces of order $3/2$", "abstract": "This paper introduces a Sobolev-like space of order $3/2$, denoted as\n$\\widehat{H}^{3/2}$, for Lagrangian finite elements, especially for $C^0$\nelements. It is motivated by the limitations of current stability analysis of\nthe evolving surface finite element method (ESFEM), which relies exclusively on\nan energy estimate framework. To establish a PDE-based analysis framework for\nESFEM, we encounter a fundamental regularity mismatch: the ESFEM adopts the\n$C^0$ elements, while the PDE regularity theory requires $H^{3/2}$ regularity\nfor solutions. To overcome this difficulty, we first examine the properties of\nthe continuous $H^{3/2}$ space, then introduce a Dirichlet lift and Scott-Zhang\ntype interpolation operators to bridge to the discrete $\\widehat{H}^{3/2}$\nspace. Our new $\\widehat{H}^{3/2}$ space is shown to be compatible with the\nelliptic PDE regularity theory, the trace inequality, and the inverse\ninequality. Notably, we extend the critical domain deformation estimate in\nESFEM to the $\\widehat{H}^{3/2}$ setting. The $\\widehat{H}^{3/2}$ theory\nprovides a foundation for establishing a PDE-based convergence analysis\nframework of ESFEM.", "published": "2025-04-16 09:56:43", "link": "http://arxiv.org/abs/2504.11920v2", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Set families: restricted distances via restricted intersections", "abstract": "Denote by $f_D(n)$ the maximum size of a set family $\\mathcal{F}$ on $[n] =\n\\{1, \\dots, n\\}$ with distance set $D$. That is, $|A \\bigtriangleup B| \\in D$\nholds for every pair of distinct sets $A, B \\in \\mathcal{F}$. Kleitman's\ncelebrated discrete isodiametric inequality states that $f_D(n)$ is maximized\nat Hamming balls of radius $d/2$ when $D = \\{1, \\dots, d\\}$. We study the\ngeneralization where $D$ is a set of arithmetic progression and determine\n$f_D(n)$ asymptotically for all homogeneous $D$. In the special case when $D$\nis an interval, our result confirms a conjecture of Huang, Klurman, and\nPohoata. Moreover, we demonstrate a dichotomy in the growth of $f_D(n)$,\nshowing linear growth in $n$ when $D$ is a non-homogeneous arithmetic\nprogression. Different from previous combinatorial and spectral approaches, we\ndeduce our results by converting the restricted distance problems to restricted\nintersection problems.\n  Our proof ideas can be adapted to prove upper bounds on $t$-distance sets in\nHamming cubes (also known as binary $t$-codes), which has been extensively\nstudied by algebraic combinatorialists community, improving previous bounds\nfrom polynomial methods and optimization approaches.", "published": "2025-04-16 17:56:56", "link": "http://arxiv.org/abs/2504.12296v2", "categories": ["math.CO", "cs.DM", "05D05 (primary), 94B25, 05C35, 52C10"], "primary_category": "math.CO"}
{"title": "Clarifying Ambiguities: on the Role of Ambiguity Types in Prompting Methods for Clarification Generation", "abstract": "In information retrieval (IR), providing appropriate clarifications to better\nunderstand users' information needs is crucial for building a proactive\nsearch-oriented dialogue system. Due to the strong in-context learning ability\nof large language models (LLMs), recent studies investigate prompting methods\nto generate clarifications using few-shot or Chain of Thought (CoT) prompts.\nHowever, vanilla CoT prompting does not distinguish the characteristics of\ndifferent information needs, making it difficult to understand how LLMs resolve\nambiguities in user queries. In this work, we focus on the concept of ambiguity\nfor clarification, seeking to model and integrate ambiguities in the\nclarification process. To this end, we comprehensively study the impact of\nprompting schemes based on reasoning and ambiguity for clarification. The idea\nis to enhance the reasoning abilities of LLMs by limiting CoT to predict first\nambiguity types that can be interpreted as instructions to clarify, then\ncorrespondingly generate clarifications. We name this new prompting scheme\nAmbiguity Type-Chain of Thought (AT-CoT). Experiments are conducted on various\ndatasets containing human-annotated clarifying questions to compare AT-CoT with\nmultiple baselines. We also perform user simulations to implicitly measure the\nquality of generated clarifications under various IR scenarios.", "published": "2025-04-16 14:21:02", "link": "http://arxiv.org/abs/2504.12113v2", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Traveling wave profiles for a semi-discrete Burgers equation", "abstract": "We look for traveling waves of the semi-discrete conservation law $4\\dot u_j\n+u_{j+1}^2-u_{j-1}^2 = 0$, using variational principles related to concepts of\n``hidden convexity'' appearing in recent studies of various PDE (partial\ndifferential equations). We analyze and numerically compute with two\nvariational formulations related to dual convex optimization problems\nconstrained by either the differential-difference equation (DDE) or nonlinear\nintegral equation (NIE) that wave profiles should satisfy. We prove existence\ntheorems conditional on the existence of extrema that satisfy a strict\nconvexity criterion, and numerically exhibit a variety of localized, periodic\nand non-periodic wave phenomena.", "published": "2025-04-16 15:23:43", "link": "http://arxiv.org/abs/2504.12171v2", "categories": ["math.AP", "cs.NA", "math.NA", "nlin.PS", "Primary: 49J35, 49M29 Secondary: 34K31, 70G75, 35Q70"], "primary_category": "math.AP"}
{"title": "Dysarthria Normalization via Local Lie Group Transformations for Robust ASR", "abstract": "We present a geometry-driven method for normalizing dysarthric speech by\nmodeling time, frequency, and amplitude distortions as smooth, local Lie group\ntransformations of spectrograms. Scalar fields generate these deformations via\nexponential maps, and a neural network is trained - using only synthetically\nwarped healthy speech - to infer the fields and apply an approximate inverse at\ntest time. We introduce a spontaneous-symmetry-breaking (SSB) potential that\nencourages the model to discover non-trivial field configurations. On real\npathological speech, the system delivers consistent gains: up to 17\npercentage-point WER reduction on challenging TORGO utterances and a 16 percent\ndrop in WER variance, with no degradation on clean CommonVoice data. Character\nand phoneme error rates improve in parallel, confirming linguistic relevance.\nOur results demonstrate that geometrically structured warping provides\nconsistent, zero-shot robustness gains for dysarthric ASR.", "published": "2025-04-16 17:41:19", "link": "http://arxiv.org/abs/2504.12279v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Midsummer Meme's Dream: Investigating Market Manipulations in the Meme Coin Ecosystem", "abstract": "From viral jokes to a billion-dollar phenomenon, meme coins have become one\nof the most popular segments in cryptocurrency markets. Unlike utility-focused\ncrypto assets like Bitcoin or Ethereum, meme coins derive value primarily from\ncommunity sentiment, making them vulnerable to manipulation. This study\npresents a cross-chain analysis of the meme coin ecosystem, examining 34,988\ntokens across Ethereum, BNB Smart Chain, Solana, and Base. We characterize the\ntokenomics of meme coins and track their growth in a three-month longitudinal\nanalysis. We discover that among high-return tokens (>100%), an alarming 82.6%\nshow evidence of extensive use of artificial growth strategies designed to\ncreate a misleading appearance of market interest. These include wash trading\nand a form of manipulation we define as Liquidity Pool-Based Price Inflation\n(LPI), where small strategic purchases trigger dramatic price increases. We\nalso find evidence of schemes designed to profit at the expense of investors,\nsuch as pump and dumps and rug pulls. In particular, most of the tokens\ninvolved had previously experienced wash trading or LPI, indicating how initial\nmanipulations often set the stage for later exploitation. These findings reveal\nthat manipulations are widespread among high-performing meme coins and suggest\nthat their dramatic gains are often likely driven by coordinated efforts rather\nthan natural market dynamics.", "published": "2025-04-16 13:54:42", "link": "http://arxiv.org/abs/2507.01963v1", "categories": ["q-fin.TR", "cs.CY", "q-fin.ST"], "primary_category": "q-fin.TR"}
