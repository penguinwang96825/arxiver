{"title": "Improving Quality and Efficiency in Plan-based Neural Data-to-Text\n  Generation", "abstract": "We follow the step-by-step approach to neural data-to-text generation we\nproposed in Moryossef et al (2019), in which the generation process is divided\ninto a text-planning stage followed by a plan-realization stage. We suggest\nfour extensions to that framework: (1) we introduce a trainable neural planning\ncomponent that can generate effective plans several orders of magnitude faster\nthan the original planner; (2) we incorporate typing hints that improve the\nmodel's ability to deal with unseen relations and entities; (3) we introduce a\nverification-by-reranking stage that substantially improves the faithfulness of\nthe resulting texts; (4) we incorporate a simple but effective referring\nexpression generation module. These extensions result in a generation process\nthat is faster, more fluent, and more accurate.", "published": "2019-09-22 11:41:53", "link": "http://arxiv.org/abs/1909.09986v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving OOV Detection and Resolution with External Language Models in\n  Acoustic-to-Word ASR", "abstract": "Acoustic-to-word (A2W) end-to-end automatic speech recognition (ASR) systems\nhave attracted attention because of an extremely simplified architecture and\nfast decoding. To alleviate data sparseness issues due to infrequent words, the\ncombination with an acoustic-to-character (A2C) model is investigated.\nMoreover, the A2C model can be used to recover out-of-vocabulary (OOV) words\nthat are not covered by the A2W model, but this requires accurate detection of\nOOV words. A2W models learn contexts with both acoustic and transcripts;\ntherefore they tend to falsely recognize OOV words as words in the vocabulary.\nIn this paper, we tackle this problem by using external language models (LM),\nwhich are trained only with transcriptions and have better linguistic\ninformation to detect OOV words. The A2C model is used to resolve these OOV\nwords. Experimental evaluations show that external LMs have the effects of not\nonly reducing errors but also increasing the number of detected OOV words, and\nthe proposed method significantly improves performances in English\nconversational and Japanese lecture corpora, especially for out-of-domain\nscenario. We also investigate the impact of the vocabulary size of A2W models\nand the data size for training LMs. Moreover, our approach can reduce the\nvocabulary size several times with marginal performance degradation.", "published": "2019-09-22 12:41:05", "link": "http://arxiv.org/abs/1909.09993v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is change the only constant? Profile change perspective on\n  #LokSabhaElections2019", "abstract": "Users on Twitter are identified with the help of their profile attributes\nthat consists of username, display name, profile image, to name a few. The\nprofile attributes that users adopt can reflect their interests, belief, or\nthematic inclinations. Literature has proposed the implications and\nsignificance of profile attribute change for a random population of users.\nHowever, the use of profile attribute for endorsements and to start a movement\nhave been under-explored. In this work, we consider #LokSabhaElections2019 as a\nmovement and perform a large-scale study of the profile of users who actively\nmade changes to profile attributes centered around #LokSabhaElections2019. We\ncollect the profile metadata for 49.4M users for a period of 2 months from\nApril 5, 2019 to June 5, 2019 amid #LokSabhaElections2019. We investigate how\nthe profile changes vary for the influential leaders and their followers over\nthe social movement. We further differentiate the organic and inorganic ways to\nshow the political inclination from the prism of profile changes. We report how\nthe addition of election campaign related keywords lead to spread of behavior\ncontagion and further investigate it with respect to \"Chowkidar Movement\" in\ndetail.", "published": "2019-09-22 14:17:50", "link": "http://arxiv.org/abs/1909.10012v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inducing Constituency Trees through Neural Machine Translation", "abstract": "Latent tree learning(LTL) methods learn to parse sentences using only\nindirect supervision from a downstream task. Recent advances in latent tree\nlearning have made it possible to recover moderately high quality tree\nstructures by training with language modeling or auto-encoding objectives. In\nthis work, we explore the hypothesis that decoding in machine translation, as a\nconditional language modeling task, will produce better tree structures since\nit offers a similar training signal as language modeling, but with more\nsemantic signal. We adapt two existing latent-tree language models--PRPN\nandON-LSTM--for use in translation. We find that they indeed recover trees that\nare better in F1 score than those seen in language modeling on WSJ test set,\nwhile maintaining strong translation quality. We observe that translation is a\nbetter objective than language modeling for inducing trees, marking the first\nsuccess at latent tree learning using a machine translation objective.\nAdditionally, our findings suggest that, although translation provides better\nsignal for inducing trees than language modeling, translation models can\nperform well without exploiting the latent tree structure.", "published": "2019-09-22 18:01:36", "link": "http://arxiv.org/abs/1909.10056v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Algorithms for certain classes of Tamil Spelling correction", "abstract": "Tamil language has an agglutinative, diglossic, alpha-syllabary structure\nwhich provides a significant combinatorial explosion of morphological forms all\nof which are effectively used in Tamil prose, poetry from antiquity to the\nmodern age in an unbroken chain of continuity. However, for the language\nunderstanding, spelling correction purposes some of these present challenges as\nout-of-dictionary words. In this paper the authors propose algorithmic\ntechniques to handle specific problems of conjoined-words (out-of-dictionary)\n(transliteration)[thendRalkattRu] = [thendRal]+[kattRu] when parts are alone\npresent in word-list in efficient way. Morphological structure of Tamil makes\nit necessary to depend on synthesis-analysis approach and dictionary lists will\nnever be sufficient to truly capture the language. In this paper we have\nattempted to make a summary of various known algorithms for specific classes of\nTamil spelling errors. We believe this collection of suggestions to improve\nfuture spelling checkers. We also note do not cover many important techniques\nlike affix removal and other such techniques of key importance in rule-based\nspell checkers.", "published": "2019-09-22 18:24:41", "link": "http://arxiv.org/abs/1909.10063v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Structured Neural Network for Event Temporal Relation Extraction", "abstract": "We propose a novel deep structured learning framework for event temporal\nrelation extraction. The model consists of 1) a recurrent neural network (RNN)\nto learn scoring functions for pair-wise relations, and 2) a structured support\nvector machine (SSVM) to make joint predictions. The neural network\nautomatically learns representations that account for long-term contexts to\nprovide robust features for the structured model, while the SSVM incorporates\ndomain knowledge such as transitive closure of temporal relations as\nconstraints to make better globally consistent decisions. By jointly training\nthe two components, our model combines the benefits of both data-driven\nlearning and knowledge exploitation. Experimental results on three high-quality\nevent temporal relation datasets (TCR, MATRES, and TB-Dense) demonstrate that\nincorporated with pre-trained contextualized embeddings, the proposed model\nachieves significantly better performances than the state-of-the-art methods on\nall three datasets. We also provide thorough ablation studies to investigate\nour model.", "published": "2019-09-22 21:11:08", "link": "http://arxiv.org/abs/1909.10094v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KnowBias: Detecting Political Polarity in Long Text Content", "abstract": "We introduce a classification scheme for detecting political bias in long\ntext content such as newspaper opinion articles. Obtaining long text data and\nannotations at sufficient scale for training is difficult, but it is relatively\neasy to extract political polarity from tweets through their authorship. We\ntrain on tweets and perform inference on articles. Universal sentence encoders\nand other existing methods that aim to address this domain-adaptation scenario\ndeliver inaccurate and inconsistent predictions on articles, which we show is\ndue to a difference in opinion concentration between tweets and articles. We\npropose a two-step classification scheme that uses a neutral detector trained\non tweets to remove neutral sentences from articles in order to align opinion\nconcentration and therefore improve accuracy on that domain. Our implementation\nis available for public use at https://knowbias.ml.", "published": "2019-09-22 20:19:29", "link": "http://arxiv.org/abs/1909.12230v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting Language Models for Non-Parallel Author-Stylized Rewriting", "abstract": "Given the recent progress in language modeling using Transformer-based neural\nmodels and an active interest in generating stylized text, we present an\napproach to leverage the generalization capabilities of a language model to\nrewrite an input text in a target author's style. Our proposed approach adapts\na pre-trained language model to generate author-stylized text by fine-tuning on\nthe author-specific corpus using a denoising autoencoder (DAE) loss in a\ncascaded encoder-decoder framework. Optimizing over DAE loss allows our model\nto learn the nuances of an author's style without relying on parallel data,\nwhich has been a severe limitation of the previous related works in this space.\nTo evaluate the efficacy of our approach, we propose a linguistically-motivated\nframework to quantify stylistic alignment of the generated text to the target\nauthor at lexical, syntactic and surface levels. The evaluation framework is\nboth interpretable as it leads to several insights about the model, and\nself-contained as it does not rely on external classifiers, e.g. sentiment or\nformality classifiers. Qualitative and quantitative assessment indicates that\nthe proposed approach rewrites the input text with better alignment to the\ntarget style while preserving the original content better than state-of-the-art\nbaselines.", "published": "2019-09-22 08:13:28", "link": "http://arxiv.org/abs/1909.09962v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using Chinese Glyphs for Named Entity Recognition", "abstract": "Most Named Entity Recognition (NER) systems use additional features like\npart-of-speech (POS) tags, shallow parsing, gazetteers, etc. Such kind of\ninformation requires external knowledge like unlabeled texts and trained\ntaggers. Adding these features to NER systems have been shown to have a\npositive impact. However, sometimes creating gazetteers or taggers can take a\nlot of time and may require extensive data cleaning. In this paper for Chinese\nNER systems, we do not use these traditional features but we use lexicographic\nfeatures of Chinese characters. Chinese characters are composed of graphical\ncomponents called radicals and these components often have some semantic\nindicators. We propose CNN based models that incorporate this semantic\ninformation and use them for NER. Our models show an improvement over the\nbaseline BERT-BiLSTM-CRF model. We set a new baseline score for Chinese\nOntoNotes v5.0 and show an improvement of +.64 F1 score. We present a\nstate-of-the-art F1 score on Weibo dataset of 71.81 and show a competitive\nimprovement of +0.72 over baseline on ResumeNER dataset.", "published": "2019-09-22 01:12:18", "link": "http://arxiv.org/abs/1909.09922v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
