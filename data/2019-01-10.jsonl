{"title": "Sentence Rewriting for Semantic Parsing", "abstract": "A major challenge of semantic parsing is the vocabulary mismatch problem\nbetween natural language and target ontology. In this paper, we propose a\nsentence rewriting based semantic parsing method, which can effectively resolve\nthe mismatch problem by rewriting a sentence into a new form which has the same\nstructure with its target logical form. Specifically, we propose two\nsentence-rewriting methods for two common types of mismatch: a dictionary-based\nmethod for 1-N mismatch and a template-based method for N-1 mismatch. We\nevaluate our entence rewriting based semantic parser on the benchmark semantic\nparsing dataset -- WEBQUESTIONS. Experimental results show that our system\noutperforms the base system with a 3.4% gain in F1, and generates logical forms\nmore accurately and parses sentences more robustly.", "published": "2019-01-10 02:23:18", "link": "http://arxiv.org/abs/1901.02998v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Equalizing Gender Biases in Neural Machine Translation with Word\n  Embeddings Techniques", "abstract": "Neural machine translation has significantly pushed forward the quality of\nthe field. However, there are remaining big issues with the output translations\nand one of them is fairness. Neural models are trained on large text corpora\nwhich contain biases and stereotypes. As a consequence, models inherit these\nsocial biases. Recent methods have shown results in reducing gender bias in\nother natural language processing tools such as word embeddings. We take\nadvantage of the fact that word embeddings are used in neural machine\ntranslation to propose a method to equalize gender biases in neural machine\ntranslation using these representations. Specifically, we propose, experiment\nand analyze the integration of two debiasing techniques over GloVe embeddings\nin the Transformer translation architecture. We evaluate our proposed system on\nthe WMT English-Spanish benchmark task, showing gains up to one BLEU point. As\nfor the gender bias evaluation, we generate a test set of occupations and we\nshow that our proposed system learns to equalize existing biases from the\nbaseline system.", "published": "2019-01-10 12:06:31", "link": "http://arxiv.org/abs/1901.03116v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotion Detection using Data Driven Models", "abstract": "Text is the major method that is used for communication now a days, each and\nevery day lots of text are created. In this paper the text data is used for the\nclassification of the emotions. Emotions are the way of expression of the\npersons feelings which has an high influence on the decision making tasks.\nDatasets are collected which are available publically and combined together\nbased on the three emotions that are considered here positive, negative and\nneutral. In this paper we have proposed the text representation method TFIDF\nand keras embedding and then given to the classical machine learning algorithms\nof which Logistics Regression gives the highest accuracy of about 75.6%, after\nwhich it is passed to the deep learning algorithm which is the CNN which gives\nthe state of art accuracy of about 45.25%. For the research purpose the\ndatasets that has been collected are released.", "published": "2019-01-10 13:15:46", "link": "http://arxiv.org/abs/1901.03141v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reverse-Engineering Satire, or \"Paper on Computational Humor Accepted\n  Despite Making Serious Advances\"", "abstract": "Humor is an essential human trait. Efforts to understand humor have called\nout links between humor and the foundations of cognition, as well as the\nimportance of humor in social engagement. As such, it is a promising and\nimportant subject of study, with relevance for artificial intelligence and\nhuman-computer interaction. Previous computational work on humor has mostly\noperated at a coarse level of granularity, e.g., predicting whether an entire\nsentence, paragraph, document, etc., is humorous. As a step toward deep\nunderstanding of humor, we seek fine-grained models of attributes that make a\ngiven text humorous. Starting from the observation that satirical news\nheadlines tend to resemble serious news headlines, we build and analyze a\ncorpus of satirical headlines paired with nearly identical but serious\nheadlines. The corpus is constructed via Unfun.me, an online game that\nincentivizes players to make minimal edits to satirical headlines with the goal\nof making other players believe the results are serious headlines. The edit\noperations used to successfully remove humor pinpoint the words and concepts\nthat play a key role in making the original, satirical headline funny. Our\nanalysis reveals that the humor tends to reside toward the end of headlines,\nand primarily in noun phrases, and that most satirical headlines follow a\ncertain logical pattern, which we term false analogy. Overall, this paper\ndeepens our understanding of the syntactic and semantic structure of satirical\nnews headlines and provides insights for building humor-producing systems.", "published": "2019-01-10 16:25:53", "link": "http://arxiv.org/abs/1901.03253v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation", "abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following\nnavigational instruction in photo-realistic unknown environments. This\nchallenging task demands that the agent be aware of which instruction was\ncompleted, which instruction is needed next, which way to go, and its\nnavigation progress towards the goal. In this paper, we introduce a\nself-monitoring agent with two complementary components: (1) visual-textual\nco-grounding module to locate the instruction completed in the past, the\ninstruction required for the next action, and the next moving direction from\nsurrounding images and (2) progress monitor to ensure the grounded instruction\ncorrectly reflects the navigation progress. We test our self-monitoring agent\non a standard benchmark and analyze our proposed approach through a series of\nablation studies that elucidate the contributions of the primary components.\nUsing our proposed method, we set the new state of the art by a significant\nmargin (8% absolute increase in success rate on the unseen test set). Code is\navailable at https://github.com/chihyaoma/selfmonitoring-agent .", "published": "2019-01-10 06:46:50", "link": "http://arxiv.org/abs/1901.03035v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Cosine-similarity penalty to discriminate sound classes in\n  weakly-supervised sound event detection", "abstract": "The design of new methods and models when only weakly-labeled data are\navailable is of paramount importance in order to reduce the costs of manual\nannotation and the considerable human effort associated with it. In this work,\nwe address Sound Event Detection in the case where a weakly annotated dataset\nis available for training. The weak annotations provide tags of audio events\nbut do not provide temporal boundaries. The objective is twofold: 1) audio\ntagging, i.e. multi-label classification at recording level, 2) sound event\ndetection, i.e. localization of the event boundaries within the recordings.\nThis work focuses mainly on the second objective. We explore an approach\ninspired by Multiple Instance Learning, in which we train a convolutional\nrecurrent neural network to give predictions at frame-level, using a custom\nloss function based on the weak labels and the statistics of the frame-based\npredictions. Since some sound classes cannot be distinguished with this\napproach, we improve the method by penalizing similarity between the\npredictions of the positive classes during training. On the test set used in\nthe DCASE 2018 challenge, consisting of 288 recordings and 10 sound classes,\nthe addition of a penalty resulted in a localization F-score of 34.75%, and\nbrought 10% relative improvement compared to not using the penalty. Our best\nmodel achieved a 26.20% F-score on the DCASE-2018 official Eval subset close to\nthe 10-system ensemble approach that ranked second in the challenge with a\n29.9% F-score.", "published": "2019-01-10 13:21:06", "link": "http://arxiv.org/abs/1901.03146v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Data Augmentation of Room Classifiers using Generative Adversarial\n  Networks", "abstract": "The classification of acoustic environments allows for machines to better\nunderstand the auditory world around them. The use of deep learning in order to\nteach machines to discriminate between different rooms is a new area of\nresearch. Similarly to other learning tasks, this task suffers from the\nhigh-dimensionality and the limited availability of training data. Data\naugmentation methods have proven useful in addressing this issue in the tasks\nof sound event detection and scene classification. This paper proposes a method\nfor data augmentation for the task of room classification from reverberant\nspeech. Generative Adversarial Networks (GANs) are trained that generate\nartificial data as if they were measured in real rooms. This provides\nadditional training examples to the classifiers without the need for any\nadditional data collection, which is time-consuming and often impractical. A\nrepresentation of acoustic environments is proposed, which is used to train the\nGANs. The representation is based on a sparse model for the early reflections,\na stochastic model for the reverberant tail and a mixing mechanism between the\ntwo. In the experiments shown, the proposed data augmentation method increases\nthe test accuracy of a CNN-RNN room classifier from 89.4% to 95.5%.", "published": "2019-01-10 16:34:15", "link": "http://arxiv.org/abs/1901.03257v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
