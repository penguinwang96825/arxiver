{"title": "Convolutional Neural Network with Word Embeddings for Chinese Word\n  Segmentation", "abstract": "Character-based sequence labeling framework is flexible and efficient for\nChinese word segmentation (CWS). Recently, many character-based neural models\nhave been applied to CWS. While they obtain good performance, they have two\nobvious weaknesses. The first is that they heavily rely on manually designed\nbigram feature, i.e. they are not good at capturing n-gram features\nautomatically. The second is that they make no use of full word information.\nFor the first weakness, we propose a convolutional neural model, which is able\nto capture rich n-gram features without any feature engineering. For the second\none, we propose an effective approach to integrate the proposed model with word\nembeddings. We evaluate the model on two benchmark datasets: PKU and MSR.\nWithout any feature engineering, the model obtains competitive performance --\n95.7% on PKU and 97.3% on MSR. Armed with word embeddings, the model achieves\nstate-of-the-art performance on both datasets -- 96.5% on PKU and 98.0% on MSR,\nwithout using any external labeled resource.", "published": "2017-11-13 03:47:52", "link": "http://arxiv.org/abs/1711.04411v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word, Subword or Character? An Empirical Study of Granularity in\n  Chinese-English NMT", "abstract": "Neural machine translation (NMT), a new approach to machine translation, has\nbeen proved to outperform conventional statistical machine translation (SMT)\nacross a variety of language pairs. Translation is an open-vocabulary problem,\nbut most existing NMT systems operate with a fixed vocabulary, which causes the\nincapability of translating rare words. This problem can be alleviated by using\ndifferent translation granularities, such as character, subword and hybrid\nword-character. Translation involving Chinese is one of the most difficult\ntasks in machine translation, however, to the best of our knowledge, there has\nnot been any other work exploring which translation granularity is most\nsuitable for Chinese in NMT. In this paper, we conduct an extensive comparison\nusing Chinese-English NMT as a case study. Furthermore, we discuss the\nadvantages and disadvantages of various translation granularities in detail.\nOur experiments show that subword model performs best for Chinese-to-English\ntranslation with the vocabulary which is not so big while hybrid word-character\nmodel is most suitable for English-to-Chinese translation. Moreover,\nexperiments of different granularities show that Hybrid_BPE method can achieve\nbest result on Chinese-to-English translation task.", "published": "2017-11-13 07:42:56", "link": "http://arxiv.org/abs/1711.04457v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating prose style transfer with the Bible", "abstract": "In the prose style transfer task a system, provided with text input and a\ntarget prose style, produces output which preserves the meaning of the input\ntext but alters the style. These systems require parallel data for evaluation\nof results and usually make use of parallel data for training. Currently, there\nare few publicly available corpora for this task. In this work, we identify a\nhigh-quality source of aligned, stylistically distinct text in different\nversions of the Bible. We provide a standardized split, into training,\ndevelopment and testing data, of the public domain versions in our corpus. This\ncorpus is highly parallel since many Bible versions are included. Sentences are\naligned due to the presence of chapter and verse numbers within all versions of\nthe text. In addition to the corpus, we present the results, as measured by the\nBLEU and PINC metrics, of several models trained on our data which can serve as\nbaselines for future research. While we present these data as a style transfer\ncorpus, we believe that it is of unmatched quality and may be useful for other\nnatural language tasks as well.", "published": "2017-11-13 17:59:26", "link": "http://arxiv.org/abs/1711.04731v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QuickEdit: Editing Text & Translations by Crossing Words Out", "abstract": "We propose a framework for computer-assisted text editing. It applies to\ntranslation post-editing and to paraphrasing. Our proposal relies on very\nsimple interactions: a human editor modifies a sentence by marking tokens they\nwould like the system to change. Our model then generates a new sentence which\nreformulates the initial sentence by avoiding marked words. The approach builds\nupon neural sequence-to-sequence modeling and introduces a neural network which\ntakes as input a sentence along with change markers. Our model is trained on\ntranslation bitext by simulating post-edits. We demonstrate the advantage of\nour approach for translation post-editing through simulated post-edits. We also\nevaluate our model for paraphrasing through a user study.", "published": "2017-11-13 19:23:47", "link": "http://arxiv.org/abs/1711.04805v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Faithful to the Original: Fact Aware Neural Abstractive Summarization", "abstract": "Unlike extractive summarization, abstractive summarization has to fuse\ndifferent parts of the source text, which inclines to create fake facts. Our\npreliminary study reveals nearly 30% of the outputs from a state-of-the-art\nneural summarization system suffer from this problem. While previous\nabstractive summarization approaches usually focus on the improvement of\ninformativeness, we argue that faithfulness is also a vital prerequisite for a\npractical abstractive summarization system. To avoid generating fake facts in a\nsummary, we leverage open information extraction and dependency parse\ntechnologies to extract actual fact descriptions from the source text. The\ndual-attention sequence-to-sequence framework is then proposed to force the\ngeneration conditioned on both the source text and the extracted fact\ndescriptions. Experiments on the Gigaword benchmark dataset demonstrate that\nour model can greatly reduce fake summaries by 80%. Notably, the fact\ndescriptions also bring significant improvement on informativeness since they\noften condense the meaning of the source text.", "published": "2017-11-13 06:34:29", "link": "http://arxiv.org/abs/1711.04434v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "SQLNet: Generating Structured Queries From Natural Language Without\n  Reinforcement Learning", "abstract": "Synthesizing SQL queries from natural language is a long-standing open\nproblem and has been attracting considerable interest recently. Toward solving\nthe problem, the de facto approach is to employ a sequence-to-sequence-style\nmodel. Such an approach will necessarily require the SQL queries to be\nserialized. Since the same SQL query may have multiple equivalent\nserializations, training a sequence-to-sequence-style model is sensitive to the\nchoice from one of them. This phenomenon is documented as the \"order-matters\"\nproblem. Existing state-of-the-art approaches rely on reinforcement learning to\nreward the decoder when it generates any of the equivalent serializations.\nHowever, we observe that the improvement from reinforcement learning is\nlimited.\n  In this paper, we propose a novel approach, i.e., SQLNet, to fundamentally\nsolve this problem by avoiding the sequence-to-sequence structure when the\norder does not matter. In particular, we employ a sketch-based approach where\nthe sketch contains a dependency graph so that one prediction can be done by\ntaking into consideration only the previous predictions that it depends on. In\naddition, we propose a sequence-to-set model as well as the column attention\nmechanism to synthesize the query based on the sketch. By combining all these\nnovel techniques, we show that SQLNet can outperform the prior art by 9% to 13%\non the WikiSQL task.", "published": "2017-11-13 06:41:29", "link": "http://arxiv.org/abs/1711.04436v1", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Digitising Cultural Complexity: Representing Rich Cultural Data in a Big\n  Data environment", "abstract": "One of the major terminological forces driving ICT integration in research\ntoday is that of \"big data.\" While the phrase sounds inclusive and integrative,\n\"big data\" approaches are highly selective, excluding input that cannot be\neffectively structured, represented, or digitised. Data of this complex sort is\nprecisely the kind that human activity produces, but the technological\nimperative to enhance signal through the reduction of noise does not\naccommodate this richness. Data and the computational approaches that\nfacilitate \"big data\" have acquired a perceived objectivity that belies their\ncurated, malleable, reactive, and performative nature. In an input environment\nwhere anything can \"be data\" once it is entered into the system as \"data,\" data\ncleaning and processing, together with the metadata and information\narchitectures that structure and facilitate our cultural archives acquire a\ncapacity to delimit what data are. This engenders a process of simplification\nthat has major implications for the potential for future innovation within\nresearch environments that depend on rich material yet are increasingly\nmediated by digital technologies. This paper presents the preliminary findings\nof the European-funded KPLEX (Knowledge Complexity) project which investigates\nthe delimiting effect digital mediation and datafication has on rich, complex\ncultural data. The paper presents a systematic review of existing implicit\ndefinitions of data, elaborating on the implications of these definitions and\nhighlighting the ways in which metadata and computational technologies can\nrestrict the interpretative potential of data. It sheds light on the gap\nbetween analogue or augmented digital practices and fully computational ones,\nand the strategies researchers have developed to deal with this gap. The paper\nproposes a reconceptualisation of data as it is functionally employed within\ndigitally-mediated research so as to incorporate and acknowledge the richness\nand complexity of our source materials.", "published": "2017-11-13 07:31:24", "link": "http://arxiv.org/abs/1711.04452v1", "categories": ["cs.CL", "cs.CY", "cs.DB", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Targeted Advertising Based on Browsing History", "abstract": "Audience interest, demography, purchase behavior and other possible\nclassifications are ex- tremely important factors to be carefully studied in a\ntargeting campaign. This information can help advertisers and publishers\ndeliver advertisements to the right audience group. How- ever, it is not easy\nto collect such information, especially for the online audience with whom we\nhave limited interaction and minimum deterministic knowledge. In this paper, we\npro- pose a predictive framework that can estimate online audience demographic\nattributes based on their browsing histories. Under the proposed framework,\nfirst, we retrieve the content of the websites visited by audience, and\nrepresent the content as website feature vectors; second, we aggregate the\nvectors of websites that audience have visited and arrive at feature vectors\nrepresenting the users; finally, the support vector machine is exploited to\npredict the audience demographic attributes. The key to achieving good\nprediction performance is preparing representative features of the audience.\nWord Embedding, a widely used tech- nique in natural language processing tasks,\ntogether with term frequency-inverse document frequency weighting scheme is\nused in the proposed method. This new representation ap- proach is unsupervised\nand very easy to implement. The experimental results demonstrate that the new\naudience feature representation method is more powerful than existing baseline\nmethods, leading to a great improvement in prediction accuracy.", "published": "2017-11-13 10:06:22", "link": "http://arxiv.org/abs/1711.04498v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Phonemic and Graphemic Multilingual CTC Based Speech Recognition", "abstract": "Training automatic speech recognition (ASR) systems requires large amounts of\ndata in the target language in order to achieve good performance. Whereas large\ntraining corpora are readily available for languages like English, there exists\na long tail of languages which do suffer from a lack of resources. One method\nto handle data sparsity is to use data from additional source languages and\nbuild a multilingual system. Recently, ASR systems based on recurrent neural\nnetworks (RNNs) trained with connectionist temporal classification (CTC) have\ngained substantial research interest. In this work, we extended our previous\napproach towards training CTC-based systems multilingually. Our systems feature\na global phone set, based on the joint phone sets of each source language. We\nevaluated the use of different language combinations as well as the addition of\nLanguage Feature Vectors (LFVs). As contrastive experiment, we built systems\nbased on graphemes as well. Systems having a multilingual phone set are known\nto suffer in performance compared to their monolingual counterparts. With our\nproposed approach, we could reduce the gap between these mono- and multilingual\nsetups, using either graphemes or phonemes.", "published": "2017-11-13 13:13:40", "link": "http://arxiv.org/abs/1711.04564v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Multilingual Adaptation of RNN Based ASR Systems", "abstract": "In this work, we focus on multilingual systems based on recurrent neural\nnetworks (RNNs), trained using the Connectionist Temporal Classification (CTC)\nloss function. Using a multilingual set of acoustic units poses difficulties.\nTo address this issue, we proposed Language Feature Vectors (LFVs) to train\nlanguage adaptive multilingual systems. Language adaptation, in contrast to\nspeaker adaptation, needs to be applied not only on the feature level, but also\nto deeper layers of the network. In this work, we therefore extended our\nprevious approach by introducing a novel technique which we call \"modulation\".\nBased on this method, we modulated the hidden layers of RNNs using LFVs. We\nevaluated this approach in both full and low resource conditions, as well as\nfor grapheme and phone based systems. Lower error rates throughout the\ndifferent conditions could be achieved by the use of the modulation.", "published": "2017-11-13 13:22:54", "link": "http://arxiv.org/abs/1711.04569v2", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Audio-to-score alignment of piano music using RNN-based automatic music\n  transcription", "abstract": "We propose a framework for audio-to-score alignment on piano performance that\nemploys automatic music transcription (AMT) using neural networks. Even though\nthe AMT result may contain some errors, the note prediction output can be\nregarded as a learned feature representation that is directly comparable to\nMIDI note or chroma representation. To this end, we employ two recurrent neural\nnetworks that work as the AMT-based feature extractors to the alignment\nalgorithm. One predicts the presence of 88 notes or 12 chroma in frame-level\nand the other detects note onsets in 12 chroma. We combine the two types of\nlearned features for the audio-to-score alignment. For comparability, we apply\ndynamic time warping as an alignment algorithm without any additional\npost-processing. We evaluate the proposed framework on the MAPS dataset and\ncompare it to previous work. The result shows that the alignment framework with\nthe learned features significantly improves the accuracy, achieving less than\n10 ms in mean onset error.", "published": "2017-11-13 09:15:19", "link": "http://arxiv.org/abs/1711.04480v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Blind Source Separation Using Mixtures of Alpha-Stable Distributions", "abstract": "We propose a new blind source separation algorithm based on mixtures of\nalpha-stable distributions. Complex symmetric alpha-stable distributions have\nbeen recently showed to better model audio signals in the time-frequency domain\nthan classical Gaussian distributions thanks to their larger dynamic range.\nHowever, inference of these models is notoriously hard to perform because their\nprobability density functions do not have a closed-form expression in general.\nHere, we introduce a novel method for estimating mixture of alpha-stable\ndistributions based on characteristic function matching. We apply this to the\nblind estimation of binary masks in individual frequency bands from\nmultichannel convolutive audio mixes. We show that the proposed method yields\nbetter separation performance than Gaussian-based binary-masking methods.", "published": "2017-11-13 07:53:35", "link": "http://arxiv.org/abs/1711.04460v3", "categories": ["stat.ML", "cs.SD", "eess.AS"], "primary_category": "stat.ML"}
{"title": "Invariances and Data Augmentation for Supervised Music Transcription", "abstract": "This paper explores a variety of models for frame-based music transcription,\nwith an emphasis on the methods needed to reach state-of-the-art on human\nrecordings. The translation-invariant network discussed in this paper, which\ncombines a traditional filterbank with a convolutional neural network, was the\ntop-performing model in the 2017 MIREX Multiple Fundamental Frequency\nEstimation evaluation. This class of models shares parameters in the\nlog-frequency domain, which exploits the frequency invariance of music to\nreduce the number of model parameters and avoid overfitting to the training\ndata. All models in this paper were trained with supervision by labeled data\nfrom the MusicNet dataset, augmented by random label-preserving pitch-shift\ntransformations.", "published": "2017-11-13 20:47:57", "link": "http://arxiv.org/abs/1711.04845v1", "categories": ["stat.ML", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "stat.ML"}
