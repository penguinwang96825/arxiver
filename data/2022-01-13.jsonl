{"title": "Document-level Relation Extraction with Context Guided Mention\n  Integration and Inter-pair Reasoning", "abstract": "Document-level Relation Extraction (DRE) aims to recognize the relations\nbetween two entities. The entity may correspond to multiple mentions that span\nbeyond sentence boundary. Few previous studies have investigated the mention\nintegration, which may be problematic because coreferential mentions do not\nequally contribute to a specific relation. Moreover, prior efforts mainly focus\non reasoning at entity-level rather than capturing the global interactions\nbetween entity pairs. In this paper, we propose two novel techniques, Context\nGuided Mention Integration and Inter-pair Reasoning (CGM2IR), to improve the\nDRE. Instead of simply applying average pooling, the contexts are utilized to\nguide the integration of coreferential mentions in a weighted sum manner.\nAdditionally, inter-pair reasoning executes an iterative algorithm on the\nentity pair graph, so as to model the interdependency of relations. We evaluate\nour CGM2IR model on three widely used benchmark datasets, namely DocRED, CDR,\nand GDA. Experimental results show that our model outperforms previous\nstate-of-the-art models.", "published": "2022-01-13 08:00:23", "link": "http://arxiv.org/abs/2201.04826v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LARD: Large-scale Artificial Disfluency Generation", "abstract": "Disfluency detection is a critical task in real-time dialogue systems.\nHowever, despite its importance, it remains a relatively unexplored field,\nmainly due to the lack of appropriate datasets. At the same time, existing\ndatasets suffer from various issues, including class imbalance issues, which\ncan significantly affect the performance of the model on rare classes, as it is\ndemonstrated in this paper. To this end, we propose LARD, a method for\ngenerating complex and realistic artificial disfluencies with little effort.\nThe proposed method can handle three of the most common types of disfluencies:\nrepetitions, replacements and restarts. In addition, we release a new\nlarge-scale dataset with disfluencies that can be used on four different tasks:\ndisfluency detection, classification, extraction and correction. Experimental\nresults on the LARD dataset demonstrate that the data produced by the proposed\nmethod can be effectively used for detecting and removing disfluencies, while\nalso addressing limitations of existing datasets.", "published": "2022-01-13 16:02:36", "link": "http://arxiv.org/abs/2201.05041v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speech Resources in the Tamasheq Language", "abstract": "In this paper we present two datasets for Tamasheq, a developing language\nmainly spoken in Mali and Niger. These two datasets were made available for the\nIWSLT 2022 low-resource speech translation track, and they consist of\ncollections of radio recordings from daily broadcast news in Niger (Studio\nKalangou) and Mali (Studio Tamani). We share (i) a massive amount of unlabeled\naudio data (671 hours) in five languages: French from Niger, Fulfulde, Hausa,\nTamasheq and Zarma, and (ii) a smaller 17 hours parallel corpus of audio\nrecordings in Tamasheq, with utterance-level translations in the French\nlanguage. All this data is shared under the Creative Commons BY-NC-ND 3.0\nlicense. We hope these resources will inspire the speech community to develop\nand benchmark models using the Tamasheq language.", "published": "2022-01-13 16:24:06", "link": "http://arxiv.org/abs/2201.05051v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grow-and-Clip: Informative-yet-Concise Evidence Distillation for Answer\n  Explanation", "abstract": "Interpreting the predictions of existing Question Answering (QA) models is\ncritical to many real-world intelligent applications, such as QA systems for\nhealthcare, education, and finance. However, existing QA models lack\ninterpretability and provide no feedback or explanation for end-users to help\nthem understand why a specific prediction is the answer to a question. In this\nresearch, we argue that the evidences of an answer is critical to enhancing the\ninterpretability of QA models. Unlike previous research that simply extracts\nseveral sentence(s) in the context as evidence, we are the first to explicitly\ndefine the concept of evidence as the supporting facts in a context which are\ninformative, concise, and readable. Besides, we provide effective strategies to\nquantitatively measure the informativeness, conciseness and readability of\nevidence. Furthermore, we propose Grow-and-Clip Evidence Distillation (GCED)\nalgorithm to extract evidences from the contexts by trade-off informativeness,\nconciseness, and readability. We conduct extensive experiments on the SQuAD and\nTriviaQA datasets with several baseline models to evaluate the effect of GCED\non interpreting answers to questions. Human evaluation are also carried out to\ncheck the quality of distilled evidences. Experimental results show that\nautomatic distilled evidences have human-like informativeness, conciseness and\nreadability, which can enhance the interpretability of the answers to\nquestions.", "published": "2022-01-13 17:18:17", "link": "http://arxiv.org/abs/2201.05088v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NorDiaChange: Diachronic Semantic Change Dataset for Norwegian", "abstract": "We describe NorDiaChange: the first diachronic semantic change dataset for\nNorwegian. NorDiaChange comprises two novel subsets, covering about 80\nNorwegian nouns manually annotated with graded semantic change over time. Both\ndatasets follow the same annotation procedure and can be used interchangeably\nas train and test splits for each other. NorDiaChange covers the time periods\nrelated to pre- and post-war events, oil and gas discovery in Norway, and\ntechnological developments. The annotation was done using the DURel framework\nand two large historical Norwegian corpora. NorDiaChange is published in full\nunder a permissive licence, complete with raw annotation data and inferred\ndiachronic word usage graphs (DWUGs).", "published": "2022-01-13 18:27:33", "link": "http://arxiv.org/abs/2201.05123v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Datasheet for the Pile", "abstract": "This datasheet describes the Pile, a 825 GiB dataset of human-authored text\ncompiled by EleutherAI for use in large-scale language modeling. The Pile is\ncomprised of 22 different text sources, ranging from original scrapes done for\nthis project, to text data made available by the data owners, to third-party\nscrapes available online.", "published": "2022-01-13 23:45:24", "link": "http://arxiv.org/abs/2201.07311v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recognizing semantic relation in sentence pairs using Tree-RNNs and\n  Typed dependencies", "abstract": "Recursive neural networks (Tree-RNNs) based on dependency trees are\nubiquitous in modeling sentence meanings as they effectively capture semantic\nrelationships between non-neighborhood words. However, recognizing semantically\ndissimilar sentences with the same words and syntax is still a challenge to\nTree-RNNs. This work proposes an improvement to Dependency Tree-RNN (DT-RNN)\nusing the grammatical relationship type identified in the dependency parse. Our\nexperiments on semantic relatedness scoring (SRS) and recognizing textual\nentailment (RTE) in sentence pairs using SICK (Sentence Involving Compositional\nKnowledge) dataset show encouraging results. The model achieved a 2%\nimprovement in classification accuracy for the RTE task over the DT-RNN model.\nThe results show that Pearson's and Spearman's correlation measures between the\nmodel's predicted similarity scores and human ratings are higher than those of\nstandard DT-RNNs.", "published": "2022-01-13 06:59:27", "link": "http://arxiv.org/abs/2201.04810v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Augmented Network Towards Multiview Representation\n  Learning for Aspect-based Sentiment Analysis", "abstract": "Aspect-based sentiment analysis (ABSA) is a fine-grained task of sentiment\nanalysis. To better comprehend long complicated sentences and obtain accurate\naspect-specific information, linguistic and commonsense knowledge are generally\nrequired in this task. However, most current methods employ complicated and\ninefficient approaches to incorporate external knowledge, e.g., directly\nsearching the graph nodes. Additionally, the complementarity between external\nknowledge and linguistic information has not been thoroughly studied. To this\nend, we propose a knowledge graph augmented network KGAN, which aims to\neffectively incorporate external knowledge with explicitly syntactic and\ncontextual information. In particular, KGAN captures the sentiment feature\nrepresentations from multiple different perspectives, i.e., context-, syntax-\nand knowledge-based. First, KGAN learns the contextual and syntactic\nrepresentations in parallel to fully extract the semantic features. Then, KGAN\nintegrates the knowledge graphs into the embedding space, based on which the\naspect-specific knowledge representations are further obtained via an attention\nmechanism. Last, we propose a hierarchical fusion module to complement these\nmulti-view representations in a local-to-global manner. Extensive experiments\non five popular ABSA benchmarks demonstrate the effectiveness and robustness of\nour KGAN. Notably, with the help of the pretrained model of RoBERTa, KGAN\nachieves a new record of state-of-the-art performance among all datasets.", "published": "2022-01-13 08:25:53", "link": "http://arxiv.org/abs/2201.04831v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-task Pre-training Language Model for Semantic Network Completion", "abstract": "Semantic networks, such as the knowledge graph, can represent the knowledge\nleveraging the graph structure. Although the knowledge graph shows promising\nvalues in natural language processing, it suffers from incompleteness. This\npaper focuses on knowledge graph completion by predicting linkage between\nentities, which is a fundamental yet critical task. Semantic matching is a\npotential solution as it can deal with unseen entities, which the translational\ndistance based methods struggle with. However, to achieve competitive\nperformance as translational distance based methods, semantic matching based\nmethods require large-scale datasets for the training purpose, which are\ntypically unavailable in practical settings. Therefore, we employ the language\nmodel and introduce a novel knowledge graph architecture named LP-BERT, which\ncontains two main stages: multi-task pre-training and knowledge graph\nfine-tuning. In the pre-training phase, three tasks are taken to drive the\nmodel to learn the relationship from triples by predicting either entities or\nrelations. While in the fine-tuning phase, inspired by contrastive learning, we\ndesign a triple-style negative sampling in a batch, which greatly increases the\nproportion of negative sampling while keeping the training time almost\nunchanged. Furthermore, we propose a new data augmentation method utilizing the\ninverse relationship of triples to improve the performance and robustness of\nthe model. To demonstrate the effectiveness of our method, we conduct extensive\nexperiments on three widely-used datasets, WN18RR, FB15k-237, and UMLS. The\nexperimental results demonstrate the superiority of our methods, and our\napproach achieves state-of-the-art results on WN18RR and FB15k-237 datasets.\nSignificantly, Hits@10 indicator is improved by 5% from previous\nstate-of-the-art result on the WN18RR dataset while reaching 100% on the UMLS\ndataset.", "published": "2022-01-13 09:18:30", "link": "http://arxiv.org/abs/2201.04843v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Quadratic 0-1 Programming Approach for Word Sense Disambiguation", "abstract": "Word Sense Disambiguation (WSD) is the task to determine the sense of an\nambiguous word in a given context. Previous approaches for WSD have focused on\nsupervised and knowledge-based methods, but inter-sense interactions patterns\nor regularities for disambiguation remain to be found. We argue the following\ncause as one of the major difficulties behind finding the right patterns: for a\nparticular context, the intended senses of a sequence of ambiguous words are\ndependent on each other, i.e. the choice of one word's sense is associated with\nthe choice of another word's sense, making WSD a combinatorial optimization\nproblem.In this work, we approach the interactions between senses of different\ntarget words by a Quadratic 0-1 Integer Programming model (QIP) that maximizes\nthe objective function consisting of (1) the similarity between candidate\nsenses of a target word and the word in a context (the sense-word similarity),\nand (2) the semantic interactions (relatedness) between senses of all words in\nthe context (the sense-sense relatedness).", "published": "2022-01-13 10:46:06", "link": "http://arxiv.org/abs/2201.04877v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Compressing Word Embeddings Using Syllables", "abstract": "This work examines the possibility of using syllable embeddings, instead of\nthe often used $n$-gram embeddings, as subword embeddings. We investigate this\nfor two languages: English and Dutch. To this end, we also translated two\nstandard English word embedding evaluation datasets, WordSim353 and\nSemEval-2017, to Dutch. Furthermore, we provide the research community with\ndata sets of syllabic decompositions for both languages. We compare our\napproach to full word and $n$-gram embeddings. Compared to full word\nembeddings, we obtain English models that are 20 to 30 times smaller while\nretaining 80% of the performance. For Dutch, models are 15 times smaller for\n70% performance retention. Although less accurate than the $n$-gram baseline we\nused, our models can be trained in a matter of minutes, as opposed to hours for\nthe $n$-gram approach. We identify a path toward upgrading performance in\nfuture work. All code is made publicly available, as well as our collected\nEnglish and Dutch syllabic decompositions and Dutch evaluation set\ntranslations.", "published": "2022-01-13 12:09:44", "link": "http://arxiv.org/abs/2201.04913v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Feature-rich multiplex lexical networks reveal mental strategies of\n  early language learning", "abstract": "Knowledge in the human mind exhibits a dualistic vector/network nature.\nModelling words as vectors is key to natural language processing, whereas\nnetworks of word associations can map the nature of semantic memory. We\nreconcile these paradigms - fragmented across linguistics, psychology and\ncomputer science - by introducing FEature-Rich MUltiplex LEXical (FERMULEX)\nnetworks. This novel framework merges structural similarities in networks and\nvector features of words, which can be combined or explored independently.\nSimilarities model heterogenous word associations across\nsemantic/syntactic/phonological aspects of knowledge. Words are enriched with\nmulti-dimensional feature embeddings including frequency, age of acquisition,\nlength and polysemy. These aspects enable unprecedented explorations of\ncognitive knowledge. Through CHILDES data, we use FERMULEX networks to model\nnormative language acquisition by 1000 toddlers between 18 and 30 months.\nSimilarities and embeddings capture word homophily via conformity, which\nmeasures assortative mixing via distance and features. Conformity unearths a\nlanguage kernel of frequent/polysemous/short nouns and verbs key for basic\nsentence production, supporting recent evidence of children's syntactic\nconstructs emerging at 30 months. This kernel is invisible to network\ncore-detection and feature-only clustering: It emerges from the dual\nvector/network nature of words. Our quantitative analysis reveals two key\nstrategies in early word learning. Modelling word acquisition as random walks\non FERMULEX topology, we highlight non-uniform filling of communicative\ndevelopmental inventories (CDIs). Conformity-based walkers lead to accurate\n(75%), precise (55%) and partially well-recalled (34%) predictions of early\nword learning in CDIs, providing quantitative support to previous empirical\nfindings and developmental theories.", "published": "2022-01-13 16:44:51", "link": "http://arxiv.org/abs/2201.05061v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "The Combinatorics of \\textit{Salva Veritate} Principles", "abstract": "Various concepts of grammatical compositionality arise in many theories of\nboth natural and artificial languages, and often play a key role in accounts of\nthe syntax-semantics interface. We propose that many instances of\ncompositionality should entail non-trivial combinatorial claims about the\nexpressive power of languages which satisfy these compositional properties. As\nan example, we present a formal analysis demonstrating that a particular class\nof languages which admit salva vertitate substitutions - a property which we\nclaim to be a particularly strong example of compositional principle - must\nalso satisfy a very natural combinatorial constraint identified in this paper.", "published": "2022-01-13 19:00:56", "link": "http://arxiv.org/abs/2201.05173v1", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
{"title": "Neural Approaches to Conversational Information Retrieval", "abstract": "A conversational information retrieval (CIR) system is an information\nretrieval (IR) system with a conversational interface which allows users to\ninteract with the system to seek information via multi-turn conversations of\nnatural language, in spoken or written form. Recent progress in deep learning\nhas brought tremendous improvements in natural language processing (NLP) and\nconversational AI, leading to a plethora of commercial conversational services\nthat allow naturally spoken and typed interaction, increasing the need for more\nhuman-centric interactions in IR. As a result, we have witnessed a resurgent\ninterest in developing modern CIR systems in both research communities and\nindustry. This book surveys recent advances in CIR, focusing on neural\napproaches that have been developed in the last few years. This book is based\non the authors' tutorial at SIGIR'2020 (Gao et al., 2020b), with IR and NLP\ncommunities as the primary target audience. However, audiences with other\nbackground, such as machine learning and human-computer interaction, will also\nfind it an accessible introduction to CIR. We hope that this book will prove a\nvaluable resource for students, researchers, and software developers. This\nmanuscript is a working draft. Comments are welcome.", "published": "2022-01-13 19:04:59", "link": "http://arxiv.org/abs/2201.05176v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Making a (Counterfactual) Difference One Rationale at a Time", "abstract": "Rationales, snippets of extracted text that explain an inference, have\nemerged as a popular framework for interpretable natural language processing\n(NLP). Rationale models typically consist of two cooperating modules: a\nselector and a classifier with the goal of maximizing the mutual information\n(MMI) between the \"selected\" text and the document label. Despite their\npromises, MMI-based methods often pick up on spurious text patterns and result\nin models with nonsensical behaviors. In this work, we investigate whether\ncounterfactual data augmentation (CDA), without human assistance, can improve\nthe performance of the selector by lowering the mutual information between\nspurious signals and the document label. Our counterfactuals are produced in an\nunsupervised fashion using class-dependent generative models. From an\ninformation theoretic lens, we derive properties of the unaugmented dataset for\nwhich our CDA approach would succeed. The effectiveness of CDA is empirically\nevaluated by comparing against several baselines including an improved\nMMI-based rationale schema on two multi aspect datasets. Our results show that\nCDA produces rationales that better capture the signal of interest.", "published": "2022-01-13 19:05:02", "link": "http://arxiv.org/abs/2201.05177v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NLP in Human Rights Research -- Extracting Knowledge Graphs About Police\n  and Army Units and Their Commanders", "abstract": "In this working paper we explore the use of an NLP system to assist the work\nof Security Force Monitor (SFM). SFM creates data about the organizational\nstructure, command personnel and operations of police, army and other security\nforces, which assists human rights researchers, journalists and litigators in\ntheir work to help identify and bring to account specific units and personnel\nalleged to have committed abuses of human rights and international criminal\nlaw. This working paper presents an NLP system that extracts from English\nlanguage news reports the names of security force units and the biographical\ndetails of their personnel, and infers the formal relationship between them.\nPublished alongside this working paper are the system's code and training\ndataset. We find that the experimental NLP system performs the task at a fair\nto good level. Its performance is sufficient to justify further development\ninto a live workflow that will give insight into whether its performance\ntranslates into savings in time and resource that would make it an effective\ntechnical intervention.", "published": "2022-01-13 21:57:21", "link": "http://arxiv.org/abs/2201.05230v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Interactive Data Analysis with Next-step Natural Language Query\n  Recommendation", "abstract": "Natural language interfaces (NLIs) provide users with a convenient way to\ninteractively analyze data through natural language queries. Nevertheless,\ninteractive data analysis is a demanding process, especially for novice data\nanalysts. When exploring large and complex SQL databases from different\ndomains, data analysts do not necessarily have sufficient knowledge about\ndifferent data tables and application domains. It makes them unable to\nsystematically elicit a series of topically-related and meaningful queries for\ninsight discovery in target domains. We develop a NLI with a step-wise query\nrecommendation module to assist users in choosing appropriate next-step\nexploration actions. The system adopts a data-driven approach to suggest\nsemantically relevant and context-aware queries for application domains of\nusers' interest based on their query logs. Also, the system helps users\norganize query histories and results into a dashboard to communicate the\ndiscovered data insights. With a comparative user study, we show that our\nsystem can facilitate a more effective and systematic data analysis process\nthan a baseline without the recommendation module.", "published": "2022-01-13 10:20:06", "link": "http://arxiv.org/abs/2201.04868v2", "categories": ["cs.HC", "cs.CL", "cs.DB"], "primary_category": "cs.HC"}
{"title": "Towards Automated Error Analysis: Learning to Characterize Errors", "abstract": "Characterizing the patterns of errors that a system makes helps researchers\nfocus future development on increasing its accuracy and robustness. We propose\na novel form of \"meta learning\" that automatically learns interpretable rules\nthat characterize the types of errors that a system makes, and demonstrate\nthese rules' ability to help understand and improve two NLP systems. Our\napproach works by collecting error cases on validation data, extracting\nmeta-features describing these samples, and finally learning rules that\ncharacterize errors using these features. We apply our approach to VilBERT, for\nVisual Question Answering, and RoBERTa, for Common Sense Question Answering.\nOur system learns interpretable rules that provide insights into systemic\nerrors these systems make on the given tasks. Using these insights, we are also\nable to \"close the loop\" and modestly improve performance of these systems.", "published": "2022-01-13 15:08:44", "link": "http://arxiv.org/abs/2201.05017v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Optimal alphabet for single text compression", "abstract": "A text written using symbols from a given alphabet can be compressed using\nthe Huffman code, which minimizes the length of the encoded text. It is\nnecessary, however, to employ a text-specific codebook, i.e. the\nsymbol-codeword dictionary, to decode the original text. Thus, the compression\nperformance should be evaluated by the full code length, i.e. the length of the\nencoded text plus the length of the codebook. We studied several alphabets for\ncompressing texts -- letters, n-grams of letters, syllables, words, and\nphrases. If only sufficiently short texts are retained, an alphabet of letters\nor two-grams of letters is optimal. For the majority of Project Gutenberg\ntexts, the best alphabet (the one that minimizes the full code length) is given\nby syllables or words, depending on the representation of the codebook. Letter\n3 and 4-grams, having on average comparable length to syllables/words, perform\nnoticeably worse than syllables or words. Word 2-grams also are never the best\nalphabet, on the account of having a very large codebook. We also show that the\ncodebook representation is important -- switching from a naive representation\nto a compact one significantly improves the matters for alphabets with large\nnumber of symbols, most notably the words.\n  Thus, meaning-expressing elements of the language (syllables or words)\nprovide the best compression alphabet.", "published": "2022-01-13 22:16:51", "link": "http://arxiv.org/abs/2201.05234v2", "categories": ["cs.IT", "cs.CL", "math.IT", "physics.data-an"], "primary_category": "cs.IT"}
{"title": "Comparison of Classification Algorithms for COVID19 Detection using\n  Cough Acoustic Signals", "abstract": "The epidemic disease, called the new coronavirus (COVID19), firstly occurred\nin Wuhan, China in December 2019. COVID19 was announced as an epidemic by World\nHealth Organization soon after. Some of the symptoms of this disease are fever,\ncough, shortness of breath and difficulty in breathing. In more severe cases,\ndeath may occur as a result of infection. The most significant question in\nfighting the pandemic and controlling the epidemic is the early diagnosis of\nCOVID19(+) patients and the follow-up of these patients. Therefore, various\ndiagnostic mechanisms are used. Additionally to the RT-PCR test, medical\nimaging methods have been utilized, especially in the detection of COVID19(+)\npatients. In this study, an alternative approach was proposed by using cough\ndata, which is one of the most prominent symptoms of COVID19(+) patients. The\ncough acoustic public dataset on the Virufy website was used. The entire data\nwas normalized using z-normalization technique. The performance of the features\nobtained via the 5-layer empirical mode decomposition method and the\nperformances of different classifiers has been compared. As the classifier\nalgorithm, 5 different algorithms were used. The highest accuracy and F1-score\nperformances were obtained by using Ensemble-Bagged-Trees algorithm as 90.6%\nand 90.5%, respectively. On the other hand, other classification algorithms\nused in the study are Support Vector Machines, Logistic Regression, Linear\nDiscriminant Analysis and k-Nearest Neigbors, respectively. According to the\nresults obtained, choosing the right classifier algorithm provides high\nresults. Thus, it is clear that using cough acoustic data, those with\nCOVID19(+) can be detected easily and effectively.", "published": "2022-01-13 10:25:39", "link": "http://arxiv.org/abs/2201.04872v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Beyond chord vocabularies: Exploiting pitch-relationships in a chord\n  estimation metric", "abstract": "Chord estimation metrics treat chord labels as independent of one another.\nThis fails to represent the pitch relationships between the chords in a\nmeaningful way, resulting in evaluations that must make compromises with\ncomplex chord vocabularies and that often require time-consuming qualitative\nanalyses to determine details about how a chord estimation algorithm performs.\nThis paper presents an accuracy metric for chord estimation that compares the\npitch content of the estimated chords against the ground truth that captures\nboth the correct notes that are estimated and additional notes that are\ninserted into the estimate. This is not a stand-alone evaluation protocol but\nrather a metric that can be integrated as a weighting into existing evaluation\napproaches.", "published": "2022-01-13 23:34:03", "link": "http://arxiv.org/abs/2201.05244v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Effectiveness of Time Stretching for Enhancing Dysarthric Speech for\n  Improved Dysarthric Speech Recognition", "abstract": "In this paper, we investigate several existing and a new state-of-the-art\ngenerative adversarial network-based (GAN) voice conversion method for\nenhancing dysarthric speech for improved dysarthric speech recognition. We\ncompare key components of existing methods as part of a rigorous ablation study\nto find the most effective solution to improve dysarthric speech recognition.\nWe find that straightforward signal processing methods such as stationary noise\nremoval and vocoder-based time stretching lead to dysarthric speech recognition\nresults comparable to those obtained when using state-of-the-art GAN-based\nvoice conversion methods as measured using a phoneme recognition task.\nAdditionally, our proposed solution of a combination of MaskCycleGAN-VC and\ntime stretched enhancement is able to improve the phoneme recognition results\nfor certain dysarthric speakers compared to our time stretched baseline.", "published": "2022-01-13 11:56:13", "link": "http://arxiv.org/abs/2201.04908v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fish sounds: towards the evaluation of marine acoustic biodiversity\n  through data-driven audio source separation", "abstract": "The marine ecosystem is changing at an alarming rate, exhibiting biodiversity\nloss and the migration of tropical species to temperate basins. Monitoring the\nunderwater environments and their inhabitants is of fundamental importance to\nunderstand the evolution of these systems and implement safeguard policies.\nHowever, assessing and tracking biodiversity is often a complex task,\nespecially in large and uncontrolled environments, such as the oceans. One of\nthe most popular and effective methods for monitoring marine biodiversity is\npassive acoustics monitoring (PAM), which employs hydrophones to capture\nunderwater sound. Many aquatic animals produce sounds characteristic of their\nown species; these signals travel efficiently underwater and can be detected\neven at great distances. Furthermore, modern technologies are becoming more and\nmore convenient and precise, allowing for very accurate and careful data\nacquisition. To date, audio captured with PAM devices is frequently manually\nprocessed by marine biologists and interpreted with traditional signal\nprocessing techniques for the detection of animal vocalizations. This is a\nchallenging task, as PAM recordings are often over long periods of time.\nMoreover, one of the causes of biodiversity loss is sound pollution; in data\nobtained from regions with loud anthropic noise, it is hard to separate the\nartificial from the fish sound manually. Nowadays, machine learning and, in\nparticular, deep learning represents the state of the art for processing audio\nsignals. Specifically, sound separation networks are able to identify and\nseparate human voices and musical instruments. In this work, we show that the\nsame techniques can be successfully used to automatically extract fish\nvocalizations in PAM recordings, opening up the possibility for biodiversity\nmonitoring at a large scale.", "published": "2022-01-13 14:57:34", "link": "http://arxiv.org/abs/2201.05013v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
