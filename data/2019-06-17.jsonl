{"title": "Robust Zero-Shot Cross-Domain Slot Filling with Example Values", "abstract": "Task-oriented dialog systems increasingly rely on deep learning-based slot\nfilling models, usually needing extensive labeled training data for target\ndomains. Often, however, little to no target domain training data may be\navailable, or the training and target domain schemas may be misaligned, as is\ncommon for web forms on similar websites. Prior zero-shot slot filling models\nuse slot descriptions to learn concepts, but are not robust to misaligned\nschemas. We propose utilizing both the slot description and a small number of\nexamples of slot values, which may be easily available, to learn semantic\nrepresentations of slots which are transferable across domains and robust to\nmisaligned schemas. Our approach outperforms state-of-the-art models on two\nmulti-domain datasets, especially in the low-data setting.", "published": "2019-06-17 07:07:01", "link": "http://arxiv.org/abs/1906.06870v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interconnected Question Generation with Coreference Alignment and\n  Conversation Flow Modeling", "abstract": "We study the problem of generating interconnected questions in\nquestion-answering style conversations. Compared with previous works which\ngenerate questions based on a single sentence (or paragraph), this setting is\ndifferent in two major aspects: (1) Questions are highly conversational. Almost\nhalf of them refer back to conversation history using coreferences. (2) In a\ncoherent conversation, questions have smooth transitions between turns. We\npropose an end-to-end neural model with coreference alignment and conversation\nflow modeling. The coreference alignment modeling explicitly aligns coreferent\nmentions in conversation history with corresponding pronominal references in\ngenerated questions, which makes generated questions interconnected to\nconversation history. The conversation flow modeling builds a coherent\nconversation by starting questioning on the first few sentences in a text\npassage and smoothly shifting the focus to later parts. Extensive experiments\nshow that our system outperforms several baselines and can generate highly\nconversational questions. The code implementation is released at\nhttps://github.com/Evan-Gao/conversational-QG.", "published": "2019-06-17 08:30:55", "link": "http://arxiv.org/abs/1906.06893v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Manipulating the Difficulty of C-Tests", "abstract": "We propose two novel manipulation strategies for increasing and decreasing\nthe difficulty of C-tests automatically. This is a crucial step towards\ngenerating learner-adaptive exercises for self-directed language learning and\npreparing language assessment tests. To reach the desired difficulty level, we\nmanipulate the size and the distribution of gaps based on absolute and relative\ngap difficulty predictions. We evaluate our approach in corpus-based\nexperiments and in a user study with 60 participants. We find that both\nstrategies are able to generate C-tests with the desired difficulty level.", "published": "2019-06-17 08:57:38", "link": "http://arxiv.org/abs/1906.06905v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Interactive Multi-Task Learning Network for End-to-End Aspect-Based\n  Sentiment Analysis", "abstract": "Aspect-based sentiment analysis produces a list of aspect terms and their\ncorresponding sentiments for a natural language sentence. This task is usually\ndone in a pipeline manner, with aspect term extraction performed first,\nfollowed by sentiment predictions toward the extracted aspect terms. While\neasier to develop, such an approach does not fully exploit joint information\nfrom the two subtasks and does not use all available sources of training\ninformation that might be helpful, such as document-level labeled sentiment\ncorpus. In this paper, we propose an interactive multi-task learning network\n(IMN) which is able to jointly learn multiple related tasks simultaneously at\nboth the token level as well as the document level. Unlike conventional\nmulti-task learning methods that rely on learning common features for the\ndifferent tasks, IMN introduces a message passing architecture where\ninformation is iteratively passed to different tasks through a shared set of\nlatent variables. Experimental results demonstrate superior performance of the\nproposed method against multiple baselines on three benchmark datasets.", "published": "2019-06-17 09:00:01", "link": "http://arxiv.org/abs/1906.06906v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERE: An accurate distantly supervised biomedical entity relation\n  extraction network", "abstract": "Automated entity relation extraction (RE) from literature provides an\nimportant source for constructing biomedical database, which is more efficient\nand extensible than manual curation. However, existing RE models usually ignore\nthe information contained in sentence structures and target entities. In this\npaper, we propose BERE, a deep learning based model which uses Gumbel Tree-GRU\nto learn sentence structures and joint embedding to incorporate entity\ninformation. It also employs word-level attention for improved relation\nextraction and sentence-level attention to suit the distantly supervised\ndataset. Because the existing dataset are relatively small, we further\nconstruct a much larger drug-target interaction extraction (DTIE) dataset by\ndistant supervision. Experiments conducted on both DDIExtraction 2013 task and\nDTIE dataset show our model's effectiveness over state-of-the-art baselines in\nterms of F1 measures and PR curves.", "published": "2019-06-17 09:33:27", "link": "http://arxiv.org/abs/1906.06916v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open Domain Event Extraction Using Neural Latent Variable Models", "abstract": "We consider open domain event extraction, the task of extracting unconstraint\ntypes of events from news clusters. A novel latent variable neural model is\nconstructed, which is scalable to very large corpus. A dataset is collected and\nmanually annotated, with task-specific evaluation metrics being designed.\nResults show that the proposed unsupervised model gives better performance\ncompared to the state-of-the-art method for event schema induction.", "published": "2019-06-17 11:01:20", "link": "http://arxiv.org/abs/1906.06947v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Making Fast Graph-based Algorithms with Graph Metric Embeddings", "abstract": "The computation of distance measures between nodes in graphs is inefficient\nand does not scale to large graphs. We explore dense vector representations as\nan effective way to approximate the same information: we introduce a simple yet\nefficient and effective approach for learning graph embeddings. Instead of\ndirectly operating on the graph structure, our method takes structural measures\nof pairwise node similarities into account and learns dense node\nrepresentations reflecting user-defined graph distance measures, such as\ne.g.the shortest path distance or distance measures that take information\nbeyond the graph structure into account. We demonstrate a speed-up of several\norders of magnitude when predicting word similarity by vector operations on our\nembeddings as opposed to directly computing the respective path-based measures,\nwhile outperforming various other graph embeddings on semantic similarity and\nword sense disambiguation tasks and show evaluations on the WordNet graph and\ntwo knowledge base graphs.", "published": "2019-06-17 14:01:33", "link": "http://arxiv.org/abs/1906.07040v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coupling Retrieval and Meta-Learning for Context-Dependent Semantic\n  Parsing", "abstract": "In this paper, we present an approach to incorporate retrieved datapoints as\nsupporting evidence for context-dependent semantic parsing, such as generating\nsource code conditioned on the class environment. Our approach naturally\ncombines a retrieval model and a meta-learner, where the former learns to find\nsimilar datapoints from the training data, and the latter considers retrieved\ndatapoints as a pseudo task for fast adaptation. Specifically, our retriever is\na context-aware encoder-decoder model with a latent variable which takes\ncontext environment into consideration, and our meta-learner learns to utilize\nretrieved datapoints in a model-agnostic meta-learning paradigm for fast\nadaptation. We conduct experiments on CONCODE and CSQA datasets, where the\ncontext refers to class environment in JAVA codes and conversational history,\nrespectively. We use sequence-to-action model as the base semantic parser,\nwhich performs the state-of-the-art accuracy on both datasets. Results show\nthat both the context-aware retriever and the meta-learning strategy improve\naccuracy, and our approach performs better than retrieve-and-edit baselines.", "published": "2019-06-17 16:18:28", "link": "http://arxiv.org/abs/1906.07108v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Barack's Wife Hillary: Using Knowledge-Graphs for Fact-Aware Language\n  Modeling", "abstract": "Modeling human language requires the ability to not only generate fluent text\nbut also encode factual knowledge. However, traditional language models are\nonly capable of remembering facts seen at training time, and often have\ndifficulty recalling them. To address this, we introduce the knowledge graph\nlanguage model (KGLM), a neural language model with mechanisms for selecting\nand copying facts from a knowledge graph that are relevant to the context.\nThese mechanisms enable the model to render information it has never seen\nbefore, as well as generate out-of-vocabulary tokens. We also introduce the\nLinked WikiText-2 dataset, a corpus of annotated text aligned to the Wikidata\nknowledge graph whose contents (roughly) match the popular WikiText-2\nbenchmark. In experiments, we demonstrate that the KGLM achieves significantly\nbetter performance than a strong baseline language model. We additionally\ncompare different language model's ability to complete sentences requiring\nfactual knowledge, showing that the KGLM outperforms even very large language\nmodels in generating facts.", "published": "2019-06-17 19:48:41", "link": "http://arxiv.org/abs/1906.07241v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Structured Distributional Model of Sentence Meaning and Processing", "abstract": "Most compositional distributional semantic models represent sentence meaning\nwith a single vector. In this paper, we propose a Structured Distributional\nModel (SDM) that combines word embeddings with formal semantics and is based on\nthe assumption that sentences represent events and situations. The semantic\nrepresentation of a sentence is a formal structure derived from Discourse\nRepresentation Theory and containing distributional vectors. This structure is\ndynamically and incrementally built by integrating knowledge about events and\ntheir typical participants, as they are activated by lexical items. Event\nknowledge is modeled as a graph extracted from parsed corpora and encoding\nroles and relationships between participants that are represented as\ndistributional vectors. SDM is grounded on extensive psycholinguistic research\nshowing that generalized knowledge about events stored in semantic memory plays\na key role in sentence comprehension. We evaluate SDM on two recently\nintroduced compositionality datasets, and our results show that combining a\nsimple compositional model with event knowledge constantly improves\nperformances, even with different types of word embeddings.", "published": "2019-06-17 21:31:40", "link": "http://arxiv.org/abs/1906.07280v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tabula nearly rasa: Probing the Linguistic Knowledge of Character-Level\n  Neural Language Models Trained on Unsegmented Text", "abstract": "Recurrent neural networks (RNNs) have reached striking performance in many\nnatural language processing tasks. This has renewed interest in whether these\ngeneric sequence processing devices are inducing genuine linguistic knowledge.\nNearly all current analytical studies, however, initialize the RNNs with a\nvocabulary of known words, and feed them tokenized input during training. We\npresent a multi-lingual study of the linguistic knowledge encoded in RNNs\ntrained as character-level language models, on input data with word boundaries\nremoved. These networks face a tougher and more cognitively realistic task,\nhaving to discover any useful linguistic unit from scratch based on input\nstatistics. The results show that our \"near tabula rasa\" RNNs are mostly able\nto solve morphological, syntactic and semantic tasks that intuitively\npresuppose word-level knowledge, and indeed they learned, to some extent, to\ntrack word boundaries. Our study opens the door to speculations about the\nnecessity of an explicit, rigid word lexicon in language learning and usage.", "published": "2019-06-17 21:53:10", "link": "http://arxiv.org/abs/1906.07285v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KaWAT: A Word Analogy Task Dataset for Indonesian", "abstract": "We introduced KaWAT (Kata Word Analogy Task), a new word analogy task dataset\nfor Indonesian. We evaluated on it several existing pretrained Indonesian word\nembeddings and embeddings trained on Indonesian online news corpus. We also\ntested them on two downstream tasks and found that pretrained word embeddings\nhelped either by reducing the training epochs or yielding significant\nperformance gains.", "published": "2019-06-17 06:32:24", "link": "http://arxiv.org/abs/1906.09912v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multi-Task Architecture on Relevance-based Neural Query Translation", "abstract": "We describe a multi-task learning approach to train a Neural Machine\nTranslation (NMT) model with a Relevance-based Auxiliary Task (RAT) for search\nquery translation. The translation process for Cross-lingual Information\nRetrieval (CLIR) task is usually treated as a black box and it is performed as\nan independent step. However, an NMT model trained on sentence-level parallel\ndata is not aware of the vocabulary distribution of the retrieval corpus. We\naddress this problem with our multi-task learning architecture that achieves\n16% improvement over a strong NMT baseline on Italian-English query-document\ndataset. We show using both quantitative and qualitative analysis that our\nmodel generates balanced and precise translations with the regularization\neffect it achieves from multi-task learning paradigm.", "published": "2019-06-17 05:37:27", "link": "http://arxiv.org/abs/1906.06849v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Recursive Style Breach Detection with Multifaceted Ensemble Learning", "abstract": "We present a supervised approach for style change detection, which aims at\npredicting whether there are changes in the style in a given text document, as\nwell as at finding the exact positions where such changes occur. In particular,\nwe combine a TF.IDF representation of the document with features specifically\nengineered for the task, and we make predictions via an ensemble of diverse\nclassifiers including SVM, Random Forest, AdaBoost, MLP, and LightGBM. Whenever\nthe model detects that style change is present, we apply it recursively,\nlooking to find the specific positions of the change. Our approach powered the\nwinning system for the PAN@CLEF 2018 task on Style Change Detection.", "published": "2019-06-17 09:33:30", "link": "http://arxiv.org/abs/1906.06917v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Context-aware Embedding for Targeted Aspect-based Sentiment Analysis", "abstract": "Attention-based neural models were employed to detect the different aspects\nand sentiment polarities of the same target in targeted aspect-based sentiment\nanalysis (TABSA). However, existing methods do not specifically pre-train\nreasonable embeddings for targets and aspects in TABSA. This may result in\ntargets or aspects having the same vector representations in different contexts\nand losing the context-dependent information. To address this problem, we\npropose a novel method to refine the embeddings of targets and aspects. Such\npivotal embedding refinement utilizes a sparse coefficient vector to adjust the\nembeddings of target and aspect from the context. Hence the embeddings of\ntargets and aspects can be refined from the highly correlative words instead of\nusing context-independent or randomly initialized vectors. Experiment results\non two benchmark datasets show that our approach yields the state-of-the-art\nperformance in TABSA task.", "published": "2019-06-17 10:59:50", "link": "http://arxiv.org/abs/1906.06945v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Avoiding Reasoning Shortcuts: Adversarial Evaluation, Training, and\n  Model Development for Multi-Hop QA", "abstract": "Multi-hop question answering requires a model to connect multiple pieces of\nevidence scattered in a long context to answer the question. In this paper, we\nshow that in the multi-hop HotpotQA (Yang et al., 2018) dataset, the examples\noften contain reasoning shortcuts through which models can directly locate the\nanswer by word-matching the question with a sentence in the context. We\ndemonstrate this issue by constructing adversarial documents that create\ncontradicting answers to the shortcut but do not affect the validity of the\noriginal answer. The performance of strong baseline models drops significantly\non our adversarial evaluation, indicating that they are indeed exploiting the\nshortcuts rather than performing multi-hop reasoning. After adversarial\ntraining, the baseline's performance improves but is still limited on the\nadversarial evaluation. Hence, we use a control unit that dynamically attends\nto the question at different reasoning hops to guide the model's multi-hop\nreasoning. We show that this 2-hop model trained on the regular data is more\nrobust to the adversaries than the baseline model. After adversarial training,\nthis 2-hop model not only achieves improvements over its counterpart trained on\nregular data, but also outperforms the adversarially-trained 1-hop baseline. We\nhope that these insights and initial improvements will motivate the development\nof new models that combine explicit compositional reasoning with adversarial\ntraining.", "published": "2019-06-17 17:03:57", "link": "http://arxiv.org/abs/1906.07132v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Finding Your Voice: The Linguistic Development of Mental Health\n  Counselors", "abstract": "Mental health counseling is an enterprise with profound societal importance\nwhere conversations play a primary role. In order to acquire the conversational\nskills needed to face a challenging range of situations, mental health\ncounselors must rely on training and on continued experience with actual\nclients. However, in the absence of large scale longitudinal studies, the\nnature and significance of this developmental process remain unclear. For\nexample, prior literature suggests that experience might not translate into\nconsequential changes in counselor behavior. This has led some to even argue\nthat counseling is a profession without expertise.\n  In this work, we develop a computational framework to quantify the extent to\nwhich individuals change their linguistic behavior with experience and to study\nthe nature of this evolution. We use our framework to conduct a large\nlongitudinal study of mental health counseling conversations, tracking over\n3,400 counselors across their tenure. We reveal that overall, counselors do\nindeed change their conversational behavior to become more diverse across\ninteractions, developing an individual voice that distinguishes them from other\ncounselors. Furthermore, a finer-grained investigation shows that the rate and\nnature of this diversification vary across functionally different\nconversational components.", "published": "2019-06-17 18:00:04", "link": "http://arxiv.org/abs/1906.07194v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Constrained Decoding for Neural NLG from Compositional Representations\n  in Task-Oriented Dialogue", "abstract": "Generating fluent natural language responses from structured semantic\nrepresentations is a critical step in task-oriented conversational systems.\nAvenues like the E2E NLG Challenge have encouraged the development of neural\napproaches, particularly sequence-to-sequence (Seq2Seq) models for this\nproblem. The semantic representations used, however, are often underspecified,\nwhich places a higher burden on the generation model for sentence planning, and\nalso limits the extent to which generated responses can be controlled in a live\nsystem. In this paper, we (1) propose using tree-structured semantic\nrepresentations, like those used in traditional rule-based NLG systems, for\nbetter discourse-level structuring and sentence-level planning; (2) introduce a\nchallenging dataset using this representation for the weather domain; (3)\nintroduce a constrained decoding approach for Seq2Seq models that leverages\nthis representation to improve semantic correctness; and (4) demonstrate\npromising results on our dataset and the E2E dataset.", "published": "2019-06-17 18:54:51", "link": "http://arxiv.org/abs/1906.07220v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generalizing Back-Translation in Neural Machine Translation", "abstract": "Back-translation - data augmentation by translating target monolingual data -\nis a crucial component in modern neural machine translation (NMT). In this\nwork, we reformulate back-translation in the scope of cross-entropy\noptimization of an NMT model, clarifying its underlying mathematical\nassumptions and approximations beyond its heuristic usage. Our formulation\ncovers broader synthetic data generation schemes, including sampling from a\ntarget-to-source NMT model. With this formulation, we point out fundamental\nproblems of the sampling-based approaches and propose to remedy them by (i)\ndisabling label smoothing for the target-to-source model and (ii) sampling from\na restricted search space. Our statements are investigated on the WMT 2018\nGerman - English news translation task.", "published": "2019-06-17 22:13:24", "link": "http://arxiv.org/abs/1906.07286v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analyses of Multi-collection Corpora via Compound Topic Modeling", "abstract": "As electronically stored data grow in daily life, obtaining novel and\nrelevant information becomes challenging in text mining. Thus people have\nsought statistical methods based on term frequency, matrix algebra, or topic\nmodeling for text mining. Popular topic models have centered on one single text\ncollection, which is deficient for comparative text analyses. We consider a\nsetting where one can partition the corpus into subcollections. Each\nsubcollection shares a common set of topics, but there exists relative\nvariation in topic proportions among collections. Including any prior knowledge\nabout the corpus (e.g. organization structure), we propose the compound latent\nDirichlet allocation (cLDA) model, improving on previous work, encouraging\ngeneralizability, and depending less on user-input parameters. To identify the\nparameters of interest in cLDA, we study Markov chain Monte Carlo (MCMC) and\nvariational inference approaches extensively, and suggest an efficient MCMC\nmethod. We evaluate cLDA qualitatively and quantitatively using both synthetic\nand real-world corpora. The usability study on some real-world corpora\nillustrates the superiority of cLDA to explore the underlying topics\nautomatically but also model their connections and variations across multiple\ncollections.", "published": "2019-06-17 06:59:25", "link": "http://arxiv.org/abs/1907.01636v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML", "62F15, 60J22", "I.2; I.7; G.3"], "primary_category": "cs.IR"}
{"title": "ParNet: Position-aware Aggregated Relation Network for Image-Text\n  matching", "abstract": "Exploring fine-grained relationship between entities(e.g. objects in image or\nwords in sentence) has great contribution to understand multimedia content\nprecisely. Previous attention mechanism employed in image-text matching either\ntakes multiple self attention steps to gather correspondences or uses image\nobjects (or words) as context to infer image-text similarity. However, they\nonly take advantage of semantic information without considering that objects'\nrelative position also contributes to image understanding. To this end, we\nintroduce a novel position-aware relation module to model both the semantic and\nspatial relationship simultaneously for image-text matching in this paper.\nGiven an image, our method utilizes the location of different objects to\ncapture spatial relationship innovatively. With the combination of semantic and\nspatial relationship, it's easier to understand the content of different\nmodalities (images and sentences) and capture fine-grained latent\ncorrespondences of image-text pairs. Besides, we employ a two-step aggregated\nrelation module to capture interpretable alignment of image-text pairs. The\nfirst step, we call it intra-modal relation mechanism, in which we computes\nresponses between different objects in an image or different words in a\nsentence separately; The second step, we call it inter-modal relation\nmechanism, in which the query plays a role of textual context to refine the\nrelationship among object proposals in an image. In this way, our\nposition-aware aggregated relation network (ParNet) not only knows which\nentities are relevant by attending on different objects (words) adaptively, but\nalso adjust the inter-modal correspondence according to the latent alignments\naccording to query's content. Our approach achieves the state-of-the-art\nresults on MS-COCO dataset.", "published": "2019-06-17 08:26:43", "link": "http://arxiv.org/abs/1906.06892v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Adversarial Training for Multilingual Acoustic Modeling", "abstract": "Multilingual training has been shown to improve acoustic modeling performance\nby sharing and transferring knowledge in modeling different languages.\nKnowledge sharing is usually achieved by using common lower-level layers for\ndifferent languages in a deep neural network. Recently, the domain adversarial\nnetwork was proposed to reduce domain mismatch of training data and learn\ndomain-invariant features. It is thus worth exploring whether adversarial\ntraining can further promote knowledge sharing in multilingual models. In this\nwork, we apply the domain adversarial network to encourage the shared layers of\na multilingual model to learn language-invariant features. Bidirectional Long\nShort-Term Memory (LSTM) recurrent neural networks (RNN) are used as building\nblocks. We show that shared layers learned this way contain less language\nidentification information and lead to better performance. In an automatic\nspeech recognition task for seven languages, the resultant acoustic model\nimproves the word error rate (WER) of the multilingual model by 4% relative on\naverage, and the monolingual models by 10%.", "published": "2019-06-17 15:42:26", "link": "http://arxiv.org/abs/1906.07093v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Combining Adversarial Training and Disentangled Speech Representation\n  for Robust Zero-Resource Subword Modeling", "abstract": "This study addresses the problem of unsupervised subword unit discovery from\nuntranscribed speech. It forms the basis of the ultimate goal of ZeroSpeech\n2019, building text-to-speech systems without text labels. In this work, unit\ndiscovery is formulated as a pipeline of phonetically discriminative feature\nlearning and unit inference. One major difficulty in robust unsupervised\nfeature learning is dealing with speaker variation. Here the robustness towards\nspeaker variation is achieved by applying adversarial training and FHVAE based\ndisentangled speech representation learning. A comparison of the two approaches\nas well as their combination is studied in a DNN-bottleneck feature (DNN-BNF)\narchitecture. Experiments are conducted on ZeroSpeech 2019 and 2017.\nExperimental results on ZeroSpeech 2017 show that both approaches are effective\nwhile the latter is more prominent, and that their combination brings further\nmarginal improvement in across-speaker condition. Results on ZeroSpeech 2019\nshow that in the ABX discriminability task, our approaches significantly\noutperform the official baseline, and are competitive to or even outperform the\nofficial topline. The proposed unit sequence smoothing algorithm improves\nsynthesis quality, at a cost of slight decrease in ABX discriminability.", "published": "2019-06-17 19:40:46", "link": "http://arxiv.org/abs/1906.07234v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Transfer Learning for End-to-End Speech Synthesis from Deep\n  Pre-Trained Language Models", "abstract": "Modern text-to-speech (TTS) systems are able to generate audio that sounds\nalmost as natural as human speech. However, the bar of developing high-quality\nTTS systems remains high since a sizable set of studio-quality <text, audio>\npairs is usually required. Compared to commercial data used to develop\nstate-of-the-art systems, publicly available data are usually worse in terms of\nboth quality and size. Audio generated by TTS systems trained on publicly\navailable data tends to not only sound less natural, but also exhibits more\nbackground noise. In this work, we aim to lower TTS systems' reliance on\nhigh-quality data by providing them the textual knowledge extracted by deep\npre-trained language models during training. In particular, we investigate the\nuse of BERT to assist the training of Tacotron-2, a state of the art TTS\nconsisting of an encoder and an attention-based decoder. BERT representations\nlearned from large amounts of unlabeled text data are shown to contain very\nrich semantic and syntactic information about the input text, and have\npotential to be leveraged by a TTS system to compensate the lack of\nhigh-quality data. We incorporate BERT as a parallel branch to the Tacotron-2\nencoder with its own attention head. For an input text, it is simultaneously\npassed into BERT and the Tacotron-2 encoder. The representations extracted by\nthe two branches are concatenated and then fed to the decoder. As a preliminary\nstudy, although we have not found incorporating BERT into Tacotron-2 generates\nmore natural or cleaner speech at a human-perceivable level, we observe\nimprovements in other aspects such as the model is being significantly better\nat knowing when to stop decoding such that there is much less babbling at the\nend of the synthesized audio and faster convergence during training.", "published": "2019-06-17 23:48:05", "link": "http://arxiv.org/abs/1906.07307v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multi-Stream End-to-End Speech Recognition", "abstract": "Attention-based methods and Connectionist Temporal Classification (CTC)\nnetwork have been promising research directions for end-to-end (E2E) Automatic\nSpeech Recognition (ASR). The joint CTC/Attention model has achieved great\nsuccess by utilizing both architectures during multi-task training and joint\ndecoding. In this work, we present a multi-stream framework based on joint\nCTC/Attention E2E ASR with parallel streams represented by separate encoders\naiming to capture diverse information. On top of the regular attention\nnetworks, the Hierarchical Attention Network (HAN) is introduced to steer the\ndecoder toward the most informative encoders. A separate CTC network is\nassigned to each stream to force monotonic alignments. Two representative\nframework have been proposed and discussed, which are Multi-Encoder\nMulti-Resolution (MEM-Res) framework and Multi-Encoder Multi-Array (MEM-Array)\nframework, respectively. In MEM-Res framework, two heterogeneous encoders with\ndifferent architectures, temporal resolutions and separate CTC networks work in\nparallel to extract complimentary information from same acoustics. Experiments\nare conducted on Wall Street Journal (WSJ) and CHiME-4, resulting in relative\nWord Error Rate (WER) reduction of 18.0-32.1% and the best WER of 3.6% in the\nWSJ eval92 test set. The MEM-Array framework aims at improving the far-field\nASR robustness using multiple microphone arrays which are activated by separate\nencoders. Compared with the best single-array results, the proposed framework\nhas achieved relative WER reduction of 3.7% and 9.7% in AMI and DIRHA\nmulti-array corpora, respectively, which also outperforms conventional fusion\nstrategies.", "published": "2019-06-17 23:00:15", "link": "http://arxiv.org/abs/1906.08041v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Low-resource Deep Entity Resolution with Transfer and Active Learning", "abstract": "Entity resolution (ER) is the task of identifying different representations\nof the same real-world entities across databases. It is a key step for\nknowledge base creation and text mining. Recent adaptation of deep learning\nmethods for ER mitigates the need for dataset-specific feature engineering by\nconstructing distributed representations of entity records. While these methods\nachieve state-of-the-art performance over benchmark data, they require large\namounts of labeled data, which are typically unavailable in realistic ER\napplications. In this paper, we develop a deep learning-based method that\ntargets low-resource settings for ER through a novel combination of transfer\nlearning and active learning. We design an architecture that allows us to learn\na transferable model from a high-resource setting to a low-resource one. To\nfurther adapt to the target dataset, we incorporate active learning that\ncarefully selects a few informative examples to fine-tune the transferred\nmodel. Empirical evaluation demonstrates that our method achieves comparable,\nif not better, performance compared to state-of-the-art learning-based methods\nwhile using an order of magnitude fewer labels.", "published": "2019-06-17 20:33:24", "link": "http://arxiv.org/abs/1906.08042v1", "categories": ["cs.DB", "cs.CL", "cs.LG"], "primary_category": "cs.DB"}
{"title": "Real to H-space Encoder for Speech Recognition", "abstract": "Deep neural networks (DNNs) and more precisely recurrent neural networks\n(RNNs) are at the core of modern automatic speech recognition systems, due to\ntheir efficiency to process input sequences. Recently, it has been shown that\ndifferent input representations, based on multidimensional algebras, such as\ncomplex and quaternion numbers, are able to bring to neural networks a more\nnatural, compressive and powerful representation of the input signal by\noutperforming common real-valued NNs. Indeed, quaternion-valued neural networks\n(QNNs) better learn both internal dependencies, such as the relation between\nthe Mel-filter-bank value of a specific time frame and its time derivatives,\nand global dependencies, describing the relations that exist between time\nframes. Nonetheless, QNNs are limited to quaternion-valued input signals, and\nit is difficult to benefit from this powerful representation with real-valued\ninput data. This paper proposes to tackle this weakness by introducing a\nreal-to-quaternion encoder that allows QNNs to process any one dimensional\ninput features, such as traditional Mel-filter-banks for automatic speech\nrecognition.", "published": "2019-06-17 20:07:45", "link": "http://arxiv.org/abs/1906.08043v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploiting Unsupervised Pre-training and Automated Feature Engineering\n  for Low-resource Hate Speech Detection in Polish", "abstract": "This paper presents our contribution to PolEval 2019 Task 6: Hate speech and\nbullying detection. We describe three parallel approaches that we followed:\nfine-tuning a pre-trained ULMFiT model to our classification task, fine-tuning\na pre-trained BERT model to our classification task, and using the TPOT library\nto find the optimal pipeline. We present results achieved by these three tools\nand review their advantages and disadvantages in terms of user experience. Our\nteam placed second in subtask 2 with a shallow model found by TPOT: a~logistic\nregression classifier with non-trivial feature engineering.", "published": "2019-06-17 13:11:26", "link": "http://arxiv.org/abs/1906.09325v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Benchmarking Neural Machine Translation for Southern African Languages", "abstract": "Unlike major Western languages, most African languages are very\nlow-resourced. Furthermore, the resources that do exist are often scattered and\ndifficult to obtain and discover. As a result, the data and code for existing\nresearch has rarely been shared. This has lead a struggle to reproduce reported\nresults, and few publicly available benchmarks for African machine translation\nmodels exist. To start to address these problems, we trained neural machine\ntranslation models for 5 Southern African languages on publicly-available\ndatasets. Code is provided for training the models and evaluate the models on a\nnewly released evaluation set, with the aim of spur future research in the\nfield for Southern African languages.", "published": "2019-06-17 18:47:28", "link": "http://arxiv.org/abs/1906.10511v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Advancing Speech Recognition With No Speech Or With Noisy Speech", "abstract": "In this paper we demonstrate end-to-end continuous speech recognition (CSR)\nusing electroencephalography (EEG) signals with no speech signal as input. An\nattention model based automatic speech recognition (ASR) and connectionist\ntemporal classification (CTC) based ASR systems were implemented for performing\nrecognition. We further demonstrate CSR for noisy speech by fusing with EEG\nfeatures.", "published": "2019-06-17 23:06:51", "link": "http://arxiv.org/abs/1906.08871v9", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Evaluation of post-processing algorithms for polyphonic sound event\n  detection", "abstract": "Sound event detection (SED) aims at identifying audio events (audio tagging\ntask) in recordings and then locating them temporally (localization task). This\nlast task ends with the segmentation of the frame-level class predictions, that\ndetermines the onsets and offsets of the audio events. Yet, this step is often\noverlooked in scientific publications. In this paper, we focus on the\npost-processing algorithms used to identify the audio event boundaries.\nDifferent post-processing steps are investigated, through smoothing,\nthresholding, and optimization. In particular, we evaluate different approaches\nfor temporal segmentation, namely statistic-based and parametric methods.\nExperiments are carried out on the DCASE 2018 challenge task 4 data. We\ncompared post-processing algorithms on the temporal prediction curves of two\nmodels: one based on the challenge's baseline and a Multiple Instance Learning\n(MIL) model. Results show the crucial impact of the post-processing methods on\nthe final detection score. Statistic-based methods yield a 22.9% event-based\nF-score on the evaluation set with our MIL model. Moreover, the best results\nwere obtained using class-dependent parametric methods with 32.0% F-score.", "published": "2019-06-17 09:13:13", "link": "http://arxiv.org/abs/1906.06909v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DigiVoice: Voice Biomarker Featurization and Analysis Pipeline", "abstract": "In recent years, data-driven models have enabled significant advances in\nmedicine. Simultaneously, voice has shown potential for analysis in precision\nmedicine as a biomarker for screening illnesses. There has been a growing trend\nto pursue voice data to understand neuropsychiatric diseases. In this paper, we\npresent DigiVoice, a comprehensive feature extraction and analysis pipeline for\nvoice data. DigiVoice supports raw .WAV files and text transcriptions in order\nto analyze the entire content of voice. DigiVoice supports feature extraction\nincluding acoustic, natural language, linguistic complexity, and semantic\ncoherence features. DigiVoice also supports machine learning capabilities\nincluding data visualization, feature selection, feature transformation, and\nmodeling. To our knowledge, DigiVoice provides the most comprehensive voice\nfeature set for data analysis to date. With DigiVoice, we plan to accelerate\nresearch to correlate voice biomarkers with illness to enable data-driven\ntreatment. We have worked closely with our industry partner, NeuroLex\nLaboratories, to make voice computing open source and accessible. DigiVoice\nenables researchers to leverage our technology across the domains of voice\ncomputing and precision medicine without domain-specific expertise. Our work\nallows any researchers to use voice as a biomarker in their past, current, or\nfuture studies.", "published": "2019-06-17 18:57:05", "link": "http://arxiv.org/abs/1906.07222v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On combining features for single-channel robust speech recognition in\n  reverberant environments", "abstract": "This paper addresses the combination of complementary parallel speech\nrecognition systems to reduce the error rate of speech recognition systems\noperating in real highly-reverberant environments. First, the testing\nenvironment consists of recordings of speech in a calibrated real room with\nreverberation times from 0.47 to 1.77 seconds and speaker-to-microphone\ndistances of 0.16 to 2.56 meters. We combined systems both at the level of the\nDNN outputs and at the level of the final ASR outputs. Second, recognition\nexperiments with the reverb challenge are also reported. The results presented\nhere show that the combination of features can lead to WER improvements between\n7% and 18% with speech recorded in real reverberant environments. Also, the\ncombination at DNN-output level is much more effective than at the\nsystem-output level. However, cascading both schemes can still lead to smaller\nreductions in WER.", "published": "2019-06-17 23:04:37", "link": "http://arxiv.org/abs/1906.07299v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Modeling Music Modality with a Key-Class Invariant Pitch Chroma CNN", "abstract": "This paper presents a convolutional neural network (CNN) that uses input from\na polyphonic pitch estimation system to predict perceived minor/major modality\nin music audio. The pitch activation input is structured to allow the first CNN\nlayer to compute two pitch chromas focused on different octaves. The following\nlayers perform harmony analysis across chroma and time scales. Through max\npooling across pitch, the CNN becomes invariant with regards to the key class\n(i.e., key disregarding mode) of the music. A multilayer perceptron combines\nthe modality activation output with spectral features for the final prediction.\nThe study uses a dataset of 203 excerpts rated by around 20 listeners each, a\nsmall challenging data size requiring a carefully designed parameter sharing.\nWith an R2 of about 0.71, the system clearly outperforms previous systems as\nwell as individual human listeners. A final ablation study highlights the\nimportance of using pitch activations processed across longer time scales, and\nusing pooling to facilitate invariance with regards to the key class.", "published": "2019-06-17 17:33:07", "link": "http://arxiv.org/abs/1906.07145v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Unsupervised Subword Modeling via Disentangled Speech\n  Representation Learning and Transformation", "abstract": "This study tackles unsupervised subword modeling in the zero-resource\nscenario, learning frame-level speech representation that is phonetically\ndiscriminative and speaker-invariant, using only untranscribed speech for\ntarget languages. Frame label acquisition is an essential step in solving this\nproblem. High quality frame labels should be in good consistency with golden\ntranscriptions and robust to speaker variation. We propose to improve frame\nlabel acquisition in our previously adopted deep neural network-bottleneck\nfeature (DNN-BNF) architecture by applying the factorized hierarchical\nvariational autoencoder (FHVAE). FHVAEs learn to disentangle linguistic content\nand speaker identity information encoded in speech. By discarding or unifying\nspeaker information, speaker-invariant features are learned and fed as inputs\nto DPGMM frame clustering and DNN-BNF training. Experiments conducted on\nZeroSpeech 2017 show that our proposed approaches achieve $2.4\\%$ and $0.6\\%$\nabsolute ABX error rate reductions in across- and within-speaker conditions,\ncomparing to the baseline DNN-BNF system without applying FHVAEs. Our proposed\napproaches significantly outperform vocal tract length normalization in\nimproving frame labeling and subword modeling.", "published": "2019-06-17 19:52:40", "link": "http://arxiv.org/abs/1906.07245v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Weighted delay-and-sum beamforming guided by visual tracking for\n  human-robot interaction", "abstract": "This paper describes the integration of weighted delay-and-sum beamforming\nwith speech source localization using image processing and robot head visual\nservoing for source tracking. We take into consideration the fact that the\ndirectivity gain provided by the beamforming depends on the angular distance\nbetween its main lobe and the main response axis of the microphone array. A\nvisual servoing scheme is used to reduce the angular distance between the\ncenter of the video frame of a robot camera and a target object. Additionally,\nthe beamforming strategy presented combines two information sources: the\ndirection of the target object obtained with image processing and the audio\nsignals provided by a microphone array. These sources of information were\nintegrated by making use of a weighted delay-and-sum beamforming method.\nExperiments were carried out with a real mobile robotic testbed built with a\nPR2 robot. Static and dynamic robot head as well as the use of one and two\nexternal noise sources were considered. The results presented here show that\nthe appropriate integration of visual source tracking with visual servoing and\na beamforming method can lead to a reduction in WER as high as 34% compared to\nbeamforming alone.", "published": "2019-06-17 22:57:02", "link": "http://arxiv.org/abs/1906.07298v1", "categories": ["eess.AS", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Robust End-to-End Speaker Verification Using EEG", "abstract": "In this paper we demonstrate that performance of a speaker verification\nsystem can be improved by concatenating electroencephalography (EEG) signal\nfeatures with speech signal features or only using EEG signal features. We use\nstate-of-the-art end-to-end deep learning model for performing speaker\nverification and we demonstrate our results for noisy speech. Our results\nindicate that EEG signals can improve the robustness of speaker verification\nsystems, especially in noiser environment.", "published": "2019-06-17 20:11:24", "link": "http://arxiv.org/abs/1906.08044v5", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Speech Recognition With No Speech Or With Noisy Speech Beyond English", "abstract": "In this paper we demonstrate continuous noisy speech recognition using\nconnectionist temporal classification (CTC) model on limited Chinese vocabulary\nusing electroencephalography (EEG) features with no speech signal as input and\nwe further demonstrate single CTC model based continuous noisy speech\nrecognition on limited joint English and Chinese vocabulary using EEG features\nwith no speech signal as input. We demonstrate our results using various EEG\nfeature sets recently introduced in [1] as well as we propose a new deep\nlearning architecture in this paper which can perform continuous speech\nrecognition using raw EEG signals on limited joint English and Chinese\nvocabulary.", "published": "2019-06-17 20:25:59", "link": "http://arxiv.org/abs/1906.08045v5", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP", "stat.ML"], "primary_category": "eess.AS"}
