{"title": "Identifying civilians killed by police with distantly supervised\n  entity-event extraction", "abstract": "We propose a new, socially-impactful task for natural language processing:\nfrom a news corpus, extract names of persons who have been killed by police. We\npresent a newly collected police fatality corpus, which we release publicly,\nand present a model to solve this problem that uses EM-based distant\nsupervision with logistic regression and convolutional neural network\nclassifiers. Our model outperforms two off-the-shelf event extractor systems,\nand it can suggest candidate victim names in some cases faster than one of the\nmajor manually-collected police fatality databases.", "published": "2017-07-22 01:47:36", "link": "http://arxiv.org/abs/1707.07086v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Predicting the Gender of Indonesian Names", "abstract": "We investigated a way to predict the gender of a name using character-level\nLong-Short Term Memory (char-LSTM). We compared our method with some\nconventional machine learning methods, namely Naive Bayes, logistic regression,\nand XGBoost with n-grams as the features. We evaluated the models on a dataset\nconsisting of the names of Indonesian people. It is not common to use a family\nname as the surname in Indonesian culture, except in some ethnicities.\nTherefore, we inferred the gender from both full names and first names. The\nresults show that we can achieve 92.25% accuracy from full names, while using\nfirst names only yields 90.65% accuracy. These results are better than the ones\nfrom applying the classical machine learning algorithms to n-grams.", "published": "2017-07-22 09:35:10", "link": "http://arxiv.org/abs/1707.07129v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Native Language Identification on Text and Speech", "abstract": "This paper presents an ensemble system combining the output of multiple SVM\nclassifiers to native language identification (NLI). The system was submitted\nto the NLI Shared Task 2017 fusion track which featured students essays and\nspoken responses in form of audio transcriptions and iVectors by non-native\nEnglish speakers of eleven native languages. Our system competed in the\nchallenge under the team name ZCD and was based on an ensemble of SVM\nclassifiers trained on character n-grams achieving 83.58% accuracy and ranking\n3rd in the shared task.", "published": "2017-07-22 16:04:49", "link": "http://arxiv.org/abs/1707.07182v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"i have a feeling trump will win..................\": Forecasting Winners\n  and Losers from User Predictions on Twitter", "abstract": "Social media users often make explicit predictions about upcoming events.\nSuch statements vary in the degree of certainty the author expresses toward the\noutcome:\"Leonardo DiCaprio will win Best Actor\" vs. \"Leonardo DiCaprio may win\"\nor \"No way Leonardo wins!\". Can popular beliefs on social media predict who\nwill win? To answer this question, we build a corpus of tweets annotated for\nveridicality on which we train a log-linear classifier that detects positive\nveridicality with high precision. We then forecast uncertain outcomes using the\nwisdom of crowds, by aggregating users' explicit predictions. Our method for\nforecasting winners is fully automated, relying only on a set of contenders as\ninput. It requires no training data of past outcomes and outperforms sentiment\nand tweet volume baselines on a broad range of contest prediction tasks. We\nfurther demonstrate how our approach can be used to measure the reliability of\nindividual accounts' predictions and retrospectively identify surprise\noutcomes.", "published": "2017-07-22 20:34:53", "link": "http://arxiv.org/abs/1707.07212v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OBJ2TEXT: Generating Visually Descriptive Language from Object Layouts", "abstract": "Generating captions for images is a task that has recently received\nconsiderable attention. In this work we focus on caption generation for\nabstract scenes, or object layouts where the only information provided is a set\nof objects and their locations. We propose OBJ2TEXT, a sequence-to-sequence\nmodel that encodes a set of objects and their locations as an input sequence\nusing an LSTM network, and decodes this representation using an LSTM language\nmodel. We show that our model, despite encoding object layouts as a sequence,\ncan represent spatial relationships between objects, and generate descriptions\nthat are globally coherent and semantically relevant. We test our approach in a\ntask of object-layout captioning by using only object annotations as inputs. We\nadditionally show that our model, combined with a state-of-the-art object\ndetector, improves an image captioning model from 0.863 to 0.950 (CIDEr score)\nin the test benchmark of the standard MS-COCO Captioning task.", "published": "2017-07-22 04:17:42", "link": "http://arxiv.org/abs/1707.07102v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Attention-Based End-to-End Speech Recognition on Voice Search", "abstract": "Recently, there has been a growing interest in end-to-end speech recognition\nthat directly transcribes speech to text without any predefined alignments. In\nthis paper, we explore the use of attention-based encoder-decoder model for\nMandarin speech recognition on a voice search task. Previous attempts have\nshown that applying attention-based encoder-decoder to Mandarin speech\nrecognition was quite difficult due to the logographic orthography of Mandarin,\nthe large vocabulary and the conditional dependency of the attention model. In\nthis paper, we use character embedding to deal with the large vocabulary.\nSeveral tricks are used for effective model training, including L2\nregularization, Gaussian weight noise and frame skipping. We compare two\nattention mechanisms and use attention smoothing to cover long context in the\nattention model. Taken together, these tricks allow us to finally achieve a\ncharacter error rate (CER) of 3.58% and a sentence error rate (SER) of 7.43% on\nthe MiTV voice search dataset. While together with a trigram language model,\nCER and SER reach 2.81% and 5.77%, respectively.", "published": "2017-07-22 13:53:28", "link": "http://arxiv.org/abs/1707.07167v3", "categories": ["cs.CL", "cs.SD"], "primary_category": "cs.CL"}
{"title": "MoodSwipe: A Soft Keyboard that Suggests Messages Based on\n  User-Specified Emotions", "abstract": "We present MoodSwipe, a soft keyboard that suggests text messages given the\nuser-specified emotions utilizing the real dialog data. The aim of MoodSwipe is\nto create a convenient user interface to enjoy the technology of emotion\nclassification and text suggestion, and at the same time to collect labeled\ndata automatically for developing more advanced technologies. While users\nselect the MoodSwipe keyboard, they can type as usual but sense the emotion\nconveyed by their text and receive suggestions for their message as a benefit.\nIn MoodSwipe, the detected emotions serve as the medium for suggested texts,\nwhere viewing the latter is the incentive to correcting the former. We conduct\nseveral experiments to show the superiority of the emotion classification\nmodels trained on the dialog data, and further to verify good emotion cues are\nimportant context for text suggestion.", "published": "2017-07-22 16:32:16", "link": "http://arxiv.org/abs/1707.07191v1", "categories": ["cs.CL", "cs.HC", "H.5.2; H.5.3; I.2.7"], "primary_category": "cs.CL"}
