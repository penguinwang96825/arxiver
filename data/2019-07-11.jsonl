{"title": "No Word is an Island -- A Transformation Weighting Model for Semantic\n  Composition", "abstract": "Composition models of distributional semantics are used to construct phrase\nrepresentations from the representations of their words. Composition models are\ntypically situated on two ends of a spectrum. They either have a small number\nof parameters but compose all phrases in the same way, or they perform\nword-specific compositions at the cost of a far larger number of parameters. In\nthis paper we propose transformation weighting (TransWeight), a composition\nmodel that consistently outperforms existing models on nominal compounds,\nadjective-noun phrases and adverb-adjective phrases in English, German and\nDutch. TransWeight drastically reduces the number of parameters needed compared\nto the best model in the literature by composing similar words in the same way.", "published": "2019-07-11 08:41:53", "link": "http://arxiv.org/abs/1907.05048v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incrementalizing RASA's Open-Source Natural Language Understanding\n  Pipeline", "abstract": "As spoken dialogue systems and chatbots are gaining more widespread adoption,\ncommercial and open-sourced services for natural language understanding are\nemerging. In this paper, we explain how we altered the open-source RASA natural\nlanguage understanding pipeline to process incrementally (i.e., word-by-word),\nfollowing the incremental unit framework proposed by Schlangen and Skantze. To\ndo so, we altered existing RASA components to process incrementally, and added\nan update-incremental intent recognition model as a component to RASA. Our\nevaluations on the Snips dataset show that our changes allow RASA to function\nas an effective incremental natural language understanding service.", "published": "2019-07-11 17:35:20", "link": "http://arxiv.org/abs/1907.05403v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Solving Hard Coreference Problems", "abstract": "Coreference resolution is a key problem in natural language understanding\nthat still escapes reliable solutions. One fundamental difficulty has been that\nof resolving instances involving pronouns since they often require deep\nlanguage understanding and use of background knowledge. In this paper, we\npropose an algorithmic solution that involves a new representation for the\nknowledge required to address hard coreference problems, along with a\nconstrained optimization framework that uses this knowledge in coreference\ndecision making. Our representation, Predicate Schemas, is instantiated with\nknowledge acquired in an unsupervised way, and is compiled automatically into\nconstraints that impact the coreference decision. We present a general\ncoreference resolution system that significantly improves state-of-the-art\nperformance on hard, Winograd-style, pronoun resolution cases, while still\nperforming at the state-of-the-art level on standard coreference resolution\ndatasets.", "published": "2019-07-11 23:40:56", "link": "http://arxiv.org/abs/1907.05524v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Massively Multilingual Neural Machine Translation in the Wild: Findings\n  and Challenges", "abstract": "We introduce our efforts towards building a universal neural machine\ntranslation (NMT) system capable of translating between any language pair. We\nset a milestone towards this goal by building a single massively multilingual\nNMT model handling 103 languages trained on over 25 billion examples. Our\nsystem demonstrates effective transfer learning ability, significantly\nimproving translation quality of low-resource languages, while keeping\nhigh-resource language translation quality on-par with competitive bilingual\nbaselines. We provide in-depth analysis of various aspects of model building\nthat are crucial to achieving quality and practicality in universal NMT. While\nwe prototype a high-quality universal translation system, our extensive\nempirical analysis exposes issues that need to be further addressed, and we\nsuggest directions for future research.", "published": "2019-07-11 06:47:30", "link": "http://arxiv.org/abs/1907.05019v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MeetUp! A Corpus of Joint Activity Dialogues in a Visual Environment", "abstract": "Building computer systems that can converse about their visual environment is\none of the oldest concerns of research in Artificial Intelligence and\nComputational Linguistics (see, for example, Winograd's 1972 SHRDLU system).\nOnly recently, however, have methods from computer vision and natural language\nprocessing become powerful enough to make this vision seem more attainable.\nPushed especially by developments in computer vision, many data sets and\ncollection environments have recently been published that bring together verbal\ninteraction and visual processing. Here, we argue that these datasets tend to\noversimplify the dialogue part, and we propose a task---MeetUp!---that requires\nboth visual and conversational grounding, and that makes stronger demands on\nrepresentations of the discourse. MeetUp! is a two-player coordination game\nwhere players move in a visual environment, with the objective of finding each\nother. To do so, they must talk about what they see, and achieve mutual\nunderstanding. We describe a data collection and show that the resulting\ndialogues indeed exhibit the dialogue phenomena of interest, while also\nchallenging the language & vision aspect.", "published": "2019-07-11 10:06:20", "link": "http://arxiv.org/abs/1907.05084v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Self-Regulated Interactive Sequence-to-Sequence Learning", "abstract": "Not all types of supervision signals are created equal: Different types of\nfeedback have different costs and effects on learning. We show how\nself-regulation strategies that decide when to ask for which kind of feedback\nfrom a teacher (or from oneself) can be cast as a learning-to-learn problem\nleading to improved cost-aware sequence-to-sequence learning. In experiments on\ninteractive neural machine translation, we find that the self-regulator\ndiscovers an $\\epsilon$-greedy strategy for the optimal cost-quality trade-off\nby mixing different feedback types including corrections, error markups, and\nself-supervision. Furthermore, we demonstrate its robustness under domain shift\nand identify it as a promising alternative to active learning.", "published": "2019-07-11 13:42:46", "link": "http://arxiv.org/abs/1907.05190v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Collaborative Multi-Agent Dialogue Model Training Via Reinforcement\n  Learning", "abstract": "We present the first complete attempt at concurrently training conversational\nagents that communicate only via self-generated language. Using DSTC2 as seed\ndata, we trained natural language understanding (NLU) and generation (NLG)\nnetworks for each agent and let the agents interact online. We model the\ninteraction as a stochastic collaborative game where each agent (player) has a\nrole (\"assistant\", \"tourist\", \"eater\", etc.) and their own objectives, and can\nonly interact via natural language they generate. Each agent, therefore, needs\nto learn to operate optimally in an environment with multiple sources of\nuncertainty (its own NLU and NLG, the other agent's NLU, Policy, and NLG). In\nour evaluation, we show that the stochastic-game agents outperform deep\nlearning based supervised baselines.", "published": "2019-07-11 22:05:48", "link": "http://arxiv.org/abs/1907.05507v2", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Activitynet 2019 Task 3: Exploring Contexts for Dense Captioning Events\n  in Videos", "abstract": "Contextual reasoning is essential to understand events in long untrimmed\nvideos. In this work, we systematically explore different captioning models\nwith various contexts for the dense-captioning events in video task, which aims\nto generate captions for different events in the untrimmed video. We propose\nfive types of contexts as well as two categories of event captioning models,\nand evaluate their contributions for event captioning from both accuracy and\ndiversity aspects. The proposed captioning models are plugged into our pipeline\nsystem for the dense video captioning challenge. The overall system achieves\nthe state-of-the-art performance on the dense-captioning events in video task\nwith 9.91 METEOR score on the challenge testing set.", "published": "2019-07-11 10:29:04", "link": "http://arxiv.org/abs/1907.05092v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "General Evaluation for Instruction Conditioned Navigation using Dynamic\n  Time Warping", "abstract": "In instruction conditioned navigation, agents interpret natural language and\ntheir surroundings to navigate through an environment. Datasets for studying\nthis task typically contain pairs of these instructions and reference\ntrajectories. Yet, most evaluation metrics used thus far fail to properly\naccount for the latter, relying instead on insufficient similarity comparisons.\nWe address fundamental flaws in previously used metrics and show how Dynamic\nTime Warping (DTW), a long known method of measuring similarity between two\ntime series, can be used for evaluation of navigation agents. For such, we\ndefine the normalized Dynamic Time Warping (nDTW) metric, that softly penalizes\ndeviations from the reference path, is naturally sensitive to the order of the\nnodes composing each path, is suited for both continuous and graph-based\nevaluations, and can be efficiently calculated. Further, we define SDTW, which\nconstrains nDTW to only successful paths. We collect human similarity judgments\nfor simulated paths and find nDTW correlates better with human rankings than\nall other metrics. We also demonstrate that using nDTW as a reward signal for\nReinforcement Learning navigation agents improves their performance on both the\nRoom-to-Room (R2R) and Room-for-Room (R4R) datasets. The R4R results in\nparticular highlight the superiority of SDTW over previous success-constrained\nmetrics.", "published": "2019-07-11 18:42:03", "link": "http://arxiv.org/abs/1907.05446v2", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Knowledge-incorporating ESIM models for Response Selection in\n  Retrieval-based Dialog Systems", "abstract": "Goal-oriented dialog systems, which can be trained end-to-end without\nmanually encoding domain-specific features, show tremendous promise in the\ncustomer support use-case e.g. flight booking, hotel reservation, technical\nsupport, student advising etc. These dialog systems must learn to interact with\nexternal domain knowledge to achieve the desired goal e.g. recommending courses\nto a student, booking a table at a restaurant etc. This paper presents extended\nEnhanced Sequential Inference Model (ESIM) models: a) K-ESIM (Knowledge-ESIM),\nwhich incorporates the external domain knowledge and b) T-ESIM (Targeted-ESIM),\nwhich leverages information from similar conversations to improve the\nprediction accuracy. Our proposed models and the baseline ESIM model are\nevaluated on the Ubuntu and Advising datasets in the Sentence Selection track\nof the latest Dialog System Technology Challenge (DSTC7), where the goal is to\nfind the correct next utterance, given a partial conversation, from a set of\ncandidates. Our preliminary results suggest that incorporating external\nknowledge sources and leveraging information from similar dialogs leads to\nperformance improvements for predicting the next utterance.", "published": "2019-07-11 15:55:24", "link": "http://arxiv.org/abs/1907.05792v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Multichannel Loss Function for Supervised Speech Source Separation by\n  Mask-based Beamforming", "abstract": "In this paper, we propose two mask-based beamforming methods using a deep\nneural network (DNN) trained by multichannel loss functions. Beamforming\ntechnique using time-frequency (TF)-masks estimated by a DNN have been applied\nto many applications where TF-masks are used for estimating spatial covariance\nmatrices. To train a DNN for mask-based beamforming, loss functions designed\nfor monaural speech enhancement/separation have been employed. Although such a\ntraining criterion is simple, it does not directly correspond to the\nperformance of mask-based beamforming. To overcome this problem, we use\nmultichannel loss functions which evaluate the estimated spatial covariance\nmatrices based on the multichannel Itakura--Saito divergence. DNNs trained by\nthe multichannel loss functions can be applied to construct several\nbeamformers. Experimental results confirmed their effectiveness and robustness\nto microphone configurations.", "published": "2019-07-11 03:41:20", "link": "http://arxiv.org/abs/1907.04984v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Polyphonic Sound Event and Sound Activity Detection: A Multi-task\n  approach", "abstract": "Polyphonic Sound Event Detection (SED) in real-world recordings is a\nchallenging task because of the dynamic polyphony level, intensity, and\nduration of sound events. Current polyphonic SED systems fail to model the\ntemporal structure of sound events explicitly and instead attempt to look at\nwhich sound events are present at each audio frame. Consequently, the\nevent-wise detection performance is much lower than the segment-wise detection\nperformance. In this work, we propose a joint model approach to improve the\ntemporal localization of sound events using a multi-task learning setup. The\nfirst task predicts which sound events are present at each time frame; we call\nthis branch 'Sound Event Detection (SED) model', while the second task predicts\nif a sound event is present or not at each frame; we call this branch 'Sound\nActivity Detection (SAD) model'. We verify the proposed joint model by\ncomparing it with a separate implementation of both tasks aggregated together\nfrom individual task predictions. Our experiments on the URBAN-SED dataset show\nthat the proposed joint model can alleviate False Positive (FP) and False\nNegative (FN) errors and improve both the segment-wise and the event-wise\nmetrics.", "published": "2019-07-11 11:41:44", "link": "http://arxiv.org/abs/1907.05122v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "My lips are concealed: Audio-visual speech enhancement through\n  obstructions", "abstract": "Our objective is an audio-visual model for separating a single speaker from a\nmixture of sounds such as other speakers and background noise. Moreover, we\nwish to hear the speaker even when the visual cues are temporarily absent due\nto occlusion. To this end we introduce a deep audio-visual speech enhancement\nnetwork that is able to separate a speaker's voice by conditioning on both the\nspeaker's lip movements and/or a representation of their voice. The voice\nrepresentation can be obtained by either (i) enrollment, or (ii) by\nself-enrollment -- learning the representation on-the-fly given sufficient\nunobstructed visual input. The model is trained by blending audios, and by\nintroducing artificial occlusions around the mouth region that prevent the\nvisual modality from dominating. The method is speaker-independent, and we\ndemonstrate it on real examples of speakers unheard (and unseen) during\ntraining. The method also improves over previous models in particular for cases\nof occlusion in the visual modality.", "published": "2019-07-11 02:05:48", "link": "http://arxiv.org/abs/1907.04975v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Heard More Than Heard: An Audio Steganography Method Based on GAN", "abstract": "Audio steganography is a collection of techniques for concealing the\nexistence of information by embedding it within a non-secret audio, which is\nreferred to as carrier. Distinct from cryptography, the steganography put\nemphasis on the hiding of the secret existence. The existing audio\nsteganography methods mainly depend on human handcraft, while we proposed an\naudio steganography algorithm which automatically generated from adversarial\ntraining. The method consists of three neural networks: encoder which embeds\nthe secret message in the carrier, decoder which extracts the message, and\ndiscriminator which determine the carriers contain secret messages. All the\nnetworks are simultaneously trained to create embedding, extracting and\ndiscriminating process. The system is trained with different training settings\non two datasets. Competed the majority of audio steganographic schemes, the\nproposed scheme could produce high fidelity steganographic audio which contains\nsecret audio. Besides, the additional experiments verify the robustness and\nsecurity of our algorithm.", "published": "2019-07-11 03:46:21", "link": "http://arxiv.org/abs/1907.04986v1", "categories": ["cs.MM", "cs.CR", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Optimized Sharing of Coefficients in Parallel Filter Banks", "abstract": "Filters are the basic and most important blocks of most signal processing\napplications. In many applications, a group of parallel filters are used as\nfilter banks. Parallel filter banks naturally require much more computations.\nEspecially on chip applications, the resources are limited and shared among\nmany algorithms. For this purpose, many filter optimization schemes are\nproposed to reduce the number of resources that filtering operations require.\nIn this work, a novel optimization algorithm is proposed to decrease the number\nof operations in a group of parallel filters. The filter coefficients are\ngrouped in a two stage process which enables increased coefficient sharing\nbetween different filters. The algorithm is capable of decreasing the number of\nregisters, look-up tables and DSP48s by up to 50\\% of a regular parallel filter\nbank, without requiring increased sampling rate.", "published": "2019-07-11 16:19:19", "link": "http://arxiv.org/abs/1907.05351v1", "categories": ["eess.SP", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "eess.SP"}
{"title": "Deep auscultation: Predicting respiratory anomalies and diseases via\n  recurrent neural networks", "abstract": "Respiratory diseases are among the most common causes of severe illness and\ndeath worldwide. Prevention and early diagnosis are essential to limit or even\nreverse the trend that characterizes the diffusion of such diseases. In this\nregard, the development of advanced computational tools for the analysis of\nrespiratory auscultation sounds can become a game changer for detecting\ndisease-related anomalies, or diseases themselves. In this work, we propose a\nnovel learning framework for respiratory auscultation sound data. Our approach\ncombines state-of-the-art feature extraction techniques and advanced\ndeep-neural-network architectures. Remarkably, to the best of our knowledge, we\nare the first to model a recurrent-neural-network based learning framework to\nsupport the clinician in detecting respiratory diseases, at either level of\nabnormal sounds or pathology classes. Results obtained on the ICBHI benchmark\ndataset show that our approach outperforms competing methods on both\nanomaly-driven and pathology-driven prediction tasks, thus advancing the\nstate-of-the-art in respiratory disease analysis.", "published": "2019-07-11 17:16:42", "link": "http://arxiv.org/abs/1907.05708v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
