{"title": "A Framework for Hierarchical Multilingual Machine Translation", "abstract": "Multilingual machine translation has recently been in vogue given its\npotential for improving machine translation performance for low-resource\nlanguages via transfer learning. Empirical examinations demonstrating the\nsuccess of existing multilingual machine translation strategies, however, are\nlimited to experiments in specific language groups. In this paper, we present a\nhierarchical framework for building multilingual machine translation strategies\nthat takes advantage of a typological language family tree for enabling\ntransfer among similar languages while avoiding the negative effects that\nresult from incorporating languages that are too different to each other.\nExhaustive experimentation on a dataset with 41 languages demonstrates the\nvalidity of the proposed framework, especially when it comes to improving the\nperformance of low-resource languages via the use of typologically related\nfamilies for which richer sets of resources are available.", "published": "2020-05-12 01:24:43", "link": "http://arxiv.org/abs/2005.05507v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neighborhood Matching Network for Entity Alignment", "abstract": "Structural heterogeneity between knowledge graphs is an outstanding challenge\nfor entity alignment. This paper presents Neighborhood Matching Network (NMN),\na novel entity alignment framework for tackling the structural heterogeneity\nchallenge. NMN estimates the similarities between entities to capture both the\ntopological structure and the neighborhood difference. It provides two\ninnovative components for better learning representations for entity alignment.\nIt first uses a novel graph sampling method to distill a discriminative\nneighborhood for each entity. It then adopts a cross-graph neighborhood\nmatching module to jointly encode the neighborhood difference for a given\nentity pair. Such strategies allow NMN to effectively construct\nmatching-oriented entity representations while ignoring noisy neighbors that\nhave a negative impact on the alignment task. Extensive experiments performed\non three entity alignment datasets show that NMN can well estimate the\nneighborhood similarity in more tough cases and significantly outperforms 12\nprevious state-of-the-art methods.", "published": "2020-05-12 08:26:15", "link": "http://arxiv.org/abs/2005.05607v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis", "abstract": "Recently, sentiment analysis has seen remarkable advance with the help of\npre-training approaches. However, sentiment knowledge, such as sentiment words\nand aspect-sentiment pairs, is ignored in the process of pre-training, despite\nthe fact that they are widely used in traditional sentiment analysis\napproaches. In this paper, we introduce Sentiment Knowledge Enhanced\nPre-training (SKEP) in order to learn a unified sentiment representation for\nmultiple sentiment analysis tasks. With the help of automatically-mined\nknowledge, SKEP conducts sentiment masking and constructs three sentiment\nknowledge prediction objectives, so as to embed sentiment information at the\nword, polarity and aspect level into pre-trained sentiment representation. In\nparticular, the prediction of aspect-sentiment pairs is converted into\nmulti-label classification, aiming to capture the dependency between words in a\npair. Experiments on three kinds of sentiment tasks show that SKEP\nsignificantly outperforms strong pre-training baseline, and achieves new\nstate-of-the-art results on most of the test datasets. We release our code at\nhttps://github.com/baidu/Senta.", "published": "2020-05-12 09:23:32", "link": "http://arxiv.org/abs/2005.05635v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Frobenius Algebraic Analysis for Parasitic Gaps", "abstract": "The interpretation of parasitic gaps is an ostensible case of non-linearity\nin natural language composition. Existing categorial analyses, both in the\ntypelogical and in the combinatory traditions, rely on explicit forms of\nsyntactic copying. We identify two types of parasitic gapping where the\nduplication of semantic content can be confined to the lexicon. Parasitic gaps\nin adjuncts are analysed as forms of generalized coordination with a\npolymorphic type schema for the head of the adjunct phrase. For parasitic gaps\naffecting arguments of the same predicate, the polymorphism is associated with\nthe lexical item that introduces the primary gap. Our analysis is formulated in\nterms of Lambek calculus extended with structural control modalities. A\ncompositional translation relates syntactic types and derivations to the\ninterpreting compact closed category of finite dimensional vector spaces and\nlinear maps with Frobenius algebras over it. When interpreted over the\nnecessary semantic spaces, the Frobenius algebras provide the tools to model\nthe proposed instances of lexical polymorphism.", "published": "2020-05-12 09:36:15", "link": "http://arxiv.org/abs/2005.05639v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Robustness of Language Encoders against Grammatical Errors", "abstract": "We conduct a thorough study to diagnose the behaviors of pre-trained language\nencoders (ELMo, BERT, and RoBERTa) when confronted with natural grammatical\nerrors. Specifically, we collect real grammatical errors from non-native\nspeakers and conduct adversarial attacks to simulate these errors on clean text\ndata. We use this approach to facilitate debugging models on downstream\napplications. Results confirm that the performance of all tested models is\naffected but the degree of impact varies. To interpret model behaviors, we\nfurther design a linguistic acceptability task to reveal their abilities in\nidentifying ungrammatical sentences and the position of errors. We find that\nfixed contextual encoders with a simple classifier trained on the prediction of\nsentence correctness are able to locate error positions. We also design a cloze\ntest for BERT and discover that BERT captures the interaction between errors\nand specific tokens in context. Our results shed light on understanding the\nrobustness and behaviors of language encoders against grammatical errors.", "published": "2020-05-12 11:01:44", "link": "http://arxiv.org/abs/2005.05683v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Multiword Expression Type Helps Lexical Complexity Assessment", "abstract": "Multiword expressions (MWEs) represent lexemes that should be treated as\nsingle lexical units due to their idiosyncratic nature. Multiple NLP\napplications have been shown to benefit from MWE identification, however the\nresearch on lexical complexity of MWEs is still an under-explored area. In this\nwork, we re-annotate the Complex Word Identification Shared Task 2018 dataset\nof Yimam et al. (2017), which provides complexity scores for a range of\nlexemes, with the types of MWEs. We release the MWE-annotated dataset with this\npaper, and we believe this dataset represents a valuable resource for the text\nsimplification community. In addition, we investigate which types of\nexpressions are most problematic for native and non-native readers. Finally, we\nshow that a lexical complexity assessment system benefits from the information\nabout MWE types.", "published": "2020-05-12 11:25:07", "link": "http://arxiv.org/abs/2005.05692v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reassessing Claims of Human Parity and Super-Human Performance in\n  Machine Translation at WMT 2019", "abstract": "We reassess the claims of human parity and super-human performance made at\nthe news shared task of WMT 2019 for three translation directions:\nEnglish-to-German, English-to-Russian and German-to-English. First we identify\nthree potential issues in the human evaluation of that shared task: (i) the\nlimited amount of intersentential context available, (ii) the limited\ntranslation proficiency of the evaluators and (iii) the use of a reference\ntranslation. We then conduct a modified evaluation taking these issues into\naccount. Our results indicate that all the claims of human parity and\nsuper-human performance made at WMT 2019 should be refuted, except the claim of\nhuman parity for English-to-German. Based on our findings, we put forward a set\nof recommendations and open questions for future assessments of human parity in\nmachine translation.", "published": "2020-05-12 13:09:29", "link": "http://arxiv.org/abs/2005.05738v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Document Modeling with Graph Attention Networks for Multi-grained\n  Machine Reading Comprehension", "abstract": "Natural Questions is a new challenging machine reading comprehension\nbenchmark with two-grained answers, which are a long answer (typically a\nparagraph) and a short answer (one or more entities inside the long answer).\nDespite the effectiveness of existing methods on this benchmark, they treat\nthese two sub-tasks individually during training while ignoring their\ndependencies. To address this issue, we present a novel multi-grained machine\nreading comprehension framework that focuses on modeling documents at their\nhierarchical nature, which are different levels of granularity: documents,\nparagraphs, sentences, and tokens. We utilize graph attention networks to\nobtain different levels of representations so that they can be learned\nsimultaneously. The long and short answers can be extracted from\nparagraph-level representation and token-level representation, respectively. In\nthis way, we can model the dependencies between the two-grained answers to\nprovide evidence for each other. We jointly train the two sub-tasks, and our\nexperiments show that our approach significantly outperforms previous systems\nat both long and short answer criteria.", "published": "2020-05-12 14:20:09", "link": "http://arxiv.org/abs/2005.05806v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Syntactic Structure for Better Language Modeling: A Syntactic\n  Distance Approach", "abstract": "It is commonly believed that knowledge of syntactic structure should improve\nlanguage modeling. However, effectively and computationally efficiently\nincorporating syntactic structure into neural language models has been a\nchallenging topic. In this paper, we make use of a multi-task objective, i.e.,\nthe models simultaneously predict words as well as ground truth parse trees in\na form called \"syntactic distances\", where information between these two\nseparate objectives shares the same intermediate representation. Experimental\nresults on the Penn Treebank and Chinese Treebank datasets show that when\nground truth parse trees are provided as additional training signals, the model\nis able to achieve lower perplexity and induce trees with better quality.", "published": "2020-05-12 15:35:00", "link": "http://arxiv.org/abs/2005.05864v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Modality Relevance for Reasoning on Language and Vision", "abstract": "This work deals with the challenge of learning and reasoning over language\nand vision data for the related downstream tasks such as visual question\nanswering (VQA) and natural language for visual reasoning (NLVR). We design a\nnovel cross-modality relevance module that is used in an end-to-end framework\nto learn the relevance representation between components of various input\nmodalities under the supervision of a target task, which is more generalizable\nto unobserved data compared to merely reshaping the original representation\nspace. In addition to modeling the relevance between the textual entities and\nvisual entities, we model the higher-order relevance between entity relations\nin the text and object relations in the image. Our proposed approach shows\ncompetitive performance on two different language and vision tasks using public\nbenchmarks and improves the state-of-the-art published results. The learned\nalignments of input spaces and their relevance representations by NLVR task\nboost the training efficiency of VQA task.", "published": "2020-05-12 20:17:25", "link": "http://arxiv.org/abs/2005.06035v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simultaneous paraphrasing and translation by fine-tuning Transformer\n  models", "abstract": "This paper describes the third place submission to the shared task on\nsimultaneous translation and paraphrasing for language education at the 4th\nworkshop on Neural Generation and Translation (WNGT) for ACL 2020. The final\nsystem leverages pre-trained translation models and uses a Transformer\narchitecture combined with an oversampling strategy to achieve a competitive\nperformance. This system significantly outperforms the baseline on Hungarian\n(27% absolute improvement in Weighted Macro F1 score) and Portuguese (33%\nabsolute improvement) languages.", "published": "2020-05-12 06:34:42", "link": "http://arxiv.org/abs/2005.05570v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning and Evaluating Emotion Lexicons for 91 Languages", "abstract": "Emotion lexicons describe the affective meaning of words and thus constitute\na centerpiece for advanced sentiment and emotion analysis. Yet, manually\ncurated lexicons are only available for a handful of languages, leaving most\nlanguages of the world without such a precious resource for downstream\napplications. Even worse, their coverage is often limited both in terms of the\nlexical units they contain and the emotional variables they feature. In order\nto break this bottleneck, we here introduce a methodology for creating almost\narbitrarily large emotion lexicons for any target language. Our approach\nrequires nothing but a source language emotion lexicon, a bilingual word\ntranslation model, and a target language embedding model. Fulfilling these\nrequirements for 91 languages, we are able to generate representationally rich\nhigh-coverage lexicons comprising eight emotional variables with more than 100k\nlexical entries each. We evaluated the automatically generated lexicons against\nhuman judgment from 26 datasets, spanning 12 typologically diverse languages,\nand found that our approach produces results in line with state-of-the-art\nmonolingual approaches to lexicon creation and even surpasses human reliability\nfor some languages and variables. Code and data are available at\nhttps://github.com/JULIELab/MEmoLon archived under DOI\nhttps://doi.org/10.5281/zenodo.3779901.", "published": "2020-05-12 10:32:03", "link": "http://arxiv.org/abs/2005.05672v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dynamic Memory Induction Networks for Few-Shot Text Classification", "abstract": "This paper proposes Dynamic Memory Induction Networks (DMIN) for few-shot\ntext classification. The model utilizes dynamic routing to provide more\nflexibility to memory-based few-shot learning in order to better adapt the\nsupport sets, which is a critical capacity of few-shot classification models.\nBased on that, we further develop induction models with query information,\naiming to enhance the generalization ability of meta-learning. The proposed\nmodel achieves new state-of-the-art results on the miniRCV1 and ODIC dataset,\nimproving the best performance (accuracy) by 2~4%. Detailed analysis is further\nperformed to show the effectiveness of each component.", "published": "2020-05-12 12:41:14", "link": "http://arxiv.org/abs/2005.05727v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Do not let the history haunt you -- Mitigating Compounding Errors in\n  Conversational Question Answering", "abstract": "The Conversational Question Answering (CoQA) task involves answering a\nsequence of inter-related conversational questions about a contextual\nparagraph. Although existing approaches employ human-written ground-truth\nanswers for answering conversational questions at test time, in a realistic\nscenario, the CoQA model will not have any access to ground-truth answers for\nthe previous questions, compelling the model to rely upon its own previously\npredicted answers for answering the subsequent questions. In this paper, we\nfind that compounding errors occur when using previously predicted answers at\ntest time, significantly lowering the performance of CoQA systems. To solve\nthis problem, we propose a sampling strategy that dynamically selects between\ntarget answers and model predictions during training, thereby closely\nsimulating the situation at test time. Further, we analyse the severity of this\nphenomena as a function of the question type, conversation length and domain\ntype.", "published": "2020-05-12 13:29:38", "link": "http://arxiv.org/abs/2005.05754v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "WinoWhy: A Deep Diagnosis of Essential Commonsense Knowledge for\n  Answering Winograd Schema Challenge", "abstract": "In this paper, we present the first comprehensive categorization of essential\ncommonsense knowledge for answering the Winograd Schema Challenge (WSC). For\neach of the questions, we invite annotators to first provide reasons for making\ncorrect decisions and then categorize them into six major knowledge categories.\nBy doing so, we better understand the limitation of existing methods (i.e.,\nwhat kind of knowledge cannot be effectively represented or inferred with\nexisting methods) and shed some light on the commonsense knowledge that we need\nto acquire in the future for better commonsense reasoning. Moreover, to\ninvestigate whether current WSC models can understand the commonsense or they\nsimply solve the WSC questions based on the statistical bias of the dataset, we\nleverage the collected reasons to develop a new task called WinoWhy, which\nrequires models to distinguish plausible reasons from very similar but wrong\nreasons for all WSC questions. Experimental results prove that even though\npre-trained language representation models have achieved promising progress on\nthe original WSC dataset, they are still struggling at WinoWhy. Further\nexperiments show that even though supervised models can achieve better\nperformance, the performance of these models can be sensitive to the dataset\ndistribution. WinoWhy and all codes are available at:\nhttps://github.com/HKUST-KnowComp/WinoWhy.", "published": "2020-05-12 13:40:06", "link": "http://arxiv.org/abs/2005.05763v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "A Report on the 2020 Sarcasm Detection Shared Task", "abstract": "Detecting sarcasm and verbal irony is critical for understanding people's\nactual sentiments and beliefs. Thus, the field of sarcasm analysis has become a\npopular research problem in natural language processing. As the community\nworking on computational approaches for sarcasm detection is growing, it is\nimperative to conduct benchmarking studies to analyze the current\nstate-of-the-art, facilitating progress in this area. We report on the shared\ntask on sarcasm detection we conducted as a part of the 2nd Workshop on\nFigurative Language Processing (FigLang 2020) at ACL 2020.", "published": "2020-05-12 14:27:19", "link": "http://arxiv.org/abs/2005.05814v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Intersectional Bias in Hate Speech and Abusive Language Datasets", "abstract": "Algorithms are widely applied to detect hate speech and abusive language in\nsocial media. We investigated whether the human-annotated data used to train\nthese algorithms are biased. We utilized a publicly available annotated Twitter\ndataset (Founta et al. 2018) and classified the racial, gender, and party\nidentification dimensions of 99,996 tweets. The results showed that African\nAmerican tweets were up to 3.7 times more likely to be labeled as abusive, and\nAfrican American male tweets were up to 77% more likely to be labeled as\nhateful compared to the others. These patterns were statistically significant\nand robust even when party identification was added as a control variable. This\nstudy provides the first systematic evidence on intersectional bias in datasets\nof hate speech and abusive language.", "published": "2020-05-12 16:58:48", "link": "http://arxiv.org/abs/2005.05921v3", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Semantic Scaffolds for Pseudocode-to-Code Generation", "abstract": "We propose a method for program generation based on semantic scaffolds,\nlightweight structures representing the high-level semantic and syntactic\ncomposition of a program. By first searching over plausible scaffolds then\nusing these as constraints for a beam search over programs, we achieve better\ncoverage of the search space when compared with existing techniques. We apply\nour hierarchical search method to the SPoC dataset for pseudocode-to-code\ngeneration, in which we are given line-level natural language pseudocode\nannotations and aim to produce a program satisfying execution-based test cases.\nBy using semantic scaffolds during inference, we achieve a 10% absolute\nimprovement in top-100 accuracy over the previous state-of-the-art.\nAdditionally, we require only 11 candidates to reach the top-3000 performance\nof the previous best approach when tested against unseen problems,\ndemonstrating a substantial improvement in efficiency.", "published": "2020-05-12 17:10:13", "link": "http://arxiv.org/abs/2005.05927v1", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Psychometric Analysis and Coupling of Emotions Between State Bulletins\n  and Twitter in India during COVID-19 Infodemic", "abstract": "COVID-19 infodemic has been spreading faster than the pandemic itself. The\nmisinformation riding upon the infodemic wave poses a major threat to people's\nhealth and governance systems. Since social media is the largest source of\ninformation, managing the infodemic not only requires mitigating of\nmisinformation but also an early understanding of psychological patterns\nresulting from it. During the COVID-19 crisis, Twitter alone has seen a sharp\n45% increase in the usage of its curated events page, and a 30% increase in its\ndirect messaging usage, since March 6th 2020. In this study, we analyze the\npsychometric impact and coupling of the COVID-19 infodemic with the official\nbulletins related to COVID-19 at the national and state level in India. We look\nat these two sources with a psycho-linguistic lens of emotions and quantified\nthe extent and coupling between the two. We modified path, a deep skip-gram\nbased open-sourced lexicon builder for effective capture of health-related\nemotions. We were then able to capture the time-evolution of health-related\nemotions in social media and official bulletins. An analysis of lead-lag\nrelationships between the time series of extracted emotions from official\nbulletins and social media using Granger's causality showed that state\nbulletins were leading the social media for some emotions such as Medical\nEmergency. Further insights that are potentially relevant for the policymaker\nand the communicators actively engaged in mitigating misinformation are also\ndiscussed. Our paper also introduces CoronaIndiaDataset2, the first social\nmedia based COVID-19 dataset at national and state levels from India with over\n5.6 million national and 2.6 million state-level tweets. Finally, we present\nour findings as COVibes, an interactive web application capturing psychometric\ninsights captured upon the CoronaIndiaDataset, both at a national and state\nlevel.", "published": "2020-05-12 01:51:07", "link": "http://arxiv.org/abs/2005.05513v2", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "DiscreTalk: Text-to-Speech as a Machine Translation Problem", "abstract": "This paper proposes a new end-to-end text-to-speech (E2E-TTS) model based on\nneural machine translation (NMT). The proposed model consists of two\ncomponents; a non-autoregressive vector quantized variational autoencoder\n(VQ-VAE) model and an autoregressive Transformer-NMT model. The VQ-VAE model\nlearns a mapping function from a speech waveform into a sequence of discrete\nsymbols, and then the Transformer-NMT model is trained to estimate this\ndiscrete symbol sequence from a given input text. Since the VQ-VAE model can\nlearn such a mapping in a fully-data-driven manner, we do not need to consider\nhyperparameters of the feature extraction required in the conventional E2E-TTS\nmodels. Thanks to the use of discrete symbols, we can use various techniques\ndeveloped in NMT and automatic speech recognition (ASR) such as beam search,\nsubword units, and fusions with a language model. Furthermore, we can avoid an\nover smoothing problem of predicted features, which is one of the common issues\nin TTS. The experimental evaluation with the JSUT corpus shows that the\nproposed method outperforms the conventional Transformer-TTS model with a\nnon-autoregressive neural vocoder in naturalness, achieving the performance\ncomparable to the reconstruction of the VQ-VAE model.", "published": "2020-05-12 02:45:09", "link": "http://arxiv.org/abs/2005.05525v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "AdaDurIAN: Few-shot Adaptation for Neural Text-to-Speech with DurIAN", "abstract": "This paper investigates how to leverage a DurIAN-based average model to\nenable a new speaker to have both accurate pronunciation and fluent\ncross-lingual speaking with very limited monolingual data. A weakness of the\nrecently proposed end-to-end text-to-speech (TTS) systems is that robust\nalignment is hard to achieve, which hinders it to scale well with very limited\ndata. To cope with this issue, we introduce AdaDurIAN by training an improved\nDurIAN-based average model and leverage it to few-shot learning with the shared\nspeaker-independent content encoder across different speakers. Several few-shot\nlearning tasks in our experiments show AdaDurIAN can outperform the baseline\nend-to-end system by a large margin. Subjective evaluations also show that\nAdaDurIAN yields higher mean opinion score (MOS) of naturalness and more\npreferences of speaker similarity. In addition, we also apply AdaDurIAN to\nemotion transfer tasks and demonstrate its promising performance.", "published": "2020-05-12 09:41:03", "link": "http://arxiv.org/abs/2005.05642v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Prta: A System to Support the Analysis of Propaganda Techniques in the\n  News", "abstract": "Recent events, such as the 2016 US Presidential Campaign, Brexit and the\nCOVID-19 \"infodemic\", have brought into the spotlight the dangers of online\ndisinformation. There has been a lot of research focusing on fact-checking and\ndisinformation detection. However, little attention has been paid to the\nspecific rhetorical and psychological techniques used to convey propaganda\nmessages. Revealing the use of such techniques can help promote media literacy\nand critical thinking, and eventually contribute to limiting the impact of\n\"fake news\" and disinformation campaigns. Prta (Propaganda Persuasion\nTechniques Analyzer) allows users to explore the articles crawled on a regular\nbasis by highlighting the spans in which propaganda techniques occur and to\ncompare them on the basis of their use of propaganda techniques. The system\nfurther reports statistics about the use of such techniques, overall and over\ntime, or according to filtering criteria specified by the user based on time\ninterval, keywords, and/or political orientation of the media. Moreover, it\nallows users to analyze any text or URL through a dedicated interface or via an\nAPI. The system is available online: https://www.tanbih.org/prta", "published": "2020-05-12 15:20:55", "link": "http://arxiv.org/abs/2005.05854v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.NE", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Flowtron: an Autoregressive Flow-based Generative Network for\n  Text-to-Speech Synthesis", "abstract": "In this paper we propose Flowtron: an autoregressive flow-based generative\nnetwork for text-to-speech synthesis with control over speech variation and\nstyle transfer. Flowtron borrows insights from IAF and revamps Tacotron in\norder to provide high-quality and expressive mel-spectrogram synthesis.\nFlowtron is optimized by maximizing the likelihood of the training data, which\nmakes training simple and stable. Flowtron learns an invertible mapping of data\nto a latent space that can be manipulated to control many aspects of speech\nsynthesis (pitch, tone, speech rate, cadence, accent). Our mean opinion scores\n(MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech\nquality. In addition, we provide results on control of speech variation,\ninterpolation between samples and style transfer between speakers seen and\nunseen during training. Code and pre-trained models will be made publicly\navailable at https://github.com/NVIDIA/flowtron", "published": "2020-05-12 17:57:17", "link": "http://arxiv.org/abs/2005.05957v3", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "That is a Known Lie: Detecting Previously Fact-Checked Claims", "abstract": "The recent proliferation of \"fake news\" has triggered a number of responses,\nmost notably the emergence of several manual fact-checking initiatives. As a\nresult and over time, a large number of fact-checked claims have been\naccumulated, which increases the likelihood that a new claim in social media or\na new statement by a politician might have already been fact-checked by some\ntrusted fact-checking organization, as viral claims often come back after a\nwhile in social media, and politicians like to repeat their favorite\nstatements, true or false, over and over again. As manual fact-checking is very\ntime-consuming (and fully automatic fact-checking has credibility issues), it\nis important to try to save this effort and to avoid wasting time on claims\nthat have already been fact-checked. Interestingly, despite the importance of\nthe task, it has been largely ignored by the research community so far. Here,\nwe aim to bridge this gap. In particular, we formulate the task and we discuss\nhow it relates to, but also differs from, previous work. We further create a\nspecialized dataset, which we release to the research community. Finally, we\npresent learning-to-rank experiments that demonstrate sizable improvements over\nstate-of-the-art retrieval and textual similarity approaches.", "published": "2020-05-12 21:25:37", "link": "http://arxiv.org/abs/2005.06058v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.SI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A computational model implementing subjectivity with the 'Room Theory'.\n  The case of detecting Emotion from Text", "abstract": "This work introduces a new method to consider subjectivity and general\ncontext dependency in text analysis and uses as example the detection of\nemotions conveyed in text. The proposed method takes into account subjectivity\nusing a computational version of the Framework Theory by Marvin Minsky (1974)\nleveraging on the Word2Vec approach to text vectorization by Mikolov et al.\n(2013), used to generate distributed representation of words based on the\ncontext where they appear. Our approach is based on three components: 1. a\nframework/'room' representing the point of view; 2. a benchmark representing\nthe criteria for the analysis - in this case the emotion classification, from a\nstudy of human emotions by Robert Plutchik (1980); and 3. the document to be\nanalyzed. By using similarity measure between words, we are able to extract the\nrelative relevance of the elements in the benchmark - intensities of emotions\nin our case study - for the document to be analyzed. Our method provides a\nmeasure that take into account the point of view of the entity reading the\ndocument. This method could be applied to all the cases where evaluating\nsubjectivity is relevant to understand the relative value or meaning of a text.\nSubjectivity can be not limited to human reactions, but it could be used to\nprovide a text with an interpretation related to a given domain (\"room\"). To\nevaluate our method, we used a test case in the political domain.", "published": "2020-05-12 21:26:04", "link": "http://arxiv.org/abs/2005.06059v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Automatic Estimation of Intelligibility Measure for Consonants in Speech", "abstract": "In this article, we provide a model to estimate a real-valued measure of the\nintelligibility of individual speech segments. We trained regression models\nbased on Convolutional Neural Networks (CNN) for stop consonants\n\\textipa{/p,t,k,b,d,g/} associated with vowel \\textipa{/A/}, to estimate the\ncorresponding Signal to Noise Ratio (SNR) at which the Consonant-Vowel (CV)\nsound becomes intelligible for Normal Hearing (NH) ears. The intelligibility\nmeasure for each sound is called SNR$_{90}$, and is defined to be the SNR level\nat which human participants are able to recognize the consonant at least 90\\%\ncorrectly, on average, as determined in prior experiments with NH subjects.\nPerformance of the CNN is compared to a baseline prediction based on automatic\nspeech recognition (ASR), specifically, a constant offset subtracted from the\nSNR at which the ASR becomes capable of correctly labeling the consonant.\nCompared to baseline, our models were able to accurately estimate the\nSNR$_{90}$~intelligibility measure with less than 2 [dB$^2$] Mean Squared Error\n(MSE) on average, while the baseline ASR-defined measure computes\nSNR$_{90}$~with a variance of 5.2 to 26.6 [dB$^2$], depending on the consonant.", "published": "2020-05-12 21:45:20", "link": "http://arxiv.org/abs/2005.06065v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Automated Extraction of Socio-political Events from News (AESPEN):\n  Workshop and Shared Task Report", "abstract": "We describe our effort on automated extraction of socio-political events from\nnews in the scope of a workshop and a shared task we organized at Language\nResources and Evaluation Conference (LREC 2020). We believe the event\nextraction studies in computational linguistics and social and political\nsciences should further support each other in order to enable large scale\nsocio-political event information collection across sources, countries, and\nlanguages. The event consists of regular research papers and a shared task,\nwhich is about event sentence coreference identification (ESCI), tracks. All\nsubmissions were reviewed by five members of the program committee. The\nworkshop attracted research papers related to evaluation of machine learning\nmethodologies, language resources, material conflict forecasting, and a shared\ntask participation report in the scope of socio-political event information\ncollection. It has shown us the volume and variety of both the data sources and\nevent information collection approaches related to socio-political events and\nthe need to fill the gap between automated text processing techniques and\nrequirements of social and political sciences.", "published": "2020-05-12 22:07:14", "link": "http://arxiv.org/abs/2005.06070v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Discriminative Multi-modality Speech Recognition", "abstract": "Vision is often used as a complementary modality for audio speech recognition\n(ASR), especially in the noisy environment where performance of solo audio\nmodality significantly deteriorates. After combining visual modality, ASR is\nupgraded to the multi-modality speech recognition (MSR). In this paper, we\npropose a two-stage speech recognition model. In the first stage, the target\nvoice is separated from background noises with help from the corresponding\nvisual information of lip movements, making the model 'listen' clearly. At the\nsecond stage, the audio modality combines visual modality again to better\nunderstand the speech by a MSR sub-network, further improving the recognition\nrate. There are some other key contributions: we introduce a pseudo-3D residual\nconvolution (P3D)-based visual front-end to extract more discriminative\nfeatures; we upgrade the temporal convolution block from 1D ResNet with the\ntemporal convolutional network (TCN), which is more suitable for the temporal\ntasks; the MSR sub-network is built on the top of Element-wise-Attention Gated\nRecurrent Unit (EleAtt-GRU), which is more effective than Transformer in long\nsequences. We conducted extensive experiments on the LRS3-TED and the LRW\ndatasets. Our two-stage model (audio enhanced multi-modality speech\nrecognition, AE-MSR) consistently achieves the state-of-the-art performance by\na significant margin, which demonstrates the necessity and effectiveness of\nAE-MSR.", "published": "2020-05-12 07:56:03", "link": "http://arxiv.org/abs/2005.05592v2", "categories": ["cs.CV", "cs.CL", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "COVID-19Base: A knowledgebase to explore biomedical entities related to\n  COVID-19", "abstract": "We are presenting COVID-19Base, a knowledgebase highlighting the biomedical\nentities related to COVID-19 disease based on literature mining. To develop\nCOVID-19Base, we mine the information from publicly available scientific\nliterature and related public resources. We considered seven topic-specific\ndictionaries, including human genes, human miRNAs, human lncRNAs, diseases,\nProtein Databank, drugs, and drug side effects, are integrated to mine all\nscientific evidence related to COVID-19. We have employed an automated\nliterature mining and labeling system through a novel approach to measure the\neffectiveness of drugs against diseases based on natural language processing,\nsentiment analysis, and deep learning. To the best of our knowledge, this is\nthe first knowledgebase dedicated to COVID-19, which integrates such large\nvariety of related biomedical entities through literature mining. Proper\ninvestigation of the mined biomedical entities along with the identified\ninteractions among those, reported in COVID-19Base, would help the research\ncommunity to discover possible ways for the therapeutic treatment of COVID-19.", "published": "2020-05-12 17:55:00", "link": "http://arxiv.org/abs/2005.05954v1", "categories": ["cs.IR", "cs.CL", "cs.DL", "cs.LG", "q-bio.QM"], "primary_category": "cs.IR"}
{"title": "TalkNet: Fully-Convolutional Non-Autoregressive Speech Synthesis Model", "abstract": "We propose TalkNet, a convolutional non-autoregressive neural model for\nspeech synthesis. The model consists of two feed-forward convolutional\nnetworks. The first network predicts grapheme durations. An input text is\nexpanded by repeating each symbol according to the predicted duration. The\nsecond network generates a mel-spectrogram from the expanded text. To train a\ngrapheme duration predictor, we add the grapheme duration to the training\ndataset using a pre-trained Connectionist Temporal Classification (CTC)-based\nspeech recognition model. The explicit duration prediction eliminates word\nskipping and repeating. Experiments on the LJSpeech dataset show that the\nspeech quality nearly matches auto-regressive models. The model is very compact\n-- it has 10.8M parameters, almost 3x less than the present state-of-the-art\ntext-to-speech models. The non-autoregressive architecture allows for fast\ntraining and inference.", "published": "2020-05-12 01:52:28", "link": "http://arxiv.org/abs/2005.05514v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "FeatherWave: An efficient high-fidelity neural vocoder with multi-band\n  linear prediction", "abstract": "In this paper, we propose the FeatherWave, yet another variant of WaveRNN\nvocoder combining the multi-band signal processing and the linear predictive\ncoding. The LPCNet, a recently proposed neural vocoder which utilized the\nlinear predictive characteristic of speech signal in the WaveRNN architecture,\ncan generate high quality speech with a speed faster than real-time on a single\nCPU core. However, LPCNet is still not efficient enough for online speech\ngeneration tasks. To address this issue, we adopt the multi-band linear\npredictive coding for WaveRNN vocoder. The multi-band method enables the model\nto generate several speech samples in parallel at one step. Therefore, it can\nsignificantly improve the efficiency of speech synthesis. The proposed model\nwith 4 sub-bands needs less than 1.6 GFLOPS for speech generation. In our\nexperiments, it can generate 24 kHz high-fidelity audio 9x faster than\nreal-time on a single CPU, which is much faster than the LPCNet vocoder.\nFurthermore, our subjective listening test shows that the FeatherWave can\ngenerate speech with better quality than LPCNet.", "published": "2020-05-12 05:19:51", "link": "http://arxiv.org/abs/2005.05551v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The IOA System for Deep Noise Suppression Challenge using a Framework\n  Combining Dynamic Attention and Recursive Learning", "abstract": "This technical report describes our system that is submitted to the Deep\nNoise Suppression Challenge and presents the results for the non-real-time\ntrack. To refine the estimation results stage by stage, we utilize recursive\nlearning, a type of training protocol which aggravates the information through\nmultiple stages with a memory mechanism. The attention generator network is\ndesigned to dynamically control the feature distribution of the noise reduction\nnetwork. To improve the phase recovery accuracy, we take the complex spectral\nmapping procedure by decoding both real and imaginary spectra. For the final\nblind test set, the average MOS improvements of the submitted system in\nnoreverb, reverb, and realrec categories are 0.49, 0.24, and 0.36,\nrespectively.", "published": "2020-05-12 15:23:25", "link": "http://arxiv.org/abs/2005.05855v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Creative Quantum Computing: Inverse FFT, Sound Synthesis, Adaptive\n  Sequencing and Musical Composition", "abstract": "Quantum computing is emerging as an alternative computing technology, which\nis built on the principles of subatomic physics. In spite of continuing\nprogress in developing increasingly more sophisticated hardware and software,\naccess to quantum computing still requires specialist expertise that is largely\nconfined to research laboratories. Moreover, the target applications for these\ndevelopments remain primarily scientific. This chapter introduces research\naimed at improving this scenario. Our research is aimed at extending the range\nof applications of quantum computing towards the arts and creative\napplications, music being our point of departure. This chapter reports on\ninitial outcomes, whereby quantum information processing controls an inverse\nFast Fourier Transform (FFT) sound synthesizer and an adaptive musical\nsequencer. A composition called Zeno is presented to illustrate a practical\nreal-world application.", "published": "2020-05-12 14:52:02", "link": "http://arxiv.org/abs/2005.05832v2", "categories": ["cs.SD", "cs.ET", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Infant Crying Detection in Real-World Environments", "abstract": "Most existing cry detection models have been tested with data collected in\ncontrolled settings. Thus, the extent to which they generalize to noisy and\nlived environments is unclear. In this paper, we evaluate several established\nmachine learning approaches including a model leveraging both deep spectrum and\nacoustic features. This model was able to recognize crying events with F1 score\n0.613 (Precision: 0.672, Recall: 0.552), showing improved external validity\nover existing methods at cry detection in everyday real-world settings. As part\nof our evaluation, we collect and annotate a novel dataset of infant crying\ncompiled from over 780 hours of labeled real-world audio data, captured via\nrecorders worn by infants in their homes, which we make publicly available. Our\nfindings confirm that a cry detection model trained on in-lab data\nunderperforms when presented with real-world data (in-lab test F1: 0.656,\nreal-world test F1: 0.236), highlighting the value of our new dataset and\nmodel.", "published": "2020-05-12 18:25:44", "link": "http://arxiv.org/abs/2005.07036v6", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Generalized Multi-view Shared Subspace Learning using View Bootstrapping", "abstract": "A key objective in multi-view learning is to model the information common to\nmultiple parallel views of a class of objects/events to improve downstream\nlearning tasks. In this context, two open research questions remain: How can we\nmodel hundreds of views per event? Can we learn robust multi-view embeddings\nwithout any knowledge of how these views are acquired? We present a neural\nmethod based on multi-view correlation to capture the information shared across\na large number of views by subsampling them in a view-agnostic manner during\ntraining. To provide an upper bound on the number of views to subsample for a\ngiven embedding dimension, we analyze the error of the bootstrapped multi-view\ncorrelation objective using matrix concentration theory. Our experiments on\nspoken word recognition, 3D object classification and pose-invariant face\nrecognition demonstrate the robustness of view bootstrapping to model a large\nnumber of views. Results underscore the applicability of our method for a\nview-agnostic learning setting.", "published": "2020-05-12 20:35:14", "link": "http://arxiv.org/abs/2005.06038v1", "categories": ["cs.LG", "cs.CV", "cs.SD", "eess.AS", "eess.SP", "stat.ML"], "primary_category": "cs.LG"}
