{"title": "Improving Implicit Semantic Role Labeling by Predicting Semantic Frame\n  Arguments", "abstract": "Implicit semantic role labeling (iSRL) is the task of predicting the semantic\nroles of a predicate that do not appear as explicit arguments, but rather\nregard common sense knowledge or are mentioned earlier in the discourse. We\nintroduce an approach to iSRL based on a predictive recurrent neural semantic\nframe model (PRNSFM) that uses a large unannotated corpus to learn the\nprobability of a sequence of semantic arguments given a predicate. We leverage\nthe sequence probabilities predicted by the PRNSFM to estimate selectional\npreferences for predicates and their arguments. On the NomBank iSRL test set,\nour approach improves state-of-the-art performance on implicit semantic role\nlabeling with less reliance than prior work on manually constructed language\nresources.", "published": "2017-04-10 04:48:53", "link": "http://arxiv.org/abs/1704.02709v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity Linking for Queries by Searching Wikipedia Sentences", "abstract": "We present a simple yet effective approach for linking entities in queries.\nThe key idea is to search sentences similar to a query from Wikipedia articles\nand directly use the human-annotated entities in the similar sentences as\ncandidate entities for the query. Then, we employ a rich set of features, such\nas link-probability, context-matching, word embeddings, and relatedness among\ncandidate entities as well as their related entities, to rank the candidates\nunder a regression based framework. The advantages of our approach lie in two\naspects, which contribute to the ranking process and final linking result.\nFirst, it can greatly reduce the number of candidate entities by filtering out\nirrelevant entities with the words in the query. Second, we can obtain the\nquery sensitive prior probability in addition to the static link-probability\nderived from all Wikipedia articles. We conduct experiments on two benchmark\ndatasets on entity linking for queries, namely the ERD14 dataset and the GERDAQ\ndataset. Experimental results show that our method outperforms state-of-the-art\nsystems and yields 75.0% in F1 on the ERD14 dataset and 56.9% on the GERDAQ\ndataset.", "published": "2017-04-10 10:19:53", "link": "http://arxiv.org/abs/1704.02788v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Character-Word LSTM Language Models", "abstract": "We present a Character-Word Long Short-Term Memory Language Model which both\nreduces the perplexity with respect to a baseline word-level language model and\nreduces the number of parameters of the model. Character information can reveal\nstructural (dis)similarities between words and can even be used when a word is\nout-of-vocabulary, thus improving the modeling of infrequent and unknown words.\nBy concatenating word and character embeddings, we achieve up to 2.77% relative\nimprovement on English compared to a baseline model with a similar amount of\nparameters and 4.57% on Dutch. Moreover, we also outperform baseline word-level\nmodels with a larger number of parameters.", "published": "2017-04-10 11:42:09", "link": "http://arxiv.org/abs/1704.02813v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Classification of the Complexity of Nonfiction Texts in\n  Portuguese for Early School Years", "abstract": "Recent research shows that most Brazilian students have serious problems\nregarding their reading skills. The full development of this skill is key for\nthe academic and professional future of every citizen. Tools for classifying\nthe complexity of reading materials for children aim to improve the quality of\nthe model of teaching reading and text comprehension. For English, Fengs work\n[11] is considered the state-of-art in grade level prediction and achieved 74%\nof accuracy in automatically classifying 4 levels of textual complexity for\nclose school grades. There are no classifiers for nonfiction texts for close\ngrades in Portuguese. In this article, we propose a scheme for manual\nannotation of texts in 5 grade levels, which will be used for customized\nreading to avoid the lack of interest by students who are more advanced in\nreading and the blocking of those that still need to make further progress. We\nobtained 52% of accuracy in classifying texts into 5 levels and 74% in 3\nlevels. The results prove to be promising when compared to the state-of-art\nwork.9", "published": "2017-04-10 18:44:08", "link": "http://arxiv.org/abs/1704.03013v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic semantic role labeling on non-revised syntactic trees of\n  journalistic texts", "abstract": "Semantic Role Labeling (SRL) is a Natural Language Processing task that\nenables the detection of events described in sentences and the participants of\nthese events. For Brazilian Portuguese (BP), there are two studies recently\nconcluded that perform SRL in journalistic texts. [1] obtained F1-measure\nscores of 79.6, using the PropBank.Br corpus, which has syntactic trees\nmanually revised, [8], without using a treebank for training, obtained\nF1-measure scores of 68.0 for the same corpus. However, the use of manually\nrevised syntactic trees for this task does not represent a real scenario of\napplication. The goal of this paper is to evaluate the performance of SRL on\nrevised and non-revised syntactic trees using a larger and balanced corpus of\nBP journalistic texts. First, we have shown that [1]'s system also performs\nbetter than [8]'s system on the larger corpus. Second, the SRL system trained\non non-revised syntactic trees performs better over non-revised trees than a\nsystem trained on gold-standard data.", "published": "2017-04-10 19:02:18", "link": "http://arxiv.org/abs/1704.03016v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Word Embeddings for Unsupervised Textual User-Generated\n  Content Normalization", "abstract": "Text normalization techniques based on rules, lexicons or supervised training\nrequiring large corpora are not scalable nor domain interchangeable, and this\nmakes them unsuitable for normalizing user-generated content (UGC). Current\ntools available for Brazilian Portuguese make use of such techniques. In this\nwork we propose a technique based on distributed representation of words (or\nword embeddings). It generates continuous numeric vectors of\nhigh-dimensionality to represent words. The vectors explicitly encode many\nlinguistic regularities and patterns, as well as syntactic and semantic word\nrelationships. Words that share semantic similarity are represented by similar\nvectors. Based on these features, we present a totally unsupervised, expandable\nand language and domain independent method for learning normalization lexicons\nfrom word embeddings. Our approach obtains high correction rate of orthographic\nerrors and internet slang in product reviews, outperforming the current\navailable tools for Brazilian Portuguese.", "published": "2017-04-10 17:37:22", "link": "http://arxiv.org/abs/1704.02963v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Voice Conversion Using Sequence-to-Sequence Learning of Context\n  Posterior Probabilities", "abstract": "Voice conversion (VC) using sequence-to-sequence learning of context\nposterior probabilities is proposed. Conventional VC using shared context\nposterior probabilities predicts target speech parameters from the context\nposterior probabilities estimated from the source speech parameters. Although\nconventional VC can be built from non-parallel data, it is difficult to convert\nspeaker individuality such as phonetic property and speaking rate contained in\nthe posterior probabilities because the source posterior probabilities are\ndirectly used for predicting target speech parameters. In this work, we assume\nthat the training data partly include parallel speech data and propose\nsequence-to-sequence learning between the source and target posterior\nprobabilities. The conversion models perform non-linear and variable-length\ntransformation from the source probability sequence to the target one. Further,\nwe propose a joint training algorithm for the modules. In contrast to\nconventional VC, which separately trains the speech recognition that estimates\nposterior probabilities and the speech synthesis that predicts target speech\nparameters, our proposed method jointly trains these modules along with the\nproposed probability conversion modules. Experimental results demonstrate that\nour approach outperforms the conventional VC.", "published": "2017-04-10 12:35:33", "link": "http://arxiv.org/abs/1704.02360v4", "categories": ["cs.SD", "cs.CL", "cs.LG"], "primary_category": "cs.SD"}
{"title": "Word Embeddings via Tensor Factorization", "abstract": "Most popular word embedding techniques involve implicit or explicit\nfactorization of a word co-occurrence based matrix into low rank factors. In\nthis paper, we aim to generalize this trend by using numerical methods to\nfactor higher-order word co-occurrence based arrays, or \\textit{tensors}. We\npresent four word embeddings using tensor factorization and analyze their\nadvantages and disadvantages. One of our main contributions is a novel joint\nsymmetric tensor factorization technique related to the idea of coupled tensor\nfactorization. We show that embeddings based on tensor factorization can be\nused to discern the various meanings of polysemous words without being\nexplicitly trained to do so, and motivate the intuition behind why this works\nin a way that doesn't with existing methods. We also modify an existing word\nembedding evaluation metric known as Outlier Detection [Camacho-Collados and\nNavigli, 2016] to evaluate the quality of the order-$N$ relations that a word\nembedding captures, and show that tensor-based methods outperform existing\nmatrix-based methods at this task. Experimentally, we show that all of our word\nembeddings either outperform or are competitive with state-of-the-art baselines\ncommonly used today on a variety of recent datasets. Suggested applications of\ntensor factorization-based word embeddings are given, and all source code and\npre-trained vectors are publicly available online.", "published": "2017-04-10 02:24:37", "link": "http://arxiv.org/abs/1704.02686v2", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "SemEval 2017 Task 10: ScienceIE - Extracting Keyphrases and Relations\n  from Scientific Publications", "abstract": "We describe the SemEval task of extracting keyphrases and relations between\nthem from scientific documents, which is crucial for understanding which\npublications describe which processes, tasks and materials. Although this was a\nnew task, we had a total of 26 submissions across 3 evaluation scenarios. We\nexpect the task and the findings reported in this paper to be relevant for\nresearchers working on understanding scientific content, as well as the broader\nknowledge base population and information extraction communities.", "published": "2017-04-10 13:43:40", "link": "http://arxiv.org/abs/1704.02853v3", "categories": ["cs.CL", "cs.AI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Pay Attention to Those Sets! Learning Quantification from Images", "abstract": "Major advances have recently been made in merging language and vision\nrepresentations. But most tasks considered so far have confined themselves to\nthe processing of objects and lexicalised relations amongst objects (content\nwords). We know, however, that humans (even pre-school children) can abstract\nover raw data to perform certain types of higher-level reasoning, expressed in\nnatural language by function words. A case in point is given by their ability\nto learn quantifiers, i.e. expressions like 'few', 'some' and 'all'. From\nformal semantics and cognitive linguistics, we know that quantifiers are\nrelations over sets which, as a simplification, we can see as proportions. For\ninstance, in 'most fish are red', most encodes the proportion of fish which are\nred fish. In this paper, we study how well current language and vision\nstrategies model such relations. We show that state-of-the-art attention\nmechanisms coupled with a traditional linguistic formalisation of quantifiers\ngives best performance on the task. Additionally, we provide insights on the\nrole of 'gist' representations in quantification. A 'logical' strategy to\ntackle the task would be to first obtain a numerosity estimation for the two\ninvolved sets and then compare their cardinalities. We however argue that\nprecisely identifying the composition of the sets is not only beyond current\nstate-of-the-art models but perhaps even detrimental to a task that is most\nefficiently performed by refining the approximate numerosity estimator of the\nsystem.", "published": "2017-04-10 16:03:31", "link": "http://arxiv.org/abs/1704.02923v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep\n  Reinforcement Learning", "abstract": "Building a dialogue agent to fulfill complex tasks, such as travel planning,\nis challenging because the agent has to learn to collectively complete multiple\nsubtasks. For example, the agent needs to reserve a hotel and book a flight so\nthat there leaves enough time for commute between arrival and hotel check-in.\nThis paper addresses this challenge by formulating the task in the mathematical\nframework of options over Markov Decision Processes (MDPs), and proposing a\nhierarchical deep reinforcement learning approach to learning a dialogue\nmanager that operates at different temporal scales. The dialogue manager\nconsists of: (1) a top-level dialogue policy that selects among subtasks or\noptions, (2) a low-level dialogue policy that selects primitive actions to\ncomplete the subtask given by the top-level policy, and (3) a global state\ntracker that helps ensure all cross-subtask constraints be satisfied.\nExperiments on a travel planning task with simulated and real users show that\nour approach leads to significant improvements over three baselines, two based\non handcrafted rules and the other based on flat deep reinforcement learning.", "published": "2017-04-10 23:24:46", "link": "http://arxiv.org/abs/1704.03084v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
