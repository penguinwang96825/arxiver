{"title": "Joint Models for Answer Verification in Question Answering Systems", "abstract": "This paper studies joint models for selecting correct answer sentences among\nthe top $k$ provided by answer sentence selection (AS2) modules, which are core\ncomponents of retrieval-based Question Answering (QA) systems. Our work shows\nthat a critical step to effectively exploit an answer set regards modeling the\ninterrelated information between pair of answers. For this purpose, we build a\nthree-way multi-classifier, which decides if an answer supports, refutes, or is\nneutral with respect to another one. More specifically, our neural architecture\nintegrates a state-of-the-art AS2 model with the multi-classifier, and a joint\nlayer connecting all components. We tested our models on WikiQA, TREC-QA, and a\nreal-world dataset. The results show that our models obtain the new state of\nthe art in AS2.", "published": "2021-07-09 05:34:36", "link": "http://arxiv.org/abs/2107.04217v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Syntactic Dense Embedding with Correlation Graph for Automatic\n  Readability Assessment", "abstract": "Deep learning models for automatic readability assessment generally discard\nlinguistic features traditionally used in machine learning models for the task.\nWe propose to incorporate linguistic features into neural network models by\nlearning syntactic dense embeddings based on linguistic features. To cope with\nthe relationships between the features, we form a correlation graph among\nfeatures and use it to learn their embeddings so that similar features will be\nrepresented by similar embeddings. Experiments with six data sets of two\nproficiency levels demonstrate that our proposed methodology can complement\nBERT-only model to achieve significantly better performances for automatic\nreadability assessment.", "published": "2021-07-09 07:26:17", "link": "http://arxiv.org/abs/2107.04268v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Robust Deep Ensemble Classifier for Figurative Language Detection", "abstract": "Recognition and classification of Figurative Language (FL) is an open problem\nof Sentiment Analysis in the broader field of Natural Language Processing (NLP)\ndue to the contradictory meaning contained in phrases with metaphorical\ncontent. The problem itself contains three interrelated FL recognition tasks:\nsarcasm, irony and metaphor which, in the present paper, are dealt with\nadvanced Deep Learning (DL) techniques. First, we introduce a data\nprepossessing framework towards efficient data representation formats so that\nto optimize the respective inputs to the DL models. In addition, special\nfeatures are extracted in order to characterize the syntactic, expressive,\nemotional and temper content reflected in the respective social media text\nreferences. These features aim to capture aspects of the social network user's\nwriting method. Finally, features are fed to a robust, Deep Ensemble Soft\nClassifier (DESC) which is based on the combination of different DL techniques.\nUsing three different benchmark datasets (one of them containing various FL\nforms) we conclude that the DESC model achieves a very good performance, worthy\nof comparison with relevant methodologies and state-of-the-art technologies in\nthe challenging field of FL recognition.", "published": "2021-07-09 11:26:37", "link": "http://arxiv.org/abs/2107.04372v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking for Biomedical Natural Language Processing Tasks with a\n  Domain Specific ALBERT", "abstract": "The availability of biomedical text data and advances in natural language\nprocessing (NLP) have made new applications in biomedical NLP possible.\nLanguage models trained or fine tuned using domain specific corpora can\noutperform general models, but work to date in biomedical NLP has been limited\nin terms of corpora and tasks. We present BioALBERT, a domain-specific\nadaptation of A Lite Bidirectional Encoder Representations from Transformers\n(ALBERT), trained on biomedical (PubMed and PubMed Central) and clinical\n(MIMIC-III) corpora and fine tuned for 6 different tasks across 20 benchmark\ndatasets. Experiments show that BioALBERT outperforms the state of the art on\nnamed entity recognition (+11.09% BLURB score improvement), relation extraction\n(+0.80% BLURB score), sentence similarity (+1.05% BLURB score), document\nclassification (+0.62% F1-score), and question answering (+2.83% BLURB score).\nIt represents a new state of the art in 17 out of 20 benchmark datasets. By\nmaking BioALBERT models and data available, our aim is to help the biomedical\nNLP community avoid computational costs of training and establish a new set of\nbaselines for future efforts across a broad range of biomedical NLP tasks.", "published": "2021-07-09 11:47:13", "link": "http://arxiv.org/abs/2107.04374v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Noisy Training Improves E2E ASR for the Edge", "abstract": "Automatic speech recognition (ASR) has become increasingly ubiquitous on\nmodern edge devices. Past work developed streaming End-to-End (E2E) all-neural\nspeech recognizers that can run compactly on edge devices. However, E2E ASR\nmodels are prone to overfitting and have difficulties in generalizing to unseen\ntesting data. Various techniques have been proposed to regularize the training\nof ASR models, including layer normalization, dropout, spectrum data\naugmentation and speed distortions in the inputs. In this work, we present a\nsimple yet effective noisy training strategy to further improve the E2E ASR\nmodel training. By introducing random noise to the parameter space during\ntraining, our method can produce smoother models at convergence that generalize\nbetter. We apply noisy training to improve both dense and sparse\nstate-of-the-art Emformer models and observe consistent WER reduction.\nSpecifically, when training Emformers with 90% sparsity, we achieve 12% and 14%\nWER improvements on the LibriSpeech Test-other and Test-clean data set,\nrespectively.", "published": "2021-07-09 20:56:20", "link": "http://arxiv.org/abs/2107.04677v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Initial Investigation of Non-Native Spoken Question-Answering", "abstract": "Text-based machine comprehension (MC) systems have a wide-range of\napplications, and standard corpora exist for developing and evaluating\napproaches. There has been far less research on spoken question answering (SQA)\nsystems. The SQA task considered in this paper is to extract the answer from a\ncandidate$\\text{'}$s spoken response to a question in a prompt-response style\nlanguage assessment test. Applying these MC approaches to this SQA task rather\nthan, for example, off-topic response detection provides far more detailed\ninformation that can be used for further downstream processing. One significant\nchallenge is the lack of appropriately annotated speech corpora to train\nsystems for this task. Hence, a transfer-learning style approach is adopted\nwhere a system trained on text-based MC is evaluated on an SQA task with\nnon-native speakers. Mismatches must be considered between text documents and\nspoken responses; non-native spoken grammar and written grammar. In practical\nSQA, ASR systems are used, necessitating an investigation of the impact of ASR\nerrors. We show that a simple text-based ELECTRA MC model trained on SQuAD2.0\ntransfers well for SQA. It is found that there is an approximately linear\nrelationship between ASR errors and the SQA assessment scores but grammar\nmismatches have minimal impact.", "published": "2021-07-09 21:59:16", "link": "http://arxiv.org/abs/2107.04691v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Levi Graph AMR Parser using Heterogeneous Attention", "abstract": "Coupled with biaffine decoders, transformers have been effectively adapted to\ntext-to-graph transduction and achieved state-of-the-art performance on AMR\nparsing. Many prior works, however, rely on the biaffine decoder for either or\nboth arc and label predictions although most features used by the decoder may\nbe learned by the transformer already. This paper presents a novel approach to\nAMR parsing by combining heterogeneous data (tokens, concepts, labels) as one\ninput to a transformer to learn attention, and use only attention matrices from\nthe transformer to predict all elements in AMR graphs (concepts, arcs, labels).\nAlthough our models use significantly fewer parameters than the previous\nstate-of-the-art graph parser, they show similar or better accuracy on AMR 2.0\nand 3.0.", "published": "2021-07-09 00:06:17", "link": "http://arxiv.org/abs/2107.04152v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Measuring and Improving Model-Moderator Collaboration using Uncertainty\n  Estimation", "abstract": "Content moderation is often performed by a collaboration between humans and\nmachine learning models. However, it is not well understood how to design the\ncollaborative process so as to maximize the combined moderator-model system\nperformance. This work presents a rigorous study of this problem, focusing on\nan approach that incorporates model uncertainty into the collaborative process.\nFirst, we introduce principled metrics to describe the performance of the\ncollaborative system under capacity constraints on the human moderator,\nquantifying how efficiently the combined system utilizes human decisions. Using\nthese metrics, we conduct a large benchmark study evaluating the performance of\nstate-of-the-art uncertainty models under different collaborative review\nstrategies. We find that an uncertainty-based strategy consistently outperforms\nthe widely used strategy based on toxicity scores, and moreover that the choice\nof review strategy drastically changes the overall system performance. Our\nresults demonstrate the importance of rigorous metrics for understanding and\ndeveloping effective moderator-model systems for content moderation, as well as\nthe utility of uncertainty estimation in this domain.", "published": "2021-07-09 05:07:25", "link": "http://arxiv.org/abs/2107.04212v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Survey on Low-Resource Neural Machine Translation", "abstract": "Neural approaches have achieved state-of-the-art accuracy on machine\ntranslation but suffer from the high cost of collecting large scale parallel\ndata. Thus, a lot of research has been conducted for neural machine translation\n(NMT) with very limited parallel data, i.e., the low-resource setting. In this\npaper, we provide a survey for low-resource NMT and classify related works into\nthree categories according to the auxiliary data they used: (1) exploiting\nmonolingual data of source and/or target languages, (2) exploiting data from\nauxiliary languages, and (3) exploiting multi-modal data. We hope that our\nsurvey can help researchers to better understand this field and inspire them to\ndesign better algorithms, and help industry practitioners to choose appropriate\nalgorithms for their applications.", "published": "2021-07-09 06:26:38", "link": "http://arxiv.org/abs/2107.04239v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "UniRE: A Unified Label Space for Entity Relation Extraction", "abstract": "Many joint entity relation extraction models setup two separated label spaces\nfor the two sub-tasks (i.e., entity detection and relation classification). We\nargue that this setting may hinder the information interaction between entities\nand relations. In this work, we propose to eliminate the different treatment on\nthe two sub-tasks' label spaces. The input of our model is a table containing\nall word pairs from a sentence. Entities and relations are represented by\nsquares and rectangles in the table. We apply a unified classifier to predict\neach cell's label, which unifies the learning of two sub-tasks. For testing, an\neffective (yet fast) approximate decoder is proposed for finding squares and\nrectangles from tables. Experiments on three benchmarks (ACE04, ACE05, SciERC)\nshow that, using only half the number of parameters, our model achieves\ncompetitive accuracy with the best extractor, and is faster.", "published": "2021-07-09 08:09:37", "link": "http://arxiv.org/abs/2107.04292v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using Machine Translation to Localize Task Oriented NLG Output", "abstract": "One of the challenges in a task oriented natural language application like\nthe Google Assistant, Siri, or Alexa is to localize the output to many\nlanguages. This paper explores doing this by applying machine translation to\nthe English output. Using machine translation is very scalable, as it can work\nwith any English output and can handle dynamic text, but otherwise the problem\nis a poor fit. The required quality bar is close to perfection, the range of\nsentences is extremely narrow, and the sentences are often very different than\nthe ones in the machine translation training data. This combination of\nrequirements is novel in the field of domain adaptation for machine\ntranslation. We are able to reach the required quality bar by building on\nexisting ideas and adding new ones: finetuning on in-domain translations,\nadding sentences from the Web, adding semantic annotations, and using automatic\nerror detection. The paper shares our approach and results, together with a\ndistillation model to serve the translation models at scale.", "published": "2021-07-09 15:56:45", "link": "http://arxiv.org/abs/2107.04512v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can Deep Neural Networks Predict Data Correlations from Column Names?", "abstract": "Recent publications suggest using natural language analysis on database\nschema elements to guide tuning and profiling efforts. The underlying\nhypothesis is that state-of-the-art language processing methods, so-called\nlanguage models, are able to extract information on data properties from schema\ntext.\n  This paper examines that hypothesis in the context of data correlation\nanalysis: is it possible to find column pairs with correlated data by analyzing\ntheir names via language models? First, the paper introduces a novel benchmark\nfor data correlation analysis, created by analyzing thousands of Kaggle data\nsets (and available for download). Second, it uses that data to study the\nability of language models to predict correlation, based on column names. The\nanalysis covers different language models, various correlation metrics, and a\nmultitude of accuracy metrics. It pinpoints factors that contribute to\nsuccessful predictions, such as the length of column names as well as the ratio\nof words. Finally, \\rev{the study analyzes the impact of column types on\nprediction performance.} The results show that schema text can be a useful\nsource of information and inform future research efforts, targeted at\nNLP-enhanced database tuning and data profiling.", "published": "2021-07-09 17:11:54", "link": "http://arxiv.org/abs/2107.04553v2", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Incorporating Multi-Target in Multi-Stage Speech Enhancement Model for\n  Better Generalization", "abstract": "Recent single-channel speech enhancement methods based on deep neural\nnetworks (DNNs) have achieved remarkable results, but there are still\ngeneralization problems in real scenes. Like other data-driven methods,\nDNN-based speech enhancement models produce significant performance degradation\non untrained data. In this study, we make full use of the contribution of\nmulti-target joint learning to the model generalization capability, and propose\na lightweight and low-computing dilated convolutional network (DCN) model for a\nmore robust speech denoising task. Our goal is to integrate the masking target,\nthe mapping target, and the parameters of the traditional speech enhancement\nestimator into a DCN model to maximize their complementary advantages. To do\nthis, we build a multi-stage learning framework to deal with multiple targets\nin stages to achieve their joint learning, namely `MT-in-MS'. Our experimental\nresults show that compared with the state-of-the-art time domain and\ntime-frequency domain models, this proposed low-cost DCN model can achieve\nbetter generalization performance in speaker, noise, and channel mismatch\ncases.", "published": "2021-07-09 06:12:32", "link": "http://arxiv.org/abs/2107.04232v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Loss Prediction: End-to-End Active Learning Approach For Speech\n  Recognition", "abstract": "End-to-end speech recognition systems usually require huge amounts of\nlabeling resource, while annotating the speech data is complicated and\nexpensive. Active learning is the solution by selecting the most valuable\nsamples for annotation. In this paper, we proposed to use a predicted loss that\nestimates the uncertainty of the sample. The CTC (Connectionist Temporal\nClassification) and attention loss are informative for speech recognition since\nthey are computed based on all decoding paths and alignments. We defined an\nend-to-end active learning pipeline, training an ASR/LP (Automatic Speech\nRecognition/Loss Prediction) joint model. The proposed approach was validated\non an English and a Chinese speech recognition task. The experiments show that\nour approach achieves competitive results, outperforming random selection,\nleast confidence, and estimated loss method.", "published": "2021-07-09 08:03:51", "link": "http://arxiv.org/abs/2107.04289v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Representation Learning to Classify and Detect Adversarial Attacks\n  against Speaker and Speech Recognition Systems", "abstract": "Adversarial attacks have become a major threat for machine learning\napplications. There is a growing interest in studying these attacks in the\naudio domain, e.g, speech and speaker recognition; and find defenses against\nthem. In this work, we focus on using representation learning to\nclassify/detect attacks w.r.t. the attack algorithm, threat model or\nsignal-to-adversarial-noise ratio. We found that common attacks in the\nliterature can be classified with accuracies as high as 90%. Also,\nrepresentations trained to classify attacks against speaker identification can\nbe used also to classify attacks against speaker verification and speech\nrecognition. We also tested an attack verification task, where we need to\ndecide whether two speech utterances contain the same attack. We observed that\nour models did not generalize well to attack algorithms not included in the\nattack representation model training. Motivated by this, we evaluated an\nunknown attack detection task. We were able to detect unknown attacks with\nequal error rates of about 19%, which is promising.", "published": "2021-07-09 13:55:31", "link": "http://arxiv.org/abs/2107.04448v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "On lattice-free boosted MMI training of HMM and CTC-based full-context\n  ASR models", "abstract": "Hybrid automatic speech recognition (ASR) models are typically sequentially\ntrained with CTC or LF-MMI criteria. However, they have vastly different\nlegacies and are usually implemented in different frameworks. In this paper, by\ndecoupling the concepts of modeling units and label topologies and building\nproper numerator/denominator graphs accordingly, we establish a generalized\nframework for hybrid acoustic modeling (AM). In this framework, we show that\nLF-MMI is a powerful training criterion applicable to both limited-context and\nfull-context models, for wordpiece/mono-char/bi-char/chenone units, with both\nHMM/CTC topologies. From this framework, we propose three novel training\nschemes: chenone(ch)/wordpiece(wp)-CTC-bMMI, and wordpiece(wp)-HMM-bMMI with\ndifferent advantages in training performance, decoding efficiency and decoding\ntime-stamp accuracy. The advantages of different training schemes are evaluated\ncomprehensively on Librispeech, and wp-CTC-bMMI and ch-CTC-bMMI are evaluated\non two real world ASR tasks to show their effectiveness. Besides, we also show\nbi-char(bc) HMM-MMI models can serve as better alignment models than\ntraditional non-neural GMM-HMMs.", "published": "2021-07-09 00:16:42", "link": "http://arxiv.org/abs/2107.04154v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Dropout Regularization for Self-Supervised Learning of Transformer\n  Encoder Speech Representation", "abstract": "Predicting the altered acoustic frames is an effective way of self-supervised\nlearning for speech representation. However, it is challenging to prevent the\npretrained model from overfitting. In this paper, we proposed to introduce two\ndropout regularization methods into the pretraining of transformer encoder: (1)\nattention dropout, (2) layer dropout. Both of the two dropout methods encourage\nthe model to utilize global speech information, and avoid just copying local\nspectrum features when reconstructing the masked frames. We evaluated the\nproposed methods on phoneme classification and speaker recognition tasks. The\nexperiments demonstrate that our dropout approaches achieve competitive\nresults, and improve the performance of classification accuracy on downstream\ntasks.", "published": "2021-07-09 05:57:21", "link": "http://arxiv.org/abs/2107.04227v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-path Convolutional Neural Networks Efficiently Improve Feature\n  Extraction in Continuous Adventitious Lung Sound Detection", "abstract": "We previously established a large lung sound database, HF_Lung_V2 (Lung_V2).\nWe trained convolutional-bidirectional gated recurrent unit (CNN-BiGRU)\nnetworks for detecting inhalation, exhalation, continuous adventitious sound\n(CAS) and discontinuous adventitious sound at the recording level on the basis\nof Lung_V2. However, the performance of CAS detection was poor due to many\nreasons, one of which is the highly diversified CAS patterns. To make the\noriginal CNN-BiGRU model learn the CAS patterns more effectively and not cause\ntoo much computing burden, three strategies involving minimal modifications of\nthe network architecture of the CNN layers were investigated: (1) making the\nCNN layers a bit deeper by using the residual blocks, (2) making the CNN layers\na bit wider by increasing the number of CNN kernels, and (3) separating the\nfeature input into multiple paths (the model was denoted by Multi-path\nCNN-BiGRU). The performance of CAS segment and event detection were evaluated.\nResults showed that improvement in CAS detection was observed among all the\nproposed architecture-modified models. The F1 score for CAS event detection of\nthe proposed models increased from 0.445 to 0.491-0.530, which was deemed\nsignificant. However, the Multi-path CNN-BiGRU model outperformed the other\nmodels in terms of the number of winning titles (five) in total nine evaluation\nmetrics. In addition, the Multi-path CNN-BiGRU model did not cause extra\ncomputing burden (0.97-fold inference time) compared to the original CNN-BiGRU\nmodel. Conclusively, the Multi-path CNN layers can efficiently improve the\neffectiveness of feature extraction and subsequently result in better CAS\ndetection.", "published": "2021-07-09 05:55:57", "link": "http://arxiv.org/abs/2107.04226v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Dual-Purpose Deep Learning Model for Auscultated Lung and Tracheal\n  Sound Analysis Based on Mixed Set Training", "abstract": "Many deep learning-based computerized respiratory sound analysis methods have\npreviously been developed. However, these studies focus on either lung sound\nonly or tracheal sound only. The effectiveness of using a lung sound analysis\nalgorithm on tracheal sound and vice versa has never been investigated.\nFurthermore, no one knows whether using lung and tracheal sounds together in\ntraining a respiratory sound analysis model is beneficial. In this study, we\nfirst constructed a tracheal sound database, HF_Tracheal_V1, containing 10448\n15-s tracheal sound recordings, 21741 inhalation labels, 15858 exhalation\nlabels, and 6414 continuous adventitious sound (CAS) labels. HF_Tracheal_V1 and\nour previously built lung sound database, HF_Lung_V2, were either combined\n(mixed set), used one after the other (domain adaptation), or used alone to\ntrain convolutional neural network bidirectional gate recurrent unit models for\ninhalation, exhalation, and CAS detection in lung and tracheal sounds. The\nresults revealed that the models trained using lung sound alone performed\npoorly in tracheal sound analysis and vice versa. However, mixed set training\nor domain adaptation improved the performance for 1) inhalation and exhalation\ndetection in lung sounds and 2) inhalation, exhalation, and CAS detection in\ntracheal sounds compared to positive controls (the models trained using lung\nsound alone and used in lung sound analysis and vice versa). In particular, the\nmodel trained on the mixed set had great flexibility to serve two purposes,\nlung and tracheal sound analyses, at the same time.", "published": "2021-07-09 06:04:18", "link": "http://arxiv.org/abs/2107.04229v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Blind Source Separation in Polyphonic Music Recordings Using Deep Neural\n  Networks Trained via Policy Gradients", "abstract": "We propose a method for the blind separation of sounds of musical instruments\nin audio signals. We describe the individual tones via a parametric model,\ntraining a dictionary to capture the relative amplitudes of the harmonics. The\nmodel parameters are predicted via a U-Net, which is a type of deep neural\nnetwork. The network is trained without ground truth information, based on the\ndifference between the model prediction and the individual time frames of the\nshort-time Fourier transform. Since some of the model parameters do not yield a\nuseful backpropagation gradient, we model them stochastically and employ the\npolicy gradient instead. To provide phase information and account for\ninaccuracies in the dictionary-based representation, we also let the network\noutput a direct prediction, which we then use to resynthesize the audio signals\nfor the individual instruments. Due to the flexibility of the neural network,\ninharmonicity can be incorporated seamlessly and no preprocessing of the input\nspectra is required. Our algorithm yields high-quality separation results with\nparticularly low interference on a variety of different audio samples, both\nacoustic and synthetic, provided that the sample contains enough data for the\ntraining and that the spectral characteristics of the musical instruments are\nsufficiently stable to be approximated by the dictionary.", "published": "2021-07-09 06:17:04", "link": "http://arxiv.org/abs/2107.04235v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "EasyCom: An Augmented Reality Dataset to Support Algorithms for Easy\n  Communication in Noisy Environments", "abstract": "Augmented Reality (AR) as a platform has the potential to facilitate the\nreduction of the cocktail party effect. Future AR headsets could potentially\nleverage information from an array of sensors spanning many different\nmodalities. Training and testing signal processing and machine learning\nalgorithms on tasks such as beam-forming and speech enhancement require high\nquality representative data. To the best of the author's knowledge, as of\npublication there are no available datasets that contain synchronized\negocentric multi-channel audio and video with dynamic movement and\nconversations in a noisy environment. In this work, we describe, evaluate and\nrelease a dataset that contains over 5 hours of multi-modal data useful for\ntraining and testing algorithms for the application of improving conversations\nfor an AR glasses wearer. We provide speech intelligibility, quality and\nsignal-to-noise ratio improvement results for a baseline method and show\nimprovements across all tested metrics. The dataset we are releasing contains\nAR glasses egocentric multi-channel microphone array audio, wide field-of-view\nRGB video, speech source pose, headset microphone audio, annotated voice\nactivity, speech transcriptions, head bounding boxes, target of speech and\nsource identification labels. We have created and are releasing this dataset to\nfacilitate research in multi-modal AR solutions to the cocktail party problem.", "published": "2021-07-09 02:00:47", "link": "http://arxiv.org/abs/2107.04174v2", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
