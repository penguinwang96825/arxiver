{"title": "Improving Language Modelling with Noise-contrastive estimation", "abstract": "Neural language models do not scale well when the vocabulary is large.\nNoise-contrastive estimation (NCE) is a sampling-based method that allows for\nfast learning with large vocabularies. Although NCE has shown promising\nperformance in neural machine translation, it was considered to be an\nunsuccessful approach for language modelling. A sufficient investigation of the\nhyperparameters in the NCE-based neural language models was also missing. In\nthis paper, we showed that NCE can be a successful approach in neural language\nmodelling when the hyperparameters of a neural network are tuned appropriately.\nWe introduced the 'search-then-converge' learning rate schedule for NCE and\ndesigned a heuristic that specifies how to use this schedule. The impact of the\nother important hyperparameters, such as the dropout rate and the weight\ninitialisation range, was also demonstrated. We showed that appropriate tuning\nof NCE-based neural language models outperforms the state-of-the-art\nsingle-model methods on a popular benchmark.", "published": "2017-09-22 13:59:17", "link": "http://arxiv.org/abs/1709.07758v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence Correction Based on Large-scale Language Modelling", "abstract": "With the further development of informatization, more and more data is stored\nin the form of text. There are some loss of text during their generation and\ntransmission. The paper aims to establish a language model based on the\nlarge-scale corpus to complete the restoration of missing text. In this paper,\nwe introduce a novel measurement to find the missing words, and a way of\nestablishing a comprehensive candidate lexicon to insert the correct choice of\nwords. The paper also introduces some effective optimization methods, which\nlargely improve the efficiency of the text restoration and shorten the time of\ndealing with 1000 sentences into 3.6 seconds. \\keywords{ language model,\nsentence correction, word imputation, parallel optimization", "published": "2017-09-22 14:28:44", "link": "http://arxiv.org/abs/1709.07777v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation", "abstract": "Draft of textbook chapter on neural machine translation. a comprehensive\ntreatment of the topic, ranging from introduction to neural networks,\ncomputation graphs, description of the currently dominant attentional\nsequence-to-sequence model, recent refinements, alternative architectures and\nchallenges. Written as chapter for the textbook Statistical Machine\nTranslation. Used in the JHU Fall 2017 class on machine translation.", "published": "2017-09-22 15:28:24", "link": "http://arxiv.org/abs/1709.07809v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Challenging Neural Dialogue Models with Natural Data: Memory Networks\n  Fail on Incremental Phenomena", "abstract": "Natural, spontaneous dialogue proceeds incrementally on a word-by-word basis;\nand it contains many sorts of disfluency such as mid-utterance/sentence\nhesitations, interruptions, and self-corrections. But training data for machine\nlearning approaches to dialogue processing is often either cleaned-up or wholly\nsynthetic in order to avoid such phenomena. The question then arises of how\nwell systems trained on such clean data generalise to real spontaneous\ndialogue, or indeed whether they are trainable at all on naturally occurring\ndialogue data. To answer this question, we created a new corpus called bAbI+ by\nsystematically adding natural spontaneous incremental dialogue phenomena such\nas restarts and self-corrections to the Facebook AI Research's bAbI dialogues\ndataset. We then explore the performance of a state-of-the-art retrieval model,\nMemN2N, on this more natural dataset. Results show that the semantic accuracy\nof the MemN2N model drops drastically; and that although it is in principle\nable to learn to process the constructions in bAbI+, it needs an impractical\namount of training data to do so. Finally, we go on to show that an\nincremental, semantic parser -- DyLan -- shows 100% semantic accuracy on both\nbAbI and bAbI+, highlighting the generalisation properties of linguistically\ninformed dialogue models.", "published": "2017-09-22 16:43:48", "link": "http://arxiv.org/abs/1709.07840v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Bootstrapping incremental dialogue systems from minimal data: the\n  generalisation power of dialogue grammars", "abstract": "We investigate an end-to-end method for automatically inducing task-based\ndialogue systems from small amounts of unannotated dialogue data. It combines\nan incremental semantic grammar - Dynamic Syntax and Type Theory with Records\n(DS-TTR) - with Reinforcement Learning (RL), where language generation and\ndialogue management are a joint decision problem. The systems thus produced are\nincremental: dialogues are processed word-by-word, shown previously to be\nessential in supporting natural, spontaneous dialogue. We hypothesised that the\nrich linguistic knowledge within the grammar should enable a combinatorially\nlarge number of dialogue variations to be processed, even when trained on very\nfew dialogues. Our experiments show that our model can process 74% of the\nFacebook AI bAbI dataset even when trained on only 0.13% of the data (5\ndialogues). It can in addition process 65% of bAbI+, a corpus we created by\nsystematically adding incremental dialogue phenomena such as restarts and\nself-corrections to bAbI. We compare our model with a state-of-the-art\nretrieval model, MemN2N. We find that, in terms of semantic accuracy, MemN2N\nshows very poor robustness to the bAbI+ transformations even when trained on\nthe full bAbI dataset.", "published": "2017-09-22 17:24:33", "link": "http://arxiv.org/abs/1709.07858v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Mitigating the Impact of Speech Recognition Errors on Chatbot using\n  Sequence-to-Sequence Model", "abstract": "We apply sequence-to-sequence model to mitigate the impact of speech\nrecognition errors on open domain end-to-end dialog generation. We cast the\ntask as a domain adaptation problem where ASR transcriptions and original text\nare in two different domains. In this paper, our proposed model includes two\nindividual encoders for each domain data and make their hidden states similar\nto ensure the decoder predict the same dialog text. The method shows that the\nsequence-to-sequence model can learn the ASR transcriptions and original text\npair having the same meaning and eliminate the speech recognition errors.\nExperimental results on Cornell movie dialog dataset demonstrate that the\ndomain adaption system help the spoken dialog system generate more similar\nresponses with the original text answers.", "published": "2017-09-22 17:33:32", "link": "http://arxiv.org/abs/1709.07862v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Code Attention: Translating Code to Comments by Exploiting Domain\n  Features", "abstract": "Appropriate comments of code snippets provide insight for code functionality,\nwhich are helpful for program comprehension. However, due to the great cost of\nauthoring with the comments, many code projects do not contain adequate\ncomments. Automatic comment generation techniques have been proposed to\ngenerate comments from pieces of code in order to alleviate the human efforts\nin annotating the code. Most existing approaches attempt to exploit certain\ncorrelations (usually manually given) between code and generated comments,\nwhich could be easily violated if the coding patterns change and hence the\nperformance of comment generation declines. In this paper, we first build\nC2CGit, a large dataset from open projects in GitHub, which is more than\n20$\\times$ larger than existing datasets. Then we propose a new attention\nmodule called Code Attention to translate code to comments, which is able to\nutilize the domain features of code snippets, such as symbols and identifiers.\nWe make ablation studies to determine effects of different parts in Code\nAttention. Experimental results demonstrate that the proposed module has better\nperformance over existing approaches in both BLEU and METEOR.", "published": "2017-09-22 09:08:47", "link": "http://arxiv.org/abs/1709.07642v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Attention-based Wav2Text with Feature Transfer Learning", "abstract": "Conventional automatic speech recognition (ASR) typically performs\nmulti-level pattern recognition tasks that map the acoustic speech waveform\ninto a hierarchy of speech units. But, it is widely known that information loss\nin the earlier stage can propagate through the later stages. After the\nresurgence of deep learning, interest has emerged in the possibility of\ndeveloping a purely end-to-end ASR system from the raw waveform to the\ntranscription without any predefined alignments and hand-engineered models.\nHowever, the successful attempts in end-to-end architecture still used\nspectral-based features, while the successful attempts in using raw waveform\nwere still based on the hybrid deep neural network - Hidden Markov model\n(DNN-HMM) framework. In this paper, we construct the first end-to-end\nattention-based encoder-decoder model to process directly from raw speech\nwaveform to the text transcription. We called the model as \"Attention-based\nWav2Text\". To assist the training process of the end-to-end model, we propose\nto utilize a feature transfer learning. Experimental results also reveal that\nthe proposed Attention-based Wav2Text model directly with raw waveform could\nachieve a better result in comparison with the attentional encoder-decoder\nmodel trained on standard front-end filterbank features.", "published": "2017-09-22 15:38:09", "link": "http://arxiv.org/abs/1709.07814v1", "categories": ["cs.CL", "cs.LG", "cs.SD"], "primary_category": "cs.CL"}
{"title": "FiLM: Visual Reasoning with a General Conditioning Layer", "abstract": "We introduce a general-purpose conditioning method for neural networks called\nFiLM: Feature-wise Linear Modulation. FiLM layers influence neural network\ncomputation via a simple, feature-wise affine transformation based on\nconditioning information. We show that FiLM layers are highly effective for\nvisual reasoning - answering image-related questions which require a\nmulti-step, high-level process - a task which has proven difficult for standard\ndeep learning methods that do not explicitly model reasoning. Specifically, we\nshow on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error\nfor the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are\nrobust to ablations and architectural modifications, and 4) generalize well to\nchallenging, new data from few examples or even zero-shot.", "published": "2017-09-22 17:54:12", "link": "http://arxiv.org/abs/1709.07871v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.CV"}
{"title": "Unsupervised Learning of Disentangled and Interpretable Representations\n  from Sequential Data", "abstract": "We present a factorized hierarchical variational autoencoder, which learns\ndisentangled and interpretable representations from sequential data without\nsupervision. Specifically, we exploit the multi-scale nature of information in\nsequential data by formulating it explicitly within a factorized hierarchical\ngraphical model that imposes sequence-dependent priors and sequence-independent\npriors to different sets of latent variables. The model is evaluated on two\nspeech corpora to demonstrate, qualitatively, its ability to transform speakers\nor linguistic content by manipulating different sets of latent variables; and\nquantitatively, its ability to outperform an i-vector baseline for speaker\nverification and reduce the word error rate by as much as 35% in mismatched\ntrain/test scenarios for automatic speech recognition tasks.", "published": "2017-09-22 18:36:50", "link": "http://arxiv.org/abs/1709.07902v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Computational Content Analysis of Negative Tweets for Obesity, Diet,\n  Diabetes, and Exercise", "abstract": "Social media based digital epidemiology has the potential to support faster\nresponse and deeper understanding of public health related threats. This study\nproposes a new framework to analyze unstructured health related textual data\nvia Twitter users' post (tweets) to characterize the negative health sentiments\nand non-health related concerns in relations to the corpus of negative\nsentiments, regarding Diet Diabetes Exercise, and Obesity (DDEO). Through the\ncollection of 6 million Tweets for one month, this study identified the\nprominent topics of users as it relates to the negative sentiments. Our\nproposed framework uses two text mining methods, sentiment analysis and topic\nmodeling, to discover negative topics. The negative sentiments of Twitter users\nsupport the literature narratives and the many morbidity issues that are\nassociated with DDEO and the linkage between obesity and diabetes. The\nframework offers a potential method to understand the publics' opinions and\nsentiments regarding DDEO. More importantly, this research provides new\nopportunities for computational social scientists, medical experts, and public\nhealth professionals to collectively address DDEO-related issues.", "published": "2017-09-22 19:18:42", "link": "http://arxiv.org/abs/1709.07915v1", "categories": ["cs.SI", "cs.CL", "stat.AP", "stat.CO", "stat.ML"], "primary_category": "cs.SI"}
{"title": "Characterizing Diabetes, Diet, Exercise, and Obesity Comments on Twitter", "abstract": "Social media provide a platform for users to express their opinions and share\ninformation. Understanding public health opinions on social media, such as\nTwitter, offers a unique approach to characterizing common health issues such\nas diabetes, diet, exercise, and obesity (DDEO), however, collecting and\nanalyzing a large scale conversational public health data set is a challenging\nresearch task. The goal of this research is to analyze the characteristics of\nthe general public's opinions in regard to diabetes, diet, exercise and obesity\n(DDEO) as expressed on Twitter. A multi-component semantic and linguistic\nframework was developed to collect Twitter data, discover topics of interest\nabout DDEO, and analyze the topics. From the extracted 4.5 million tweets, 8%\nof tweets discussed diabetes, 23.7% diet, 16.6% exercise, and 51.7% obesity.\nThe strongest correlation among the topics was determined between exercise and\nobesity. Other notable correlations were: diabetes and obesity, and diet and\nobesity DDEO terms were also identified as subtopics of each of the DDEO\ntopics. The frequent subtopics discussed along with Diabetes, excluding the\nDDEO terms themselves, were blood pressure, heart attack, yoga, and Alzheimer.\nThe non-DDEO subtopics for Diet included vegetarian, pregnancy, celebrities,\nweight loss, religious, and mental health, while subtopics for Exercise\nincluded computer games, brain, fitness, and daily plan. Non-DDEO subtopics for\nObesity included Alzheimer, cancer, and children. With 2.67 billion social\nmedia users in 2016, publicly available data such as Twitter posts can be\nutilized to support clinical providers, public health experts, and social\nscientists in better understanding common public opinions in regard to\ndiabetes, diet, exercise, and obesity.", "published": "2017-09-22 19:19:49", "link": "http://arxiv.org/abs/1709.07916v1", "categories": ["cs.SI", "cs.CL", "stat.AP", "stat.CO", "stat.ML"], "primary_category": "cs.SI"}
{"title": "Techniques and Challenges in Speech Synthesis", "abstract": "The aim of this project was to develop and implement an English language\nText-to-Speech synthesis system. This involved a study of mechanisms of human\nspeech production, a review of techniques in speech synthesis, and analysis of\ntests used to evaluate the effectiveness of synthesized speech. It was\ndetermined that a diphone synthesis system was the most effective choice for\nthe scope of this project. A method of automatically identifying and extracting\ndiphones from prompted speech was designed, allowing for the creation of a\ndiphone database by a speaker in less than 40 minutes. CMUdict was used to\ndetermine the pronunciation of known words. A system for smoothing the\ntransitions between diphone recordings was designed and implemented. CMUdict\nwas then used to train a maximum-likelihood prediction system to determine the\ncorrect pronunciation of unknown English language alphabetic words. Then, a\nPart Of Speech tagger was designed to find the lexical class of words within a\nsentence.\n  A method of altering the pitch, duration, and volume of the produced voice\nover time was designed, being a combination of the TD-PSOLA algorithm and a\nnovel approach referred to as Unvoiced Speech Duration Shifting. This minimises\ndistortion of the voice when shifting the pitch or duration, while maximising\ncomputational efficiency by operating in the time domain. This approach was\nused to add correct lexical stress to vowels within words. A text tokenisation\nsystem was developed to handle arbitrary text input, allowing pronunciation of\nnumerical input tokens and use of appropriate pauses for punctuation. Methods\nfor further improving sentence level speech naturalness were discussed.\nFinally, the system was tested with listeners for its intelligibility and\nnaturalness.", "published": "2017-09-22 00:45:12", "link": "http://arxiv.org/abs/1709.07552v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
