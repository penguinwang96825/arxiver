{"title": "Scaling Recurrent Neural Network Language Models", "abstract": "This paper investigates the scaling properties of Recurrent Neural Network\nLanguage Models (RNNLMs). We discuss how to train very large RNNs on GPUs and\naddress the questions of how RNNLMs scale with respect to model size,\ntraining-set size, computational costs and memory. Our analysis shows that\ndespite being more costly to train, RNNLMs obtain much lower perplexities on\nstandard benchmarks than n-gram models. We train the largest known RNNs and\npresent relative word error rates gains of 18% on an ASR task. We also present\nthe new lowest perplexities on the recently released billion word language\nmodelling benchmark, 1 BLEU point gain on machine translation and a 17%\nrelative hit rate gain in word prediction.", "published": "2015-02-02 15:27:37", "link": "http://arxiv.org/abs/1502.00512v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
