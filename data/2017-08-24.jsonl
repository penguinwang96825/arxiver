{"title": "NNVLP: A Neural Network-Based Vietnamese Language Processing Toolkit", "abstract": "This paper demonstrates neural network-based toolkit namely NNVLP for\nessential Vietnamese language processing tasks including part-of-speech (POS)\ntagging, chunking, named entity recognition (NER). Our toolkit is a combination\nof bidirectional Long Short-Term Memory (Bi-LSTM), Convolutional Neural Network\n(CNN), Conditional Random Field (CRF), using pre-trained word embeddings as\ninput, which achieves state-of-the-art results on these three tasks. We provide\nboth API and web demo for this toolkit.", "published": "2017-08-24 01:26:48", "link": "http://arxiv.org/abs/1708.07241v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Discrete and Neural Features for Sequence Labeling", "abstract": "Neural network models have recently received heated research attention in the\nnatural language processing community. Compared with traditional models with\ndiscrete features, neural models have two main advantages. First, they take\nlow-dimensional, real-valued embedding vectors as inputs, which can be trained\nover large raw data, thereby addressing the issue of feature sparsity in\ndiscrete models. Second, deep neural networks can be used to automatically\ncombine input features, and including non-local features that capture semantic\npatterns that cannot be expressed using discrete indicator features. As a\nresult, neural network models have achieved competitive accuracies compared\nwith the best discrete models for a range of NLP tasks.\n  On the other hand, manual feature templates have been carefully investigated\nfor most NLP tasks over decades and typically cover the most useful indicator\npattern for solving the problems. Such information can be complementary the\nfeatures automatically induced from neural networks, and therefore combining\ndiscrete and neural features can potentially lead to better accuracy compared\nwith models that leverage discrete or neural features only.\n  In this paper, we systematically investigate the effect of discrete and\nneural feature combination for a range of fundamental NLP tasks based on\nsequence labeling, including word segmentation, POS tagging and named entity\nrecognition for Chinese and English, respectively. Our results on standard\nbenchmarks show that state-of-the-art neural models can give accuracies\ncomparable to the best discrete models in the literature for most tasks and\ncombing discrete and neural features unanimously yield better results.", "published": "2017-08-24 05:24:26", "link": "http://arxiv.org/abs/1708.07279v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CloudScan - A configuration-free invoice analysis system using recurrent\n  neural networks", "abstract": "We present CloudScan; an invoice analysis system that requires zero\nconfiguration or upfront annotation. In contrast to previous work, CloudScan\ndoes not rely on templates of invoice layout, instead it learns a single global\nmodel of invoices that naturally generalizes to unseen invoice layouts. The\nmodel is trained using data automatically extracted from end-user provided\nfeedback. This automatic training data extraction removes the requirement for\nusers to annotate the data precisely. We describe a recurrent neural network\nmodel that can capture long range context and compare it to a baseline logistic\nregression model corresponding to the current CloudScan production system. We\ntrain and evaluate the system on 8 important fields using a dataset of 326,471\ninvoices. The recurrent neural network and baseline model achieve 0.891 and\n0.887 average F1 scores respectively on seen invoice layouts. For the harder\ntask of unseen invoice layouts, the recurrent neural network model outperforms\nthe baseline with 0.840 average F1 compared to 0.788.", "published": "2017-08-24 13:40:06", "link": "http://arxiv.org/abs/1708.07403v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "M2D: Monolog to Dialog Generation for Conversational Story Telling", "abstract": "Storytelling serves many different social functions, e.g. stories are used to\npersuade, share troubles, establish shared values, learn social behaviors, and\nentertain. Moreover, stories are often told conversationally through dialog,\nand previous work suggests that information provided dialogically is more\nengaging than when provided in monolog. In this paper, we present algorithms\nfor converting a deep representation of a story into a dialogic storytelling,\nthat can vary aspects of the telling, including the personality of the\nstorytellers. We conduct several experiments to test whether dialogic\nstorytellings are more engaging, and whether automatically generated variants\nin linguistic form that correspond to personality differences can be recognized\nin an extended storytelling dialog.", "published": "2017-08-24 16:06:18", "link": "http://arxiv.org/abs/1708.07476v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Study on Neural Network Language Modeling", "abstract": "An exhaustive study on neural network language modeling (NNLM) is performed\nin this paper. Different architectures of basic neural network language models\nare described and examined. A number of different improvements over basic\nneural network language models, including importance sampling, word classes,\ncaching and bidirectional recurrent neural network (BiRNN), are studied\nseparately, and the advantages and disadvantages of every technique are\nevaluated. Then, the limits of neural network language modeling are explored\nfrom the aspects of model architecture and knowledge representation. Part of\nthe statistical information from a word sequence will loss when it is processed\nword by word in a certain order, and the mechanism of training neural network\nby updating weight matrixes and vectors imposes severe restrictions on any\nsignificant enhancement of NNLM. For knowledge representation, the knowledge\nrepresented by neural network language models is the approximate probabilistic\ndistribution of word sequences from a certain training data set rather than the\nknowledge of a language itself or the information conveyed by word sequences in\na natural language. Finally, some directions for improving neural network\nlanguage modeling further is discussed.", "published": "2017-08-24 02:14:50", "link": "http://arxiv.org/abs/1708.07252v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Image Analysis Approach to the Calligraphy of Books", "abstract": "Text network analysis has received increasing attention as a consequence of\nits wide range of applications. In this work, we extend a previous work founded\non the study of topological features of mesoscopic networks. Here, the\ngeometrical properties of visualized networks are quantified in terms of\nseveral image analysis techniques and used as subsidies for authorship\nattribution. It was found that the visual features account for performance\nsimilar to that achieved by using topological measurements. In addition, the\ncombination of these two types of features improved the performance.", "published": "2017-08-24 03:12:22", "link": "http://arxiv.org/abs/1708.07265v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Supervised Speech Separation Based on Deep Learning: An Overview", "abstract": "Speech separation is the task of separating target speech from background\ninterference. Traditionally, speech separation is studied as a signal\nprocessing problem. A more recent approach formulates speech separation as a\nsupervised learning problem, where the discriminative patterns of speech,\nspeakers, and background noise are learned from training data. Over the past\ndecade, many supervised separation algorithms have been put forward. In\nparticular, the recent introduction of deep learning to supervised speech\nseparation has dramatically accelerated progress and boosted separation\nperformance. This article provides a comprehensive overview of the research on\ndeep learning based supervised speech separation in the last several years. We\nfirst introduce the background of speech separation and the formulation of\nsupervised separation. Then we discuss three main components of supervised\nseparation: learning machines, training targets, and acoustic features. Much of\nthe overview is on separation algorithms where we review monaural methods,\nincluding speech enhancement (speech-nonspeech separation), speaker separation\n(multi-talker separation), and speech dereverberation, as well as\nmulti-microphone techniques. The important issue of generalization, unique to\nsupervised learning, is discussed. This overview provides a historical\nperspective on how advances are made. In addition, we discuss a number of\nconceptual issues, including what constitutes the target source.", "published": "2017-08-24 18:51:50", "link": "http://arxiv.org/abs/1708.07524v2", "categories": ["cs.CL", "cs.LG", "cs.NE", "cs.SD"], "primary_category": "cs.CL"}
{"title": "A dependency look at the reality of constituency", "abstract": "A comment on \"Neurophysiological dynamics of phrase-structure building during\nsentence processing\" by Nelson et al (2017), Proceedings of the National\nAcademy of Sciences USA 114(18), E3669-E3678.", "published": "2017-08-24 04:59:53", "link": "http://arxiv.org/abs/1708.07722v3", "categories": ["cs.CL", "cs.SI", "physics.soc-ph"], "primary_category": "cs.CL"}
