{"title": "Quantifying origin and character of long-range correlations in narrative\n  texts", "abstract": "In natural language using short sentences is considered efficient for\ncommunication. However, a text composed exclusively of such sentences looks\ntechnical and reads boring. A text composed of long ones, on the other hand,\ndemands significantly more effort for comprehension. Studying characteristics\nof the sentence length variability (SLV) in a large corpus of world-famous\nliterary texts shows that an appealing and aesthetic optimum appears somewhere\nin between and involves selfsimilar, cascade-like alternation of various\nlengths sentences. A related quantitative observation is that the power spectra\nS(f) of thus characterized SLV universally develop a convincing `1/f^beta'\nscaling with the average exponent beta =~ 1/2, close to what has been\nidentified before in musical compositions or in the brain waves. An\noverwhelming majority of the studied texts simply obeys such fractal attributes\nbut especially spectacular in this respect are hypertext-like, \"stream of\nconsciousness\" novels. In addition, they appear to develop structures\ncharacteristic of irreducibly interwoven sets of fractals called multifractals.\nScaling of S(f) in the present context implies existence of the long-range\ncorrelations in texts and appearance of multifractality indicates that they\ncarry even a nonlinear component. A distinct role of the full stops in inducing\nthe long-range correlations in texts is evidenced by the fact that the above\nquantitative characteristics on the long-range correlations manifest themselves\nin variation of the full stops recurrence times along texts, thus in SLV, but\nto a much lesser degree in the recurrence times of the most frequent words. In\nthis latter case the nonlinear correlations, thus multifractality, disappear\neven completely for all the texts considered. Treated as one extra word, the\nfull stops at the same time appear to obey the Zipfian rank-frequency\ndistribution, however.", "published": "2014-12-29 12:00:25", "link": "http://arxiv.org/abs/1412.8319v2", "categories": ["cs.CL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Probing the topological properties of complex networks modeling short\n  written texts", "abstract": "In recent years, graph theory has been widely employed to probe several\nlanguage properties. More specifically, the so-called word adjacency model has\nbeen proven useful for tackling several practical problems, especially those\nrelying on textual stylistic analysis. The most common approach to treat texts\nas networks has simply considered either large pieces of texts or entire books.\nThis approach has certainly worked well -- many informative discoveries have\nbeen made this way -- but it raises an uncomfortable question: could there be\nimportant topological patterns in small pieces of texts? To address this\nproblem, the topological properties of subtexts sampled from entire books was\nprobed. Statistical analyzes performed on a dataset comprising 50 novels\nrevealed that most of the traditional topological measurements are stable for\nshort subtexts. When the performance of the authorship recognition task was\nanalyzed, it was found that a proper sampling yields a discriminability similar\nto the one found with full texts. Surprisingly, the support vector machine\nclassification based on the characterization of short texts outperformed the\none performed with entire books. These findings suggest that a local\ntopological analysis of large documents might improve its global\ncharacterization. Most importantly, it was verified, as a proof of principle,\nthat short texts can be analyzed with the methods and concepts of complex\nnetworks. As a consequence, the techniques described here can be extended in a\nstraightforward fashion to analyze texts as time-varying complex networks.", "published": "2014-12-29 23:09:13", "link": "http://arxiv.org/abs/1412.8504v1", "categories": ["cs.CL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Simple Image Description Generator via a Linear Phrase-Based Approach", "abstract": "Generating a novel textual description of an image is an interesting problem\nthat connects computer vision and natural language processing. In this paper,\nwe present a simple model that is able to generate descriptive sentences given\na sample image. This model has a strong focus on the syntax of the\ndescriptions. We train a purely bilinear model that learns a metric between an\nimage representation (generated from a previously trained Convolutional Neural\nNetwork) and phrases that are used to described them. The system is then able\nto infer phrases from a given image sample. Based on caption syntax statistics,\nwe propose a simple language model that can produce relevant descriptions for a\ngiven test image using the phrases inferred. Our approach, which is\nconsiderably simpler than state-of-the-art models, achieves comparable results\non the recently release Microsoft COCO dataset.", "published": "2014-12-29 18:43:10", "link": "http://arxiv.org/abs/1412.8419v3", "categories": ["cs.CL", "cs.CV", "cs.NE"], "primary_category": "cs.CL"}
