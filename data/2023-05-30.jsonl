{"title": "Grokking of Hierarchical Structure in Vanilla Transformers", "abstract": "For humans, language production and comprehension is sensitive to the\nhierarchical structure of sentences. In natural language processing, past work\nhas questioned how effectively neural sequence models like transformers capture\nthis hierarchical structure when generalizing to structurally novel inputs. We\nshow that transformer language models can learn to generalize hierarchically\nafter training for extremely long periods -- far beyond the point when\nin-domain accuracy has saturated. We call this phenomenon \\emph{structural\ngrokking}. On multiple datasets, structural grokking exhibits inverted U-shaped\nscaling in model depth: intermediate-depth models generalize better than both\nvery deep and very shallow transformers. When analyzing the relationship\nbetween model-internal properties and grokking, we find that optimal depth for\ngrokking can be identified using the tree-structuredness metric of\n\\citet{murty2023projections}. Overall, our work provides strong evidence that,\nwith extended training, vanilla transformers discover and use hierarchical\nstructure.", "published": "2023-05-30 04:34:13", "link": "http://arxiv.org/abs/2305.18741v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph Reasoning for Question Answering with Triplet Retrieval", "abstract": "Answering complex questions often requires reasoning over knowledge graphs\n(KGs). State-of-the-art methods often utilize entities in questions to retrieve\nlocal subgraphs, which are then fed into KG encoder, e.g. graph neural networks\n(GNNs), to model their local structures and integrated into language models for\nquestion answering. However, this paradigm constrains retrieved knowledge in\nlocal subgraphs and discards more diverse triplets buried in KGs that are\ndisconnected but useful for question answering. In this paper, we propose a\nsimple yet effective method to first retrieve the most relevant triplets from\nKGs and then rerank them, which are then concatenated with questions to be fed\ninto language models. Extensive results on both CommonsenseQA and OpenbookQA\ndatasets show that our method can outperform state-of-the-art up to 4.6%\nabsolute accuracy.", "published": "2023-05-30 04:46:28", "link": "http://arxiv.org/abs/2305.18742v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shuo Wen Jie Zi: Rethinking Dictionaries and Glyphs for Chinese Language\n  Pre-training", "abstract": "We introduce CDBERT, a new learning paradigm that enhances the semantics\nunderstanding ability of the Chinese PLMs with dictionary knowledge and\nstructure of Chinese characters. We name the two core modules of CDBERT as\nShuowen and Jiezi, where Shuowen refers to the process of retrieving the most\nappropriate meaning from Chinese dictionaries and Jiezi refers to the process\nof enhancing characters' glyph representations with structure understanding. To\nfacilitate dictionary understanding, we propose three pre-training tasks, i.e.,\nMasked Entry Modeling, Contrastive Learning for Synonym and Antonym, and\nExample Learning. We evaluate our method on both modern Chinese understanding\nbenchmark CLUE and ancient Chinese benchmark CCLUE. Moreover, we propose a new\npolysemy discrimination task PolyMRC based on the collected dictionary of\nancient Chinese. Our paradigm demonstrates consistent improvements on previous\nChinese PLMs across all tasks. Moreover, our approach yields significant\nboosting on few-shot setting of ancient Chinese understanding.", "published": "2023-05-30 05:48:36", "link": "http://arxiv.org/abs/2305.18760v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Research on Multilingual News Clustering Based on Cross-Language Word\n  Embeddings", "abstract": "Classifying the same event reported by different countries is of significant\nimportance for public opinion control and intelligence gathering. Due to the\ndiverse types of news, relying solely on transla-tors would be costly and\ninefficient, while depending solely on translation systems would incur\nconsiderable performance overheads in invoking translation interfaces and\nstoring translated texts. To address this issue, we mainly focus on the\nclustering problem of cross-lingual news. To be specific, we use a combination\nof sentence vector representations of news headlines in a mixed semantic space\nand the topic probability distributions of news content to represent a news\narticle. In the training of cross-lingual models, we employ knowledge\ndistillation techniques to fit two semantic spaces into a mixed semantic space.\nWe abandon traditional static clustering methods like K-Means and AGNES in\nfavor of the incremental clustering algorithm Single-Pass, which we further\nmodify to better suit cross-lingual news clustering scenarios. Our main\ncontributions are as follows: (1) We adopt the English standard BERT as the\nteacher model and XLM-Roberta as the student model, training a cross-lingual\nmodel through knowledge distillation that can represent sentence-level\nbilingual texts in both Chinese and English. (2) We use the LDA topic model to\nrepresent news as a combina-tion of cross-lingual vectors for headlines and\ntopic probability distributions for con-tent, introducing concepts such as\ntopic similarity to address the cross-lingual issue in news content\nrepresentation. (3) We adapt the Single-Pass clustering algorithm for the news\ncontext to make it more applicable. Our optimizations of Single-Pass include\nad-justing the distance algorithm between samples and clusters, adding cluster\nmerging operations, and incorporating a news time parameter.", "published": "2023-05-30 09:24:55", "link": "http://arxiv.org/abs/2305.18880v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Where's the Point? Self-Supervised Multilingual Punctuation-Agnostic\n  Sentence Segmentation", "abstract": "Many NLP pipelines split text into sentences as one of the crucial\npreprocessing steps. Prior sentence segmentation tools either rely on\npunctuation or require a considerable amount of sentence-segmented training\ndata: both central assumptions might fail when porting sentence segmenters to\ndiverse languages on a massive scale. In this work, we thus introduce a\nmultilingual punctuation-agnostic sentence segmentation method, currently\ncovering 85 languages, trained in a self-supervised fashion on unsegmented\ntext, by making use of newline characters which implicitly perform segmentation\ninto paragraphs. We further propose an approach that adapts our method to the\nsegmentation in a given corpus by using only a small number (64-256) of\nsentence-segmented examples. The main results indicate that our method\noutperforms all the prior best sentence-segmentation tools by an average of\n6.1% F1 points. Furthermore, we demonstrate that proper sentence segmentation\nhas a point: the use of a (powerful) sentence segmenter makes a considerable\ndifference for a downstream application such as machine translation (MT). By\nusing our method to match sentence segmentation to the segmentation used during\ntraining of MT models, we achieve an average improvement of 2.3 BLEU points\nover the best prior segmentation tool, as well as massive gains over a trivial\nsegmenter that splits text into equally sized blocks.", "published": "2023-05-30 09:49:42", "link": "http://arxiv.org/abs/2305.18893v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empirical Sufficiency Lower Bounds for Language Modeling with\n  Locally-Bootstrapped Semantic Structures", "abstract": "In this work we build upon negative results from an attempt at language\nmodeling with predicted semantic structure, in order to establish empirical\nlower bounds on what could have made the attempt successful. More specifically,\nwe design a concise binary vector representation of semantic structure at the\nlexical level and evaluate in-depth how good an incremental tagger needs to be\nin order to achieve better-than-baseline performance with an end-to-end\nsemantic-bootstrapping language model. We envision such a system as consisting\nof a (pretrained) sequential-neural component and a hierarchical-symbolic\ncomponent working together to generate text with low surprisal and high\nlinguistic interpretability. We find that (a) dimensionality of the semantic\nvector representation can be dramatically reduced without losing its main\nadvantages and (b) lower bounds on prediction quality cannot be established via\na single score alone, but need to take the distributions of signal and noise\ninto account.", "published": "2023-05-30 10:09:48", "link": "http://arxiv.org/abs/2305.18915v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fighting Bias with Bias: Promoting Model Robustness by Amplifying\n  Dataset Biases", "abstract": "NLP models often rely on superficial cues known as dataset biases to achieve\nimpressive performance, and can fail on examples where these biases do not\nhold. Recent work sought to develop robust, unbiased models by filtering biased\nexamples from training sets. In this work, we argue that such filtering can\nobscure the true capabilities of models to overcome biases, which might never\nbe removed in full from the dataset. We suggest that in order to drive the\ndevelopment of models robust to subtle biases, dataset biases should be\namplified in the training set. We introduce an evaluation framework defined by\na bias-amplified training set and an anti-biased test set, both automatically\nextracted from existing datasets. Experiments across three notions of bias,\nfour datasets and two models show that our framework is substantially more\nchallenging for models than the original data splits, and even more challenging\nthan hand-crafted challenge sets. Our evaluation framework can use any existing\ndataset, even those considered obsolete, to test model robustness. We hope our\nwork will guide the development of robust models that do not rely on\nsuperficial biases and correlations. To this end, we publicly release our code\nand data.", "published": "2023-05-30 10:10:42", "link": "http://arxiv.org/abs/2305.18917v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Document-Level Multi-Event Extraction with Event Proxy Nodes and\n  Hausdorff Distance Minimization", "abstract": "Document-level multi-event extraction aims to extract the structural\ninformation from a given document automatically. Most recent approaches usually\ninvolve two steps: (1) modeling entity interactions; (2) decoding entity\ninteractions into events. However, such approaches ignore a global view of\ninter-dependency of multiple events. Moreover, an event is decoded by\niteratively merging its related entities as arguments, which might suffer from\nerror propagation and is computationally inefficient. In this paper, we propose\nan alternative approach for document-level multi-event extraction with event\nproxy nodes and Hausdorff distance minimization. The event proxy nodes,\nrepresenting pseudo-events, are able to build connections with other event\nproxy nodes, essentially capturing global information. The Hausdorff distance\nmakes it possible to compare the similarity between the set of predicted events\nand the set of ground-truth events. By directly minimizing Hausdorff distance,\nthe model is trained towards the global optimum directly, which improves\nperformance and reduces training time. Experimental results show that our model\noutperforms previous state-of-the-art method in F1-score on two datasets with\nonly a fraction of training time.", "published": "2023-05-30 10:33:05", "link": "http://arxiv.org/abs/2305.18926v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multilingual Evaluation of NER Robustness to Adversarial Inputs", "abstract": "Adversarial evaluations of language models typically focus on English alone.\nIn this paper, we performed a multilingual evaluation of Named Entity\nRecognition (NER) in terms of its robustness to small perturbations in the\ninput. Our results showed the NER models we explored across three languages\n(English, German and Hindi) are not very robust to such changes, as indicated\nby the fluctuations in the overall F1 score as well as in a more fine-grained\nevaluation. With that knowledge, we further explored whether it is possible to\nimprove the existing NER models using a part of the generated adversarial data\nsets as augmented training data to train a new NER model or as fine-tuning data\nto adapt an existing NER model. Our results showed that both these approaches\nimprove performance on the original as well as adversarial test sets. While\nthere is no significant difference between the two approaches for English,\nre-training is significantly better than fine-tuning for German and Hindi.", "published": "2023-05-30 10:50:49", "link": "http://arxiv.org/abs/2305.18933v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DEPLAIN: A German Parallel Corpus with Intralingual Translations into\n  Plain Language for Sentence and Document Simplification", "abstract": "Text simplification is an intralingual translation task in which documents,\nor sentences of a complex source text are simplified for a target audience. The\nsuccess of automatic text simplification systems is highly dependent on the\nquality of parallel data used for training and evaluation. To advance sentence\nsimplification and document simplification in German, this paper presents\nDEplain, a new dataset of parallel, professionally written and manually aligned\nsimplifications in plain German (\"plain DE\" or in German: \"Einfache Sprache\").\nDEplain consists of a news domain (approx. 500 document pairs, approx. 13k\nsentence pairs) and a web-domain corpus (approx. 150 aligned documents, approx.\n2k aligned sentence pairs). In addition, we are building a web harvester and\nexperimenting with automatic alignment methods to facilitate the integration of\nnon-aligned and to be published parallel documents. Using this approach, we are\ndynamically increasing the web domain corpus, so it is currently extended to\napprox. 750 document pairs and approx. 3.5k aligned sentence pairs. We show\nthat using DEplain to train a transformer-based seq2seq text simplification\nmodel can achieve promising results. We make available the corpus, the adapted\nalignment methods for German, the web harvester and the trained models here:\nhttps://github.com/rstodden/DEPlain.", "published": "2023-05-30 11:07:46", "link": "http://arxiv.org/abs/2305.18939v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Wave to Syntax: Probing spoken language models for syntax", "abstract": "Understanding which information is encoded in deep models of spoken and\nwritten language has been the focus of much research in recent years, as it is\ncrucial for debugging and improving these architectures. Most previous work has\nfocused on probing for speaker characteristics, acoustic and phonological\ninformation in models of spoken language, and for syntactic information in\nmodels of written language. Here we focus on the encoding of syntax in several\nself-supervised and visually grounded models of spoken language. We employ two\ncomplementary probing methods, combined with baselines and reference\nrepresentations to quantify the degree to which syntactic structure is encoded\nin the activations of the target models. We show that syntax is captured most\nprominently in the middle layers of the networks, and more explicitly within\nmodels with more parameters.", "published": "2023-05-30 11:43:18", "link": "http://arxiv.org/abs/2305.18957v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross Encoding as Augmentation: Towards Effective Educational Text\n  Classification", "abstract": "Text classification in education, usually called auto-tagging, is the\nautomated process of assigning relevant tags to educational content, such as\nquestions and textbooks. However, auto-tagging suffers from a data scarcity\nproblem, which stems from two major challenges: 1) it possesses a large tag\nspace and 2) it is multi-label. Though a retrieval approach is reportedly good\nat low-resource scenarios, there have been fewer efforts to directly address\nthe data scarcity problem. To mitigate these issues, here we propose a novel\nretrieval approach CEAA that provides effective learning in educational text\nclassification. Our main contributions are as follows: 1) we leverage transfer\nlearning from question-answering datasets, and 2) we propose a simple but\neffective data augmentation method introducing cross-encoder style texts to a\nbi-encoder architecture for more efficient inference. An extensive set of\nexperiments shows that our proposed method is effective in multi-label\nscenarios and low-resource tags compared to state-of-the-art models.", "published": "2023-05-30 12:19:30", "link": "http://arxiv.org/abs/2305.18977v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Back to Patterns: Efficient Japanese Morphological Analysis with\n  Feature-Sequence Trie", "abstract": "Accurate neural models are much less efficient than non-neural models and are\nuseless for processing billions of social media posts or handling user queries\nin real time with a limited budget. This study revisits the fastest\npattern-based NLP methods to make them as accurate as possible, thus yielding a\nstrikingly simple yet surprisingly accurate morphological analyzer for\nJapanese. The proposed method induces reliable patterns from a morphological\ndictionary and annotated data. Experimental results on two standard datasets\nconfirm that the method exhibits comparable accuracy to learning-based\nbaselines, while boasting a remarkable throughput of over 1,000,000 sentences\nper second on a single modern CPU. The source code is available at\nhttps://www.tkl.iis.u-tokyo.ac.jp/~ynaga/jagger/", "published": "2023-05-30 14:00:30", "link": "http://arxiv.org/abs/2305.19045v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Together We Make Sense -- Learning Meta-Sense Embeddings from Pretrained\n  Static Sense Embeddings", "abstract": "Sense embedding learning methods learn multiple vectors for a given ambiguous\nword, corresponding to its different word senses. For this purpose, different\nmethods have been proposed in prior work on sense embedding learning that use\ndifferent sense inventories, sense-tagged corpora and learning methods.\nHowever, not all existing sense embeddings cover all senses of ambiguous words\nequally well due to the discrepancies in their training resources. To address\nthis problem, we propose the first-ever meta-sense embedding method --\nNeighbour Preserving Meta-Sense Embeddings, which learns meta-sense embeddings\nby combining multiple independently trained source sense embeddings such that\nthe sense neighbourhoods computed from the source embeddings are preserved in\nthe meta-embedding space. Our proposed method can combine source sense\nembeddings that cover different sets of word senses. Experimental results on\nWord Sense Disambiguation (WSD) and Word-in-Context (WiC) tasks show that the\nproposed meta-sense embedding method consistently outperforms several\ncompetitive baselines.", "published": "2023-05-30 14:53:44", "link": "http://arxiv.org/abs/2305.19092v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does Conceptual Representation Require Embodiment? Insights From Large\n  Language Models", "abstract": "To what extent can language alone give rise to complex concepts, or is\nembodied experience essential? Recent advancements in large language models\n(LLMs) offer fresh perspectives on this question. Although LLMs are trained on\nrestricted modalities, they exhibit human-like performance in diverse\npsychological tasks. Our study compared representations of 4,442 lexical\nconcepts between humans and ChatGPTs (GPT-3.5 and GPT-4) across multiple\ndimensions, including five key domains: emotion, salience, mental\nvisualization, sensory, and motor experience. We identify two main findings: 1)\nBoth models strongly align with human representations in non-sensorimotor\ndomains but lag in sensory and motor areas, with GPT-4 outperforming GPT-3.5;\n2) GPT-4's gains are associated with its additional visual learning, which also\nappears to benefit related dimensions like haptics and imageability. These\nresults highlight the limitations of language in isolation, and that the\nintegration of diverse modalities of inputs leads to a more human-like\nconceptual representation.", "published": "2023-05-30 15:06:28", "link": "http://arxiv.org/abs/2305.19103v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Encouraging Divergent Thinking in Large Language Models through\n  Multi-Agent Debate", "abstract": "Modern large language models (LLMs) like ChatGPT have shown remarkable\nperformance on general language tasks but still struggle on complex reasoning\ntasks, which drives the research on cognitive behaviors of LLMs to explore\nhuman-like problem-solving strategies. Along this direction, one representative\nstrategy is self-reflection, which asks an LLM to refine the solution with the\nfeedback generated by itself iteratively. However, our study shows that such\nreflection-style methods suffer from the Degeneration-of-Thought (DoT) problem:\nonce the LLM has established confidence in its solutions, it is unable to\ngenerate novel thoughts later through reflection even if its initial stance is\nincorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD)\nframework, in which multiple agents express their arguments in the state of\n\"tit for tat\" and a judge manages the debate process to obtain a final\nsolution. Clearly, our MAD framework encourages divergent thinking in LLMs\nwhich would be helpful for tasks that require deep levels of contemplation.\nExperiment results on two challenging datasets, commonsense machine translation\nand counter-intuitive arithmetic reasoning, demonstrate the effectiveness of\nour MAD framework. Extensive analyses suggest that the adaptive break of debate\nand the modest level of \"tit for tat\" state are required for MAD to obtain good\nperformance. Moreover, we find that LLMs might not be a fair judge if different\nLLMs are used for agents. Code is available at\nhttps://github.com/Skytliang/Multi-Agents-Debate.", "published": "2023-05-30 15:25:45", "link": "http://arxiv.org/abs/2305.19118v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Tale of Two Laws of Semantic Change: Predicting Synonym Changes with\n  Distributional Semantic Models", "abstract": "Lexical Semantic Change is the study of how the meaning of words evolves\nthrough time. Another related question is whether and how lexical relations\nover pairs of words, such as synonymy, change over time. There are currently\ntwo competing, apparently opposite hypotheses in the historical linguistic\nliterature regarding how synonymous words evolve: the Law of Differentiation\n(LD) argues that synonyms tend to take on different meanings over time, whereas\nthe Law of Parallel Change (LPC) claims that synonyms tend to undergo the same\nsemantic change and therefore remain synonyms. So far, there has been little\nresearch using distributional models to assess to what extent these laws apply\non historical corpora. In this work, we take a first step toward detecting\nwhether LD or LPC operates for given word pairs. After recasting the problem\ninto a more tractable task, we combine two linguistic resources to propose the\nfirst complete evaluation framework on this problem and provide empirical\nevidence in favor of a dominance of LD. We then propose various computational\napproaches to the problem using Distributional Semantic Models and grounded in\nrecent literature on Lexical Semantic Change detection. Our best approaches\nachieve a balanced accuracy above 0.6 on our dataset. We discuss challenges\nstill faced by these approaches, such as polysemy or the potential confusion\nbetween synonymy and hypernymy.", "published": "2023-05-30 15:50:29", "link": "http://arxiv.org/abs/2305.19143v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BLEU Meets COMET: Combining Lexical and Neural Metrics Towards Robust\n  Machine Translation Evaluation", "abstract": "Although neural-based machine translation evaluation metrics, such as COMET\nor BLEURT, have achieved strong correlations with human judgements, they are\nsometimes unreliable in detecting certain phenomena that can be considered as\ncritical errors, such as deviations in entities and numbers. In contrast,\ntraditional evaluation metrics, such as BLEU or chrF, which measure lexical or\ncharacter overlap between translation hypotheses and human references, have\nlower correlations with human judgements but are sensitive to such deviations.\nIn this paper, we investigate several ways of combining the two approaches in\norder to increase robustness of state-of-the-art evaluation methods to\ntranslations with critical errors. We show that by using additional information\nduring training, such as sentence-level features and word-level tags, the\ntrained metrics improve their capability to penalize translations with specific\ntroublesome phenomena, which leads to gains in correlation with human judgments\nand on recent challenge sets on several language pairs.", "published": "2023-05-30 15:50:46", "link": "http://arxiv.org/abs/2305.19144v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An AMR-based Link Prediction Approach for Document-level Event Argument\n  Extraction", "abstract": "Recent works have introduced Abstract Meaning Representation (AMR) for\nDocument-level Event Argument Extraction (Doc-level EAE), since AMR provides a\nuseful interpretation of complex semantic structures and helps to capture\nlong-distance dependency. However, in these works AMR is used only implicitly,\nfor instance, as additional features or training signals. Motivated by the fact\nthat all event structures can be inferred from AMR, this work reformulates EAE\nas a link prediction problem on AMR graphs. Since AMR is a generic structure\nand does not perfectly suit EAE, we propose a novel graph structure, Tailored\nAMR Graph (TAG), which compresses less informative subgraphs and edge types,\nintegrates span information, and highlights surrounding events in the same\ndocument. With TAG, we further propose a novel method using graph neural\nnetworks as a link prediction model to find event arguments. Our extensive\nexperiments on WikiEvents and RAMS show that this simpler approach outperforms\nthe state-of-the-art models by 3.63pt and 2.33pt F1, respectively, and do so\nwith reduced 56% inference time. The code is availabel at\nhttps://github.com/ayyyq/TARA.", "published": "2023-05-30 16:07:48", "link": "http://arxiv.org/abs/2305.19162v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SWiPE: A Dataset for Document-Level Simplification of Wikipedia Pages", "abstract": "Text simplification research has mostly focused on sentence-level\nsimplification, even though many desirable edits - such as adding relevant\nbackground information or reordering content - may require document-level\ncontext. Prior work has also predominantly framed simplification as a\nsingle-step, input-to-output task, only implicitly modeling the fine-grained,\nspan-level edits that elucidate the simplification process. To address both\ngaps, we introduce the SWiPE dataset, which reconstructs the document-level\nediting process from English Wikipedia (EW) articles to paired Simple Wikipedia\n(SEW) articles. In contrast to prior work, SWiPE leverages the entire revision\nhistory when pairing pages in order to better identify simplification edits. We\nwork with Wikipedia editors to annotate 5,000 EW-SEW document pairs, labeling\nmore than 40,000 edits with proposed 19 categories. To scale our efforts, we\npropose several models to automatically label edits, achieving an F-1 score of\nup to 70.6, indicating that this is a tractable but challenging NLU task.\nFinally, we categorize the edits produced by several simplification models and\nfind that SWiPE-trained models generate more complex edits while reducing\nunwanted edits.", "published": "2023-05-30 16:52:42", "link": "http://arxiv.org/abs/2305.19204v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Magic of IF: Investigating Causal Reasoning Abilities in Large\n  Language Models of Code", "abstract": "Causal reasoning, the ability to identify cause-and-effect relationship, is\ncrucial in human thinking. Although large language models (LLMs) succeed in\nmany NLP tasks, it is still challenging for them to conduct complex causal\nreasoning like abductive reasoning and counterfactual reasoning. Given the fact\nthat programming code may express causal relations more often and explicitly\nwith conditional statements like ``if``, we want to explore whether Code-LLMs\nacquire better causal reasoning abilities. Our experiments show that compared\nto text-only LLMs, Code-LLMs with code prompts are significantly better in\ncausal reasoning. We further intervene on the prompts from different aspects,\nand discover that the programming structure is crucial in code prompt design,\nwhile Code-LLMs are robust towards format perturbations.", "published": "2023-05-30 17:02:58", "link": "http://arxiv.org/abs/2305.19213v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Concise Answers to Complex Questions: Summarization of Long-form Answers", "abstract": "Long-form question answering systems provide rich information by presenting\nparagraph-level answers, often containing optional background or auxiliary\ninformation. While such comprehensive answers are helpful, not all information\nis required to answer the question (e.g. users with domain knowledge do not\nneed an explanation of background). Can we provide a concise version of the\nanswer by summarizing it, while still addressing the question? We conduct a\nuser study on summarized answers generated from state-of-the-art models and our\nnewly proposed extract-and-decontextualize approach. We find a large proportion\nof long-form answers (over 90%) in the ELI5 domain can be adequately summarized\nby at least one system, while complex and implicit answers are challenging to\ncompress. We observe that decontextualization improves the quality of the\nextractive summary, exemplifying its potential in the summarization task. To\npromote future work, we provide an extractive summarization dataset covering 1K\nlong-form answers and our user study annotations. Together, we present the\nfirst study on summarizing long-form answers, taking a step forward for QA\nagents that can provide answers at multiple granularities.", "published": "2023-05-30 17:59:33", "link": "http://arxiv.org/abs/2305.19271v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Breeding Machine Translations: Evolutionary approach to survive and\n  thrive in the world of automated evaluation", "abstract": "We propose a genetic algorithm (GA) based method for modifying n-best lists\nproduced by a machine translation (MT) system. Our method offers an innovative\napproach to improving MT quality and identifying weaknesses in evaluation\nmetrics. Using common GA operations (mutation and crossover) on a list of\nhypotheses in combination with a fitness function (an arbitrary MT metric), we\nobtain novel and diverse outputs with high metric scores. With a combination of\nmultiple MT metrics as the fitness function, the proposed method leads to an\nincrease in translation quality as measured by other held-out automatic\nmetrics. With a single metric (including popular ones such as COMET) as the\nfitness function, we find blind spots and flaws in the metric. This allows for\nan automated search for adversarial examples in an arbitrary metric, without\nprior assumptions on the form of such example. As a demonstration of the\nmethod, we create datasets of adversarial examples and use them to show that\nreference-free COMET is substantially less robust than the reference-based\nversion.", "published": "2023-05-30 18:00:25", "link": "http://arxiv.org/abs/2305.19330v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "infoVerse: A Universal Framework for Dataset Characterization with\n  Multidimensional Meta-information", "abstract": "The success of NLP systems often relies on the availability of large,\nhigh-quality datasets. However, not all samples in these datasets are equally\nvaluable for learning, as some may be redundant or noisy. Several methods for\ncharacterizing datasets based on model-driven meta-information (e.g., model's\nconfidence) have been developed, but the relationship and complementary effects\nof these methods have received less attention. In this paper, we introduce\ninfoVerse, a universal framework for dataset characterization, which provides a\nnew feature space that effectively captures multidimensional characteristics of\ndatasets by incorporating various model-driven meta-information. infoVerse\nreveals distinctive regions of the dataset that are not apparent in the\noriginal semantic space, hence guiding users (or models) in identifying which\nsamples to focus on for exploration, assessment, or annotation. Additionally,\nwe propose a novel sampling method on infoVerse to select a set of data points\nthat maximizes informativeness. In three real-world applications (data pruning,\nactive learning, and data annotation), the samples chosen on infoVerse space\nconsistently outperform strong baselines in all applications. Our code and demo\nare publicly available.", "published": "2023-05-30 18:12:48", "link": "http://arxiv.org/abs/2305.19344v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Examining risks of racial biases in NLP tools for child protective\n  services", "abstract": "Although much literature has established the presence of demographic bias in\nnatural language processing (NLP) models, most work relies on curated bias\nmetrics that may not be reflective of real-world applications. At the same\ntime, practitioners are increasingly using algorithmic tools in high-stakes\nsettings, with particular recent interest in NLP. In this work, we focus on one\nsuch setting: child protective services (CPS). CPS workers often write copious\nfree-form text notes about families they are working with, and CPS agencies are\nactively seeking to deploy NLP models to leverage these data. Given\nwell-established racial bias in this setting, we investigate possible ways\ndeployed NLP is liable to increase racial disparities. We specifically examine\nword statistics within notes and algorithmic fairness in risk prediction,\ncoreference resolution, and named entity recognition (NER). We document\nconsistent algorithmic unfairness in NER models, possible algorithmic\nunfairness in coreference resolution models, and little evidence of exacerbated\nracial bias in risk prediction. While there is existing pronounced criticism of\nrisk prediction, our results expose previously undocumented risks of racial\nbias in realistic information extraction systems, highlighting potential\nconcerns in deploying them, even though they may appear more benign. Our work\nserves as a rare realistic examination of NLP algorithmic fairness in a\npotential deployed setting and a timely investigation of a specific risk\nassociated with deploying NLP in CPS settings.", "published": "2023-05-30 21:00:47", "link": "http://arxiv.org/abs/2305.19409v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Multi-Instance Multi-Label Learning for Detecting\n  Propaganda Techniques", "abstract": "Since the introduction of the SemEval 2020 Task 11 (Martino et al., 2020a),\nseveral approaches have been proposed in the literature for classifying\npropaganda based on the rhetorical techniques used to influence readers. These\nmethods, however, classify one span at a time, ignoring dependencies from the\nlabels of other spans within the same context. In this paper, we approach\npropaganda technique classification as a Multi-Instance Multi-Label (MIML)\nlearning problem (Zhou et al., 2012) and propose a simple RoBERTa-based model\n(Zhuang et al., 2021) for classifying all spans in an article simultaneously.\nFurther, we note that, due to the annotation process where annotators\nclassified the spans by following a decision tree, there is an inherent\nhierarchical relationship among the different techniques, which existing\napproaches ignore. We incorporate these hierarchical label dependencies by\nadding an auxiliary classifier for each node in the decision tree to the\ntraining objective and ensembling the predictions from the original and\nauxiliary classifiers at test time. Overall, our model leads to an absolute\nimprovement of 2.47% micro-F1 over the model from the shared task winning team\nin a cross-validation setup and is the best performing non-ensemble model on\nthe shared task leaderboard.", "published": "2023-05-30 21:23:19", "link": "http://arxiv.org/abs/2305.19419v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Utilization of Multinomial Naive Bayes Algorithm and Term Frequency\n  Inverse Document Frequency (TF-IDF Vectorizer) in Checking the Credibility of\n  News Tweet in the Philippines", "abstract": "The digitalization of news media become a good indicator of progress and\nsignal to more threats. Media disinformation or fake news is one of these\nthreats, and it is necessary to take any action in fighting disinformation.\nThis paper utilizes ground truth-based annotations and TF-IDF as feature\nextraction for the news articles which is then used as a training data set for\nMultinomial Naive Bayes. The model has an accuracy of 99.46% in training and\n88.98% in predicting unseen data. Tagging fake news as real news is a\nconcerning point on the prediction that is indicated in the F1 score of 89.68%.\nThis could lead to a negative impact. To prevent this to happen it is suggested\nto further improve the corpus collection, and use an ensemble machine learning\nto reinforce the prediction", "published": "2023-05-30 15:41:15", "link": "http://arxiv.org/abs/2306.00018v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chatbots put to the test in math and logic problems: A preliminary\n  comparison and assessment of ChatGPT-3.5, ChatGPT-4, and Google Bard", "abstract": "A comparison between three chatbots which are based on large language models,\nnamely ChatGPT-3.5, ChatGPT-4 and Google Bard is presented, focusing on their\nability to give correct answers to mathematics and logic problems. In\nparticular, we check their ability to Understand the problem at hand; Apply\nappropriate algorithms or methods for its solution; and Generate a coherent\nresponse and a correct answer. We use 30 questions that are clear, without any\nambiguities, fully described with plain text only, and have a unique, well\ndefined correct answer. The questions are divided into two sets of 15 each. The\nquestions of Set A are 15 \"Original\" problems that cannot be found online,\nwhile Set B contains 15 \"Published\" problems that one can find online, usually\nwith their solution. Each question is posed three times to each chatbot. The\nanswers are recorded and discussed, highlighting their strengths and\nweaknesses. It has been found that for straightforward arithmetic, algebraic\nexpressions, or basic logic puzzles, chatbots may provide accurate solutions,\nalthough not in every attempt. However, for more complex mathematical problems\nor advanced logic tasks, their answers, although written in a usually\n\"convincing\" way, may not be reliable. Consistency is also an issue, as many\ntimes a chatbot will provide conflicting answers when given the same question\nmore than once. A comparative quantitative evaluation of the three chatbots is\nmade through scoring their final answers based on correctness. It was found\nthat ChatGPT-4 outperforms ChatGPT-3.5 in both sets of questions. Bard comes\nthird in the original questions of Set A, behind the other two chatbots, while\nit has the best performance (first place) in the published questions of Set B.\nThis is probably because Bard has direct access to the internet, in contrast to\nChatGPT chatbots which do not have any communication with the outside world.", "published": "2023-05-30 11:18:05", "link": "http://arxiv.org/abs/2305.18618v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.0; I.2.7"], "primary_category": "cs.CL"}
{"title": "Likelihood-Based Diffusion Language Models", "abstract": "Despite a growing interest in diffusion-based language models, existing work\nhas not shown that these models can attain nontrivial likelihoods on standard\nlanguage modeling benchmarks. In this work, we take the first steps towards\nclosing the likelihood gap between autoregressive and diffusion-based language\nmodels, with the goal of building and releasing a diffusion model which\noutperforms a small but widely-known autoregressive model. We pursue this goal\nthrough algorithmic improvements, scaling laws, and increased compute. On the\nalgorithmic front, we introduce several methodological improvements for the\nmaximum-likelihood training of diffusion language models. We then study scaling\nlaws for our diffusion models and find compute-optimal training regimes which\ndiffer substantially from autoregressive models. Using our methods and scaling\nanalysis, we train and release Plaid 1B, a large diffusion language model which\noutperforms GPT-2 124M in likelihood on benchmark datasets and generates fluent\nsamples in unconditional and zero-shot control settings.", "published": "2023-05-30 16:43:31", "link": "http://arxiv.org/abs/2305.18619v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KEYword based Sampling (KEYS) for Large Language Models", "abstract": "Question answering (Q/A) can be formulated as a generative task (Mitra, 2017)\nwhere the task is to generate an answer given the question and the passage\n(knowledge, if available). Recent advances in QA task is focused a lot on\nlanguage model advancements and less on other areas such as sampling(Krishna et\nal., 2021), (Nakano et al., 2021). Keywords play very important role for humans\nin language generation. (Humans formulate keywords and use grammar to connect\nthose keywords and work). In the research community, very little focus is on\nhow humans generate answers to a question and how this behavior can be\nincorporated in a language model. In this paper, we want to explore these two\nareas combined, i.e., how sampling can be to used generate answers which are\nclose to human-like behavior and factually correct. Hence, the type of decoding\nalgorithm we think should be used for Q/A tasks should also depend on the\nkeywords. These keywords can be obtained from the question, passage or internet\nresults. We use knowledge distillation techniques to extract keywords and\nsample using these extracted keywords on top of vanilla decoding algorithms\nwhen formulating the answer to generate a human-like answer. In this paper, we\nshow that our decoding method outperforms most commonly used decoding methods\nfor Q/A task", "published": "2023-05-30 01:35:04", "link": "http://arxiv.org/abs/2305.18679v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Domain Specialization as the Key to Make Large Language Models\n  Disruptive: A Comprehensive Survey", "abstract": "Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing (NLP), providing a highly useful, task-agnostic foundation\nfor a wide range of applications. However, directly applying LLMs to solve\nsophisticated problems in specific domains meets many hurdles, caused by the\nheterogeneity of domain data, the sophistication of domain knowledge, the\nuniqueness of domain objectives, and the diversity of the constraints (e.g.,\nvarious social norms, cultural conformity, religious beliefs, and ethical\nstandards in the domain applications). Domain specification techniques are key\nto make large language models disruptive in many applications. Specifically, to\nsolve these hurdles, there has been a notable increase in research and\npractices conducted in recent years on the domain specialization of LLMs. This\nemerging field of study, with its substantial potential for impact,\nnecessitates a comprehensive and systematic review to better summarize and\nguide ongoing work in this area. In this article, we present a comprehensive\nsurvey on domain specification techniques for large language models, an\nemerging direction critical for large language model applications. First, we\npropose a systematic taxonomy that categorizes the LLM domain-specialization\ntechniques based on the accessibility to LLMs and summarizes the framework for\nall the subcategories as well as their relations and differences to each other.\nSecond, we present an extensive taxonomy of critical application domains that\ncan benefit dramatically from specialized LLMs, discussing their practical\nsignificance and open challenges. Last, we offer our insights into the current\nresearch status and future trends in this area.", "published": "2023-05-30 03:00:30", "link": "http://arxiv.org/abs/2305.18703v7", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AdapterEM: Pre-trained Language Model Adaptation for Generalized Entity\n  Matching using Adapter-tuning", "abstract": "Entity Matching (EM) involves identifying different data representations\nreferring to the same entity from multiple data sources and is typically\nformulated as a binary classification problem. It is a challenging problem in\ndata integration due to the heterogeneity of data representations.\nState-of-the-art solutions have adopted NLP techniques based on pre-trained\nlanguage models (PrLMs) via the fine-tuning paradigm, however, sequential\nfine-tuning of overparameterized PrLMs can lead to catastrophic forgetting,\nespecially in low-resource scenarios. In this study, we propose a\nparameter-efficient paradigm for fine-tuning PrLMs based on adapters, small\nneural networks encapsulated between layers of a PrLM, by optimizing only the\nadapter and classifier weights while the PrLMs parameters are frozen.\nAdapter-based methods have been successfully applied to multilingual speech\nproblems achieving promising results, however, the effectiveness of these\nmethods when applied to EM is not yet well understood, particularly for\ngeneralized EM with heterogeneous data. Furthermore, we explore using (i)\npre-trained adapters and (ii) invertible adapters to capture token-level\nlanguage representations and demonstrate their benefits for transfer learning\non the generalized EM benchmark. Our results show that our solution achieves\ncomparable or superior performance to full-scale PrLM fine-tuning and\nprompt-tuning baselines while utilizing a significantly smaller computational\nfootprint $\\approx 13\\%$ of the PrLM parameters.", "published": "2023-05-30 04:03:23", "link": "http://arxiv.org/abs/2305.18725v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "An Annotated Dataset for Explainable Interpersonal Risk Factors of\n  Mental Disturbance in Social Media Posts", "abstract": "With a surge in identifying suicidal risk and its severity in social media\nposts, we argue that a more consequential and explainable research is required\nfor optimal impact on clinical psychology practice and personalized mental\nhealthcare. The success of computational intelligence techniques for inferring\nmental illness from social media resources, points to natural language\nprocessing as a lens for determining Interpersonal Risk Factors (IRF) in human\nwritings. Motivated with limited availability of datasets for social NLP\nresearch community, we construct and release a new annotated dataset with\nhuman-labelled explanations and classification of IRF affecting mental\ndisturbance on social media: (i) Thwarted Belongingness (TBe), and (ii)\nPerceived Burdensomeness (PBu). We establish baseline models on our dataset\nfacilitating future research directions to develop real-time personalized AI\nmodels by detecting patterns of TBe and PBu in emotional spectrum of user's\nhistorical social media profile.", "published": "2023-05-30 04:08:40", "link": "http://arxiv.org/abs/2305.18727v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "LonXplain: Lonesomeness as a Consequence of Mental Disturbance in Reddit\n  Posts", "abstract": "Social media is a potential source of information that infers latent mental\nstates through Natural Language Processing (NLP). While narrating real-life\nexperiences, social media users convey their feeling of loneliness or isolated\nlifestyle, impacting their mental well-being. Existing literature on\npsychological theories points to loneliness as the major consequence of\ninterpersonal risk factors, propounding the need to investigate loneliness as a\nmajor aspect of mental disturbance. We formulate lonesomeness detection in\nsocial media posts as an explainable binary classification problem, discovering\nthe users at-risk, suggesting the need of resilience for early control. To the\nbest of our knowledge, there is no existing explainable dataset, i.e., one with\nhuman-readable, annotated text spans, to facilitate further research and\ndevelopment in loneliness detection causing mental disturbance. In this work,\nthree experts: a senior clinical psychologist, a rehabilitation counselor, and\na social NLP researcher define annotation schemes and perplexity guidelines to\nmark the presence or absence of lonesomeness, along with the marking of\ntext-spans in original posts as explanation, in 3,521 Reddit posts. We expect\nthe public release of our dataset, LonXplain, and traditional classifiers as\nbaselines via GitHub.", "published": "2023-05-30 04:21:24", "link": "http://arxiv.org/abs/2305.18736v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Adapting Multi-Lingual ASR Models for Handling Multiple Talkers", "abstract": "State-of-the-art large-scale universal speech models (USMs) show a decent\nautomatic speech recognition (ASR) performance across multiple domains and\nlanguages. However, it remains a challenge for these models to recognize\noverlapped speech, which is often seen in meeting conversations. We propose an\napproach to adapt USMs for multi-talker ASR. We first develop an enhanced\nversion of serialized output training to jointly perform multi-talker ASR and\nutterance timestamp prediction. That is, we predict the ASR hypotheses for all\nspeakers, count the speakers, and estimate the utterance timestamps at the same\ntime. We further introduce a lightweight adapter module to maintain the\nmultilingual property of the USMs even when we perform the adaptation with only\na single language. Experimental results obtained using the AMI and AliMeeting\ncorpora show that our proposed approach effectively transfers the USMs to a\nstrong multilingual multi-talker ASR model with timestamp prediction\ncapability.", "published": "2023-05-30 05:05:52", "link": "http://arxiv.org/abs/2305.18747v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "GPT4Tools: Teaching Large Language Model to Use Tools via\n  Self-instruction", "abstract": "This paper aims to efficiently enable Large Language Models (LLMs) to use\nmultimodal tools. Advanced proprietary LLMs, such as ChatGPT and GPT-4, have\nshown great potential for tool usage through sophisticated prompt engineering.\nNevertheless, these models typically rely on prohibitive computational costs\nand publicly inaccessible data. To address these challenges, we propose the\nGPT4Tools based on self-instruct to enable open-source LLMs, such as LLaMA and\nOPT, to use tools. It generates an instruction-following dataset by prompting\nan advanced teacher with various multi-modal contexts. By using the Low-Rank\nAdaptation (LoRA) optimization, our approach facilitates the open-source LLMs\nto solve a range of visual problems, including visual comprehension and image\ngeneration. Moreover, we provide a benchmark to evaluate the ability of LLMs to\nuse tools, which is performed in both zero-shot and fine-tuning ways. Extensive\nexperiments demonstrate the effectiveness of our method on various language\nmodels, which not only significantly improves the accuracy of invoking seen\ntools, but also enables the zero-shot capacity for unseen tools. The code and\ndemo are available at https://github.com/StevenGrove/GPT4Tools.", "published": "2023-05-30 05:27:21", "link": "http://arxiv.org/abs/2305.18752v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "VSTAR: A Video-grounded Dialogue Dataset for Situated Semantic\n  Understanding with Scene and Topic Transitions", "abstract": "Video-grounded dialogue understanding is a challenging problem that requires\nmachine to perceive, parse and reason over situated semantics extracted from\nweakly aligned video and dialogues. Most existing benchmarks treat both\nmodalities the same as a frame-independent visual understanding task, while\nneglecting the intrinsic attributes in multimodal dialogues, such as scene and\ntopic transitions. In this paper, we present Video-grounded Scene&Topic AwaRe\ndialogue (VSTAR) dataset, a large scale video-grounded dialogue understanding\ndataset based on 395 TV series. Based on VSTAR, we propose two benchmarks for\nvideo-grounded dialogue understanding: scene segmentation and topic\nsegmentation, and one benchmark for video-grounded dialogue generation.\nComprehensive experiments are performed on these benchmarks to demonstrate the\nimportance of multimodal information and segments in video-grounded dialogue\nunderstanding and generation.", "published": "2023-05-30 05:40:37", "link": "http://arxiv.org/abs/2305.18756v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Scalable Performance Analysis for Vision-Language Models", "abstract": "Joint vision-language models have shown great performance over a diverse set\nof tasks. However, little is known about their limitations, as the high\ndimensional space learned by these models makes it difficult to identify\nsemantic errors. Recent work has addressed this problem by designing highly\ncontrolled probing task benchmarks. Our paper introduces a more scalable\nsolution that relies on already annotated benchmarks. Our method consists of\nextracting a large set of diverse features from a vision-language benchmark and\nmeasuring their correlation with the output of the target model. We confirm\nprevious findings that CLIP behaves like a bag of words model and performs\nbetter with nouns and verbs; we also uncover novel insights such as CLIP\ngetting confused by concrete words. Our framework is available at\nhttps://github.com/MichiganNLP/Scalable-VLM-Probing and can be used with other\nmultimodal models and benchmarks.", "published": "2023-05-30 06:40:08", "link": "http://arxiv.org/abs/2305.18786v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Universality and Limitations of Prompt Tuning", "abstract": "Despite the demonstrated empirical efficacy of prompt tuning to adapt a\npretrained language model for a new task, the theoretical underpinnings of the\ndifference between \"tuning parameters before the input\" against \"the tuning of\nmodel weights\" are limited. We thus take one of the first steps to understand\nthe role of soft-prompt tuning for transformer-based architectures. By\nconsidering a general purpose architecture, we analyze prompt tuning from the\nlens of both: universal approximation and limitations with finite-depth\nfixed-weight pretrained transformers for continuous-valued functions. Our\nuniversality result guarantees the existence of a strong transformer with a\nprompt to approximate any sequence-to-sequence function in the set of Lipschitz\nfunctions. The limitations of prompt tuning for limited-depth transformers are\nfirst proved by constructing a set of datasets, that cannot be memorized by a\nprompt of any length for a given single encoder layer. We also provide a lower\nbound on the required number of tunable prompt parameters and compare the\nresult with the number of parameters required for a low-rank update (based on\nLoRA) for a single-layer setting. We finally extend our analysis to multi-layer\nsettings by providing sufficient conditions under which the transformer can at\nbest learn datasets from invertible functions only. Our theoretical claims are\nalso corroborated by empirical results.", "published": "2023-05-30 06:47:07", "link": "http://arxiv.org/abs/2305.18787v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "STT4SG-350: A Speech Corpus for All Swiss German Dialect Regions", "abstract": "We present STT4SG-350 (Speech-to-Text for Swiss German), a corpus of Swiss\nGerman speech, annotated with Standard German text at the sentence level. The\ndata is collected using a web app in which the speakers are shown Standard\nGerman sentences, which they translate to Swiss German and record. We make the\ncorpus publicly available. It contains 343 hours of speech from all dialect\nregions and is the largest public speech corpus for Swiss German to date.\nApplication areas include automatic speech recognition (ASR), text-to-speech,\ndialect identification, and speaker recognition. Dialect information, age\ngroup, and gender of the 316 speakers are provided. Genders are equally\nrepresented and the corpus includes speakers of all ages. Roughly the same\namount of speech is provided per dialect region, which makes the corpus ideally\nsuited for experiments with speech technology for different dialects. We\nprovide training, validation, and test splits of the data. The test set\nconsists of the same spoken sentences for each dialect region and allows a fair\nevaluation of the quality of speech technologies in different dialects. We\ntrain an ASR model on the training set and achieve an average BLEU score of\n74.7 on the test set. The model beats the best published BLEU scores on 2 other\nSwiss German ASR test sets, demonstrating the quality of the corpus.", "published": "2023-05-30 08:49:38", "link": "http://arxiv.org/abs/2305.18855v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multitask learning for recognizing stress and depression in social media", "abstract": "Stress and depression are prevalent nowadays across people of all ages due to\nthe quick paces of life. People use social media to express their feelings.\nThus, social media constitute a valuable form of information for the early\ndetection of stress and depression. Although many research works have been\nintroduced targeting the early recognition of stress and depression, there are\nstill limitations. There have been proposed multi-task learning settings, which\nuse depression and emotion (or figurative language) as the primary and\nauxiliary tasks respectively. However, although stress is inextricably linked\nwith depression, researchers face these two tasks as two separate tasks. To\naddress these limitations, we present the first study, which exploits two\ndifferent datasets collected under different conditions, and introduce two\nmultitask learning frameworks, which use depression and stress as the main and\nauxiliary tasks respectively. Specifically, we use a depression dataset and a\nstressful dataset including stressful posts from ten subreddits of five\ndomains. In terms of the first approach, each post passes through a shared BERT\nlayer, which is updated by both tasks. Next, two separate BERT encoder layers\nare exploited, which are updated by each task separately. Regarding the second\napproach, it consists of shared and task-specific layers weighted by attention\nfusion networks. We conduct a series of experiments and compare our approaches\nwith existing research initiatives, single-task learning, and transfer\nlearning. Experiments show multiple advantages of our approaches over\nstate-of-the-art ones.", "published": "2023-05-30 10:04:01", "link": "http://arxiv.org/abs/2305.18907v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Event-Centric Query Expansion in Web Search", "abstract": "In search engines, query expansion (QE) is a crucial technique to improve\nsearch experience. Previous studies often rely on long-term search log mining,\nwhich leads to slow updates and is sub-optimal for time-sensitive news\nsearches. In this work, we present Event-Centric Query Expansion (EQE), a novel\nQE system that addresses these issues by mining the best expansion from a\nsignificant amount of potential events rapidly and accurately. This system\nconsists of four stages, i.e., event collection, event reformulation, semantic\nretrieval and online ranking. Specifically, we first collect and filter news\nheadlines from websites. Then we propose a generation model that incorporates\ncontrastive learning and prompt-tuning techniques to reformulate these\nheadlines to concise candidates. Additionally, we fine-tune a dual-tower\nsemantic model to function as an encoder for event retrieval and explore a\ntwo-stage contrastive training approach to enhance the accuracy of event\nretrieval. Finally, we rank the retrieved events and select the optimal one as\nQE, which is then used to improve the retrieval of event-related documents.\nThrough offline analysis and online A/B testing, we observe that the EQE system\nsignificantly improves many metrics compared to the baseline. The system has\nbeen deployed in Tencent QQ Browser Search and served hundreds of millions of\nusers. The dataset and baseline codes are available at\nhttps://open-event-hub.github.io/eqe .", "published": "2023-05-30 13:19:53", "link": "http://arxiv.org/abs/2305.19019v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Complex Query Answering on Eventuality Knowledge Graph with Implicit\n  Logical Constraints", "abstract": "Querying knowledge graphs (KGs) using deep learning approaches can naturally\nleverage the reasoning and generalization ability to learn to infer better\nanswers. Traditional neural complex query answering (CQA) approaches mostly\nwork on entity-centric KGs. However, in the real world, we also need to make\nlogical inferences about events, states, and activities (i.e., eventualities or\nsituations) to push learning systems from System I to System II, as proposed by\nYoshua Bengio. Querying logically from an EVentuality-centric KG (EVKG) can\nnaturally provide references to such kind of intuitive and logical inference.\nThus, in this paper, we propose a new framework to leverage neural methods to\nanswer complex logical queries based on an EVKG, which can satisfy not only\ntraditional first-order logic constraints but also implicit logical constraints\nover eventualities concerning their occurrences and orders. For instance, if we\nknow that \"Food is bad\" happens before \"PersonX adds soy sauce\", then \"PersonX\nadds soy sauce\" is unlikely to be the cause of \"Food is bad\" due to implicit\ntemporal constraint. To facilitate consistent reasoning on EVKGs, we propose\nComplex Eventuality Query Answering (CEQA), a more rigorous definition of CQA\nthat considers the implicit logical constraints governing the temporal order\nand occurrence of eventualities. In this manner, we propose to leverage theorem\nprovers for constructing benchmark datasets to ensure the answers satisfy\nimplicit logical constraints. We also propose a Memory-Enhanced Query Encoding\n(MEQE) approach to significantly improve the performance of state-of-the-art\nneural query encoders on the CEQA task.", "published": "2023-05-30 14:29:24", "link": "http://arxiv.org/abs/2305.19068v2", "categories": ["cs.CL", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Comparing and combining some popular NER approaches on Biomedical tasks", "abstract": "We compare three simple and popular approaches for NER: 1) SEQ\n(sequence-labeling with a linear token classifier) 2) SeqCRF (sequence-labeling\nwith Conditional Random Fields), and 3) SpanPred (span-prediction with boundary\ntoken embeddings). We compare the approaches on 4 biomedical NER tasks: GENIA,\nNCBI-Disease, LivingNER (Spanish), and SocialDisNER (Spanish). The SpanPred\nmodel demonstrates state-of-the-art performance on LivingNER and SocialDisNER,\nimproving F1 by 1.3 and 0.6 F1 respectively. The SeqCRF model also demonstrates\nstate-of-the-art performance on LivingNER and SocialDisNER, improving F1 by 0.2\nF1 and 0.7 respectively. The SEQ model is competitive with the state-of-the-art\non the LivingNER dataset. We explore some simple ways of combining the three\napproaches. We find that majority voting consistently gives high precision and\nhigh F1 across all 4 datasets. Lastly, we implement a system that learns to\ncombine the predictions of SEQ and SpanPred, generating systems that\nconsistently give high recall and high F1 across all 4 datasets. On the GENIA\ndataset, we find that our learned combiner system significantly boosts F1(+1.2)\nand recall(+2.1) over the systems being combined. We release all the\nwell-documented code necessary to reproduce all systems at\nhttps://github.com/flyingmothman/bionlp.", "published": "2023-05-30 15:29:30", "link": "http://arxiv.org/abs/2305.19120v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Controlled Text Generation with Hidden Representation Transformations", "abstract": "We propose CHRT (Control Hidden Representation Transformation) - a controlled\nlanguage generation framework that steers large language models to generate\ntext pertaining to certain attributes (such as toxicity). CHRT gains attribute\ncontrol by modifying the hidden representation of the base model through\nlearned transformations. We employ a contrastive-learning framework to learn\nthese transformations that can be combined to gain multi-attribute control. The\neffectiveness of CHRT is experimentally shown by comparing it with seven\nbaselines over three attributes. CHRT outperforms all the baselines in the task\nof detoxification, positive sentiment steering, and text simplification while\nminimizing the loss in linguistic qualities. Further, our approach has the\nlowest inference latency of only 0.01 seconds more than the base model, making\nit the most suitable for high-performance production environments. We\nopen-source our code and release two novel datasets to further propel\ncontrolled language generation research.", "published": "2023-05-30 17:21:17", "link": "http://arxiv.org/abs/2305.19230v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Grammar Prompting for Domain-Specific Language Generation with Large\n  Language Models", "abstract": "Large language models (LLMs) can learn to perform a wide range of natural\nlanguage tasks from just a handful of in-context examples. However, for\ngenerating strings from highly structured languages (e.g., semantic parsing to\ncomplex domain-specific languages), it is challenging for the LLM to generalize\nfrom just a few exemplars. We propose \\emph{grammar prompting}, a simple\napproach to enable LLMs to use external knowledge and domain-specific\nconstraints, expressed through a grammar in Backus--Naur Form (BNF), during\nin-context learning. Grammar prompting augments each demonstration example with\na specialized grammar that is minimally sufficient for generating the\nparticular output example, where the specialized grammar is a subset of the\nfull DSL grammar. For inference, the LLM first predicts a BNF grammar given a\ntest input, and then generates the output according to the rules of the\ngrammar. Experiments demonstrate that grammar prompting can enable LLMs to\nperform competitively on a diverse set of DSL generation tasks, including\nsemantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and\nSMILES-based molecule generation.", "published": "2023-05-30 17:26:01", "link": "http://arxiv.org/abs/2305.19234v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Preserving Pre-trained Features Helps Calibrate Fine-tuned Language\n  Models", "abstract": "Large pre-trained language models (PLMs) have demonstrated strong performance\non natural language understanding (NLU) tasks through fine-tuning. However,\nfine-tuned models still suffer from overconfident predictions, especially in\nout-of-domain settings. In this paper, we tackle the problem of calibrating\nfine-tuned language models. We demonstrate that the PLMs are well-calibrated on\nthe masked language modeling task with robust predictive confidence under\ndomain shift, yet the fine-tuned models fail to retain such property due to\ncatastrophic forgetting, which impacts the calibration on the downstream\nclassification task. In light of these observations, we evaluate the\ncalibration of several methods that preserve pre-trained features and show that\npreserving pre-trained features can improve the calibration of fine-tuned\nlanguage models. Among these methods, our proposed method that encourages the\nfine-tuned model to learn generative representations with auxiliary language\nmodeling objective achieves competitive accuracy and the lowest expected\ncalibration error compared to several strong baselines under both in-domain and\nout-of-domain settings on three downstream NLU tasks.", "published": "2023-05-30 17:35:31", "link": "http://arxiv.org/abs/2305.19249v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Jointly Reparametrized Multi-Layer Adaptation for Efficient and Private\n  Tuning", "abstract": "Efficient finetuning of pretrained language transformers is becoming\nincreasingly prevalent for solving natural language processing tasks. While\neffective, it can still require a large number of tunable parameters. This can\nbe a drawback for low-resource applications and training with\ndifferential-privacy constraints, where excessive noise may be introduced\nduring finetuning. To this end, we propose a novel language transformer\nfinetuning strategy that introduces task-specific parameters in multiple\ntransformer layers. These parameters are derived from fixed random projections\nof a single trainable vector, enabling finetuning with significantly fewer\nparameters while maintaining performance. We achieve within 5% of full\nfinetuning performance on GLUE tasks with as few as 4,100 parameters per task,\noutperforming other parameter-efficient finetuning approaches that use a\nsimilar number of per-task parameters. Besides, the random projections can be\nprecomputed at inference, avoiding additional computational latency. All these\nmake our method particularly appealing for low-resource applications. Finally,\nour method achieves the best or comparable utility compared to several recent\nfinetuning methods when training with the same privacy constraints,\nunderscoring its effectiveness and potential real-world impact.", "published": "2023-05-30 17:55:06", "link": "http://arxiv.org/abs/2305.19264v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Less Likely Brainstorming: Using Language Models to Generate Alternative\n  Hypotheses", "abstract": "A human decision-maker benefits the most from an AI assistant that corrects\nfor their biases. For problems such as generating interpretation of a radiology\nreport given findings, a system predicting only highly likely outcomes may be\nless useful, where such outcomes are already obvious to the user. To alleviate\nbiases in human decision-making, it is worth considering a broad differential\ndiagnosis, going beyond the most likely options. We introduce a new task, \"less\nlikely brainstorming,\" that asks a model to generate outputs that humans think\nare relevant but less likely to happen. We explore the task in two settings: a\nbrain MRI interpretation generation setting and an everyday commonsense\nreasoning setting. We found that a baseline approach of training with less\nlikely hypotheses as targets generates outputs that humans evaluate as either\nlikely or irrelevant nearly half of the time; standard MLE training is not\neffective. To tackle this problem, we propose a controlled text generation\nmethod that uses a novel contrastive learning strategy to encourage models to\ndifferentiate between generating likely and less likely outputs according to\nhumans. We compare our method with several state-of-the-art controlled text\ngeneration models via automatic and human evaluations and show that our models'\ncapability of generating less likely outputs is improved.", "published": "2023-05-30 18:05:34", "link": "http://arxiv.org/abs/2305.19339v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Blockwise Parallel Transformer for Large Context Models", "abstract": "Transformers have emerged as the cornerstone of state-of-the-art natural\nlanguage processing models, showcasing exceptional performance across a wide\nrange of AI applications. However, the memory demands posed by the\nself-attention mechanism and the large feedforward network in Transformers\nlimit their ability to handle long sequences, thereby creating challenges for\ntasks involving multiple long sequences or long-term dependencies. We present a\ndistinct approach, Blockwise Parallel Transformer (BPT), that leverages\nblockwise computation of self-attention and feedforward network fusion to\nminimize memory costs. By processing longer input sequences while maintaining\nmemory efficiency, BPT enables training sequences 32 times longer than vanilla\nTransformers and up to 4 times longer than previous memory-efficient methods.\nExtensive experiments on language modeling and reinforcement learning tasks\ndemonstrate the effectiveness of BPT in reducing memory requirements and\nimproving performance.", "published": "2023-05-30 19:25:51", "link": "http://arxiv.org/abs/2305.19370v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Quantum Natural Language Processing based Sentiment Analysis using\n  lambeq Toolkit", "abstract": "Sentiment classification is one the best use case of classical natural\nlanguage processing (NLP) where we can witness its power in various daily life\ndomains such as banking, business and marketing industry. We already know how\nclassical AI and machine learning can change and improve technology. Quantum\nnatural language processing (QNLP) is a young and gradually emerging technology\nwhich has the potential to provide quantum advantage for NLP tasks. In this\npaper we show the first application of QNLP for sentiment analysis and achieve\nperfect test set accuracy for three different kinds of simulations and a decent\naccuracy for experiments ran on a noisy quantum device. We utilize the lambeq\nQNLP toolkit and $t|ket>$ by Cambridge Quantum (Quantinuum) to bring out the\nresults.", "published": "2023-05-30 19:54:02", "link": "http://arxiv.org/abs/2305.19383v1", "categories": ["quant-ph", "cs.CL"], "primary_category": "quant-ph"}
{"title": "DyGen: Learning from Noisy Labels via Dynamics-Enhanced Generative\n  Modeling", "abstract": "Learning from noisy labels is a challenge that arises in many real-world\napplications where training data can contain incorrect or corrupted labels.\nWhen fine-tuning language models with noisy labels, models can easily overfit\nthe label noise, leading to decreased performance. Most existing methods for\nlearning from noisy labels use static input features for denoising, but these\nmethods are limited by the information they can provide on true label\ndistributions and can result in biased or incorrect predictions. In this work,\nwe propose the Dynamics-Enhanced Generative Model (DyGen), which uses dynamic\npatterns in the embedding space during the fine-tuning process of language\nmodels to improve noisy label predictions. DyGen uses the variational\nauto-encoding framework to infer the posterior distributions of true labels\nfrom noisy labels and training dynamics. Additionally, a co-regularization\nmechanism is used to minimize the impact of potentially noisy labels and\npriors. DyGen demonstrates an average accuracy improvement of 3.10% on two\nsynthetic noise datasets and 1.48% on three real-world noise datasets compared\nto the previous state-of-the-art. Extensive experiments and analyses show the\neffectiveness of each component in DyGen. Our code is available for\nreproducibility on GitHub.", "published": "2023-05-30 20:19:41", "link": "http://arxiv.org/abs/2305.19395v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Resource-Efficient Fine-Tuning Strategies for Automatic MOS Prediction\n  in Text-to-Speech for Low-Resource Languages", "abstract": "We train a MOS prediction model based on wav2vec 2.0 using the open-access\ndata sets BVCC and SOMOS. Our test with neural TTS data in the low-resource\nlanguage (LRL) West Frisian shows that pre-training on BVCC before fine-tuning\non SOMOS leads to the best accuracy for both fine-tuned and zero-shot\nprediction. Further fine-tuning experiments show that using more than 30\npercent of the total data does not lead to significant improvements. In\naddition, fine-tuning with data from a single listener shows promising\nsystem-level accuracy, supporting the viability of one-participant pilot tests.\nThese findings can all assist the resource-conscious development of TTS for\nLRLs by progressing towards better zero-shot MOS prediction and informing the\ndesign of listening tests, especially in early-stage evaluation.", "published": "2023-05-30 20:19:56", "link": "http://arxiv.org/abs/2305.19396v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "ScoNe: Benchmarking Negation Reasoning in Language Models With\n  Fine-Tuning and In-Context Learning", "abstract": "A number of recent benchmarks seek to assess how well models handle natural\nlanguage negation. However, these benchmarks lack the controlled example\nparadigms that would allow us to infer whether a model had learned how negation\nmorphemes semantically scope. To fill these analytical gaps, we present the\nScoped Negation NLI (ScoNe-NLI) benchmark, which contains contrast sets of six\nexamples with up to two negations where either zero, one, or both negative\nmorphemes affect the NLI label. We use ScoNe-NLI to assess fine-tuning and\nin-context learning strategies. We find that RoBERTa and DeBERTa models solve\nScoNe-NLI after many shot fine-tuning. For in-context learning, we test\nInstructGPT models and find that most prompt strategies are not successful,\nincluding those using step-by-step reasoning. To better understand this result,\nwe extend ScoNe with ScoNe-NLG, a sentence completion test set that embeds\nnegation reasoning in short narratives. Here, InstructGPT is successful, which\nreveals the model can correctly reason about negation, but struggles to do so\non prompt-adapted NLI examples outside of its core pretraining regime.", "published": "2023-05-30 21:43:11", "link": "http://arxiv.org/abs/2305.19426v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Machine Learning Approach for Cancer Entities Association and\n  Classification", "abstract": "According to the World Health Organization (WHO), cancer is the second\nleading cause of death globally. Scientific research on different types of\ncancers grows at an ever-increasing rate, publishing large volumes of research\narticles every year. The insight information and the knowledge of the drug,\ndiagnostics, risk, symptoms, treatments, etc., related to genes are significant\nfactors that help explore and advance the cancer research progression. Manual\nscreening of such a large volume of articles is very laborious and\ntime-consuming to formulate any hypothesis. The study uses the two most\nnon-trivial NLP, Natural Language Processing functions, Entity Recognition, and\ntext classification to discover knowledge from biomedical literature. Named\nEntity Recognition (NER) recognizes and extracts the predefined entities\nrelated to cancer from unstructured text with the support of a user-friendly\ninterface and built-in dictionaries. Text classification helps to explore the\ninsights into the text and simplifies data categorization, querying, and\narticle screening. Machine learning classifiers are also used to build the\nclassification model and Structured Query Languages (SQL) is used to identify\nthe hidden relations that may lead to significant predictions.", "published": "2023-05-30 07:36:12", "link": "http://arxiv.org/abs/2306.00013v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PreQuant: A Task-agnostic Quantization Approach for Pre-trained Language\n  Models", "abstract": "While transformer-based pre-trained language models (PLMs) have dominated a\nnumber of NLP applications, these models are heavy to deploy and expensive to\nuse. Therefore, effectively compressing large-scale PLMs becomes an\nincreasingly important problem. Quantization, which represents high-precision\ntensors with low-bit fix-point format, is a viable solution. However, most\nexisting quantization methods are task-specific, requiring customized training\nand quantization with a large number of trainable parameters on each individual\ntask. Inspired by the observation that the over-parameterization nature of PLMs\nmakes it possible to freeze most of the parameters during the fine-tuning\nstage, in this work, we propose a novel ``quantize before fine-tuning''\nframework, PreQuant, that differs from both quantization-aware training and\npost-training quantization. PreQuant is compatible with various quantization\nstrategies, with outlier-aware parameter-efficient fine-tuning incorporated to\ncorrect the induced quantization error. We demonstrate the effectiveness of\nPreQuant on the GLUE benchmark using BERT, RoBERTa, and T5. We also provide an\nempirical investigation into the workflow of PreQuant, which sheds light on its\nefficacy.", "published": "2023-05-30 08:41:33", "link": "http://arxiv.org/abs/2306.00014v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse\n  Engineering of Language at Scale", "abstract": "Large language models (LLMs) have achieved a milestone that undenia-bly\nchanged many held beliefs in artificial intelligence (AI). However, there\nremains many limitations of these LLMs when it comes to true language\nunderstanding, limitations that are a byproduct of the under-lying architecture\nof deep neural networks. Moreover, and due to their subsymbolic nature,\nwhatever knowledge these models acquire about how language works will always be\nburied in billions of microfeatures (weights), none of which is meaningful on\nits own, making such models hopelessly unexplainable. To address these\nlimitations, we suggest com-bining the strength of symbolic representations\nwith what we believe to be the key to the success of LLMs, namely a successful\nbottom-up re-verse engineering of language at scale. As such we argue for a\nbottom-up reverse engineering of language in a symbolic setting. Hints on what\nthis project amounts to have been suggested by several authors, and we discuss\nin some detail here how this project could be accomplished.", "published": "2023-05-30 15:15:40", "link": "http://arxiv.org/abs/2306.00017v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-Verification Improves Few-Shot Clinical Information Extraction", "abstract": "Extracting patient information from unstructured text is a critical task in\nhealth decision-support and clinical research. Large language models (LLMs)\nhave shown the potential to accelerate clinical curation via few-shot\nin-context learning, in contrast to supervised learning which requires much\nmore costly human annotations. However, despite drastic advances in modern LLMs\nsuch as GPT-4, they still struggle with issues regarding accuracy and\ninterpretability, especially in mission-critical domains such as health. Here,\nwe explore a general mitigation framework using self-verification, which\nleverages the LLM to provide provenance for its own extraction and check its\nown outputs. This is made possible by the asymmetry between verification and\ngeneration, where the latter is often much easier than the former. Experimental\nresults show that our method consistently improves accuracy for various LLMs in\nstandard clinical information extraction tasks. Additionally, self-verification\nyields interpretations in the form of a short text span corresponding to each\noutput, which makes it very efficient for human experts to audit the results,\npaving the way towards trustworthy extraction of clinical information in\nresource-constrained scenarios. To facilitate future research in this\ndirection, we release our code and prompts.", "published": "2023-05-30 22:05:11", "link": "http://arxiv.org/abs/2306.00024v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Conceptual Design Generation Using Large Language Models", "abstract": "Concept generation is a creative step in the conceptual design phase, where\ndesigners often turn to brainstorming, mindmapping, or crowdsourcing design\nideas to complement their own knowledge of the domain. Recent advances in\nnatural language processing (NLP) and machine learning (ML) have led to the\nrise of Large Language Models (LLMs) capable of generating seemingly creative\noutputs from textual prompts. The success of these models has led to their\nintegration and application across a variety of domains, including art,\nentertainment, and other creative work. In this paper, we leverage LLMs to\ngenerate solutions for a set of 12 design problems and compare them to a\nbaseline of crowdsourced solutions. We evaluate the differences between\ngenerated and crowdsourced design solutions through multiple perspectives,\nincluding human expert evaluations and computational metrics. Expert\nevaluations indicate that the LLM-generated solutions have higher average\nfeasibility and usefulness while the crowdsourced solutions have more novelty.\nWe experiment with prompt engineering and find that leveraging few-shot\nlearning can lead to the generation of solutions that are more similar to the\ncrowdsourced solutions. These findings provide insight into the quality of\ndesign solutions generated with LLMs and begins to evaluate prompt engineering\ntechniques that could be leveraged by practitioners to generate higher-quality\ndesign solutions synergistically with LLMs.", "published": "2023-05-30 19:32:39", "link": "http://arxiv.org/abs/2306.01779v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "History Repeats: Overcoming Catastrophic Forgetting For Event-Centric\n  Temporal Knowledge Graph Completion", "abstract": "Temporal knowledge graph (TKG) completion models typically rely on having\naccess to the entire graph during training. However, in real-world scenarios,\nTKG data is often received incrementally as events unfold, leading to a dynamic\nnon-stationary data distribution over time. While one could incorporate\nfine-tuning to existing methods to allow them to adapt to evolving TKG data,\nthis can lead to forgetting previously learned patterns. Alternatively,\nretraining the model with the entire updated TKG can mitigate forgetting but is\ncomputationally burdensome. To address these challenges, we propose a general\ncontinual training framework that is applicable to any TKG completion method,\nand leverages two key ideas: (i) a temporal regularization that encourages\nrepurposing of less important model parameters for learning new knowledge, and\n(ii) a clustering-based experience replay that reinforces the past knowledge by\nselectively preserving only a small portion of the past data. Our experimental\nresults on widely used event-centric TKG datasets demonstrate the effectiveness\nof our proposed continual training framework in adapting to new events while\nreducing catastrophic forgetting. Further, we perform ablation studies to show\nthe effectiveness of each component of our proposed framework. Finally, we\ninvestigate the relation between the memory dedicated to experience replay and\nthe benefit gained from our clustering-based sampling strategy.", "published": "2023-05-30 01:21:36", "link": "http://arxiv.org/abs/2305.18675v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6; I.2.7"], "primary_category": "cs.LG"}
{"title": "Graph Neural Networks for Contextual ASR with the Tree-Constrained\n  Pointer Generator", "abstract": "The incorporation of biasing words obtained through contextual knowledge is\nof paramount importance in automatic speech recognition (ASR) applications.\nThis paper proposes an innovative method for achieving end-to-end contextual\nASR using graph neural network (GNN) encodings based on the tree-constrained\npointer generator method. GNN node encodings facilitate lookahead for future\nword pieces in the process of ASR decoding at each tree node by incorporating\ninformation about all word pieces on the tree branches rooted from it. This\nresults in a more precise prediction of the generation probability of the\nbiasing words. The study explores three GNN encoding techniques, namely tree\nrecursive neural networks, graph convolutional network (GCN), and GraphSAGE,\nalong with different combinations of the complementary GCN and GraphSAGE\nstructures. The performance of the systems was evaluated using the Librispeech\nand AMI corpus, following the visual-grounded contextual ASR pipeline. The\nfindings indicate that using GNN encodings achieved consistent and significant\nreductions in word error rate (WER), particularly for words that are rare or\nhave not been seen during the training process. Notably, the most effective\ncombination of GNN encodings obtained more than 60% WER reduction for rare and\nunseen words compared to standard end-to-end systems.", "published": "2023-05-30 08:20:58", "link": "http://arxiv.org/abs/2305.18824v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Generate then Select: Open-ended Visual Question Answering Guided by\n  World Knowledge", "abstract": "The open-ended Visual Question Answering (VQA) task requires AI models to\njointly reason over visual and natural language inputs using world knowledge.\nRecently, pre-trained Language Models (PLM) such as GPT-3 have been applied to\nthe task and shown to be powerful world knowledge sources. However, these\nmethods suffer from low knowledge coverage caused by PLM bias -- the tendency\nto generate certain tokens over other tokens regardless of prompt changes, and\nhigh dependency on the PLM quality -- only models using GPT-3 can achieve the\nbest result.\n  To address the aforementioned challenges, we propose RASO: a new VQA pipeline\nthat deploys a generate-then-select strategy guided by world knowledge for the\nfirst time. Rather than following the de facto standard to train a multi-modal\nmodel that directly generates the VQA answer, RASO first adopts PLM to generate\nall the possible answers, and then trains a lightweight answer selection model\nfor the correct answer. As proved in our analysis, RASO expands the knowledge\ncoverage from in-domain training data by a large margin. We provide extensive\nexperimentation and show the effectiveness of our pipeline by advancing the\nstate-of-the-art by 4.1% on OK-VQA, without additional computation cost. Code\nand models are released at http://cogcomp.org/page/publication_view/1010", "published": "2023-05-30 08:34:13", "link": "http://arxiv.org/abs/2305.18842v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph-Augmented Language Models for Knowledge-Grounded\n  Dialogue Generation", "abstract": "Language models have achieved impressive performances on dialogue generation\ntasks. However, when generating responses for a conversation that requires\nfactual knowledge, they are far from perfect, due to an absence of mechanisms\nto retrieve, encode, and reflect the knowledge in the generated responses. Some\nknowledge-grounded dialogue generation methods tackle this problem by\nleveraging facts from Knowledge Graphs (KGs); however, they do not guarantee\nthat the model utilizes a relevant piece of knowledge from the KG. To overcome\nthis limitation, we propose SUbgraph Retrieval-augmented GEneration (SURGE), a\nframework for generating context-relevant and knowledge-grounded dialogues with\nthe KG. Specifically, our SURGE framework first retrieves the relevant subgraph\nfrom the KG, and then enforces consistency across facts by perturbing their\nword embeddings conditioned by the retrieved subgraph. Then, we utilize\ncontrastive learning to ensure that the generated texts have high similarity to\nthe retrieved subgraphs. We validate our SURGE framework on OpendialKG and\nKOMODIS datasets, showing that it generates high-quality dialogues that\nfaithfully reflect the knowledge from KG.", "published": "2023-05-30 08:36:45", "link": "http://arxiv.org/abs/2305.18846v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dissecting Chain-of-Thought: Compositionality through In-Context\n  Filtering and Learning", "abstract": "Chain-of-thought (CoT) is a method that enables language models to handle\ncomplex reasoning tasks by decomposing them into simpler steps. Despite its\nsuccess, the underlying mechanics of CoT are not yet fully understood. In an\nattempt to shed light on this, our study investigates the impact of CoT on the\nability of transformers to in-context learn a simple to study, yet general\nfamily of compositional functions: multi-layer perceptrons (MLPs). In this\nsetting, we find that the success of CoT can be attributed to breaking down\nin-context learning of a compositional function into two distinct phases:\nfocusing on and filtering data related to each step of the composition and\nin-context learning the single-step composition function. Through both\nexperimental and theoretical evidence, we demonstrate how CoT significantly\nreduces the sample complexity of in-context learning (ICL) and facilitates the\nlearning of complex functions that non-CoT methods struggle with. Furthermore,\nwe illustrate how transformers can transition from vanilla in-context learning\nto mastering a compositional function with CoT by simply incorporating\nadditional layers that perform the necessary data-filtering for CoT via the\nattention mechanism. In addition to these test-time benefits, we show CoT helps\naccelerate pretraining by learning shortcuts to represent complex functions and\nfiltering plays an important role in this process. These findings collectively\nprovide insights into the mechanics of CoT, inviting further investigation of\nits role in complex reasoning tasks.", "published": "2023-05-30 09:02:00", "link": "http://arxiv.org/abs/2305.18869v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Investigating model performance in language identification: beyond\n  simple error statistics", "abstract": "Language development experts need tools that can automatically identify\nlanguages from fluent, conversational speech, and provide reliable estimates of\nusage rates at the level of an individual recording. However, language\nidentification systems are typically evaluated on metrics such as equal error\nrate and balanced accuracy, applied at the level of an entire speech corpus.\nThese overview metrics do not provide information about model performance at\nthe level of individual speakers, recordings, or units of speech with different\nlinguistic characteristics. Overview statistics may therefore mask systematic\nerrors in model performance for some subsets of the data, and consequently,\nhave worse performance on data derived from some subsets of human speakers,\ncreating a kind of algorithmic bias. In the current paper, we investigate how\nwell a number of language identification systems perform on individual\nrecordings and speech units with different linguistic properties in the MERLIon\nCCS Challenge. The Challenge dataset features accented English-Mandarin\ncode-switched child-directed speech.", "published": "2023-05-30 10:32:53", "link": "http://arxiv.org/abs/2305.18925v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Voice Conversion With Just Nearest Neighbors", "abstract": "Any-to-any voice conversion aims to transform source speech into a target\nvoice with just a few examples of the target speaker as a reference. Recent\nmethods produce convincing conversions, but at the cost of increased complexity\n-- making results difficult to reproduce and build on. Instead, we keep it\nsimple. We propose k-nearest neighbors voice conversion (kNN-VC): a\nstraightforward yet effective method for any-to-any conversion. First, we\nextract self-supervised representations of the source and reference speech. To\nconvert to the target speaker, we replace each frame of the source\nrepresentation with its nearest neighbor in the reference. Finally, a\npretrained vocoder synthesizes audio from the converted representation.\nObjective and subjective evaluations show that kNN-VC improves speaker\nsimilarity with similar intelligibility scores to existing methods. Code,\nsamples, trained models: https://bshall.github.io/knn-vc", "published": "2023-05-30 12:19:07", "link": "http://arxiv.org/abs/2305.18975v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "GPT Models in Construction Industry: Opportunities, Limitations, and a\n  Use Case Validation", "abstract": "Large Language Models(LLMs) trained on large data sets came into prominence\nin 2018 after Google introduced BERT. Subsequently, different LLMs such as GPT\nmodels from OpenAI have been released. These models perform well on diverse\ntasks and have been gaining widespread applications in fields such as business\nand education. However, little is known about the opportunities and challenges\nof using LLMs in the construction industry. Thus, this study aims to assess GPT\nmodels in the construction industry. A critical review, expert discussion and\ncase study validation are employed to achieve the study objectives. The\nfindings revealed opportunities for GPT models throughout the project\nlifecycle. The challenges of leveraging GPT models are highlighted and a use\ncase prototype is developed for materials selection and optimization. The\nfindings of the study would be of benefit to researchers, practitioners and\nstakeholders, as it presents research vistas for LLMs in the construction\nindustry.", "published": "2023-05-30 12:50:51", "link": "http://arxiv.org/abs/2305.18997v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "MiniSUPERB: Lightweight Benchmark for Self-supervised Speech Models", "abstract": "SUPERB was proposed to evaluate the generalizability of self-supervised\nlearning (SSL) speech models across various tasks. However, it incurs high\ncomputational costs due to the large datasets and diverse tasks. In this paper,\nwe introduce MiniSUPERB, a lightweight benchmark that efficiently evaluates SSL\nspeech models with comparable results to SUPERB but lower computational costs\nsignificantly. We carefully select representative tasks, sample datasets, and\nextract model representations offline. Our approach achieves a Spearman's rank\ncorrelation of 0.954 and 0.982 with SUPERB Paper and SUPERB Challenge,\nrespectively. Additionally, we reduce the computational cost by 97% in terms of\nMultiply-ACcumulate operations (MACs). Furthermore, we evaluate SSL speech\nmodels in few-shot scenarios and observe significant variations in their\nperformance. To our knowledge, this is the first study to examine both the\ncomputational cost of the model itself and the cost of evaluating it on a\nbenchmark.", "published": "2023-05-30 13:07:33", "link": "http://arxiv.org/abs/2305.19011v3", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "LANCE: Stress-testing Visual Models by Generating Language-guided\n  Counterfactual Images", "abstract": "We propose an automated algorithm to stress-test a trained visual model by\ngenerating language-guided counterfactual test images (LANCE). Our method\nleverages recent progress in large language modeling and text-based image\nediting to augment an IID test set with a suite of diverse, realistic, and\nchallenging test images without altering model weights. We benchmark the\nperformance of a diverse set of pre-trained models on our generated data and\nobserve significant and consistent performance drops. We further analyze model\nsensitivity across different types of edits, and demonstrate its applicability\nat surfacing previously unknown class-level model biases in ImageNet. Code is\navailable at https://github.com/virajprabhu/lance.", "published": "2023-05-30 16:09:16", "link": "http://arxiv.org/abs/2305.19164v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Strategic Reasoning with Language Models", "abstract": "Strategic reasoning enables agents to cooperate, communicate, and compete\nwith other agents in diverse situations. Existing approaches to solving\nstrategic games rely on extensive training, yielding strategies that do not\ngeneralize to new scenarios or games without retraining. Large Language Models\n(LLMs), with their ability to comprehend and generate complex, context-rich\nlanguage, could prove powerful as tools for strategic gameplay. This paper\nintroduces an approach that uses pretrained LLMs with few-shot chain-of-thought\nexamples to enable strategic reasoning for AI agents. Our approach uses\nsystematically generated demonstrations of reasoning about states, values, and\nbeliefs to prompt the model. Using extensive variations of simple matrix games,\nwe show that strategies that are derived based on systematically generated\nprompts generalize almost perfectly to new game structures, alternate\nobjectives, and hidden information. Additionally, we demonstrate our approach\ncan lead to human-like negotiation strategies in realistic scenarios without\nany extra training or fine-tuning. Our results highlight the ability of LLMs,\nguided by systematic reasoning demonstrations, to adapt and excel in diverse\nstrategic scenarios.", "published": "2023-05-30 16:09:19", "link": "http://arxiv.org/abs/2305.19165v1", "categories": ["cs.AI", "cs.CL", "cs.GT", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Generating with Confidence: Uncertainty Quantification for Black-box\n  Large Language Models", "abstract": "Large language models (LLMs) specializing in natural language generation\n(NLG) have recently started exhibiting promising capabilities across a variety\nof domains. However, gauging the trustworthiness of responses generated by LLMs\nremains an open challenge, with limited research on uncertainty quantification\n(UQ) for NLG. Furthermore, existing literature typically assumes white-box\naccess to language models, which is becoming unrealistic either due to the\nclosed-source nature of the latest LLMs or computational constraints. In this\nwork, we investigate UQ in NLG for *black-box* LLMs. We first differentiate\n*uncertainty* vs *confidence*: the former refers to the ``dispersion'' of the\npotential predictions for a fixed input, and the latter refers to the\nconfidence on a particular prediction/generation. We then propose and compare\nseveral confidence/uncertainty measures, applying them to *selective NLG* where\nunreliable results could either be ignored or yielded for further assessment.\nExperiments were carried out with several popular LLMs on question-answering\ndatasets (for evaluation purposes). Results reveal that a simple measure for\nthe semantic dispersion can be a reliable predictor of the quality of LLM\nresponses, providing valuable insights for practitioners on uncertainty\nmanagement when adopting LLMs. The code to replicate our experiments is\navailable at https://github.com/zlin7/UQ-NLG.", "published": "2023-05-30 16:31:26", "link": "http://arxiv.org/abs/2305.19187v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "PanoGen: Text-Conditioned Panoramic Environment Generation for\n  Vision-and-Language Navigation", "abstract": "Vision-and-Language Navigation (VLN) requires the agent to follow language\ninstructions to navigate through 3D environments. One main challenge in VLN is\nthe limited availability of photorealistic training environments, which makes\nit hard to generalize to new and unseen environments. To address this problem,\nwe propose PanoGen, a generation method that can potentially create an infinite\nnumber of diverse panoramic environments conditioned on text. Specifically, we\ncollect room descriptions by captioning the room images in existing\nMatterport3D environments, and leverage a state-of-the-art text-to-image\ndiffusion model to generate the new panoramic environments. We use recursive\noutpainting over the generated images to create consistent 360-degree panorama\nviews. Our new panoramic environments share similar semantic information with\nthe original environments by conditioning on text descriptions, which ensures\nthe co-occurrence of objects in the panorama follows human intuition, and\ncreates enough diversity in room appearance and layout with image outpainting.\nLastly, we explore two ways of utilizing PanoGen in VLN pre-training and\nfine-tuning. We generate instructions for paths in our PanoGen environments\nwith a speaker built on a pre-trained vision-and-language model for VLN\npre-training, and augment the visual observation with our panoramic\nenvironments during agents' fine-tuning to avoid overfitting to seen\nenvironments. Empirically, learning with our PanoGen environments achieves the\nnew state-of-the-art on the Room-to-Room, Room-for-Room, and CVDN datasets.\nPre-training with our PanoGen speaker data is especially effective for CVDN,\nwhich has under-specified instructions and needs commonsense knowledge. Lastly,\nwe show that the agent can benefit from training with more generated panoramic\nenvironments, suggesting promising results for scaling up the PanoGen\nenvironments.", "published": "2023-05-30 16:39:54", "link": "http://arxiv.org/abs/2305.19195v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Translation-Enhanced Multilingual Text-to-Image Generation", "abstract": "Research on text-to-image generation (TTI) still predominantly focuses on the\nEnglish language due to the lack of annotated image-caption data in other\nlanguages; in the long run, this might widen inequitable access to TTI\ntechnology. In this work, we thus investigate multilingual TTI (termed mTTI)\nand the current potential of neural machine translation (NMT) to bootstrap mTTI\nsystems. We provide two key contributions. 1) Relying on a multilingual\nmulti-modal encoder, we provide a systematic empirical study of standard\nmethods used in cross-lingual NLP when applied to mTTI: Translate Train,\nTranslate Test, and Zero-Shot Transfer. 2) We propose Ensemble Adapter (EnsAd),\na novel parameter-efficient approach that learns to weigh and consolidate the\nmultilingual text knowledge within the mTTI framework, mitigating the language\ngap and thus improving mTTI performance. Our evaluations on standard mTTI\ndatasets COCO-CN, Multi30K Task2, and LAION-5B demonstrate the potential of\ntranslation-enhanced mTTI systems and also validate the benefits of the\nproposed EnsAd which derives consistent gains across all datasets. Further\ninvestigations on model variants, ablation studies, and qualitative analyses\nprovide additional insights on the inner workings of the proposed mTTI\napproaches.", "published": "2023-05-30 17:03:52", "link": "http://arxiv.org/abs/2305.19216v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Melody-to-Lyric Generation", "abstract": "Automatic melody-to-lyric generation is a task in which song lyrics are\ngenerated to go with a given melody. It is of significant practical interest\nand more challenging than unconstrained lyric generation as the music imposes\nadditional constraints onto the lyrics. The training data is limited as most\nsongs are copyrighted, resulting in models that underfit the complicated\ncross-modal relationship between melody and lyrics. In this work, we propose a\nmethod for generating high-quality lyrics without training on any aligned\nmelody-lyric data. Specifically, we design a hierarchical lyric generation\nframework that first generates a song outline and second the complete lyrics.\nThe framework enables disentanglement of training (based purely on text) from\ninference (melody-guided text generation) to circumvent the shortage of\nparallel data.\n  We leverage the segmentation and rhythm alignment between melody and lyrics\nto compile the given melody into decoding constraints as guidance during\ninference. The two-step hierarchical design also enables content control via\nthe lyric outline, a much-desired feature for democratizing collaborative song\ncreation. Experimental results show that our model can generate high-quality\nlyrics that are more on-topic, singable, intelligible, and coherent than strong\nbaselines, for example SongMASS, a SOTA model trained on a parallel dataset,\nwith a 24% relative overall quality improvement based on human ratings.", "published": "2023-05-30 17:20:25", "link": "http://arxiv.org/abs/2305.19228v2", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Stutter Seldom Comes Alone -- Cross-Corpus Stuttering Detection as a\n  Multi-label Problem", "abstract": "Most stuttering detection and classification research has viewed stuttering\nas a multi-class classification problem or a binary detection task for each\ndysfluency type; however, this does not match the nature of stuttering, in\nwhich one dysfluency seldom comes alone but rather co-occurs with others. This\npaper explores multi-language and cross-corpus end-to-end stuttering detection\nas a multi-label problem using a modified wav2vec 2.0 system with an\nattention-based classification head and multi-task learning. We evaluate the\nmethod using combinations of three datasets containing English and German\nstuttered speech, one containing speech modified by fluency shaping. The\nexperimental results and an error analysis show that multi-label stuttering\ndetection systems trained on cross-corpus and multi-language data achieve\ncompetitive results but performance on samples with multiple labels stays below\nover-all detection results.", "published": "2023-05-30 17:42:20", "link": "http://arxiv.org/abs/2305.19255v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Make-A-Voice: Unified Voice Synthesis With Discrete Representation", "abstract": "Various applications of voice synthesis have been developed independently\ndespite the fact that they generate \"voice\" as output in common. In addition,\nthe majority of voice synthesis models currently rely on annotated audio data,\nbut it is crucial to scale them to self-supervised datasets in order to\neffectively capture the wide range of acoustic variations present in human\nvoice, including speaker identity, emotion, and prosody. In this work, we\npropose Make-A-Voice, a unified framework for synthesizing and manipulating\nvoice signals from discrete representations. Make-A-Voice leverages a\n\"coarse-to-fine\" approach to model the human voice, which involves three\nstages: 1) semantic stage: model high-level transformation between linguistic\ncontent and self-supervised semantic tokens, 2) acoustic stage: introduce\nvarying control signals as acoustic conditions for semantic-to-acoustic\nmodeling, and 3) generation stage: synthesize high-fidelity waveforms from\nacoustic tokens. Make-A-Voice offers notable benefits as a unified voice\nsynthesis framework: 1) Data scalability: the major backbone (i.e., acoustic\nand generation stage) does not require any annotations, and thus the training\ndata could be scaled up. 2) Controllability and conditioning flexibility: we\ninvestigate different conditioning mechanisms and effectively handle three\nvoice synthesis applications, including text-to-speech (TTS), voice conversion\n(VC), and singing voice synthesis (SVS) by re-synthesizing the discrete voice\nrepresentations with prompt guidance. Experimental results demonstrate that\nMake-A-Voice exhibits superior audio quality and style similarity compared with\ncompetitive baseline models. Audio samples are available at\nhttps://Make-A-Voice.github.io", "published": "2023-05-30 17:59:26", "link": "http://arxiv.org/abs/2305.19269v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SheetCopilot: Bringing Software Productivity to the Next Level through\n  Large Language Models", "abstract": "Computer end users have spent billions of hours completing daily tasks like\ntabular data processing and project timeline scheduling. Most of these tasks\nare repetitive and error-prone, yet most end users lack the skill to automate\nthese burdensome works. With the advent of large language models (LLMs),\ndirecting software with natural language user requests become a reachable goal.\nIn this work, we propose a SheetCopilot agent that takes natural language task\nand control spreadsheet to fulfill the requirements. We propose a set of atomic\nactions as an abstraction of spreadsheet software functionalities. We further\ndesign a state machine-based task planning framework for LLMs to robustly\ninteract with spreadsheets. We curate a representative dataset containing 221\nspreadsheet control tasks and establish a fully automated evaluation pipeline\nfor rigorously benchmarking the ability of LLMs in software control tasks. Our\nSheetCopilot correctly completes 44.3\\% of tasks for a single generation,\noutperforming the strong code generation baseline by a wide margin. Our project\npage:https://sheetcopilot.github.io/.", "published": "2023-05-30 17:59:30", "link": "http://arxiv.org/abs/2305.19308v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Stable Anisotropic Regularization", "abstract": "Given the success of Large Language Models (LLMs), there has been\nconsiderable interest in studying the properties of model activations. The\nliterature overwhelmingly agrees that LLM representations are dominated by a\nfew \"outlier dimensions\" with exceedingly high variance and magnitude. Several\nstudies in Natural Language Processing (NLP) have sought to mitigate the impact\nof such outlier dimensions and force LLMs to be isotropic (i.e., have uniform\nvariance across all dimensions in embedding space). Isotropy is thought to be a\ndesirable property for LLMs that improves model performance and more closely\naligns textual representations with human intuition. However, many of the\nclaims regarding isotropy in NLP have been based on the average cosine\nsimilarity of embeddings, which has recently been shown to be a flawed measure\nof isotropy. In this paper, we propose I-STAR: IsoScore*-based STable\nAnisotropic Regularization, a novel regularization method that can be used to\nincrease or decrease levels of isotropy in embedding space during training.\nI-STAR uses IsoScore*, the first accurate measure of isotropy that is both\ndifferentiable and stable on mini-batch computations. In contrast to several\nprevious works, we find that decreasing isotropy in contextualized embeddings\nimproves performance on the majority of tasks and models considered in this\npaper.", "published": "2023-05-30 18:57:45", "link": "http://arxiv.org/abs/2305.19358v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mining Themes in Clinical Notes to Identify Phenotypes and to Predict\n  Length of Stay in Patients admitted with Heart Failure", "abstract": "Heart failure is a syndrome which occurs when the heart is not able to pump\nblood and oxygen to support other organs in the body. Identifying the\nunderlying themes in the diagnostic codes and procedure reports of patients\nadmitted for heart failure could reveal the clinical phenotypes associated with\nheart failure and to group patients based on their similar characteristics\nwhich could also help in predicting patient outcomes like length of stay. These\nclinical phenotypes usually have a probabilistic latent structure and hence, as\nthere has been no previous work on identifying phenotypes in clinical notes of\nheart failure patients using a probabilistic framework and to predict length of\nstay of these patients using data-driven artificial intelligence-based methods,\nwe apply natural language processing technique, topic modeling, to identify the\nthemes present in diagnostic codes and in procedure reports of 1,200 patients\nadmitted for heart failure at the University of Illinois Hospital and Health\nSciences System (UI Health). Topic modeling identified twelve themes each in\ndiagnostic codes and procedure reports which revealed information about\ndifferent phenotypes related to various perspectives about heart failure, to\nstudy patients' profiles and to discover new relationships among medical\nconcepts. Each theme had a set of keywords and each clinical note was labeled\nwith two themes - one corresponding to its diagnostic code and the other\ncorresponding to its procedure reports along with their percentage\ncontribution. We used these themes and their percentage contribution to predict\nlength of stay. We found that the themes discovered in diagnostic codes and\nprocedure reports using topic modeling together were able to predict length of\nstay of the patients with an accuracy of 61.1% and an Area under the Receiver\nOperating Characteristic Curve (ROC AUC) value of 0.828.", "published": "2023-05-30 19:30:40", "link": "http://arxiv.org/abs/2305.19373v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Contextual Vision Transformers for Robust Representation Learning", "abstract": "We introduce Contextual Vision Transformers (ContextViT), a method designed\nto generate robust image representations for datasets experiencing shifts in\nlatent factors across various groups. Derived from the concept of in-context\nlearning, ContextViT incorporates an additional context token to encapsulate\ngroup-specific information. This integration allows the model to adjust the\nimage representation in accordance with the group-specific context.\nSpecifically, for a given input image, ContextViT maps images with identical\ngroup membership into this context token, which is appended to the input image\ntokens. Additionally, we introduce a context inference network to predict such\ntokens on-the-fly, given a batch of samples from the group. This enables\nContextViT to adapt to new testing distributions during inference time. We\ndemonstrate the efficacy of ContextViT across a wide range of applications. In\nsupervised fine-tuning, we show that augmenting pre-trained ViTs with our\nproposed context conditioning mechanism results in consistent improvements in\nout-of-distribution generalization on iWildCam and FMoW. We also investigate\nself-supervised representation learning with ContextViT. Our experiments on the\nCamelyon17 pathology imaging benchmark and the JUMP-CP microscopy imaging\nbenchmark demonstrate that ContextViT excels in learning stable image\nfeaturizations amidst distribution shift, consistently outperforming its ViT\ncounterpart.", "published": "2023-05-30 20:31:26", "link": "http://arxiv.org/abs/2305.19402v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "GPT4GEO: How a Language Model Sees the World's Geography", "abstract": "Large language models (LLMs) have shown remarkable capabilities across a\nbroad range of tasks involving question answering and the generation of\ncoherent text and code. Comprehensively understanding the strengths and\nweaknesses of LLMs is beneficial for safety, downstream applications and\nimproving performance. In this work, we investigate the degree to which GPT-4\nhas acquired factual geographic knowledge and is capable of using this\nknowledge for interpretative reasoning, which is especially important for\napplications that involve geographic data, such as geospatial analysis, supply\nchain management, and disaster response. To this end, we design and conduct a\nseries of diverse experiments, starting from factual tasks such as location,\ndistance and elevation estimation to more complex questions such as generating\ncountry outlines and travel networks, route finding under constraints and\nsupply chain analysis. We provide a broad characterisation of what GPT-4\n(without plugins or Internet access) knows about the world, highlighting both\npotentially surprising capabilities but also limitations.", "published": "2023-05-30 18:28:04", "link": "http://arxiv.org/abs/2306.00020v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explaining Hate Speech Classification with Model Agnostic Methods", "abstract": "There have been remarkable breakthroughs in Machine Learning and Artificial\nIntelligence, notably in the areas of Natural Language Processing and Deep\nLearning. Additionally, hate speech detection in dialogues has been gaining\npopularity among Natural Language Processing researchers with the increased use\nof social media. However, as evidenced by the recent trends, the need for the\ndimensions of explainability and interpretability in AI models has been deeply\nrealised. Taking note of the factors above, the research goal of this paper is\nto bridge the gap between hate speech prediction and the explanations generated\nby the system to support its decision. This has been achieved by first\npredicting the classification of a text and then providing a posthoc, model\nagnostic and surrogate interpretability approach for explainability and to\nprevent model bias. The bidirectional transformer model BERT has been used for\nprediction because of its state of the art efficiency over other Machine\nLearning models. The model agnostic algorithm LIME generates explanations for\nthe output of a trained classifier and predicts the features that influence the\nmodel decision. The predictions generated from the model were evaluated\nmanually, and after thorough evaluation, we observed that the model performs\nefficiently in predicting and explaining its prediction. Lastly, we suggest\nfurther directions for the expansion of the provided research work.", "published": "2023-05-30 19:52:56", "link": "http://arxiv.org/abs/2306.00021v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Weakly-supervised forced alignment of disfluent speech using\n  phoneme-level modeling", "abstract": "The study of speech disorders can benefit greatly from time-aligned data.\nHowever, audio-text mismatches in disfluent speech cause rapid performance\ndegradation for modern speech aligners, hindering the use of automatic\napproaches. In this work, we propose a simple and effective modification of\nalignment graph construction of CTC-based models using Weighted Finite State\nTransducers. The proposed weakly-supervised approach alleviates the need for\nverbatim transcription of speech disfluencies for forced alignment. During the\ngraph construction, we allow the modeling of common speech disfluencies, i.e.\nrepetitions and omissions. Further, we show that by assessing the degree of\naudio-text mismatch through the use of Oracle Error Rate, our method can be\neffectively used in the wild. Our evaluation on a corrupted version of the\nTIMIT test set and the UCLASS dataset shows significant improvements,\nparticularly for recall, achieving a 23-25% relative improvement over our\nbaselines.", "published": "2023-05-30 09:57:36", "link": "http://arxiv.org/abs/2306.00996v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Selection of Text-to-speech Data to Augment ASR Training", "abstract": "This paper presents a method for selecting appropriate synthetic speech\nsamples from a given large text-to-speech (TTS) dataset as supplementary\ntraining data for an automatic speech recognition (ASR) model. We trained a\nneural network, which can be optimised using cross-entropy loss or Arcface\nloss, to measure the similarity of a synthetic data to real speech. We found\nthat incorporating synthetic samples with considerable dissimilarity to real\nspeech, owing in part to lexical differences, into ASR training is crucial for\nboosting recognition performance. Experimental results on Librispeech test sets\nindicate that, in order to maintain the same speech recognition accuracy as\nwhen using all TTS data, our proposed solution can reduce the size of the TTS\ndata down below its $30\\,\\%$, which is superior to several baseline methods.", "published": "2023-05-30 17:24:28", "link": "http://arxiv.org/abs/2306.00998v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Utilizing Social Media Attributes for Enhanced Keyword Detection: An\n  IDF-LDA Model Applied to Sina Weibo", "abstract": "With the rapid development of social media such as Twitter and Weibo,\ndetecting keywords from a huge volume of text data streams in real-time has\nbecome a critical problem. The keyword detection problem aims at searching\nimportant information from massive text data to reflect the most important\nevents or topics. However, social media data usually has unique features: the\ndocuments are usually short, the language is colloquial, and the data is likely\nto have significant temporal patterns. Therefore, it could be challenging to\ndiscover critical information from these text streams. In this paper, we\npropose a novel method to address the keyword detection problem in social\nmedia. Our model combines the Inverse Document Frequency (IDF) and Latent\nDirichlet Allocation (LDA) models to better cope with the distinct attributes\nof social media data, such as the number of likes, comments, and retweets. By\nweighting the importance of each document based on these attributes, our method\ncan effectively detect more representative keywords over time. Comprehensive\nexperiments conducted under various conditions on Weibo data illustrate that\nour approach outperforms the baselines in various evaluation metrics, including\nprecision and recall for multiple problem settings.", "published": "2023-05-30 08:35:39", "link": "http://arxiv.org/abs/2306.07978v1", "categories": ["cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "How Does Information Bottleneck Help Deep Learning?", "abstract": "Numerous deep learning algorithms have been inspired by and understood via\nthe notion of information bottleneck, where unnecessary information is (often\nimplicitly) minimized while task-relevant information is maximized. However, a\nrigorous argument for justifying why it is desirable to control information\nbottlenecks has been elusive. In this paper, we provide the first rigorous\nlearning theory for justifying the benefit of information bottleneck in deep\nlearning by mathematically relating information bottleneck to generalization\nerrors. Our theory proves that controlling information bottleneck is one way to\ncontrol generalization errors in deep learning, although it is not the only or\nnecessary way. We investigate the merit of our new mathematical findings with\nexperiments across a range of architectures and learning settings. In many\ncases, generalization errors are shown to correlate with the degree of\ninformation bottleneck: i.e., the amount of the unnecessary information at\nhidden layers. This paper provides a theoretical foundation for current and\nfuture methods through the lens of information bottleneck. Our new\ngeneralization bounds scale with the degree of information bottleneck, unlike\nthe previous bounds that scale with the number of parameters, VC dimension,\nRademacher complexity, stability or robustness. Our code is publicly available\nat: https://github.com/xu-ji/information-bottleneck", "published": "2023-05-30 09:28:25", "link": "http://arxiv.org/abs/2305.18887v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "An empirical study on speech restoration guided by self supervised\n  speech representation", "abstract": "Enhancing speech quality is an indispensable yet difficult task as it is\noften complicated by a range of degradation factors. In addition to additive\nnoise, reverberation, clipping, and speech attenuation can all adversely affect\nspeech quality. Speech restoration aims to recover speech components from these\ndistortions. This paper focuses on exploring the impact of self-supervised\nspeech representation learning on the speech restoration task. Specifically, we\nemploy speech representation in various speech restoration networks and\nevaluate their performance under complicated distortion scenarios. Our\nexperiments demonstrate that the contextual information provided by the\nself-supervised speech representation can enhance speech restoration\nperformance in various distortion scenarios, while also increasing robustness\nagainst the duration of speech attenuation and mismatched test conditions.", "published": "2023-05-30 04:26:48", "link": "http://arxiv.org/abs/2305.18739v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "MERLIon CCS Challenge: A English-Mandarin code-switching child-directed\n  speech corpus for language identification and diarization", "abstract": "To enhance the reliability and robustness of language identification (LID)\nand language diarization (LD) systems for heterogeneous populations and\nscenarios, there is a need for speech processing models to be trained on\ndatasets that feature diverse language registers and speech patterns. We\npresent the MERLIon CCS challenge, featuring a first-of-its-kind Zoom video\ncall dataset of parent-child shared book reading, of over 30 hours with over\n300 recordings, annotated by multilingual transcribers using a high-fidelity\nlinguistic transcription protocol. The audio corpus features spontaneous and\nin-the-wild English-Mandarin code-switching, child-directed speech in\nnon-standard accents with diverse language-mixing patterns recorded in a\nvariety of home environments. This report describes the corpus, as well as LID\nand LD results for our baseline and several systems submitted to the MERLIon\nCCS challenge using the corpus.", "published": "2023-05-30 09:26:20", "link": "http://arxiv.org/abs/2305.18881v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Dual Transformer Decoder based Features Fusion Network for Automated\n  Audio Captioning", "abstract": "Automated audio captioning (AAC) which generates textual descriptions of\naudio content. Existing AAC models achieve good results but only use the\nhigh-dimensional representation of the encoder. There is always insufficient\ninformation learning of high-dimensional methods owing to high-dimensional\nrepresentations having a large amount of information. In this paper, a new\nencoder-decoder model called the Low- and High-Dimensional Feature Fusion\n(LHDFF) is proposed. LHDFF uses a new PANNs encoder called Residual PANNs\n(RPANNs) to fuse low- and high-dimensional features. Low-dimensional features\ncontain limited information about specific audio scenes. The fusion of low- and\nhigh-dimensional features can improve model performance by repeatedly\nemphasizing specific audio scene information. To fully exploit the fused\nfeatures, LHDFF uses a dual transformer decoder structure to generate captions\nin parallel. Experimental results show that LHDFF outperforms existing audio\ncaptioning models.", "published": "2023-05-30 05:28:07", "link": "http://arxiv.org/abs/2305.18753v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Understanding temporally weakly supervised training: A case study for\n  keyword spotting", "abstract": "The currently most prominent algorithm to train keyword spotting (KWS) models\nwith deep neural networks (DNNs) requires strong supervision i.e., precise\nknowledge of the spoken keyword location in time. Thus, most KWS approaches\ntreat the presence of redundant data, such as noise, within their training set\nas an obstacle. A common training paradigm to deal with data redundancies is to\nuse temporally weakly supervised learning, which only requires providing labels\non a coarse scale. This study explores the limits of DNN training using\ntemporally weak labeling with applications in KWS. We train a simple end-to-end\nclassifier on the common Google Speech Commands dataset with increased\ndifficulty by randomly appending and adding noise to the training dataset. Our\nresults indicate that temporally weak labeling can achieve comparable results\nto strongly supervised baselines while having a less stringent labeling\nrequirement. In the presence of noise, weakly supervised models are capable to\nlocalize and extract target keywords without explicit supervision, leading to a\nperformance increase compared to strongly supervised approaches.", "published": "2023-05-30 07:12:29", "link": "http://arxiv.org/abs/2305.18794v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LibriTTS-R: A Restored Multi-Speaker Text-to-Speech Corpus", "abstract": "This paper introduces a new speech dataset called ``LibriTTS-R'' designed for\ntext-to-speech (TTS) use. It is derived by applying speech restoration to the\nLibriTTS corpus, which consists of 585 hours of speech data at 24 kHz sampling\nrate from 2,456 speakers and the corresponding texts. The constituent samples\nof LibriTTS-R are identical to those of LibriTTS, with only the sound quality\nimproved. Experimental results show that the LibriTTS-R ground-truth samples\nshowed significantly improved sound quality compared to those in LibriTTS. In\naddition, neural end-to-end TTS trained with LibriTTS-R achieved speech\nnaturalness on par with that of the ground-truth samples. The corpus is freely\navailable for download from \\url{http://www.openslr.org/141/}.", "published": "2023-05-30 07:30:21", "link": "http://arxiv.org/abs/2305.18802v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker anonymization using orthogonal Householder neural network", "abstract": "Speaker anonymization aims to conceal a speaker's identity while preserving\ncontent information in speech. Current mainstream neural-network speaker\nanonymization systems disentangle speech into prosody-related, content, and\nspeaker representations. The speaker representation is then anonymized by a\nselection-based speaker anonymizer that uses a mean vector over a set of\nrandomly selected speaker vectors from an external pool of English speakers.\nHowever, the resulting anonymized vectors are subject to severe privacy leakage\nagainst powerful attackers, reduction in speaker diversity, and language\nmismatch problems for unseen-language speaker anonymization. To generate\ndiverse, language-neutral speaker vectors, this paper proposes an anonymizer\nbased on an orthogonal Householder neural network (OHNN). Specifically, the\nOHNN acts like a rotation to transform the original speaker vectors into\nanonymized speaker vectors, which are constrained to follow the distribution\nover the original speaker vector space. A basic classification loss is\nintroduced to ensure that anonymized speaker vectors from different speakers\nhave unique speaker identities. To further protect speaker identities, an\nimproved classification loss and similarity loss are used to push\noriginal-anonymized sample pairs away from each other. Experiments on\nVoicePrivacy Challenge datasets in English and the \\textit{AISHELL-3} dataset\nin Mandarin demonstrate the proposed anonymizer's effectiveness.", "published": "2023-05-30 08:16:10", "link": "http://arxiv.org/abs/2305.18823v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pseudo-Siamese Network based Timbre-reserved Black-box Adversarial\n  Attack in Speaker Identification", "abstract": "In this study, we propose a timbre-reserved adversarial attack approach for\nspeaker identification (SID) to not only exploit the weakness of the SID model\nbut also preserve the timbre of the target speaker in a black-box attack\nsetting. Particularly, we generate timbre-reserved fake audio by adding an\nadversarial constraint during the training of the voice conversion model. Then,\nwe leverage a pseudo-Siamese network architecture to learn from the black-box\nSID model constraining both intrinsic similarity and structural similarity\nsimultaneously. The intrinsic similarity loss is to learn an intrinsic\ninvariance, while the structural similarity loss is to ensure that the\nsubstitute SID model shares a similar decision boundary to the fixed black-box\nSID model. The substitute model can be used as a proxy to generate\ntimbre-reserved fake audio for attacking. Experimental results on the Audio\nDeepfake Detection (ADD) challenge dataset indicate that the attack success\nrate of our proposed approach yields up to 60.58% and 55.38% in the white-box\nand black-box scenarios, respectively, and can deceive both human beings and\nmachines.", "published": "2023-05-30 13:20:31", "link": "http://arxiv.org/abs/2305.19020v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Prospective Validation of Motor-Based Intervention with Automated\n  Mispronunciation Detection of Rhotics in Residual Speech Sound Disorders", "abstract": "Because lab accuracy of clinical speech technology systems may be\noveroptimistic, clinical validation is vital to demonstrate system\nreproducibility - in this case, the ability of the PERCEPT-R Classifier to\npredict clinician judgment of American English /r/ during ChainingAI\nmotor-based speech sound disorder intervention. All five participants\nexperienced statistically-significant improvement in untreated words following\n10 sessions of combined human-ChainingAI treatment. These gains, despite a wide\nrange of PERCEPT-human and human-human (F1-score) agreement, raise questions\nabout best measuring classification performance for clinical speech that may be\nperceptually ambiguous.", "published": "2023-05-30 14:53:01", "link": "http://arxiv.org/abs/2305.19090v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Predicting Preferred Dialogue-to-Background Loudness Difference in\n  Dialogue-Separated Audio", "abstract": "Dialogue Enhancement (DE) enables the rebalancing of dialogue and background\nsounds to fit personal preferences and needs in the context of broadcast audio.\nWhen individual audio stems are unavailable from production, Dialogue\nSeparation (DS) can be applied to the final audio mixture to obtain estimates\nof these stems. This work focuses on Preferred Loudness Differences (PLDs)\nbetween dialogue and background sounds. While previous studies determined the\nPLD through a listening test employing original stems from production, stems\nestimated by DS are used in the present study. In addition, a larger variety of\nsignal classes is considered. PLDs vary substantially across individuals\n(average interquartile range: 5.7 LU). Despite this variability, PLDs are found\nto be highly dependent on the signal type under consideration, and it is shown\nthat median PLDs can be predicted using objective intelligibility metrics. Two\nexisting baseline prediction methods - intended for use with original stems -\ndisplayed a Mean Absolute Error (MAE) of 7.5 LU and 5 LU, respectively. A\nmodified baseline (MAE: 3.2 LU) and an alternative approach (MAE: 2.5 LU) are\nproposed. Results support the viability of processing final broadcast mixtures\nwith DS and offering an alternative remixing that accounts for median PLDs.", "published": "2023-05-30 15:04:48", "link": "http://arxiv.org/abs/2305.19100v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "E-PANNs: Sound Recognition Using Efficient Pre-trained Audio Neural\n  Networks", "abstract": "Sounds carry an abundance of information about activities and events in our\neveryday environment, such as traffic noise, road works, music, or people\ntalking. Recent machine learning methods, such as convolutional neural networks\n(CNNs), have been shown to be able to automatically recognize sound activities,\na task known as audio tagging. One such method, pre-trained audio neural\nnetworks (PANNs), provides a neural network which has been pre-trained on over\n500 sound classes from the publicly available AudioSet dataset, and can be used\nas a baseline or starting point for other tasks. However, the existing PANNs\nmodel has a high computational complexity and large storage requirement. This\ncould limit the potential for deploying PANNs on resource-constrained devices,\nsuch as on-the-edge sound sensors, and could lead to high energy consumption if\nmany such devices were deployed. In this paper, we reduce the computational\ncomplexity and memory requirement of the PANNs model by taking a pruning\napproach to eliminate redundant parameters from the PANNs model. The resulting\nEfficient PANNs (E-PANNs) model, which requires 36\\% less computations and 70\\%\nless memory, also slightly improves the sound recognition (audio tagging)\nperformance. The code for the E-PANNs model has been released under an open\nsource license.", "published": "2023-05-30 00:08:55", "link": "http://arxiv.org/abs/2305.18665v1", "categories": ["cs.SD", "cs.AI", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Towards single integrated spoofing-aware speaker verification embeddings", "abstract": "This study aims to develop a single integrated spoofing-aware speaker\nverification (SASV) embeddings that satisfy two aspects. First, rejecting\nnon-target speakers' input as well as target speakers' spoofed inputs should be\naddressed. Second, competitive performance should be demonstrated compared to\nthe fusion of automatic speaker verification (ASV) and countermeasure (CM)\nembeddings, which outperformed single embedding solutions by a large margin in\nthe SASV2022 challenge. We analyze that the inferior performance of single SASV\nembeddings comes from insufficient amount of training data and distinct nature\nof ASV and CM tasks. To this end, we propose a novel framework that includes\nmulti-stage training and a combination of loss functions. Copy synthesis,\ncombined with several vocoders, is also exploited to address the lack of\nspoofed data. Experimental results show dramatic improvements, achieving a\nSASV-EER of 1.06% on the evaluation protocol of the SASV2022 challenge.", "published": "2023-05-30 14:15:39", "link": "http://arxiv.org/abs/2305.19051v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using\n  Spatial Transformer Networks", "abstract": "Thanks to the latest deep learning algorithms, silent speech interfaces (SSI)\nare now able to synthesize intelligible speech from articulatory movement data\nunder certain conditions. However, the resulting models are rather\nspeaker-specific, making a quick switch between users troublesome. Even for the\nsame speaker, these models perform poorly cross-session, i.e. after dismounting\nand re-mounting the recording equipment. To aid quick speaker and session\nadaptation of ultrasound tongue imaging-based SSI models, we extend our deep\nnetworks with a spatial transformer network (STN) module, capable of performing\nan affine transformation on the input images. Although the STN part takes up\nonly about 10% of the network, our experiments show that adapting just the STN\nmodule might allow to reduce MSE by 88% on the average, compared to retraining\nthe whole network. The improvement is even larger (around 92%) when adapting\nthe network to different recording sessions from the same speaker.", "published": "2023-05-30 15:41:47", "link": "http://arxiv.org/abs/2305.19130v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Leveraging Semantic Information for Efficient Self-Supervised Emotion\n  Recognition with Audio-Textual Distilled Models", "abstract": "In large part due to their implicit semantic modeling, self-supervised\nlearning (SSL) methods have significantly increased the performance of valence\nrecognition in speech emotion recognition (SER) systems. Yet, their large size\nmay often hinder practical implementations. In this work, we take HuBERT as an\nexample of an SSL model and analyze the relevance of each of its layers for\nSER. We show that shallow layers are more important for arousal recognition\nwhile deeper layers are more important for valence. This observation motivates\nthe importance of additional textual information for accurate valence\nrecognition, as the distilled framework lacks the depth of its large-scale SSL\nteacher. Thus, we propose an audio-textual distilled SSL framework that, while\nhaving only ~20% of the trainable parameters of a large SSL model, achieves on\npar performance across the three emotion dimensions (arousal, valence,\ndominance) on the MSP-Podcast v1.10 dataset.", "published": "2023-05-30 16:29:33", "link": "http://arxiv.org/abs/2305.19184v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio classification using ML methods", "abstract": "Machine Learning systems have achieved outstanding performance in different\ndomains. In this paper machine learning methods have been applied to\nclassification task to classify music genre. The code shows how to extract\nfeatures from audio files and classify them using supervised learning into 2\ngenres namely classical and metal. Algorithms used are LogisticRegression, SVC\nusing different kernals (linear, sigmoid, rbf and poly), KNeighborsClassifier ,\nRandomForestClassifier, DecisionTreeClassifier and GaussianNB.", "published": "2023-05-30 15:42:13", "link": "http://arxiv.org/abs/2305.19304v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Unified Audio-Visual Learning Framework for Localization, Separation,\n  and Recognition", "abstract": "The ability to accurately recognize, localize and separate sound sources is\nfundamental to any audio-visual perception task. Historically, these abilities\nwere tackled separately, with several methods developed independently for each\ntask. However, given the interconnected nature of source localization,\nseparation, and recognition, independent models are likely to yield suboptimal\nperformance as they fail to capture the interdependence between these tasks. To\naddress this problem, we propose a unified audio-visual learning framework\n(dubbed OneAVM) that integrates audio and visual cues for joint localization,\nseparation, and recognition. OneAVM comprises a shared audio-visual encoder and\ntask-specific decoders trained with three objectives. The first objective\naligns audio and visual representations through a localized audio-visual\ncorrespondence loss. The second tackles visual source separation using a\ntraditional mix-and-separate framework. Finally, the third objective reinforces\nvisual feature separation and localization by mixing images in pixel space and\naligning their representations with those of all corresponding sound sources.\nExtensive experiments on MUSIC, VGG-Instruments, VGG-Music, and VGGSound\ndatasets demonstrate the effectiveness of OneAVM for all three tasks,\naudio-visual source localization, separation, and nearest neighbor recognition,\nand empirically demonstrate a strong positive transfer between them.", "published": "2023-05-30 23:53:12", "link": "http://arxiv.org/abs/2305.19458v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Trustworthy Sensor Fusion against Inaudible Command Attacks in Advanced\n  Driver-Assistance System", "abstract": "There are increasing concerns about malicious attacks on autonomous vehicles.\nIn particular, inaudible voice command attacks pose a significant threat as\nvoice commands become available in autonomous driving systems. How to\nempirically defend against these inaudible attacks remains an open question.\nPrevious research investigates utilizing deep learning-based multimodal fusion\nfor defense, without considering the model uncertainty in trustworthiness. As\ndeep learning has been applied to increasingly sensitive tasks, uncertainty\nmeasurement is crucial in helping improve model robustness, especially in\nmission-critical scenarios. In this paper, we propose the Multimodal Fusion\nFramework (MFF) as an intelligent security system to defend against inaudible\nvoice command attacks. MFF fuses heterogeneous audio-vision modalities using\nVGG family neural networks and achieves the detection accuracy of 92.25% in the\ncomparative fusion method empirical study. Additionally, extensive experiments\non audio-vision tasks reveal the model's uncertainty. Using Expected\nCalibration Errors, we measure calibration errors and Monte-Carlo Dropout to\nestimate the predictive distribution for the proposed models. Our findings show\nempirically to train robust multimodal models, improve standard accuracy and\nprovide a further step toward interpretability. Finally, we discuss the pros\nand cons of our approach and its applicability for Advanced Driver Assistance\nSystems.", "published": "2023-05-30 00:57:51", "link": "http://arxiv.org/abs/2306.05358v1", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
