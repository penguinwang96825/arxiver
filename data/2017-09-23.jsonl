{"title": "Long Short-Term Memory for Japanese Word Segmentation", "abstract": "This study presents a Long Short-Term Memory (LSTM) neural network approach\nto Japanese word segmentation (JWS). Previous studies on Chinese word\nsegmentation (CWS) succeeded in using recurrent neural networks such as LSTM\nand gated recurrent units (GRU). However, in contrast to Chinese, Japanese\nincludes several character types, such as hiragana, katakana, and kanji, that\nproduce orthographic variations and increase the difficulty of word\nsegmentation. Additionally, it is important for JWS tasks to consider a global\ncontext, and yet traditional JWS approaches rely on local features. In order to\naddress this problem, this study proposes employing an LSTM-based approach to\nJWS. The experimental results indicate that the proposed model achieves\nstate-of-the-art accuracy with respect to various Japanese corpora.", "published": "2017-09-23 06:15:37", "link": "http://arxiv.org/abs/1709.08011v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Independent Acquisition of Abbreviations", "abstract": "This paper addresses automatic extraction of abbreviations (encompassing\nacronyms and initialisms) and corresponding long-form expansions from plain\nunstructured text. We create and are going to release a multilingual resource\nfor abbreviations and their corresponding expansions, built automatically by\nexploiting Wikipedia redirect and disambiguation pages, that can be used as a\nbenchmark for evaluation. We address a shortcoming of previous work where only\nthe redirect pages were used, and so every abbreviation had only a single\nexpansion, even though multiple different expansions are possible for many of\nthe abbreviations. We also develop a principled machine learning based approach\nto scoring expansion candidates using different techniques such as indicators\nof near synonymy, topical relatedness, and surface similarity. We show improved\nperformance over seven languages, including two with a non-Latin alphabet,\nrelative to strong baselines.", "published": "2017-09-23 16:43:31", "link": "http://arxiv.org/abs/1709.08074v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Statistical Parametric Speech Synthesis Incorporating Generative\n  Adversarial Networks", "abstract": "A method for statistical parametric speech synthesis incorporating generative\nadversarial networks (GANs) is proposed. Although powerful deep neural networks\n(DNNs) techniques can be applied to artificially synthesize speech waveform,\nthe synthetic speech quality is low compared with that of natural speech. One\nof the issues causing the quality degradation is an over-smoothing effect often\nobserved in the generated speech parameters. A GAN introduced in this paper\nconsists of two neural networks: a discriminator to distinguish natural and\ngenerated samples, and a generator to deceive the discriminator. In the\nproposed framework incorporating the GANs, the discriminator is trained to\ndistinguish natural and generated speech parameters, while the acoustic models\nare trained to minimize the weighted sum of the conventional minimum generation\nloss and an adversarial loss for deceiving the discriminator. Since the\nobjective of the GANs is to minimize the divergence (i.e., distribution\ndifference) between the natural and generated speech parameters, the proposed\nmethod effectively alleviates the over-smoothing effect on the generated speech\nparameters. We evaluated the effectiveness for text-to-speech and voice\nconversion, and found that the proposed method can generate more natural\nspectral parameters and $F_0$ than conventional minimum generation error\ntraining algorithm regardless its hyper-parameter settings. Furthermore, we\ninvestigated the effect of the divergence of various GANs, and found that a\nWasserstein GAN minimizing the Earth-Mover's distance works the best in terms\nof improving synthetic speech quality.", "published": "2017-09-23 12:10:32", "link": "http://arxiv.org/abs/1709.08041v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
