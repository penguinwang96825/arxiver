{"title": "Relational Representation Learning in Visually-Rich Documents", "abstract": "Relational understanding is critical for a number of visually-rich documents\n(VRDs) understanding tasks. Through multi-modal pre-training, recent studies\nprovide comprehensive contextual representations and exploit them as prior\nknowledge for downstream tasks. In spite of their impressive results, we\nobserve that the widespread relational hints (e.g., relation of key/value\nfields on receipts) built upon contextual knowledge are not excavated yet. To\nmitigate this gap, we propose DocReL, a Document Relational Representation\nLearning framework. The major challenge of DocReL roots in the variety of\nrelations. From the simplest pairwise relation to the complex global structure,\nit is infeasible to conduct supervised training due to the definition of\nrelation varies and even conflicts in different tasks. To deal with the\nunpredictable definition of relations, we propose a novel contrastive learning\ntask named Relational Consistency Modeling (RCM), which harnesses the fact that\nexisting relations should be consistent in differently augmented positive\nviews. RCM provides relational representations which are more compatible to the\nurgent need of downstream tasks, even without any knowledge about the exact\ndefinition of relation. DocReL achieves better performance on a wide variety of\nVRD relational understanding tasks, including table structure recognition, key\ninformation extraction and reading order detection.", "published": "2022-05-05 02:54:31", "link": "http://arxiv.org/abs/2205.02411v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BORT: Back and Denoising Reconstruction for End-to-End Task-Oriented\n  Dialog", "abstract": "A typical end-to-end task-oriented dialog system transfers context into\ndialog state, and upon which generates a response, which usually faces the\nproblem of error propagation from both previously generated inaccurate dialog\nstates and responses, especially in low-resource scenarios. To alleviate these\nissues, we propose BORT, a back and denoising reconstruction approach for\nend-to-end task-oriented dialog system. Squarely, to improve the accuracy of\ndialog states, back reconstruction is used to reconstruct the original input\ncontext from the generated dialog states since inaccurate dialog states cannot\nrecover the corresponding input context. To enhance the denoising capability of\nthe model to reduce the impact of error propagation, denoising reconstruction\nis used to reconstruct the corrupted dialog state and response. Extensive\nexperiments conducted on MultiWOZ 2.0 and CamRest676 show the effectiveness of\nBORT. Furthermore, BORT demonstrates its advanced capabilities in the zero-shot\ndomain and low-resource scenarios.", "published": "2022-05-05 07:02:31", "link": "http://arxiv.org/abs/2205.02471v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Simple Contrastive Learning Objective for Alleviating Neural Text\n  Degeneration", "abstract": "The cross-entropy objective has proved to be an all-purpose training\nobjective for autoregressive language models (LMs). However, without\nconsidering the penalization of problematic tokens, LMs trained using\ncross-entropy exhibit text degeneration. To address this, unlikelihood training\nhas been proposed to reduce the probability of unlikely tokens predicted by\nLMs. But unlikelihood does not consider the relationship between the label\ntokens and unlikely token candidates, thus showing marginal improvements in\ndegeneration. We propose a new contrastive token learning objective that\ninherits the advantages of cross-entropy and unlikelihood training and avoids\ntheir limitations. The key idea is to teach a LM to generate high probabilities\nfor label tokens and low probabilities of negative candidates. Comprehensive\nexperiments on language modeling and open-domain dialogue generation tasks show\nthat the proposed contrastive token objective yields much less repetitive\ntexts, with a higher generation quality than baseline approaches, achieving the\nnew state-of-the-art performance on text degeneration.", "published": "2022-05-05 08:50:50", "link": "http://arxiv.org/abs/2205.02517v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Theories of \"Gender\" in NLP Bias Research", "abstract": "The rise of concern around Natural Language Processing (NLP) technologies\ncontaining and perpetuating social biases has led to a rich and rapidly growing\narea of research. Gender bias is one of the central biases being analyzed, but\nto date there is no comprehensive analysis of how \"gender\" is theorized in the\nfield. We survey nearly 200 articles concerning gender bias in NLP to discover\nhow the field conceptualizes gender both explicitly (e.g. through definitions\nof terms) and implicitly (e.g. through how gender is operationalized in\npractice). In order to get a better idea of emerging trajectories of thought,\nwe split these articles into two sections by time.\n  We find that the majority of the articles do not make their theorization of\ngender explicit, even if they clearly define \"bias.\" Almost none use a model of\ngender that is intersectional or inclusive of nonbinary genders; and many\nconflate sex characteristics, social gender, and linguistic gender in ways that\ndisregard the existence and experience of trans, nonbinary, and intersex\npeople. There is an increase between the two time-sections in statements\nacknowledging that gender is a complicated reality, however, very few articles\nmanage to put this acknowledgment into practice. In addition to analyzing these\nfindings, we provide specific recommendations to facilitate interdisciplinary\nwork, and to incorporate theory and methodology from Gender Studies. Our hope\nis that this will produce more inclusive gender bias research in NLP.", "published": "2022-05-05 09:20:53", "link": "http://arxiv.org/abs/2205.02526v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LUNA: Learning Slot-Turn Alignment for Dialogue State Tracking", "abstract": "Dialogue state tracking (DST) aims to predict the current dialogue state\ngiven the dialogue history. Existing methods generally exploit the utterances\nof all dialogue turns to assign value for each slot. This could lead to\nsuboptimal results due to the information introduced from irrelevant utterances\nin the dialogue history, which may be useless and can even cause confusion. To\naddress this problem, we propose LUNA, a sLot-tUrN Alignment enhanced approach.\nIt first explicitly aligns each slot with its most relevant utterance, then\nfurther predicts the corresponding value based on this aligned utterance\ninstead of all dialogue utterances. Furthermore, we design a slot ranking\nauxiliary task to learn the temporal correlation among slots which could\nfacilitate the alignment. Comprehensive experiments are conducted on\nmulti-domain task-oriented dialogue datasets, i.e., MultiWOZ 2.0, MultiWOZ 2.1,\nand MultiWOZ 2.2. The results show that LUNA achieves new state-of-the-art\nresults on these datasets.", "published": "2022-05-05 10:18:23", "link": "http://arxiv.org/abs/2205.02550v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Balancing Multi-Domain Corpora Learning for Open-Domain Response\n  Generation", "abstract": "Open-domain conversational systems are assumed to generate equally good\nresponses on multiple domains. Previous work achieved good performance on the\nsingle corpus, but training and evaluating on multiple corpora from different\ndomains are less studied. This paper explores methods of generating relevant\nresponses for each of multiple multi-domain corpora. We first examine\ninterleaved learning which intermingles multiple corpora as the baseline. We\nthen investigate two multi-domain learning methods, labeled learning and\nmulti-task labeled learning, which encode each corpus through a unique corpus\nembedding. Furthermore, we propose Domain-specific Frequency (DF), a novel\nword-level importance weight that measures the relative importance of a word\nfor a specific corpus compared to other corpora. Based on DF, we propose\nweighted learning, a method that integrates DF to the loss function. We also\nadopt DF as a new evaluation metric. Extensive experiments show that our\nmethods gain significant improvements on both automatic and human evaluation.\nWe share our code and data for reproducibility", "published": "2022-05-05 11:10:54", "link": "http://arxiv.org/abs/2205.02570v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "METGEN: A Module-Based Entailment Tree Generation Framework for Answer\n  Explanation", "abstract": "Knowing the reasoning chains from knowledge to the predicted answers can help\nconstruct an explainable question answering (QA) system. Advances on QA\nexplanation propose to explain the answers with entailment trees composed of\nmultiple entailment steps. While current work proposes to generate entailment\ntrees with end-to-end generative models, the steps in the generated trees are\nnot constrained and could be unreliable. In this paper, we propose METGEN, a\nModule-based Entailment Tree GENeration framework that has multiple modules and\na reasoning controller. Given a question and several supporting knowledge,\nMETGEN can iteratively generate the entailment tree by conducting single-step\nentailment with separate modules and selecting the reasoning flow with the\ncontroller. As each module is guided to perform a specific type of entailment\nreasoning, the steps generated by METGEN are more reliable and valid.\nExperiment results on the standard benchmark show that METGEN can outperform\nprevious state-of-the-art models with only 9% of the parameters.", "published": "2022-05-05 12:06:02", "link": "http://arxiv.org/abs/2205.02593v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Global and Local Hierarchies for Hierarchical Text\n  Classification", "abstract": "Hierarchical text classification aims to leverage label hierarchy in\nmulti-label text classification. Existing methods encode label hierarchy in a\nglobal view, where label hierarchy is treated as the static hierarchical\nstructure containing all labels. Since global hierarchy is static and\nirrelevant to text samples, it makes these methods hard to exploit hierarchical\ninformation. Contrary to global hierarchy, local hierarchy as a structured\nlabels hierarchy corresponding to each text sample. It is dynamic and relevant\nto text samples, which is ignored in previous methods. To exploit global and\nlocal hierarchies,we propose Hierarchy-guided BERT with Global and Local\nhierarchies (HBGL), which utilizes the large-scale parameters and prior\nlanguage knowledge of BERT to model both global and local\nhierarchies.Moreover,HBGL avoids the intentional fusion of semantic and\nhierarchical modules by directly modeling semantic and hierarchical information\nwith BERT.Compared with the state-of-the-art method HGCLR,our method achieves\nsignificant improvement on three benchmark datasets.", "published": "2022-05-05 12:48:41", "link": "http://arxiv.org/abs/2205.02613v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WDV: A Broad Data Verbalisation Dataset Built from Wikidata", "abstract": "Data verbalisation is a task of great importance in the current field of\nnatural language processing, as there is great benefit in the transformation of\nour abundant structured and semi-structured data into human-readable formats.\nVerbalising Knowledge Graph (KG) data focuses on converting interconnected\ntriple-based claims, formed of subject, predicate, and object, into text.\nAlthough KG verbalisation datasets exist for some KGs, there are still gaps in\ntheir fitness for use in many scenarios. This is especially true for Wikidata,\nwhere available datasets either loosely couple claim sets with textual\ninformation or heavily focus on predicates around biographies, cities, and\ncountries. To address these gaps, we propose WDV, a large KG claim\nverbalisation dataset built from Wikidata, with a tight coupling between\ntriples and text, covering a wide variety of entities and predicates. We also\nevaluate the quality of our verbalisations through a reusable workflow for\nmeasuring human-centred fluency and adequacy scores. Our data and code are\nopenly available in the hopes of furthering research towards KG verbalisation.", "published": "2022-05-05 13:10:12", "link": "http://arxiv.org/abs/2205.02627v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient yet Competitive Speech Translation: FBK@IWSLT2022", "abstract": "The primary goal of this FBK's systems submission to the IWSLT 2022 offline\nand simultaneous speech translation tasks is to reduce model training costs\nwithout sacrificing translation quality. As such, we first question the need of\nASR pre-training, showing that it is not essential to achieve competitive\nresults. Second, we focus on data filtering, showing that a simple method that\nlooks at the ratio between source and target characters yields a quality\nimprovement of 1 BLEU. Third, we compare different methods to reduce the\ndetrimental effect of the audio segmentation mismatch between training data\nmanually segmented at sentence level and inference data that is automatically\nsegmented. Towards the same goal of training cost reduction, we participate in\nthe simultaneous task with the same model trained for offline ST. The\neffectiveness of our lightweight training strategy is shown by the high score\nobtained on the MuST-C en-de corpus (26.7 BLEU) and is confirmed in\nhigh-resource data conditions by a 1.6 BLEU improvement on the IWSLT2020 test\nset over last year's winning system.", "published": "2022-05-05 13:13:48", "link": "http://arxiv.org/abs/2205.02629v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Implicit N-grams Induced by Recurrence", "abstract": "Although self-attention based models such as Transformers have achieved\nremarkable successes on natural language processing (NLP) tasks, recent studies\nreveal that they have limitations on modeling sequential transformations (Hahn,\n2020), which may prompt re-examinations of recurrent neural networks (RNNs)\nthat demonstrated impressive results on handling sequential data. Despite many\nprior attempts to interpret RNNs, their internal mechanisms have not been fully\nunderstood, and the question on how exactly they capture sequential features\nremains largely unclear. In this work, we present a study that shows there\nactually exist some explainable components that reside within the hidden\nstates, which are reminiscent of the classical n-grams features. We evaluated\nsuch extracted explainable features from trained RNNs on downstream sentiment\nanalysis tasks and found they could be used to model interesting linguistic\nphenomena such as negation and intensification. Furthermore, we examined the\nefficacy of using such n-gram components alone as encoders on tasks such as\nsentiment analysis and language modeling, revealing they could be playing\nimportant roles in contributing to the overall performance of RNNs. We hope our\nfindings could add interpretability to RNN architectures, and also provide\ninspirations for proposing new architectures for sequential data.", "published": "2022-05-05 15:53:46", "link": "http://arxiv.org/abs/2205.02724v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CATs are Fuzzy PETs: A Corpus and Analysis of Potentially Euphemistic\n  Terms", "abstract": "Euphemisms have not received much attention in natural language processing,\ndespite being an important element of polite and figurative language.\nEuphemisms prove to be a difficult topic, not only because they are subject to\nlanguage change, but also because humans may not agree on what is a euphemism\nand what is not. Nevertheless, the first step to tackling the issue is to\ncollect and analyze examples of euphemisms. We present a corpus of potentially\neuphemistic terms (PETs) along with example texts from the GloWbE corpus.\nAdditionally, we present a subcorpus of texts where these PETs are not being\nused euphemistically, which may be useful for future applications. We also\ndiscuss the results of multiple analyses run on the corpus. Firstly, we find\nthat sentiment analysis on the euphemistic texts supports that PETs generally\ndecrease negative and offensive sentiment. Secondly, we observe cases of\ndisagreement in an annotation task, where humans are asked to label PETs as\neuphemistic or not in a subset of our corpus text examples. We attribute the\ndisagreement to a variety of potential reasons, including if the PET was a\ncommonly accepted term (CAT).", "published": "2022-05-05 16:01:39", "link": "http://arxiv.org/abs/2205.02728v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entity Cloze By Date: What LMs Know About Unseen Entities", "abstract": "Language models (LMs) are typically trained once on a large-scale corpus and\nused for years without being updated. However, in a dynamic world, new entities\nconstantly arise. We propose a framework to analyze what LMs can infer about\nnew entities that did not exist when the LMs were pretrained. We derive a\ndataset of entities indexed by their origination date and paired with their\nEnglish Wikipedia articles, from which we can find sentences about each entity.\nWe evaluate LMs' perplexity on masked spans within these sentences. We show\nthat models more informed about the entities, such as those with access to a\ntextual definition of them, achieve lower perplexity on this benchmark. Our\nexperimental results demonstrate that making inferences about new entities\nremains difficult for LMs. Given its wide coverage on entity knowledge and\ntemporal indexing, our dataset can be used to evaluate LMs and techniques\ndesigned to modify or extend their knowledge. Our automatic data collection\npipeline can be easily used to continually update our benchmark.", "published": "2022-05-05 17:59:31", "link": "http://arxiv.org/abs/2205.02832v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CompactIE: Compact Facts in Open Information Extraction", "abstract": "A major drawback of modern neural OpenIE systems and benchmarks is that they\nprioritize high coverage of information in extractions over compactness of\ntheir constituents. This severely limits the usefulness of OpenIE extractions\nin many downstream tasks. The utility of extractions can be improved if\nextractions are compact and share constituents. To this end, we study the\nproblem of identifying compact extractions with neural-based methods. We\npropose CompactIE, an OpenIE system that uses a novel pipelined approach to\nproduce compact extractions with overlapping constituents. It first detects\nconstituents of the extractions and then links them to build extractions. We\ntrain our system on compact extractions obtained by processing existing\nbenchmarks. Our experiments on CaRB and Wire57 datasets indicate that CompactIE\nfinds 1.5x-2x more compact extractions than previous systems, with high\nprecision, establishing a new state-of-the-art performance in OpenIE.", "published": "2022-05-05 18:27:41", "link": "http://arxiv.org/abs/2205.02880v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PREME: Preference-based Meeting Exploration through an Interactive\n  Questionnaire", "abstract": "The recent increase in the volume of online meetings necessitates automated\ntools for managing and organizing the material, especially when an attendee has\nmissed the discussion and needs assistance in quickly exploring it. In this\nwork, we propose a novel end-to-end framework for generating interactive\nquestionnaires for preference-based meeting exploration. As a result, users are\nsupplied with a list of suggested questions reflecting their preferences. Since\nthe task is new, we introduce an automatic evaluation strategy. Namely, it\nmeasures how much the generated questions via questionnaire are answerable to\nensure factual correctness and covers the source meeting for the depth of\npossible exploration.", "published": "2022-05-05 00:12:04", "link": "http://arxiv.org/abs/2205.02370v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Interactive Grounded Language Understanding in a Collaborative\n  Environment: IGLU 2021", "abstract": "Human intelligence has the remarkable ability to quickly adapt to new tasks\nand environments. Starting from a very young age, humans acquire new skills and\nlearn how to solve new tasks either by imitating the behavior of others or by\nfollowing provided natural language instructions. To facilitate research in\nthis direction, we propose \\emph{IGLU: Interactive Grounded Language\nUnderstanding in a Collaborative Environment}.\n  The primary goal of the competition is to approach the problem of how to\nbuild interactive agents that learn to solve a task while provided with\ngrounded natural language instructions in a collaborative environment.\nUnderstanding the complexity of the challenge, we split it into sub-tasks to\nmake it feasible for participants.", "published": "2022-05-05 01:20:09", "link": "http://arxiv.org/abs/2205.02388v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Robust Conversational Agents against Imperceptible Toxicity Triggers", "abstract": "Warning: this paper contains content that maybe offensive or upsetting.\nRecent research in Natural Language Processing (NLP) has advanced the\ndevelopment of various toxicity detection models with the intention of\nidentifying and mitigating toxic language from existing systems. Despite the\nabundance of research in this area, less attention has been given to\nadversarial attacks that force the system to generate toxic language and the\ndefense against them. Existing work to generate such attacks is either based on\nhuman-generated attacks which is costly and not scalable or, in case of\nautomatic attacks, the attack vector does not conform to human-like language,\nwhich can be detected using a language model loss. In this work, we propose\nattacks against conversational agents that are imperceptible, i.e., they fit\nthe conversation in terms of coherency, relevancy, and fluency, while they are\neffective and scalable, i.e., they can automatically trigger the system into\ngenerating toxic language. We then propose a defense mechanism against such\nattacks which not only mitigates the attack but also attempts to maintain the\nconversational flow. Through automatic and human evaluations, we show that our\ndefense is effective at avoiding toxic language generation even against\nimperceptible toxicity triggers while the generated language fits the\nconversation in terms of coherency and relevancy. Lastly, we establish the\ngeneralizability of such a defense mechanism on language generation models\nbeyond conversational agents.", "published": "2022-05-05 01:48:39", "link": "http://arxiv.org/abs/2205.02392v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Optimising Equal Opportunity Fairness in Model Training", "abstract": "Real-world datasets often encode stereotypes and societal biases. Such biases\ncan be implicitly captured by trained models, leading to biased predictions and\nexacerbating existing societal preconceptions. Existing debiasing methods, such\nas adversarial training and removing protected information from\nrepresentations, have been shown to reduce bias. However, a disconnect between\nfairness criteria and training objectives makes it difficult to reason\ntheoretically about the effectiveness of different techniques. In this work, we\npropose two novel training objectives which directly optimise for the\nwidely-used criterion of {\\it equal opportunity}, and show that they are\neffective in reducing bias while maintaining high performance over two\nclassification tasks.", "published": "2022-05-05 01:57:58", "link": "http://arxiv.org/abs/2205.02393v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Cross-modal Contrastive Learning for Speech Translation", "abstract": "How can we learn unified representations for spoken utterances and their\nwritten text? Learning similar representations for semantically similar speech\nand text is important for speech translation. To this end, we propose ConST, a\ncross-modal contrastive learning method for end-to-end speech-to-text\ntranslation. We evaluate ConST and a variety of previous baselines on a popular\nbenchmark MuST-C. Experiments show that the proposed ConST consistently\noutperforms the previous methods on, and achieves an average BLEU of 29.4. The\nanalysis further verifies that ConST indeed closes the representation gap of\ndifferent modalities -- its learned representation improves the accuracy of\ncross-modal speech-text retrieval from 4% to 88%. Code and models are available\nat https://github.com/ReneeYe/ConST.", "published": "2022-05-05 05:14:01", "link": "http://arxiv.org/abs/2205.02444v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Introducing the Welsh Text Summarisation Dataset and Baseline Systems", "abstract": "Welsh is an official language in Wales and is spoken by an estimated 884,300\npeople (29.2% of the population of Wales). Despite this status and estimated\nincrease in speaker numbers since the last (2011) census, Welsh remains a\nminority language undergoing revitalization and promotion by Welsh Government\nand relevant stakeholders. As part of the effort to increase the availability\nof Welsh digital technology, this paper introduces the first Welsh\nsummarisation dataset, which we provide freely for research purposes to help\nadvance the work on Welsh text summarization. The dataset was created by Welsh\nspeakers by manually summarising Welsh Wikipedia articles. In addition, the\npaper discusses the implementation and evaluation of different summarisation\nsystems for Welsh. The summarization systems and results will serve as\nbenchmarks for the development of summarises in other minority language\ncontexts.", "published": "2022-05-05 10:12:45", "link": "http://arxiv.org/abs/2205.02545v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "User-Driven Research of Medical Note Generation Software", "abstract": "A growing body of work uses Natural Language Processing (NLP) methods to\nautomatically generate medical notes from audio recordings of doctor-patient\nconsultations. However, there are very few studies on how such systems could be\nused in clinical practice, how clinicians would adjust to using them, or how\nsystem design should be influenced by such considerations. In this paper, we\npresent three rounds of user studies, carried out in the context of developing\na medical note generation system. We present, analyse and discuss the\nparticipating clinicians' impressions and views of how the system ought to be\nadapted to be of value to them. Next, we describe a three-week test run of the\nsystem in a live telehealth clinical practice. Major findings include (i) the\nemergence of five different note-taking behaviours; (ii) the importance of the\nsystem generating notes in real time during the consultation; and (iii) the\nidentification of a number of clinical use cases that could prove challenging\nfor automatic note generation systems.", "published": "2022-05-05 10:18:06", "link": "http://arxiv.org/abs/2205.02549v2", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Language Models Can See: Plugging Visual Controls in Text Generation", "abstract": "Generative language models (LMs) such as GPT-2/3 can be prompted to generate\ntext with remarkable quality. While they are designed for text-prompted\ngeneration, it remains an open question how the generation process could be\nguided by modalities beyond text such as images. In this work, we propose a\ntraining-free framework, called MAGIC (iMAge-Guided text generatIon with CLIP),\nfor plugging in visual controls in the generation process and enabling LMs to\nperform multimodal tasks (e.g., image captioning) in a zero-shot manner. MAGIC\nis a simple yet efficient plug-and-play framework, which directly combines an\noff-the-shelf LM (i.e., GPT-2) and an image-text matching model (i.e., CLIP)\nfor image-grounded text generation. During decoding, MAGIC influences the\ngeneration of the LM by introducing a CLIP-induced score, called magic score,\nwhich regularizes the generated result to be semantically related to a given\nimage while being coherent to the previously generated context. Notably, the\nproposed decoding scheme does not involve any gradient update operation,\ntherefore being computationally efficient. On the challenging task of zero-shot\nimage captioning, MAGIC outperforms the state-of-the-art method by notable\nmargins with a nearly 27 times decoding speedup. MAGIC is a flexible framework\nand is theoretically compatible with any text generation tasks that incorporate\nimage grounding. In the experiments, we showcase that it is also capable of\nperforming visually grounded story generation given both an image and a text\nprompt.", "published": "2022-05-05 13:56:18", "link": "http://arxiv.org/abs/2205.02655v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "RaFoLa: A Rationale-Annotated Corpus for Detecting Indicators of Forced\n  Labour", "abstract": "Forced labour is the most common type of modern slavery, and it is\nincreasingly gaining the attention of the research and social community. Recent\nstudies suggest that artificial intelligence (AI) holds immense potential for\naugmenting anti-slavery action. However, AI tools need to be developed\ntransparently in cooperation with different stakeholders. Such tools are\ncontingent on the availability and access to domain-specific data, which are\nscarce due to the near-invisible nature of forced labour. To the best of our\nknowledge, this paper presents the first openly accessible English corpus\nannotated for multi-class and multi-label forced labour detection. The corpus\nconsists of 989 news articles retrieved from specialised data sources and\nannotated according to risk indicators defined by the International Labour\nOrganization (ILO). Each news article was annotated for two aspects: (1)\nindicators of forced labour as classification labels and (2) snippets of the\ntext that justify labelling decisions. We hope that our data set can help\npromote research on explainability for multi-class and multi-label text\nclassification. In this work, we explain our process for collecting the data\nunderpinning the proposed corpus, describe our annotation guidelines and\npresent some statistical analysis of its content. Finally, we summarise the\nresults of baseline experiments based on different variants of the\nBidirectional Encoder Representation from Transformer (BERT) model.", "published": "2022-05-05 14:43:31", "link": "http://arxiv.org/abs/2205.02684v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Diversifying Neural Dialogue Generation via Negative Distillation", "abstract": "Generative dialogue models suffer badly from the generic response problem,\nlimiting their applications to a few toy scenarios. Recently, an interesting\napproach, namely negative training, has been proposed to alleviate this problem\nby reminding the model not to generate high-frequency responses during\ntraining. However, its performance is hindered by two issues, ignoring\nlow-frequency but generic responses and bringing low-frequency but meaningless\nresponses. In this paper, we propose a novel negative training paradigm, called\nnegative distillation, to keep the model away from the undesirable generic\nresponses while avoiding the above problems. First, we introduce a negative\nteacher model that can produce query-wise generic responses, and then the\nstudent model is required to maximize the distance with multi-level negative\nknowledge. Empirical results show that our method outperforms previous negative\ntraining methods significantly.", "published": "2022-05-05 17:14:56", "link": "http://arxiv.org/abs/2205.02795v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dangling-Aware Entity Alignment with Mixed High-Order Proximities", "abstract": "We study dangling-aware entity alignment in knowledge graphs (KGs), which is\nan underexplored but important problem. As different KGs are naturally\nconstructed by different sets of entities, a KG commonly contains some dangling\nentities that cannot find counterparts in other KGs. Therefore, dangling-aware\nentity alignment is more realistic than the conventional entity alignment where\nprior studies simply ignore dangling entities. We propose a framework using\nmixed high-order proximities on dangling-aware entity alignment. Our framework\nutilizes both the local high-order proximity in a nearest neighbor subgraph and\nthe global high-order proximity in an embedding space for both dangling\ndetection and entity alignment. Extensive experiments with two evaluation\nsettings shows that our framework more precisely detects dangling entities, and\nbetter aligns matchable entities. Further investigations demonstrate that our\nframework can mitigate the hubness problem on dangling-aware entity alignment.", "published": "2022-05-05 02:39:55", "link": "http://arxiv.org/abs/2205.02406v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Assistive Recipe Editing through Critiquing", "abstract": "There has recently been growing interest in the automatic generation of\ncooking recipes that satisfy some form of dietary restrictions, thanks in part\nto the availability of online recipe data. Prior studies have used pre-trained\nlanguage models, or relied on small paired recipe data (e.g., a recipe paired\nwith a similar one that satisfies a dietary constraint). However, pre-trained\nlanguage models generate inconsistent or incoherent recipes, and paired\ndatasets are not available at scale. We address these deficiencies with\nRecipeCrit, a hierarchical denoising auto-encoder that edits recipes given\ningredient-level critiques. The model is trained for recipe completion to learn\nsemantic relationships within recipes. Our work's main innovation is our\nunsupervised critiquing module that allows users to edit recipes by interacting\nwith the predicted ingredients; the system iteratively rewrites recipes to\nsatisfy users' feedback. Experiments on the Recipe1M recipe dataset show that\nour model can more effectively edit recipes compared to strong\nlanguage-modeling baselines, creating recipes that satisfy user constraints and\nare more correct, serendipitous, coherent, and relevant as measured by human\njudges.", "published": "2022-05-05 05:52:27", "link": "http://arxiv.org/abs/2205.02454v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "COGMEN: COntextualized GNN based Multimodal Emotion recognitioN", "abstract": "Emotions are an inherent part of human interactions, and consequently, it is\nimperative to develop AI systems that understand and recognize human emotions.\nDuring a conversation involving various people, a person's emotions are\ninfluenced by the other speaker's utterances and their own emotional state over\nthe utterances. In this paper, we propose COntextualized Graph Neural Network\nbased Multimodal Emotion recognitioN (COGMEN) system that leverages local\ninformation (i.e., inter/intra dependency between speakers) and global\ninformation (context). The proposed model uses Graph Neural Network (GNN) based\narchitecture to model the complex dependencies (local and global information)\nin a conversation. Our model gives state-of-the-art (SOTA) results on IEMOCAP\nand MOSEI datasets, and detailed ablation experiments show the importance of\nmodeling information at both levels.", "published": "2022-05-05 05:54:24", "link": "http://arxiv.org/abs/2205.02455v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speaker Recognition in the Wild", "abstract": "In this paper, we propose a pipeline to find the number of speakers, as well\nas audios belonging to each of these now identified speakers in a source of\naudio data where number of speakers or speaker labels are not known a priori.\nWe used this approach as a part of our Data Preparation pipeline for Speech\nRecognition in Indic Languages\n(https://github.com/Open-Speech-EkStep/vakyansh-wav2vec2-experimentation). To\nunderstand and evaluate the accuracy of our proposed pipeline, we introduce two\nmetrics: Cluster Purity, and Cluster Uniqueness. Cluster Purity quantifies how\n\"pure\" a cluster is. Cluster Uniqueness, on the other hand, quantifies what\npercentage of clusters belong only to a single dominant speaker. We discuss\nmore on these metrics in section \\ref{sec:metrics}. Since we develop this\nutility to aid us in identifying data based on speaker IDs before training an\nAutomatic Speech Recognition (ASR) model, and since most of this data takes\nconsiderable effort to scrape, we also conclude that 98\\% of data gets mapped\nto the top 80\\% of clusters (computed by removing any clusters with less than a\nfixed number of utterances -- we do this to get rid of some very small clusters\nand use this threshold as 30), in the test set chosen.", "published": "2022-05-05 07:17:17", "link": "http://arxiv.org/abs/2205.02475v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FastRE: Towards Fast Relation Extraction with Convolutional Encoder and\n  Improved Cascade Binary Tagging Framework", "abstract": "Recent work for extracting relations from texts has achieved excellent\nperformance. However, most existing methods pay less attention to the\nefficiency, making it still challenging to quickly extract relations from\nmassive or streaming text data in realistic scenarios. The main efficiency\nbottleneck is that these methods use a Transformer-based pre-trained language\nmodel for encoding, which heavily affects the training speed and inference\nspeed. To address this issue, we propose a fast relation extraction model\n(FastRE) based on convolutional encoder and improved cascade binary tagging\nframework. Compared to previous work, FastRE employs several innovations to\nimprove efficiency while also keeping promising performance. Concretely, FastRE\nadopts a novel convolutional encoder architecture combined with dilated\nconvolution, gated unit and residual connection, which significantly reduces\nthe computation cost of training and inference, while maintaining the\nsatisfactory performance. Moreover, to improve the cascade binary tagging\nframework, FastRE first introduces a type-relation mapping mechanism to\naccelerate tagging efficiency and alleviate relation redundancy, and then\nutilizes a position-dependent adaptive thresholding strategy to obtain higher\ntagging accuracy and better model generalization. Experimental results\ndemonstrate that FastRE is well balanced between efficiency and performance,\nand achieves 3-10x training speed, 7-15x inference speed faster, and 1/100\nparameters compared to the state-of-the-art models, while the performance is\nstill competitive.", "published": "2022-05-05 07:59:51", "link": "http://arxiv.org/abs/2205.02490v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "One Size Does Not Fit All: The Case for Personalised Word Complexity\n  Models", "abstract": "Complex Word Identification (CWI) aims to detect words within a text that a\nreader may find difficult to understand. It has been shown that CWI systems can\nimprove text simplification, readability prediction and vocabulary acquisition\nmodelling. However, the difficulty of a word is a highly idiosyncratic notion\nthat depends on a reader's first language, proficiency and reading experience.\nIn this paper, we show that personal models are best when predicting word\ncomplexity for individual readers. We use a novel active learning framework\nthat allows models to be tailored to individuals and release a dataset of\ncomplexity annotations and models as a benchmark for further research.", "published": "2022-05-05 10:53:31", "link": "http://arxiv.org/abs/2205.02564v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Natural Language Inference with Self-Attention for Veracity Assessment\n  of Pandemic Claims", "abstract": "We present a comprehensive work on automated veracity assessment from dataset\ncreation to developing novel methods based on Natural Language Inference (NLI),\nfocusing on misinformation related to the COVID-19 pandemic. We first describe\nthe construction of the novel PANACEA dataset consisting of heterogeneous\nclaims on COVID-19 and their respective information sources. The dataset\nconstruction includes work on retrieval techniques and similarity measurements\nto ensure a unique set of claims. We then propose novel techniques for\nautomated veracity assessment based on Natural Language Inference including\ngraph convolutional networks and attention based approaches. We have carried\nout experiments on evidence retrieval and veracity assessment on the dataset\nusing the proposed techniques and found them competitive with SOTA methods, and\nprovided a detailed discussion.", "published": "2022-05-05 12:11:31", "link": "http://arxiv.org/abs/2205.02596v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Quantifying Language Variation Acoustically with Few Resources", "abstract": "Deep acoustic models represent linguistic information based on massive\namounts of data. Unfortunately, for regional languages and dialects such\nresources are mostly not available. However, deep acoustic models might have\nlearned linguistic information that transfers to low-resource languages. In\nthis study, we evaluate whether this is the case through the task of\ndistinguishing low-resource (Dutch) regional varieties. By extracting\nembeddings from the hidden layers of various wav2vec 2.0 models (including new\nmodels which are pre-trained and/or fine-tuned on Dutch) and using dynamic time\nwarping, we compute pairwise pronunciation differences averaged over 10 words\nfor over 100 individual dialects from four (regional) languages. We then\ncluster the resulting difference matrix in four groups and compare these to a\ngold standard, and a partitioning on the basis of comparing phonetic\ntranscriptions. Our results show that acoustic models outperform the\n(traditional) transcription-based approach without requiring phonetic\ntranscriptions, with the best performance achieved by the multilingual XLSR-53\nmodel fine-tuned on Dutch. On the basis of only six seconds of speech, the\nresulting clustering closely matches the gold standard.", "published": "2022-05-05 15:00:56", "link": "http://arxiv.org/abs/2205.02694v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Ontology Reuse: the Real Test of Ontological Design", "abstract": "Reusing ontologies in practice is still very challenging, especially when\nmultiple ontologies are (jointly) involved. Moreover, despite recent advances,\nthe realization of systematic ontology quality assurance remains a difficult\nproblem. In this work, the quality of thirty biomedical ontologies, and the\nComputer Science Ontology are investigated, from the perspective of a practical\nuse case. Special scrutiny is given to cross-ontology references, which are\nvital for combining ontologies. Diverse methods to detect potential issues are\nproposed, including natural language processing and network analysis. Moreover,\nseveral suggestions for improving ontologies and their quality assurance\nprocesses are presented. It is argued that while the advancing automatic tools\nfor ontology quality assurance are crucial for ontology improvement, they will\nnot solve the problem entirely. It is ontology reuse that is the ultimate\nmethod for continuously verifying and improving ontology quality, as well as\nfor guiding its future development. Specifically, multiple issues can be found\nand fixed primarily through practical and diverse ontology reuse scenarios.", "published": "2022-05-05 19:13:37", "link": "http://arxiv.org/abs/2205.02892v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Interactive Model Cards: A Human-Centered Approach to Model\n  Documentation", "abstract": "Deep learning models for natural language processing (NLP) are increasingly\nadopted and deployed by analysts without formal training in NLP or machine\nlearning (ML). However, the documentation intended to convey the model's\ndetails and appropriate use is tailored primarily to individuals with ML or NLP\nexpertise. To address this gap, we conduct a design inquiry into interactive\nmodel cards, which augment traditionally static model cards with affordances\nfor exploring model documentation and interacting with the models themselves.\nOur investigation consists of an initial conceptual study with experts in ML,\nNLP, and AI Ethics, followed by a separate evaluative study with non-expert\nanalysts who use ML models in their work. Using a semi-structured interview\nformat coupled with a think-aloud protocol, we collected feedback from a total\nof 30 participants who engaged with different versions of standard and\ninteractive model cards. Through a thematic analysis of the collected data, we\nidentified several conceptual dimensions that summarize the strengths and\nlimitations of standard and interactive model cards, including: stakeholders;\ndesign; guidance; understandability & interpretability; sensemaking &\nskepticism; and trust & safety. Our findings demonstrate the importance of\ncarefully considered design and interactivity for orienting and supporting\nnon-expert analysts using deep learning models, along with a need for\nconsideration of broader sociotechnical contexts and organizational dynamics.\nWe have also identified design elements, such as language, visual cues, and\nwarnings, among others, that support interactivity and make non-interactive\ncontent accessible. We summarize our findings as design guidelines and discuss\ntheir implications for a human-centered approach towards AI/ML documentation.", "published": "2022-05-05 19:19:28", "link": "http://arxiv.org/abs/2205.02894v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "68T01"], "primary_category": "cs.HC"}
{"title": "Conversational Analysis of Daily Dialog Data using Polite Emotional\n  Dialogue Acts", "abstract": "Many socio-linguistic cues are used in conversational analysis, such as\nemotion, sentiment, and dialogue acts. One of the fundamental cues is\npoliteness, which linguistically possesses properties such as social manners\nuseful in conversational analysis. This article presents findings of polite\nemotional dialogue act associations, where we can correlate the relationships\nbetween the socio-linguistic cues. We confirm our hypothesis that the\nutterances with the emotion classes Anger and Disgust are more likely to be\nimpolite. At the same time, Happiness and Sadness are more likely to be polite.\nA less expectable phenomenon occurs with dialogue acts Inform and Commissive\nwhich contain more polite utterances than Question and Directive. Finally, we\nconclude on the future work of these findings to extend the learning of social\nbehaviours using politeness.", "published": "2022-05-05 21:03:47", "link": "http://arxiv.org/abs/2205.02921v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Region-to-region kernel interpolation of acoustic transfer function with\n  directional weighting", "abstract": "A method of interpolating the acoustic transfer function (ATF) between\nregions that takes into account both the physical properties of the ATF and the\ndirectionality of region configurations is proposed. Most spatial ATF\ninterpolation methods are limited to estimation in the region of receivers. A\nkernel method for region-to-region ATF interpolation makes it possible to\nestimate the ATFs for both source and receiver regions from a discrete set of\nATF measurements. We newly formulate the reproducing kernel Hilbert space and\nassociated kernel function incorporating directional weight to enhance the\ninterpolation accuracy. We also investigate hyperparameter optimization methods\nfor this kernel function. Numerical experiments indicate that the proposed\nmethod outperforms the method without the use of directional weighting.", "published": "2022-05-05 16:30:13", "link": "http://arxiv.org/abs/2205.02750v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "M2R2: Missing-Modality Robust emotion Recognition framework with\n  iterative data augmentation", "abstract": "This paper deals with the utterance-level modalities missing problem with\nuncertain patterns on emotion recognition in conversation (ERC) task. Present\nmodels generally predict the speaker's emotions by its current utterance and\ncontext, which is degraded by modality missing considerably. Our work proposes\na framework Missing-Modality Robust emotion Recognition (M2R2), which trains\nemotion recognition model with iterative data augmentation by learned common\nrepresentation. Firstly, a network called Party Attentive Network (PANet) is\ndesigned to classify emotions, which tracks all the speakers' states and\ncontext. Attention mechanism between speaker with other participants and\ndialogue topic is used to decentralize dependence on multi-time and multi-party\nutterances instead of the possible incomplete one. Moreover, the Common\nRepresentation Learning (CRL) problem is defined for modality-missing problem.\nData imputation methods improved by the adversarial strategy are used here to\nconstruct extra features to augment data. Extensive experiments and case\nstudies validate the effectiveness of our methods over baselines for\nmodality-missing emotion recognition on two different datasets.", "published": "2022-05-05 09:16:31", "link": "http://arxiv.org/abs/2205.02524v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound Event Classification in an Industrial Environment: Pipe Leakage\n  Detection Use Case", "abstract": "In this work, a multi-stage Machine Learning (ML) pipeline is proposed for\npipe leakage detection in an industrial environment. As opposed to other\nindustrial and urban environments, the environment under study includes many\ninterfering background noises, complicating the identification of leaks.\nFurthermore, the harsh environmental conditions limit the amount of data\ncollected and impose the use of low-complexity algorithms. To address the\nenvironment's constraints, the developed ML pipeline applies multiple steps,\neach addressing the environment's challenges. The proposed ML pipeline first\nreduces the data dimensionality by feature selection techniques and then\nincorporates time correlations by extracting time-based features. The resultant\nfeatures are fed to a Support Vector Machine (SVM) of low-complexity that\ngeneralizes well to a small amount of data. An extensive experimental procedure\nwas carried out on two datasets, one with background industrial noise and one\nwithout, to evaluate the validity of the proposed pipeline. The SVM\nhyper-parameters and parameters specific to the pipeline steps were tuned as\npart of the experimental procedure. The best models obtained from the dataset\nwith industrial noise and leaks were applied to datasets without noise and with\nand without leaks to test their generalizability. The results show that the\nmodel produces excellent results with 99\\% accuracy and an F1-score of 0.93 and\n0.9 for the respective datasets.", "published": "2022-05-05 15:26:22", "link": "http://arxiv.org/abs/2205.02706v1", "categories": ["cs.LG", "cs.NE", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
