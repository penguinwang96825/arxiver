{"title": "PolicyGPT: Automated Analysis of Privacy Policies with Large Language\n  Models", "abstract": "Privacy policies serve as the primary conduit through which online service\nproviders inform users about their data collection and usage procedures.\nHowever, in a bid to be comprehensive and mitigate legal risks, these policy\ndocuments are often quite verbose. In practical use, users tend to click the\nAgree button directly rather than reading them carefully. This practice exposes\nusers to risks of privacy leakage and legal issues. Recently, the advent of\nLarge Language Models (LLM) such as ChatGPT and GPT-4 has opened new\npossibilities for text analysis, especially for lengthy documents like privacy\npolicies. In this study, we investigate a privacy policy text analysis\nframework PolicyGPT based on the LLM. This framework was tested using two\ndatasets. The first dataset comprises of privacy policies from 115 websites,\nwhich were meticulously annotated by legal experts, categorizing each segment\ninto one of 10 classes. The second dataset consists of privacy policies from\n304 popular mobile applications, with each sentence manually annotated and\nclassified into one of another 10 categories. Under zero-shot learning\nconditions, PolicyGPT demonstrated robust performance. For the first dataset,\nit achieved an accuracy rate of 97%, while for the second dataset, it attained\nan 87% accuracy rate, surpassing that of the baseline machine learning and\nneural network models.", "published": "2023-09-19 01:22:42", "link": "http://arxiv.org/abs/2309.10238v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixed-Distil-BERT: Code-mixed Language Modeling for Bangla, English, and\n  Hindi", "abstract": "One of the most popular downstream tasks in the field of Natural Language\nProcessing is text classification. Text classification tasks have become more\ndaunting when the texts are code-mixed. Though they are not exposed to such\ntext during pre-training, different BERT models have demonstrated success in\ntackling Code-Mixed NLP challenges. Again, in order to enhance their\nperformance, Code-Mixed NLP models have depended on combining synthetic data\nwith real-world data. It is crucial to understand how the BERT models'\nperformance is impacted when they are pretrained using corresponding code-mixed\nlanguages. In this paper, we introduce Tri-Distil-BERT, a multilingual model\npre-trained on Bangla, English, and Hindi, and Mixed-Distil-BERT, a model\nfine-tuned on code-mixed data. Both models are evaluated across multiple NLP\ntasks and demonstrate competitive performance against larger models like mBERT\nand XLM-R. Our two-tiered pre-training approach offers efficient alternatives\nfor multilingual and code-mixed language understanding, contributing to\nadvancements in the field.", "published": "2023-09-19 02:59:41", "link": "http://arxiv.org/abs/2309.10272v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Baichuan 2: Open Large-scale Language Models", "abstract": "Large language models (LLMs) have demonstrated remarkable performance on a\nvariety of natural language tasks based on just a few examples of natural\nlanguage instructions, reducing the need for extensive feature engineering.\nHowever, most powerful LLMs are closed-source or limited in their capability\nfor languages other than English. In this technical report, we present Baichuan\n2, a series of large-scale multilingual language models containing 7 billion\nand 13 billion parameters, trained from scratch, on 2.6 trillion tokens.\nBaichuan 2 matches or outperforms other open-source models of similar size on\npublic benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan\n2 excels in vertical domains such as medicine and law. We will release all\npre-training model checkpoints to benefit the research community in better\nunderstanding the training dynamics of Baichuan 2.", "published": "2023-09-19 04:13:22", "link": "http://arxiv.org/abs/2309.10305v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rigorously Assessing Natural Language Explanations of Neurons", "abstract": "Natural language is an appealing medium for explaining how large language\nmodels process and store information, but evaluating the faithfulness of such\nexplanations is challenging. To help address this, we develop two modes of\nevaluation for natural language explanations that claim individual neurons\nrepresent a concept in a text input. In the observational mode, we evaluate\nclaims that a neuron $a$ activates on all and only input strings that refer to\na concept picked out by the proposed explanation $E$. In the intervention mode,\nwe construe $E$ as a claim that the neuron $a$ is a causal mediator of the\nconcept denoted by $E$. We apply our framework to the GPT-4-generated\nexplanations of GPT-2 XL neurons of Bills et al. (2023) and show that even the\nmost confident explanations have high error rates and little to no causal\nefficacy. We close the paper by critically assessing whether natural language\nis a good choice for explanations and whether neurons are the best level of\nanalysis.", "published": "2023-09-19 04:49:45", "link": "http://arxiv.org/abs/2309.10312v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KoBigBird-large: Transformation of Transformer for Korean Language\n  Understanding", "abstract": "This work presents KoBigBird-large, a large size of Korean BigBird that\nachieves state-of-the-art performance and allows long sequence processing for\nKorean language understanding. Without further pretraining, we only transform\nthe architecture and extend the positional encoding with our proposed Tapered\nAbsolute Positional Encoding Representations (TAPER). In experiments,\nKoBigBird-large shows state-of-the-art overall performance on Korean language\nunderstanding benchmarks and the best performance on document classification\nand question answering tasks for longer sequences against the competitive\nbaseline models. We publicly release our model here.", "published": "2023-09-19 05:48:57", "link": "http://arxiv.org/abs/2309.10339v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompt, Condition, and Generate: Classification of Unsupported Claims\n  with In-Context Learning", "abstract": "Unsupported and unfalsifiable claims we encounter in our daily lives can\ninfluence our view of the world. Characterizing, summarizing, and -- more\ngenerally -- making sense of such claims, however, can be challenging. In this\nwork, we focus on fine-grained debate topics and formulate a new task of\ndistilling, from such claims, a countable set of narratives. We present a\ncrowdsourced dataset of 12 controversial topics, comprising more than 120k\narguments, claims, and comments from heterogeneous sources, each annotated with\na narrative label. We further investigate how large language models (LLMs) can\nbe used to synthesise claims using In-Context Learning. We find that generated\nclaims with supported evidence can be used to improve the performance of\nnarrative classification models and, additionally, that the same model can\ninfer the stance and aspect using a few training examples. Such a model can be\nuseful in applications which rely on narratives , e.g. fact-checking.", "published": "2023-09-19 06:42:37", "link": "http://arxiv.org/abs/2309.10359v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PICK: Polished & Informed Candidate Scoring for Knowledge-Grounded\n  Dialogue Systems", "abstract": "Grounding dialogue response generation on external knowledge is proposed to\nproduce informative and engaging responses. However, current knowledge-grounded\ndialogue (KGD) systems often fail to align the generated responses with\nhuman-preferred qualities due to several issues like hallucination and the lack\nof coherence. Upon analyzing multiple language model generations, we observe\nthe presence of alternative generated responses within a single decoding\nprocess. These alternative responses are more faithful and exhibit a comparable\nor higher level of relevance to prior conversational turns compared to the\noptimal responses prioritized by the decoding processes. To address these\nchallenges and driven by these observations, we propose Polished \\& Informed\nCandidate Scoring (PICK), a generation re-scoring framework that empowers\nmodels to generate faithful and relevant responses without requiring additional\nlabeled data or model tuning. Through comprehensive automatic and human\nevaluations, we demonstrate the effectiveness of PICK in generating responses\nthat are more faithful while keeping them relevant to the dialogue history.\nFurthermore, PICK consistently improves the system's performance with both\noracle and retrieved knowledge in all decoding strategies. We provide the\ndetailed implementation in https://github.com/bryanwilie/pick .", "published": "2023-09-19 08:27:09", "link": "http://arxiv.org/abs/2309.10413v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Open-Domain Table Question Answering via Syntax- and\n  Structure-aware Dense Retrieval", "abstract": "Open-domain table question answering aims to provide answers to a question by\nretrieving and extracting information from a large collection of tables.\nExisting studies of open-domain table QA either directly adopt text retrieval\nmethods or consider the table structure only in the encoding layer for table\nretrieval, which may cause syntactical and structural information loss during\ntable scoring. To address this issue, we propose a syntax- and structure-aware\nretrieval method for the open-domain table QA task. It provides syntactical\nrepresentations for the question and uses the structural header and value\nrepresentations for the tables to avoid the loss of fine-grained syntactical\nand structural information. Then, a syntactical-to-structural aggregator is\nused to obtain the matching score between the question and a candidate table by\nmimicking the human retrieval process. Experimental results show that our\nmethod achieves the state-of-the-art on the NQ-tables dataset and overwhelms\nstrong baselines on a newly curated open-domain Text-to-SQL dataset.", "published": "2023-09-19 10:40:09", "link": "http://arxiv.org/abs/2309.10506v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NSOAMT -- New Search Only Approach to Machine Translation", "abstract": "Translation automation mechanisms and tools have been developed for several\nyears to bring people who speak different languages together. A \"new search\nonly approach to machine translation\" was adopted to tackle some of the\nslowness and inaccuracy of the other technologies. The idea is to develop a\nsolution that, by indexing an incremental set of words that combine a certain\nsemantic meaning, makes it possible to create a process of correspondence\nbetween their native language record and the language of translation. This\nresearch principle assumes that the vocabulary used in a given type of\npublication/document is relatively limited in terms of language style and word\ndiversity, which enhances the greater effect of instantaneously and rigor in\nthe translation process through the indexing process. A volume of electronic\ntext documents where processed and loaded into a database, and analyzed and\nmeasured in order confirm the previous premise. Although the observed and\nprojected metric values did not give encouraging results, it was possible to\ndevelop and make available a translation tool using this approach.", "published": "2023-09-19 11:12:21", "link": "http://arxiv.org/abs/2309.10526v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FRACAS: A FRench Annotated Corpus of Attribution relations in newS", "abstract": "Quotation extraction is a widely useful task both from a sociological and\nfrom a Natural Language Processing perspective. However, very little data is\navailable to study this task in languages other than English. In this paper, we\npresent a manually annotated corpus of 1676 newswire texts in French for\nquotation extraction and source attribution. We first describe the composition\nof our corpus and the choices that were made in selecting the data. We then\ndetail the annotation guidelines and annotation process, as well as a few\nstatistics about the final corpus and the obtained balance between quote types\n(direct, indirect and mixed, which are particularly challenging). We end by\ndetailing our inter-annotator agreement between the 8 annotators who worked on\nmanual labelling, which is substantially high for such a difficult linguistic\nphenomenon.", "published": "2023-09-19 13:19:54", "link": "http://arxiv.org/abs/2309.10604v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Medical Dialogue Generation with Abstract Meaning\n  Representations", "abstract": "Medical Dialogue Generation serves a critical role in telemedicine by\nfacilitating the dissemination of medical expertise to patients. Existing\nstudies focus on incorporating textual representations, which have limited\ntheir ability to represent the semantics of text, such as ignoring important\nmedical entities. To enhance the model's understanding of the textual semantics\nand the medical knowledge including entities and relations, we introduce the\nuse of Abstract Meaning Representations (AMR) to construct graphical\nrepresentations that delineate the roles of language constituents and medical\nentities within the dialogues. In this paper, We propose a novel framework that\nmodels dialogues between patients and healthcare professionals using AMR\ngraphs, where the neural networks incorporate textual and graphical knowledge\nwith a dual attention mechanism. Experimental results show that our framework\noutperforms strong baseline models in medical dialogue generation,\ndemonstrating the effectiveness of AMR graphs in enhancing the representations\nof medical knowledge and logical relationships. Furthermore, to support future\nresearch in this domain, we provide the corresponding source code at\nhttps://github.com/Bernard-Yang/MedDiaAMR.", "published": "2023-09-19 13:31:49", "link": "http://arxiv.org/abs/2309.10608v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model\n  Pre-trained from Scratch", "abstract": "Large language models (LLMs) with billions of parameters have demonstrated\noutstanding performance on various natural language processing tasks. This\nreport presents OpenBA, an open-sourced 15B bilingual asymmetric seq2seq model,\nto contribute an LLM variant to the Chinese-oriented open-source model\ncommunity. We enhance OpenBA with effective and efficient techniques as well as\nadopt a three-stage training strategy to train the model from scratch. Our\nsolution can also achieve very competitive performance with only 380B tokens,\nwhich is better than LLaMA-70B on the BELEBELE benchmark, BLOOM-176B on the\nMMLU benchmark, GLM-130B on the C-Eval (hard) benchmark. This report provides\nthe main details to pre-train an analogous model, including pre-training data\nprocessing, Bilingual Flan data collection, the empirical observations that\ninspire our model architecture design, training objectives of different stages,\nand other enhancement techniques. Additionally, we also provide the fine-tuning\ndetails of OpenBA on four downstream tasks. We have refactored our code to\nfollow the design principles of the Huggingface Transformers Library, making it\nmore convenient for developers to use, and released checkpoints of different\ntraining stages at https://huggingface.co/openBA. More details of our project\nare available at https://github.com/OpenNLG/openBA.git.", "published": "2023-09-19 15:46:40", "link": "http://arxiv.org/abs/2309.10706v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Embedded Programs for Hybrid Language Symbolic\n  Reasoning", "abstract": "How can we perform computations over natural language representations to\nsolve tasks that require symbolic and numeric reasoning? We propose natural\nlanguage embedded programs (NLEP) as a unifying framework for addressing\nmath/symbolic reasoning, natural language understanding, and instruction\nfollowing tasks. Our approach prompts a language model to generate full Python\nprograms that define functions over data structures which contain natural\nlanguage representations of structured knowledge. A Python interpreter then\nexecutes the generated code and prints the output. Despite using a task-general\nprompt, we find that this approach can improve upon strong baselines across a\nrange of different tasks including math and symbolic reasoning, text\nclassification, question answering, and instruction following. We found that\nthe generated programs are interpretable since they outline the exact reasoning\nprocess followed by the program interpreter.", "published": "2023-09-19 17:54:21", "link": "http://arxiv.org/abs/2309.10814v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RedPenNet for Grammatical Error Correction: Outputs to Tokens,\n  Attentions to Spans", "abstract": "The text editing tasks, including sentence fusion, sentence splitting and\nrephrasing, text simplification, and Grammatical Error Correction (GEC), share\na common trait of dealing with highly similar input and output sequences. This\narea of research lies at the intersection of two well-established fields: (i)\nfully autoregressive sequence-to-sequence approaches commonly used in tasks\nlike Neural Machine Translation (NMT) and (ii) sequence tagging techniques\ncommonly used to address tasks such as Part-of-speech tagging, Named-entity\nrecognition (NER), and similar. In the pursuit of a balanced architecture,\nresearchers have come up with numerous imaginative and unconventional\nsolutions, which we're discussing in the Related Works section. Our approach to\naddressing text editing tasks is called RedPenNet and is aimed at reducing\narchitectural and parametric redundancies presented in specific\nSequence-To-Edits models, preserving their semi-autoregressive advantages. Our\nmodels achieve $F_{0.5}$ scores of 77.60 on the BEA-2019 (test), which can be\nconsidered as state-of-the-art the only exception for system combination and\n67.71 on the UAGEC+Fluency (test) benchmarks.\n  This research is being conducted in the context of the UNLP 2023 workshop,\nwhere it was presented as a paper as a paper for the Shared Task in Grammatical\nError Correction (GEC) for Ukrainian. This study aims to apply the RedPenNet\napproach to address the GEC problem in the Ukrainian language.", "published": "2023-09-19 19:48:30", "link": "http://arxiv.org/abs/2309.10898v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Specializing Small Language Models towards Complex Style Transfer via\n  Latent Attribute Pre-Training", "abstract": "In this work, we introduce the concept of complex text style transfer tasks,\nand constructed complex text datasets based on two widely applicable scenarios.\nOur dataset is the first large-scale data set of its kind, with 700 rephrased\nsentences and 1,000 sentences from the game Genshin Impact. While large\nlanguage models (LLM) have shown promise in complex text style transfer, they\nhave drawbacks such as data privacy concerns, network instability, and high\ndeployment costs. To address these issues, we explore the effectiveness of\nsmall models (less than T5-3B) with implicit style pre-training through\ncontrastive learning. We also propose a method for automated evaluation of text\ngeneration quality based on alignment with human evaluations using ChatGPT.\nFinally, we compare our approach with existing methods and show that our model\nachieves state-of-art performances of few-shot text style transfer models.", "published": "2023-09-19 21:01:40", "link": "http://arxiv.org/abs/2309.10929v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Family of Pretrained Transformer Language Models for Russian", "abstract": "Transformer language models (LMs) are fundamental to NLP research\nmethodologies and applications in various languages. However, developing such\nmodels specifically for the Russian language has received little attention.\nThis paper introduces a collection of 13 Russian Transformer LMs, which spans\nencoder (ruBERT, ruRoBERTa, ruELECTRA), decoder (ruGPT-3), and encoder-decoder\n(ruT5, FRED-T5) architectures. We provide a report on the model architecture\ndesign and pretraining, and the results of evaluating their generalization\nabilities on Russian language understanding and generation datasets and\nbenchmarks. By pretraining and releasing these specialized Transformer LMs, we\naim to broaden the scope of the NLP research directions and enable the\ndevelopment of industrial solutions for the Russian language.", "published": "2023-09-19 21:07:52", "link": "http://arxiv.org/abs/2309.10931v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MBR and QE Finetuning: Training-time Distillation of the Best and Most\n  Expensive Decoding Methods", "abstract": "Recent research in decoding methods for Natural Language Generation (NLG)\ntasks has shown that MAP decoding is not optimal, because model probabilities\ndo not always align with human preferences. Stronger decoding methods,\nincluding Quality Estimation (QE) reranking and Minimum Bayes' Risk (MBR)\ndecoding, have since been proposed to mitigate the model-perplexity-vs-quality\nmismatch. While these decoding methods achieve state-of-the-art performance,\nthey are prohibitively expensive to compute. In this work, we propose MBR\nfinetuning and QE finetuning which distill the quality gains from these\ndecoding methods at training time, while using an efficient decoding algorithm\nat inference time. Using the canonical NLG task of Neural Machine Translation\n(NMT), we show that even with self-training, these finetuning methods\nsignificantly outperform the base model. Moreover, when using an external LLM\nas a teacher model, these finetuning methods outperform finetuning on\nhuman-generated references. These findings suggest new ways to leverage\nmonolingual data to achieve improvements in model quality that are on par with,\nor even exceed, improvements from human-curated data, while maintaining maximum\nefficiency during decoding.", "published": "2023-09-19 23:39:07", "link": "http://arxiv.org/abs/2309.10966v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QASnowball: An Iterative Bootstrapping Framework for High-Quality\n  Question-Answering Data Generation", "abstract": "Recent years have witnessed the success of question answering (QA),\nespecially its potential to be a foundation paradigm for tackling diverse NLP\ntasks. However, obtaining sufficient data to build an effective and stable QA\nsystem still remains an open problem. For this problem, we introduce an\niterative bootstrapping framework for QA data augmentation (named QASnowball),\nwhich can iteratively generate large-scale high-quality QA data based on a seed\nset of supervised examples. Specifically, QASnowball consists of three modules,\nan answer extractor to extract core phrases in unlabeled documents as candidate\nanswers, a question generator to generate questions based on documents and\ncandidate answers, and a QA data filter to filter out high-quality QA data.\nMoreover, QASnowball can be self-enhanced by reseeding the seed set to\nfine-tune itself in different iterations, leading to continual improvements in\nthe generation quality. We conduct experiments in the high-resource English\nscenario and the medium-resource Chinese scenario, and the experimental results\nshow that the data generated by QASnowball can facilitate QA models: (1)\ntraining models on the generated data achieves comparable results to using\nsupervised data, and (2) pre-training on the generated data and fine-tuning on\nsupervised data can achieve better performance. Our code and generated data\nwill be released to advance further work.", "published": "2023-09-19 05:20:36", "link": "http://arxiv.org/abs/2309.10326v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PoSE: Efficient Context Window Extension of LLMs via Positional\n  Skip-wise Training", "abstract": "Large Language Models (LLMs) are trained with a pre-defined context length,\nrestricting their use in scenarios requiring long inputs. Previous efforts for\nadapting LLMs to a longer length usually requires fine-tuning with this target\nlength (Full-length fine-tuning), suffering intensive training cost. To\ndecouple train length from target length for efficient context window\nextension, we propose Positional Skip-wisE (PoSE) training that smartly\nsimulates long inputs using a fixed context window. This is achieved by first\ndividing the original context window into several chunks, then designing\ndistinct skipping bias terms to manipulate the position indices of each chunk.\nThese bias terms and the lengths of each chunk are altered for every training\nexample, allowing the model to adapt to all positions within target length.\nExperimental results show that PoSE greatly reduces memory and time overhead\ncompared with Full-length fine-tuning, with minimal impact on performance.\nLeveraging this advantage, we have successfully extended the LLaMA model to\n128k tokens using a 2k training context window. Furthermore, we empirically\nconfirm that PoSE is compatible with all RoPE-based LLMs and position\ninterpolation strategies. Notably, our method can potentially support infinite\nlength, limited only by memory usage in inference. With ongoing progress for\nefficient inference, we believe PoSE can further scale the context window\nbeyond 128k.", "published": "2023-09-19 08:03:38", "link": "http://arxiv.org/abs/2309.10400v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Writer-Defined AI Personas for On-Demand Feedback Generation", "abstract": "Compelling writing is tailored to its audience. This is challenging, as\nwriters may struggle to empathize with readers, get feedback in time, or gain\naccess to the target group. We propose a concept that generates on-demand\nfeedback, based on writer-defined AI personas of any target audience. We\nexplore this concept with a prototype (using GPT-3.5) in two user studies (N=5\nand N=11): Writers appreciated the concept and strategically used personas for\ngetting different perspectives. The feedback was seen as helpful and inspired\nrevisions of text and personas, although it was often verbose and unspecific.\nWe discuss the impact of on-demand feedback, the limited representativity of\ncontemporary AI systems, and further ideas for defining AI personas. This work\ncontributes to the vision of supporting writers with AI by expanding the\nsocio-technical perspective in AI tool design: To empower creators, we also\nneed to keep in mind their relationship to an audience.", "published": "2023-09-19 08:49:35", "link": "http://arxiv.org/abs/2309.10433v2", "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "primary_category": "cs.HC"}
{"title": "Reformulating Sequential Recommendation: Learning Dynamic User Interest\n  with Content-enriched Language Modeling", "abstract": "Recommender systems are indispensable in the realm of online applications,\nand sequential recommendation has enjoyed considerable prevalence due to its\ncapacity to encapsulate the dynamic shifts in user interests. However, previous\nsequential modeling methods still have limitations in capturing contextual\ninformation. The primary reason is the lack of understanding of domain-specific\nknowledge and item-related textual content. Fortunately, the emergence of\npowerful language models has unlocked the potential to incorporate extensive\nworld knowledge into recommendation algorithms, enabling them to go beyond\nsimple item attributes and truly understand the world surrounding user\npreferences. To achieve this, we propose LANCER, which leverages the semantic\nunderstanding capabilities of pre-trained language models to generate\npersonalized recommendations. Our approach bridges the gap between language\nmodels and recommender systems, resulting in more human-like recommendations.\nWe demonstrate the effectiveness of our approach through a series of\nexperiments conducted on multiple benchmark datasets, showing promising results\nand providing valuable insights into the influence of our model on sequential\nrecommendation tasks. Furthermore, our experimental codes are publicly\navailable at https://github.com/Gnimixy/lancer.", "published": "2023-09-19 08:54:47", "link": "http://arxiv.org/abs/2309.10435v4", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Exploring Iterative Enhancement for Improving Learnersourced\n  Multiple-Choice Question Explanations with Large Language Models", "abstract": "Large language models exhibit superior capabilities in processing and\nunderstanding language, yet their applications in educational contexts remain\nunderexplored. Learnersourcing enhances learning by engaging students in\ncreating their own educational content. When learnersourcing multiple-choice\nquestions, creating explanations for the solution of a question is a crucial\nstep; it helps other students understand the solution and promotes a deeper\nunderstanding of related concepts. However, it is often difficult for students\nto craft effective solution explanations, due to limited subject understanding.\nTo help scaffold the task of automated explanation generation, we present and\nevaluate a framework called \"ILearner-LLM\", that iteratively enhances the\ngenerated explanations for the given questions with large language models.\nComprising an explanation generation model and an explanation evaluation model,\nthe framework generates high-quality student-aligned explanations by\niteratively feeding the quality rating score from the evaluation model back\ninto the instruction prompt of the explanation generation model. Experimental\nresults demonstrate the effectiveness of our ILearner-LLM on LLaMA2-13B and\nGPT-4 to generate higher quality explanations that are closer to those written\nby students on five PeerWise datasets. Our findings represent a promising path\nto enrich the learnersourcing experience for students and to enhance the\ncapabilities of large language models for educational applications.", "published": "2023-09-19 09:04:15", "link": "http://arxiv.org/abs/2309.10444v5", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Toward Unified Controllable Text Generation via Regular Expression\n  Instruction", "abstract": "Controllable text generation is a fundamental aspect of natural language\ngeneration, with numerous methods proposed for different constraint types.\nHowever, these approaches often require significant architectural or decoding\nmodifications, making them challenging to apply to additional constraints or\nresolve different constraint combinations. To address this, our paper\nintroduces Regular Expression Instruction (REI), which utilizes an\ninstruction-based mechanism to fully exploit regular expressions' advantages to\nuniformly model diverse constraints. Specifically, our REI supports all popular\nfine-grained controllable generation constraints, i.e., lexical, positional,\nand length, as well as their complex combinations, via regular expression-style\ninstructions. Our method only requires fine-tuning on medium-scale language\nmodels or few-shot, in-context learning on large language models, and requires\nno further adjustment when applied to various constraint combinations.\nExperiments demonstrate that our straightforward approach yields high success\nrates and adaptability to various constraints while maintaining competitiveness\nin automatic metrics and outperforming most previous baselines.", "published": "2023-09-19 09:05:14", "link": "http://arxiv.org/abs/2309.10447v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Evaluation of GPT-4 on the ETHICS Dataset", "abstract": "This report summarizes a short study of the performance of GPT-4 on the\nETHICS dataset. The ETHICS dataset consists of five sub-datasets covering\ndifferent fields of ethics: Justice, Deontology, Virtue Ethics, Utilitarianism,\nand Commonsense Ethics. The moral judgments were curated so as to have a high\ndegree of agreement with the aim of representing shared human values rather\nthan moral dilemmas. GPT-4's performance is much better than that of previous\nmodels and suggests that learning to work with common human values is not the\nhard problem for AI ethics.", "published": "2023-09-19 10:01:50", "link": "http://arxiv.org/abs/2309.10492v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OpenMSD: Towards Multilingual Scientific Documents Similarity\n  Measurement", "abstract": "We develop and evaluate multilingual scientific documents similarity\nmeasurement models in this work. Such models can be used to find related works\nin different languages, which can help multilingual researchers find and\nexplore papers more efficiently. We propose the first multilingual scientific\ndocuments dataset, Open-access Multilingual Scientific Documents (OpenMSD),\nwhich has 74M papers in 103 languages and 778M citation pairs. With OpenMSD, we\npretrain science-specialized language models, and explore different strategies\nto derive \"related\" paper pairs to fine-tune the models, including using a\nmixture of citation, co-citation, and bibliographic-coupling pairs. To further\nimprove the models' performance for non-English papers, we explore the use of\ngenerative language models to enrich the non-English papers with English\nsummaries. This allows us to leverage the models' English capabilities to\ncreate better representations for non-English papers. Our best model\nsignificantly outperforms strong baselines by 7-16% (in mean average\nprecision).", "published": "2023-09-19 11:38:39", "link": "http://arxiv.org/abs/2309.10539v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unsupervised Deep Cross-Language Entity Alignment", "abstract": "Cross-lingual entity alignment is the task of finding the same semantic\nentities from different language knowledge graphs. In this paper, we propose a\nsimple and novel unsupervised method for cross-language entity alignment. We\nutilize the deep learning multi-language encoder combined with a machine\ntranslator to encode knowledge graph text, which reduces the reliance on label\ndata. Unlike traditional methods that only emphasize global or local alignment,\nour method simultaneously considers both alignment strategies. We first view\nthe alignment task as a bipartite matching problem and then adopt the\nre-exchanging idea to accomplish alignment. Compared with the traditional\nbipartite matching algorithm that only gives one optimal solution, our\nalgorithm generates ranked matching results which enabled many potentials\ndownstream tasks. Additionally, our method can adapt two different types of\noptimization (minimal and maximal) in the bipartite matching process, which\nprovides more flexibility. Our evaluation shows, we each scored 0.966, 0.990,\nand 0.996 Hits@1 rates on the DBP15K dataset in Chinese, Japanese, and French\nto English alignment tasks. We outperformed the state-of-the-art method in\nunsupervised and semi-supervised categories. Compared with the state-of-the-art\nsupervised method, our method outperforms 2.6% and 0.4% in Ja-En and Fr-En\nalignment tasks while marginally lower by 0.2% in the Zh-En alignment task.", "published": "2023-09-19 13:12:48", "link": "http://arxiv.org/abs/2309.10598v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "NusaWrites: Constructing High-Quality Corpora for Underrepresented and\n  Extremely Low-Resource Languages", "abstract": "Democratizing access to natural language processing (NLP) technology is\ncrucial, especially for underrepresented and extremely low-resource languages.\nPrevious research has focused on developing labeled and unlabeled corpora for\nthese languages through online scraping and document translation. While these\nmethods have proven effective and cost-efficient, we have identified\nlimitations in the resulting corpora, including a lack of lexical diversity and\ncultural relevance to local communities. To address this gap, we conduct a case\nstudy on Indonesian local languages. We compare the effectiveness of online\nscraping, human translation, and paragraph writing by native speakers in\nconstructing datasets. Our findings demonstrate that datasets generated through\nparagraph writing by native speakers exhibit superior quality in terms of\nlexical diversity and cultural content. In addition, we present the\n\\datasetname{} benchmark, encompassing 12 underrepresented and extremely\nlow-resource languages spoken by millions of individuals in Indonesia. Our\nempirical experiment results using existing multilingual large language models\nconclude the need to extend these models to more underrepresented languages. We\nrelease the NusaWrites dataset at https://github.com/IndoNLP/nusa-writes.", "published": "2023-09-19 14:42:33", "link": "http://arxiv.org/abs/2309.10661v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Estimating Contamination via Perplexity: Quantifying Memorisation in\n  Language Model Evaluation", "abstract": "Data contamination in model evaluation is getting increasingly prevalent as\nthe massive training corpora of large language models often unintentionally\ninclude benchmark samples. Therefore, contamination analysis has became an\ninevitable part of reliable model evaluation. However, existing method of\ncontamination analysis requires the access of the entire training data which is\noften confidential for recent models. This prevent the community to rigorously\naudit these models and conduct accurate assessment of their capability. In this\npaper, we propose a novel method to quantify contamination without the access\nof the full training set, that measure the extent of contamination with\nperplexity. Our analysis provides evidence of significant memorisation of\nrecent foundation models in popular reading comprehension, summarisation\nbenchmarks, while multiple choice appears less contaminated.", "published": "2023-09-19 15:02:58", "link": "http://arxiv.org/abs/2309.10677v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FRASIMED: a Clinical French Annotated Resource Produced through\n  Crosslingual BERT-Based Annotation Projection", "abstract": "Natural language processing (NLP) applications such as named entity\nrecognition (NER) for low-resource corpora do not benefit from recent advances\nin the development of large language models (LLMs) where there is still a need\nfor larger annotated datasets. This research article introduces a methodology\nfor generating translated versions of annotated datasets through crosslingual\nannotation projection. Leveraging a language agnostic BERT-based approach, it\nis an efficient solution to increase low-resource corpora with few human\nefforts and by only using already available open data resources. Quantitative\nand qualitative evaluations are often lacking when it comes to evaluating the\nquality and effectiveness of semi-automatic data generation strategies. The\nevaluation of our crosslingual annotation projection approach showed both\neffectiveness and high accuracy in the resulting dataset. As a practical\napplication of this methodology, we present the creation of French Annotated\nResource with Semantic Information for Medical Entities Detection (FRASIMED),\nan annotated corpus comprising 2'051 synthetic clinical cases in French. The\ncorpus is now available for researchers and practitioners to develop and refine\nFrench natural language processing (NLP) applications in the clinical field\n(https://zenodo.org/record/8355629), making it the largest open annotated\ncorpus with linked medical concepts in French.", "published": "2023-09-19 17:17:28", "link": "http://arxiv.org/abs/2309.10770v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Modeling interdisciplinary interactions among Physics, Mathematics &\n  Computer Science", "abstract": "Interdisciplinarity has over the recent years have gained tremendous\nimportance and has become one of the key ways of doing cutting edge research.\nIn this paper we attempt to model the citation flow across three different\nfields -- Physics (PHY), Mathematics (MA) and Computer Science (CS). For\ninstance, is there a specific pattern in which these fields cite one another?\nWe carry out experiments on a dataset comprising more than 1.2 million articles\ntaken from these three fields. We quantify the citation interactions among\nthese three fields through temporal bucket signatures. We present numerical\nmodels based on variants of the recently proposed relay-linking framework to\nexplain the citation dynamics across the three disciplines. These models make a\nmodest attempt to unfold the underlying principles of how citation links could\nhave been formed across the three fields over time.", "published": "2023-09-19 17:52:50", "link": "http://arxiv.org/abs/2309.10811v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "SlimPajama-DC: Understanding Data Combinations for LLM Training", "abstract": "This paper aims to understand the impacts of various data combinations (e.g.,\nweb text, Wikipedia, GitHub, books) on the pretraining of large language models\nusing SlimPajama. SlimPajama is a rigorously deduplicated, multi-source\ndataset, which has been refined and further deduplicated to 627B tokens from\nthe extensive 1.2T token RedPajama dataset contributed by Together. We have\ntermed our research as SlimPajama-DC, an empirical analysis designed to uncover\nfundamental characteristics and best practices associated with employing\nSlimPajama in the training of large language models. During our research with\nSlimPajama, two pivotal observations emerged: (1) Global deduplication vs.\nlocal deduplication. We analyze and discuss how global (across different\nsources of datasets) and local (within the single source of dataset)\ndeduplications affect the performance of trained models. (2) Proportions of\nhighly-deduplicated multi-source datasets in the combination. To study this, we\nconstruct six configurations on SlimPajama dataset and train individual ones\nusing 1.3B Cerebras-GPT model with Alibi and SwiGLU. Our best configuration\noutperforms the 1.3B model trained on RedPajama using the same number of\ntraining tokens by a significant margin. All our 1.3B models are trained on\nCerebras 16$\\times$ CS-2 cluster with a total of 80 PFLOP/s in bf16 mixed\nprecision. We further extend our discoveries (such as increasing data diversity\nis crucial after global deduplication) on a 7B model with large batch-size\ntraining. Our SlimPajama-DC models are available at:\nhttps://huggingface.co/MBZUAI-LLM/SlimPajama-DC and the separate SlimPajama-DC\ndatasets are available at:\nhttps://huggingface.co/datasets/MBZUAI-LLM/SlimPajama-627B-DC.", "published": "2023-09-19 17:59:54", "link": "http://arxiv.org/abs/2309.10818v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What Learned Representations and Influence Functions Can Tell Us About\n  Adversarial Examples", "abstract": "Adversarial examples, deliberately crafted using small perturbations to fool\ndeep neural networks, were first studied in image processing and more recently\nin NLP. While approaches to detecting adversarial examples in NLP have largely\nrelied on search over input perturbations, image processing has seen a range of\ntechniques that aim to characterise adversarial subspaces over the learned\nrepresentations.\n  In this paper, we adapt two such approaches to NLP, one based on nearest\nneighbors and influence functions and one on Mahalanobis distances. The former\nin particular produces a state-of-the-art detector when compared against\nseveral strong baselines; moreover, the novel use of influence functions\nprovides insight into how the nature of adversarial example subspaces in NLP\nrelate to those in image processing, and also how they differ depending on the\nkind of NLP task.", "published": "2023-09-19 20:28:24", "link": "http://arxiv.org/abs/2309.10916v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Benchmarks for Pir\u00e1 2.0, a Reading Comprehension Dataset about the\n  Ocean, the Brazilian Coast, and Climate Change", "abstract": "Pir\\'a is a reading comprehension dataset focused on the ocean, the Brazilian\ncoast, and climate change, built from a collection of scientific abstracts and\nreports on these topics. This dataset represents a versatile language resource,\nparticularly useful for testing the ability of current machine learning models\nto acquire expert scientific knowledge. Despite its potential, a detailed set\nof baselines has not yet been developed for Pir\\'a. By creating these\nbaselines, researchers can more easily utilize Pir\\'a as a resource for testing\nmachine learning models across a wide range of question answering tasks. In\nthis paper, we define six benchmarks over the Pir\\'a dataset, covering closed\ngenerative question answering, machine reading comprehension, information\nretrieval, open question answering, answer triggering, and multiple choice\nquestion answering. As part of this effort, we have also produced a curated\nversion of the original dataset, where we fixed a number of grammar issues,\nrepetitions, and other shortcomings. Furthermore, the dataset has been extended\nin several new directions, so as to face the aforementioned benchmarks:\ntranslation of supporting texts from English into Portuguese, classification\nlabels for answerability, automatic paraphrases of questions and answers, and\nmultiple choice candidates. The results described in this paper provide several\npoints of reference for researchers interested in exploring the challenges\nprovided by the Pir\\'a dataset.", "published": "2023-09-19 21:56:45", "link": "http://arxiv.org/abs/2309.10945v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "In-Context Learning for Text Classification with Many Labels", "abstract": "In-context learning (ICL) using large language models for tasks with many\nlabels is challenging due to the limited context window, which makes it\ndifficult to fit a sufficient number of examples in the prompt. In this paper,\nwe use a pre-trained dense retrieval model to bypass this limitation, giving\nthe model only a partial view of the full label space for each inference call.\nTesting with recent open-source LLMs (OPT, LLaMA), we set new state of the art\nperformance in few-shot settings for three common intent classification\ndatasets, with no finetuning. We also surpass fine-tuned performance on\nfine-grained sentiment classification in certain cases. We analyze the\nperformance across number of in-context examples and different model scales,\nshowing that larger models are necessary to effectively and consistently make\nuse of larger context lengths for ICL. By running several ablations, we analyze\nthe model's use of: a) the similarity of the in-context examples to the current\ninput, b) the semantic content of the class names, and c) the correct\ncorrespondence between examples and labels. We demonstrate that all three are\nneeded to varying degrees depending on the domain, contrary to certain recent\nworks.", "published": "2023-09-19 22:41:44", "link": "http://arxiv.org/abs/2309.10954v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Health Data Interoperability with Large Language Models: A\n  FHIR Study", "abstract": "In this study, we investigated the ability of the large language model (LLM)\nto enhance healthcare data interoperability. We leveraged the LLM to convert\nclinical texts into their corresponding FHIR resources. Our experiments,\nconducted on 3,671 snippets of clinical text, demonstrated that the LLM not\nonly streamlines the multi-step natural language processing and human\ncalibration processes but also achieves an exceptional accuracy rate of over\n90% in exact matches when compared to human annotations.", "published": "2023-09-19 20:09:35", "link": "http://arxiv.org/abs/2310.12989v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What is the Best Automated Metric for Text to Motion Generation?", "abstract": "There is growing interest in generating skeleton-based human motions from\nnatural language descriptions. While most efforts have focused on developing\nbetter neural architectures for this task, there has been no significant work\non determining the proper evaluation metric. Human evaluation is the ultimate\naccuracy measure for this task, and automated metrics should correlate well\nwith human quality judgments. Since descriptions are compatible with many\nmotions, determining the right metric is critical for evaluating and designing\neffective generative models. This paper systematically studies which metrics\nbest align with human evaluations and proposes new metrics that align even\nbetter. Our findings indicate that none of the metrics currently used for this\ntask show even a moderate correlation with human judgments on a sample level.\nHowever, for assessing average model performance, commonly used metrics such as\nR-Precision and less-used coordinate errors show strong correlations.\nAdditionally, several recently developed metrics are not recommended due to\ntheir low correlation compared to alternatives. We also introduce a novel\nmetric based on a multimodal BERT-like model, MoBERT, which offers strongly\nhuman-correlated sample-level evaluations while maintaining near-perfect\nmodel-level correlation. Our results demonstrate that this new metric exhibits\nextensive benefits over all current alternatives.", "published": "2023-09-19 01:59:54", "link": "http://arxiv.org/abs/2309.10248v1", "categories": ["cs.CL", "cs.GR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Speech PTM, Text LLM, and Emotional TTS for Speech Emotion\n  Recognition", "abstract": "In this paper, we explored how to boost speech emotion recognition (SER) with\nthe state-of-the-art speech pre-trained model (PTM), data2vec, text generation\ntechnique, GPT-4, and speech synthesis technique, Azure TTS. First, we\ninvestigated the representation ability of different speech self-supervised\npre-trained models, and we found that data2vec has a good representation\nability on the SER task. Second, we employed a powerful large language model\n(LLM), GPT-4, and emotional text-to-speech (TTS) model, Azure TTS, to generate\nemotionally congruent text and speech. We carefully designed the text prompt\nand dataset construction, to obtain the synthetic emotional speech data with\nhigh quality. Third, we studied different ways of data augmentation to promote\nthe SER task with synthetic speech, including random mixing, adversarial\ntraining, transfer learning, and curriculum learning. Experiments and ablation\nstudies on the IEMOCAP dataset demonstrate the effectiveness of our method,\ncompared with other data augmentation methods, and data augmentation with other\nsynthetic data.", "published": "2023-09-19 03:52:01", "link": "http://arxiv.org/abs/2309.10294v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Using fine-tuning and min lookahead beam search to improve Whisper", "abstract": "The performance of Whisper in low-resource languages is still far from\nperfect. In addition to a lack of training data on low-resource languages, we\nidentify some limitations in the beam search algorithm used in Whisper. To\naddress these issues, we fine-tune Whisper on additional data and propose an\nimproved decoding algorithm. On the Vietnamese language, fine-tuning\nWhisper-Tiny with LoRA leads to an improvement of 38.49 in WER over the\nzero-shot Whisper-Tiny setting which is a further reduction of 1.45 compared to\nfull-parameter fine-tuning. Additionally, by using Filter-Ends and Min\nLookahead decoding algorithms, the WER reduces by 2.26 on average over a range\nof languages compared to standard beam search. These results generalise to\nlarger Whisper model sizes. We also prove a theorem that Min Lookahead\noutperforms the standard beam search algorithm used in Whisper.", "published": "2023-09-19 04:04:14", "link": "http://arxiv.org/abs/2309.10299v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Investigating the Catastrophic Forgetting in Multimodal Large Language\n  Models", "abstract": "Following the success of GPT4, there has been a surge in interest in\nmultimodal large language model (MLLM) research. This line of research focuses\non developing general-purpose LLMs through fine-tuning pre-trained LLMs and\nvision models. However, catastrophic forgetting, a notorious phenomenon where\nthe fine-tuned model fails to retain similar performance compared to the\npre-trained model, still remains an inherent problem in multimodal LLMs (MLLM).\nIn this paper, we introduce EMT: Evaluating MulTimodality for evaluating the\ncatastrophic forgetting in MLLMs, by treating each MLLM as an image classifier.\nWe first apply EMT to evaluate several open-source fine-tuned MLLMs and we\ndiscover that almost all evaluated MLLMs fail to retain the same performance\nlevels as their vision encoders on standard image classification tasks.\nMoreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess\nperformance throughout the fine-tuning. Interestingly, our results suggest that\nearly-stage fine-tuning on an image dataset improves performance across other\nimage datasets, by enhancing the alignment of text and visual features.\nHowever, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in\na significant loss of generalizability, even when the image encoder remains\nfrozen. Our results suggest that MLLMs have yet to demonstrate performance on\npar with their vision models on standard image classification tasks and the\ncurrent MLLM fine-tuning procedure still has room for improvement.", "published": "2023-09-19 04:51:13", "link": "http://arxiv.org/abs/2309.10313v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explaining Agent Behavior with Large Language Models", "abstract": "Intelligent agents such as robots are increasingly deployed in real-world,\nsafety-critical settings. It is vital that these agents are able to explain the\nreasoning behind their decisions to human counterparts, however, their behavior\nis often produced by uninterpretable models such as deep neural networks. We\npropose an approach to generate natural language explanations for an agent's\nbehavior based only on observations of states and actions, agnostic to the\nunderlying model representation. We show how a compact representation of the\nagent's behavior can be learned and used to produce plausible explanations with\nminimal hallucination while affording user interaction with a pre-trained large\nlanguage model. Through user studies and empirical experiments, we show that\nour approach generates explanations as helpful as those generated by a human\ndomain expert while enabling beneficial interactions such as clarification and\ncounterfactual queries.", "published": "2023-09-19 06:13:24", "link": "http://arxiv.org/abs/2309.10346v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Improving Speaker Diarization using Semantic Information: Joint Pairwise\n  Constraints Propagation", "abstract": "Speaker diarization has gained considerable attention within speech\nprocessing research community. Mainstream speaker diarization rely primarily on\nspeakers' voice characteristics extracted from acoustic signals and often\noverlook the potential of semantic information. Considering the fact that\nspeech signals can efficiently convey the content of a speech, it is of our\ninterest to fully exploit these semantic cues utilizing language models. In\nthis work we propose a novel approach to effectively leverage semantic\ninformation in clustering-based speaker diarization systems. Firstly, we\nintroduce spoken language understanding modules to extract speaker-related\nsemantic information and utilize these information to construct pairwise\nconstraints. Secondly, we present a novel framework to integrate these\nconstraints into the speaker diarization pipeline, enhancing the performance of\nthe entire system. Extensive experiments conducted on the public dataset\ndemonstrate the consistent superiority of our proposed approach over\nacoustic-only speaker diarization systems.", "published": "2023-09-19 09:13:30", "link": "http://arxiv.org/abs/2309.10456v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Harnessing the Zero-Shot Power of Instruction-Tuned Large Language Model\n  in End-to-End Speech Recognition", "abstract": "We propose to utilize an instruction-tuned large language model (LLM) for\nguiding the text generation process in automatic speech recognition (ASR).\nModern large language models (LLMs) are adept at performing various text\ngeneration tasks through zero-shot learning, prompted with instructions\ndesigned for specific objectives. This paper explores the potential of LLMs to\nderive linguistic information that can facilitate text generation in end-to-end\nASR models. Specifically, we instruct an LLM to correct grammatical errors in\nan ASR hypothesis and use the LLM-derived representations to refine the output\nfurther. The proposed model is built on the joint CTC and attention\narchitecture, with the LLM serving as a front-end feature extractor for the\ndecoder. The ASR hypothesis, subject to correction, is obtained from the\nencoder via CTC decoding and fed into the LLM along with a specific\ninstruction. The decoder subsequently takes as input the LLM output to perform\ntoken predictions, combining acoustic information from the encoder and the\npowerful linguistic information provided by the LLM. Experimental results show\nthat the proposed LLM-guided model achieves a relative gain of approximately\n13\\% in word error rates across major benchmarks.", "published": "2023-09-19 11:10:50", "link": "http://arxiv.org/abs/2309.10524v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Model Leeching: An Extraction Attack Targeting LLMs", "abstract": "Model Leeching is a novel extraction attack targeting Large Language Models\n(LLMs), capable of distilling task-specific knowledge from a target LLM into a\nreduced parameter model. We demonstrate the effectiveness of our attack by\nextracting task capability from ChatGPT-3.5-Turbo, achieving 73% Exact Match\n(EM) similarity, and SQuAD EM and F1 accuracy scores of 75% and 87%,\nrespectively for only $50 in API cost. We further demonstrate the feasibility\nof adversarial attack transferability from an extracted model extracted via\nModel Leeching to perform ML attack staging against a target LLM, resulting in\nan 11% increase to attack success rate when applied to ChatGPT-3.5-Turbo.", "published": "2023-09-19 11:45:29", "link": "http://arxiv.org/abs/2309.10544v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "A Neighbourhood-Aware Differential Privacy Mechanism for Static Word\n  Embeddings", "abstract": "We propose a Neighbourhood-Aware Differential Privacy (NADP) mechanism\nconsidering the neighbourhood of a word in a pretrained static word embedding\nspace to determine the minimal amount of noise required to guarantee a\nspecified privacy level. We first construct a nearest neighbour graph over the\nwords using their embeddings, and factorise it into a set of connected\ncomponents (i.e. neighbourhoods). We then separately apply different levels of\nGaussian noise to the words in each neighbourhood, determined by the set of\nwords in that neighbourhood. Experiments show that our proposed NADP mechanism\nconsistently outperforms multiple previously proposed DP mechanisms such as\nLaplacian, Gaussian, and Mahalanobis in multiple downstream tasks, while\nguaranteeing higher levels of privacy.", "published": "2023-09-19 11:58:08", "link": "http://arxiv.org/abs/2309.10551v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Multimodal Modeling For Spoken Language Identification", "abstract": "Spoken language identification refers to the task of automatically predicting\nthe spoken language in a given utterance. Conventionally, it is modeled as a\nspeech-based language identification task. Prior techniques have been\nconstrained to a single modality; however in the case of video data there is a\nwealth of other metadata that may be beneficial for this task. In this work, we\npropose MuSeLI, a Multimodal Spoken Language Identification method, which\ndelves into the use of various metadata sources to enhance language\nidentification. Our study reveals that metadata such as video title,\ndescription and geographic location provide substantial information to identify\nthe spoken language of the multimedia recording. We conduct experiments using\ntwo diverse public datasets of YouTube videos, and obtain state-of-the-art\nresults on the language identification task. We additionally conduct an\nablation study that describes the distinct contribution of each modality for\nlanguage recognition.", "published": "2023-09-19 12:21:39", "link": "http://arxiv.org/abs/2309.10567v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Large language models can accurately predict searcher preferences", "abstract": "Relevance labels, which indicate whether a search result is valuable to a\nsearcher, are key to evaluating and optimising search systems. The best way to\ncapture the true preferences of users is to ask them for their careful feedback\non which results would be useful, but this approach does not scale to produce a\nlarge number of labels. Getting relevance labels at scale is usually done with\nthird-party labellers, who judge on behalf of the user, but there is a risk of\nlow-quality data if the labeller doesn't understand user needs. To improve\nquality, one standard approach is to study real users through interviews, user\nstudies and direct feedback, find areas where labels are systematically\ndisagreeing with users, then educate labellers about user needs through judging\nguidelines, training and monitoring. This paper introduces an alternate\napproach for improving label quality. It takes careful feedback from real\nusers, which by definition is the highest-quality first-party gold data that\ncan be derived, and develops an large language model prompt that agrees with\nthat data.\n  We present ideas and observations from deploying language models for\nlarge-scale relevance labelling at Bing, and illustrate with data from TREC. We\nhave found large language models can be effective, with accuracy as good as\nhuman labellers and similar capability to pick the hardest queries, best runs,\nand best groups. Systematic changes to the prompts make a difference in\naccuracy, but so too do simple paraphrases. To measure agreement with real\nsearchers needs high-quality \"gold\" labels, but with these we find that models\nproduce better labels than third-party workers, for a fraction of the cost, and\nthese labels let us train notably better rankers.", "published": "2023-09-19 13:55:39", "link": "http://arxiv.org/abs/2309.10621v3", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "CFGPT: Chinese Financial Assistant with Large Language Model", "abstract": "Large language models (LLMs) have demonstrated great potential in natural\nlanguage processing tasks within the financial domain. In this work, we present\na Chinese Financial Generative Pre-trained Transformer framework, named CFGPT,\nwhich includes a dataset~(CFData) for pre-training and supervised fine-tuning,\na financial LLM~(CFLLM) to adeptly manage financial texts, and a deployment\nframework~(CFAPP) designed to navigate real-world financial applications. The\nCFData comprising both a pre-training dataset and a supervised fine-tuning\ndataset, where the pre-training dataset collates Chinese financial data and\nanalytics, alongside a smaller subset of general-purpose text with 584M\ndocuments and 141B tokens in total, and the supervised fine-tuning dataset is\ntailored for six distinct financial tasks, embodying various facets of\nfinancial analysis and decision-making with 1.5M instruction pairs and 1.5B\ntokens in total. The CFLLM, which is based on InternLM-7B to balance the model\ncapability and size, is trained on CFData in two stage, continued pre-training\nand supervised fine-tuning. The CFAPP is centered on large language models\n(LLMs) and augmented with additional modules to ensure multifaceted\nfunctionality in real-world application. Our codes are released at\nhttps://github.com/TongjiFinLab/CFGPT.", "published": "2023-09-19 14:34:01", "link": "http://arxiv.org/abs/2309.10654v2", "categories": ["cs.CL", "cs.AI", "cs.CE"], "primary_category": "cs.CL"}
{"title": "MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language\n  Feedback", "abstract": "To solve complex tasks, large language models (LLMs) often require multiple\nrounds of interactions with the user, sometimes assisted by external tools.\nHowever, current evaluation protocols often emphasize benchmark performance\nwith single-turn exchanges, neglecting the nuanced interactions among the user,\nLLMs, and external tools, while also underestimating the importance of natural\nlanguage feedback from users. These oversights contribute to discrepancies\nbetween research benchmark evaluations and real-world use cases. We introduce\nMINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn\ninteractions by (1) using tools and (2) leveraging natural language feedback.\nTo ensure reproducibility, we provide an evaluation framework where LLMs can\naccess tools by executing Python code and receive users' natural language\nfeedback simulated by GPT-4. We repurpose a diverse set of established\nevaluation datasets focusing on reasoning, coding, and decision-making and\ncarefully curate them into a compact subset for efficient evaluation. Our\nanalysis of 20 open- and closed-source LLMs offers intriguing findings. (a)\nLLMs generally benefit from tools and language feedback, with performance gains\n(absolute, same below) of 1-8% for each turn of tool use and 2-17% with natural\nlanguage feedback. (b) Better single-turn performance does not guarantee better\nmulti-turn performance. (c) Surprisingly, on the LLMs evaluated, supervised\ninstruction-finetuning (SIFT) and reinforcement learning from human feedback\n(RLHF) generally hurt multi-turn capabilities. We expect MINT can help measure\nprogress and incentivize research in improving LLMs' capabilities in multi-turn\ninteractions, especially for open-source communities where multi-turn human\nevaluation can be less accessible compared to commercial LLMs with a larger\nuser base.", "published": "2023-09-19 15:25:42", "link": "http://arxiv.org/abs/2309.10691v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Large Language Models' Ability Using a Psychiatric Screening\n  Tool Based on Metaphor and Sarcasm Scenarios", "abstract": "Metaphors and sarcasm are precious fruits of our highly evolved social\ncommunication skills. However, children with the condition then known as\nAsperger syndrome are known to have difficulties in comprehending sarcasm, even\nif they possess adequate verbal IQs for understanding metaphors. Accordingly,\nresearchers had employed a screening test that assesses metaphor and sarcasm\ncomprehension to distinguish Asperger syndrome from other conditions with\nsimilar external behaviors (e.g., attention-deficit/hyperactivity disorder).\nThis study employs a standardized test to evaluate recent large language\nmodels' (LLMs) understanding of nuanced human communication. The results\nindicate improved metaphor comprehension with increased model parameters;\nhowever, no similar improvement was observed for sarcasm comprehension.\nConsidering that a human's ability to grasp sarcasm has been associated with\nthe amygdala, a pivotal cerebral region for emotional learning, a distinctive\nstrategy for training LLMs would be imperative to imbue them with the ability\nin a cognitively grounded manner.", "published": "2023-09-19 16:41:19", "link": "http://arxiv.org/abs/2309.10744v3", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Interactive Distillation of Large Single-Topic Corpora of Scientific\n  Papers", "abstract": "Highly specific datasets of scientific literature are important for both\nresearch and education. However, it is difficult to build such datasets at\nscale. A common approach is to build these datasets reductively by applying\ntopic modeling on an established corpus and selecting specific topics. A more\nrobust but time-consuming approach is to build the dataset constructively in\nwhich a subject matter expert (SME) handpicks documents. This method does not\nscale and is prone to error as the dataset grows. Here we showcase a new tool,\nbased on machine learning, for constructively generating targeted datasets of\nscientific literature. Given a small initial \"core\" corpus of papers, we build\na citation network of documents. At each step of the citation network, we\ngenerate text embeddings and visualize the embeddings through dimensionality\nreduction. Papers are kept in the dataset if they are \"similar\" to the core or\nare otherwise pruned through human-in-the-loop selection. Additional insight\ninto the papers is gained through sub-topic modeling using SeNMFk. We\ndemonstrate our new tool for literature review by applying it to two different\nfields in machine learning.", "published": "2023-09-19 17:18:36", "link": "http://arxiv.org/abs/2309.10772v1", "categories": ["cs.IR", "cs.CL", "cs.DL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Language as the Medium: Multimodal Video Classification through text\n  only", "abstract": "Despite an exciting new wave of multimodal machine learning models, current\napproaches still struggle to interpret the complex contextual relationships\nbetween the different modalities present in videos. Going beyond existing\nmethods that emphasize simple activities or objects, we propose a new\nmodel-agnostic approach for generating detailed textual descriptions that\ncaptures multimodal video information. Our method leverages the extensive\nknowledge learnt by large language models, such as GPT-3.5 or Llama2, to reason\nabout textual descriptions of the visual and aural modalities, obtained from\nBLIP-2, Whisper and ImageBind. Without needing additional finetuning of\nvideo-text models or datasets, we demonstrate that available LLMs have the\nability to use these multimodal textual descriptions as proxies for ``sight''\nor ``hearing'' and perform zero-shot multimodal classification of videos\nin-context. Our evaluations on popular action recognition benchmarks, such as\nUCF-101 or Kinetics, show these context-rich descriptions can be successfully\nused in video understanding tasks. This method points towards a promising new\nresearch direction in multimodal classification, demonstrating how an interplay\nbetween textual, visual and auditory machine learning models can enable more\nholistic video understanding.", "published": "2023-09-19 17:32:21", "link": "http://arxiv.org/abs/2309.10783v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Semantic Text Compression for Classification", "abstract": "We study semantic compression for text where meanings contained in the text\nare conveyed to a source decoder, e.g., for classification. The main motivator\nto move to such an approach of recovering the meaning without requiring exact\nreconstruction is the potential resource savings, both in storage and in\nconveying the information to another node. Towards this end, we propose\nsemantic quantization and compression approaches for text where we utilize\nsentence embeddings and the semantic distortion metric to preserve the meaning.\nOur results demonstrate that the proposed semantic approaches result in\nsubstantial (orders of magnitude) savings in the required number of bits for\nmessage representation at the expense of very modest accuracy loss compared to\nthe semantic agnostic baseline. We compare the results of proposed approaches\nand observe that resource savings enabled by semantic quantization can be\nfurther amplified by semantic clustering. Importantly, we observe the\ngeneralizability of the proposed methodology which produces excellent results\non many benchmark text classification datasets with a diverse array of\ncontexts.", "published": "2023-09-19 17:50:57", "link": "http://arxiv.org/abs/2309.10809v1", "categories": ["cs.IT", "cs.CL", "math.IT"], "primary_category": "cs.IT"}
{"title": "Classifying Organizations for Food System Ontologies using Natural\n  Language Processing", "abstract": "Our research explores the use of natural language processing (NLP) methods to\nautomatically classify entities for the purpose of knowledge graph population\nand integration with food system ontologies. We have created NLP models that\ncan automatically classify organizations with respect to categories associated\nwith environmental issues as well as Standard Industrial Classification (SIC)\ncodes, which are used by the U.S. government to characterize business\nactivities. As input, the NLP models are provided with text snippets retrieved\nby the Google search engine for each organization, which serves as a textual\ndescription of the organization that is used for learning. Our experimental\nresults show that NLP models can achieve reasonably good performance for these\ntwo classification tasks, and they rely on a general framework that could be\napplied to many other classification problems as well. We believe that NLP\nmodels represent a promising approach for automatically harvesting information\nto populate knowledge graphs and aligning the information with existing\nontologies through shared categories and concepts.", "published": "2023-09-19 19:07:48", "link": "http://arxiv.org/abs/2309.10880v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR", "H.3.1; I.2.7; J.3; J.4; K.4.3"], "primary_category": "cs.CL"}
{"title": "Self-Augmentation Improves Zero-Shot Cross-Lingual Transfer", "abstract": "Zero-shot cross-lingual transfer is a central task in multilingual NLP,\nallowing models trained in languages with more sufficient training resources to\ngeneralize to other low-resource languages. Earlier efforts on this task use\nparallel corpora, bilingual dictionaries, or other annotated alignment data to\nimprove cross-lingual transferability, which are typically expensive to obtain.\nIn this paper, we propose a simple yet effective method, SALT, to improve the\nzero-shot cross-lingual transfer of the multilingual pretrained language models\nwithout the help of such external data. By incorporating code-switching and\nembedding mixup with self-augmentation, SALT effectively distills cross-lingual\nknowledge from the multilingual PLM and enhances its transferability on\ndownstream tasks. Experimental results on XNLI and PAWS-X show that our method\nis able to improve zero-shot cross-lingual transferability without external\ndata. Our code is available at https://github.com/luka-group/SALT.", "published": "2023-09-19 19:30:56", "link": "http://arxiv.org/abs/2309.10891v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semi-automatic staging area for high-quality structured data extraction\n  from scientific literature", "abstract": "We propose a semi-automatic staging area for efficiently building an accurate\ndatabase of experimental physical properties of superconductors from\nliterature, called SuperCon2, to enrich the existing manually-built\nsuperconductor database SuperCon. Here we report our curation interface\n(SuperCon2 Interface) and a workflow managing the state transitions of each\nexamined record, to validate the dataset of superconductors from PDF documents\ncollected using Grobid-superconductors in a previous work. This curation\nworkflow allows both automatic and manual operations, the former contains\n``anomaly detection'' that scans new data identifying outliers, and a\n``training data collector'' mechanism that collects training data examples\nbased on manual corrections. Such training data collection policy is effective\nin improving the machine-learning models with a reduced number of examples. For\nmanual operations, the interface (SuperCon2 interface) is developed to increase\nefficiency during manual correction by providing a smart interface and an\nenhanced PDF document viewer. We show that our interface significantly improves\nthe curation quality by boosting precision and recall as compared with the\ntraditional ``manual correction''. Our semi-automatic approach would provide a\nsolution for achieving a reliable database with text-data mining of scientific\ndocuments.", "published": "2023-09-19 20:53:13", "link": "http://arxiv.org/abs/2309.10923v2", "categories": ["cs.CL", "cond-mat.supr-con", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semi-Autoregressive Streaming ASR With Label Context", "abstract": "Non-autoregressive (NAR) modeling has gained significant interest in speech\nprocessing since these models achieve dramatically lower inference time than\nautoregressive (AR) models while also achieving good transcription accuracy.\nSince NAR automatic speech recognition (ASR) models must wait for the\ncompletion of the entire utterance before processing, some works explore\nstreaming NAR models based on blockwise attention for low-latency applications.\nHowever, streaming NAR models significantly lag in accuracy compared to\nstreaming AR and non-streaming NAR models. To address this, we propose a\nstreaming \"semi-autoregressive\" ASR model that incorporates the labels emitted\nin previous blocks as additional context using a Language Model (LM)\nsubnetwork. We also introduce a novel greedy decoding algorithm that addresses\ninsertion and deletion errors near block boundaries while not significantly\nincreasing the inference time. Experiments show that our method outperforms the\nexisting streaming NAR model by 19% relative on Tedlium2, 16%/8% on\nLibrispeech-100 clean/other test sets, and 19%/8% on the\nSwitchboard(SWB)/Callhome(CH) test sets. It also reduced the accuracy gap with\nstreaming AR and non-streaming NAR models while achieving 2.5x lower latency.\nWe also demonstrate that our approach can effectively utilize external text\ndata to pre-train the LM subnetwork to further improve streaming ASR accuracy.", "published": "2023-09-19 20:55:58", "link": "http://arxiv.org/abs/2309.10926v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LMDX: Language Model-based Document Information Extraction and\n  Localization", "abstract": "Large Language Models (LLM) have revolutionized Natural Language Processing\n(NLP), improving state-of-the-art and exhibiting emergent capabilities across\nvarious tasks. However, their application in extracting information from\nvisually rich documents, which is at the core of many document processing\nworkflows and involving the extraction of key entities from semi-structured\ndocuments, has not yet been successful. The main obstacles to adopting LLMs for\nthis task include the absence of layout encoding within LLMs, which is critical\nfor high quality extraction, and the lack of a grounding mechanism to localize\nthe predicted entities within the document. In this paper, we introduce\nLanguage Model-based Document Information Extraction and Localization (LMDX), a\nmethodology to reframe the document information extraction task for a LLM. LMDX\nenables extraction of singular, repeated, and hierarchical entities, both with\nand without training data, while providing grounding guarantees and localizing\nthe entities within the document. Finally, we apply LMDX to the PaLM 2-S and\nGemini Pro LLMs and evaluate it on VRDU and CORD benchmarks, setting a new\nstate-of-the-art and showing how LMDX enables the creation of high quality,\ndata-efficient parsers.", "published": "2023-09-19 22:32:56", "link": "http://arxiv.org/abs/2309.10952v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Weakly Supervised Reasoning by Neuro-Symbolic Approaches", "abstract": "Deep learning has largely improved the performance of various natural\nlanguage processing (NLP) tasks. However, most deep learning models are\nblack-box machinery, and lack explicit interpretation. In this chapter, we will\nintroduce our recent progress on neuro-symbolic approaches to NLP, which\ncombines different schools of AI, namely, symbolism and connectionism.\nGenerally, we will design a neural system with symbolic latent structures for\nan NLP task, and apply reinforcement learning or its relaxation to perform\nweakly supervised reasoning in the downstream task. Our framework has been\nsuccessfully applied to various tasks, including table query reasoning,\nsyntactic structure reasoning, information extraction reasoning, and rule\nreasoning. For each application, we will introduce the background, our\napproach, and experimental results.", "published": "2023-09-19 06:10:51", "link": "http://arxiv.org/abs/2309.13072v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM Platform Security: Applying a Systematic Evaluation Framework to\n  OpenAI's ChatGPT Plugins", "abstract": "Large language model (LLM) platforms, such as ChatGPT, have recently begun\noffering an app ecosystem to interface with third-party services on the\ninternet. While these apps extend the capabilities of LLM platforms, they are\ndeveloped by arbitrary third parties and thus cannot be implicitly trusted.\nApps also interface with LLM platforms and users using natural language, which\ncan have imprecise interpretations. In this paper, we propose a framework that\nlays a foundation for LLM platform designers to analyze and improve the\nsecurity, privacy, and safety of current and future third-party integrated LLM\nplatforms. Our framework is a formulation of an attack taxonomy that is\ndeveloped by iteratively exploring how LLM platform stakeholders could leverage\ntheir capabilities and responsibilities to mount attacks against each other. As\npart of our iterative process, we apply our framework in the context of\nOpenAI's plugin (apps) ecosystem. We uncover plugins that concretely\ndemonstrate the potential for the types of issues that we outline in our attack\ntaxonomy. We conclude by discussing novel challenges and by providing\nrecommendations to improve the security, privacy, and safety of present and\nfuture LLM-based computing platforms.", "published": "2023-09-19 02:20:10", "link": "http://arxiv.org/abs/2309.10254v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Language Modeling Is Compression", "abstract": "It has long been established that predictive models can be transformed into\nlossless compressors and vice versa. Incidentally, in recent years, the machine\nlearning community has focused on training increasingly large and powerful\nself-supervised (language) models. Since these large language models exhibit\nimpressive predictive capabilities, they are well-positioned to be strong\ncompressors. In this work, we advocate for viewing the prediction problem\nthrough the lens of compression and evaluate the compression capabilities of\nlarge (foundation) models. We show that large language models are powerful\ngeneral-purpose predictors and that the compression viewpoint provides novel\ninsights into scaling laws, tokenization, and in-context learning. For example,\nChinchilla 70B, while trained primarily on text, compresses ImageNet patches to\n43.4% and LibriSpeech samples to 16.4% of their raw size, beating\ndomain-specific compressors like PNG (58.5%) or FLAC (30.3%), respectively.\nFinally, we show that the prediction-compression equivalence allows us to use\nany compressor (like gzip) to build a conditional generative model.", "published": "2023-09-19 14:50:38", "link": "http://arxiv.org/abs/2309.10668v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "End-to-End Speech Recognition Contextualization with Large Language\n  Models", "abstract": "In recent years, Large Language Models (LLMs) have garnered significant\nattention from the research community due to their exceptional performance and\ngeneralization capabilities. In this paper, we introduce a novel method for\ncontextualizing speech recognition models incorporating LLMs. Our approach\ncasts speech recognition as a mixed-modal language modeling task based on a\npretrained LLM. We provide audio features, along with optional text tokens for\ncontext, to train the system to complete transcriptions in a decoder-only\nfashion. As a result, the system is implicitly incentivized to learn how to\nleverage unstructured contextual information during training. Our empirical\nresults demonstrate a significant improvement in performance, with a 6% WER\nreduction when additional textual context is provided. Moreover, we find that\nour method performs competitively and improve by 7.5% WER overall and 17% WER\non rare words against a baseline contextualized RNN-T system that has been\ntrained on more than twenty five times larger speech dataset. Overall, we\ndemonstrate that by only adding a handful number of trainable parameters via\nadapters, we can unlock contextualized speech recognition capability for the\npretrained LLM while keeping the same text-only input functionality.", "published": "2023-09-19 20:28:57", "link": "http://arxiv.org/abs/2309.10917v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MelodyGLM: Multi-task Pre-training for Symbolic Melody Generation", "abstract": "Pre-trained language models have achieved impressive results in various music\nunderstanding and generation tasks. However, existing pre-training methods for\nsymbolic melody generation struggle to capture multi-scale, multi-dimensional\nstructural information in note sequences, due to the domain knowledge\ndiscrepancy between text and music. Moreover, the lack of available large-scale\nsymbolic melody datasets limits the pre-training improvement. In this paper, we\npropose MelodyGLM, a multi-task pre-training framework for generating melodies\nwith long-term structure. We design the melodic n-gram and long span sampling\nstrategies to create local and global blank infilling tasks for modeling the\nlocal and global structures in melodies. Specifically, we incorporate pitch\nn-grams, rhythm n-grams, and their combined n-grams into the melodic n-gram\nblank infilling tasks for modeling the multi-dimensional structures in\nmelodies. To this end, we have constructed a large-scale symbolic melody\ndataset, MelodyNet, containing more than 0.4 million melody pieces. MelodyNet\nis utilized for large-scale pre-training and domain-specific n-gram lexicon\nconstruction. Both subjective and objective evaluations demonstrate that\nMelodyGLM surpasses the standard and previous pre-training methods. In\nparticular, subjective evaluations show that, on the melody continuation task,\nMelodyGLM gains average improvements of 0.82, 0.87, 0.78, and 0.94 in\nconsistency, rhythmicity, structure, and overall quality, respectively.\nNotably, MelodyGLM nearly matches the quality of human-composed melodies on the\nmelody inpainting task.", "published": "2023-09-19 16:34:24", "link": "http://arxiv.org/abs/2309.10738v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.IR", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring Speech Enhancement for Low-resource Speech Synthesis", "abstract": "High-quality and intelligible speech is essential to text-to-speech (TTS)\nmodel training, however, obtaining high-quality data for low-resource languages\nis challenging and expensive. Applying speech enhancement on Automatic Speech\nRecognition (ASR) corpus mitigates the issue by augmenting the training data,\nwhile how the nonlinear speech distortion brought by speech enhancement models\naffects TTS training still needs to be investigated. In this paper, we train a\nTF-GridNet speech enhancement model and apply it to low-resource datasets that\nwere collected for the ASR task, then train a discrete unit based TTS model on\nthe enhanced speech. We use Arabic datasets as an example and show that the\nproposed pipeline significantly improves the low-resource TTS system compared\nwith other baseline methods in terms of ASR WER metric. We also run empirical\nanalysis on the correlation between speech enhancement and TTS performances.", "published": "2023-09-19 17:42:57", "link": "http://arxiv.org/abs/2309.10795v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "PDPCRN: Parallel Dual-Path CRN with Bi-directional Inter-Branch\n  Interactions for Multi-Channel Speech Enhancement", "abstract": "Multi-channel speech enhancement seeks to utilize spatial information to\ndistinguish target speech from interfering signals. While deep learning\napproaches like the dual-path convolutional recurrent network (DPCRN) have made\nstrides, challenges persist in effectively modeling inter-channel correlations\nand amalgamating multi-level information. In response, we introduce the\nParallel Dual-Path Convolutional Recurrent Network (PDPCRN). This acoustic\nmodeling architecture has two key innovations. First, a parallel design with\nseparate branches extracts complementary features. Second, bi-directional\nmodules enable cross-branch communication. Together, these facilitate diverse\nrepresentation fusion and enhanced modeling. Experimental validation on TIMIT\ndatasets underscores the prowess of PDPCRN. Notably, against baseline models\nlike the standard DPCRN, PDPCRN not only outperforms in PESQ and STOI metrics\nbut also boasts a leaner computational footprint with reduced parameters.", "published": "2023-09-19 07:27:38", "link": "http://arxiv.org/abs/2309.10379v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Hierarchical Modeling of Spatial Cues via Spherical Harmonics for\n  Multi-Channel Speech Enhancement", "abstract": "Multi-channel speech enhancement utilizes spatial information from multiple\nmicrophones to extract the target speech. However, most existing methods do not\nexplicitly model spatial cues, instead relying on implicit learning from\nmulti-channel spectra. To better leverage spatial information, we propose\nexplicitly incorporating spatial modeling by applying spherical harmonic\ntransforms (SHT) to the multi-channel input. In detail, a hierarchical\nframework is introduced whereby lower order harmonics capturing broader spatial\npatterns are estimated first, then combined with higher orders to recursively\npredict finer spatial details. Experiments on TIMIT demonstrate the proposed\nmethod can effectively recover target spatial patterns and achieve improved\nperformance over baseline models, using fewer parameters and computations.\nExplicitly modeling spatial information hierarchically enables more effective\nmulti-channel speech enhancement.", "published": "2023-09-19 07:46:14", "link": "http://arxiv.org/abs/2309.10393v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Incorporating Ultrasound Tongue Images for Audio-Visual Speech\n  Enhancement", "abstract": "Audio-visual speech enhancement (AV-SE) aims to enhance degraded speech along\nwith extra visual information such as lip videos, and has been shown to be more\neffective than audio-only speech enhancement. This paper proposes the\nincorporation of ultrasound tongue images to improve the performance of\nlip-based AV-SE systems further. To address the challenge of acquiring\nultrasound tongue images during inference, we first propose to employ knowledge\ndistillation during training to investigate the feasibility of leveraging\ntongue-related information without directly inputting ultrasound tongue images.\nSpecifically, we guide an audio-lip speech enhancement student model to learn\nfrom a pre-trained audio-lip-tongue speech enhancement teacher model, thus\ntransferring tongue-related knowledge. To better model the alignment between\nthe lip and tongue modalities, we further propose the introduction of a\nlip-tongue key-value memory network into the AV-SE model. This network enables\nthe retrieval of tongue features based on readily available lip features,\nthereby assisting the subsequent speech enhancement task. Experimental results\ndemonstrate that both methods significantly improve the quality and\nintelligibility of the enhanced speech compared to traditional lip-based AV-SE\nbaselines. Moreover, both proposed methods exhibit strong generalization\nperformance on unseen speakers and in the presence of unseen noises.\nFurthermore, phone error rate (PER) analysis of automatic speech recognition\n(ASR) reveals that while all phonemes benefit from introducing ultrasound\ntongue images, palatal and velar consonants benefit most.", "published": "2023-09-19 09:12:27", "link": "http://arxiv.org/abs/2309.10455v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Bridging the Spoof Gap: A Unified Parallel Aggregation Network for Voice\n  Presentation Attacks", "abstract": "Automatic Speaker Verification (ASV) systems are increasingly used in voice\nbio-metrics for user authentication but are susceptible to logical and physical\nspoofing attacks, posing security risks. Existing research mainly tackles\nlogical or physical attacks separately, leading to a gap in unified spoofing\ndetection. Moreover, when existing systems attempt to handle both types of\nattacks, they often exhibit significant disparities in the Equal Error Rate\n(EER). To bridge this gap, we present a Parallel Stacked Aggregation Network\nthat processes raw audio. Our approach employs a split-transform-aggregation\ntechnique, dividing utterances into convolved representations, applying\ntransformations, and aggregating the results to identify logical (LA) and\nphysical (PA) spoofing attacks. Evaluation of the ASVspoof-2019 and VSDC\ndatasets shows the effectiveness of the proposed system. It outperforms\nstate-of-the-art solutions, displaying reduced EER disparities and superior\nperformance in detecting spoofing attacks. This highlights the proposed\nmethod's generalizability and superiority. In a world increasingly reliant on\nvoice-based security, our unified spoofing detection system provides a robust\ndefense against a spectrum of voice spoofing attacks, safeguarding ASVs and\nuser data effectively.", "published": "2023-09-19 12:12:59", "link": "http://arxiv.org/abs/2309.10560v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Active Noise Control System Based on Soundfield Interpolation Using a\n  Physics-informed Neural Network", "abstract": "Conventional multiple-point active noise control (ANC) systems require\nplacing error microphones within the region of interest (ROI), inconveniencing\nusers. This paper designs a feasible monitoring microphone arrangement placed\noutside the ROI, providing a user with more freedom of movement. The soundfield\nwithin the ROI is interpolated from the microphone signals using a\nphysics-informed neural network (PINN). PINN exploits the acoustic wave\nequation to assist soundfield interpolation under a limited number of\nmonitoring microphones, and demonstrates better interpolation performance than\nthe spherical harmonic method in simulations. An ANC system is designed to take\nadvantage of the interpolated signal to reduce noise signal within the ROI. The\nPINN-assisted ANC system reduces noise more than that of the multiple-point ANC\nsystem in simulations.", "published": "2023-09-19 13:20:47", "link": "http://arxiv.org/abs/2309.10605v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "USED: Universal Speaker Extraction and Diarization", "abstract": "Speaker extraction and diarization are two enabling techniques for real-world\nspeech applications. Speaker extraction aims to extract a target speaker's\nvoice from a speech mixture, while speaker diarization demarcates speech\nsegments by speaker, annotating `who spoke when'. Previous studies have\ntypically treated the two tasks independently. In practical applications, it is\nmore meaningful to have knowledge about `who spoke what and when', which is\ncaptured by the two tasks. The two tasks share a similar objective of\ndisentangling speakers. Speaker extraction operates in the frequency domain,\nwhereas diarization is in the temporal domain. It is logical to believe that\nspeaker activities obtained from speaker diarization can benefit speaker\nextraction, while the extracted speech offers more accurate speaker activity\ndetection than the speech mixture. In this paper, we propose a unified model\ncalled Universal Speaker Extraction and Diarization (USED) to address output\ninconsistency and scenario mismatch issues. It is designed to manage speech\nmixtures with varying overlap ratios and variable number of speakers. We show\nthat the USED model significantly outperforms the competitive baselines for\nspeaker extraction and diarization tasks on LibriMix and SparseLibriMix\ndatasets. We further validate the diarization performance on CALLHOME, a\ndataset based on real recordings, and experimental results indicate that our\nmodel surpasses recently proposed approaches.", "published": "2023-09-19 14:56:31", "link": "http://arxiv.org/abs/2309.10674v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Efficient Multi-Channel Speech Enhancement with Spherical Harmonics\n  Injection for Directional Encoding", "abstract": "Multi-channel speech enhancement extracts speech using multiple microphones\nthat capture spatial cues. Effectively utilizing directional information is key\nfor multi-channel enhancement. Deep learning shows great potential on\nmulti-channel speech enhancement and often takes short-time Fourier Transform\n(STFT) as inputs directly. To fully leverage the spatial information, we\nintroduce a method using spherical harmonics transform (SHT) coefficients as\nauxiliary model inputs. These coefficients concisely represent spatial\ndistributions. Specifically, our model has two encoders, one for the STFT and\nanother for the SHT. By fusing both encoders in the decoder to estimate the\nenhanced STFT, we effectively incorporate spatial context. Evaluations on TIMIT\nunder varying noise and reverberation show our model outperforms established\nbenchmarks. Remarkably, this is achieved with fewer computations and\nparameters. By leveraging spherical harmonics to incorporate directional cues,\nour model efficiently improves the performance of the multi-channel speech\nenhancement.", "published": "2023-09-19 07:50:50", "link": "http://arxiv.org/abs/2309.10832v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Discrete Audio Representation as an Alternative to Mel-Spectrograms for\n  Speaker and Speech Recognition", "abstract": "Discrete audio representation, aka audio tokenization, has seen renewed\ninterest driven by its potential to facilitate the application of text language\nmodeling approaches in audio domain. To this end, various compression and\nrepresentation-learning based tokenization schemes have been proposed. However,\nthere is limited investigation into the performance of compression-based audio\ntokens compared to well-established mel-spectrogram features across various\nspeaker and speech related tasks. In this paper, we evaluate compression based\naudio tokens on three tasks: Speaker Verification, Diarization and\n(Multi-lingual) Speech Recognition. Our findings indicate that (i) the models\ntrained on audio tokens perform competitively, on average within $1\\%$ of\nmel-spectrogram features for all the tasks considered, and do not surpass them\nyet. (ii) these models exhibit robustness for out-of-domain narrowband data,\nparticularly in speaker tasks. (iii) audio tokens allow for compression to 20x\ncompared to mel-spectrogram features with minimal loss of performance in speech\nand speaker related tasks, which is crucial for low bit-rate applications, and\n(iv) the examined Residual Vector Quantization (RVQ) based audio tokenizer\nexhibits a low-pass frequency response characteristic, offering a plausible\nexplanation for the observed results, and providing insight for future\ntokenizer designs.", "published": "2023-09-19 20:49:05", "link": "http://arxiv.org/abs/2309.10922v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Crowdotic: A Privacy-Preserving Hospital Waiting Room Crowd Density\n  Estimation with Non-speech Audio", "abstract": "Privacy-preserving crowd density analysis finds application across a wide\nrange of scenarios, substantially enhancing smart building operation and\nmanagement while upholding privacy expectations in various spaces. We propose a\nnon-speech audio-based approach for crowd analytics, leveraging a\ntransformer-based model. Our results demonstrate that non-speech audio alone\ncan be used to conduct such analysis with remarkable accuracy. To the best of\nour knowledge, this is the first time when non-speech audio signals are\nproposed for predicting occupancy. As far as we know, there has been no other\nsimilar approach of its kind prior to this. To accomplish this, we deployed our\nsensor-based platform in the waiting room of a large hospital with IRB approval\nover a period of several months to capture non-speech audio and thermal images\nfor the training and evaluation of our models. The proposed non-speech-based\napproach outperformed the thermal camera-based model and all other baselines.\nIn addition to demonstrating superior performance without utilizing speech\naudio, we conduct further analysis using differential privacy techniques to\nprovide additional privacy guarantees. Overall, our work demonstrates the\nviability of employing non-speech audio data for accurate occupancy estimation,\nwhile also ensuring the exclusion of speech-related content and providing\nrobust privacy protections through differential privacy guarantees.", "published": "2023-09-19 03:08:20", "link": "http://arxiv.org/abs/2309.10280v2", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring Sentence Type Effects on the Lombard Effect and\n  Intelligibility Enhancement: A Comparative Study of Natural and Grid\n  Sentences", "abstract": "This study explores how sentence types affect the Lombard effect and\nintelligibility enhancement, focusing on comparisons between natural and grid\nsentences. Using the Lombard Chinese-TIMIT (LCT) corpus and the Enhanced\nMAndarin Lombard Grid (EMALG) corpus, we analyze changes in phonetic and\nacoustic features across different noise levels. Our results show that grid\nsentences produce more pronounced Lombard effects than natural sentences. Then,\nwe develop and test a normal-to-Lombard conversion model, trained separately on\nLCT and EMALG corpora. Through subjective and objective evaluations, natural\nsentences are superior in maintaining speech quality in intelligibility\nenhancement. In contrast, grid sentences could provide superior intelligibility\ndue to the more pronounced Lombard effect. This study provides a valuable\nperspective on enhancing speech communication in noisy environments.", "published": "2023-09-19 09:54:36", "link": "http://arxiv.org/abs/2309.10485v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FoleyGen: Visually-Guided Audio Generation", "abstract": "Recent advancements in audio generation have been spurred by the evolution of\nlarge-scale deep learning models and expansive datasets. However, the task of\nvideo-to-audio (V2A) generation continues to be a challenge, principally\nbecause of the intricate relationship between the high-dimensional visual and\nauditory data, and the challenges associated with temporal synchronization. In\nthis study, we introduce FoleyGen, an open-domain V2A generation system built\non a language modeling paradigm. FoleyGen leverages an off-the-shelf neural\naudio codec for bidirectional conversion between waveforms and discrete tokens.\nThe generation of audio tokens is facilitated by a single Transformer model,\nwhich is conditioned on visual features extracted from a visual encoder. A\nprevalent problem in V2A generation is the misalignment of generated audio with\nthe visible actions in the video. To address this, we explore three novel\nvisual attention mechanisms. We further undertake an exhaustive evaluation of\nmultiple visual encoders, each pretrained on either single-modal or multi-modal\ntasks. The experimental results on VGGSound dataset show that our proposed\nFoleyGen outperforms previous systems across all objective metrics and human\nevaluations.", "published": "2023-09-19 11:33:43", "link": "http://arxiv.org/abs/2309.10537v1", "categories": ["eess.AS", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Motif-Centric Representation Learning for Symbolic Music", "abstract": "Music motif, as a conceptual building block of composition, is crucial for\nmusic structure analysis and automatic composition. While human listeners can\nidentify motifs easily, existing computational models fall short in\nrepresenting motifs and their developments. The reason is that the nature of\nmotifs is implicit, and the diversity of motif variations extends beyond simple\nrepetitions and modulations. In this study, we aim to learn the implicit\nrelationship between motifs and their variations via representation learning,\nusing the Siamese network architecture and a pretraining and fine-tuning\npipeline. A regularization-based method, VICReg, is adopted for pretraining,\nwhile contrastive learning is used for fine-tuning. Experimental results on a\nretrieval-based task show that these two methods complement each other,\nyielding an improvement of 12.6% in the area under the precision-recall curve.\nLastly, we visualize the acquired motif representations, offering an intuitive\ncomprehension of the overall structure of a music piece. As far as we know,\nthis work marks a noteworthy step forward in computational modeling of music\nmotifs. We believe that this work lays the foundations for future applications\nof motifs in automatic music composition and music information retrieval.", "published": "2023-09-19 13:09:03", "link": "http://arxiv.org/abs/2309.10597v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning Tri-modal Embeddings for Zero-Shot Soundscape Mapping", "abstract": "We focus on the task of soundscape mapping, which involves predicting the\nmost probable sounds that could be perceived at a particular geographic\nlocation. We utilise recent state-of-the-art models to encode geotagged audio,\na textual description of the audio, and an overhead image of its capture\nlocation using contrastive pre-training. The end result is a shared embedding\nspace for the three modalities, which enables the construction of soundscape\nmaps for any geographic region from textual or audio queries. Using the\nSoundingEarth dataset, we find that our approach significantly outperforms the\nexisting SOTA, with an improvement of image-to-audio Recall@100 from 0.256 to\n0.450. Our code is available at https://github.com/mvrl/geoclap.", "published": "2023-09-19 14:49:50", "link": "http://arxiv.org/abs/2309.10667v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "ConsistencyTTA: Accelerating Diffusion-Based Text-to-Audio Generation\n  with Consistency Distillation", "abstract": "Diffusion models are instrumental in text-to-audio (TTA) generation.\nUnfortunately, they suffer from slow inference due to an excessive number of\nqueries to the underlying denoising network per generation. To address this\nbottleneck, we introduce ConsistencyTTA, a framework requiring only a single\nnon-autoregressive network query, thereby accelerating TTA by hundreds of\ntimes. We achieve so by proposing \"CFG-aware latent consistency model,\" which\nadapts consistency generation into a latent space and incorporates\nclassifier-free guidance (CFG) into model training. Moreover, unlike diffusion\nmodels, ConsistencyTTA can be finetuned closed-loop with audio-space text-aware\nmetrics, such as CLAP score, to further enhance the generations. Our objective\nand subjective evaluation on the AudioCaps dataset shows that compared to\ndiffusion-based counterparts, ConsistencyTTA reduces inference computation by\n400x while retaining generation quality and diversity.", "published": "2023-09-19 16:36:33", "link": "http://arxiv.org/abs/2309.10740v3", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual\n  Representation Models", "abstract": "Audio-visual representation learning aims to develop systems with human-like\nperception by utilizing correlation between auditory and visual information.\nHowever, current models often focus on a limited set of tasks, and\ngeneralization abilities of learned representations are unclear. To this end,\nwe propose the AV-SUPERB benchmark that enables general-purpose evaluation of\nunimodal audio/visual and bimodal fusion representations on 7 datasets covering\n5 audio-visual tasks in speech and audio processing. We evaluate 5 recent\nself-supervised models and show that none of these models generalize to all\ntasks, emphasizing the need for future study on improving universal model\nperformance. In addition, we show that representations may be improved with\nintermediate-task fine-tuning and audio event classification with AudioSet\nserves as a strong intermediate task. We release our benchmark with evaluation\ncode and a model submission platform to encourage further research in\naudio-visual learning.", "published": "2023-09-19 17:35:16", "link": "http://arxiv.org/abs/2309.10787v2", "categories": ["eess.AS", "cs.CV", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Test-Time Training for Speech", "abstract": "In this paper, we study the application of Test-Time Training (TTT) as a\nsolution to handling distribution shifts in speech applications. In particular,\nwe introduce distribution-shifts to the test datasets of standard\nspeech-classification tasks -- for example, speaker-identification and\nemotion-detection -- and explore how Test-Time Training (TTT) can help adjust\nto the distribution-shift. In our experiments that include distribution shifts\ndue to background noise and natural variations in speech such as gender and\nage, we identify some key-challenges with TTT including sensitivity to\noptimization hyperparameters (e.g., number of optimization steps and subset of\nparameters chosen for TTT) and scalability (e.g., as each example gets its own\nset of parameters, TTT is not scalable). Finally, we propose using BitFit -- a\nparameter-efficient fine-tuning algorithm proposed for text applications that\nonly considers the bias parameters for fine-tuning -- as a solution to the\naforementioned challenges and demonstrate that it is consistently more stable\nthan fine-tuning all the parameters of the model.", "published": "2023-09-19 21:06:22", "link": "http://arxiv.org/abs/2309.10930v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Posterior sampling algorithms for unsupervised speech enhancement with\n  recurrent variational autoencoder", "abstract": "In this paper, we address the unsupervised speech enhancement problem based\non recurrent variational autoencoder (RVAE). This approach offers promising\ngeneralization performance over the supervised counterpart. Nevertheless, the\ninvolved iterative variational expectation-maximization (VEM) process at test\ntime, which relies on a variational inference method, results in high\ncomputational complexity. To tackle this issue, we present efficient sampling\ntechniques based on Langevin dynamics and Metropolis-Hasting algorithms,\nadapted to the EM-based speech enhancement with RVAE. By directly sampling from\nthe intractable posterior distribution within the EM process, we circumvent the\nintricacies of variational inference. We conduct a series of experiments,\ncomparing the proposed methods with VEM and a state-of-the-art supervised\nspeech enhancement approach based on diffusion models. The results reveal that\nour sampling-based algorithms significantly outperform VEM, not only in terms\nof computational efficiency but also in overall performance. Furthermore, when\ncompared to the supervised baseline, our methods showcase robust generalization\nperformance in mismatched test conditions.", "published": "2023-09-19 08:59:32", "link": "http://arxiv.org/abs/2309.10439v1", "categories": ["cs.CV", "cs.SD", "eess.AS", "eess.SP", "stat.ML"], "primary_category": "cs.CV"}
{"title": "Unsupervised speech enhancement with diffusion-based generative models", "abstract": "Recently, conditional score-based diffusion models have gained significant\nattention in the field of supervised speech enhancement, yielding\nstate-of-the-art performance. However, these methods may face challenges when\ngeneralising to unseen conditions. To address this issue, we introduce an\nalternative approach that operates in an unsupervised manner, leveraging the\ngenerative power of diffusion models. Specifically, in a training phase, a\nclean speech prior distribution is learnt in the short-time Fourier transform\n(STFT) domain using score-based diffusion models, allowing it to\nunconditionally generate clean speech from Gaussian noise. Then, we develop a\nposterior sampling methodology for speech enhancement by combining the learnt\nclean speech prior with a noise model for speech signal inference. The noise\nparameters are simultaneously learnt along with clean speech estimation through\nan iterative expectationmaximisation (EM) approach. To the best of our\nknowledge, this is the first work exploring diffusion-based generative models\nfor unsupervised speech enhancement, demonstrating promising results compared\nto a recent variational auto-encoder (VAE)-based unsupervised approach and a\nstate-of-the-art diffusion-based supervised method. It thus opens a new\ndirection for future research in unsupervised speech enhancement.", "published": "2023-09-19 09:11:31", "link": "http://arxiv.org/abs/2309.10450v1", "categories": ["cs.CV", "cs.SD", "eess.AS", "eess.SP", "stat.ML"], "primary_category": "cs.CV"}
{"title": "Diffusion-based speech enhancement with a weighted generative-supervised\n  learning loss", "abstract": "Diffusion-based generative models have recently gained attention in speech\nenhancement (SE), providing an alternative to conventional supervised methods.\nThese models transform clean speech training samples into Gaussian noise\ncentered at noisy speech, and subsequently learn a parameterized model to\nreverse this process, conditionally on noisy speech. Unlike supervised methods,\ngenerative-based SE approaches usually rely solely on an unsupervised loss,\nwhich may result in less efficient incorporation of conditioned noisy speech.\nTo address this issue, we propose augmenting the original diffusion training\nobjective with a mean squared error (MSE) loss, measuring the discrepancy\nbetween estimated enhanced speech and ground-truth clean speech at each reverse\nprocess iteration. Experimental results demonstrate the effectiveness of our\nproposed methodology.", "published": "2023-09-19 09:13:35", "link": "http://arxiv.org/abs/2309.10457v1", "categories": ["cs.CV", "cs.SD", "eess.AS", "eess.SP", "stat.ML"], "primary_category": "cs.CV"}
{"title": "Sound Source Localization is All about Cross-Modal Alignment", "abstract": "Humans can easily perceive the direction of sound sources in a visual scene,\ntermed sound source localization. Recent studies on learning-based sound source\nlocalization have mainly explored the problem from a localization perspective.\nHowever, prior arts and existing benchmarks do not account for a more important\naspect of the problem, cross-modal semantic understanding, which is essential\nfor genuine sound source localization. Cross-modal semantic understanding is\nimportant in understanding semantically mismatched audio-visual events, e.g.,\nsilent objects, or off-screen sounds. To account for this, we propose a\ncross-modal alignment task as a joint task with sound source localization to\nbetter learn the interaction between audio and visual modalities. Thereby, we\nachieve high localization performance with strong cross-modal semantic\nunderstanding. Our method outperforms the state-of-the-art approaches in both\nsound source localization and cross-modal retrieval. Our work suggests that\njointly tackling both tasks is necessary to conquer genuine sound source\nlocalization.", "published": "2023-09-19 16:04:50", "link": "http://arxiv.org/abs/2309.10724v1", "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
