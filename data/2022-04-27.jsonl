{"title": "Executive Function: A Contrastive Value Policy for Resampling and\n  Relabeling Perceptions via Hindsight Summarization?", "abstract": "We develop the few-shot continual learning task from first principles and\nhypothesize an evolutionary motivation and mechanism of action for executive\nfunction as a contrastive value policy which resamples and relabels perception\ndata via hindsight summarization to minimize attended prediction error, similar\nto an online prompt engineering problem. This is made feasible by the use of a\nmemory policy and a pretrained network with inductive biases for a grammar of\nlearning and is trained to maximize evolutionary survival. We show how this\nmodel of executive function can be used to implement hypothesis testing as a\nstream of consciousness and may explain observations of human few-shot learning\nand neuroanatomy.", "published": "2022-04-27 00:07:44", "link": "http://arxiv.org/abs/2204.12639v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Better Query Graph Selection for Knowledge Base Question Answering", "abstract": "This paper presents a novel approach based on semantic parsing to improve the\nperformance of Knowledge Base Question Answering (KBQA). Specifically, we focus\non how to select an optimal query graph from a candidate set so as to retrieve\nthe answer from knowledge base (KB). In our approach, we first propose to\nlinearize the query graph into a sequence, which is used to form a sequence\npair with the question. It allows us to use mature sequence modeling, such as\nBERT, to encode the sequence pair. Then we use a ranking method to sort\ncandidate query graphs. In contrast to the previous studies, our approach can\nefficiently model semantic interactions between the graph and the question as\nwell as rank the candidate graphs from a global view. The experimental results\nshow that our system achieves the top performance on ComplexQuestions and the\nsecond best performance on WebQuestions.", "published": "2022-04-27 01:53:06", "link": "http://arxiv.org/abs/2204.12662v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data-Driven Adaptive Simultaneous Machine Translation", "abstract": "In simultaneous translation (SimulMT), the most widely used strategy is the\nwait-k policy thanks to its simplicity and effectiveness in balancing\ntranslation quality and latency. However, wait-k suffers from two major\nlimitations: (a) it is a fixed policy that can not adaptively adjust latency\ngiven context, and (b) its training is much slower than full-sentence\ntranslation. To alleviate these issues, we propose a novel and efficient\ntraining scheme for adaptive SimulMT by augmenting the training corpus with\nadaptive prefix-to-prefix pairs, while the training complexity remains the same\nas that of training full-sentence translation models. Experiments on two\nlanguage pairs show that our method outperforms all strong baselines in terms\nof translation quality and latency.", "published": "2022-04-27 02:40:21", "link": "http://arxiv.org/abs/2204.12672v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Document-Level Relation Extraction with Sentences Importance Estimation\n  and Focusing", "abstract": "Document-level relation extraction (DocRE) aims to determine the relation\nbetween two entities from a document of multiple sentences. Recent studies\ntypically represent the entire document by sequence- or graph-based models to\npredict the relations of all entity pairs. However, we find that such a model\nis not robust and exhibits bizarre behaviors: it predicts correctly when an\nentire test document is fed as input, but errs when non-evidence sentences are\nremoved. To this end, we propose a Sentence Importance Estimation and Focusing\n(SIEF) framework for DocRE, where we design a sentence importance score and a\nsentence focusing loss, encouraging DocRE models to focus on evidence\nsentences. Experimental results on two domains show that our SIEF not only\nimproves overall performance, but also makes DocRE models more robust.\nMoreover, SIEF is a general framework, shown to be effective when combined with\na variety of base DocRE models.", "published": "2022-04-27 03:20:07", "link": "http://arxiv.org/abs/2204.12679v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building Knowledge-Grounded Dialogue Systems with Graph-Based Semantic\n  Modeling", "abstract": "The knowledge-grounded dialogue task aims to generate responses that convey\ninformation from given knowledge documents. However, it is a challenge for the\ncurrent sequence-based model to acquire knowledge from complex documents and\nintegrate it to perform correct responses without the aid of an explicit\nsemantic structure. To address these issues, we propose a novel graph\nstructure, Grounded Graph ($G^2$), that models the semantic structure of both\ndialogue and knowledge to facilitate knowledge selection and integration for\nknowledge-grounded dialogue generation. We also propose a Grounded Graph Aware\nTransformer ($G^2AT$) model that fuses multi-forms knowledge (both sequential\nand graphic) to enhance knowledge-grounded response generation. Our experiments\nresults show that our proposed model outperforms the previous state-of-the-art\nmethods with more than 10\\% gains in response generation and nearly 20\\%\nimprovement in factual consistency. Further, our model reveals good\ngeneralization ability and robustness. By incorporating semantic structures as\nprior knowledge in deep neural networks, our model provides an effective way to\naid language generation.", "published": "2022-04-27 03:31:46", "link": "http://arxiv.org/abs/2204.12681v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distant finetuning with discourse relations for stance classification", "abstract": "Approaches for the stance classification task, an important task for\nunderstanding argumentation in debates and detecting fake news, have been\nrelying on models which deal with individual debate topics. In this paper, in\norder to train a system independent from topics, we propose a new method to\nextract data with silver labels from raw text to finetune a model for stance\nclassification. The extraction relies on specific discourse relation\ninformation, which is shown as a reliable and accurate source for providing\nstance information. We also propose a 3-stage training framework where the\nnoisy level in the data used for finetuning decreases over different stages\ngoing from the most noisy to the least noisy. Detailed experiments show that\nthe automatically annotated dataset as well as the 3-stage training help\nimprove model performance in stance classification. Our approach ranks 1st\namong 26 competing teams in the stance classification track of the NLPCC 2021\nshared task Argumentative Text Understanding for AI Debater, which confirms the\neffectiveness of our approach.", "published": "2022-04-27 04:24:35", "link": "http://arxiv.org/abs/2204.12693v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Limitations of Dataset Balancing: The Lost Battle Against\n  Spurious Correlations", "abstract": "Recent work has shown that deep learning models in NLP are highly sensitive\nto low-level correlations between simple features and specific output labels,\nleading to overfitting and lack of generalization. To mitigate this problem, a\ncommon practice is to balance datasets by adding new instances or by filtering\nout \"easy\" instances (Sakaguchi et al., 2020), culminating in a recent proposal\nto eliminate single-word correlations altogether (Gardner et al., 2021). In\nthis opinion paper, we identify that despite these efforts,\nincreasingly-powerful models keep exploiting ever-smaller spurious\ncorrelations, and as a result even balancing all single-word features is\ninsufficient for mitigating all of these correlations. In parallel, a truly\nbalanced dataset may be bound to \"throw the baby out with the bathwater\" and\nmiss important signal encoding common sense and world knowledge. We highlight\nseveral alternatives to dataset balancing, focusing on enhancing datasets with\nricher contexts, allowing models to abstain and interact with users, and\nturning from large-scale fine-tuning to zero- or few-shot setups.", "published": "2022-04-27 05:42:40", "link": "http://arxiv.org/abs/2204.12708v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CREER: A Large-Scale Corpus for Relation Extraction and Entity\n  Recognition", "abstract": "We describe the design and use of the CREER dataset, a large corpus annotated\nwith rich English grammar and semantic attributes. The CREER dataset uses the\nStanford CoreNLP Annotator to capture rich language structures from Wikipedia\nplain text. This dataset follows widely used linguistic and semantic\nannotations so that it can be used for not only most natural language\nprocessing tasks but also scaling the dataset. This large supervised dataset\ncan serve as the basis for improving the performance of NLP tasks in the\nfuture. We publicize the dataset through the link:\nhttps://140.116.82.111/share.cgi?ssid=000dOJ4", "published": "2022-04-27 05:43:21", "link": "http://arxiv.org/abs/2204.12710v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Propose-and-Refine: A Two-Stage Set Prediction Network for Nested Named\n  Entity Recognition", "abstract": "Nested named entity recognition (nested NER) is a fundamental task in natural\nlanguage processing. Various span-based methods have been proposed to detect\nnested entities with span representations. However, span-based methods do not\nconsider the relationship between a span and other entities or phrases, which\nis helpful in the NER task. Besides, span-based methods have trouble predicting\nlong entities due to limited span enumeration length. To mitigate these issues,\nwe present the Propose-and-Refine Network (PnRNet), a two-stage set prediction\nnetwork for nested NER. In the propose stage, we use a span-based predictor to\ngenerate some coarse entity predictions as entity proposals. In the refine\nstage, proposals interact with each other, and richer contextual information is\nincorporated into the proposal representations. The refined proposal\nrepresentations are used to re-predict entity boundaries and classes. In this\nway, errors in coarse proposals can be eliminated, and the boundary prediction\nis no longer constrained by the span enumeration length limitation.\nAdditionally, we build multi-scale sentence representations, which better model\nthe hierarchical structure of sentences and provide richer contextual\ninformation than token-level representations. Experiments show that PnRNet\nachieves state-of-the-art performance on four nested NER datasets and one flat\nNER dataset.", "published": "2022-04-27 06:58:45", "link": "http://arxiv.org/abs/2204.12732v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Control Globally, Understand Locally: A Global-to-Local Hierarchical\n  Graph Network for Emotional Support Conversation", "abstract": "Emotional support conversation aims at reducing the emotional distress of the\nhelp-seeker, which is a new and challenging task. It requires the system to\nexplore the cause of help-seeker's emotional distress and understand their\npsychological intention to provide supportive responses. However, existing\nmethods mainly focus on the sequential contextual information, ignoring the\nhierarchical relationships with the global cause and local psychological\nintention behind conversations, thus leads to a weak ability of emotional\nsupport. In this paper, we propose a Global-to-Local Hierarchical Graph Network\nto capture the multi-source information (global cause, local intentions and\ndialog history) and model hierarchical relationships between them, which\nconsists of a multi-source encoder, a hierarchical graph reasoner, and a\nglobal-guide decoder. Furthermore, a novel training objective is designed to\nmonitor semantic information of the global cause. Experimental results on the\nemotional support conversation dataset, ESConv, confirm that the proposed GLHG\nhas achieved the state-of-the-art performance on the automatic and human\nevaluations. The code will be released in here\n\\footnote{\\small{~https://github.com/pengwei-iie/GLHG}}.", "published": "2022-04-27 07:43:29", "link": "http://arxiv.org/abs/2204.12749v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Understanding of Code-mixed Language Semantics using\n  Hierarchical Transformer", "abstract": "Being a popular mode of text-based communication in multilingual communities,\ncode-mixing in online social media has became an important subject to study.\nLearning the semantics and morphology of code-mixed language remains a key\nchallenge, due to scarcity of data and unavailability of robust and\nlanguage-invariant representation learning technique. Any morphologically-rich\nlanguage can benefit from character, subword, and word-level embeddings, aiding\nin learning meaningful correlations. In this paper, we explore a hierarchical\ntransformer-based architecture (HIT) to learn the semantics of code-mixed\nlanguages. HIT consists of multi-headed self-attention and outer product\nattention components to simultaneously comprehend the semantic and syntactic\nstructures of code-mixed texts. We evaluate the proposed method across 6 Indian\nlanguages (Bengali, Gujarati, Hindi, Tamil, Telugu and Malayalam) and Spanish\nfor 9 NLP tasks on 17 datasets. The HIT model outperforms state-of-the-art\ncode-mixed representation learning and multilingual language models in all\ntasks. We further demonstrate the generalizability of the HIT architecture\nusing masked language modeling-based pre-training, zero-shot learning, and\ntransfer learning approaches. Our empirical results show that the pre-training\nobjectives significantly improve the performance on downstream tasks.", "published": "2022-04-27 07:50:18", "link": "http://arxiv.org/abs/2204.12753v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Plug-and-Play Adaptation for Continuously-updated QA", "abstract": "Language models (LMs) have shown great potential as implicit knowledge bases\n(KBs). And for their practical use, knowledge in LMs need to be updated\nperiodically. However, existing tasks to assess LMs' efficacy as KBs do not\nadequately consider multiple large-scale updates. To this end, we first propose\na novel task--Continuously-updated QA (CuQA)--in which multiple large-scale\nupdates are made to LMs, and the performance is measured with respect to the\nsuccess in adding and updating knowledge while retaining existing knowledge. We\nthen present LMs with plug-in modules that effectively handle the updates.\nExperiments conducted on zsRE QA and NQ datasets show that our method\noutperforms existing approaches. We find that our method is 4x more effective\nin terms of updates/forgets ratio, compared to a fine-tuning baseline.", "published": "2022-04-27 09:11:16", "link": "http://arxiv.org/abs/2204.12785v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Method of Query Graph Reranking for Knowledge Base Question Answering", "abstract": "This paper presents a novel reranking method to better choose the optimal\nquery graph, a sub-graph of knowledge graph, to retrieve the answer for an\ninput question in Knowledge Base Question Answering (KBQA). Existing methods\nsuffer from a severe problem that there is a significant gap between top-1\nperformance and the oracle score of top-n results. To address this problem, our\nmethod divides the choosing procedure into two steps: query graph ranking and\nquery graph reranking. In the first step, we provide top-n query graphs for\neach question. Then we propose to rerank the top-n query graphs by combining\nwith the information of answer type. Experimental results on two widely used\ndatasets show that our proposed method achieves the best results on the\nWebQuestions dataset and the second best on the ComplexQuestions dataset.", "published": "2022-04-27 09:57:54", "link": "http://arxiv.org/abs/2204.12808v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SkillSpan: Hard and Soft Skill Extraction from English Job Postings", "abstract": "Skill Extraction (SE) is an important and widely-studied task useful to gain\ninsights into labor market dynamics. However, there is a lacuna of datasets and\nannotation guidelines; available datasets are few and contain crowd-sourced\nlabels on the span-level or labels from a predefined skill inventory. To\naddress this gap, we introduce SKILLSPAN, a novel SE dataset consisting of\n14.5K sentences and over 12.5K annotated spans. We release its respective\nguidelines created over three different sources annotated for hard and soft\nskills by domain experts. We introduce a BERT baseline (Devlin et al., 2019).\nTo improve upon this baseline, we experiment with language models that are\noptimized for long spans (Joshi et al., 2020; Beltagy et al., 2020), continuous\npre-training on the job posting domain (Han and Eisenstein, 2019; Gururangan et\nal., 2020), and multi-task learning (Caruana, 1997). Our results show that the\ndomain-adapted models significantly outperform their non-adapted counterparts,\nand single-task outperforms multi-task learning.", "published": "2022-04-27 10:07:36", "link": "http://arxiv.org/abs/2204.12811v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LyS_ACoru\u00f1a at SemEval-2022 Task 10: Repurposing Off-the-Shelf Tools\n  for Sentiment Analysis as Semantic Dependency Parsing", "abstract": "This paper addressed the problem of structured sentiment analysis using a\nbi-affine semantic dependency parser, large pre-trained language models, and\npublicly available translation models. For the monolingual setup, we\nconsidered: (i) training on a single treebank, and (ii) relaxing the setup by\ntraining on treebanks coming from different languages that can be adequately\nprocessed by cross-lingual language models. For the zero-shot setup and a given\ntarget treebank, we relied on: (i) a word-level translation of available\ntreebanks in other languages to get noisy, unlikely-grammatical, but annotated\ndata (we release as much of it as licenses allow), and (ii) merging those\ntranslated treebanks to obtain training data. In the post-evaluation phase, we\nalso trained cross-lingual models that simply merged all the English treebanks\nand did not use word-level translations, and yet obtained better results.\nAccording to the official results, we ranked 8th and 9th in the monolingual and\ncross-lingual setups.", "published": "2022-04-27 10:21:28", "link": "http://arxiv.org/abs/2204.12820v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Query2Particles: Knowledge Graph Reasoning with Particle Embeddings", "abstract": "Answering complex logical queries on incomplete knowledge graphs (KGs) with\nmissing edges is a fundamental and important task for knowledge graph\nreasoning. The query embedding method is proposed to answer these queries by\njointly encoding queries and entities to the same embedding space. Then the\nanswer entities are selected according to the similarities between the entity\nembeddings and the query embedding. As the answers to a complex query are\nobtained from a combination of logical operations over sub-queries, the\nembeddings of the answer entities may not always follow a uni-modal\ndistribution in the embedding space. Thus, it is challenging to simultaneously\nretrieve a set of diverse answers from the embedding space using a single and\nconcentrated query representation such as a vector or a hyper-rectangle. To\nbetter cope with queries with diversified answers, we propose Query2Particles\n(Q2P), a complex KG query answering method. Q2P encodes each query into\nmultiple vectors, named particle embeddings. By doing so, the candidate answers\ncan be retrieved from different areas over the embedding space using the\nmaximal similarities between the entity embeddings and any of the particle\nembeddings. Meanwhile, the corresponding neural logic operations are defined to\nsupport its reasoning over arbitrary first-order logic queries. The experiments\nshow that Query2Particles achieves state-of-the-art performance on the complex\nquery answering tasks on FB15k, FB15K-237, and NELL knowledge graphs.", "published": "2022-04-27 11:16:08", "link": "http://arxiv.org/abs/2204.12847v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extremal GloVe: Theoretically Accurate Distributed Word Embedding by\n  Tail Inference", "abstract": "Distributed word embeddings such as Word2Vec and GloVe have been widely\nadopted in industrial context settings. Major technical applications of GloVe\ninclude recommender systems and natural language processing. The fundamental\ntheory behind GloVe relies on the selection of a weighting function in the\nweighted least squres formulation that computes the powered ratio of word\noccurrence count and the maximum word count in the corpus. However, the initial\nformulation of GloVe is not theoretically sound in two aspects, namely the\nselection of the weighting function and its power exponent is ad-hoc. In this\npaper, we utilize the theory of extreme value analysis and propose a\ntheoretically accurate version of GloVe. By reformulating the weighted least\nsquares loss function as the expected loss function and accurately choosing the\npower exponent, we create a theoretically accurate version of GloVe. We\ndemonstrate the competitiveness of our algorithm and show that the initial\nformulation of GloVe with the suggested optimal parameter can be viewed as a\nspecial case of our paradigm.", "published": "2022-04-27 15:29:10", "link": "http://arxiv.org/abs/2204.13009v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for\n  Dialog Response Generation", "abstract": "Dialog response generation in open domain is an important research topic\nwhere the main challenge is to generate relevant and diverse responses. In this\npaper, we propose a new dialog pre-training framework called DialogVED, which\nintroduces continuous latent variables into the enhanced encoder-decoder\npre-training framework to increase the relevance and diversity of responses.\nWith the help of a large dialog corpus (Reddit), we pre-train the model using\nthe following 4 tasks adopted in language models (LMs) and variational\nautoencoders (VAEs): 1) masked language model; 2) response generation; 3)\nbag-of-words prediction; and 4) KL divergence reduction. We also add additional\nparameters to model the turn structure in dialogs to improve the performance of\nthe pre-trained model. We conduct experiments on PersonaChat, DailyDialog, and\nDSTC7-AVSD benchmarks for response generation. Experimental results show that\nour model achieves the new state-of-the-art results on all these datasets.", "published": "2022-04-27 16:18:15", "link": "http://arxiv.org/abs/2204.13031v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BiTimeBERT: Extending Pre-Trained Language Representations with\n  Bi-Temporal Information", "abstract": "Time is an important aspect of documents and is used in a range of NLP and IR\ntasks. In this work, we investigate methods for incorporating temporal\ninformation during pre-training to further improve the performance on\ntime-related tasks. Compared with common pre-trained language models like BERT\nwhich utilize synchronic document collections (e.g., BookCorpus and Wikipedia)\nas the training corpora, we use long-span temporal news article collection for\nbuilding word representations. We introduce BiTimeBERT, a novel language\nrepresentation model trained on a temporal collection of news articles via two\nnew pre-training tasks, which harnesses two distinct temporal signals to\nconstruct time-aware language representations. The experimental results show\nthat BiTimeBERT consistently outperforms BERT and other existing pre-trained\nmodels with substantial gains on different downstream NLP tasks and\napplications for which time is of importance (e.g., the accuracy improvement\nover BERT is 155\\% on the event time estimation task).", "published": "2022-04-27 16:20:09", "link": "http://arxiv.org/abs/2204.13032v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Borrow -- Relation Representation for Without-Mention\n  Entity-Pairs for Knowledge Graph Completion", "abstract": "Prior work on integrating text corpora with knowledge graphs (KGs) to improve\nKnowledge Graph Embedding (KGE) have obtained good performance for entities\nthat co-occur in sentences in text corpora. Such sentences (textual mentions of\nentity-pairs) are represented as Lexicalised Dependency Paths (LDPs) between\ntwo entities. However, it is not possible to represent relations between\nentities that do not co-occur in a single sentence using LDPs. In this paper,\nwe propose and evaluate several methods to address this problem, where we\nborrow LDPs from the entity pairs that co-occur in sentences in the corpus\n(i.e. with mention entity pairs) to represent entity pairs that do not co-occur\nin any sentence in the corpus (i.e. without mention entity pairs). We propose a\nsupervised borrowing method, SuperBorrow, that learns to score the suitability\nof an LDP to represent a without-mention entity pair using pre-trained entity\nembeddings and contextualised LDP representations. Experimental results show\nthat SuperBorrow improves the link prediction performance of multiple\nwidely-used prior KGE methods such as TransE, DistMult, ComplEx and RotatE.", "published": "2022-04-27 17:49:05", "link": "http://arxiv.org/abs/2204.13097v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RigoBERTa: A State-of-the-Art Language Model For Spanish", "abstract": "This paper presents RigoBERTa, a State-of-the-Art Language Model for Spanish.\nRigoBERTa is trained over a well-curated corpus formed up from different\nsubcorpora with key features. It follows the DeBERTa architecture, which has\nseveral advantages over other architectures of similar size as BERT or RoBERTa.\nRigoBERTa performance is assessed over 13 NLU tasks in comparison with other\navailable Spanish language models, namely, MarIA, BERTIN and BETO. RigoBERTa\noutperformed the three models in 10 out of the 13 tasks, achieving new\n\"State-of-the-Art\" results.", "published": "2022-04-27 11:53:25", "link": "http://arxiv.org/abs/2205.10233v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Span-level Bidirectional Network for Aspect Sentiment Triplet\n  Extraction", "abstract": "Aspect Sentiment Triplet Extraction (ASTE) is a new fine-grained sentiment\nanalysis task that aims to extract triplets of aspect terms, sentiments, and\nopinion terms from review sentences. Recently, span-level models achieve\ngratifying results on ASTE task by taking advantage of the predictions of all\npossible spans. Since all possible spans significantly increases the number of\npotential aspect and opinion candidates, it is crucial and challenging to\nefficiently extract the triplet elements among them. In this paper, we present\na span-level bidirectional network which utilizes all possible spans as input\nand extracts triplets from spans bidirectionally. Specifically, we devise both\nthe aspect decoder and opinion decoder to decode the span representations and\nextract triples from aspect-to-opinion and opinion-to-aspect directions. With\nthese two decoders complementing with each other, the whole network can extract\ntriplets from spans more comprehensively. Moreover, considering that mutual\nexclusion cannot be guaranteed between the spans, we design a similar span\nseparation loss to facilitate the downstream task of distinguishing the correct\nspan by expanding the KL divergence of similar spans during the training\nprocess; in the inference process, we adopt an inference strategy to remove\nconflicting triplets from the results base on their confidence scores.\nExperimental results show that our framework not only significantly outperforms\nstate-of-the-art methods, but achieves better performance in predicting\ntriplets with multi-token entities and extracting triplets in sentences contain\nmulti-triplets.", "published": "2022-04-27 02:55:43", "link": "http://arxiv.org/abs/2204.12674v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UBERT: A Novel Language Model for Synonymy Prediction at Scale in the\n  UMLS Metathesaurus", "abstract": "The UMLS Metathesaurus integrates more than 200 biomedical source\nvocabularies. During the Metathesaurus construction process, synonymous terms\nare clustered into concepts by human editors, assisted by lexical similarity\nalgorithms. This process is error-prone and time-consuming. Recently, a deep\nlearning model (LexLM) has been developed for the UMLS Vocabulary Alignment\n(UVA) task. This work introduces UBERT, a BERT-based language model, pretrained\non UMLS terms via a supervised Synonymy Prediction (SP) task replacing the\noriginal Next Sentence Prediction (NSP) task. The effectiveness of UBERT for\nUMLS Metathesaurus construction process is evaluated using the UMLS Vocabulary\nAlignment (UVA) task. We show that UBERT outperforms the LexLM, as well as\nbiomedical BERT-based models. Key to the performance of UBERT are the synonymy\nprediction task specifically developed for UBERT, the tight alignment of\ntraining data to the UVA task, and the similarity of the models used for\npretrained UBERT.", "published": "2022-04-27 06:03:24", "link": "http://arxiv.org/abs/2204.12716v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Thorough Examination on Zero-shot Dense Retrieval", "abstract": "Recent years have witnessed the significant advance in dense retrieval (DR)\nbased on powerful pre-trained language models (PLM). DR models have achieved\nexcellent performance in several benchmark datasets, while they are shown to be\nnot as competitive as traditional sparse retrieval models (e.g., BM25) in a\nzero-shot retrieval setting. However, in the related literature, there still\nlacks a detailed and comprehensive study on zero-shot retrieval. In this paper,\nwe present the first thorough examination of the zero-shot capability of DR\nmodels. We aim to identify the key factors and analyze how they affect\nzero-shot retrieval performance. In particular, we discuss the effect of\nseveral key factors related to source training set, analyze the potential bias\nfrom the target dataset, and review and compare existing zero-shot DR models.\nOur findings provide important evidence to better understand and develop\nzero-shot DR models.", "published": "2022-04-27 07:59:07", "link": "http://arxiv.org/abs/2204.12755v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Learn from Structural Scope: Improving Aspect-Level Sentiment Analysis\n  with Hybrid Graph Convolutional Networks", "abstract": "Aspect-level sentiment analysis aims to determine the sentiment polarity\ntowards a specific target in a sentence. The main challenge of this task is to\neffectively model the relation between targets and sentiments so as to filter\nout noisy opinion words from irrelevant targets. Most recent efforts capture\nrelations through target-sentiment pairs or opinion spans from a word-level or\nphrase-level perspective. Based on the observation that targets and sentiments\nessentially establish relations following the grammatical hierarchy of\nphrase-clause-sentence structure, it is hopeful to exploit comprehensive\nsyntactic information for better guiding the learning process. Therefore, we\nintroduce the concept of Scope, which outlines a structural text region related\nto a specific target. To jointly learn structural Scope and predict the\nsentiment polarity, we propose a hybrid graph convolutional network (HGCN) to\nsynthesize information from constituency tree and dependency tree, exploring\nthe potential of linking two syntax parsing methods to enrich the\nrepresentation. Experimental results on four public datasets illustrate that\nour HGCN model outperforms current state-of-the-art baselines.", "published": "2022-04-27 09:10:22", "link": "http://arxiv.org/abs/2204.12784v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Modern Baselines for SPARQL Semantic Parsing", "abstract": "In this work, we focus on the task of generating SPARQL queries from natural\nlanguage questions, which can then be executed on Knowledge Graphs (KGs). We\nassume that gold entity and relations have been provided, and the remaining\ntask is to arrange them in the right order along with SPARQL vocabulary, and\ninput tokens to produce the correct SPARQL query. Pre-trained Language Models\n(PLMs) have not been explored in depth on this task so far, so we experiment\nwith BART, T5 and PGNs (Pointer Generator Networks) with BERT embeddings,\nlooking for new baselines in the PLM era for this task, on DBpedia and Wikidata\nKGs. We show that T5 requires special input tokenisation, but produces state of\nthe art performance on LC-QuAD 1.0 and LC-QuAD 2.0 datasets, and outperforms\ntask-specific models from previous works. Moreover, the methods enable semantic\nparsing for questions where a part of the input needs to be copied to the\noutput query, thus enabling a new paradigm in KG semantic parsing.", "published": "2022-04-27 09:26:59", "link": "http://arxiv.org/abs/2204.12793v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Probing Simile Knowledge from Pre-trained Language Models", "abstract": "Simile interpretation (SI) and simile generation (SG) are challenging tasks\nfor NLP because models require adequate world knowledge to produce predictions.\nPrevious works have employed many hand-crafted resources to bring\nknowledge-related into models, which is time-consuming and labor-intensive. In\nrecent years, pre-trained language models (PLMs) based approaches have become\nthe de-facto standard in NLP since they learn generic knowledge from a large\ncorpus. The knowledge embedded in PLMs may be useful for SI and SG tasks.\nNevertheless, there are few works to explore it. In this paper, we probe simile\nknowledge from PLMs to solve the SI and SG tasks in the unified framework of\nsimile triple completion for the first time. The backbone of our framework is\nto construct masked sentences with manual patterns and then predict the\ncandidate words in the masked position. In this framework, we adopt a secondary\ntraining process (Adjective-Noun mask Training) with the masked language model\n(MLM) loss to enhance the prediction diversity of candidate words in the masked\nposition. Moreover, pattern ensemble (PE) and pattern search (PS) are applied\nto improve the quality of predicted words. Finally, automatic and human\nevaluations demonstrate the effectiveness of our framework in both SI and SG\ntasks.", "published": "2022-04-27 09:55:40", "link": "http://arxiv.org/abs/2204.12807v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AdaCoach: A Virtual Coach for Training Customer Service Agents", "abstract": "With the development of online business, customer service agents gradually\nplay a crucial role as an interface between the companies and their customers.\nMost companies spend a lot of time and effort on hiring and training customer\nservice agents. To this end, we propose AdaCoach: A Virtual Coach for Training\nCustomer Service Agents, to promote the ability of newly hired service agents\nbefore they get to work. AdaCoach is designed to simulate real customers who\nseek help and actively initiate the dialogue with the customer service agents.\nBesides, AdaCoach uses an automated dialogue evaluation model to score the\nperformance of the customer agent in the training process, which can provide\nnecessary assistance when the newly hired customer service agent encounters\nproblems. We apply recent NLP technologies to ensure efficient run-time\nperformance in the deployed system. To the best of our knowledge, this is the\nfirst system that trains the customer service agent through human-computer\ninteraction. Until now, the system has already supported more than 500,000\nsimulation training and cultivated over 1000 qualified customer service agents.", "published": "2022-04-27 13:39:27", "link": "http://arxiv.org/abs/2204.12935v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An End-to-End Dialogue Summarization System for Sales Calls", "abstract": "Summarizing sales calls is a routine task performed manually by salespeople.\nWe present a production system which combines generative models fine-tuned for\ncustomer-agent setting, with a human-in-the-loop user experience for an\ninteractive summary curation process. We address challenging aspects of\ndialogue summarization task in a real-world setting including long input\ndialogues, content validation, lack of labeled data and quality evaluation. We\nshow how GPT-3 can be leveraged as an offline data labeler to handle training\ndata scarcity and accommodate privacy constraints in an industrial setting.\nExperiments show significant improvements by our models in tackling the\nsummarization and content validation tasks on public datasets.", "published": "2022-04-27 14:02:50", "link": "http://arxiv.org/abs/2204.12951v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "NLU++: A Multi-Label, Slot-Rich, Generalisable Dataset for Natural\n  Language Understanding in Task-Oriented Dialogue", "abstract": "We present NLU++, a novel dataset for natural language understanding (NLU) in\ntask-oriented dialogue (ToD) systems, with the aim to provide a much more\nchallenging evaluation environment for dialogue NLU models, up to date with the\ncurrent application and industry requirements. NLU++ is divided into two\ndomains (BANKING and HOTELS) and brings several crucial improvements over\ncurrent commonly used NLU datasets. 1) NLU++ provides fine-grained domain\nontologies with a large set of challenging multi-intent sentences, introducing\nand validating the idea of intent modules that can be combined into complex\nintents that convey complex user goals, combined with finer-grained and thus\nmore challenging slot sets. 2) The ontology is divided into domain-specific and\ngeneric (i.e., domain-universal) intent modules that overlap across domains,\npromoting cross-domain reusability of annotated examples. 3) The dataset design\nhas been inspired by the problems observed in industrial ToD systems, and 4) it\nhas been collected, filtered and carefully annotated by dialogue NLU experts,\nyielding high-quality annotated data. Finally, we benchmark a series of current\nstate-of-the-art NLU models on NLU++; the results demonstrate the challenging\nnature of the dataset, especially in low-data regimes, the validity of `intent\nmodularisation', and call for further research on ToD NLU.", "published": "2022-04-27 16:00:23", "link": "http://arxiv.org/abs/2204.13021v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Teachable Reasoning Systems: Using a Dynamic Memory of User\n  Feedback for Continual System Improvement", "abstract": "Our goal is a teachable reasoning system for question-answering (QA), where a\nuser can interact with faithful answer explanations, and correct its errors so\nthat the system improves over time. Our approach is to augment a QA model with\na dynamic memory of user feedback, containing user-supplied corrections to\nerroneous model beliefs that users identify during interaction. Retrievals from\nmemory are used as additional context for QA, to help avoid previous mistakes\nin similar new situations - a novel application of memory-based continuous\nlearning. With simulated feedback, we find that our system (called TeachMe)\ncontinually improves with time, and without model retraining, requiring\nfeedback on only 25% of training examples to reach within 1% of the upper-bound\n(feedback on all examples). Similarly, in experiments with real users, we\nobserve a similar trend, with performance improving by over 15% on a hidden\ntest set after teaching. This suggests new opportunities for using frozen\nlanguage models in an interactive setting where users can inspect, debug, and\ncorrect the model's beliefs, leading to improved system's performance over\ntime.", "published": "2022-04-27 17:15:07", "link": "http://arxiv.org/abs/2204.13074v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Counterfactual Explanations for Natural Language Interfaces", "abstract": "A key challenge facing natural language interfaces is enabling users to\nunderstand the capabilities of the underlying system. We propose a novel\napproach for generating explanations of a natural language interface based on\nsemantic parsing. We focus on counterfactual explanations, which are post-hoc\nexplanations that describe to the user how they could have minimally modified\ntheir utterance to achieve their desired goal. In particular, the user provides\nan utterance along with a demonstration of their desired goal; then, our\nalgorithm synthesizes a paraphrase of their utterance that is guaranteed to\nachieve their goal. In two user studies, we demonstrate that our approach\nsubstantially improves user performance, and that it generates explanations\nthat more closely match the user's intent compared to two ablations.", "published": "2022-04-27 20:53:02", "link": "http://arxiv.org/abs/2204.13192v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adaptable Text Matching via Meta-Weight Regulator", "abstract": "Neural text matching models have been used in a range of applications such as\nquestion answering and natural language inference, and have yielded a good\nperformance. However, these neural models are of a limited adaptability,\nresulting in a decline in performance when encountering test examples from a\ndifferent dataset or even a different task. The adaptability is particularly\nimportant in the few-shot setting: in many cases, there is only a limited\namount of labeled data available for a target dataset or task, while we may\nhave access to a richly labeled source dataset or task. However, adapting a\nmodel trained on the abundant source data to a few-shot target dataset or task\nis challenging. To tackle this challenge, we propose a Meta-Weight Regulator\n(MWR), which is a meta-learning approach that learns to assign weights to the\nsource examples based on their relevance to the target loss. Specifically, MWR\nfirst trains the model on the uniformly weighted source examples, and measures\nthe efficacy of the model on the target examples via a loss function. By\niteratively performing a (meta) gradient descent, high-order gradients are\npropagated to the source examples. These gradients are then used to update the\nweights of source examples, in a way that is relevant to the target\nperformance. As MWR is model-agnostic, it can be applied to any backbone neural\nmodel. Extensive experiments are conducted with various backbone text matching\nmodels, on four widely used datasets and two tasks. The results demonstrate\nthat our proposed approach significantly outperforms a number of existing\nadaptation methods and effectively improves the cross-dataset and cross-task\nadaptability of the neural text matching models in the few-shot setting.", "published": "2022-04-27 02:28:40", "link": "http://arxiv.org/abs/2204.12668v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Why does Self-Supervised Learning for Speech Recognition Benefit Speaker\n  Recognition?", "abstract": "Recently, self-supervised learning (SSL) has demonstrated strong performance\nin speaker recognition, even if the pre-training objective is designed for\nspeech recognition. In this paper, we study which factor leads to the success\nof self-supervised learning on speaker-related tasks, e.g. speaker verification\n(SV), through a series of carefully designed experiments. Our empirical results\non the Voxceleb-1 dataset suggest that the benefit of SSL to SV task is from a\ncombination of mask speech prediction loss, data scale, and model size, while\nthe SSL quantizer has a minor impact. We further employ the integrated\ngradients attribution method and loss landscape visualization to understand the\neffectiveness of self-supervised learning for speaker recognition performance.", "published": "2022-04-27 08:35:57", "link": "http://arxiv.org/abs/2204.12765v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Ultra Fast Speech Separation Model with Teacher Student Learning", "abstract": "Transformer has been successfully applied to speech separation recently with\nits strong long-dependency modeling capacity using a self-attention mechanism.\nHowever, Transformer tends to have heavy run-time costs due to the deep encoder\nlayers, which hinders its deployment on edge devices. A small Transformer model\nwith fewer encoder layers is preferred for computational efficiency, but it is\nprone to performance degradation. In this paper, an ultra fast speech\nseparation Transformer model is proposed to achieve both better performance and\nefficiency with teacher student learning (T-S learning). We introduce\nlayer-wise T-S learning and objective shifting mechanisms to guide the small\nstudent model to learn intermediate representations from the large teacher\nmodel. Compared with the small Transformer model trained from scratch, the\nproposed T-S learning method reduces the word error rate (WER) by more than 5%\nfor both multi-channel and single-channel speech separation on LibriCSS\ndataset. Utilizing more unlabeled speech data, our ultra fast speech separation\nmodels achieve more than 10% relative WER reduction.", "published": "2022-04-27 09:02:45", "link": "http://arxiv.org/abs/2204.12777v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning to Parallelize in a Shared-Memory Environment with Transformers", "abstract": "In past years, the world has switched to many-core and multi-core shared\nmemory architectures. As a result, there is a growing need to utilize these\narchitectures by introducing shared memory parallelization schemes to software\napplications. OpenMP is the most comprehensive API that implements such\nschemes, characterized by a readable interface. Nevertheless, introducing\nOpenMP into code is challenging due to pervasive pitfalls in management of\nparallel shared memory. To facilitate the performance of this task, many\nsource-to-source (S2S) compilers have been created over the years, tasked with\ninserting OpenMP directives into code automatically. In addition to having\nlimited robustness to their input format, these compilers still do not achieve\nsatisfactory coverage and precision in locating parallelizable code and\ngenerating appropriate directives. In this work, we propose leveraging recent\nadvances in ML techniques, specifically in natural language processing (NLP),\nto replace S2S compilers altogether. We create a database (corpus), Open-OMP,\nspecifically for this goal. Open-OMP contains over 28,000 code snippets, half\nof which contain OpenMP directives while the other half do not need\nparallelization at all with high probability. We use the corpus to train\nsystems to automatically classify code segments in need of parallelization, as\nwell as suggest individual OpenMP clauses. We train several transformer models,\nnamed PragFormer, for these tasks, and show that they outperform\nstatistically-trained baselines and automatic S2S parallelization compilers in\nboth classifying the overall need for an OpenMP directive and the introduction\nof private and reduction clauses.\n  Our source code and database are available at:\nhttps://github.com/Scientific-Computing-Lab-NRCN/PragFormer.", "published": "2022-04-27 10:39:52", "link": "http://arxiv.org/abs/2204.12835v4", "categories": ["cs.DC", "cs.CL", "cs.LG"], "primary_category": "cs.DC"}
{"title": "Study on the Fairness of Speaker Verification Systems on\n  Underrepresented Accents in English", "abstract": "Speaker verification (SV) systems are currently being used to make sensitive\ndecisions like giving access to bank accounts or deciding whether the voice of\na suspect coincides with that of the perpetrator of a crime. Ensuring that\nthese systems are fair and do not disfavor any particular group is crucial. In\nthis work, we analyze the performance of several state-of-the-art SV systems\nacross groups defined by the accent of the speakers when speaking English. To\nthis end, we curated a new dataset based on the VoxCeleb corpus where we\ncarefully selected samples from speakers with accents from different countries.\nWe use this dataset to evaluate system performance for several SV systems\ntrained with VoxCeleb data. We show that, while discrimination performance is\nreasonably robust across accent groups, calibration performance degrades\ndramatically on some accents that are not well represented in the training\ndata. Finally, we show that a simple data balancing approach mitigates this\nundesirable bias, being particularly effective when applied to our\nrecently-proposed discriminative condition-aware backend.", "published": "2022-04-27 01:25:53", "link": "http://arxiv.org/abs/2204.12649v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Masked Spectrogram Prediction For Self-Supervised Audio Pre-Training", "abstract": "Transformer-based models attain excellent results and generalize well when\ntrained on sufficient amounts of data. However, constrained by the limited data\navailable in the audio domain, most transformer-based models for audio tasks\nare finetuned from pre-trained models in other domains (e.g. image), which has\na notable gap with the audio domain. Other methods explore the self-supervised\nlearning approaches directly in the audio domain but currently do not perform\nwell in the downstream tasks. In this paper, we present a novel self-supervised\nlearning method for transformer-based audio models, called masked spectrogram\nprediction (MaskSpec), to learn powerful audio representations from unlabeled\naudio data (AudioSet used in this paper). Our method masks random patches of\nthe input spectrogram and reconstructs the masked regions with an\nencoder-decoder architecture. Without using extra model weights or supervision,\nexperimental results on multiple downstream datasets demonstrate MaskSpec\nachieves a significant performance gain against the supervised methods and\noutperforms the previous pre-trained models. In particular, our best model\nreaches the performance of 0.471 (mAP) on AudioSet, 0.854 (mAP) on OpenMIC2018,\n0.982 (accuracy) on ESC-50, 0.976 (accuracy) on SCV2, and 0.823 (accuracy) on\nDCASE2019 Task1A respectively.", "published": "2022-04-27 08:39:56", "link": "http://arxiv.org/abs/2204.12768v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unsupervised Word Segmentation using K Nearest Neighbors", "abstract": "In this paper, we propose an unsupervised kNN-based approach for word\nsegmentation in speech utterances. Our method relies on self-supervised\npre-trained speech representations, and compares each audio segment of a given\nutterance to its K nearest neighbors within the training set. Our main\nassumption is that a segment containing more than one word would occur less\noften than a segment containing a single word. Our method does not require\nphoneme discovery and is able to operate directly on pre-trained audio\nrepresentations. This is in contrast to current methods that use a two-stage\napproach; first detecting the phonemes in the utterance and then detecting\nword-boundaries according to statistics calculated on phoneme patterns.\nExperiments on two datasets demonstrate improved results over previous\nsingle-stage methods and competitive results on state-of-the-art two-stage\nmethods.", "published": "2022-04-27 17:43:50", "link": "http://arxiv.org/abs/2204.13094v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Multimodal Speech Recognition by Data Augmentation and Speech\n  Representations", "abstract": "Multimodal speech recognition aims to improve the performance of automatic\nspeech recognition (ASR) systems by leveraging additional visual information\nthat is usually associated to the audio input. While previous approaches make\ncrucial use of strong visual representations, e.g. by finetuning pretrained\nimage recognition networks, significantly less attention has been paid to its\ncounterpart: the speech component. In this work, we investigate ways of\nimproving the base speech recognition system by following similar techniques to\nthe ones used for the visual encoder, namely, transferring representations and\ndata augmentation. First, we show that starting from a pretrained ASR\nsignificantly improves the state-of-the-art performance; remarkably, even when\nbuilding upon a strong unimodal system, we still find gains by including the\nvisual modality. Second, we employ speech data augmentation techniques to\nencourage the multimodal system to attend to the visual stimuli. This technique\nreplaces previously used word masking and comes with the benefits of being\nconceptually simpler and yielding consistent improvements in the multimodal\nsetting. We provide empirical results on three multimodal datasets, including\nthe newly introduced Localized Narratives.", "published": "2022-04-27 21:39:20", "link": "http://arxiv.org/abs/2204.13206v1", "categories": ["cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.SD"}
