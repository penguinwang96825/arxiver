{"title": "A Conditional Variational Framework for Dialog Generation", "abstract": "Deep latent variable models have been shown to facilitate the response\ngeneration for open-domain dialog systems. However, these latent variables are\nhighly randomized, leading to uncontrollable generated responses. In this\npaper, we propose a framework allowing conditional response generation based on\nspecific attributes. These attributes can be either manually assigned or\nautomatically detected. Moreover, the dialog states for both speakers are\nmodeled separately in order to reflect personal features. We validate this\nframework on two different scenarios, where the attribute refers to genericness\nand sentiment states respectively. The experiment result testified the\npotential of our model, where meaningful responses can be generated in\naccordance with the specified attributes.", "published": "2017-04-30 13:52:49", "link": "http://arxiv.org/abs/1705.00316v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings", "abstract": "We consider the problem of learning general-purpose, paraphrastic sentence\nembeddings, revisiting the setting of Wieting et al. (2016b). While they found\nLSTM recurrent networks to underperform word averaging, we present several\ndevelopments that together produce the opposite conclusion. These include\ntraining on sentence pairs rather than phrase pairs, averaging states to\nrepresent sequences, and regularizing aggressively. These improve LSTMs in both\ntransfer learning and supervised settings. We also introduce a new recurrent\narchitecture, the Gated Recurrent Averaging Network, that is inspired by\naveraging and LSTMs while outperforming them both. We analyze our learned\nmodels, finding evidence of preferences for particular parts of speech and\ndependency relations.", "published": "2017-04-30 19:18:22", "link": "http://arxiv.org/abs/1705.00364v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tree-Structured Neural Machine for Linguistics-Aware Sentence Generation", "abstract": "Different from other sequential data, sentences in natural language are\nstructured by linguistic grammars. Previous generative conversational models\nwith chain-structured decoder ignore this structure in human language and might\ngenerate plausible responses with less satisfactory relevance and fluency. In\nthis study, we aim to incorporate the results from linguistic analysis into the\nprocess of sentence generation for high-quality conversation generation.\nSpecifically, we use a dependency parser to transform each response sentence\ninto a dependency tree and construct a training corpus of sentence-tree pairs.\nA tree-structured decoder is developed to learn the mapping from a sentence to\nits tree, where different types of hidden states are used to depict the local\ndependencies from an internal tree node to its children. For training\nacceleration, we propose a tree canonicalization method, which transforms trees\ninto equivalent ternary trees. Then, with a proposed tree-structured search\nmethod, the model is able to generate the most probable responses in the form\nof dependency trees, which are finally flattened into sequences as the system\noutput. Experimental results demonstrate that the proposed X2Tree framework\noutperforms baseline methods over 11.15% increase of acceptance ratio.", "published": "2017-04-30 15:09:10", "link": "http://arxiv.org/abs/1705.00321v4", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Quantifying Mental Health from Social Media with Neural User Embeddings", "abstract": "Mental illnesses adversely affect a significant proportion of the population\nworldwide. However, the methods traditionally used for estimating and\ncharacterizing the prevalence of mental health conditions are time-consuming\nand expensive. Consequently, best-available estimates concerning the prevalence\nof mental health conditions are often years out of date. Automated approaches\nto supplement these survey methods with broad, aggregated information derived\nfrom social media content provides a potential means for near real-time\nestimates at scale. These may, in turn, provide grist for supporting,\nevaluating and iteratively improving upon public health programs and\ninterventions.\n  We propose a novel model for automated mental health status quantification\nthat incorporates user embeddings. This builds upon recent work exploring\nrepresentation learning methods that induce embeddings by leveraging social\nmedia post histories. Such embeddings capture latent characteristics of\nindividuals (e.g., political leanings) and encode a soft notion of homophily.\nIn this paper, we investigate whether user embeddings learned from twitter post\nhistories encode information that correlates with mental health statuses. To\nthis end, we estimated user embeddings for a set of users known to be affected\nby depression and post-traumatic stress disorder (PTSD), and for a set of\ndemographically matched `control' users. We then evaluated these embeddings\nwith respect to: (i) their ability to capture homophilic relations with respect\nto mental health status; and (ii) the performance of downstream mental health\nprediction models based on these features. Our experimental results demonstrate\nthat the user embeddings capture similarities between users with respect to\nmental conditions, and are predictive of mental health.", "published": "2017-04-30 16:12:28", "link": "http://arxiv.org/abs/1705.00335v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
