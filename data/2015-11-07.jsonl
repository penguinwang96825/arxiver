{"title": "The Goldilocks Principle: Reading Children's Books with Explicit Memory\n  Representations", "abstract": "We introduce a new test of how well language models capture meaning in\nchildren's books. Unlike standard language modelling benchmarks, it\ndistinguishes the task of predicting syntactic function words from that of\npredicting lower-frequency words, which carry greater semantic content. We\ncompare a range of state-of-the-art models, each with a different way of\nencoding what has been previously read. We show that models which store\nexplicit representations of long-term contexts outperform state-of-the-art\nneural language models at predicting semantic content words, although this\nadvantage is not observed for syntactic function words. Interestingly, we find\nthat the amount of text encoded in a single memory representation is highly\ninfluential to the performance: there is a sweet-spot, not too big and not too\nsmall, between single words and full sentences that allows the most meaningful\ninformation in a text to be effectively retained and recalled. Further, the\nattention over such window-based memories can be trained effectively through\nself-supervision. We then assess the generality of this principle by applying\nit to the CNN QA benchmark, which involves identifying named entities in\nparaphrased summaries of news articles, and achieve state-of-the-art\nperformance.", "published": "2015-11-07 04:36:20", "link": "http://arxiv.org/abs/1511.02301v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stacked Attention Networks for Image Question Answering", "abstract": "This paper presents stacked attention networks (SANs) that learn to answer\nnatural language questions from images. SANs use semantic representation of a\nquestion as query to search for the regions in an image that are related to the\nanswer. We argue that image question answering (QA) often requires multiple\nsteps of reasoning. Thus, we develop a multiple-layer SAN in which we query an\nimage multiple times to infer the answer progressively. Experiments conducted\non four image QA data sets demonstrate that the proposed SANs significantly\noutperform previous state-of-the-art approaches. The visualization of the\nattention layers illustrates the progress that the SAN locates the relevant\nvisual clues that lead to the answer of the question layer-by-layer.", "published": "2015-11-07 00:43:32", "link": "http://arxiv.org/abs/1511.02274v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Generation and Comprehension of Unambiguous Object Descriptions", "abstract": "We propose a method that can generate an unambiguous description (known as a\nreferring expression) of a specific object or region in an image, and which can\nalso comprehend or interpret such an expression to infer which object is being\ndescribed. We show that our method outperforms previous methods that generate\ndescriptions of objects without taking into account other potentially ambiguous\nobjects in the scene. Our model is inspired by recent successes of deep\nlearning methods for image captioning, but while image captioning is difficult\nto evaluate, our task allows for easy objective evaluation. We also present a\nnew large-scale dataset for referring expressions, based on MS-COCO. We have\nreleased the dataset and a toolbox for visualization and evaluation, see\nhttps://github.com/mjhucla/Google_Refexp_toolbox", "published": "2015-11-07 02:17:36", "link": "http://arxiv.org/abs/1511.02283v3", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.RO", "I.2.6; I.2.7; I.2.10"], "primary_category": "cs.CV"}
{"title": "Review-Level Sentiment Classification with Sentence-Level Polarity\n  Correction", "abstract": "We propose an effective technique to solving review-level sentiment\nclassification problem by using sentence-level polarity correction. Our\npolarity correction technique takes into account the consistency of the\npolarities (positive and negative) of sentences within each product review\nbefore performing the actual machine learning task. While sentences with\ninconsistent polarities are removed, sentences with consistent polarities are\nused to learn state-of-the-art classifiers. The technique achieved better\nresults on different types of products reviews and outperforms baseline models\nwithout the correction technique. Experimental results show an average of 82%\nF-measure on four different product review domains.", "published": "2015-11-07 18:38:22", "link": "http://arxiv.org/abs/1511.02385v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
