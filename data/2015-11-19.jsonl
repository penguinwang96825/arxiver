{"title": "Gaussian Mixture Embeddings for Multiple Word Prototypes", "abstract": "Recently, word representation has been increasingly focused on for its\nexcellent properties in representing the word semantics. Previous works mainly\nsuffer from the problem of polysemy phenomenon. To address this problem, most\nof previous models represent words as multiple distributed vectors. However, it\ncannot reflect the rich relations between words by representing words as points\nin the embedded space. In this paper, we propose the Gaussian mixture skip-gram\n(GMSG) model to learn the Gaussian mixture embeddings for words based on\nskip-gram framework. Each word can be regarded as a gaussian mixture\ndistribution in the embedded space, and each gaussian component represents a\nword sense. Since the number of senses varies from word to word, we further\npropose the Dynamic GMSG (D-GMSG) model by adaptively increasing the sense\nnumber of words during training. Experiments on four benchmarks show the\neffectiveness of our proposed model.", "published": "2015-11-19 16:46:49", "link": "http://arxiv.org/abs/1511.06246v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Good, Better, Best: Choosing Word Embedding Context", "abstract": "We propose two methods of learning vector representations of words and\nphrases that each combine sentence context with structural features extracted\nfrom dependency trees. Using several variations of neural network classifier,\nwe show that these combined methods lead to improved performance when used as\ninput features for supervised term-matching.", "published": "2015-11-19 19:13:58", "link": "http://arxiv.org/abs/1511.06312v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reasoning in Vector Space: An Exploratory Study of Question Answering", "abstract": "Question answering tasks have shown remarkable progress with distributed\nvector representation. In this paper, we investigate the recently proposed\nFacebook bAbI tasks which consist of twenty different categories of questions\nthat require complex reasoning. Because the previous work on bAbI are all\nend-to-end models, errors could come from either an imperfect understanding of\nsemantics or in certain steps of the reasoning. For clearer analysis, we\npropose two vector space models inspired by Tensor Product Representation (TPR)\nto perform knowledge encoding and logical reasoning based on common-sense\ninference. They together achieve near-perfect accuracy on all categories\nincluding positional reasoning and path finding that have proved difficult for\nmost of the previous approaches. We hypothesize that the difficulties in these\ncategories are due to the multi-relations in contrast to uni-relational\ncharacteristic of other categories. Our exploration sheds light on designing\nmore sophisticated dataset and moving one step toward integrating transparent\nand interpretable formalism of TPR into existing learning paradigms.", "published": "2015-11-19 22:30:10", "link": "http://arxiv.org/abs/1511.06426v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Approach to Speed-up the Word Sense Disambiguation Procedure through\n  Sense Filtering", "abstract": "In this paper, we are going to focus on speed up of the Word Sense\nDisambiguation procedure by filtering the relevant senses of an ambiguous word\nthrough Part-of-Speech Tagging. First, this proposed approach performs the\nPart-of-Speech Tagging operation before the disambiguation procedure using\nBigram approximation. As a result, the exact Part-of-Speech of the ambiguous\nword at a particular text instance is derived. In the next stage, only those\ndictionary definitions (glosses) are retrieved from an online dictionary, which\nare associated with that particular Part-of-Speech to disambiguate the exact\nsense of the ambiguous word. In the training phase, we have used Brown Corpus\nfor Part-of-Speech Tagging and WordNet as an online dictionary. The proposed\napproach reduces the execution time upto half (approximately) of the normal\nexecution time for a text, containing around 200 sentences. Not only that, we\nhave found several instances, where the correct sense of an ambiguous word is\nfound for using the Part-of-Speech Tagging before the Disambiguation procedure.", "published": "2015-11-19 11:29:10", "link": "http://arxiv.org/abs/1610.06601v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Hybrid Approach to Word Sense Disambiguation Combining Supervised and\n  Unsupervised Learning", "abstract": "In this paper, we are going to find meaning of words based on distinct\nsituations. Word Sense Disambiguation is used to find meaning of words based on\nlive contexts using supervised and unsupervised approaches. Unsupervised\napproaches use online dictionary for learning, and supervised approaches use\nmanual learning sets. Hand tagged data are populated which might not be\neffective and sufficient for learning procedure. This limitation of information\nis main flaw of the supervised approach. Our proposed approach focuses to\novercome the limitation using learning set which is enriched in dynamic way\nmaintaining new data. Trivial filtering method is utilized to achieve\nappropriate training data. We introduce a mixed methodology having Modified\nLesk approach and Bag-of-Words having enriched bags using learning methods. Our\napproach establishes the superiority over individual Modified Lesk and\nBag-of-Words approaches based on experimentation.", "published": "2015-11-19 10:36:52", "link": "http://arxiv.org/abs/1611.01083v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detection of Slang Words in e-Data using semi-Supervised Learning", "abstract": "The proposed algorithmic approach deals with finding the sense of a word in\nan electronic data. Now a day,in different communication mediums like internet,\nmobile services etc. people use few words, which are slang in nature. This\napproach detects those abusive words using supervised learning procedure. But\nin the real life scenario, the slang words are not used in complete word forms\nalways. Most of the times, those words are used in different abbreviated forms\nlike sounds alike forms, taboo morphemes etc. This proposed approach can detect\nthose abbreviated forms also using semi supervised learning procedure. Using\nthe synset and concept analysis of the text, the probability of a suspicious\nword to be a slang word is also evaluated.", "published": "2015-11-19 11:38:42", "link": "http://arxiv.org/abs/1702.04241v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer Learning for Speech and Language Processing", "abstract": "Transfer learning is a vital technique that generalizes models trained for\none setting or task to other settings or tasks. For example in speech\nrecognition, an acoustic model trained for one language can be used to\nrecognize speech in another language, with little or no re-training data.\nTransfer learning is closely related to multi-task learning (cross-lingual vs.\nmultilingual), and is traditionally studied in the name of `model adaptation'.\nRecent advance in deep learning shows that transfer learning becomes much\neasier and more effective with high-level abstract features learned by deep\nmodels, and the `transfer' can be conducted not only between data distributions\nand data types, but also between model structures (e.g., shallow nets and deep\nnets) or even model types (e.g., Bayesian models and neural models). This\nreview paper summarizes some recent prominent research towards this direction,\nparticularly for speech and language processing. We also report some results\nfrom our group and highlight the potential of this very interesting research\nfield.", "published": "2015-11-19 05:54:45", "link": "http://arxiv.org/abs/1511.06066v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge Base Population using Semantic Label Propagation", "abstract": "A crucial aspect of a knowledge base population system that extracts new\nfacts from text corpora, is the generation of training data for its relation\nextractors. In this paper, we present a method that maximizes the effectiveness\nof newly trained relation extractors at a minimal annotation cost. Manual\nlabeling can be significantly reduced by Distant Supervision, which is a method\nto construct training data automatically by aligning a large text corpus with\nan existing knowledge base of known facts. For example, all sentences\nmentioning both 'Barack Obama' and 'US' may serve as positive training\ninstances for the relation born_in(subject,object). However, distant\nsupervision typically results in a highly noisy training set: many training\nsentences do not really express the intended relation. We propose to combine\ndistant supervision with minimal manual supervision in a technique called\nfeature labeling, to eliminate noise from the large and noisy initial training\nset, resulting in a significant increase of precision. We further improve on\nthis approach by introducing the Semantic Label Propagation method, which uses\nthe similarity between low-dimensional representations of candidate training\ninstances, to extend the training set in order to increase recall while\nmaintaining high precision. Our proposed strategy for generating training data\nis studied and evaluated on an established test collection designed for\nknowledge base population tasks. The experimental results show that the\nSemantic Label Propagation strategy leads to substantial performance gains when\ncompared to existing approaches, while requiring an almost negligible manual\nannotation effort.", "published": "2015-11-19 15:51:31", "link": "http://arxiv.org/abs/1511.06219v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Alternative structures for character-level RNNs", "abstract": "Recurrent neural networks are convenient and efficient models for language\nmodeling. However, when applied on the level of characters instead of words,\nthey suffer from several problems. In order to successfully model long-term\ndependencies, the hidden representation needs to be large. This in turn implies\nhigher computational costs, which can become prohibitive in practice. We\npropose two alternative structural modifications to the classical RNN model.\nThe first one consists on conditioning the character level representation on\nthe previous word representation. The other one uses the character history to\ncondition the output probability. We evaluate the performance of the two\nproposed modifications on challenging, multi-lingual real world data.", "published": "2015-11-19 18:46:21", "link": "http://arxiv.org/abs/1511.06303v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Communicating Semantics: Reference by Description", "abstract": "Messages often refer to entities such as people, places and events. Correct\nidentification of the intended reference is an essential part of communication.\nLack of shared unique names often complicates entity reference. Shared\nknowledge can be used to construct uniquely identifying descriptive references\nfor entities with ambiguous names. We introduce a mathematical model for\n`Reference by Description', derive results on the conditions under which, with\nhigh probability, programs can construct unambiguous references to most\nentities in the domain of discourse and provide empirical validation of these\nresults.", "published": "2015-11-19 20:14:43", "link": "http://arxiv.org/abs/1511.06341v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generating Sentences from a Continuous Space", "abstract": "The standard recurrent neural network language model (RNNLM) generates\nsentences one word at a time and does not work from an explicit global sentence\nrepresentation. In this work, we introduce and study an RNN-based variational\nautoencoder generative model that incorporates distributed latent\nrepresentations of entire sentences. This factorization allows it to explicitly\nmodel holistic properties of sentences such as style, topic, and high-level\nsyntactic features. Samples from the prior over these sentence representations\nremarkably produce diverse and well-formed sentences through simple\ndeterministic decoding. By examining paths through this latent space, we are\nable to generate coherent novel sentences that interpolate between known\nsentences. We present techniques for solving the difficult learning problem\npresented by this model, demonstrate its effectiveness in imputing missing\nwords, explore many interesting properties of the model's latent sentence\nspace, and present negative results on the use of the model in language\nmodeling.", "published": "2015-11-19 20:38:45", "link": "http://arxiv.org/abs/1511.06349v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Dynamic Adaptive Network Intelligence", "abstract": "Accurate representational learning of both the explicit and implicit\nrelationships within data is critical to the ability of machines to perform\nmore complex and abstract reasoning tasks. We describe the efficient weakly\nsupervised learning of such inferences by our Dynamic Adaptive Network\nIntelligence (DANI) model. We report state-of-the-art results for DANI over\nquestion answering tasks in the bAbI dataset that have proved difficult for\ncontemporary approaches to learning representation (Weston et al., 2015).", "published": "2015-11-19 21:07:27", "link": "http://arxiv.org/abs/1511.06379v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In\n  Neural Word Embeddings", "abstract": "Neural word representations have proven useful in Natural Language Processing\n(NLP) tasks due to their ability to efficiently model complex semantic and\nsyntactic word relationships. However, most techniques model only one\nrepresentation per word, despite the fact that a single word can have multiple\nmeanings or \"senses\". Some techniques model words by using multiple vectors\nthat are clustered based on context. However, recent neural approaches rarely\nfocus on the application to a consuming NLP algorithm. Furthermore, the\ntraining process of recent word-sense models is expensive relative to\nsingle-sense embedding processes. This paper presents a novel approach which\naddresses these concerns by modeling multiple embeddings for each word based on\nsupervised disambiguation, which provides a fast and accurate way for a\nconsuming NLP model to select a sense-disambiguated embedding. We demonstrate\nthat these embeddings can disambiguate both contrastive senses such as nominal\nand verbal senses as well as nuanced senses such as sarcasm. We further\nevaluate Part-of-Speech disambiguated embeddings on neural dependency parsing,\nyielding a greater than 8% average error reduction in unlabeled attachment\nscores across 6 languages.", "published": "2015-11-19 21:22:42", "link": "http://arxiv.org/abs/1511.06388v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilingual Relation Extraction using Compositional Universal Schema", "abstract": "Universal schema builds a knowledge base (KB) of entities and relations by\njointly embedding all relation types from input KBs as well as textual patterns\nexpressing relations from raw text. In most previous applications of universal\nschema, each textual pattern is represented as a single embedding, preventing\ngeneralization to unseen patterns. Recent work employs a neural network to\ncapture patterns' compositional semantics, providing generalization to all\npossible input text. In response, this paper introduces significant further\nimprovements to the coverage and flexibility of universal schema relation\nextraction: predictions for entities unseen in training and multilingual\ntransfer learning to domains with no annotation. We evaluate our model through\nextensive experiments on the English and Spanish TAC KBP benchmark,\noutperforming the top system from TAC 2013 slot-filling using no handwritten\npatterns or additional annotation. We also consider a multilingual setting in\nwhich English training data entities overlap with the seed KB, but Spanish text\ndoes not. Despite having no annotation for Spanish data, we train an accurate\npredictor, with additional improvements obtained by tying word embeddings\nacross languages. Furthermore, we find that multilingual training improves\nEnglish relation extraction accuracy. Our approach is thus suited to\nbroad-coverage automated knowledge base construction in a variety of languages\nand domains.", "published": "2015-11-19 21:42:23", "link": "http://arxiv.org/abs/1511.06396v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Compressing Word Embeddings", "abstract": "Recent methods for learning vector space representations of words have\nsucceeded in capturing fine-grained semantic and syntactic regularities using\nvector arithmetic. However, these vector space representations (created through\nlarge-scale text analysis) are typically stored verbatim, since their internal\nstructure is opaque. Using word-analogy tests to monitor the level of detail\nstored in compressed re-representations of the same vector space, the\ntrade-offs between the reduction in memory usage and expressiveness are\ninvestigated. A simple scheme is outlined that can reduce the memory footprint\nof a state-of-the-art embedding by a factor of 10, with only minimal impact on\nperformance. Then, using the same `bit budget', a binary (approximate)\nfactorisation of the same space is also explored, with the aim of creating an\nequivalent representation with better interpretability.", "published": "2015-11-19 21:42:47", "link": "http://arxiv.org/abs/1511.06397v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Recurrent Models for Auditory Attention in Multi-Microphone Distance\n  Speech Recognition", "abstract": "Integration of multiple microphone data is one of the key ways to achieve\nrobust speech recognition in noisy environments or when the speaker is located\nat some distance from the input device. Signal processing techniques such as\nbeamforming are widely used to extract a speech signal of interest from\nbackground noise. These techniques, however, are highly dependent on prior\nspatial information about the microphones and the environment in which the\nsystem is being used. In this work, we present a neural attention network that\ndirectly combines multi-channel audio to generate phonetic states without\nrequiring any prior knowledge of the microphone layout or any explicit signal\npreprocessing for speech enhancement. We embed an attention mechanism within a\nRecurrent Neural Network (RNN) based acoustic model to automatically tune its\nattention to a more reliable input source. Unlike traditional multi-channel\npreprocessing, our system can be optimized towards the desired output in one\nstep. Although attention-based models have recently achieved impressive results\non sequence-to-sequence learning, no attention mechanisms have previously been\napplied to learn potentially asynchronous and non-stationary multiple inputs.\nWe evaluate our neural attention model on the CHiME-3 challenge task, and show\nthat the model achieves comparable performance to beamforming using a purely\ndata-driven method.", "published": "2015-11-19 21:56:53", "link": "http://arxiv.org/abs/1511.06407v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Joint Word Representation Learning using a Corpus and a Semantic Lexicon", "abstract": "Methods for learning word representations using large text corpora have\nreceived much attention lately due to their impressive performance in numerous\nnatural language processing (NLP) tasks such as, semantic similarity\nmeasurement, and word analogy detection. Despite their success, these\ndata-driven word representation learning methods do not consider the rich\nsemantic relational structure between words in a co-occurring context. On the\nother hand, already much manual effort has gone into the construction of\nsemantic lexicons such as the WordNet that represent the meanings of words by\ndefining the various relationships that exist among the words in a language. We\nconsider the question, can we improve the word representations learnt using a\ncorpora by integrating the knowledge from semantic lexicons?. For this purpose,\nwe propose a joint word representation learning method that simultaneously\npredicts the co-occurrences of two words in a sentence subject to the\nrelational constrains given by the semantic lexicon. We use relations that\nexist between words in the lexicon to regularize the word representations\nlearnt from the corpus. Our proposed method statistically significantly\noutperforms previously proposed methods for incorporating semantic lexicons\ninto word representations on several benchmark datasets for semantic similarity\nand word analogy.", "published": "2015-11-19 22:58:10", "link": "http://arxiv.org/abs/1511.06438v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neural Variational Inference for Text Processing", "abstract": "Recent advances in neural variational inference have spawned a renaissance in\ndeep latent variable models. In this paper we introduce a generic variational\ninference framework for generative and conditional models of text. While\ntraditional variational methods derive an analytic approximation for the\nintractable distributions over latent variables, here we construct an inference\nnetwork conditioned on the discrete text input to provide the variational\ndistribution. We validate this framework on two very different text modelling\napplications, generative document modelling and supervised question answering.\nOur neural variational document model combines a continuous stochastic document\nrepresentation with a bag-of-words generative model and achieves the lowest\nreported perplexities on two standard test corpora. The neural answer selection\nmodel employs a stochastic representation layer within an attention mechanism\nto extract the semantics between a question and answer pair. On two question\nanswering benchmarks this model exceeds all previous published benchmarks.", "published": "2015-11-19 01:23:28", "link": "http://arxiv.org/abs/1511.06038v4", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Overcoming Language Variation in Sentiment Analysis with Social\n  Attention", "abstract": "Variation in language is ubiquitous, particularly in newer forms of writing\nsuch as social media. Fortunately, variation is not random, it is often linked\nto social properties of the author. In this paper, we show how to exploit\nsocial networks to make sentiment analysis more robust to social language\nvariation. The key idea is linguistic homophily: the tendency of socially\nlinked individuals to use language in similar ways. We formalize this idea in a\nnovel attention-based neural network architecture, in which attention is\ndivided among several basis models, depending on the author's position in the\nsocial network. This has the effect of smoothing the classification function\nacross the social network, and makes it possible to induce personalized\nclassifiers even for authors for whom there is no labeled data or demographic\nmetadata. This model significantly improves the accuracies of sentiment\nanalysis on Twitter and on review data.", "published": "2015-11-19 03:54:15", "link": "http://arxiv.org/abs/1511.06052v4", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Learning Deep Structure-Preserving Image-Text Embeddings", "abstract": "This paper proposes a method for learning joint embeddings of images and text\nusing a two-branch neural network with multiple layers of linear projections\nfollowed by nonlinearities. The network is trained using a large margin\nobjective that combines cross-view ranking constraints with within-view\nneighborhood structure preservation constraints inspired by metric learning\nliterature. Extensive experiments show that our approach gains significant\nimprovements in accuracy for image-to-text and text-to-image retrieval. Our\nmethod achieves new state-of-the-art results on the Flickr30K and MSCOCO\nimage-sentence datasets and shows promise on the new task of phrase\nlocalization on the Flickr30K Entities dataset.", "published": "2015-11-19 07:17:49", "link": "http://arxiv.org/abs/1511.06078v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Multi-task Sequence to Sequence Learning", "abstract": "Sequence to sequence learning has recently emerged as a new paradigm in\nsupervised learning. To date, most of its applications focused on only one task\nand not much work explored this framework for multiple tasks. This paper\nexamines three multi-task learning (MTL) settings for sequence to sequence\nmodels: (a) the oneto-many setting - where the encoder is shared between\nseveral tasks such as machine translation and syntactic parsing, (b) the\nmany-to-one setting - useful when only the decoder can be shared, as in the\ncase of translation and image caption generation, and (c) the many-to-many\nsetting - where multiple encoders and decoders are shared, which is the case\nwith unsupervised objectives and translation. Our results show that training on\na small amount of parsing and image caption data can improve the translation\nquality between English and German by up to 1.5 BLEU points over strong\nsingle-task baselines on the WMT benchmarks. Furthermore, we have established a\nnew state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we\nreveal interesting properties of the two unsupervised learning objectives,\nautoencoder and skip-thought, in the MTL context: autoencoder helps less in\nterms of perplexities but more on BLEU scores compared to skip-thought.", "published": "2015-11-19 10:24:14", "link": "http://arxiv.org/abs/1511.06114v4", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Order-Embeddings of Images and Language", "abstract": "Hypernymy, textual entailment, and image captioning can be seen as special\ncases of a single visual-semantic hierarchy over words, sentences, and images.\nIn this paper we advocate for explicitly modeling the partial order structure\nof this hierarchy. Towards this goal, we introduce a general method for\nlearning ordered representations, and show how it can be applied to a variety\nof tasks involving images and language. We show that the resulting\nrepresentations improve performance over current approaches for hypernym\nprediction and image-caption retrieval.", "published": "2015-11-19 20:56:14", "link": "http://arxiv.org/abs/1511.06361v6", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Order Matters: Sequence to sequence for sets", "abstract": "Sequences have become first class citizens in supervised learning thanks to\nthe resurgence of recurrent neural networks. Many complex tasks that require\nmapping from or to a sequence of observations can now be formulated with the\nsequence-to-sequence (seq2seq) framework which employs the chain rule to\nefficiently represent the joint probability of sequences. In many cases,\nhowever, variable sized inputs and/or outputs might not be naturally expressed\nas sequences. For instance, it is not clear how to input a set of numbers into\na model where the task is to sort them; similarly, we do not know how to\norganize outputs when they correspond to random variables and the task is to\nmodel their unknown joint probability. In this paper, we first show using\nvarious examples that the order in which we organize input and/or output data\nmatters significantly when learning an underlying model. We then discuss an\nextension of the seq2seq framework that goes beyond sequences and handles input\nsets in a principled way. In addition, we propose a loss which, by searching\nover possible orders during training, deals with the lack of structure of\noutput sets. We show empirical evidence of our claims regarding ordering, and\non the modifications to the seq2seq framework on benchmark language modeling\nand parsing tasks, as well as two artificial tasks -- sorting numbers and\nestimating the joint probability of unknown graphical models.", "published": "2015-11-19 21:31:26", "link": "http://arxiv.org/abs/1511.06391v4", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Skip-Thought Memory Networks", "abstract": "Question Answering (QA) is fundamental to natural language processing in that\nmost nlp problems can be phrased as QA (Kumar et al., 2015). Current weakly\nsupervised memory network models that have been proposed so far struggle at\nanswering questions that involve relations among multiple entities (such as\nfacebook's bAbi qa5-three-arg-relations in (Weston et al., 2015)). To address\nthis problem of learning multi-argument multi-hop semantic relations for the\npurpose of QA, we propose a method that combines the jointly learned long-term\nread-write memory and attentive inference components of end-to-end memory\nnetworks (MemN2N) (Sukhbaatar et al., 2015) with distributed sentence vector\nrepresentations encoded by a Skip-Thought model (Kiros et al., 2015). This\nchoice to append Skip-Thought Vectors to the existing MemN2N framework is\nmotivated by the fact that Skip-Thought Vectors have been shown to accurately\nmodel multi-argument semantic relations (Kiros et al., 2015).", "published": "2015-11-19 22:15:46", "link": "http://arxiv.org/abs/1511.06420v2", "categories": ["cs.NE", "cs.CL", "cs.LG"], "primary_category": "cs.NE"}
