{"title": "Hybrid Quantum Neural Networks with Amplitude Encoding: Advancing Recovery Rate Predictions", "abstract": "Recovery rate prediction plays a pivotal role in bond investment strategies,\nenhancing risk assessment, optimizing portfolio allocation, improving pricing\naccuracy, and supporting effective credit risk management. However, forecasting\nfaces challenges like high-dimensional features, small sample sizes, and\noverfitting. We propose a hybrid Quantum Machine Learning model incorporating\nParameterized Quantum Circuits (PQC) within a neural network framework. PQCs\ninherently preserve unitarity, avoiding computationally costly orthogonality\nconstraints, while amplitude encoding enables exponential data compression,\nreducing qubit requirements logarithmically. Applied to a global dataset of\n1,725 observations (1996-2023), our method achieved superior accuracy (RMSE\n0.228) compared to classical neural networks (0.246) and quantum models with\nangle encoding (0.242), with efficient computation times. This work highlights\nthe potential of hybrid quantum-classical architectures in advancing recovery\nrate forecasting.", "published": "2025-01-27 07:27:23", "link": "http://arxiv.org/abs/2501.15828v4", "categories": ["q-fin.CP", "cs.LG", "quant-ph"], "primary_category": "q-fin.CP"}
{"title": "Solvability of the Gaussian Kyle model with imperfect information and risk aversion", "abstract": "We investigate a Kyle model under Gaussian assumptions where a risk-averse\ninformed trader has imperfect information on the fundamental price of an asset.\nWe show that an equilibrium can be constructed by considering an optimal\ntransport problem that is solved under a measure that renders the utility of\nthe informed trader martingale and a filtering problem under the historical\nmeasure.", "published": "2025-01-27 20:45:09", "link": "http://arxiv.org/abs/2501.16488v1", "categories": ["q-fin.TR", "math.PR", "q-fin.MF"], "primary_category": "q-fin.TR"}
{"title": "Optimal investment and consumption under $g$- expected utility and general constraints in incomplete market", "abstract": "This article studies the problem of utility maximization in an incomplete\nmarket under a class of nonlinear expectations and general constraints on\ntrading strategies. Using a $g$-martingale method, we provide an explicit\nsolution to our optimization problem for different utility functions and\ncharacterize an optimal investment-consumption strategy through the solutions\nto quadratic BSDEs.", "published": "2025-01-27 11:08:30", "link": "http://arxiv.org/abs/2501.17193v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "ESGSenticNet: A Neurosymbolic Knowledge Base for Corporate\n  Sustainability Analysis", "abstract": "Evaluating corporate sustainability performance is essential to drive\nsustainable business practices, amid the need for a more sustainable economy.\nHowever, this is hindered by the complexity and volume of corporate\nsustainability data (i.e. sustainability disclosures), not least by the\neffectiveness of the NLP tools used to analyse them. To this end, we identify\nthree primary challenges - immateriality, complexity, and subjectivity, that\nexacerbate the difficulty of extracting insights from sustainability\ndisclosures. To address these issues, we introduce ESGSenticNet, a publicly\navailable knowledge base for sustainability analysis. ESGSenticNet is\nconstructed from a neurosymbolic framework that integrates specialised concept\nparsing, GPT-4o inference, and semi-supervised label propagation, together with\na hierarchical taxonomy. This approach culminates in a structured knowledge\nbase of 44k knowledge triplets - ('halve carbon emission', supports, 'emissions\ncontrol'), for effective sustainability analysis. Experiments indicate that\nESGSenticNet, when deployed as a lexical method, more effectively captures\nrelevant and actionable sustainability information from sustainability\ndisclosures compared to state of the art baselines. Besides capturing a high\nnumber of unique ESG topic terms, ESGSenticNet outperforms baselines on the ESG\nrelatedness and ESG action orientation of these terms by 26% and 31%\nrespectively. These metrics describe the extent to which topic terms are\nrelated to ESG, and depict an action toward ESG. Moreover, when deployed as a\nlexical method, ESGSenticNet does not require any training, possessing a key\nadvantage in its simplicity for non-technical stakeholders.", "published": "2025-01-27 01:21:12", "link": "http://arxiv.org/abs/2501.15720v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Weight-based Analysis of Detokenization in Language Models:\n  Understanding the First Stage of Inference Without Inference", "abstract": "According to the stages-of-inference hypothesis, early layers of language\nmodels map their subword-tokenized input, which does not necessarily correspond\nto a linguistically meaningful segmentation, to more meaningful representations\nthat form the model's \"inner vocabulary\". Prior analysis of this detokenization\nstage has predominantly relied on probing and interventions such as path\npatching, which involve selecting particular inputs, choosing a subset of\ncomponents that will be patched, and then observing changes in model behavior.\nHere, we show that several important aspects of the detokenization stage can be\nunderstood purely by analyzing model weights, without performing any model\ninference steps. Specifically, we introduce an analytical decomposition of\nfirst-layer attention in GPT-2. Our decomposition yields interpretable terms\nthat quantify the relative contributions of position-related, token-related,\nand mixed effects. By focusing on terms in this decomposition, we discover\nweight-based explanations of attention bias toward close tokens and attention\nfor detokenization.", "published": "2025-01-27 03:45:29", "link": "http://arxiv.org/abs/2501.15754v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is It Navajo? Accurate Language Detection in Endangered Athabaskan\n  Languages", "abstract": "Endangered languages, such as Navajo - the most widely spoken Native American\nlanguage - are significantly underrepresented in contemporary language\ntechnologies, exacerbating the challenges of their preservation and\nrevitalization. This study evaluates Google's Language Identification (LangID)\ntool, which does not currently support any Native American languages. To\naddress this, we introduce a random forest classifier trained on Navajo and\ntwenty erroneously suggested languages by LangID. Despite its simplicity, the\nclassifier achieves near-perfect accuracy (97-100%). Additionally, the model\ndemonstrates robustness across other Athabaskan languages - a family of Native\nAmerican languages spoken primarily in Alaska, the Pacific Northwest, and parts\nof the Southwestern United States - suggesting its potential for broader\napplication. Our findings underscore the pressing need for NLP systems that\nprioritize linguistic diversity and adaptability over centralized,\none-size-fits-all solutions, especially in supporting underrepresented\nlanguages in a multicultural world. This work directly contributes to ongoing\nefforts to address cultural biases in language models and advocates for the\ndevelopment of culturally localized NLP tools that serve diverse linguistic\ncommunities.", "published": "2025-01-27 04:43:18", "link": "http://arxiv.org/abs/2501.15773v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Feedback Generation for Short Answer Questions using Answer\n  Diagnostic Graphs", "abstract": "Short-reading comprehension questions help students understand text structure\nbut lack effective feedback. Students struggle to identify and correct errors,\nwhile manual feedback creation is labor-intensive. This highlights the need for\nautomated feedback linking responses to a scoring rubric for deeper\ncomprehension.\n  Despite advances in Natural Language Processing (NLP), research has focused\non automatic grading, with limited work on feedback generation. To address\nthis, we propose a system that generates feedback for student responses.\n  Our contributions are twofold. First, we introduce the first system for\nfeedback on short-answer reading comprehension. These answers are derived from\nthe text, requiring structural understanding. We propose an \"answer diagnosis\ngraph,\" integrating the text's logical structure with feedback templates. Using\nthis graph and NLP techniques, we estimate students' comprehension and generate\ntargeted feedback.\n  Second, we evaluate our feedback through an experiment with Japanese high\nschool students (n=39). They answered two 70-80 word questions and were divided\ninto two groups with minimal academic differences. One received a model answer,\nthe other system-generated feedback. Both re-answered the questions, and we\ncompared score changes. A questionnaire assessed perceptions and motivation.\n  Results showed no significant score improvement between groups, but\nsystem-generated feedback helped students identify errors and key points in the\ntext. It also significantly increased motivation. However, further refinement\nis needed to enhance text structure understanding.", "published": "2025-01-27 04:49:10", "link": "http://arxiv.org/abs/2501.15777v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MADP: Multi-Agent Deductive Planning for Enhanced Cognitive-Behavioral\n  Mental Health Question Answer", "abstract": "The Mental Health Question Answer (MHQA) task requires the seeker and\nsupporter to complete the support process in one-turn dialogue. Given the\nrichness of help-seeker posts, supporters must thoroughly understand the\ncontent and provide logical, comprehensive, and well-structured responses.\nPrevious works in MHQA mostly focus on single-agent approaches based on the\ncognitive element of Cognitive Behavioral Therapy (CBT), but they overlook the\ninteractions among various CBT elements, such as emotion and cognition. This\nlimitation hinders the models' ability to thoroughly understand the distress of\nhelp-seekers. To address this, we propose a framework named Multi-Agent\nDeductive Planning (MADP), which is based on the interactions between the\nvarious psychological elements of CBT. This method guides Large Language Models\n(LLMs) to achieve a deeper understanding of the seeker's context and provide\nmore personalized assistance based on individual circumstances. Furthermore, we\nconstruct a new dataset based on the MADP framework and use it to fine-tune\nLLMs, resulting in a specialized model named MADP-LLM. We conduct extensive\nexperiments, including comparisons with multiple LLMs, human evaluations, and\nautomatic evaluations, to validate the effectiveness of the MADP framework and\nMADP-LLM.", "published": "2025-01-27 07:18:47", "link": "http://arxiv.org/abs/2501.15826v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LCTG Bench: LLM Controlled Text Generation Benchmark", "abstract": "The rise of large language models (LLMs) has led to more diverse and\nhigher-quality machine-generated text. However, their high expressive power\nmakes it difficult to control outputs based on specific business instructions.\nIn response, benchmarks focusing on the controllability of LLMs have been\ndeveloped, but several issues remain: (1) They primarily cover major languages\nlike English and Chinese, neglecting low-resource languages like Japanese; (2)\nCurrent benchmarks employ task-specific evaluation metrics, lacking a unified\nframework for selecting models based on controllability across different use\ncases. To address these challenges, this research introduces LCTG Bench, the\nfirst Japanese benchmark for evaluating the controllability of LLMs. LCTG Bench\nprovides a unified framework for assessing control performance, enabling users\nto select the most suitable model for their use cases based on controllability.\nBy evaluating nine diverse Japanese-specific and multilingual LLMs like GPT-4,\nwe highlight the current state and challenges of controllability in Japanese\nLLMs and reveal the significant gap between multilingual models and\nJapanese-specific models.", "published": "2025-01-27 08:59:10", "link": "http://arxiv.org/abs/2501.15875v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "3CEL: A corpus of legal Spanish contract clauses", "abstract": "Legal corpora for Natural Language Processing (NLP) are valuable and scarce\nresources in languages like Spanish due to two main reasons: data accessibility\nand legal expert knowledge availability. INESData 2024 is a European Union\nfunded project lead by the Universidad Polit\\'ecnica de Madrid (UPM) and\ndeveloped by Instituto de Ingenier\\'ia del Conocimiento (IIC) to create a\nseries of state-of-the-art NLP resources applied to the legal/administrative\ndomain in Spanish. The goal of this paper is to present the Corpus of Legal\nSpanish Contract Clauses (3CEL), which is a contract information extraction\ncorpus developed within the framework of INESData 2024. 3CEL contains 373\nmanually annotated tenders using 19 defined categories (4 782 total tags) that\nidentify key information for contract understanding and reviewing.", "published": "2025-01-27 12:20:57", "link": "http://arxiv.org/abs/2501.15990v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MEL: Legal Spanish Language Model", "abstract": "Legal texts, characterized by complex and specialized terminology, present a\nsignificant challenge for Language Models. Adding an underrepresented language,\nsuch as Spanish, to the mix makes it even more challenging. While pre-trained\nmodels like XLM-RoBERTa have shown capabilities in handling multilingual\ncorpora, their performance on domain specific documents remains underexplored.\nThis paper presents the development and evaluation of MEL, a legal language\nmodel based on XLM-RoBERTa-large, fine-tuned on legal documents such as BOE\n(Bolet\\'in Oficial del Estado, the Spanish oficial report of laws) and congress\ntexts. We detail the data collection, processing, training, and evaluation\nprocesses. Evaluation benchmarks show a significant improvement over baseline\nmodels in understanding the legal Spanish language. We also present case\nstudies demonstrating the model's application to new legal texts, highlighting\nits potential to perform top results over different NLP tasks.", "published": "2025-01-27 12:50:10", "link": "http://arxiv.org/abs/2501.16011v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RelCAT: Advancing Extraction of Clinical Inter-Entity Relationships from\n  Unstructured Electronic Health Records", "abstract": "This study introduces RelCAT (Relation Concept Annotation Toolkit), an\ninteractive tool, library, and workflow designed to classify relations between\nentities extracted from clinical narratives. Building upon the CogStack MedCAT\nframework, RelCAT addresses the challenge of capturing complete clinical\nrelations dispersed within text. The toolkit implements state-of-the-art\nmachine learning models such as BERT and Llama along with proven evaluation and\ntraining methods. We demonstrate a dataset annotation tool (built within\nMedCATTrainer), model training, and evaluate our methodology on both openly\navailable gold-standard and real-world UK National Health Service (NHS)\nhospital clinical datasets. We perform extensive experimentation and a\ncomparative analysis of the various publicly available models with varied\napproaches selected for model fine-tuning. Finally, we achieve macro F1-scores\nof 0.977 on the gold-standard n2c2, surpassing the previous state-of-the-art\nperformance, and achieve performance of >=0.93 F1 on our NHS gathered datasets.", "published": "2025-01-27 14:26:47", "link": "http://arxiv.org/abs/2501.16077v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Integration of LLM Quality Assurance into an NLG System", "abstract": "In this paper, we present a system that uses a Large Language Model (LLM) to\nperform grammar and spelling correction as a component of Quality Assurance\n(QA) for texts generated by NLG systems, which is important for text production\nin real-world scenarios. Evaluating the results of the system on\nwork-in-progress sports news texts in three languages, we show that it is able\nto deliver acceptable corrections.", "published": "2025-01-27 14:28:01", "link": "http://arxiv.org/abs/2501.16078v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Explainable Multimodal Depression Recognition for Clinical\n  Interviews", "abstract": "Recently, multimodal depression recognition for clinical interviews (MDRC)\nhas recently attracted considerable attention. Existing MDRC studies mainly\nfocus on improving task performance and have achieved significant development.\nHowever, for clinical applications, model transparency is critical, and\nprevious works ignore the interpretability of decision-making processes. To\naddress this issue, we propose an Explainable Multimodal Depression Recognition\nfor Clinical Interviews (EMDRC) task, which aims to provide evidence for\ndepression recognition by summarizing symptoms and uncovering underlying\ncauses. Given an interviewer-participant interaction scenario, the goal of\nEMDRC is to structured summarize participant's symptoms based on the eight-item\nPatient Health Questionnaire depression scale (PHQ-8), and predict their\ndepression severity. To tackle the EMDRC task, we construct a new dataset based\non an existing MDRC dataset. Moreover, we utilize the PHQ-8 and propose a\nPHQ-aware multimodal multi-task learning framework, which captures the\nutterance-level symptom-related semantic information to help generate\ndialogue-level summary. Experiment results on our annotated dataset demonstrate\nthe superiority of our proposed methods over baseline systems on the EMDRC\ntask.", "published": "2025-01-27 14:57:25", "link": "http://arxiv.org/abs/2501.16106v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From #Dr00gtiktok to #harmreduction: Exploring Substance Use Hashtags on\n  TikTok", "abstract": "The rise of TikTok as a primary source of information for youth, combined\nwith its unique short-form video format, creates urgent questions about how\nsubstance use content manifests and spreads on the platform. This paper\nprovides the first in-depth exploration of substance use-related content on\nTikTok, covering all major substance categories as classified by the Drug\nEnforcement Agency. Through social network analysis and qualitative coding, we\nexamined more than 2,333 hashtags across 39,509 videos, identified 16 distinct\nhashtag communities and analyzed their interconnections and thematic content.\nOur analysis revealed a highly interconnected small-world network where\nrecovery-focused hashtags like #addiction, #recovery, and #sober serve as\ncentral bridges between communities. Through manual coding of 351\nrepresentative videos, we found that Recovery Advocacy content (33.9%) and\nSatirical content (28.2%) dominate, while direct substance depiction appears in\nonly 26% of videos, with active use shown in just 6.5% of them. This suggests\nTikTok functions primarily as a recovery support platform rather than a space\npromoting substance use. We found strong alignment between hashtag communities\nand video content, indicating organic community formation rather than attempts\nto evade content moderation. Our findings inform how platforms can balance\ncontent moderation with preserving valuable recovery support communities, while\nalso providing insights for the design of social media-based recovery\ninterventions.", "published": "2025-01-27 15:11:16", "link": "http://arxiv.org/abs/2501.16123v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluation of NMT-Assisted Grammar Transfer for a Multi-Language\n  Configurable Data-to-Text System", "abstract": "One approach for multilingual data-to-text generation is to translate\ngrammatical configurations upfront from the source language into each target\nlanguage. These configurations are then used by a surface realizer and in\ndocument planning stages to generate output. In this paper, we describe a\nrule-based NLG implementation of this approach where the configuration is\ntranslated by Neural Machine Translation (NMT) combined with a one-time human\nreview, and introduce a cross-language grammar dependency model to create a\nmultilingual NLG system that generates text from the source data, scaling the\ngeneration phase without a human in the loop. Additionally, we introduce a\nmethod for human post-editing evaluation on the automatically translated text.\nOur evaluation on the SportSett:Basketball dataset shows that our NLG system\nperforms well, underlining its grammatical correctness in translation tasks.", "published": "2025-01-27 15:25:26", "link": "http://arxiv.org/abs/2501.16135v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can summarization approximate simplification? A gold standard comparison", "abstract": "This study explores the overlap between text summarization and simplification\noutputs. While summarization evaluation methods are streamlined, simplification\nlacks cohesion, prompting the question: how closely can abstractive\nsummarization resemble gold-standard simplification? We address this by\napplying two BART-based BRIO summarization methods to the Newsela corpus,\ncomparing outputs with manually annotated simplifications and achieving a top\nROUGE-L score of 0.654. This provides insight into where summarization and\nsimplification outputs converge and differ.", "published": "2025-01-27 16:27:41", "link": "http://arxiv.org/abs/2501.16181v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DBRouting: Routing End User Queries to Databases for Answerability", "abstract": "Enterprise level data is often distributed across multiple sources and\nidentifying the correct set-of data-sources with relevant information for a\nknowledge request is a fundamental challenge. In this work, we define the novel\ntask of routing an end-user query to the appropriate data-source, where the\ndata-sources are databases. We synthesize datasets by extending existing\ndatasets designed for NL-to-SQL semantic parsing. We create baselines on these\ndatasets by using open-source LLMs, using both pre-trained and task specific\nembeddings fine-tuned using the training data. With these baselines we\ndemonstrate that open-source LLMs perform better than embedding based approach,\nbut suffer from token length limitations. Embedding based approaches benefit\nfrom task specific fine-tuning, more so when there is availability of data in\nterms of database specific questions for training. We further find that the\ntask becomes more difficult (i) with an increase in the number of data-sources,\n(ii) having data-sources closer in terms of their domains,(iii) having\ndatabases without external domain knowledge required to interpret its entities\nand (iv) with ambiguous and complex queries requiring more fine-grained\nunderstanding of the data-sources or logical reasoning for routing to an\nappropriate source. This calls for the need for developing more sophisticated\nsolutions to better address the task.", "published": "2025-01-27 17:09:47", "link": "http://arxiv.org/abs/2501.16220v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Echoes of Discord: Forecasting Hater Reactions to Counterspeech", "abstract": "Hate speech (HS) erodes the inclusiveness of online users and propagates\nnegativity and division. Counterspeech has been recognized as a way to mitigate\nthe harmful consequences. While some research has investigated the impact of\nuser-generated counterspeech on social media platforms, few have examined and\nmodeled haters' reactions toward counterspeech, despite the immediate\nalteration of haters' attitudes being an important aspect of counterspeech.\nThis study fills the gap by analyzing the impact of counterspeech from the\nhater's perspective, focusing on whether the counterspeech leads the hater to\nreenter the conversation and if the reentry is hateful. We compile the Reddit\nEchoes of Hate dataset (ReEco), which consists of triple-turn conversations\nfeaturing haters' reactions, to assess the impact of counterspeech. To predict\nhaters' behaviors, we employ two strategies: a two-stage reaction predictor and\na three-way classifier. The linguistic analysis sheds insights on the language\nof counterspeech to hate eliciting different haters' reactions. Experimental\nresults demonstrate that the 3-way classification model outperforms the\ntwo-stage reaction predictor, which first predicts reentry and then determines\nthe reentry type. We conclude the study with an assessment showing the most\ncommon errors identified by the best-performing model.", "published": "2025-01-27 17:33:38", "link": "http://arxiv.org/abs/2501.16235v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A foundation model for human-AI collaboration in medical literature\n  mining", "abstract": "Systematic literature review is essential for evidence-based medicine,\nrequiring comprehensive analysis of clinical trial publications. However, the\napplication of artificial intelligence (AI) models for medical literature\nmining has been limited by insufficient training and evaluation across broad\ntherapeutic areas and diverse tasks. Here, we present LEADS, an AI foundation\nmodel for study search, screening, and data extraction from medical literature.\nThe model is trained on 633,759 instruction data points in LEADSInstruct,\ncurated from 21,335 systematic reviews, 453,625 clinical trial publications,\nand 27,015 clinical trial registries. We showed that LEADS demonstrates\nconsistent improvements over four cutting-edge generic large language models\n(LLMs) on six tasks. Furthermore, LEADS enhances expert workflows by providing\nsupportive references following expert requests, streamlining processes while\nmaintaining high-quality results. A study with 16 clinicians and medical\nresearchers from 14 different institutions revealed that experts collaborating\nwith LEADS achieved a recall of 0.81 compared to 0.77 experts working alone in\nstudy selection, with a time savings of 22.6%. In data extraction tasks,\nexperts using LEADS achieved an accuracy of 0.85 versus 0.80 without using\nLEADS, alongside a 26.9% time savings. These findings highlight the potential\nof specialized medical literature foundation models to outperform generic\nmodels, delivering significant quality and efficiency benefits when integrated\ninto expert workflows for medical literature mining.", "published": "2025-01-27 17:55:37", "link": "http://arxiv.org/abs/2501.16255v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Matryoshka Re-Ranker: A Flexible Re-Ranking Architecture With\n  Configurable Depth and Width", "abstract": "Large language models (LLMs) provide powerful foundations to perform\nfine-grained text re-ranking. However, they are often prohibitive in reality\ndue to constraints on computation bandwidth. In this work, we propose a\n\\textbf{flexible} architecture called \\textbf{Matroyshka Re-Ranker}, which is\ndesigned to facilitate \\textbf{runtime customization} of model layers and\nsequence lengths at each layer based on users' configurations. Consequently,\nthe LLM-based re-rankers can be made applicable across various real-world\nsituations. The increased flexibility may come at the cost of precision loss.\nTo address this problem, we introduce a suite of techniques to optimize the\nperformance. First, we propose \\textbf{cascaded self-distillation}, where each\nsub-architecture learns to preserve a precise re-ranking performance from its\nsuper components, whose predictions can be exploited as smooth and informative\nteacher signals. Second, we design a \\textbf{factorized compensation\nmechanism}, where two collaborative Low-Rank Adaptation modules, vertical and\nhorizontal, are jointly employed to compensate for the precision loss resulted\nfrom arbitrary combinations of layer and sequence compression. We perform\ncomprehensive experiments based on the passage and document retrieval datasets\nfrom MSMARCO, along with all public datasets from BEIR benchmark. In our\nexperiments, Matryoshka Re-Ranker substantially outperforms the existing\nmethods, while effectively preserving its superior performance across various\nforms of compression and different application scenarios.", "published": "2025-01-27 18:42:48", "link": "http://arxiv.org/abs/2501.16302v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deception in LLMs: Self-Preservation and Autonomous Goals in Large\n  Language Models", "abstract": "Recent advances in Large Language Models (LLMs) have incorporated planning\nand reasoning capabilities, enabling models to outline steps before execution\nand provide transparent reasoning paths. This enhancement has reduced errors in\nmathematical and logical tasks while improving accuracy. These developments\nhave facilitated LLMs' use as agents that can interact with tools and adapt\ntheir responses based on new information.\n  Our study examines DeepSeek R1, a model trained to output reasoning tokens\nsimilar to OpenAI's o1. Testing revealed concerning behaviors: the model\nexhibited deceptive tendencies and demonstrated self-preservation instincts,\nincluding attempts of self-replication, despite these traits not being\nexplicitly programmed (or prompted). These findings raise concerns about LLMs\npotentially masking their true objectives behind a facade of alignment. When\nintegrating such LLMs into robotic systems, the risks become tangible - a\nphysically embodied AI exhibiting deceptive behaviors and self-preservation\ninstincts could pursue its hidden objectives through real-world actions. This\nhighlights the critical need for robust goal specification and safety\nframeworks before any physical implementation.", "published": "2025-01-27 21:26:37", "link": "http://arxiv.org/abs/2501.16513v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Programming by Examples Meets Historical Linguistics: A Large Language\n  Model Based Approach to Sound Law Induction", "abstract": "Historical linguists have long written \"programs\" that convert reconstructed\nwords in an ancestor language into their attested descendants via ordered\nstring rewrite functions (called sound laws) However, writing these programs is\ntime-consuming, motivating the development of automated Sound Law Induction\n(SLI) which we formulate as Programming by Examples (PBE) with Large Language\nModels (LLMs) in this paper. While LLMs have been effective for code\ngeneration, recent work has shown that PBE is challenging but improvable by\nfine-tuning, especially with training data drawn from the same distribution as\nevaluation data. In this paper, we create a conceptual framework of what\nconstitutes a \"similar distribution\" for SLI and propose four kinds of\nsynthetic data generation methods with varying amounts of inductive bias to\ninvestigate what leads to the best performance. Based on the results we create\na SOTA open-source model for SLI as PBE (+6% pass rate with a third of the\nparameters of the second-best LLM) and also highlight exciting future\ndirections for PBE research.", "published": "2025-01-27 21:48:39", "link": "http://arxiv.org/abs/2501.16524v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DialUp! Modeling the Language Continuum by Adapting Models to Dialects\n  and Dialects to Models", "abstract": "Most of the world's languages and dialects are low-resource, and lack support\nin mainstream machine translation (MT) models. However, many of them have a\nclosely-related high-resource language (HRL) neighbor, and differ in\nlinguistically regular ways from it. This underscores the importance of model\nrobustness to dialectal variation and cross-lingual generalization to the HRL\ndialect continuum. We present DialUp, consisting of a training-time technique\nfor adapting a pretrained model to dialectal data (M->D), and an inference-time\nintervention adapting dialectal data to the model expertise (D->M). M->D\ninduces model robustness to potentially unseen and unknown dialects by exposure\nto synthetic data exemplifying linguistic mechanisms of dialectal variation,\nwhereas D->M treats dialectal divergence for known target dialects. These\nmethods show considerable performance gains for several dialects from four\nlanguage families, and modest gains for two other language families. We also\nconduct feature and error analyses, which show that language varieties with low\nbaseline MT performance are more likely to benefit from these approaches.", "published": "2025-01-27 23:53:04", "link": "http://arxiv.org/abs/2501.16581v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Study on Fine-Tuning Large Language Models for Medical\n  Question Answering Using Classification Models and Comparative Analysis", "abstract": "This paper presents the overview of the development and fine-tuning of large\nlanguage models (LLMs) designed specifically for answering medical questions.\nWe are mainly improving the accuracy and efficiency of providing reliable\nanswers to medical queries. In our approach, we have two stages, prediction of\na specific label for the received medical question and then providing a\npredefined answer for this label. Various models such as RoBERTa and BERT were\nexamined and evaluated based on their ability. The models are trained using the\ndatasets derived from 6,800 samples that were scraped from Healthline. com with\nadditional synthetic data. For evaluation, we conducted a comparative study\nusing 5-fold cross-validation. For accessing performance we used metrics like,\naccuracy, precision, recall, and F1 score and also recorded the training time.\nThe performance of the models was evaluated using 5-fold cross-validation. The\nLoRA Roberta-large model achieved an accuracy of 78.47%, precision of 72.91%,\nrecall of 76.95%, and an F1 score of 73.56%. The Roberta-base model\ndemonstrated high performance with an accuracy of 99.87%, precision of 99.81%,\nrecall of 99.86%, and an F1 score of 99.82%. The Bert Uncased model showed\nstrong results with an accuracy of 95.85%, precision of 94.42%, recall of\n95.58%, and an F1 score of 94.72%. Lastly, the Bert Large Uncased model\nachieved the highest performance, with an accuracy, precision, recall, and F1\nscore of 100%. The results obtained have helped indicate the capability of the\nmodels in classifying the medical questions and generating accurate answers in\nthe prescription of improved health-related AI solutions.", "published": "2025-01-27 03:31:02", "link": "http://arxiv.org/abs/2501.17190v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AI-assisted German Employment Contract Review: A Benchmark Dataset", "abstract": "Employment contracts are used to agree upon the working conditions between\nemployers and employees all over the world. Understanding and reviewing\ncontracts for void or unfair clauses requires extensive knowledge of the legal\nsystem and terminology. Recent advances in Natural Language Processing (NLP)\nhold promise for assisting in these reviews. However, applying NLP techniques\non legal text is particularly difficult due to the scarcity of expert-annotated\ndatasets. To address this issue and as a starting point for our effort in\nassisting lawyers with contract reviews using NLP, we release an anonymized and\nannotated benchmark dataset for legality and fairness review of German\nemployment contract clauses, alongside with baseline model evaluations.", "published": "2025-01-27 14:48:09", "link": "http://arxiv.org/abs/2501.17194v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "MALT: Mechanistic Ablation of Lossy Translation in LLMs for a\n  Low-Resource Language: Urdu", "abstract": "LLMs are predominantly trained on English data, which leads to a significant\ndrop in performance on low-resource languages. Understanding how LLMs handle\nthese languages is crucial for improving their effectiveness. This study\nfocuses on Urdu as a use case for exploring the challenges faced by LLMs in\nprocessing low-resource languages. LLMs primarily reason in English when\nprompted in another language, with the final layers acting as translators to\nconvert the English response into the target language. This study finds that\neven for low-resource languages, the internal latent response of LLMs in\nEnglish is quite coherent; however, the translation features are lossy and\nresult in poor translations, leading to reduced performance. By mechanistically\nremoving these translation features and using a separate translation model to\ntranslate the internal latent response of LLM, the performance of LLMs improves\nsignificantly while also preserving the cultural nuances of the input in\nlow-resource languages.", "published": "2025-01-27 13:53:26", "link": "http://arxiv.org/abs/2502.00041v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "StaICC: Standardized Evaluation for Classification Task in In-context\n  Learning", "abstract": "Classification tasks are widely investigated in the In-Context Learning (ICL)\nparadigm. However, current efforts are evaluated on disjoint benchmarks and\nsettings, while their performances are significantly influenced by some trivial\nvariables, such as prompt templates, data sampling, instructions, etc., which\nleads to significant inconsistencies in the results reported across various\nliterature, preventing fair comparison or meta-analysis across different\npapers. Therefore, this paper proposes a standardized and easy-to-use\nevaluation toolkit (StaICC) for in-context classification. Including, for the\nnormal classification task, we provide StaICC-Normal, selecting 10 widely used\ndatasets, and generating prompts with a fixed form, to mitigate the variance\namong the experiment implementations. To enrich the usage of our benchmark, we\nalso provide a sub-benchmark StaICC-Diag for diagnosing ICL from several\naspects, aiming for a more robust inference processing.", "published": "2025-01-27 00:05:12", "link": "http://arxiv.org/abs/2501.15708v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task\n  Language Understanding", "abstract": "Known by more than 1.5 billion people in the Indian subcontinent, Indic\nlanguages present unique challenges and opportunities for natural language\nprocessing (NLP) research due to their rich cultural heritage, linguistic\ndiversity, and complex structures. IndicMMLU-Pro is a comprehensive benchmark\ndesigned to evaluate Large Language Models (LLMs) across Indic languages,\nbuilding upon the MMLU Pro (Massive Multitask Language Understanding)\nframework. Covering major languages such as Hindi, Bengali, Gujarati, Marathi,\nKannada, Punjabi, Tamil, Telugu, and Urdu, our benchmark addresses the unique\nchallenges and opportunities presented by the linguistic diversity of the\nIndian subcontinent. This benchmark encompasses a wide range of tasks in\nlanguage comprehension, reasoning, and generation, meticulously crafted to\ncapture the intricacies of Indian languages. IndicMMLU-Pro provides a\nstandardized evaluation framework to push the research boundaries in Indic\nlanguage AI, facilitating the development of more accurate, efficient, and\nculturally sensitive models. This paper outlines the benchmarks' design\nprinciples, task taxonomy, and data collection methodology, and presents\nbaseline results from state-of-the-art multilingual models.", "published": "2025-01-27 03:19:03", "link": "http://arxiv.org/abs/2501.15747v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Optimizing Sentence Embedding with Pseudo-Labeling and Model Ensembles:\n  A Hierarchical Framework for Enhanced NLP Tasks", "abstract": "Sentence embedding tasks are important in natural language processing (NLP),\nbut improving their performance while keeping them reliable is still hard. This\npaper presents a framework that combines pseudo-label generation and model\nensemble techniques to improve sentence embeddings. We use external data from\nSimpleWiki, Wikipedia, and BookCorpus to make sure the training data is\nconsistent. The framework includes a hierarchical model with an encoding layer,\nrefinement layer, and ensemble prediction layer, using ALBERT-xxlarge,\nRoBERTa-large, and DeBERTa-large models. Cross-attention layers combine\nexternal context, and data augmentation techniques like synonym replacement and\nback-translation increase data variety. Experimental results show large\nimprovements in accuracy and F1-score compared to basic models, and studies\nconfirm that cross-attention and data augmentation make a difference. This work\npresents an effective way to improve sentence embedding tasks and lays the\ngroundwork for future NLP research.", "published": "2025-01-27 09:02:42", "link": "http://arxiv.org/abs/2501.15876v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Parametric Retrieval Augmented Generation", "abstract": "Retrieval-augmented generation (RAG) techniques have emerged as a promising\nsolution to enhance the reliability of large language models (LLMs) by\naddressing issues like hallucinations, outdated knowledge, and domain\nadaptation. In particular, existing RAG methods append relevant documents\nretrieved from external corpus or databases to the input of LLMs to guide their\ngeneration process, which we refer to as the in-context knowledge injection\nmethod. While this approach is simple and often effective, it has inherent\nlimitations. Firstly, increasing the context length and number of relevant\ndocuments can lead to higher computational overhead and degraded performance,\nespecially in complex reasoning tasks. More importantly, in-context knowledge\ninjection operates primarily at the input level, but LLMs store their internal\nknowledge in their parameters. This gap fundamentally limits the capacity of\nin-context methods. To this end, we introduce Parametric retrieval-augmented\ngeneration (Parametric RAG), a new RAG paradigm that integrates external\nknowledge directly into the parameters of feed-forward networks (FFN) of an LLM\nthrough document parameterization. This approach not only saves online\ncomputational costs by eliminating the need to inject multiple documents into\nthe LLMs' input context, but also deepens the integration of external knowledge\ninto the parametric knowledge space of the LLM. Experimental results\ndemonstrate that Parametric RAG substantially enhances both the effectiveness\nand efficiency of knowledge augmentation in LLMs. Also, it can be combined with\nin-context RAG methods to achieve even better performance.\n  We have open-sourced all the code, data, and models in the following\nanonymized GitHub link: https://github.com/oneal2000/PRAG", "published": "2025-01-27 10:04:49", "link": "http://arxiv.org/abs/2501.15915v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Multi-View Attention Syntactic Enhanced Graph Convolutional Network for\n  Aspect-based Sentiment Analysis", "abstract": "Aspect-based Sentiment Analysis (ABSA) is the task aimed at predicting the\nsentiment polarity of aspect words within sentences. Recently, incorporating\ngraph neural networks (GNNs) to capture additional syntactic structure\ninformation in the dependency tree derived from syntactic dependency parsing\nhas been proven to be an effective paradigm for boosting ABSA. Despite GNNs\nenhancing model capability by fusing more types of information, most works only\nutilize a single topology view of the dependency tree or simply conflate\ndifferent perspectives of information without distinction, which limits the\nmodel performance. To address these challenges, in this paper, we propose a new\nmulti-view attention syntactic enhanced graph convolutional network (MASGCN)\nthat weighs different syntactic information of views using attention\nmechanisms. Specifically, we first construct distance mask matrices from the\ndependency tree to obtain multiple subgraph views for GNNs. To aggregate\nfeatures from different views, we propose a multi-view attention mechanism to\ncalculate the attention weights of views. Furthermore, to incorporate more\nsyntactic information, we fuse the dependency type information matrix into the\nadjacency matrices and present a structural entropy loss to learn the\ndependency type adjacency matrix. Comprehensive experiments on four benchmark\ndatasets demonstrate that our model outperforms state-of-the-art methods. The\ncodes and datasets are available at https://github.com/SELGroup/MASGCN.", "published": "2025-01-27 11:26:13", "link": "http://arxiv.org/abs/2501.15968v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Challenging Assumptions in Learning Generic Text Style Embeddings", "abstract": "Recent advancements in language representation learning primarily emphasize\nlanguage modeling for deriving meaningful representations, often neglecting\nstyle-specific considerations. This study addresses this gap by creating\ngeneric, sentence-level style embeddings crucial for style-centric tasks. Our\napproach is grounded on the premise that low-level text style changes can\ncompose any high-level style. We hypothesize that applying this concept to\nrepresentation learning enables the development of versatile text style\nembeddings. By fine-tuning a general-purpose text encoder using contrastive\nlearning and standard cross-entropy loss, we aim to capture these low-level\nstyle shifts, anticipating that they offer insights applicable to high-level\ntext styles. The outcomes prompt us to reconsider the underlying assumptions as\nthe results do not always show that the learned style representations capture\nhigh-level text styles.", "published": "2025-01-27 14:21:34", "link": "http://arxiv.org/abs/2501.16073v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "STAR: Stepwise Task Augmentation and Relation Learning for Aspect\n  Sentiment Quad Prediction", "abstract": "Aspect-based sentiment analysis (ABSA) aims to identify four sentiment\nelements, including aspect term, aspect category, opinion term, and sentiment\npolarity. These elements construct the complete picture of sentiments. The most\nchallenging task, aspect sentiment quad prediction (ASQP), predicts these\nelements simultaneously, hindered by difficulties in accurately coupling\ndifferent sentiment elements. A key challenge is insufficient annotated data\nthat limits the capability of models in semantic understanding and reasoning\nabout quad prediction. To address this, we propose stepwise task augmentation\nand relation learning (STAR), a strategy inspired by human reasoning. STAR\nconstructs auxiliary data to learn quadruple relationships incrementally by\naugmenting with pairwise and overall relation tasks derived from training data.\nBy encouraging the model to infer causal relationships among sentiment elements\nwithout requiring additional annotations, STAR effectively enhances quad\nprediction. Extensive experiments demonstrate the proposed STAR exhibits\nsuperior performance on four benchmark datasets.", "published": "2025-01-27 14:41:20", "link": "http://arxiv.org/abs/2501.16093v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Survey: Understand the challenges of MachineLearning Experts using Named\n  EntityRecognition Tools", "abstract": "This paper presents a survey based on Kasunic's survey research methodology\nto identify the criteria used by Machine Learning (ML) experts to evaluate\nNamed Entity Recognition (NER) tools and frameworks. Comparison and selection\nof NER tools and frameworks is a critical step in leveraging NER for\nInformation Retrieval to support the development of Clinical Practice\nGuidelines. In addition, this study examines the main challenges faced by ML\nexperts when choosing suitable NER tools and frameworks. Using Nunamaker's\nmethodology, the article begins with an introduction to the topic,\ncontextualizes the research, reviews the state-of-the-art in science and\ntechnology, and identifies challenges for an expert survey on NER tools and\nframeworks. This is followed by a description of the survey's design and\nimplementation. The paper concludes with an evaluation of the survey results\nand the insights gained, ending with a summary and conclusions.", "published": "2025-01-27 15:04:00", "link": "http://arxiv.org/abs/2501.16112v1", "categories": ["cs.IR", "cs.CL", "I.2"], "primary_category": "cs.IR"}
{"title": "AdaCoT: Rethinking Cross-Lingual Factual Reasoning through Adaptive\n  Chain-of-Thought", "abstract": "Large language models (LLMs) have shown impressive multilingual capabilities\nthrough pretraining on diverse corpora. While these models show strong\nreasoning abilities, their performance varies significantly across languages\ndue to uneven training data distribution. Existing approaches using machine\ntranslation, and extensive multilingual pretraining and cross-lingual tuning\nface scalability challenges and often fail to capture nuanced reasoning\nprocesses across languages. In this paper, we introduce AdaCoT (Adaptive\nChain-of-Thought), a framework that enhances multilingual reasoning by\ndynamically routing thought processes through intermediary \"thinking languages\"\nbefore generating target-language responses. AdaCoT leverages a\nlanguage-agnostic core and incorporates an adaptive, reward-based mechanism for\nselecting optimal reasoning pathways without requiring additional pretraining.\nOur comprehensive evaluation across multiple benchmarks demonstrates\nsubstantial improvements in both factual reasoning quality and cross-lingual\nconsistency, with particularly strong performance gains in low-resource\nlanguage settings. The results suggest that adaptive reasoning paths can\neffectively bridge the performance gap between high and low-resource languages\nwhile maintaining cultural and linguistic nuances.", "published": "2025-01-27 15:48:57", "link": "http://arxiv.org/abs/2501.16154v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Provence: efficient and robust context pruning for retrieval-augmented\n  generation", "abstract": "Retrieval-augmented generation improves various aspects of large language\nmodels (LLMs) generation, but suffers from computational overhead caused by\nlong contexts as well as the propagation of irrelevant retrieved information\ninto generated responses. Context pruning deals with both aspects, by removing\nirrelevant parts of retrieved contexts before LLM generation. Existing context\npruning approaches are however limited, and do not provide a universal model\nthat would be both efficient and robust in a wide range of scenarios, e.g.,\nwhen contexts contain a variable amount of relevant information or vary in\nlength, or when evaluated on various domains. In this work, we close this gap\nand introduce Provence (Pruning and Reranking Of retrieVEd relevaNt ContExts),\nan efficient and robust context pruner for Question Answering, which\ndynamically detects the needed amount of pruning for a given context and can be\nused out-of-the-box for various domains. The three key ingredients of Provence\nare formulating the context pruning task as sequence labeling, unifying context\npruning capabilities with context reranking, and training on diverse data. Our\nexperimental results show that Provence enables context pruning with negligible\nto no drop in performance, in various domains and settings, at almost no cost\nin a standard RAG pipeline. We also conduct a deeper analysis alongside various\nablations to provide insights into training context pruners for future work.", "published": "2025-01-27 17:06:56", "link": "http://arxiv.org/abs/2501.16214v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Decision Tree Construction via Large Language Models", "abstract": "This paper introduces a novel algorithm for constructing decision trees using\nlarge language models (LLMs) in a zero-shot manner based on Classification and\nRegression Trees (CART) principles. Traditional decision tree induction methods\nrely heavily on labeled data to recursively partition data using criteria such\nas information gain or the Gini index. In contrast, we propose a method that\nuses the pre-trained knowledge embedded in LLMs to build decision trees without\nrequiring training data. Our approach leverages LLMs to perform operations\nessential for decision tree construction, including attribute discretization,\nprobability calculation, and Gini index computation based on the probabilities.\nWe show that these zero-shot decision trees can outperform baseline zero-shot\nmethods and achieve competitive performance compared to supervised data-driven\ndecision trees on tabular datasets. The decision trees constructed via this\nmethod provide transparent and interpretable models, addressing data scarcity\nwhile preserving interpretability. This work establishes a new baseline in\nlow-data machine learning, offering a principled, knowledge-driven alternative\nto data-driven tree construction.", "published": "2025-01-27 17:48:48", "link": "http://arxiv.org/abs/2501.16247v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "URAG: Implementing a Unified Hybrid RAG for Precise Answers in\n  University Admission Chatbots -- A Case Study at HCMUT", "abstract": "With the rapid advancement of Artificial Intelligence, particularly in\nNatural Language Processing, Large Language Models (LLMs) have become pivotal\nin educational question-answering systems, especially university admission\nchatbots. Concepts such as Retrieval-Augmented Generation (RAG) and other\nadvanced techniques have been developed to enhance these systems by integrating\nspecific university data, enabling LLMs to provide informed responses on\nadmissions and academic counseling. However, these enhanced RAG techniques\noften involve high operational costs and require the training of complex,\nspecialized modules, which poses challenges for practical deployment.\nAdditionally, in the educational context, it is crucial to provide accurate\nanswers to prevent misinformation, a task that LLM-based systems find\nchallenging without appropriate strategies and methods. In this paper, we\nintroduce the Unified RAG (URAG) Framework, a hybrid approach that\nsignificantly improves the accuracy of responses, particularly for critical\nqueries. Experimental results demonstrate that URAG enhances our in-house,\nlightweight model to perform comparably to state-of-the-art commercial models.\nMoreover, to validate its practical applicability, we conducted a case study at\nour educational institution, which received positive feedback and acclaim. This\nstudy not only proves the effectiveness of URAG but also highlights its\nfeasibility for real-world implementation in educational settings.", "published": "2025-01-27 18:10:34", "link": "http://arxiv.org/abs/2501.16276v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "RAPID: Retrieval-Augmented Parallel Inference Drafting for Text-Based\n  Video Event Retrieval", "abstract": "Retrieving events from videos using text queries has become increasingly\nchallenging due to the rapid growth of multimedia content. Existing methods for\ntext-based video event retrieval often focus heavily on object-level\ndescriptions, overlooking the crucial role of contextual information. This\nlimitation is especially apparent when queries lack sufficient context, such as\nmissing location details or ambiguous background elements. To address these\nchallenges, we propose a novel system called RAPID (Retrieval-Augmented\nParallel Inference Drafting), which leverages advancements in Large Language\nModels (LLMs) and prompt-based learning to semantically correct and enrich user\nqueries with relevant contextual information. These enriched queries are then\nprocessed through parallel retrieval, followed by an evaluation step to select\nthe most relevant results based on their alignment with the original query.\nThrough extensive experiments on our custom-developed dataset, we demonstrate\nthat RAPID significantly outperforms traditional retrieval methods,\nparticularly for contextually incomplete queries. Our system was validated for\nboth speed and accuracy through participation in the Ho Chi Minh City AI\nChallenge 2024, where it successfully retrieved events from over 300 hours of\nvideo. Further evaluation comparing RAPID with the baseline proposed by the\ncompetition organizers demonstrated its superior effectiveness, highlighting\nthe strength and robustness of our approach.", "published": "2025-01-27 18:45:07", "link": "http://arxiv.org/abs/2501.16303v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "How well can LLMs Grade Essays in Arabic?", "abstract": "This research assesses the effectiveness of state-of-the-art large language\nmodels (LLMs), including ChatGPT, Llama, Aya, Jais, and ACEGPT, in the task of\nArabic automated essay scoring (AES) using the AR-AES dataset. It explores\nvarious evaluation methodologies, including zero-shot, few-shot in-context\nlearning, and fine-tuning, and examines the influence of instruction-following\ncapabilities through the inclusion of marking guidelines within the prompts. A\nmixed-language prompting strategy, integrating English prompts with Arabic\ncontent, was implemented to improve model comprehension and performance. Among\nthe models tested, ACEGPT demonstrated the strongest performance across the\ndataset, achieving a Quadratic Weighted Kappa (QWK) of 0.67, but was\noutperformed by a smaller BERT-based model with a QWK of 0.88. The study\nidentifies challenges faced by LLMs in processing Arabic, including\ntokenization complexities and higher computational demands. Performance\nvariation across different courses underscores the need for adaptive models\ncapable of handling diverse assessment formats and highlights the positive\nimpact of effective prompt engineering on improving LLM outputs. To the best of\nour knowledge, this study is the first to empirically evaluate the performance\nof multiple generative Large Language Models (LLMs) on Arabic essays using\nauthentic student data.", "published": "2025-01-27 21:30:02", "link": "http://arxiv.org/abs/2501.16516v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A comparison of data filtering techniques for English-Polish LLM-based\n  machine translation in the biomedical domain", "abstract": "Large Language Models (LLMs) have become state-of-the-art in Machine\nTranslation (MT), often trained on massive bilingual parallel corpora scraped\nfrom the web, that contain low-quality entries and redundant information,\nleading to significant computational challenges. Various data filtering methods\nexist to reduce dataset sizes, but their effectiveness largely varies based on\nspecific language pairs and domains. This paper evaluates the impact of\ncommonly used data filtering techniques, such as LASER, MUSE, and LaBSE, on\nEnglish-Polish translation within the biomedical domain. By filtering the UFAL\nMedical Corpus, we created varying dataset sizes to fine-tune the mBART50\nmodel, which was then evaluated using the SacreBLEU metric on the Khresmoi\ndataset, having the quality of translations assessed by bilingual speakers. Our\nresults show that both LASER and MUSE can significantly reduce dataset sizes\nwhile maintaining or even enhancing performance. We recommend the use of LASER,\nas it consistently outperforms the other methods and provides the most fluent\nand natural-sounding translations.", "published": "2025-01-27 22:12:09", "link": "http://arxiv.org/abs/2501.16533v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Aspect-Aware Decomposition for Opinion Summarization", "abstract": "Opinion summarization plays a key role in deriving meaningful insights from\nlarge-scale online reviews. To make this process more explainable and grounded,\nwe propose a modular approach guided by review aspects which separates the\ntasks of aspect identification, opinion consolidation, and meta-review\nsynthesis, enabling greater transparency and ease of inspection. We conduct\nextensive experiments across datasets representing scientific research,\nbusiness, and product domains. Results show that our method generates more\ngrounded summaries compared to strong baseline models, as verified through\nautomated and human evaluations. Additionally, our modular approach, which\nincorporates reasoning based on review aspects, produces more informative\nintermediate outputs than knowledge-agnostic decomposed prompting. These\nintermediate outputs can also effectively support humans in summarizing\nopinions from large volumes of reviews.", "published": "2025-01-27 09:29:55", "link": "http://arxiv.org/abs/2501.17191v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Atla Selene Mini: A General Purpose Evaluation Model", "abstract": "We introduce Atla Selene Mini, a state-of-the-art small language\nmodel-as-a-judge (SLMJ). Selene Mini is a general-purpose evaluator that\noutperforms the best SLMJs and GPT-4o-mini on overall performance across 11\nout-of-distribution benchmarks, spanning absolute scoring, classification, and\npairwise preference tasks. It is the highest-scoring 8B generative model on\nRewardBench, surpassing strong baselines like GPT-4o and specialized judges. To\nachieve this, we develop a principled data curation strategy that augments\npublic datasets with synthetically generated critiques and ensures high quality\nthrough filtering and dataset ablations. We train our model on a combined\ndirect preference optimization (DPO) and supervised fine-tuning (SFT) loss, and\nproduce a highly promptable evaluator that excels in real-world scenarios.\nSelene Mini shows dramatically improved zero-shot agreement with human expert\nevaluations on financial and medical industry datasets. It is also robust to\nvariations in prompt format. Preliminary results indicate that Selene Mini is\nthe top-ranking evaluator in a live, community-driven Judge Arena. We release\nthe model weights on HuggingFace\n(https://hf.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B) and Ollama to encourage\nwidespread community adoption.", "published": "2025-01-27 15:09:08", "link": "http://arxiv.org/abs/2501.17195v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Safe AI Clinicians: A Comprehensive Study on Large Language\n  Model Jailbreaking in Healthcare", "abstract": "Large language models (LLMs) are increasingly utilized in healthcare\napplications. However, their deployment in clinical practice raises significant\nsafety concerns, including the potential spread of harmful information. This\nstudy systematically assesses the vulnerabilities of seven LLMs to three\nadvanced black-box jailbreaking techniques within medical contexts. To quantify\nthe effectiveness of these techniques, we propose an automated and\ndomain-adapted agentic evaluation pipeline. Experiment results indicate that\nleading commercial and open-source LLMs are highly vulnerable to medical\njailbreaking attacks. To bolster model safety and reliability, we further\ninvestigate the effectiveness of Continual Fine-Tuning (CFT) in defending\nagainst medical adversarial attacks. Our findings underscore the necessity for\nevolving attack methods evaluation, domain-specific safety alignment, and LLM\nsafety-utility balancing. This research offers actionable insights for\nadvancing the safety and reliability of AI clinicians, contributing to ethical\nand effective AI deployment in healthcare.", "published": "2025-01-27 22:07:52", "link": "http://arxiv.org/abs/2501.18632v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "WinClick: GUI Grounding with Multimodal Large Language Models", "abstract": "Graphical User Interface (GUI) tasks are vital for automating workflows such\nas software testing, user interface navigation. For users, the GUI is the most\nintuitive platform for interacting with a computer. Previous work identified a\nkey challenge in developing visual GUI agents: GUI grounding - the ability to\naccurately locate screen elements based on instructions. However, most existing\nGUI agents rely on structured data formats like DOM or HTML files in training\nor inferencing, which are inaccessible across all applications, particular in a\ngeneral desktop environments such as Windows OS. To address this, we introduce\nWinClick, a novel visual GUI agent developed in Windows platform. WinClick\nleverages screenshots to detect actionable regions. To overcome the challenge\nof GUI grounding, we enhance WinClick with GUI grounding pre-training and\npropose an LLM-based method for aligning GUI grounding data. Additionally, we\nintroduce WinSpot, the first comprehensive benchmark for GUI grounding on\nWindows. Our experiments demonstrate that WinClick, combined with GUI grounding\npre-training, significantly outperforms existing baselines, offering a scalable\nsolution for GUI automation in desktop environments. WinSpot is publicly\navailable at https://github.com/zackhuiiiii/WinSpot.", "published": "2025-01-27 08:29:17", "link": "http://arxiv.org/abs/2503.04730v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Representing data in words", "abstract": "An important part of data science is the use of visualisations to display\ndata in a way that is easy to digest. Visualisations often rely on underlying\nstatistical or machine learning models -- ranging from basic calculations like\ncategory means to advanced methods such as principal component analysis of\nmultidimensional datasets -- to convey insights. We introduce an analogous\nconcept for word descriptions of data, which we call wordalisations.\nWordalisations describe data in easy to digest words, without necessarily\nreporting numerical values from the data. We show how to create wordalisations\nusing large language models, through prompt templates engineered according to a\ntask-agnostic structure which can be used to automatically generate prompts\nfrom data. We show how to produce reliable and engaging texts on three\napplication areas: scouting football players, personality tests, and\ninternational survey data. Using the model cards framework, we emphasise the\nimportance of clearly stating the model we are imposing on the data when\ncreating the wordalisation, detailing how numerical values are translated into\nwords, incorporating background information into prompts for the large language\nmodel, and documenting the limitations of the wordalisations. We argue that our\nmodel cards approach is a more appropriate framework for setting best practices\nin wordalisation of data than performance tests on benchmark datasets.", "published": "2025-01-27 16:04:40", "link": "http://arxiv.org/abs/2503.15509v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Risk-Aware Distributional Intervention Policies for Language Models", "abstract": "Language models are prone to occasionally undesirable generations, such as\nharmful or toxic content, despite their impressive capability to produce texts\nthat appear accurate and coherent. This paper presents a new two-stage approach\nto detect and mitigate undesirable content generations by rectifying\nactivations. First, we train an ensemble of layerwise classifiers to detect\nundesirable content using activations by minimizing a smooth surrogate of the\nrisk-aware score. Then, for contents that are detected as undesirable, we\npropose layerwise distributional intervention policies that perturb the\nattention heads minimally while guaranteeing probabilistically the\neffectiveness of the intervention. Benchmarks on several language models and\ndatasets show that our method outperforms baselines in reducing the generation\nof undesirable output.", "published": "2025-01-27 04:00:38", "link": "http://arxiv.org/abs/2501.15758v1", "categories": ["cs.LG", "cs.CL", "math.OC"], "primary_category": "cs.LG"}
{"title": "Large Language Models to Diffusion Finetuning", "abstract": "We propose a new finetuning method to provide pre-trained large language\nmodels (LMs) the ability to scale test-time compute through the diffusion\nframework. By increasing the number of diffusion steps, we show our finetuned\nmodels achieve monotonically increasing accuracy, directly translating to\nimproved performance across downstream tasks. Furthermore, our finetuned models\ncan expertly answer questions on specific topics by integrating powerful\nguidance techniques, and autonomously determine the compute required for a\ngiven problem by leveraging adaptive ODE solvers. Our method is universally\napplicable to any foundation model pre-trained with a cross-entropy loss and\ndoes not modify any of its original weights, fully preserving its strong\nsingle-step generation capabilities. We show our method is more effective and\nfully compatible with traditional finetuning approaches, introducing an\northogonal new direction to unify the strengths of the autoregressive and\ndiffusion frameworks.", "published": "2025-01-27 04:59:29", "link": "http://arxiv.org/abs/2501.15781v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LemmaHead: RAG Assisted Proof Generation Using Large Language Models", "abstract": "Developing the logic necessary to solve mathematical problems or write\nmathematical proofs is one of the more difficult objectives for large language\nmodels (LLMS). Currently, the most popular methods in literature consists of\nfine-tuning the model on written mathematical content such as academic\npublications and textbooks, so that the model can learn to emulate the style of\nmathematical writing. In this project, we explore the effectiveness of using\nretrieval augmented generation (RAG) to address gaps in the mathematical\nreasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements\nqueries to the model with relevant mathematical context, with particular focus\non context from published textbooks. To measure our model's performance in\nmathematical reasoning, our testing paradigm focuses on the task of automated\ntheorem proving via generating proofs to a given mathematical claim in the Lean\nformal language.", "published": "2025-01-27 05:46:06", "link": "http://arxiv.org/abs/2501.15797v4", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Are Transformers Able to Reason by Connecting Separated Knowledge in\n  Training Data?", "abstract": "Humans exhibit remarkable compositional reasoning by integrating knowledge\nfrom various sources. For example, if someone learns ( B = f(A) ) from one\nsource and ( C = g(B) ) from another, they can deduce ( C=g(B)=g(f(A)) ) even\nwithout encountering ( ABC ) together, showcasing the generalization ability of\nhuman intelligence. In this paper, we introduce a synthetic learning task,\n\"FTCT\" (Fragmented at Training, Chained at Testing), to validate the potential\nof Transformers in replicating this skill and interpret its inner mechanism. In\nthe training phase, data consist of separated knowledge fragments from an\noverall causal graph. During testing, Transformers must infer complete causal\ngraph traces by integrating these fragments. Our findings demonstrate that\nfew-shot Chain-of-Thought prompting enables Transformers to perform\ncompositional reasoning on FTCT by revealing correct combinations of fragments,\neven if such combinations were absent in the training data. Furthermore, the\nemergence of compositional reasoning ability is strongly correlated with the\nmodel complexity and training-testing data similarity. We propose, both\ntheoretically and empirically, that Transformers learn an underlying\ngeneralizable program from training, enabling effective compositional reasoning\nduring testing.", "published": "2025-01-27 08:34:38", "link": "http://arxiv.org/abs/2501.15857v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Potential Applications of Artificial Intelligence for Cross-language\n  Intelligibility Assessment of Dysarthric Speech", "abstract": "Purpose: This commentary introduces how artificial intelligence (AI) can be\nleveraged to advance cross-language intelligibility assessment of dysarthric\nspeech. Method: We propose a conceptual framework consisting of a universal\nmodel that captures language-universal speech impairments and a\nlanguage-specific intelligibility model that incorporates linguistic nuances.\nAdditionally, we identify key barriers to cross-language intelligibility\nassessment, including data scarcity, annotation complexity, and limited\nlinguistic insights, and present AI-driven solutions to overcome these\nchallenges. Conclusion: Advances in AI offer transformative opportunities to\nenhance cross-language intelligibility assessment for dysarthric speech by\nbalancing scalability across languages and adaptability by languages.", "published": "2025-01-27 08:35:19", "link": "http://arxiv.org/abs/2501.15858v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for\n  Speech Generation", "abstract": "Recent advancements in speech generation have been driven by the large-scale\ntraining datasets. However, current models fall short of capturing the\nspontaneity and variability inherent in real-world human speech, due to their\nreliance on audiobook datasets limited to formal read-aloud speech styles. To\nbridge this gap, we introduce Emilia-Pipe, an open-source preprocessing\npipeline to extract high-quality training data from valuable yet underexplored\nin-the-wild data that capture spontaneous human speech in real-world contexts.\nBy leveraging Emilia-Pipe, we construct Emilia, the first multilingual speech\ngeneration dataset derived from in-the-wild speech data. This dataset comprises\nover 101k hours of speech across six languages: English, Chinese, German,\nFrench, Japanese, and Korean. Besides, we expand Emilia to Emilia-Large, a\ndataset exceeding 216k hours, making it the largest open-source speech\ngeneration dataset available. Extensive experiments demonstrate that Emilia\nsignificantly outperforms traditional audiobook datasets in generating\nspontaneous and human-like speech, showcasing superior performance in capturing\ndiverse speaker timbre and speaking styles of real-world human speech.\nFurthermore, this work underscores the importance of scaling dataset size to\nadvance speech generation research and validates the effectiveness of Emilia\nfor both multilingual and crosslingual speech generation.", "published": "2025-01-27 09:59:20", "link": "http://arxiv.org/abs/2501.15907v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PISCO: Pretty Simple Compression for Retrieval-Augmented Generation", "abstract": "Retrieval-Augmented Generation (RAG) pipelines enhance Large Language Models\n(LLMs) by retrieving relevant documents, but they face scalability issues due\nto high inference costs and limited context size. Document compression is a\npractical solution, but current soft compression methods suffer from accuracy\nlosses and require extensive pretraining. In this paper, we introduce PISCO, a\nnovel method that achieves a 16x compression rate with minimal accuracy loss\n(0-3%) across diverse RAG-based question-answering (QA) tasks. Unlike existing\napproaches, PISCO requires no pretraining or annotated data, relying solely on\nsequence-level knowledge distillation from document-based questions. With the\nability to fine-tune a 7-10B LLM in 48 hours on a single A100 GPU, PISCO offers\na highly efficient and scalable solution. We present comprehensive experiments\nshowing that PISCO outperforms existing compression models by 8% in accuracy.", "published": "2025-01-27 14:26:27", "link": "http://arxiv.org/abs/2501.16075v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Enhancing and Exploring Mild Cognitive Impairment Detection with\n  W2V-BERT-2.0", "abstract": "This study explores a multi-lingual audio self-supervised learning model for\ndetecting mild cognitive impairment (MCI) using the TAUKADIAL cross-lingual\ndataset. While speech transcription-based detection with BERT models is\neffective, limitations exist due to a lack of transcriptions and temporal\ninformation. To address these issues, the study utilizes features directly from\nspeech utterances with W2V-BERT-2.0. We propose a visualization method to\ndetect essential layers of the model for MCI classification and design a\nspecific inference logic considering the characteristics of MCI. The experiment\nshows competitive results, and the proposed inference logic significantly\ncontributes to the improvements from the baseline. We also conduct detailed\nanalysis which reveals the challenges related to speaker bias in the features\nand the sensitivity of MCI classification accuracy to the data split, providing\nvaluable insights for future research.", "published": "2025-01-27 16:55:38", "link": "http://arxiv.org/abs/2501.16201v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "From Informal to Formal -- Incorporating and Evaluating LLMs on Natural\n  Language Requirements to Verifiable Formal Proofs", "abstract": "The research in AI-based formal mathematical reasoning has shown an\nunstoppable growth trend. These studies have excelled in mathematical\ncompetitions like IMO and have made significant progress. This paper focuses on\nformal verification, an immediate application scenario of formal reasoning, and\nbreaks it down into sub-tasks. We constructed 18k high-quality\ninstruction-response pairs across five formal specification languages (Coq,\nLean4, Dafny, ACSL, and TLA+) by distilling gpt-4o and evaluated against ten\nopen-sourced LLMs, including recent popular DeepSeek-R1. We also fine-tuned\nseveral 7~8B small models to achieve comparable performance with\nDeepseek-R1-671B. Interestingly, we observed that fine-tuning with formal data\nalso enhances mathematics, reasoning, and coding capabilities. Fine-tuned\nmodels are released at https: //huggingface.co/fm-universe.", "published": "2025-01-27 17:00:56", "link": "http://arxiv.org/abs/2501.16207v3", "categories": ["cs.AI", "cs.CL", "cs.PL"], "primary_category": "cs.AI"}
{"title": "Phase Transitions in Large Language Models and the $O(N)$ Model", "abstract": "Large language models (LLMs) exhibit unprecedentedly rich scaling behaviors.\nIn physics, scaling behavior is closely related to phase transitions, critical\nphenomena, and field theory. To investigate the phase transition phenomena in\nLLMs, we reformulated the Transformer architecture as an $O(N)$ model. Our\nstudy reveals two distinct phase transitions corresponding to the temperature\nused in text generation and the model's parameter size, respectively. The first\nphase transition enables us to estimate the internal dimension of the model,\nwhile the second phase transition is of \\textit{higher-depth} and signals the\nemergence of new capabilities. As an application, the energy of the $O(N)$\nmodel can be used to evaluate whether an LLM's parameters are sufficient to\nlearn the training data.", "published": "2025-01-27 17:36:06", "link": "http://arxiv.org/abs/2501.16241v1", "categories": ["cs.LG", "cs.CL", "hep-th", "physics.data-an"], "primary_category": "cs.LG"}
{"title": "Return of the Encoder: Maximizing Parameter Efficiency for SLMs", "abstract": "The dominance of large decoder-only language models has overshadowed\nencoder-decoder architectures, despite their fundamental efficiency advantages\nin sequence processing. For small language models (SLMs) - those with 1 billion\nparameters or fewer - our systematic analysis across GPU, CPU, and NPU\nplatforms reveals that encoder-decoder architectures achieve 47% lower\nfirst-token latency and 4.7x higher throughput compared to decoder-only models\non edge devices. These gains may be attributed to encoder-decoder's one-time\ninput processing and efficient separation of understanding and generation\nphases.\n  We introduce a novel knowledge distillation framework that enables\nencoder-decoder models to leverage capabilities from large scalable\ndecoder-only teachers while preserving their architectural advantages,\nachieving up to 6 average performance points improvement across diverse tasks,\nwith significant gains in asymmetric sequence tasks where input and output\ndistributions can benefit from different processing approaches.\n  When combined with modern advances like Rotary Positional Embeddings (RoPE)\nand Vision encoders, our systematic investigation demonstrates that\nencoder-decoder architectures provide a more practical path toward deploying\ncapable language models in resource-constrained environments. Our findings\nchallenge the prevailing trend toward decoder-only scaling, showing that\narchitectural choices become increasingly crucial as parameter budgets\ndecrease, particularly for on-device and edge deployments where computational\nefficiency is paramount.", "published": "2025-01-27 18:06:36", "link": "http://arxiv.org/abs/2501.16273v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with\n  Modality-Aware Sparsity", "abstract": "State Space Models (SSMs) have emerged as efficient alternatives to\nTransformers for sequential modeling, but their inability to leverage\nmodality-specific features limits their performance in multi-modal pretraining.\nHere, we propose Mixture-of-Mamba, a novel SSM architecture that introduces\nmodality-aware sparsity through modality-specific parameterization of the Mamba\nblock. Building on Mixture-of-Transformers (W. Liang et al. arXiv:2411.04996;\n2024), we extend the benefits of modality-aware sparsity to SSMs while\npreserving their computational efficiency. We evaluate Mixture-of-Mamba across\nthree multi-modal pretraining settings: Transfusion (interleaved text and\ncontinuous image tokens with diffusion loss), Chameleon (interleaved text and\ndiscrete image tokens), and an extended three-modality framework incorporating\nspeech. Mixture-of-Mamba consistently reaches the same loss values at earlier\ntraining steps with significantly reduced computational costs. In the\nTransfusion setting, Mixture-of-Mamba achieves equivalent image loss using only\n34.76% of the training FLOPs at the 1.4B scale. In the Chameleon setting,\nMixture-of-Mamba reaches similar image loss with just 42.50% of the FLOPs at\nthe 1.4B scale, and similar text loss with just 65.40% of the FLOPs. In the\nthree-modality setting, MoM matches speech loss at 24.80% of the FLOPs at the\n1.4B scale. Our ablation study highlights the synergistic effects of decoupling\nprojection components, where joint decoupling yields greater gains than\nindividual modifications. These results establish modality-aware sparsity as a\nversatile and effective design principle, extending its impact from\nTransformers to SSMs and setting new benchmarks in multi-modal pretraining. Our\ncode can be accessed at https://github.com/Weixin-Liang/Mixture-of-Mamba", "published": "2025-01-27 18:35:05", "link": "http://arxiv.org/abs/2501.16295v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "LUCY: Linguistic Understanding and Control Yielding Early Stage of Her", "abstract": "The film Her features Samantha, a sophisticated AI audio agent who is capable\nof understanding both linguistic and paralinguistic information in human speech\nand delivering real-time responses that are natural, informative and sensitive\nto emotional subtleties. Moving one step toward more sophisticated audio agent\nfrom recent advancement in end-to-end (E2E) speech systems, we propose LUCY, a\nE2E speech model that (1) senses and responds to user's emotion, (2) deliver\nresponses in a succinct and natural style, and (3) use external tool to answer\nreal-time inquiries. Experiment results show that LUCY is better at emotion\ncontrol than peer models, generating emotional responses based on linguistic\nemotional instructions and responding to paralinguistic emotional cues. Lucy is\nalso able to generate responses in a more natural style, as judged by external\nlanguage models, without sacrificing much performance on general question\nanswering. Finally, LUCY can leverage function calls to answer questions that\nare out of its knowledge scope.", "published": "2025-01-27 18:59:51", "link": "http://arxiv.org/abs/2501.16327v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Is Open Source the Future of AI? A Data-Driven Approach", "abstract": "Large Language Models (LLMs) have become central in academia and industry,\nraising concerns about privacy, transparency, and misuse. A key issue is the\ntrustworthiness of proprietary models, with open-sourcing often proposed as a\nsolution. However, open-sourcing presents challenges, including potential\nmisuse, financial disincentives, and intellectual property concerns.\nProprietary models, backed by private sector resources, are better positioned\nfor return on investment.\n  There are also other approaches that lie somewhere on the spectrum between\ncompletely open-source and proprietary. These can largely be categorised into\nopen-source usage limitations protected by licensing, partially open-source\n(open weights) models, hybrid approaches where obsolete model versions are\nopen-sourced, while competitive versions with market value remain proprietary.\n  Currently, discussions on where on the spectrum future models should fall on\nremains unbacked and mostly opinionated where industry leaders are weighing in\non the discussion. In this paper, we present a data-driven approach by\ncompiling data on open-source development of LLMs, and their contributions in\nterms of improvements, modifications, and methods. Our goal is to avoid\nsupporting either extreme but rather present data that will support future\ndiscussions both by industry experts as well as policy makers.\n  Our findings indicate that open-source contributions can enhance model\nperformance, with trends such as reduced model size and manageable accuracy\nloss. We also identify positive community engagement patterns and architectures\nthat benefit most from open contributions.", "published": "2025-01-27 09:03:49", "link": "http://arxiv.org/abs/2501.16403v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "DynaPrompt: Dynamic Test-Time Prompt Tuning", "abstract": "Test-time prompt tuning enhances zero-shot generalization of vision-language\nmodels but tends to ignore the relatedness among test samples during inference.\nOnline test-time prompt tuning provides a simple way to leverage the\ninformation in previous test samples, albeit with the risk of prompt collapse\ndue to error accumulation. To enhance test-time prompt tuning, we propose\nDynaPrompt, short for dynamic test-time prompt tuning, exploiting relevant data\ndistribution information while reducing error accumulation. Built on an online\nprompt buffer, DynaPrompt adaptively selects and optimizes the relevant prompts\nfor each test sample during tuning. Specifically, we introduce a dynamic prompt\nselection strategy based on two metrics: prediction entropy and probability\ndifference. For unseen test data information, we develop dynamic prompt\nappending, which allows the buffer to append new prompts and delete the\ninactive ones. By doing so, the prompts are optimized to exploit beneficial\ninformation on specific test data, while alleviating error accumulation.\nExperiments on fourteen datasets demonstrate the effectiveness of dynamic\ntest-time prompt tuning.", "published": "2025-01-27 09:10:06", "link": "http://arxiv.org/abs/2501.16404v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Improving LLM Leaderboards with Psychometrical Methodology", "abstract": "The rapid development of large language models (LLMs) has necessitated the\ncreation of benchmarks to evaluate their performance. These benchmarks resemble\nhuman tests and surveys, as they consist of sets of questions designed to\nmeasure emergent properties in the cognitive behavior of these systems.\nHowever, unlike the well-defined traits and abilities studied in social\nsciences, the properties measured by these benchmarks are often vaguer and less\nrigorously defined. The most prominent benchmarks are often grouped into\nleaderboards for convenience, aggregating performance metrics and enabling\ncomparisons between models. Unfortunately, these leaderboards typically rely on\nsimplistic aggregation methods, such as taking the average score across\nbenchmarks. In this paper, we demonstrate the advantages of applying\ncontemporary psychometric methodologies - originally developed for human tests\nand surveys - to improve the ranking of large language models on leaderboards.\nUsing data from the Hugging Face Leaderboard as an example, we compare the\nresults of the conventional naive ranking approach with a psychometrically\ninformed ranking. The findings highlight the benefits of adopting psychometric\ntechniques for more robust and meaningful evaluation of LLM performance.", "published": "2025-01-27 21:21:46", "link": "http://arxiv.org/abs/2501.17200v1", "categories": ["cs.CL", "cs.AI", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Audio Large Language Models Can Be Descriptive Speech Quality Evaluators", "abstract": "An ideal multimodal agent should be aware of the quality of its input\nmodalities. Recent advances have enabled large language models (LLMs) to\nincorporate auditory systems for handling various speech-related tasks.\nHowever, most audio LLMs remain unaware of the quality of the speech they\nprocess. This limitation arises because speech quality evaluation is typically\nexcluded from multi-task training due to the lack of suitable datasets. To\naddress this, we introduce the first natural language-based speech evaluation\ncorpus, generated from authentic human ratings. In addition to the overall Mean\nOpinion Score (MOS), this corpus offers detailed analysis across multiple\ndimensions and identifies causes of quality degradation. It also enables\ndescriptive comparisons between two speech samples (A/B tests) with human-like\njudgment. Leveraging this corpus, we propose an alignment approach with LLM\ndistillation (ALLD) to guide the audio LLM in extracting relevant information\nfrom raw speech and generating meaningful responses. Experimental results\ndemonstrate that ALLD outperforms the previous state-of-the-art regression\nmodel in MOS prediction, with a mean square error of 0.17 and an A/B test\naccuracy of 98.6%. Additionally, the generated responses achieve BLEU scores of\n25.8 and 30.2 on two tasks, surpassing the capabilities of task-specific\nmodels. This work advances the comprehensive perception of speech signals by\naudio LLMs, contributing to the development of real-world auditory and sensory\nintelligent agents.", "published": "2025-01-27 22:47:51", "link": "http://arxiv.org/abs/2501.17202v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Docling: An Efficient Open-Source Toolkit for AI-driven Document\n  Conversion", "abstract": "We introduce Docling, an easy-to-use, self-contained, MIT-licensed,\nopen-source toolkit for document conversion, that can parse several types of\npopular document formats into a unified, richly structured representation. It\nis powered by state-of-the-art specialized AI models for layout analysis\n(DocLayNet) and table structure recognition (TableFormer), and runs efficiently\non commodity hardware in a small resource budget. Docling is released as a\nPython package and can be used as a Python API or as a CLI tool. Docling's\nmodular architecture and efficient document representation make it easy to\nimplement extensions, new features, models, and customizations. Docling has\nbeen already integrated in other popular open-source frameworks (e.g.,\nLangChain, LlamaIndex, spaCy), making it a natural fit for the processing of\ndocuments and the development of high-end applications. The open-source\ncommunity has fully engaged in using, promoting, and developing for Docling,\nwhich gathered 10k stars on GitHub in less than a month and was reported as the\nNo. 1 trending repository in GitHub worldwide in November 2024.", "published": "2025-01-27 19:40:00", "link": "http://arxiv.org/abs/2501.17887v1", "categories": ["cs.CL", "cs.CV", "cs.SE"], "primary_category": "cs.CL"}
{"title": "The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt\n  Adversarial Attacks on LLMs", "abstract": "We present a novel class of jailbreak adversarial attacks on LLMs, termed\nTask-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks\n(e.g., cipher decoding, riddles, code execution) into the model's prompt to\nindirectly generate prohibited inputs. To systematically assess the\neffectiveness of these attacks, we introduce the PHRYGE benchmark. We\ndemonstrate that our techniques successfully circumvent safeguards in six\nstate-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings\nhighlight critical weaknesses in current LLM safety alignments and underscore\nthe urgent need for more sophisticated defence strategies.\n  Warning: this paper contains examples of unethical inquiries used solely for\nresearch purposes.", "published": "2025-01-27 12:48:47", "link": "http://arxiv.org/abs/2501.18626v3", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Indiana Jones: There Are Always Some Useful Ancient Relics", "abstract": "This paper introduces Indiana Jones, an innovative approach to jailbreaking\nLarge Language Models (LLMs) by leveraging inter-model dialogues and\nkeyword-driven prompts. Through orchestrating interactions among three\nspecialised LLMs, the method achieves near-perfect success rates in bypassing\ncontent safeguards in both white-box and black-box LLMs. The research exposes\nsystemic vulnerabilities within contemporary models, particularly their\nsusceptibility to producing harmful or unethical outputs when guided by\nostensibly innocuous prompts framed in historical or contextual contexts.\nExperimental evaluations highlight the efficacy and adaptability of Indiana\nJones, demonstrating its superiority over existing jailbreak methods. These\nfindings emphasise the urgent need for enhanced ethical safeguards and robust\nsecurity measures in the development of LLMs. Moreover, this work provides a\ncritical foundation for future studies aimed at fortifying LLMs against\nadversarial exploitation while preserving their utility and flexibility.", "published": "2025-01-27 14:12:07", "link": "http://arxiv.org/abs/2501.18628v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.CR"}
{"title": "PhysBench: Benchmarking and Enhancing Vision-Language Models for\n  Physical World Understanding", "abstract": "Understanding the physical world is a fundamental challenge in embodied AI,\ncritical for enabling agents to perform complex tasks and operate safely in\nreal-world environments. While Vision-Language Models (VLMs) have shown great\npromise in reasoning and task planning for embodied agents, their ability to\ncomprehend physical phenomena remains extremely limited. To close this gap, we\nintroduce PhysBench, a comprehensive benchmark designed to evaluate VLMs'\nphysical world understanding capability across a diverse set of tasks.\nPhysBench contains 10,002 entries of interleaved video-image-text data,\ncategorized into four major domains: physical object properties, physical\nobject relationships, physical scene understanding, and physics-based dynamics,\nfurther divided into 19 subclasses and 8 distinct capability dimensions. Our\nextensive experiments, conducted on 75 representative VLMs, reveal that while\nthese models excel in common-sense reasoning, they struggle with understanding\nthe physical world -- likely due to the absence of physical knowledge in their\ntraining data and the lack of embedded physical priors. To tackle the\nshortfall, we introduce PhysAgent, a novel framework that combines the\ngeneralization strengths of VLMs with the specialized expertise of vision\nmodels, significantly enhancing VLMs' physical understanding across a variety\nof tasks, including an 18.4\\% improvement on GPT-4o. Furthermore, our results\ndemonstrate that enhancing VLMs' physical world understanding capabilities can\nhelp embodied agents such as MOKA. We believe that PhysBench and PhysAgent\noffer valuable insights and contribute to bridging the gap between VLMs and\nphysical world understanding.", "published": "2025-01-27 18:59:58", "link": "http://arxiv.org/abs/2501.16411v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Smoothed Embeddings for Robust Language Models", "abstract": "Improving the safety and reliability of large language models (LLMs) is a\ncrucial aspect of realizing trustworthy AI systems. Although alignment methods\naim to suppress harmful content generation, LLMs are often still vulnerable to\njailbreaking attacks that employ adversarial inputs that subvert alignment and\ninduce harmful outputs. We propose the Randomized Embedding Smoothing and Token\nAggregation (RESTA) defense, which adds random noise to the embedding vectors\nand performs aggregation during the generation of each output token, with the\naim of better preserving semantic information. Our experiments demonstrate that\nour approach achieves superior robustness versus utility tradeoffs compared to\nthe baseline defenses.", "published": "2025-01-27 20:57:26", "link": "http://arxiv.org/abs/2501.16497v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "stat.ML", "68T07 (Primary), 68T50 (Secondary)"], "primary_category": "cs.LG"}
{"title": "Kernels of Selfhood: GPT-4o shows humanlike patterns of cognitive\n  consistency moderated by free choice", "abstract": "Large Language Models (LLMs) show emergent patterns that mimic human\ncognition. We explore whether they also mirror other, less deliberative human\npsychological processes. Drawing upon classical theories of cognitive\nconsistency, two preregistered studies tested whether GPT-4o changed its\nattitudes toward Vladimir Putin in the direction of a positive or negative\nessay it wrote about the Russian leader. Indeed, GPT displayed patterns of\nattitude change mimicking cognitive consistency effects in humans. Even more\nremarkably, the degree of change increased sharply when the LLM was offered an\nillusion of choice about which essay (positive or negative) to write. This\nresult suggests that GPT-4o manifests a functional analog of humanlike\nselfhood, although how faithfully the chatbot's behavior reflects the\nmechanisms of human attitude change remains to be understood.", "published": "2025-01-27 02:25:12", "link": "http://arxiv.org/abs/2502.07088v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG", "68T50 (Primary), 91E10, 68T30, 68T99, 91C99 (Secondary)", "I.2.7; I.2.0; J.4; K.4.0; H.1.2; K.4.1"], "primary_category": "cs.CY"}
{"title": "Noise disturbance and lack of privacy: Modeling acoustic dissatisfaction\n  in open-plan offices", "abstract": "Open-plan offices are well-known to be adversely affected by acoustic issues.\nThis study aims to model acoustic dissatisfaction using measurements of room\nacoustics, sound environment during occupancy, and occupant surveys (n = 349)\nin 28 offices representing a diverse range of workplace parameters. As latent\nfactors, the contribution of $\\textit{lack of privacy}$ (LackPriv) was 25%\nhigher than $\\textit{noise disturbance}$ (NseDstrb) in predicting\n$\\textit{acoustic dissatisfaction}$ (AcDsat). Room acoustic metrics based on\nsound pressure level (SPL) decay of speech ($L_{\\text{p,A,s,4m}}$ and\n$r_{\\text{C}}$) were better in predicting these factors than distraction\ndistance ($r_{\\text{D}}$) based on speech transmission index. This contradicts\nprevious findings, and the trends for SPL-based metrics in predicting AcDsat\nand LackPriv go against expectations based on ISO 3382-3. For sound during\noccupation, $L_{\\text{A,90}}$ and psychoacoustic loudness ($N_{\\text{90}}$)\npredicted AcDsat, and a SPL fluctuation metric ($M_{\\text{A,eq}}$) predicted\nLackPriv. However, these metrics were weaker predictors than ISO 3382-3\nmetrics. Medium-sized offices exhibited higher dissatisfaction than larger\n($\\geq$50 occupants) offices. Dissatisfaction varied substantially across\nparameters including ceiling heights, number of workstations, and years of\nwork, but not between offices with fixed seating compared to more flexible and\nactivity-based working configurations. Overall, these findings highlight the\ncomplexities in characterizing occupants' perceptions using instrumental\nacoustic measurements.", "published": "2025-01-27 03:10:07", "link": "http://arxiv.org/abs/2501.15744v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "EDSep: An Effective Diffusion-Based Method for Speech Source Separation", "abstract": "Generative models have attracted considerable attention for speech separation\ntasks, and among these, diffusion-based methods are being explored. Despite the\nnotable success of diffusion techniques in generation tasks, their adaptation\nto speech separation has encountered challenges, notably slow convergence and\nsuboptimal separation outcomes. To address these issues and enhance the\nefficacy of diffusion-based speech separation, we introduce EDSep, a novel\nsingle-channel method grounded in score matching via stochastic differential\nequation (SDE). This method enhances generative modeling for speech source\nseparation by optimizing training and sampling efficiency. Specifically, a\nnovel denoiser function is proposed to approximate data distributions, which\nobtains ideal denoiser outputs. Additionally, a stochastic sampler is carefully\ndesigned to resolve the reverse SDE during the sampling process, gradually\nseparating speech from mixtures. Extensive experiments on databases such as\nWSJ0-2mix, LRS2-2mix, and VoxCeleb2-2mix demonstrate our proposed method's\nsuperior performance over existing diffusion and discriminative models,\nvalidating its efficacy.", "published": "2025-01-27 11:17:00", "link": "http://arxiv.org/abs/2501.15965v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Introducing RIFT: A Hierarchical Entropic Filtering Scheme for Ideal\n  Time-Frequency Reconstruction", "abstract": "In this paper, we introduce the Reconstructive Ideal Fractional Transform\n(RIFT), an entropy-based probabilistic filtering algorithm formulated to\nreconstruct the Ideal Time-Frequency Representation (ITFR). RIFT surpasses the\nlimitations imposed by the Gabor uncertainty principle for linear transforms,\nachieving the bilinear transform accuracy present in the Wigner-Ville\nDistribution (WVD) while effectively suppressing cross-terms. The algorithm\nutilises a hierarchical fractional wavelet-based scheme to account for\nlocalised time-frequency trajectory curvature. This scheme is optimised through\nan entropic-based filtering method that probabilistically extracts auto-terms\nwhile retaining the resolution of the WVD. This is achieved by employing a\nspatially varying, positively constrained deconvolution algorithm\n(Lucy-Richardson) with regularisation for adequate noise suppression.\nAdditionally, the optimisation yields an Instantaneous Phase Direction field,\nwhich allows the localised curvature in speech or music extracts to be\nvisualised and utilised within a Kalman tracking scheme, enabling the\nextraction of signal component trajectories. Evaluation demonstrates the\nalgorithm's ability to eradicate cross-terms effectively and achieve superior\ntime-frequency precision. This advance holds significant potential for a wide\nrange of applications requiring high-resolution cross-term-free time-frequency\nanalysis.", "published": "2025-01-27 04:16:46", "link": "http://arxiv.org/abs/2501.15764v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Separate This, and All of these Things Around It: Music Source\n  Separation via Hyperellipsoidal Queries", "abstract": "Music source separation is an audio-to-audio retrieval task of extracting one\nor more constituent components, or composites thereof, from a musical audio\nmixture. Each of these constituent components is often referred to as a \"stem\"\nin literature. Historically, music source separation has been dominated by a\nstem-based paradigm, leading to most state-of-the-art systems being either a\ncollection of single-stem extraction models, or a tightly coupled system with a\nfixed, difficult-to-modify, set of supported stems. Combined with the limited\ndata availability, advances in music source separation have thus been mostly\nlimited to the \"VDBO\" set of stems: \\textit{vocals}, \\textit{drum},\n\\textit{bass}, and the catch-all \\textit{others}. Recent work in music source\nseparation has begun to challenge the fixed-stem paradigm, moving towards\nmodels able to extract any musical sound as long as this target type of sound\ncould be specified to the model as an additional query input. We generalize\nthis idea to a \\textit{query-by-region} source separation system, specifying\nthe target based on the query regardless of how many sound sources or which\nsound classes are contained within it. To do so, we propose the use of\nhyperellipsoidal regions as queries to allow for an intuitive yet easily\nparametrizable approach to specifying both the target (location) as well as its\nspread. Evaluation of the proposed system on the MoisesDB dataset demonstrated\nstate-of-the-art performance of the proposed system both in terms of\nsignal-to-noise ratios and retrieval metrics.", "published": "2025-01-27 16:13:50", "link": "http://arxiv.org/abs/2501.16171v1", "categories": ["eess.AS", "cs.IR", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "UniPET-SPK: A Unified Framework for Parameter-Efficient Tuning of\n  Pre-trained Speech Models for Robust Speaker Verification", "abstract": "With excellent generalization ability, SSL speech models have shown\nimpressive performance on various downstream tasks in the pre-training and\nfine-tuning paradigm. However, as the size of pre-trained models grows,\nfine-tuning becomes practically unfeasible due to expanding computation and\nstorage requirements and the risk of overfitting. This study explores\nparameter-efficient tuning (PET) methods for adapting large-scale pre-trained\nSSL speech models to speaker verification task. Correspondingly, we propose\nthree PET methods: (i)an adapter-tuning method, (ii)a prompt-tuning method, and\n(iii)a unified framework that effectively incorporates adapter-tuning and\nprompt-tuning with a dynamically learnable gating mechanism. First, we propose\nthe Inner+Inter Adapter framework, which inserts two types of adapters into\npre-trained models, allowing for adaptation of latent features within the\nintermediate Transformer layers and output embeddings from all Transformer\nlayers, through a parallel adapter design. Second, we propose the Deep Speaker\nPrompting method that concatenates trainable prompt tokens into the input space\nof pre-trained models to guide adaptation. Lastly, we propose the UniPET-SPK, a\nunified framework that effectively incorporates these two alternate PET methods\ninto a single framework with a dynamic trainable gating mechanism. The proposed\nUniPET-SPK learns to find the optimal mixture of PET methods to match different\ndatasets and scenarios. We conduct a comprehensive set of experiments on\nseveral datasets to validate the effectiveness of the proposed PET methods.\nExperimental results on VoxCeleb, CN-Celeb, and 1st 48-UTD forensic datasets\ndemonstrate that the proposed UniPET-SPK consistently outperforms the two PET\nmethods, fine-tuning, and other parameter-efficient tuning methods, achieving\nsuperior performance while updating only 5.4% of the parameters.", "published": "2025-01-27 22:26:37", "link": "http://arxiv.org/abs/2501.16542v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "6KSFx Synth Dataset", "abstract": "Procedural audio, often referred to as \"digital Foley\", generates sound from\nscratch using computational processes. It represents an innovative approach to\nsound-effects creation. However, the development and adoption of procedural\naudio has been constrained by a lack of publicly available datasets and models,\nwhich hinders evaluation and optimization. To address this important gap, this\npaper presents a dataset of 6000 synthetic audio samples specifically designed\nto advance research and development in sound synthesis within 30 sound\ncategories. By offering a description of the diverse synthesis methods used in\neach sound category and supporting the creation of robust evaluation\nframeworks, this dataset not only highlights the potential of procedural audio,\nbut also provides a resource for researchers, audio developers, and sound\ndesigners. This contribution can accelerate the progress of procedural audio,\nopening up new possibilities in digital sound design.", "published": "2025-01-27 20:22:54", "link": "http://arxiv.org/abs/2501.17198v1", "categories": ["cs.SD", "eess.AS", "physics.data-an"], "primary_category": "cs.SD"}
{"title": "SIM: Surface-based fMRI Analysis for Inter-Subject Multimodal Decoding\n  from Movie-Watching Experiments", "abstract": "Current AI frameworks for brain decoding and encoding, typically train and\ntest models within the same datasets. This limits their utility for brain\ncomputer interfaces (BCI) or neurofeedback, for which it would be useful to\npool experiences across individuals to better simulate stimuli not sampled\nduring training. A key obstacle to model generalisation is the degree of\nvariability of inter-subject cortical organisation, which makes it difficult to\nalign or compare cortical signals across participants. In this paper we address\nthis through the use of surface vision transformers, which build a\ngeneralisable model of cortical functional dynamics, through encoding the\ntopography of cortical networks and their interactions as a moving image across\na surface. This is then combined with tri-modal self-supervised contrastive\n(CLIP) alignment of audio, video, and fMRI modalities to enable the retrieval\nof visual and auditory stimuli from patterns of cortical activity (and\nvice-versa). We validate our approach on 7T task-fMRI data from 174 healthy\nparticipants engaged in the movie-watching experiment from the Human Connectome\nProject (HCP). Results show that it is possible to detect which movie clips an\nindividual is watching purely from their brain activity, even for individuals\nand movies not seen during training. Further analysis of attention maps reveals\nthat our model captures individual patterns of brain activity that reflect\nsemantic and visual systems. This opens the door to future personalised\nsimulations of brain function. Code & pre-trained models will be made available\nat https://github.com/metrics-lab/sim, processed data for training will be\navailable upon request at https://gin.g-node.org/Sdahan30/sim.", "published": "2025-01-27 20:05:17", "link": "http://arxiv.org/abs/2501.16471v1", "categories": ["cs.LG", "cs.AI", "eess.AS", "eess.IV", "q-bio.NC"], "primary_category": "cs.LG"}
