{"title": "Enhanced Momentum with Momentum Transformers", "abstract": "The primary objective of this research is to build a Momentum Transformer\nthat is expected to outperform benchmark time-series momentum and\nmean-reversion trading strategies. We extend the ideas introduced in the paper\nTrading with the Momentum Transformer: An Intelligent and Interpretable\nArchitecture to equities as the original paper primarily only builds upon\nfutures and equity indices. Unlike conventional Long Short-Term Memory (LSTM)\nmodels, which operate sequentially and are optimized for processing local\npatterns, an attention mechanism equips our architecture with direct access to\nall prior time steps in the training window. This hybrid design, combining\nattention with an LSTM, enables the model to capture long-term dependencies,\nenhance performance in scenarios accounting for transaction costs, and\nseamlessly adapt to evolving market conditions, such as those witnessed during\nthe Covid Pandemic. We average 4.14% returns which is similar to the original\npapers results. Our Sharpe is lower at an average of 1.12 due to much higher\nvolatility which may be due to stocks being inherently more volatile than\nfutures and indices.", "published": "2024-12-17 04:11:30", "link": "http://arxiv.org/abs/2412.12516v1", "categories": ["q-fin.CP", "cs.LG"], "primary_category": "q-fin.CP"}
{"title": "Volatility-Volume Order Slicing via Statistical Analysis", "abstract": "This paper addresses the challenges faced in large-volume trading, where\nexecuting substantial orders can result in significant market impact and\nslippage. To mitigate these effects, this study proposes a\nvolatility-volume-based order slicing strategy that leverages Exponential\nWeighted Moving Average and Markov Chain Monte Carlo simulations. These methods\nare used to dynamically estimate future trading volumes and price ranges,\nenabling traders to adapt their strategies by segmenting order execution sizes\nbased on these predictions. Results show that the proposed approach improves\ntrade execution efficiency, reduces market impact, and offers a more adaptive\nsolution for volatile market conditions. The findings have practical\nimplications for large-volume trading, providing a foundation for further\nresearch into adaptive execution strategies.", "published": "2024-12-17 02:37:19", "link": "http://arxiv.org/abs/2412.12482v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Pontryagin-Guided Policy Optimization for Merton's Portfolio Problem", "abstract": "We present a Pontryagin-Guided Direct Policy Optimization (PG-DPO) framework\nfor Merton's portfolio problem, unifying modern neural-network-based policy\nparameterization with the adjoint viewpoint from Pontryagin's maximum principle\n(PMP). Instead of approximating the value function (as done in deep BSDE\nmethods), we track a policy-fixed BSDE for the adjoint processes, which allows\neach gradient update to align with continuous-time PMP conditions. This setup\nyields locally optimal consumption and investment policies that are closely\ntied to classical stochastic control. We further incorporate an alignment\npenalty that nudges the learned policy toward Pontryagin-derived solutions,\nenhancing both convergence speed and training stability. Numerical experiments\nconfirm that PG-DPO effectively handles both consumption and investment,\nachieving strong performance and interpretability without requiring large\noffline datasets or model-free reinforcement learning.", "published": "2024-12-17 17:14:56", "link": "http://arxiv.org/abs/2412.13101v4", "categories": ["math.OC", "q-fin.MF"], "primary_category": "math.OC"}
{"title": "AI-Enhanced Factor Analysis for Predicting S&P 500 Stock Dynamics", "abstract": "This project investigates the interplay of technical, market, and statistical\nfactors in predicting stock market performance, with a primary focus on S&P 500\ncompanies. Utilizing a comprehensive dataset spanning multiple years, the\nanalysis constructs advanced financial metrics, such as momentum indicators,\nvolatility measures, and liquidity adjustments. The machine learning framework\nis employed to identify patterns, relationships, and predictive capabilities of\nthese factors. The integration of traditional financial analytics with machine\nlearning enables enhanced predictive accuracy, offering valuable insights into\nmarket behavior and guiding investment strategies. This research highlights the\npotential of combining domain-specific financial expertise with modern\ncomputational tools to address complex market dynamics.", "published": "2024-12-17 01:07:41", "link": "http://arxiv.org/abs/2412.12438v1", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
{"title": "Productivity of Short Term Assets as a Signal of Future Stock Performance", "abstract": "This paper investigates cash productivity as a signal for future stock\nperformance, building on the cash-return framework of Faulkender and Wang\n(2006). Using financial and market data from WRDS, we calculate cash returns as\na proxy for operational efficiency and evaluate a long-only strategy applied to\nNasdaq-listed non-financial firms. Results show limited predictive power across\nthe broader Nasdaq universe but strong performance in a handpicked portfolio,\nwhich achieves significant positive alpha after controlling for the Fama-French\nthree factors. These findings underscore the importance of refined universe\nselection. While promising, the strategy requires further validation, including\nthe incorporation of transaction costs and performance testing across economic\ncycles. Our results suggest that cash productivity, when combined with other\ncomplementary signals and careful universe selection, can be a valuable tool\nfor generating excess returns.", "published": "2024-12-17 20:26:51", "link": "http://arxiv.org/abs/2412.13311v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "An Application of the Ornstein-Uhlenbeck Process to Pairs Trading", "abstract": "We conduct a preliminary analysis of a pairs trading strategy using the\nOrnstein-Uhlenbeck (OU) process to model stock price spreads. We compare this\napproach to a naive pairs trading strategy that uses a rolling window to\ncalculate mean and standard deviation parameters. Our findings suggest that the\nOU model captures signals and trends effectively but underperforms the naive\nmodel on a risk-return basis, likely due to non-stationary pairs and parameter\ntuning limitations.", "published": "2024-12-17 01:42:50", "link": "http://arxiv.org/abs/2412.12458v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "Assessing the Limitations of Large Language Models in Clinical Fact\n  Decomposition", "abstract": "Verifying factual claims is critical for using large language models (LLMs)\nin healthcare. Recent work has proposed fact decomposition, which uses LLMs to\nrewrite source text into concise sentences conveying a single piece of\ninformation, as an approach for fine-grained fact verification. Clinical\ndocumentation poses unique challenges for fact decomposition due to dense\nterminology and diverse note types. To explore these challenges, we present\nFactEHR, a dataset consisting of full document fact decompositions for 2,168\nclinical notes spanning four types from three hospital systems. Our evaluation,\nincluding review by clinicians, highlights significant variability in the\nquality of fact decomposition for four commonly used LLMs, with some LLMs\ngenerating 2.6x more facts per sentence than others. The results underscore the\nneed for better LLM capabilities to support factual verification in clinical\ntext. To facilitate future research in this direction, we plan to release our\ncode at \\url{https://github.com/som-shahlab/factehr}.", "published": "2024-12-17 00:07:05", "link": "http://arxiv.org/abs/2412.12422v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Persona-SQ: A Personalized Suggested Question Generation Framework For\n  Real-world Documents", "abstract": "Suggested questions (SQs) provide an effective initial interface for users to\nengage with their documents in AI-powered reading applications. In practical\nreading sessions, users have diverse backgrounds and reading goals, yet current\nSQ features typically ignore such user information, resulting in homogeneous or\nineffective questions. We introduce a pipeline that generates personalized SQs\nby incorporating reader profiles (professions and reading goals) and\ndemonstrate its utility in two ways: 1) as an improved SQ generation pipeline\nthat produces higher quality and more diverse questions compared to current\nbaselines, and 2) as a data generator to fine-tune extremely small models that\nperform competitively with much larger models on SQ generation. Our approach\ncan not only serve as a drop-in replacement in current SQ systems to\nimmediately improve their performance but also help develop on-device SQ models\nthat can run locally to deliver fast and private SQ experience.", "published": "2024-12-17 01:15:40", "link": "http://arxiv.org/abs/2412.12445v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Boundary of Large Language Models: A Survey", "abstract": "Although large language models (LLMs) store vast amount of knowledge in their\nparameters, they still have limitations in the memorization and utilization of\ncertain knowledge, leading to undesired behaviors such as generating untruthful\nand inaccurate responses. This highlights the critical need to understand the\nknowledge boundary of LLMs, a concept that remains inadequately defined in\nexisting research. In this survey, we propose a comprehensive definition of the\nLLM knowledge boundary and introduce a formalized taxonomy categorizing\nknowledge into four distinct types. Using this foundation, we systematically\nreview the field through three key lenses: the motivation for studying LLM\nknowledge boundaries, methods for identifying these boundaries, and strategies\nfor mitigating the challenges they present. Finally, we discuss open challenges\nand potential research directions in this area. We aim for this survey to offer\nthe community a comprehensive overview, facilitate access to key issues, and\ninspire further advancements in LLM knowledge research.", "published": "2024-12-17 02:14:02", "link": "http://arxiv.org/abs/2412.12472v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLSR: Neuron-Level Safety Realignment of Large Language Models Against\n  Harmful Fine-Tuning", "abstract": "The emergence of finetuning-as-a-service has revealed a new vulnerability in\nlarge language models (LLMs). A mere handful of malicious data uploaded by\nusers can subtly manipulate the finetuning process, resulting in an\nalignment-broken model. Existing methods to counteract fine-tuning attacks\ntypically require substantial computational resources. Even with\nparameter-efficient techniques like LoRA, gradient updates remain essential. To\naddress these challenges, we propose \\textbf{N}euron-\\textbf{L}evel\n\\textbf{S}afety \\textbf{R}ealignment (\\textbf{NLSR}), a training-free framework\nthat restores the safety of LLMs based on the similarity difference of\nsafety-critical neurons before and after fine-tuning. The core of our framework\nis first to construct a safety reference model from an initially aligned model\nto amplify safety-related features in neurons. We then utilize this reference\nmodel to identify safety-critical neurons, which we prepare as patches.\nFinally, we selectively restore only those neurons that exhibit significant\nsimilarity differences by transplanting these prepared patches, thereby\nminimally altering the fine-tuned model. Extensive experiments demonstrate\nsignificant safety enhancements in fine-tuned models across multiple downstream\ntasks, while greatly maintaining task-level accuracy. Our findings suggest\nregions of some safety-critical neurons show noticeable differences after\nfine-tuning, which can be effectively corrected by transplanting neurons from\nthe reference model without requiring additional training. The code will be\navailable at \\url{https://github.com/xinykou/NLSR}", "published": "2024-12-17 02:59:04", "link": "http://arxiv.org/abs/2412.12497v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DocFusion: A Unified Framework for Document Parsing Tasks", "abstract": "Document parsing is essential for analyzing complex document structures and\nextracting fine-grained information, supporting numerous downstream\napplications. However, existing methods often require integrating multiple\nindependent models to handle various parsing tasks, leading to high complexity\nand maintenance overhead. To address this, we propose DocFusion, a lightweight\ngenerative model with only 0.28B parameters. It unifies task representations\nand achieves collaborative training through an improved objective function.\nExperiments reveal and leverage the mutually beneficial interaction among\nrecognition tasks, and integrating recognition data significantly enhances\ndetection performance. The final results demonstrate that DocFusion achieves\nstate-of-the-art (SOTA) performance across four key tasks.", "published": "2024-12-17 03:20:00", "link": "http://arxiv.org/abs/2412.12505v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can You Trust LLM Judgments? Reliability of LLM-as-a-Judge", "abstract": "Large Language Models (LLMs) have become increasingly powerful and\nubiquitous, but their stochastic nature poses challenges to the reliability of\ntheir outputs. While deterministic settings can improve consistency, they do\nnot guarantee reliability, as a single sample from the model's probability\ndistribution can still be misleading. Building upon the concept of\nLLM-as-a-judge, we introduce a novel framework for rigorously evaluating the\nreliability of LLM judgments, leveraging McDonald's omega. We evaluate the\nreliability of LLMs when judging the outputs of other LLMs on standard\nsingle-turn and multi-turn benchmarks, simultaneously investigating the impact\nof temperature on reliability. By analyzing these results, we demonstrate the\nlimitations of fixed randomness and the importance of considering multiple\nsamples, which we show has significant implications for downstream\napplications. Our findings highlight the need for a nuanced understanding of\nLLM reliability and the potential risks associated with over-reliance on\nsingle-shot evaluations. This work provides a crucial step towards building\nmore trustworthy and reliable LLM-based systems and applications.", "published": "2024-12-17 03:37:31", "link": "http://arxiv.org/abs/2412.12509v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When to Speak, When to Abstain: Contrastive Decoding with Abstention", "abstract": "Large Language Models (LLMs) demonstrate exceptional performance across\ndiverse tasks by leveraging pre-trained (i.e., parametric) and external (i.e.,\ncontextual) knowledge. While substantial efforts have been made to enhance the\nutilization of both forms of knowledge, situations in which models lack\nrelevant information remain underexplored. To investigate this challenge, we\nfirst present a controlled testbed featuring four distinct knowledge access\nscenarios, including the aforementioned edge case, revealing that conventional\nLLM usage exhibits insufficient robustness in handling all instances.\nAddressing this limitation, we propose Contrastive Decoding with Abstention\n(CDA), a novel training-free decoding method that allows LLMs to generate\nresponses when relevant knowledge is available and to abstain otherwise. CDA\nestimates the relevance of both knowledge sources for a given input, adaptively\ndeciding which type of information to prioritize and which to exclude. Through\nextensive experiments, we demonstrate that CDA can effectively perform accurate\ngeneration and abstention simultaneously, enhancing reliability and preserving\nuser trust.", "published": "2024-12-17 04:38:08", "link": "http://arxiv.org/abs/2412.12527v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Task-Agnostic Language Model Watermarking via High Entropy Passthrough\n  Layers", "abstract": "In the era of costly pre-training of large language models, ensuring the\nintellectual property rights of model owners, and insuring that said models are\nresponsibly deployed, is becoming increasingly important. To this end, we\npropose model watermarking via passthrough layers, which are added to existing\npre-trained networks and trained using a self-supervised loss such that the\nmodel produces high-entropy output when prompted with a unique private key, and\nacts normally otherwise. Unlike existing model watermarking methods, our method\nis fully task-agnostic, and can be applied to both classification and\nsequence-to-sequence tasks without requiring advanced access to downstream\nfine-tuning datasets. We evaluate the proposed passthrough layers on a wide\nrange of downstream tasks, and show experimentally our watermarking method\nachieves a near-perfect watermark extraction accuracy and false-positive rate\nin most cases without damaging original model performance. Additionally, we\nshow our method is robust to both downstream fine-tuning, fine-pruning, and\nlayer removal attacks, and can be trained in a fraction of the time required to\ntrain the original model. Code is available in the paper.", "published": "2024-12-17 05:46:50", "link": "http://arxiv.org/abs/2412.12563v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with\n  Large Language Models", "abstract": "Aspect-based sentiment analysis (ABSA), a sequence labeling task, has\nattracted increasing attention in multilingual contexts. While previous\nresearch has focused largely on fine-tuning or training models specifically for\nABSA, we evaluate large language models (LLMs) under zero-shot conditions to\nexplore their potential to tackle this challenge with minimal task-specific\nadaptation. We conduct a comprehensive empirical evaluation of a series of LLMs\non multilingual ABSA tasks, investigating various prompting strategies,\nincluding vanilla zero-shot, chain-of-thought (CoT), self-improvement,\nself-debate, and self-consistency, across nine different models. Results\nindicate that while LLMs show promise in handling multilingual ABSA, they\ngenerally fall short of fine-tuned, task-specific models. Notably, simpler\nzero-shot prompts often outperform more complex strategies, especially in\nhigh-resource languages like English. These findings underscore the need for\nfurther refinement of LLM-based approaches to effectively address ABSA task\nacross diverse languages.", "published": "2024-12-17 05:48:48", "link": "http://arxiv.org/abs/2412.12564v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FCMR: Robust Evaluation of Financial Cross-Modal Multi-Hop Reasoning", "abstract": "Real-world decision-making often requires integrating and reasoning over\ninformation from multiple modalities. While recent multimodal large language\nmodels (MLLMs) have shown promise in such tasks, their ability to perform\nmulti-hop reasoning across diverse sources remains insufficiently evaluated.\nExisting benchmarks, such as MMQA, face challenges due to (1) data\ncontamination and (2) a lack of complex queries that necessitate operations\nacross more than two modalities, hindering accurate performance assessment. To\naddress this, we present Financial Cross-Modal Multi-Hop Reasoning (FCMR), a\nbenchmark created to analyze the reasoning capabilities of MLLMs by urging them\nto combine information from textual reports, tables, and charts within the\nfinancial domain. FCMR is categorized into three difficulty levels-Easy,\nMedium, and Hard-facilitating a step-by-step evaluation. In particular,\nproblems at the Hard level require precise cross-modal three-hop reasoning and\nare designed to prevent the disregard of any modality. Experiments on this new\nbenchmark reveal that even state-of-the-art MLLMs struggle, with the\nbest-performing model (Claude 3.5 Sonnet) achieving only 30.4% accuracy on the\nmost challenging tier. We also conduct analysis to provide insights into the\ninner workings of the models, including the discovery of a critical bottleneck\nin the information retrieval phase.", "published": "2024-12-17 05:50:55", "link": "http://arxiv.org/abs/2412.12567v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantifying Lexical Semantic Shift via Unbalanced Optimal Transport", "abstract": "Lexical semantic change detection aims to identify shifts in word meanings\nover time. While existing methods using embeddings from a diachronic corpus\npair estimate the degree of change for target words, they offer limited insight\ninto changes at the level of individual usage instances. To address this, we\napply Unbalanced Optimal Transport (UOT) to sets of contextualized word\nembeddings, capturing semantic change through the excess and deficit in the\nalignment between usage instances. In particular, we propose Sense Usage Shift\n(SUS), a measure that quantifies changes in the usage frequency of a word sense\nat each usage instance. By leveraging SUS, we demonstrate that several\nchallenges in semantic change detection can be addressed in a unified manner,\nincluding quantifying instance-level semantic change and word-level tasks such\nas measuring the magnitude of semantic change and the broadening or narrowing\nof meaning.", "published": "2024-12-17 06:00:54", "link": "http://arxiv.org/abs/2412.12569v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Process-Supervised Reward Models for Verifying Clinical Note Generation:\n  A Scalable Approach Guided by Domain Expertise", "abstract": "Process-supervised reward models (PRMs), which verify large language model\n(LLM) outputs step-by-step, have achieved significant success in mathematical\nand coding problems. However, their application to other domains remains\nlargely unexplored. In this work, we train a PRM to provide step-level reward\nsignals for clinical notes generated by LLMs from patient-doctor dialogues.\nGuided by real-world clinician expertise, we carefully designed step\ndefinitions for clinical notes and utilized Gemini-Pro 1.5 to automatically\ngenerate process supervision data at scale. Our proposed PRM, trained on the\nLLaMA-3.1 8B instruct model, outperformed both Gemini-Pro 1.5 and the vanilla\noutcome-supervised reward model (ORM) in two key evaluations: (1) selecting\ngold-reference samples from error-containing ones, achieving 98.8% accuracy\n(versus 70.0% for the vanilla ORM and 93.8% for Gemini-Pro 1.5), and (2)\nselecting physician-preferred notes, achieving 56.2% accuracy (compared to\n37.5% for the vanilla ORM and 50.0% for Gemini-Pro 1.5). Additionally, we\nconducted ablation studies to determine optimal loss functions and data\nselection strategies, along with physician reader studies to explore predictors\nof downstream Best-of-N performance. Our promising results suggest the\npotential of PRMs to extend beyond the clinical domain, offering a scalable and\neffective solution for diverse generative tasks.", "published": "2024-12-17 06:24:34", "link": "http://arxiv.org/abs/2412.12583v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PerSphere: A Comprehensive Framework for Multi-Faceted Perspective\n  Retrieval and Summarization", "abstract": "As online platforms and recommendation algorithms evolve, people are\nincreasingly trapped in echo chambers, leading to biased understandings of\nvarious issues. To combat this issue, we have introduced PerSphere, a benchmark\ndesigned to facilitate multi-faceted perspective retrieval and summarization,\nthus breaking free from these information silos. For each query within\nPerSphere, there are two opposing claims, each supported by distinct,\nnon-overlapping perspectives drawn from one or more documents. Our goal is to\naccurately summarize these documents, aligning the summaries with the\nrespective claims and their underlying perspectives. This task is structured as\na two-step end-to-end pipeline that includes comprehensive document retrieval\nand multi-faceted summarization. Furthermore, we propose a set of metrics to\nevaluate the comprehensiveness of the retrieval and summarization content.\nExperimental results on various counterparts for the pipeline show that recent\nmodels struggle with such a complex task. Analysis shows that the main\nchallenge lies in long context and perspective extraction, and we propose a\nsimple but effective multi-agent summarization system, offering a promising\nsolution to enhance performance on PerSphere.", "published": "2024-12-17 06:44:06", "link": "http://arxiv.org/abs/2412.12588v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs are Also Effective Embedding Models: An In-depth Overview", "abstract": "Large language models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art performance across various tasks. Recently, their\neffectiveness as embedding models has gained attention, marking a paradigm\nshift from traditional encoder-only models like ELMo and BERT to decoder-only,\nlarge-scale LLMs such as GPT, LLaMA, and Mistral. This survey provides an\nin-depth overview of this transition, beginning with foundational techniques\nbefore the LLM era, followed by LLM-based embedding models through two main\nstrategies to derive embeddings from LLMs. 1) Direct prompting: We mainly\ndiscuss the prompt designs and the underlying rationale for deriving\ncompetitive embeddings. 2) Data-centric tuning: We cover extensive aspects that\naffect tuning an embedding model, including model architecture, training\nobjectives, data constructions, etc. Upon the above, we also cover advanced\nmethods, such as handling longer texts, and multilingual and cross-modal data.\nFurthermore, we discuss factors affecting choices of embedding models, such as\nperformance/efficiency comparisons, dense vs sparse embeddings, pooling\nstrategies, and scaling law. Lastly, the survey highlights the limitations and\nchallenges in adapting LLMs for embeddings, including cross-task embedding\nquality, trade-offs between efficiency and accuracy, low-resource,\nlong-context, data bias, robustness, etc. This survey serves as a valuable\nresource for researchers and practitioners by synthesizing current\nadvancements, highlighting key challenges, and offering a comprehensive\nframework for future work aimed at enhancing the effectiveness and efficiency\nof LLMs as embedding models.", "published": "2024-12-17 06:48:24", "link": "http://arxiv.org/abs/2412.12591v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MultiLingPoT: Enhancing Mathematical Reasoning with Multilingual Program\n  Fine-tuning", "abstract": "Program-of-Thought (PoT), which aims to use programming language instead of\nnatural language as an intermediate step in reasoning, is an important way for\nLLMs to solve mathematical problems. Since different programming languages\nexcel in different areas, it is natural to use the most suitable language for\nsolving specific problems. However, current PoT research only focuses on single\nlanguage PoT, ignoring the differences between different programming languages.\nTherefore, this paper proposes an multilingual program reasoning method,\nMultiLingPoT. This method allows the model to answer questions using multiple\nprogramming languages by fine-tuning on multilingual data. Additionally, prior\nand posterior hybrid methods are used to help the model select the most\nsuitable language for each problem. Our experimental results show that the\ntraining of MultiLingPoT improves each program's mathematical reasoning by\nabout 2.5\\%. Moreover, with proper mixing, the performance of MultiLingPoT can\nbe further improved, achieving a 6\\% increase compared to the single-language\nPoT with the data augmentation.Resources of this paper can be found at\nhttps://github.com/Nianqi-Li/MultiLingPoT.", "published": "2024-12-17 07:14:03", "link": "http://arxiv.org/abs/2412.12609v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Jailbreaking? One Step Is Enough!", "abstract": "Large language models (LLMs) excel in various tasks but remain vulnerable to\njailbreak attacks, where adversaries manipulate prompts to generate harmful\noutputs. Examining jailbreak prompts helps uncover the shortcomings of LLMs.\nHowever, current jailbreak methods and the target model's defenses are engaged\nin an independent and adversarial process, resulting in the need for frequent\nattack iterations and redesigning attacks for different models. To address\nthese gaps, we propose a Reverse Embedded Defense Attack (REDA) mechanism that\ndisguises the attack intention as the \"defense\". intention against harmful\ncontent. Specifically, REDA starts from the target response, guiding the model\nto embed harmful content within its defensive measures, thereby relegating\nharmful content to a secondary role and making the model believe it is\nperforming a defensive task. The attacking model considers that it is guiding\nthe target model to deal with harmful content, while the target model thinks it\nis performing a defensive task, creating an illusion of cooperation between the\ntwo. Additionally, to enhance the model's confidence and guidance in\n\"defensive\" intentions, we adopt in-context learning (ICL) with a small number\nof attack examples and construct a corresponding dataset of attack examples.\nExtensive evaluations demonstrate that the REDA method enables cross-model\nattacks without the need to redesign attack strategies for different models,\nenables successful jailbreak in one iteration, and outperforms existing methods\non both open-source and closed-source models.", "published": "2024-12-17 07:33:41", "link": "http://arxiv.org/abs/2412.12621v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Make Imagination Clearer! Stable Diffusion-based Visual Imagination for\n  Multimodal Machine Translation", "abstract": "Visual information has been introduced for enhancing machine translation\n(MT), and its effectiveness heavily relies on the availability of large amounts\nof bilingual parallel sentence pairs with manual image annotations. In this\npaper, we introduce a stable diffusion-based imagination network into a\nmultimodal large language model (MLLM) to explicitly generate an image for each\nsource sentence, thereby advancing the multimodel MT. Particularly, we build\nheuristic human feedback with reinforcement learning to ensure the consistency\nof the generated image with the source sentence without the supervision of\nimage annotation, which breaks the bottleneck of using visual information in\nMT. Furthermore, the proposed method enables imaginative visual information to\nbe integrated into large-scale text-only MT in addition to multimodal MT.\nExperimental results show that our model significantly outperforms existing\nmultimodal MT and text-only MT, especially achieving an average improvement of\nmore than 14 BLEU points on Multi30K multimodal MT benchmarks.", "published": "2024-12-17 07:41:23", "link": "http://arxiv.org/abs/2412.12627v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-based Discriminative Reasoning for Knowledge Graph Question\n  Answering", "abstract": "Large language models (LLMs) based on generative pre-trained Transformer have\nachieved remarkable performance on knowledge graph question-answering (KGQA)\ntasks. However, LLMs often produce ungrounded subgraph planning or reasoning\nresults in KGQA due to the hallucinatory behavior brought by the generative\nparadigm. To tackle this issue, we propose READS to reformulate the KGQA\nprocess into discriminative subtasks, which simplifies the search space for\neach subtasks. Based on the subtasks, we design a new corresponding\ndiscriminative inference strategy to conduct the reasoning for KGQA, thereby\nalleviating hallucination and ungrounded reasoning issues in LLMs. Experimental\nresults show that the proposed approach outperforms multiple strong comparison\nmethods, along with achieving state-of-the-art performance on widely used\nbenchmarks WebQSP and CWQ.", "published": "2024-12-17 08:07:16", "link": "http://arxiv.org/abs/2412.12643v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "iPrOp: Interactive Prompt Optimization for Large Language Models with a\n  Human in the Loop", "abstract": "Prompt engineering has made significant contributions to the era of large\nlanguage models, yet its effectiveness depends on the skills of a prompt\nauthor. Automatic prompt optimization can support the prompt development\nprocess, but requires annotated data. This paper introduces $\\textit{iPrOp}$, a\nnovel Interactive Prompt Optimization system, to bridge manual prompt\nengineering and automatic prompt optimization. With human intervention in the\noptimization loop, $\\textit{iPrOp}$ offers users the flexibility to assess\nevolving prompts. We present users with prompt variations, selected instances,\nlarge language model predictions accompanied by corresponding explanations, and\nperformance metrics derived from a subset of the training data. This approach\nempowers users to choose and further refine the provided prompts based on their\nindividual preferences and needs. This system not only assists non-technical\ndomain experts in generating optimal prompts tailored to their specific tasks\nor domains, but also enables to study the intrinsic parameters that influence\nthe performance of prompt optimization. Our evaluation shows that our system\nhas the capability to generate improved prompts, leading to enhanced task\nperformance.", "published": "2024-12-17 08:09:15", "link": "http://arxiv.org/abs/2412.12644v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Train More Parameters But Mind Their Placement: Insights into Language\n  Adaptation with PEFT", "abstract": "Smaller LLMs still face significant challenges even in medium-resourced\nlanguages, particularly when it comes to language-specific knowledge -- a\nproblem not easily resolved with machine-translated data. In this case study on\nIcelandic, we aim to enhance the generation performance of an LLM by\nspecialising it using unstructured text corpora. A key focus is on preventing\ninterference with the models' capabilities of handling longer context during\nthis adaptation. Through ablation studies using various parameter-efficient\nfine-tuning (PEFT) methods and setups, we find that increasing the number of\ntrainable parameters leads to better and more robust language adaptation. LoRAs\nplaced in the feed-forward layers and bottleneck adapters show promising\nresults with sufficient parameters, while prefix tuning and (IA)3 are not\nsuitable. Although improvements are consistent in 0-shot summarisation, some\nadapted models struggle with longer context lengths, an issue that can be\nmitigated by adapting only the final layers.", "published": "2024-12-17 08:44:00", "link": "http://arxiv.org/abs/2412.12674v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Document-level Paraphrased Machine Generated Content:\n  Mimicking Human Writing Style and Involving Discourse Features", "abstract": "The availability of high-quality APIs for Large Language Models (LLMs) has\nfacilitated the widespread creation of Machine-Generated Content (MGC), posing\nchallenges such as academic plagiarism and the spread of misinformation.\nExisting MGC detectors often focus solely on surface-level information,\noverlooking implicit and structural features. This makes them susceptible to\ndeception by surface-level sentence patterns, particularly for longer texts and\nin texts that have been subsequently paraphrased.\n  To overcome these challenges, we introduce novel methodologies and datasets.\nBesides the publicly available dataset Plagbench, we developed the paraphrased\nLong-Form Question and Answer (paraLFQA) and paraphrased Writing Prompts\n(paraWP) datasets using GPT and DIPPER, a discourse paraphrasing tool, by\nextending artifacts from their original versions. To address the challenge of\ndetecting highly similar paraphrased texts, we propose MhBART, an\nencoder-decoder model designed to emulate human writing style while\nincorporating a novel difference score mechanism. This model outperforms strong\nclassifier baselines and identifies deceptive sentence patterns. To better\ncapture the structure of longer texts at document level, we propose\nDTransformer, a model that integrates discourse analysis through PDTB\npreprocessing to encode structural features. It results in substantial\nperformance gains across both datasets -- 15.5\\% absolute improvement on\nparaLFQA, 4\\% absolute improvement on paraWP, and 1.5\\% absolute improvement on\nM4 compared to SOTA approaches.", "published": "2024-12-17 08:47:41", "link": "http://arxiv.org/abs/2412.12679v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XTransplant: A Probe into the Upper Bound Performance of Multilingual\n  Capability and Culture Adaptability in LLMs via Mutual Cross-lingual\n  Feed-forward Transplantation", "abstract": "Current large language models (LLMs) often exhibit imbalances in multilingual\ncapabilities and cultural adaptability, largely due to their English-centric\npretraining data. To address this imbalance, we propose a probing method named\nXTransplant that explores cross-lingual latent interactions via cross-lingual\nfeed-forward transplantation during inference stage, with the hope of enabling\nthe model to leverage the strengths of both English and non-English languages.\nThrough extensive pilot experiments, we empirically prove that both the\nmultilingual capabilities and cultural adaptability of LLMs hold the potential\nto be significantly improved by XTransplant, respectively from En -> non-En and\nnon-En -> En, highlighting the underutilization of current LLMs' multilingual\npotential. And the patterns observed in these pilot experiments further\nmotivate an offline scaling inference strategy, which demonstrates consistent\nperformance improvements in multilingual and culture-aware tasks, sometimes\neven surpassing multilingual supervised fine-tuning. And we do hope our further\nanalysis and discussion could help gain deeper insights into XTransplant\nmechanism.", "published": "2024-12-17 09:05:30", "link": "http://arxiv.org/abs/2412.12686v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Trigger$^3$: Refining Query Correction via Adaptive Model Selector", "abstract": "In search scenarios, user experience can be hindered by erroneous queries due\nto typos, voice errors, or knowledge gaps. Therefore, query correction is\ncrucial for search engines. Current correction models, usually small models\ntrained on specific data, often struggle with queries beyond their training\nscope or those requiring contextual understanding. While the advent of Large\nLanguage Models (LLMs) offers a potential solution, they are still limited by\ntheir pre-training data and inference cost, particularly for complex queries,\nmaking them not always effective for query correction. To tackle these, we\npropose Trigger$^3$, a large-small model collaboration framework that\nintegrates the traditional correction model and LLM for query correction,\ncapable of adaptively choosing the appropriate correction method based on the\nquery and the correction results from the traditional correction model and LLM.\nTrigger$^3$ first employs a correction trigger to filter out correct queries.\nIncorrect queries are then corrected by the traditional correction model. If\nthis fails, an LLM trigger is activated to call the LLM for correction.\nFinally, for queries that no model can correct, a fallback trigger decides to\nreturn the original query. Extensive experiments demonstrate Trigger$^3$\noutperforms correction baselines while maintaining efficiency.", "published": "2024-12-17 09:16:54", "link": "http://arxiv.org/abs/2412.12701v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression", "abstract": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV.", "published": "2024-12-17 09:20:31", "link": "http://arxiv.org/abs/2412.12706v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Naturalness in LLM-Generated Utterances through Disfluency\n  Insertion", "abstract": "Disfluencies are a natural feature of spontaneous human speech but are\ntypically absent from the outputs of Large Language Models (LLMs). This absence\ncan diminish the perceived naturalness of synthesized speech, which is an\nimportant criteria when building conversational agents that aim to mimick human\nbehaviours. We show how the insertion of disfluencies can alleviate this\nshortcoming. The proposed approach involves (1) fine-tuning an LLM with\nLow-Rank Adaptation (LoRA) to incorporate various types of disfluencies into\nLLM-generated utterances and (2) synthesizing those utterances using a\ntext-to-speech model that supports the generation of speech phenomena such as\ndisfluencies. We evaluated the quality of the generated speech across two\nmetrics: intelligibility and perceived spontaneity. We demonstrate through a\nuser study that the insertion of disfluencies significantly increase the\nperceived spontaneity of the generated speech. This increase came, however,\nalong with a slight reduction in intelligibility.", "published": "2024-12-17 09:25:44", "link": "http://arxiv.org/abs/2412.12710v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EventFull: Complete and Consistent Event Relation Annotation", "abstract": "Event relation detection is a fundamental NLP task, leveraged in many\ndownstream applications, whose modeling requires datasets annotated with event\nrelations of various types. However, systematic and complete annotation of\nthese relations is costly and challenging, due to the quadratic number of event\npairs that need to be considered. Consequently, many current event relation\ndatasets lack systematicity and completeness. In response, we introduce\n\\textit{EventFull}, the first tool that supports consistent, complete and\nefficient annotation of temporal, causal and coreference relations via a\nunified and synergetic process. A pilot study demonstrates that EventFull\naccelerates and simplifies the annotation process while yielding high\ninter-annotator agreement.", "published": "2024-12-17 09:55:41", "link": "http://arxiv.org/abs/2412.12733v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is it the end of (generative) linguistics as we know it?", "abstract": "A significant debate has emerged in response to a paper written by Steven\nPiantadosi (Piantadosi, 2023) and uploaded to the LingBuzz platform, the open\narchive for generative linguistics. Piantadosi's dismissal of Chomsky's\napproach is ruthless, but generative linguists deserve it. In this paper, I\nwill adopt three idealized perspectives -- computational, theoretical, and\nexperimental -- to focus on two fundamental issues that lend partial support to\nPiantadosi's critique: (a) the evidence challenging the Poverty of Stimulus\n(PoS) hypothesis and (b) the notion of simplicity as conceived within\nmainstream Minimalism. In conclusion, I argue that, to reclaim a central role\nin language studies, generative linguistics -- representing a prototypical\ntheoretical perspective on language -- needs a serious update leading to (i)\nmore precise, consistent, and complete formalizations of foundational\nintuitions and (ii) the establishment and utilization of a standardized dataset\nof crucial empirical evidence to evaluate the theory's adequacy. On the other\nhand, ignoring the formal perspective leads to major drawbacks in both\ncomputational and experimental approaches. Neither descriptive nor explanatory\nadequacy can be easily achieved without the precise formulation of general\nprinciples that can be challenged empirically.", "published": "2024-12-17 11:00:34", "link": "http://arxiv.org/abs/2412.12797v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Preference-Oriented Supervised Fine-Tuning: Favoring Target Model Over\n  Aligned Large Language Models", "abstract": "Alignment, endowing a pre-trained Large language model (LLM) with the ability\nto follow instructions, is crucial for its real-world applications.\nConventional supervised fine-tuning (SFT) methods formalize it as causal\nlanguage modeling typically with a cross-entropy objective, requiring a large\namount of high-quality instruction-response pairs. However, the quality of\nwidely used SFT datasets can not be guaranteed due to the high cost and\nintensive labor for the creation and maintenance in practice. To overcome the\nlimitations associated with the quality of SFT datasets, we introduce a novel\n\\textbf{p}reference-\\textbf{o}riented supervised \\textbf{f}ine-\\textbf{t}uning\napproach, namely PoFT. The intuition is to boost SFT by imposing a particular\npreference: \\textit{favoring the target model over aligned LLMs on the same SFT\ndata.} This preference encourages the target model to predict a higher\nlikelihood than that predicted by the aligned LLMs, incorporating assessment\ninformation on data quality (i.e., predicted likelihood by the aligned LLMs)\ninto the training process. Extensive experiments are conducted, and the results\nvalidate the effectiveness of the proposed method. PoFT achieves stable and\nconsistent improvements over the SFT baselines across different training\ndatasets and base models. Moreover, we prove that PoFT can be integrated with\nexisting SFT data filtering methods to achieve better performance, and further\nimproved by following preference optimization procedures, such as DPO.", "published": "2024-12-17 12:49:14", "link": "http://arxiv.org/abs/2412.12865v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question: How do Large Language Models perform on the Question Answering\n  tasks? Answer:", "abstract": "Large Language Models (LLMs) have been showing promising results for various\nNLP-tasks without the explicit need to be trained for these tasks by using\nfew-shot or zero-shot prompting techniques. A common NLP-task is\nquestion-answering (QA). In this study, we propose a comprehensive performance\ncomparison between smaller fine-tuned models and out-of-the-box\ninstruction-following LLMs on the Stanford Question Answering Dataset 2.0\n(SQuAD2), specifically when using a single-inference prompting technique. Since\nthe dataset contains unanswerable questions, previous work used a double\ninference method. We propose a prompting style which aims to elicit the same\nability without the need for double inference, saving compute time and\nresources. Furthermore, we investigate their generalization capabilities by\ncomparing their performance on similar but different QA datasets, without\nfine-tuning neither model, emulating real-world uses where the context and\nquestions asked may differ from the original training distribution, for example\nswapping Wikipedia for news articles.\n  Our results show that smaller, fine-tuned models outperform current\nState-Of-The-Art (SOTA) LLMs on the fine-tuned task, but recent SOTA models are\nable to close this gap on the out-of-distribution test and even outperform the\nfine-tuned models on 3 of the 5 tested QA datasets.", "published": "2024-12-17 13:19:38", "link": "http://arxiv.org/abs/2412.12893v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Truthful Text Sanitization Guided by Inference Attacks", "abstract": "The purpose of text sanitization is to rewrite those text spans in a document\nthat may directly or indirectly identify an individual, to ensure they no\nlonger disclose personal information. Text sanitization must strike a balance\nbetween preventing the leakage of personal information (privacy protection)\nwhile also retaining as much of the document's original content as possible\n(utility preservation). We present an automated text sanitization strategy\nbased on generalizations, which are more abstract (but still informative) terms\nthat subsume the semantic content of the original text spans. The approach\nrelies on instruction-tuned large language models (LLMs) and is divided into\ntwo stages. The LLM is first applied to obtain truth-preserving replacement\ncandidates and rank them according to their abstraction level. Those candidates\nare then evaluated for their ability to protect privacy by conducting inference\nattacks with the LLM. Finally, the system selects the most informative\nreplacement shown to be resistant to those attacks. As a consequence of this\ntwo-stage process, the chosen replacements effectively balance utility and\nprivacy. We also present novel metrics to automatically evaluate these two\naspects without the need to manually annotate data. Empirical results on the\nText Anonymization Benchmark show that the proposed approach leads to enhanced\nutility, with only a marginal increase in the risk of re-identifying protected\nindividuals compared to fully suppressing the original information.\nFurthermore, the selected replacements are shown to be more truth-preserving\nand abstractive than previous methods.", "published": "2024-12-17 14:07:01", "link": "http://arxiv.org/abs/2412.12928v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Fine-grained Visual Understanding in VLMs through Text-Only\n  Training", "abstract": "Visual-Language Models (VLMs) have become a powerful tool for bridging the\ngap between visual and linguistic understanding. However, the conventional\nlearning approaches for VLMs often suffer from limitations, such as the high\nresource requirements of collecting and training image-text paired data. Recent\nresearch has suggested that language understanding plays a crucial role in the\nperformance of VLMs, potentially indicating that text-only training could be a\nviable approach. In this work, we investigate the feasibility of enhancing\nfine-grained visual understanding in VLMs through text-only training. Inspired\nby how humans develop visual concept understanding, where rich textual\ndescriptions can guide visual recognition, we hypothesize that VLMs can also\nbenefit from leveraging text-based representations to improve their visual\nrecognition abilities. We conduct comprehensive experiments on two distinct\ndomains: fine-grained species classification and cultural visual understanding\ntasks. Our findings demonstrate that text-only training can be comparable to\nconventional image-text training while significantly reducing computational\ncosts. This suggests a more efficient and cost-effective pathway for advancing\nVLM capabilities, particularly valuable in resource-constrained environments.", "published": "2024-12-17 14:18:50", "link": "http://arxiv.org/abs/2412.12940v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MOPO: Multi-Objective Prompt Optimization for Affective Text Generation", "abstract": "How emotions are expressed depends on the context and domain. On X (formerly\nTwitter), for instance, an author might simply use the hashtag #anger, while in\na news headline, emotions are typically written in a more polite, indirect\nmanner. To enable conditional text generation models to create emotionally\nconnotated texts that fit a domain, users need to have access to a parameter\nthat allows them to choose the appropriate way to express an emotion. To\nachieve this, we introduce MOPO, a Multi-Objective Prompt Optimization\nmethodology. MOPO optimizes prompts according to multiple objectives (which\ncorrespond here to the output probabilities assigned by emotion classifiers\ntrained for different domains). In contrast to single objective optimization,\nMOPO outputs a set of prompts, each with a different weighting of the multiple\nobjectives. Users can then choose the most appropriate prompt for their\ncontext. We evaluate MOPO using three objectives, determined by various\ndomain-specific emotion classifiers. MOPO improves performance by up to 15 pp\nacross all objectives with a minimal loss (1-2 pp) for any single objective\ncompared to single-objective optimization. These minor performance losses are\noffset by a broader generalization across multiple objectives - which is not\npossible with single-objective optimization. Additionally, MOPO reduces\ncomputational requirements by simultaneously optimizing for multiple\nobjectives, eliminating separate optimization procedures for each objective.", "published": "2024-12-17 14:28:14", "link": "http://arxiv.org/abs/2412.12948v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recipient Profiling: Predicting Characteristics from Messages", "abstract": "It has been shown in the field of Author Profiling that texts may\ninadvertently reveal sensitive information about their authors, such as gender\nor age. This raises important privacy concerns that have been extensively\naddressed in the literature, in particular with the development of methods to\nhide such information. We argue that, when these texts are in fact messages\nexchanged between individuals, this is not the end of the story. Indeed, in\nthis case, a second party, the intended recipient, is also involved and should\nbe considered. In this work, we investigate the potential privacy leaks\naffecting them, that is we propose and address the problem of Recipient\nProfiling. We provide empirical evidence that such a task is feasible on\nseveral publicly accessible datasets\n(https://huggingface.co/datasets/sileod/recipient_profiling). Furthermore, we\nshow that the learned models can be transferred to other datasets, albeit with\na loss in accuracy.", "published": "2024-12-17 14:35:33", "link": "http://arxiv.org/abs/2412.12954v1", "categories": ["cs.CL", "68T50, 68P20, 94A60", "I.2.7; K.4.1; H.3.3"], "primary_category": "cs.CL"}
{"title": "Learning from Noisy Labels via Self-Taught On-the-Fly Meta Loss\n  Rescaling", "abstract": "Correct labels are indispensable for training effective machine learning\nmodels. However, creating high-quality labels is expensive, and even\nprofessionally labeled data contains errors and ambiguities. Filtering and\ndenoising can be applied to curate labeled data prior to training, at the cost\nof additional processing and loss of information. An alternative is on-the-fly\nsample reweighting during the training process to decrease the negative impact\nof incorrect or ambiguous labels, but this typically requires clean seed data.\nIn this work we propose unsupervised on-the-fly meta loss rescaling to reweight\ntraining samples. Crucially, we rely only on features provided by the model\nbeing trained, to learn a rescaling function in real time without knowledge of\nthe true clean data distribution. We achieve this via a novel meta learning\nsetup that samples validation data for the meta update directly from the noisy\ntraining corpus by employing the rescaling function being trained. Our proposed\nmethod consistently improves performance across various NLP tasks with minimal\ncomputational overhead. Further, we are among the first to attempt on-the-fly\ntraining data reweighting on the challenging task of dialogue modeling, where\nnoisy and ambiguous labels are common. Our strategy is robust in the face of\nnoisy and clean data, handles class imbalance, and prevents overfitting to\nnoisy labels. Our self-taught loss rescaling improves as the model trains,\nshowing the ability to keep learning from the model's own signals. As training\nprogresses, the impact of correctly labeled data is scaled up, while the impact\nof wrongly labeled data is suppressed.", "published": "2024-12-17 14:37:50", "link": "http://arxiv.org/abs/2412.12955v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SnakModel: Lessons Learned from Training an Open Danish Large Language\n  Model", "abstract": "We present SnakModel, a Danish large language model (LLM) based on Llama2-7B,\nwhich we continuously pre-train on 13.6B Danish words, and further tune on 3.7M\nDanish instructions. As best practices for creating LLMs for smaller language\ncommunities have yet to be established, we examine the effects of early\nmodeling and training decisions on downstream performance throughout the entire\ntraining pipeline, including (1) the creation of a strictly curated corpus of\nDanish text from diverse sources; (2) the language modeling and\ninstruction-tuning training process itself, including the analysis of\nintermediate training dynamics, and ablations across different hyperparameters;\n(3) an evaluation on eight language and culturally-specific tasks. Across these\nexperiments SnakModel achieves the highest overall performance, outperforming\nmultiple contemporary Llama2-7B-based models. By making SnakModel, the majority\nof our pre-training corpus, and the associated code available under open\nlicenses, we hope to foster further research and development in Danish Natural\nLanguage Processing, and establish training guidelines for languages with\nsimilar resource constraints.", "published": "2024-12-17 14:38:21", "link": "http://arxiv.org/abs/2412.12956v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptations of AI models for querying the LandMatrix database in natural\n  language", "abstract": "The Land Matrix initiative (https://landmatrix.org) and its global\nobservatory aim to provide reliable data on large-scale land acquisitions to\ninform debates and actions in sectors such as agriculture, extraction, or\nenergy in low- and middle-income countries. Although these data are recognized\nin the academic world, they remain underutilized in public policy, mainly due\nto the complexity of access and exploitation, which requires technical\nexpertise and a good understanding of the database schema.\n  The objective of this work is to simplify access to data from different\ndatabase systems. The methods proposed in this article are evaluated using data\nfrom the Land Matrix. This work presents various comparisons of Large Language\nModels (LLMs) as well as combinations of LLM adaptations (Prompt Engineering,\nRAG, Agents) to query different database systems (GraphQL and REST queries).\nThe experiments are reproducible, and a demonstration is available online:\nhttps://github.com/tetis-nlp/landmatrix-graphql-python.", "published": "2024-12-17 14:44:27", "link": "http://arxiv.org/abs/2412.12961v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlocking LLMs: Addressing Scarce Data and Bias Challenges in Mental\n  Health", "abstract": "Large language models (LLMs) have shown promising capabilities in healthcare\nanalysis but face several challenges like hallucinations, parroting, and bias\nmanifestation. These challenges are exacerbated in complex, sensitive, and\nlow-resource domains. Therefore, in this work we introduce IC-AnnoMI, an\nexpert-annotated motivational interviewing (MI) dataset built upon AnnoMI by\ngenerating in-context conversational dialogues leveraging LLMs, particularly\nChatGPT. IC-AnnoMI employs targeted prompts accurately engineered through cues\nand tailored information, taking into account therapy style (empathy,\nreflection), contextual relevance, and false semantic change. Subsequently, the\ndialogues are annotated by experts, strictly adhering to the Motivational\nInterviewing Skills Code (MISC), focusing on both the psychological and\nlinguistic dimensions of MI dialogues. We comprehensively evaluate the\nIC-AnnoMI dataset and ChatGPT's emotional reasoning ability and understanding\nof domain intricacies by modeling novel classification tasks employing several\nclassical machine learning and current state-of-the-art transformer approaches.\nFinally, we discuss the effects of progressive prompting strategies and the\nimpact of augmented data in mitigating the biases manifested in IC-AnnoM. Our\ncontributions provide the MI community with not only a comprehensive dataset\nbut also valuable insights for using LLMs in empathetic text generation for\nconversational therapy in supervised settings.", "published": "2024-12-17 15:01:07", "link": "http://arxiv.org/abs/2412.12981v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RCLMuFN: Relational Context Learning and Multiplex Fusion Network for\n  Multimodal Sarcasm Detection", "abstract": "Sarcasm typically conveys emotions of contempt or criticism by expressing a\nmeaning that is contrary to the speaker's true intent. Accurate detection of\nsarcasm aids in identifying and filtering undesirable information on the\nInternet, thereby reducing malicious defamation and rumor-mongering.\nNonetheless, the task of automatic sarcasm detection remains highly challenging\nfor machines, as it critically depends on intricate factors such as relational\ncontext. Most existing multimodal sarcasm detection methods focus on\nintroducing graph structures to establish entity relationships between text and\nimages while neglecting to learn the relational context between text and\nimages, which is crucial evidence for understanding the meaning of sarcasm. In\naddition, the meaning of sarcasm changes with the evolution of different\ncontexts, but existing methods may not be accurate in modeling such dynamic\nchanges, limiting the generalization ability of the models. To address the\nabove issues, we propose a relational context learning and multiplex fusion\nnetwork (RCLMuFN) for multimodal sarcasm detection. Firstly, we employ four\nfeature extractors to comprehensively extract features from raw text and\nimages, aiming to excavate potential features that may have been previously\noverlooked. Secondly, we utilize the relational context learning module to\nlearn the contextual information of text and images and capture the dynamic\nproperties through shallow and deep interactions. Finally, we employ a\nmultiplex feature fusion module to enhance the generalization of the model by\npenetratingly integrating multimodal features derived from various interaction\ncontexts. Extensive experiments on two multimodal sarcasm detection datasets\nshow that our proposed method achieves state-of-the-art performance.", "published": "2024-12-17 15:29:31", "link": "http://arxiv.org/abs/2412.13008v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in\n  Financial Domain", "abstract": "As a typical and practical application of Large Language Models (LLMs),\nRetrieval-Augmented Generation (RAG) techniques have gained extensive\nattention, particularly in vertical domains where LLMs may lack domain-specific\nknowledge. In this paper, we introduce an omnidirectional and automatic RAG\nbenchmark, OmniEval, in the financial domain. Our benchmark is characterized by\nits multi-dimensional evaluation framework, including (1) a matrix-based RAG\nscenario evaluation system that categorizes queries into five task classes and\n16 financial topics, leading to a structured assessment of diverse query\nscenarios; (2) a multi-dimensional evaluation data generation approach, which\ncombines GPT-4-based automatic generation and human annotation, achieving an\n87.47\\% acceptance ratio in human evaluations on generated instances; (3) a\nmulti-stage evaluation system that evaluates both retrieval and generation\nperformance, result in a comprehensive evaluation on the RAG pipeline; and (4)\nrobust evaluation metrics derived from rule-based and LLM-based ones, enhancing\nthe reliability of assessments through manual annotations and supervised\nfine-tuning of an LLM evaluator. Our experiments demonstrate the\ncomprehensiveness of OmniEval, which includes extensive test datasets and\nhighlights the performance variations of RAG systems across diverse topics and\ntasks, revealing significant opportunities for RAG models to improve their\ncapabilities in vertical domains. We open source the code of our benchmark in\n\\href{https://github.com/RUC-NLPIR/OmniEval}{https://github.com/RUC-NLPIR/OmniEval}.", "published": "2024-12-17 15:38:42", "link": "http://arxiv.org/abs/2412.13018v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Explainability of Sentence-level Metrics via Edit-level\n  Attribution for Grammatical Error Correction", "abstract": "Various evaluation metrics have been proposed for Grammatical Error\nCorrection (GEC), but many, particularly reference-free metrics, lack\nexplainability. This lack of explainability hinders researchers from analyzing\nthe strengths and weaknesses of GEC models and limits the ability to provide\ndetailed feedback for users. To address this issue, we propose attributing\nsentence-level scores to individual edits, providing insight into how specific\ncorrections contribute to the overall performance. For the attribution method,\nwe use Shapley values, from cooperative game theory, to compute the\ncontribution of each edit. Experiments with existing sentence-level metrics\ndemonstrate high consistency across different edit granularities and show\napproximately 70\\% alignment with human evaluations. In addition, we analyze\nbiases in the metrics based on the attribution results, revealing trends such\nas the tendency to ignore orthographic edits. Our implementation is available\nat \\url{https://github.com/naist-nlp/gec-attribute}.", "published": "2024-12-17 17:31:17", "link": "http://arxiv.org/abs/2412.13110v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntactic Transfer to Kyrgyz Using the Treebank Translation Method", "abstract": "The Kyrgyz language, as a low-resource language, requires significant effort\nto create high-quality syntactic corpora. This study proposes an approach to\nsimplify the development process of a syntactic corpus for Kyrgyz. We present a\ntool for transferring syntactic annotations from Turkish to Kyrgyz based on a\ntreebank translation method. The effectiveness of the proposed tool was\nevaluated using the TueCL treebank. The results demonstrate that this approach\nachieves higher syntactic annotation accuracy compared to a monolingual model\ntrained on the Kyrgyz KTMU treebank. Additionally, the study introduces a\nmethod for assessing the complexity of manual annotation for the resulting\nsyntactic trees, contributing to further optimization of the annotation\nprocess.", "published": "2024-12-17 18:12:33", "link": "http://arxiv.org/abs/2412.13146v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Algorithmic Fidelity of Large Language Models in Generating Synthetic\n  German Public Opinions: A Case Study", "abstract": "In recent research, large language models (LLMs) have been increasingly used\nto investigate public opinions. This study investigates the algorithmic\nfidelity of LLMs, i.e., the ability to replicate the socio-cultural context and\nnuanced opinions of human participants. Using open-ended survey data from the\nGerman Longitudinal Election Studies (GLES), we prompt different LLMs to\ngenerate synthetic public opinions reflective of German subpopulations by\nincorporating demographic features into the persona prompts. Our results show\nthat Llama performs better than other LLMs at representing subpopulations,\nparticularly when there is lower opinion diversity within those groups. Our\nfindings further reveal that the LLM performs better for supporters of\nleft-leaning parties like The Greens and The Left compared to other parties,\nand matches the least with the right-party AfD. Additionally, the inclusion or\nexclusion of specific variables in the prompts can significantly impact the\nmodels' predictions. These findings underscore the importance of aligning LLMs\nto more effectively model diverse public opinions while minimizing political\nbiases and enhancing robustness in representativeness.", "published": "2024-12-17 18:46:32", "link": "http://arxiv.org/abs/2412.13169v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compressed Chain of Thought: Efficient Reasoning Through Dense\n  Representations", "abstract": "Chain-of-thought (CoT) decoding enables language models to improve reasoning\nperformance at the cost of high generation latency in decoding. Recent\nproposals have explored variants of contemplation tokens, a term we introduce\nthat refers to special tokens used during inference to allow for extra\ncomputation. Prior work has considered fixed-length sequences drawn from a\ndiscrete set of embeddings as contemplation tokens. Here we propose Compressed\nChain-of-Thought (CCoT), a framework to generate contentful and continuous\ncontemplation tokens of variable sequence length. The generated contemplation\ntokens are compressed representations of explicit reasoning chains, and our\nmethod can be applied to off-the-shelf decoder language models. Through\nexperiments, we illustrate how CCoT enables additional reasoning over dense\ncontentful representations to achieve corresponding improvements in accuracy.\nMoreover, the reasoning improvements can be adaptively modified on demand by\ncontrolling the number of contemplation tokens generated.", "published": "2024-12-17 18:50:33", "link": "http://arxiv.org/abs/2412.13171v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DnDScore: Decontextualization and Decomposition for Factuality\n  Verification in Long-Form Text Generation", "abstract": "The decompose-then-verify strategy for verification of Large Language Model\n(LLM) generations decomposes claims that are then independently verified.\nDecontextualization augments text (claims) to ensure it can be verified outside\nof the original context, enabling reliable verification. While decomposition\nand decontextualization have been explored independently, their interactions in\na complete system have not been investigated. Their conflicting purposes can\ncreate tensions: decomposition isolates atomic facts while decontextualization\ninserts relevant information. Furthermore, a decontextualized subclaim presents\na challenge to the verification step: what part of the augmented text should be\nverified as it now contains multiple atomic facts? We conduct an evaluation of\ndifferent decomposition, decontextualization, and verification strategies and\nfind that the choice of strategy matters in the resulting factuality scores.\nAdditionally, we introduce DnDScore, a decontextualization aware verification\nmethod which validates subclaims in the context of contextual information.", "published": "2024-12-17 18:54:01", "link": "http://arxiv.org/abs/2412.13175v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-Context Learning Distillation for Efficient Few-Shot Fine-Tuning", "abstract": "We applied few-shot in-context learning on the OPT-1.3B model for the natural\nlanguage inference task and employed knowledge distillation to internalize the\ncontext information, reducing model parameter from 1.3B to 125M and achieving a\nsize reduction from 2.5GB to 0.25GB. Compared to using in-context learning\nalone on similarly sized models, this context distillation approach achieved a\nnearly 50% improvement in out-of-domain accuracy, demonstrating superior\nknowledge transfer capabilities over prompt-based methods. Furthermore, this\napproach reduced memory consumption by up to 60% while delivering a 20%\nimprovement in out-of-domain accuracy compared to conventional pattern-based\nfine-tuning.", "published": "2024-12-17 18:49:21", "link": "http://arxiv.org/abs/2412.13243v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Persona Classification in Dialogue Systems: A Graph Neural\n  Network Approach", "abstract": "In recent years, Large Language Models (LLMs) gain considerable attention for\ntheir potential to enhance personalized experiences in virtual assistants and\nchatbots. A key area of interest is the integration of personas into LLMs to\nimprove dialogue naturalness and user engagement. This study addresses the\nchallenge of persona classification, a crucial component in dialogue\nunderstanding, by proposing a framework that combines text embeddings with\nGraph Neural Networks (GNNs) for effective persona classification. Given the\nabsence of dedicated persona classification datasets, we create a manually\nannotated dataset to facilitate model training and evaluation. Our method\ninvolves extracting semantic features from persona statements using text\nembeddings and constructing a graph where nodes represent personas and edges\ncapture their similarities. The GNN component uses this graph structure to\npropagate relevant information, thereby improving classification performance.\nExperimental results show that our approach, in particular the integration of\nGNNs, significantly improves classification performance, especially with\nlimited data. Our contributions include the development of a persona\nclassification framework and the creation of a dataset.", "published": "2024-12-17 19:27:24", "link": "http://arxiv.org/abs/2412.13283v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Refining Answer Distributions for Improved Large Language Model\n  Reasoning", "abstract": "Large Language Models (LLMs) have exhibited an impressive capability to\nperform reasoning tasks, especially if they are encouraged to generate a\nsequence of intermediate steps. Reasoning performance can be improved by\nsuitably combining multiple LLM responses, generated either in parallel in a\nsingle query, or via sequential interactions with LLMs throughout the reasoning\nprocess. Existing strategies for combination, such as self-consistency and\nprogressive-hint-prompting, make inefficient usage of the LLM responses. We\npresent Refined Answer Distributions, a novel and principled algorithmic\nframework to enhance the reasoning capabilities of LLMs. Our approach can be\nviewed as an iterative sampling strategy for forming a Monte Carlo\napproximation of an underlying distribution of answers, with the goal of\nidentifying the mode -- the most likely answer. Empirical evaluation on several\nreasoning benchmarks demonstrates the superiority of the proposed approach.", "published": "2024-12-17 19:45:53", "link": "http://arxiv.org/abs/2412.13292v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extending LLMs to New Languages: A Case Study of Llama and Persian\n  Adaptation", "abstract": "Large language models (LLMs) have made great progress in classification and\ntext generation tasks. However, they are mainly trained on English data and\noften struggle with low-resource languages. In this study, we explore adding a\nnew language, i.e., Persian, to Llama (a model with a limited understanding of\nPersian) using parameter-efficient fine-tuning. We employ a multi-stage\napproach involving pretraining on monolingual Persian data, aligning\nrepresentations through bilingual pretraining and instruction datasets, and\ninstruction-tuning with task-specific datasets. We evaluate the model's\nperformance at each stage on generation and classification tasks. Our findings\nsuggest that incorporating the Persian language, through bilingual data\nalignment, can enhance classification accuracy for Persian tasks, with no\nadverse impact and sometimes even improvements on English tasks. Additionally,\nthe results highlight the model's initial strength as a critical factor when\nworking with limited training data, with cross-lingual alignment offering\nminimal benefits for the low-resource language. Knowledge transfer from English\nto Persian has a marginal effect, primarily benefiting simple classification\ntasks.", "published": "2024-12-17 23:18:06", "link": "http://arxiv.org/abs/2412.13375v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SummExecEdit: A Factual Consistency Benchmark in Summarization with\n  Executable Edits", "abstract": "Detecting factual inconsistencies in summarization is critical, yet existing\nbenchmarks lack the necessary challenge and interpretability for robust\nevaluation. In this paper, we introduce SummExecEdit, a novel benchmark\nleveraging executable edits to assess models on their ability to both detect\nfactual errors and provide accurate explanations. The top-performing model,\nClaude3-Opus, achieves a joint detection and explanation score of only 0.49 in\nour benchmark, with individual scores of 0.67 for detection and 0.73 for\nexplanation. Furthermore, we identify four primary types of explanation errors,\nwith 45.4% of errors focusing on completely unrelated parts of the summary.", "published": "2024-12-17 23:26:44", "link": "http://arxiv.org/abs/2412.13378v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Automated Explainable Educational Assessment System Built on LLMs", "abstract": "In this demo, we present AERA Chat, an automated and explainable educational\nassessment system designed for interactive and visual evaluations of student\nresponses. This system leverages large language models (LLMs) to generate\nautomated marking and rationale explanations, addressing the challenge of\nlimited explainability in automated educational assessment and the high costs\nassociated with annotation. Our system allows users to input questions and\nstudent answers, providing educators and researchers with insights into\nassessment accuracy and the quality of LLM-assessed rationales. Additionally,\nit offers advanced visualization and robust evaluation tools, enhancing the\nusability for educational assessment and facilitating efficient rationale\nverification. Our demo video can be found at https://youtu.be/qUSjz-sxlBc.", "published": "2024-12-17 23:29:18", "link": "http://arxiv.org/abs/2412.13381v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Capabilities of Large Language Models for Multi-label\n  Emotion Understanding", "abstract": "Large Language Models (LLMs) show promising learning and reasoning abilities.\nCompared to other NLP tasks, multilingual and multi-label emotion evaluation\ntasks are under-explored in LLMs. In this paper, we present EthioEmo, a\nmulti-label emotion classification dataset for four Ethiopian languages,\nnamely, Amharic (amh), Afan Oromo (orm), Somali (som), and Tigrinya (tir). We\nperform extensive experiments with an additional English multi-label emotion\ndataset from SemEval 2018 Task 1. Our evaluation includes encoder-only,\nencoder-decoder, and decoder-only language models. We compare zero and few-shot\napproaches of LLMs to fine-tuning smaller language models. The results show\nthat accurate multi-label emotion classification is still insufficient even for\nhigh-resource languages such as English, and there is a large gap between the\nperformance of high-resource and low-resource languages. The results also show\nvarying performance levels depending on the language and model type. EthioEmo\nis available publicly to further improve the understanding of emotions in\nlanguage models and how people convey emotions through various languages.", "published": "2024-12-17 07:42:39", "link": "http://arxiv.org/abs/2412.17837v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Core Context Aware Attention for Long Context Language Modeling", "abstract": "Transformer-based Large Language Models (LLMs) have exhibited remarkable\nsuccess in various natural language processing tasks primarily attributed to\nself-attention mechanism, which requires a token to consider all preceding\ntokens as its context to compute the attention score. However, when the context\nlength L becomes very large (e.g., 32K), more redundant context information\nwill be included w.r.t. any tokens, making the self-attention suffer from two\nmain limitations: 1) The computational and memory complexity scales\nquadratically w.r.t. L; 2) The presence of redundant context information may\nhamper the model to capture dependencies among crucial tokens, which may\ndegrade the representation performance. In this paper, we propose a\nplug-and-play Core Context Aware (CCA) Attention for efficient long-range\ncontext modeling, which consists of two components: 1) Globality-pooling\nattention that divides input tokens into groups and then dynamically merges\ntokens within each group into one core token based on their significance; 2)\nLocality-preserved attention that incorporates neighboring tokens into the\nattention calculation. The two complementary attentions will then be fused to\nthe final attention, maintaining comprehensive modeling ability as the full\nself-attention. In this way, the core context information w.r.t. a given token\nwill be automatically focused and strengthened, while the context information\nin redundant groups will be diminished during the learning process. As a\nresult, the computational and memory complexity will be significantly reduced.\nMore importantly, the CCA-Attention can improve the long-context modeling\nability by diminishing the redundant context information. Extensive\nexperimental results demonstrate that our CCA-Attention significantly\noutperforms state-of-the-art models in terms of computational efficiency and\nlong-context modeling ability.", "published": "2024-12-17 01:54:08", "link": "http://arxiv.org/abs/2412.12465v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RareAgents: Advancing Rare Disease Care through LLM-Empowered\n  Multi-disciplinary Team", "abstract": "Rare diseases, despite their low individual incidence, collectively impact\naround 300 million people worldwide due to the vast number of diseases. The\ninvolvement of multiple organs and systems, and the shortage of specialized\ndoctors with relevant experience make diagnosing and treating rare diseases\nmore challenging than common diseases. Recently, agents powered by large\nlanguage models (LLMs) have demonstrated notable applications across various\ndomains. In the medical field, some agent methods have outperformed direct\nprompts in question-answering tasks from medical examinations. However, current\nagent frameworks are not well-adapted to real-world clinical scenarios,\nespecially those involving the complex demands of rare diseases. To bridge this\ngap, we introduce RareAgents, the first LLM-driven multi-disciplinary team\nframework designed specifically for the complex clinical context of rare\ndiseases. RareAgents integrates advanced Multidisciplinary Team (MDT)\ncoordination, memory mechanisms, and medical tools utilization, leveraging\nLlama-3.1-8B/70B as the base model. Experimental results show that RareAgents\noutperforms state-of-the-art domain-specific models, GPT-4o, and current agent\nframeworks in differential diagnosis and medication recommendation for rare\ndiseases. Furthermore, we contribute a novel rare disease dataset,\nMIMIC-IV-Ext-Rare, to support further advancements in this field.", "published": "2024-12-17 02:22:24", "link": "http://arxiv.org/abs/2412.12475v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LinguaLIFT: An Effective Two-stage Instruction Tuning Framework for\n  Low-Resource Language Reasoning", "abstract": "Large language models (LLMs) have exhibited impressive multilingual reasoning\ncapabilities, driven by extensive multilingual pre-training corpora and\ninstruction fine-tuning data. However, a performance gap exists between high-\nand low-resource language reasoning tasks due to the language imbalance in the\npre-training corpus, which is exacerbated by evaluation bias in existing\nreasoning benchmarks lacking low-resource language coverage. To alleviate this\nissue, we propose LinguaLIFT, a two-stage instruction tuning framework for\nadvancing low-resource language reasoning. LinguaLIFT employs a language\nalignment layer to capture multilingual alignment in a code-switched tuning way\nwithout requiring multilingual instruction or parallel data, thereby\ntransferring the cross-lingual reasoning capabilities to low-resource languages\nthrough English-only instruction tuning data. To comprehensively evaluate the\nmultilingual reasoning capabilities, we introduce the Multilingual Math World\nProblem (MMWP) benchmark, which spans 21 low-resource, 17 medium-resource, and\n10 high-resource languages. Experimental results show that LinguaLIFT\noutperforms several competitive baselines across MMWP and four widely used\nbenchmarks.", "published": "2024-12-17 03:03:17", "link": "http://arxiv.org/abs/2412.12499v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Data Quantity: Key Factors Driving Performance in Multilingual\n  Language Models", "abstract": "Multilingual language models (MLLMs) are crucial for handling text across\nvarious languages, yet they often show performance disparities due to\ndifferences in resource availability and linguistic characteristics. While the\nimpact of pre-train data percentage and model size on performance is\nwell-known, our study reveals additional critical factors that significantly\ninfluence MLLM effectiveness. Analyzing a wide range of features, including\ngeographical, linguistic, and resource-related aspects, we focus on the SIB-200\ndataset for classification and the Flores-200 dataset for machine translation,\nusing regression models and SHAP values across 204 languages. Our findings\nidentify token similarity and country similarity as pivotal factors, alongside\npre-train data and model size, in enhancing model performance. Token similarity\nfacilitates cross-lingual transfer, while country similarity highlights the\nimportance of shared cultural and linguistic contexts. These insights offer\nvaluable guidance for developing more equitable and effective multilingual\nlanguage models, particularly for underrepresented languages.", "published": "2024-12-17 03:05:26", "link": "http://arxiv.org/abs/2412.12500v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Understand You Better? An MBTI Personality\n  Detection Dataset Aligned with Population Traits", "abstract": "The Myers-Briggs Type Indicator (MBTI) is one of the most influential\npersonality theories reflecting individual differences in thinking, feeling,\nand behaving. MBTI personality detection has garnered considerable research\ninterest and has evolved significantly over the years. However, this task tends\nto be overly optimistic, as it currently does not align well with the natural\ndistribution of population personality traits. Specifically, (1) the\nself-reported labels in existing datasets result in incorrect labeling issues,\nand (2) the hard labels fail to capture the full range of population\npersonality distributions. In this paper, we optimize the task by constructing\nMBTIBench, the first manually annotated high-quality MBTI personality detection\ndataset with soft labels, under the guidance of psychologists. As for the first\nchallenge, MBTIBench effectively solves the incorrect labeling issues, which\naccount for 29.58% of the data. As for the second challenge, we estimate soft\nlabels by deriving the polarity tendency of samples. The obtained soft labels\nconfirm that there are more people with non-extreme personality traits.\nExperimental results not only highlight the polarized predictions and biases in\nLLMs as key directions for future research, but also confirm that soft labels\ncan provide more benefits to other psychological tasks than hard labels. The\ncode and data are available at https://github.com/Personality-NLP/MbtiBench.", "published": "2024-12-17 03:46:51", "link": "http://arxiv.org/abs/2412.12510v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Solid-SQL: Enhanced Schema-linking based In-context Learning for Robust\n  Text-to-SQL", "abstract": "Recently, large language models (LLMs) have significantly improved the\nperformance of text-to-SQL systems. Nevertheless, many state-of-the-art (SOTA)\napproaches have overlooked the critical aspect of system robustness. Our\nexperiments reveal that while LLM-driven methods excel on standard datasets,\ntheir accuracy is notably compromised when faced with adversarial\nperturbations. To address this challenge, we propose a robust text-to-SQL\nsolution, called Solid-SQL, designed to integrate with various LLMs. We focus\non the pre-processing stage, training a robust schema-linking model enhanced by\nLLM-based data augmentation. Additionally, we design a two-round, structural\nsimilarity-based example retrieval strategy for in-context learning. Our method\nachieves SOTA SQL execution accuracy levels of 82.1% and 58.9% on the general\nSpider and Bird benchmarks, respectively. Furthermore, experimental results\nshow that Solid-SQL delivers an average improvement of 11.6% compared to\nbaselines on the perturbed Spider-Syn, Spider-Realistic, and Dr. Spider\nbenchmarks.", "published": "2024-12-17 04:22:22", "link": "http://arxiv.org/abs/2412.12522v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMCL-GEC: Advancing Grammatical Error Correction with LLM-Driven\n  Curriculum Learning", "abstract": "While large-scale language models (LLMs) have demonstrated remarkable\ncapabilities in specific natural language processing (NLP) tasks, they may\nstill lack proficiency compared to specialized models in certain domains, such\nas grammatical error correction (GEC). Drawing inspiration from the concept of\ncurriculum learning, we have delved into refining LLMs into proficient GEC\nexperts by devising effective curriculum learning (CL) strategies. In this\npaper, we introduce a novel approach, termed LLM-based curriculum learning,\nwhich capitalizes on the robust semantic comprehension and discriminative\nprowess inherent in LLMs to gauge the complexity of GEC training data. Unlike\ntraditional curriculum learning techniques, our method closely mirrors human\nexpert-designed curriculums. Leveraging the proposed LLM-based CL method, we\nsequentially select varying levels of curriculums ranging from easy to hard,\nand iteratively train and refine using the pretrianed T5 and LLaMA series\nmodels. Through rigorous testing and analysis across diverse benchmark\nassessments in English GEC, including the CoNLL14 test, BEA19 test, and BEA19\ndevelopment sets, our approach showcases a significant performance boost over\nbaseline models and conventional curriculum learning methodologies.", "published": "2024-12-17 05:09:07", "link": "http://arxiv.org/abs/2412.12541v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What External Knowledge is Preferred by LLMs? Characterizing and\n  Exploring Chain of Evidence in Imperfect Context", "abstract": "Incorporating external knowledge into large language models (LLMs) has\nemerged as a promising approach to mitigate outdated knowledge and\nhallucination in LLMs. However, external knowledge is often imperfect. In\naddition to useful knowledge, external knowledge is rich in irrelevant or\nmisinformation in the context that can impair the reliability of LLM responses.\nThis paper focuses on LLMs' preferred external knowledge in imperfect contexts\nwhen handling multi-hop QA. Inspired by criminal procedural law's Chain of\nEvidence (CoE), we characterize that knowledge preferred by LLMs should\nmaintain both relevance to the question and mutual support among knowledge\npieces. Accordingly, we propose an automated CoE discrimination approach and\nexplore LLMs' preferences from their effectiveness, faithfulness and\nrobustness, as well as CoE's usability in a naive Retrieval-Augmented\nGeneration (RAG) case. The evaluation on five LLMs reveals that CoE enhances\nLLMs through more accurate generation, stronger answer faithfulness, better\nrobustness against knowledge conflict, and improved performance in a popular\nRAG case.", "published": "2024-12-17 07:49:49", "link": "http://arxiv.org/abs/2412.12632v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Falcon: Faster and Parallel Inference of Large Language Models through\n  Enhanced Semi-Autoregressive Drafting and Custom-Designed Decoding Tree", "abstract": "Striking an optimal balance between minimal drafting latency and high\nspeculation accuracy to enhance the inference speed of Large Language Models\nremains a significant challenge in speculative decoding. In this paper, we\nintroduce Falcon, an innovative semi-autoregressive speculative decoding\nframework fashioned to augment both the drafter's parallelism and output\nquality. Falcon incorporates the Coupled Sequential Glancing Distillation\ntechnique, which fortifies inter-token dependencies within the same block,\nleading to increased speculation accuracy. We offer a comprehensive theoretical\nanalysis to illuminate the underlying mechanisms. Additionally, we introduce a\nCustom-Designed Decoding Tree, which permits the drafter to generate multiple\ntokens in a single forward pass and accommodates multiple forward passes as\nneeded, thereby boosting the number of drafted tokens and significantly\nimproving the overall acceptance rate. Comprehensive evaluations on benchmark\ndatasets such as MT-Bench, HumanEval, and GSM8K demonstrate Falcon's superior\nacceleration capabilities. The framework achieves a lossless speedup ratio\nranging from 2.91x to 3.51x when tested on the Vicuna and LLaMA2-Chat model\nseries. These results outstrip existing speculative decoding methods for LLMs,\nincluding Eagle, Medusa, Lookahead, SPS, and PLD, while maintaining a compact\ndrafter architecture equivalent to merely two Transformer layers.", "published": "2024-12-17 08:02:08", "link": "http://arxiv.org/abs/2412.12639v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ClustEm4Ano: Clustering Text Embeddings of Nominal Textual Attributes\n  for Microdata Anonymization", "abstract": "This work introduces ClustEm4Ano, an anonymization pipeline that can be used\nfor generalization and suppression-based anonymization of nominal textual\ntabular data. It automatically generates value generalization hierarchies\n(VGHs) that, in turn, can be used to generalize attributes in\nquasi-identifiers. The pipeline leverages embeddings to generate semantically\nclose value generalizations through iterative clustering. We applied KMeans and\nHierarchical Agglomerative Clustering on $13$ different predefined text\nembeddings (both open and closed-source (via APIs)). Our approach is\nexperimentally tested on a well-known benchmark dataset for anonymization: The\nUCI Machine Learning Repository's Adult dataset. ClustEm4Ano supports\nanonymization procedures by offering more possibilities compared to using\narbitrarily chosen VGHs. Experiments demonstrate that these VGHs can outperform\nmanually constructed ones in terms of downstream efficacy (especially for small\n$k$-anonymity ($2 \\leq k \\leq 30$)) and therefore can foster the quality of\nanonymized datasets. Our implementation is made public.", "published": "2024-12-17 08:16:04", "link": "http://arxiv.org/abs/2412.12649v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SentiQNF: A Novel Approach to Sentiment Analysis Using Quantum\n  Algorithms and Neuro-Fuzzy Systems", "abstract": "Sentiment analysis is an essential component of natural language processing,\nused to analyze sentiments, attitudes, and emotional tones in various contexts.\nIt provides valuable insights into public opinion, customer feedback, and user\nexperiences. Researchers have developed various classical machine learning and\nneuro-fuzzy approaches to address the exponential growth of data and the\ncomplexity of language structures in sentiment analysis. However, these\napproaches often fail to determine the optimal number of clusters, interpret\nresults accurately, handle noise or outliers efficiently, and scale effectively\nto high-dimensional data. Additionally, they are frequently insensitive to\ninput variations. In this paper, we propose a novel hybrid approach for\nsentiment analysis called the Quantum Fuzzy Neural Network (QFNN), which\nleverages quantum properties and incorporates a fuzzy layer to overcome the\nlimitations of classical sentiment analysis algorithms. In this study, we test\nthe proposed approach on two Twitter datasets: the Coronavirus Tweets Dataset\n(CVTD) and the General Sentimental Tweets Dataset (GSTD), and compare it with\nclassical and hybrid algorithms. The results demonstrate that QFNN outperforms\nall classical, quantum, and hybrid algorithms, achieving 100% and 90% accuracy\nin the case of CVTD and GSTD, respectively. Furthermore, QFNN demonstrates its\nrobustness against six different noise models, providing the potential to\ntackle the computational complexity associated with sentiment analysis on a\nlarge scale in a noisy environment. The proposed approach expedites sentiment\ndata processing and precisely analyses different forms of textual data, thereby\nenhancing sentiment classification and insights associated with sentiment\nanalysis.", "published": "2024-12-17 09:54:17", "link": "http://arxiv.org/abs/2412.12731v2", "categories": ["cs.CL", "quant-ph"], "primary_category": "cs.CL"}
{"title": "Revealing the impact of synthetic native samples and multi-tasking\n  strategies in Hindi-English code-mixed humour and sarcasm detection", "abstract": "In this paper, we reported our experiments with various strategies to improve\ncode-mixed humour and sarcasm detection. We did all of our experiments for\nHindi-English code-mixed scenario, as we have the linguistic expertise for the\nsame. We experimented with three approaches, namely (i) native sample mixing,\n(ii) multi-task learning (MTL), and (iii) prompting very large multilingual\nlanguage models (VMLMs). In native sample mixing, we added monolingual task\nsamples in code-mixed training sets. In MTL learning, we relied on native and\ncode-mixed samples of a semantically related task (hate detection in our case).\nFinally, in our third approach, we evaluated the efficacy of VMLMs via few-shot\ncontext prompting. Some interesting findings we got are (i) adding native\nsamples improved humor (raising the F1-score up to 6.76%) and sarcasm (raising\nthe F1-score up to 8.64%) detection, (ii) training MLMs in an MTL framework\nboosted performance for both humour (raising the F1-score up to 10.67%) and\nsarcasm (increment up to 12.35% in F1-score) detection, and (iii) prompting\nVMLMs couldn't outperform the other approaches. Finally, our ablation studies\nand error analysis discovered the cases where our model is yet to improve. We\nprovided our code for reproducibility.", "published": "2024-12-17 10:26:54", "link": "http://arxiv.org/abs/2412.12761v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey of Calibration Process for Black-Box LLMs", "abstract": "Large Language Models (LLMs) demonstrate remarkable performance in semantic\nunderstanding and generation, yet accurately assessing their output reliability\nremains a significant challenge. While numerous studies have explored\ncalibration techniques, they primarily focus on White-Box LLMs with accessible\nparameters. Black-Box LLMs, despite their superior performance, pose heightened\nrequirements for calibration techniques due to their API-only interaction\nconstraints. Although recent researches have achieved breakthroughs in\nblack-box LLMs calibration, a systematic survey of these methodologies is still\nlacking. To bridge this gap, we presents the first comprehensive survey on\ncalibration techniques for black-box LLMs. We first define the Calibration\nProcess of LLMs as comprising two interrelated key steps: Confidence Estimation\nand Calibration. Second, we conduct a systematic review of applicable methods\nwithin black-box settings, and provide insights on the unique challenges and\nconnections in implementing these key steps. Furthermore, we explore typical\napplications of Calibration Process in black-box LLMs and outline promising\nfuture research directions, providing new perspectives for enhancing\nreliability and human-machine alignment. This is our GitHub link:\nhttps://github.com/LiangruXie/Calibration-Process-in-Black-Box-LLMs", "published": "2024-12-17 10:31:21", "link": "http://arxiv.org/abs/2412.12767v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Cross-Dialect Information Retrieval: Information Access in Low-Resource\n  and High-Variance Languages", "abstract": "A large amount of local and culture-specific knowledge (e.g., people,\ntraditions, food) can only be found in documents written in dialects. While\nthere has been extensive research conducted on cross-lingual information\nretrieval (CLIR), the field of cross-dialect retrieval (CDIR) has received\nlimited attention. Dialect retrieval poses unique challenges due to the limited\navailability of resources to train retrieval models and the high variability in\nnon-standardized languages. We study these challenges on the example of German\ndialects and introduce the first German dialect retrieval dataset, dubbed\nWikiDIR, which consists of seven German dialects extracted from Wikipedia.\nUsing WikiDIR, we demonstrate the weakness of lexical methods in dealing with\nhigh lexical variation in dialects. We further show that commonly used\nzero-shot cross-lingual transfer approach with multilingual encoders do not\ntransfer well to extremely low-resource setups, motivating the need for\nresource-lean and dialect-specific retrieval models. We finally demonstrate\nthat (document) translation is an effective way to reduce the dialect gap in\nCDIR.", "published": "2024-12-17 11:21:09", "link": "http://arxiv.org/abs/2412.12806v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Detecting Emotional Incongruity of Sarcasm by Commonsense Reasoning", "abstract": "This paper focuses on sarcasm detection, which aims to identify whether given\nstatements convey criticism, mockery, or other negative sentiment opposite to\nthe literal meaning. To detect sarcasm, humans often require a comprehensive\nunderstanding of the semantics in the statement and even resort to external\ncommonsense to infer the fine-grained incongruity. However, existing methods\nlack commonsense inferential ability when they face complex real-world\nscenarios, leading to unsatisfactory performance. To address this problem, we\npropose a novel framework for sarcasm detection, which conducts incongruity\nreasoning based on commonsense augmentation, called EICR. Concretely, we first\nemploy retrieval-augmented large language models to supplement the missing but\nindispensable commonsense background knowledge. To capture complex contextual\nassociations, we construct a dependency graph and obtain the optimized topology\nvia graph refinement. We further introduce an adaptive reasoning skeleton that\nintegrates prior rules to extract sentiment-inconsistent subgraphs explicitly.\nTo eliminate the possible spurious relations between words and labels, we\nemploy adversarial contrastive learning to enhance the robustness of the\ndetector. Experiments conducted on five datasets demonstrate the effectiveness\nof EICR.", "published": "2024-12-17 11:25:55", "link": "http://arxiv.org/abs/2412.12808v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DSGram: Dynamic Weighting Sub-Metrics for Grammatical Error Correction\n  in the Era of Large Language Models", "abstract": "Evaluating the performance of Grammatical Error Correction (GEC) models has\nbecome increasingly challenging, as large language model (LLM)-based GEC\nsystems often produce corrections that diverge from provided gold references.\nThis discrepancy undermines the reliability of traditional reference-based\nevaluation metrics. In this study, we propose a novel evaluation framework for\nGEC models, DSGram, integrating Semantic Coherence, Edit Level, and Fluency,\nand utilizing a dynamic weighting mechanism. Our framework employs the Analytic\nHierarchy Process (AHP) in conjunction with large language models to ascertain\nthe relative importance of various evaluation criteria. Additionally, we\ndevelop a dataset incorporating human annotations and LLM-simulated sentences\nto validate our algorithms and fine-tune more cost-effective models.\nExperimental results indicate that our proposed approach enhances the\neffectiveness of GEC model evaluations.", "published": "2024-12-17 11:54:16", "link": "http://arxiv.org/abs/2412.12832v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Benchmarking and Understanding Compositional Relational Reasoning of\n  LLMs", "abstract": "Compositional relational reasoning (CRR) is a hallmark of human intelligence,\nbut we lack a clear understanding of whether and how existing transformer large\nlanguage models (LLMs) can solve CRR tasks. To enable systematic exploration of\nthe CRR capability of LLMs, we first propose a new synthetic benchmark called\nGeneralized Associative Recall (GAR) by integrating and generalizing the\nessence of several tasks in mechanistic interpretability (MI) study in a\nunified framework. Evaluation shows that GAR is challenging enough for existing\nLLMs, revealing their fundamental deficiency in CRR. Meanwhile, it is easy\nenough for systematic MI study. Then, to understand how LLMs solve GAR tasks,\nwe use attribution patching to discover the core circuits reused by Vicuna-33B\nacross different tasks and a set of vital attention heads. Intervention\nexperiments show that the correct functioning of these heads significantly\nimpacts task performance. Especially, we identify two classes of heads whose\nactivations represent the abstract notion of true and false in GAR tasks\nrespectively. They play a fundamental role in CRR across various models and\ntasks. The dataset and code are available at https://github.com/Caiyun-AI/GAR.", "published": "2024-12-17 12:10:38", "link": "http://arxiv.org/abs/2412.12841v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DISC: Plug-and-Play Decoding Intervention with Similarity of Characters\n  for Chinese Spelling Check", "abstract": "One key characteristic of the Chinese spelling check (CSC) task is that\nincorrect characters are usually similar to the correct ones in either\nphonetics or glyph. To accommodate this, previous works usually leverage\nconfusion sets, which suffer from two problems, i.e., difficulty in determining\nwhich character pairs to include and lack of probabilities to distinguish items\nin the set. In this paper, we propose a light-weight plug-and-play DISC (i.e.,\ndecoding intervention with similarity of characters) module for CSC models.DISC\nmeasures phonetic and glyph similarities between characters and incorporates\nthis similarity information only during the inference phase. This method can be\neasily integrated into various existing CSC models, such as ReaLiSe, SCOPE, and\nReLM, without additional training costs. Experiments on three CSC benchmarks\ndemonstrate that our proposed method significantly improves model performance,\napproaching and even surpassing the current state-of-the-art models.", "published": "2024-12-17 12:44:06", "link": "http://arxiv.org/abs/2412.12863v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented\n  Verification and Refinement", "abstract": "Existing large language models (LLMs) show exceptional problem-solving\ncapabilities but might struggle with complex reasoning tasks. Despite the\nsuccesses of chain-of-thought and tree-based search methods, they mainly depend\non the internal knowledge of LLMs to search over intermediate reasoning steps,\nlimited to dealing with simple tasks involving fewer reasoning steps. In this\npaper, we propose \\textbf{RAG-Star}, a novel RAG approach that integrates the\nretrieved information to guide the tree-based deliberative reasoning process\nthat relies on the inherent knowledge of LLMs. By leveraging Monte Carlo Tree\nSearch, RAG-Star iteratively plans intermediate sub-queries and answers for\nreasoning based on the LLM itself. To consolidate internal and external\nknowledge, we propose an retrieval-augmented verification that utilizes query-\nand answer-aware reward modeling to provide feedback for the inherent reasoning\nof LLMs. Our experiments involving Llama-3.1-8B-Instruct and GPT-4o demonstrate\nthat RAG-Star significantly outperforms previous RAG and reasoning methods.", "published": "2024-12-17 13:05:36", "link": "http://arxiv.org/abs/2412.12881v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NAVCON: A Cognitively Inspired and Linguistically Grounded Corpus for\n  Vision and Language Navigation", "abstract": "We present NAVCON, a large-scale annotated Vision-Language Navigation (VLN)\ncorpus built on top of two popular datasets (R2R and RxR). The paper introduces\nfour core, cognitively motivated and linguistically grounded, navigation\nconcepts and an algorithm for generating large-scale silver annotations of\nnaturally occurring linguistic realizations of these concepts in navigation\ninstructions. We pair the annotated instructions with video clips of an agent\nacting on these instructions. NAVCON contains 236, 316 concept annotations for\napproximately 30, 0000 instructions and 2.7 million aligned images (from\napproximately 19, 000 instructions) showing what the agent sees when executing\nan instruction. To our knowledge, this is the first comprehensive resource of\nnavigation concepts. We evaluated the quality of the silver annotations by\nconducting human evaluation studies on NAVCON samples. As further validation of\nthe quality and usefulness of the resource, we trained a model for detecting\nnavigation concepts and their linguistic realizations in unseen instructions.\nAdditionally, we show that few-shot learning with GPT-4o performs well on this\ntask using large-scale silver annotations of NAVCON.", "published": "2024-12-17 15:48:25", "link": "http://arxiv.org/abs/2412.13026v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Harnessing Event Sensory Data for Error Pattern Prediction in Vehicles:\n  A Language Model Approach", "abstract": "In this paper, we draw an analogy between processing natural languages and\nprocessing multivariate event streams from vehicles in order to predict\n$\\textit{when}$ and $\\textit{what}$ error pattern is most likely to occur in\nthe future for a given car. Our approach leverages the temporal dynamics and\ncontextual relationships of our event data from a fleet of cars. Event data is\ncomposed of discrete values of error codes as well as continuous values such as\ntime and mileage. Modelled by two causal Transformers, we can anticipate\nvehicle failures and malfunctions before they happen. Thus, we introduce\n$\\textit{CarFormer}$, a Transformer model trained via a new self-supervised\nlearning strategy, and $\\textit{EPredictor}$, an autoregressive Transformer\ndecoder model capable of predicting $\\textit{when}$ and $\\textit{what}$ error\npattern will most likely occur after some error code apparition. Despite the\nchallenges of high cardinality of event types, their unbalanced frequency of\nappearance and limited labelled data, our experimental results demonstrate the\nexcellent predictive ability of our novel model. Specifically, with sequences\nof $160$ error codes on average, our model is able with only half of the error\ncodes to achieve $80\\%$ F1 score for predicting $\\textit{what}$ error pattern\nwill occur and achieves an average absolute error of $58.4 \\pm 13.2$h\n$\\textit{when}$ forecasting the time of occurrence, thus enabling confident\npredictive maintenance and enhancing vehicle safety.", "published": "2024-12-17 16:05:30", "link": "http://arxiv.org/abs/2412.13041v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LMUnit: Fine-grained Evaluation with Natural Language Unit Tests", "abstract": "As language models become integral to critical workflows, assessing their\nbehavior remains a fundamental challenge -- human evaluation is costly and\nnoisy, while automated metrics provide only coarse, difficult-to-interpret\nsignals. We introduce natural language unit tests, a paradigm that decomposes\nresponse quality into explicit, testable criteria, along with a unified scoring\nmodel, LMUnit, which combines multi-objective training across preferences,\ndirect ratings, and natural language rationales. Through controlled human\nstudies, we show this paradigm significantly improves inter-annotator agreement\nand enables more effective LLM development workflows. LMUnit achieves\nstate-of-the-art performance on evaluation benchmarks (FLASK, BigGenBench) and\ncompetitive results on RewardBench. These results validate both our proposed\nparadigm and scoring model, suggesting a promising path forward for language\nmodel evaluation and development.", "published": "2024-12-17 17:01:15", "link": "http://arxiv.org/abs/2412.13091v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Uchaguzi-2022: A Dataset of Citizen Reports on the 2022 Kenyan Election", "abstract": "Online reporting platforms have enabled citizens around the world to\ncollectively share their opinions and report in real time on events impacting\ntheir local communities. Systematically organizing (e.g., categorizing by\nattributes) and geotagging large amounts of crowdsourced information is crucial\nto ensuring that accurate and meaningful insights can be drawn from this data\nand used by policy makers to bring about positive change. These tasks, however,\ntypically require extensive manual annotation efforts. In this paper we present\nUchaguzi-2022, a dataset of 14k categorized and geotagged citizen reports\nrelated to the 2022 Kenyan General Election containing mentions of\nelection-related issues such as official misconduct, vote count irregularities,\nand acts of violence. We use this dataset to investigate whether language\nmodels can assist in scalably categorizing and geotagging reports, thus\nhighlighting its potential application in the AI for Social Good space.", "published": "2024-12-17 17:08:35", "link": "http://arxiv.org/abs/2412.13098v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark", "abstract": "Evaluation plays a crucial role in the advancement of information retrieval\n(IR) models. However, current benchmarks, which are based on predefined domains\nand human-labeled data, face limitations in addressing evaluation needs for\nemerging domains both cost-effectively and efficiently. To address this\nchallenge, we propose the Automated Heterogeneous Information Retrieval\nBenchmark (AIR-Bench). AIR-Bench is distinguished by three key features: 1)\nAutomated. The testing data in AIR-Bench is automatically generated by large\nlanguage models (LLMs) without human intervention. 2) Heterogeneous. The\ntesting data in AIR-Bench is generated with respect to diverse tasks, domains\nand languages. 3) Dynamic. The domains and languages covered by AIR-Bench are\nconstantly augmented to provide an increasingly comprehensive evaluation\nbenchmark for community developers. We develop a reliable and robust data\ngeneration pipeline to automatically create diverse and high-quality evaluation\ndatasets based on real-world corpora. Our findings demonstrate that the\ngenerated testing data in AIR-Bench aligns well with human-labeled testing\ndata, making AIR-Bench a dependable benchmark for evaluating IR models. The\nresources in AIR-Bench are publicly available at\nhttps://github.com/AIR-Bench/AIR-Bench.", "published": "2024-12-17 17:15:21", "link": "http://arxiv.org/abs/2412.13102v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "AI PERSONA: Towards Life-long Personalization of LLMs", "abstract": "In this work, we introduce the task of life-long personalization of large\nlanguage models. While recent mainstream efforts in the LLM community mainly\nfocus on scaling data and compute for improved capabilities of LLMs, we argue\nthat it is also very important to enable LLM systems, or language agents, to\ncontinuously adapt to the diverse and ever-changing profiles of every distinct\nuser and provide up-to-date personalized assistance. We provide a clear task\nformulation and introduce a simple, general, effective, and scalable framework\nfor life-long personalization of LLM systems and language agents. To facilitate\nfuture research on LLM personalization, we also introduce methods to synthesize\nrealistic benchmarks and robust evaluation metrics. We will release all codes\nand data for building and benchmarking life-long personalized LLM systems.", "published": "2024-12-17 17:17:03", "link": "http://arxiv.org/abs/2412.13103v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Are Your LLMs Capable of Stable Reasoning?", "abstract": "The rapid advancement of Large Language Models (LLMs) has demonstrated\nremarkable progress in complex reasoning tasks. However, a significant\ndiscrepancy persists between benchmark performances and real-world\napplications. We identify this gap as primarily stemming from current\nevaluation protocols and metrics, which inadequately capture the full spectrum\nof LLM capabilities, particularly in complex reasoning tasks where both\naccuracy and consistency are crucial. This work makes two key contributions.\nFirst, we introduce G-Pass@k, a novel evaluation metric that provides a\ncontinuous assessment of model performance across multiple sampling attempts,\nquantifying both the model's peak performance potential and its stability.\nSecond, we present LiveMathBench, a dynamic benchmark comprising challenging,\ncontemporary mathematical problems designed to minimize data leakage risks\nduring evaluation. Through extensive experiments using G-Pass@k on\nstate-of-the-art LLMs with LiveMathBench, we provide comprehensive insights\ninto both their maximum capabilities and operational consistency. Our findings\nreveal substantial room for improvement in LLMs' \"realistic\" reasoning\ncapabilities, highlighting the need for more robust evaluation methods. The\nbenchmark and detailed results are available at:\nhttps://github.com/open-compass/GPassK.", "published": "2024-12-17 18:12:47", "link": "http://arxiv.org/abs/2412.13147v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Expansion Span: Combining Fading Memory and Retrieval in Hybrid State\n  Space Models", "abstract": "The \"state\" of State Space Models (SSMs) represents their memory, which fades\nexponentially over an unbounded span. By contrast, Attention-based models have\n\"eidetic\" (i.e., verbatim, or photographic) memory over a finite span (context\nsize). Hybrid architectures combine State Space layers with Attention, but\nstill cannot recall the distant past and can access only the most recent tokens\neidetically. Unlike current methods of combining SSM and Attention layers, we\nallow the state to be allocated based on relevancy rather than recency. In this\nway, for every new set of query tokens, our models can \"eidetically\" access\ntokens from beyond the Attention span of current Hybrid SSMs without requiring\nextra hardware resources. We describe a method to expand the memory span of the\nhybrid state by \"reserving\" a fraction of the Attention context for tokens\nretrieved from arbitrarily distant in the past, thus expanding the eidetic\nmemory span of the overall state. We call this reserved fraction of tokens the\n\"expansion span,\" and the mechanism to retrieve and aggregate it \"Span-Expanded\nAttention\" (SE-Attn). To adapt Hybrid models to using SE-Attn, we propose a\nnovel fine-tuning method that extends LoRA to Hybrid models (HyLoRA) and allows\nefficient adaptation on long spans of tokens. We show that SE-Attn enables us\nto efficiently adapt pre-trained Hybrid models on sequences of tokens up to 8\ntimes longer than the ones used for pre-training. We show that HyLoRA with\nSE-Attn is cheaper and more performant than alternatives like LongLoRA when\napplied to Hybrid models on natural language benchmarks with long-range\ndependencies, such as PG-19, RULER, and other common natural language\ndownstream tasks.", "published": "2024-12-17 20:55:42", "link": "http://arxiv.org/abs/2412.13328v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Training Dynamics of a 1.7B LLaMa Model: A Data-Efficient Approach", "abstract": "Pretraining large language models is a complex endeavor influenced by\nmultiple factors, including model architecture, data quality, training\ncontinuity, and hardware constraints. In this paper, we share insights gained\nfrom the experience of training DMaS-LLaMa-Lite, a fully open source,\n1.7-billion-parameter, LLaMa-based model, on approximately 20 billion tokens of\ncarefully curated data. We chronicle the full training trajectory, documenting\nhow evolving validation loss levels and downstream benchmarks reflect\ntransitions from incoherent text to fluent, contextually grounded output.\nBeyond pretraining, we extend our analysis to include a post-training phase\nfocused on instruction tuning, where the model was refined to produce more\ncontextually appropriate, user-aligned responses. We highlight practical\nconsiderations such as the importance of restoring optimizer states when\nresuming from checkpoints, and the impact of hardware changes on training\nstability and throughput. While qualitative evaluation provides an intuitive\nunderstanding of model improvements, our analysis extends to various\nperformance benchmarks, demonstrating how high-quality data and thoughtful\nscaling enable competitive results with significantly fewer training tokens. By\ndetailing these experiences and offering training logs, checkpoints, and sample\noutputs, we aim to guide future researchers and practitioners in refining their\npretraining strategies. The training script is available on Github at\nhttps://github.com/McGill-DMaS/DMaS-LLaMa-Lite-Training-Code. The model\ncheckpoints are available on Huggingface at\nhttps://huggingface.co/collections/McGill-DMaS/dmas-llama-lite-6761d97ba903f82341954ceb.", "published": "2024-12-17 21:15:52", "link": "http://arxiv.org/abs/2412.13335v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DateLogicQA: Benchmarking Temporal Biases in Large Language Models", "abstract": "This paper introduces DateLogicQA, a benchmark with 190 questions covering\ndiverse date formats, temporal contexts, and reasoning types. We propose the\nSemantic Integrity Metric to assess tokenization quality and analyse two\nbiases: Representation-Level Bias, affecting embeddings, and Logical-Level\nBias, influencing reasoning outputs. Our findings provide a comprehensive\nevaluation of LLMs' capabilities and limitations in temporal reasoning,\nhighlighting key challenges in handling temporal data accurately. The GitHub\nrepository for our work is available at\nhttps://github.com/gagan3012/EAIS-Temporal-Bias", "published": "2024-12-17 23:25:47", "link": "http://arxiv.org/abs/2412.13377v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ReXTrust: A Model for Fine-Grained Hallucination Detection in\n  AI-Generated Radiology Reports", "abstract": "The increasing adoption of AI-generated radiology reports necessitates robust\nmethods for detecting hallucinations--false or unfounded statements that could\nimpact patient care. We present ReXTrust, a novel framework for fine-grained\nhallucination detection in AI-generated radiology reports. Our approach\nleverages sequences of hidden states from large vision-language models to\nproduce finding-level hallucination risk scores. We evaluate ReXTrust on a\nsubset of the MIMIC-CXR dataset and demonstrate superior performance compared\nto existing approaches, achieving an AUROC of 0.8751 across all findings and\n0.8963 on clinically significant findings. Our results show that white-box\napproaches leveraging model hidden states can provide reliable hallucination\ndetection for medical AI systems, potentially improving the safety and\nreliability of automated radiology reporting.", "published": "2024-12-17 02:07:33", "link": "http://arxiv.org/abs/2412.15264v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Chinese SafetyQA: A Safety Short-form Factuality Benchmark for Large\n  Language Models", "abstract": "With the rapid advancement of Large Language Models (LLMs), significant\nsafety concerns have emerged. Fundamentally, the safety of large language\nmodels is closely linked to the accuracy, comprehensiveness, and clarity of\ntheir understanding of safety knowledge, particularly in domains such as law,\npolicy and ethics. This factuality ability is crucial in determining whether\nthese models can be deployed and applied safely and compliantly within specific\nregions. To address these challenges and better evaluate the factuality ability\nof LLMs to answer short questions, we introduce the Chinese SafetyQA benchmark.\nChinese SafetyQA has several properties (i.e., Chinese, Diverse, High-quality,\nStatic, Easy-to-evaluate, Safety-related, Harmless). Based on Chinese SafetyQA,\nwe perform a comprehensive evaluation on the factuality abilities of existing\nLLMs and analyze how these capabilities relate to LLM abilities, e.g., RAG\nability and robustness against attacks.", "published": "2024-12-17 03:03:44", "link": "http://arxiv.org/abs/2412.15265v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Structural Memory of LLM Agents", "abstract": "Memory plays a pivotal role in enabling large language model~(LLM)-based\nagents to engage in complex and long-term interactions, such as question\nanswering (QA) and dialogue systems. While various memory modules have been\nproposed for these tasks, the impact of different memory structures across\ntasks remains insufficiently explored. This paper investigates how memory\nstructures and memory retrieval methods affect the performance of LLM-based\nagents. Specifically, we evaluate four types of memory structures, including\nchunks, knowledge triples, atomic facts, and summaries, along with mixed memory\nthat combines these components. In addition, we evaluate three widely used\nmemory retrieval methods: single-step retrieval, reranking, and iterative\nretrieval. Extensive experiments conducted across four tasks and six datasets\nyield the following key insights: (1) Different memory structures offer\ndistinct advantages, enabling them to be tailored to specific tasks; (2) Mixed\nmemory structures demonstrate remarkable resilience in noisy environments; (3)\nIterative retrieval consistently outperforms other methods across various\nscenarios. Our investigation aims to inspire further research into the design\nof memory systems for LLM-based agents.", "published": "2024-12-17 04:30:00", "link": "http://arxiv.org/abs/2412.15266v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic\n  Knowledge Graph", "abstract": "The rapid growth of social media platforms has raised significant concerns\nregarding online content toxicity. When Large Language Models (LLMs) are used\nfor toxicity detection, two key challenges emerge: 1) the absence of\ndomain-specific toxic knowledge leads to false negatives; 2) the excessive\nsensitivity of LLMs to toxic speech results in false positives, limiting\nfreedom of speech. To address these issues, we propose a novel method called\nMetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance\nhatred and toxicity detection. First, we construct a comprehensive meta-toxic\nknowledge graph by utilizing LLMs to extract toxic information through a\nthree-step pipeline, with toxic benchmark datasets serving as corpora. Second,\nwe query the graph via retrieval and ranking processes to supplement accurate,\nrelevant toxic knowledge. Extensive experiments and in-depth case studies\nacross multiple datasets demonstrate that our MetaTox significantly decreases\nthe false positive rate while boosting overall toxicity detection performance.\nOur code will be available soon.", "published": "2024-12-17 06:28:28", "link": "http://arxiv.org/abs/2412.15268v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A MapReduce Approach to Effectively Utilize Long Context Information in\n  Retrieval Augmented Language Models", "abstract": "While holding great promise for improving and facilitating healthcare, large\nlanguage models (LLMs) struggle to produce up-to-date responses on evolving\ntopics due to outdated knowledge or hallucination. Retrieval-augmented\ngeneration (RAG) is a pivotal innovation that improves the accuracy and\nrelevance of LLM responses by integrating LLMs with a search engine and\nexternal sources of knowledge. However, the quality of RAG responses can be\nlargely impacted by the rank and density of key information in the retrieval\nresults, such as the \"lost-in-the-middle\" problem. In this work, we aim to\nimprove the robustness and reliability of the RAG workflow in the medical\ndomain. Specifically, we propose a map-reduce strategy, BriefContext, to combat\nthe \"lost-in-the-middle\" issue without modifying the model weights. We\ndemonstrated the advantage of the workflow with various LLM backbones and on\nmultiple QA datasets. This method promises to improve the safety and\nreliability of LLMs deployed in healthcare domains.", "published": "2024-12-17 11:18:14", "link": "http://arxiv.org/abs/2412.15271v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Do Voters Get the Information They Want? Understanding Authentic Voter\n  FAQs in the US and How to Improve for Informed Electoral Participation", "abstract": "Accurate information is crucial for democracy as it empowers voters to make\ninformed decisions about their representatives and keeping them accountable. In\nthe US, state election commissions (SECs), often required by law, are the\nprimary providers of Frequently Asked Questions (FAQs) to voters, and secondary\nsources like non-profits such as League of Women Voters (LWV) try to complement\ntheir information shortfall. However, surprisingly, to the best of our\nknowledge, there is neither a single source with comprehensive FAQs nor a study\nanalyzing the data at national level to identify current practices and ways to\nimprove the status quo. This paper addresses it by providing the {\\bf first\ndataset on Voter FAQs covering all the US states}. Second, we introduce metrics\nfor FAQ information quality (FIQ) with respect to questions, answers, and\nanswers to corresponding questions. Third, we use FIQs to analyze US FAQs to\nidentify leading, mainstream and lagging content practices and corresponding\nstates. Finally, we identify what states across the spectrum can do to improve\nFAQ quality and thus, the overall information ecosystem. Across all 50 U.S.\nstates, 12% were identified as leaders and 8% as laggards for\nFIQS\\textsubscript{voter}, while 14% were leaders and 12% laggards for\nFIQS\\textsubscript{developer}.", "published": "2024-12-17 16:29:53", "link": "http://arxiv.org/abs/2412.15273v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Memory-Augmented Agent Training for Business Document Understanding", "abstract": "Traditional enterprises face significant challenges in processing business\ndocuments, where tasks like extracting transport references from invoices\nremain largely manual despite their crucial role in logistics operations. While\nLarge Language Models offer potential automation, their direct application to\nspecialized business domains often yields unsatisfactory results. We introduce\nMatrix (Memory-Augmented agent Training through Reasoning and Iterative\neXploration), a novel paradigm that enables LLM agents to progressively build\ndomain expertise through experience-driven memory refinement and iterative\nlearning. To validate this approach, we collaborate with one of the world's\nlargest logistics companies to create a dataset of Universal Business Language\nformat invoice documents, focusing on the task of transport reference\nextraction. Experiments demonstrate that Matrix outperforms prompting a single\nLLM by 30.3%, vanilla LLM agent by 35.2%. We further analyze the metrics of the\noptimized systems and observe that the agent system requires less API calls,\nfewer costs and can analyze longer documents on average. Our methods establish\na new approach to transform general-purpose LLMs into specialized business\ntools through systematic memory enhancement in document processing tasks.", "published": "2024-12-17 18:35:04", "link": "http://arxiv.org/abs/2412.15274v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "V-SQL: A View-based Two-stage Text-to-SQL Framework", "abstract": "The text-to-SQL task aims to convert natural language into Structured Query\nLanguage (SQL) without bias. Recently, text-to-SQL methods based on large\nlanguage models (LLMs) have garnered significant attention. The core of\nmainstream text-to-SQL frameworks is schema linking, which aligns user queries\nwith relevant tables and columns in the database. Previous methods focused on\nschema linking while neglecting to enhance LLMs' understanding of database\nschema. The complex coupling relationships between tables in the database\nconstrain the SQL generation capabilities of LLMs. To tackle this issue, this\npaper proposes a simple yet effective strategy called view-based schema. This\nstrategy aids LLMs in understanding the database schema by decoupling tightly\ncoupled tables into low-coupling views. We then introduce V-SQL, a view-based\ntwo-stage text-to-SQL framework. V-SQL involves the view-based schema strategy\nto enhance LLMs' understanding of database schema. Results on the authoritative\ndatasets Bird indicate that V-SQL achieves competitive performance compared to\nexisting state-of-the-art methods.", "published": "2024-12-17 02:27:50", "link": "http://arxiv.org/abs/2502.15686v1", "categories": ["cs.DB", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Momentum Posterior Regularization for Multi-hop Dense Retrieval", "abstract": "Multi-hop question answering (QA) often requires sequential retrieval\n(multi-hop retrieval), where each hop retrieves missing knowledge based on\ninformation from previous hops. To facilitate more effective retrieval, we aim\nto distill knowledge from a posterior retrieval, which has access to posterior\ninformation like an answer, into a prior retrieval used during inference when\nsuch information is unavailable. Unfortunately, current methods for knowledge\ndistillation in one-time retrieval are ineffective for multi-hop QA due to two\nissues: 1) Posterior information is often defined as the response (i.e. the\nanswer), which may not clearly connect to the query without intermediate\nretrieval; and 2) The large knowledge gap between prior and posterior\nretrievals makes existing distillation methods unstable, even resulting in\nperformance loss. As such, we propose MoPo (Momentum Posterior Regularization)\nwith two key innovations: 1) Posterior information of one hop is defined as a\nquery-focus summary from the golden knowledge of the previous and current hops;\n2) We develop an effective training strategy where the posterior retrieval is\nupdated along with the prior retrieval via momentum moving average method,\nallowing smoother and effective distillation. Experiments on HotpotQA and\nStrategyQA demonstrate that MoPo outperforms existing baselines in both\nretrieval and downstream QA tasks.", "published": "2024-12-17 05:27:08", "link": "http://arxiv.org/abs/2502.20399v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Refining Dimensions for Improving Clustering-based Cross-lingual Topic\n  Models", "abstract": "Recent works in clustering-based topic models perform well in monolingual\ntopic identification by introducing a pipeline to cluster the contextualized\nrepresentations. However, the pipeline is suboptimal in identifying topics\nacross languages due to the presence of language-dependent dimensions (LDDs)\ngenerated by multilingual language models. To address this issue, we introduce\na novel, SVD-based dimension refinement component into the pipeline of the\nclustering-based topic model. This component effectively neutralizes the\nnegative impact of LDDs, enabling the model to accurately identify topics\nacross languages. Our experiments on three datasets demonstrate that the\nupdated pipeline with the dimension refinement component generally outperforms\nother state-of-the-art cross-lingual topic models.", "published": "2024-12-17 00:50:23", "link": "http://arxiv.org/abs/2412.12433v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PERC: Plan-As-Query Example Retrieval for Underrepresented Code\n  Generation", "abstract": "Code generation with large language models has shown significant promise,\nespecially when employing retrieval-augmented generation (RAG) with few-shot\nexamples. However, selecting effective examples that enhance generation quality\nremains a challenging task, particularly when the target programming language\n(PL) is underrepresented. In this study, we present two key findings: (1)\nretrieving examples whose presented algorithmic plans can be referenced for\ngenerating the desired behavior significantly improves generation accuracy, and\n(2) converting code into pseudocode effectively captures such algorithmic\nplans, enhancing retrieval quality even when the source and the target PLs are\ndifferent. Based on these findings, we propose Plan-as-query Example Retrieval\nfor few-shot prompting in Code generation (PERC), a novel framework that\nutilizes algorithmic plans to identify and retrieve effective examples. We\nvalidate the effectiveness of PERC through extensive experiments on the\nCodeContests, HumanEval and MultiPL-E benchmarks: PERC consistently outperforms\nthe state-of-the-art RAG methods in code generation, both when the source and\ntarget programming languages match or differ, highlighting its adaptability and\nrobustness in diverse coding environments.", "published": "2024-12-17 01:23:45", "link": "http://arxiv.org/abs/2412.12447v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Graph Learning in the Era of LLMs: A Survey from the Perspective of\n  Data, Models, and Tasks", "abstract": "With the increasing prevalence of cross-domain Text-Attributed Graph (TAG)\nData (e.g., citation networks, recommendation systems, social networks, and\nai4science), the integration of Graph Neural Networks (GNNs) and Large Language\nModels (LLMs) into a unified Model architecture (e.g., LLM as enhancer, LLM as\ncollaborators, LLM as predictor) has emerged as a promising technological\nparadigm. The core of this new graph learning paradigm lies in the synergistic\ncombination of GNNs' ability to capture complex structural relationships and\nLLMs' proficiency in understanding informative contexts from the rich textual\ndescriptions of graphs. Therefore, we can leverage graph description texts with\nrich semantic context to fundamentally enhance Data quality, thereby improving\nthe representational capacity of model-centric approaches in line with\ndata-centric machine learning principles. By leveraging the strengths of these\ndistinct neural network architectures, this integrated approach addresses a\nwide range of TAG-based Task (e.g., graph learning, graph reasoning, and graph\nquestion answering), particularly in complex industrial scenarios (e.g.,\nsupervised, few-shot, and zero-shot settings). In other words, we can treat\ntext as a medium to enable cross-domain generalization of graph learning Model,\nallowing a single graph model to effectively handle the diversity of downstream\ngraph-based Task across different data domains. This work serves as a\nfoundational reference for researchers and practitioners looking to advance\ngraph learning methodologies in the rapidly evolving landscape of LLM. We\nconsistently maintain the related open-source materials at\n\\url{https://github.com/xkLi-Allen/Awesome-GNN-in-LLMs-Papers}.", "published": "2024-12-17 01:41:17", "link": "http://arxiv.org/abs/2412.12456v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DB"], "primary_category": "cs.LG"}
{"title": "LITA: An Efficient LLM-assisted Iterative Topic Augmentation Framework", "abstract": "Topic modeling is widely used for uncovering thematic structures within text\ncorpora, yet traditional models often struggle with specificity and coherence\nin domain-focused applications. Guided approaches, such as SeededLDA and CorEx,\nincorporate user-provided seed words to improve relevance but remain\nlabor-intensive and static. Large language models (LLMs) offer potential for\ndynamic topic refinement and discovery, yet their application often incurs high\nAPI costs. To address these challenges, we propose the LLM-assisted Iterative\nTopic Augmentation framework (LITA), an LLM-assisted approach that integrates\nuser-provided seeds with embedding-based clustering and iterative refinement.\nLITA identifies a small number of ambiguous documents and employs an LLM to\nreassign them to existing or new topics, minimizing API costs while enhancing\ntopic quality. Experiments on two datasets across topic quality and clustering\nperformance metrics demonstrate that LITA outperforms five baseline models,\nincluding LDA, SeededLDA, CorEx, BERTopic, and PromptTopic. Our work offers an\nefficient and adaptable framework for advancing topic modeling and text\nclustering.", "published": "2024-12-17 01:43:44", "link": "http://arxiv.org/abs/2412.12459v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Human-in-the-Loop Generation of Adversarial Texts: A Case Study on\n  Tibetan Script", "abstract": "DNN-based language models perform excellently on various tasks, but even SOTA\nLLMs are susceptible to textual adversarial attacks. Adversarial texts play\ncrucial roles in multiple subfields of NLP. However, current research has the\nfollowing issues. (1) Most textual adversarial attack methods target\nrich-resourced languages. How do we generate adversarial texts for less-studied\nlanguages? (2) Most textual adversarial attack methods are prone to generating\ninvalid or ambiguous adversarial texts. How do we construct high-quality\nadversarial robustness benchmarks? (3) New language models may be immune to\npart of previously generated adversarial texts. How do we update adversarial\nrobustness benchmarks? To address the above issues, we introduce HITL-GAT, a\nsystem based on a general approach to human-in-the-loop generation of\nadversarial texts. HITL-GAT contains four stages in one pipeline: victim model\nconstruction, adversarial example generation, high-quality benchmark\nconstruction, and adversarial robustness evaluation. Additionally, we utilize\nHITL-GAT to make a case study on Tibetan script which can be a reference for\nthe adversarial research of other less-studied languages.", "published": "2024-12-17 02:29:54", "link": "http://arxiv.org/abs/2412.12478v3", "categories": ["cs.CL", "cs.CR", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Boosting Long-Context Management via Query-Guided Activation Refilling", "abstract": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.", "published": "2024-12-17 02:43:54", "link": "http://arxiv.org/abs/2412.12486v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Unleashing the Potential of Model Bias for Generalized Category\n  Discovery", "abstract": "Generalized Category Discovery is a significant and complex task that aims to\nidentify both known and undefined novel categories from a set of unlabeled\ndata, leveraging another labeled dataset containing only known categories. The\nprimary challenges stem from model bias induced by pre-training on only known\ncategories and the lack of precise supervision for novel ones, leading to\ncategory bias towards known categories and category confusion among different\nnovel categories, which hinders models' ability to identify novel categories\neffectively. To address these challenges, we propose a novel framework named\nSelf-Debiasing Calibration (SDC). Unlike prior methods that regard model bias\ntowards known categories as an obstacle to novel category identification, SDC\nprovides a novel insight into unleashing the potential of the bias to\nfacilitate novel category learning. Specifically, the output of the biased\nmodel serves two key purposes. First, it provides an accurate modeling of\ncategory bias, which can be utilized to measure the degree of bias and debias\nthe output of the current training model. Second, it offers valuable insights\nfor distinguishing different novel categories by transferring knowledge between\nsimilar categories. Based on these insights, SDC dynamically adjusts the output\nlogits of the current training model using the output of the biased model. This\napproach produces less biased logits to effectively address the issue of\ncategory bias towards known categories, and generates more accurate pseudo\nlabels for unlabeled data, thereby mitigating category confusion for novel\ncategories. Experiments on three benchmark datasets show that SDC outperforms\nSOTA methods, especially in the identification of novel categories. Our code\nand data are available at \\url{https://github.com/Lackel/SDC}.", "published": "2024-12-17 03:05:27", "link": "http://arxiv.org/abs/2412.12501v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "EXIT: Context-Aware Extractive Compression for Enhancing\n  Retrieval-Augmented Generation", "abstract": "We introduce EXIT, an extractive context compression framework that enhances\nboth the effectiveness and efficiency of retrieval-augmented generation (RAG)\nin question answering (QA). Current RAG systems often struggle when retrieval\nmodels fail to rank the most relevant documents, leading to the inclusion of\nmore context at the expense of latency and accuracy. While abstractive\ncompression methods can drastically reduce token counts, their token-by-token\ngeneration process significantly increases end-to-end latency. Conversely,\nexisting extractive methods reduce latency but rely on independent,\nnon-adaptive sentence selection, failing to fully utilize contextual\ninformation. EXIT addresses these limitations by classifying sentences from\nretrieved documents - while preserving their contextual dependencies - enabling\nparallelizable, context-aware extraction that adapts to query complexity and\nretrieval quality. Our evaluations on both single-hop and multi-hop QA tasks\nshow that EXIT consistently surpasses existing compression methods and even\nuncompressed baselines in QA accuracy, while also delivering substantial\nreductions in inference time and token count. By improving both effectiveness\nand efficiency, EXIT provides a promising direction for developing scalable,\nhigh-quality QA solutions in RAG pipelines. Our code is available at\nhttps://github.com/ThisIsHwang/EXIT", "published": "2024-12-17 05:38:27", "link": "http://arxiv.org/abs/2412.12559v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Multi-Dimensional Insights: Benchmarking Real-World Personalization in\n  Large Multimodal Models", "abstract": "The rapidly developing field of large multimodal models (LMMs) has led to the\nemergence of diverse models with remarkable capabilities. However, existing\nbenchmarks fail to comprehensively, objectively and accurately evaluate whether\nLMMs align with the diverse needs of humans in real-world scenarios. To bridge\nthis gap, we propose the Multi-Dimensional Insights (MDI) benchmark, which\nincludes over 500 images covering six common scenarios of human life. Notably,\nthe MDI-Benchmark offers two significant advantages over existing evaluations:\n(1) Each image is accompanied by two types of questions: simple questions to\nassess the model's understanding of the image, and complex questions to\nevaluate the model's ability to analyze and reason beyond basic content. (2)\nRecognizing that people of different age groups have varying needs and\nperspectives when faced with the same scenario, our benchmark stratifies\nquestions into three age categories: young people, middle-aged people, and\nolder people. This design allows for a detailed assessment of LMMs'\ncapabilities in meeting the preferences and needs of different age groups. With\nMDI-Benchmark, the strong model like GPT-4o achieve 79% accuracy on age-related\ntasks, indicating that existing LMMs still have considerable room for\nimprovement in addressing real-world applications. Looking ahead, we anticipate\nthat the MDI-Benchmark will open new pathways for aligning real-world\npersonalization in LMMs. The MDI-Benchmark data and evaluation code are\navailable at https://mdi-benchmark.github.io/", "published": "2024-12-17 07:06:10", "link": "http://arxiv.org/abs/2412.12606v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Auto-Cypher: Improving LLMs on Cypher generation via LLM-supervised\n  generation-verification framework", "abstract": "Graph databases like Neo4j are gaining popularity for handling complex,\ninterconnected data, over traditional relational databases in modeling and\nquerying relationships. While translating natural language into SQL queries is\nwell-researched, generating Cypher queries for Neo4j remains relatively\nunderexplored. In this work, we present an automated, LLM-Supervised, pipeline\nto generate high-quality synthetic data for Text2Cypher. Our Cypher data\ngeneration pipeline introduces LLM-As-Database-Filler, a novel strategy for\nensuring Cypher query correctness, thus resulting in high quality generations.\nUsing our pipeline, we generate high quality Text2Cypher data - SynthCypher\ncontaining 29.8k instances across various domains and queries with varying\ncomplexities. Training open-source LLMs like LLaMa-3.1-8B, Mistral-7B, and\nQWEN-7B on SynthCypher results in performance gains of up to 40% on the\nText2Cypher test split and 30% on the SPIDER benchmark, adapted for graph\ndatabases.", "published": "2024-12-17 07:21:25", "link": "http://arxiv.org/abs/2412.12612v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MedMax: Mixed-Modal Instruction Tuning for Training Biomedical\n  Assistants", "abstract": "Recent advancements in mixed-modal generative models have enabled flexible\nintegration of information across image-text content. These models have opened\nnew avenues for developing unified biomedical assistants capable of analyzing\nbiomedical images, answering complex questions about them, and predicting the\nimpact of medical procedures on a patient's health. However, existing resources\nface challenges such as limited data availability, narrow domain coverage, and\nrestricted sources (e.g., medical papers). To address these gaps, we present\nMedMax, the first large-scale multimodal biomedical instruction-tuning dataset\nfor mixed-modal foundation models. With 1.47 million instances, MedMax\nencompasses a diverse range of tasks, including multimodal content generation\n(interleaved image-text data), biomedical image captioning and generation,\nvisual chatting, and report understanding. These tasks span diverse medical\ndomains such as radiology and histopathology. Subsequently, we fine-tune a\nmixed-modal foundation model on the MedMax dataset, achieving significant\nperformance improvements: a 26% gain over the Chameleon model and an 18.3%\nimprovement over GPT-4o across 12 downstream biomedical visual\nquestion-answering tasks. Additionally, we introduce a unified evaluation suite\nfor biomedical tasks, providing a robust framework to guide the development of\nnext-generation mixed-modal biomedical AI assistants.", "published": "2024-12-17 08:30:00", "link": "http://arxiv.org/abs/2412.12661v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "GIRAFFE: Design Choices for Extending the Context Length of Visual\n  Language Models", "abstract": "Visual Language Models (VLMs) demonstrate impressive capabilities in\nprocessing multimodal inputs, yet applications such as visual agents, which\nrequire handling multiple images and high-resolution videos, demand enhanced\nlong-range modeling. Moreover, existing open-source VLMs lack systematic\nexploration into extending their context length, and commercial models often\nprovide limited details. To tackle this, we aim to establish an effective\nsolution that enhances long context performance of VLMs while preserving their\ncapacities in short context scenarios. Towards this goal, we make the best\ndesign choice through extensive experiment settings from data curation to\ncontext window extending and utilizing: (1) we analyze data sources and length\ndistributions to construct ETVLM - a data recipe to balance the performance\nacross scenarios; (2) we examine existing position extending methods, identify\ntheir limitations and propose M-RoPE++ as an enhanced approach; we also choose\nto solely instruction-tune the backbone with mixed-source data; (3) we discuss\nhow to better utilize extended context windows and propose hybrid-resolution\ntraining. Built on the Qwen-VL series model, we propose Giraffe, which is\neffectively extended to 128K lengths. Evaluated on extensive long context VLM\nbenchmarks such as VideoMME and Viusal Haystacks, our Giraffe achieves\nstate-of-the-art performance among similarly sized open-source long VLMs and is\ncompetitive with commercial model GPT-4V. We will open-source the code, data,\nand models.", "published": "2024-12-17 09:57:21", "link": "http://arxiv.org/abs/2412.12735v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Your Next State-of-the-Art Could Come from Another Domain: A\n  Cross-Domain Analysis of Hierarchical Text Classification", "abstract": "Text classification with hierarchical labels is a prevalent and challenging\ntask in natural language processing. Examples include assigning ICD codes to\npatient records, tagging patents into IPC classes, assigning EUROVOC\ndescriptors to European legal texts, and more. Despite its widespread\napplications, a comprehensive understanding of state-of-the-art methods across\ndifferent domains has been lacking. In this paper, we provide the first\ncomprehensive cross-domain overview with empirical analysis of state-of-the-art\nmethods. We propose a unified framework that positions each method within a\ncommon structure to facilitate research. Our empirical analysis yields key\ninsights and guidelines, confirming the necessity of learning across different\nresearch areas to design effective methods. Notably, under our unified\nevaluation pipeline, we achieved new state-of-the-art results by applying\ntechniques beyond their original domains.", "published": "2024-12-17 10:08:57", "link": "http://arxiv.org/abs/2412.12744v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Selective Shot Learning for Code Explanation", "abstract": "Code explanation plays a crucial role in the software engineering domain,\naiding developers in grasping code functionality efficiently. Recent work shows\nthat the performance of LLMs for code explanation improves in a few-shot\nsetting, especially when the few-shot examples are selected intelligently.\nState-of-the-art approaches for such Selective Shot Learning (SSL) include\ntoken-based and embedding-based methods. However, these SSL approaches have\nbeen evaluated on proprietary LLMs, without much exploration on open-source\nCode-LLMs. Additionally, these methods lack consideration for programming\nlanguage syntax. To bridge these gaps, we present a comparative study and\npropose a novel SSL method (SSL_ner) that utilizes entity information for\nfew-shot example selection. We present several insights and show the\neffectiveness of SSL_ner approach over state-of-the-art methods across two\ndatasets. To the best of our knowledge, this is the first systematic\nbenchmarking of open-source Code-LLMs while assessing the performances of the\nvarious few-shot examples selection approaches for the code explanation task.", "published": "2024-12-17 12:26:14", "link": "http://arxiv.org/abs/2412.12852v1", "categories": ["cs.SE", "cs.CL", "cs.IR"], "primary_category": "cs.SE"}
{"title": "An Agentic Approach to Automatic Creation of P&ID Diagrams from Natural\n  Language Descriptions", "abstract": "The Piping and Instrumentation Diagrams (P&IDs) are foundational to the\ndesign, construction, and operation of workflows in the engineering and process\nindustries. However, their manual creation is often labor-intensive,\nerror-prone, and lacks robust mechanisms for error detection and correction.\nWhile recent advancements in Generative AI, particularly Large Language Models\n(LLMs) and Vision-Language Models (VLMs), have demonstrated significant\npotential across various domains, their application in automating generation of\nengineering workflows remains underexplored. In this work, we introduce a novel\ncopilot for automating the generation of P&IDs from natural language\ndescriptions. Leveraging a multi-step agentic workflow, our copilot provides a\nstructured and iterative approach to diagram creation directly from Natural\nLanguage prompts. We demonstrate the feasibility of the generation process by\nevaluating the soundness and completeness of the workflow, and show improved\nresults compared to vanilla zero-shot and few-shot generation approaches.", "published": "2024-12-17 13:21:26", "link": "http://arxiv.org/abs/2412.12898v1", "categories": ["cs.LG", "cs.CE", "cs.CL", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Enabling Low-Resource Language Retrieval: Establishing Baselines for\n  Urdu MS MARCO", "abstract": "As the Information Retrieval (IR) field increasingly recognizes the\nimportance of inclusivity, addressing the needs of low-resource languages\nremains a significant challenge. This paper introduces the first large-scale\nUrdu IR dataset, created by translating the MS MARCO dataset through machine\ntranslation. We establish baseline results through zero-shot learning for IR in\nUrdu and subsequently apply the mMARCO multilingual IR methodology to this\nnewly translated dataset. Our findings demonstrate that the fine-tuned model\n(Urdu-mT5-mMARCO) achieves a Mean Reciprocal Rank (MRR@10) of 0.247 and a\nRecall@10 of 0.439, representing significant improvements over zero-shot\nresults and showing the potential for expanding IR access for Urdu speakers. By\nbridging access gaps for speakers of low-resource languages, this work not only\nadvances multilingual IR research but also emphasizes the ethical and societal\nimportance of inclusive IR technologies. This work provides valuable insights\ninto the challenges and solutions for improving language representation and\nlays the groundwork for future research, especially in South Asian languages,\nwhich can benefit from the adaptable methods used in this study.", "published": "2024-12-17 15:21:28", "link": "http://arxiv.org/abs/2412.12997v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "CLASP: Contrastive Language-Speech Pretraining for Multilingual\n  Multimodal Information Retrieval", "abstract": "This study introduces CLASP (Contrastive Language-Speech Pretraining), a\nmultilingual, multimodal representation tailored for audio-text information\nretrieval. CLASP leverages the synergy between spoken content and textual data.\nDuring training, we utilize our newly introduced speech-text dataset, which\nencompasses 15 diverse categories ranging from fiction to religion. CLASP's\naudio component integrates audio spectrograms with a pre-trained\nself-supervised speech model, while its language encoding counterpart employs a\nsentence encoder pre-trained on over 100 languages. This unified lightweight\nmodel bridges the gap between various modalities and languages, enhancing its\neffectiveness in handling and retrieving multilingual and multimodal data. Our\nevaluations across multiple languages demonstrate that CLASP establishes new\nbenchmarks in HITS@1, MRR, and meanR metrics, outperforming traditional\nASR-based retrieval methods that rely on transcribing speech into text for\nsubsequent text retrieval, especially in specific scenarios.", "published": "2024-12-17 16:38:10", "link": "http://arxiv.org/abs/2412.13071v2", "categories": ["cs.CL", "cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "BanglishRev: A Large-Scale Bangla-English and Code-mixed Dataset of\n  Product Reviews in E-Commerce", "abstract": "This work presents the BanglishRev Dataset, the largest e-commerce product\nreview dataset to date for reviews written in Bengali, English, a mixture of\nboth and Banglish, Bengali words written with English alphabets. The dataset\ncomprises of 1.74 million written reviews from 3.2 million ratings information\ncollected from a total of 128k products being sold in online e-commerce\nplatforms targeting the Bengali population. It includes an extensive array of\nrelated metadata for each of the reviews including the rating given by the\nreviewer, date the review was posted and date of purchase, number of likes,\ndislikes, response from the seller, images associated with the review etc. With\nsentiment analysis being the most prominent usage of review datasets,\nexperimentation with a binary sentiment analysis model with the review rating\nserving as an indicator of positive or negative sentiment was conducted to\nevaluate the effectiveness of the large amount of data presented in BanglishRev\nfor sentiment analysis tasks. A BanglishBERT model is trained on the data from\nBanglishRev with reviews being considered labeled positive if the rating is\ngreater than 3 and negative if the rating is less than or equal to 3. The model\nis evaluated by being testing against a previously published manually annotated\ndataset for e-commerce reviews written in a mixture of Bangla, English and\nBanglish. The experimental model achieved an exceptional accuracy of 94\\% and\nF1 score of 0.94, demonstrating the dataset's efficacy for sentiment analysis.\nSome of the intriguing patterns and observations seen within the dataset and\nfuture research directions where the dataset can be utilized is also discussed\nand explored. The dataset can be accessed through\nhttps://huggingface.co/datasets/BanglishRev/bangla-english-and-code-mixed-ecommerce-review-dataset.", "published": "2024-12-17 18:39:10", "link": "http://arxiv.org/abs/2412.13161v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Toxicity Detection towards Adaptability to Changing Perturbations", "abstract": "Toxicity detection is crucial for maintaining the peace of the society. While\nexisting methods perform well on normal toxic contents or those generated by\nspecific perturbation methods, they are vulnerable to evolving perturbation\npatterns. However, in real-world scenarios, malicious users tend to create new\nperturbation patterns for fooling the detectors. For example, some users may\ncircumvent the detector of large language models (LLMs) by adding `I am a\nscientist' at the beginning of the prompt. In this paper, we introduce a novel\nproblem, i.e., continual learning jailbreak perturbation patterns, into the\ntoxicity detection field. To tackle this problem, we first construct a new\ndataset generated by 9 types of perturbation patterns, 7 of them are summarized\nfrom prior work and 2 of them are developed by us. We then systematically\nvalidate the vulnerability of current methods on this new perturbation\npattern-aware dataset via both the zero-shot and fine tuned cross-pattern\ndetection. Upon this, we present the domain incremental learning paradigm and\nthe corresponding benchmark to ensure the detector's robustness to dynamically\nemerging types of perturbed toxic text. Our code and dataset are provided in\nthe appendix and will be publicly available at GitHub, by which we wish to\noffer new research opportunities for the security-relevant communities.", "published": "2024-12-17 05:04:57", "link": "http://arxiv.org/abs/2412.15267v3", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "The Reliability Paradox: Exploring How Shortcut Learning Undermines\n  Language Model Calibration", "abstract": "The advent of pre-trained language models (PLMs) has enabled significant\nperformance gains in the field of natural language processing. However, recent\nstudies have found PLMs to suffer from miscalibration, indicating a lack of\naccuracy in the confidence estimates provided by these models. Current\nevaluation methods for PLM calibration often assume that lower calibration\nerror estimates indicate more reliable predictions. However, fine-tuned PLMs\noften resort to shortcuts, leading to overconfident predictions that create the\nillusion of enhanced performance but lack generalizability in their decision\nrules. The relationship between PLM reliability, as measured by calibration\nerror, and shortcut learning, has not been thoroughly explored thus far. This\npaper aims to investigate this relationship, studying whether lower calibration\nerror implies reliable decision rules for a language model. Our findings reveal\nthat models with seemingly superior calibration portray higher levels of\nnon-generalizable decision rules. This challenges the prevailing notion that\nwell-calibrated models are inherently reliable. Our study highlights the need\nto bridge the current gap between language model calibration and generalization\nobjectives, urging the development of comprehensive frameworks to achieve truly\nrobust and reliable language models.", "published": "2024-12-17 08:04:28", "link": "http://arxiv.org/abs/2412.15269v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SimGRAG: Leveraging Similar Subgraphs for Knowledge Graphs Driven\n  Retrieval-Augmented Generation", "abstract": "Recent advancements in large language models (LLMs) have shown impressive\nversatility across various tasks. To eliminate its hallucinations,\nretrieval-augmented generation (RAG) has emerged as a powerful approach,\nleveraging external knowledge sources like knowledge graphs (KGs). In this\npaper, we study the task of KG-driven RAG and propose a novel Similar Graph\nEnhanced Retrieval-Augmented Generation (SimGRAG) method. It effectively\naddresses the challenge of aligning query texts and KG structures through a\ntwo-stage process: (1) query-to-pattern, which uses an LLM to transform queries\ninto a desired graph pattern, and (2) pattern-to-subgraph, which quantifies the\nalignment between the pattern and candidate subgraphs using a graph semantic\ndistance (GSD) metric. We also develop an optimized retrieval algorithm that\nefficiently identifies the top-$k$ subgraphs within 1-second latency on a\n10-million-scale KG. Extensive experiments show that SimGRAG outperforms\nstate-of-the-art KG-driven RAG methods in both question answering and fact\nverification, offering superior plug-and-play usability and scalability.", "published": "2024-12-17 15:40:08", "link": "http://arxiv.org/abs/2412.15272v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Fooling LLM graders into giving better grades through neural activity\n  guided adversarial prompting", "abstract": "The deployment of artificial intelligence (AI) in critical decision-making\nand evaluation processes raises concerns about inherent biases that malicious\nactors could exploit to distort decision outcomes. We propose a systematic\nmethod to reveal such biases in AI evaluation systems and apply it to automated\nessay grading as an example. Our approach first identifies hidden neural\nactivity patterns that predict distorted decision outcomes and then optimizes\nan adversarial input suffix to amplify such patterns. We demonstrate that this\ncombination can effectively fool large language model (LLM) graders into\nassigning much higher grades than humans would. We further show that this\nwhite-box attack transfers to black-box attacks on other models, including\ncommercial closed-source models like Gemini. They further reveal the existence\nof a \"magic word\" that plays a pivotal role in the efficacy of the attack. We\ntrace the origin of this magic word bias to the structure of commonly-used chat\ntemplates for supervised fine-tuning of LLMs and show that a minor change in\nthe template can drastically reduce the bias. This work not only uncovers\nvulnerabilities in current LLMs but also proposes a systematic method to\nidentify and remove hidden biases, contributing to the goal of ensuring AI\nsafety and security.", "published": "2024-12-17 19:08:22", "link": "http://arxiv.org/abs/2412.15275v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Is Your World Simulator a Good Story Presenter? A Consecutive\n  Events-Based Benchmark for Future Long Video Generation", "abstract": "The current state-of-the-art video generative models can produce\ncommercial-grade videos with highly realistic details. However, they still\nstruggle to coherently present multiple sequential events in the stories\nspecified by the prompts, which is foreseeable an essential capability for\nfuture long video generation scenarios. For example, top T2V generative models\nstill fail to generate a video of the short simple story 'how to put an\nelephant into a refrigerator.' While existing detail-oriented benchmarks\nprimarily focus on fine-grained metrics like aesthetic quality and\nspatial-temporal consistency, they fall short of evaluating models' abilities\nto handle event-level story presentation. To address this gap, we introduce\nStoryEval, a story-oriented benchmark specifically designed to assess\ntext-to-video (T2V) models' story-completion capabilities. StoryEval features\n423 prompts spanning 7 classes, each representing short stories composed of 2-4\nconsecutive events. We employ advanced vision-language models, such as GPT-4V\nand LLaVA-OV-Chat-72B, to verify the completion of each event in the generated\nvideos, applying a unanimous voting method to enhance reliability. Our methods\nensure high alignment with human evaluations, and the evaluation of 11 models\nreveals its challenge, with none exceeding an average story-completion rate of\n50%. StoryEval provides a new benchmark for advancing T2V models and highlights\nthe challenges and opportunities in developing next-generation solutions for\ncoherent story-driven video generation.", "published": "2024-12-17 23:00:42", "link": "http://arxiv.org/abs/2412.16211v1", "categories": ["cs.CV", "cs.CL", "cs.GR"], "primary_category": "cs.CV"}
{"title": "Baichuan4-Finance Technical Report", "abstract": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding, generation, and reasoning, yet their potential in\nfinance remains underexplored due to the complexity and specialization of\nfinancial knowledge. In this work, we report the development of the\nBaichuan4-Finance series, including a comprehensive suite of foundational\nBaichuan4-Finance-Base and an aligned language model Baichuan4-Finance, which\nare built upon Baichuan4-Turbo base model and tailored for finance domain.\nFirstly, we have dedicated significant effort to building a detailed pipeline\nfor improving data quality. Moreover, in the continual pre-training phase, we\npropose a novel domain self-constraint training strategy, which enables\nBaichuan4-Finance-Base to acquire financial knowledge without losing general\ncapabilities. After Supervised Fine-tuning and Reinforcement Learning from\nHuman Feedback and AI Feedback, the chat model Baichuan4-Finance is able to\ntackle various financial certification questions and real-world scenario\napplications. We evaluate Baichuan4-Finance on many widely used general\ndatasets and two holistic financial benchmarks. The evaluation results show\nthat Baichuan4-Finance-Base surpasses almost all competitive baselines on\nfinancial tasks by significant margins without sacrificing performance on\ngeneral LLM benchmarks. At the same time, Baichuan4-Finance demonstrates even\nmore impressive performance on financial application scenarios, showcasing its\npotential to foster community innovation in the financial LLM field.", "published": "2024-12-17 08:05:32", "link": "http://arxiv.org/abs/2412.15270v2", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Modality-Inconsistent Continual Learning of Multimodal Large Language\n  Models", "abstract": "In this paper, we introduce Modality-Inconsistent Continual Learning (MICL),\na new continual learning scenario for Multimodal Large Language Models (MLLMs)\nthat involves tasks with inconsistent modalities (image, audio, or video) and\nvarying task types (captioning or question-answering). Unlike existing\nvision-only or modality-incremental settings, MICL combines modality and task\ntype shifts, both of which drive catastrophic forgetting. To address these\nchallenges, we propose MoInCL, which employs a Pseudo Targets Generation Module\nto mitigate forgetting caused by task type shifts in previously seen\nmodalities. It also incorporates Instruction-based Knowledge Distillation to\npreserve the model's ability to handle previously learned modalities when new\nones are introduced. We benchmark MICL using a total of six tasks and conduct\nexperiments to validate the effectiveness of our proposed MoInCL. The\nexperimental results highlight the superiority of MoInCL, showing significant\nimprovements over representative and state-of-the-art continual learning\nbaselines.", "published": "2024-12-17 16:13:56", "link": "http://arxiv.org/abs/2412.13050v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Hierarchical Control of Emotion Rendering in Speech Synthesis", "abstract": "Emotional text-to-speech synthesis (TTS) aims to generate realistic emotional\nspeech from input text. However, quantitatively controlling multi-level emotion\nrendering remains challenging. In this paper, we propose a diffusion-based\nemotional TTS framework with a novel approach for emotion intensity modeling to\nfacilitate fine-grained control over emotion rendering at the phoneme, word,\nand utterance levels. We introduce a hierarchical emotion distribution (ED)\nextractor that captures a quantifiable ED embedding across different speech\nsegment levels. Additionally, we explore various acoustic features and assess\ntheir impact on emotion intensity modeling. During TTS training, the\nhierarchical ED embedding effectively captures the variance in emotion\nintensity from the reference audio and correlates it with linguistic and\nspeaker information. The TTS model not only generates emotional speech during\ninference, but also quantitatively controls the emotion rendering over the\nspeech constituents. Both objective and subjective evaluations demonstrate the\neffectiveness of our framework in terms of speech quality, emotional\nexpressiveness, and hierarchical emotion control.", "published": "2024-12-17 03:02:05", "link": "http://arxiv.org/abs/2412.12498v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Libri2Vox Dataset: Target Speaker Extraction with Diverse Speaker\n  Conditions and Synthetic Data", "abstract": "Target speaker extraction (TSE) is essential in speech processing\napplications, particularly in scenarios with complex acoustic environments.\nCurrent TSE systems face challenges in limited data diversity and a lack of\nrobustness in real-world conditions, primarily because they are trained on\nartificially mixed datasets with limited speaker variability and unrealistic\nnoise profiles. To address these challenges, we propose Libri2Vox, a new\ndataset that combines clean target speech from the LibriTTS dataset with\ninterference speech from the noisy VoxCeleb2 dataset, providing a large and\ndiverse set of speakers under realistic noisy conditions. We also augment\nLibri2Vox with synthetic speakers generated using state-of-the-art speech\ngenerative models to enhance speaker diversity. Additionally, to further\nimprove the effectiveness of incorporating synthetic data, curriculum learning\nis implemented to progressively train TSE models with increasing levels of\ndifficulty. Extensive experiments across multiple TSE architectures reveal\nvarying degrees of improvement, with SpeakerBeam demonstrating the most\nsubstantial gains: a 1.39 dB improvement in signal-to-distortion ratio (SDR) on\nthe Libri2Talker test set compared to baseline training. Building upon these\nresults, we further enhanced performance through our speaker similarity-based\ncurriculum learning approach with the Conformer architecture, achieving an\nadditional 0.78 dB improvement over conventional random sampling methods in\nwhich data samples are randomly selected from the entire dataset. These results\ndemonstrate the complementary benefits of diverse real-world data, synthetic\nspeaker augmentation, and structured training strategies in building robust TSE\nsystems.", "published": "2024-12-17 04:06:53", "link": "http://arxiv.org/abs/2412.12512v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NTC-KWS: Noise-aware CTC for Robust Keyword Spotting", "abstract": "In recent years, there has been a growing interest in designing\nsmall-footprint yet effective Connectionist Temporal Classification based\nkeyword spotting (CTC-KWS) systems. They are typically deployed on low-resource\ncomputing platforms, where limitations on model size and computational capacity\ncreate bottlenecks under complicated acoustic scenarios. Such constraints often\nresult in overfitting and confusion between keywords and background noise,\nleading to high false alarms. To address these issues, we propose a noise-aware\nCTC-based KWS (NTC-KWS) framework designed to enhance model robustness in noisy\nenvironments, particularly under extremely low signal-to-noise ratios. Our\napproach introduces two additional noise-modeling wildcard arcs into the\ntraining and decoding processes based on weighted finite state transducer\n(WFST) graphs: self-loop arcs to address noise insertion errors and bypass arcs\nto handle masking and interference caused by excessive noise. Experiments on\nclean and noisy Hey Snips show that NTC-KWS outperforms state-of-the-art (SOTA)\nend-to-end systems and CTC-KWS baselines across various acoustic conditions,\nwith particularly strong performance in low SNR scenarios.", "published": "2024-12-17 07:25:09", "link": "http://arxiv.org/abs/2412.12614v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Streaming Keyword Spotting Boosted by Cross-layer Discrimination\n  Consistency", "abstract": "Connectionist Temporal Classification (CTC), a non-autoregressive training\ncriterion, is widely used in online keyword spotting (KWS). However, existing\nCTC-based KWS decoding strategies either rely on Automatic Speech Recognition\n(ASR), which performs suboptimally due to its broad search over the acoustic\nspace without keyword-specific optimization, or on KWS-specific decoding\ngraphs, which are complex to implement and maintain. In this work, we propose a\nstreaming decoding algorithm enhanced by Cross-layer Discrimination Consistency\n(CDC), tailored for CTC-based KWS. Specifically, we introduce a streamlined yet\neffective decoding algorithm capable of detecting the start of the keyword at\nany arbitrary position. Furthermore, we leverage discrimination consistency\ninformation across layers to better differentiate between positive and false\nalarm samples. Our experiments on both clean and noisy Hey Snips datasets show\nthat the proposed streaming decoding strategy outperforms ASR-based and\ngraph-based KWS baselines. The CDC-boosted decoding further improves\nperformance, yielding an average absolute recall improvement of 6.8% and a\n46.3% relative reduction in the miss rate compared to the graph-based KWS\nbaseline, with a very low false alarm rate of 0.05 per hour.", "published": "2024-12-17 07:58:46", "link": "http://arxiv.org/abs/2412.12635v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CAMEL: Cross-Attention Enhanced Mixture-of-Experts and Language Bias for\n  Code-Switching Speech Recognition", "abstract": "Code-switching automatic speech recognition (ASR) aims to transcribe speech\nthat contains two or more languages accurately. To better capture\nlanguage-specific speech representations and address language confusion in\ncode-switching ASR, the mixture-of-experts (MoE) architecture and an additional\nlanguage diarization (LD) decoder are commonly employed. However, most\nresearches remain stagnant in simple operations like weighted summation or\nconcatenation to fuse languagespecific speech representations, leaving\nsignificant opportunities to explore the enhancement of integrating language\nbias information. In this paper, we introduce CAMEL, a cross-attention-based\nMoE and language bias approach for code-switching ASR. Specifically, after each\nMoE layer, we fuse language-specific speech representations with\ncross-attention, leveraging its strong contextual modeling abilities.\nAdditionally, we design a source attention-based mechanism to incorporate the\nlanguage information from the LD decoder output into text embeddings.\nExperimental results demonstrate that our approach achieves state-of-the-art\nperformance on the SEAME, ASRU200, and ASRU700+LibriSpeech460 Mandarin-English\ncode-switching ASR datasets.", "published": "2024-12-17 10:25:06", "link": "http://arxiv.org/abs/2412.12760v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TAME: Temporal Audio-based Mamba for Enhanced Drone Trajectory\n  Estimation and Classification", "abstract": "The increasing prevalence of compact UAVs has introduced significant risks to\npublic safety, while traditional drone detection systems are often bulky and\ncostly. To address these challenges, we present TAME, the Temporal Audio-based\nMamba for Enhanced Drone Trajectory Estimation and Classification. This\ninnovative anti-UAV detection model leverages a parallel selective state-space\nmodel to simultaneously capture and learn both the temporal and spectral\nfeatures of audio, effectively analyzing propagation of sound. To further\nenhance temporal features, we introduce a Temporal Feature Enhancement Module,\nwhich integrates spectral features into temporal data using residual\ncross-attention. This enhanced temporal information is then employed for\nprecise 3D trajectory estimation and classification. Our model sets a new\nstandard of performance on the MMUAD benchmarks, demonstrating superior\naccuracy and effectiveness. The code and trained models are publicly available\non GitHub https://github.com/AmazingDay1/TAME.", "published": "2024-12-17 16:00:31", "link": "http://arxiv.org/abs/2412.13037v7", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Synthetic Speech Classification: IEEE Signal Processing Cup 2022\n  challenge", "abstract": "The aim of this project is to implement and design arobust synthetic speech\nclassifier for the IEEE Signal ProcessingCup 2022 challenge. Here, we learn a\nsynthetic speech attributionmodel using the speech generated from various\ntext-to-speech(TTS) algorithms as well as unknown TTS algorithms. Weexperiment\nwith both the classical machine learning methodssuch as support vector machine,\nGaussian mixture model, anddeep learning based methods such as ResNet, VGG16,\nand twoshallow end-to-end networks. We observe that deep learningbased methods\nwith raw data demonstrate the best performance.", "published": "2024-12-17 19:15:02", "link": "http://arxiv.org/abs/2412.13279v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Speech Synthesis from Multimodal Articulatory Representations", "abstract": "The amount of articulatory data available for training deep learning models\nis much less compared to acoustic speech data. In order to improve\narticulatory-to-acoustic synthesis performance in these low-resource settings,\nwe propose a multimodal pre-training framework. On single-speaker speech\nsynthesis tasks from real-time magnetic resonance imaging and surface\nelectromyography inputs, the intelligibility of synthesized outputs improves\nnoticeably. For example, compared to prior work, utilizing our proposed\ntransfer learning methods improves the MRI-to-speech performance by 36% word\nerror rate. In addition to these intelligibility results, our multimodal\npre-trained models consistently outperform unimodal baselines on three\nobjective and subjective synthesis quality metrics.", "published": "2024-12-17 23:55:00", "link": "http://arxiv.org/abs/2412.13387v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Phoneme-Level Feature Discrepancies: A Key to Detecting Sophisticated\n  Speech Deepfakes", "abstract": "Recent advancements in text-to-speech and speech conversion technologies have\nenabled the creation of highly convincing synthetic speech. While these\ninnovations offer numerous practical benefits, they also cause significant\nsecurity challenges when maliciously misused. Therefore, there is an urgent\nneed to detect these synthetic speech signals. Phoneme features provide a\npowerful speech representation for deepfake detection. However, previous\nphoneme-based detection approaches typically focused on specific phonemes,\noverlooking temporal inconsistencies across the entire phoneme sequence. In\nthis paper, we develop a new mechanism for detecting speech deepfakes by\nidentifying the inconsistencies of phoneme-level speech features. We design an\nadaptive phoneme pooling technique that extracts sample-specific phoneme-level\nfeatures from frame-level speech data. By applying this technique to features\nextracted by pre-trained audio models on previously unseen deepfake datasets,\nwe demonstrate that deepfake samples often exhibit phoneme-level\ninconsistencies when compared to genuine speech. To further enhance detection\naccuracy, we propose a deepfake detector that uses a graph attention network to\nmodel the temporal dependencies of phoneme-level features. Additionally, we\nintroduce a random phoneme substitution augmentation technique to increase\nfeature diversity during training. Extensive experiments on four benchmark\ndatasets demonstrate the superior performance of our method over existing\nstate-of-the-art detection methods.", "published": "2024-12-17 07:31:19", "link": "http://arxiv.org/abs/2412.12619v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio Array-Based 3D UAV Trajectory Estimation with LiDAR\n  Pseudo-Labeling", "abstract": "As small unmanned aerial vehicles (UAVs) become increasingly prevalent, there\nis growing concern regarding their impact on public safety and privacy,\nhighlighting the need for advanced tracking and trajectory estimation\nsolutions. In response, this paper introduces a novel framework that utilizes\naudio array for 3D UAV trajectory estimation. Our approach incorporates a\nself-supervised learning model, starting with the conversion of audio data into\nmel-spectrograms, which are analyzed through an encoder to extract crucial\ntemporal and spectral information. Simultaneously, UAV trajectories are\nestimated using LiDAR point clouds via unsupervised methods. These LiDAR-based\nestimations act as pseudo labels, enabling the training of an Audio Perception\nNetwork without requiring labeled data. In this architecture, the LiDAR-based\nsystem operates as the Teacher Network, guiding the Audio Perception Network,\nwhich serves as the Student Network. Once trained, the model can independently\npredict 3D trajectories using only audio signals, with no need for LiDAR data\nor external ground truth during deployment. To further enhance precision, we\napply Gaussian Process modeling for improved spatiotemporal tracking. Our\nmethod delivers top-tier performance on the MMAUD dataset, establishing a new\nbenchmark in trajectory estimation using self-supervised learning techniques\nwithout reliance on ground truth annotations.", "published": "2024-12-17 09:16:28", "link": "http://arxiv.org/abs/2412.12698v5", "categories": ["cs.RO", "cs.SD", "eess.AS"], "primary_category": "cs.RO"}
