{"title": "Automatic Generation of Model and Data Cards: A Step Towards Responsible\n  AI", "abstract": "In an era of model and data proliferation in machine learning/AI especially\nmarked by the rapid advancement of open-sourced technologies, there arises a\ncritical need for standardized consistent documentation. Our work addresses the\ninformation incompleteness in current human-generated model and data cards. We\npropose an automated generation approach using Large Language Models (LLMs).\nOur key contributions include the establishment of CardBench, a comprehensive\ndataset aggregated from over 4.8k model cards and 1.4k data cards, coupled with\nthe development of the CardGen pipeline comprising a two-step retrieval\nprocess. Our approach exhibits enhanced completeness, objectivity, and\nfaithfulness in generated model and data cards, a significant step in\nresponsible AI documentation practices ensuring better accountability and\ntraceability.", "published": "2024-05-10 06:14:07", "link": "http://arxiv.org/abs/2405.06258v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pruning as a Domain-specific LLM Extractor", "abstract": "Large Language Models (LLMs) have exhibited remarkable proficiency across a\nwide array of NLP tasks. However, the escalation in model size also engenders\nsubstantial deployment costs. While few efforts have explored model pruning\ntechniques to reduce the size of LLMs, they mainly center on general or\ntask-specific weights. This leads to suboptimal performance due to lacking\nspecificity on the target domain or generality on different tasks when applied\nto domain-specific challenges. This work introduces an innovative unstructured\ndual-pruning methodology, D-Pruner, for domain-specific compression on LLM. It\nextracts a compressed, domain-specific, and task-agnostic LLM by identifying\nLLM weights that are pivotal for general capabilities, like linguistic\ncapability and multi-task solving, and domain-specific knowledge. More\nspecifically, we first assess general weight importance by quantifying the\nerror incurred upon their removal with the help of an open-domain calibration\ndataset. Then, we utilize this general weight importance to refine the training\nloss, so that it preserves generality when fitting into a specific domain.\nMoreover, by efficiently approximating weight importance with the refined\ntraining loss on a domain-specific calibration dataset, we obtain a pruned\nmodel emphasizing generality and specificity. Our comprehensive experiments\nacross various tasks in healthcare and legal domains show the effectiveness of\nD-Pruner in domain-specific compression. Our code is available at\nhttps://github.com/psunlpgroup/D-Pruner.", "published": "2024-05-10 07:05:02", "link": "http://arxiv.org/abs/2405.06275v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Akal Badi ya Bias: An Exploratory Study of Gender Bias in Hindi Language\n  Technology", "abstract": "Existing research in measuring and mitigating gender bias predominantly\ncenters on English, overlooking the intricate challenges posed by non-English\nlanguages and the Global South. This paper presents the first comprehensive\nstudy delving into the nuanced landscape of gender bias in Hindi, the third\nmost spoken language globally. Our study employs diverse mining techniques,\ncomputational models, field studies and sheds light on the limitations of\ncurrent methodologies. Given the challenges faced with mining gender biased\nstatements in Hindi using existing methods, we conducted field studies to\nbootstrap the collection of such sentences. Through field studies involving\nrural and low-income community women, we uncover diverse perceptions of gender\nbias, underscoring the necessity for context-specific approaches. This paper\nadvocates for a community-centric research design, amplifying voices often\nmarginalized in previous studies. Our findings not only contribute to the\nunderstanding of gender bias in Hindi but also establish a foundation for\nfurther exploration of Indic languages. By exploring the intricacies of this\nunderstudied context, we call for thoughtful engagement with gender bias,\npromoting inclusivity and equity in linguistic and cultural contexts beyond the\nGlobal North.", "published": "2024-05-10 09:26:12", "link": "http://arxiv.org/abs/2405.06346v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Potential and Limitations of LLMs in Capturing Structured Semantics: A\n  Case Study on SRL", "abstract": "Large Language Models (LLMs) play a crucial role in capturing structured\nsemantics to enhance language understanding, improve interpretability, and\nreduce bias. Nevertheless, an ongoing controversy exists over the extent to\nwhich LLMs can grasp structured semantics. To assess this, we propose using\nSemantic Role Labeling (SRL) as a fundamental task to explore LLMs' ability to\nextract structured semantics. In our assessment, we employ the prompting\napproach, which leads to the creation of our few-shot SRL parser, called\nPromptSRL. PromptSRL enables LLMs to map natural languages to explicit semantic\nstructures, which provides an interpretable window into the properties of LLMs.\nWe find interesting potential: LLMs can indeed capture semantic structures, and\nscaling-up doesn't always mirror potential. Additionally, limitations of LLMs\nare observed in C-arguments, etc. Lastly, we are surprised to discover that\nsignificant overlap in the errors is made by both LLMs and untrained humans,\naccounting for almost 30% of all errors.", "published": "2024-05-10 11:44:05", "link": "http://arxiv.org/abs/2405.06410v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Replicate ITS Feedback on Open-Ended Math\n  Questions?", "abstract": "Intelligent Tutoring Systems (ITSs) often contain an automated feedback\ncomponent, which provides a predefined feedback message to students when they\ndetect a predefined error. To such a feedback component, we often resort to\ntemplate-based approaches. These approaches require significant effort from\nhuman experts to detect a limited number of possible student errors and provide\ncorresponding feedback. This limitation is exemplified in open-ended math\nquestions, where there can be a large number of different incorrect errors. In\nour work, we examine the capabilities of large language models (LLMs) to\ngenerate feedback for open-ended math questions, similar to that of an\nestablished ITS that uses a template-based approach. We fine-tune both\nopen-source and proprietary LLMs on real student responses and corresponding\nITS-provided feedback. We measure the quality of the generated feedback using\ntext similarity metrics. We find that open-source and proprietary models both\nshow promise in replicating the feedback they see during training, but do not\ngeneralize well to previously unseen student errors. These results suggest that\ndespite being able to learn the formatting of feedback, LLMs are not able to\nfully understand mathematical errors made by students.", "published": "2024-05-10 11:53:53", "link": "http://arxiv.org/abs/2405.06414v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "E2TP: Element to Tuple Prompting Improves Aspect Sentiment Tuple\n  Prediction", "abstract": "Generative approaches have significantly influenced Aspect-Based Sentiment\nAnalysis (ABSA), garnering considerable attention. However, existing studies\noften predict target text components monolithically, neglecting the benefits of\nutilizing single elements for tuple prediction. In this paper, we introduce\nElement to Tuple Prompting (E2TP), employing a two-step architecture. The\nformer step focuses on predicting single elements, while the latter step\ncompletes the process by mapping these predicted elements to their\ncorresponding tuples. E2TP is inspired by human problem-solving, breaking down\ntasks into manageable parts, using the first step's output as a guide in the\nsecond step. Within this strategy, three types of paradigms, namely\nE2TP($diet$), E2TP($f_1$), and E2TP($f_2$), are designed to facilitate the\ntraining process. Beyond dataset-specific experiments, our paper addresses\ncross-domain scenarios, demonstrating the effectiveness and generalizability of\nthe approach. By conducting a comprehensive analysis on various benchmarks, we\nshow that E2TP achieves new state-of-the-art results in nearly all cases.", "published": "2024-05-10 13:04:21", "link": "http://arxiv.org/abs/2405.06454v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LyS at SemEval-2024 Task 3: An Early Prototype for End-to-End Multimodal\n  Emotion Linking as Graph-Based Parsing", "abstract": "This paper describes our participation in SemEval 2024 Task 3, which focused\non Multimodal Emotion Cause Analysis in Conversations. We developed an early\nprototype for an end-to-end system that uses graph-based methods from\ndependency parsing to identify causal emotion relations in multi-party\nconversations. Our model comprises a neural transformer-based encoder for\ncontextualizing multimodal conversation data and a graph-based decoder for\ngenerating the adjacency matrix scores of the causal graph. We ranked 7th out\nof 15 valid and official submissions for Subtask 1, using textual inputs only.\nWe also discuss our participation in Subtask 2 during post-evaluation using\nmulti-modal inputs.", "published": "2024-05-10 14:03:37", "link": "http://arxiv.org/abs/2405.06483v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aspect-based Sentiment Evaluation of Chess Moves (ASSESS): an NLP-based\n  Method for Evaluating Chess Strategies from Textbooks", "abstract": "The chess domain is well-suited for creating an artificial intelligence (AI)\nsystem that mimics real-world challenges, including decision-making. Throughout\nthe years, minimal attention has been paid to investigating insights derived\nfrom unstructured chess data sources. In this study, we examine the complicated\nrelationships between multiple referenced moves in a chess-teaching textbook,\nand propose a novel method designed to encapsulate chess knowledge derived from\nmove-action phrases. This study investigates the feasibility of using a\nmodified sentiment analysis method as a means for evaluating chess moves based\non text. Our proposed Aspect-Based Sentiment Analysis (ABSA) method represents\nan advancement in evaluating the sentiment associated with referenced chess\nmoves. By extracting insights from move-action phrases, our approach aims to\nprovide a more fine-grained and contextually aware `chess move'-based sentiment\nclassification. Through empirical experiments and analysis, we evaluate the\nperformance of our fine-tuned ABSA model, presenting results that confirm the\nefficiency of our approach in advancing aspect-based sentiment classification\nwithin the chess domain. This research contributes to the area of game-playing\nby machines and shows the practical applicability of leveraging NLP techniques\nto understand the context of strategic games.", "published": "2024-05-10 14:23:43", "link": "http://arxiv.org/abs/2405.06499v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompting Large Language Models with Knowledge Graphs for Question\n  Answering Involving Long-tail Facts", "abstract": "Although Large Language Models (LLMs) are effective in performing various NLP\ntasks, they still struggle to handle tasks that require extensive, real-world\nknowledge, especially when dealing with long-tail facts (facts related to\nlong-tail entities). This limitation highlights the need to supplement LLMs\nwith non-parametric knowledge. To address this issue, we analysed the effects\nof different types of non-parametric knowledge, including textual passage and\nknowledge graphs (KGs). Since LLMs have probably seen the majority of factual\nquestion-answering datasets already, to facilitate our analysis, we proposed a\nfully automatic pipeline for creating a benchmark that requires knowledge of\nlong-tail facts for answering the involved questions. Using this pipeline, we\nintroduce the LTGen benchmark. We evaluate state-of-the-art LLMs in different\nknowledge settings using the proposed benchmark. Our experiments show that LLMs\nalone struggle with answering these questions, especially when the long-tail\nlevel is high or rich knowledge is required. Nonetheless, the performance of\nthe same models improved significantly when they were prompted with\nnon-parametric knowledge. We observed that, in most cases, prompting LLMs with\nKG triples surpasses passage-based prompting using a state-of-the-art\nretriever. In addition, while prompting LLMs with both KG triples and documents\ndoes not consistently improve knowledge coverage, it can dramatically reduce\nhallucinations in the generated content.", "published": "2024-05-10 15:10:20", "link": "http://arxiv.org/abs/2405.06524v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Can Natural Language Processing Do for Peer Review?", "abstract": "The number of scientific articles produced every year is growing rapidly.\nProviding quality control over them is crucial for scientists and, ultimately,\nfor the public good. In modern science, this process is largely delegated to\npeer review -- a distributed procedure in which each submission is evaluated by\nseveral independent experts in the field. Peer review is widely used, yet it is\nhard, time-consuming, and prone to error. Since the artifacts involved in peer\nreview -- manuscripts, reviews, discussions -- are largely text-based, Natural\nLanguage Processing has great potential to improve reviewing. As the emergence\nof large language models (LLMs) has enabled NLP assistance for many new tasks,\nthe discussion on machine-assisted peer review is picking up the pace. Yet,\nwhere exactly is help needed, where can NLP help, and where should it stand\naside? The goal of our paper is to provide a foundation for the future efforts\nin NLP for peer-reviewing assistance. We discuss peer review as a general\nprocess, exemplified by reviewing at AI conferences. We detail each step of the\nprocess from manuscript submission to camera-ready revision, and discuss the\nassociated challenges and opportunities for NLP assistance, illustrated by\nexisting work. We then turn to the big challenges in NLP for peer review as a\nwhole, including data acquisition and licensing, operationalization and\nexperimentation, and ethical issues. To help consolidate community efforts, we\ncreate a companion repository that aggregates key datasets pertaining to peer\nreview. Finally, we issue a detailed call for action for the scientific\ncommunity, NLP and AI researchers, policymakers, and funding bodies to help\nbring the research in NLP for peer review forward. We hope that our work will\nhelp set the agenda for research in machine-assisted scientific quality control\nin the age of AI, within the NLP community and beyond.", "published": "2024-05-10 16:06:43", "link": "http://arxiv.org/abs/2405.06563v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linearizing Large Language Models", "abstract": "Linear transformers have emerged as a subquadratic-time alternative to\nsoftmax attention and have garnered significant interest due to their\nfixed-size recurrent state that lowers inference cost. However, their original\nformulation suffers from poor scaling and underperforms compute-matched\ntransformers. Recent linear models such as RWKV and Mamba have attempted to\naddress these shortcomings by proposing novel time-mixing and gating\narchitectures, but pre-training large language models requires significant data\nand compute investments. Thus, the search for subquadratic architectures is\nlimited by the availability of compute and quality pre-training datasets. As a\ncost-effective alternative to pre-training linear transformers, we propose\nScalable UPtraining for Recurrent Attention (SUPRA). We present a method to\nuptrain existing large pre-trained transformers into Recurrent Neural Networks\n(RNNs) with a modest compute budget. This allows us to leverage the strong\npre-training data and performance of existing transformer LLMs, while requiring\n5% of the training cost. We find that our linearization technique leads to\ncompetitive performance on standard benchmarks, but we identify persistent\nin-context learning and long-context modeling shortfalls for even the largest\nlinear models. Our code and models can be found at\nhttps://github.com/TRI-ML/linear_open_lm.", "published": "2024-05-10 17:59:08", "link": "http://arxiv.org/abs/2405.06640v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-Generated Black-box Explanations Can Be Adversarially Helpful", "abstract": "Large Language Models (LLMs) are becoming vital tools that help us solve and\nunderstand complex problems by acting as digital assistants. LLMs can generate\nconvincing explanations, even when only given the inputs and outputs of these\nproblems, i.e., in a ``black-box'' approach. However, our research uncovers a\nhidden risk tied to this approach, which we call *adversarial helpfulness*.\nThis happens when an LLM's explanations make a wrong answer look right,\npotentially leading people to trust incorrect solutions. In this paper, we show\nthat this issue affects not just humans, but also LLM evaluators. Digging\ndeeper, we identify and examine key persuasive strategies employed by LLMs. Our\nfindings reveal that these models employ strategies such as reframing the\nquestions, expressing an elevated level of confidence, and cherry-picking\nevidence to paint misleading answers in a credible light. To examine if LLMs\nare able to navigate complex-structured knowledge when generating adversarially\nhelpful explanations, we create a special task based on navigating through\ngraphs. Most LLMs are not able to find alternative paths along simple graphs,\nindicating that their misleading explanations aren't produced by only logical\ndeductions using complex knowledge. These findings shed light on the\nlimitations of the black-box explanation setting and allow us to provide advice\non the safe usage of LLMs.", "published": "2024-05-10 20:23:46", "link": "http://arxiv.org/abs/2405.06800v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Ghanaian NLP Landscape: A First Look", "abstract": "Despite comprising one-third of global languages, African languages are\ncritically underrepresented in Artificial Intelligence (AI), threatening\nlinguistic diversity and cultural heritage. Ghanaian languages, in particular,\nface an alarming decline, with documented extinction and several at risk. This\nstudy pioneers a comprehensive survey of Natural Language Processing (NLP)\nresearch focused on Ghanaian languages, identifying methodologies, datasets,\nand techniques employed. Additionally, we create a detailed roadmap outlining\nchallenges, best practices, and future directions, aiming to improve\naccessibility for researchers. This work serves as a foundational resource for\nGhanaian NLP research and underscores the critical need for integrating global\nlinguistic diversity into AI development.", "published": "2024-05-10 21:39:09", "link": "http://arxiv.org/abs/2405.06818v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HC$^2$L: Hybrid and Cooperative Contrastive Learning for Cross-lingual\n  Spoken Language Understanding", "abstract": "State-of-the-art model for zero-shot cross-lingual spoken language\nunderstanding performs cross-lingual unsupervised contrastive learning to\nachieve the label-agnostic semantic alignment between each utterance and its\ncode-switched data. However, it ignores the precious intent/slot labels, whose\nlabel information is promising to help capture the label-aware semantics\nstructure and then leverage supervised contrastive learning to improve both\nsource and target languages' semantics. In this paper, we propose Hybrid and\nCooperative Contrastive Learning to address this problem. Apart from\ncross-lingual unsupervised contrastive learning, we design a holistic approach\nthat exploits source language supervised contrastive learning, cross-lingual\nsupervised contrastive learning and multilingual supervised contrastive\nlearning to perform label-aware semantics alignments in a comprehensive manner.\nEach kind of supervised contrastive learning mechanism includes both\nsingle-task and joint-task scenarios. In our model, one contrastive learning\nmechanism's input is enhanced by others. Thus the total four contrastive\nlearning mechanisms are cooperative to learn more consistent and discriminative\nrepresentations in the virtuous cycle during the training process. Experiments\nshow that our model obtains consistent improvements over 9 languages, achieving\nnew state-of-the-art performance.", "published": "2024-05-10 02:40:49", "link": "http://arxiv.org/abs/2405.06204v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models", "abstract": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding.", "published": "2024-05-10 03:06:24", "link": "http://arxiv.org/abs/2405.06219v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "For the Misgendered Chinese in Gender Bias Research: Multi-Task Learning\n  with Knowledge Distillation for Pinyin Name-Gender Prediction", "abstract": "Achieving gender equality is a pivotal factor in realizing the UN's Global\nGoals for Sustainable Development. Gender bias studies work towards this and\nrely on name-based gender inference tools to assign individual gender labels\nwhen gender information is unavailable. However, these tools often inaccurately\npredict gender for Chinese Pinyin names, leading to potential bias in such\nstudies. With the growing participation of Chinese in international activities,\nthis situation is becoming more severe. Specifically, current tools focus on\npronunciation (Pinyin) information, neglecting the fact that the latent\nconnections between Pinyin and Chinese characters (Hanzi) behind convey\ncritical information. As a first effort, we formulate the Pinyin name-gender\nguessing problem and design a Multi-Task Learning Network assisted by Knowledge\nDistillation that enables the Pinyin embeddings in the model to possess\nsemantic features of Chinese characters and to learn gender information from\nChinese character names. Our open-sourced method surpasses commercial\nname-gender guessing tools by 9.70\\% to 20.08\\% relatively, and also\noutperforms the state-of-the-art algorithms.", "published": "2024-05-10 03:16:07", "link": "http://arxiv.org/abs/2405.06221v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "SaudiBERT: A Large Language Model Pretrained on Saudi Dialect Corpora", "abstract": "In this paper, we introduce SaudiBERT, a monodialect Arabic language model\npretrained exclusively on Saudi dialectal text. To demonstrate the model's\neffectiveness, we compared SaudiBERT with six different multidialect Arabic\nlanguage models across 11 evaluation datasets, which are divided into two\ngroups: sentiment analysis and text classification. SaudiBERT achieved average\nF1-scores of 86.15\\% and 87.86\\% in these groups respectively, significantly\noutperforming all other comparative models. Additionally, we present two novel\nSaudi dialectal corpora: the Saudi Tweets Mega Corpus (STMC), which contains\nover 141 million tweets in Saudi dialect, and the Saudi Forums Corpus (SFC),\nwhich includes 15.2 GB of text collected from five Saudi online forums. Both\ncorpora are used in pretraining the proposed model, and they are the largest\nSaudi dialectal corpora ever reported in the literature. The results confirm\nthe effectiveness of SaudiBERT in understanding and analyzing Arabic text\nexpressed in Saudi dialect, achieving state-of-the-art results in most tasks\nand surpassing other language models included in the study. SaudiBERT model is\npublicly available on \\url{https://huggingface.co/faisalq/SaudiBERT}.", "published": "2024-05-10 04:22:54", "link": "http://arxiv.org/abs/2405.06239v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Aspect-oriented Consumer Health Answer Summarization", "abstract": "Community Question-Answering (CQA) forums have revolutionized how people seek\ninformation, especially those related to their healthcare needs, placing their\ntrust in the collective wisdom of the public. However, there can be several\nanswers in response to a single query, which makes it hard to grasp the key\ninformation related to the specific health concern. Typically, CQA forums\nfeature a single top-voted answer as a representative summary for each query.\nHowever, a single answer overlooks the alternative solutions and other\ninformation frequently offered in other responses. Our research focuses on\naspect-based summarization of health answers to address this limitation.\nSummarization of responses under different aspects such as suggestions,\ninformation, personal experiences, and questions can enhance the usability of\nthe platforms. We formalize a multi-stage annotation guideline and contribute a\nunique dataset comprising aspect-based human-written health answer summaries.\nWe build an automated multi-faceted answer summarization pipeline with this\ndataset based on task-specific fine-tuning of several state-of-the-art models.\nThe pipeline leverages question similarity to retrieve relevant answer\nsentences, subsequently classifying them into the appropriate aspect type.\nFollowing this, we employ several recent abstractive summarization models to\ngenerate aspect-based summaries. Finally, we present a comprehensive human\nanalysis and find that our summaries rank high in capturing relevant content\nand a wide range of solutions.", "published": "2024-05-10 07:52:43", "link": "http://arxiv.org/abs/2405.06295v1", "categories": ["cs.CL", "cs.AI", "H.4.3; I.2.7; J.3; J.7; K.6.4"], "primary_category": "cs.CL"}
{"title": "A NLP Approach to \"Review Bombing\" in Metacritic PC Videogames User\n  Ratings", "abstract": "Many videogames suffer \"review bombing\" -a large volume of unusually low\nscores that in many cases do not reflect the real quality of the product- when\nrated by users. By taking Metacritic's 50,000+ user score aggregations for PC\ngames in English language, we use a Natural Language Processing (NLP) approach\nto try to understand the main words and concepts appearing in such cases,\nreaching a 0.88 accuracy on a validation set when distinguishing between just\nbad ratings and review bombings. By uncovering and analyzing the patterns\ndriving this phenomenon, these results could be used to further mitigate these\nsituations.", "published": "2024-05-10 08:31:04", "link": "http://arxiv.org/abs/2405.06306v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Decoding Emotions in Abstract Art: Cognitive Plausibility of CLIP in\n  Recognizing Color-Emotion Associations", "abstract": "This study investigates the cognitive plausibility of a pretrained multimodal\nmodel, CLIP, in recognizing emotions evoked by abstract visual art. We employ a\ndataset comprising images with associated emotion labels and textual rationales\nof these labels provided by human annotators. We perform linguistic analyses of\nrationales, zero-shot emotion classification of images and rationales, apply\nsimilarity-based prediction of emotion, and investigate color-emotion\nassociations. The relatively low, yet above baseline, accuracy in recognizing\nemotion for abstract images and rationales suggests that CLIP decodes emotional\ncomplexities in a manner not well aligned with human cognitive processes.\nFurthermore, we explore color-emotion interactions in images and rationales.\nExpected color-emotion associations, such as red relating to anger, are\nidentified in images and texts annotated with emotion labels by both humans and\nCLIP, with the latter showing even stronger interactions. Our results highlight\nthe disparity between human processing and machine processing when connecting\nimage features and emotions.", "published": "2024-05-10 08:45:23", "link": "http://arxiv.org/abs/2405.06319v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "LMD3: Language Model Data Density Dependence", "abstract": "We develop a methodology for analyzing language model task performance at the\nindividual example level based on training data density estimation. Experiments\nwith paraphrasing as a controlled intervention on finetuning data demonstrate\nthat increasing the support in the training distribution for specific test\nqueries results in a measurable increase in density, which is also a\nsignificant predictor of the performance increase caused by the intervention.\nExperiments with pretraining data demonstrate that we can explain a significant\nfraction of the variance in model perplexity via density measurements. We\nconclude that our framework can provide statistical evidence of the dependence\nof a target model's predictions on subsets of its training data, and can more\ngenerally be used to characterize the support (or lack thereof) in the training\ndata for a given test task.", "published": "2024-05-10 09:03:27", "link": "http://arxiv.org/abs/2405.06331v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LLM Discussion: Enhancing the Creativity of Large Language Models via\n  Discussion Framework and Role-Play", "abstract": "Large language models (LLMs) have shown exceptional proficiency in natural\nlanguage processing but often fall short of generating creative and original\nresponses to open-ended questions. To enhance LLM creativity, our key insight\nis to emulate the human process of inducing collective creativity through\nengaging discussions with participants from diverse backgrounds and\nperspectives. To this end, we propose LLM Discussion, a three-phase discussion\nframework that facilitates vigorous and diverging idea exchanges and ensures\nconvergence to creative answers. Moreover, we adopt a role-playing technique by\nassigning distinct roles to LLMs to combat the homogeneity of LLMs. We evaluate\nthe efficacy of the proposed framework with the Alternative Uses Test,\nSimilarities Test, Instances Test, and Scientific Creativity Test through both\nLLM evaluation and human study. The results show that our proposed framework\noutperforms single-LLM approaches and existing multi-LLM frameworks across\nvarious creativity metrics. The code is available at\nhttps://github.com/lawraa/LLM-Discussion.", "published": "2024-05-10 10:19:14", "link": "http://arxiv.org/abs/2405.06373v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Are EEG-to-Text Models Working?", "abstract": "This work critically analyzes existing models for open-vocabulary EEG-to-Text\ntranslation. We identify a crucial limitation: previous studies often employed\nimplicit teacher-forcing during evaluation, artificially inflating performance\nmetrics. Additionally, they lacked a critical benchmark - comparing model\nperformance on pure noise inputs. We propose a methodology to differentiate\nbetween models that truly learn from EEG signals and those that simply memorize\ntraining data. Our analysis reveals that model performance on noise data can be\ncomparable to that on EEG data. These findings highlight the need for stricter\nevaluation practices in EEG-to-Text research, emphasizing transparent reporting\nand rigorous benchmarking with noise inputs. This approach will lead to more\nreliable assessments of model capabilities and pave the way for robust\nEEG-to-Text communication systems. Code is available at\nhttps://github.com/NeuSpeech/EEG-To-Text", "published": "2024-05-10 13:10:55", "link": "http://arxiv.org/abs/2405.06459v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pseudo-Prompt Generating in Pre-trained Vision-Language Models for\n  Multi-Label Medical Image Classification", "abstract": "The task of medical image recognition is notably complicated by the presence\nof varied and multiple pathological indications, presenting a unique challenge\nin multi-label classification with unseen labels. This complexity underlines\nthe need for computer-aided diagnosis methods employing multi-label zero-shot\nlearning. Recent advancements in pre-trained vision-language models (VLMs) have\nshowcased notable zero-shot classification abilities on medical images.\nHowever, these methods have limitations on leveraging extensive pre-trained\nknowledge from broader image datasets, and often depend on manual prompt\nconstruction by expert radiologists. By automating the process of prompt\ntuning, prompt learning techniques have emerged as an efficient way to adapt\nVLMs to downstream tasks. Yet, existing CoOp-based strategies fall short in\nperforming class-specific prompts on unseen categories, limiting\ngeneralizability in fine-grained scenarios. To overcome these constraints, we\nintroduce a novel prompt generation approach inspirited by text generation in\nnatural language processing (NLP). Our method, named Pseudo-Prompt Generating\n(PsPG), capitalizes on the priori knowledge of multi-modal features. Featuring\na RNN-based decoder, PsPG autoregressively generates class-tailored embedding\nvectors, i.e., pseudo-prompts. Comparative evaluations on various multi-label\nchest radiograph datasets affirm the superiority of our approach against\nleading medical vision-language and multi-label prompt learning methods. The\nsource code is available at https://github.com/fallingnight/PsPG", "published": "2024-05-10 13:27:32", "link": "http://arxiv.org/abs/2405.06468v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ATSumm: Auxiliary information enhanced approach for abstractive disaster\n  Tweet Summarization with sparse training data", "abstract": "The abundance of situational information on Twitter poses a challenge for\nusers to manually discern vital and relevant information during disasters. A\nconcise and human-interpretable overview of this information helps\ndecision-makers in implementing efficient and quick disaster response. Existing\nabstractive summarization approaches can be categorized as sentence-based or\nkey-phrase-based approaches. This paper focuses on sentence-based approach,\nwhich is typically implemented as a dual-phase procedure in literature. The\ninitial phase, known as the extractive phase, involves identifying the most\nrelevant tweets. The subsequent phase, referred to as the abstractive phase,\nentails generating a more human-interpretable summary. In this study, we adopt\nthe methodology from prior research for the extractive phase. For the\nabstractive phase of summarization, most existing approaches employ deep\nlearning-based frameworks, which can either be pre-trained or require training\nfrom scratch. However, to achieve the appropriate level of performance, it is\nimperative to have substantial training data for both methods, which is not\nreadily available. This work presents an Abstractive Tweet Summarizer (ATSumm)\nthat effectively addresses the issue of data sparsity by using auxiliary\ninformation. We introduced the Auxiliary Pointer Generator Network (AuxPGN)\nmodel, which utilizes a unique attention mechanism called Key-phrase attention.\nThis attention mechanism incorporates auxiliary information in the form of\nkey-phrases and their corresponding importance scores from the input tweets. We\nevaluate the proposed approach by comparing it with 10 state-of-the-art\napproaches across 13 disaster datasets. The evaluation results indicate that\nATSumm achieves superior performance compared to state-of-the-art approaches,\nwith improvement of 4-80% in ROUGE-N F1-score.", "published": "2024-05-10 15:36:56", "link": "http://arxiv.org/abs/2405.06541v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Mitigating Hallucinations in Large Language Models via\n  Self-Refinement-Enhanced Knowledge Retrieval", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, although their susceptibility to hallucination poses\nsignificant challenges for their deployment in critical areas such as\nhealthcare. To address this issue, retrieving relevant facts from knowledge\ngraphs (KGs) is considered a promising method. Existing KG-augmented approaches\ntend to be resource-intensive, requiring multiple rounds of retrieval and\nverification for each factoid, which impedes their application in real-world\nscenarios.\n  In this study, we propose Self-Refinement-Enhanced Knowledge Graph Retrieval\n(Re-KGR) to augment the factuality of LLMs' responses with less retrieval\nefforts in the medical field. Our approach leverages the attribution of\nnext-token predictive probability distributions across different tokens, and\nvarious model layers to primarily identify tokens with a high potential for\nhallucination, reducing verification rounds by refining knowledge triples\nassociated with these tokens. Moreover, we rectify inaccurate content using\nretrieved knowledge in the post-processing stage, which improves the\ntruthfulness of generated responses. Experimental results on a medical dataset\ndemonstrate that our approach can enhance the factual capability of LLMs across\nvarious foundational models as evidenced by the highest scores on truthfulness.", "published": "2024-05-10 15:40:50", "link": "http://arxiv.org/abs/2405.06545v1", "categories": ["cs.CL", "cs.LG", "I.2.7; H.3.3"], "primary_category": "cs.CL"}
{"title": "Sampling the Swadesh List to Identify Similar Languages with Tree Spaces", "abstract": "Communication plays a vital role in human interaction. Studying language is a\nworthwhile task and more recently has become quantitative in nature with\ndevelopments of fields like quantitative comparative linguistics and\nlexicostatistics. With respect to the authors own native languages, the\nancestry of the English language and the Latin alphabet are of the primary\ninterest. The Indo-European Tree traces many modern languages back to the\nProto-Indo-European root. Swadesh's cognates played a large role in developing\nthat historical perspective where some of the primary branches are Germanic,\nCeltic, Italic, and Balto-Slavic. This paper will use data analysis on open\nbooks where the simplest singular space is the 3-spider - a union T3 of three\nrays with their endpoints glued at a point 0 - which can represent these tree\nspaces for language clustering. These trees are built using a single linkage\nmethod for clustering based on distances between samples from languages which\nuse the Latin Script. Taking three languages at a time, the barycenter is\ndetermined. Some initial results have found both non-sticky and sticky sample\nmeans. If the mean exhibits non-sticky properties, then one language may come\nfrom a different ancestor than the other two. If the mean is considered sticky,\nthen the languages may share a common ancestor or all languages may have\ndifferent ancestry.", "published": "2024-05-10 15:46:05", "link": "http://arxiv.org/abs/2405.06549v1", "categories": ["stat.AP", "cs.CL"], "primary_category": "stat.AP"}
{"title": "ADSumm: Annotated Ground-truth Summary Datasets for Disaster Tweet\n  Summarization", "abstract": "Online social media platforms, such as Twitter, provide valuable information\nduring disaster events. Existing tweet disaster summarization approaches\nprovide a summary of these events to aid government agencies, humanitarian\norganizations, etc., to ensure effective disaster response. In the literature,\nthere are two types of approaches for disaster summarization, namely,\nsupervised and unsupervised approaches. Although supervised approaches are\ntypically more effective, they necessitate a sizable number of disaster event\nsummaries for testing and training. However, there is a lack of good number of\ndisaster summary datasets for training and evaluation. This motivates us to add\nmore datasets to make supervised learning approaches more efficient. In this\npaper, we present ADSumm, which adds annotated ground-truth summaries for eight\ndisaster events which consist of both natural and man-made disaster events\nbelonging to seven different countries. Our experimental analysis shows that\nthe newly added datasets improve the performance of the supervised\nsummarization approaches by 8-28% in terms of ROUGE-N F1-score. Moreover, in\nnewly annotated dataset, we have added a category label for each input tweet\nwhich helps to ensure good coverage from different categories in summary.\nAdditionally, we have added two other features relevance label and key-phrase,\nwhich provide information about the quality of a tweet and explanation about\nthe inclusion of the tweet into summary, respectively. For ground-truth summary\ncreation, we provide the annotation procedure adapted in detail, which has not\nbeen described in existing literature. Experimental analysis shows the quality\nof ground-truth summary is very good with Coverage, Relevance and Diversity.", "published": "2024-05-10 15:49:01", "link": "http://arxiv.org/abs/2405.06551v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Explaining Text Similarity in Transformer Models", "abstract": "As Transformers have become state-of-the-art models for natural language\nprocessing (NLP) tasks, the need to understand and explain their predictions is\nincreasingly apparent. Especially in unsupervised applications, such as\ninformation retrieval tasks, similarity models built on top of foundation model\nrepresentations have been widely applied. However, their inner prediction\nmechanisms have mostly remained opaque. Recent advances in explainable AI have\nmade it possible to mitigate these limitations by leveraging improved\nexplanations for Transformers through layer-wise relevance propagation (LRP).\nUsing BiLRP, an extension developed for computing second-order explanations in\nbilinear similarity models, we investigate which feature interactions drive\nsimilarity in NLP models. We validate the resulting explanations and\ndemonstrate their utility in three corpus-level use cases, analyzing\ngrammatical interactions, multilingual semantics, and biomedical text\nretrieval. Our findings contribute to a deeper understanding of different\nsemantic similarity tasks and models, highlighting how novel explainable AI\nmethods enable in-depth analyses and corpus-level insights.", "published": "2024-05-10 17:11:31", "link": "http://arxiv.org/abs/2405.06604v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Characterizing the Accuracy -- Efficiency Trade-off of Low-rank\n  Decomposition in Language Models", "abstract": "Recent large language models (LLMs) employ billions of parameters to enable\nbroad problem-solving capabilities. Such language models also tend to be\nmemory-bound because of the dominance of matrix-vector and matrix-matrix\nmultiplications with low arithmetic intensity. Therefore, optimizing the memory\nfootprint and traffic is an important optimization direction for LLMs today.\nModel compression methods such as quantization and parameter pruning have been\nactively explored to achieve memory footprint and traffic optimization.\nHowever, the accuracy-efficiency trade-off of rank pruning (i.e., low-rank\ndecomposition) for LLMs is not well-understood yet. Therefore, in this work, we\ncharacterize the accuracy-efficiency trade-off of a low-rank decomposition\nmethod, specifically Tucker decomposition, on recent language models, including\nan open-source LLM, Llama 2. We formalize the low-rank decomposition design\nspace and show that the decomposition design space is enormous (e.g.,\nO($2^{39}$) for Llama2-7B). To navigate such a vast design space, we formulate\nit and perform thorough case studies of accuracy-efficiency trade-offs using\nsix widely used LLM benchmarks on BERT and Llama 2 models. Our results show\nthat we can achieve a 9\\% model size reduction with minimal accuracy drops,\nwhich range from 4\\%p (\\%p refers to \"percentage point,\" which refers to the\nabsolute difference between two percentage numbers; 74\\% -> 78\\% = 4\\%p\nincrease) to 10\\%p, depending on the difficulty of the benchmark, without any\nretraining to recover accuracy after decomposition. The results show that\nlow-rank decomposition can be a promising direction for LLM-based applications\nthat require real-time service at scale (e.g., AI agent and real-time coding\nassistant), where the latency is as important as the model accuracy.", "published": "2024-05-10 17:40:02", "link": "http://arxiv.org/abs/2405.06626v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Enhancing Traffic Prediction with Textual Data Using Large Language\n  Models", "abstract": "Traffic prediction is pivotal for rational transportation supply scheduling\nand allocation. Existing researches into short-term traffic prediction,\nhowever, face challenges in adequately addressing exceptional circumstances and\nintegrating non-numerical contextual information like weather into models.\nWhile, Large language models offer a promising solution due to their inherent\nworld knowledge. However, directly using them for traffic prediction presents\ndrawbacks such as high cost, lack of determinism, and limited mathematical\ncapability. To mitigate these issues, this study proposes a novel approach.\nInstead of directly employing large models for prediction, it utilizes them to\nprocess textual information and obtain embeddings. These embeddings are then\ncombined with historical traffic data and inputted into traditional\nspatiotemporal forecasting models. The study investigates two types of special\nscenarios: regional-level and node-level. For regional-level scenarios, textual\ninformation is represented as a node connected to the entire network. For\nnode-level scenarios, embeddings from the large model represent additional\nnodes connected only to corresponding nodes. This approach shows a significant\nimprovement in prediction accuracy according to our experiment of New York Bike\ndataset.", "published": "2024-05-10 03:14:26", "link": "http://arxiv.org/abs/2405.06719v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Opportunities for Persian Digital Humanities Research with Artificial\n  Intelligence Language Models; Case Study: Forough Farrokhzad", "abstract": "This study explores the integration of advanced Natural Language Processing\n(NLP) and Artificial Intelligence (AI) techniques to analyze and interpret\nPersian literature, focusing on the poetry of Forough Farrokhzad. Utilizing\ncomputational methods, we aim to unveil thematic, stylistic, and linguistic\npatterns in Persian poetry. Specifically, the study employs AI models including\ntransformer-based language models for clustering of the poems in an\nunsupervised framework. This research underscores the potential of AI in\nenhancing our understanding of Persian literary heritage, with Forough\nFarrokhzad's work providing a comprehensive case study. This approach not only\ncontributes to the field of Persian Digital Humanities but also sets a\nprecedent for future research in Persian literary studies using computational\ntechniques.", "published": "2024-05-10 18:24:55", "link": "http://arxiv.org/abs/2405.06760v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LIVE: LaTex Interactive Visual Editing", "abstract": "LaTex coding is one of the main methods of writing an academic paper. When\nwriting a paper, abundant proper visual or graphic components will represent\nmore information volume than the textual data. However, most of the\nimplementation of LaTex graphic items are designed as static items that have\nsome weaknesses in representing more informative figures or tables with an\ninteractive reading experience. To address this problem, we propose LIVE, a\nnovel design methods idea to design interactive LaTex graphic items. To make a\nlucid representation of the main idea of LIVE, we designed several novels\nrepresenting implementations that are interactive and enough explanation for\nthe basic level principles. Using LIVE can design more graphic items, which we\ncall the Gitems, and easily and automatically get the relationship of the\nmutual application of a specific range of papers, which will add more vitality\nand performance factors into writing of traditional papers especially the\nreview papers. For vividly representing the functions of LIVE, we use the\npapers from NeRF as the example reference papers. The code of the\nimplementation project is open source.", "published": "2024-05-10 18:28:00", "link": "http://arxiv.org/abs/2405.06762v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Summarizing Radiology Reports Findings into Impressions", "abstract": "Patient hand-off and triage are two fundamental problems in health care.\nOften doctors must painstakingly summarize complex findings to efficiently\ncommunicate with specialists and quickly make decisions on which patients have\nthe most urgent cases. In pursuit of these challenges, we present (1) a model\nwith state-of-art radiology report summarization performance using (2) a novel\nmethod for augmenting medical data, and (3) an analysis of the model\nlimitations and radiology knowledge gain. We also provide a data processing\npipeline for future models developed on the the MIMIC CXR dataset. Our best\nperforming model was a fine-tuned BERT-to-BERT encoder-decoder with 58.75/100\nROUGE-L F1, which outperformed specialized checkpoints with more sophisticated\nattention mechanisms. We investigate these aspects in this work.", "published": "2024-05-10 20:29:25", "link": "http://arxiv.org/abs/2405.06802v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Execution-Based Evaluation of Natural Language to Bash and PowerShell\n  for Incident Remediation", "abstract": "Given recent advancements of Large Language Models (LLMs), code generation\ntasks attract immense attention for wide application in different domains. In\nan effort to evaluate and select a best model to automatically remediate system\nincidents discovered by Application Performance Monitoring (APM) platforms, it\nis crucial to verify if the generated code is syntactically and semantically\ncorrect, and whether it can be executed correctly as intended. However, current\nmethods for evaluating the quality of code generated by LLMs heavily rely on\nsurface form similarity metrics (e.g. BLEU, ROUGE, and exact/partial match)\nwhich have numerous limitations. In contrast, execution based evaluation\nfocuses more on code functionality and does not constrain the code generation\nto any fixed solution. Nevertheless, designing and implementing such\nexecution-based evaluation platform is not a trivial task. There are several\nworks creating execution-based evaluation platforms for popular programming\nlanguages such as SQL, Python, Java, but limited or no attempts for scripting\nlanguages such as Bash and PowerShell. In this paper, we present the first\nexecution-based evaluation platform in which we created three test suites\n(total 125 handcrafted test cases) to evaluate Bash (both single-line commands\nand multiple-line scripts) and PowerShell codes generated by LLMs. We benchmark\nseven closed and open-source LLMs using our platform with different techniques\n(zero-shot vs. few-shot learning).", "published": "2024-05-10 20:45:34", "link": "http://arxiv.org/abs/2405.06807v2", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "An Assessment of Model-On-Model Deception", "abstract": "The trustworthiness of highly capable language models is put at risk when\nthey are able to produce deceptive outputs. Moreover, when models are\nvulnerable to deception it undermines reliability. In this paper, we introduce\na method to investigate complex, model-on-model deceptive scenarios. We create\na dataset of over 10,000 misleading explanations by asking Llama-2 7B, 13B,\n70B, and GPT-3.5 to justify the wrong answer for questions in the MMLU. We find\nthat, when models read these explanations, they are all significantly deceived.\nWorryingly, models of all capabilities are successful at misleading others,\nwhile more capable models are only slightly better at resisting deception. We\nrecommend the development of techniques to detect and defend against deception.", "published": "2024-05-10 23:24:18", "link": "http://arxiv.org/abs/2405.12999v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Lost in Transcription: Identifying and Quantifying the Accuracy Biases\n  of Automatic Speech Recognition Systems Against Disfluent Speech", "abstract": "Automatic speech recognition (ASR) systems, increasingly prevalent in\neducation, healthcare, employment, and mobile technology, face significant\nchallenges in inclusivity, particularly for the 80 million-strong global\ncommunity of people who stutter. These systems often fail to accurately\ninterpret speech patterns deviating from typical fluency, leading to critical\nusability issues and misinterpretations. This study evaluates six leading ASRs,\nanalyzing their performance on both a real-world dataset of speech samples from\nindividuals who stutter and a synthetic dataset derived from the widely-used\nLibriSpeech benchmark. The synthetic dataset, uniquely designed to incorporate\nvarious stuttering events, enables an in-depth analysis of each ASR's handling\nof disfluent speech. Our comprehensive assessment includes metrics such as word\nerror rate (WER), character error rate (CER), and semantic accuracy of the\ntranscripts. The results reveal a consistent and statistically significant\naccuracy bias across all ASRs against disfluent speech, manifesting in\nsignificant syntactical and semantic inaccuracies in transcriptions. These\nfindings highlight a critical gap in current ASR technologies, underscoring the\nneed for effective bias mitigation strategies. Addressing this bias is\nimperative not only to improve the technology's usability for people who\nstutter but also to ensure their equitable and inclusive participation in the\nrapidly evolving digital landscape.", "published": "2024-05-10 00:16:58", "link": "http://arxiv.org/abs/2405.06150v1", "categories": ["cs.CL", "cs.CY", "eess.AS"], "primary_category": "cs.CL"}
{"title": "VLSM-Adapter: Finetuning Vision-Language Segmentation Efficiently with\n  Lightweight Blocks", "abstract": "Foundation Vision-Language Models (VLMs) trained using large-scale\nopen-domain images and text pairs have recently been adapted to develop\nVision-Language Segmentation Models (VLSMs) that allow providing text prompts\nduring inference to guide image segmentation. If robust and powerful VLSMs can\nbe built for medical images, it could aid medical professionals in many\nclinical tasks where they must spend substantial time delineating the target\nstructure of interest. VLSMs for medical images resort to fine-tuning base VLM\nor VLSM pretrained on open-domain natural image datasets due to fewer annotated\nmedical image datasets; this fine-tuning is resource-consuming and expensive as\nit usually requires updating all or a significant fraction of the pretrained\nparameters. Recently, lightweight blocks called adapters have been proposed in\nVLMs that keep the pretrained model frozen and only train adapters during\nfine-tuning, substantially reducing the computing resources required. We\nintroduce a novel adapter, VLSM-Adapter, that can fine-tune pretrained\nvision-language segmentation models using transformer encoders. Our experiments\nin widely used CLIP-based segmentation models show that with only 3 million\ntrainable parameters, the VLSM-Adapter outperforms state-of-the-art and is\ncomparable to the upper bound end-to-end fine-tuning. The source code is\navailable at: https://github.com/naamiinepal/vlsm-adapter.", "published": "2024-05-10 02:23:56", "link": "http://arxiv.org/abs/2405.06196v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language\n  Models", "abstract": "As one of the most advanced techniques in AI, Retrieval-Augmented Generation\n(RAG) can offer reliable and up-to-date external knowledge, providing huge\nconvenience for numerous tasks. Particularly in the era of AI-Generated Content\n(AIGC), the powerful capacity of retrieval in providing additional knowledge\nenables RAG to assist existing generative AI in producing high-quality outputs.\nRecently, Large Language Models (LLMs) have demonstrated revolutionary\nabilities in language understanding and generation, while still facing inherent\nlimitations, such as hallucinations and out-of-date internal knowledge. Given\nthe powerful abilities of RAG in providing the latest and helpful auxiliary\ninformation, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged\nto harness external and authoritative knowledge bases, rather than solely\nrelying on the model's internal knowledge, to augment the generation quality of\nLLMs. In this survey, we comprehensively review existing research studies in\nRA-LLMs, covering three primary technical perspectives: architectures, training\nstrategies, and applications. As the preliminary knowledge, we briefly\nintroduce the foundations and recent advances of LLMs. Then, to illustrate the\npractical significance of RAG for LLMs, we systematically review mainstream\nrelevant work by their architectures, training strategies, and application\nareas, detailing specifically the challenges of each and the corresponding\ncapabilities of RA-LLMs. Finally, to deliver deeper insights, we discuss\ncurrent limitations and several promising directions for future research.\nUpdated information about this survey can be found at\nhttps://advanced-recommender-systems.github.io/RAG-Meets-LLMs/", "published": "2024-05-10 02:48:45", "link": "http://arxiv.org/abs/2405.06211v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced\n  In-Context Learning in Healthcare", "abstract": "The integration of Large Language Models (LLMs) into healthcare diagnostics\noffers a promising avenue for clinical decision-making. This study outlines the\ndevelopment of a novel method for zero-shot/few-shot in-context learning (ICL)\nby integrating medical domain knowledge using a multi-layered structured\nprompt. We also explore the efficacy of two communication styles between the\nuser and LLMs: the Numerical Conversational (NC) style, which processes data\nincrementally, and the Natural Language Single-Turn (NL-ST) style, which\nemploys long narrative prompts.\n  Our study systematically evaluates the diagnostic accuracy and risk factors,\nincluding gender bias and false negative rates, using a dataset of 920 patient\nrecords in various few-shot scenarios. Results indicate that traditional\nclinical machine learning (ML) models generally outperform LLMs in zero-shot\nand few-shot settings. However, the performance gap narrows significantly when\nemploying few-shot examples alongside effective explainable AI (XAI) methods as\nsources of domain knowledge. Moreover, with sufficient time and an increased\nnumber of examples, the conversational style (NC) nearly matches the\nperformance of ML models. Most notably, LLMs demonstrate comparable or superior\ncost-sensitive accuracy relative to ML models.\n  This research confirms that, with appropriate domain knowledge and tailored\ncommunication strategies, LLMs can significantly enhance diagnostic processes.\nThe findings highlight the importance of optimizing the number of training\nexamples and communication styles to improve accuracy and reduce biases in LLM\napplications.", "published": "2024-05-10 06:52:44", "link": "http://arxiv.org/abs/2405.06270v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Correlation Dimension of Natural Language in a Statistical Manifold", "abstract": "The correlation dimension of natural language is measured by applying the\nGrassberger-Procaccia algorithm to high-dimensional sequences produced by a\nlarge-scale language model. This method, previously studied only in a Euclidean\nspace, is reformulated in a statistical manifold via the Fisher-Rao distance.\nLanguage exhibits a multifractal, with global self-similarity and a universal\ndimension around 6.5, which is smaller than those of simple discrete random\nsequences and larger than that of a Barab\\'asi-Albert process. Long memory is\nthe key to producing self-similarity. Our method is applicable to any\nprobabilistic model of real-world discrete sequences, and we show an\napplication to music data.", "published": "2024-05-10 08:48:03", "link": "http://arxiv.org/abs/2405.06321v2", "categories": ["cs.CL", "cond-mat.stat-mech", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Instruction Following in Language Models through Proxy-Based\n  Uncertainty Estimation", "abstract": "Assessing response quality to instructions in language models is vital but\nchallenging due to the complexity of human language across different contexts.\nThis complexity often results in ambiguous or inconsistent interpretations,\nmaking accurate assessment difficult. To address this issue, we propose a novel\nUncertainty-aware Reward Model (URM) that introduces a robust uncertainty\nestimation for the quality of paired responses based on Bayesian approximation.\nTrained with preference datasets, our uncertainty-enabled proxy not only scores\nrewards for responses but also evaluates their inherent uncertainty. Empirical\nresults demonstrate significant benefits of incorporating the proposed proxy\ninto language model training. Our method boosts the instruction following\ncapability of language models by refining data curation for training and\nimproving policy optimization objectives, thereby surpassing existing methods\nby a large margin on benchmarks such as Vicuna and MT-bench. These findings\nhighlight that our proposed approach substantially advances language model\ntraining and paves a new way of harnessing uncertainty within language models.", "published": "2024-05-10 12:14:11", "link": "http://arxiv.org/abs/2405.06424v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multimodal LLMs Struggle with Basic Visual Network Analysis: a VNA\n  Benchmark", "abstract": "We evaluate the zero-shot ability of GPT-4 and LLaVa to perform simple Visual\nNetwork Analysis (VNA) tasks on small-scale graphs. We evaluate the Vision\nLanguage Models (VLMs) on 5 tasks related to three foundational network science\nconcepts: identifying nodes of maximal degree on a rendered graph, identifying\nwhether signed triads are balanced or unbalanced, and counting components. The\ntasks are structured to be easy for a human who understands the underlying\ngraph theoretic concepts, and can all be solved by counting the appropriate\nelements in graphs. We find that while GPT-4 consistently outperforms LLaVa,\nboth models struggle with every visual network analysis task we propose. We\npublicly release the first benchmark for the evaluation of VLMs on foundational\nVNA tasks.", "published": "2024-05-10 17:51:35", "link": "http://arxiv.org/abs/2405.06634v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Federated Document Visual Question Answering: A Pilot Study", "abstract": "An important handicap of document analysis research is that documents tend to\nbe copyrighted or contain private information, which prohibits their open\npublication and the creation of centralised, large-scale document datasets.\nInstead, documents are scattered in private data silos, making extensive\ntraining over heterogeneous data a tedious task. In this work, we explore the\nuse of a federated learning (FL) scheme as a way to train a shared model on\ndecentralised private document data. We focus on the problem of Document VQA, a\ntask particularly suited to this approach, as the type of reasoning\ncapabilities required from the model can be quite different in diverse domains.\nEnabling training over heterogeneous document datasets can thus substantially\nenrich DocVQA models. We assemble existing DocVQA datasets from diverse domains\nto reflect the data heterogeneity in real-world applications. We explore the\nself-pretraining technique in this multi-modal setting, where the same data is\nused for both pretraining and finetuning, making it relevant for privacy\npreservation. We further propose combining self-pretraining with a Federated\nDocVQA training method using centralized adaptive optimization that outperforms\nthe FedAvg baseline. With extensive experiments, we also present a\nmulti-faceted analysis on training DocVQA models with FL, which provides\ninsights for future research on this task. We show that our pretraining\nstrategies can effectively learn and scale up under federated training with\ndiverse DocVQA datasets and tuning hyperparameters is essential for practical\ndocument tasks under federation.", "published": "2024-05-10 17:53:05", "link": "http://arxiv.org/abs/2405.06636v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Value Augmented Sampling for Language Model Alignment and\n  Personalization", "abstract": "Aligning Large Language Models (LLMs) to cater to different human\npreferences, learning new skills, and unlearning harmful behavior is an\nimportant problem. Search-based methods, such as Best-of-N or Monte-Carlo Tree\nSearch, are performant, but impractical for LLM adaptation due to their high\ninference cost. On the other hand, using Reinforcement Learning (RL) for\nadaptation is computationally efficient, but performs worse due to the\noptimization challenges in co-training the value function and the policy. We\npresent a new framework for reward optimization, Value Augmented Sampling\n(VAS), that can maximize different reward functions using data sampled from\nonly the initial, frozen LLM. VAS solves for the optimal reward-maximizing\npolicy without co-training the policy and the value function, making the\noptimization stable, outperforming established baselines, such as PPO and DPO,\non standard benchmarks, and achieving comparable results to Best-of-128 with\nlower inference cost. Unlike existing RL methods that require changing the\nweights of the LLM, VAS does not require access to the weights of the\npre-trained LLM. Thus, it can even adapt LLMs (e.g., ChatGPT), which are\navailable only as APIs. In addition, our algorithm unlocks the new capability\nof composing several rewards and controlling the extent of each one during\ndeployment time, paving the road ahead for the future of aligned, personalized\nLLMs.", "published": "2024-05-10 17:59:04", "link": "http://arxiv.org/abs/2405.06639v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "On the Shape of Brainscores for Large Language Models (LLMs)", "abstract": "With the rise of Large Language Models (LLMs), the novel metric \"Brainscore\"\nemerged as a means to evaluate the functional similarity between LLMs and human\nbrain/neural systems. Our efforts were dedicated to mining the meaning of the\nnovel score by constructing topological features derived from both human fMRI\ndata involving 190 subjects, and 39 LLMs plus their untrained counterparts.\nSubsequently, we trained 36 Linear Regression Models and conducted thorough\nstatistical analyses to discern reliable and valid features from our\nconstructed ones. Our findings reveal distinctive feature combinations\nconducive to interpreting existing brainscores across various brain regions of\ninterest (ROIs) and hemispheres, thereby significantly contributing to\nadvancing interpretable machine learning (iML) studies. The study is enriched\nby our further discussions and analyses concerning existing brainscores. To our\nknowledge, this study represents the first attempt to comprehend the novel\nmetric brainscore within this interdisciplinary domain.", "published": "2024-05-10 13:22:20", "link": "http://arxiv.org/abs/2405.06725v3", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "q-bio.NC"}
{"title": "CANAL -- Cyber Activity News Alerting Language Model: Empirical Approach\n  vs. Expensive LLM", "abstract": "In today's digital landscape, where cyber attacks have become the norm, the\ndetection of cyber attacks and threats is critically imperative across diverse\ndomains. Our research presents a new empirical framework for cyber threat\nmodeling, adept at parsing and categorizing cyber-related information from news\narticles, enhancing real-time vigilance for market stakeholders. At the core of\nthis framework is a fine-tuned BERT model, which we call CANAL - Cyber Activity\nNews Alerting Language Model, tailored for cyber categorization using a novel\nsilver labeling approach powered by Random Forest. We benchmark CANAL against\nlarger, costlier LLMs, including GPT-4, LLaMA, and Zephyr, highlighting their\nzero to few-shot learning in cyber news classification. CANAL demonstrates\nsuperior performance by outperforming all other LLM counterparts in both\naccuracy and cost-effectiveness. Furthermore, we introduce the Cyber Signal\nDiscovery module, a strategic component designed to efficiently detect emerging\ncyber signals from news articles. Collectively, CANAL and Cyber Signal\nDiscovery module equip our framework to provide a robust and cost-effective\nsolution for businesses that require agile responses to cyber intelligence.", "published": "2024-05-10 18:57:35", "link": "http://arxiv.org/abs/2405.06772v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "68T50, 68T07 (Primary) 03B65, 91F20 (Secondary)", "I.2.7; I.2.1; I.5.1; I.5.2; I.5.4; H.3.3"], "primary_category": "cs.CR"}
{"title": "Large Language Model in Financial Regulatory Interpretation", "abstract": "This study explores the innovative use of Large Language Models (LLMs) as\nanalytical tools for interpreting complex financial regulations. The primary\nobjective is to design effective prompts that guide LLMs in distilling verbose\nand intricate regulatory texts, such as the Basel III capital requirement\nregulations, into a concise mathematical framework that can be subsequently\ntranslated into actionable code. This novel approach aims to streamline the\nimplementation of regulatory mandates within the financial reporting and risk\nmanagement systems of global banking institutions. A case study was conducted\nto assess the performance of various LLMs, demonstrating that GPT-4 outperforms\nother models in processing and collecting necessary information, as well as\nexecuting mathematical calculations. The case study utilized numerical\nsimulations with asset holdings -- including fixed income, equities, currency\npairs, and commodities -- to demonstrate how LLMs can effectively implement the\nBasel III capital adequacy requirements.\n  Keywords: Large Language Models, Prompt Engineering, LLMs in Finance, Basel\nIII, Minimum Capital Requirements, LLM Ethics", "published": "2024-05-10 20:45:40", "link": "http://arxiv.org/abs/2405.06808v2", "categories": ["q-fin.RM", "cs.AI", "cs.CL"], "primary_category": "q-fin.RM"}
{"title": "Learning from String Sequences", "abstract": "The Universal Similarity Metric (USM) has been demonstrated to give\npractically useful measures of \"similarity\" between sequence data. Here we have\nused the USM as an alternative distance metric in a K-Nearest Neighbours (K-NN)\nlearner to allow effective pattern recognition of variable length sequence\ndata. We compare this USM approach with the commonly used string-to-word vector\napproach. Our experiments have used two data sets of divergent domains: (1)\nspam email filtering and (2) protein subcellular localization. Our results with\nthis data reveal that the USM-based K-NN learner (1) gives predictions with\nhigher classification accuracy than those output by techniques that use the\nstring-to-word vector approach, and (2) can be used to generate reliable\nprobability forecasts.", "published": "2024-05-10 08:09:53", "link": "http://arxiv.org/abs/2405.06301v1", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Look Once to Hear: Target Speech Hearing with Noisy Examples", "abstract": "In crowded settings, the human brain can focus on speech from a target\nspeaker, given prior knowledge of how they sound. We introduce a novel\nintelligent hearable system that achieves this capability, enabling target\nspeech hearing to ignore all interfering speech and noise, but the target\nspeaker. A naive approach is to require a clean speech example to enroll the\ntarget speaker. This is however not well aligned with the hearable application\ndomain since obtaining a clean example is challenging in real world scenarios,\ncreating a unique user interface problem. We present the first enrollment\ninterface where the wearer looks at the target speaker for a few seconds to\ncapture a single, short, highly noisy, binaural example of the target speaker.\nThis noisy example is used for enrollment and subsequent speech extraction in\nthe presence of interfering speakers and noise. Our system achieves a signal\nquality improvement of 7.01 dB using less than 5 seconds of noisy enrollment\naudio and can process 8 ms of audio chunks in 6.24 ms on an embedded CPU. Our\nuser studies demonstrate generalization to real-world static and mobile\nspeakers in previously unseen indoor and outdoor multipath environments.\nFinally, our enrollment interface for noisy examples does not cause performance\ndegradation compared to clean examples, while being convenient and\nuser-friendly. Taking a step back, this paper takes an important step towards\nenhancing the human auditory perception with artificial intelligence. We\nprovide code and data at: https://github.com/vb000/LookOnceToHear.", "published": "2024-05-10 07:44:18", "link": "http://arxiv.org/abs/2405.06289v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Investigation of Incorporating Mamba for Speech Enhancement", "abstract": "This work aims to study a scalable state-space model (SSM), Mamba, for the\nspeech enhancement (SE) task. We exploit a Mamba-based regression model to\ncharacterize speech signals and build an SE system upon Mamba, termed SEMamba.\nWe explore the properties of Mamba by integrating it as the core model in both\nbasic and advanced SE systems, along with utilizing signal-level distances as\nwell as metric-oriented loss functions. SEMamba demonstrates promising results\nand attains a PESQ score of 3.55 on the VoiceBank-DEMAND dataset. When combined\nwith the perceptual contrast stretching technique, the proposed SEMamba yields\na new state-of-the-art PESQ score of 3.69.", "published": "2024-05-10 16:18:49", "link": "http://arxiv.org/abs/2405.06573v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Music Emotion Prediction Using Recurrent Neural Networks", "abstract": "This study explores the application of recurrent neural networks to recognize\nemotions conveyed in music, aiming to enhance music recommendation systems and\nsupport therapeutic interventions by tailoring music to fit listeners'\nemotional states. We utilize Russell's Emotion Quadrant to categorize music\ninto four distinct emotional regions and develop models capable of accurately\npredicting these categories. Our approach involves extracting a comprehensive\nset of audio features using Librosa and applying various recurrent neural\nnetwork architectures, including standard RNNs, Bidirectional RNNs, and Long\nShort-Term Memory (LSTM) networks. Initial experiments are conducted using a\ndataset of 900 audio clips, labeled according to the emotional quadrants. We\ncompare the performance of our neural network models against a set of baseline\nclassifiers and analyze their effectiveness in capturing the temporal dynamics\ninherent in musical expression. The results indicate that simpler RNN\narchitectures may perform comparably or even superiorly to more complex models,\nparticularly in smaller datasets. We've also applied the following experiments\non larger datasets: one is augmented based on our original dataset, and the\nother is from other sources. This research not only enhances our understanding\nof the emotional impact of music but also demonstrates the potential of neural\nnetworks in creating more personalized and emotionally resonant music\nrecommendation and therapy systems.", "published": "2024-05-10 18:03:20", "link": "http://arxiv.org/abs/2405.06747v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Time-of-arrival Estimation and Phase Unwrapping of Head-related Transfer\n  Functions With Integer Linear Programming", "abstract": "In binaural audio synthesis, aligning head-related impulse responses (HRIRs)\nin time has been an important pre-processing step, enabling accurate spatial\ninterpolation and efficient data compression. The maximum correlation time\ndelay between spatially nearby HRIRs has previously been used to get accurate\nand smooth alignment by solving a matrix equation in which the solution has the\nminimum Euclidean distance to the time delay. However, the Euclidean criterion\ncould lead to an over-smoothing solution in practice. In this paper, we solve\nthe smoothing issue by formulating the task as solving an integer linear\nprogramming problem equivalent to minimising an $L^1$-norm. Moreover, we\nincorporate 1) the cross-correlation of inter-aural HRIRs, and 2) HRIRs with\ntheir minimum-phase responses to have more reference measurements for\noptimisation. We show the proposed method can get more accurate alignments than\nthe Euclidean-based method by comparing the spectral reconstruction loss of\ntime-aligned HRIRs using spherical harmonics representation on seven HRIRs\nconsisting of human and dummy heads. The extra correlation features and the\n$L^1$-norm are also beneficial in extremely noisy conditions. In addition, this\nmethod can be applied to phase unwrapping of head-related transfer functions,\nwhere the unwrapped phase could be a compact feature for downstream tasks.", "published": "2024-05-10 20:34:52", "link": "http://arxiv.org/abs/2405.06804v2", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "FunnelNet: An End-to-End Deep Learning Framework to Monitor Digital\n  Heart Murmur in Real-Time", "abstract": "Objective: Heart murmurs are abnormal sounds caused by turbulent blood flow\nwithin the heart. Several diagnostic methods are available to detect heart\nmurmurs and their severity, such as cardiac auscultation, echocardiography,\nphonocardiogram (PCG), etc. However, these methods have limitations, including\nextensive training and experience among healthcare providers, cost and\naccessibility of echocardiography, as well as noise interference and PCG data\nprocessing. This study aims to develop a novel end-to-end real-time heart\nmurmur detection approach using traditional and depthwise separable\nconvolutional networks. Methods: Continuous wavelet transform (CWT) was applied\nto extract meaningful features from the PCG data. The proposed network has\nthree parts: the Squeeze net, the Bottleneck, and the Expansion net. The\nSqueeze net generates a compressed data representation, whereas the Bottleneck\nlayer reduces computational complexity using a depthwise-separable\nconvolutional network. The Expansion net is responsible for up-sampling the\ncompressed data to a higher dimension, capturing tiny details of the\nrepresentative data. Results: For evaluation, we used four publicly available\ndatasets and achieved state-of-the-art performance in all datasets.\nFurthermore, we tested our proposed network on two resource-constrained\ndevices: a Raspberry PI and an Android device, stripping it down into a tiny\nmachine learning model (TinyML), achieving a maximum of 99.70%. Conclusion: The\nproposed model offers a deep learning framework for real-time accurate heart\nmurmur detection within limited resources. Significance: It will significantly\nresult in more accessible and practical medical services and reduced diagnosis\ntime to assist medical professionals. The code is publicly available at TBA.", "published": "2024-05-10 03:12:17", "link": "http://arxiv.org/abs/2405.09570v1", "categories": ["eess.SP", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
