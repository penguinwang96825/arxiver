{"title": "How do media talk about the Covid-19 pandemic? Metaphorical thematic\n  clustering in Italian online newspapers", "abstract": "The contribution presents a study on figurative language of the first months\nof the COVID-19 crisis in Italian online newspapers. Particularly, we contrast\ntopics and metaphorical language used by journalists in the first and second\nphase of the government response to the pandemic in Spring 2020. The analysis\nis conducted on a journalistic corpus collected between February 24th and June\n3rd, 2020. The analysis is performed using both quantitative and qualitative\napproaches, combining Structural Topic Modelling (Roberts et al. 2016),\nConceptual Metaphor Theory (Lakoff & Johnson, 1980), and qualitative-corpus\nbased metaphor analysis (Charteris-Black, 2004). We find a significant shift in\ntopics discussed across Phase 1 and Phase 2, and interesting overlaps in\ntopic-specific metaphors. Using qualitative corpus analysis, we present a more\nin-depth case study discussing metaphorical collocations of the topics of\nEconomy and Society", "published": "2022-04-05 10:55:33", "link": "http://arxiv.org/abs/2204.02106v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved and Efficient Conversational Slot Labeling through Question\n  Answering", "abstract": "Transformer-based pretrained language models (PLMs) offer unmatched\nperformance across the majority of natural language understanding (NLU) tasks,\nincluding a body of question answering (QA) tasks. We hypothesize that\nimprovements in QA methodology can also be directly exploited in dialog NLU;\nhowever, dialog tasks must be \\textit{reformatted} into QA tasks. In\nparticular, we focus on modeling and studying \\textit{slot labeling} (SL), a\ncrucial component of NLU for dialog, through the QA optics, aiming to improve\nboth its performance and efficiency, and make it more effective and resilient\nto working with limited task data. To this end, we make a series of\ncontributions: 1) We demonstrate how QA-tuned PLMs can be applied to the SL\ntask, reaching new state-of-the-art performance, with large gains especially\npronounced in such low-data regimes. 2) We propose to leverage contextual\ninformation, required to tackle ambiguous values, simply through natural\nlanguage. 3) Efficiency and compactness of QA-oriented fine-tuning are boosted\nthrough the use of lightweight yet effective adapter modules. 4) Trading-off\nsome of the quality of QA datasets for their size, we experiment with larger\nautomatically generated QA datasets for QA-tuning, arriving at even higher\nperformance. Finally, our analysis suggests that our novel QA-based slot\nlabeling models, supported by the PLMs, reach a performance ceiling in\nhigh-data regimes, calling for more challenging and more nuanced benchmarks in\nfuture work.", "published": "2022-04-05 11:34:35", "link": "http://arxiv.org/abs/2204.02123v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Anchors' Opinion in Hinghlish News Delivery", "abstract": "Humans like to express their opinions and crave the opinions of others.\nMining and detecting opinions from various sources are beneficial to\nindividuals, organisations, and even governments. One such organisation is news\nmedia, where a general norm is not to showcase opinions from their side.\nAnchors are the face of the digital media, and it is required for them not to\nbe opinionated. However, at times, they diverge from the accepted norm and\ninsert their opinions into otherwise straightforward news reports, either\npurposefully or unintentionally. This is primarily seen in debates as it\nrequires the anchors to be spontaneous, thus making them vulnerable to add\ntheir opinions. The consequence of such mishappening might lead to biased news\nor even supporting a certain agenda at the worst. To this end, we propose a\nnovel task of anchors' opinion detection in debates. We curate code-mixed news\ndebates and develop the ODIN dataset. A total of 2054 anchors' utterances in\nthe dataset are marked as opinionated or non-opinionated. Lastly, we propose\nDetONADe, an interactive attention-based framework for classifying anchors'\nutterances and obtain the best weighted-F1 score of 0.703. A thorough analysis\nand evaluation show many interesting patterns in the dataset and predictions.", "published": "2022-04-05 12:26:46", "link": "http://arxiv.org/abs/2204.02155v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EntSUM: A Data Set for Entity-Centric Summarization", "abstract": "Controllable summarization aims to provide summaries that take into account\nuser-specified aspects and preferences to better assist them with their\ninformation need, as opposed to the standard summarization setup which build a\nsingle generic summary of a document. We introduce a human-annotated data set\nEntSUM for controllable summarization with a focus on named entities as the\naspects to control. We conduct an extensive quantitative analysis to motivate\nthe task of entity-centric summarization and show that existing methods for\ncontrollable summarization fail to generate entity-centric summaries. We\npropose extensions to state-of-the-art summarization approaches that achieve\nsubstantially better results on our data set. Our analysis and results show the\nchallenging nature of this task and of the proposed data set.", "published": "2022-04-05 13:45:54", "link": "http://arxiv.org/abs/2204.02213v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PaLM: Scaling Language Modeling with Pathways", "abstract": "Large language models have been shown to achieve remarkable performance\nacross a variety of natural language tasks using few-shot learning, which\ndrastically reduces the number of task-specific training examples needed to\nadapt the model to a particular application. To further our understanding of\nthe impact of scale on few-shot learning, we trained a 540-billion parameter,\ndensely activated, Transformer language model, which we call Pathways Language\nModel PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML\nsystem which enables highly efficient training across multiple TPU Pods. We\ndemonstrate continued benefits of scaling by achieving state-of-the-art\nfew-shot learning results on hundreds of language understanding and generation\nbenchmarks. On a number of these tasks, PaLM 540B achieves breakthrough\nperformance, outperforming the finetuned state-of-the-art on a suite of\nmulti-step reasoning tasks, and outperforming average human performance on the\nrecently released BIG-bench benchmark. A significant number of BIG-bench tasks\nshowed discontinuous improvements from model scale, meaning that performance\nsteeply increased as we scaled to our largest model. PaLM also has strong\ncapabilities in multilingual tasks and source code generation, which we\ndemonstrate on a wide array of benchmarks. We additionally provide a\ncomprehensive analysis on bias and toxicity, and study the extent of training\ndata memorization with respect to model scale. Finally, we discuss the ethical\nconsiderations related to large language models and discuss potential\nmitigation strategies.", "published": "2022-04-05 16:11:45", "link": "http://arxiv.org/abs/2204.02311v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynatask: A Framework for Creating Dynamic AI Benchmark Tasks", "abstract": "We introduce Dynatask: an open source system for setting up custom NLP tasks\nthat aims to greatly lower the technical knowledge and effort required for\nhosting and evaluating state-of-the-art NLP models, as well as for conducting\nmodel in the loop data collection with crowdworkers. Dynatask is integrated\nwith Dynabench, a research platform for rethinking benchmarking in AI that\nfacilitates human and model in the loop data collection and evaluation. To\ncreate a task, users only need to write a short task configuration file from\nwhich the relevant web interfaces and model hosting infrastructure are\nautomatically generated. The system is available at https://dynabench.org/ and\nthe full library can be found at https://github.com/facebookresearch/dynabench.", "published": "2022-04-05 00:32:04", "link": "http://arxiv.org/abs/2204.01906v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data Augmentation for Intent Classification with Off-the-shelf Large\n  Language Models", "abstract": "Data augmentation is a widely employed technique to alleviate the problem of\ndata scarcity. In this work, we propose a prompting-based approach to generate\nlabelled training data for intent classification with off-the-shelf language\nmodels (LMs) such as GPT-3. An advantage of this method is that no\ntask-specific LM-fine-tuning for data generation is required; hence the method\nrequires no hyper-parameter tuning and is applicable even when the available\ntraining data is very scarce. We evaluate the proposed method in a few-shot\nsetting on four diverse intent classification tasks. We find that GPT-generated\ndata significantly boosts the performance of intent classifiers when intents in\nconsideration are sufficiently distinct from each other. In tasks with\nsemantically close intents, we observe that the generated data is less helpful.\nOur analysis shows that this is because GPT often generates utterances that\nbelong to a closely-related intent instead of the desired one. We present\npreliminary evidence that a prompting-based GPT classifier could be helpful in\nfiltering the generated data to enhance its quality.", "published": "2022-04-05 03:29:26", "link": "http://arxiv.org/abs/2204.01959v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The COVMis-Stance dataset: Stance Detection on Twitter for COVID-19\n  Misinformation", "abstract": "During the COVID-19 pandemic, large amounts of COVID-19 misinformation are\nspreading on social media. We are interested in the stance of Twitter users\ntowards COVID-19 misinformation. However, due to the relative recent nature of\nthe pandemic, only a few stance detection datasets fit our task. We have\nconstructed a new stance dataset consisting of 2631 tweets annotated with the\nstance towards COVID-19 misinformation. In contexts with limited labeled data,\nwe fine-tune our models by leveraging the MNLI dataset and two existing stance\ndetection datasets (RumourEval and COVIDLies), and evaluate the model\nperformance on our dataset. Our experimental results show that the model\nperforms the best when fine-tuned sequentially on the MNLI dataset and the\ncombination of the undersampled RumourEval and COVIDLies datasets. Our code and\ndataset are publicly available at\nhttps://github.com/yanfangh/covid-rumor-stance", "published": "2022-04-05 05:47:15", "link": "http://arxiv.org/abs/2204.02000v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Fact Checking with Insufficient Evidence", "abstract": "Automating the fact checking (FC) process relies on information obtained from\nexternal sources. In this work, we posit that it is crucial for FC models to\nmake veracity predictions only when there is sufficient evidence and otherwise\nindicate when it is not enough. To this end, we are the first to study what\ninformation FC models consider sufficient by introducing a novel task and\nadvancing it with three main contributions. First, we conduct an in-depth\nempirical analysis of the task with a new fluency-preserving method for\nomitting information from the evidence at the constituent and sentence level.\nWe identify when models consider the remaining evidence (in)sufficient for FC,\nbased on three trained models with different Transformer architectures and\nthree FC datasets. Second, we ask annotators whether the omitted evidence was\nimportant for FC, resulting in a novel diagnostic dataset, SufficientFacts, for\nFC with omitted evidence. We find that models are least successful in detecting\nmissing evidence when adverbial modifiers are omitted (21% accuracy), whereas\nit is easiest for omitted date modifiers (63% accuracy). Finally, we propose a\nnovel data augmentation strategy for contrastive self-learning of missing\nevidence by employing the proposed omission method combined with tri-training.\nIt improves performance for Evidence Sufficiency Prediction by up to 17.8 F1\nscore, which in turn improves FC performance by up to 2.6 F1 score.", "published": "2022-04-05 06:12:42", "link": "http://arxiv.org/abs/2204.02007v1", "categories": ["cs.CL", "cs.LG", "cs.CL"], "primary_category": "cs.CL"}
{"title": "$\\textit{latent}$-GLAT: Glancing at Latent Variables for Parallel Text\n  Generation", "abstract": "Recently, parallel text generation has received widespread attention due to\nits success in generation efficiency. Although many advanced techniques are\nproposed to improve its generation quality, they still need the help of an\nautoregressive model for training to overcome the one-to-many multi-modal\nphenomenon in the dataset, limiting their applications. In this paper, we\npropose $\\textit{latent}$-GLAT, which employs the discrete latent variables to\ncapture word categorical information and invoke an advanced curriculum learning\ntechnique, alleviating the multi-modality problem. Experiment results show that\nour method outperforms strong baselines without the help of an autoregressive\nmodel, which further broadens the application scenarios of the parallel\ndecoding paradigm.", "published": "2022-04-05 07:34:12", "link": "http://arxiv.org/abs/2204.02030v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HyperBox: A Supervised Approach for Hypernym Discovery using Box\n  Embeddings", "abstract": "Hypernymy plays a fundamental role in many AI tasks like taxonomy learning,\nontology learning, etc. This has motivated the development of many automatic\nidentification methods for extracting this relation, most of which rely on word\ndistribution. We present a novel model HyperBox to learn box embeddings for\nhypernym discovery. Given an input term, HyperBox retrieves its suitable\nhypernym from a target corpus. For this task, we use the dataset published for\nSemEval 2018 Shared Task on Hypernym Discovery. We compare the performance of\nour model on two specific domains of knowledge: medical and music.\nExperimentally, we show that our model outperforms existing methods on the\nmajority of the evaluation metrics. Moreover, our model generalize well over\nunseen hypernymy pairs using only a small set of training data.", "published": "2022-04-05 08:46:50", "link": "http://arxiv.org/abs/2204.02058v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Design considerations for a hierarchical semantic compositional\n  framework for medical natural language understanding", "abstract": "Medical natural language processing (NLP) systems are a key enabling\ntechnology for transforming Big Data from clinical report repositories to\ninformation used to support disease models and validate intervention methods.\nHowever, current medical NLP systems fall considerably short when faced with\nthe task of logically interpreting clinical text. In this paper, we describe a\nframework inspired by mechanisms of human cognition in an attempt to jump the\nNLP performance curve. The design centers about a hierarchical semantic\ncompositional model (HSCM) which provides an internal substrate for guiding the\ninterpretation process. The paper describes insights from four key cognitive\naspects including semantic memory, semantic composition, semantic activation,\nand hierarchical predictive coding. We discuss the design of a generative\nsemantic model and an associated semantic parser used to transform a free-text\nsentence into a logical representation of its meaning. The paper discusses\nsupportive and antagonistic arguments for the key features of the architecture\nas a long-term foundational framework.", "published": "2022-04-05 09:04:34", "link": "http://arxiv.org/abs/2204.02067v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multilinguals at SemEval-2022 Task 11: Transformer Based Architecture\n  for Complex NER", "abstract": "We investigate the task of complex NER for the English language. The task is\nnon-trivial due to the semantic ambiguity of the textual structure and the\nrarity of occurrence of such entities in the prevalent literature. Using\npre-trained language models such as BERT, we obtain a competitive performance\non this task. We qualitatively analyze the performance of multiple\narchitectures for this task. All our models are able to outperform the baseline\nby a significant margin. Our best performing model beats the baseline F1-score\nby over 9%.", "published": "2022-04-05 12:58:57", "link": "http://arxiv.org/abs/2204.02173v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-View Transformer for 3D Visual Grounding", "abstract": "The 3D visual grounding task aims to ground a natural language description to\nthe targeted object in a 3D scene, which is usually represented in 3D point\nclouds. Previous works studied visual grounding under specific views. The\nvision-language correspondence learned by this way can easily fail once the\nview changes. In this paper, we propose a Multi-View Transformer (MVT) for 3D\nvisual grounding. We project the 3D scene to a multi-view space, in which the\nposition information of the 3D scene under different views are modeled\nsimultaneously and aggregated together. The multi-view space enables the\nnetwork to learn a more robust multi-modal representation for 3D visual\ngrounding and eliminates the dependence on specific views. Extensive\nexperiments show that our approach significantly outperforms all\nstate-of-the-art methods. Specifically, on Nr3D and Sr3D datasets, our method\noutperforms the best competitor by 11.2% and 7.1% and even surpasses recent\nwork with extra 2D assistance by 5.9% and 6.6%. Our code is available at\nhttps://github.com/sega-hsj/MVT-3DVG.", "published": "2022-04-05 12:59:43", "link": "http://arxiv.org/abs/2204.02174v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Improving Generalizability in Implicitly Abusive Language Detection with\n  Concept Activation Vectors", "abstract": "Robustness of machine learning models on ever-changing real-world data is\ncritical, especially for applications affecting human well-being such as\ncontent moderation. New kinds of abusive language continually emerge in online\ndiscussions in response to current events (e.g., COVID-19), and the deployed\nabuse detection systems should be updated regularly to remain accurate. In this\npaper, we show that general abusive language classifiers tend to be fairly\nreliable in detecting out-of-domain explicitly abusive utterances but fail to\ndetect new types of more subtle, implicit abuse. Next, we propose an\ninterpretability technique, based on the Testing Concept Activation Vector\n(TCAV) method from computer vision, to quantify the sensitivity of a trained\nmodel to the human-defined concepts of explicit and implicit abusive language,\nand use that to explain the generalizability of the model on new data, in this\ncase, COVID-related anti-Asian hate speech. Extending this technique, we\nintroduce a novel metric, Degree of Explicitness, for a single instance and\nshow that the new metric is beneficial in suggesting out-of-domain unlabeled\nexamples to effectively enrich the training data with informative, implicitly\nabusive texts.", "published": "2022-04-05 14:52:18", "link": "http://arxiv.org/abs/2204.02261v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Neural Reranking for Cross-Lingual and Multilingual\n  Retrieval", "abstract": "State-of-the-art neural (re)rankers are notoriously data-hungry which --\ngiven the lack of large-scale training data in languages other than English --\nmakes them rarely used in multilingual and cross-lingual retrieval settings.\nCurrent approaches therefore commonly transfer rankers trained on English data\nto other languages and cross-lingual setups by means of multilingual encoders:\nthey fine-tune all parameters of pretrained massively multilingual Transformers\n(MMTs, e.g., multilingual BERT) on English relevance judgments, and then deploy\nthem in the target language(s). In this work, we show that two\nparameter-efficient approaches to cross-lingual transfer, namely Sparse\nFine-Tuning Masks (SFTMs) and Adapters, allow for a more lightweight and more\neffective zero-shot transfer to multilingual and cross-lingual retrieval tasks.\nWe first train language adapters (or SFTMs) via Masked Language Modelling and\nthen train retrieval (i.e., reranking) adapters (SFTMs) on top, while keeping\nall other parameters fixed. At inference, this modular design allows us to\ncompose the ranker by applying the (re)ranking adapter (or SFTM) trained with\nsource language data together with the language adapter (or SFTM) of a target\nlanguage. We carry out a large scale evaluation on the CLEF-2003 and HC4\nbenchmarks and additionally, as another contribution, extend the former with\nqueries in three new languages: Kyrgyz, Uyghur and Turkish. The proposed\nparameter-efficient methods outperform standard zero-shot transfer with full\nMMT fine-tuning, while being more modular and reducing training times. The\ngains are particularly pronounced for low-resource languages, where our\napproaches also substantially outperform the competitive machine\ntranslation-based rankers.", "published": "2022-04-05 15:44:27", "link": "http://arxiv.org/abs/2204.02292v2", "categories": ["cs.CL", "cs.IR", "H.3.3; I.2.7"], "primary_category": "cs.CL"}
{"title": "Towards Best Practices for Training Multilingual Dense Retrieval Models", "abstract": "Dense retrieval models using a transformer-based bi-encoder design have\nemerged as an active area of research. In this work, we focus on the task of\nmonolingual retrieval in a variety of typologically diverse languages using one\nsuch design. Although recent work with multilingual transformers demonstrates\nthat they exhibit strong cross-lingual generalization capabilities, there\nremain many open research questions, which we tackle here. Our study is\norganized as a \"best practices\" guide for training multilingual dense retrieval\nmodels, broken down into three main scenarios: where a multilingual transformer\nis available, but relevance judgments are not available in the language of\ninterest; where both models and training data are available; and, where\ntraining data are available not but models. In considering these scenarios, we\ngain a better understanding of the role of multi-stage fine-tuning, the\nstrength of cross-lingual transfer under various conditions, the usefulness of\nout-of-language data, and the advantages of multilingual vs. monolingual\ntransformers. Our recommendations offer a guide for practitioners building\nsearch applications, particularly for low-resource languages, and while our\nwork leaves open a number of research questions, we provide a solid foundation\nfor future work.", "published": "2022-04-05 17:12:53", "link": "http://arxiv.org/abs/2204.02363v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "CLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations", "abstract": "Providing explanations in the context of Visual Question Answering (VQA)\npresents a fundamental problem in machine learning. To obtain detailed insights\ninto the process of generating natural language explanations for VQA, we\nintroduce the large-scale CLEVR-X dataset that extends the CLEVR dataset with\nnatural language explanations. For each image-question pair in the CLEVR\ndataset, CLEVR-X contains multiple structured textual explanations which are\nderived from the original scene graphs. By construction, the CLEVR-X\nexplanations are correct and describe the reasoning and visual information that\nis necessary to answer a given question. We conducted a user study to confirm\nthat the ground-truth explanations in our proposed dataset are indeed complete\nand relevant. We present baseline results for generating natural language\nexplanations in the context of VQA using two state-of-the-art frameworks on the\nCLEVR-X dataset. Furthermore, we provide a detailed analysis of the explanation\ngeneration quality for different question and answer types. Additionally, we\nstudy the influence of using different numbers of ground-truth explanations on\nthe convergence of natural language generation (NLG) metrics. The CLEVR-X\ndataset is publicly available at\n\\url{https://explainableml.github.io/CLEVR-X/}.", "published": "2022-04-05 17:38:04", "link": "http://arxiv.org/abs/2204.02380v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Considerations for Multilingual Wikipedia Research", "abstract": "English Wikipedia has long been an important data source for much research\nand natural language machine learning modeling. The growth of non-English\nlanguage editions of Wikipedia, greater computational resources, and calls for\nequity in the performance of language and multimodal models have led to the\ninclusion of many more language editions of Wikipedia in datasets and models.\nBuilding better multilingual and multimodal models requires more than just\naccess to expanded datasets; it also requires a better understanding of what is\nin the data and how this content was generated. This paper seeks to provide\nsome background to help researchers think about what differences might arise\nbetween different language editions of Wikipedia and how that might affect\ntheir models. It details three major ways in which content differences between\nlanguage editions arise (local context, community and governance, and\ntechnology) and recommendations for good practices when using multilingual and\nmultimodal data for research and modeling.", "published": "2022-04-05 20:34:15", "link": "http://arxiv.org/abs/2204.02483v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Inferring Rewards from Language in Context", "abstract": "In classic instruction following, language like \"I'd like the JetBlue flight\"\nmaps to actions (e.g., selecting that flight). However, language also conveys\ninformation about a user's underlying reward function (e.g., a general\npreference for JetBlue), which can allow a model to carry out desirable actions\nin new contexts. We present a model that infers rewards from language\npragmatically: reasoning about how speakers choose utterances not only to\nelicit desired actions, but also to reveal information about their preferences.\nOn a new interactive flight-booking task with natural language, our model more\naccurately infers rewards and predicts optimal actions in unseen environments,\nin comparison to past work that first maps language to actions (instruction\nfollowing) and then maps actions to rewards (inverse reinforcement learning).", "published": "2022-04-05 23:04:18", "link": "http://arxiv.org/abs/2204.02515v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Complementary Joint Training Approach Using Unpaired Speech and Text\n  for Low-Resource Automatic Speech Recognition", "abstract": "Unpaired data has shown to be beneficial for low-resource automatic speech\nrecognition~(ASR), which can be involved in the design of hybrid models with\nmulti-task training or language model dependent pre-training. In this work, we\nleverage unpaired data to train a general sequence-to-sequence model. Unpaired\nspeech and text are used in the form of data pairs by generating the\ncorresponding missing parts in prior to model training. Inspired by the\ncomplementarity of speech-PseudoLabel pair and SynthesizedAudio-text pair in\nboth acoustic features and linguistic features, we propose a complementary\njoint training~(CJT) method that trains a model alternatively with two data\npairs. Furthermore, label masking for pseudo-labels and gradient restriction\nfor synthesized audio are proposed to further cope with the deviations from\nreal data, termed as CJT++. Experimental results show that compared to\nspeech-only training, the proposed basic CJT achieves great performance\nimprovements on clean/other test sets, and the CJT++ re-training yields further\nperformance enhancements. It is also apparent that the proposed method\noutperforms the wav2vec2.0 model with the same model size and beam size,\nparticularly in extreme low-resource cases.", "published": "2022-04-05 07:02:53", "link": "http://arxiv.org/abs/2204.02023v1", "categories": ["cs.SD", "cs.CL", "eess.AS", "I.2.7"], "primary_category": "cs.SD"}
{"title": "Abstractive summarization of hospitalisation histories with transformer\n  networks", "abstract": "In this paper we present a novel approach to abstractive summarization of\npatient hospitalisation histories. We applied an encoder-decoder framework with\nLongformer neural network as an encoder and BERT as a decoder. Our experiments\nshow improved quality on some summarization tasks compared with\npointer-generator networks. We also conducted a study with experienced\nphysicians evaluating the results of our model in comparison with PGN baseline\nand human-generated abstracts, which showed the effectiveness of our model.", "published": "2022-04-05 13:38:39", "link": "http://arxiv.org/abs/2204.02208v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Repeat after me: Self-supervised learning of acoustic-to-articulatory\n  mapping by vocal imitation", "abstract": "We propose a computational model of speech production combining a pre-trained\nneural articulatory synthesizer able to reproduce complex speech stimuli from a\nlimited set of interpretable articulatory parameters, a DNN-based internal\nforward model predicting the sensory consequences of articulatory commands, and\nan internal inverse model based on a recurrent neural network recovering\narticulatory commands from the acoustic speech input. Both forward and inverse\nmodels are jointly trained in a self-supervised way from raw acoustic-only\nspeech data from different speakers. The imitation simulations are evaluated\nobjectively and subjectively and display quite encouraging performances.", "published": "2022-04-05 15:02:49", "link": "http://arxiv.org/abs/2204.02269v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SwapMix: Diagnosing and Regularizing the Over-Reliance on Visual Context\n  in Visual Question Answering", "abstract": "While Visual Question Answering (VQA) has progressed rapidly, previous works\nraise concerns about robustness of current VQA models. In this work, we study\nthe robustness of VQA models from a novel perspective: visual context. We\nsuggest that the models over-rely on the visual context, i.e., irrelevant\nobjects in the image, to make predictions. To diagnose the model's reliance on\nvisual context and measure their robustness, we propose a simple yet effective\nperturbation technique, SwapMix. SwapMix perturbs the visual context by\nswapping features of irrelevant context objects with features from other\nobjects in the dataset. Using SwapMix we are able to change answers to more\nthan 45 % of the questions for a representative VQA model. Additionally, we\ntrain the models with perfect sight and find that the context over-reliance\nhighly depends on the quality of visual representations. In addition to\ndiagnosing, SwapMix can also be applied as a data augmentation strategy during\ntraining in order to regularize the context over-reliance. By swapping the\ncontext object features, the model reliance on context can be suppressed\neffectively. Two representative VQA models are studied using SwapMix: a\nco-attention model MCAN and a large-scale pretrained model LXMERT. Our\nexperiments on the popular GQA dataset show the effectiveness of SwapMix for\nboth diagnosing model robustness and regularizing the over-reliance on visual\ncontext. The code for our method is available at\nhttps://github.com/vipulgupta1011/swapmix", "published": "2022-04-05 15:32:25", "link": "http://arxiv.org/abs/2204.02285v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Can language models learn from explanations in context?", "abstract": "Language Models (LMs) can perform new tasks by adapting to a few in-context\nexamples. For humans, explanations that connect examples to task principles can\nimprove learning. We therefore investigate whether explanations of few-shot\nexamples can help LMs. We annotate questions from 40 challenging tasks with\nanswer explanations, and various matched control explanations. We evaluate how\ndifferent types of explanations, instructions, and controls affect zero- and\nfew-shot performance. We analyze these results using statistical multilevel\nmodeling techniques that account for the nested dependencies among conditions,\ntasks, prompts, and models. We find that explanations can improve performance\n-- even without tuning. Furthermore, explanations hand-tuned for performance on\na small validation set offer substantially larger benefits, and building a\nprompt by selecting examples and explanations together substantially improves\nperformance over selecting examples alone. Finally, even untuned explanations\noutperform carefully matched controls, suggesting that the benefits are due to\nthe link between an example and its explanation, rather than lower-level\nfeatures. However, only large models benefit. In summary, explanations can\nsupport the in-context learning of large LMs on challenging tasks.", "published": "2022-04-05 16:33:44", "link": "http://arxiv.org/abs/2204.02329v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Combining Spectral and Self-Supervised Features for Low Resource Speech\n  Recognition and Translation", "abstract": "Self-Supervised Learning (SSL) models have been successfully applied in\nvarious deep learning-based speech tasks, particularly those with a limited\namount of data. However, the quality of SSL representations depends highly on\nthe relatedness between the SSL training domain(s) and the target data domain.\nOn the contrary, spectral feature (SF) extractors such as log Mel-filterbanks\nare hand-crafted non-learnable components, and could be more robust to domain\nshifts. The present work examines the assumption that combining non-learnable\nSF extractors to SSL models is an effective approach to low resource speech\ntasks. We propose a learnable and interpretable framework to combine SF and SSL\nrepresentations. The proposed framework outperforms significantly both baseline\nand SSL models on Automatic Speech Recognition (ASR) and Speech Translation\n(ST) tasks on three low resource datasets. We additionally design a mixture of\nexperts based combination model. This last model reveals that the relative\ncontribution of SSL models over conventional SF extractors is very small in\ncase of domain mismatch between SSL training set and the target language data.", "published": "2022-04-05 20:09:15", "link": "http://arxiv.org/abs/2204.02470v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards End-to-end Unsupervised Speech Recognition", "abstract": "Unsupervised speech recognition has shown great potential to make Automatic\nSpeech Recognition (ASR) systems accessible to every language. However,\nexisting methods still heavily rely on hand-crafted pre-processing. Similar to\nthe trend of making supervised speech recognition end-to-end, we introduce\nwav2vec-U 2.0 which does away with all audio-side pre-processing and improves\naccuracy through better architecture. In addition, we introduce an auxiliary\nself-supervised objective that ties model predictions back to the input.\nExperiments show that wav2vec-U 2.0 improves unsupervised recognition results\nacross different languages while being conceptually simpler.", "published": "2022-04-05 21:22:38", "link": "http://arxiv.org/abs/2204.02492v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Transformer-Based Contrastive Learning Approach for Few-Shot Sign\n  Language Recognition", "abstract": "Sign language recognition from sequences of monocular images or 2D poses is a\nchallenging field, not only due to the difficulty to infer 3D information from\n2D data, but also due to the temporal relationship between the sequences of\ninformation. Additionally, the wide variety of signs and the constant need to\nadd new ones on production environments makes it infeasible to use traditional\nclassification techniques. We propose a novel Contrastive Transformer-based\nmodel, which demonstrate to learn rich representations from body key points\nsequences, allowing better comparison between vector embedding. This allows us\nto apply these techniques to perform one-shot or few-shot tasks, such as\nclassification and translation. The experiments showed that the model could\ngeneralize well and achieved competitive results for sign classes never seen in\nthe training process.", "published": "2022-04-05 11:42:55", "link": "http://arxiv.org/abs/2204.02803v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "How Different are Pre-trained Transformers for Text Ranking?", "abstract": "In recent years, large pre-trained transformers have led to substantial gains\nin performance over traditional retrieval models and feedback approaches.\nHowever, these results are primarily based on the MS Marco/TREC Deep Learning\nTrack setup, with its very particular setup, and our understanding of why and\nhow these models work better is fragmented at best. We analyze effective\nBERT-based cross-encoders versus traditional BM25 ranking for the passage\nretrieval task where the largest gains have been observed, and investigate two\nmain questions. On the one hand, what is similar? To what extent does the\nneural ranker already encompass the capacity of traditional rankers? Is the\ngain in performance due to a better ranking of the same documents (prioritizing\nprecision)? On the other hand, what is different? Can it retrieve effectively\ndocuments missed by traditional systems (prioritizing recall)? We discover\nsubstantial differences in the notion of relevance identifying strengths and\nweaknesses of BERT that may inspire research for future improvement. Our\nresults contribute to our understanding of (black-box) neural rankers relative\nto (well-understood) traditional rankers, help understand the particular\nexperimental setting of MS-Marco-based test collections.", "published": "2022-04-05 10:48:52", "link": "http://arxiv.org/abs/2204.07233v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "On the Transferability of Pre-trained Language Models for Low-Resource\n  Programming Languages", "abstract": "A recent study by Ahmed and Devanbu reported that using a corpus of code\nwritten in multilingual datasets to fine-tune multilingual Pre-trained Language\nModels (PLMs) achieves higher performance as opposed to using a corpus of code\nwritten in just one programming language. However, no analysis was made with\nrespect to fine-tuning monolingual PLMs. Furthermore, some programming\nlanguages are inherently different and code written in one language usually\ncannot be interchanged with the others, i.e., Ruby and Java code possess very\ndifferent structure. To better understand how monolingual and multilingual PLMs\naffect different programming languages, we investigate 1) the performance of\nPLMs on Ruby for two popular Software Engineering tasks: Code Summarization and\nCode Search, 2) the strategy (to select programming languages) that works well\non fine-tuning multilingual PLMs for Ruby, and 3) the performance of the\nfine-tuned PLMs on Ruby given different code lengths.\n  In this work, we analyze over a hundred of pre-trained and fine-tuned models.\nOur results show that 1) multilingual PLMs have a lower Performance-to-Time\nRatio (the BLEU, METEOR, or MRR scores over the fine-tuning duration) as\ncompared to monolingual PLMs, 2) our proposed strategy to select target\nprogramming languages to fine-tune multilingual PLMs is effective: it reduces\nthe time to fine-tune yet achieves higher performance in Code Summarization and\nCode Search tasks, and 3) our proposed strategy consistently shows good\nperformance on different code lengths.", "published": "2022-04-05 21:11:12", "link": "http://arxiv.org/abs/2204.09653v1", "categories": ["cs.PL", "cs.CL", "cs.SE"], "primary_category": "cs.PL"}
{"title": "LAMNER: Code Comment Generation Using Character Language Model and Named\n  Entity Recognition", "abstract": "Code comment generation is the task of generating a high-level natural\nlanguage description for a given code method or function. Although researchers\nhave been studying multiple ways to generate code comments automatically,\nprevious work mainly considers representing a code token in its entirety\nsemantics form only (e.g., a language model is used to learn the semantics of a\ncode token), and additional code properties such as the tree structure of a\ncode are included as an auxiliary input to the model. There are two\nlimitations: 1) Learning the code token in its entirety form may not be able to\ncapture information succinctly in source code, and 2) The code token does not\ncontain additional syntactic information, inherently important in programming\nlanguages.\n  In this paper, we present LAnguage Model and Named Entity Recognition\n(LAMNER), a code comment generator capable of encoding code constructs\neffectively and capturing the structural property of a code token. A\ncharacter-level language model is used to learn the semantic representation to\nencode a code token. For the structural property of a token, a Named Entity\nRecognition model is trained to learn the different types of code tokens. These\nrepresentations are then fed into an encoder-decoder architecture to generate\ncode comments. We evaluate the generated comments from LAMNER and other\nbaselines on a popular Java dataset with four commonly used metrics. Our\nresults show that LAMNER is effective and improves over the best baseline model\nin BLEU-1, BLEU-2, BLEU-3, BLEU-4, ROUGE-L, METEOR, and CIDEr by 14.34%,\n18.98%, 21.55%, 23.00%, 10.52%, 1.44%, and 25.86%, respectively. Additionally,\nwe fused LAMNER's code representation with the baseline models, and the fused\nmodels consistently showed improvement over the non-fused models. The human\nevaluation further shows that LAMNER produces high-quality code comments.", "published": "2022-04-05 20:53:06", "link": "http://arxiv.org/abs/2204.09654v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "An Exploratory Study on Code Attention in BERT", "abstract": "Many recent models in software engineering introduced deep neural models\nbased on the Transformer architecture or use transformer-based Pre-trained\nLanguage Models (PLM) trained on code. Although these models achieve the state\nof the arts results in many downstream tasks such as code summarization and bug\ndetection, they are based on Transformer and PLM, which are mainly studied in\nthe Natural Language Processing (NLP) field. The current studies rely on the\nreasoning and practices from NLP for these models in code, despite the\ndifferences between natural languages and programming languages. There is also\nlimited literature on explaining how code is modeled.\n  Here, we investigate the attention behavior of PLM on code and compare it\nwith natural language. We pre-trained BERT, a Transformer based PLM, on code\nand explored what kind of information it learns, both semantic and syntactic.\nWe run several experiments to analyze the attention values of code constructs\non each other and what BERT learns in each layer. Our analyses show that BERT\npays more attention to syntactic entities, specifically identifiers and\nseparators, in contrast to the most attended token [CLS] in NLP. This\nobservation motivated us to leverage identifiers to represent the code sequence\ninstead of the [CLS] token when used for code clone detection. Our results show\nthat employing embeddings from identifiers increases the performance of BERT by\n605% and 4% F1-score in its lower layers and the upper layers, respectively.\nWhen identifiers' embeddings are used in CodeBERT, a code-based PLM, the\nperformance is improved by 21-24% in the F1-score of clone detection. The\nfindings can benefit the research community by using code-specific\nrepresentations instead of applying the common embeddings used in NLP, and open\nnew directions for developing smaller models with similar performance.", "published": "2022-04-05 21:23:10", "link": "http://arxiv.org/abs/2204.10200v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL"], "primary_category": "cs.SE"}
{"title": "Unsupervised Data Selection via Discrete Speech Representation for ASR", "abstract": "Self-supervised learning of speech representations has achieved impressive\nresults in improving automatic speech recognition (ASR). In this paper, we show\nthat data selection is important for self-supervised learning. We propose a\nsimple and effective unsupervised data selection method which selects\nacoustically similar speech to a target domain. It takes the discrete speech\nrepresentation available in common self-supervised learning frameworks as\ninput, and applies a contrastive data selection method on the discrete tokens.\nThrough extensive empirical studies we show that our proposed method reduces\nthe amount of required pre-training data and improves the downstream ASR\nperformance. Pre-training on a selected subset of 6% of the general data pool\nresults in 11.8% relative improvements in LibriSpeech test-other compared to\npre-training on the full set. On Multilingual LibriSpeech French, German, and\nSpanish test sets, selecting 6% data for pre-training reduces word error rate\nby more than 15% relatively compared to the full set, and achieves competitive\nresults compared to current state-of-the-art performances.", "published": "2022-04-05 04:35:47", "link": "http://arxiv.org/abs/2204.01981v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Exploring the influence of fine-tuning data on wav2vec 2.0 model for\n  blind speech quality prediction", "abstract": "Recent studies have shown how self-supervised models can produce accurate\nspeech quality predictions. Speech representations generated by the pre-trained\nwav2vec 2.0 model allows constructing robust predicting models using small\namounts of annotated data. This opens the possibility of developing strong\nmodels in scenarios where labelled data is scarce. It is known that fine-tuning\nimproves the model's performance; however, it is unclear how the data (e.g.,\nlanguage, amount of samples) used for fine-tuning is influencing that\nperformance. In this paper, we explore how using different speech corpus to\nfine-tune the wav2vec 2.0 can influence its performance. We took four speech\ndatasets containing degradations found in common conferencing applications and\nfine-tuned wav2vec 2.0 targeting different languages and data size scenarios.\nThe fine-tuned models were tested across all four conferencing datasets plus an\nadditional dataset containing synthetic speech and they were compared against\nthree external baseline models. Results showed that fine-tuned models were able\nto compete with baseline models. Larger fine-tune data guarantee better\nperformance; meanwhile, diversity in language helped the models deal with\nspecific languages. Further research is needed to evaluate other wav2vec 2.0\nmodels pre-trained with multi-lingual datasets and to develop prediction models\nthat are more resilient to language diversity.", "published": "2022-04-05 11:57:31", "link": "http://arxiv.org/abs/2204.02135v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Disentangled Speech Representation Learning Based on Factorized\n  Hierarchical Variational Autoencoder with Self-Supervised Objective", "abstract": "Disentangled representation learning aims to extract explanatory features or\nfactors and retain salient information. Factorized hierarchical variational\nautoencoder (FHVAE) presents a way to disentangle a speech signal into\nsequential-level and segmental-level features, which represent speaker identity\nand speech content information, respectively. As a self-supervised objective,\nautoregressive predictive coding (APC), on the other hand, has been used in\nextracting meaningful and transferable speech features for multiple downstream\ntasks. Inspired by the success of these two representation learning methods,\nthis paper proposes to integrate the APC objective into the FHVAE framework\naiming at benefiting from the additional self-supervision target. The main\nproposed method requires neither more training data nor more computational cost\nat test time, but obtains improved meaningful representations while maintaining\ndisentanglement. The experiments were conducted on the TIMIT dataset. Results\ndemonstrate that FHVAE equipped with the additional self-supervised objective\nis able to learn features providing superior performance for tasks including\nspeech recognition and speaker recognition. Furthermore, voice conversion, as\none application of disentangled representation learning, has been applied and\nevaluated. The results show performance similar to baseline of the new\nframework on voice conversion.", "published": "2022-04-05 12:47:17", "link": "http://arxiv.org/abs/2204.02166v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Complex Recurrent Variational Autoencoder with Application to Speech\n  Enhancement", "abstract": "As an extension of variational autoencoder (VAE), complex VAE uses complex\nGaussian distributions to model latent variables and data. This work proposes a\ncomplex recurrent VAE framework, specifically in which complex-valued recurrent\nneural network and L1 reconstruction loss are used. Firstly, to account for the\ntemporal property of speech signals, this work introduces complex-valued\nrecurrent neural network in the complex VAE framework. Besides, L1 loss is used\nas the reconstruction loss in this framework. To exemplify the use of the\ncomplex generative model in speech processing, we choose speech enhancement as\nthe specific application in this paper. Experiments are based on the TIMIT\ndataset. The results show that the proposed method offers improvements on\nobjective metrics in speech intelligibility and signal quality.", "published": "2022-04-05 13:15:18", "link": "http://arxiv.org/abs/2204.02195v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Comparison of Deep Learning MOS Predictors for Speech Synthesis\n  Quality", "abstract": "Speech synthesis quality prediction has made remarkable progress with the\ndevelopment of supervised and self-supervised learning (SSL) MOS predictors but\nsome aspects related to the data are still unclear and require further study.\nIn this paper, we evaluate several MOS predictors based on wav2vec 2.0 and the\nNISQA speech quality prediction model to explore the role of the training data,\nthe influence of the system type, and the role of cross-domain features in SSL\nmodels. Our evaluation is based on the VoiceMOS challenge dataset. Results show\nthat SSL-based models show the highest correlation and lowest mean squared\nerror compared to supervised models. The key point of this study is that\nbenchmarking the statistical performance of MOS predictors alone is not\nsufficient to rank models since potential issues hidden in the data could bias\nthe evaluated performances.", "published": "2022-04-05 14:40:15", "link": "http://arxiv.org/abs/2204.02249v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Low-Latency Speech Separation Guided Diarization for Telephone\n  Conversations", "abstract": "In this paper, we carry out an analysis on the use of speech separation\nguided diarization (SSGD) in telephone conversations. SSGD performs diarization\nby separating the speakers signals and then applying voice activity detection\non each estimated speaker signal. In particular, we compare two low-latency\nspeech separation models. Moreover, we show a post-processing algorithm that\nsignificantly reduces the false alarm errors of a SSGD pipeline. We perform our\nexperiments on two datasets: Fisher Corpus Part 1 and CALLHOME, evaluating both\nseparation and diarization metrics. Notably, our SSGD DPRNN-based online model\nachieves 11.1% DER on CALLHOME, comparable with most state-of-the-art\nend-to-end neural diarization models despite being trained on an order of\nmagnitude less data and having considerably lower latency, i.e., 0.1 vs. 10\nseconds. We also show that the separated signals can be readily fed to a speech\nrecognition back-end with performance close to the oracle source signals.", "published": "2022-04-05 16:01:45", "link": "http://arxiv.org/abs/2204.02306v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Mixed supervised Learning Framework for Target Sound Detection", "abstract": "Target sound detection (TSD) aims to detect the target sound from mixture\naudio given the reference information. Previous works have shown that TSD\nmodels can be trained on fully-annotated (frame-level label) or\nweakly-annotated (clip-level label) data. However, there are some clear\nevidences show that the performance of the model trained on weakly-annotated\ndata is worse than that trained on fully-annotated data. To fill this gap, we\nprovide a mixed supervision perspective, in which learning novel categories\n(target domain) using weak annotations with the help of full annotations of\nexisting base categories (source domain). To realize this, a mixed supervised\nlearning framework is proposed, which contains two mutually-helping student\nmodels (\\textit{f\\_student} and \\textit{w\\_student}) that learn from\nfully-annotated and weakly-annotated data, respectively. The motivation is that\n\\textit{f\\_student} learned from fully-annotated data has a better ability to\ncapture detailed information than \\textit{w\\_student}. Thus, we first let\n\\textit{f\\_student} guide \\textit{w\\_student} to learn the ability to capture\ndetails, so \\textit{w\\_student} can perform better in the target domain. Then\nwe let \\textit{w\\_student} guide \\textit{f\\_student} to fine-tune on the target\ndomain. The process can be repeated several times so that the two students\nperform very well in the target domain. To evaluate our method, we built three\nTSD datasets based on UrbanSound and Audioset. Experimental results show that\nour methods offer about 8\\% improvement in event-based F-score as compared with\na recent baseline.", "published": "2022-04-05 09:59:09", "link": "http://arxiv.org/abs/2204.02088v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Non-Linear Speech coding with MLP, RBF and Elman based prediction", "abstract": "In this paper we propose a nonlinear scalar predictor based on a combination\nof Multi Layer Perceptron, Radial Basis Functions and Elman networks. This\nsystem is applied to speech coding in an ADPCM backward scheme. The combination\nof this predictors improves the results of one predictor alone. A comparative\nstudy of this three neural networks for speech prediction is also presented.", "published": "2022-04-05 10:48:13", "link": "http://arxiv.org/abs/2204.02101v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RaDur: A Reference-aware and Duration-robust Network for Target Sound\n  Detection", "abstract": "Target sound detection (TSD) aims to detect the target sound from a mixture\naudio given the reference information. Previous methods use a conditional\nnetwork to extract a sound-discriminative embedding from the reference audio,\nand then use it to detect the target sound from the mixture audio. However, the\nnetwork performs much differently when using different reference audios (e.g.\nperforms poorly for noisy and short-duration reference audios), and tends to\nmake wrong decisions for transient events (i.e. shorter than $1$ second). To\novercome these problems, in this paper, we present a reference-aware and\nduration-robust network (RaDur) for TSD. More specifically, in order to make\nthe network more aware of the reference information, we propose an embedding\nenhancement module to take into account the mixture audio while generating the\nembedding, and apply the attention pooling to enhance the features of target\nsound-related frames and weaken the features of noisy frames. In addition, a\nduration-robust focal loss is proposed to help model different-duration events.\nTo evaluate our method, we build two TSD datasets based on UrbanSound and\nAudioset. Extensive experiments show the effectiveness of our methods.", "published": "2022-04-05 12:08:13", "link": "http://arxiv.org/abs/2204.02143v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022", "abstract": "We present the UTokyo-SaruLab mean opinion score (MOS) prediction system\nsubmitted to VoiceMOS Challenge 2022. The challenge is to predict the MOS\nvalues of speech samples collected from previous Blizzard Challenges and Voice\nConversion Challenges for two tracks: a main track for in-domain prediction and\nan out-of-domain (OOD) track for which there is less labeled data from\ndifferent listening tests. Our system is based on ensemble learning of strong\nand weak learners. Strong learners incorporate several improvements to the\nprevious fine-tuning models of self-supervised learning (SSL) models, while\nweak learners use basic machine-learning methods to predict scores from SSL\nfeatures. In the Challenge, our system had the highest score on several metrics\nfor both the main and OOD tracks. In addition, we conducted ablation studies to\ninvestigate the effectiveness of our proposed methods.", "published": "2022-04-05 12:23:51", "link": "http://arxiv.org/abs/2204.02152v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adversarial Learning of Intermediate Acoustic Feature for End-to-End\n  Lightweight Text-to-Speech", "abstract": "To simplify the generation process, several text-to-speech (TTS) systems\nimplicitly learn intermediate latent representations instead of relying on\npredefined features (e.g., mel-spectrogram). However, their generation quality\nis unsatisfactory as these representations lack speech variances. In this\npaper, we improve TTS performance by adding \\emph{prosody embeddings} to the\nlatent representations. During training, we extract reference prosody\nembeddings from mel-spectrograms, and during inference, we estimate these\nembeddings from text using generative adversarial networks (GANs). Using GANs,\nwe reliably estimate the prosody embeddings in a fast way, which have complex\ndistributions due to the dynamic nature of speech. We also show that the\nprosody embeddings work as efficient features for learning a robust alignment\nbetween text and acoustic features. Our proposed model surpasses several\npublicly available models with less parameters and computational complexity in\ncomparative experiments.", "published": "2022-04-05 12:58:47", "link": "http://arxiv.org/abs/2204.02172v2", "categories": ["cs.SD", "eess.AS", "68T07 (Primary) 68T50, 68T99 (Secondary)", "I.2.7; I.2.6"], "primary_category": "cs.SD"}
{"title": "How Information on Acoustic Scenes and Sound Events Mutually Benefits\n  Event Detection and Scene Classification Tasks", "abstract": "Acoustic scene classification (ASC) and sound event detection (SED) are\nfundamental tasks in environmental sound analysis, and many methods based on\ndeep learning have been proposed. Considering that information on acoustic\nscenes and sound events helps SED and ASC mutually, some researchers have\nproposed a joint analysis of acoustic scenes and sound events by multitask\nlearning (MTL). However, conventional works have not investigated in detail how\nacoustic scenes and sound events mutually benefit SED and ASC. We, therefore,\ninvestigate the impact of information on acoustic scenes and sound events on\nthe performance of SED and ASC by using domain adversarial training based on a\ngradient reversal layer (GRL) or model training with fake labels. Experimental\nresults obtained using the TUT Acoustic Scenes 2016/2017 and TUT Sound Events\n2016/2017 show that pieces of information on acoustic scenes and sound events\nare effectively used to detect sound events and classify acoustic scenes,\nrespectively. Moreover, upon comparing GRL- and fake-label-based methods with\nsingle-task-based ASC and SED methods, single-task-based methods are found to\nachieve better performance. This result implies that even when using\nsingle-task-based ASC and SED methods, information on acoustic scenes may be\nimplicitly utilized for SED and vice versa.", "published": "2022-04-05 15:19:45", "link": "http://arxiv.org/abs/2204.02279v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Hear No Evil: Towards Adversarial Robustness of Automatic Speech\n  Recognition via Multi-Task Learning", "abstract": "As automatic speech recognition (ASR) systems are now being widely deployed\nin the wild, the increasing threat of adversarial attacks raises serious\nquestions about the security and reliability of using such systems. On the\nother hand, multi-task learning (MTL) has shown success in training models that\ncan resist adversarial attacks in the computer vision domain. In this work, we\ninvestigate the impact of performing such multi-task learning on the\nadversarial robustness of ASR models in the speech domain. We conduct extensive\nMTL experimentation by combining semantically diverse tasks such as accent\nclassification and ASR, and evaluate a wide range of adversarial settings. Our\nthorough analysis reveals that performing MTL with semantically diverse tasks\nconsistently makes it harder for an adversarial attack to succeed. We also\ndiscuss in detail the serious pitfalls and their related remedies that have a\nsignificant impact on the robustness of MTL models. Our proposed MTL approach\nshows considerable absolute improvements in adversarially targeted WER ranging\nfrom 17.25 up to 59.90 compared to single-task learning baselines (attention\ndecoder and CTC respectively). Ours is the first in-depth study that uncovers\nadversarial robustness gains from multi-task learning for ASR.", "published": "2022-04-05 17:40:19", "link": "http://arxiv.org/abs/2204.02381v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "What can predictive speech coders learn from speaker recognizers?", "abstract": "This paper compares the speech coder and speaker recognizer applications,\nshowing some parallelism between them. In this paper, some approaches used for\nspeaker recognition are applied to speech coding in order to improve the\nprediction accuracy. Experimental results show an improvement in Segmental SNR\n(SEGSNR).", "published": "2022-04-05 10:57:46", "link": "http://arxiv.org/abs/2204.02400v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning to Adapt to Domain Shifts with Few-shot Samples in Anomalous\n  Sound Detection", "abstract": "Anomaly detection has many important applications, such as monitoring\nindustrial equipment. Despite recent advances in anomaly detection with\ndeep-learning methods, it is unclear how existing solutions would perform under\nout-of-distribution scenarios, e.g., due to shifts in machine load or\nenvironmental noise. Grounded in the application of machine health monitoring,\nwe propose a framework that adapts to new conditions with few-shot samples.\nBuilding upon prior work, we adopt a classification-based approach for anomaly\ndetection and show its equivalence to mixture density estimation of the normal\nsamples. We incorporate an episodic training procedure to match the few-shot\nsetting during inference. We define multiple auxiliary classification tasks\nbased on meta-information and leverage gradient-based meta-learning to improve\ngeneralization to different shifts. We evaluate our proposed method on a\nrecently-released dataset of audio measurements from different machine types.\nIt improved upon two baselines by around 10% and is on par with best-performing\nmodel reported on the dataset.", "published": "2022-04-05 00:22:25", "link": "http://arxiv.org/abs/2204.01905v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Application of a Spectral Method to Simulate Quasi-Three-Dimensional\n  Underwater Acoustic Fields", "abstract": "The calculation of a three-dimensional underwater acoustic field has always\nbeen a key problem in computational ocean acoustics. Traditionally, this\nsolution is usually obtained by directly solving the acoustic Helmholtz\nequation using a finite difference or finite element algorithm. Solving the\nthree-dimensional Helmholtz equation directly is computationally expensive. For\nquasi-three-dimensional problems, the Helmholtz equation can be processed by\nthe integral transformation approach, which can greatly reduce the\ncomputational cost. In this paper, a numerical algorithm for a\nquasi-three-dimensional sound field that combines an integral transformation\ntechnique, stepwise coupled modes and a spectral method is designed. The\nquasi-three-dimensional problem is transformed into a two-dimensional problem\nusing an integral transformation strategy. A stepwise approximation is then\nused to discretize the range dependence of the two-dimensional problem; this\napproximation is essentially a physical discretization that further reduces the\nrange-dependent two-dimensional problem to a one-dimensional problem. Finally,\nthe Chebyshev--Tau spectral method is employed to accurately solve the\none-dimensional problem. We provide the corresponding numerical program SPEC3D\nfor the proposed algorithm and describe several representative numerical\nexamples. In the numerical experiments, the consistency between SPEC3D and the\nanalytical solution/high-precision finite difference program COACH verifies the\nreliability and capability of the proposed algorithm. A comparison of running\ntimes illustrates that the algorithm proposed in this paper is significantly\nfaster than the full three-dimensional algorithm in terms of computational\nspeed.", "published": "2022-04-05 03:11:57", "link": "http://arxiv.org/abs/2204.01954v4", "categories": ["physics.comp-ph", "cs.SD", "eess.AS"], "primary_category": "physics.comp-ph"}
{"title": "Audio-visual multi-channel speech separation, dereverberation and\n  recognition", "abstract": "Despite the rapid advance of automatic speech recognition (ASR) technologies,\naccurate recognition of cocktail party speech characterised by the interference\nfrom overlapping speakers, background noise and room reverberation remains a\nhighly challenging task to date. Motivated by the invariance of visual modality\nto acoustic signal corruption, audio-visual speech enhancement techniques have\nbeen developed, although predominantly targeting overlapping speech separation\nand recognition tasks. In this paper, an audio-visual multi-channel speech\nseparation, dereverberation and recognition approach featuring a full\nincorporation of visual information into all three stages of the system is\nproposed. The advantage of the additional visual modality over using audio only\nis demonstrated on two neural dereverberation approaches based on DNN-WPE and\nspectral mapping respectively. The learning cost function mismatch between the\nseparation and dereverberation models and their integration with the back-end\nrecognition system is minimised using fine-tuning on the MSE and LF-MMI\ncriteria. Experiments conducted on the LRS2 dataset suggest that the proposed\naudio-visual multi-channel speech separation, dereverberation and recognition\nsystem outperforms the baseline audio-visual multi-channel speech separation\nand recognition system containing no dereverberation module by a statistically\nsignificant word error rate (WER) reduction of 2.06% absolute (8.77% relative).", "published": "2022-04-05 04:16:03", "link": "http://arxiv.org/abs/2204.01977v2", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On the Relevance of Bandwidth Extension for Speaker Verification", "abstract": "In this paper, we consider the effect of a bandwidth extension of narrow-band\nspeech signals (0.3-3.4 kHz) to 0.3-8 kHz on speaker verification. Using\ncovariance matrix based verification systems together with detection error\ntrade-off curves, we compare the performance between systems operating on\nnarrow-band, wide-band (0-8 kHz), and bandwidth-extended speech. The\nexperiments were conducted using different short-time spectral\nparameterizations derived from microphone and ISDN speech databases. The\nstudied bandwidth-extension algorithm did not introduce artifacts that affected\nthe speaker verification task, and we achieved improvements between 1 and 10\npercent (depending on the model order) over the verification system designed\nfor narrow-band speech when mel-frequency cepstral coefficients for the\nshort-time spectral parameterization were used.", "published": "2022-04-05 08:12:09", "link": "http://arxiv.org/abs/2204.02040v1", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VocaLiST: An Audio-Visual Synchronisation Model for Lips and Voices", "abstract": "In this paper, we address the problem of lip-voice synchronisation in videos\ncontaining human face and voice. Our approach is based on determining if the\nlips motion and the voice in a video are synchronised or not, depending on\ntheir audio-visual correspondence score. We propose an audio-visual cross-modal\ntransformer-based model that outperforms several baseline models in the\naudio-visual synchronisation task on the standard lip-reading speech benchmark\ndataset LRS2. While the existing methods focus mainly on lip synchronisation in\nspeech videos, we also consider the special case of the singing voice. The\nsinging voice is a more challenging use case for synchronisation due to\nsustained vowel sounds. We also investigate the relevance of lip\nsynchronisation models trained on speech datasets in the context of singing\nvoice. Finally, we use the frozen visual features learned by our lip\nsynchronisation model in the singing voice separation task to outperform a\nbaseline audio-visual model which was trained end-to-end. The demos, source\ncode, and the pre-trained models are available on\nhttps://ipcv.github.io/VocaLiST/", "published": "2022-04-05 10:02:39", "link": "http://arxiv.org/abs/2204.02090v2", "categories": ["cs.CV", "cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "MetaAudio: A Few-Shot Audio Classification Benchmark", "abstract": "Currently available benchmarks for few-shot learning (machine learning with\nfew training examples) are limited in the domains they cover, primarily\nfocusing on image classification. This work aims to alleviate this reliance on\nimage-based benchmarks by offering the first comprehensive, public and fully\nreproducible audio based alternative, covering a variety of sound domains and\nexperimental settings. We compare the few-shot classification performance of a\nvariety of techniques on seven audio datasets (spanning environmental sounds to\nhuman-speech). Extending this, we carry out in-depth analyses of joint training\n(where all datasets are used during training) and cross-dataset adaptation\nprotocols, establishing the possibility of a generalised audio few-shot\nclassification algorithm. Our experimentation shows gradient-based\nmeta-learning methods such as MAML and Meta-Curvature consistently outperform\nboth metric and baseline methods. We also demonstrate that the joint training\nroutine helps overall generalisation for the environmental sound databases\nincluded, as well as being a somewhat-effective method of tackling the\ncross-dataset/domain setting.", "published": "2022-04-05 11:33:44", "link": "http://arxiv.org/abs/2204.02121v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Design Guidelines for Inclusive Speaker Verification Evaluation Datasets", "abstract": "Speaker verification (SV) provides billions of voice-enabled devices with\naccess control, and ensures the security of voice-driven technologies. As a\ntype of biometrics, it is necessary that SV is unbiased, with consistent and\nreliable performance across speakers irrespective of their demographic, social\nand economic attributes. Current SV evaluation practices are insufficient for\nevaluating bias: they are over-simplified and aggregate users, not\nrepresentative of real-life usage scenarios, and consequences of errors are not\naccounted for. This paper proposes design guidelines for constructing SV\nevaluation datasets that address these short-comings. We propose a schema for\ngrading the difficulty of utterance pairs, and present an algorithm for\ngenerating inclusive SV datasets. We empirically validate our proposed method\nin a set of experiments on the VoxCeleb1 dataset. Our results confirm that the\ncount of utterance pairs/speaker, and the difficulty grading of utterance pairs\nhave a significant effect on evaluation performance and variability. Our work\ncontributes to the development of SV evaluation practices that are inclusive\nand fair.", "published": "2022-04-05 15:28:26", "link": "http://arxiv.org/abs/2204.02281v2", "categories": ["eess.AS", "cs.CY", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Learning Speech Emotion Representations in the Quaternion Domain", "abstract": "The modeling of human emotion expression in speech signals is an important,\nyet challenging task. The high resource demand of speech emotion recognition\nmodels, combined with the the general scarcity of emotion-labelled data are\nobstacles to the development and application of effective solutions in this\nfield. In this paper, we present an approach to jointly circumvent these\ndifficulties. Our method, named RH-emo, is a novel semi-supervised architecture\naimed at extracting quaternion embeddings from real-valued monoaural\nspectrograms, enabling the use of quaternion-valued networks for speech emotion\nrecognition tasks. RH-emo is a hybrid real/quaternion autoencoder network that\nconsists of a real-valued encoder in parallel to a real-valued emotion\nclassifier and a quaternion-valued decoder. On the one hand, the classifier\npermits to optimize each latent axis of the embeddings for the classification\nof a specific emotion-related characteristic: valence, arousal, dominance and\noverall emotion. On the other hand, the quaternion reconstruction enables the\nlatent dimension to develop intra-channel correlations that are required for an\neffective representation as a quaternion entity. We test our approach on speech\nemotion recognition tasks using four popular datasets: Iemocap, Ravdess, EmoDb\nand Tess, comparing the performance of three well-established real-valued CNN\narchitectures (AlexNet, ResNet-50, VGG) and their quaternion-valued equivalent\nfed with the embeddings created with RH-emo. We obtain a consistent improvement\nin the test accuracy for all datasets, while drastically reducing the\nresources' demand of models. Moreover, we performed additional experiments and\nablation studies that confirm the effectiveness of our approach. The RH-emo\nrepository is available at: https://github.com/ispamm/rhemo.", "published": "2022-04-05 17:45:09", "link": "http://arxiv.org/abs/2204.02385v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Voice Trigger Detection with Metric Learning", "abstract": "Voice trigger detection is an important task, which enables activating a\nvoice assistant when a target user speaks a keyword phrase. A detector is\ntypically trained on speech data independent of speaker information and used\nfor the voice trigger detection task. However, such a speaker independent voice\ntrigger detector typically suffers from performance degradation on speech from\nunderrepresented groups, such as accented speakers. In this work, we propose a\nnovel voice trigger detector that can use a small number of utterances from a\ntarget speaker to improve detection accuracy. Our proposed model employs an\nencoder-decoder architecture. While the encoder performs speaker independent\nvoice trigger detection, similar to the conventional detector, the decoder\npredicts a personalized embedding for each utterance. A personalized voice\ntrigger score is then obtained as a similarity score between the embeddings of\nenrollment utterances and a test utterance. The personalized embedding allows\nadapting to target speaker's speech when computing the voice trigger score,\nhence improving voice trigger detection accuracy. Experimental results show\nthat the proposed approach achieves a 38% relative reduction in a false\nrejection rate (FRR) compared to a baseline speaker independent voice trigger\nmodel.", "published": "2022-04-05 18:59:27", "link": "http://arxiv.org/abs/2204.02455v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Training-Free Robust Multimodal Learning via Sample-Wise Jacobian\n  Regularization", "abstract": "Multimodal fusion emerges as an appealing technique to improve model\nperformances on many tasks. Nevertheless, the robustness of such fusion methods\nis rarely involved in the present literature. In this paper, we propose a\ntraining-free robust late-fusion method by exploiting conditional independence\nassumption and Jacobian regularization. Our key is to minimize the Frobenius\nnorm of a Jacobian matrix, where the resulting optimization problem is relaxed\nto a tractable Sylvester equation. Furthermore, we provide a theoretical error\nbound of our method and some insights about the function of the extra modality.\nSeveral numerical experiments on AV-MNIST, RAVDESS, and VGGsound demonstrate\nthe efficacy of our method under both adversarial attacks and random\ncorruptions.", "published": "2022-04-05 20:38:33", "link": "http://arxiv.org/abs/2204.02485v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "User-Level Differential Privacy against Attribute Inference Attack of\n  Speech Emotion Recognition in Federated Learning", "abstract": "Many existing privacy-enhanced speech emotion recognition (SER) frameworks\nfocus on perturbing the original speech data through adversarial training\nwithin a centralized machine learning setup. However, this privacy protection\nscheme can fail since the adversary can still access the perturbed data. In\nrecent years, distributed learning algorithms, especially federated learning\n(FL), have gained popularity to protect privacy in machine learning\napplications. While FL provides good intuition to safeguard privacy by keeping\nthe data on local devices, prior work has shown that privacy attacks, such as\nattribute inference attacks, are achievable for SER systems trained using FL.\nIn this work, we propose to evaluate the user-level differential privacy (UDP)\nin mitigating the privacy leaks of the SER system in FL. UDP provides\ntheoretical privacy guarantees with privacy parameters $\\epsilon$ and $\\delta$.\nOur results show that the UDP can effectively decrease attribute information\nleakage while keeping the utility of the SER system with the adversary\naccessing one model update. However, the efficacy of the UDP suffers when the\nFL system leaks more model updates to the adversary. We make the code publicly\navailable to reproduce the results in\nhttps://github.com/usc-sail/fed-ser-leakage.", "published": "2022-04-05 21:35:30", "link": "http://arxiv.org/abs/2204.02500v2", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer", "abstract": "Objects play a crucial role in our everyday activities. Though multisensory\nobject-centric learning has shown great potential lately, the modeling of\nobjects in prior work is rather unrealistic. ObjectFolder 1.0 is a recent\ndataset that introduces 100 virtualized objects with visual, acoustic, and\ntactile sensory data. However, the dataset is small in scale and the\nmultisensory data is of limited quality, hampering generalization to real-world\nscenarios. We present ObjectFolder 2.0, a large-scale, multisensory dataset of\ncommon household objects in the form of implicit neural representations that\nsignificantly enhances ObjectFolder 1.0 in three aspects. First, our dataset is\n10 times larger in the amount of objects and orders of magnitude faster in\nrendering time. Second, we significantly improve the multisensory rendering\nquality for all three modalities. Third, we show that models learned from\nvirtual objects in our dataset successfully transfer to their real-world\ncounterparts in three challenging tasks: object scale estimation, contact\nlocalization, and shape reconstruction. ObjectFolder 2.0 offers a new path and\ntestbed for multisensory learning in computer vision and robotics. The dataset\nis available at https://github.com/rhgao/ObjectFolder.", "published": "2022-04-05 17:55:01", "link": "http://arxiv.org/abs/2204.02389v1", "categories": ["cs.CV", "cs.LG", "cs.RO", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
