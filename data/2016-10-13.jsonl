{"title": "A Neural Network for Coordination Boundary Prediction", "abstract": "We propose a neural-network based model for coordination boundary prediction.\nThe network is designed to incorporate two signals: the similarity between\nconjuncts and the observation that replacing the whole coordination phrase with\na conjunct tends to produce a coherent sentences. The modeling makes use of\nseveral LSTM networks. The model is trained solely on conjunction annotations\nin a Treebank, without using external resources. We show improvements on\npredicting coordination boundaries on the PTB compared to two state-of-the-art\nparsers; as well as improvement over previous coordination boundary prediction\nsystems on the Genia corpus.", "published": "2016-10-13 06:42:51", "link": "http://arxiv.org/abs/1610.03946v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fast, Scalable Phrase-Based SMT Decoding", "abstract": "The utilization of statistical machine translation (SMT) has grown enormously\nover the last decade, many using open-source software developed by the NLP\ncommunity. As commercial use has increased, there is need for software that is\noptimized for commercial requirements, in particular, fast phrase-based\ndecoding and more efficient utilization of modern multicore servers.\n  In this paper we re-examine the major components of phrase-based decoding and\ndecoder implementation with particular emphasis on speed and scalability on\nmulticore machines. The result is a drop-in replacement for the Moses decoder\nwhich is up to fifteen times faster and scales monotonically with the number of\ncores.", "published": "2016-10-13 21:25:34", "link": "http://arxiv.org/abs/1610.04265v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compressing Neural Language Models by Sparse Word Representations", "abstract": "Neural networks are among the state-of-the-art techniques for language\nmodeling. Existing neural language models typically map discrete words to\ndistributed, dense vector representations. After information processing of the\npreceding context words by hidden layers, an output layer estimates the\nprobability of the next word. Such approaches are time- and memory-intensive\nbecause of the large numbers of parameters for word embeddings and the output\nlayer. In this paper, we propose to compress neural language models by sparse\nword representations. In the experiments, the number of parameters in our model\nincreases very slowly with the growth of the vocabulary size, which is almost\nimperceptible. Moreover, our approach not only reduces the parameter space to a\nlarge extent, but also improves the performance in terms of the perplexity\nmeasure.", "published": "2016-10-13 06:55:54", "link": "http://arxiv.org/abs/1610.03950v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dialogue Session Segmentation by Embedding-Enhanced TextTiling", "abstract": "In human-computer conversation systems, the context of a user-issued\nutterance is particularly important because it provides useful background\ninformation of the conversation. However, it is unwise to track all previous\nutterances in the current session as not all of them are equally important. In\nthis paper, we address the problem of session segmentation. We propose an\nembedding-enhanced TextTiling approach, inspired by the observation that\nconversation utterances are highly noisy, and that word embeddings provide a\nrobust way of capturing semantics. Experimental results show that our approach\nachieves better performance than the TextTiling, MMD approaches.", "published": "2016-10-13 07:07:50", "link": "http://arxiv.org/abs/1610.03955v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Gated End-to-End Memory Networks", "abstract": "Machine reading using differentiable reasoning models has recently shown\nremarkable progress. In this context, End-to-End trainable Memory Networks,\nMemN2N, have demonstrated promising performance on simple natural language\nbased reasoning tasks such as factual reasoning and basic deduction. However,\nother tasks, namely multi-fact question-answering, positional reasoning or\ndialog related tasks, remain challenging particularly due to the necessity of\nmore complex interactions between the memory and controller modules composing\nthis family of models. In this paper, we introduce a novel end-to-end memory\naccess regulation mechanism inspired by the current progress on the connection\nshort-cutting principle in the field of computer vision. Concretely, we develop\na Gated End-to-End trainable Memory Network architecture, GMemN2N. From the\nmachine learning perspective, this new capability is learned in an end-to-end\nfashion without the use of any additional supervision signal which is, as far\nas our knowledge goes, the first of its kind. Our experiments show significant\nimprovements on the most challenging tasks in the 20 bAbI dataset, without the\nuse of any domain knowledge. Then, we show improvements on the dialog bAbI\ntasks including the real human-bot conversion-based Dialog State Tracking\nChallenge (DSTC-2) dataset. On these two datasets, our model sets the new state\nof the art.", "published": "2016-10-13 19:38:03", "link": "http://arxiv.org/abs/1610.04211v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Mapping Between fMRI Responses to Movies and their Natural Language\n  Annotations", "abstract": "Several research groups have shown how to correlate fMRI responses to the\nmeanings of presented stimuli. This paper presents new methods for doing so\nwhen only a natural language annotation is available as the description of the\nstimulus. We study fMRI data gathered from subjects watching an episode of BBCs\nSherlock [1], and learn bidirectional mappings between fMRI responses and\nnatural language representations. We show how to leverage data from multiple\nsubjects watching the same movie to improve the accuracy of the mappings,\nallowing us to succeed at a scene classification task with 72% accuracy (random\nguessing would give 4%) and at a scene ranking task with average rank in the\ntop 4% (random guessing would give 50%). The key ingredients are (a) the use of\nthe Shared Response Model (SRM) and its variant SRM-ICA [2, 3] to aggregate\nfMRI data from multiple subjects, both of which are shown to be superior to\nstandard PCA in producing low-dimensional representations for the tasks in this\npaper; (b) a sentence embedding technique adapted from the natural language\nprocessing (NLP) literature [4] that produces semantic vector representation of\nthe annotations; (c) using previous timestep information in the featurization\nof the predictor data.", "published": "2016-10-13 02:20:45", "link": "http://arxiv.org/abs/1610.03914v3", "categories": ["q-bio.NC", "cs.CL", "cs.LG"], "primary_category": "q-bio.NC"}
{"title": "A Survey of Voice Translation Methodologies - Acoustic Dialect Decoder", "abstract": "Speech Translation has always been about giving source text or audio input\nand waiting for system to give translated output in desired form. In this\npaper, we present the Acoustic Dialect Decoder (ADD) - a voice to voice\near-piece translation device. We introduce and survey the recent advances made\nin the field of Speech Engineering, to employ in the ADD, particularly focusing\non the three major processing steps of Recognition, Translation and Synthesis.\nWe tackle the problem of machine understanding of natural language by designing\na recognition unit for source audio to text, a translation unit for source\nlanguage text to target language text, and a synthesis unit for target language\ntext to target language speech. Speech from the surroundings will be recorded\nby the recognition unit present on the ear-piece and translation will start as\nsoon as one sentence is successfully read. This way, we hope to give translated\noutput as and when input is being read. The recognition unit will use Hidden\nMarkov Models (HMMs) Based Tool-Kit (HTK), hybrid RNN systems with gated memory\ncells, and the synthesis unit, HMM based speech synthesis system HTS. This\nsystem will initially be built as an English to Tamil translation device.", "published": "2016-10-13 04:10:58", "link": "http://arxiv.org/abs/1610.03934v1", "categories": ["cs.CL", "cs.NE", "cs.SD", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Exploiting Sentence and Context Representations in Deep Neural Models\n  for Spoken Language Understanding", "abstract": "This paper presents a deep learning architecture for the semantic decoder\ncomponent of a Statistical Spoken Dialogue System. In a slot-filling dialogue,\nthe semantic decoder predicts the dialogue act and a set of slot-value pairs\nfrom a set of n-best hypotheses returned by the Automatic Speech Recognition.\nMost current models for spoken language understanding assume (i) word-aligned\nsemantic annotations as in sequence taggers and (ii) delexicalisation, or a\nmapping of input words to domain-specific concepts using heuristics that try to\ncapture morphological variation but that do not scale to other domains nor to\nlanguage variation (e.g., morphology, synonyms, paraphrasing ). In this work\nthe semantic decoder is trained using unaligned semantic annotations and it\nuses distributed semantic representation learning to overcome the limitations\nof explicit delexicalisation. The proposed architecture uses a convolutional\nneural network for the sentence representation and a long-short term memory\nnetwork for the context representation. Results are presented for the publicly\navailable DSTC2 corpus and an In-car corpus which is similar to DSTC2 but has a\nsignificantly higher word error rate (WER).", "published": "2016-10-13 15:11:40", "link": "http://arxiv.org/abs/1610.04120v1", "categories": ["cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.AI"}
