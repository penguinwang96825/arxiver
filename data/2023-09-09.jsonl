{"title": "Exploring Large Language Models for Communication Games: An Empirical\n  Study on Werewolf", "abstract": "Communication games, which we refer to as incomplete information games that\nheavily depend on natural language communication, hold significant research\nvalue in fields such as economics, social science, and artificial intelligence.\nIn this work, we explore the problem of how to engage large language models\n(LLMs) in communication games, and in response, propose a tuning-free\nframework. Our approach keeps LLMs frozen, and relies on the retrieval and\nreflection on past communications and experiences for improvement. An empirical\nstudy on the representative and widely-studied communication game,\n``Werewolf'', demonstrates that our framework can effectively play Werewolf\ngame without tuning the parameters of the LLMs. More importantly, strategic\nbehaviors begin to emerge in our experiments, suggesting that it will be a\nfruitful journey to engage LLMs in communication games and associated domains.", "published": "2023-09-09 01:56:40", "link": "http://arxiv.org/abs/2309.04658v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Embedding structure matters: Comparing methods to adapt multilingual\n  vocabularies to new languages", "abstract": "Pre-trained multilingual language models underpin a large portion of modern\nNLP tools outside of English. A strong baseline for specializing these models\nfor specific languages is Language-Adaptive Pre-Training (LAPT). However,\nretaining a large cross-lingual vocabulary and embedding matrix comes at\nconsiderable excess computational cost during adaptation. In this study, we\npropose several simple techniques to replace a cross-lingual vocabulary with a\ncompact, language-specific one. Namely, we address strategies for\nre-initializing the token embedding matrix after vocabulary specialization. We\nthen provide a systematic experimental comparison of our techniques, in\naddition to the recently-proposed Focus method. We demonstrate that: 1)\nEmbedding-replacement techniques in the monolingual transfer literature are\ninadequate for adapting multilingual models. 2) Replacing cross-lingual\nvocabularies with smaller specialized ones provides an efficient method to\nimprove performance in low-resource languages. 3) Simple embedding\nre-initialization techniques based on script-wise sub-distributions rival\ntechniques such as Focus, which rely on similarity scores obtained from an\nauxiliary model.", "published": "2023-09-09 04:27:18", "link": "http://arxiv.org/abs/2309.04679v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EPA: Easy Prompt Augmentation on Large Language Models via Multiple\n  Sources and Multiple Targets", "abstract": "Large language models (LLMs) have shown promising performance on various NLP\ntasks via task prompting. And their performance can be further improved by\nappending task demonstrations to the head of the prompt. And usually, a better\nperformance can be achieved with more demonstrations. However, asking the users\nto write the demonstrations can be cumbersome. As a simple yet cost-effective\nworkaround, this paper proposes a novel method called EPA (\\textbf{E}asy\n\\textbf{P}rompt \\textbf{A}ugmentation)\\footnote{While this paper considers\naugmenting prompts via demonstrations, we name it EPA as the name EDA is\nalready taken by a well-known NLP method \\citep{wei-zou-2019-eda}.} that\neffectively minimizes user efforts in writing demonstrations while improving\nthe model performance at the same time. EPA achieves these goals by\nautomatically augmenting the demonstrations with multiple sources/targets,\nwhere each of them paraphrases each other. This is well motivated as augmenting\ndata via paraphrasing effectively improves neural language models. EPA thus\nemploys paraphrasing as an augmentation method for in-context learning.\nExtensive experiments indicate that EPA effectively improves both NLU and NLG\ntasks, covering from natural language inference to machine translation in\ntranslating tens of languages.\\footnote{Code and data will be released upon\npublication.}", "published": "2023-09-09 09:03:50", "link": "http://arxiv.org/abs/2309.04725v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MMHQA-ICL: Multimodal In-context Learning for Hybrid Question Answering\n  over Text, Tables and Images", "abstract": "In the real world, knowledge often exists in a multimodal and heterogeneous\nform. Addressing the task of question answering with hybrid data types,\nincluding text, tables, and images, is a challenging task (MMHQA). Recently,\nwith the rise of large language models (LLM), in-context learning (ICL) has\nbecome the most popular way to solve QA problems. We propose MMHQA-ICL\nframework for addressing this problems, which includes stronger heterogeneous\ndata retriever and an image caption module. Most importantly, we propose a\nType-specific In-context Learning Strategy for MMHQA, enabling LLMs to leverage\ntheir powerful performance in this task. We are the first to use end-to-end LLM\nprompting method for this task. Experimental results demonstrate that our\nframework outperforms all baselines and methods trained on the full dataset,\nachieving state-of-the-art results under the few-shot setting on the\nMultimodalQA dataset.", "published": "2023-09-09 13:35:01", "link": "http://arxiv.org/abs/2309.04790v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FaNS: a Facet-based Narrative Similarity Metric", "abstract": "Similar Narrative Retrieval is a crucial task since narratives are essential\nfor explaining and understanding events, and multiple related narratives often\nhelp to create a holistic view of the event of interest. To accurately identify\nsemantically similar narratives, this paper proposes a novel narrative\nsimilarity metric called Facet-based Narrative Similarity (FaNS), based on the\nclassic 5W1H facets (Who, What, When, Where, Why, and How), which are extracted\nby leveraging the state-of-the-art Large Language Models (LLMs). Unlike\nexisting similarity metrics that only focus on overall lexical/semantic match,\nFaNS provides a more granular matching along six different facets independently\nand then combines them. To evaluate FaNS, we created a comprehensive dataset by\ncollecting narratives from AllSides, a third-party news portal. Experimental\nresults demonstrate that the FaNS metric exhibits a higher correlation (37\\%\nhigher) than traditional text similarity metrics that directly measure the\nlexical/semantic match between narratives, demonstrating its effectiveness in\ncomparing the finer details between a pair of narratives.", "published": "2023-09-09 15:29:24", "link": "http://arxiv.org/abs/2309.04823v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neurons in Large Language Models: Dead, N-gram, Positional", "abstract": "We analyze a family of large language models in such a lightweight manner\nthat can be done on a single GPU. Specifically, we focus on the OPT family of\nmodels ranging from 125m to 66b parameters and rely only on whether an FFN\nneuron is activated or not. First, we find that the early part of the network\nis sparse and represents many discrete features. Here, many neurons (more than\n70% in some layers of the 66b model) are \"dead\", i.e. they never activate on a\nlarge collection of diverse data. At the same time, many of the alive neurons\nare reserved for discrete features and act as token and n-gram detectors.\nInterestingly, their corresponding FFN updates not only promote next token\ncandidates as could be expected, but also explicitly focus on removing the\ninformation about triggering them tokens, i.e., current input. To the best of\nour knowledge, this is the first example of mechanisms specialized at removing\n(rather than adding) information from the residual stream. With scale, models\nbecome more sparse in a sense that they have more dead neurons and token\ndetectors. Finally, some neurons are positional: them being activated or not\ndepends largely (or solely) on position and less so (or not at all) on textual\ndata. We find that smaller models have sets of neurons acting as position range\nindicators while larger models operate in a less explicit manner.", "published": "2023-09-09 15:51:36", "link": "http://arxiv.org/abs/2309.04827v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Finetuning Large Language Models For Vietnamese Chatbot", "abstract": "Large language models (LLMs), such as GPT-4, PaLM, and LLaMa, have been shown\nto achieve remarkable performance across a variety of natural language tasks.\nRecent advancements in instruction tuning bring LLMs with ability in following\nuser's instructions and producing human-like responses. However, the high costs\nassociated with training and implementing LLMs pose challenges to academic\nresearch. Furthermore, the availability of pretrained LLMs and instruction-tune\ndatasets for Vietnamese language is limited. To tackle these concerns, we\nleverage large-scale instruction-following datasets from open-source projects,\nnamely Alpaca, GPT4All, and Chat-Doctor, which cover general domain and\nspecific medical domain. To the best of our knowledge, these are the first\ninstructional dataset for Vietnamese. Subsequently, we utilize\nparameter-efficient tuning through Low-Rank Adaptation (LoRA) on two open LLMs:\nBloomz (Multilingual) and GPTJ-6B (Vietnamese), resulting four models:\nBloomz-Chat, Bloomz-Doctor, GPTJ-Chat, GPTJ-Doctor.Finally, we assess the\neffectiveness of our methodology on a per-sample basis, taking into\nconsideration the helpfulness, relevance, accuracy, level of detail in their\nresponses. This evaluation process entails the utilization of GPT-4 as an\nautomated scoring mechanism. Despite utilizing a low-cost setup, our method\ndemonstrates about 20-30\\% improvement over the original models in our\nevaluation tasks.", "published": "2023-09-09 00:11:53", "link": "http://arxiv.org/abs/2309.04646v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MADLAD-400: A Multilingual And Document-Level Large Audited Dataset", "abstract": "We introduce MADLAD-400, a manually audited, general domain 3T token\nmonolingual dataset based on CommonCrawl, spanning 419 languages. We discuss\nthe limitations revealed by self-auditing MADLAD-400, and the role data\nauditing had in the dataset creation process. We then train and release a\n10.7B-parameter multilingual machine translation model on 250 billion tokens\ncovering over 450 languages using publicly available data, and find that it is\ncompetitive with models that are significantly larger, and report the results\non different domains. In addition, we train a 8B-parameter language model, and\nassess the results on few-shot translation. We make the baseline models\navailable to the research community.", "published": "2023-09-09 02:34:01", "link": "http://arxiv.org/abs/2309.04662v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FIAT: Fusing learning paradigms with Instruction-Accelerated Tuning", "abstract": "Learning paradigms for large language models (LLMs) currently tend to fall\nwithin either in-context learning (ICL) or full fine-tuning. Each of these\ncomes with their own trade-offs based on available data, model size, compute\ncost, ease-of-use, and final quality with neither solution performing well\nacross-the-board. In this article, we first describe ICL and fine-tuning\nparadigms in a way that highlights their natural connections. Based on these\nconnections, we propose a new learning paradigm called FIAT that fuses the best\nof these paradigms together, enabling prompt-engineered instructions and\nchain-of-thought reasoning with the very largest models while also using\nsimilar methods to perform parameter updates on a modestly-sized LLM with\nparameter-efficient tuning. We evaluate FIAT's effectiveness on a variety of\nmultilingual tasks and observe that FIAT performs better than both ICL and\nfine-tuning at scales ranging from 100-10,000 training examples. We hope that\nFIAT provides a practical way of harnessing the full potential of LLMs without\nneeding to make a hard choice between learning paradigms.", "published": "2023-09-09 02:43:48", "link": "http://arxiv.org/abs/2309.04663v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Code-Style In-Context Learning for Knowledge-Based Question Answering", "abstract": "Current methods for Knowledge-Based Question Answering (KBQA) usually rely on\ncomplex training techniques and model frameworks, leading to many limitations\nin practical applications. Recently, the emergence of In-Context Learning (ICL)\ncapabilities in Large Language Models (LLMs) provides a simple and\ntraining-free semantic parsing paradigm for KBQA: Given a small number of\nquestions and their labeled logical forms as demo examples, LLMs can understand\nthe task intent and generate the logic form for a new question. However,\ncurrent powerful LLMs have little exposure to logic forms during pre-training,\nresulting in a high format error rate. To solve this problem, we propose a\ncode-style in-context learning method for KBQA, which converts the generation\nprocess of unfamiliar logical form into the more familiar code generation\nprocess for LLMs. Experimental results on three mainstream datasets show that\nour method dramatically mitigated the formatting error problem in generating\nlogic forms while realizing a new SOTA on WebQSP, GrailQA, and GraphQ under the\nfew-shot setting. The code and supplementary files are released at\nhttps://github.com/Arthurizijar/KB-Coder .", "published": "2023-09-09 06:27:00", "link": "http://arxiv.org/abs/2309.04695v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data Augmentation for Conversational AI", "abstract": "Advancements in conversational systems have revolutionized information\naccess, surpassing the limitations of single queries. However, developing\ndialogue systems requires a large amount of training data, which is a challenge\nin low-resource domains and languages. Traditional data collection methods like\ncrowd-sourcing are labor-intensive and time-consuming, making them ineffective\nin this context. Data augmentation (DA) is an affective approach to alleviate\nthe data scarcity problem in conversational systems. This tutorial provides a\ncomprehensive and up-to-date overview of DA approaches in the context of\nconversational systems. It highlights recent advances in conversation\naugmentation, open domain and task-oriented conversation generation, and\ndifferent paradigms of evaluating these models. We also discuss current\nchallenges and future directions in order to help researchers and practitioners\nto further advance the field in this area.", "published": "2023-09-09 09:56:35", "link": "http://arxiv.org/abs/2309.04739v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment\n  to Cultural Reasoning", "abstract": "We present SeaEval, a benchmark for multilingual foundation models. In\naddition to characterizing how these models understand and reason with natural\nlanguage, we also investigate how well they comprehend cultural practices,\nnuances, and values. Alongside standard accuracy metrics, we investigate the\nbrittleness of foundation models in the dimensions of semantics and\nmultilinguality. Our analyses span both open-sourced and closed models, leading\nto empirical results across classic NLP tasks, reasoning, and cultural\ncomprehension. Key findings indicate (1) Most models exhibit varied behavior\nwhen given paraphrased instructions. (2) Many models still suffer from exposure\nbias (e.g., positional bias, majority label bias). (3) For questions rooted in\nfactual, scientific, and commonsense knowledge, consistent responses are\nexpected across multilingual queries that are semantically equivalent. Yet,\nmost models surprisingly demonstrate inconsistent performance on these queries.\n(4) Multilingually-trained models have not attained \"balanced multilingual\"\ncapabilities. Our endeavors underscore the need for more generalizable semantic\nrepresentations and enhanced multilingual contextualization. SeaEval can serve\nas a launchpad for more thorough investigations and evaluations for\nmultilingual and multicultural scenarios.", "published": "2023-09-09 11:42:22", "link": "http://arxiv.org/abs/2309.04766v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Distributional Data Augmentation Methods for Low Resource Language", "abstract": "Text augmentation is a technique for constructing synthetic data from an\nunder-resourced corpus to improve predictive performance. Synthetic data\ngeneration is common in numerous domains. However, recently text augmentation\nhas emerged in natural language processing (NLP) to improve downstream tasks.\nOne of the current state-of-the-art text augmentation techniques is easy data\naugmentation (EDA), which augments the training data by injecting and replacing\nsynonyms and randomly permuting sentences. One major obstacle with EDA is the\nneed for versatile and complete synonym dictionaries, which cannot be easily\nfound in low-resource languages. To improve the utility of EDA, we propose two\nextensions, easy distributional data augmentation (EDDA) and type specific\nsimilar word replacement (TSSR), which uses semantic word context information\nand part-of-speech tags for word replacement and augmentation. In an extensive\nempirical evaluation, we show the utility of the proposed methods, measured by\nF1 score, on two representative datasets in Swedish as an example of a\nlow-resource language. With the proposed methods, we show that augmented data\nimprove classification performances in low-resource settings.", "published": "2023-09-09 19:01:59", "link": "http://arxiv.org/abs/2309.04862v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards LLM-based Autograding for Short Textual Answers", "abstract": "Grading exams is an important, labor-intensive, subjective, repetitive, and\nfrequently challenging task. The feasibility of autograding textual responses\nhas greatly increased thanks to the availability of large language models\n(LLMs) such as ChatGPT and the substantial influx of data brought about by\ndigitalization. However, entrusting AI models with decision-making roles raises\nethical considerations, mainly stemming from potential biases and issues\nrelated to generating false information. Thus, in this manuscript, we provide\nan evaluation of a large language model for the purpose of autograding, while\nalso highlighting how LLMs can support educators in validating their grading\nprocedures. Our evaluation is targeted towards automatic short textual answers\ngrading (ASAG), spanning various languages and examinations from two distinct\ncourses. Our findings suggest that while \"out-of-the-box\" LLMs provide a\nvaluable tool to provide a complementary perspective, their readiness for\nindependent automated grading remains a work in progress, necessitating human\noversight.", "published": "2023-09-09 22:25:56", "link": "http://arxiv.org/abs/2309.11508v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Toward Reproducing Network Research Results Using Large Language Models", "abstract": "Reproducing research results in the networking community is important for\nboth academia and industry. The current best practice typically resorts to\nthree approaches: (1) looking for publicly available prototypes; (2) contacting\nthe authors to get a private prototype; and (3) manually implementing a\nprototype following the description of the publication. However, most published\nnetwork research does not have public prototypes and private prototypes are\nhard to get. As such, most reproducing efforts are spent on manual\nimplementation based on the publications, which is both time and labor\nconsuming and error-prone. In this paper, we boldly propose reproducing network\nresearch results using the emerging large language models (LLMs). In\nparticular, we first prove its feasibility with a small-scale experiment, in\nwhich four students with essential networking knowledge each reproduces a\ndifferent networking system published in prominent conferences and journals by\nprompt engineering ChatGPT. We report the experiment's observations and lessons\nand discuss future open research questions of this proposal. This work raises\nno ethical issue.", "published": "2023-09-09 08:07:54", "link": "http://arxiv.org/abs/2309.04716v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards Better Multi-modal Keyphrase Generation via Visual Entity\n  Enhancement and Multi-granularity Image Noise Filtering", "abstract": "Multi-modal keyphrase generation aims to produce a set of keyphrases that\nrepresent the core points of the input text-image pair. In this regard,\ndominant methods mainly focus on multi-modal fusion for keyphrase generation.\nNevertheless, there are still two main drawbacks: 1) only a limited number of\nsources, such as image captions, can be utilized to provide auxiliary\ninformation. However, they may not be sufficient for the subsequent keyphrase\ngeneration. 2) the input text and image are often not perfectly matched, and\nthus the image may introduce noise into the model. To address these\nlimitations, in this paper, we propose a novel multi-modal keyphrase generation\nmodel, which not only enriches the model input with external knowledge, but\nalso effectively filters image noise. First, we introduce external visual\nentities of the image as the supplementary input to the model, which benefits\nthe cross-modal semantic alignment for keyphrase generation. Second, we\nsimultaneously calculate an image-text matching score and image region-text\ncorrelation scores to perform multi-granularity image noise filtering.\nParticularly, we introduce the correlation scores between image regions and\nground-truth keyphrases to refine the calculation of the previously-mentioned\ncorrelation scores. To demonstrate the effectiveness of our model, we conduct\nseveral groups of experiments on the benchmark dataset.\n  Experimental results and in-depth analyses show that our model achieves the\nstate-of-the-art performance. Our code is available on\nhttps://github.com/DeepLearnXMU/MM-MKP.", "published": "2023-09-09 09:41:36", "link": "http://arxiv.org/abs/2309.04734v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Leveraging Large Language Models for Exploiting ASR Uncertainty", "abstract": "While large language models excel in a variety of natural language processing\n(NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they\nmust either rely on off-the-shelf automatic speech recognition (ASR) systems\nfor transcription, or be equipped with an in-built speech modality. This work\nfocuses on the former scenario, where LLM's accuracy on SLU tasks is\nconstrained by the accuracy of a fixed ASR system on the spoken input.\nSpecifically, we tackle speech-intent classification task, where a high\nword-error-rate can limit the LLM's ability to understand the spoken intent.\nInstead of chasing a high accuracy by designing complex or specialized\narchitectures regardless of deployment costs, we seek to answer how far we can\ngo without substantially changing the underlying ASR and LLM, which can\npotentially be shared by multiple unrelated tasks. To this end, we propose\nprompting the LLM with an n-best list of ASR hypotheses instead of only the\nerror-prone 1-best hypothesis. We explore prompt-engineering to explain the\nconcept of n-best lists to the LLM; followed by the finetuning of Low-Rank\nAdapters on the downstream tasks. Our approach using n-best lists proves to be\neffective on a device-directed speech detection task as well as on a keyword\nspotting task, where systems using n-best list prompts outperform those using\n1-best ASR hypothesis; thus paving the way for an efficient method to exploit\nASR uncertainty via LLMs for speech-based applications.", "published": "2023-09-09 17:02:33", "link": "http://arxiv.org/abs/2309.04842v2", "categories": ["cs.CL", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Speech Emotion Recognition with Distilled Prosodic and Linguistic Affect\n  Representations", "abstract": "We propose EmoDistill, a novel speech emotion recognition (SER) framework\nthat leverages cross-modal knowledge distillation during training to learn\nstrong linguistic and prosodic representations of emotion from speech. During\ninference, our method only uses a stream of speech signals to perform unimodal\nSER thus reducing computation overhead and avoiding run-time transcription and\nprosodic feature extraction errors. During training, our method distills\ninformation at both embedding and logit levels from a pair of pre-trained\nProsodic and Linguistic teachers that are fine-tuned for SER. Experiments on\nthe IEMOCAP benchmark demonstrate that our method outperforms other unimodal\nand multimodal techniques by a considerable margin, and achieves\nstate-of-the-art performance of 77.49% unweighted accuracy and 78.91% weighted\naccuracy. Detailed ablation studies demonstrate the impact of each component of\nour method.", "published": "2023-09-09 17:30:35", "link": "http://arxiv.org/abs/2309.04849v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reverse-Engineering Decoding Strategies Given Blackbox Access to a\n  Language Generation System", "abstract": "Neural language models are increasingly deployed into APIs and websites that\nallow a user to pass in a prompt and receive generated text. Many of these\nsystems do not reveal generation parameters. In this paper, we present methods\nto reverse-engineer the decoding method used to generate text (i.e., top-$k$ or\nnucleus sampling). Our ability to discover which decoding strategy was used has\nimplications for detecting generated text. Additionally, the process of\ndiscovering the decoding strategy can reveal biases caused by selecting\ndecoding settings which severely truncate a model's predicted distributions. We\nperform our attack on several families of open-source language models, as well\nas on production systems (e.g., ChatGPT).", "published": "2023-09-09 18:19:47", "link": "http://arxiv.org/abs/2309.04858v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Analysis of Disinformation and Fake News Detection Using Fine-Tuned\n  Large Language Model", "abstract": "The paper considers the possibility of fine-tuning Llama 2 large language\nmodel (LLM) for the disinformation analysis and fake news detection. For\nfine-tuning, the PEFT/LoRA based approach was used. In the study, the model was\nfine-tuned for the following tasks: analysing a text on revealing\ndisinformation and propaganda narratives, fact checking, fake news detection,\nmanipulation analytics, extracting named entities with their sentiments. The\nobtained results show that the fine-tuned Llama 2 model can perform a deep\nanalysis of texts and reveal complex styles and narratives. Extracted\nsentiments for named entities can be considered as predictive features in\nsupervised machine learning models.", "published": "2023-09-09 07:10:19", "link": "http://arxiv.org/abs/2309.04704v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mask-CTC-based Encoder Pre-training for Streaming End-to-End Speech\n  Recognition", "abstract": "Achieving high accuracy with low latency has always been a challenge in\nstreaming end-to-end automatic speech recognition (ASR) systems. By attending\nto more future contexts, a streaming ASR model achieves higher accuracy but\nresults in larger latency, which hurts the streaming performance. In the\nMask-CTC framework, an encoder network is trained to learn the feature\nrepresentation that anticipates long-term contexts, which is desirable for\nstreaming ASR. Mask-CTC-based encoder pre-training has been shown beneficial in\nachieving low latency and high accuracy for triggered attention-based ASR.\nHowever, the effectiveness of this method has not been demonstrated for various\nmodel architectures, nor has it been verified that the encoder has the expected\nlook-ahead capability to reduce latency. This study, therefore, examines the\neffectiveness of Mask-CTCbased pre-training for models with different\narchitectures, such as Transformer-Transducer and contextual block streaming\nASR. We also discuss the effect of the proposed pre-training method on\nobtaining accurate output spike timing.", "published": "2023-09-09 01:05:59", "link": "http://arxiv.org/abs/2309.04654v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring Music Genre Classification: Algorithm Analysis and Deployment\n  Architecture", "abstract": "Music genre classification has become increasingly critical with the advent\nof various streaming applications. Nowadays, we find it impossible to imagine\nusing the artist's name and song title to search for music in a sophisticated\nmusic app. It is always difficult to classify music correctly because the\ninformation linked to music, such as region, artist, album, or non-album, is so\nvariable. This paper presents a study on music genre classification using a\ncombination of Digital Signal Processing (DSP) and Deep Learning (DL)\ntechniques. A novel algorithm is proposed that utilizes both DSP and DL methods\nto extract relevant features from audio signals and classify them into various\ngenres. The algorithm was tested on the GTZAN dataset and achieved high\naccuracy. An end-to-end deployment architecture is also proposed for\nintegration into music-related applications. The performance of the algorithm\nis analyzed and future directions for improvement are discussed. The proposed\nDSP and DL-based music genre classification algorithm and deployment\narchitecture demonstrate a promising approach for music genre classification.", "published": "2023-09-09 19:01:12", "link": "http://arxiv.org/abs/2309.04861v2", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AudRandAug: Random Image Augmentations for Audio Classification", "abstract": "Data augmentation has proven to be effective in training neural networks.\nRecently, a method called RandAug was proposed, randomly selecting data\naugmentation techniques from a predefined search space. RandAug has\ndemonstrated significant performance improvements for image-related tasks while\nimposing minimal computational overhead. However, no prior research has\nexplored the application of RandAug specifically for audio data augmentation,\nwhich converts audio into an image-like pattern. To address this gap, we\nintroduce AudRandAug, an adaptation of RandAug for audio data. AudRandAug\nselects data augmentation policies from a dedicated audio search space. To\nevaluate the effectiveness of AudRandAug, we conducted experiments using\nvarious models and datasets. Our findings indicate that AudRandAug outperforms\nother existing data augmentation methods regarding accuracy performance.", "published": "2023-09-09 11:25:03", "link": "http://arxiv.org/abs/2309.04762v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generalized Minimum Error with Fiducial Points Criterion for Robust\n  Learning", "abstract": "The conventional Minimum Error Entropy criterion (MEE) has its limitations,\nshowing reduced sensitivity to error mean values and uncertainty regarding\nerror probability density function locations. To overcome this, a MEE with\nfiducial points criterion (MEEF), was presented. However, the efficacy of the\nMEEF is not consistent due to its reliance on a fixed Gaussian kernel. In this\npaper, a generalized minimum error with fiducial points criterion (GMEEF) is\npresented by adopting the Generalized Gaussian Density (GGD) function as\nkernel. The GGD extends the Gaussian distribution by introducing a shape\nparameter that provides more control over the tail behavior and peakedness. In\naddition, due to the high computational complexity of GMEEF criterion, the\nquantized idea is introduced to notably lower the computational load of the\nGMEEF-type algorithm. Finally, the proposed criterions are introduced to the\ndomains of adaptive filter, kernel recursive algorithm, and multilayer\nperceptron. Several numerical simulations, which contain system identification,\nacoustic echo cancellation, times series prediction, and supervised\nclassification, indicate that the novel algorithms' performance performs\nexcellently.", "published": "2023-09-09 03:07:50", "link": "http://arxiv.org/abs/2309.04670v1", "categories": ["eess.SP", "cs.SD", "cs.SY", "eess.AS", "eess.IV", "eess.SY", "I.5.3; I.5.4; I.4.9"], "primary_category": "eess.SP"}
