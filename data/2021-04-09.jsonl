{"title": "Design and Implementation of English To Yor\u00f9b\u00e1 Verb Phrase Machine\n  Translation System", "abstract": "We aim to develop an English-to-Yoruba machine translation system which can\ntranslate English verb phrase text to its Yoruba equivalent.Words from both\nlanguages Source Language and Target Language were collected for the verb\nphrase group in the home domain. The lexical translation is done by assigning\nvalues of the matching word in the dictionary. The syntax of the two languages\nwas realized using Context-Free Grammar, we validated the rewrite rules with\nfinite state automata. The human evaluation method was used and expert fluency\nwas scored. The evaluation shows the system performed better than that of\nsampled Google translation with over 70 percent of the response matching that\nof the system's output.", "published": "2021-04-09 00:50:57", "link": "http://arxiv.org/abs/2104.04125v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Annotating and Modeling Fine-grained Factuality in Summarization", "abstract": "Recent pre-trained abstractive summarization systems have started to achieve\ncredible performance, but a major barrier to their use in practice is their\npropensity to output summaries that are not faithful to the input and that\ncontain factual errors. While a number of annotated datasets and statistical\nmodels for assessing factuality have been explored, there is no clear picture\nof what errors are most important to target or where current techniques are\nsucceeding and failing. We explore both synthetic and human-labeled data\nsources for training models to identify factual errors in summarization, and\nstudy factuality at the word-, dependency-, and sentence-level. Our\nobservations are threefold. First, exhibited factual errors differ\nsignificantly across datasets, and commonly-used training sets of simple\nsynthetic errors do not reflect errors made on abstractive datasets like XSum.\nSecond, human-labeled data with fine-grained annotations provides a more\neffective training signal than sentence-level annotations or synthetic data.\nFinally, we show that our best factuality detection model enables training of\nmore factual XSum summarization models by allowing us to identify non-factual\ntokens in the training data.", "published": "2021-04-09 11:20:44", "link": "http://arxiv.org/abs/2104.04302v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Noisy-Labeled NER with Confidence Estimation", "abstract": "Recent studies in deep learning have shown significant progress in named\nentity recognition (NER). Most existing works assume clean data annotation, yet\na fundamental challenge in real-world scenarios is the large amount of noise\nfrom a variety of sources (e.g., pseudo, weak, or distant annotations). This\nwork studies NER under a noisy labeled setting with calibrated confidence\nestimation. Based on empirical observations of different training dynamics of\nnoisy and clean labels, we propose strategies for estimating confidence scores\nbased on local and global independence assumptions. We partially marginalize\nout labels of low confidence with a CRF model. We further propose a calibration\nmethod for confidence scores based on the structure of entity labels. We\nintegrate our approach into a self-training framework for boosting performance.\nExperiments in general noisy settings with four languages and distantly labeled\nsettings demonstrate the effectiveness of our method. Our code can be found at\nhttps://github.com/liukun95/Noisy-NER-Confidence-Estimation", "published": "2021-04-09 11:56:46", "link": "http://arxiv.org/abs/2104.04318v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A preliminary study on evaluating Consultation Notes with Post-Editing", "abstract": "Automatic summarisation has the potential to aid physicians in streamlining\nclerical tasks such as note taking. But it is notoriously difficult to evaluate\nthese systems and demonstrate that they are safe to be used in a clinical\nsetting. To circumvent this issue, we propose a semi-automatic approach whereby\nphysicians post-edit generated notes before submitting them. We conduct a\npreliminary study on the time saving of automatically generated consultation\nnotes with post-editing. Our evaluators are asked to listen to mock\nconsultations and to post-edit three generated notes. We time this and find\nthat it is faster than writing the note from scratch. We present insights and\nlessons learnt from this experiment.", "published": "2021-04-09 14:42:00", "link": "http://arxiv.org/abs/2104.04402v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards objectively evaluating the quality of generated medical\n  summaries", "abstract": "We propose a method for evaluating the quality of generated text by asking\nevaluators to count facts, and computing precision, recall, f-score, and\naccuracy from the raw counts. We believe this approach leads to a more\nobjective and easier to reproduce evaluation. We apply this to the task of\nmedical report summarisation, where measuring objective quality and accuracy is\nof paramount importance.", "published": "2021-04-09 15:02:56", "link": "http://arxiv.org/abs/2104.04412v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Larger-Context Tagging: When and Why Does It Work?", "abstract": "The development of neural networks and pretraining techniques has spawned\nmany sentence-level tagging systems that achieved superior performance on\ntypical benchmarks. However, a relatively less discussed topic is what if more\ncontext information is introduced into current top-scoring tagging systems.\nAlthough several existing works have attempted to shift tagging systems from\nsentence-level to document-level, there is still no consensus conclusion about\nwhen and why it works, which limits the applicability of the larger-context\napproach in tagging tasks. In this paper, instead of pursuing a\nstate-of-the-art tagging system by architectural exploration, we focus on\ninvestigating when and why the larger-context training, as a general strategy,\ncan work.\n  To this end, we conduct a thorough comparative study on four proposed\naggregators for context information collecting and present an attribute-aided\nevaluation method to interpret the improvement brought by larger-context\ntraining. Experimentally, we set up a testbed based on four tagging tasks and\nthirteen datasets. Hopefully, our preliminary observations can deepen the\nunderstanding of larger-context training and enlighten more follow-up works on\nthe use of contextual information.", "published": "2021-04-09 15:35:30", "link": "http://arxiv.org/abs/2104.04434v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Did they answer? Subjective acts and intents in conversational discourse", "abstract": "Discourse signals are often implicit, leaving it up to the interpreter to\ndraw the required inferences. At the same time, discourse is embedded in a\nsocial context, meaning that interpreters apply their own assumptions and\nbeliefs when resolving these inferences, leading to multiple, valid\ninterpretations. However, current discourse data and frameworks ignore the\nsocial aspect, expecting only a single ground truth. We present the first\ndiscourse dataset with multiple and subjective interpretations of English\nconversation in the form of perceived conversation acts and intents. We\ncarefully analyze our dataset and create computational models to (1) confirm\nour hypothesis that taking into account the bias of the interpreters leads to\nbetter predictions of the interpretations, (2) and show disagreements are\nnuanced and require a deeper understanding of the different contextual factors.\nWe share our dataset and code at http://github.com/elisaF/subjective_discourse.", "published": "2021-04-09 16:34:19", "link": "http://arxiv.org/abs/2104.04470v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explaining Neural Network Predictions on Sentence Pairs via Learning\n  Word-Group Masks", "abstract": "Explaining neural network models is important for increasing their\ntrustworthiness in real-world applications. Most existing methods generate\npost-hoc explanations for neural network models by identifying individual\nfeature attributions or detecting interactions between adjacent features.\nHowever, for models with text pairs as inputs (e.g., paraphrase\nidentification), existing methods are not sufficient to capture feature\ninteractions between two texts and their simple extension of computing all\nword-pair interactions between two texts is computationally inefficient. In\nthis work, we propose the Group Mask (GMASK) method to implicitly detect word\ncorrelations by grouping correlated words from the input text pair together and\nmeasure their contribution to the corresponding NLP tasks as a whole. The\nproposed method is evaluated with two different model architectures\n(decomposable attention model and BERT) across four datasets, including natural\nlanguage inference and paraphrase identification tasks. Experiments show the\neffectiveness of GMASK in providing faithful explanations to these models.", "published": "2021-04-09 17:14:34", "link": "http://arxiv.org/abs/2104.04488v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Connecting Attributions and QA Model Behavior on Realistic\n  Counterfactuals", "abstract": "When a model attribution technique highlights a particular part of the input,\na user might understand this highlight as making a statement about\ncounterfactuals (Miller, 2019): if that part of the input were to change, the\nmodel's prediction might change as well. This paper investigates how well\ndifferent attribution techniques align with this assumption on realistic\ncounterfactuals in the case of reading comprehension (RC). RC is a particularly\nchallenging test case, as token-level attributions that have been extensively\nstudied in other NLP tasks such as sentiment analysis are less suitable to\nrepresent the reasoning that RC models perform. We construct counterfactual\nsets for three different RC settings, and through heuristics that can connect\nattribution methods' outputs to high-level model behavior, we can evaluate how\nuseful different attribution methods and even different formats are for\nunderstanding counterfactuals. We find that pairwise attributions are better\nsuited to RC than token-level attributions across these different RC settings,\nwith our best performance coming from a modification that we propose to an\nexisting pairwise attribution method.", "published": "2021-04-09 17:55:21", "link": "http://arxiv.org/abs/2104.04515v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UPB at SemEval-2021 Task 8: Extracting Semantic Information on\n  Measurements as Multi-Turn Question Answering", "abstract": "Extracting semantic information on measurements and counts is an important\ntopic in terms of analyzing scientific discourses. The 8th task of\nSemEval-2021: Counts and Measurements (MeasEval) aimed to boost research in\nthis direction by providing a new dataset on which participants train their\nmodels to extract meaningful information on measurements from scientific texts.\nThe competition is composed of five subtasks that build on top of each other:\n(1) quantity span identification, (2) unit extraction from the identified\nquantities and their value modifier classification, (3) span identification for\nmeasured entities and measured properties, (4) qualifier span identification,\nand (5) relation extraction between the identified quantities, measured\nentities, measured properties, and qualifiers. We approached these challenges\nby first identifying the quantities, extracting their units of measurement,\nclassifying them with corresponding modifiers, and afterwards using them to\njointly solve the last three subtasks in a multi-turn question answering\nmanner. Our best performing model obtained an overlapping F1-score of 36.91% on\nthe test set.", "published": "2021-04-09 18:23:30", "link": "http://arxiv.org/abs/2104.04549v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Comparison of Instance Attribution Methods for NLP", "abstract": "Widespread adoption of deep models has motivated a pressing need for\napproaches to interpret network outputs and to facilitate model debugging.\nInstance attribution methods constitute one means of accomplishing these goals\nby retrieving training instances that (may have) led to a particular\nprediction. Influence functions (IF; Koh and Liang 2017) provide machinery for\ndoing this by quantifying the effect that perturbing individual train instances\nwould have on a specific test prediction. However, even approximating the IF is\ncomputationally expensive, to the degree that may be prohibitive in many cases.\nMight simpler approaches (e.g., retrieving train examples most similar to a\ngiven test point) perform comparably? In this work, we evaluate the degree to\nwhich different potential instance attribution agree with respect to the\nimportance of training samples. We find that simple retrieval methods yield\ntraining instances that differ from those identified via gradient-based methods\n(such as IFs), but that nonetheless exhibit desirable characteristics similar\nto more complex attribution methods. Code for all methods and experiments in\nthis paper is available at:\nhttps://github.com/successar/instance_attributions_NLP.", "published": "2021-04-09 01:03:17", "link": "http://arxiv.org/abs/2104.04128v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Road to Know-Where: An Object-and-Room Informed Sequential BERT for\n  Indoor Vision-Language Navigation", "abstract": "Vision-and-Language Navigation (VLN) requires an agent to find a path to a\nremote location on the basis of natural-language instructions and a set of\nphoto-realistic panoramas. Most existing methods take the words in the\ninstructions and the discrete views of each panorama as the minimal unit of\nencoding. However, this requires a model to match different nouns (e.g., TV,\ntable) against the same input view feature. In this work, we propose an\nobject-informed sequential BERT to encode visual perceptions and linguistic\ninstructions at the same fine-grained level, namely objects and words. Our\nsequential BERT also enables the visual-textual clues to be interpreted in\nlight of the temporal context, which is crucial to multi-round VLN tasks.\nAdditionally, we enable the model to identify the relative direction (e.g.,\nleft/right/front/back) of each navigable location and the room type (e.g.,\nbedroom, kitchen) of its current and final navigation goal, as such information\nis widely mentioned in instructions implying the desired next and final\nlocations. We thus enable the model to know-where the objects lie in the\nimages, and to know-where they stand in the scene. Extensive experiments\ndemonstrate the effectiveness compared against several state-of-the-art methods\non three indoor VLN tasks: REVERIE, NDH, and R2R. Project repository:\nhttps://github.com/YuankaiQi/ORIST", "published": "2021-04-09 02:44:39", "link": "http://arxiv.org/abs/2104.04167v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "BERT-based Chinese Text Classification for Emergency Domain with a Novel\n  Loss Function", "abstract": "This paper proposes an automatic Chinese text categorization method for\nsolving the emergency event report classification problem. Since bidirectional\nencoder representations from transformers (BERT) has achieved great success in\nnatural language processing domain, it is employed to derive emergency text\nfeatures in this study. To overcome the data imbalance problem in the\ndistribution of emergency event categories, a novel loss function is proposed\nto improve the performance of the BERT-based model. Meanwhile, to avoid the\nimpact of the extreme learning rate, the Adabound optimization algorithm that\nachieves a gradual smooth transition from Adam to SGD is employed to learn\nparameters of the model. To verify the feasibility and effectiveness of the\nproposed method, a Chinese emergency text dataset collected from the Internet\nis employed. Compared with benchmarking methods, the proposed method has\nachieved the best performance in terms of accuracy, weighted-precision,\nweighted-recall, and weighted-F1 values. Therefore, it is promising to employ\nthe proposed method for real applications in smart emergency management\nsystems.", "published": "2021-04-09 05:25:00", "link": "http://arxiv.org/abs/2104.04197v1", "categories": ["cs.CL", "cs.AI", "68T50"], "primary_category": "cs.CL"}
{"title": "Video-aided Unsupervised Grammar Induction", "abstract": "We investigate video-aided grammar induction, which learns a constituency\nparser from both unlabeled text and its corresponding video. Existing methods\nof multi-modal grammar induction focus on learning syntactic grammars from\ntext-image pairs, with promising results showing that the information from\nstatic images is useful in induction. However, videos provide even richer\ninformation, including not only static objects but also actions and state\nchanges useful for inducing verb phrases. In this paper, we explore rich\nfeatures (e.g. action, object, scene, audio, face, OCR and speech) from videos,\ntaking the recent Compound PCFG model as the baseline. We further propose a\nMulti-Modal Compound PCFG model (MMC-PCFG) to effectively aggregate these rich\nfeatures from different modalities. Our proposed MMC-PCFG is trained end-to-end\nand outperforms each individual modality and previous state-of-the-art systems\non three benchmarks, i.e. DiDeMo, YouCook2 and MSRVTT, confirming the\neffectiveness of leveraging video information for unsupervised grammar\ninduction.", "published": "2021-04-09 14:01:36", "link": "http://arxiv.org/abs/2104.04369v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Studying Alignment in a Collaborative Learning Activity via Automatic\n  Methods: The Link Between What We Say and Do", "abstract": "A dialogue is successful when there is alignment between the speakers at\ndifferent linguistic levels. In this work, we consider the dialogue occurring\nbetween interlocutors engaged in a collaborative learning task, where they are\nnot only evaluated on how well they performed, but also on how much they\nlearnt. The main contribution of this work is to propose new automatic measures\nto study alignment; focusing on verbal (lexical) alignment, and behavioral\nalignment (when an instruction given by one was followed with concrete actions\nby another). A second contribution of our work is to study how spontaneous\nspeech phenomena are used in the process of alignment. Lastly, we make public\nthe dataset to study alignment in educational dialogues. Our results show that\nall teams verbally and behaviourally align to some degree regardless of their\nperformance and learning, and our measures capture that teams that did not\nsucceed in the task were simply slower to collaborate. Thus we find that teams\nthat performed better, were faster to align. Furthermore, our methodology\ncaptures a productive period that includes the time where the interlocutors\ncame up with their best solutions. We also find that well-performing teams\nverbalise the marker \"oh\" more when they are behaviourally aligned, compared to\nother times in the dialogue; showing that this marker is an important cue in\nalignment. To the best of our knowledge, we are the first to study the role of\n\"oh\" as an information management marker in a behavioral context (i.e. in\nconnection to actions taken in a physical environment), compared to only a\nverbal one. Our measures contribute to the research in the field of educational\ndialogue and the intersection between dialogue and collaborative learning\nresearch.", "published": "2021-04-09 15:26:12", "link": "http://arxiv.org/abs/2104.04429v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Knowledge-Aware Graph-Enhanced GPT-2 for Dialogue State Tracking", "abstract": "Dialogue State Tracking is central to multi-domain task-oriented dialogue\nsystems, responsible for extracting information from user utterances. We\npresent a novel hybrid architecture that augments GPT-2 with representations\nderived from Graph Attention Networks in such a way to allow causal, sequential\nprediction of slot values. The model architecture captures inter-slot\nrelationships and dependencies across domains that otherwise can be lost in\nsequential prediction. We report improvements in state tracking performance in\nMultiWOZ 2.0 against a strong GPT-2 baseline and investigate a simplified\nsparse training scenario in which DST models are trained only on session-level\nannotations but evaluated at the turn level. We further report detailed\nanalyses to demonstrate the effectiveness of graph models in DST by showing\nthat the proposed graph modules capture inter-slot dependencies and improve the\npredictions of values that are common to multiple domains.", "published": "2021-04-09 16:27:34", "link": "http://arxiv.org/abs/2104.04466v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Large-Scale Language Model Training on GPU Clusters Using\n  Megatron-LM", "abstract": "Large language models have led to state-of-the-art accuracies across a range\nof tasks. However, training these models efficiently is challenging for two\nreasons: a) GPU memory capacity is limited, making it impossible to fit large\nmodels on even a multi-GPU server, and b) the number of compute operations\nrequired to train these models can result in unrealistically long training\ntimes. Consequently, new methods of model parallelism such as tensor and\npipeline parallelism have been proposed. Unfortunately, naive usage of these\nmethods leads to fundamental scaling issues at thousands of GPUs, e.g., due to\nexpensive cross-node communication or devices spending significant time waiting\non other devices to make progress.\n  In this paper, we show how different types of parallelism methods (tensor,\npipeline, and data parallelism) can be composed to scale to thousands of GPUs\nand models with trillions of parameters. We survey techniques for pipeline\nparallelism and propose a novel interleaved pipeline parallelism schedule that\ncan improve throughput by 10+% with memory footprint comparable to existing\napproaches. We quantitatively study the trade-offs between tensor, pipeline,\nand data parallelism, and provide intuition as to how to configure distributed\ntraining of a large model. Our approach allows us to perform training\niterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs\nwith achieved per-GPU throughput of 52% of theoretical peak. Our code is open\nsourced at https://github.com/nvidia/megatron-lm.", "published": "2021-04-09 16:43:11", "link": "http://arxiv.org/abs/2104.04473v5", "categories": ["cs.CL", "cs.DC"], "primary_category": "cs.CL"}
{"title": "Chinese Character Decomposition for Neural MT with Multi-Word\n  Expressions", "abstract": "Chinese character decomposition has been used as a feature to enhance Machine\nTranslation (MT) models, combining radicals into character and word level\nmodels. Recent work has investigated ideograph or stroke level embedding.\nHowever, questions remain about different decomposition levels of Chinese\ncharacter representations, radical and strokes, best suited for MT. To\ninvestigate the impact of Chinese decomposition embedding in detail, i.e.,\nradical, stroke, and intermediate levels, and how well these decompositions\nrepresent the meaning of the original character sequences, we carry out\nanalysis with both automated and human evaluation of MT. Furthermore, we\ninvestigate if the combination of decomposed Multiword Expressions (MWEs) can\nenhance the model learning. MWE integration into MT has seen more than a decade\nof exploration. However, decomposed MWEs has not previously been explored.", "published": "2021-04-09 17:28:49", "link": "http://arxiv.org/abs/2104.04497v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AdCOFE: Advanced Contextual Feature Extraction in Conversations for\n  emotion classification", "abstract": "Emotion recognition in conversations is an important step in various virtual\nchat bots which require opinion-based feedback, like in social media threads,\nonline support and many more applications. Current Emotion recognition in\nconversations models face issues like (a) loss of contextual information in\nbetween two dialogues of a conversation, (b) failure to give appropriate\nimportance to significant tokens in each utterance and (c) inability to pass on\nthe emotional information from previous utterances.The proposed model of\nAdvanced Contextual Feature Extraction (AdCOFE) addresses these issues by\nperforming unique feature extraction using knowledge graphs, sentiment lexicons\nand phrases of natural language at all levels (word and position embedding) of\nthe utterances. Experiments on the Emotion recognition in conversations dataset\nshow that AdCOFE is beneficial in capturing emotions in conversations.", "published": "2021-04-09 17:58:19", "link": "http://arxiv.org/abs/2104.04517v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Characterization of Time-variant and Time-invariant Assessment of\n  Suicidality on Reddit using C-SSRS", "abstract": "Suicide is the 10th leading cause of death in the U.S (1999-2019). However,\npredicting when someone will attempt suicide has been nearly impossible. In the\nmodern world, many individuals suffering from mental illness seek emotional\nsupport and advice on well-known and easily-accessible social media platforms\nsuch as Reddit. While prior artificial intelligence research has demonstrated\nthe ability to extract valuable information from social media on suicidal\nthoughts and behaviors, these efforts have not considered both severity and\ntemporality of risk. The insights made possible by access to such data have\nenormous clinical potential - most dramatically envisioned as a trigger to\nemploy timely and targeted interventions (i.e., voluntary and involuntary\npsychiatric hospitalization) to save lives. In this work, we address this\nknowledge gap by developing deep learning algorithms to assess suicide risk in\nterms of severity and temporality from Reddit data based on the Columbia\nSuicide Severity Rating Scale (C-SSRS). In particular, we employ two deep\nlearning approaches: time-variant and time-invariant modeling, for user-level\nsuicide risk assessment, and evaluate their performance against a\nclinician-adjudicated gold standard Reddit corpus annotated based on the\nC-SSRS. Our results suggest that the time-variant approach outperforms the\ntime-invariant method in the assessment of suicide-related ideations and\nsupportive behaviors (AUC:0.78), while the time-invariant model performed\nbetter in predicting suicide-related behaviors and suicide attempt (AUC:0.64).\nThe proposed approach can be integrated with clinical diagnostic interviews for\nimproving suicide risk assessments.", "published": "2021-04-09 01:39:41", "link": "http://arxiv.org/abs/2104.04140v1", "categories": ["cs.SI", "cs.AI", "cs.CL", "H.4; I.2; J.3; J.4"], "primary_category": "cs.SI"}
{"title": "Incorporating External Knowledge to Enhance Tabular Reasoning", "abstract": "Reasoning about tabular information presents unique challenges to modern NLP\napproaches which largely rely on pre-trained contextualized embeddings of text.\nIn this paper, we study these challenges through the problem of tabular natural\nlanguage inference. We propose easy and effective modifications to how\ninformation is presented to a model for this task. We show via systematic\nexperiments that these strategies substantially improve tabular inference\nperformance.", "published": "2021-04-09 08:25:01", "link": "http://arxiv.org/abs/2104.04243v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "On Architectures and Training for Raw Waveform Feature Extraction in ASR", "abstract": "With the success of neural network based modeling in automatic speech\nrecognition (ASR), many studies investigated acoustic modeling and learning of\nfeature extractors directly based on the raw waveform. Recently, one line of\nresearch has focused on unsupervised pre-training of feature extractors on\naudio-only data to improve downstream ASR performance. In this work, we\ninvestigate the usefulness of one of these front-end frameworks, namely\nwav2vec, in a setting without additional untranscribed data for hybrid ASR\nsystems. We compare this framework both to the manually defined standard\nGammatone feature set, as well as to features extracted as part of the acoustic\nmodel of an ASR system trained supervised. We study the benefits of using the\npre-trained feature extractor and explore how to additionally exploit an\nexisting acoustic model trained with different features. Finally, we\nsystematically examine combinations of the described features in order to\nfurther advance the performance.", "published": "2021-04-09 11:04:58", "link": "http://arxiv.org/abs/2104.04298v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Language model fusion for streaming end to end speech recognition", "abstract": "Streaming processing of speech audio is required for many contemporary\npractical speech recognition tasks. Even with the large corpora of manually\ntranscribed speech data available today, it is impossible for such corpora to\ncover adequately the long tail of linguistic content that's important for tasks\nsuch as open-ended dictation and voice search. We seek to address both the\nstreaming and the tail recognition challenges by using a language model (LM)\ntrained on unpaired text data to enhance the end-to-end (E2E) model. We extend\nshallow fusion and cold fusion approaches to streaming Recurrent Neural Network\nTransducer (RNNT), and also propose two new competitive fusion approaches that\nfurther enhance the RNNT architecture. Our results on multiple languages with\nvarying training set sizes show that these fusion methods improve streaming\nRNNT performance through introducing extra linguistic features. Cold fusion\nworks consistently better on streaming RNNT with up to a 8.5% WER improvement.", "published": "2021-04-09 17:14:28", "link": "http://arxiv.org/abs/2104.04487v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Lookup-Table Recurrent Language Models for Long Tail Speech Recognition", "abstract": "We introduce Lookup-Table Language Models (LookupLM), a method for scaling up\nthe size of RNN language models with only a constant increase in the floating\npoint operations, by increasing the expressivity of the embedding table. In\nparticular, we instantiate an (additional) embedding table which embeds the\nprevious n-gram token sequence, rather than a single token. This allows the\nembedding table to be scaled up arbitrarily -- with a commensurate increase in\nperformance -- without changing the token vocabulary. Since embeddings are\nsparsely retrieved from the table via a lookup; increasing the size of the\ntable adds neither extra operations to each forward pass nor extra parameters\nthat need to be stored on limited GPU/TPU memory. We explore scaling n-gram\nembedding tables up to nearly a billion parameters. When trained on a 3-billion\nsentence corpus, we find that LookupLM improves long tail log perplexity by\n2.44 and long tail WER by 23.4% on a downstream speech recognition task over a\nstandard RNN language model baseline, an improvement comparable to a scaling up\nthe baseline by 6.2x the number of floating point operations.", "published": "2021-04-09 18:31:30", "link": "http://arxiv.org/abs/2104.04552v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Text2Chart: A Multi-Staged Chart Generator from Natural Language Text", "abstract": "Generation of scientific visualization from analytical natural language text\nis a challenging task. In this paper, we propose Text2Chart, a multi-staged\nchart generator method. Text2Chart takes natural language text as input and\nproduce visualization as two-dimensional charts. Text2Chart approaches the\nproblem in three stages. Firstly, it identifies the axis elements of a chart\nfrom the given text known as x and y entities. Then it finds a mapping of\nx-entities with its corresponding y-entities. Next, it generates a chart type\nsuitable for the given text: bar, line or pie. Combination of these three\nstages is capable of generating visualization from the given analytical text.\nWe have also constructed a dataset for this problem. Experiments show that\nText2Chart achieves best performances with BERT based encodings with LSTM\nmodels in the first stage to label x and y entities, Random Forest classifier\nfor the mapping stage and fastText embedding with LSTM for the chart type\nprediction. In our experiments, all the stages show satisfactory results and\neffectiveness considering formation of charts from analytical text, achieving a\ncommendable overall performance.", "published": "2021-04-09 19:42:24", "link": "http://arxiv.org/abs/2104.04584v1", "categories": ["cs.CL", "cs.HC", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Probabilistic Box Embeddings for Uncertain Knowledge Graph Reasoning", "abstract": "Knowledge bases often consist of facts which are harvested from a variety of\nsources, many of which are noisy and some of which conflict, resulting in a\nlevel of uncertainty for each triple. Knowledge bases are also often\nincomplete, prompting the use of embedding methods to generalize from known\nfacts, however, existing embedding methods only model triple-level uncertainty,\nand reasoning results lack global consistency. To address these shortcomings,\nwe propose BEUrRE, a novel uncertain knowledge graph embedding method with\ncalibrated probabilistic semantics. BEUrRE models each entity as a box (i.e.\naxis-aligned hyperrectangle) and relations between two entities as affine\ntransforms on the head and tail entity boxes. The geometry of the boxes allows\nfor efficient calculation of intersections and volumes, endowing the model with\ncalibrated probabilistic semantics and facilitating the incorporation of\nrelational constraints. Extensive experiments on two benchmark datasets show\nthat BEUrRE consistently outperforms baselines on confidence prediction and\nfact ranking due to its probabilistic calibration and ability to capture\nhigh-order dependencies among facts.", "published": "2021-04-09 21:01:52", "link": "http://arxiv.org/abs/2104.04597v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Accented Speech Recognition Inspired by Human Perception", "abstract": "While improvements have been made in automatic speech recognition performance\nover the last several years, machines continue to have significantly lower\nperformance on accented speech than humans. In addition, the most significant\nimprovements on accented speech primarily arise by overwhelming the problem\nwith hundreds or even thousands of hours of data. Humans typically require much\nless data to adapt to a new accent. This paper explores methods that are\ninspired by human perception to evaluate possible performance improvements for\nrecognition of accented speech, with a specific focus on recognizing speech\nwith a novel accent relative to that of the training data. Our experiments are\nrun on small, accessible datasets that are available to the research community.\nWe explore four methodologies: pre-exposure to multiple accents, grapheme and\nphoneme-based pronunciations, dropout (to improve generalization to a novel\naccent), and the identification of the layers in the neural network that can\nspecifically be associated with accent modeling. Our results indicate that\nmethods based on human perception are promising in reducing WER and\nunderstanding how accented speech is modeled in neural networks for novel\naccents.", "published": "2021-04-09 22:35:09", "link": "http://arxiv.org/abs/2104.04627v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "WLV-RIT at SemEval-2021 Task 5: A Neural Transformer Framework for\n  Detecting Toxic Spans", "abstract": "In recent years, the widespread use of social media has led to an increase in\nthe generation of toxic and offensive content on online platforms. In response,\nsocial media platforms have worked on developing automatic detection methods\nand employing human moderators to cope with this deluge of offensive content.\nWhile various state-of-the-art statistical models have been applied to detect\ntoxic posts, there are only a few studies that focus on detecting the words or\nexpressions that make a post offensive. This motivates the organization of the\nSemEval-2021 Task 5: Toxic Spans Detection competition, which has provided\nparticipants with a dataset containing toxic spans annotation in English posts.\nIn this paper, we present the WLV-RIT entry for the SemEval-2021 Task 5. Our\nbest performing neural transformer model achieves an $0.68$ F1-Score.\nFurthermore, we develop an open-source framework for multilingual detection of\noffensive spans, i.e., MUDES, based on neural transformers that detect toxic\nspans in texts.", "published": "2021-04-09 22:52:26", "link": "http://arxiv.org/abs/2104.04630v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TransWiC at SemEval-2021 Task 2: Transformer-based Multilingual and\n  Cross-lingual Word-in-Context Disambiguation", "abstract": "Identifying whether a word carries the same meaning or different meaning in\ntwo contexts is an important research area in natural language processing which\nplays a significant role in many applications such as question answering,\ndocument summarisation, information retrieval and information extraction. Most\nof the previous work in this area rely on language-specific resources making it\ndifficult to generalise across languages. Considering this limitation, our\napproach to SemEval-2021 Task 2 is based only on pretrained transformer models\nand does not use any language-specific processing and resources. Despite that,\nour best model achieves 0.90 accuracy for English-English subtask which is very\ncompatible compared to the best result of the subtask; 0.93 accuracy. Our\napproach also achieves satisfactory results in other monolingual and\ncross-lingual language pairs as well.", "published": "2021-04-09 23:06:05", "link": "http://arxiv.org/abs/2104.04632v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KI-BERT: Infusing Knowledge Context for Better Language and Domain\n  Understanding", "abstract": "Contextualized entity representations learned by state-of-the-art\ntransformer-based language models (TLMs) like BERT, GPT, T5, etc., leverage the\nattention mechanism to learn the data context from training data corpus.\nHowever, these models do not use the knowledge context. Knowledge context can\nbe understood as semantics about entities and their relationship with\nneighboring entities in knowledge graphs. We propose a novel and effective\ntechnique to infuse knowledge context from multiple knowledge graphs for\nconceptual and ambiguous entities into TLMs during fine-tuning. It projects\nknowledge graph embeddings in the homogeneous vector-space, introduces new\ntoken-types for entities, aligns entity position ids, and a selective attention\nmechanism. We take BERT as a baseline model and implement the\n\"Knowledge-Infused BERT\" by infusing knowledge context from ConceptNet and\nWordNet, which significantly outperforms BERT and other recent knowledge-aware\nBERT variants like ERNIE, SenseBERT, and BERT_CS over eight different subtasks\nof GLUE benchmark. The KI-BERT-base model even significantly outperforms\nBERT-large for domain-specific tasks like SciTail and academic subsets of QQP,\nQNLI, and MNLI.", "published": "2021-04-09 16:15:31", "link": "http://arxiv.org/abs/2104.08145v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transformers: \"The End of History\" for NLP?", "abstract": "Recent advances in neural architectures, such as the Transformer, coupled\nwith the emergence of large-scale pre-trained models such as BERT, have\nrevolutionized the field of Natural Language Processing (NLP), pushing the\nstate of the art for a number of NLP tasks. A rich family of variations of\nthese models has been proposed, such as RoBERTa, ALBERT, and XLNet, but\nfundamentally, they all remain limited in their ability to model certain kinds\nof information, and they cannot cope with certain information sources, which\nwas easy for pre-existing models. Thus, here we aim to shed light on some\nimportant theoretical limitations of pre-trained BERT-style models that are\ninherent in the general Transformer architecture. First, we demonstrate in\npractice on two general types of tasks -- segmentation and segment labeling --\nand on four datasets that these limitations are indeed harmful and that\naddressing them, even in some very simple and naive ways, can yield sizable\nimprovements over vanilla RoBERTa and XLNet models. Then, we offer a more\ngeneral discussion on desiderata for future additions to the Transformer\narchitecture that would increase its expressiveness, which we hope could help\nin the design of the next generation of deep NLP architectures.", "published": "2021-04-09 08:29:42", "link": "http://arxiv.org/abs/2105.00813v2", "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "What is the ground truth? Reliability of multi-annotator data for audio\n  tagging", "abstract": "Crowdsourcing has become a common approach for annotating large amounts of\ndata. It has the advantage of harnessing a large workforce to produce large\namounts of data in a short time, but comes with the disadvantage of employing\nnon-expert annotators with different backgrounds. This raises the problem of\ndata reliability, in addition to the general question of how to combine the\nopinions of multiple annotators in order to estimate the ground truth. This\npaper presents a study of the annotations and annotators' reliability for audio\ntagging. We adapt the use of Krippendorf's alpha and multi-annotator competence\nestimation (MACE) for a multi-labeled data scenario, and present how MACE can\nbe used to estimate a candidate ground truth based on annotations from\nnon-expert users with different levels of expertise and competence.", "published": "2021-04-09 06:58:46", "link": "http://arxiv.org/abs/2104.04214v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Speaker-conditioned Target Speaker Extraction based on Customized LSTM\n  Cells", "abstract": "Speaker-conditioned target speaker extraction systems rely on auxiliary\ninformation about the target speaker to extract the target speaker signal from\na mixture of multiple speakers. Typically, a deep neural network is applied to\nisolate the relevant target speaker characteristics. In this paper, we focus on\na single-channel target speaker extraction system based on a CNN-LSTM separator\nnetwork and a speaker embedder network requiring reference speech of the target\nspeaker. In the LSTM layer of the separator network, we propose to customize\nthe LSTM cells in order to only remember the specific voice patterns\ncorresponding to the target speaker by modifying the information processing in\nthe forget gate. Experimental results for two-speaker mixtures using the\nLibrispeech dataset show that this customization significantly improves the\ntarget speaker extraction performance compared to using standard LSTM cells.", "published": "2021-04-09 07:59:36", "link": "http://arxiv.org/abs/2104.04234v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Speech based Depression Severity Level Classification Using a\n  Multi-Stage Dilated CNN-LSTM Model", "abstract": "Speech based depression classification has gained immense popularity over the\nrecent years. However, most of the classification studies have focused on\nbinary classification to distinguish depressed subjects from non-depressed\nsubjects. In this paper, we formulate the depression classification task as a\nseverity level classification problem to provide more granularity to the\nclassification outcomes. We use articulatory coordination features (ACFs)\ndeveloped to capture the changes of neuromotor coordination that happens as a\nresult of psychomotor slowing, a necessary feature of Major Depressive\nDisorder. The ACFs derived from the vocal tract variables (TVs) are used to\ntrain a dilated Convolutional Neural Network based depression classification\nmodel to obtain segment-level predictions. Then, we propose a Recurrent Neural\nNetwork based approach to obtain session-level predictions from segment-level\npredictions. We show that strengths of the segment-wise classifier are\namplified when a session-wise classifier is trained on embeddings obtained from\nit. The model trained on ACFs derived from TVs show relative improvement of\n27.47% in Unweighted Average Recall (UAR) at the session-level classification\ntask, compared to the ACFs derived from Mel Frequency Cepstral Coefficients\n(MFCCs).", "published": "2021-04-09 05:10:08", "link": "http://arxiv.org/abs/2104.04195v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "The NTNU Taiwanese ASR System for Formosa Speech Recognition Challenge\n  2020", "abstract": "This paper describes the NTNU ASR system participating in the Formosa Speech\nRecognition Challenge 2020 (FSR-2020) supported by the Formosa Speech in the\nWild project (FSW). FSR-2020 aims at fostering the development of Taiwanese\nspeech recognition. Apart from the issues on tonal and dialectical variations\nof the Taiwanese language, speech artificially contaminated with different\ntypes of real-world noise also has to be dealt with in the final test stage;\nall of these make FSR-2020 much more challenging than before. To work around\nthe under-resourced issue, the main technical aspects of our ASR system include\nvarious deep learning techniques, such as transfer learning, semi-supervised\nlearning, front-end speech enhancement and model ensemble, as well as data\ncleansing and data augmentation conducted on the training data. With the best\nconfiguration, our system obtains 13.1 % syllable error rate (SER) on the\nfinal-test set, achieving the first place among all participating systems on\nTrack 3.", "published": "2021-04-09 07:26:12", "link": "http://arxiv.org/abs/2104.04221v4", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Joint Online Multichannel Acoustic Echo Cancellation, Speech\n  Dereverberation and Source Separation", "abstract": "This paper presents a joint source separation algorithm that simultaneously\nreduces acoustic echo, reverberation and interfering sources. Target speeches\nare separated from the mixture by maximizing independence with respect to the\nother sources. It is shown that the separation process can be decomposed into\ncascading sub-processes that separately relate to acoustic echo cancellation,\nspeech dereverberation and source separation, all of which are solved using the\nauxiliary function based independent component/vector analysis techniques, and\ntheir solving orders are exchangeable. The cascaded solution not only leads to\nlower computational complexity but also better separation performance than the\nvanilla joint algorithm.", "published": "2021-04-09 12:13:38", "link": "http://arxiv.org/abs/2104.04325v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Quality Assessment in Crowdsourcing: Comparison Category Rating\n  Method", "abstract": "Traditionally, Quality of Experience (QoE) for a communication system is\nevaluated through a subjective test. The most common test method for speech QoE\nis the Absolute Category Rating (ACR), in which participants listen to a set of\nstimuli, processed by the underlying test conditions, and rate their perceived\nquality for each stimulus on a specific scale. The Comparison Category Rating\n(CCR) is another standard approach in which participants listen to both\nreference and processed stimuli and rate their quality compared to the other\none. The CCR method is particularly suitable for systems that improve the\nquality of input speech. This paper evaluates an adaptation of the CCR test\nprocedure for assessing speech quality in the crowdsourcing set-up. The CCR\nmethod was introduced in the ITU-T Rec. P.800 for laboratory-based experiments.\nWe adapted the test for the crowdsourcing approach following the guidelines\nfrom ITU-T Rec. P.800 and P.808. We show that the results of the CCR procedure\nvia crowdsourcing are highly reproducible. We also compared the CCR test\nresults with widely used ACR test procedures obtained in the laboratory and\ncrowdsourcing. Our results show that the CCR procedure in crowdsourcing is a\nreliable and valid test method.", "published": "2021-04-09 14:04:06", "link": "http://arxiv.org/abs/2104.04371v1", "categories": ["cs.MM", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Heaps' Law and Vocabulary Richness in the History of Classical Music\n  Harmony", "abstract": "Music is a fundamental human construct, and harmony provides the building\nblocks of musical language. Using the Kunstderfuge corpus of classical music,\nwe analyze the historical evolution of the richness of harmonic vocabulary of\n76 classical composers, covering almost 6 centuries. Such corpus comprises\nabout 9500 pieces, resulting in more than 5 million tokens of music codewords.\nThe fulfilment of Heaps' law for the relation between the size of the harmonic\nvocabulary of a composer (in codeword types) and the total length of his works\n(in codeword tokens), with an exponent around 0.35, allows us to define a\nrelative measure of vocabulary richness that has a transparent interpretation.\nWhen coupled with the considered corpus, this measure allows us to quantify\nharmony richness across centuries, unveiling a clear increasing linear trend.\nIn this way, we are able to rank the composers in terms of richness of\nvocabulary, in the same way as for other related metrics, such as entropy. We\nfind that the latter is particularly highly correlated with our measure of\nrichness. Our approach is not specific for music and can be applied to other\nsystems built by tokens of different types, as for instance natural language.", "published": "2021-04-09 11:13:39", "link": "http://arxiv.org/abs/2104.04143v1", "categories": ["cs.SD", "eess.AS", "physics.soc-ph"], "primary_category": "cs.SD"}
