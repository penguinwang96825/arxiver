{"title": "BiFSMN: Binary Neural Network for Keyword Spotting", "abstract": "The deep neural networks, such as the Deep-FSMN, have been widely studied for\nkeyword spotting (KWS) applications. However, computational resources for these\nnetworks are significantly constrained since they usually run on-call on edge\ndevices. In this paper, we present BiFSMN, an accurate and extreme-efficient\nbinary neural network for KWS. We first construct a High-frequency Enhancement\nDistillation scheme for the binarization-aware training, which emphasizes the\nhigh-frequency information from the full-precision network's representation\nthat is more crucial for the optimization of the binarized network. Then, to\nallow the instant and adaptive accuracy-efficiency trade-offs at runtime, we\nalso propose a Thinnable Binarization Architecture to further liberate the\nacceleration potential of the binarized network from the topology perspective.\nMoreover, we implement a Fast Bitwise Computation Kernel for BiFSMN on ARMv8\ndevices which fully utilizes registers and increases instruction throughput to\npush the limit of deployment efficiency. Extensive experiments show that BiFSMN\noutperforms existing binarization methods by convincing margins on various\ndatasets and is even comparable with the full-precision counterpart (e.g., less\nthan 3% drop on Speech Commands V1-12). We highlight that benefiting from the\nthinnable architecture and the optimized 1-bit implementation, BiFSMN can\nachieve an impressive 22.3x speedup and 15.5x storage-saving on real-world edge\nhardware. Our code is released at https://github.com/htqin/BiFSMN.", "published": "2022-02-14 05:16:53", "link": "http://arxiv.org/abs/2202.06483v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Matching from Different Perspectives", "abstract": "In this paper, we pay attention to the issue which is usually overlooked,\ni.e., \\textit{similarity should be determined from different perspectives}. To\nexplore this issue, we release a Multi-Perspective Text Similarity (MPTS)\ndataset, in which sentence similarities are labeled from twelve perspectives.\nFurthermore, we conduct a series of experimental analysis on this task by\nretrofitting some famous text matching models. Finally, we obtain several\nconclusions and baseline models, laying the foundation for the following\ninvestigation of this issue. The dataset and code are publicly available at\nGithub\\footnote{\\url{https://github.com/autoliuweijie/MPTS}", "published": "2022-02-14 07:18:39", "link": "http://arxiv.org/abs/2202.06517v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FlowEval: A Consensus-Based Dialogue Evaluation Framework Using Segment\n  Act Flows", "abstract": "Despite recent progress in open-domain dialogue evaluation, how to develop\nautomatic metrics remains an open problem. We explore the potential of dialogue\nevaluation featuring dialog act information, which was hardly explicitly\nmodeled in previous methods. However, defined at the utterance level in\ngeneral, dialog act is of coarse granularity, as an utterance can contain\nmultiple segments possessing different functions. Hence, we propose segment\nact, an extension of dialog act from utterance level to segment level, and\ncrowdsource a large-scale dataset for it. To utilize segment act flows,\nsequences of segment acts, for evaluation, we develop the first consensus-based\ndialogue evaluation framework, FlowEval. This framework provides a\nreference-free approach for dialog evaluation by finding pseudo-references.\nExtensive experiments against strong baselines on three benchmark datasets\ndemonstrate the effectiveness and other desirable characteristics of our\nFlowEval, pointing out a potential path for better dialogue evaluation.", "published": "2022-02-14 11:37:20", "link": "http://arxiv.org/abs/2202.06633v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neighborhood Contrastive Learning for Scientific Document\n  Representations with Citation Embeddings", "abstract": "Learning scientific document representations can be substantially improved\nthrough contrastive learning objectives, where the challenge lies in creating\npositive and negative training samples that encode the desired similarity\nsemantics. Prior work relies on discrete citation relations to generate\ncontrast samples. However, discrete citations enforce a hard cut-off to\nsimilarity. This is counter-intuitive to similarity-based learning, and ignores\nthat scientific papers can be very similar despite lacking a direct citation -\na core problem of finding related research. Instead, we use controlled nearest\nneighbor sampling over citation graph embeddings for contrastive learning. This\ncontrol allows us to learn continuous similarity, to sample hard-to-learn\nnegatives and positives, and also to avoid collisions between negative and\npositive samples by controlling the sampling margin between them. The resulting\nmethod SciNCL outperforms the state-of-the-art on the SciDocs benchmark.\nFurthermore, we demonstrate that it can train (or tune) models\nsample-efficiently, and that it can be combined with recent training-efficient\nmethods. Perhaps surprisingly, even training a general-domain language model\nthis way outperforms baselines pretrained in-domain.", "published": "2022-02-14 12:57:37", "link": "http://arxiv.org/abs/2202.06671v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ArgSciChat: A Dataset for Argumentative Dialogues on Scientific Papers", "abstract": "The applications of conversational agents for scientific disciplines (as\nexpert domains) are understudied due to the lack of dialogue data to train such\nagents. While most data collection frameworks, such as Amazon Mechanical Turk,\nfoster data collection for generic domains by connecting crowd workers and task\ndesigners, these frameworks are not much optimized for data collection in\nexpert domains. Scientists are rarely present in these frameworks due to their\nlimited time budget. Therefore, we introduce a novel framework to collect\ndialogues between scientists as domain experts on scientific papers. Our\nframework lets scientists present their scientific papers as groundings for\ndialogues and participate in dialogue they like its paper title. We use our\nframework to collect a novel argumentative dialogue dataset, ArgSciChat. It\nconsists of 498 messages collected from 41 dialogues on 20 scientific papers.\nAlongside extensive analysis on ArgSciChat, we evaluate a recent conversational\nagent on our dataset. Experimental results show that this agent poorly performs\non ArgSciChat, motivating further research on argumentative scientific agents.\nWe release our framework and the dataset.", "published": "2022-02-14 13:27:19", "link": "http://arxiv.org/abs/2202.06690v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exhaustivity and anti-exhaustivity in the RSA framework: Testing the\n  effect of prior beliefs", "abstract": "During communication, the interpretation of utterances is sensitive to a\nlistener's probabilistic prior beliefs, something which is captured by one\ncurrently influential model of pragmatics, the Rational Speech Act (RSA)\nframework. In this paper we focus on cases when this sensitivity to priors\nleads to counterintuitive predictions of the framework. Our domain of interest\nis exhaustivity effects, whereby a sentence such as \"Mary came\" is understood\nto mean that only Mary came. We show that in the baseline RSA model, under\ncertain conditions, anti-exhaustive readings are predicted (e.g., \"Mary came\"\nwould be used to convey that both Mary and Peter came). The specific question\nwe ask is the following: should exhaustive interpretations be derived as purely\npragmatic inferences (as in the classical Gricean view, endorsed in the\nbaseline RSA model), or should they rather be generated by an encapsulated\nsemantic mechanism (as argued in some of the recent formal literature)? To\nanswer this question, we provide a detailed theoretical analysis of different\nRSA models and evaluate them against data obtained in a new study which tested\nthe effects of prior beliefs on both production and comprehension, improving on\nprevious empirical work. We found no anti-exhaustivity effects, but observed\nthat message choice is sensitive to priors, as predicted by the RSA framework\noverall. The best models turn out to be those which include an encapsulated\nexhaustivity mechanism (as other studies concluded on the basis of very\ndifferent data). We conclude that, on the one hand, in the division of labor\nbetween semantics and pragmatics, semantics plays a larger role than is often\nthought, but, on the other hand, the tradeoff between informativity and cost\nwhich characterizes all RSA models does play a central role for genuine\npragmatic effects.", "published": "2022-02-14 20:35:03", "link": "http://arxiv.org/abs/2202.07023v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Matching Tweets With Applicable Fact-Checks Across Languages", "abstract": "An important challenge for news fact-checking is the effective dissemination\nof existing fact-checks. This in turn brings the need for reliable methods to\ndetect previously fact-checked claims. In this paper, we focus on automatically\nfinding existing fact-checks for claims made in social media posts (tweets). We\nconduct both classification and retrieval experiments, in monolingual (English\nonly), multilingual (Spanish, Portuguese), and cross-lingual (Hindi-English)\nsettings using multilingual transformer models such as XLM-RoBERTa and\nmultilingual embeddings such as LaBSE and SBERT. We present promising results\nfor \"match\" classification (86% average accuracy) in four language pairs. We\nalso find that a BM25 baseline outperforms or is on par with state-of-the-art\nmultilingual embedding models for the retrieval task during our monolingual\nexperiments. We highlight and discuss NLP challenges while addressing this\nproblem in different languages, and we introduce a novel curated dataset of\nfact-checks and corresponding tweets for future research.", "published": "2022-02-14 23:33:02", "link": "http://arxiv.org/abs/2202.07094v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Intention, Emotion and External World in Dialogue Systems", "abstract": "Intention, emotion and action are important elements in human activities.\nModeling the interaction process between individuals by analyzing the\nrelationships between these elements is a challenging task. However, previous\nwork mainly focused on modeling intention and emotion independently, and\nneglected of exploring the mutual relationships between intention and emotion.\nIn this paper, we propose a RelAtion Interaction Network (RAIN), consisting of\nIntention Relation Module and Emotion Relation Module, to jointly model mutual\nrelationships and explicitly integrate historical intention information. The\nexperiments on the dataset show that our model can take full advantage of the\nintention, emotion and action between individuals and achieve a remarkable\nimprovement over BERT-style baselines. Qualitative analysis verifies the\nimportance of the mutual interaction between the intention and emotion.", "published": "2022-02-14 04:10:34", "link": "http://arxiv.org/abs/2202.06476v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Source Code Summarization with Structural Relative Position Guided\n  Transformer", "abstract": "Source code summarization aims at generating concise and clear natural\nlanguage descriptions for programming languages. Well-written code summaries\nare beneficial for programmers to participate in the software development and\nmaintenance process. To learn the semantic representations of source code,\nrecent efforts focus on incorporating the syntax structure of code into neural\nnetworks such as Transformer. Such Transformer-based approaches can better\ncapture the long-range dependencies than other neural networks including\nRecurrent Neural Networks (RNNs), however, most of them do not consider the\nstructural relative correlations between tokens, e.g., relative positions in\nAbstract Syntax Trees (ASTs), which is beneficial for code semantics learning.\nTo model the structural dependency, we propose a Structural Relative Position\nguided Transformer, named SCRIPT. SCRIPT first obtains the structural relative\npositions between tokens via parsing the ASTs of source code, and then passes\nthem into two types of Transformer encoders. One Transformer directly adjusts\nthe input according to the structural relative distance; and the other\nTransformer encodes the structural relative positions during computing the\nself-attention scores. Finally, we stack these two types of Transformer\nencoders to learn representations of source code. Experimental results show\nthat the proposed SCRIPT outperforms the state-of-the-art methods by at least\n1.6%, 1.4% and 2.8% with respect to BLEU, ROUGE-L and METEOR on benchmark\ndatasets, respectively. We further show that how the proposed SCRIPT captures\nthe structural relative dependencies.", "published": "2022-02-14 07:34:33", "link": "http://arxiv.org/abs/2202.06521v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "QA4QG: Using Question Answering to Constrain Multi-Hop Question\n  Generation", "abstract": "Multi-hop question generation (MQG) aims to generate complex questions which\nrequire reasoning over multiple pieces of information of the input passage.\nMost existing work on MQG has focused on exploring graph-based networks to\nequip the traditional Sequence-to-sequence framework with reasoning ability.\nHowever, these models do not take full advantage of the constraint between\nquestions and answers. Furthermore, studies on multi-hop question answering\n(QA) suggest that Transformers can replace the graph structure for multi-hop\nreasoning. Therefore, in this work, we propose a novel framework, QA4QG, a\nQA-augmented BART-based framework for MQG. It augments the standard BART model\nwith an additional multi-hop QA module to further constrain the generated\nquestion. Our results on the HotpotQA dataset show that QA4QG outperforms all\nstate-of-the-art models, with an increase of 8 BLEU-4 and 8 ROUGE points\ncompared to the best results previously reported. Our work suggests the\nadvantage of introducing pre-trained language models and QA module for the MQG\ntask.", "published": "2022-02-14 08:16:47", "link": "http://arxiv.org/abs/2202.06538v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "I-Tuning: Tuning Frozen Language Models with Image for Lightweight Image\n  Captioning", "abstract": "Image Captioning is a traditional vision-and-language task that aims to\ngenerate the language description of an image. Recent studies focus on scaling\nup the model size and the number of training data, which significantly increase\nthe cost of model training. Different to these heavy-cost models, we introduce\na lightweight image captioning framework (I-Tuning), which contains a small\nnumber of trainable parameters. We design a novel I-Tuning cross-attention\nmodule to connect the non-trainable pre-trained language decoder GPT2 and\nvision encoder CLIP-ViT. Since most parameters are not required to be updated\nduring training, our framework is lightweight and fast. Experimental results\nconducted on three image captioning benchmarks reveal that our framework\nachieves comparable or better performance than the large-scale baseline\nsystems. But our models contain up to 10 times fewer trainable parameters and\nrequire much fewer data for training compared with state-of-the-art baselines.", "published": "2022-02-14 09:36:50", "link": "http://arxiv.org/abs/2202.06574v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Research on Dual Channel News Headline Classification Based on ERNIE\n  Pre-training Model", "abstract": "The classification of news headlines is an important direction in the field\nof NLP, and its data has the characteristics of compactness, uniqueness and\nvarious forms. Aiming at the problem that the traditional neural network model\ncannot adequately capture the underlying feature information of the data and\ncannot jointly extract key global features and deep local features, a\ndual-channel network model DC-EBAD based on the ERNIE pre-training model is\nproposed. Use ERNIE to extract the lexical, semantic and contextual feature\ninformation at the bottom of the text, generate dynamic word vector\nrepresentations fused with context, and then use the BiLSTM-AT network channel\nto secondary extract the global features of the data and use the attention\nmechanism to give key parts higher The weight of the DPCNN channel is used to\novercome the long-distance text dependence problem and obtain deep local\nfeatures. The local and global feature vectors are spliced, and finally passed\nto the fully connected layer, and the final classification result is output\nthrough Softmax. The experimental results show that the proposed model improves\nthe accuracy, precision and F1-score of news headline classification compared\nwith the traditional neural network model and the single-channel model under\nthe same conditions. It can be seen that it can perform well in the\nmulti-classification application of news headline text under large data volume.", "published": "2022-02-14 10:44:12", "link": "http://arxiv.org/abs/2202.06600v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Out of Thin Air: Is Zero-Shot Cross-Lingual Keyword Detection Better\n  Than Unsupervised?", "abstract": "Keyword extraction is the task of retrieving words that are essential to the\ncontent of a given document. Researchers proposed various approaches to tackle\nthis problem. At the top-most level, approaches are divided into ones that\nrequire training - supervised and ones that do not - unsupervised. In this\nstudy, we are interested in settings, where for a language under investigation,\nno training data is available. More specifically, we explore whether pretrained\nmultilingual language models can be employed for zero-shot cross-lingual\nkeyword extraction on low-resource languages with limited or no available\nlabeled training data and whether they outperform state-of-the-art unsupervised\nkeyword extractors. The comparison is conducted on six news article datasets\ncovering two high-resource languages, English and Russian, and four\nlow-resource languages, Croatian, Estonian, Latvian, and Slovenian. We find\nthat the pretrained models fine-tuned on a multilingual corpus covering\nlanguages that do not appear in the test set (i.e. in a zero-shot setting),\nconsistently outscore unsupervised models in all six languages.", "published": "2022-02-14 12:06:45", "link": "http://arxiv.org/abs/2202.06650v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Aspect Based Sentiment Analysis Using Spectral Temporal Graph Neural\n  Network", "abstract": "The objective of Aspect Based Sentiment Analysis is to capture the sentiment\nof reviewers associated with different aspects. However, complexity of the\nreview sentences, presence of double negation and specific usage of words found\nin different domains make it difficult to predict the sentiment accurately and\noverall a challenging natural language understanding task. While recurrent\nneural network, attention mechanism and more recently, graph attention based\nmodels are prevalent, in this paper we propose graph Fourier transform based\nnetwork with features created in the spectral domain. While this approach has\nfound considerable success in the forecasting domain, it has not been explored\nearlier for any natural language processing task. The method relies on creating\nand learning an underlying graph from the raw data and thereby using the\nadjacency matrix to shift to the graph Fourier domain. Subsequently, Fourier\ntransform is used to switch to the frequency (spectral) domain where new\nfeatures are created. These series of transformation proved to be extremely\nefficient in learning the right representation as we have found that our model\nachieves the best result on both the SemEval-2014 datasets, i.e., \"Laptop\" and\n\"Restaurants\" domain. Our proposed model also found competitive results on the\ntwo other recently proposed datasets from the e-commerce domain.", "published": "2022-02-14 14:55:00", "link": "http://arxiv.org/abs/2202.06776v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sequence-to-Sequence Resources for Catalan", "abstract": "In this work, we introduce sequence-to-sequence language resources for\nCatalan, a moderately under-resourced language, towards two tasks, namely:\nSummarization and Machine Translation (MT). We present two new abstractive\nsummarization datasets in the domain of newswire. We also introduce a parallel\nCatalan-English corpus, paired with three different brand new test sets.\nFinally, we evaluate the data presented with competing state of the art models,\nand we develop baselines for these tasks using a newly created Catalan BART. We\nrelease the resulting resources of this work under open license to encourage\nthe development of language technology in Catalan.", "published": "2022-02-14 16:58:19", "link": "http://arxiv.org/abs/2202.06871v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Introducing the ICBe Dataset: Very High Recall and Precision Event\n  Extraction from Narratives about International Crises", "abstract": "How do international crises unfold? We conceptualize of international\nrelations as a strategic chess game between adversaries and develop a\nsystematic way to measure pieces, moves, and gambits accurately and\nconsistently over a hundred years of history. We introduce a new ontology and\ndataset of international events called ICBe based on a very high-quality corpus\nof narratives from the International Crisis Behavior (ICB) Project. We\ndemonstrate that ICBe has higher coverage, recall, and precision than existing\nstate of the art datasets and conduct two detailed case studies of the Cuban\nMissile Crisis (1962) and Crimea-Donbas Crisis (2014). We further introduce two\nnew event visualizations (event icongraphy and crisis maps), an automated\nbenchmark for measuring event recall using natural language processing\n(sythnetic narratives), and an ontology reconstruction task for objectively\nmeasuring event precision. We make the data, online appendix, replication\nmaterial, and visualizations of every historical episode available at a\ncompanion website www.crisisevents.org and the github repository.", "published": "2022-02-14 23:03:52", "link": "http://arxiv.org/abs/2202.07081v2", "categories": ["stat.AP", "cs.CL"], "primary_category": "stat.AP"}
{"title": "MetaShift: A Dataset of Datasets for Evaluating Contextual Distribution\n  Shifts and Training Conflicts", "abstract": "Understanding the performance of machine learning models across diverse data\ndistributions is critically important for reliable applications. Motivated by\nthis, there is a growing focus on curating benchmark datasets that capture\ndistribution shifts. While valuable, the existing benchmarks are limited in\nthat many of them only contain a small number of shifts and they lack\nsystematic annotation about what is different across different shifts. We\npresent MetaShift--a collection of 12,868 sets of natural images across 410\nclasses--to address this challenge. We leverage the natural heterogeneity of\nVisual Genome and its annotations to construct MetaShift. The key construction\nidea is to cluster images using its metadata, which provides context for each\nimage (e.g. \"cats with cars\" or \"cats in bathroom\") that represent distinct\ndata distributions. MetaShift has two important benefits: first, it contains\norders of magnitude more natural data shifts than previously available. Second,\nit provides explicit explanations of what is unique about each of its data sets\nand a distance score that measures the amount of distribution shift between any\ntwo of its data sets. We demonstrate the utility of MetaShift in benchmarking\nseveral recent proposals for training models to be robust to data shifts. We\nfind that the simple empirical risk minimization performs the best when shifts\nare moderate and no method had a systematic advantage for large shifts. We also\nshow how MetaShift can help to visualize conflicts between data subsets during\nmodel training.", "published": "2022-02-14 07:40:03", "link": "http://arxiv.org/abs/2202.06523v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Deduplicating Training Data Mitigates Privacy Risks in Language Models", "abstract": "Past work has shown that large language models are susceptible to privacy\nattacks, where adversaries generate sequences from a trained model and detect\nwhich sequences are memorized from the training set. In this work, we show that\nthe success of these attacks is largely due to duplication in commonly used\nweb-scraped training sets. We first show that the rate at which language models\nregenerate training sequences is superlinearly related to a sequence's count in\nthe training set. For instance, a sequence that is present 10 times in the\ntraining data is on average generated ~1000 times more often than a sequence\nthat is present only once. We next show that existing methods for detecting\nmemorized sequences have near-chance accuracy on non-duplicated training\nsequences. Finally, we find that after applying methods to deduplicate training\ndata, language models are considerably more secure against these types of\nprivacy attacks. Taken together, our results motivate an increased focus on\ndeduplication in privacy-sensitive applications and a reevaluation of the\npracticality of existing privacy attacks.", "published": "2022-02-14 08:20:15", "link": "http://arxiv.org/abs/2202.06539v3", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Punctuation restoration in Swedish through fine-tuned KB-BERT", "abstract": "Presented here is a method for automatic punctuation restoration in Swedish\nusing a BERT model. The method is based on KB-BERT, a publicly available,\nneural network language model pre-trained on a Swedish corpus by National\nLibrary of Sweden. This model has then been fine-tuned for this specific task\nusing a corpus of government texts. With a lower-case and unpunctuated Swedish\ntext as input, the model is supposed to return a grammatically correct\npunctuated copy of the text as output. A successful solution to this problem\nbrings benefits for an array of NLP domains, such as speech-to-text and\nautomated text. Only the punctuation marks period, comma and question marks\nwere considered for the project, due to a lack of data for more rare marks such\nas semicolon. Additionally, some marks are somewhat interchangeable with the\nmore common, such as exclamation points and periods. Thus, the data set had all\nexclamation points replaced with periods. The fine-tuned Swedish BERT model,\ndubbed prestoBERT, achieved an overall F1-score of 78.9. The proposed model\nscored similarly to international counterparts, with Hungarian and Chinese\nmodels obtaining F1-scores of 82.2 and 75.6 respectively. As further\ncomparison, a human evaluation case study was carried out. The human test group\nachieved an overall F1-score of 81.7, but scored substantially worse than\nprestoBERT on both period and comma. Inspecting output sentences from the model\nand humans show satisfactory results, despite the difference in F1-score. The\ndisconnect seems to stem from an unnecessary focus on replicating the exact\nsame punctuation used in the test set, rather than providing any of the number\nof correct interpretations. If the loss function could be rewritten to reward\nall grammatically correct outputs, rather than only the one original example,\nthe performance could improve significantly for both prestoBERT and the human\ngroup.", "published": "2022-02-14 14:39:40", "link": "http://arxiv.org/abs/2202.06769v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Permutation invariant matrix statistics and computational language tasks", "abstract": "The Linguistic Matrix Theory programme introduced by Kartsaklis, Ramgoolam\nand Sadrzadeh is an approach to the statistics of matrices that are generated\nin type-driven distributional semantics, based on permutation invariant\npolynomial functions which are regarded as the key observables encoding the\nsignificant statistics. In this paper we generalize the previous results on the\napproximate Gaussianity of matrix distributions arising from compositional\ndistributional semantics. We also introduce a geometry of observable vectors\nfor words, defined by exploiting the graph-theoretic basis for the permutation\ninvariants and the statistical characteristics of the ensemble of matrices\nassociated with the words. We describe successful applications of this unified\nframework to a number of tasks in computational linguistics, associated with\nthe distinctions between synonyms, antonyms, hypernyms and hyponyms.", "published": "2022-02-14 16:06:29", "link": "http://arxiv.org/abs/2202.06829v2", "categories": ["cs.CL", "cond-mat.stat-mech", "hep-th"], "primary_category": "cs.CL"}
{"title": "Repairing the Cracked Foundation: A Survey of Obstacles in Evaluation\n  Practices for Generated Text", "abstract": "Evaluation practices in natural language generation (NLG) have many known\nflaws, but improved evaluation approaches are rarely widely adopted. This issue\nhas become more urgent, since neural NLG models have improved to the point\nwhere they can often no longer be distinguished based on the surface-level\nfeatures that older metrics rely on. This paper surveys the issues with human\nand automatic model evaluations and with commonly used datasets in NLG that\nhave been pointed out over the past 20 years. We summarize, categorize, and\ndiscuss how researchers have been addressing these issues and what their\nfindings mean for the current state of model evaluations. Building on those\ninsights, we lay out a long-term vision for NLG evaluation and propose concrete\nsteps for researchers to improve their evaluation processes. Finally, we\nanalyze 66 NLG papers from recent NLP conferences in how well they already\nfollow these suggestions and identify which areas require more drastic changes\nto the status quo.", "published": "2022-02-14 18:51:07", "link": "http://arxiv.org/abs/2202.06935v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transformer Memory as a Differentiable Search Index", "abstract": "In this paper, we demonstrate that information retrieval can be accomplished\nwith a single Transformer, in which all information about the corpus is encoded\nin the parameters of the model. To this end, we introduce the Differentiable\nSearch Index (DSI), a new paradigm that learns a text-to-text model that maps\nstring queries directly to relevant docids; in other words, a DSI model answers\nqueries directly using only its parameters, dramatically simplifying the whole\nretrieval process. We study variations in how documents and their identifiers\nare represented, variations in training procedures, and the interplay between\nmodels and corpus sizes. Experiments demonstrate that given appropriate design\nchoices, DSI significantly outperforms strong baselines such as dual encoder\nmodels. Moreover, DSI demonstrates strong generalization capabilities,\noutperforming a BM25 baseline in a zero-shot setup.", "published": "2022-02-14 19:12:43", "link": "http://arxiv.org/abs/2202.06991v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Regional Differences in Information Privacy Concerns After the\n  Facebook-Cambridge Analytica Data Scandal", "abstract": "While there is increasing global attention to data privacy, most of their\ncurrent theoretical understanding is based on research conducted in a few\ncountries. Prior work argues that people's cultural backgrounds might shape\ntheir privacy concerns; thus, we could expect people from different world\nregions to conceptualize them in diverse ways. We collected and analyzed a\nlarge-scale dataset of tweets about the #CambridgeAnalytica scandal in Spanish\nand English to start exploring this hypothesis. We employed word embeddings and\nqualitative analysis to identify which information privacy concerns are present\nand characterize language and regional differences in emphasis on these\nconcerns. Our results suggest that related concepts, such as regulations, can\nbe added to current information privacy frameworks. We also observe a greater\nemphasis on data collection in English than in Spanish. Additionally, data from\nNorth America exhibits a narrower focus on awareness compared to other regions\nunder study. Our results call for more diverse sources of data and nuanced\nanalysis of data privacy concerns around the globe.", "published": "2022-02-14 22:35:19", "link": "http://arxiv.org/abs/2202.07075v2", "categories": ["cs.SI", "cs.CL", "cs.CY", "K.4; I.2"], "primary_category": "cs.SI"}
{"title": "One Step at a Time: Long-Horizon Vision-and-Language Navigation with\n  Milestones", "abstract": "We study the problem of developing autonomous agents that can follow human\ninstructions to infer and perform a sequence of actions to complete the\nunderlying task. Significant progress has been made in recent years, especially\nfor tasks with short horizons. However, when it comes to long-horizon tasks\nwith extended sequences of actions, an agent can easily ignore some\ninstructions or get stuck in the middle of the long instructions and eventually\nfail the task. To address this challenge, we propose a model-agnostic\nmilestone-based task tracker (M-TRACK) to guide the agent and monitor its\nprogress. Specifically, we propose a milestone builder that tags the\ninstructions with navigation and interaction milestones which the agent needs\nto complete step by step, and a milestone checker that systemically checks the\nagent's progress in its current milestone and determines when to proceed to the\nnext. On the challenging ALFRED dataset, our M-TRACK leads to a notable 33% and\n52% relative improvement in unseen success rate over two competitive base\nmodels.", "published": "2022-02-14 20:46:33", "link": "http://arxiv.org/abs/2202.07028v3", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Multi-Task Deep Residual Echo Suppression with Echo-aware Loss", "abstract": "This paper introduces the NWPU Team's entry to the ICASSP 2022 AEC Challenge.\nWe take a hybrid approach that cascades a linear AEC with a neural post-filter.\nThe former is used to deal with the linear echo components while the latter\nsuppresses the residual non-linear echo components. We use gated convolutional\nF-T-LSTM neural network (GFTNN) as the backbone and shape the post-filter by a\nmulti-task learning (MTL) framework, where a voice activity detection (VAD)\nmodule is adopted as an auxiliary task along with echo suppression, with the\naim to avoid over suppression that may cause speech distortion. Moreover, we\nadopt an echo-aware loss function, where the mean square error (MSE) loss can\nbe optimized particularly for every time-frequency bin (TF-bin) according to\nthe signal-to-echo ratio (SER), leading to further suppression on the echo.\nExtensive ablation study shows that the time delay estimation (TDE) module in\nneural post-filter leads to better perceptual quality, and an adaptive filter\nwith better convergence will bring consistent performance gain for the\npost-filter. Besides, we find that using the linear echo as the input of our\nneural post-filter is a better choice than using the reference signal directly.\nIn the ICASSP 2022 AEC-Challenge, our approach has ranked the 1st place on word\naccuracy (WAcc) (0.817) and the 3rd place on both mean opinion score (MOS)\n(4.502) and the final score (0.864).", "published": "2022-02-14 16:35:04", "link": "http://arxiv.org/abs/2202.06850v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "EMGSE: Acoustic/EMG Fusion for Multimodal Speech Enhancement", "abstract": "Multimodal learning has been proven to be an effective method to improve\nspeech enhancement (SE) performance, especially in challenging situations such\nas low signal-to-noise ratios, speech noise, or unseen noise types. In previous\nstudies, several types of auxiliary data have been used to construct multimodal\nSE systems, such as lip images, electropalatography, or electromagnetic\nmidsagittal articulography. In this paper, we propose a novel EMGSE framework\nfor multimodal SE, which integrates audio and facial electromyography (EMG)\nsignals. Facial EMG is a biological signal containing articulatory movement\ninformation, which can be measured in a non-invasive way. Experimental results\nshow that the proposed EMGSE system can achieve better performance than the\naudio-only SE system. The benefits of fusing EMG signals with acoustic signals\nfor SE are notable under challenging circumstances. Furthermore, this study\nreveals that cheek EMG is sufficient for SE.", "published": "2022-02-14 06:39:13", "link": "http://arxiv.org/abs/2202.06507v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "q-bio.QM"], "primary_category": "eess.AS"}
{"title": "Tight integration of neural- and clustering-based diarization through\n  deep unfolding of infinite Gaussian mixture model", "abstract": "Speaker diarization has been investigated extensively as an important central\ntask for meeting analysis. Recent trend shows that integration of end-to-end\nneural (EEND)-and clustering-based diarization is a promising approach to\nhandle realistic conversational data containing overlapped speech with an\narbitrarily large number of speakers, and achieved state-of-the-art results on\nvarious tasks. However, the approaches proposed so far have not realized {\\it\ntight} integration yet, because the clustering employed therein was not optimal\nin any sense for clustering the speaker embeddings estimated by the EEND\nmodule. To address this problem, this paper introduces a {\\it trainable}\nclustering algorithm into the integration framework, by deep-unfolding a\nnon-parametric Bayesian model called the infinite Gaussian mixture model\n(iGMM). Specifically, the speaker embeddings are optimized during training such\nthat it better fits iGMM clustering, based on a novel clustering loss based on\nAdjusted Rand Index (ARI). Experimental results based on CALLHOME data show\nthat the proposed approach outperforms the conventional approach in terms of\ndiarization error rate (DER), especially by substantially reducing speaker\nconfusion errors, that indeed reflects the effectiveness of the proposed iGMM\nintegration.", "published": "2022-02-14 07:45:21", "link": "http://arxiv.org/abs/2202.06524v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Partially Fake Audio Detection by Self-attention-based Fake Span\n  Discovery", "abstract": "The past few years have witnessed the significant advances of speech\nsynthesis and voice conversion technologies. However, such technologies can\nundermine the robustness of broadly implemented biometric identification models\nand can be harnessed by in-the-wild attackers for illegal uses. The ASVspoof\nchallenge mainly focuses on synthesized audios by advanced speech synthesis and\nvoice conversion models, and replay attacks. Recently, the first Audio Deep\nSynthesis Detection challenge (ADD 2022) extends the attack scenarios into more\naspects. Also ADD 2022 is the first challenge to propose the partially fake\naudio detection task. Such brand new attacks are dangerous and how to tackle\nsuch attacks remains an open question. Thus, we propose a novel framework by\nintroducing the question-answering (fake span discovery) strategy with the\nself-attention mechanism to detect partially fake audios. The proposed fake\nspan detection module tasks the anti-spoofing model to predict the start and\nend positions of the fake clip within the partially fake audio, address the\nmodel's attention into discovering the fake spans rather than other shortcuts\nwith less generalization, and finally equips the model with the discrimination\ncapacity between real and partially fake audios. Our submission ranked second\nin the partially fake audio detection track of ADD 2022.", "published": "2022-02-14 13:20:55", "link": "http://arxiv.org/abs/2202.06684v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Low-latency Monaural Speech Enhancement with Deep Filter-bank Equalizer", "abstract": "It is highly desirable that speech enhancement algorithms can achieve good\nperformance while keeping low latency for many applications, such as digital\nhearing aids, acoustically transparent hearing devices, and public address\nsystems. To improve the performance of traditional low-latency speech\nenhancement algorithms, a deep filter-bank equalizer (FBE) framework was\nproposed, which integrated a deep learning-based subband noise reduction\nnetwork with a deep learning-based shortened digital filter mapping network. In\nthe first network, a deep learning model was trained with a controllable small\nframe shift to satisfy the low-latency demand, i.e., $\\le$ 4 ms, so as to\nobtain (complex) subband gains, which could be regarded as an adaptive digital\nfilter in each frame. In the second network, to reduce the latency, this\nadaptive digital filter was implicitly shortened by a deep learning-based\nframework, and was then applied to noisy speech to reconstruct the enhanced\nspeech without the overlap-add method. Experimental results on the WSJ0-SI84\ncorpus indicated that the proposed deep FBE with only 4-ms latency achieved\nmuch better performance than traditional low-latency speech enhancement\nalgorithms in terms of the indices such as PESQ, STOI, and the amount of noise\nreduction.", "published": "2022-02-14 14:34:12", "link": "http://arxiv.org/abs/2202.06764v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Visual Acoustic Matching", "abstract": "We introduce the visual acoustic matching task, in which an audio clip is\ntransformed to sound like it was recorded in a target environment. Given an\nimage of the target environment and a waveform for the source audio, the goal\nis to re-synthesize the audio to match the target room acoustics as suggested\nby its visible geometry and materials. To address this novel task, we propose a\ncross-modal transformer model that uses audio-visual attention to inject visual\nproperties into the audio and generate realistic audio output. In addition, we\ndevise a self-supervised training objective that can learn acoustic matching\nfrom in-the-wild Web videos, despite their lack of acoustically mismatched\naudio. We demonstrate that our approach successfully translates human speech to\na variety of real-world environments depicted in images, outperforming both\ntraditional acoustic matching and more heavily supervised baselines.", "published": "2022-02-14 17:05:22", "link": "http://arxiv.org/abs/2202.06875v2", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Spiking Cochlea with System-level Local Automatic Gain Control", "abstract": "Including local automatic gain control (AGC) circuitry into a silicon cochlea\ndesign has been challenging because of transistor mismatch and model\ncomplexity. To address this, we present an alternative system-level algorithm\nthat implements channel-specific AGC in a silicon spiking cochlea by measuring\nthe output spike activity of individual channels. The bandpass filter gain of a\nchannel is adapted dynamically to the input amplitude so that the average\noutput spike rate stays within a defined range. Because this AGC mechanism only\nneeds counting and adding operations, it can be implemented at low hardware\ncost in a future design. We evaluate the impact of the local AGC algorithm on a\nclassification task where the input signal varies over 32 dB input range. Two\nclassifier types receiving cochlea spike features were tested on a speech\nversus noise classification task. The logistic regression classifier achieves\nan average of 6% improvement and 40.8% relative improvement in accuracy when\nthe AGC is enabled. The deep neural network classifier shows a similar\nimprovement for the AGC case and achieves a higher mean accuracy of 96%\ncompared to the best accuracy of 91% from the logistic regression classifier.", "published": "2022-02-14 13:58:13", "link": "http://arxiv.org/abs/2202.06707v1", "categories": ["eess.SP", "cs.CV", "cs.LG", "cs.SD", "cs.SY", "eess.AS", "eess.SY"], "primary_category": "eess.SP"}
