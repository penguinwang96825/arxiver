{"title": "Discrete Structural Planning for Neural Machine Translation", "abstract": "Structural planning is important for producing long sentences, which is a\nmissing part in current language generation models. In this work, we add a\nplanning phase in neural machine translation to control the coarse structure of\noutput sentences. The model first generates some planner codes, then predicts\nreal output words conditioned on them. The codes are learned to capture the\ncoarse structure of the target sentence. In order to obtain the codes, we\ndesign an end-to-end neural network with a discretization bottleneck, which\npredicts the simplified part-of-speech tags of target sentences. Experiments\nshow that the translation performance are generally improved by planning ahead.\nWe also find that translations with different structures can be obtained by\nmanipulating the planner codes.", "published": "2018-08-14 05:13:23", "link": "http://arxiv.org/abs/1808.04525v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Primal Meaning Recommendation via On-line Encyclopedia", "abstract": "Polysemy is a very common phenomenon in modern languages. Under many\ncircumstances, there exists a primal meaning for the expression. We define the\nprimal meaning of an expression to be a frequently used sense of that\nexpression from which its other frequent senses can be deduced. Many of the new\nappearing meanings of the expressions are either originated from a primal\nmeaning, or are merely literal references to the original expression, e.g.,\napple (fruit), Apple (Inc), and Apple (movie). When constructing a knowledge\nbase from on-line encyclopedia data, it would be more efficient to be aware of\nthe information about the importance of the senses. In this paper, we would\nlike to explore a way to automatically recommend the primal meaning of an\nexpression based on the textual descriptions of the multiple senses of an\nexpression from on-line encyclopedia websites. We propose a hybrid model that\ncaptures both the pattern of the description and the relationship between\ndifferent descriptions with both weakly supervised and unsupervised models. The\nexperiment results show that our method yields a good result with a P@1\n(precision) score of 83.3 per cent, and a MAP (mean average precision) of 90.5\nper cent, surpassing the UMFS-WE baseline by a big margin (P@1 is 61.1 per cent\nand MAP is 76.3 per cent).", "published": "2018-08-14 12:37:31", "link": "http://arxiv.org/abs/1808.04660v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Neural Networks for Cross-lingual Sequence Tagging", "abstract": "We study cross-lingual sequence tagging with little or no labeled data in the\ntarget language. Adversarial training has previously been shown to be effective\nfor training cross-lingual sentence classifiers. However, it is not clear if\nlanguage-agnostic representations enforced by an adversarial language\ndiscriminator will also enable effective transfer for token-level prediction\ntasks. Therefore, we experiment with different types of adversarial training on\ntwo tasks: dependency parsing and sentence compression. We show that\nadversarial training consistently leads to improved cross-lingual performance\non each task compared to a conventionally trained baseline.", "published": "2018-08-14 15:12:55", "link": "http://arxiv.org/abs/1808.04736v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieve and Refine: Improved Sequence Generation Models For Dialogue", "abstract": "Sequence generation models for dialogue are known to have several problems:\nthey tend to produce short, generic sentences that are uninformative and\nunengaging. Retrieval models on the other hand can surface interesting\nresponses, but are restricted to the given retrieval set leading to erroneous\nreplies that cannot be tuned to the specific context. In this work we develop a\nmodel that combines the two approaches to avoid both their deficiencies: first\nretrieve a response and then refine it -- the final sequence generator treating\nthe retrieval as additional context. We show on the recent CONVAI2 challenge\ntask our approach produces responses superior to both standard retrieval and\ngeneration models in human evaluations.", "published": "2018-08-14 16:13:44", "link": "http://arxiv.org/abs/1808.04776v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Classifier Ensembles for Dialect and Language Variety Identification", "abstract": "In this paper we present ensemble-based systems for dialect and language\nvariety identification using the datasets made available by the organizers of\nthe VarDial Evaluation Campaign 2018. We present a system developed to\ndiscriminate between Flemish and Dutch in subtitles and a system trained to\ndiscriminate between four Arabic dialects: Egyptian, Levantine, Gulf, North\nAfrican, and Modern Standard Arabic in speech broadcasts. Finally, we compare\nthe performance of these two systems with the other systems submitted to the\nDiscriminating between Dutch and Flemish in Subtitles (DFS) and the Arabic\nDialect Identification (ADI) shared tasks at VarDial 2018.", "published": "2018-08-14 17:22:25", "link": "http://arxiv.org/abs/1808.04800v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Jointly Identifying and Fixing Inconsistent Readings from Information\n  Extraction Systems", "abstract": "KGCleaner is a framework to identify and correct errors in data produced and\ndelivered by an information extraction system. These tasks have been\nunderstudied and KGCleaner is the first to address both. We introduce a\nmulti-task model that jointly learns to predict if an extracted relation is\ncredible and repair it if not. We evaluate our approach and other models as\ninstance of our framework on two collections: a Wikidata corpus of nearly 700K\nfacts and 5M fact-relevant sentences and a collection of 30K facts from the\n2015 TAC Knowledge Base Population task. For credibility classification,\nparameter efficient simple shallow neural network can achieve an absolute\nperformance gain of 30 $F_1$ points on Wikidata and comparable performance on\nTAC. For the repair task, significant performance (at more than twice) gain can\nbe obtained depending on the nature of the dataset and the models.", "published": "2018-08-14 17:55:13", "link": "http://arxiv.org/abs/1808.04816v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Two Local Models for Neural Constituent Parsing", "abstract": "Non-local features have been exploited by syntactic parsers for capturing\ndependencies between sub output structures. Such features have been a key to\nthe success of state-of-the-art statistical parsers. With the rise of deep\nlearning, however, it has been shown that local output decisions can give\nhighly competitive accuracies, thanks to the power of dense neural input\nrepresentations that embody global syntactic information. We investigate two\nconceptually simple local neural models for constituent parsing, which make\nlocal decisions to constituent spans and CFG rules, respectively. Consistent\nwith previous findings along the line, our best model gives highly competitive\nresults, achieving the labeled bracketing F1 scores of 92.4% on PTB and 87.3%\non CTB 5.1.", "published": "2018-08-14 18:34:17", "link": "http://arxiv.org/abs/1808.04850v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Top-Down Tree Structured Text Generation", "abstract": "Text generation is a fundamental building block in natural language\nprocessing tasks. Existing sequential models performs autoregression directly\nover the text sequence and have difficulty generating long sentences of complex\nstructures. This paper advocates a simple approach that treats sentence\ngeneration as a tree-generation task. By explicitly modelling syntactic\nstructures in a constituent syntactic tree and performing top-down,\nbreadth-first tree generation, our model fixes dependencies appropriately and\nperforms implicit global planning. This is in contrast to transition-based\ndepth-first generation process, which has difficulty dealing with incomplete\ntexts when parsing and also does not incorporate future contexts in planning.\nOur preliminary results on two generation tasks and one parsing task\ndemonstrate that this is an effective strategy.", "published": "2018-08-14 19:12:44", "link": "http://arxiv.org/abs/1808.04865v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Embedding Grammars", "abstract": "Classic grammars and regular expressions can be used for a variety of\npurposes, including parsing, intent detection, and matching. However, the\ncomparisons are performed at a structural level, with constituent elements\n(words or characters) matched exactly. Recent advances in word embeddings show\nthat semantically related words share common features in a vector-space\nrepresentation, suggesting the possibility of a hybrid grammar and word\nembedding. In this paper, we blend the structure of standard context-free\ngrammars with the semantic generalization capabilities of word embeddings to\ncreate hybrid semantic grammars. These semantic grammars generalize the\nspecific terminals used by the programmer to other words and phrases with\nrelated meanings, allowing the construction of compact grammars that match an\nentire region of the vector space rather than matching specific elements.", "published": "2018-08-14 20:52:43", "link": "http://arxiv.org/abs/1808.04891v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntree2Vec - An algorithm to augment syntactic hierarchy into word\n  embeddings", "abstract": "Word embeddings aims to map sense of the words into a lower dimensional\nvector space in order to reason over them. Training embeddings on domain\nspecific data helps express concepts more relevant to their use case but comes\nat a cost of accuracy when data is less. Our effort is to minimise this by\ninfusing syntactic knowledge into the embeddings. We propose a graph based\nembedding algorithm inspired from node2vec. Experimental results have shown\nthat our algorithm improves the syntactic strength and gives robust performance\non meagre data.", "published": "2018-08-14 14:40:56", "link": "http://arxiv.org/abs/1808.05907v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explaining Queries over Web Tables to Non-Experts", "abstract": "Designing a reliable natural language (NL) interface for querying tables has\nbeen a longtime goal of researchers in both the data management and natural\nlanguage processing (NLP) communities. Such an interface receives as input an\nNL question, translates it into a formal query, executes the query and returns\nthe results. Errors in the translation process are not uncommon, and users\ntypically struggle to understand whether their query has been mapped correctly.\nWe address this problem by explaining the obtained formal queries to non-expert\nusers. Two methods for query explanations are presented: the first translates\nqueries into NL, while the second method provides a graphic representation of\nthe query cell-based provenance (in its execution on a given table). Our\nsolution augments a state-of-the-art NL interface over web tables, enhancing it\nin both its training and deployment phase. Experiments, including a user study\nconducted on Amazon Mechanical Turk, show our solution to improve both the\ncorrectness and reliability of an NL interface.", "published": "2018-08-14 10:23:32", "link": "http://arxiv.org/abs/1808.04614v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "R-grams: Unsupervised Learning of Semantic Units in Natural Language", "abstract": "This paper investigates data-driven segmentation using Re-Pair or Byte Pair\nEncoding-techniques. In contrast to previous work which has primarily been\nfocused on subword units for machine translation, we are interested in the\ngeneral properties of such segments above the word level. We call these\nsegments r-grams, and discuss their properties and the effect they have on the\ntoken frequency distribution. The proposed approach is evaluated by\ndemonstrating its viability in embedding techniques, both in monolingual and\nmultilingual test settings. We also provide a number of qualitative examples of\nthe proposed methodology, demonstrating its viability as a language-invariant\nsegmentation procedure.", "published": "2018-08-14 13:15:43", "link": "http://arxiv.org/abs/1808.04670v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Cross-Platform Rumor Verification Pivoting on Multimedia\n  Content", "abstract": "With the increasing popularity of smart devices, rumors with multimedia\ncontent become more and more common on social networks. The multimedia\ninformation usually makes rumors look more convincing. Therefore, finding an\nautomatic approach to verify rumors with multimedia content is a pressing task.\nPrevious rumor verification research only utilizes multimedia as input\nfeatures. We propose not to use the multimedia content but to find external\ninformation in other news platforms pivoting on it. We introduce a new features\nset, cross-lingual cross-platform features that leverage the semantic\nsimilarity between the rumors and the external information. When implemented,\nmachine learning methods utilizing such features achieved the state-of-the-art\nrumor verification results.", "published": "2018-08-14 22:04:34", "link": "http://arxiv.org/abs/1808.04911v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Improved Language Modeling by Decoding the Past", "abstract": "Highly regularized LSTMs achieve impressive results on several benchmark\ndatasets in language modeling. We propose a new regularization method based on\ndecoding the last token in the context using the predicted distribution of the\nnext token. This biases the model towards retaining more contextual\ninformation, in turn improving its ability to predict the next token. With\nnegligible overhead in the number of parameters and training time, our Past\nDecode Regularization (PDR) method achieves a word level perplexity of 55.6 on\nthe Penn Treebank and 63.5 on the WikiText-2 datasets using a single softmax.\nWe also show gains by using PDR in combination with a mixture-of-softmaxes,\nachieving a word level perplexity of 53.8 and 60.5 on these datasets. In\naddition, our method achieves 1.169 bits-per-character on the Penn Treebank\nCharacter dataset for character level language modeling. These results\nconstitute a new state-of-the-art in their respective settings.", "published": "2018-08-14 18:44:58", "link": "http://arxiv.org/abs/1808.05908v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Text-to-Image-to-Text Translation using Cycle Consistent Adversarial\n  Networks", "abstract": "Text-to-Image translation has been an active area of research in the recent\npast. The ability for a network to learn the meaning of a sentence and generate\nan accurate image that depicts the sentence shows ability of the model to think\nmore like humans. Popular methods on text to image translation make use of\nGenerative Adversarial Networks (GANs) to generate high quality images based on\ntext input, but the generated images don't always reflect the meaning of the\nsentence given to the model as input. We address this issue by using a\ncaptioning network to caption on generated images and exploit the distance\nbetween ground truth captions and generated captions to improve the network\nfurther. We show extensive comparisons between our method and existing methods.", "published": "2018-08-14 05:45:25", "link": "http://arxiv.org/abs/1808.04538v1", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "How Much Reading Does Reading Comprehension Require? A Critical\n  Investigation of Popular Benchmarks", "abstract": "Many recent papers address reading comprehension, where examples consist of\n(question, passage, answer) tuples. Presumably, a model must combine\ninformation from both questions and passages to predict corresponding answers.\nHowever, despite intense interest in the topic, with hundreds of published\npapers vying for leaderboard dominance, basic questions about the difficulty of\nmany popular benchmarks remain unanswered. In this paper, we establish sensible\nbaselines for the bAbI, SQuAD, CBT, CNN, and Who-did-What datasets, finding\nthat question- and passage-only models often perform surprisingly well. On $14$\nout of $20$ bAbI tasks, passage-only models achieve greater than $50\\%$\naccuracy, sometimes matching the full model. Interestingly, while CBT provides\n$20$-sentence stories only the last is needed for comparably accurate\nprediction. By comparison, SQuAD and CNN appear better-constructed.", "published": "2018-08-14 23:59:26", "link": "http://arxiv.org/abs/1808.04926v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
