{"title": "Nonsymbolic Text Representation", "abstract": "We introduce the first generic text representation model that is completely\nnonsymbolic, i.e., it does not require the availability of a segmentation or\ntokenization method that attempts to identify words or other symbolic units in\ntext. This applies to training the parameters of the model on a training corpus\nas well as to applying it when computing the representation of a new text. We\nshow that our model performs better than prior work on an information\nextraction and a text denoising task.", "published": "2016-10-03 10:30:13", "link": "http://arxiv.org/abs/1610.00479v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Semantic Simulations of Linguistically Underspecified Motion\n  Events", "abstract": "In this paper, we describe a system for generating three-dimensional visual\nsimulations of natural language motion expressions. We use a rich formal model\nof events and their participants to generate simulations that satisfy the\nminimal constraints entailed by the associated utterance, relying on semantic\nknowledge of physical objects and motion events. This paper outlines technical\nconsiderations and discusses implementing the aforementioned semantic models\ninto such a system.", "published": "2016-10-03 15:40:08", "link": "http://arxiv.org/abs/1610.00602v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Orthographic Syllable as basic unit for SMT between Related Languages", "abstract": "We explore the use of the orthographic syllable, a variable-length\nconsonant-vowel sequence, as a basic unit of translation between related\nlanguages which use abugida or alphabetic scripts. We show that orthographic\nsyllable level translation significantly outperforms models trained over other\nbasic units (word, morpheme and character) when training over small parallel\ncorpora.", "published": "2016-10-03 16:53:10", "link": "http://arxiv.org/abs/1610.00634v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distributed Representations of Lexical Sets and Prototypes in Causal\n  Alternation Verbs", "abstract": "Lexical sets contain the words filling an argument slot of a verb, and are in\npart determined by selectional preferences. The purpose of this paper is to\nunravel the properties of lexical sets through distributional semantics. We\ninvestigate 1) whether lexical set behave as prototypical categories with a\ncentre and a periphery; 2) whether they are polymorphic, i.e. composed by\nsubcategories; 3) whether the distance between lexical sets of different\narguments is explanatory of verb properties. In particular, our case study are\nlexical sets of causative-inchoative verbs in Italian. Having studied several\nvector models, we find that 1) based on spatial distance from the centroid,\nobject fillers are scattered uniformly across the category, whereas\nintransitive subject fillers lie on its edge; 2) a correlation exists between\nthe amount of verb senses and that of clusters discovered automatically,\nespecially for intransitive subjects; 3) the distance between the centroids of\nobject and intransitive subject is correlated with other properties of verbs,\nsuch as their cross-lingual tendency to appear in the intransitive pattern\nrather than transitive one. This paper is noncommittal with respect to the\nhypothesis that this connection is underpinned by a semantic reason, namely the\nspontaneity of the event denoted by the verb.", "published": "2016-10-03 21:50:27", "link": "http://arxiv.org/abs/1610.00765v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Translate in Real-time with Neural Machine Translation", "abstract": "Translating in real-time, a.k.a. simultaneous translation, outputs\ntranslation words before the input sentence ends, which is a challenging\nproblem for conventional machine translation methods. We propose a neural\nmachine translation (NMT) framework for simultaneous translation in which an\nagent learns to make decisions on when to translate from the interaction with a\npre-trained NMT environment. To trade off quality and delay, we extensively\nexplore various targets for delay and design a method for beam-search\napplicable in the simultaneous MT setting. Experiments against state-of-the-art\nbaselines on two language pairs demonstrate the efficacy of the proposed\nframework both quantitatively and qualitatively.", "published": "2016-10-03 02:11:03", "link": "http://arxiv.org/abs/1610.00388v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Arabic-Hebrew parallel corpus of TED talks", "abstract": "We describe an Arabic-Hebrew parallel corpus of TED talks built upon WIT3,\nthe Web inventory that repurposes the original content of the TED website in a\nway which is more convenient for MT researchers. The benchmark consists of\nabout 2,000 talks, whose subtitles in Arabic and Hebrew have been accurately\naligned and rearranged in sentences, for a total of about 3.5M tokens per\nlanguage. Talks have been partitioned in train, development and test sets\nsimilarly in all respects to the MT tasks of the IWSLT 2016 evaluation\ncampaign. In addition to describing the benchmark, we list the problems\nencountered in preparing it and the novel methods designed to solve them.\nBaseline MT results and some measures on sentence length are provided as an\nextrinsic evaluation of the quality of the benchmark.", "published": "2016-10-03 14:44:58", "link": "http://arxiv.org/abs/1610.00572v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Semi-supervised Learning with Sparse Autoencoders in Phone\n  Classification", "abstract": "We propose the application of a semi-supervised learning method to improve\nthe performance of acoustic modelling for automatic speech recognition based on\ndeep neural net- works. As opposed to unsupervised initialisation followed by\nsupervised fine tuning, our method takes advantage of both unlabelled and\nlabelled data simultaneously through mini- batch stochastic gradient descent.\nWe tested the method with varying proportions of labelled vs unlabelled\nobservations in frame-based phoneme classification on the TIMIT database. Our\nexperiments show that the method outperforms standard supervised training for\nan equal amount of labelled data and provides competitive error rates compared\nto state-of-the-art graph-based semi-supervised learning techniques.", "published": "2016-10-03 12:52:26", "link": "http://arxiv.org/abs/1610.00520v1", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
