{"title": "I-WAS: a Data Augmentation Method with GPT-2 for Simile Detection", "abstract": "Simile detection is a valuable task for many natural language processing\n(NLP)-based applications, particularly in the field of literature. However,\nexisting research on simile detection often relies on corpora that are limited\nin size and do not adequately represent the full range of simile forms. To\naddress this issue, we propose a simile data augmentation method based on\n\\textbf{W}ord replacement And Sentence completion using the GPT-2 language\nmodel. Our iterative process called I-WAS, is designed to improve the quality\nof the augmented sentences. To better evaluate the performance of our method in\nreal-world applications, we have compiled a corpus containing a more diverse\nset of simile forms for experimentation. Our experimental results demonstrate\nthe effectiveness of our proposed data augmentation method for simile\ndetection.", "published": "2023-08-08 07:47:10", "link": "http://arxiv.org/abs/2308.04109v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Collective Human Opinions in Semantic Textual Similarity", "abstract": "Despite the subjective nature of semantic textual similarity (STS) and\npervasive disagreements in STS annotation, existing benchmarks have used\naveraged human ratings as the gold standard. Averaging masks the true\ndistribution of human opinions on examples of low agreement, and prevents\nmodels from capturing the semantic vagueness that the individual ratings\nrepresent. In this work, we introduce USTS, the first Uncertainty-aware STS\ndataset with ~15,000 Chinese sentence pairs and 150,000 labels, to study\ncollective human opinions in STS. Analysis reveals that neither a scalar nor a\nsingle Gaussian fits a set of observed judgements adequately. We further show\nthat current STS models cannot capture the variance caused by human\ndisagreement on individual instances, but rather reflect the predictive\nconfidence over the aggregate dataset.", "published": "2023-08-08 08:00:52", "link": "http://arxiv.org/abs/2308.04114v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Model Prompt Chaining for Long Legal Document\n  Classification", "abstract": "Prompting is used to guide or steer a language model in generating an\nappropriate response that is consistent with the desired outcome. Chaining is a\nstrategy used to decompose complex tasks into smaller, manageable components.\nIn this study, we utilize prompt chaining for extensive legal document\nclassification tasks, which present difficulties due to their intricate\ndomain-specific language and considerable length. Our approach begins with the\ncreation of a concise summary of the original document, followed by a semantic\nsearch for related exemplar texts and their corresponding annotations from a\ntraining corpus. Finally, we prompt for a label - based on the task - to\nassign, by leveraging the in-context learning from the few-shot prompt. We\ndemonstrate that through prompt chaining, we can not only enhance the\nperformance over zero-shot, but also surpass the micro-F1 score achieved by\nlarger models, such as ChatGPT zero-shot, using smaller models.", "published": "2023-08-08 08:57:01", "link": "http://arxiv.org/abs/2308.04138v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Monotonic Aggregation for Open-domain QA", "abstract": "Question answering (QA) is a critical task for speech-based retrieval from\nknowledge sources, by sifting only the answers without requiring to read\nsupporting documents. Specifically, open-domain QA aims to answer user\nquestions on unrestricted knowledge sources. Ideally, adding a source should\nnot decrease the accuracy, but we find this property (denoted as\n\"monotonicity\") does not hold for current state-of-the-art methods. We identify\nthe cause, and based on that we propose Judge-Specialist framework. Our\nframework consists of (1) specialist retrievers/readers to cover individual\nsources, and (2) judge, a dedicated language model to select the final answer.\nOur experiments show that our framework not only ensures monotonicity, but also\noutperforms state-of-the-art multi-source QA methods on Natural Questions.\nAdditionally, we show that our models robustly preserve the monotonicity\nagainst noise from speech recognition. We publicly release our code and\nsetting.", "published": "2023-08-08 10:23:04", "link": "http://arxiv.org/abs/2308.04176v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CLASSLA-Stanza: The Next Step for Linguistic Processing of South Slavic\n  Languages", "abstract": "We present CLASSLA-Stanza, a pipeline for automatic linguistic annotation of\nthe South Slavic languages, which is based on the Stanza natural language\nprocessing pipeline. We describe the main improvements in CLASSLA-Stanza with\nrespect to Stanza, and give a detailed description of the model training\nprocess for the latest 2.1 release of the pipeline. We also report performance\nscores produced by the pipeline for different languages and varieties.\nCLASSLA-Stanza exhibits consistently high performance across all the supported\nlanguages and outperforms or expands its parent pipeline Stanza at all the\nsupported tasks. We also present the pipeline's new functionality enabling\nefficient processing of web data and the reasons that led to its\nimplementation.", "published": "2023-08-08 13:41:41", "link": "http://arxiv.org/abs/2308.04255v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Learning-Based Knowledge Injection for Metaphor Detection: A\n  Comprehensive Review", "abstract": "Metaphor as an advanced cognitive modality works by extracting familiar\nconcepts in the target domain in order to understand vague and abstract\nconcepts in the source domain. This helps humans to quickly understand and\nmaster new domains and thus adapt to changing environments. With the continuous\ndevelopment of metaphor research in the natural language community, many\nstudies using knowledge-assisted models to detect textual metaphors have\nemerged in recent years. Compared to not using knowledge, systems that\nintroduce various kinds of knowledge achieve greater performance gains and\nreach SOTA in a recent study. Based on this, the goal of this paper is to\nprovide a comprehensive review of research advances in the application of deep\nlearning for knowledge injection in metaphor detection tasks. We will first\nsystematically summarize and generalize the mainstream knowledge and knowledge\ninjection principles. Then, the datasets, evaluation metrics, and benchmark\nmodels used in metaphor detection tasks are examined. Finally, we explore the\ncurrent issues facing knowledge injection methods and provide an outlook on\nfuture research directions.", "published": "2023-08-08 14:51:16", "link": "http://arxiv.org/abs/2308.04306v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Evaluation Models from Large Language Models for Sequence\n  Generation", "abstract": "Automatic evaluation of sequence generation, traditionally reliant on metrics\nlike BLEU and ROUGE, often fails to capture the semantic accuracy of generated\ntext sequences due to their emphasis on n-gram overlap. A promising solution to\nthis problem is to develop model-based metrics, such as BLEURT and COMET.\nHowever, these approaches are typically hindered by the scarcity of labeled\nevaluation data, which is necessary to train the evaluation models. In this\nwork, we build upon this challenge by proposing the Customized Sequence\nEvaluation Metric (CSEM), a three-stage evaluation model training method that\nutilizes large language models to generate labeled data for model-based metric\ndevelopment, thereby eliminating the need for human-labeled data. Additionally,\nwe expand the scope of CSEM to support various evaluation types, including\nsingle-aspect, multi-aspect, reference-free, and reference-based evaluations,\nenabling the customization of metrics to suit diverse real-world scenarios.\nExperimental results on the SummEval benchmark demonstrate that CSEM can\neffectively train an evaluation model without human-labeled data. Further\nexperiments in reinforcement learning and reranking show that metrics developed\nthrough CSEM outperform traditional evaluation metrics, leading to substantial\nimprovements in sequence quality as evaluated by both commonly used metrics and\nChatGPT.", "published": "2023-08-08 16:41:16", "link": "http://arxiv.org/abs/2308.04386v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Character-level NMT and language similarity", "abstract": "We explore the effectiveness of character-level neural machine translation\nusing Transformer architecture for various levels of language similarity and\nsize of the training dataset on translation between Czech and Croatian, German,\nHungarian, Slovak, and Spanish. We evaluate the models using automatic MT\nmetrics and show that translation between similar languages benefits from\ncharacter-level input segmentation, while for less related languages,\ncharacter-level vanilla Transformer-base often lags behind subword-level\nsegmentation. We confirm previous findings that it is possible to close the gap\nby finetuning the already trained subword-level models to character-level.", "published": "2023-08-08 17:01:42", "link": "http://arxiv.org/abs/2308.04398v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Bi-directional Multi-hop Inference Model for Joint Dialog Sentiment\n  Classification and Act Recognition", "abstract": "The joint task of Dialog Sentiment Classification (DSC) and Act Recognition\n(DAR) aims to predict the sentiment label and act label for each utterance in a\ndialog simultaneously. However, current methods encode the dialog context in\nonly one direction, which limits their ability to thoroughly comprehend the\ncontext. Moreover, these methods overlook the explicit correlations between\nsentiment and act labels, which leads to an insufficient ability to capture\nrich sentiment and act clues and hinders effective and accurate reasoning. To\naddress these issues, we propose a Bi-directional Multi-hop Inference Model\n(BMIM) that leverages a feature selection network and a bi-directional\nmulti-hop inference network to iteratively extract and integrate rich sentiment\nand act clues in a bi-directional manner. We also employ contrastive learning\nand dual learning to explicitly model the correlations of sentiment and act\nlabels. Our experiments on two widely-used datasets show that BMIM outperforms\nstate-of-the-art baselines by at least 2.6% on F1 score in DAR and 1.4% on F1\nscore in DSC. Additionally, Our proposed model not only improves the\nperformance but also enhances the interpretability of the joint sentiment and\nact prediction task.", "published": "2023-08-08 17:53:24", "link": "http://arxiv.org/abs/2308.04424v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DialogRE^C+: An Extension of DialogRE to Investigate How Much\n  Coreference Helps Relation Extraction in Dialogs", "abstract": "Dialogue relation extraction (DRE) that identifies the relations between\nargument pairs in dialogue text, suffers much from the frequent occurrence of\npersonal pronouns, or entity and speaker coreference. This work introduces a\nnew benchmark dataset DialogRE^C+, introducing coreference resolution into the\nDRE scenario. With the aid of high-quality coreference knowledge, the reasoning\nof argument relations is expected to be enhanced. In DialogRE^C+ dataset, we\nmanually annotate total 5,068 coreference chains over 36,369 argument mentions\nbased on the existing DialogRE data, where four different coreference chain\ntypes namely speaker chain, person chain, location chain and organization chain\nare explicitly marked. We further develop 4 coreference-enhanced graph-based\nDRE models, which learn effective coreference representations for improving the\nDRE task. We also train a coreference resolution model based on our annotations\nand evaluate the effect of automatically extracted coreference chains\ndemonstrating the practicality of our dataset and its potential to other\ndomains and tasks.", "published": "2023-08-08 18:03:29", "link": "http://arxiv.org/abs/2308.04498v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Disentanglement and Fusion on Modality and Context in\n  Conversational Multimodal Emotion Recognition", "abstract": "It has been a hot research topic to enable machines to understand human\nemotions in multimodal contexts under dialogue scenarios, which is tasked with\nmultimodal emotion analysis in conversation (MM-ERC). MM-ERC has received\nconsistent attention in recent years, where a diverse range of methods has been\nproposed for securing better task performance. Most existing works treat MM-ERC\nas a standard multimodal classification problem and perform multimodal feature\ndisentanglement and fusion for maximizing feature utility. Yet after revisiting\nthe characteristic of MM-ERC, we argue that both the feature multimodality and\nconversational contextualization should be properly modeled simultaneously\nduring the feature disentanglement and fusion steps. In this work, we target\nfurther pushing the task performance by taking full consideration of the above\ninsights. On the one hand, during feature disentanglement, based on the\ncontrastive learning technique, we devise a Dual-level Disentanglement\nMechanism (DDM) to decouple the features into both the modality space and\nutterance space. On the other hand, during the feature fusion stage, we propose\na Contribution-aware Fusion Mechanism (CFM) and a Context Refusion Mechanism\n(CRM) for multimodal and context integration, respectively. They together\nschedule the proper integrations of multimodal and context features.\nSpecifically, CFM explicitly manages the multimodal feature contributions\ndynamically, while CRM flexibly coordinates the introduction of dialogue\ncontexts. On two public MM-ERC datasets, our system achieves new\nstate-of-the-art performance consistently. Further analyses demonstrate that\nall our proposed mechanisms greatly facilitate the MM-ERC task by making full\nuse of the multimodal and context features adaptively. Note that our proposed\nmethods have the great potential to facilitate a broader range of other\nconversational multimodal tasks.", "published": "2023-08-08 18:11:27", "link": "http://arxiv.org/abs/2308.04502v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ahead of the Text: Leveraging Entity Preposition for Financial Relation\n  Extraction", "abstract": "In the context of the ACM KDF-SIGIR 2023 competition, we undertook an entity\nrelation task on a dataset of financial entity relations called REFind. Our\ntop-performing solution involved a multi-step approach. Initially, we inserted\nthe provided entities at their corresponding locations within the text.\nSubsequently, we fine-tuned the transformer-based language model roberta-large\nfor text classification by utilizing a labeled training set to predict the\nentity relations. Lastly, we implemented a post-processing phase to identify\nand handle improbable predictions generated by the model. As a result of our\nmethodology, we achieved the 1st place ranking on the competition's public\nleaderboard.", "published": "2023-08-08 18:56:52", "link": "http://arxiv.org/abs/2308.04534v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Single-Sentence Reader: A Novel Approach for Addressing Answer Position\n  Bias", "abstract": "Machine Reading Comprehension (MRC) models tend to take advantage of spurious\ncorrelations (also known as dataset bias or annotation artifacts in the\nresearch community). Consequently, these models may perform the MRC task\nwithout fully comprehending the given context and question, which is\nundesirable since it may result in low robustness against distribution shift.\nThe main focus of this paper is answer-position bias, where a significant\npercentage of training questions have answers located solely in the first\nsentence of the context. We propose a Single-Sentence Reader as a new approach\nfor addressing answer position bias in MRC. Remarkably, in our experiments with\nsix different models, our proposed Single-Sentence Readers trained on biased\ndataset achieve results that nearly match those of models trained on normal\ndataset, proving their effectiveness in addressing the answer position bias.\nOur study also discusses several challenges our Single-Sentence Readers\nencounter and proposes a potential solution.", "published": "2023-08-08 20:29:13", "link": "http://arxiv.org/abs/2308.04566v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative\n  AI Tool", "abstract": "Large Language Model (LLM) based Generative AI systems have seen significant\nprogress in recent years. Integrating a knowledge retrieval architecture allows\nfor seamless integration of private data into publicly available Generative AI\nsystems using pre-trained LLM without requiring additional model fine-tuning.\nMoreover, Retrieval-Centric Generation (RCG) approach, a promising future\nresearch direction that explicitly separates roles of LLMs and retrievers in\ncontext interpretation and knowledge memorization, potentially leads to more\nefficient implementation. SimplyRetrieve is an open-source tool with the goal\nof providing a localized, lightweight, and user-friendly interface to these\nsophisticated advancements to the machine learning community. SimplyRetrieve\nfeatures a GUI and API based RCG platform, assisted by a Private Knowledge Base\nConstructor and a Retrieval Tuning Module. By leveraging these capabilities,\nusers can explore the potential of RCG for improving generative AI performance\nwhile maintaining privacy standards. The tool is available at\nhttps://github.com/RCGAI/SimplyRetrieve with an MIT license.", "published": "2023-08-08 02:00:43", "link": "http://arxiv.org/abs/2308.03983v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Continual Pre-Training of Large Language Models: How to (re)warm your\n  model?", "abstract": "Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to restart the process over again once new data becomes available. A much\ncheaper and more efficient solution would be to enable the continual\npre-training of these models, i.e. updating pre-trained models with new data\ninstead of re-training them from scratch. However, the distribution shift\ninduced by novel data typically results in degraded performance on past data.\nTaking a step towards efficient continual pre-training, in this work, we\nexamine the effect of different warm-up strategies. Our hypothesis is that the\nlearning rate must be re-increased to improve compute efficiency when training\non a new dataset. We study the warmup phase of models pre-trained on the Pile\n(upstream data, 300B tokens) as we continue to pre-train on SlimPajama\n(downstream data, 297B tokens), following a linear warmup and cosine decay\nschedule. We conduct all experiments on the Pythia 410M language model\narchitecture and evaluate performance through validation perplexity. We\nexperiment with different pre-training checkpoints, various maximum learning\nrates, and various warmup lengths. Our results show that while rewarming models\nfirst increases the loss on upstream and downstream data, in the longer run it\nimproves the downstream performance, outperforming models trained from\nscratch$\\unicode{x2013}$even for a large downstream dataset.", "published": "2023-08-08 03:18:18", "link": "http://arxiv.org/abs/2308.04014v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Comparative Study on TF-IDF feature Weighting Method and its Analysis\n  using Unstructured Dataset", "abstract": "Text Classification is the process of categorizing text into the relevant\ncategories and its algorithms are at the core of many Natural Language\nProcessing (NLP). Term Frequency-Inverse Document Frequency (TF-IDF) and NLP\nare the most highly used information retrieval methods in text classification.\nWe have investigated and analyzed the feature weighting method for text\nclassification on unstructured data. The proposed model considered two features\nN-Grams and TF-IDF on the IMDB movie reviews and Amazon Alexa reviews dataset\nfor sentiment analysis. Then we have used the state-of-the-art classifier to\nvalidate the method i.e., Support Vector Machine (SVM), Logistic Regression,\nMultinomial Naive Bayes (Multinomial NB), Random Forest, Decision Tree, and\nk-nearest neighbors (KNN). From those two feature extractions, a significant\nincrease in feature extraction with TF-IDF features rather than based on\nN-Gram. TF-IDF got the maximum accuracy (93.81%), precision (94.20%), recall\n(93.81%), and F1-score (91.99%) value in Random Forest classifier.", "published": "2023-08-08 04:27:34", "link": "http://arxiv.org/abs/2308.04037v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "InfeRE: Step-by-Step Regex Generation via Chain of Inference", "abstract": "Automatically generating regular expressions (abbrev. regexes) from natural\nlanguage description (NL2RE) has been an emerging research area. Prior studies\ntreat regex as a linear sequence of tokens and generate the final expressions\nautoregressively in a single pass. They did not take into account the\nstep-by-step internal text-matching processes behind the final results. This\nsignificantly hinders the efficacy and interpretability of regex generation by\nneural language models. In this paper, we propose a new paradigm called InfeRE,\nwhich decomposes the generation of regexes into chains of step-by-step\ninference. To enhance the robustness, we introduce a self-consistency decoding\nmechanism that ensembles multiple outputs sampled from different models. We\nevaluate InfeRE on two publicly available datasets, NL-RX-Turk and KB13, and\ncompare the results with state-of-the-art approaches and the popular tree-based\ngeneration approach TRANX. Experimental results show that InfeRE substantially\noutperforms previous baselines, yielding 16.3% and 14.7% improvement in DFA@5\naccuracy on two datasets, respectively. Particularly, InfeRE outperforms the\npopular tree-based generation approach by 18.1% and 11.3% on both datasets,\nrespectively, in terms of DFA@5 accuracy.", "published": "2023-08-08 04:37:41", "link": "http://arxiv.org/abs/2308.04041v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "DataTales: Investigating the use of Large Language Models for Authoring\n  Data-Driven Articles", "abstract": "Authoring data-driven articles is a complex process requiring authors to not\nonly analyze data for insights but also craft a cohesive narrative that\neffectively communicates the insights. Text generation capabilities of\ncontemporary large language models (LLMs) present an opportunity to assist the\nauthoring of data-driven articles and expedite the writing process. In this\nwork, we investigate the feasibility and perceived value of leveraging LLMs to\nsupport authors of data-driven articles. We designed a prototype system,\nDataTales, that leverages a LLM to generate textual narratives accompanying a\ngiven chart. Using DataTales as a design probe, we conducted a qualitative\nstudy with 11 professionals to evaluate the concept, from which we distilled\naffordances and opportunities to further integrate LLMs as valuable data-driven\narticle authoring assistants.", "published": "2023-08-08 06:21:58", "link": "http://arxiv.org/abs/2308.04076v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Social Media, Topic Modeling and Sentiment Analysis in Municipal\n  Decision Support", "abstract": "Many cities around the world are aspiring to become. However, smart\ninitiatives often give little weight to the opinions of average citizens.\n  Social media are one of the most important sources of citizen opinions. This\npaper presents a prototype of a framework for processing social media posts\nwith municipal decision-making in mind. The framework consists of a sequence of\nthree steps: (1) determining the sentiment polarity of each social media post\n(2) identifying prevalent topics and mapping these topics to individual posts,\nand (3) aggregating these two pieces of information into a fuzzy number\nrepresenting the overall sentiment expressed towards each topic. Optionally,\nthe fuzzy number can be reduced into a tuple of two real numbers indicating the\n\"amount\" of positive and negative opinion expressed towards each topic.\n  The framework is demonstrated on tweets published from Ostrava, Czechia over\na period of about two months. This application illustrates how fuzzy numbers\nrepresent sentiment in a richer way and capture the diversity of opinions\nexpressed on social media.", "published": "2023-08-08 08:27:57", "link": "http://arxiv.org/abs/2308.04124v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Studying Socially Unacceptable Discourse Classification (SUD) through\n  different eyes: \"Are we on the same page ?\"", "abstract": "We study Socially Unacceptable Discourse (SUD) characterization and detection\nin online text. We first build and present a novel corpus that contains a large\nvariety of manually annotated texts from different online sources used so far\nin state-of-the-art Machine learning (ML) SUD detection solutions. This global\ncontext allows us to test the generalization ability of SUD classifiers that\nacquire knowledge around the same SUD categories, but from different contexts.\nFrom this perspective, we can analyze how (possibly) different annotation\nmodalities influence SUD learning by discussing open challenges and open\nresearch directions. We also provide several data insights which can support\ndomain experts in the annotation task.", "published": "2023-08-08 10:42:33", "link": "http://arxiv.org/abs/2308.04180v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Gloss Alignment Using Word Embeddings", "abstract": "Capturing and annotating Sign language datasets is a time consuming and\ncostly process. Current datasets are orders of magnitude too small to\nsuccessfully train unconstrained \\acf{slt} models. As a result, research has\nturned to TV broadcast content as a source of large-scale training data,\nconsisting of both the sign language interpreter and the associated audio\nsubtitle. However, lack of sign language annotation limits the usability of\nthis data and has led to the development of automatic annotation techniques\nsuch as sign spotting. These spottings are aligned to the video rather than the\nsubtitle, which often results in a misalignment between the subtitle and\nspotted signs. In this paper we propose a method for aligning spottings with\ntheir corresponding subtitles using large spoken language models. Using a\nsingle modality means our method is computationally inexpensive and can be\nutilized in conjunction with existing alignment techniques. We quantitatively\ndemonstrate the effectiveness of our method on the \\acf{mdgs} and \\acf{bobsl}\ndatasets, recovering up to a 33.22 BLEU-1 score in word alignment.", "published": "2023-08-08 13:26:53", "link": "http://arxiv.org/abs/2308.04248v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unmasking Nationality Bias: A Study of Human Perception of Nationalities\n  in AI-Generated Articles", "abstract": "We investigate the potential for nationality biases in natural language\nprocessing (NLP) models using human evaluation methods. Biased NLP models can\nperpetuate stereotypes and lead to algorithmic discrimination, posing a\nsignificant challenge to the fairness and justice of AI systems. Our study\nemploys a two-step mixed-methods approach that includes both quantitative and\nqualitative analysis to identify and understand the impact of nationality bias\nin a text generation model. Through our human-centered quantitative analysis,\nwe measure the extent of nationality bias in articles generated by AI sources.\nWe then conduct open-ended interviews with participants, performing qualitative\ncoding and thematic analysis to understand the implications of these biases on\nhuman readers. Our findings reveal that biased NLP models tend to replicate and\namplify existing societal biases, which can translate to harm if used in a\nsociotechnical setting. The qualitative analysis from our interviews offers\ninsights into the experience readers have when encountering such articles,\nhighlighting the potential to shift a reader's perception of a country. These\nfindings emphasize the critical role of public perception in shaping AI's\nimpact on society and the need to correct biases in AI systems.", "published": "2023-08-08 15:46:27", "link": "http://arxiv.org/abs/2308.04346v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Shepherd: A Critic for Language Model Generation", "abstract": "As large language models improve, there is increasing interest in techniques\nthat leverage these models' capabilities to refine their own outputs. In this\nwork, we introduce Shepherd, a language model specifically tuned to critique\nresponses and suggest refinements, extending beyond the capabilities of an\nuntuned model to identify diverse errors and provide suggestions to remedy\nthem. At the core of our approach is a high quality feedback dataset, which we\ncurate from community feedback and human annotations. Even though Shepherd is\nsmall (7B parameters), its critiques are either equivalent or preferred to\nthose from established models including ChatGPT. Using GPT-4 for evaluation,\nShepherd reaches an average win-rate of 53-87% compared to competitive\nalternatives. In human evaluation, Shepherd strictly outperforms other models\nand on average closely ties with ChatGPT.", "published": "2023-08-08 21:23:23", "link": "http://arxiv.org/abs/2308.04592v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Accelerating LLM Inference with Staged Speculative Decoding", "abstract": "Recent advances with large language models (LLM) illustrate their diverse\ncapabilities. We propose a novel algorithm, staged speculative decoding, to\naccelerate LLM inference in small-batch, on-device scenarios. We address the\nlow arithmetic intensity of small-batch inference by improving upon previous\nwork in speculative decoding. First, we restructure the speculative batch as a\ntree, which reduces generation costs and increases the expected tokens per\nbatch. Second, we add a second stage of speculative decoding. Taken together,\nwe reduce single-batch decoding latency by 3.16x with a 762M parameter GPT-2-L\nmodel while perfectly preserving output quality.", "published": "2023-08-08 23:29:55", "link": "http://arxiv.org/abs/2308.04623v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Benchmarking LLM powered Chatbots: Methods and Metrics", "abstract": "Autonomous conversational agents, i.e. chatbots, are becoming an increasingly\ncommon mechanism for enterprises to provide support to customers and partners.\nIn order to rate chatbots, especially ones powered by Generative AI tools like\nLarge Language Models (LLMs) we need to be able to accurately assess their\nperformance. This is where chatbot benchmarking becomes important. In this\npaper, we propose the use of a novel benchmark that we call the E2E (End to\nEnd) benchmark, and show how the E2E benchmark can be used to evaluate accuracy\nand usefulness of the answers provided by chatbots, especially ones powered by\nLLMs. We evaluate an example chatbot at different levels of sophistication\nbased on both our E2E benchmark, as well as other available metrics commonly\nused in the state of art, and observe that the proposed benchmark show better\nresults compared to others. In addition, while some metrics proved to be\nunpredictable, the metric associated with the E2E benchmark, which uses cosine\nsimilarity performed well in evaluating chatbots. The performance of our best\nmodels shows that there are several benefits of using the cosine similarity\nscore as a metric in the E2E benchmark.", "published": "2023-08-08 23:30:20", "link": "http://arxiv.org/abs/2308.04624v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Comparative Study of Sentence Embedding Models for Assessing Semantic\n  Variation", "abstract": "Analyzing the pattern of semantic variation in long real-world texts such as\nbooks or transcripts is interesting from the stylistic, cognitive, and\nlinguistic perspectives. It is also useful for applications such as text\nsegmentation, document summarization, and detection of semantic novelty. The\nrecent emergence of several vector-space methods for sentence embedding has\nmade such analysis feasible. However, this raises the issue of how consistent\nand meaningful the semantic representations produced by various methods are in\nthemselves. In this paper, we compare several recent sentence embedding methods\nvia time-series of semantic similarity between successive sentences and\nmatrices of pairwise sentence similarity for multiple books of literature. In\ncontrast to previous work using target tasks and curated datasets to compare\nsentence embedding methods, our approach provides an evaluation of the methods\n'in the wild'. We find that most of the sentence embedding methods considered\ndo infer highly correlated patterns of semantic similarity in a given document,\nbut show interesting differences.", "published": "2023-08-08 23:31:10", "link": "http://arxiv.org/abs/2308.04625v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Top K Relevant Passage Retrieval for Biomedical Question Answering", "abstract": "Question answering is a task that answers factoid questions using a large\ncollection of documents. It aims to provide precise answers in response to the\nuser's questions in natural language. Question answering relies on efficient\npassage retrieval to select candidate contexts, where traditional sparse vector\nspace models, such as TF-IDF or BM25, are the de facto method. On the web,\nthere is no single article that could provide all the possible answers\navailable on the internet to the question of the problem asked by the user. The\nexisting Dense Passage Retrieval model has been trained on Wikipedia dump from\nDec. 20, 2018, as the source documents for answering questions. Question\nanswering (QA) has made big strides with several open-domain and machine\ncomprehension systems built using large-scale annotated datasets. However, in\nthe clinical domain, this problem remains relatively unexplored. According to\nmultiple surveys, Biomedical Questions cannot be answered correctly from\nWikipedia Articles. In this work, we work on the existing DPR framework for the\nbiomedical domain and retrieve answers from the Pubmed articles which is a\nreliable source to answer medical questions. When evaluated on a BioASQ QA\ndataset, our fine-tuned dense retriever results in a 0.81 F1 score.", "published": "2023-08-08 04:06:11", "link": "http://arxiv.org/abs/2308.04028v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Five-Dollar Model: Generating Game Maps and Sprites from Sentence\n  Embeddings", "abstract": "The five-dollar model is a lightweight text-to-image generative architecture\nthat generates low dimensional images from an encoded text prompt. This model\ncan successfully generate accurate and aesthetically pleasing content in low\ndimensional domains, with limited amounts of training data. Despite the small\nsize of both the model and datasets, the generated images are still able to\nmaintain the encoded semantic meaning of the textual prompt. We apply this\nmodel to three small datasets: pixel art video game maps, video game sprite\nimages, and down-scaled emoji images and apply novel augmentation strategies to\nimprove the performance of our model on these limited datasets. We evaluate our\nmodels performance using cosine similarity score between text-image pairs\ngenerated by the CLIP VIT-B/32 model.", "published": "2023-08-08 05:16:51", "link": "http://arxiv.org/abs/2308.04052v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Hybrid-RACA: Hybrid Retrieval-Augmented Composition Assistance for\n  Real-time Text Prediction", "abstract": "Large language models (LLMs) enhanced with retrieval augmentation has shown\ngreat performance in many applications. However, the computational demands for\nthese models pose a challenge when applying them to real-time tasks, such as\ncomposition assistance. To address this, we propose Hybrid Retrieval-Augmented\nComposition Assistance (Hybrid-RACA), a novel system for real-time text\nprediction that efficiently combines a cloud-based LLM with a smaller\nclient-side model through retrieval augmented memory. This integration enables\nthe client model to generate better responses, benefiting from the LLM's\ncapabilities and cloud-based data. Meanwhile, via a novel asynchronous memory\nupdate mechanism, the client model can deliver real-time completions to user\ninputs without the need to wait for responses from the cloud. Our experiments\non five datasets demonstrate that Hybrid-RACA offers strong performance while\nmaintaining low latency.", "published": "2023-08-08 12:27:20", "link": "http://arxiv.org/abs/2308.04215v3", "categories": ["cs.CL", "cs.AI", "cs.DC"], "primary_category": "cs.CL"}
{"title": "OpinionConv: Conversational Product Search with Grounded Opinions", "abstract": "When searching for products, the opinions of others play an important role in\nmaking informed decisions. Subjective experiences about a product can be a\nvaluable source of information. This is also true in sales conversations, where\na customer and a sales assistant exchange facts and opinions about products.\nHowever, training an AI for such conversations is complicated by the fact that\nlanguage models do not possess authentic opinions for their lack of real-world\nexperience. We address this problem by leveraging product reviews as a rich\nsource of product opinions to ground conversational AI in true subjective\nnarratives. With OpinionConv, we develop the first conversational AI for\nsimulating sales conversations. To validate the generated conversations, we\nconduct several user studies showing that the generated opinions are perceived\nas realistic. Our assessors also confirm the importance of opinions as an\ninformative basis for decision-making.", "published": "2023-08-08 12:45:01", "link": "http://arxiv.org/abs/2308.04226v1", "categories": ["cs.HC", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.HC"}
{"title": "In-Context Alignment: Chat with Vanilla Language Models Before\n  Fine-Tuning", "abstract": "In this note, we explore inference-time alignment through in-context\nlearning. We consider a vanilla pretrained language model Llama-2 before any\nfine-tuning and retrieve an average of 9 demonstration alignment examples when\nthe model is prompted to follow chat-style instructions. Compared to direct\nprompting, the in-context alignment without changing model weights leads to a\n7x increase in win-rate w.r.t. the text-davinci-003 model from OpenAI, making\nthe vanilla language model comparable to strong baselines with alignment\nfine-tuning.", "published": "2023-08-08 14:17:17", "link": "http://arxiv.org/abs/2308.04275v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Comparative Analysis of the wav2vec 2.0 Feature Extractor", "abstract": "Automatic speech recognition (ASR) systems typically use handcrafted feature\nextraction pipelines. To avoid their inherent information loss and to achieve\nmore consistent modeling from speech to transcribed text, neural raw waveform\nfeature extractors (FEs) are an appealing approach. Also the wav2vec 2.0 model,\nwhich has recently gained large popularity, uses a convolutional FE which\noperates directly on the speech waveform. However, it is not yet studied\nextensively in the literature. In this work, we study its capability to replace\nthe standard feature extraction methods in a connectionist temporal\nclassification (CTC) ASR model and compare it to an alternative neural FE. We\nshow that both are competitive with traditional FEs on the LibriSpeech\nbenchmark and analyze the effect of the individual components. Furthermore, we\nanalyze the learned filters and show that the most important information for\nthe ASR system is obtained by a set of bandpass filters.", "published": "2023-08-08 14:29:35", "link": "http://arxiv.org/abs/2308.04286v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore", "abstract": "The legality of training language models (LMs) on copyrighted or otherwise\nrestricted data is under intense debate. However, as we show, model performance\nsignificantly degrades if trained only on low-risk text (e.g., out-of-copyright\nbooks or government documents), due to its limited size and domain coverage. We\npresent SILO, a new language model that manages this risk-performance tradeoff\nduring inference. SILO is built by (1) training a parametric LM on Open License\nCorpus (OLC), a new corpus we curate with 228B tokens of public domain and\npermissively licensed text and (2) augmenting it with a more general and easily\nmodifiable nonparametric datastore (e.g., containing copyrighted books or news)\nthat is only queried during inference. The datastore allows use of high-risk\ndata without training on it, supports sentence-level data attribution, and\nenables data producers to opt out from the model by removing content from the\nstore. These capabilities can foster compliance with data-use regulations such\nas the fair use doctrine in the United States and the GDPR in the European\nUnion. Our experiments show that the parametric LM struggles on domains not\ncovered by OLC. However, access to the datastore greatly improves out of domain\nperformance, closing 90% of the performance gap with an LM trained on the Pile,\na more diverse corpus with mostly high-risk text. We also analyze which\nnonparametric approach works best, where the remaining errors lie, and how\nperformance scales with datastore size. Our results suggest that it is possible\nto build high quality language models while mitigating their legal risk.", "published": "2023-08-08 17:58:15", "link": "http://arxiv.org/abs/2308.04430v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DisCoCat for Donkey Sentences", "abstract": "We demonstrate how to parse Geach's Donkey sentences in a compositional\ndistributional model of meaning. We build on previous work on the DisCoCat\n(Distributional Compositional Categorical) framework, including extensions that\nmodel discourse, determiners, and relative pronouns. We present a type-logical\nsyntax for parsing donkey sentences, for which we define both relational and\nvector space semantics.", "published": "2023-08-08 18:35:22", "link": "http://arxiv.org/abs/2308.04519v1", "categories": ["cs.CL", "cs.AI", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Towards an AI to Win Ghana's National Science and Maths Quiz", "abstract": "Can an AI win Ghana's National Science and Maths Quiz (NSMQ)? That is the\nquestion we seek to answer in the NSMQ AI project, an open-source project that\nis building AI to compete live in the NSMQ and win. The NSMQ is an annual live\nscience and mathematics competition for senior secondary school students in\nGhana in which 3 teams of 2 students compete by answering questions across\nbiology, chemistry, physics, and math in 5 rounds over 5 progressive stages\nuntil a winning team is crowned for that year. The NSMQ is an exciting live\nquiz competition with interesting technical challenges across speech-to-text,\ntext-to-speech, question-answering, and human-computer interaction. In this\nongoing work that began in January 2023, we give an overview of the project,\ndescribe each of the teams, progress made thus far, and the next steps toward\nour planned launch and debut of the AI in October for NSMQ 2023. An AI that\nconquers this grand challenge can have real-world impact on education such as\nenabling millions of students across Africa to have one-on-one learning support\nfrom this AI.", "published": "2023-08-08 15:26:58", "link": "http://arxiv.org/abs/2308.04333v1", "categories": ["cs.HC", "cs.CL", "cs.CY", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Investigating Speaker Embedding Disentanglement on Natural Read Speech", "abstract": "Disentanglement is the task of learning representations that identify and\nseparate factors that explain the variation observed in data. Disentangled\nrepresentations are useful to increase the generalizability, explainability,\nand fairness of data-driven models. Only little is known about how well such\ndisentanglement works for speech representations. A major challenge when\ntackling disentanglement for speech representations are the unknown generative\nfactors underlying the speech signal. In this work, we investigate to what\ndegree speech representations encoding speaker identity can be disentangled. To\nquantify disentanglement, we identify acoustic features that are highly\nspeaker-variant and can serve as proxies for the factors of variation\nunderlying speech. We find that disentanglement of the speaker embedding is\nlimited when trained with standard objectives promoting disentanglement but can\nbe improved over vanilla representation learning to some extent.", "published": "2023-08-08 12:43:59", "link": "http://arxiv.org/abs/2308.04225v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Target Speech Extraction with Conditional Diffusion Model", "abstract": "Diffusion model-based speech enhancement has received increased attention\nsince it can generate very natural enhanced signals and generalizes well to\nunseen conditions. Diffusion models have been explored for several sub-tasks of\nspeech enhancement, such as speech denoising, dereverberation, and source\nseparation. In this paper, we investigate their use for target speech\nextraction (TSE), which consists of estimating the clean speech signal of a\ntarget speaker in a mixture of multi-talkers. TSE is realized by conditioning\nthe extraction process on a clue identifying the target speaker. We show we can\nrealize TSE using a conditional diffusion model conditioned on the clue.\nBesides, we introduce ensemble inference to reduce potential extraction errors\ncaused by the diffusion process. In experiments on Libri2mix corpus, we show\nthat the proposed diffusion model-based TSE combined with ensemble inference\noutperforms a comparable TSE system trained discriminatively.", "published": "2023-08-08 02:06:11", "link": "http://arxiv.org/abs/2308.03987v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MSAC: Multiple Speech Attribute Control Method for Reliable Speech\n  Emotion Recognition", "abstract": "Despite notable progress, speech emotion recognition (SER) remains\nchallenging due to the intricate and ambiguous nature of speech emotion,\nparticularly in wild world. While current studies primarily focus on\nrecognition and generalization abilities, our research pioneers an\ninvestigation into the reliability of SER methods in the presence of semantic\ndata shifts and explores how to exert fine-grained control over various\nattributes inherent in speech signals to enhance speech emotion modeling. In\nthis paper, we first introduce MSAC-SERNet, a novel unified SER framework\ncapable of simultaneously handling both single-corpus and cross-corpus SER.\nSpecifically, concentrating exclusively on the speech emotion attribute, a\nnovel CNN-based SER model is presented to extract discriminative emotional\nrepresentations, guided by additive margin softmax loss. Considering\ninformation overlap between various speech attributes, we propose a novel\nlearning paradigm based on correlations of different speech attributes, termed\nMultiple Speech Attribute Control (MSAC), which empowers the proposed SER model\nto simultaneously capture fine-grained emotion-related features while\nmitigating the negative impact of emotion-agnostic representations.\nFurthermore, we make a first attempt to examine the reliability of the\nMSAC-SERNet framework using out-of-distribution detection methods. Experiments\non both single-corpus and cross-corpus SER scenarios indicate that MSAC-SERNet\nnot only consistently outperforms the baseline in all aspects, but achieves\nsuperior performance compared to state-of-the-art SER approaches.", "published": "2023-08-08 03:43:24", "link": "http://arxiv.org/abs/2308.04025v3", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Expression Prompt Collaboration Transformer for Universal Referring\n  Video Object Segmentation", "abstract": "Audio-guided Video Object Segmentation (A-VOS) and Referring Video Object\nSegmentation (R-VOS) are two highly related tasks that both aim to segment\nspecific objects from video sequences according to expression prompts. However,\ndue to the challenges of modeling representations for different modalities,\nexisting methods struggle to strike a balance between interaction flexibility\nand localization precision. In this paper, we address this problem from two\nperspectives: the alignment of audio and text and the deep interaction among\naudio, text, and visual modalities. First, we propose a universal architecture,\nthe Expression Prompt Collaboration Transformer, herein EPCFormer. Next, we\npropose an Expression Alignment (EA) mechanism for audio and text. The proposed\nEPCFormer exploits the fact that audio and text prompts referring to the same\nobjects are semantically equivalent by using contrastive learning for both\ntypes of expressions. Then, to facilitate deep interactions among audio, text,\nand visual modalities, we introduce an Expression-Visual Attention (EVA)\nmodule. The knowledge of video object segmentation in terms of the expression\nprompts can seamlessly transfer between the two tasks by deeply exploring\ncomplementary cues between text and audio. Experiments on well-recognized\nbenchmarks demonstrate that our EPCFormer attains state-of-the-art results on\nboth tasks. The source code will be made publicly available at\nhttps://github.com/lab206/EPCFormer.", "published": "2023-08-08 09:48:00", "link": "http://arxiv.org/abs/2308.04162v2", "categories": ["cs.CV", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Dual input neural networks for positional sound source localization", "abstract": "In many signal processing applications, metadata may be advantageously used\nin conjunction with a high dimensional signal to produce a desired output. In\nthe case of classical Sound Source Localization (SSL) algorithms, information\nfrom a high dimensional, multichannel audio signals received by many\ndistributed microphones is combined with information describing acoustic\nproperties of the scene, such as the microphones' coordinates in space, to\nestimate the position of a sound source. We introduce Dual Input Neural\nNetworks (DI-NNs) as a simple and effective way to model these two data types\nin a neural network. We train and evaluate our proposed DI-NN on scenarios of\nvarying difficulty and realism and compare it against an alternative\narchitecture, a classical Least-Squares (LS) method as well as a classical\nConvolutional Recurrent Neural Network (CRNN). Our results show that the DI-NN\nsignificantly outperforms the baselines, achieving a five times lower\nlocalization error than the LS method and two times lower than the CRNN in a\ntest dataset of real recordings.", "published": "2023-08-08 09:59:56", "link": "http://arxiv.org/abs/2308.04169v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Breaking Speaker Recognition with PaddingBack", "abstract": "Machine Learning as a Service (MLaaS) has gained popularity due to\nadvancements in Deep Neural Networks (DNNs). However, untrusted third-party\nplatforms have raised concerns about AI security, particularly in backdoor\nattacks. Recent research has shown that speech backdoors can utilize\ntransformations as triggers, similar to image backdoors. However, human ears\ncan easily be aware of these transformations, leading to suspicion. In this\npaper, we propose PaddingBack, an inaudible backdoor attack that utilizes\nmalicious operations to generate poisoned samples, rendering them\nindistinguishable from clean ones. Instead of using external perturbations as\ntriggers, we exploit the widely-used speech signal operation, padding, to break\nspeaker recognition systems. Experimental results demonstrate the effectiveness\nof our method, achieving a significant attack success rate while retaining\nbenign accuracy. Furthermore, PaddingBack demonstrates the ability to resist\ndefense methods and maintain its stealthiness against human perception.", "published": "2023-08-08 10:36:44", "link": "http://arxiv.org/abs/2308.04179v2", "categories": ["cs.CR", "cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.CR"}
{"title": "Advancing Natural-Language Based Audio Retrieval with PaSST and Large\n  Audio-Caption Data Sets", "abstract": "This work presents a text-to-audio-retrieval system based on pre-trained text\nand spectrogram transformers. Our method projects recordings and textual\ndescriptions into a shared audio-caption space in which related examples from\ndifferent modalities are close. Through a systematic analysis, we examine how\neach component of the system influences retrieval performance. As a result, we\nidentify two key components that play a crucial role in driving performance:\nthe self-attention-based audio encoder for audio embedding and the utilization\nof additional human-generated and synthetic data sets during pre-training. We\nfurther experimented with augmenting ClothoV2 captions with available keywords\nto increase their variety; however, this only led to marginal improvements. Our\nsystem ranked first in the 2023's DCASE Challenge, and it outperforms the\ncurrent state of the art on the ClothoV2 benchmark by 5.6 pp. mAP@10.", "published": "2023-08-08 13:46:55", "link": "http://arxiv.org/abs/2308.04258v1", "categories": ["eess.AS", "cs.IR", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "OmniDataComposer: A Unified Data Structure for Multimodal Data Fusion\n  and Infinite Data Generation", "abstract": "This paper presents OmniDataComposer, an innovative approach for multimodal\ndata fusion and unlimited data generation with an intent to refine and\nuncomplicate interplay among diverse data modalities. Coming to the core\nbreakthrough, it introduces a cohesive data structure proficient in processing\nand merging multimodal data inputs, which include video, audio, and text.\n  Our crafted algorithm leverages advancements across multiple operations such\nas video/image caption extraction, dense caption extraction, Automatic Speech\nRecognition (ASR), Optical Character Recognition (OCR), Recognize Anything\nModel(RAM), and object tracking. OmniDataComposer is capable of identifying\nover 6400 categories of objects, substantially broadening the spectrum of\nvisual information. It amalgamates these diverse modalities, promoting\nreciprocal enhancement among modalities and facilitating cross-modal data\ncorrection. \\textbf{The final output metamorphoses each video input into an\nelaborate sequential document}, virtually transmuting videos into thorough\nnarratives, making them easier to be processed by large language models.\n  Future prospects include optimizing datasets for each modality to encourage\nunlimited data generation. This robust base will offer priceless insights to\nmodels like ChatGPT, enabling them to create higher quality datasets for video\ncaptioning and easing question-answering tasks based on video content.\nOmniDataComposer inaugurates a new stage in multimodal learning, imparting\nenormous potential for augmenting AI's understanding and generation of complex,\nreal-world data.", "published": "2023-08-08 08:30:16", "link": "http://arxiv.org/abs/2308.04126v2", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Auditory Attention Decoding with Task-Related Multi-View Contrastive\n  Learning", "abstract": "The human brain can easily focus on one speaker and suppress others in\nscenarios such as a cocktail party. Recently, researchers found that auditory\nattention can be decoded from the electroencephalogram (EEG) data. However,\nmost existing deep learning methods are difficult to use prior knowledge of\ndifferent views (that is attended speech and EEG are task-related views) and\nextract an unsatisfactory representation. Inspired by Broadbent's filter model,\nwe decode auditory attention in a multi-view paradigm and extract the most\nrelevant and important information utilizing the missing view. Specifically, we\npropose an auditory attention decoding (AAD) method based on multi-view VAE\nwith task-related multi-view contrastive (TMC) learning. Employing TMC learning\nin multi-view VAE can utilize the missing view to accumulate prior knowledge of\ndifferent views into the fusion of representation, and extract the approximate\ntask-related representation. We examine our method on two popular AAD datasets,\nand demonstrate the superiority of our method by comparing it to the\nstate-of-the-art method.", "published": "2023-08-08 13:17:37", "link": "http://arxiv.org/abs/2308.04244v1", "categories": ["cs.SD", "cs.HC", "eess.AS", "q-bio.NC", "q-bio.QM"], "primary_category": "cs.SD"}
{"title": "TranSTYLer: Multimodal Behavioral Style Transfer for Facial and Body\n  Gestures Generation", "abstract": "This paper addresses the challenge of transferring the behavior expressivity\nstyle of a virtual agent to another one while preserving behaviors shape as\nthey carry communicative meaning. Behavior expressivity style is viewed here as\nthe qualitative properties of behaviors. We propose TranSTYLer, a multimodal\ntransformer based model that synthesizes the multimodal behaviors of a source\nspeaker with the style of a target speaker. We assume that behavior\nexpressivity style is encoded across various modalities of communication,\nincluding text, speech, body gestures, and facial expressions. The model\nemploys a style and content disentanglement schema to ensure that the\ntransferred style does not interfere with the meaning conveyed by the source\nbehaviors. Our approach eliminates the need for style labels and allows the\ngeneralization to styles that have not been seen during the training phase. We\ntrain our model on the PATS corpus, which we extended to include dialog acts\nand 2D facial landmarks. Objective and subjective evaluations show that our\nmodel outperforms state of the art models in style transfer for both seen and\nunseen styles during training. To tackle the issues of style and content\nleakage that may arise, we propose a methodology to assess the degree to which\nbehavior and gestures associated with the target style are successfully\ntransferred, while ensuring the preservation of the ones related to the source\ncontent.", "published": "2023-08-08 15:42:35", "link": "http://arxiv.org/abs/2308.10843v1", "categories": ["cs.MM", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Deep Learning for Steganalysis of Diverse Data Types: A review of\n  methods, taxonomy, challenges and future directions", "abstract": "Steganography and steganalysis are two interrelated aspects of the field of\ninformation security. Steganography seeks to conceal communications, whereas\nsteganalysis is aimed to either find them or even, if possible, recover the\ndata they contain. Steganography and steganalysis have attracted a great deal\nof interest, particularly from law enforcement. Steganography is often used by\ncybercriminals and even terrorists to avoid being captured while in possession\nof incriminating evidence, even encrypted, since cryptography is prohibited or\nrestricted in many countries. Therefore, knowledge of cutting-edge techniques\nto uncover concealed information is crucial in exposing illegal acts. Over the\nlast few years, a number of strong and reliable steganography and steganalysis\ntechniques have been introduced in the literature. This review paper provides a\ncomprehensive overview of deep learning-based steganalysis techniques used to\ndetect hidden information within digital media. The paper covers all types of\ncover in steganalysis, including image, audio, and video, and discusses the\nmost commonly used deep learning techniques. In addition, the paper explores\nthe use of more advanced deep learning techniques, such as deep transfer\nlearning (DTL) and deep reinforcement learning (DRL), to enhance the\nperformance of steganalysis systems. The paper provides a systematic review of\nrecent research in the field, including data sets and evaluation metrics used\nin recent studies. It also presents a detailed analysis of DTL-based\nsteganalysis approaches and their performance on different data sets. The\nreview concludes with a discussion on the current state of deep learning-based\nsteganalysis, challenges, and future research directions.", "published": "2023-08-08 18:37:24", "link": "http://arxiv.org/abs/2308.04522v3", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.MM", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CR"}
