{"title": "Learning an Effective Context-Response Matching Model with\n  Self-Supervised Tasks for Retrieval-based Dialogues", "abstract": "Building an intelligent dialogue system with the ability to select a proper\nresponse according to a multi-turn context is a great challenging task.\nExisting studies focus on building a context-response matching model with\nvarious neural architectures or PLMs and typically learning with a single\nresponse prediction task. These approaches overlook many potential training\nsignals contained in dialogue data, which might be beneficial for context\nunderstanding and produce better features for response prediction. Besides, the\nresponse retrieved from existing dialogue systems supervised by the\nconventional way still faces some critical challenges, including incoherence\nand inconsistency. To address these issues, in this paper, we propose learning\na context-response matching model with auxiliary self-supervised tasks designed\nfor the dialogue data based on pre-trained language models. Specifically, we\nintroduce four self-supervised tasks including next session prediction,\nutterance restoration, incoherence detection and consistency discrimination,\nand jointly train the PLM-based response selection model with these auxiliary\ntasks in a multi-task manner. By this means, the auxiliary tasks can guide the\nlearning of the matching model to achieve a better local optimum and select a\nmore proper response. Experiment results on two benchmarks indicate that the\nproposed auxiliary self-supervised tasks bring significant improvement for\nmulti-turn response selection in retrieval-based dialogues, and our model\nachieves new state-of-the-art results on both datasets.", "published": "2020-09-14 08:44:46", "link": "http://arxiv.org/abs/2009.06265v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Development of a Dataset and a Deep Learning Baseline Named Entity\n  Recognizer for Three Low Resource Languages: Bhojpuri, Maithili and Magahi", "abstract": "In Natural Language Processing (NLP) pipelines, Named Entity Recognition\n(NER) is one of the preliminary problems, which marks proper nouns and other\nnamed entities such as Location, Person, Organization, Disease etc. Such\nentities, without a NER module, adversely affect the performance of a machine\ntranslation system. NER helps in overcoming this problem by recognising and\nhandling such entities separately, although it can be useful in Information\nExtraction systems also. Bhojpuri, Maithili and Magahi are low resource\nlanguages, usually known as Purvanchal languages. This paper focuses on the\ndevelopment of a NER benchmark dataset for the Machine Translation systems\ndeveloped to translate from these languages to Hindi by annotating parts of\ntheir available corpora. Bhojpuri, Maithili and Magahi corpora of sizes 228373,\n157468 and 56190 tokens, respectively, were annotated using 22 entity labels.\nThe annotation considers coarse-grained annotation labels followed by the\ntagset used in one of the Hindi NER datasets. We also report a Deep Learning\nbased baseline that uses an LSTM-CNNs-CRF model. The lower baseline F1-scores\nfrom the NER tool obtained by using Conditional Random Fields models are 96.73\nfor Bhojpuri, 93.33 for Maithili and 95.04 for Magahi. The Deep Learning-based\ntechnique (LSTM-CNNs-CRF) achieved 96.25 for Bhojpuri, 93.33 for Maithili and\n95.44 for Magahi.", "published": "2020-09-14 14:07:50", "link": "http://arxiv.org/abs/2009.06451v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comparison of Two Fluctuation Analyses for Natural Language Clustering\n  Phenomena: Taylor and Ebeling & Neiman Methods", "abstract": "This article considers the fluctuation analysis methods of Taylor and Ebeling\n& Neiman. While both have been applied to various phenomena in the statistical\nmechanics domain, their similarities and differences have not been clarified.\nAfter considering their analytical aspects, this article presents a large-scale\napplication of these methods to text. It is found that both methods can\ndistinguish real text from independently and identically distributed (i.i.d.)\nsequences. Furthermore, it is found that the Taylor exponents acquired from\nwords can roughly distinguish text categories; this is also the case for\nEbeling and Neiman exponents, but to a lesser extent. Additionally, both\nmethods show some possibility of capturing script kinds.", "published": "2020-09-14 08:30:24", "link": "http://arxiv.org/abs/2009.06257v1", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "GeDi: Generative Discriminator Guided Sequence Generation", "abstract": "While large-scale language models (LMs) are able to imitate the distribution\nof natural language well enough to generate realistic text, it is difficult to\ncontrol which regions of the distribution they generate. This is especially\nproblematic because datasets used for training large LMs usually contain\nsignificant toxicity, hate, bias, and negativity. We propose GeDi as an\nefficient method for using smaller LMs as generative discriminators to guide\ngeneration from large LMs to make them safer and more controllable. GeDi guides\ngeneration at each step by computing classification probabilities for all\npossible next tokens via Bayes rule by normalizing over two class-conditional\ndistributions; one conditioned on the desired attribute, or control code, and\nanother conditioned on the undesired attribute, or anti control code. We find\nthat GeDi gives stronger controllability than the state of the art method while\nalso achieving generation speeds more than 30 times faster. Additionally,\ntraining GeDi on only four topics allows us to controllably generate new topics\nzero-shot from just a keyword, unlocking a new capability that previous\ncontrollable generation methods do not have. Lastly, we show that GeDi can make\nGPT-2 (1.5B parameters) significantly less toxic without sacrificing linguistic\nquality, making it by far the most practical existing method for detoxifying\nlarge language models while maintaining a fast generation speed.", "published": "2020-09-14 17:45:36", "link": "http://arxiv.org/abs/2009.06367v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Finitist's Manifesto: Do we need to Reformulate the Foundations of\n  Mathematics?", "abstract": "There is a problem with the foundations of classical mathematics, and\npotentially even with the foundations of computer science, that mathematicians\nhave by-and-large ignored. This essay is a call for practicing mathematicians\nwho have been sleep-walking in their infinitary mathematical paradise to take\nheed. Much of mathematics relies upon either (i) the \"existence'\" of objects\nthat contain an infinite number of elements, (ii) our ability, \"in theory\", to\ncompute with an arbitrary level of precision, or (iii) our ability, \"in\ntheory\", to compute for an arbitrarily large number of time steps. All of\ncalculus relies on the notion of a limit. The monumental results of real and\ncomplex analysis rely on a seamless notion of the \"continuum\" of real numbers,\nwhich extends in the plane to the complex numbers and gives us, among other\nthings, \"rigorous\" definitions of continuity, the derivative, various different\nintegrals, as well as the fundamental theorems of calculus and of algebra --\nthe former of which says that the derivative and integral can be viewed as\ninverse operations, and the latter of which says that every polynomial over\n$\\mathbb{C}$ has a complex root. This essay is an inquiry into whether there is\nany way to assign meaning to the notions of \"existence\" and \"in theory'\" in (i)\nto (iii) above.", "published": "2020-09-14 14:44:08", "link": "http://arxiv.org/abs/2009.06485v1", "categories": ["math.LO", "cs.CL", "03A05", "F.4.0"], "primary_category": "math.LO"}
{"title": "At your Command! An Empirical Study on How LaypersonsTeach Robots New\n  Functions", "abstract": "Even though intelligent systems such as Siri or Google Assistant are\nenjoyable (and useful) dialog partners, users can only access predefined\nfunctionality. Enabling end-users to extend the functionality of intelligent\nsystems will be the next big thing. To promote research in this area we carried\nout an empirical study on how laypersons teach robots new functions by means of\nnatural language instructions. The result is a labeled corpus consisting of\n3168 submissions given by 870 subjects. The analysis of the dataset revealed\nthat many participants used certain wordings to express their wish to teach new\nfunctionality; two corresponding trigrams are among the most frequent. On the\ncontrary, more than one third (36.93%) did not verbalize the teaching intent at\nall. We labeled the semantic constituents in the utterances: declaration\n(including the name of the function) and intermediate steps. The full corpus is\npublicly available: http://dx.doi.org/10.21227/zecn-6c61", "published": "2020-09-14 15:16:25", "link": "http://arxiv.org/abs/2009.06510v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Food safety risk prediction with Deep Learning models using categorical\n  embeddings on European Union data", "abstract": "The world is becoming more globalized every day and people can buy products\nfrom almost every country in the world in their local stores. Given the\ndifferent food and feed safety laws from country to country, the European Union\nbegan to register in 1977 all irregularities related to traded products to\nensure cross-border monitoring of information and a quick reaction when risks\nto public health are detected in the food chain. This information has also an\nenormous potential as a preventive tool, in order to warn actors involved in\nfood safety and optimize their resources. In this paper, a set of data related\nto food issues was scraped and analysed with Machine Learning techniques to\npredict some features of future notifications, so that pre-emptive measures can\nbe taken. The novelty of the work relies on two points: the use of categorical\nembeddings with Deep Learning models (Multilayer Perceptron and 1-Dimension\nConvolutional Neural Networks) and its application to solve the problem of\npredicting food issues in the European Union. The models allow several features\nto be predicted: product category, hazard category and finally the proper\naction to be taken. Results show that the system can predict these features\nwith an accuracy ranging from 74.08% to 93.06%.", "published": "2020-09-14 19:36:58", "link": "http://arxiv.org/abs/2009.06704v1", "categories": ["cs.LG", "cs.CL", "I.2.6"], "primary_category": "cs.LG"}
{"title": "Composing Answer from Multi-spans for Reading Comprehension", "abstract": "This paper presents a novel method to generate answers for non-extraction\nmachine reading comprehension (MRC) tasks whose answers cannot be simply\nextracted as one span from the given passages. Using a pointer network-style\nextractive decoder for such type of MRC may result in unsatisfactory\nperformance when the ground-truth answers are given by human annotators or\nhighly re-paraphrased from parts of the passages. On the other hand, using\ngenerative decoder cannot well guarantee the resulted answers with well-formed\nsyntax and semantics when encountering long sentences. Therefore, to alleviate\nthe obvious drawbacks of both sides, we propose an answer making-up method from\nextracted multi-spans that are learned by our model as highly confident\n$n$-gram candidates in the given passage. That is, the returned answers are\ncomposed of discontinuous multi-spans but not just one consecutive span in the\ngiven passages anymore. The proposed method is simple but effective: empirical\nexperiments on MS MARCO show that the proposed method has a better performance\non accurately generating long answers, and substantially outperforms two\ncompetitive typical one-span and Seq2Seq baseline decoders.", "published": "2020-09-14 01:44:42", "link": "http://arxiv.org/abs/2009.06141v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Not-NUTs at W-NUT 2020 Task 2: A BERT-based System in Identifying\n  Informative COVID-19 English Tweets", "abstract": "As of 2020 when the COVID-19 pandemic is full-blown on a global scale,\npeople's need to have access to legitimate information regarding COVID-19 is\nmore urgent than ever, especially via online media where the abundance of\nirrelevant information overshadows the more informative ones. In response to\nsuch, we proposed a model that, given an English tweet, automatically\nidentifies whether that tweet bears informative content regarding COVID-19 or\nnot. By ensembling different BERTweet model configurations, we have achieved\ncompetitive results that are only shy of those by top performing teams by\nroughly 1% in terms of F1 score on the informative class. In the\npost-competition period, we have also experimented with various other\napproaches that potentially boost generalization to a new dataset.", "published": "2020-09-14 15:49:16", "link": "http://arxiv.org/abs/2009.06372v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EasyASR: A Distributed Machine Learning Platform for End-to-end\n  Automatic Speech Recognition", "abstract": "We present EasyASR, a distributed machine learning platform for training and\nserving large-scale Automatic Speech Recognition (ASR) models, as well as\ncollecting and processing audio data at scale. Our platform is built upon the\nMachine Learning Platform for AI of Alibaba Cloud. Its main functionality is to\nsupport efficient learning and inference for end-to-end ASR models on\ndistributed GPU clusters. It allows users to learn ASR models with either\npre-defined or user-customized network architectures via simple user interface.\nOn EasyASR, we have produced state-of-the-art results over several public\ndatasets for Mandarin speech recognition.", "published": "2020-09-14 14:47:02", "link": "http://arxiv.org/abs/2009.06487v2", "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Filling the Gap of Utterance-aware and Speaker-aware Representation for\n  Multi-turn Dialogue", "abstract": "A multi-turn dialogue is composed of multiple utterances from two or more\ndifferent speaker roles. Thus utterance- and speaker-aware clues are supposed\nto be well captured in models. However, in the existing retrieval-based\nmulti-turn dialogue modeling, the pre-trained language models (PrLMs) as\nencoder represent the dialogues coarsely by taking the pairwise dialogue\nhistory and candidate response as a whole, the hierarchical information on\neither utterance interrelation or speaker roles coupled in such representations\nis not well addressed. In this work, we propose a novel model to fill such a\ngap by modeling the effective utterance-aware and speaker-aware representations\nentailed in a dialogue history. In detail, we decouple the contextualized word\nrepresentations by masking mechanisms in Transformer-based PrLM, making each\nword only focus on the words in current utterance, other utterances, two\nspeaker roles (i.e., utterances of sender and utterances of receiver),\nrespectively. Experimental results show that our method boosts the strong\nELECTRA baseline substantially in four public benchmark datasets, and achieves\nvarious new state-of-the-art performance over previous methods. A series of\nablation studies are conducted to demonstrate the effectiveness of our method.", "published": "2020-09-14 15:07:19", "link": "http://arxiv.org/abs/2009.06504v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Controllable neural text-to-speech synthesis using intuitive prosodic\n  features", "abstract": "Modern neural text-to-speech (TTS) synthesis can generate speech that is\nindistinguishable from natural speech. However, the prosody of generated\nutterances often represents the average prosodic style of the database instead\nof having wide prosodic variation. Moreover, the generated prosody is solely\ndefined by the input text, which does not allow for different styles for the\nsame sentence. In this work, we train a sequence-to-sequence neural network\nconditioned on acoustic speech features to learn a latent prosody space with\nintuitive and meaningful dimensions. Experiments show that a model conditioned\non sentence-wise pitch, pitch range, phone duration, energy, and spectral tilt\ncan effectively control each prosodic dimension and generate a wide variety of\nspeaking styles, while maintaining similar mean opinion score (4.23) to our\nTacotron baseline (4.26).", "published": "2020-09-14 22:37:44", "link": "http://arxiv.org/abs/2009.06775v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "On Robustness and Bias Analysis of BERT-based Relation Extraction", "abstract": "Fine-tuning pre-trained models have achieved impressive performance on\nstandard natural language processing benchmarks. However, the resultant model\ngeneralizability remains poorly understood. We do not know, for example, how\nexcellent performance can lead to the perfection of generalization models. In\nthis study, we analyze a fine-tuned BERT model from different perspectives\nusing relation extraction. We also characterize the differences in\ngeneralization techniques according to our proposed improvements. From\nempirical experimentation, we find that BERT suffers a bottleneck in terms of\nrobustness by way of randomizations, adversarial and counterfactual tests, and\nbiases (i.e., selection and semantic). These findings highlight opportunities\nfor future improvements. Our open-sourced testbed DiagnoseRE is available in\n\\url{https://github.com/zjunlp/DiagnoseRE}.", "published": "2020-09-14 05:24:28", "link": "http://arxiv.org/abs/2009.06206v5", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Contrastive Triple Extraction with Generative Transformer", "abstract": "Triple extraction is an essential task in information extraction for natural\nlanguage processing and knowledge graph construction. In this paper, we revisit\nthe end-to-end triple extraction task for sequence generation. Since generative\ntriple extraction may struggle to capture long-term dependencies and generate\nunfaithful triples, we introduce a novel model, contrastive triple extraction\nwith a generative transformer. Specifically, we introduce a single shared\ntransformer module for encoder-decoder-based generation. To generate faithful\nresults, we propose a novel triplet contrastive training object. Moreover, we\nintroduce two mechanisms to further improve model performance (i.e., batch-wise\ndynamic attention-masking and triple-wise calibration). Experimental results on\nthree datasets (i.e., NYT, WebNLG, and MIE) show that our approach achieves\nbetter performance than that of baselines.", "published": "2020-09-14 05:29:24", "link": "http://arxiv.org/abs/2009.06207v8", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Transformers: A Survey", "abstract": "Transformer model architectures have garnered immense interest lately due to\ntheir effectiveness across a range of domains like language, vision and\nreinforcement learning. In the field of natural language processing for\nexample, Transformers have become an indispensable staple in the modern deep\nlearning stack. Recently, a dizzying number of \"X-former\" models have been\nproposed - Reformer, Linformer, Performer, Longformer, to name a few - which\nimprove upon the original Transformer architecture, many of which make\nimprovements around computational and memory efficiency. With the aim of\nhelping the avid researcher navigate this flurry, this paper characterizes a\nlarge and thoughtful selection of recent efficiency-flavored \"X-former\" models,\nproviding an organized and comprehensive overview of existing work and models\nacross multiple domains.", "published": "2020-09-14 20:38:14", "link": "http://arxiv.org/abs/2009.06732v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.IR"], "primary_category": "cs.LG"}
{"title": "ICASSP 2021 Deep Noise Suppression Challenge", "abstract": "The Deep Noise Suppression (DNS) challenge is designed to foster innovation\nin the area of noise suppression to achieve superior perceptual speech quality.\nWe recently organized a DNS challenge special session at INTERSPEECH 2020. We\nopen sourced training and test datasets for researchers to train their noise\nsuppression models. We also open sourced a subjective evaluation framework and\nused the tool to evaluate and pick the final winners. Many researchers from\nacademia and industry made significant contributions to push the field forward.\nWe also learned that as a research community, we still have a long way to go in\nachieving excellent speech quality in challenging noisy real-time conditions.\nIn this challenge, we are expanding both our training and test datasets. There\nare two tracks with one focusing on real-time denoising and the other focusing\non real-time personalized deep noise suppression. We also make a non-intrusive\nobjective speech quality metric called DNSMOS available for participants to use\nduring their development stages. The final evaluation will be based on\nsubjective tests.", "published": "2020-09-14 00:21:40", "link": "http://arxiv.org/abs/2009.06122v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Machine Learning Approach to Detect Suicidal Ideation in US Veterans\n  Based on Acoustic and Linguistic Features of Speech", "abstract": "Preventing Veteran suicide is a national priority. The US Department of\nVeterans Affairs (VA) collects, analyzes, and publishes data to inform suicide\nprevention strategies. Current approaches for detecting suicidal ideation\nmostly rely on patient self report which are inadequate and time consuming. In\nthis research study, our goal was to automate suicidal ideation detection from\nacoustic and linguistic features of an individual's speech using machine\nlearning (ML) algorithms. Using voice data collected from Veterans enrolled in\na large interventional study on Gulf War Illness at the Washington DC VA\nMedical Center, we conducted an evaluation of the performance of different ML\napproaches in achieving our objective. By fitting both classical ML and deep\nlearning models to the dataset, we identified the algorithms that were most\neffective for each feature set. Among classical machine learning algorithms,\nthe Support Vector Machine (SVM) trained on acoustic features performed best in\nclassifying suicidal Veterans. Among deep learning methods, the Convolutional\nNeural Network (CNN) trained on the linguistic features performed best. Our\nstudy shows that speech analysis in a machine learning pipeline is a promising\napproach for detecting suicidality among Veterans.", "published": "2020-09-14 00:01:45", "link": "http://arxiv.org/abs/2009.09069v2", "categories": ["cs.CY", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CY"}
