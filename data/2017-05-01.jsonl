{"title": "Duluth at SemEval--2016 Task 14 : Extending Gloss Overlaps to Enrich\n  Semantic Taxonomies", "abstract": "This paper describes the Duluth systems that participated in Task 14 of\nSemEval 2016, Semantic Taxonomy Enrichment. There were three related systems in\nthe formal evaluation which are discussed here, along with numerous\npost--evaluation runs. All of these systems identified synonyms between WordNet\nand other dictionaries by measuring the gloss overlaps between them. These\nsystems perform better than the random baseline and one post--evaluation\nvariation was within a respectable margin of the median result attained by all\nparticipating systems.", "published": "2017-05-01 00:36:59", "link": "http://arxiv.org/abs/1705.00390v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dependency Parsing with Dilated Iterated Graph CNNs", "abstract": "Dependency parses are an effective way to inject linguistic knowledge into\nmany downstream tasks, and many practitioners wish to efficiently parse\nsentences at scale. Recent advances in GPU hardware have enabled neural\nnetworks to achieve significant gains over the previous best models, these\nmodels still fail to leverage GPUs' capability for massive parallelism due to\ntheir requirement of sequential processing of the sentence. In response, we\npropose Dilated Iterated Graph Convolutional Neural Networks (DIG-CNNs) for\ngraph-based dependency parsing, a graph convolutional architecture that allows\nfor efficient end-to-end GPU parsing. In experiments on the English Penn\nTreeBank benchmark, we show that DIG-CNNs perform on par with some of the best\nneural network parsers.", "published": "2017-05-01 02:39:00", "link": "http://arxiv.org/abs/1705.00403v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Model Transfer for Tagging Low-resource Languages using a Bilingual\n  Dictionary", "abstract": "Cross-lingual model transfer is a compelling and popular method for\npredicting annotations in a low-resource language, whereby parallel corpora\nprovide a bridge to a high-resource language and its associated annotated\ncorpora. However, parallel data is not readily available for many languages,\nlimiting the applicability of these approaches. We address these drawbacks in\nour framework which takes advantage of cross-lingual word embeddings trained\nsolely on a high coverage bilingual dictionary. We propose a novel neural\nnetwork model for joint training from both sources of data based on\ncross-lingual word embeddings, and show substantial empirical improvements over\nbaseline techniques. We also propose several active learning heuristics, which\nresult in improvements over competitive benchmark methods.", "published": "2017-05-01 05:58:56", "link": "http://arxiv.org/abs/1705.00424v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Augmentation for Low-Resource Neural Machine Translation", "abstract": "The quality of a Neural Machine Translation system depends substantially on\nthe availability of sizable parallel corpora. For low-resource language pairs\nthis is not the case, resulting in poor translation quality. Inspired by work\nin computer vision, we propose a novel data augmentation approach that targets\nlow-frequency words by generating new sentence pairs containing rare words in\nnew, synthetically created contexts. Experimental results on simulated\nlow-resource settings show that our method improves translation quality by up\nto 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.", "published": "2017-05-01 08:12:15", "link": "http://arxiv.org/abs/1705.00440v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Topic-Sensitive Word Representations", "abstract": "Distributed word representations are widely used for modeling words in NLP\ntasks. Most of the existing models generate one representation per word and do\nnot consider different meanings of a word. We present two approaches to learn\nmultiple topic-sensitive representations per word by using Hierarchical\nDirichlet Process. We observe that by modeling topics and integrating topic\ndistributions for each document we obtain representations that are able to\ndistinguish between different meanings of a given word. Our models yield\nstatistically significant improvements for the lexical substitution task\nindicating that commonly used single word representations, even when combined\nwith contextual information, are insufficient for this task.", "published": "2017-05-01 08:16:56", "link": "http://arxiv.org/abs/1705.00441v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lancaster A at SemEval-2017 Task 5: Evaluation metrics matter:\n  predicting sentiment from financial news headlines", "abstract": "This paper describes our participation in Task 5 track 2 of SemEval 2017 to\npredict the sentiment of financial news headlines for a specific company on a\ncontinuous scale between -1 and 1. We tackled the problem using a number of\napproaches, utilising a Support Vector Regression (SVR) and a Bidirectional\nLong Short-Term Memory (BLSTM). We found an improvement of 4-6% using the LSTM\nmodel over the SVR and came fourth in the track. We report a number of\ndifferent evaluations using a finance specific word embedding model and reflect\non the effects of using different evaluation metrics.", "published": "2017-05-01 15:57:41", "link": "http://arxiv.org/abs/1705.00571v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Natural Language Response Suggestion for Smart Reply", "abstract": "This paper presents a computationally efficient machine-learned method for\nnatural language response suggestion. Feed-forward neural networks using n-gram\nembedding features encode messages into vectors which are optimized to give\nmessage-response pairs a high dot-product value. An optimized search finds\nresponse suggestions. The method is evaluated in a large-scale commercial\ne-mail application, Inbox by Gmail. Compared to a sequence-to-sequence\napproach, the new system achieves the same quality at a small fraction of the\ncomputational requirements and latency.", "published": "2017-05-01 18:24:15", "link": "http://arxiv.org/abs/1705.00652v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speech-Based Visual Question Answering", "abstract": "This paper introduces speech-based visual question answering (VQA), the task\nof generating an answer given an image and a spoken question. Two methods are\nstudied: an end-to-end, deep neural network that directly uses audio waveforms\nas input versus a pipelined approach that performs ASR (Automatic Speech\nRecognition) on the question, followed by text-based visual question answering.\nFurthermore, we investigate the robustness of both methods by injecting various\nlevels of noise into the spoken question and find both methods to be tolerate\nnoise at similar levels.", "published": "2017-05-01 10:43:28", "link": "http://arxiv.org/abs/1705.00464v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Labelled network subgraphs reveal stylistic subtleties in written texts", "abstract": "The vast amount of data and increase of computational capacity have allowed\nthe analysis of texts from several perspectives, including the representation\nof texts as complex networks. Nodes of the network represent the words, and\nedges represent some relationship, usually word co-occurrence. Even though\nnetworked representations have been applied to study some tasks, such\napproaches are not usually combined with traditional models relying upon\nstatistical paradigms. Because networked models are able to grasp textual\npatterns, we devised a hybrid classifier, called labelled subgraphs, that\ncombines the frequency of common words with small structures found in the\ntopology of the network, known as motifs. Our approach is illustrated in two\ncontexts, authorship attribution and translationese identification. In the\nformer, a set of novels written by different authors is analyzed. To identify\ntranslationese, texts from the Canadian Hansard and the European parliament\nwere classified as to original and translated instances. Our results suggest\nthat labelled subgraphs are able to represent texts and it should be further\nexplored in other tasks, such as the analysis of text complexity, language\nproficiency, and machine translation.", "published": "2017-05-01 14:36:21", "link": "http://arxiv.org/abs/1705.00545v3", "categories": ["cs.CL", "physics.data-an"], "primary_category": "cs.CL"}
{"title": "The Promise of Premise: Harnessing Question Premises in Visual Question\n  Answering", "abstract": "In this paper, we make a simple observation that questions about images often\ncontain premises - objects and relationships implied by the question - and that\nreasoning about premises can help Visual Question Answering (VQA) models\nrespond more intelligently to irrelevant or previously unseen questions. When\npresented with a question that is irrelevant to an image, state-of-the-art VQA\nmodels will still answer purely based on learned language biases, resulting in\nnon-sensical or even misleading answers. We note that a visual question is\nirrelevant to an image if at least one of its premises is false (i.e. not\ndepicted in the image). We leverage this observation to construct a dataset for\nQuestion Relevance Prediction and Explanation (QRPE) by searching for false\npremises. We train novel question relevance detection models and show that\nmodels that reason about premises consistently outperform models that do not.\nWe also find that forcing standard VQA models to reason about premises during\ntraining can lead to improvements on tasks requiring compositional reasoning.", "published": "2017-05-01 17:41:37", "link": "http://arxiv.org/abs/1705.00601v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "\"Liar, Liar Pants on Fire\": A New Benchmark Dataset for Fake News\n  Detection", "abstract": "Automatic fake news detection is a challenging problem in deception\ndetection, and it has tremendous real-world political and social impacts.\nHowever, statistical approaches to combating fake news has been dramatically\nlimited by the lack of labeled benchmark datasets. In this paper, we present\nliar: a new, publicly available dataset for fake news detection. We collected a\ndecade-long, 12.8K manually labeled short statements in various contexts from\nPolitiFact.com, which provides detailed analysis report and links to source\ndocuments for each case. This dataset can be used for fact-checking research as\nwell. Notably, this new dataset is an order of magnitude larger than previously\nlargest public fake news datasets of similar type. Empirically, we investigate\nautomatic fake news detection based on surface-level linguistic patterns. We\nhave designed a novel, hybrid convolutional neural network to integrate\nmeta-data with text. We show that this hybrid approach can improve a text-only\ndeep learning model.", "published": "2017-05-01 18:20:47", "link": "http://arxiv.org/abs/1705.00648v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Query-adaptive Video Summarization via Quality-aware Relevance\n  Estimation", "abstract": "Although the problem of automatic video summarization has recently received a\nlot of attention, the problem of creating a video summary that also highlights\nelements relevant to a search query has been less studied. We address this\nproblem by posing query-relevant summarization as a video frame subset\nselection problem, which lets us optimise for summaries which are\nsimultaneously diverse, representative of the entire video, and relevant to a\ntext query. We quantify relevance by measuring the distance between frames and\nqueries in a common textual-visual semantic embedding space induced by a neural\nnetwork. In addition, we extend the model to capture query-independent\nproperties, such as frame quality. We compare our method against previous state\nof the art on textual-visual embeddings for thumbnail selection and show that\nour model outperforms them on relevance prediction. Furthermore, we introduce a\nnew dataset, annotated with diversity and query-specific relevance labels. On\nthis dataset, we train and test our complete model for video summarization and\nshow that it outperforms standard baselines such as Maximal Marginal Relevance.", "published": "2017-05-01 16:28:18", "link": "http://arxiv.org/abs/1705.00581v2", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "A polynomial time algorithm for the Lambek calculus with brackets of\n  bounded order", "abstract": "Lambek calculus is a logical foundation of categorial grammar, a linguistic\nparadigm of grammar as logic and parsing as deduction. Pentus (2010) gave a\npolynomial-time algorithm for determ- ining provability of bounded depth\nformulas in the Lambek calculus with empty antecedents allowed. Pentus'\nalgorithm is based on tabularisation of proof nets. Lambek calculus with\nbrackets is a conservative extension of Lambek calculus with bracket\nmodalities, suitable for the modeling of syntactical domains. In this paper we\ngive an algorithm for provability the Lambek calculus with brackets allowing\nempty antecedents. Our algorithm runs in polynomial time when both the formula\ndepth and the bracket nesting depth are bounded. It combines a Pentus-style\ntabularisation of proof nets with an automata-theoretic treatment of\nbracketing.", "published": "2017-05-01 20:12:11", "link": "http://arxiv.org/abs/1705.00694v2", "categories": ["cs.LO", "cs.CL", "cs.DS", "cs.FL"], "primary_category": "cs.LO"}
{"title": "From Imitation to Prediction, Data Compression vs Recurrent Neural\n  Networks for Natural Language Processing", "abstract": "In recent studies [1][13][12] Recurrent Neural Networks were used for\ngenerative processes and their surprising performance can be explained by their\nability to create good predictions. In addition, data compression is also based\non predictions. What the problem comes down to is whether a data compressor\ncould be used to perform as well as recurrent neural networks in natural\nlanguage processing tasks. If this is possible,then the problem comes down to\ndetermining if a compression algorithm is even more intelligent than a neural\nnetwork in specific tasks related to human language. In our journey we\ndiscovered what we think is the fundamental difference between a Data\nCompression Algorithm and a Recurrent Neural Network.", "published": "2017-05-01 20:23:13", "link": "http://arxiv.org/abs/1705.00697v1", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Comparison of Uniform and Random Sampling for Speech and Music Signals", "abstract": "In this paper, we will provide a comparison between uniform and random\nsampling for speech and music signals. There are various sampling and recovery\nmethods for audio signals. Here, we only investigate uniform and random schemes\nfor sampling and basic low-pass filtering and iterative method with adaptive\nthresholding for recovery. The simulation results indicate that uniform\nsampling with cubic spline interpolation outperforms other sampling and\nrecovery methods.", "published": "2017-05-01 21:23:22", "link": "http://arxiv.org/abs/1705.01457v2", "categories": ["eess.AS", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
