{"title": "Cross-Lingual News Event Correlation for Stock Market Trend Prediction", "abstract": "In the modern economic landscape, integrating financial services with\nFinancial Technology (FinTech) has become essential, particularly in stock\ntrend analysis. This study addresses the gap in comprehending financial\ndynamics across diverse global economies by creating a structured financial\ndataset and proposing a cross-lingual Natural Language-based Financial\nForecasting (NLFF) pipeline for comprehensive financial analysis. Utilizing\nsentiment analysis, Named Entity Recognition (NER), and semantic textual\nsimilarity, we conducted an analytical examination of news articles to extract,\nmap, and visualize financial event timelines, uncovering the correlation\nbetween news events and stock market trends. Our method demonstrated a\nmeaningful correlation between stock price movements and cross-linguistic news\nsentiments, validated by processing two-year cross-lingual news data on two\nprominent sectors of the Pakistan Stock Exchange. This study offers significant\ninsights into key events, ensuring a substantial decision margin for investors\nthrough effective visualization and providing optimal investment opportunities.", "published": "2024-09-16 06:45:40", "link": "http://arxiv.org/abs/2410.00024v1", "categories": ["q-fin.ST", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "Rediscovering the Latent Dimensions of Personality with Large Language\n  Models as Trait Descriptors", "abstract": "Assessing personality traits using large language models (LLMs) has emerged\nas an interesting and challenging area of research. While previous methods\nemploy explicit questionnaires, often derived from the Big Five model of\npersonality, we hypothesize that LLMs implicitly encode notions of personality\nwhen modeling next-token responses. To demonstrate this, we introduce a novel\napproach that uncovers latent personality dimensions in LLMs by applying\nsingular value de-composition (SVD) to the log-probabilities of\ntrait-descriptive adjectives. Our experiments show that LLMs \"rediscover\" core\npersonality traits such as extraversion, agreeableness, conscientiousness,\nneuroticism, and openness without relying on direct questionnaire inputs, with\nthe top-5 factors corresponding to Big Five traits explaining 74.3% of the\nvariance in the latent space. Moreover, we can use the derived principal\ncomponents to assess personality along the Big Five dimensions, and achieve\nimprovements in average personality prediction accuracy of up to 5% over\nfine-tuned models, and up to 21% over direct LLM-based scoring techniques.", "published": "2024-09-16 00:24:40", "link": "http://arxiv.org/abs/2409.09905v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Householder Pseudo-Rotation: A Novel Approach to Activation Editing in\n  LLMs with Direction-Magnitude Perspective", "abstract": "Activation Editing, which involves directly editting the internal\nrepresentations of large language models (LLMs) to alter their behaviors and\nachieve desired properties, has emerged as a promising area of research.\nExisting works primarily treat LLMs' activations as points in space and modify\nthem by adding steering vectors. However, this approach is limited in its\nability to achieve greater performance improvement while maintaining the\nnecessary consistency of activation magnitudes. To overcome these issues, we\npropose a novel editing method that views activations in terms of their\ndirections and magnitudes. Our method, named Householder Pseudo-Rotation (HPR),\nmimics the rotation transformation, thus preserving activation norms and\nresulting in an improved performance on various safety benchmarks.", "published": "2024-09-16 07:29:40", "link": "http://arxiv.org/abs/2409.10053v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs for clinical risk prediction", "abstract": "This study compares the efficacy of GPT-4 and clinalytix Medical AI in\npredicting the clinical risk of delirium development. Findings indicate that\nGPT-4 exhibited significant deficiencies in identifying positive cases and\nstruggled to provide reliable probability estimates for delirium risk, while\nclinalytix Medical AI demonstrated superior accuracy. A thorough analysis of\nthe large language model's (LLM) outputs elucidated potential causes for these\ndiscrepancies, consistent with limitations reported in extant literature. These\nresults underscore the challenges LLMs face in accurately diagnosing conditions\nand interpreting complex clinical data. While LLMs hold substantial potential\nin healthcare, they are currently unsuitable for independent clinical\ndecision-making. Instead, they should be employed in assistive roles,\ncomplementing clinical expertise. Continued human oversight remains essential\nto ensure optimal outcomes for both patients and healthcare providers.", "published": "2024-09-16 11:34:40", "link": "http://arxiv.org/abs/2409.10191v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes\n  the Emoji Potential in LLMs", "abstract": "The manipulation of the personality traits of large language models (LLMs)\nhas emerged as a key area of research. Methods like prompt-based In-Context\nKnowledge Editing (IKE) and gradient-based Model Editor Networks (MEND) have\nbeen explored but show irregularity and variability; IKE depends on the prompt,\nleading to variability and sensitivity, while MEND yields inconsistent and\ngibberish outputs. To address this, we employed Opinion QA Based\nParameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank\nAdaptation (QLoRA), to manipulate the Big Five personality traits: Openness,\nConscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT,\nmodels such as Mistral-7B-Instruct and LLaMA-2-7B-chat showed a latent\nbehaviour by generating emojis for certain traits, despite no emojis being\npresent in the PEFT data. For instance, LLaMA-2-7B-chat generated emojis in\n99.5\\% of extraversion-related test instances, while Mistral-7B-Instruct did so\nin 92.5\\% of openness-related test instances. ICL Explainability analysis\nindicated that the LLMs used emojis intentionally to express these traits.\nMechanistic Interpretability analysis showed that this latent behaviour of LLMs\ncould be traced to specific neurons that became activated or amplified after\nPEFT. This paper provides a number of novel contributions. First, introducing\nan Opinion QA dataset for PEFT-driven personality manipulation; second,\ndeveloping metric models to benchmark LLM personality traits; third,\ndemonstrating PEFT's superiority over IKE in personality manipulation; and\nfinally, analysing and validating emoji usage through explainability methods\nsuch as Mechanistic Interpretability and In-context learning Explainability\nmethods.", "published": "2024-09-16 12:55:14", "link": "http://arxiv.org/abs/2409.10245v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Sexism in German Online Newspaper Comments with Open-Source\n  Text Embeddings (Team GDA, GermEval2024 Shared Task 1: GerMS-Detect, Subtasks\n  1 and 2, Closed Track)", "abstract": "Sexism in online media comments is a pervasive challenge that often manifests\nsubtly, complicating moderation efforts as interpretations of what constitutes\nsexism can vary among individuals. We study monolingual and multilingual\nopen-source text embeddings to reliably detect sexism and misogyny in\nGerman-language online comments from an Austrian newspaper. We observed\nclassifiers trained on text embeddings to mimic closely the individual\njudgements of human annotators. Our method showed robust performance in the\nGermEval 2024 GerMS-Detect Subtask 1 challenge, achieving an average macro F1\nscore of 0.597 (4th place, as reported on Codabench). It also accurately\npredicted the distribution of human annotations in GerMS-Detect Subtask 2, with\nan average Jensen-Shannon distance of 0.301 (2nd place). The computational\nefficiency of our approach suggests potential for scalable applications across\nvarious languages and linguistic contexts.", "published": "2024-09-16 14:56:59", "link": "http://arxiv.org/abs/2409.10341v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Schrodinger's Memory: Large Language Models", "abstract": "Memory is the foundation of all human activities; without memory, it would be\nnearly impossible for people to perform any task in daily life. With the\ndevelopment of Large Language Models (LLMs), their language capabilities are\nbecoming increasingly comparable to those of humans. But do LLMs have memory?\nBased on current performance, LLMs do appear to exhibit memory. So, what is the\nunderlying mechanism of this memory? Previous research has lacked a deep\nexploration of LLMs' memory capabilities and the underlying theory. In this\npaper, we use Universal Approximation Theorem (UAT) to explain the memory\nmechanism in LLMs. We also conduct experiments to verify the memory\ncapabilities of various LLMs, proposing a new method to assess their abilities\nbased on these memory ability. We argue that LLM memory operates like\nSchr\\\"odinger's memory, meaning that it only becomes observable when a specific\nmemory is queried. We can only determine if the model retains a memory based on\nits output in response to the query; otherwise, it remains indeterminate.\nFinally, we expand on this concept by comparing the memory capabilities of the\nhuman brain and LLMs, highlighting the similarities and differences in their\noperational mechanisms.", "published": "2024-09-16 17:18:11", "link": "http://arxiv.org/abs/2409.10482v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DILA: Dictionary Label Attention for Mechanistic Interpretability in\n  High-dimensional Multi-label Medical Coding Prediction", "abstract": "Predicting high-dimensional or extreme multilabels, such as in medical\ncoding, requires both accuracy and interpretability. Existing works often rely\non local interpretability methods, failing to provide comprehensive\nexplanations of the overall mechanism behind each label prediction within a\nmultilabel set. We propose a mechanistic interpretability module called\nDIctionary Label Attention (\\method) that disentangles uninterpretable dense\nembeddings into a sparse embedding space, where each nonzero element (a\ndictionary feature) represents a globally learned medical concept. Through\nhuman evaluations, we show that our sparse embeddings are more human\nunderstandable than its dense counterparts by at least 50 percent. Our\nautomated dictionary feature identification pipeline, leveraging large language\nmodels (LLMs), uncovers thousands of learned medical concepts by examining and\nsummarizing the highest activating tokens for each dictionary feature. We\nrepresent the relationships between dictionary features and medical codes\nthrough a sparse interpretable matrix, enhancing the mechanistic and global\nunderstanding of the model's predictions while maintaining competitive\nperformance and scalability without extensive human annotation.", "published": "2024-09-16 17:45:40", "link": "http://arxiv.org/abs/2409.10504v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Multi-candidate Speculative Decoding", "abstract": "Speculative Decoding (SD) is a technique to accelerate the inference of Large\nLanguage Models (LLMs) by using a lower complexity draft model to propose\ncandidate tokens verified by a larger target model. To further improve\nefficiency, Multi-Candidate Speculative Decoding (MCSD) improves upon this by\nsampling multiple candidate tokens from the draft model at each step and\nverifying them in parallel, thus increasing the chances of accepting a token\nand reducing generation time. Existing MCSD methods rely on the draft model to\ninitialize the multi-candidate sequences and use static length and tree\nattention structure for draft generation. However, such an approach suffers\nfrom the draft and target model's output distribution differences, especially\nin a dynamic generation context. In this work, we introduce a new version of\nMCSD that includes a target model initialized multi-candidate generation, a\ndynamic sliced topology-aware causal mask for dynamic length adjustment, and\ndecision models to optimize early stopping. We experimented with our method on\nLlama 2-7B and its variants and observed a maximum 27.5% speedup compared to\nour MCSD baseline across three benchmarks with Llama 2-7B as the target model\nand JackFram 68M as the draft model. Additionally, we evaluate the effects of\nusing the target model initialized multi-candidate process with different draft\nmodels on output quality.", "published": "2024-09-16 18:20:38", "link": "http://arxiv.org/abs/2409.10644v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Punctuation in Ancient Chinese Texts: A Multi-Layered LSTM\n  and Attention-Based Approach", "abstract": "It was only until the 20th century when the Chinese language began using\npunctuation. In fact, many ancient Chinese texts contain thousands of lines\nwith no distinct punctuation marks or delimiters in sight. The lack of\npunctuation in such texts makes it difficult for humans to identify when there\npauses or breaks between particular phrases and understand the semantic meaning\nof the written text (Mogahed, 2012). As a result, unless one was educated in\nthe ancient time period, many readers of ancient Chinese would have\nsignificantly different interpretations of the texts. We propose an approach to\npredict the location (and type) of punctuation in ancient Chinese texts that\nextends the work of Oh et al (2017) by leveraging a bidirectional multi-layered\nLSTM with a multi-head attention mechanism as inspired by Luong et al.'s (2015)\ndiscussion of attention-based architectures. We find that the use of\nmulti-layered LSTMs and multi-head attention significantly outperforms RNNs\nthat don't incorporate such components when evaluating ancient Chinese texts.", "published": "2024-09-16 23:36:15", "link": "http://arxiv.org/abs/2409.10783v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SFR-RAG: Towards Contextually Faithful LLMs", "abstract": "Retrieval Augmented Generation (RAG), a paradigm that integrates external\ncontextual information with large language models (LLMs) to enhance factual\naccuracy and relevance, has emerged as a pivotal area in generative AI. The\nLLMs used in RAG applications are required to faithfully and completely\ncomprehend the provided context and users' questions, avoid hallucination,\nhandle unanswerable, counterfactual or otherwise low-quality and irrelevant\ncontexts, perform complex multi-hop reasoning and produce reliable citations.\nIn this paper, we introduce SFR-RAG, a small LLM that is instruction-tuned with\nan emphasis on context-grounded generation and hallucination minimization. We\nalso present ContextualBench, a new evaluation framework compiling multiple\npopular and diverse RAG benchmarks, such as HotpotQA and TriviaQA, with\nconsistent RAG settings to ensure reproducibility and consistency in model\nassessments. Experimental results demonstrate that our SFR-RAG-9B model\noutperforms leading baselines such as Command-R+ (104B) and GPT-4o, achieving\nstate-of-the-art results in 3 out of 7 benchmarks in ContextualBench with\nsignificantly fewer parameters. The model is also shown to be resilient to\nalteration in the contextual information and behave appropriately when relevant\ncontext is removed. Additionally, the SFR-RAG model maintains competitive\nperformance in general instruction-following tasks and function-calling\ncapabilities.", "published": "2024-09-16 01:08:18", "link": "http://arxiv.org/abs/2409.09916v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Data Contamination Detection for Modern Large Language Models:\n  Limitations, Inconsistencies, and Oracle Challenges", "abstract": "As large language models achieve increasingly impressive results, questions\narise about whether such performance is from generalizability or mere data\nmemorization. Thus, numerous data contamination detection methods have been\nproposed. However, these approaches are often validated with traditional\nbenchmarks and early-stage LLMs, leaving uncertainty about their effectiveness\nwhen evaluating state-of-the-art LLMs on the contamination of more challenging\nbenchmarks. To address this gap and provide a dual investigation of SOTA LLM\ncontamination status and detection method robustness, we evaluate five\ncontamination detection approaches with four state-of-the-art LLMs across eight\nchallenging datasets often used in modern LLM evaluation. Our analysis reveals\nthat (1) Current methods have non-trivial limitations in their assumptions and\npractical applications; (2) Notable difficulties exist in detecting\ncontamination introduced during instruction fine-tuning with answer\naugmentation; and (3) Limited consistencies between SOTA contamination\ndetection techniques. These findings highlight the complexity of contamination\ndetection in advanced LLMs and the urgent need for further research on robust\nand generalizable contamination evaluation. Our code is available at\nhttps://github.com/vsamuel2003/data-contamination.", "published": "2024-09-16 02:04:33", "link": "http://arxiv.org/abs/2409.09927v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Gaps or Hallucinations? Gazing into Machine-Generated Legal Analysis for\n  Fine-grained Text Evaluations", "abstract": "Large Language Models (LLMs) show promise as a writing aid for professionals\nperforming legal analyses. However, LLMs can often hallucinate in this setting,\nin ways difficult to recognize by non-professionals and existing text\nevaluation metrics. In this work, we pose the question: when can\nmachine-generated legal analysis be evaluated as acceptable? We introduce the\nneutral notion of gaps, as opposed to hallucinations in a strict erroneous\nsense, to refer to the difference between human-written and machine-generated\nlegal analysis. Gaps do not always equate to invalid generation. Working with\nlegal experts, we consider the CLERC generation task proposed in Hou et al.\n(2024b), leading to a taxonomy, a fine-grained detector for predicting gap\ncategories, and an annotated dataset for automatic evaluation. Our best\ndetector achieves 67% F1 score and 80% precision on the test set. Employing\nthis detector as an automated metric on legal analysis generated by SOTA LLMs,\nwe find around 80% contain hallucinations of different kinds.", "published": "2024-09-16 02:38:38", "link": "http://arxiv.org/abs/2409.09947v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "SelECT-SQL: Self-correcting ensemble Chain-of-Thought for Text-to-SQL", "abstract": "In recent years,Text-to-SQL, the problem of automatically converting\nquestions posed in natural language to formal SQL queries, has emerged as an\nimportant problem at the intersection of natural language processing and data\nmanagement research. Large language models (LLMs) have delivered impressive\nperformance when used in an off-the-shelf performance, but still fall\nsignificantly short of expected expert-level performance. Errors are especially\nprobable when a nuanced understanding is needed of database schemas, questions,\nand SQL clauses to do proper Text-to-SQL conversion. We introduce SelECT-SQL, a\nnovel in-context learning solution that uses an algorithmic combination of\nchain-of-thought (CoT) prompting, self-correction, and ensemble methods to\nyield a new state-of-the-art result on challenging Text-to-SQL benchmarks.\nSpecifically, when configured using GPT-3.5-Turbo as the base LLM, SelECT-SQL\nachieves 84.2% execution accuracy on the Spider leaderboard's development set,\nexceeding both the best results of other baseline GPT-3.5-Turbo-based solutions\n(81.1%), and the peak performance (83.5%) of the GPT-4 result reported on the\nleaderboard.", "published": "2024-09-16 05:40:18", "link": "http://arxiv.org/abs/2409.10007v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HALO: Hallucination Analysis and Learning Optimization to Empower LLMs\n  with Retrieval-Augmented Context for Guided Clinical Decision Making", "abstract": "Large language models (LLMs) have significantly advanced natural language\nprocessing tasks, yet they are susceptible to generating inaccurate or\nunreliable responses, a phenomenon known as hallucination. In critical domains\nsuch as health and medicine, these hallucinations can pose serious risks. This\npaper introduces HALO, a novel framework designed to enhance the accuracy and\nreliability of medical question-answering (QA) systems by focusing on the\ndetection and mitigation of hallucinations. Our approach generates multiple\nvariations of a given query using LLMs and retrieves relevant information from\nexternal open knowledge bases to enrich the context. We utilize maximum\nmarginal relevance scoring to prioritize the retrieved context, which is then\nprovided to LLMs for answer generation, thereby reducing the risk of\nhallucinations. The integration of LangChain further streamlines this process,\nresulting in a notable and robust increase in the accuracy of both open-source\nand commercial LLMs, such as Llama-3.1 (from 44% to 65%) and ChatGPT (from 56%\nto 70%). This framework underscores the critical importance of addressing\nhallucinations in medical QA systems, ultimately improving clinical\ndecision-making and patient care. The open-source HALO is available at:\nhttps://github.com/ResponsibleAILab/HALO.", "published": "2024-09-16 05:50:39", "link": "http://arxiv.org/abs/2409.10011v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AceParse: A Comprehensive Dataset with Diverse Structured Texts for\n  Academic Literature Parsing", "abstract": "With the development of data-centric AI, the focus has shifted from\nmodel-driven approaches to improving data quality. Academic literature, as one\nof the crucial types, is predominantly stored in PDF formats and needs to be\nparsed into texts before further processing. However, parsing diverse\nstructured texts in academic literature remains challenging due to the lack of\ndatasets that cover various text structures. In this paper, we introduce\nAceParse, the first comprehensive dataset designed to support the parsing of a\nwide range of structured texts, including formulas, tables, lists, algorithms,\nand sentences with embedded mathematical expressions. Based on AceParse, we\nfine-tuned a multimodal model, named AceParser, which accurately parses various\nstructured texts within academic literature. This model outperforms the\nprevious state-of-the-art by 4.1% in terms of F1 score and by 5% in Jaccard\nSimilarity, demonstrating the potential of multimodal models in academic\nliterature parsing. Our dataset is available at\nhttps://github.com/JHW5981/AceParse.", "published": "2024-09-16 06:06:34", "link": "http://arxiv.org/abs/2409.10016v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Benchmarking Large Language Model Uncertainty for Prompt Optimization", "abstract": "Prompt optimization algorithms for Large Language Models (LLMs) excel in\nmulti-step reasoning but still lack effective uncertainty estimation. This\npaper introduces a benchmark dataset to evaluate uncertainty metrics, focusing\non Answer, Correctness, Aleatoric, and Epistemic Uncertainty. Through analysis\nof models like GPT-3.5-Turbo and Meta-Llama-3.1-8B-Instruct, we show that\ncurrent metrics align more with Answer Uncertainty, which reflects output\nconfidence and diversity, rather than Correctness Uncertainty, highlighting the\nneed for improved metrics that are optimization-objective-aware to better guide\nprompt optimization. Our code and dataset are available at\nhttps://github.com/0Frett/PO-Uncertainty-Benchmarking.", "published": "2024-09-16 07:13:30", "link": "http://arxiv.org/abs/2409.10044v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Increasing faithfulness in human-human dialog summarization with Spoken\n  Language Understanding tasks", "abstract": "Dialogue summarization aims to provide a concise and coherent summary of\nconversations between multiple speakers. While recent advancements in language\nmodels have enhanced this process, summarizing dialogues accurately and\nfaithfully remains challenging due to the need to understand speaker\ninteractions and capture relevant information. Indeed, abstractive models used\nfor dialog summarization may generate summaries that contain inconsistencies.\nWe suggest using the semantic information proposed for performing Spoken\nLanguage Understanding (SLU) in human-machine dialogue systems for\ngoal-oriented human-human dialogues to obtain a more semantically faithful\nsummary regarding the task. This study introduces three key contributions:\nFirst, we propose an exploration of how incorporating task-related information\ncan enhance the summarization process, leading to more semantically accurate\nsummaries. Then, we introduce a new evaluation criterion based on task\nsemantics. Finally, we propose a new dataset version with increased annotated\ndata standardized for research on task-oriented dialogue summarization. The\nstudy evaluates these methods using the DECODA corpus, a collection of French\nspoken dialogues from a call center. Results show that integrating models with\ntask-related information improves summary accuracy, even with varying word\nerror rates.", "published": "2024-09-16 08:15:35", "link": "http://arxiv.org/abs/2409.10070v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM-DER:A Named Entity Recognition Method Based on Large Language Models\n  for Chinese Coal Chemical Domain", "abstract": "Domain-specific Named Entity Recognition (NER), whose goal is to recognize\ndomain-specific entities and their categories, provides an important support\nfor constructing domain knowledge graphs. Currently, deep learning-based\nmethods are widely used and effective in NER tasks, but due to the reliance on\nlarge-scale labeled data. As a result, the scarcity of labeled data in a\nspecific domain will limit its application.Therefore, many researches started\nto introduce few-shot methods and achieved some results. However, the entity\nstructures in specific domains are often complex, and the current few-shot\nmethods are difficult to adapt to NER tasks with complex features.Taking the\nChinese coal chemical industry domain as an example,there exists a complex\nstructure of multiple entities sharing a single entity, as well as multiple\nrelationships for the same pair of entities, which affects the NER task under\nthe sample less condition.In this paper, we propose a Large Language Models\n(LLMs)-based entity recognition framework LLM-DER for the domain-specific\nentity recognition problem in Chinese, which enriches the entity information by\ngenerating a list of relationships containing entity types through LLMs, and\ndesigning a plausibility and consistency evaluation method to remove\nmisrecognized entities, which can effectively solve the complex structural\nentity recognition problem in a specific domain.The experimental results of\nthis paper on the Resume dataset and the self-constructed coal chemical dataset\nCoal show that LLM-DER performs outstandingly in domain-specific entity\nrecognition, not only outperforming the existing GPT-3.5-turbo baseline, but\nalso exceeding the fully-supervised baseline, verifying its effectiveness in\nentity recognition.", "published": "2024-09-16 08:28:05", "link": "http://arxiv.org/abs/2409.10077v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "StruEdit: Structured Outputs Enable the Fast and Accurate Knowledge\n  Editing for Large Language Models", "abstract": "As the modern tool of choice for question answering, large language models\n(LLMs) are expected to deliver answers with up-to-date knowledge. To achieve\nsuch ideal question-answering systems, locating and then editing outdated\nknowledge in the natural language outputs is a general target of popular\nknowledge editing methods. However, this target is challenging, as both\nidentifying which tokens to edit in the reasoning steps and ensuring the\ncoherence of the revised reasoning chain are difficult tasks. We argue that\nthese challenges stem from the unstructured nature of natural language outputs.\nTo address the above challenges, we propose $\\textbf{Stru}$ctural\n$\\textbf{Edit}$ing ($\\textbf{StruEdit}$), an improved baseline for knowledge\nediting. We first prompt LLMs to produce structured outputs consisting of\nreasoning triplets. Then, StruEdit removes any potentially outdated knowledge\nand efficiently refills the structured outputs with up-to-date information in a\nsingle step. Experimental results show that StruEdit consistently delivers the\nhighest accuracy with lowest latency compared with other knowledge editing\nmethods.", "published": "2024-09-16 09:48:56", "link": "http://arxiv.org/abs/2409.10132v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMs4OL 2024 Overview: The 1st Large Language Models for Ontology\n  Learning Challenge", "abstract": "This paper outlines the LLMs4OL 2024, the first edition of the Large Language\nModels for Ontology Learning Challenge. LLMs4OL is a community development\ninitiative collocated with the 23rd International Semantic Web Conference\n(ISWC) to explore the potential of Large Language Models (LLMs) in Ontology\nLearning (OL), a vital process for enhancing the web with structured knowledge\nto improve interoperability. By leveraging LLMs, the challenge aims to advance\nunderstanding and innovation in OL, aligning with the goals of the Semantic Web\nto create a more intelligent and user-friendly web. In this paper, we give an\noverview of the 2024 edition of the LLMs4OL challenge and summarize the\ncontributions.", "published": "2024-09-16 10:15:30", "link": "http://arxiv.org/abs/2409.10146v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Augmenting Automatic Speech Recognition Models with Disfluency Detection", "abstract": "Speech disfluency commonly occurs in conversational and spontaneous speech.\nHowever, standard Automatic Speech Recognition (ASR) models struggle to\naccurately recognize these disfluencies because they are typically trained on\nfluent transcripts. Current research mainly focuses on detecting disfluencies\nwithin transcripts, overlooking their exact location and duration in the\nspeech. Additionally, previous work often requires model fine-tuning and\naddresses limited types of disfluencies.\n  In this work, we present an inference-only approach to augment any ASR model\nwith the ability to detect open-set disfluencies. We first demonstrate that ASR\nmodels have difficulty transcribing speech disfluencies. Next, this work\nproposes a modified Connectionist Temporal Classification(CTC)-based forced\nalignment algorithm from \\cite{kurzinger2020ctc} to predict word-level\ntimestamps while effectively capturing disfluent speech. Additionally, we\ndevelop a model to classify alignment gaps between timestamps as either\ncontaining disfluent speech or silence. This model achieves an accuracy of\n81.62% and an F1-score of 80.07%. We test the augmentation pipeline of\nalignment gap detection and classification on a disfluent dataset. Our results\nshow that we captured 74.13% of the words that were initially missed by the\ntranscription, demonstrating the potential of this pipeline for downstream\ntasks.", "published": "2024-09-16 11:13:14", "link": "http://arxiv.org/abs/2409.10177v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MGSA: Multi-Granularity Graph Structure Attention for Knowledge\n  Graph-to-Text Generation", "abstract": "The Knowledge Graph-to-Text Generation task aims to convert structured\nknowledge graphs into coherent and human-readable natural language text. Recent\nefforts in this field have focused on enhancing pre-trained language models\n(PLMs) by incorporating graph structure information to capture the intricate\nstructure details of knowledge graphs. However, most of these approaches tend\nto capture only single-granularity structure information, concentrating either\non the relationships between entities within the original graph or on the\nrelationships between words within the same entity or across different\nentities. This narrow focus results in a significant limitation: models that\nconcentrate solely on entity-level structure fail to capture the nuanced\nsemantic relationships between words, while those that focus only on word-level\nstructure overlook the broader relationships between original entire entities.\nTo overcome these limitations, this paper introduces the Multi-granularity\nGraph Structure Attention (MGSA), which is based on PLMs. The encoder of the\nmodel architecture features an entity-level structure encoding module, a\nword-level structure encoding module, and an aggregation module that\nsynthesizes information from both structure. This multi-granularity structure\nencoding approach allows the model to simultaneously capture both entity-level\nand word-level structure information, providing a more comprehensive\nunderstanding of the knowledge graph's structure information, thereby\nsignificantly improving the quality of the generated text. We conducted\nextensive evaluations of the MGSA model using two widely recognized KG-to-Text\nGeneration benchmark datasets, WebNLG and EventNarrative, where it consistently\noutperformed models that rely solely on single-granularity structure\ninformation, demonstrating the effectiveness of our approach.", "published": "2024-09-16 14:01:03", "link": "http://arxiv.org/abs/2409.10294v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The 20 questions game to distinguish large language models", "abstract": "In a parallel with the 20 questions game, we present a method to determine\nwhether two large language models (LLMs), placed in a black-box context, are\nthe same or not. The goal is to use a small set of (benign) binary questions,\ntypically under 20. We formalize the problem and first establish a baseline\nusing a random selection of questions from known benchmark datasets, achieving\nan accuracy of nearly 100% within 20 questions. After showing optimal bounds\nfor this problem, we introduce two effective questioning heuristics able to\ndiscriminate 22 LLMs by using half as many questions for the same task. These\nmethods offer significant advantages in terms of stealth and are thus of\ninterest to auditors or copyright owners facing suspicions of model leaks.", "published": "2024-09-16 14:50:29", "link": "http://arxiv.org/abs/2409.10338v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Knowledge-Enhanced Disease Diagnosis Method Based on Prompt Learning\n  and BERT Integration", "abstract": "This paper proposes a knowledge-enhanced disease diagnosis method based on a\nprompt learning framework. The method retrieves structured knowledge from\nexternal knowledge graphs related to clinical cases, encodes it, and injects it\ninto the prompt templates to enhance the language model's understanding and\nreasoning capabilities for the task.We conducted experiments on three public\ndatasets: CHIP-CTC, IMCS-V2-NER, and KUAKE-QTR. The results show that the\nproposed method significantly outperforms existing models across multiple\nevaluation metrics, with an F1 score improvement of 2.4% on the CHIP-CTC\ndataset, 3.1% on the IMCS-V2-NER dataset,and 4.2% on the KUAKE-QTR dataset.\nAdditionally,ablation studies confirmed the critical role of the knowledge\ninjection module,as the removal of this module resulted in a significant drop\nin F1 score. The experimental results demonstrate that the proposed method not\nonly effectively improves the accuracy of disease diagnosis but also enhances\nthe interpretability of the predictions, providing more reliable support and\nevidence for clinical diagnosis.", "published": "2024-09-16 15:34:58", "link": "http://arxiv.org/abs/2409.10403v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Incorporating Classifier-Free Guidance in Diffusion Model-Based\n  Recommendation", "abstract": "This paper presents a diffusion-based recommender system that incorporates\nclassifier-free guidance. Most current recommender systems provide\nrecommendations using conventional methods such as collaborative or\ncontent-based filtering. Diffusion is a new approach to generative AI that\nimproves on previous generative AI approaches such as Variational Autoencoders\n(VAEs) and Generative Adversarial Networks (GANs). We incorporate diffusion in\na recommender system that mirrors the sequence users take when browsing and\nrating items. Although a few current recommender systems incorporate diffusion,\nthey do not incorporate classifier-free guidance, a new innovation in diffusion\nmodels as a whole. In this paper, we present a diffusion recommender system\nthat augments the underlying recommender system model for improved performance\nand also incorporates classifier-free guidance. Our findings show improvements\nover state-of-the-art recommender systems for most metrics for several\nrecommendation tasks on a variety of datasets. In particular, our approach\ndemonstrates the potential to provide better recommendations when data is\nsparse.", "published": "2024-09-16 17:27:27", "link": "http://arxiv.org/abs/2409.10494v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Causal Language Modeling Can Elicit Search and Reasoning Capabilities on\n  Logic Puzzles", "abstract": "Causal language modeling using the Transformer architecture has yielded\nremarkable capabilities in Large Language Models (LLMs) over the last few\nyears. However, the extent to which fundamental search and reasoning\ncapabilities emerged within LLMs remains a topic of ongoing debate. In this\nwork, we study if causal language modeling can learn a complex task such as\nsolving Sudoku puzzles. To solve a Sudoku, the model is first required to\nsearch over all empty cells of the puzzle to decide on a cell to fill and then\napply an appropriate strategy to fill the decided cell. Sometimes, the\napplication of a strategy only results in thinning down the possible values in\na cell rather than concluding the exact value of the cell. In such cases,\nmultiple strategies are applied one after the other to fill a single cell. We\nobserve that Transformer models trained on this synthetic task can indeed learn\nto solve Sudokus (our model solves $94.21\\%$ of the puzzles fully correctly)\nwhen trained on a logical sequence of steps taken by a solver. We find that\ntraining Transformers with the logical sequence of steps is necessary and\nwithout such training, they fail to learn Sudoku. We also extend our analysis\nto Zebra puzzles (known as Einstein puzzles) and show that the model solves\n$92.04 \\%$ of the puzzles fully correctly. In addition, we study the internal\nrepresentations of the trained Transformer and find that through linear\nprobing, we can decode information about the set of possible values in any\ngiven cell from them, pointing to the presence of a strong reasoning engine\nimplicit in the Transformer weights.", "published": "2024-09-16 17:42:15", "link": "http://arxiv.org/abs/2409.10502v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval", "abstract": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds.", "published": "2024-09-16 17:59:52", "link": "http://arxiv.org/abs/2409.10516v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Visualizing Temporal Topic Embeddings with a Compass", "abstract": "Dynamic topic modeling is useful at discovering the development and change in\nlatent topics over time. However, present methodology relies on algorithms that\nseparate document and word representations. This prevents the creation of a\nmeaningful embedding space where changes in word usage and documents can be\ndirectly analyzed in a temporal context. This paper proposes an expansion of\nthe compass-aligned temporal Word2Vec methodology into dynamic topic modeling.\nSuch a method allows for the direct comparison of word and document embeddings\nacross time in dynamic topics. This enables the creation of visualizations that\nincorporate temporal word embeddings within the context of documents into topic\nvisualizations. In experiments against the current state-of-the-art, our\nproposed method demonstrates overall competitive performance in topic relevancy\nand diversity across temporal datasets of varying size. Simultaneously, it\nprovides insightful visualizations focused on temporal word embeddings while\nmaintaining the insights provided by global topic evolution, advancing our\nunderstanding of how topics evolve over time.", "published": "2024-09-16 18:29:19", "link": "http://arxiv.org/abs/2409.10649v2", "categories": ["cs.CL", "cs.GR"], "primary_category": "cs.CL"}
{"title": "NaviQAte: Functionality-Guided Web Application Navigation", "abstract": "End-to-end web testing is challenging due to the need to explore diverse web\napplication functionalities. Current state-of-the-art methods, such as\nWebCanvas, are not designed for broad functionality exploration; they rely on\nspecific, detailed task descriptions, limiting their adaptability in dynamic\nweb environments. We introduce NaviQAte, which frames web application\nexploration as a question-and-answer task, generating action sequences for\nfunctionalities without requiring detailed parameters. Our three-phase approach\nutilizes advanced large language models like GPT-4o for complex decision-making\nand cost-effective models, such as GPT-4o mini, for simpler tasks. NaviQAte\nfocuses on functionality-guided web application navigation, integrating\nmulti-modal inputs such as text and images to enhance contextual understanding.\nEvaluations on the Mind2Web-Live and Mind2Web-Live-Abstracted datasets show\nthat NaviQAte achieves a 44.23% success rate in user task navigation and a\n38.46% success rate in functionality navigation, representing a 15% and 33%\nimprovement over WebCanvas. These results underscore the effectiveness of our\napproach in advancing automated web application testing.", "published": "2024-09-16 21:18:39", "link": "http://arxiv.org/abs/2409.10741v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Semantics Preserving Emoji Recommendation with Large Language Models", "abstract": "Emojis have become an integral part of digital communication, enriching text\nby conveying emotions, tone, and intent. Existing emoji recommendation methods\nare primarily evaluated based on their ability to match the exact emoji a user\nchooses in the original text. However, they ignore the essence of users'\nbehavior on social media in that each text can correspond to multiple\nreasonable emojis. To better assess a model's ability to align with such\nreal-world emoji usage, we propose a new semantics preserving evaluation\nframework for emoji recommendation, which measures a model's ability to\nrecommend emojis that maintain the semantic consistency with the user's text.\nTo evaluate how well a model preserves semantics, we assess whether the\npredicted affective state, demographic profile, and attitudinal stance of the\nuser remain unchanged. If these attributes are preserved, we consider the\nrecommended emojis to have maintained the original semantics. The advanced\nabilities of Large Language Models (LLMs) in understanding and generating\nnuanced, contextually relevant output make them well-suited for handling the\ncomplexities of semantics preserving emoji recommendation. To this end, we\nconstruct a comprehensive benchmark to systematically assess the performance of\nsix proprietary and open-source LLMs using different prompting techniques on\nour task. Our experiments demonstrate that GPT-4o outperforms other LLMs,\nachieving a semantics preservation score of 79.23%. Additionally, we conduct\ncase studies to analyze model biases in downstream classification tasks and\nevaluate the diversity of the recommended emojis.", "published": "2024-09-16 22:27:46", "link": "http://arxiv.org/abs/2409.10760v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Model Tells Itself Where to Attend: Faithfulness Meets Automatic\n  Attention Steering", "abstract": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious real-world tasks. However, they often struggle to fully comprehend and\neffectively utilize their input contexts, resulting in responses that are\nunfaithful or hallucinated. This difficulty increases for contexts that are\nlong or contain distracting information, which can divert LLMs from fully\ncapturing essential evidence. To address this issue, many works use prompting\nto help LLMs utilize contextual information more faithfully. For instance,\niterative prompting highlights key information in two steps that first ask the\nLLM to identify important pieces of context and then derive answers\naccordingly. However, prompting methods are constrained to highlighting key\ninformation implicitly in token space, which is often insufficient to fully\nsteer the model's attention. To improve model faithfulness more reliably, we\npropose AutoPASTA, a method that automatically identifies key contextual\ninformation and explicitly highlights it by steering an LLM's attention scores.\nLike prompting, AutoPASTA is applied at inference time and does not require\nchanging any model parameters. Our experiments on open-book QA demonstrate that\nAutoPASTA effectively enables models to grasp essential contextual information,\nleading to substantially improved model faithfulness and performance, e.g., an\naverage improvement of 7.95% for LLAMA3-70B-Instruct. Code will be publicly\navailable at https://github.com/QingruZhang/AutoPASTA .", "published": "2024-09-16 23:52:41", "link": "http://arxiv.org/abs/2409.10790v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do Large Language Models Need a Content Delivery Network?", "abstract": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.", "published": "2024-09-16 18:46:24", "link": "http://arxiv.org/abs/2409.13761v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Comprehensive Study on Sentiment Analysis: From Rule-based to modern LLM\n  based system", "abstract": "This paper provides a comprehensive survey of sentiment analysis within the\ncontext of artificial intelligence (AI) and large language models (LLMs).\nSentiment analysis, a critical aspect of natural language processing (NLP), has\nevolved significantly from traditional rule-based methods to advanced deep\nlearning techniques. This study examines the historical development of\nsentiment analysis, highlighting the transition from lexicon-based and\npattern-based approaches to more sophisticated machine learning and deep\nlearning models. Key challenges are discussed, including handling bilingual\ntexts, detecting sarcasm, and addressing biases. The paper reviews\nstate-of-the-art approaches, identifies emerging trends, and outlines future\nresearch directions to advance the field. By synthesizing current methodologies\nand exploring future opportunities, this survey aims to understand sentiment\nanalysis in the AI and LLM context thoroughly.", "published": "2024-09-16 04:44:52", "link": "http://arxiv.org/abs/2409.09989v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "On the Diagram of Thought", "abstract": "Current large language models (LLMs) demonstrate impressive capabilities but\nstruggle with complex, multi-step reasoning tasks. Existing methods often\ntackle this by requiring external control mechanisms or multi-model\norchestration, which introduces system complexity and typically lacks formal\nguarantees of reasoning soundness. We introduce the Diagram of Thought (DoT), a\nframework wherein a single auto-regressive LLM internally constructs and\nnavigates a Directed Acyclic Graph (DAG). This DAG represents the iterative\nreasoning process, encompassing steps like proposing ideas, critiquing them,\nrefining based on feedback, and synthesizing conclusions. This\nself-orchestrated, self-contained process is guided by learned role-specific\ntokens (e.g., <proposer>, <critic>, <summarizer>) embedded within the standard\ngeneration loop, thereby eliminating external dependencies. Crucially, we\nestablish a rigorous mathematical foundation for DoT using Topos Theory. We\nformalize the reasoning DAG as a diagram within a suitable topos and prove that\nthe final synthesis step, aggregating validated information, corresponds\nsemantically to computing the colimit of the relevant sub-diagram. This\nformalization provides theoretical guarantees concerning the logical\nconsistency and robustness of the synthesized outcome. DoT thus offers a\nunified, self-contained, interpretable, efficient, and formally grounded\napproach designed to significantly advance the complex reasoning capabilities\nof LLMs.", "published": "2024-09-16 07:01:41", "link": "http://arxiv.org/abs/2409.10038v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MindGuard: Towards Accessible and Sitgma-free Mental Health First Aid\n  via Edge LLM", "abstract": "Mental health disorders are among the most prevalent diseases worldwide,\naffecting nearly one in four people. Despite their widespread impact, the\nintervention rate remains below 25%, largely due to the significant cooperation\nrequired from patients for both diagnosis and intervention. The core issue\nbehind this low treatment rate is stigma, which discourages over half of those\naffected from seeking help. This paper presents MindGuard, an accessible,\nstigma-free, and professional mobile mental healthcare system designed to\nprovide mental health first aid. The heart of MindGuard is an innovative edge\nLLM, equipped with professional mental health knowledge, that seamlessly\nintegrates objective mobile sensor data with subjective Ecological Momentary\nAssessment records to deliver personalized screening and intervention\nconversations. We conduct a broad evaluation of MindGuard using open datasets\nspanning four years and real-world deployment across various mobile devices\ninvolving 20 subjects for two weeks. Remarkably, MindGuard achieves results\ncomparable to GPT-4 and outperforms its counterpart with more than 10 times the\nmodel size. We believe that MindGuard paves the way for mobile LLM\napplications, potentially revolutionizing mental healthcare practices by\nsubstituting self-reporting and intervention conversations with passive,\nintegrated monitoring within daily life, thus ensuring accessible and\nstigma-free mental health support.", "published": "2024-09-16 07:58:56", "link": "http://arxiv.org/abs/2409.10064v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Trustworthiness in Retrieval-Augmented Generation Systems: A Survey", "abstract": "Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal\nparadigm in the development of Large Language Models (LLMs). While much of the\ncurrent research in this field focuses on performance optimization,\nparticularly in terms of accuracy and efficiency, the trustworthiness of RAG\nsystems remains an area still under exploration. From a positive perspective,\nRAG systems are promising to enhance LLMs by providing them with useful and\nup-to-date knowledge from vast external databases, thereby mitigating the\nlong-standing problem of hallucination. While from a negative perspective, RAG\nsystems are at the risk of generating undesirable contents if the retrieved\ninformation is either inappropriate or poorly utilized. To address these\nconcerns, we propose a unified framework that assesses the trustworthiness of\nRAG systems across six key dimensions: factuality, robustness, fairness,\ntransparency, accountability, and privacy. Within this framework, we thoroughly\nreview the existing literature on each dimension. Additionally, we create the\nevaluation benchmark regarding the six dimensions and conduct comprehensive\nevaluations for a variety of proprietary and open-source models. Finally, we\nidentify the potential challenges for future research based on our\ninvestigation results. Through this work, we aim to lay a structured foundation\nfor future investigations and provide practical insights for enhancing the\ntrustworthiness of RAG systems in real-world applications.", "published": "2024-09-16 09:06:44", "link": "http://arxiv.org/abs/2409.10102v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Self-Supervised Syllable Discovery Based on Speaker-Disentangled HuBERT", "abstract": "Self-supervised speech representation learning has become essential for\nextracting meaningful features from untranscribed audio. Recent advances\nhighlight the potential of deriving discrete symbols from the features\ncorrelated with linguistic units, which enables text-less training across\ndiverse tasks. In particular, sentence-level Self-Distillation of the\npretrained HuBERT (SD-HuBERT) induces syllabic structures within latent speech\nframe representations extracted from an intermediate Transformer layer. In\nSD-HuBERT, sentence-level representation is accumulated from speech frame\nfeatures through self-attention layers using a special CLS token. However, we\nobserve that the information aggregated in the CLS token correlates more with\nspeaker identity than with linguistic content. To address this, we propose a\nspeech-only self-supervised fine-tuning approach that separates syllabic units\nfrom speaker information. Our method introduces speaker perturbation as data\naugmentation and adopts a frame-level training objective to prevent the CLS\ntoken from aggregating paralinguistic information. Experimental results show\nthat our approach surpasses the current state-of-the-art method in most\nsyllable segmentation and syllabic unit quality metrics on Librispeech,\nunderscoring its effectiveness in promoting syllabic organization within\nspeech-only models.", "published": "2024-09-16 09:07:08", "link": "http://arxiv.org/abs/2409.10103v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Quantile Regression for Distributional Reward Models in RLHF", "abstract": "Reinforcement learning from human feedback (RLHF) has become a key method for\naligning large language models (LLMs) with human preferences through the use of\nreward models. However, traditional reward models typically generate point\nestimates, which oversimplify the diversity and complexity of human values and\npreferences. In this paper, we introduce Quantile Reward Models (QRMs), a novel\napproach to reward modeling that learns a distribution over rewards instead of\na single scalar value. Our method uses quantile regression to estimate a full,\npotentially multimodal distribution over preferences, providing a more powerful\nand nuanced representation of preferences. This distributional approach can\nbetter capture the diversity of human values, addresses label noise, and\naccommodates conflicting preferences by modeling them as distinct modes in the\ndistribution. Our experimental results show that QRM outperforms comparable\ntraditional point-estimate models on RewardBench. Furthermore, we demonstrate\nthat the additional information provided by the distributional estimates can be\nutilized in downstream applications, such as risk-aware reinforcement learning,\nresulting in LLM policies that generate fewer extremely negative responses. Our\ncode and model are released at https://github.com/Nicolinho/QRM.", "published": "2024-09-16 10:54:04", "link": "http://arxiv.org/abs/2409.10164v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "jina-embeddings-v3: Multilingual Embeddings With Task LoRA", "abstract": "We introduce jina-embeddings-v3, a novel text embedding model with 570\nmillion parameters, achieves state-of-the-art performance on multilingual data\nand long-context retrieval tasks, supporting context lengths of up to 8192\ntokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA)\nadapters to generate high-quality embeddings for query-document retrieval,\nclustering, classification, and text matching. Evaluation on the MTEB benchmark\nshows that jina-embeddings-v3 outperforms the latest proprietary embeddings\nfrom OpenAI and Cohere on English tasks, while achieving superior performance\ncompared to multilingual-e5-large-instruct across all multilingual tasks. With\na default output dimension of 1024, users can flexibly reduce the embedding\ndimensions to as low as 32 without compromising performance, enabled by\nMatryoshka Representation Learning.", "published": "2024-09-16 11:10:29", "link": "http://arxiv.org/abs/2409.10173v3", "categories": ["cs.CL", "cs.AI", "cs.IR", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Fit and Prune: Fast and Training-free Visual Token Pruning for\n  Multi-modal Large Language Models", "abstract": "Recent progress in Multimodal Large Language Models(MLLMs) often use large\nimage tokens to compensate the visual shortcoming of MLLMs, which not only\nexhibits obvious redundancy but also greatly exacerbates the already high\ncomputation. Token pruning is an effective solution for speeding up MLLMs, but\nwhen and how to drop tokens still remains a challenge. In this paper, we\npropose a novel and training-free approach for the effective visual token\npruning of MLLMs, termed FitPrune, which can quickly produce a complete pruning\nrecipe for MLLMs according to a pre-defined budget. Specifically, FitPrune\nconsiders token pruning as a statistical problem of MLLM and its objective is\nto find out an optimal pruning scheme that can minimize the divergence of the\nattention distributions before and after pruning. In practice, FitPrune can be\nquickly accomplished based on the attention statistics from a small batch of\ninference data, avoiding the expensive trials of MLLMs. According to the\npruning recipe, an MLLM can directly remove the redundant visual tokens of\ndifferent examples during inference. To validate FitPrune, we apply it to a set\nof recent MLLMs, including LLaVA-1.5, LLaVA-HR and LLaVA-NEXT, and conduct\nextensive experiments on a set of benchmarks. The experimental results show\nthat our FitPrune can not only reduce the computational complexity to a large\nextent, while retaining high performance, e.g., -54.9% FLOPs for LLaVA-NEXT\nwith only 0.5% accuracy drop. Notably, the pruning recipe can be obtained in\nabout 5 minutes. Our code is available at https://github.com/ywh187/FitPrune.", "published": "2024-09-16 11:43:19", "link": "http://arxiv.org/abs/2409.10197v2", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for\n  Empathetic Response Generation via a RL-Diffusion Framework", "abstract": "Empathetic response generation necessitates the integration of emotional and\nintentional dynamics to foster meaningful interactions. Existing research\neither neglects the intricate interplay between emotion and intent, leading to\nsuboptimal controllability of empathy, or resorts to large language models\n(LLMs), which incur significant computational overhead. In this paper, we\nintroduce ReflectDiffu, a lightweight and comprehensive framework for\nempathetic response generation. This framework incorporates emotion contagion\nto augment emotional expressiveness and employs an emotion-reasoning mask to\npinpoint critical emotional elements. Additionally, it integrates intent\nmimicry within reinforcement learning for refinement during diffusion. By\nharnessing an intent twice reflect the mechanism of\nExploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional\ndecision-making into precise intent actions, thereby addressing empathetic\nresponse misalignments stemming from emotional misrecognition. Through\nreflection, the framework maps emotional states to intents, markedly enhancing\nboth response empathy and flexibility. Comprehensive experiments reveal that\nReflectDiffu outperforms existing models regarding relevance, controllability,\nand informativeness, achieving state-of-the-art results in both automatic and\nhuman evaluations.", "published": "2024-09-16 13:56:17", "link": "http://arxiv.org/abs/2409.10289v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Instigating Cooperation among LLM Agents Using Adaptive Information\n  Modulation", "abstract": "This paper introduces a novel framework combining LLM agents as proxies for\nhuman strategic behavior with reinforcement learning (RL) to engage these\nagents in evolving strategic interactions within team environments. Our\napproach extends traditional agent-based simulations by using strategic LLM\nagents (SLA) and introducing dynamic and adaptive governance through a\npro-social promoting RL agent (PPA) that modulates information access across\nagents in a network, optimizing social welfare and promoting pro-social\nbehavior. Through validation in iterative games, including the prisoner\ndilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.\nThe PPA agent effectively learns to adjust information transparency, resulting\nin enhanced cooperation rates. This framework offers significant insights into\nAI-mediated social dynamics, contributing to the deployment of AI in real-world\nteam settings.", "published": "2024-09-16 15:15:51", "link": "http://arxiv.org/abs/2409.10372v3", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.GT"], "primary_category": "cs.AI"}
{"title": "Meta-Whisper: Speech-Based Meta-ICL for ASR on Low-Resource Languages", "abstract": "This paper presents Meta-Whisper, a novel approach to improve automatic\nspeech recognition (ASR) for low-resource languages using the Whisper model. By\nleveraging Meta In-Context Learning (Meta-ICL) and a k-Nearest Neighbors (KNN)\nalgorithm for sample selection, Meta-Whisper enhances Whisper's ability to\nrecognize speech in unfamiliar languages without extensive fine-tuning.\nExperiments on the ML-SUPERB dataset show that Meta-Whisper significantly\nreduces the Character Error Rate (CER) for low-resource languages compared to\nthe original Whisper model. This method offers a promising solution for\ndeveloping more adaptable multilingual ASR systems, particularly for languages\nwith limited resources.", "published": "2024-09-16 16:04:16", "link": "http://arxiv.org/abs/2409.10429v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Efficient Self-Learning Framework For Interactive Spoken Dialog\n  Systems", "abstract": "Dialog systems, such as voice assistants, are expected to engage with users\nin complex, evolving conversations. Unfortunately, traditional automatic speech\nrecognition (ASR) systems deployed in such applications are usually trained to\nrecognize each turn independently and lack the ability to adapt to the\nconversational context or incorporate user feedback. In this work, we introduce\na general framework for ASR in dialog systems that can go beyond learning from\nsingle-turn utterances and learn over time how to adapt to both explicit\nsupervision and implicit user feedback present in multi-turn conversations. We\naccomplish that by leveraging advances in student-teacher learning and\ncontext-aware dialog processing, and designing contrastive self-supervision\napproaches with Ohm, a new online hard-negative mining approach. We show that\nleveraging our new framework compared to traditional training leads to relative\nWER reductions of close to 10% in real-world dialog systems, and up to 26% on\npublic synthetic data.", "published": "2024-09-16 17:59:50", "link": "http://arxiv.org/abs/2409.10515v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios", "abstract": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV.", "published": "2024-09-16 17:36:50", "link": "http://arxiv.org/abs/2409.10593v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Exploring Fine-tuned Generative Models for Keyphrase Selection: A Case\n  Study for Russian", "abstract": "Keyphrase selection plays a pivotal role within the domain of scholarly\ntexts, facilitating efficient information retrieval, summarization, and\nindexing. In this work, we explored how to apply fine-tuned generative\ntransformer-based models to the specific task of keyphrase selection within\nRussian scientific texts. We experimented with four distinct generative models,\nsuch as ruT5, ruGPT, mT5, and mBART, and evaluated their performance in both\nin-domain and cross-domain settings. The experiments were conducted on the\ntexts of Russian scientific abstracts from four domains: mathematics & computer\nscience, history, medicine, and linguistics. The use of generative models,\nnamely mBART, led to gains in in-domain performance (up to 4.9% in BERTScore,\n9.0% in ROUGE-1, and 12.2% in F1-score) over three keyphrase extraction\nbaselines for the Russian language. Although the results for cross-domain usage\nwere significantly lower, they still demonstrated the capability to surpass\nbaseline performances in several cases, underscoring the promising potential\nfor further exploration and refinement in this research field.", "published": "2024-09-16 18:15:28", "link": "http://arxiv.org/abs/2409.10640v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; I.7.m; H.3.3"], "primary_category": "cs.CL"}
{"title": "A Bayesian Interpretation of Adaptive Low-Rank Adaptation", "abstract": "Motivated by the sensitivity-based importance score of the adaptive low-rank\nadaptation (AdaLoRA), we utilize more theoretically supported metrics,\nincluding the signal-to-noise ratio (SNR), along with the Improved Variational\nOnline Newton (IVON) optimizer, for adaptive parameter budget allocation. The\nresulting Bayesian counterpart not only has matched or surpassed the\nperformance of using the sensitivity-based importance metric but is also a\nfaster alternative to AdaLoRA with Adam. Our theoretical analysis reveals a\nsignificant connection between the two metrics, providing a Bayesian\nperspective on the efficacy of sensitivity as an importance score. Furthermore,\nour findings suggest that the magnitude, rather than the variance, is the\nprimary indicator of the importance of parameters.", "published": "2024-09-16 19:14:35", "link": "http://arxiv.org/abs/2409.10673v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Model-in-the-Loop (MILO): Accelerating Multimodal AI Data Annotation\n  with LLMs", "abstract": "The growing demand for AI training data has transformed data annotation into\na global industry, but traditional approaches relying on human annotators are\noften time-consuming, labor-intensive, and prone to inconsistent quality. We\npropose the Model-in-the-Loop (MILO) framework, which integrates AI/ML models\ninto the annotation process. Our research introduces a collaborative paradigm\nthat leverages the strengths of both professional human annotators and large\nlanguage models (LLMs). By employing LLMs as pre-annotation and real-time\nassistants, and judges on annotator responses, MILO enables effective\ninteraction patterns between human annotators and LLMs. Three empirical studies\non multimodal data annotation demonstrate MILO's efficacy in reducing handling\ntime, improving data quality, and enhancing annotator experiences. We also\nintroduce quality rubrics for flexible evaluation and fine-grained feedback on\nopen-ended annotations. The MILO framework has implications for accelerating\nAI/ML development, reducing reliance on human annotation alone, and promoting\nbetter alignment between human and machine values.", "published": "2024-09-16 20:05:57", "link": "http://arxiv.org/abs/2409.10702v2", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Self-supervised Speech Models for Word-Level Stuttered Speech Detection", "abstract": "Clinical diagnosis of stuttering requires an assessment by a licensed\nspeech-language pathologist. However, this process is time-consuming and\nrequires clinicians with training and experience in stuttering and fluency\ndisorders. Unfortunately, only a small percentage of speech-language\npathologists report being comfortable working with individuals who stutter,\nwhich is inadequate to accommodate for the 80 million individuals who stutter\nworldwide. Developing machine learning models for detecting stuttered speech\nwould enable universal and automated screening for stuttering, enabling speech\npathologists to identify and follow up with patients who are most likely to be\ndiagnosed with a stuttering speech disorder. Previous research in this area has\npredominantly focused on utterance-level detection, which is not sufficient for\nclinical settings where word-level annotation of stuttering is the norm. In\nthis study, we curated a stuttered speech dataset with word-level annotations\nand introduced a word-level stuttering speech detection model leveraging\nself-supervised speech models. Our evaluation demonstrates that our model\nsurpasses previous approaches in word-level stuttering speech detection.\nAdditionally, we conducted an extensive ablation analysis of our method,\nproviding insight into the most important aspects of adapting self-supervised\nspeech models for stuttered speech detection.", "published": "2024-09-16 20:18:20", "link": "http://arxiv.org/abs/2409.10704v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-Attention Limits Working Memory Capacity of Transformer-Based\n  Models", "abstract": "Recent work on Transformer-based large language models (LLMs) has revealed\nstriking limits in their working memory capacity, similar to what has been\nfound in human behavioral studies. Specifically, these models' performance\ndrops significantly on N-back tasks as N increases. However, there is still a\nlack of mechanistic interpretability as to why this phenomenon would arise.\nInspired by the executive attention theory from behavioral sciences, we\nhypothesize that the self-attention mechanism within Transformer-based models\nmight be responsible for their working memory capacity limits. To test this\nhypothesis, we train vanilla decoder-only transformers to perform N-back tasks\nand find that attention scores gradually aggregate to the N-back positions over\ntraining, suggesting that the model masters the task by learning a strategy to\npay attention to the relationship between the current position and the N-back\nposition. Critically, we find that the total entropy of the attention score\nmatrix increases as N increases, suggesting that the dispersion of attention\nscores might be the cause of the capacity limit observed in N-back tasks. Our\nfindings thus offer insights into the shared role of attention in both human\nand artificial intelligence. Moreover, the limitations of the self-attention\nmechanism revealed in the current study could inform future efforts to design\nmore powerful model architectures with enhanced working memory capacity and\ncognitive capabilities.", "published": "2024-09-16 20:38:35", "link": "http://arxiv.org/abs/2409.10715v2", "categories": ["cs.CL", "cs.AI", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Generalized Measures of Anticipation and Responsivity in Online Language\n  Processing", "abstract": "We introduce a generalization of classic information-theoretic measures of\npredictive uncertainty in online language processing, based on the simulation\nof expected continuations of incremental linguistic contexts. Our framework\nprovides a formal definition of anticipatory and responsive measures, and it\nequips experimenters with the tools to define new, more expressive measures\nbeyond standard next-symbol entropy and surprisal. While extracting these\nstandard quantities from language models is convenient, we demonstrate that\nusing Monte Carlo simulation to estimate alternative responsive and\nanticipatory measures pays off empirically: New special cases of our\ngeneralized formula exhibit enhanced predictive power compared to surprisal for\nhuman cloze completion probability as well as ELAN, LAN, and N400 amplitudes,\nand greater complementarity with surprisal in predicting reading times.", "published": "2024-09-16 21:05:15", "link": "http://arxiv.org/abs/2409.10728v2", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Harnessing Large Language Models: Fine-tuned BERT for Detecting\n  Charismatic Leadership Tactics in Natural Language", "abstract": "This work investigates the identification of Charismatic Leadership Tactics\n(CLTs) in natural language using a fine-tuned Bidirectional Encoder\nRepresentations from Transformers (BERT) model. Based on an own extensive\ncorpus of CLTs generated and curated for this task, our methodology entails\ntraining a machine learning model that is capable of accurately identifying the\npresence of these tactics in natural language. A performance evaluation is\nconducted to assess the effectiveness of our model in detecting CLTs. We find\nthat the total accuracy over the detection of all CLTs is 98.96\\% The results\nof this study have significant implications for research in psychology and\nmanagement, offering potential methods to simplify the currently elaborate\nassessment of charisma in texts.", "published": "2024-09-16 07:14:54", "link": "http://arxiv.org/abs/2409.18984v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lab-AI -- Retrieval-Augmented Language Model for Personalized Lab Test\n  Interpretation in Clinical Medicine", "abstract": "Accurate interpretation of lab results is crucial in clinical medicine, yet\nmost patient portals use universal normal ranges, ignoring factors like age and\ngender. This study introduces Lab-AI, an interactive system that offers\npersonalized normal ranges using Retrieval-Augmented Generation (RAG) from\ncredible health sources. Lab-AI has two modules: factor retrieval and normal\nrange retrieval. We tested these on 68 lab tests-30 with conditional factors\nand 38 without. For tests with factors, normal ranges depend on\npatient-specific information. Our results show that GPT-4-turbo with RAG\nachieved a 0.95 F1 score for factor retrieval and 0.993 accuracy for normal\nrange retrieval. GPT-4-turbo with RAG outperformed the best non-RAG system by\n29.1% in factor retrieval and showed 60.9% and 52.9% improvements in\nquestion-level and lab-level performance, respectively, for normal range\nretrieval. These findings highlight Lab-AI's potential to enhance patient\nunderstanding of lab results.", "published": "2024-09-16 20:36:17", "link": "http://arxiv.org/abs/2409.18986v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Improving Spoken Language Modeling with Phoneme Classification: A Simple\n  Fine-tuning Approach", "abstract": "Recent progress in Spoken Language Modeling has shown that learning language\ndirectly from speech is feasible. Generating speech through a pipeline that\noperates at the text level typically loses nuances, intonations, and non-verbal\nvocalizations. Modeling directly from speech opens up the path to more natural\nand expressive systems. On the other hand, speech-only systems require up to\nthree orders of magnitude more data to catch up to their text-based\ncounterparts in terms of their semantic abilities. We show that fine-tuning\nspeech representation models on phoneme classification leads to more\ncontext-invariant representations, and language models trained on these units\nachieve comparable lexical comprehension to ones trained on hundred times more\ndata.", "published": "2024-09-16 10:29:15", "link": "http://arxiv.org/abs/2410.00025v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "AI Conversational Interviewing: Transforming Surveys with LLMs as\n  Adaptive Interviewers", "abstract": "Traditional methods for eliciting people's opinions face a trade-off between\ndepth and scale: structured surveys enable large-scale data collection but\nlimit respondents' ability to voice their opinions in their own words, while\nconversational interviews provide deeper insights but are resource-intensive.\nThis study explores the potential of replacing human interviewers with large\nlanguage models (LLMs) to conduct scalable conversational interviews. Our goal\nis to assess the performance of AI Conversational Interviewing and to identify\nopportunities for improvement in a controlled environment. We conducted a\nsmall-scale, in-depth study with university students who were randomly assigned\nto a conversational interview by either AI or human interviewers, both\nemploying identical questionnaires on political topics. Various quantitative\nand qualitative measures assessed interviewer adherence to guidelines, response\nquality, participant engagement, and overall interview efficacy. The findings\nindicate the viability of AI Conversational Interviewing in producing quality\ndata comparable to traditional methods, with the added benefit of scalability.\nWe publish our data and materials for re-use and present specific\nrecommendations for effective implementation.", "published": "2024-09-16 16:03:08", "link": "http://arxiv.org/abs/2410.01824v2", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "2D or not 2D: How Does the Dimensionality of Gesture Representation\n  Affect 3D Co-Speech Gesture Generation?", "abstract": "Co-speech gestures are fundamental for communication. The advent of recent\ndeep learning techniques has facilitated the creation of lifelike, synchronous\nco-speech gestures for Embodied Conversational Agents. \"In-the-wild\" datasets,\naggregating video content from platforms like YouTube via human pose detection\ntechnologies, provide a feasible solution by offering 2D skeletal sequences\naligned with speech. Concurrent developments in lifting models enable the\nconversion of these 2D sequences into 3D gesture databases. However, it is\nimportant to note that the 3D poses estimated from the 2D extracted poses are,\nin essence, approximations of the ground-truth, which remains in the 2D domain.\nThis distinction raises questions about the impact of gesture representation\ndimensionality on the quality of generated motions - a topic that, to our\nknowledge, remains largely unexplored. Our study examines the effect of using\neither 2D or 3D joint coordinates as training data on the performance of\nspeech-to-gesture deep generative models. We employ a lifting model for\nconverting generated 2D pose sequences into 3D and assess how gestures created\ndirectly in 3D stack up against those initially generated in 2D and then\nconverted to 3D. We perform an objective evaluation using widely used metrics\nin the gesture generation field as well as a user study to qualitatively\nevaluate the different approaches.", "published": "2024-09-16 15:06:12", "link": "http://arxiv.org/abs/2409.10357v2", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "A Literature Review of Keyword Spotting Technologies for Urdu", "abstract": "This literature review surveys the advancements of keyword spotting (KWS)\ntechnologies, specifically focusing on Urdu, Pakistan's low-resource language\n(LRL), which has complex phonetics. Despite the global strides in speech\ntechnology, Urdu presents unique challenges requiring more tailored solutions.\nThe review traces the evolution from foundational Gaussian Mixture Models to\nsophisticated neural architectures like deep neural networks and transformers,\nhighlighting significant milestones such as integrating multi-task learning and\nself-supervised approaches that leverage unlabeled data. It examines emerging\ntechnologies' role in enhancing KWS systems' performance within multilingual\nand resource-constrained settings, emphasizing the need for innovations that\ncater to languages like Urdu. Thus, this review underscores the need for\ncontext-specific research addressing the inherent complexities of Urdu and\nsimilar URLs and the means of regions communicating through such languages for\na more inclusive approach to speech technology.", "published": "2024-09-16 11:39:10", "link": "http://arxiv.org/abs/2409.16317v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Study on Zero-shot Non-intrusive Speech Assessment using Large\n  Language Models", "abstract": "This work investigates two strategies for zero-shot non-intrusive speech\nassessment leveraging large language models. First, we explore the audio\nanalysis capabilities of GPT-4o. Second, we propose GPT-Whisper, which uses\nWhisper as an audio-to-text module and evaluates the naturalness of text via\ntargeted prompt engineering. We evaluate the assessment metrics predicted by\nGPT-4o and GPT-Whisper, examining their correlation with human-based quality\nand intelligibility assessments and the character error rate (CER) of automatic\nspeech recognition. Experimental results show that GPT-4o alone is less\neffective for audio analysis, while GPT-Whisper achieves higher prediction\naccuracy, has moderate correlation with speech quality and intelligibility, and\nhas higher correlation with CER. Compared to SpeechLMScore and DNSMOS,\nGPT-Whisper excels in intelligibility metrics, but performs slightly worse than\nSpeechLMScore in quality estimation. Furthermore, GPT-Whisper outperforms\nsupervised non-intrusive models MOS-SSL and MTI-Net in Spearman's rank\ncorrelation for CER of Whisper. These findings validate GPT-Whisper's potential\nfor zero-shot speech assessment without requiring additional training data.", "published": "2024-09-16 01:06:49", "link": "http://arxiv.org/abs/2409.09914v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DNN-based ensemble singing voice synthesis with interactions between\n  singers", "abstract": "We propose a singing voice synthesis (SVS) method for a more unified ensemble\nsinging voice by modeling interactions between singers. Most existing SVS\nmethods aim to synthesize a solo voice, and do not consider interactions\nbetween singers, i.e., adjusting one's own voice to the others' voices. Since\nthe production of ensemble voices from solo singing voices ignores the\ninteractions, it can degrade the unity of the vocal ensemble. Therefore, we\npropose a SVS that reproduces the interactions. It is based on an architecture\nthat uses musical scores of multiple voice parts, and loss functions that\nsimulate the interactions' effect to acoustic features. Experimental results\nshow that our methods improve the unity of the vocal ensemble.", "published": "2024-09-16 04:43:33", "link": "http://arxiv.org/abs/2409.09988v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "TBDM-Net: Bidirectional Dense Networks with Gender Information for\n  Speech Emotion Recognition", "abstract": "This paper presents a novel deep neural network-based architecture tailored\nfor Speech Emotion Recognition (SER). The architecture capitalises on dense\ninterconnections among multiple layers of bidirectional dilated convolutions. A\nlinear kernel dynamically fuses the outputs of these layers to yield the final\nemotion class prediction. This innovative architecture is denoted as TBDM-Net:\nTemporally-Aware Bi-directional Dense Multi-Scale Network. We conduct a\ncomprehensive performance evaluation of TBDM-Net, including an ablation study,\nacross six widely-acknowledged SER datasets for unimodal speech emotion\nrecognition. Additionally, we explore the influence of gender-informed emotion\nprediction by appending either golden or predicted gender labels to the\narchitecture's inputs or predictions. The implementation of TBDM-Net is\naccessible at: https://github.com/adrianastan/tbdm-net", "published": "2024-09-16 07:36:14", "link": "http://arxiv.org/abs/2409.10056v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "StyleTTS-ZS: Efficient High-Quality Zero-Shot Text-to-Speech Synthesis\n  with Distilled Time-Varying Style Diffusion", "abstract": "The rapid development of large-scale text-to-speech (TTS) models has led to\nsignificant advancements in modeling diverse speaker prosody and voices.\nHowever, these models often face issues such as slow inference speeds, reliance\non complex pre-trained neural codec representations, and difficulties in\nachieving naturalness and high similarity to reference speakers. To address\nthese challenges, this work introduces StyleTTS-ZS, an efficient zero-shot TTS\nmodel that leverages distilled time-varying style diffusion to capture diverse\nspeaker identities and prosodies. We propose a novel approach that represents\nhuman speech using input text and fixed-length time-varying discrete style\ncodes to capture diverse prosodic variations, trained adversarially with\nmulti-modal discriminators. A diffusion model is then built to sample this\ntime-varying style code for efficient latent diffusion. Using classifier-free\nguidance, StyleTTS-ZS achieves high similarity to the reference speaker in the\nstyle diffusion process. Furthermore, to expedite sampling, the style diffusion\nmodel is distilled with perceptual loss using only 10k samples, maintaining\nspeech quality and similarity while reducing inference speed by 90%. Our model\nsurpasses previous state-of-the-art large-scale zero-shot TTS models in both\nnaturalness and similarity, offering a 10-20 faster sampling speed, making it\nan attractive alternative for efficient large-scale zero-shot TTS systems. The\naudio demo, code and models are available at https://styletts-zs.github.io/.", "published": "2024-09-16 07:39:58", "link": "http://arxiv.org/abs/2409.10058v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker Contrastive Learning for Source Speaker Tracing", "abstract": "As a form of biometric authentication technology, the security of speaker\nverification systems is of utmost importance. However, SV systems are\ninherently vulnerable to various types of attacks that can compromise their\naccuracy and reliability. One such attack is voice conversion, which modifies a\npersons speech to sound like another person by altering various vocal\ncharacteristics. This poses a significant threat to SV systems. To address this\nchallenge, the Source Speaker Tracing Challenge in IEEE SLT2024 aims to\nidentify the source speaker information in manipulated speech signals.\nSpecifically, SSTC focuses on source speaker verification against voice\nconversion to determine whether two converted speech samples originate from the\nsame source speaker. In this study, we propose a speaker contrastive\nlearning-based approach for source speaker tracing to learn the latent source\nspeaker information in converted speech. To learn a more source-speaker-related\nrepresentation, we employ speaker contrastive loss during the training of the\nembedding extractor. This speaker contrastive loss helps identify the true\nsource speaker embedding among several distractor speaker embeddings, enabling\nthe embedding extractor to learn the potentially possessing source speaker\ninformation present in the converted speech. Experiments demonstrate that our\nproposed speaker contrastive learning system achieves the lowest EER of 16.788%\non the challenge test set, securing first place in the challenge.", "published": "2024-09-16 08:22:11", "link": "http://arxiv.org/abs/2409.10072v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Room impulse response prototyping using receiver distance estimations\n  for high quality room equalisation algorithms", "abstract": "Room equalisation aims to increase the quality of loudspeaker reproduction in\nreverberant environments, compensating for colouration caused by imperfect room\nreflections and frequency dependant loudspeaker directivity. A common technique\nin the field of room equalisation, is to invert a prototype Room Impulse\nResponse (RIR). Rather than inverting a single RIR at the listening position, a\nprototype response is composed of several responses distributed around the\nlistening area. This paper proposes a method of impulse response prototyping,\nusing estimated receiver positions, to form a weighted average prototype\nresponse. A method of receiver distance estimation is described, supporting the\nimplementation of the prototype RIR. The proposed prototyping method is\ncompared to other methods by measuring their post equalisation spectral\ndeviation at several positions in a simulated room.", "published": "2024-09-16 09:48:54", "link": "http://arxiv.org/abs/2409.10131v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "RF-GML: Reference-Free Generative Machine Listener", "abstract": "This paper introduces a novel reference-free (RF) audio quality metric called\nthe RF-Generative Machine Listener (RF-GML), designed to evaluate coded mono,\nstereo, and binaural audio at a 48 kHz sample rate. RF-GML leverages transfer\nlearning from a state-of-the-art full-reference (FR) Generative Machine\nListener (GML) with minimal architectural modifications. The term \"generative\"\nrefers to the model's ability to generate an arbitrary number of simulated\nlistening scores. Unlike existing RF models, RF-GML accurately predicts\nsubjective quality scores across diverse content types and codecs. Extensive\nevaluations demonstrate its superiority in rating unencoded audio and\ndistinguishing different levels of coding artifacts. RF-GML's performance and\nversatility make it a valuable tool for coded audio quality assessment and\nmonitoring in various applications, all without the need for a reference\nsignal.", "published": "2024-09-16 12:02:17", "link": "http://arxiv.org/abs/2409.10210v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech as a Biomarker for Disease Detection", "abstract": "Speech is a rich biomarker that encodes substantial information about the\nhealth of a speaker, and thus it has been proposed for the detection of\nnumerous diseases, achieving promising results. However, questions remain about\nwhat the models trained for the automatic detection of these diseases are\nactually learning and the basis for their predictions, which can significantly\nimpact patients' lives. This work advocates for an interpretable health model,\nsuitable for detecting several diseases, motivated by the observation that\nspeech-affecting disorders often have overlapping effects on speech signals. A\nframework is presented that first defines \"reference speech\" and then leverages\nthis definition for disease detection. Reference speech is characterized\nthrough reference intervals, i.e., the typical values of clinically meaningful\nacoustic and linguistic features derived from a reference population. This\nnovel approach in the field of speech as a biomarker is inspired by the use of\nreference intervals in clinical laboratory science. Deviations of new speakers\nfrom this reference model are quantified and used as input to detect\nAlzheimer's and Parkinson's disease. The classification strategy explored is\nbased on Neural Additive Models, a type of glass-box neural network, which\nenables interpretability. The proposed framework for reference speech\ncharacterization and disease detection is designed to support the medical\ncommunity by providing clinically meaningful explanations that can serve as a\nvaluable second opinion.", "published": "2024-09-16 12:24:29", "link": "http://arxiv.org/abs/2409.10230v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "oboVox Far Field Speaker Recognition: A Novel Data Augmentation Approach\n  with Pretrained Models", "abstract": "In this study, we address the challenge of speaker recognition using a novel\ndata augmentation technique of adding noise to enrollment files. This technique\nefficiently aligns the sources of test and enrollment files, enhancing\ncomparability. Various pre-trained models were employed, with the resnet model\nachieving the highest DCF of 0.84 and an EER of 13.44. The augmentation\ntechnique notably improved these results to 0.75 DCF and 12.79 EER for the\nresnet model. Comparative analysis revealed the superiority of resnet over\nmodels such as ECPA, Mel-spectrogram, Payonnet, and Titanet large. Results,\nalong with different augmentation schemes, contribute to the success of RoboVox\nfar-field speaker recognition in this paper", "published": "2024-09-16 12:44:21", "link": "http://arxiv.org/abs/2409.10240v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Ultra-Low Latency Speech Enhancement - A Comprehensive Study", "abstract": "Speech enhancement models should meet very low latency requirements typically\nsmaller than 5 ms for hearing assistive devices. While various low-latency\ntechniques have been proposed, comparing these methods in a controlled setup\nusing DNNs remains blank. Previous papers have variations in task, training\ndata, scripts, and evaluation settings, which make fair comparison impossible.\nMoreover, all methods are tested on small, simulated datasets, making it\ndifficult to fairly assess their performance in real-world conditions, which\ncould impact the reliability of scientific findings. To address these issues,\nwe comprehensively investigate various low-latency techniques using consistent\ntraining on large-scale data and evaluate with more relevant metrics on\nreal-world data. Specifically, we explore the effectiveness of asymmetric\nwindows, learnable windows, adaptive time domain filterbanks, and the\nfuture-frame prediction technique. Additionally, we examine whether increasing\nthe model size can compensate for the reduced window size, as well as the novel\nMamba architecture in low-latency environments.", "published": "2024-09-16 15:06:47", "link": "http://arxiv.org/abs/2409.10358v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Leveraging Joint Spectral and Spatial Learning with MAMBA for\n  Multichannel Speech Enhancement", "abstract": "In multichannel speech enhancement, effectively capturing spatial and\nspectral information across different microphones is crucial for noise\nreduction. Traditional methods, such as CNN or LSTM, attempt to model the\ntemporal dynamics of full-band and sub-band spectral and spatial features.\nHowever, these approaches face limitations in fully modeling complex temporal\ndependencies, especially in dynamic acoustic environments. To overcome these\nchallenges, we modify the current advanced model McNet by introducing an\nimproved version of Mamba, a state-space model, and further propose MCMamba.\nMCMamba has been completely reengineered to integrate full-band and narrow-band\nspatial information with sub-band and full-band spectral features, providing a\nmore comprehensive approach to modeling spatial and spectral information. Our\nexperimental results demonstrate that MCMamba significantly improves the\nmodeling of spatial and spectral features in multichannel speech enhancement,\noutperforming McNet and achieving state-of-the-art performance on the CHiME-3\ndataset. Additionally, we find that Mamba performs exceptionally well in\nmodeling spectral information.", "published": "2024-09-16 15:17:44", "link": "http://arxiv.org/abs/2409.10376v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic\n  Music Generated via Text-to-Music Models", "abstract": "Text-To-Music (TTM) models have recently revolutionized the automatic music\ngeneration research field. Specifically, by reaching superior performances to\nall previous state-of-the-art models and by lowering the technical proficiency\nneeded to use them. Due to these reasons, they have readily started to be\nadopted for commercial uses and music production practices. This widespread\ndiffusion of TTMs poses several concerns regarding copyright violation and\nrightful attribution, posing the need of serious consideration of them by the\naudio forensics community. In this paper, we tackle the problem of detection\nand attribution of TTM-generated data. We propose a dataset, FakeMusicCaps that\ncontains several versions of the music-caption pairs dataset MusicCaps\nre-generated via several state-of-the-art TTM techniques. We evaluate the\nproposed dataset by performing initial experiments regarding the detection and\nattribution of TTM-generated audio.", "published": "2024-09-16 19:31:39", "link": "http://arxiv.org/abs/2409.10684v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Investigating Training Objectives for Generative Speech Enhancement", "abstract": "Generative speech enhancement has recently shown promising advancements in\nimproving speech quality in noisy environments. Multiple diffusion-based\nframeworks exist, each employing distinct training objectives and learning\ntechniques. This paper aims to explain the differences between these frameworks\nby focusing our investigation on score-based generative models and the\nSchr\\\"odinger bridge. We conduct a series of comprehensive experiments to\ncompare their performance and highlight differing training behaviors.\nFurthermore, we propose a novel perceptual loss function tailored for the\nSchr\\\"odinger bridge framework, demonstrating enhanced performance and improved\nperceptual quality of the enhanced speech signals. All experimental code and\npre-trained models are publicly available to facilitate further research and\ndevelopment in this domain.", "published": "2024-09-16 21:47:52", "link": "http://arxiv.org/abs/2409.10753v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Automatic Assessment of Self-Supervised Speech Models using Rank", "abstract": "This study explores using embedding rank as an unsupervised evaluation metric\nfor general-purpose speech encoders trained via self-supervised learning (SSL).\nTraditionally, assessing the performance of these encoders is\nresource-intensive and requires labeled data from the downstream tasks.\nInspired by the vision domain, where embedding rank has shown promise for\nevaluating image encoders without tuning on labeled downstream data, this work\nexamines its applicability in the speech domain, considering the temporal\nnature of the signals. The findings indicate rank correlates with downstream\nperformance within encoder layers across various downstream tasks and for in-\nand out-of-domain scenarios. However, rank does not reliably predict the\nbest-performing layer for specific downstream tasks, as lower-ranked layers can\noutperform higher-ranked ones. Despite this limitation, the results suggest\nthat embedding rank can be a valuable tool for monitoring training progress in\nSSL speech models, offering a less resource-demanding alternative to\ntraditional evaluation methods.", "published": "2024-09-16 23:49:41", "link": "http://arxiv.org/abs/2409.10787v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploring Prediction Targets in Masked Pre-Training for Speech\n  Foundation Models", "abstract": "Speech foundation models, such as HuBERT and its variants, are pre-trained on\nlarge amounts of unlabeled speech data and then used for a range of downstream\ntasks. These models use a masked prediction objective, where the model learns\nto predict information about masked input segments from the unmasked context.\nThe choice of prediction targets in this framework impacts their performance on\ndownstream tasks. For instance, models pre-trained with targets that capture\nprosody learn representations suited for speaker-related tasks, while those\npre-trained with targets that capture phonetics learn representations suited\nfor content-related tasks. Moreover, prediction targets can differ in the level\nof detail they capture. Models pre-trained with targets that encode\nfine-grained acoustic features perform better on tasks like denoising, while\nthose pre-trained with targets focused on higher-level abstractions are more\neffective for content-related tasks. Despite the importance of prediction\ntargets, the design choices that affect them have not been thoroughly studied.\nThis work explores the design choices and their impact on downstream task\nperformance. Our results indicate that the commonly used design choices for\nHuBERT can be suboptimal. We propose approaches to create more informative\nprediction targets and demonstrate their effectiveness through improvements\nacross various downstream tasks.", "published": "2024-09-16 23:51:42", "link": "http://arxiv.org/abs/2409.10788v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker-IPL: Unsupervised Learning of Speaker Characteristics with\n  i-Vector based Pseudo-Labels", "abstract": "Iterative self-training, or iterative pseudo-labeling (IPL) -- using an\nimproved model from the current iteration to provide pseudo-labels for the next\niteration -- has proven to be a powerful approach to enhance the quality of\nspeaker representations. Recent applications of IPL in unsupervised speaker\nrecognition start with representations extracted from very elaborate\nself-supervised methods (e.g., DINO). However, training such strong\nself-supervised models is not straightforward (they require hyper-parameter\ntuning and may not generalize to out-of-domain data) and, moreover, may not be\nneeded at all. To this end, we show that the simple, well-studied, and\nestablished i-vector generative model is enough to bootstrap the IPL process\nfor the unsupervised learning of speaker representations. We also\nsystematically study the impact of other components on the IPL process, which\nincludes the initial model, the encoder, augmentations, the number of clusters,\nand the clustering algorithm. Remarkably, we find that even with a simple and\nsignificantly weaker initial model like i-vector, IPL can still achieve speaker\nverification performance that rivals state-of-the-art methods.", "published": "2024-09-16 23:53:57", "link": "http://arxiv.org/abs/2409.10791v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DiffATR: Diffusion-based Generative Modeling for Audio-Text Retrieval", "abstract": "Existing audio-text retrieval (ATR) methods are essentially discriminative\nmodels that aim to maximize the conditional likelihood, represented as\np(candidates|query). Nevertheless, this methodology fails to consider the\nintrinsic data distribution p(query), leading to difficulties in discerning\nout-of-distribution data. In this work, we attempt to tackle this constraint\nthrough a generative perspective and model the relationship between audio and\ntext as their joint probability p(candidates,query). To this end, we present a\ndiffusion-based ATR framework (DiffATR), which models ATR as an iterative\nprocedure that progressively generates joint distribution from noise.\nThroughout its training phase, DiffATR is optimized from both generative and\ndiscriminative viewpoints: the generator is refined through a generation loss,\nwhile the feature extractor benefits from a contrastive loss, thus combining\nthe merits of both methodologies. Experiments on the AudioCaps and Clotho\ndatasets with superior performances, verify the effectiveness of our approach.\nNotably, without any alterations, our DiffATR consistently exhibits strong\nperformance in out-of-domain retrieval settings.", "published": "2024-09-16 06:33:26", "link": "http://arxiv.org/abs/2409.10025v2", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-Driven Reinforcement Learning for Head-Orientation in Naturalistic\n  Environments", "abstract": "Although deep reinforcement learning (DRL) approaches in audio signal\nprocessing have seen substantial progress in recent years, audio-driven DRL for\ntasks such as navigation, gaze control and head-orientation control in the\ncontext of human-robot interaction have received little attention. Here, we\npropose an audio-driven DRL framework in which we utilise deep Q-learning to\ndevelop an autonomous agent that orients towards a talker in the acoustic\nenvironment based on stereo speech recordings. Our results show that the agent\nlearned to perform the task at a near perfect level when trained on speech\nsegments in anechoic environments (that is, without reverberation). The\npresence of reverberation in naturalistic acoustic environments affected the\nagent's performance, although the agent still substantially outperformed a\nbaseline, randomly acting agent. Finally, we quantified the degree of\ngeneralization of the proposed DRL approach across naturalistic acoustic\nenvironments. Our experiments revealed that policies learned by agents trained\non medium or high reverb environments generalized to low reverb environments,\nbut policies learned by agents trained on anechoic or low reverb environments\ndid not generalize to medium or high reverb environments. Taken together, this\nstudy demonstrates the potential of audio-driven DRL for tasks such as\nhead-orientation control and highlights the need for training strategies that\nenable robust generalization across environments for real-world audio-driven\nDRL applications.", "published": "2024-09-16 07:20:33", "link": "http://arxiv.org/abs/2409.10048v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Optimizing Dysarthria Wake-Up Word Spotting: An End-to-End Approach for\n  SLT 2024 LRDWWS Challenge", "abstract": "Speech has emerged as a widely embraced user interface across diverse\napplications. However, for individuals with dysarthria, the inherent\nvariability in their speech poses significant challenges. This paper presents\nan end-to-end Pretrain-based Dual-filter Dysarthria Wake-up word Spotting\n(PD-DWS) system for the SLT 2024 Low-Resource Dysarthria Wake-Up Word Spotting\nChallenge. Specifically, our system improves performance from two key\nperspectives: audio modeling and dual-filter strategy. For audio modeling, we\npropose an innovative 2branch-d2v2 model based on the pre-trained data2vec2\n(d2v2), which can simultaneously model automatic speech recognition (ASR) and\nwake-up word spotting (WWS) tasks through a unified multi-task finetuning\nparadigm. Additionally, a dual-filter strategy is introduced to reduce the\nfalse accept rate (FAR) while maintaining the same false reject rate (FRR).\nExperimental results demonstrate that our PD-DWS system achieves an FAR of\n0.00321 and an FRR of 0.005, with a total score of 0.00821 on the test-B eval\nset, securing first place in the challenge.", "published": "2024-09-16 08:26:50", "link": "http://arxiv.org/abs/2409.10076v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Emo-DPO: Controllable Emotional Speech Synthesis through Direct\n  Preference Optimization", "abstract": "Current emotional text-to-speech (TTS) models predominantly conduct\nsupervised training to learn the conversion from text and desired emotion to\nits emotional speech, focusing on a single emotion per text-speech pair. These\nmodels only learn the correct emotional outputs without fully comprehending\nother emotion characteristics, which limits their capabilities of capturing the\nnuances between different emotions. We propose a controllable Emo-DPO approach,\nwhich employs direct preference optimization to differentiate subtle emotional\nnuances between emotions through optimizing towards preferred emotions over\nless preferred emotional ones. Instead of relying on traditional neural\narchitectures used in existing emotional TTS models, we propose utilizing the\nemotion-aware LLM-TTS neural architecture to leverage LLMs' in-context learning\nand instruction-following capabilities. Comprehensive experiments confirm that\nour proposed method outperforms the existing baselines.", "published": "2024-09-16 10:41:36", "link": "http://arxiv.org/abs/2409.10157v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "DreamHead: Learning Spatial-Temporal Correspondence via Hierarchical\n  Diffusion for Audio-driven Talking Head Synthesis", "abstract": "Audio-driven talking head synthesis strives to generate lifelike video\nportraits from provided audio. The diffusion model, recognized for its superior\nquality and robust generalization, has been explored for this task. However,\nestablishing a robust correspondence between temporal audio cues and\ncorresponding spatial facial expressions with diffusion models remains a\nsignificant challenge in talking head generation. To bridge this gap, we\npresent DreamHead, a hierarchical diffusion framework that learns\nspatial-temporal correspondences in talking head synthesis without compromising\nthe model's intrinsic quality and adaptability.~DreamHead learns to predict\ndense facial landmarks from audios as intermediate signals to model the spatial\nand temporal correspondences.~Specifically, a first hierarchy of\naudio-to-landmark diffusion is first designed to predict temporally smooth and\naccurate landmark sequences given audio sequence signals. Then, a second\nhierarchy of landmark-to-image diffusion is further proposed to produce\nspatially consistent facial portrait videos, by modeling spatial\ncorrespondences between the dense facial landmark and appearance. Extensive\nexperiments show that proposed DreamHead can effectively learn spatial-temporal\nconsistency with the designed hierarchical diffusion and produce high-fidelity\naudio-driven talking head videos for multiple identities.", "published": "2024-09-16 13:44:20", "link": "http://arxiv.org/abs/2409.10281v1", "categories": ["cs.MM", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "MusicLIME: Explainable Multimodal Music Understanding", "abstract": "Multimodal models are critical for music understanding tasks, as they capture\nthe complex interplay between audio and lyrics. However, as these models become\nmore prevalent, the need for explainability grows-understanding how these\nsystems make decisions is vital for ensuring fairness, reducing bias, and\nfostering trust. In this paper, we introduce MusicLIME, a model-agnostic\nfeature importance explanation method designed for multimodal music models.\nUnlike traditional unimodal methods, which analyze each modality separately\nwithout considering the interaction between them, often leading to incomplete\nor misleading explanations, MusicLIME reveals how audio and lyrical features\ninteract and contribute to predictions, providing a holistic view of the\nmodel's decision-making. Additionally, we enhance local explanations by\naggregating them into global explanations, giving users a broader perspective\nof model behavior. Through this work, we contribute to improving the\ninterpretability of multimodal music models, empowering users to make informed\nchoices, and fostering more equitable, fair, and transparent music\nunderstanding systems.", "published": "2024-09-16 17:28:21", "link": "http://arxiv.org/abs/2409.10496v5", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Mitigating Sex Bias in Audio Data-driven COPD and COVID-19 Breathing\n  Pattern Detection Models", "abstract": "In the healthcare industry, researchers have been developing machine learning\nmodels to automate diagnosing patients with respiratory illnesses based on\ntheir breathing patterns. However, these models do not consider the demographic\nbiases, particularly sex bias, that often occur when models are trained with a\nskewed patient dataset. Hence, it is essential in such an important industry to\nreduce this bias so that models can make fair diagnoses. In this work, we\nexamine the bias in models used to detect breathing patterns of two major\nrespiratory diseases, i.e., chronic obstructive pulmonary disease (COPD) and\nCOVID-19. Using decision tree models trained with audio recordings of breathing\npatterns obtained from two open-source datasets consisting of 29 COPD and 680\nCOVID-19-positive patients, we analyze the effect of sex bias on the models.\nWith a threshold optimizer and two constraints (demographic parity and\nequalized odds) to mitigate the bias, we witness 81.43% (demographic parity\ndifference) and 71.81% (equalized odds difference) improvements. These findings\nare statistically significant.", "published": "2024-09-16 19:20:11", "link": "http://arxiv.org/abs/2409.10677v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Personalized Speech Emotion Recognition in Human-Robot Interaction using\n  Vision Transformers", "abstract": "Emotions are an essential element in verbal communication, so understanding\nindividuals' affect during a human-robot interaction (HRI) becomes imperative.\nThis paper investigates the application of vision transformer models, namely\nViT (Vision Transformers) and BEiT (BERT Pre-Training of Image Transformers)\npipelines, for Speech Emotion Recognition (SER) in HRI. The focus is to\ngeneralize the SER models for individual speech characteristics by fine-tuning\nthese models on benchmark datasets and exploiting ensemble methods. For this\npurpose, we collected audio data from different human subjects having\npseudo-naturalistic conversations with the NAO robot. We then fine-tuned our\nViT and BEiT-based models and tested these models on unseen speech samples from\nthe participants. In the results, we show that fine-tuning vision transformers\non benchmark datasets and and then using either these already fine-tuned models\nor ensembling ViT/BEiT models gets us the highest classification accuracies per\nindividual when it comes to identifying four primary emotions from their\nspeech: neutral, happy, sad, and angry, as compared to fine-tuning vanilla-ViTs\nor BEiTs.", "published": "2024-09-16 19:34:34", "link": "http://arxiv.org/abs/2409.10687v3", "categories": ["eess.AS", "cs.HC", "cs.RO", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Stimulus Modality Matters: Impact of Perceptual Evaluations from\n  Different Modalities on Speech Emotion Recognition System Performance", "abstract": "Speech Emotion Recognition (SER) systems rely on speech input and emotional\nlabels annotated by humans. However, various emotion databases collect\nperceptional evaluations in different ways. For instance, the IEMOCAP dataset\nuses video clips with sounds for annotators to provide their emotional\nperceptions. However, the most significant English emotion dataset, the\nMSP-PODCAST, only provides speech for raters to choose the emotional ratings.\nNevertheless, using speech as input is the standard approach to training SER\nsystems. Therefore, the open question is the emotional labels elicited by which\nscenarios are the most effective for training SER systems. We comprehensively\ncompare the effectiveness of SER systems trained with labels elicited by\ndifferent modality stimuli and evaluate the SER systems on various testing\nconditions. Also, we introduce an all-inclusive label that combines all labels\nelicited by various modalities. We show that using labels elicited by\nvoice-only stimuli for training yields better performance on the test set,\nwhereas labels elicited by voice-only stimuli.", "published": "2024-09-16 22:32:22", "link": "http://arxiv.org/abs/2409.10762v2", "categories": ["eess.AS", "cs.MM", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Machine listening in a neonatal intensive care unit", "abstract": "Oxygenators, alarm devices, and footsteps are some of the most common sound\nsources in a hospital. Detecting them has scientific value for environmental\npsychology but comes with challenges of its own: namely, privacy preservation\nand limited labeled data. In this paper, we address these two challenges via a\ncombination of edge computing and cloud computing. For privacy preservation, we\nhave designed an acoustic sensor which computes third-octave spectrograms on\nthe fly instead of recording audio waveforms. For sample-efficient machine\nlearning, we have repurposed a pretrained audio neural network (PANN) via\nspectral transcoding and label space adaptation. A small-scale study in a\nneonatological intensive care unit (NICU) confirms that the time series of\ndetected events align with another modality of measurement: i.e., electronic\nbadges for parents and healthcare professionals. Hence, this paper demonstrates\nthe feasibility of polyphonic machine listening in a hospital ward while\nguaranteeing privacy by design.", "published": "2024-09-16 09:19:19", "link": "http://arxiv.org/abs/2409.11439v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TCG CREST System Description for the Second DISPLACE Challenge", "abstract": "In this report, we describe the speaker diarization (SD) and language\ndiarization (LD) systems developed by our team for the Second DISPLACE\nChallenge, 2024. Our contributions were dedicated to Track 1 for SD and Track 2\nfor LD in multilingual and multi-speaker scenarios. We investigated different\nspeech enhancement techniques, voice activity detection (VAD) techniques,\nunsupervised domain categorization, and neural embedding extraction\narchitectures. We also exploited the fusion of various embedding extraction\nmodels. We implemented our system with the open-source SpeechBrain toolkit. Our\nfinal submissions use spectral clustering for both the speaker and language\ndiarization. We achieve about $7\\%$ relative improvement over the challenge\nbaseline in Track 1. We did not obtain improvement over the challenge baseline\nin Track 2.", "published": "2024-09-16 05:13:34", "link": "http://arxiv.org/abs/2409.15356v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-Tuning Spectral Clustering for Speaker Diarization", "abstract": "Spectral clustering has proven effective in grouping speech representations\nfor speaker diarization tasks, although post-processing the affinity matrix\nremains difficult due to the need for careful tuning before constructing the\nLaplacian. In this study, we present a novel pruning algorithm to create a\nsparse affinity matrix called \\emph{spectral clustering on p-neighborhood\nretained affinity matrix} (SC-pNA). Our method improves on node-specific fixed\nneighbor selection by allowing a variable number of neighbors, eliminating the\nneed for external tuning data as the pruning parameters are derived directly\nfrom the affinity matrix. SC-pNA does so by identifying two clusters in every\nrow of the initial affinity matrix, and retains only the top $p\\%$ similarity\nscores from the cluster containing larger similarities. Spectral clustering is\nperformed subsequently, with the number of clusters determined as the maximum\neigengap. Experimental results on the challenging DIHARD-III dataset highlight\nthe superiority of SC-pNA, which is also computationally more efficient than\nexisting auto-tuning approaches.", "published": "2024-09-16 05:07:47", "link": "http://arxiv.org/abs/2410.00023v1", "categories": ["eess.SP", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
