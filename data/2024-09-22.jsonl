{"title": "SARF: Enhancing Stock Market Prediction with Sentiment-Augmented Random Forest", "abstract": "Stock trend forecasting, a challenging problem in the financial domain,\ninvolves ex-tensive data and related indicators. Relying solely on empirical\nanalysis often yields unsustainable and ineffective results. Machine learning\nresearchers have demonstrated that the application of random forest algorithm\ncan enhance predictions in this context, playing a crucial auxiliary role in\nforecasting stock trends. This study introduces a new approach to stock market\nprediction by integrating sentiment analysis using FinGPT generative AI model\nwith the traditional Random Forest model. The proposed technique aims to\noptimize the accuracy of stock price forecasts by leveraging the nuanced\nunderstanding of financial sentiments provided by FinGPT. We present a new\nmethodology called \"Sentiment-Augmented Random Forest\" (SARF), which\nin-corporates sentiment features into the Random Forest framework. Our\nexperiments demonstrate that SARF outperforms conventional Random Forest and\nLSTM models with an average accuracy improvement of 9.23% and lower prediction\nerrors in pre-dicting stock market movements.", "published": "2024-09-22 20:22:10", "link": "http://arxiv.org/abs/2410.07143v1", "categories": ["q-fin.ST", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "MQM-APE: Toward High-Quality Error Annotation Predictors with Automatic\n  Post-Editing in LLM Translation Evaluators", "abstract": "Large Language Models (LLMs) have shown significant potential as judges for\nMachine Translation (MT) quality assessment, providing both scores and\nfine-grained feedback. Although approaches such as GEMBA-MQM have shown\nstate-of-the-art performance on reference-free evaluation, the predicted errors\ndo not align well with those annotated by human, limiting their\ninterpretability as feedback signals. To enhance the quality of error\nannotations predicted by LLM evaluators, we introduce a universal and\ntraining-free framework, $\\textbf{MQM-APE}$, based on the idea of filtering out\nnon-impactful errors by Automatically Post-Editing (APE) the original\ntranslation based on each error, leaving only those errors that contribute to\nquality improvement. Specifically, we prompt the LLM to act as 1)\n$\\textit{evaluator}$ to provide error annotations, 2) $\\textit{post-editor}$ to\ndetermine whether errors impact quality improvement and 3) $\\textit{pairwise\nquality verifier}$ as the error filter. Experiments show that our approach\nconsistently improves both the reliability and quality of error spans against\nGEMBA-MQM, across eight LLMs in both high- and low-resource languages.\nOrthogonal to trained approaches, MQM-APE complements translation-specific\nevaluators such as Tower, highlighting its broad applicability. Further\nanalysis confirms the effectiveness of each module and offers valuable insights\ninto evaluator design and LLMs selection.", "published": "2024-09-22 06:43:40", "link": "http://arxiv.org/abs/2409.14335v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "More Effective LLM Compressed Tokens with Uniformly Spread Position\n  Identifiers and Compression Loss", "abstract": "Compressing Transformer inputs into compressd tokens allows running LLMs with\nimproved speed and cost efficiency. Based on the compression method ICAE, we\ncarefully examine the position identifier choices for compressed tokens and\nalso propose a new compression loss. We demonstrate empirically that our\nproposed methods achieve significantly higher compression ratios (15x compared\nto 4x for ICAE), while being able to attain comparable reconstruction\nperformance.", "published": "2024-09-22 08:51:18", "link": "http://arxiv.org/abs/2409.14364v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Ability of Large Language Models to Evaluate Constraint-satisfaction\n  in Agent Responses to Open-ended Requests", "abstract": "Generative AI agents are often expected to respond to complex user requests\nthat have No One Right Answer (NORA), e.g., \"design a vegetarian meal plan\nbelow 1800 calories\". Such requests may entail a set of constraints that the\nagent should adhere to. To successfully develop agents for NORA scenarios, an\naccurate automatic evaluation framework is essential, and specifically - one\ncapable of validating the satisfaction of constraints in the agent's response.\nRecently, large language models (LLMs) have been adopted as versatile\nevaluators for many NORA tasks, but their ability to evaluate\nconstraint-satisfaction in generated text remains unclear. To study this, we\ndevelop and release a novel Arithmetic Constraint-Satisfaction (ACS)\nbenchmarking dataset. The dataset consists of complex user requests with\ncorresponding constraints, agent responses and human labels indicating each\nconstraint's satisfaction level in the response. A unique property of this\ndataset is that validating many of its constraints requires reviewing the\nresponse as a whole (in contrast to many other benchmarks that require the\nvalidation of a single independent item). Moreover, it assesses LLMs in\nperforming reasoning, in-context data extraction, arithmetic calculations, and\ncounting. We then benchmark both open and proprietary LLMs on evaluating\nconstraint-satisfaction, and show that most models still have a significant\nheadroom for improvement, and that errors primarily stem from reasoning issues.\nIn addition, most models exhibit a skewed constraint-satisfaction prediction\npattern, with higher accuracy where the ground-truth label is \"satisfied\".\nLastly, few-shot prompting for our task proved to be rather challenging, since\nmany of the studied models showed a degradation in performance when it was\nintroduced.", "published": "2024-09-22 09:27:42", "link": "http://arxiv.org/abs/2409.14371v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "J2N -- Nominal Adjective Identification and its Application", "abstract": "This paper explores the challenges posed by nominal adjectives (NAs) in\nnatural language processing (NLP) tasks, particularly in part-of-speech (POS)\ntagging. We propose treating NAs as a distinct POS tag, \"JN,\" and investigate\nits impact on POS tagging, BIO chunking, and coreference resolution. Our study\nshows that reclassifying NAs can improve the accuracy of syntactic analysis and\nstructural understanding in NLP. We present experimental results using Hidden\nMarkov Models (HMMs), Maximum Entropy (MaxEnt) models, and Spacy, demonstrating\nthe feasibility and potential benefits of this approach. Additionally we\nfinetuned a bert model to identify the NA in untagged text.", "published": "2024-09-22 09:33:54", "link": "http://arxiv.org/abs/2409.14374v3", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Predicting User Stances from Target-Agnostic Information using Large\n  Language Models", "abstract": "We investigate Large Language Models' (LLMs) ability to predict a user's\nstance on a target given a collection of his/her target-agnostic social media\nposts (i.e., user-level stance prediction). While we show early evidence that\nLLMs are capable of this task, we highlight considerable variability in the\nperformance of the model across (i) the type of stance target, (ii) the\nprediction strategy and (iii) the number of target-agnostic posts supplied.\nPost-hoc analyses further hint at the usefulness of target-agnostic posts in\nproviding relevant information to LLMs through the presence of both\nsurface-level (e.g., target-relevant keywords) and user-level features (e.g.,\nencoding users' moral values). Overall, our findings suggest that LLMs might\noffer a viable method for determining public stances towards new topics based\non historical and target-agnostic data. At the same time, we also call for\nfurther research to better understand LLMs' strong performance on the stance\nprediction task and how their effectiveness varies across task contexts.", "published": "2024-09-22 11:21:16", "link": "http://arxiv.org/abs/2409.14395v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Semantic Parsing for Large Language Models: Enhancing LLM\n  Performance with Semantic Hints", "abstract": "Semantic Parsing aims to capture the meaning of a sentence and convert it\ninto a logical, structured form. Previous studies show that semantic parsing\nenhances the performance of smaller models (e.g., BERT) on downstream tasks.\nHowever, it remains unclear whether the improvements extend similarly to LLMs.\nIn this paper, our empirical findings reveal that, unlike smaller models,\ndirectly adding semantic parsing results into LLMs reduces their performance.\nTo overcome this, we propose SENSE, a novel prompting approach that embeds\nsemantic hints within the prompt. Experiments show that SENSE consistently\nimproves LLMs' performance across various tasks, highlighting the potential of\nintegrating semantic information to improve LLM capabilities.", "published": "2024-09-22 14:35:09", "link": "http://arxiv.org/abs/2409.14469v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The X Types -- Mapping the Semantics of the Twitter Sphere", "abstract": "Social networks form a valuable source of world knowledge, where influential\nentities correspond to popular accounts. Unlike factual knowledge bases (KBs),\nwhich maintain a semantic ontology, structured semantic information is not\navailable on social media. In this work, we consider a social KB of roughly\n200K popular Twitter accounts, which denotes entities of interest. We elicit\nsemantic information about those entities. In particular, we associate them\nwith a fine-grained set of 136 semantic types, e.g., determine whether a given\nentity account belongs to a politician, or a musical artist. In the lack of\nexplicit type information in Twitter, we obtain semantic labels for a subset of\nthe accounts via alignment with the KBs of DBpedia and Wikidata. Given the\nlabeled dataset, we finetune a transformer-based text encoder to generate\nsemantic embeddings of the entities based on the contents of their accounts. We\nthen exploit this evidence alongside network-based embeddings to predict the\nentities semantic types. In our experiments, we show high type prediction\nperformance on the labeled dataset. Consequently, we apply our type\nclassification model to all of the entity accounts in the social KB. Our\nanalysis of the results offers insights about the global semantics of the\nTwitter sphere. We discuss downstream applications that should benefit from\nsemantic type information and the semantic embeddings of social entities\ngenerated in this work. In particular, we demonstrate enhanced performance on\nthe key task of entity similarity assessment using this information.", "published": "2024-09-22 20:22:16", "link": "http://arxiv.org/abs/2409.14584v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can a Neural Model Guide Fieldwork? A Case Study on Morphological Data\n  Collection", "abstract": "Linguistic fieldwork is an important component in language documentation and\npreservation. However, it is a long, exhaustive, and time-consuming process.\nThis paper presents a novel model that guides a linguist during the fieldwork\nand accounts for the dynamics of linguist-speaker interactions. We introduce a\nnovel framework that evaluates the efficiency of various sampling strategies\nfor obtaining morphological data and assesses the effectiveness of\nstate-of-the-art neural models in generalising morphological structures. Our\nexperiments highlight two key strategies for improving the efficiency: (1)\nincreasing the diversity of annotated data by uniform sampling among the cells\nof the paradigm tables, and (2) using model confidence as a guide to enhance\npositive interaction by providing reliable predictions during annotation.", "published": "2024-09-22 23:40:03", "link": "http://arxiv.org/abs/2409.14628v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ESPERANTO: Evaluating Synthesized Phrases to Enhance Robustness in AI\n  Detection for Text Origination", "abstract": "While large language models (LLMs) exhibit significant utility across various\ndomains, they simultaneously are susceptible to exploitation for unethical\npurposes, including academic misconduct and dissemination of misinformation.\nConsequently, AI-generated text detection systems have emerged as a\ncountermeasure. However, these detection mechanisms demonstrate vulnerability\nto evasion techniques and lack robustness against textual manipulations. This\npaper introduces back-translation as a novel technique for evading detection,\nunderscoring the need to enhance the robustness of current detection systems.\nThe proposed method involves translating AI-generated text through multiple\nlanguages before back-translating to English. We present a model that combines\nthese back-translated texts to produce a manipulated version of the original\nAI-generated text. Our findings demonstrate that the manipulated text retains\nthe original semantics while significantly reducing the true positive rate\n(TPR) of existing detection methods. We evaluate this technique on nine AI\ndetectors, including six open-source and three proprietary systems, revealing\ntheir susceptibility to back-translation manipulation. In response to the\nidentified shortcomings of existing AI text detectors, we present a\ncountermeasure to improve the robustness against this form of manipulation. Our\nresults indicate that the TPR of the proposed method declines by only 1.85%\nafter back-translation manipulation. Furthermore, we build a large dataset of\n720k texts using eight different LLMs. Our dataset contains both human-authored\nand LLM-generated texts in various domains and writing styles to assess the\nperformance of our method and existing detectors. This dataset is publicly\nshared for the benefit of the research community.", "published": "2024-09-22 01:13:22", "link": "http://arxiv.org/abs/2409.14285v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reliable and diverse evaluation of LLM medical knowledge mastery", "abstract": "Mastering medical knowledge is crucial for medical-specific LLMs. However,\ndespite the existence of medical benchmarks like MedQA, a unified framework\nthat fully leverages existing knowledge bases to evaluate LLMs' mastery of\nmedical knowledge is still lacking. In the study, we propose a novel framework\nPretexEval that dynamically generates reliable and diverse test samples to\nevaluate LLMs for any given medical knowledge base. We notice that test samples\nproduced directly from knowledge bases by templates or LLMs may introduce\nfactual errors and also lack diversity. To address these issues, we introduce a\nnovel schema into our proposed evaluation framework that employs predicate\nequivalence transformations to produce a series of variants for any given\nmedical knowledge point. Finally, these produced predicate variants are\nconverted into textual language, resulting in a series of reliable and diverse\ntest samples to evaluate whether LLMs fully master the given medical factual\nknowledge point. Here, we use our proposed framework to systematically\ninvestigate the mastery of medical factual knowledge of 12 well-known LLMs,\nbased on two knowledge bases that are crucial for clinical diagnosis and\ntreatment. The evaluation results illustrate that current LLMs still exhibit\nsignificant deficiencies in fully mastering medical knowledge, despite\nachieving considerable success on some famous public benchmarks. These new\nfindings provide valuable insights for developing medical-specific LLMs,\nhighlighting that current LLMs urgently need to strengthen their comprehensive\nand in-depth mastery of medical knowledge before being applied to real-world\nmedical scenarios.", "published": "2024-09-22 03:13:38", "link": "http://arxiv.org/abs/2409.14302v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using Natural Language Processing to find Indication for Burnout with\n  Text Classification: From Online Data to Real-World Data", "abstract": "Burnout, classified as a syndrome in the ICD-11, arises from chronic\nworkplace stress that has not been effectively managed. It is characterized by\nexhaustion, cynicism, and reduced professional efficacy, and estimates of its\nprevalence vary significantly due to inconsistent measurement methods. Recent\nadvancements in Natural Language Processing (NLP) and machine learning offer\npromising tools for detecting burnout through textual data analysis, with\nstudies demonstrating high predictive accuracy. This paper contributes to\nburnout detection in German texts by: (a) collecting an anonymous real-world\ndataset including free-text answers and Oldenburg Burnout Inventory (OLBI)\nresponses; (b) demonstrating the limitations of a GermanBERT-based classifier\ntrained on online data; (c) presenting two versions of a curated\nBurnoutExpressions dataset, which yielded models that perform well in\nreal-world applications; and (d) providing qualitative insights from an\ninterdisciplinary focus group on the interpretability of AI models used for\nburnout detection. Our findings emphasize the need for greater collaboration\nbetween AI researchers and clinical experts to refine burnout detection models.\nAdditionally, more real-world data is essential to validate and enhance the\neffectiveness of current AI methods developed in NLP research, which are often\nbased on data automatically scraped from online sources and not evaluated in a\nreal-world context. This is essential for ensuring AI tools are well suited for\npractical applications.", "published": "2024-09-22 08:13:17", "link": "http://arxiv.org/abs/2409.14357v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Investigating Layer Importance in Large Language Models", "abstract": "Large language models (LLMs) have gained increasing attention due to their\nprominent ability to understand and process texts. Nevertheless, LLMs largely\nremain opaque. The lack of understanding of LLMs has obstructed the deployment\nin safety-critical scenarios and hindered the development of better models. In\nthis study, we advance the understanding of LLM by investigating the\nsignificance of individual layers in LLMs. We propose an efficient sampling\nmethod to faithfully evaluate the importance of layers using Shapley values, a\nwidely used explanation framework in feature attribution and data valuation. In\naddition, we conduct layer ablation experiments to assess the performance\ndegradation resulting from the exclusion of specific layers. Our findings\nreveal the existence of cornerstone layers, wherein certain early layers can\nexhibit a dominant contribution over others. Removing one cornerstone layer\nleads to a drastic collapse of the model performance, often reducing it to\nrandom guessing. Conversely, removing non-cornerstone layers results in only\nmarginal performance changes. This study identifies cornerstone layers in LLMs\nand underscores their critical role for future research.", "published": "2024-09-22 09:53:13", "link": "http://arxiv.org/abs/2409.14381v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond Persuasion: Towards Conversational Recommender System with\n  Credible Explanations", "abstract": "With the aid of large language models, current conversational recommender\nsystem (CRS) has gaining strong abilities to persuade users to accept\nrecommended items. While these CRSs are highly persuasive, they can mislead\nusers by incorporating incredible information in their explanations, ultimately\ndamaging the long-term trust between users and the CRS. To address this, we\npropose a simple yet effective method, called PC-CRS, to enhance the\ncredibility of CRS's explanations during persuasion. It guides the explanation\ngeneration through our proposed credibility-aware persuasive strategies and\nthen gradually refines explanations via post-hoc self-reflection. Experimental\nresults demonstrate the efficacy of PC-CRS in promoting persuasive and credible\nexplanations. Further analysis reveals the reason behind current methods\nproducing incredible explanations and the potential of credible explanations to\nimprove recommendation accuracy.", "published": "2024-09-22 11:35:59", "link": "http://arxiv.org/abs/2409.14399v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AggregHate: An Efficient Aggregative Approach for the Detection of\n  Hatemongers on Social Platforms", "abstract": "Automatic detection of online hate speech serves as a crucial step in the\ndetoxification of the online discourse. Moreover, accurate classification can\npromote a better understanding of the proliferation of hate as a social\nphenomenon. While most prior work focus on the detection of hateful utterances,\nwe argue that focusing on the user level is as important, albeit challenging.\nIn this paper we consider a multimodal aggregative approach for the detection\nof hate-mongers, taking into account the potentially hateful texts, user\nactivity, and the user network. We evaluate our methods on three unique\ndatasets X (Twitter), Gab, and Parler showing that a processing a user's texts\nin her social context significantly improves the detection of hate mongers,\ncompared to previously used text and graph-based methods. Our method can be\nthen used to improve the classification of coded messages, dog-whistling, and\nracial gas-lighting, as well as inform intervention measures. Moreover, our\napproach is highly efficient even for very large datasets and networks.", "published": "2024-09-22 14:29:49", "link": "http://arxiv.org/abs/2409.14464v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "A Large Language Model and Denoising Diffusion Framework for Targeted\n  Design of Microstructures with Commands in Natural Language", "abstract": "Microstructure plays a critical role in determining the macroscopic\nproperties of materials, with applications spanning alloy design, MEMS devices,\nand tissue engineering, among many others. Computational frameworks have been\ndeveloped to capture the complex relationship between microstructure and\nmaterial behavior. However, despite these advancements, the steep learning\ncurve associated with domain-specific knowledge and complex algorithms\nrestricts the broader application of these tools. To lower this barrier, we\npropose a framework that integrates Natural Language Processing (NLP), Large\nLanguage Models (LLMs), and Denoising Diffusion Probabilistic Models (DDPMs) to\nenable microstructure design using intuitive natural language commands. Our\nframework employs contextual data augmentation, driven by a pretrained LLM, to\ngenerate and expand a diverse dataset of microstructure descriptors. A\nretrained NER model extracts relevant microstructure descriptors from\nuser-provided natural language inputs, which are then used by the DDPM to\ngenerate microstructures with targeted mechanical properties and topological\nfeatures. The NLP and DDPM components of the framework are modular, allowing\nfor separate training and validation, which ensures flexibility in adapting the\nframework to different datasets and use cases. A surrogate model system is\nemployed to rank and filter generated samples based on their alignment with\ntarget properties. Demonstrated on a database of nonlinear hyperelastic\nmicrostructures, this framework serves as a prototype for accessible inverse\ndesign of microstructures, starting from intuitive natural language commands.", "published": "2024-09-22 14:45:22", "link": "http://arxiv.org/abs/2409.14473v1", "categories": ["cs.CE", "cs.CL"], "primary_category": "cs.CE"}
{"title": "Thought-Path Contrastive Learning via Premise-Oriented Data Augmentation\n  for Logical Reading Comprehension", "abstract": "Logical reading comprehension is a challenging task that entails grasping the\nunderlying semantics of text and applying reasoning to deduce the correct\nanswer. Prior researches have primarily focused on enhancing logical reasoning\ncapabilities through Chain-of-Thought (CoT) or data augmentation. However,\nprevious work constructing chain-of-thought rationales concentrates solely on\nanalyzing correct options, neglecting the incorrect alternatives. Addtionally,\nearlier efforts on data augmentation by altering contexts rely on rule-based\nmethods, which result in generated contexts that lack diversity and coherence.\nTo address these issues, we propose a Premise-Oriented Data Augmentation (PODA)\nframework. This framework can generate CoT rationales including analyses for\nboth correct and incorrect options, while constructing diverse and high-quality\ncounterfactual contexts from incorrect candidate options. We integrate\nsummarizing premises and identifying premises for each option into rationales.\nSubsequently, we employ multi-step prompts with identified premises to\nconstruct counterfactual context. To facilitate the model's capabilities to\nbetter differentiate the reasoning process associated with each option, we\nintroduce a novel thought-path contrastive learning method that compares\nreasoning paths between the original and counterfactual samples. Experimental\nresults on three representative LLMs demonstrate that our method can improve\nthe baselines substantially across two challenging logical reasoning benchmarks\n(ReClor and LogiQA 2.0). The data and code are released at\nhttps://github.com/lalalamdbf/TPReasoner.", "published": "2024-09-22 15:44:43", "link": "http://arxiv.org/abs/2409.14495v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse\n  Autoencoders", "abstract": "Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose\nthe activations of Large Language Models (LLMs) into human-interpretable\nlatents. In this paper, we pose two questions. First, to what extent do SAEs\nextract monosemantic and interpretable latents? Second, to what extent does\nvarying the sparsity or the size of the SAE affect monosemanticity /\ninterpretability? By investigating these questions in the context of a simple\nfirst-letter identification task where we have complete access to ground truth\nlabels for all tokens in the vocabulary, we are able to provide more detail\nthan prior investigations. Critically, we identify a problematic form of\nfeature-splitting we call feature absorption where seemingly monosemantic\nlatents fail to fire in cases where they clearly should. Our investigation\nsuggests that varying SAE size or sparsity is insufficient to solve this issue,\nand that there are deeper conceptual issues in need of resolution.", "published": "2024-09-22 16:11:02", "link": "http://arxiv.org/abs/2409.14507v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unleashing the Power of Emojis in Texts via Self-supervised Graph\n  Pre-Training", "abstract": "Emojis have gained immense popularity on social platforms, serving as a\ncommon means to supplement or replace text. However, existing data mining\napproaches generally either completely ignore or simply treat emojis as\nordinary Unicode characters, which may limit the model's ability to grasp the\nrich semantic information in emojis and the interaction between emojis and\ntexts. Thus, it is necessary to release the emoji's power in social media data\nmining. To this end, we first construct a heterogeneous graph consisting of\nthree types of nodes, i.e. post, word and emoji nodes to improve the\nrepresentation of different elements in posts. The edges are also well-defined\nto model how these three elements interact with each other. To facilitate the\nsharing of information among post, word and emoji nodes, we propose a graph\npre-train framework for text and emoji co-modeling, which contains two graph\npre-training tasks: node-level graph contrastive learning and edge-level link\nreconstruction learning. Extensive experiments on the Xiaohongshu and Twitter\ndatasets with two types of downstream tasks demonstrate that our approach\nproves significant improvement over previous strong baseline methods.", "published": "2024-09-22 18:29:10", "link": "http://arxiv.org/abs/2409.14552v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EchoAtt: Attend, Copy, then Adjust for More Efficient Large Language\n  Models", "abstract": "Large Language Models (LLMs), with their increasing depth and number of\nparameters, have demonstrated outstanding performance across a variety of\nnatural language processing tasks. However, this growth in scale leads to\nincreased computational demands, particularly during inference and fine-tuning.\nTo address these challenges, we introduce EchoAtt, a novel framework aimed at\noptimizing transformer-based models by analyzing and leveraging the similarity\nof attention patterns across layers. Our analysis reveals that many inner\nlayers in LLMs, especially larger ones, exhibit highly similar attention\nmatrices. By exploiting this similarity, EchoAtt enables the sharing of\nattention matrices in less critical layers, significantly reducing\ncomputational requirements without compromising performance. We incorporate\nthis approach within a knowledge distillation setup, where a pre-trained\nteacher model guides the training of a smaller student model. The student model\nselectively shares attention matrices in layers with high similarity while\ninheriting key parameters from the teacher. Our best results with\nTinyLLaMA-1.1B demonstrate that EchoAtt improves inference speed by 15\\%,\ntraining speed by 25\\%, and reduces the number of parameters by approximately\n4\\%, all while improving zero-shot performance. These findings highlight the\npotential of attention matrix sharing to enhance the efficiency of LLMs, making\nthem more practical for real-time and resource-limited applications.", "published": "2024-09-22 21:08:37", "link": "http://arxiv.org/abs/2409.14595v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can pre-trained language models generate titles for research papers?", "abstract": "The title of a research paper communicates in a succinct style the main theme\nand, sometimes, the findings of the paper. Coming up with the right title is\noften an arduous task, and therefore, it would be beneficial to authors if\ntitle generation can be automated. In this paper, we fine-tune pre-trained\nlanguage models to generate titles of papers from their abstracts.\nAdditionally, we use GPT-3.5-turbo in a zero-shot setting to generate paper\ntitles. The performance of the models is measured with ROUGE, METEOR,\nMoverScore, BERTScore and SciBERTScore metrics. We find that fine-tuned\nPEGASUS-large outperforms the other models, including fine-tuned LLaMA-3-8B and\nGPT-3.5-turbo, across most metrics. We also demonstrate that ChatGPT can\ngenerate creative titles for papers. Our observations suggest that AI-generated\npaper titles are generally accurate and appropriate.", "published": "2024-09-22 21:34:49", "link": "http://arxiv.org/abs/2409.14602v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can-Do! A Dataset and Neuro-Symbolic Grounded Framework for Embodied\n  Planning with Large Multimodal Models", "abstract": "Large multimodal models have demonstrated impressive problem-solving\nabilities in vision and language tasks, and have the potential to encode\nextensive world knowledge. However, it remains an open challenge for these\nmodels to perceive, reason, plan, and act in realistic environments. In this\nwork, we introduce Can-Do, a benchmark dataset designed to evaluate embodied\nplanning abilities through more diverse and complex scenarios than previous\ndatasets. Our dataset includes 400 multimodal samples, each consisting of\nnatural language user instructions, visual images depicting the environment,\nstate changes, and corresponding action plans. The data encompasses diverse\naspects of commonsense knowledge, physical understanding, and safety awareness.\nOur fine-grained analysis reveals that state-of-the-art models, including\nGPT-4V, face bottlenecks in visual perception, comprehension, and reasoning\nabilities. To address these challenges, we propose NeuroGround, a neurosymbolic\nframework that first grounds the plan generation in the perceived environment\nstates and then leverages symbolic planning engines to augment the\nmodel-generated plans. Experimental results demonstrate the effectiveness of\nour framework compared to strong baselines. Our code and dataset are available\nat https://embodied-planning.github.io.", "published": "2024-09-22 00:30:11", "link": "http://arxiv.org/abs/2409.14277v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Opinion Mining on Offshore Wind Energy for Environmental Engineering", "abstract": "In this paper, we conduct sentiment analysis on social media data to study\nmass opinion about offshore wind energy. We adapt three machine learning\nmodels, namely, TextBlob, VADER, and SentiWordNet because different functions\nare provided by each model. TextBlob provides subjectivity analysis as well as\npolarity classification. VADER offers cumulative sentiment scores. SentiWordNet\nconsiders sentiments with reference to context and performs classification\naccordingly. Techniques in NLP are harnessed to gather meaning from the textual\ndata in social media. Data visualization tools are suitably deployed to display\nthe overall results. This work is much in line with citizen science and smart\ngovernance via involvement of mass opinion to guide decision support. It\nexemplifies the role of Machine Learning and NLP here.", "published": "2024-09-22 01:51:43", "link": "http://arxiv.org/abs/2409.14292v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7; I.2.m; J.2"], "primary_category": "cs.LG"}
{"title": "Unveiling Narrative Reasoning Limits of Large Language Models with Trope\n  in Movie Synopses", "abstract": "Large language models (LLMs) equipped with chain-of-thoughts (CoT) prompting\nhave shown significant multi-step reasoning capabilities in factual content\nlike mathematics, commonsense, and logic. However, their performance in\nnarrative reasoning, which demands greater abstraction capabilities, remains\nunexplored. This study utilizes tropes in movie synopses to assess the abstract\nreasoning abilities of state-of-the-art LLMs and uncovers their low\nperformance. We introduce a trope-wise querying approach to address these\nchallenges and boost the F1 score by 11.8 points. Moreover, while prior studies\nsuggest that CoT enhances multi-step reasoning, this study shows CoT can cause\nhallucinations in narrative content, reducing GPT-4's performance. We also\nintroduce an Adversarial Injection method to embed trope-related text tokens\ninto movie synopses without explicit tropes, revealing CoT's heightened\nsensitivity to such injections. Our comprehensive analysis provides insights\nfor future research directions.", "published": "2024-09-22 05:50:18", "link": "http://arxiv.org/abs/2409.14324v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automotive innovation landscaping using LLM", "abstract": "The process of landscaping automotive innovation through patent analysis is\ncrucial for Research and Development teams. It aids in comprehending innovation\ntrends, technological advancements, and the latest technologies from\ncompetitors. Traditionally, this process required intensive manual efforts.\nHowever, with the advent of Large Language Models (LLMs), it can now be\nautomated, leading to faster and more efficient patent categorization &\nstate-of-the-art of inventive concept extraction. This automation can assist\nvarious R\\&D teams in extracting relevant information from extensive patent\ndatabases. This paper introduces a method based on prompt engineering to\nextract essential information for landscaping. The information includes the\nproblem addressed by the patent, the technology utilized, and the area of\ninnovation within the vehicle ecosystem (such as safety, Advanced Driver\nAssistance Systems and more).The result demonstrates the implementation of this\nmethod to create a landscape of fuel cell technology using open-source patent\ndata. This approach provides a comprehensive overview of the current state of\nfuel cell technology, offering valuable insights for future research and\ndevelopment in this field.", "published": "2024-09-22 13:22:39", "link": "http://arxiv.org/abs/2409.14436v1", "categories": ["cs.CL", "cs.AI", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Exploring Multilingual Probing in Large Language Models: A\n  Cross-Language Analysis", "abstract": "Probing techniques for large language models (LLMs) have primarily focused on\nEnglish, overlooking the vast majority of the world's languages. In this paper,\nwe extend these probing methods to a multilingual context, investigating the\nbehaviors of LLMs across diverse languages. We conduct experiments on several\nopen-source LLM models, analyzing probing accuracy, trends across layers, and\nsimilarities between probing vectors for multiple languages. Our key findings\nreveal: (1) a consistent performance gap between high-resource and low-resource\nlanguages, with high-resource languages achieving significantly higher probing\naccuracy; (2) divergent layer-wise accuracy trends, where high-resource\nlanguages show substantial improvement in deeper layers similar to English; and\n(3) higher representational similarities among high-resource languages, with\nlow-resource languages demonstrating lower similarities both among themselves\nand with high-resource languages. These results highlight significant\ndisparities in LLMs' multilingual capabilities and emphasize the need for\nimproved modeling of low-resource languages.", "published": "2024-09-22 14:14:05", "link": "http://arxiv.org/abs/2409.14459v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Word Discovery: Boundary Detection with Clustering vs.\n  Dynamic Programming", "abstract": "We look at the long-standing problem of segmenting unlabeled speech into\nword-like segments and clustering these into a lexicon. Several previous\nmethods use a scoring model coupled with dynamic programming to find an optimal\nsegmentation. Here we propose a much simpler strategy: we predict word\nboundaries using the dissimilarity between adjacent self-supervised features,\nthen we cluster the predicted segments to construct a lexicon. For a fair\ncomparison, we update the older ES-KMeans dynamic programming method with\nbetter features and boundary constraints. On the five-language ZeroSpeech\nbenchmarks, our simple approach gives similar state-of-the-art results compared\nto the new ES-KMeans+ method, while being almost five times faster. Project\nwebpage: https://s-malan.github.io/prom-seg-clus.", "published": "2024-09-22 15:16:43", "link": "http://arxiv.org/abs/2409.14486v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Can AI writing be salvaged? Mitigating Idiosyncrasies and Improving\n  Human-AI Alignment in the Writing Process through Edits", "abstract": "LLM-based applications are helping people write, and LLM-generated text is\nmaking its way into social media, journalism, and our classrooms. However, the\ndifferences between LLM-generated and human written text remain unclear. To\nexplore this, we hired professional writers to edit paragraphs in several\ncreative domains. We first found these writers agree on undesirable\nidiosyncrasies in LLM generated text, formalizing it into a seven-category\ntaxonomy (e.g. clich\\'es, unnecessary exposition). Second, we curated the LAMP\ncorpus: 1,057 LLM-generated paragraphs edited by professional writers according\nto our taxonomy. Analysis of LAMP reveals that none of the LLMs used in our\nstudy (GPT4o, Claude-3.5-Sonnet, Llama-3.1-70b) outperform each other in terms\nof writing quality, revealing common limitations across model families. Third,\nbuilding on existing work in automatic editing we evaluated methods to improve\nLLM-generated text. A large-scale preference annotation confirms that although\nexperts largely prefer text edited by other experts, automatic editing methods\nshow promise in improving alignment between LLM-generated and human-written\ntext.", "published": "2024-09-22 16:13:00", "link": "http://arxiv.org/abs/2409.14509v5", "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Beyond Words: Evaluating Large Language Models in Transportation\n  Planning", "abstract": "The resurgence and rapid advancement of Generative Artificial Intelligence\n(GenAI) in 2023 has catalyzed transformative shifts across numerous industry\nsectors, including urban transportation and logistics. This study investigates\nthe evaluation of Large Language Models (LLMs), specifically GPT-4 and\nPhi-3-mini, to enhance transportation planning. The study assesses the\nperformance and spatial comprehension of these models through a\ntransportation-informed evaluation framework that includes general geospatial\nskills, general transportation domain skills, and real-world transportation\nproblem-solving. Utilizing a mixed-methods approach, the research encompasses\nan evaluation of the LLMs' general Geographic Information System (GIS) skills,\ngeneral transportation domain knowledge as well as abilities to support human\ndecision-making in the real-world transportation planning scenarios of\ncongestion pricing. Results indicate that GPT-4 demonstrates superior accuracy\nand reliability across various GIS and transportation-specific tasks compared\nto Phi-3-mini, highlighting its potential as a robust tool for transportation\nplanners. Nonetheless, Phi-3-mini exhibits competence in specific analytical\nscenarios, suggesting its utility in resource-constrained environments. The\nfindings underscore the transformative potential of GenAI technologies in urban\ntransportation planning. Future work could explore the application of newer\nLLMs and the impact of Retrieval-Augmented Generation (RAG) techniques, on a\nbroader set of real-world transportation planning and operations challenges, to\ndeepen the integration of advanced AI models in transportation management\npractices.", "published": "2024-09-22 16:20:00", "link": "http://arxiv.org/abs/2409.14516v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "What Are They Doing? Joint Audio-Speech Co-Reasoning", "abstract": "In audio and speech processing, tasks usually focus on either the audio or\nspeech modality, even when both sounds and human speech are present in the same\naudio clip. Recent Auditory Large Language Models (ALLMs) have made it possible\nto process audio and speech simultaneously within a single model, leading to\nfurther considerations of joint audio-speech tasks.\n  In this paper, we establish a novel benchmark to investigate how well ALLMs\ncan perform joint audio-speech processing. Specifically, we introduce Joint\nAudio-Speech Co-Reasoning (JASCO), a novel task that unifies audio and speech\nprocessing, strictly requiring co-reasoning across both modalities. We also\nrelease a scene-reasoning dataset called \"What Are They Doing\". Additionally,\nwe provide deeper insights into the models' behaviors by analyzing their\ndependence on each modality.", "published": "2024-09-22 16:45:57", "link": "http://arxiv.org/abs/2409.14526v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Evaluating the Performance and Robustness of LLMs in Materials Science\n  Q&A and Property Predictions", "abstract": "Large Language Models (LLMs) have the potential to revolutionize scientific\nresearch, yet their robustness and reliability in domain-specific applications\nremain insufficiently explored. In this study, we evaluate the performance and\nrobustness of LLMs for materials science, focusing on domain-specific question\nanswering and materials property prediction across diverse real-world and\nadversarial conditions. Three distinct datasets are used in this study: 1) a\nset of multiple-choice questions from undergraduate-level materials science\ncourses, 2) a dataset including various steel compositions and yield strengths,\nand 3) a band gap dataset, containing textual descriptions of material crystal\nstructures and band gap values. The performance of LLMs is assessed using\nvarious prompting strategies, including zero-shot chain-of-thought, expert\nprompting, and few-shot in-context learning. The robustness of these models is\ntested against various forms of 'noise', ranging from realistic disturbances to\nintentionally adversarial manipulations, to evaluate their resilience and\nreliability under real-world conditions. Additionally, the study showcases\nunique phenomena of LLMs during predictive tasks, such as mode collapse\nbehavior when the proximity of prompt examples is altered and performance\nrecovery from train/test mismatch. The findings aim to provide informed\nskepticism for the broad use of LLMs in materials science and to inspire\nadvancements that enhance their robustness and reliability for practical\napplications.", "published": "2024-09-22 19:31:16", "link": "http://arxiv.org/abs/2409.14572v2", "categories": ["cs.CL", "cond-mat.mtrl-sci", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Backtracking Improves Generation Safety", "abstract": "Text generation has a fundamental limitation almost by definition: there is\nno taking back tokens that have been generated, even when they are clearly\nproblematic. In the context of language model safety, when a partial unsafe\ngeneration is produced, language models by their nature tend to happily keep on\ngenerating similarly unsafe additional text. This is in fact how safety\nalignment of frontier models gets circumvented in the wild, despite great\nefforts in improving their safety. Deviating from the paradigm of approaching\nsafety alignment as prevention (decreasing the probability of harmful\nresponses), we propose backtracking, a technique that allows language models to\n\"undo\" and recover from their own unsafe generation through the introduction of\na special [RESET] token. Our method can be incorporated into either SFT or DPO\ntraining to optimize helpfulness and harmlessness. We show that models trained\nto backtrack are consistently safer than baseline models: backtracking\nLlama-3-8B is four times more safe than the baseline model (6.1\\% $\\to$ 1.5\\%)\nin our evaluations without regression in helpfulness. Our method additionally\nprovides protection against four adversarial attacks including an adaptive\nattack, despite not being trained to do so.", "published": "2024-09-22 20:28:40", "link": "http://arxiv.org/abs/2409.14586v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SAC-KG: Exploiting Large Language Models as Skilled Automatic\n  Constructors for Domain Knowledge Graphs", "abstract": "Knowledge graphs (KGs) play a pivotal role in knowledge-intensive tasks\nacross specialized domains, where the acquisition of precise and dependable\nknowledge is crucial. However, existing KG construction methods heavily rely on\nhuman intervention to attain qualified KGs, which severely hinders the\npractical applicability in real-world scenarios. To address this challenge, we\npropose a general KG construction framework, named SAC-KG, to exploit large\nlanguage models (LLMs) as Skilled Automatic Constructors for domain Knowledge\nGraph. SAC-KG effectively involves LLMs as domain experts to generate\nspecialized and precise multi-level KGs. Specifically, SAC-KG consists of three\ncomponents: Generator, Verifier, and Pruner. For a given entity, Generator\nproduces its relations and tails from raw domain corpora, to construct a\nspecialized single-level KG. Verifier and Pruner then work together to ensure\nprecision by correcting generation errors and determining whether newly\nproduced tails require further iteration for the next-level KG.Experiments\ndemonstrate that SAC-KG automatically constructs a domain KG at the scale of\nover one million nodes and achieves a precision of 89.32%, leading to a\nsuperior performance with over 20% increase in precision rate compared to\nexisting state-of-the-art methods for the KG construction task.", "published": "2024-09-22 13:55:23", "link": "http://arxiv.org/abs/2410.02811v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Towards Within-Class Variation in Alzheimer's Disease Detection from\n  Spontaneous Speech", "abstract": "Alzheimer's Disease (AD) detection has emerged as a promising research area\nthat employs machine learning classification models to distinguish between\nindividuals with AD and those without. Unlike conventional classification\ntasks, we identify within-class variation as a critical challenge in AD\ndetection: individuals with AD exhibit a spectrum of cognitive impairments.\nGiven that many AD detection tasks lack fine-grained labels, simplistic binary\nclassification may overlook two crucial aspects: within-class differences and\ninstance-level imbalance. The former compels the model to map AD samples with\nvarying degrees of impairment to a single diagnostic label, disregarding\ncertain changes in cognitive function. While the latter biases the model\ntowards overrepresented severity levels. This work presents early efforts to\naddress these challenges. We propose two novel methods: Soft Target\nDistillation (SoTD) and Instance-level Re-balancing (InRe), targeting two\nproblems respectively. Experiments on the ADReSS and ADReSSo datasets\ndemonstrate that the proposed methods significantly improve detection accuracy.\nFurther analysis reveals that SoTD effectively harnesses the strengths of\nmultiple component models, while InRe substantially alleviates model\nover-fitting. These findings provide insights for developing more robust and\nreliable AD detection models.", "published": "2024-09-22 02:06:05", "link": "http://arxiv.org/abs/2409.16322v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD", "q-bio.NC"], "primary_category": "eess.AS"}
{"title": "Avengers Assemble: Amalgamation of Non-Semantic Features for Depression\n  Detection", "abstract": "In this study, we address the challenge of depression detection from speech,\nfocusing on the potential of non-semantic features (NSFs) to capture subtle\nmarkers of depression. While prior research has leveraged various features for\nthis task, NSFs-extracted from pre-trained models (PTMs) designed for\nnon-semantic tasks such as paralinguistic speech processing (TRILLsson),\nspeaker recognition (x-vector), and emotion recognition (emoHuBERT)-have shown\nsignificant promise. However, the potential of combining these diverse features\nhas not been fully explored. In this work, we demonstrate that the amalgamation\nof NSFs results in complementary behavior, leading to enhanced depression\ndetection performance. Furthermore, to our end, we introduce a simple novel\nframework, FuSeR, designed to effectively combine these features. Our results\nshow that FuSeR outperforms models utilizing individual NSFs as well as\nbaseline fusion techniques and obtains state-of-the-art (SOTA) performance in\nE-DAIC benchmark with RMSE of 5.51 and MAE of 4.48, establishing it as a robust\napproach for depression detection.", "published": "2024-09-22 04:40:04", "link": "http://arxiv.org/abs/2409.14312v1", "categories": ["eess.AS", "cs.SD", "68T45", "I.2.7"], "primary_category": "eess.AS"}
{"title": "Improved direction of arrival estimations with a wearable microphone\n  array for dynamic environments by reliability weighting", "abstract": "Direction-of-arrival estimation of multiple speakers in a room is an\nimportant task for a wide range of applications. In particular, challenging\nenvironments with moving speakers, reverberation and noise, lead to significant\nperformance degradation for current methods. With the aim of better\nunderstanding factors affecting performance and improving current methods, in\nthis paper multi-speaker direction-of-arrival (DOA) estimation is investigated\nusing a modified version of the local space domain distance (LSDD) algorithm in\na noisy, dynamic and reverberant environment employing a wearable microphone\narray. This study utilizes the recently published EasyCom speech dataset,\nrecorded using a wearable microphone array mounted on eyeglasses. While the\noriginal LSDD algorithm demonstrates strong performance in static environments,\nits efficacy significantly diminishes in the dynamic settings of the EasyCom\ndataset. Several enhancements to the LSDD algorithm are developed following a\ncomprehensive performance and system analysis, which enable improved DOA\nestimation under these challenging conditions. These improvements include\nincorporating a weighted reliability approach and introducing a new quality\nmeasure that reliably identifies the more accurate DOA estimates, thereby\nenhancing both the robustness and accuracy of the algorithm in challenging\nenvironments.", "published": "2024-09-22 07:14:49", "link": "http://arxiv.org/abs/2409.14346v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Robust Audio-Visual Speech Enhancement: Correcting Misassignments in\n  Complex Environments with Advanced Post-Processing", "abstract": "This paper addresses the prevalent issue of incorrect speech output in\naudio-visual speech enhancement (AVSE) systems, which is often caused by poor\nvideo quality and mismatched training and test data. We introduce a\npost-processing classifier (PPC) to rectify these erroneous outputs, ensuring\nthat the enhanced speech corresponds accurately to the intended speaker. We\nalso adopt a mixup strategy in PPC training to improve its robustness.\nExperimental results on the AVSE-challenge dataset show that integrating PPC\ninto the AVSE model can significantly improve AVSE performance, and combining\nPPC with the AVSE model trained with permutation invariant training (PIT)\nyields the best performance. The proposed method substantially outperforms the\nbaseline model by a large margin. This work highlights the potential for\nbroader applications across various modalities and architectures, providing a\npromising direction for future research in this field.", "published": "2024-09-22 18:35:43", "link": "http://arxiv.org/abs/2409.14554v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SongTrans: An unified song transcription and alignment method for lyrics\n  and notes", "abstract": "The quantity of processed data is crucial for advancing the field of singing\nvoice synthesis. While there are tools available for lyric or note\ntranscription tasks, they all need pre-processed data which is relatively\ntime-consuming (e.g., vocal and accompaniment separation). Besides, most of\nthese tools are designed to address a single task and struggle with aligning\nlyrics and notes (i.e., identifying the corresponding notes of each word in\nlyrics). To address those challenges, we first design a pipeline by optimizing\nexisting tools and annotating numerous lyric-note pairs of songs. Then, based\non the annotated data, we train a unified SongTrans model that can directly\ntranscribe lyrics and notes while aligning them simultaneously, without\nrequiring pre-processing songs. Our SongTrans model consists of two modules:\n(1) the \\textbf{Autoregressive module} predicts the lyrics, along with the\nduration and note number corresponding to each word in a lyric. (2) the\n\\textbf{Non-autoregressive module} predicts the pitch and duration of the\nnotes. Our experiments demonstrate that SongTrans achieves state-of-the-art\n(SOTA) results in both lyric and note transcription tasks. Furthermore, it is\nthe first model capable of aligning lyrics with notes. Experimental results\ndemonstrate that the SongTrans model can effectively adapt to different types\nof songs (e.g., songs with accompaniment), showcasing its versatility for\nreal-world applications.", "published": "2024-09-22 23:06:15", "link": "http://arxiv.org/abs/2409.14619v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Feature Engineering Approach for Literary and Colloquial Tamil Speech\n  Classification using 1D-CNN", "abstract": "In ideal human computer interaction (HCI), the colloquial form of a language\nwould be preferred by most users, since it is the form used in their day-to-day\nconversations. However, there is also an undeniable necessity to preserve the\nformal literary form. By embracing the new and preserving the old, both service\nto the common man (practicality) and service to the language itself\n(conservation) can be rendered. Hence, it is ideal for computers to have the\nability to accept, process, and converse in both forms of the language, as\nrequired. To address this, it is first necessary to identify the form of the\ninput speech, which in the current work is between literary and colloquial\nTamil speech. Such a front-end system must consist of a simple, effective, and\nlightweight classifier that is trained on a few effective features that are\ncapable of capturing the underlying patterns of the speech signal. To\naccomplish this, a one-dimensional convolutional neural network (1D-CNN) that\nlearns the envelope of features across time, is proposed. The network is\ntrained on a select number of handcrafted features initially, and then on Mel\nfrequency cepstral coefficients (MFCC) for comparison. The handcrafted features\nwere selected to address various aspects of speech such as the spectral and\ntemporal characteristics, prosody, and voice quality. The features are\ninitially analyzed by considering ten parallel utterances and observing the\ntrend of each feature with respect to time. The proposed 1D-CNN, trained using\nthe handcrafted features, offers an F1 score of 0.9803, while that trained on\nthe MFCC offers an F1 score of 0.9895. In light of this, feature ablation and\nfeature combination are explored. When the best ranked handcrafted features,\nfrom the feature ablation study, are combined with the MFCC, they offer the\nbest results with an F1 score of 0.9946.", "published": "2024-09-22 07:20:42", "link": "http://arxiv.org/abs/2409.14348v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Self-Supervised Audio-Visual Soundscape Stylization", "abstract": "Speech sounds convey a great deal of information about the scenes, resulting\nin a variety of effects ranging from reverberation to additional ambient\nsounds. In this paper, we manipulate input speech to sound as though it was\nrecorded within a different scene, given an audio-visual conditional example\nrecorded from that scene. Our model learns through self-supervision, taking\nadvantage of the fact that natural video contains recurring sound events and\ntextures. We extract an audio clip from a video and apply speech enhancement.\nWe then train a latent diffusion model to recover the original speech, using\nanother audio-visual clip taken from elsewhere in the video as a conditional\nhint. Through this process, the model learns to transfer the conditional\nexample's sound properties to the input speech. We show that our model can be\nsuccessfully trained using unlabeled, in-the-wild videos, and that an\nadditional visual signal can improve its sound prediction abilities. Please see\nour project webpage for video results:\nhttps://tinglok.netlify.app/files/avsoundscape/", "published": "2024-09-22 06:57:33", "link": "http://arxiv.org/abs/2409.14340v1", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
