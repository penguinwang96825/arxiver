{"title": "Document Graph for Neural Machine Translation", "abstract": "Previous works have shown that contextual information can improve the\nperformance of neural machine translation (NMT). However, most existing\ndocument-level NMT methods only consider a few number of previous sentences.\nHow to make use of the whole document as global contexts is still a challenge.\nTo address this issue, we hypothesize that a document can be represented as a\ngraph that connects relevant contexts regardless of their distances. We employ\nseveral types of relations, including adjacency, syntactic dependency, lexical\nconsistency, and coreference, to construct the document graph. Then, we\nincorporate both source and target graphs into the conventional Transformer\narchitecture with graph convolutional networks. Experiments on various NMT\nbenchmarks, including IWSLT English--French, Chinese-English, WMT\nEnglish--German and Opensubtitle English--Russian, demonstrate that using\ndocument graphs can significantly improve the translation quality. Extensive\nanalysis verifies that the document graph is beneficial for capturing discourse\nphenomena.", "published": "2020-12-07 06:48:59", "link": "http://arxiv.org/abs/2012.03477v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialogue Discourse-Aware Graph Model and Data Augmentation for Meeting\n  Summarization", "abstract": "Meeting summarization is a challenging task due to its dynamic interaction\nnature among multiple speakers and lack of sufficient training data. Existing\nmethods view the meeting as a linear sequence of utterances while ignoring the\ndiverse relations between each utterance. Besides, the limited labeled data\nfurther hinders the ability of data-hungry neural models. In this paper, we try\nto mitigate the above challenges by introducing dialogue-discourse relations.\nFirst, we present a Dialogue Discourse-Dware Meeting Summarizer (DDAMS) to\nexplicitly model the interaction between utterances in a meeting by modeling\ndifferent discourse relations. The core module is a relational graph encoder,\nwhere the utterances and discourse relations are modeled in a graph interaction\nmanner. Moreover, we devise a Dialogue Discourse-Aware Data Augmentation\n(DDADA) strategy to construct a pseudo-summarization corpus from existing input\nmeetings, which is 20 times larger than the original dataset and can be used to\npretrain DDAMS. Experimental results on AMI and ICSI meeting datasets show that\nour full system can achieve SOTA performance. Our codes will be available at:\nhttps://github.com/xcfcode/DDAMS.", "published": "2020-12-07 07:51:38", "link": "http://arxiv.org/abs/2012.03502v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "H-FND: Hierarchical False-Negative Denoising for Distant Supervision\n  Relation Extraction", "abstract": "Although distant supervision automatically generates training data for\nrelation extraction, it also introduces false-positive (FP) and false-negative\n(FN) training instances to the generated datasets. Whereas both types of errors\ndegrade the final model performance, previous work on distant supervision\ndenoising focuses more on suppressing FP noise and less on resolving the FN\nproblem. We here propose H-FND, a hierarchical false-negative denoising\nframework for robust distant supervision relation extraction, as an FN\ndenoising solution. H-FND uses a hierarchical policy which first determines\nwhether non-relation (NA) instances should be kept, discarded, or revised\nduring the training process. For those learning instances which are to be\nrevised, the policy further reassigns them appropriate relations, making them\nbetter training inputs. Experiments on SemEval-2010 and TACRED were conducted\nwith controlled FN ratios that randomly turn the relations of training and\nvalidation instances into negatives to generate FN instances. In this setting,\nH-FND can revise FN instances correctly and maintains high F1 scores even when\n50% of the instances have been turned into negatives. Experiment on NYT10 is\nfurther conducted to shows that H-FND is applicable in a realistic setting.", "published": "2020-12-07 08:58:09", "link": "http://arxiv.org/abs/2012.03536v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UBAR: Towards Fully End-to-End Task-Oriented Dialog Systems with GPT-2", "abstract": "This paper presents our task-oriented dialog system UBAR which models\ntask-oriented dialogs on a dialog session level. Specifically, UBAR is acquired\nby fine-tuning the large pre-trained unidirectional language model GPT-2 on the\nsequence of the entire dialog session which is composed of user utterance,\nbelief state, database result, system act, and system response of every dialog\nturn. Additionally, UBAR is evaluated in a more realistic setting, where its\ndialog context has access to user utterances and all content it generated such\nas belief states, system acts, and system responses. Experimental results on\nthe MultiWOZ datasets show that UBAR achieves state-of-the-art performances in\nmultiple settings, improving the combined score of response generation, policy\noptimization, and end-to-end modeling by 4.7, 3.5, and 9.4 points respectively.\nThorough analyses demonstrate that the session-level training sequence\nformulation and the generated dialog context are essential for UBAR to operate\nas a fully end-to-end task-oriented dialog system in real life. We also examine\nthe transfer ability of UBAR to new domains with limited data and provide\nvisualization and a case study to illustrate the advantages of UBAR in modeling\non a dialog session level.", "published": "2020-12-07 09:08:16", "link": "http://arxiv.org/abs/2012.03539v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structural Text Segmentation of Legal Documents", "abstract": "The growing complexity of legal cases has lead to an increasing interest in\nlegal information retrieval systems that can effectively satisfy user-specific\ninformation needs. However, such downstream systems typically require documents\nto be properly formatted and segmented, which is often done with relatively\nsimple pre-processing steps, disregarding topical coherence of segments.\nSystems generally rely on representations of individual sentences or\nparagraphs, which may lack crucial context, or document-level representations,\nwhich are too long for meaningful search results. To address this issue, we\npropose a segmentation system that can predict topical coherence of sequential\ntext segments spanning several paragraphs, effectively segmenting a document\nand providing a more balanced representation for downstream applications. We\nbuild our model on top of popular transformer networks and formulate structural\ntext segmentation as topical change detection, by performing a series of\nindependent classifications that allow for efficient fine-tuning on\ntask-specific data. We crawl a novel dataset consisting of roughly $74,000$\nonline Terms-of-Service documents, including hierarchical topic annotations,\nwhich we use for training. Results show that our proposed system significantly\noutperforms baselines, and adapts well to structural peculiarities of legal\ndocuments. We release both data and trained models to the research community\nfor future work.https://github.com/dennlinger/TopicalChange", "published": "2020-12-07 12:09:37", "link": "http://arxiv.org/abs/2012.03619v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using previous acoustic context to improve Text-to-Speech synthesis", "abstract": "Many speech synthesis datasets, especially those derived from audiobooks,\nnaturally comprise sequences of utterances. Nevertheless, such data are\ncommonly treated as individual, unordered utterances both when training a model\nand at inference time. This discards important prosodic phenomena above the\nutterance level. In this paper, we leverage the sequential nature of the data\nusing an acoustic context encoder that produces an embedding of the previous\nutterance audio. This is input to the decoder in a Tacotron 2 model. The\nembedding is also used for a secondary task, providing additional supervision.\nWe compare two secondary tasks: predicting the ordering of utterance pairs, and\npredicting the embedding of the current utterance audio. Results show that the\nrelation between consecutive utterances is informative: our proposed model\nsignificantly improves naturalness over a Tacotron 2 baseline.", "published": "2020-12-07 15:00:18", "link": "http://arxiv.org/abs/2012.03763v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Meaning-Form Correlation Has to Compose With", "abstract": "Compositionality is a widely discussed property of natural languages,\nalthough its exact definition has been elusive. We focus on the proposal that\ncompositionality can be assessed by measuring meaning-form correlation. We\nanalyze meaning-form correlation on three sets of languages: (i) artificial toy\nlanguages tailored to be compositional, (ii) a set of English dictionary\ndefinitions, and (iii) a set of English sentences drawn from literature. We\nfind that linguistic phenomena such as synonymy and ungrounded stop-words weigh\non MFC measurements, and that straightforward methods to mitigate their effects\nhave widely varying results depending on the dataset they are applied to. Data\nand code are made publicly available.", "published": "2020-12-07 16:33:23", "link": "http://arxiv.org/abs/2012.03833v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stylometry for Noisy Medieval Data: Evaluating Paul Meyer's Hagiographic\n  Hypothesis", "abstract": "Stylometric analysis of medieval vernacular texts is still a significant\nchallenge: the importance of scribal variation, be it spelling or more\nsubstantial, as well as the variants and errors introduced in the tradition,\ncomplicate the task of the would-be stylometrist. Basing the analysis on the\nstudy of the copy from a single hand of several texts can partially mitigate\nthese issues (Camps and Cafiero, 2013), but the limited availability of\ncomplete diplomatic transcriptions might make this difficult. In this paper, we\nuse a workflow combining handwritten text recognition and stylometric analysis,\napplied to the case of the hagiographic works contained in MS BnF, fr. 412. We\nseek to evaluate Paul Meyer's hypothesis about the constitution of groups of\nhagiographic works, as well as to examine potential authorial groupings in a\nvastly anonymous corpus.", "published": "2020-12-07 16:48:34", "link": "http://arxiv.org/abs/2012.03845v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Lab vs The Crowd: An Investigation into Data Quality for Neural\n  Dialogue Models", "abstract": "Challenges around collecting and processing quality data have hampered\nprogress in data-driven dialogue models. Previous approaches are moving away\nfrom costly, resource-intensive lab settings, where collection is slow but\nwhere the data is deemed of high quality. The advent of crowd-sourcing\nplatforms, such as Amazon Mechanical Turk, has provided researchers with an\nalternative cost-effective and rapid way to collect data. However, the\ncollection of fluid, natural spoken or textual interaction can be challenging,\nparticularly between two crowd-sourced workers. In this study, we compare the\nperformance of dialogue models for the same interaction task but collected in\ntwo different settings: in the lab vs. crowd-sourced. We find that fewer lab\ndialogues are needed to reach similar accuracy, less than half the amount of\nlab data as crowd-sourced data. We discuss the advantages and disadvantages of\neach data collection method.", "published": "2020-12-07 17:02:00", "link": "http://arxiv.org/abs/2012.03855v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Commercial Intent Detection Services with Practice-Driven\n  Evaluations", "abstract": "Intent detection is a key component of modern goal-oriented dialog systems\nthat accomplish a user task by predicting the intent of users' text input.\nThere are three primary challenges in designing robust and accurate intent\ndetection models. First, typical intent detection models require a large amount\nof labeled data to achieve high accuracy. Unfortunately, in practical scenarios\nit is more common to find small, unbalanced, and noisy datasets. Secondly, even\nwith large training data, the intent detection models can see a different\ndistribution of test data when being deployed in the real world, leading to\npoor accuracy. Finally, a practical intent detection model must be\ncomputationally efficient in both training and single query inference so that\nit can be used continuously and re-trained frequently. We benchmark intent\ndetection methods on a variety of datasets. Our results show that Watson\nAssistant's intent detection model outperforms other commercial solutions and\nis comparable to large pretrained language models while requiring only a\nfraction of computational resources and training data. Watson Assistant\ndemonstrates a higher degree of robustness when the training and test\ndistributions differ.", "published": "2020-12-07 18:58:57", "link": "http://arxiv.org/abs/2012.03929v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CX DB8: A queryable extractive summarizer and semantic search engine", "abstract": "Competitive Debate's increasingly technical nature has left competitors\nlooking for tools to accelerate evidence production. We find that the unique\ntype of extractive summarization performed by competitive debaters -\nsummarization with a bias towards a particular target meaning - can be\nperformed using the latest innovations in unsupervised pre-trained text\nvectorization models. We introduce CX_DB8, a queryable word-level extractive\nsummarizer and evidence creation framework, which allows for rapid, biasable\nsummarization of arbitarily sized texts. CX_DB8s usage of the embedding\nframework Flair means that as the underlying models improve, CX_DB8 will also\nimprove. We observe that CX_DB8 also functions as a semantic search engine, and\nhas application as a supplement to traditional \"find\" functionality in programs\nand webpages. CX_DB8 is currently used by competitive debaters and is made\navailable to the public at https://github.com/Hellisotherpeople/CX_DB8", "published": "2020-12-07 05:37:32", "link": "http://arxiv.org/abs/2012.03942v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Taxonomy of Empathetic Response Intents in Human Social Conversations", "abstract": "Open-domain conversational agents or chatbots are becoming increasingly\npopular in the natural language processing community. One of the challenges is\nenabling them to converse in an empathetic manner. Current neural response\ngeneration methods rely solely on end-to-end learning from large scale\nconversation data to generate dialogues. This approach can produce socially\nunacceptable responses due to the lack of large-scale quality data used to\ntrain the neural models. However, recent work has shown the promise of\ncombining dialogue act/intent modelling and neural response generation. This\nhybrid method improves the response quality of chatbots and makes them more\ncontrollable and interpretable. A key element in dialog intent modelling is the\ndevelopment of a taxonomy. Inspired by this idea, we have manually labeled 500\nresponse intents using a subset of a sizeable empathetic dialogue dataset (25K\ndialogues). Our goal is to produce a large-scale taxonomy for empathetic\nresponse intents. Furthermore, using lexical and machine learning methods, we\nautomatically analysed both speaker and listener utterances of the entire\ndataset with identified response intents and 32 emotion categories. Finally, we\nuse information visualization methods to summarize emotional dialogue exchange\npatterns and their temporal progression. These results reveal novel and\nimportant empathy patterns in human-human open-domain conversations and can\nserve as heuristics for hybrid approaches.", "published": "2020-12-07 21:56:45", "link": "http://arxiv.org/abs/2012.04080v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Big Green at WNUT 2020 Shared Task-1: Relation Extraction as\n  Contextualized Sequence Classification", "abstract": "Relation and event extraction is an important task in natural language\nprocessing. We introduce a system which uses contextualized knowledge graph\ncompletion to classify relations and events between known entities in a noisy\ntext environment. We report results which show that our system is able to\neffectively extract relations and events from a dataset of wet lab protocols.", "published": "2020-12-07 06:38:53", "link": "http://arxiv.org/abs/2012.04538v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From syntactic structure to semantic relationship: hypernym extraction\n  from definitions by recurrent neural networks using the part of speech\n  information", "abstract": "The hyponym-hypernym relation is an essential element in the semantic\nnetwork. Identifying the hypernym from a definition is an important task in\nnatural language processing and semantic analysis. While a public dictionary\nsuch as WordNet works for common words, its application in domain-specific\nscenarios is limited. Existing tools for hypernym extraction either rely on\nspecific semantic patterns or focus on the word representation, which all\ndemonstrate certain limitations.", "published": "2020-12-07 02:18:49", "link": "http://arxiv.org/abs/2012.03418v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Empirical Survey of Unsupervised Text Representation Methods on\n  Twitter Data", "abstract": "The field of NLP has seen unprecedented achievements in recent years. Most\nnotably, with the advent of large-scale pre-trained Transformer-based language\nmodels, such as BERT, there has been a noticeable improvement in text\nrepresentation. It is, however, unclear whether these improvements translate to\nnoisy user-generated text, such as tweets. In this paper, we present an\nexperimental survey of a wide range of well-known text representation\ntechniques for the task of text clustering on noisy Twitter data. Our results\nindicate that the more advanced models do not necessarily work best on tweets\nand that more exploration in this area is needed.", "published": "2020-12-07 06:14:13", "link": "http://arxiv.org/abs/2012.03468v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KgPLM: Knowledge-guided Language Model Pre-training via Generative and\n  Discriminative Learning", "abstract": "Recent studies on pre-trained language models have demonstrated their ability\nto capture factual knowledge and applications in knowledge-aware downstream\ntasks. In this work, we present a language model pre-training framework guided\nby factual knowledge completion and verification, and use the generative and\ndiscriminative approaches cooperatively to learn the model. Particularly, we\ninvestigate two learning schemes, named two-tower scheme and pipeline scheme,\nin training the generator and discriminator with shared parameter. Experimental\nresults on LAMA, a set of zero-shot cloze-style question answering tasks, show\nthat our model contains richer factual knowledge than the conventional\npre-trained language models. Furthermore, when fine-tuned and evaluated on the\nMRQA shared tasks which consists of several machine reading comprehension\ndatasets, our model achieves the state-of-the-art performance, and gains large\nimprovements on NewsQA (+1.26 F1) and TriviaQA (+1.56 F1) over RoBERTa.", "published": "2020-12-07 09:39:25", "link": "http://arxiv.org/abs/2012.03551v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PPKE: Knowledge Representation Learning by Path-based Pre-training", "abstract": "Entities may have complex interactions in a knowledge graph (KG), such as\nmulti-step relationships, which can be viewed as graph contextual information\nof the entities. Traditional knowledge representation learning (KRL) methods\nusually treat a single triple as a training unit, and neglect most of the graph\ncontextual information exists in the topological structure of KGs. In this\nstudy, we propose a Path-based Pre-training model to learn Knowledge\nEmbeddings, called PPKE, which aims to integrate more graph contextual\ninformation between entities into the KRL model. Experiments demonstrate that\nour model achieves state-of-the-art results on several benchmark datasets for\nlink prediction and relation prediction tasks, indicating that our model\nprovides a feasible way to take advantage of graph contextual information in\nKGs.", "published": "2020-12-07 10:29:30", "link": "http://arxiv.org/abs/2012.03573v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Enhanced MeanSum Method For Generating Hotel Multi-Review\n  Summarizations", "abstract": "Multi-document summaritazion is the process of taking multiple texts as input\nand producing a short summary text based on the content of input texts. Up\nuntil recently, multi-document summarizers are mostly supervised extractive.\nHowever, supervised methods require datasets of large, paired document-summary\nexamples which are rare and expensive to produce. In 2018, an unsupervised\nmulti-document abstractive summarization method(Meansum) was proposed by Chu\nand Liu, and demonstrated competitive performances comparing to extractive\nmethods. Despite good evaluation results on automatic metrics, Meansum has\nmultiple limitations, notably the inability of dealing with multiple aspects.\nThe aim of this work was to use Multi-Aspect Masker(MAM) as content selector to\naddress the issue with multi-aspect. Moreover, we propose a regularizer to\ncontrol the length of the generated summaries. Through a series of experiments\non the hotel dataset from Trip Advisor, we validate our assumption and show\nthat our improved model achieves higher ROUGE, Sentiment Accuracy than the\noriginal Meansum method and also beats/ comprarable/close to the supervised\nbaseline.", "published": "2020-12-07 13:16:01", "link": "http://arxiv.org/abs/2012.03656v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Confidence-aware Non-repetitive Multimodal Transformers for TextCaps", "abstract": "When describing an image, reading text in the visual scene is crucial to\nunderstand the key information. Recent work explores the TextCaps task, i.e.\nimage captioning with reading Optical Character Recognition (OCR) tokens, which\nrequires models to read text and cover them in generated captions. Existing\napproaches fail to generate accurate descriptions because of their (1) poor\nreading ability; (2) inability to choose the crucial words among all extracted\nOCR tokens; (3) repetition of words in predicted captions. To this end, we\npropose a Confidence-aware Non-repetitive Multimodal Transformers (CNMT) to\ntackle the above challenges. Our CNMT consists of a reading, a reasoning and a\ngeneration modules, in which Reading Module employs better OCR systems to\nenhance text reading ability and a confidence embedding to select the most\nnoteworthy tokens. To address the issue of word redundancy in captions, our\nGeneration Module includes a repetition mask to avoid predicting repeated word\nin captions. Our model outperforms state-of-the-art models on TextCaps dataset,\nimproving from 81.0 to 93.0 in CIDEr. Our source code is publicly available.", "published": "2020-12-07 13:20:12", "link": "http://arxiv.org/abs/2012.03662v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Reference Knowledgeable Network for Machine Reading Comprehension", "abstract": "Multi-choice Machine Reading Comprehension (MRC) as a challenge requires\nmodels to select the most appropriate answer from a set of candidates with a\ngiven passage and question. Most of the existing researches focus on the\nmodeling of specific tasks or complex networks, without explicitly referring to\nrelevant and credible external knowledge sources, which are supposed to greatly\nmake up for the deficiency of the given passage. Thus we propose a novel\nreference-based knowledge enhancement model called Reference Knowledgeable\nNetwork (RekNet), which simulates human reading strategies to refine critical\ninformation from the passage and quote explicit knowledge in necessity. In\ndetail, RekNet refines finegrained critical information and defines it as\nReference Span, then quotes explicit knowledge quadruples by the co-occurrence\ninformation of Reference Span and candidates. The proposed RekNet is evaluated\non three multi-choice MRC benchmarks: RACE, DREAM and Cosmos QA, obtaining\nconsistent and remarkable performance improvement with observable statistical\nsignificance level over strong baselines. Our code is available at\nhttps://github.com/Yilin1111/RekNet.", "published": "2020-12-07 14:11:33", "link": "http://arxiv.org/abs/2012.03709v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Foundations for Near-Term Quantum Natural Language Processing", "abstract": "We provide conceptual and mathematical foundations for near-term quantum\nnatural language processing (QNLP), and do so in quantum computer scientist\nfriendly terms. We opted for an expository presentation style, and provide\nreferences for supporting empirical evidence and formal statements concerning\nmathematical generality.\n  We recall how the quantum model for natural language that we employ\ncanonically combines linguistic meanings with rich linguistic structure, most\nnotably grammar. In particular, the fact that it takes a quantum-like model to\ncombine meaning and structure, establishes QNLP as quantum-native, on par with\nsimulation of quantum systems. Moreover, the now leading Noisy\nIntermediate-Scale Quantum (NISQ) paradigm for encoding classical data on\nquantum hardware, variational quantum circuits, makes NISQ exceptionally\nQNLP-friendly: linguistic structure can be encoded as a free lunch, in contrast\nto the apparently exponentially expensive classical encoding of grammar.\n  Quantum speed-up for QNLP tasks has already been established in previous work\nwith Will Zeng. Here we provide a broader range of tasks which all enjoy the\nsame advantage.\n  Diagrammatic reasoning is at the heart of QNLP. Firstly, the quantum model\ninterprets language as quantum processes via the diagrammatic formalism of\ncategorical quantum mechanics. Secondly, these diagrams are via ZX-calculus\ntranslated into quantum circuits. Parameterisations of meanings then become the\ncircuit variables to be learned.\n  Our encoding of linguistic structure within quantum circuits also embodies a\nnovel approach for establishing word-meanings that goes beyond the current\nstandards in mainstream AI, by placing linguistic structure at the heart of\nWittgenstein's meaning-is-context.", "published": "2020-12-07 14:49:33", "link": "http://arxiv.org/abs/2012.03755v1", "categories": ["quant-ph", "cs.CL"], "primary_category": "quant-ph"}
{"title": "Grammar-aware sentence classification on quantum computers", "abstract": "Natural language processing (NLP) is at the forefront of great advances in\ncontemporary AI, and it is arguably one of the most challenging areas of the\nfield. At the same time, in the area of Quantum Computing (QC), with the steady\ngrowth of quantum hardware and notable improvements towards implementations of\nquantum algorithms, we are approaching an era when quantum computers perform\ntasks that cannot be done on classical computers with a reasonable amount of\nresources. This provides a new range of opportunities for AI, and for NLP\nspecifically. In this work, we work with the Categorical Distributional\nCompositional (DisCoCat) model of natural language meaning, whose underlying\nmathematical underpinnings make it amenable to quantum instantiations. Earlier\nwork on fault-tolerant quantum algorithms has already demonstrated potential\nquantum advantage for NLP, notably employing DisCoCat. In this work, we focus\non the capabilities of noisy intermediate-scale quantum (NISQ) hardware and\nperform the first implementation of an NLP task on a NISQ processor, using the\nDisCoCat framework. Sentences are instantiated as parameterised quantum\ncircuits; word-meanings are embedded in quantum states using parameterised\nquantum-circuits and the sentence's grammatical structure faithfully manifests\nas a pattern of entangling operations which compose the word-circuits into a\nsentence-circuit. The circuits' parameters are trained using a classical\noptimiser in a supervised NLP task of binary classification. Our novel QNLP\nmodel shows concrete promise for scalability as the quality of the quantum\nhardware improves in the near future and solidifies a novel branch of\nexperimental research at the intersection of QC and AI.", "published": "2020-12-07 14:49:34", "link": "http://arxiv.org/abs/2012.03756v2", "categories": ["quant-ph", "cs.CL"], "primary_category": "quant-ph"}
{"title": "Evaluating Cross-Lingual Transfer Learning Approaches in Multilingual\n  Conversational Agent Models", "abstract": "With the recent explosion in popularity of voice assistant devices, there is\na growing interest in making them available to user populations in additional\ncountries and languages. However, to provide the highest accuracy and best\nperformance for specific user populations, most existing voice assistant models\nare developed individually for each region or language, which requires linear\ninvestment of effort. In this paper, we propose a general multilingual model\nframework for Natural Language Understanding (NLU) models, which can help\nbootstrap new language models faster and reduce the amount of effort required\nto develop each language separately. We explore how different deep learning\narchitectures affect multilingual NLU model performance. Our experimental\nresults show that these multilingual models can reach same or better\nperformance compared to monolingual models across language-specific test data\nwhile require less effort in creating features and model maintenance.", "published": "2020-12-07 17:14:52", "link": "http://arxiv.org/abs/2012.03864v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Clinical Document Understanding on COVID-19 Research with\n  Spark NLP", "abstract": "Following the global COVID-19 pandemic, the number of scientific papers\nstudying the virus has grown massively, leading to increased interest in\nautomated literate review. We present a clinical text mining system that\nimproves on previous efforts in three ways. First, it can recognize over 100\ndifferent entity types including social determinants of health, anatomy, risk\nfactors, and adverse events in addition to other commonly used clinical and\nbiomedical entities. Second, the text processing pipeline includes assertion\nstatus detection, to distinguish between clinical facts that are present,\nabsent, conditional, or about someone other than the patient. Third, the deep\nlearning models used are more accurate than previously available, leveraging an\nintegrated pipeline of state-of-the-art pretrained named entity recognition\nmodels, and improving on the previous best performing benchmarks for assertion\nstatus detection. We illustrate extracting trends and insights, e.g. most\nfrequent disorders and symptoms, and most common vital signs and EKG findings,\nfrom the COVID-19 Open Research Dataset (CORD-19). The system is built using\nthe Spark NLP library which natively supports scaling to use distributed\nclusters, leveraging GPUs, configurable and reusable NLP pipelines, healthcare\nspecific embeddings, and the ability to train models to support new entity\ntypes or human languages with no code changes.", "published": "2020-12-07 19:17:05", "link": "http://arxiv.org/abs/2012.04005v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Semantics Altering Modifications for Evaluating Comprehension in Machine\n  Reading", "abstract": "Advances in NLP have yielded impressive results for the task of machine\nreading comprehension (MRC), with approaches having been reported to achieve\nperformance comparable to that of humans. In this paper, we investigate whether\nstate-of-the-art MRC models are able to correctly process Semantics Altering\nModifications (SAM): linguistically-motivated phenomena that alter the\nsemantics of a sentence while preserving most of its lexical surface form. We\npresent a method to automatically generate and align challenge sets featuring\noriginal and altered examples. We further propose a novel evaluation\nmethodology to correctly assess the capability of MRC systems to process these\nexamples independent of the data they were optimised on, by discounting for\neffects introduced by domain shift. In a large-scale empirical study, we apply\nthe methodology in order to evaluate extractive MRC models with regard to their\ncapability to correctly process SAM-enriched data. We comprehensively cover 12\ndifferent state-of-the-art neural architecture configurations and four training\ndatasets and find that -- despite their well-known remarkable performance --\noptimised models consistently struggle to correctly process semantically\naltered data.", "published": "2020-12-07 21:00:42", "link": "http://arxiv.org/abs/2012.04056v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using multiple ASR hypotheses to boost i18n NLU performance", "abstract": "Current voice assistants typically use the best hypothesis yielded by their\nAutomatic Speech Recognition (ASR) module as input to their Natural Language\nUnderstanding (NLU) module, thereby losing helpful information that might be\nstored in lower-ranked ASR hypotheses. We explore the change in performance of\nNLU associated tasks when utilizing five-best ASR hypotheses when compared to\nstatus quo for two language datasets, German and Portuguese. To harvest\ninformation from the ASR five-best, we leverage extractive summarization and\njoint extractive-abstractive summarization models for Domain Classification\n(DC) experiments while using a sequence-to-sequence model with a pointer\ngenerator network for Intent Classification (IC) and Named Entity Recognition\n(NER) multi-task experiments. For the DC full test set, we observe significant\nimprovements of up to 7.2% and 15.5% in micro-averaged F1 scores, for German\nand Portuguese, respectively. In cases where the best ASR hypothesis was not an\nexact match to the transcribed utterance (mismatched test set), we see\nimprovements of up to 6.7% and 8.8% micro-averaged F1 scores, for German and\nPortuguese, respectively. For IC and NER multi-task experiments, when\nevaluating on the mismatched test set, we see improvements across all domains\nin German and in 17 out of 19 domains in Portuguese (improvements based on\nchange in SeMER scores). Our results suggest that the use of multiple ASR\nhypotheses, as opposed to one, can lead to significant performance improvements\nin the DC task for these non-English datasets. In addition, it could lead to\nsignificant improvement in the performance of IC and NER tasks in cases where\nthe ASR model makes mistakes.", "published": "2020-12-07 22:37:38", "link": "http://arxiv.org/abs/2012.04099v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dartmouth CS at WNUT-2020 Task 2: Informative COVID-19 Tweet\n  Classification Using BERT", "abstract": "We describe the systems developed for the WNUT-2020 shared task 2,\nidentification of informative COVID-19 English Tweets. BERT is a highly\nperformant model for Natural Language Processing tasks. We increased BERT's\nperformance in this classification task by fine-tuning BERT and concatenating\nits embeddings with Tweet-specific features and training a Support Vector\nMachine (SVM) for classification (henceforth called BERT+). We compared its\nperformance to a suite of machine learning models. We used a Twitter specific\ndata cleaning pipeline and word-level TF-IDF to extract features for the\nnon-BERT models. BERT+ was the top performing model with an F1-score of 0.8713.", "published": "2020-12-07 07:55:31", "link": "http://arxiv.org/abs/2012.04539v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improvements and Extensions on Metaphor Detection", "abstract": "Metaphors are ubiquitous in human language. The metaphor detection task (MD)\naims at detecting and interpreting metaphors from written language, which is\ncrucial in natural language understanding (NLU) research. In this paper, we\nintroduce a pre-trained Transformer-based model into MD. Our model outperforms\nthe previous state-of-the-art models by large margins in our evaluations, with\nrelative improvements on the F-1 score from 5.33% to 28.39%. Second, we extend\nMD to a classification task about the metaphoricity of an entire piece of text\nto make MD applicable in more general NLU scenes. Finally, we clean up the\nimproper or outdated annotations in one of the MD benchmark datasets and\nre-benchmark it with our Transformer-based model. This approach could be\napplied to other existing MD datasets as well, since the metaphoricity\nannotations in these benchmark datasets may be outdated. Future research\nefforts are also necessary to build an up-to-date and well-annotated dataset\nconsisting of longer and more complex texts.", "published": "2020-12-07 08:17:42", "link": "http://arxiv.org/abs/2012.04540v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MLS: A Large-Scale Multilingual Dataset for Speech Research", "abstract": "This paper introduces Multilingual LibriSpeech (MLS) dataset, a large\nmultilingual corpus suitable for speech research. The dataset is derived from\nread audiobooks from LibriVox and consists of 8 languages, including about\n44.5K hours of English and a total of about 6K hours for other languages.\nAdditionally, we provide Language Models (LM) and baseline Automatic Speech\nRecognition (ASR) models and for all the languages in our dataset. We believe\nsuch a large transcribed dataset will open new avenues in ASR and\nText-To-Speech (TTS) research. The dataset will be made freely available for\nanyone at http://www.openslr.org.", "published": "2020-12-07 01:53:45", "link": "http://arxiv.org/abs/2012.03411v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MFST: A Python OpenFST Wrapper With Support for Custom Semirings and\n  Jupyter Notebooks", "abstract": "This paper introduces mFST, a new Python library for working with\nFinite-State Machines based on OpenFST. mFST is a thin wrapper for OpenFST and\nexposes all of OpenFST's methods for manipulating FSTs. Additionally, mFST is\nthe only Python wrapper for OpenFST that exposes OpenFST's ability to define a\ncustom semirings. This makes mFST ideal for developing models that involve\nlearning the weights on a FST or creating neuralized FSTs. mFST has been\ndesigned to be easy to get started with and has been previously used in\nhomework assignments for a NLP class as well in projects for integrating FSTs\nand neural networks. In this paper, we exhibit mFST API and how to use mFST to\nbuild a simple neuralized FST with PyTorch.", "published": "2020-12-07 03:36:54", "link": "http://arxiv.org/abs/2012.03437v1", "categories": ["cs.LG", "cs.CL", "cs.MS"], "primary_category": "cs.LG"}
{"title": "Diverse Melody Generation from Chinese Lyrics via Mutual Information\n  Maximization", "abstract": "In this paper, we propose to adapt the method of mutual information\nmaximization into the task of Chinese lyrics conditioned melody generation to\nimprove the generation quality and diversity. We employ scheduled sampling and\nforce decoding techniques to improve the alignment between lyrics and melodies.\nWith our method, which we called Diverse Melody Generation (DMG), a\nsequence-to-sequence model learns to generate diverse melodies heavily\ndepending on the input style ids, while keeping the tonality and improving the\nalignment. The experimental results of subjective tests show that DMG can\ngenerate more pleasing and coherent tunes than baseline methods.", "published": "2020-12-07 15:48:01", "link": "http://arxiv.org/abs/2012.03805v1", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Frame-level SpecAugment for Deep Convolutional Neural Networks in Hybrid\n  ASR Systems", "abstract": "Inspired by SpecAugment -- a data augmentation method for end-to-end ASR\nsystems, we propose a frame-level SpecAugment method (f-SpecAugment) to improve\nthe performance of deep convolutional neural networks (CNN) for hybrid HMM\nbased ASR systems. Similar to the utterance level SpecAugment, f-SpecAugment\nperforms three transformations: time warping, frequency masking, and time\nmasking. Instead of applying the transformations at the utterance level,\nf-SpecAugment applies them to each convolution window independently during\ntraining. We demonstrate that f-SpecAugment is more effective than the\nutterance level SpecAugment for deep CNN based hybrid models. We evaluate the\nproposed f-SpecAugment on 50-layer Self-Normalizing Deep CNN (SNDCNN) acoustic\nmodels trained with up to 25000 hours of training data. We observe\nf-SpecAugment reduces WER by 0.5-4.5% relatively across different ASR tasks for\nfour languages. As the benefits of augmentation techniques tend to diminish as\ntraining data size increases, the large scale training reported is important in\nunderstanding the effectiveness of f-SpecAugment. Our experiments demonstrate\nthat even with 25k training data, f-SpecAugment is still effective. We also\ndemonstrate that f-SpecAugment has benefits approximately equivalent to\ndoubling the amount of training data for deep CNNs.", "published": "2020-12-07 22:27:13", "link": "http://arxiv.org/abs/2012.04094v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Detecting Insincere Questions from Text: A Transfer Learning Approach", "abstract": "The internet today has become an unrivalled source of information where\npeople converse on content based websites such as Quora, Reddit, StackOverflow\nand Twitter asking doubts and sharing knowledge with the world. A major arising\nproblem with such websites is the proliferation of toxic comments or instances\nof insincerity wherein the users instead of maintaining a sincere motive\nindulge in spreading toxic and divisive content. The straightforward course of\naction in confronting this situation is detecting such content beforehand and\npreventing it from subsisting online. In recent times Transfer Learning in\nNatural Language Processing has seen an unprecedented growth. Today with the\nexistence of transformers and various state of the art innovations, a\ntremendous growth has been made in various NLP domains. The introduction of\nBERT has caused quite a stir in the NLP community. As mentioned, when\npublished, BERT dominated performance benchmarks and thereby inspired many\nother authors to experiment with it and publish similar models. This led to the\ndevelopment of a whole BERT-family, each member being specialized on a\ndifferent task. In this paper we solve the Insincere Questions Classification\nproblem by fine tuning four cutting age models viz BERT, RoBERTa, DistilBERT\nand ALBERT.", "published": "2020-12-07 15:03:48", "link": "http://arxiv.org/abs/2012.07587v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EfficientTTS: An Efficient and High-Quality Text-to-Speech Architecture", "abstract": "In this work, we address the Text-to-Speech (TTS) task by proposing a\nnon-autoregressive architecture called EfficientTTS. Unlike the dominant\nnon-autoregressive TTS models, which are trained with the need of external\naligners, EfficientTTS optimizes all its parameters with a stable, end-to-end\ntraining procedure, while allowing for synthesizing high quality speech in a\nfast and efficient manner. EfficientTTS is motivated by a new monotonic\nalignment modeling approach (also introduced in this work), which specifies\nmonotonic constraints to the sequence alignment with almost no increase of\ncomputation. By combining EfficientTTS with different feed-forward network\nstructures, we develop a family of TTS models, including both\ntext-to-melspectrogram and text-to-waveform networks. We experimentally show\nthat the proposed models significantly outperform counterpart models such as\nTacotron 2 and Glow-TTS in terms of speech quality, training efficiency and\nsynthesis speed, while still producing the speeches of strong robustness and\ngreat diversity. In addition, we demonstrate that proposed approach can be\neasily extended to autoregressive models such as Tacotron 2.", "published": "2020-12-07 07:46:46", "link": "http://arxiv.org/abs/2012.03500v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards speech enhancement using a variational U-Net architecture", "abstract": "We investigate the viability of a variational U-Net architecture for\ndenoising of single-channel audio data. Deep network speech enhancement systems\ncommonly aim to estimate filter masks, or opt to work on the waveform signal,\npotentially neglecting relationships across higher dimensional spectro-temporal\nfeatures. We study the adoption of a probabilistic bottleneck into the classic\nU-Net architecture for direct spectral reconstruction. Evaluation of several\nablation network variants is carried out using signal-to-distortion ratio and\nperceptual measures, on audio data that includes known and unknown noise types\nas well as reverberation. Our experiments show that the residual (skip)\nconnections in the proposed system are a prerequisite for successful spectral\nreconstruction, i.e., without filter mask estimation. Results show, on average,\nan advantage of the proposed variational U-Net architecture over its classic,\nnon-variational version in signal enhancement performance under reverberant\nconditions of 0.31 and 6.98 in PESQ and STOI scores, respectively. Anecdotal\nevidence points to improved suppression of impulsive noise sources with the\nvariational U-Net compared to the recurrent mask estimation network baseline.", "published": "2020-12-07 11:30:35", "link": "http://arxiv.org/abs/2012.03594v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Modeling the effects of dynamic range compression on signals in noise", "abstract": "Hearing aids use dynamic range compression (DRC), a form of automatic gain\ncontrol, to make quiet sounds louder and loud sounds quieter. Compression can\nimprove listening comfort, but it can also cause distortion in noisy\nenvironments. It has been widely reported that DRC performs poorly in noise,\nbut there has been little mathematical analysis of these distortion effects.\nThis work introduces a mathematical model to study the behavior of DRC in\nnoise. Using statistical assumptions about the signal envelopes, we define an\neffective compression function that models the compression applied to one\nsignal in the presence of another. This framework is used to prove results\nabout DRC that have been previously observed experimentally: that when DRC is\napplied to a mixture of signals, uncorrelated signal envelopes become\nnegatively correlated; that the effective compression applied to each sound in\na mixture is weaker than it would have been for the signal alone; and that\ncompression can reduce the long-term signal-to-noise ratio in certain\nconditions. These theoretical results are supported by software experiments\nusing recorded speech signals.", "published": "2020-12-07 17:09:48", "link": "http://arxiv.org/abs/2012.03860v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-Instrumentalist Net: Unsupervised Generation of Music from Body\n  Movements", "abstract": "We propose a novel system that takes as an input body movements of a musician\nplaying a musical instrument and generates music in an unsupervised setting.\nLearning to generate multi-instrumental music from videos without labeling the\ninstruments is a challenging problem. To achieve the transformation, we built a\npipeline named 'Multi-instrumentalistNet' (MI Net). At its base, the pipeline\nlearns a discrete latent representation of various instruments music from\nlog-spectrogram using a Vector Quantized Variational Autoencoder (VQ-VAE) with\nmulti-band residual blocks. The pipeline is then trained along with an\nautoregressive prior conditioned on the musician's body keypoints movements\nencoded by a recurrent neural network. Joint training of the prior with the\nbody movements encoder succeeds in the disentanglement of the music into latent\nfeatures indicating the musical components and the instrumental features. The\nlatent space results in distributions that are clustered into distinct\ninstruments from which new music can be generated. Furthermore, the VQ-VAE\narchitecture supports detailed music generation with additional conditioning.\nWe show that a Midi can further condition the latent space such that the\npipeline will generate the exact content of the music being played by the\ninstrument in the video. We evaluate MI Net on two datasets containing videos\nof 13 instruments and obtain generated music of reasonable audio quality,\neasily associated with the corresponding instrument, and consistent with the\nmusic audio content.", "published": "2020-12-07 06:54:10", "link": "http://arxiv.org/abs/2012.03478v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Reverberant Sound Localization with a Robot Head Based on Direct-Path\n  Relative Transfer Function", "abstract": "This paper addresses the problem of sound-source localization (SSL) with a\nrobot head, which remains a challenge in real-world environments. In particular\nwe are interested in locating speech sources, as they are of high interest for\nhuman-robot interaction. The microphone-pair response corresponding to the\ndirect-path sound propagation is a function of the source direction. In\npractice, this response is contaminated by noise and reverberations. The\ndirect-path relative transfer function (DP-RTF) is defined as the ratio between\nthe direct-path acoustic transfer function (ATF) of the two microphones, and it\nis an important feature for SSL. We propose a method to estimate the DP-RTF\nfrom noisy and reverberant signals in the short-time Fourier transform (STFT)\ndomain. First, the convolutive transfer function (CTF) approximation is adopted\nto accurately represent the impulse response of the microphone array, and the\nfirst coefficient of the CTF is mainly composed of the direct-path ATF. At each\nfrequency, the frame-wise speech auto- and cross-power spectral density (PSD)\nare obtained by spectral subtraction. Then a set of linear equations is\nconstructed by the speech auto- and cross-PSD of multiple frames, in which the\nDP-RTF is an unknown variable, and is estimated by solving the equations.\nFinally, the estimated DP-RTFs are concatenated across frequencies and used as\na feature vector for SSL. Experiments with a robot, placed in various\nreverberant environments, show that the proposed method outperforms two\nstate-of-the-art methods.", "published": "2020-12-07 10:29:40", "link": "http://arxiv.org/abs/2012.03574v1", "categories": ["cs.SD", "cs.RO", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Imagery Classification using Length-Wise Training based on Deep\n  Learning", "abstract": "Brain-computer interface uses brain signals to control external devices\nwithout actual control behavior. Recently, speech imagery has been studied for\ndirect communication using language. Speech imagery uses brain signals\ngenerated when the user imagines speech. Unlike motor imagery, speech imagery\nstill has unknown characteristics. Additionally, electroencephalography has\nintricate and non-stationary properties resulting in insufficient decoding\nperformance. In addition, speech imagery is difficult to utilize spatial\nfeatures. In this study, we designed length-wise training that allows the model\nto classify a series of a small number of words. In addition, we proposed\nhierarchical convolutional neural network structure and loss function to\nmaximize the training strategy. The proposed method showed competitive\nperformance in speech imagery classification. Hence, we demonstrated that the\nlength of the word is a clue at improving classification performance.", "published": "2020-12-07 12:25:33", "link": "http://arxiv.org/abs/2012.03632v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "A novel dataset for the identification of computer generated melodies in\n  the CSMT challenge", "abstract": "In this paper, the dataset used for the data challenge organised by\nConference on Sound and Music Technology (CSMT) is introduced. The CSMT data\nchallenge requires participants to identify whether a given piece of melody is\ngenerated by computer or is composed by human. The dataset is formed by two\nparts: development dataset and evaluation dataset. The development dataset\ncontains only computer generated melodies whereas the evaluation dataset\ncontain both computer generated melodies and human composed melodies. The aim\nof the dataset is to examine whether it is possible to distinguish computer\ngenerated melodies by learning the feature of generated melodies.", "published": "2020-12-07 12:58:16", "link": "http://arxiv.org/abs/2012.03646v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
