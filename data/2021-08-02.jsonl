{"title": "Logic-Consistency Text Generation from Semantic Parses", "abstract": "Text generation from semantic parses is to generate textual descriptions for\nformal representation inputs such as logic forms and SQL queries. This is\nchallenging due to two reasons: (1) the complex and intensive inner logic with\nthe data scarcity constraint, (2) the lack of automatic evaluation metrics for\nlogic consistency. To address these two challenges, this paper first proposes\nSNOWBALL, a framework for logic consistent text generation from semantic parses\nthat employs an iterative training procedure by recursively augmenting the\ntraining set with quality control. Second, we propose a novel automatic metric,\nBLEC, for evaluating the logical consistency between the semantic parses and\ngenerated texts. The experimental results on two benchmark datasets, Logic2Text\nand Spider, demonstrate the SNOWBALL framework enhances the logic consistency\non both BLEC and human evaluation. Furthermore, our statistical analysis\nreveals that BLEC is more logically consistent with human evaluation than\ngeneral-purpose automatic metrics including BLEU, ROUGE and, BLEURT. Our data\nand code are available at https://github.com/Ciaranshu/relogic.", "published": "2021-08-02 01:12:18", "link": "http://arxiv.org/abs/2108.00577v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From LSAT: The Progress and Challenges of Complex Reasoning", "abstract": "Complex reasoning aims to draw a correct inference based on complex rules. As\na hallmark of human intelligence, it involves a degree of explicit reading\ncomprehension, interpretation of logical knowledge and complex rule\napplication. In this paper, we take a step forward in complex reasoning by\nsystematically studying the three challenging and domain-general tasks of the\nLaw School Admission Test (LSAT), including analytical reasoning, logical\nreasoning and reading comprehension. We propose a hybrid reasoning system to\nintegrate these three tasks and achieve impressive overall performance on the\nLSAT tests. The experimental results demonstrate that our system endows itself\na certain complex reasoning ability, especially the fundamental reading\ncomprehension and challenging logical reasoning capacities. Further analysis\nalso shows the effectiveness of combining the pre-trained models with the\ntask-specific reasoning module, and integrating symbolic knowledge into\ndiscrete interpretable reasoning steps in complex reasoning. We further shed a\nlight on the potential future directions, like unsupervised symbolic knowledge\nextraction, model interpretability, few-shot learning and comprehensive\nbenchmark for complex reasoning.", "published": "2021-08-02 05:43:03", "link": "http://arxiv.org/abs/2108.00648v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConveRT for FAQ Answering", "abstract": "Knowledgeable FAQ chatbots are a valuable resource to any organization. While\npowerful and efficient retrieval-based models exist for English, it is rarely\nthe case for other languages for which the same amount of training data is not\navailable. In this paper, we propose a novel pre-training procedure to adapt\nConveRT, an English conversational retriever model, to other languages with\nless training data available. We apply it for the first time to the task of\nDutch FAQ answering related to the COVID-19 vaccine. We show it performs better\nthan an open-source alternative in both a low-data regime and a high-data\nregime.", "published": "2021-08-02 08:44:22", "link": "http://arxiv.org/abs/2108.00719v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic recognition of suprasegmentals in speech", "abstract": "This study reports our efforts to improve automatic recognition of\nsuprasegmentals by fine-tuning wav2vec 2.0 with CTC, a method that has been\nsuccessful in automatic speech recognition. We demonstrate that the method can\nimprove the state-of-the-art on automatic recognition of syllables, tones, and\npitch accents. Utilizing segmental information, by employing tonal finals or\ntonal syllables as recognition units, can significantly improve Mandarin tone\nrecognition. Language models are helpful when tonal syllables are used as\nrecognition units, but not helpful when tones are recognition units. Finally,\nMandarin tone recognition can benefit from English phoneme recognition by\ncombining the two tasks in fine-tuning wav2vec 2.0.", "published": "2021-08-02 18:47:59", "link": "http://arxiv.org/abs/2108.01122v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Role of Phonetic Units in Speech Emotion Recognition", "abstract": "We propose a method for emotion recognition through emotiondependent speech\nrecognition using Wav2vec 2.0. Our method achieved a significant improvement\nover most previously reported results on IEMOCAP, a benchmark emotion dataset.\nDifferent types of phonetic units are employed and compared in terms of\naccuracy and robustness of emotion recognition within and across datasets and\nlanguages. Models of phonemes, broad phonetic classes, and syllables all\nsignificantly outperform the utterance model, demonstrating that phonetic units\nare helpful and should be incorporated in speech emotion recognition. The best\nperformance is from using broad phonetic classes. Further research is needed to\ninvestigate the optimal set of broad phonetic classes for the task of emotion\nrecognition. Finally, we found that Wav2vec 2.0 can be fine-tuned to recognize\ncoarser-grained or larger phonetic units than phonemes, such as broad phonetic\nclasses and syllables.", "published": "2021-08-02 19:19:47", "link": "http://arxiv.org/abs/2108.01132v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Underreporting of errors in NLG output, and what to do about it", "abstract": "We observe a severe under-reporting of the different kinds of errors that\nNatural Language Generation systems make. This is a problem, because mistakes\nare an important indicator of where systems should still be improved. If\nauthors only report overall performance metrics, the research community is left\nin the dark about the specific weaknesses that are exhibited by\n`state-of-the-art' research. Next to quantifying the extent of error\nunder-reporting, this position paper provides recommendations for error\nidentification, analysis and reporting.", "published": "2021-08-02 21:29:00", "link": "http://arxiv.org/abs/2108.01182v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Multi-scale Convolution for Dialect Identification", "abstract": "Time Delay Neural Networks (TDNN)-based methods are widely used in dialect\nidentification. However, in previous work with TDNN application, subtle variant\nis being neglected in different feature scales. To address this issue, we\npropose a new architecture, named dynamic multi-scale convolution, which\nconsists of dynamic kernel convolution, local multi-scale learning, and global\nmulti-scale pooling. Dynamic kernel convolution captures features between\nshort-term and long-term context adaptively. Local multi-scale learning, which\nrepresents multi-scale features at a granular level, is able to increase the\nrange of receptive fields for convolution operation. Besides, global\nmulti-scale pooling is applied to aggregate features from different bottleneck\nlayers in order to collect information from multiple aspects. The proposed\narchitecture significantly outperforms state-of-the-art system on the\nAP20-OLR-dialect-task of oriental language recognition (OLR) challenge 2020,\nwith the best average cost performance (Cavg) of 0.067 and the best equal error\nrate (EER) of 6.52%. Compared with the known best results, our method achieves\n9% of Cavg and 45% of EER relative improvement, respectively. Furthermore, the\nparameters of proposed model are 91% fewer than the best known model.", "published": "2021-08-02 03:37:15", "link": "http://arxiv.org/abs/2108.07787v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MuSiQue: Multihop Questions via Single-hop Question Composition", "abstract": "Multihop reasoning remains an elusive goal as existing multihop benchmarks\nare known to be largely solvable via shortcuts. Can we create a question\nanswering (QA) dataset that, by construction, \\emph{requires} proper multihop\nreasoning? To this end, we introduce a bottom-up approach that systematically\nselects composable pairs of single-hop questions that are connected, i.e.,\nwhere one reasoning step critically relies on information from another. This\nbottom-up methodology lets us explore a vast space of questions and add\nstringent filters as well as other mechanisms targeting connected reasoning. It\nprovides fine-grained control over the construction process and the properties\nof the resulting $k$-hop questions. We use this methodology to create\nMuSiQue-Ans, a new multihop QA dataset with 25K 2-4 hop questions. Relative to\nexisting datasets, MuSiQue-Ans is more difficult overall (3x increase in\nhuman-machine gap), and harder to cheat via disconnected reasoning (e.g., a\nsingle-hop model has a 30 point drop in F1). We further add unanswerable\ncontrast questions to produce a more stringent dataset, MuSiQue-Full. We hope\nour datasets will help the NLP community develop models that perform genuine\nmultihop reasoning.", "published": "2021-08-02 00:33:27", "link": "http://arxiv.org/abs/2108.00573v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Is My Model Using The Right Evidence? Systematic Probes for Examining\n  Evidence-Based Tabular Reasoning", "abstract": "Neural models command state-of-the-art performance across NLP tasks,\nincluding ones involving \"reasoning\". Models claiming to reason about the\nevidence presented to them should attend to the correct parts of the input\navoiding spurious patterns therein, be self-consistent in their predictions\nacross inputs, and be immune to biases derived from their pre-training in a\nnuanced, context-sensitive fashion. {\\em Do the prevalent *BERT-family of\nmodels do so?} In this paper, we study this question using the problem of\nreasoning on tabular data. Tabular inputs are especially well-suited for the\nstudy -- they admit systematic probes targeting the properties listed above.\nOur experiments demonstrate that a RoBERTa-based model, representative of the\ncurrent state-of-the-art, fails at reasoning on the following counts: it (a)\nignores relevant parts of the evidence, (b) is over-sensitive to annotation\nartifacts, and (c) relies on the knowledge encoded in the pre-trained language\nmodel rather than the evidence presented in its tabular inputs. Finally,\nthrough inoculation experiments, we show that fine-tuning the model on\nperturbed data does not help it overcome the above challenges.", "published": "2021-08-02 01:14:19", "link": "http://arxiv.org/abs/2108.00578v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transfer Learning for Mining Feature Requests and Bug Reports from\n  Tweets and App Store Reviews", "abstract": "Identifying feature requests and bug reports in user comments holds great\npotential for development teams. However, automated mining of RE-related\ninformation from social media and app stores is challenging since (1) about 70%\nof user comments contain noisy, irrelevant information, (2) the amount of user\ncomments grows daily making manual analysis unfeasible, and (3) user comments\nare written in different languages. Existing approaches build on traditional\nmachine learning (ML) and deep learning (DL), but fail to detect feature\nrequests and bug reports with high Recall and acceptable Precision which is\nnecessary for this task. In this paper, we investigate the potential of\ntransfer learning (TL) for the classification of user comments. Specifically,\nwe train both monolingual and multilingual BERT models and compare the\nperformance with state-of-the-art methods. We found that monolingual BERT\nmodels outperform existing baseline methods in the classification of English\nApp Reviews as well as English and Italian Tweets. However, we also observed\nthat the application of heavyweight TL models does not necessarily lead to\nbetter performance. In fact, our multilingual BERT models perform worse than\ntraditional ML methods.", "published": "2021-08-02 06:51:13", "link": "http://arxiv.org/abs/2108.00663v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Self-supervised Answer Retrieval on Clinical Notes", "abstract": "Retrieving answer passages from long documents is a complex task requiring\nsemantic understanding of both discourse and document context. We approach this\nchallenge specifically in a clinical scenario, where doctors retrieve cohorts\nof patients based on diagnoses and other latent medical aspects. We introduce\nCAPR, a rule-based self-supervision objective for training Transformer language\nmodels for domain-specific passage matching. In addition, we contribute a novel\nretrieval dataset based on clinical notes to simulate this scenario on a large\ncorpus of clinical notes. We apply our objective in four Transformer-based\narchitectures: Contextual Document Vectors, Bi-, Poly- and Cross-encoders. From\nour extensive evaluation on MIMIC-III and three other healthcare datasets, we\nreport that CAPR outperforms strong baselines in the retrieval of\ndomain-specific passages and effectively generalizes across rule-based and\nhuman-labeled passages. This makes the model powerful especially in zero-shot\nscenarios where only limited training data is available.", "published": "2021-08-02 10:42:52", "link": "http://arxiv.org/abs/2108.00775v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "LICHEE: Improving Language Model Pre-training with Multi-grained\n  Tokenization", "abstract": "Language model pre-training based on large corpora has achieved tremendous\nsuccess in terms of constructing enriched contextual representations and has\nled to significant performance gains on a diverse range of Natural Language\nUnderstanding (NLU) tasks. Despite the success, most current pre-trained\nlanguage models, such as BERT, are trained based on single-grained\ntokenization, usually with fine-grained characters or sub-words, making it hard\nfor them to learn the precise meaning of coarse-grained words and phrases. In\nthis paper, we propose a simple yet effective pre-training method named LICHEE\nto efficiently incorporate multi-grained information of input text. Our method\ncan be applied to various pre-trained language models and improve their\nrepresentation capability. Extensive experiments conducted on CLUE and\nSuperGLUE demonstrate that our method achieves comprehensive improvements on a\nwide variety of NLU tasks in both Chinese and English with little extra\ninference cost incurred, and that our best ensemble model achieves the\nstate-of-the-art performance on CLUE benchmark competition.", "published": "2021-08-02 12:08:19", "link": "http://arxiv.org/abs/2108.00801v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Relation Aware Semi-autoregressive Semantic Parsing for NL2SQL", "abstract": "Natural language to SQL (NL2SQL) aims to parse a natural language with a\ngiven database into a SQL query, which widely appears in practical Internet\napplications. Jointly encode database schema and question utterance is a\ndifficult but important task in NL2SQL. One solution is to treat the input as a\nheterogeneous graph. However, it failed to learn good word representation in\nquestion utterance. Learning better word representation is important for\nconstructing a well-designed NL2SQL system. To solve the challenging task, we\npresent a Relation aware Semi-autogressive Semantic Parsing (\\MODN) ~framework,\nwhich is more adaptable for NL2SQL. It first learns relation embedding over the\nschema entities and question words with predefined schema relations with\nELECTRA and relation aware transformer layer as backbone. Then we decode the\nquery SQL with a semi-autoregressive parser and predefined SQL syntax. From\nempirical results and case study, our model shows its effectiveness in learning\nbetter word representation in NL2SQL.", "published": "2021-08-02 12:21:08", "link": "http://arxiv.org/abs/2108.00804v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Changes in European Solidarity Before and During COVID-19: Evidence from\n  a Large Crowd- and Expert-Annotated Twitter Dataset", "abstract": "We introduce the well-established social scientific concept of social\nsolidarity and its contestation, anti-solidarity, as a new problem setting to\nsupervised machine learning in NLP to assess how European solidarity discourses\nchanged before and after the COVID-19 outbreak was declared a global pandemic.\nTo this end, we annotate 2.3k English and German tweets for (anti-)solidarity\nexpressions, utilizing multiple human annotators and two annotation approaches\n(experts vs.\\ crowds). We use these annotations to train a BERT model with\nmultiple data augmentation strategies. Our augmented BERT model that combines\nboth expert and crowd annotations outperforms the baseline BERT classifier\ntrained with expert annotations only by over 25 points, from 58\\% macro-F1 to\nalmost 85\\%. We use this high-quality model to automatically label over 270k\ntweets between September 2019 and December 2020. We then assess the\nautomatically labeled data for how statements related to European\n(anti-)solidarity discourses developed over time and in relation to one\nanother, before and during the COVID-19 crisis. Our results show that\nsolidarity became increasingly salient and contested during the crisis. While\nthe number of solidarity tweets remained on a higher level and dominated the\ndiscourse in the scrutinized time frame, anti-solidarity tweets initially\nspiked, then decreased to (almost) pre-COVID-19 values before rising to a\nstable higher level until the end of 2020.", "published": "2021-08-02 17:03:12", "link": "http://arxiv.org/abs/2108.01042v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Correcting Arabic Soft Spelling Mistakes using BiLSTM-based Machine\n  Learning", "abstract": "Soft spelling errors are a class of spelling mistakes that is widespread\namong native Arabic speakers and foreign learners alike. Some of these errors\nare typographical in nature. They occur due to orthographic variations of some\nArabic letters and the complex rules that dictate their correct usage. Many\npeople forgo these rules, and given the identical phonetic sounds, they often\nconfuse such letters. In this paper, we propose a bidirectional long short-term\nmemory network that corrects this class of errors. We develop, train, evaluate,\nand compare a set of BiLSTM networks. We approach the spelling correction\nproblem at the character level. We handle Arabic texts from both classical and\nmodern standard Arabic. We treat the problem as a one-to-one sequence\ntranscription problem. Since the soft Arabic errors class encompasses omission\nand addition mistakes, to preserve the one-to-one sequence transcription, we\npropose a simple low-resource yet effective technique that maintains the\none-to-one sequencing and avoids using a costly encoder-decoder architecture.\nWe train the BiLSTM models to correct the spelling mistakes using transformed\ninput and stochastic error injection approaches. We recommend a configuration\nthat has two BiLSTM layers, uses the dropout regularization, and is trained\nusing the latter training approach with error injection rate of 40%. The best\nmodel corrects 96.4% of the injected errors and achieves a low character error\nrate of 1.28% on a real test set of soft spelling mistakes.", "published": "2021-08-02 19:47:55", "link": "http://arxiv.org/abs/2108.01141v1", "categories": ["cs.CL", "cs.LG", "68T50 (Primary) 68T07 (Secondary)", "I.2.7; I.5.1; I.7.1"], "primary_category": "cs.CL"}
{"title": "Knowledge-intensive Language Understanding for Explainable AI", "abstract": "AI systems have seen significant adoption in various domains. At the same\ntime, further adoption in some domains is hindered by inability to fully trust\nan AI system that it will not harm a human. Besides the concerns for fairness,\nprivacy, transparency, and explainability are key to developing trusts in AI\nsystems. As stated in describing trustworthy AI \"Trust comes through\nunderstanding. How AI-led decisions are made and what determining factors were\nincluded are crucial to understand.\" The subarea of explaining AI systems has\ncome to be known as XAI. Multiple aspects of an AI system can be explained;\nthese include biases that the data might have, lack of data points in a\nparticular region of the example space, fairness of gathering the data, feature\nimportances, etc. However, besides these, it is critical to have human-centered\nexplanations that are directly related to decision-making similar to how a\ndomain expert makes decisions based on \"domain knowledge,\" that also include\nwell-established, peer-validated explicit guidelines. To understand and\nvalidate an AI system's outcomes (such as classification, recommendations,\npredictions), that lead to developing trust in the AI system, it is necessary\nto involve explicit domain knowledge that humans understand and use.", "published": "2021-08-02 21:12:30", "link": "http://arxiv.org/abs/2108.01174v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "The RareDis corpus: a corpus annotated with rare diseases, their signs\n  and symptoms", "abstract": "The RareDis corpus contains more than 5,000 rare diseases and almost 6,000\nclinical manifestations are annotated. Moreover, the Inter Annotator Agreement\nevaluation shows a relatively high agreement (F1-measure equal to 83.5% under\nexact match criteria for the entities and equal to 81.3% for the relations).\nBased on these results, this corpus is of high quality, supposing a significant\nstep for the field since there is a scarcity of available corpus annotated with\nrare diseases. This could open the door to further NLP applications, which\nwould facilitate the diagnosis and treatment of these rare diseases and,\ntherefore, would improve dramatically the quality of life of these patients.", "published": "2021-08-02 22:56:26", "link": "http://arxiv.org/abs/2108.01204v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Polarity in the Classroom: A Case Study Leveraging Peer Sentiment Toward\n  Scalable Assessment", "abstract": "Accurately grading open-ended assignments in large or massive open online\ncourses (MOOCs) is non-trivial. Peer review is a promising solution but can be\nunreliable due to few reviewers and an unevaluated review form. To date, no\nwork has 1) leveraged sentiment analysis in the peer-review process to inform\nor validate grades or 2) utilized aspect extraction to craft a review form from\nwhat students actually communicated. Our work utilizes, rather than discards,\nstudent data from review form comments to deliver better information to the\ninstructor. In this work, we detail the process by which we create our\ndomain-dependent lexicon and aspect-informed review form as well as our entire\nsentiment analysis algorithm which provides a fine-grained sentiment score from\ntext alone. We end by analyzing validity and discussing conclusions from our\ncorpus of over 6800 peer reviews from nine courses to understand the viability\nof sentiment in the classroom for increasing the information from and\nreliability of grading open-ended assignments in large courses.", "published": "2021-08-02 15:45:11", "link": "http://arxiv.org/abs/2108.10068v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "TabPert: An Effective Platform for Tabular Perturbation", "abstract": "To truly grasp reasoning ability, a Natural Language Inference model should\nbe evaluated on counterfactual data. TabPert facilitates this by assisting in\nthe generation of such counterfactual data for assessing model tabular\nreasoning issues. TabPert allows a user to update a table, change its\nassociated hypotheses, change their labels, and highlight rows that are\nimportant for hypothesis classification. TabPert also captures information\nabout the techniques used to automatically produce the table, as well as the\nstrategies employed to generate the challenging hypotheses. These\ncounterfactual tables and hypotheses, as well as the metadata, can then be used\nto explore an existing model's shortcomings methodically and quantitatively.", "published": "2021-08-02 02:37:48", "link": "http://arxiv.org/abs/2108.00603v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.SE"], "primary_category": "cs.CL"}
{"title": "StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators", "abstract": "Can a generative model be trained to produce images from a specific domain,\nguided by a text prompt only, without seeing any image? In other words: can an\nimage generator be trained \"blindly\"? Leveraging the semantic power of large\nscale Contrastive-Language-Image-Pre-training (CLIP) models, we present a\ntext-driven method that allows shifting a generative model to new domains,\nwithout having to collect even a single image. We show that through natural\nlanguage prompts and a few minutes of training, our method can adapt a\ngenerator across a multitude of domains characterized by diverse styles and\nshapes. Notably, many of these modifications would be difficult or outright\nimpossible to reach with existing methods. We conduct an extensive set of\nexperiments and comparisons across a wide range of domains. These demonstrate\nthe effectiveness of our approach and show that our shifted models maintain the\nlatent-space properties that make generative models appealing for downstream\ntasks.", "published": "2021-08-02 14:46:46", "link": "http://arxiv.org/abs/2108.00946v2", "categories": ["cs.CV", "cs.CL", "cs.GR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Decoupling recognition and transcription in Mandarin ASR", "abstract": "Much of the recent literature on automatic speech recognition (ASR) is taking\nan end-to-end approach. Unlike English where the writing system is closely\nrelated to sound, Chinese characters (Hanzi) represent meaning, not sound. We\npropose factoring audio -> Hanzi into two sub-tasks: (1) audio -> Pinyin and\n(2) Pinyin -> Hanzi, where Pinyin is a system of phonetic transcription of\nstandard Chinese. Factoring the audio -> Hanzi task in this way achieves 3.9%\nCER (character error rate) on the Aishell-1 corpus, the best result reported on\nthis dataset so far.", "published": "2021-08-02 19:09:41", "link": "http://arxiv.org/abs/2108.01129v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "PyEuroVoc: A Tool for Multilingual Legal Document Classification with\n  EuroVoc Descriptors", "abstract": "EuroVoc is a multilingual thesaurus that was built for organizing the\nlegislative documentary of the European Union institutions. It contains\nthousands of categories at different levels of specificity and its descriptors\nare targeted by legal texts in almost thirty languages. In this work we propose\na unified framework for EuroVoc classification on 22 languages by fine-tuning\nmodern Transformer-based pretrained language models. We study extensively the\nperformance of our trained models and show that they significantly improve the\nresults obtained by a similar tool - JEX - on the same dataset. The code and\nthe fine-tuned models were open sourced, together with a programmatic interface\nthat eases the process of loading the weights of a trained model and of\nclassifying a new document.", "published": "2021-08-02 19:46:21", "link": "http://arxiv.org/abs/2108.01139v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "User-Initiated Repetition-Based Recovery in Multi-Utterance Dialogue\n  Systems", "abstract": "Recognition errors are common in human communication. Similar errors often\nlead to unwanted behaviour in dialogue systems or virtual assistants. In human\ncommunication, we can recover from them by repeating misrecognized words or\nphrases; however in human-machine communication this recovery mechanism is not\navailable. In this paper, we attempt to bridge this gap and present a system\nthat allows a user to correct speech recognition errors in a virtual assistant\nby repeating misunderstood words. When a user repeats part of the phrase the\nsystem rewrites the original query to incorporate the correction. This rewrite\nallows the virtual assistant to understand the original query successfully. We\npresent an end-to-end 2-step attention pointer network that can generate the\nthe rewritten query by merging together the incorrectly understood utterance\nwith the correction follow-up. We evaluate the model on data collected for this\ntask and compare the proposed model to a rule-based baseline and a standard\npointer network. We show that rewriting the original query is an effective way\nto handle repetition-based recovery and that the proposed model outperforms the\nrule based baseline, reducing Word Error Rate by 19% relative at 2% False Alarm\nRate on annotated data.", "published": "2021-08-02 23:32:13", "link": "http://arxiv.org/abs/2108.01208v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Feature Learning of 1D Convolutional Neural Networks\n  with Contrastive Loss for Eating Detection Using an In-Ear Microphone", "abstract": "The importance of automated and objective monitoring of dietary behavior is\nbecoming increasingly accepted. The advancements in sensor technology along\nwith recent achievements in machine-learning--based signal-processing\nalgorithms have enabled the development of dietary monitoring solutions that\nyield highly accurate results. A common bottleneck for developing and training\nmachine learning algorithms is obtaining labeled data for training supervised\nalgorithms, and in particular ground truth annotations. Manual ground truth\nannotation is laborious, cumbersome, can sometimes introduce errors, and is\nsometimes impossible in free-living data collection. As a result, there is a\nneed to decrease the labeled data required for training. Additionally,\nunlabeled data, gathered in-the-wild from existing wearables (such as Bluetooth\nearbuds) can be used to train and fine-tune eating-detection models. In this\nwork, we focus on training a feature extractor for audio signals captured by an\nin-ear microphone for the task of eating detection in a self-supervised way. We\nbase our approach on the SimCLR method for image classification, proposed by\nChen et al. from the domain of computer vision. Results are promising as our\nself-supervised method achieves similar results to supervised training\nalternatives, and its overall effectiveness is comparable to current\nstate-of-the-art methods. Code is available at\nhttps://github.com/mug-auth/ssl-chewing .", "published": "2021-08-02 10:31:07", "link": "http://arxiv.org/abs/2108.00769v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Bite-Weight Estimation Using Commercial Ear Buds", "abstract": "While automatic tracking and measuring of our physical activity is a well\nestablished domain, not only in research but also in commercial products and\nevery-day life-style, automatic measurement of eating behavior is significantly\nmore limited. Despite the abundance of methods and algorithms that are\navailable in bibliography, commercial solutions are mostly limited to digital\nlogging applications for smart-phones. One factor that limits the adoption of\nsuch solutions is that they usually require specialized hardware or sensors.\nBased on this, we evaluate the potential for estimating the weight of consumed\nfood (per bite) based only on the audio signal that is captured by commercial\near buds (Samsung Galaxy Buds). Specifically, we examine a combination of\nfeatures (both audio and non-audio features) and trainable estimators (linear\nregression, support vector regression, and neural-network based estimators) and\nevaluate on an in-house dataset of 8 participants and 4 food types. Results\nindicate good potential for this approach: our best results yield mean absolute\nerror of less than 1 g for 3 out of 4 food types when training food-specific\nmodels, and 2.1 g when training on all food types together, both of which\nimprove over an existing literature approach.", "published": "2021-08-02 10:34:25", "link": "http://arxiv.org/abs/2108.00771v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Adversarial Data Augmentation for Disordered Speech Recognition", "abstract": "Automatic recognition of disordered speech remains a highly challenging task\nto date. The underlying neuro-motor conditions, often compounded with\nco-occurring physical disabilities, lead to the difficulty in collecting large\nquantities of impaired speech required for ASR system development. To this end,\ndata augmentation techniques play a vital role in current disordered speech\nrecognition systems. In contrast to existing data augmentation techniques only\nmodifying the speaking rate or overall shape of spectral contour, fine-grained\nspectro-temporal differences between disordered and normal speech are modelled\nusing deep convolutional generative adversarial networks (DCGAN) during data\naugmentation to modify normal speech spectra into those closer to disordered\nspeech. Experiments conducted on the UASpeech corpus suggest the proposed\nadversarial data augmentation approach consistently outperformed the baseline\naugmentation methods using tempo or speed perturbation on a state-of-the-art\nhybrid DNN system. An overall word error rate (WER) reduction up to 3.05\\%\n(9.7\\% relative) was obtained over the baseline system using no data\naugmentation. The final learning hidden unit contribution (LHUC) speaker\nadapted system using the best adversarial augmentation approach gives an\noverall WER of 25.89% on the UASpeech test set of 16 dysarthric speakers.", "published": "2021-08-02 13:44:36", "link": "http://arxiv.org/abs/2108.00899v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Robust Acoustic Scene Classification in the Presence of Active\n  Foreground Speech", "abstract": "We present an iVector based Acoustic Scene Classification (ASC) system suited\nfor real life settings where active foreground speech can be present. In the\nproposed system, each recording is represented by a fixed-length iVector that\nmodels the recording's important properties. A regularized Gaussian backend\nclassifier with class-specific covariance models is used to extract the\nrelevant acoustic scene information from these iVectors. To alleviate the large\nperformance degradation when a foreground speaker dominates the captured\nsignal, we investigate the use of the iVector framework on Mel-Frequency\nCepstral Coefficients (MFCCs) that are derived from an estimate of the noise\npower spectral density. This noise-floor can be extracted in a statistical\nmanner for single channel recordings. We show that the use of noise-floor\nfeatures is complementary to multi-condition training in which foreground\nspeech is added to training signal to reduce the mismatch between training and\ntesting conditions. Experimental results on the DCASE 2016 Task 1 dataset show\nthat the noise-floor based features and multi-condition training realize\nsignificant classification accuracy gains of up to more than 25 percentage\npoints (absolute) in the most adverse conditions. These promising results can\nfurther facilitate the integration of ASC in resource-constrained devices such\nas hearables.", "published": "2021-08-02 14:03:50", "link": "http://arxiv.org/abs/2108.00912v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Analyzing Speaker Information in Self-Supervised Models to Improve\n  Zero-Resource Speech Processing", "abstract": "Contrastive predictive coding (CPC) aims to learn representations of speech\nby distinguishing future observations from a set of negative examples. Previous\nwork has shown that linear classifiers trained on CPC features can accurately\npredict speaker and phone labels. However, it is unclear how the features\nactually capture speaker and phonetic information, and whether it is possible\nto normalize out the irrelevant details (depending on the downstream task). In\nthis paper, we first show that the per-utterance mean of CPC features captures\nspeaker information to a large extent. Concretely, we find that comparing means\nperforms well on a speaker verification task. Next, probing experiments show\nthat standardizing the features effectively removes speaker information. Based\non this observation, we propose a speaker normalization step to improve\nacoustic unit discovery using K-means clustering of CPC features. Finally, we\nshow that a language model trained on the resulting units achieves some of the\nbest results in the ZeroSpeech2021~Challenge.", "published": "2021-08-02 14:10:18", "link": "http://arxiv.org/abs/2108.00917v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker Adaptation with Continuous Vocoder-based DNN-TTS", "abstract": "Traditional vocoder-based statistical parametric speech synthesis can be\nadvantageous in applications that require low computational complexity. Recent\nneural vocoders, which can produce high naturalness, still cannot fulfill the\nrequirement of being real-time during synthesis. In this paper, we experiment\nwith our earlier continuous vocoder, in which the excitation is modeled with\ntwo one-dimensional parameters: continuous F0 and Maximum Voiced Frequency. We\nshow on the data of 9 speakers that an average voice can be trained for\nDNN-TTS, and speaker adaptation is feasible 400 utterances (about 14 minutes).\nObjective experiments support that the quality of speaker adaptation with\nContinuous Vocoder-based DNN-TTS is similar to the quality of the speaker\nadaptation with a WORLD Vocoder-based baseline.", "published": "2021-08-02 20:08:07", "link": "http://arxiv.org/abs/2108.01154v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cross-cultural Mood Perception in Pop Songs and its Alignment with Mood\n  Detection Algorithms", "abstract": "Do people from different cultural backgrounds perceive the mood in music the\nsame way? How closely do human ratings across different cultures approximate\nautomatic mood detection algorithms that are often trained on corpora of\npredominantly Western popular music? Analyzing 166 participants responses from\nBrazil, South Korea, and the US, we examined the similarity between the ratings\nof nine categories of perceived moods in music and estimated their alignment\nwith four popular mood detection algorithms. We created a dataset of 360 recent\npop songs drawn from major music charts of the countries and constructed\nsemantically identical mood descriptors across English, Korean, and Portuguese\nlanguages. Multiple participants from the three countries rated their\nfamiliarity, preference, and perceived moods for a given song. Ratings were\nhighly similar within and across cultures for basic mood attributes such as\nsad, cheerful, and energetic. However, we found significant cross-cultural\ndifferences for more complex characteristics such as dreamy and love. To our\nsurprise, the results of mood detection algorithms were uniformly correlated\nacross human ratings from all three countries and did not show a detectable\nbias towards any particular culture. Our study thus suggests that the mood\ndetection algorithms can be considered as an objective measure at least within\nthe popular music context.", "published": "2021-08-02 10:29:36", "link": "http://arxiv.org/abs/2108.00768v1", "categories": ["cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "Musical Speech: A Transformer-based Composition Tool", "abstract": "In this paper, we propose a new compositional tool that will generate a\nmusical outline of speech recorded/provided by the user for use as a musical\nbuilding block in their compositions. The tool allows any user to use their own\nspeech to generate musical material, while still being able to hear the direct\nconnection between their recorded speech and the resulting music. The tool is\nbuilt on our proposed pipeline. This pipeline begins with speech-based signal\nprocessing, after which some simple musical heuristics are applied, and finally\nthese pre-processed signals are passed through Transformer models trained on\nnew musical tasks. We illustrate the effectiveness of our pipeline -- which\ndoes not require a paired dataset for training -- through examples of music\ncreated by musicians making use of our tool.", "published": "2021-08-02 17:03:27", "link": "http://arxiv.org/abs/2108.01043v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Triangular body-cover model of the vocal folds with coordinated\n  activation of the five intrinsic laryngeal muscles", "abstract": "Poor laryngeal muscle coordination that results in abnormal glottal posturing\nis believed to be a primary etiologic factor in common voice disorders such as\nnon-phonotraumatic vocal hyperfunction. Abnormal activity of antagonistic\nlaryngeal muscles is hypothesized to play a key role in the alteration of\nnormal vocal fold biomechanics that results in the dysphonia associated with\nsuch disorders. Current low-order models of the vocal folds are unsatisfactory\nto test this hypothesis since they do not capture the co-contraction of\nantagonist laryngeal muscle pairs. To address this limitation, a self-sustained\ntriangular body-cover model with full intrinsic muscle control is introduced.\nThe proposed scheme shows good agreement with prior studies using finite\nelement models, excised larynges, and clinical studies in sustained and\ntime-varying vocal gestures. Simulations of vocal fold posturing obtained with\ndistinct antagonistic muscle activation yield clear differences in kinematic,\naerodynamic and acoustic measures. The proposed tool is deemed sufficiently\naccurate and flexible for future comprehensive investigations of\nnon-phonotraumatic vocal hyperfunction and other laryngeal motor control\ndisorders.", "published": "2021-08-02 18:20:37", "link": "http://arxiv.org/abs/2108.01115v2", "categories": ["physics.med-ph", "cs.SD", "eess.AS", "physics.bio-ph", "92C10", "J.2.2"], "primary_category": "physics.med-ph"}
{"title": "Creation and Detection of German Voice Deepfakes", "abstract": "Synthesizing voice with the help of machine learning techniques has made\nrapid progress over the last years [1] and first high profile fraud cases have\nbeen recently reported [2]. Given the current increase in using conferencing\ntools for online teaching, we question just how easy (i.e. needed data,\nhardware, skill set) it would be to create a convincing voice fake. We analyse\nhow much training data a participant (e.g. a student) would actually need to\nfake another participants voice (e.g. a professor). We provide an analysis of\nthe existing state of the art in creating voice deep fakes, as well as offer\ndetailed technical guidance and evidence of just how much effort is needed to\ncopy a voice. A user study with more than 100 participants shows how difficult\nit is to identify real and fake voice (on avg. only 37 percent can distinguish\nbetween real and fake voice of a professor). With a focus on German language\nand an online teaching environment we discuss the societal implications as well\nas demonstrate how to use machine learning techniques to possibly detect such\nfakes.", "published": "2021-08-02 06:17:25", "link": "http://arxiv.org/abs/2108.01469v1", "categories": ["eess.AS", "cs.CR", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
