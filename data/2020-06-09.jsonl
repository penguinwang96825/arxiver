{"title": "Universal Vector Neural Machine Translation With Effective Attention", "abstract": "Neural Machine Translation (NMT) leverages one or more trained neural\nnetworks for the translation of phrases. Sutskever introduced a sequence to\nsequence based encoder-decoder model which became the standard for NMT based\nsystems. Attention mechanisms were later introduced to address the issues with\nthe translation of long sentences and improving overall accuracy. In this\npaper, we propose a singular model for Neural Machine Translation based on\nencoder-decoder models. Most translation models are trained as one model for\none translation. We introduce a neutral/universal model representation that can\nbe used to predict more than one language depending on the source and a\nprovided target. Secondly, we introduce an attention model by adding an overall\nlearning vector to the multiplicative model. With these two changes, by using\nthe novel universal model the number of models needed for multiple language\ntranslation applications are reduced.", "published": "2020-06-09 01:13:57", "link": "http://arxiv.org/abs/2006.05003v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Recover from Multi-Modality Errors for Non-Autoregressive\n  Neural Machine Translation", "abstract": "Non-autoregressive neural machine translation (NAT) predicts the entire\ntarget sequence simultaneously and significantly accelerates inference process.\nHowever, NAT discards the dependency information in a sentence, and thus\ninevitably suffers from the multi-modality problem: the target tokens may be\nprovided by different possible translations, often causing token repetitions or\nmissing. To alleviate this problem, we propose a novel semi-autoregressive\nmodel RecoverSAT in this work, which generates a translation as a sequence of\nsegments. The segments are generated simultaneously while each segment is\npredicted token-by-token. By dynamically determining segment length and\ndeleting repetitive segments, RecoverSAT is capable of recovering from\nrepetitive and missing token errors. Experimental results on three widely-used\nbenchmark datasets show that our proposed model achieves more than 4$\\times$\nspeedup while maintaining comparable performance compared with the\ncorresponding autoregressive model.", "published": "2020-06-09 10:12:16", "link": "http://arxiv.org/abs/2006.05165v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-Aided Open-Domain Question Answering", "abstract": "Open-domain question answering (QA) aims to find the answer to a question\nfrom a large collection of documents.Though many models for single-document\nmachine comprehension have achieved strong performance, there is still much\nroom for improving open-domain QA systems since document retrieval and answer\nreranking are still unsatisfactory. Golden documents that contain the correct\nanswers may not be correctly scored by the retrieval component, and the correct\nanswers that have been extracted may be wrongly ranked after other candidate\nanswers by the reranking component. One of the reasons is derived from the\nindependent principle in which each candidate document (or answer) is scored\nindependently without considering its relationship to other documents (or\nanswers). In this work, we propose a knowledge-aided open-domain QA (KAQA)\nmethod which targets at improving relevant document retrieval and candidate\nanswer reranking by considering the relationship between a question and the\ndocuments (termed as question-document graph), and the relationship between\ncandidate documents (termed as document-document graph). The graphs are built\nusing knowledge triples from external knowledge resources. During document\nretrieval, a candidate document is scored by considering its relationship to\nthe question and other documents. During answer reranking, a candidate answer\nis reranked using not only its own context but also the clues from other\ndocuments. The experimental results show that our proposed method improves\ndocument retrieval and answer reranking, and thereby enhances the overall\nperformance of open-domain question answering.", "published": "2020-06-09 13:28:57", "link": "http://arxiv.org/abs/2006.05244v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combination of abstractive and extractive approaches for summarization\n  of long scientific texts", "abstract": "In this research work, we present a method to generate summaries of long\nscientific documents that uses the advantages of both extractive and\nabstractive approaches. Before producing a summary in an abstractive manner, we\nperform the extractive step, which then is used for conditioning the abstractor\nmodule. We used pre-trained transformer-based language models, for both\nextractor and abstractor. Our experiments showed that using extractive and\nabstractive models jointly significantly improves summarization results and\nROUGE scores.", "published": "2020-06-09 15:38:21", "link": "http://arxiv.org/abs/2006.05354v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Label Semantics for Predicting Emotional Reactions", "abstract": "Predicting how events induce emotions in the characters of a story is\ntypically seen as a standard multi-label classification task, which usually\ntreats labels as anonymous classes to predict. They ignore information that may\nbe conveyed by the emotion labels themselves. We propose that the semantics of\nemotion labels can guide a model's attention when representing the input story.\nFurther, we observe that the emotions evoked by an event are often related: an\nevent that evokes joy is unlikely to also evoke sadness. In this work, we\nexplicitly model label classes via label embeddings, and add mechanisms that\ntrack label-label correlations both during training and inference. We also\nintroduce a new semi-supervision strategy that regularizes for the correlations\non unlabeled data. Our empirical evaluations show that modeling label semantics\nyields consistent benefits, and we advance the state-of-the-art on an emotion\ninference task.", "published": "2020-06-09 20:04:02", "link": "http://arxiv.org/abs/2006.05489v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Augmented Translation Technique for low Resource language pair:\n  Sanskrit to Hindi translation", "abstract": "Neural Machine Translation (NMT) is an ongoing technique for Machine\nTranslation (MT) using enormous artificial neural network. It has exhibited\npromising outcomes and has shown incredible potential in solving challenging\nmachine translation exercises. One such exercise is the best approach to\nfurnish great MT to language sets with a little preparing information. In this\nwork, Zero Shot Translation (ZST) is inspected for a low resource language\npair. By working on high resource language pairs for which benchmarks are\navailable, namely Spanish to Portuguese, and training on data sets\n(Spanish-English and English-Portuguese) we prepare a state of proof for ZST\nsystem that gives appropriate results on the available data. Subsequently the\nsame architecture is tested for Sanskrit to Hindi translation for which data is\nsparse, by training the model on English-Hindi and Sanskrit-English language\npairs. In order to prepare and decipher with ZST system, we broaden the\npreparation and interpretation pipelines of NMT seq2seq model in tensorflow,\nincorporating ZST features. Dimensionality reduction of word embedding is\nperformed to reduce the memory usage for data storage and to achieve a faster\ntraining and translation cycles. In this work existing helpful technology has\nbeen utilized in an imaginative manner to execute our NLP issue of Sanskrit to\nHindi translation. A Sanskrit-Hindi parallel corpus of 300 is constructed for\ntesting. The data required for the construction of parallel corpus has been\ntaken from the telecasted news, published on Department of Public Information,\nstate government of Madhya Pradesh, India website.", "published": "2020-06-09 17:01:55", "link": "http://arxiv.org/abs/2006.08332v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "HausaMT v1.0: Towards English-Hausa Neural Machine Translation", "abstract": "Neural Machine Translation (NMT) for low-resource languages suffers from low\nperformance because of the lack of large amounts of parallel data and language\ndiversity. To contribute to ameliorating this problem, we built a baseline\nmodel for English-Hausa machine translation, which is considered a task for\nlow-resource language. The Hausa language is the second largest Afro-Asiatic\nlanguage in the world after Arabic and it is the third largest language for\ntrading across a larger swath of West Africa countries, after English and\nFrench. In this paper, we curated different datasets containing Hausa-English\nparallel corpus for our translation. We trained baseline models and evaluated\nthe performance of our models using the Recurrent and Transformer\nencoder-decoder architecture with two tokenization approaches: standard\nword-level tokenization and Byte Pair Encoding (BPE) subword tokenization.", "published": "2020-06-09 02:08:03", "link": "http://arxiv.org/abs/2006.05014v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Extensive Error Analysis and a Learning-Based Evaluation of Medical\n  Entity Recognition Systems to Approximate User Experience", "abstract": "When comparing entities extracted by a medical entity recognition system with\ngold standard annotations over a test set, two types of mismatches might occur,\nlabel mismatch or span mismatch. Here we focus on span mismatch and show that\nits severity can vary from a serious error to a fully acceptable entity\nextraction due to the subjectivity of span annotations. For a domain-specific\nBERT-based NER system, we showed that 25% of the errors have the same labels\nand overlapping span with gold standard entities. We collected expert judgement\nwhich shows more than 90% of these mismatches are accepted or partially\naccepted by the user. Using the training set of the NER system, we built a fast\nand lightweight entity classifier to approximate the user experience of such\nmismatches through accepting or rejecting them. The decisions made by this\nclassifier are used to calculate a learning-based F-score which is shown to be\na better approximation of a forgiving user's experience than the relaxed\nF-score. We demonstrated the results of applying the proposed evaluation metric\nfor a variety of deep learning medical entity recognition models trained with\ntwo datasets.", "published": "2020-06-09 14:15:33", "link": "http://arxiv.org/abs/2006.05281v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Examination and Extension of Strategies for Improving Personalized\n  Language Modeling via Interpolation", "abstract": "In this paper, we detail novel strategies for interpolating personalized\nlanguage models and methods to handle out-of-vocabulary (OOV) tokens to improve\npersonalized language models. Using publicly available data from Reddit, we\ndemonstrate improvements in offline metrics at the user level by interpolating\na global LSTM-based authoring model with a user-personalized n-gram model. By\noptimizing this approach with a back-off to uniform OOV penalty and the\ninterpolation coefficient, we observe that over 80% of users receive a lift in\nperplexity, with an average of 5.2% in perplexity lift per user. In doing this\nresearch we extend previous work in building NLIs and improve the robustness of\nmetrics for downstream tasks.", "published": "2020-06-09 19:29:41", "link": "http://arxiv.org/abs/2006.05469v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Paraphrase Generation using Pre-trained Language Models", "abstract": "Large scale Pre-trained Language Models have proven to be very powerful\napproach in various Natural language tasks. OpenAI's GPT-2\n\\cite{radford2019language} is notable for its capability to generate fluent,\nwell formulated, grammatically consistent text and for phrase completions. In\nthis paper we leverage this generation capability of GPT-2 to generate\nparaphrases without any supervision from labelled data. We examine how the\nresults compare with other supervised and unsupervised approaches and the\neffect of using paraphrases for data augmentation on downstream tasks such as\nclassification. Our experiments show that paraphrases generated with our model\nare of good quality, are diverse and improves the downstream task performance\nwhen used for data augmentation.", "published": "2020-06-09 19:40:19", "link": "http://arxiv.org/abs/2006.05477v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tamil Vowel Recognition With Augmented MNIST-like Data Set", "abstract": "We report generation of a MNIST [4] compatible data set [1] for Tamil vowels\nto enable building a classification DNN or other such ML/AI deep learning [2]\nmodels for Tamil OCR/Handwriting applications. We report the capability of the\n60,000 grayscale, 28x28 pixel dataset to build a 92% accuracy (training) and\n82% cross-validation 4-layer CNN, with 100,000+ parameters, in TensorFlow. We\nalso report a top-1 classification accuracy of 70% and top-2 classification\naccuracy of 92% on handwritten vowels showing, for the same network.", "published": "2020-06-09 19:17:30", "link": "http://arxiv.org/abs/2006.08367v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Human brain activity for machine attention", "abstract": "Cognitively inspired NLP leverages human-derived data to teach machines about\nlanguage processing mechanisms. Recently, neural networks have been augmented\nwith behavioral data to solve a range of NLP tasks spanning syntax and\nsemantics. We are the first to exploit neuroscientific data, namely\nelectroencephalography (EEG), to inform a neural attention model about language\nprocessing of the human brain. The challenge in working with EEG data is that\nfeatures are exceptionally rich and need extensive pre-processing to isolate\nsignals specific to text processing. We devise a method for finding such EEG\nfeatures to supervise machine attention through combining theoretically\nmotivated cropping with random forest tree splits. After this dimensionality\nreduction, the pre-processed EEG features are capable of distinguishing two\nreading tasks retrieved from a publicly available EEG corpus. We apply these\nfeatures to regularise attention on relation classification and show that EEG\nis more informative than strong baselines. This improvement depends on both the\ncognitive load of the task and the EEG frequency domain. Hence, informing\nneural attention models with EEG signals is beneficial but requires further\ninvestigation to understand which dimensions are the most useful across NLP\ntasks.", "published": "2020-06-09 08:39:07", "link": "http://arxiv.org/abs/2006.05113v2", "categories": ["cs.CL", "cs.LG", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "On the Effectiveness of Neural Text Generation based Data Augmentation\n  for Recognition of Morphologically Rich Speech", "abstract": "Advanced neural network models have penetrated Automatic Speech Recognition\n(ASR) in recent years, however, in language modeling many systems still rely on\ntraditional Back-off N-gram Language Models (BNLM) partly or entirely. The\nreason for this are the high cost and complexity of training and using neural\nlanguage models, mostly possible by adding a second decoding pass (rescoring).\nIn our recent work we have significantly improved the online performance of a\nconversational speech transcription system by transferring knowledge from a\nRecurrent Neural Network Language Model (RNNLM) to the single pass BNLM with\ntext generation based data augmentation. In the present paper we analyze the\namount of transferable knowledge and demonstrate that the neural augmented LM\n(RNN-BNLM) can help to capture almost 50% of the knowledge of the RNNLM yet by\ndropping the second decoding pass and making the system real-time capable. We\nalso systematically compare word and subword LMs and show that subword-based\nneural text augmentation can be especially beneficial in under-resourced\nconditions. In addition, we show that using the RNN-BNLM in the first pass\nfollowed by a neural second pass, offline ASR results can be even significantly\nimproved.", "published": "2020-06-09 09:01:04", "link": "http://arxiv.org/abs/2006.05129v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ConfNet2Seq: Full Length Answer Generation from Spoken Questions", "abstract": "Conversational and task-oriented dialogue systems aim to interact with the\nuser using natural responses through multi-modal interfaces, such as text or\nspeech. These desired responses are in the form of full-length natural answers\ngenerated over facts retrieved from a knowledge source. While the task of\ngenerating natural answers to questions from an answer span has been widely\nstudied, there has been little research on natural sentence generation over\nspoken content. We propose a novel system to generate full length natural\nlanguage answers from spoken questions and factoid answers. The spoken sequence\nis compactly represented as a confusion network extracted from a pre-trained\nAutomatic Speech Recognizer. This is the first attempt towards generating\nfull-length natural answers from a graph input(confusion network) to the best\nof our knowledge. We release a large-scale dataset of 259,788 samples of spoken\nquestions, their factoid answers and corresponding full-length textual answers.\nFollowing our proposed approach, we achieve comparable performance with best\nASR hypothesis.", "published": "2020-06-09 10:04:49", "link": "http://arxiv.org/abs/2006.05163v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Input-independent Attention Weights Are Expressive Enough: A Study of\n  Attention in Self-supervised Audio Transformers", "abstract": "In this paper, we seek solutions for reducing the computation complexity of\ntransformer-based models for speech representation learning. We evaluate 10\nattention algorithms; then, we pre-train the transformer-based model with those\nattention algorithms in a self-supervised fashion and treat them as feature\nextractors on downstream tasks, including phoneme classification and speaker\nclassification. With the assistance of t-SNE, PCA and some observation, the\nattention weights in self-supervised audio transformers can be categorized into\nfour general cases. Based on these cases and some analyses, we are able to use\na specific set of attention weights to initialize the model. Our approach shows\ncomparable performance to the typical self-attention yet requires 20% less time\nin both training and inference.", "published": "2020-06-09 10:40:52", "link": "http://arxiv.org/abs/2006.05174v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Re-evaluating phoneme frequencies", "abstract": "Causal processes can give rise to distinctive distributions in the linguistic\nvariables that they affect. Consequently, a secure understanding of a\nvariable's distribution can hold a key to understanding the forces that have\ncausally shaped it. A storied distribution in linguistics has been Zipf's law,\na kind of power law. In the wake of a major debate in the sciences around\npower-law hypotheses and the unreliability of earlier methods of evaluating\nthem, here we re-evaluate the distributions claimed to characterize phoneme\nfrequencies. We infer the fit of power laws and three alternative distributions\nto 166 Australian languages, using a maximum likelihood framework. We find\nevidence supporting earlier results, but also nuancing them and increasing our\nunderstanding of them. Most notably, phonemic inventories appear to have a\nZipfian-like frequency structure among their most-frequent members (though\nperhaps also a lognormal structure) but a geometric (or exponential) structure\namong the least-frequent. We compare these new insights the kinds of causal\nprocesses that affect the evolution of phonemic inventories over time, and\nidentify a potential account for why, despite there being an important role for\nphonetic substance in phonemic change, we could still expect inventories with\nhighly diverse phonetic content to share similar distributions of phoneme\nfrequencies. We conclude with priorities for future work in this promising\nprogram of research.", "published": "2020-06-09 12:05:10", "link": "http://arxiv.org/abs/2006.05206v2", "categories": ["cs.CL", "physics.soc-ph", "stat.AP", "J.5"], "primary_category": "cs.CL"}
{"title": "Graph-Aware Transformer: Is Attention All Graphs Need?", "abstract": "Graphs are the natural data structure to represent relational and structural\ninformation in many domains. To cover the broad range of graph-data\napplications including graph classification as well as graph generation, it is\ndesirable to have a general and flexible model consisting of an encoder and a\ndecoder that can handle graph data. Although the representative encoder-decoder\nmodel, Transformer, shows superior performance in various tasks especially of\nnatural language processing, it is not immediately available for graphs due to\ntheir non-sequential characteristics. To tackle this incompatibility, we\npropose GRaph-Aware Transformer (GRAT), the first Transformer-based model which\ncan encode and decode whole graphs in end-to-end fashion. GRAT is featured with\na self-attention mechanism adaptive to the edge information and an\nauto-regressive decoding mechanism based on the two-path approach consisting of\nsub-graph encoding path and node-and-edge generation path for each decoding\nstep. We empirically evaluated GRAT on multiple setups including encoder-based\ntasks such as molecule property predictions on QM9 datasets and\nencoder-decoder-based tasks such as molecule graph generation in the organic\nmolecule synthesis domain. GRAT has shown very promising results including\nstate-of-the-art performance on 4 regression tasks in QM9 benchmark.", "published": "2020-06-09 12:13:56", "link": "http://arxiv.org/abs/2006.05213v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "audino: A Modern Annotation Tool for Audio and Speech", "abstract": "In this paper, we introduce a collaborative and modern annotation tool for\naudio and speech: audino. The tool allows annotators to define and describe\ntemporal segmentation in audios. These segments can be labelled and transcribed\neasily using a dynamically generated form. An admin can centrally control user\nroles and project assignment through the admin dashboard. The dashboard also\nenables describing labels and their values. The annotations can easily be\nexported in JSON format for further analysis. The tool allows audio data and\ntheir corresponding annotations to be uploaded and assigned to a user through a\nkey-based API. The flexibility available in the annotation tool enables\nannotation for Speech Scoring, Voice Activity Detection (VAD), Speaker\nDiarisation, Speaker Identification, Speech Recognition, Emotion Recognition\ntasks and more. The MIT open source license allows it to be used for academic\nand commercial projects.", "published": "2020-06-09 13:12:44", "link": "http://arxiv.org/abs/2006.05236v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning not to Discriminate: Task Agnostic Learning for Improving\n  Monolingual and Code-switched Speech Recognition", "abstract": "Recognizing code-switched speech is challenging for Automatic Speech\nRecognition (ASR) for a variety of reasons, including the lack of code-switched\ntraining data. Recently, we showed that monolingual ASR systems fine-tuned on\ncode-switched data deteriorate in performance on monolingual speech\nrecognition, which is not desirable as ASR systems deployed in multilingual\nscenarios should recognize both monolingual and code-switched speech with high\naccuracy. Our experiments indicated that this loss in performance could be\nmitigated by using certain strategies for fine-tuning and regularization,\nleading to improvements in both monolingual and code-switched ASR. In this\nwork, we present further improvements over our previous work by using domain\nadversarial learning to train task agnostic models. We evaluate the\nclassification accuracy of an adversarial discriminator and show that it can\nlearn shared layer parameters that are task agnostic. We train end-to-end ASR\nsystems starting with a pooled model that uses monolingual and code-switched\ndata along with the adversarial discriminator. Our proposed technique leads to\nreductions in Word Error Rates (WER) in monolingual and code-switched test sets\nacross three language pairs.", "published": "2020-06-09 13:45:30", "link": "http://arxiv.org/abs/2006.05257v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Vocal markers from sustained phonation in Huntington's Disease", "abstract": "Disease-modifying treatments are currently assessed in neurodegenerative\ndiseases. Huntington's Disease represents a unique opportunity to design\nautomatic sub-clinical markers, even in premanifest gene carriers. We\ninvestigated phonatory impairments as potential clinical markers and propose\nthem for both diagnosis and gene carriers follow-up. We used two sets of\nfeatures: Phonatory features and Modulation Power Spectrum Features. We found\nthat phonation is not sufficient for the identification of sub-clinical\ndisorders of premanifest gene carriers. According to our regression results,\nPhonatory features are suitable for the predictions of clinical performance in\nHuntington's Disease.", "published": "2020-06-09 15:51:28", "link": "http://arxiv.org/abs/2006.05365v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dialog Policy Learning for Joint Clarification and Active Learning\n  Queries", "abstract": "Intelligent systems need to be able to recover from mistakes, resolve\nuncertainty, and adapt to novel concepts not seen during training. Dialog\ninteraction can enable this by the use of clarifications for correction and\nresolving uncertainty, and active learning queries to learn new concepts\nencountered during operation. Prior work on dialog systems has either focused\non exclusively learning how to perform clarification/ information seeking, or\nto perform active learning. In this work, we train a hierarchical dialog policy\nto jointly perform both clarification and active learning in the context of an\ninteractive language-based image retrieval task motivated by an online shopping\napplication, and demonstrate that jointly learning dialog policies for\nclarification and active learning is more effective than the use of static\ndialog policies for one or both of these functions.", "published": "2020-06-09 18:53:21", "link": "http://arxiv.org/abs/2006.05456v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Improving Cross-Lingual Transfer Learning for End-to-End Speech\n  Recognition with Speech Translation", "abstract": "Transfer learning from high-resource languages is known to be an efficient\nway to improve end-to-end automatic speech recognition (ASR) for low-resource\nlanguages. Pre-trained or jointly trained encoder-decoder models, however, do\nnot share the language modeling (decoder) for the same language, which is\nlikely to be inefficient for distant target languages. We introduce\nspeech-to-text translation (ST) as an auxiliary task to incorporate additional\nknowledge of the target language and enable transferring from that target\nlanguage. Specifically, we first translate high-resource ASR transcripts into a\ntarget low-resource language, with which a ST model is trained. Both ST and\ntarget ASR share the same attention-based encoder-decoder architecture and\nvocabulary. The former task then provides a fully pre-trained model for the\nlatter, bringing up to 24.6% word error rate (WER) reduction to the baseline\n(direct transfer from high-resource ASR). We show that training ST with human\ntranslations is not necessary. ST trained with machine translation (MT)\npseudo-labels brings consistent gains. It can even outperform those using human\nlabels when transferred to target ASR by leveraging only 500K MT examples. Even\nwith pseudo-labels from low-resource MT (200K examples), ST-enhanced transfer\nbrings up to 8.9% WER reduction to direct transfer.", "published": "2020-06-09 19:34:11", "link": "http://arxiv.org/abs/2006.05474v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Predicting and Analyzing Law-Making in Kenya", "abstract": "Modelling and analyzing parliamentary legislation, roll-call votes and order\nof proceedings in developed countries has received significant attention in\nrecent years. In this paper, we focused on understanding the bills introduced\nin a developing democracy, the Kenyan bicameral parliament. We developed and\ntrained machine learning models on a combination of features extracted from the\nbills to predict the outcome - if a bill will be enacted or not. We observed\nthat the texts in a bill are not as relevant as the year and month the bill was\nintroduced and the category the bill belongs to.", "published": "2020-06-09 20:21:50", "link": "http://arxiv.org/abs/2006.05493v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Functions to Study the Benefit of Multitask Learning", "abstract": "We study and quantify the generalization patterns of multitask learning (MTL)\nmodels for sequence labeling tasks. MTL models are trained to optimize a set of\nrelated tasks jointly. Although multitask learning has achieved improved\nperformance in some problems, there are also tasks that lose performance when\ntrained together. These mixed results motivate us to study the factors that\nimpact the performance of MTL models. We note that theoretical bounds and\nconvergence rates for MTL models exist, but they rely on strong assumptions\nsuch as task relatedness and the use of balanced datasets. To remedy these\nlimitations, we propose the creation of a task simulator and the use of\nSymbolic Regression to learn expressions relating model performance to possible\nfactors of influence. For MTL, we study the model performance against the\nnumber of tasks (T), the number of samples per task (n) and the task\nrelatedness measured by the adjusted mutual information (AMI). In our\nexperiments, we could empirically find formulas relating model performance with\nfactors of sqrt(n), sqrt(T), which are equivalent to sound mathematical proofs\nin Maurer[2016], and we went beyond by discovering that performance relates to\na factor of sqrt(AMI).", "published": "2020-06-09 23:51:32", "link": "http://arxiv.org/abs/2006.05561v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "EPIC30M: An Epidemics Corpus Of Over 30 Million Relevant Tweets", "abstract": "Since the start of COVID-19, several relevant corpora from various sources\nare presented in the literature that contain millions of data points. While\nthese corpora are valuable in supporting many analyses on this specific\npandemic, researchers require additional benchmark corpora that contain other\nepidemics to facilitate cross-epidemic pattern recognition and trend analysis\ntasks. During our other efforts on COVID-19 related work, we discover very\nlittle disease related corpora in the literature that are sizable and rich\nenough to support such cross-epidemic analysis tasks. In this paper, we present\nEPIC30M, a large-scale epidemic corpus that contains 30 millions micro-blog\nposts, i.e., tweets crawled from Twitter, from year 2006 to 2020. EPIC30M\ncontains a subset of 26.2 millions tweets related to three general diseases,\nnamely Ebola, Cholera and Swine Flu, and another subset of 4.7 millions tweets\nof six global epidemic outbreaks, including 2009 H1N1 Swine Flu, 2010 Haiti\nCholera, 2012 Middle-East Respiratory Syndrome (MERS), 2013 West African Ebola,\n2016 Yemen Cholera and 2018 Kivu Ebola. Furthermore, we explore and discuss the\nproperties of the corpus with statistics of key terms and hashtags and trends\nanalysis for each subset. Finally, we demonstrate the value and impact that\nEPIC30M could create through a discussion of multiple use cases of\ncross-epidemic research topics that attract growing interest in recent years.\nThese use cases span multiple research areas, such as epidemiological modeling,\npattern recognition, natural language understanding and economical modeling.", "published": "2020-06-09 13:23:00", "link": "http://arxiv.org/abs/2006.08369v2", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "A fully recurrent feature extraction for single channel speech\n  enhancement", "abstract": "Convolutional neural network (CNN) modules are widely being used to build\nhigh-end speech enhancement neural models. However, the feature extraction\npower of vanilla CNN modules has been limited by the dimensionality constraint\nof the convolution kernels that are integrated - thereby, they have limitations\nto adequately model the noise context information at the feature extraction\nstage. To this end, adding recurrency factor into the feature extracting CNN\nlayers, we introduce a robust context-aware feature extraction strategy for\nsingle-channel speech enhancement. As shown, adding recurrency results in\ncapturing the local statistics of noise attributes at the extracted features\nlevel and thus, the suggested model is effective in differentiating speech cues\neven at very noisy conditions. When evaluated against enhancement models using\nvanilla CNN modules, in unseen noise conditions, the suggested model with\nrecurrency in the feature extraction layers has produced a segmental SNR (SSNR)\ngain of up to 1.5 dB, an improvement of 0.4 in subjective quality in the Mean\nOpinion Score scale, while the parameters to be optimized are reduced by 25%.", "published": "2020-06-09 13:11:04", "link": "http://arxiv.org/abs/2006.05233v7", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Hitchhiker's Guide to the All-Interval 12-Tone Rows", "abstract": "This article revisits the generation, classification and categorization of\nall-intervals 12-tone series (AIS). Inspired by the seminal work of Morris and\nStarr in 1974 (Morris and Starr, The Structure of All-Interval Series 1974), it\nexpands their analysis using complex network theory and provides composers and\ntheorists with the re-ordering scheme that links all AISs together by chains of\nrelations.", "published": "2020-06-09 01:40:57", "link": "http://arxiv.org/abs/2006.05007v1", "categories": ["cs.SD", "cs.SI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "C-SL: Contrastive Sound Localization with Inertial-Acoustic Sensors", "abstract": "Human brain employs perceptual information about the head and eye movements\nto update the spatial relationship between the individual and the surrounding\nenvironment. Based on this cognitive process known as spatial updating, we\nintroduce contrastive sound localization (C-SL) with mobile inertial-acoustic\nsensor arrays of arbitrary geometry. C-SL uses unlabeled multi-channel audio\nrecordings and inertial measurement unit (IMU) readings collected during free\nrotational movements of the array to learn mappings from acoustical\nmeasurements to an array-centered direction-of-arrival (DOA) in a\nself-supervised manner. Contrary to conventional DOA estimation methods that\nrequire the knowledge of either the array geometry or source locations in the\ncalibration stage, C-SL is agnostic to both, and can be trained on data\ncollected in minimally constrained settings. To achieve this capability, our\nproposed method utilizes a customized contrastive loss measuring the spatial\ncontrast between source locations predicted for disjoint segments of the input\nto jointly update estimated DOAs and the acoustic-spatial mapping in linear\ntime. We provide quantitative and qualitative evaluations of C-SL comparing its\nperformance with baseline DOA estimation methods in a wide range of conditions.\nWe believe the relaxed calibration process offered by C-SL paves the way toward\ntruly personalized augmented hearing applications.", "published": "2020-06-09 06:36:44", "link": "http://arxiv.org/abs/2006.05071v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
