{"title": "Leveraging Lead Bias for Zero-shot Abstractive News Summarization", "abstract": "A typical journalistic convention in news articles is to deliver the most\nsalient information in the beginning, also known as the lead bias. While this\nphenomenon can be exploited in generating a summary, it has a detrimental\neffect on teaching a model to discriminate and extract important information in\ngeneral. We propose that this lead bias can be leveraged in our favor in a\nsimple and effective way to pre-train abstractive news summarization models on\nlarge-scale unlabeled news corpora: predicting the leading sentences using the\nrest of an article. We collect a massive news corpus and conduct data cleaning\nand filtering via statistical analysis. We then apply self-supervised\npre-training on this dataset to existing generation models BART and T5 for\ndomain adaptation. Via extensive experiments on six benchmark datasets, we show\nthat this approach can dramatically improve the summarization quality and\nachieve state-of-the-art results for zero-shot news summarization without any\nfine-tuning. For example, in the DUC2003 dataset, the ROUGE-1 score of BART\nincreases 13.7% after the lead-bias pre-training. We deploy the model in\nMicrosoft News and provide public APIs as well as a demo website for\nmulti-lingual news summarization.", "published": "2019-12-25 06:05:44", "link": "http://arxiv.org/abs/1912.11602v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Study of Multilingual Neural Machine Translation", "abstract": "Multilingual neural machine translation (NMT) has recently been investigated\nfrom different aspects (e.g., pivot translation, zero-shot translation,\nfine-tuning, or training from scratch) and in different settings (e.g., rich\nresource and low resource, one-to-many, and many-to-one translation). This\npaper concentrates on a deep understanding of multilingual NMT and conducts a\ncomprehensive study on a multilingual dataset with more than 20 languages. Our\nresults show that (1) low-resource language pairs benefit much from\nmultilingual training, while rich-resource language pairs may get hurt under\nlimited model capacity and training with similar languages benefits more than\ndissimilar languages; (2) fine-tuning performs better than training from\nscratch in the one-to-many setting while training from scratch performs better\nin the many-to-one setting; (3) the bottom layers of the encoder and top layers\nof the decoder capture more language-specific information, and just fine-tuning\nthese parts can achieve good accuracy for low-resource language pairs; (4)\ndirect translation is better than pivot translation when the source language is\nsimilar to the target language (e.g., in the same language branch), even when\nthe size of direct training data is much smaller; (5) given a fixed training\ndata budget, it is better to introduce more languages into multilingual\ntraining for zero-shot translation.", "published": "2019-12-25 08:50:45", "link": "http://arxiv.org/abs/1912.11625v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "N-gram Statistical Stemmer for Bangla Corpus", "abstract": "Stemming is a process that can be utilized to trim inflected words to stem or\nroot form. It is useful for enhancing the retrieval effectiveness, especially\nfor text search in order to solve the mismatch problems. Previous research on\nBangla stemming mostly relied on eliminating multiple suffixes from a solitary\nword through a recursive rule based procedure to recover progressively\napplicable relative root. Our proposed system has enhanced the aforementioned\nexploration by actualizing one of the stemming algorithms called N-gram\nstemming. By utilizing an affiliation measure called dice coefficient, related\nsets of words are clustered depending on their character structure. The\nsmallest word in one cluster may be considered as the stem. We additionally\nanalyzed Affinity Propagation clustering algorithms with coefficient similarity\nas well as with median similarity. Our result indicates N-gram stemming\ntechniques to be effective in general which gave us around 87% accurate\nclusters.", "published": "2019-12-25 07:31:44", "link": "http://arxiv.org/abs/1912.11612v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Explicit Sparse Transformer: Concentrated Attention Through Explicit\n  Selection", "abstract": "Self-attention based Transformer has demonstrated the state-of-the-art\nperformances in a number of natural language processing tasks. Self-attention\nis able to model long-term dependencies, but it may suffer from the extraction\nof irrelevant information in the context. To tackle the problem, we propose a\nnovel model called \\textbf{Explicit Sparse Transformer}. Explicit Sparse\nTransformer is able to improve the concentration of attention on the global\ncontext through an explicit selection of the most relevant segments. Extensive\nexperimental results on a series of natural language processing and computer\nvision tasks, including neural machine translation, image captioning, and\nlanguage modeling, all demonstrate the advantages of Explicit Sparse\nTransformer in model performance. We also show that our proposed sparse\nattention method achieves comparable or better results than the previous sparse\nattention method, but significantly reduces training and testing time. For\nexample, the inference speed is twice that of sparsemax in Transformer model.\nCode will be available at\n\\url{https://github.com/lancopku/Explicit-Sparse-Transformer}", "published": "2019-12-25 10:59:31", "link": "http://arxiv.org/abs/1912.11637v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to Answer Ambiguous Questions with Knowledge Graph", "abstract": "In the task of factoid question answering over knowledge base, many questions\nhave more than one plausible interpretation. Previous works on SimpleQuestions\nassume only one interpretation as the ground truth for each question, so they\nlack the ability to answer ambiguous questions correctly. In this paper, we\npresent a new way to utilize the dataset that takes into account the existence\nof ambiguous questions. Then we introduce a simple and effective model which\ncombines local knowledge subgraph with attention mechanism. Our experimental\nresults show that our approach achieves outstanding performance in this task.", "published": "2019-12-25 13:43:44", "link": "http://arxiv.org/abs/1912.11668v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Convolutional Quantum-Like Language Model with Mutual-Attention for\n  Product Rating Prediction", "abstract": "Recommender systems are designed to help mitigate information overload users\nexperience during online shopping. Recent work explores neural language models\nto learn user and item representations from user reviews and combines such\nrepresentations with rating information. Most existing convolutional-based\nneural models take pooling immediately after convolution and loses the\ninteraction information between the latent dimension of convolutional feature\nvectors along the way. Moreover, these models usually take all feature vectors\nat higher levels as equal and do not take into consideration that some features\nare more relevant to this specific user-item context. To bridge these gaps,\nthis paper proposes a convolutional quantum-like language model with\nmutual-attention for rating prediction (ConQAR). By introducing a quantum-like\ndensity matrix layer, interactions between latent dimensions of convolutional\nfeature vectors are well captured. With the attention weights learned from the\nmutual-attention layer, final representations of a user and an item absorb\ninformation from both itself and its counterparts for making rating prediction.\nExperiments on two large datasets show that our model outperforms multiple\nstate-of-the-art CNN-based models. We also perform an ablation test to analyze\nthe independent effects of the two components of our model. Moreover, we\nconduct a case study and present visualizations of the quantum probabilistic\ndistributions in one user and one item review document to show that the learned\ndistributions capture meaningful information about this user and item, and can\nbe potentially used as textual profiling of the user and item.", "published": "2019-12-25 22:01:59", "link": "http://arxiv.org/abs/1912.11720v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "THUEE system description for NIST 2019 SRE CTS Challenge", "abstract": "This paper describes the systems submitted by the department of electronic\nengineering, institute of microelectronics of Tsinghua university and\nTsingMicro Co. Ltd. (THUEE) to the NIST 2019 speaker recognition evaluation CTS\nchallenge. Six subsystems, including etdnn/ams, ftdnn/as, eftdnn/ams, resnet,\nmultitask and c-vector are developed in this evaluation.", "published": "2019-12-25 03:44:31", "link": "http://arxiv.org/abs/1912.11585v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A statistical test for correspondence of texts to the Zipf-Mandelbrot\n  law", "abstract": "We analyse correspondence of a text to a simple probabilistic model. The\nmodel assumes that the words are selected independently from an infinite\ndictionary. The probability distribution correspond to the Zipf---Mandelbrot\nlaw. We count sequentially the numbers of different words in the text and get\nthe process of the numbers of different words. Then we estimate\nZipf---Mandelbrot law parameters using the same sequence and construct an\nestimate of the expectation of the number of different words in the text. Then\nwe subtract the corresponding values of the estimate from the sequence and\nnormalize along the coordinate axes, obtaining a random process on a segment\nfrom 0 to 1. We prove that this process (the empirical text bridge) converges\nweakly in the uniform metric on $C (0,1)$ to a centered Gaussian process with\ncontinuous a.s. paths. We develop and implement an algorithm for approximate\ncalculation of eigenvalues of the covariance function of the limit Gaussian\nprocess, and then an algorithm for calculating the probability distribution of\nthe integral of the square of this process. We use the algorithm to analyze\nuniformity of texts in English, French, Russian and Chinese.", "published": "2019-12-25 05:59:29", "link": "http://arxiv.org/abs/1912.11600v1", "categories": ["math.ST", "cs.CL", "stat.TH"], "primary_category": "math.ST"}
{"title": "Unity in Diversity: Learning Distributed Heterogeneous Sentence\n  Representation for Extractive Summarization", "abstract": "Automated multi-document extractive text summarization is a widely studied\nresearch problem in the field of natural language understanding. Such\nextractive mechanisms compute in some form the worthiness of a sentence to be\nincluded into the summary. While the conventional approaches rely on human\ncrafted document-independent features to generate a summary, we develop a\ndata-driven novel summary system called HNet, which exploits the various\nsemantic and compositional aspects latent in a sentence to capture document\nindependent features. The network learns sentence representation in a way that,\nsalient sentences are closer in the vector space than non-salient sentences.\nThis semantic and compositional feature vector is then concatenated with the\ndocument-dependent features for sentence ranking. Experiments on the DUC\nbenchmark datasets (DUC-2001, DUC-2002 and DUC-2004) indicate that our model\nshows significant performance gain of around 1.5-2 points in terms of ROUGE\nscore compared with the state-of-the-art baselines.", "published": "2019-12-25 16:25:29", "link": "http://arxiv.org/abs/1912.11688v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hybrid MemNet for Extractive Summarization", "abstract": "Extractive text summarization has been an extensive research problem in the\nfield of natural language understanding. While the conventional approaches rely\nmostly on manually compiled features to generate the summary, few attempts have\nbeen made in developing data-driven systems for extractive summarization. To\nthis end, we present a fully data-driven end-to-end deep network which we call\nas Hybrid MemNet for single document summarization task. The network learns the\ncontinuous unified representation of a document before generating its summary.\nIt jointly captures local and global sentential information along with the\nnotion of summary worthy sentences. Experimental results on two different\ncorpora confirm that our model shows significant performance gains compared\nwith the state-of-the-art baselines.", "published": "2019-12-25 17:48:09", "link": "http://arxiv.org/abs/1912.11701v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Utterance-level Permutation Invariant Training with Latency-controlled\n  BLSTM for Single-channel Multi-talker Speech Separation", "abstract": "Utterance-level permutation invariant training (uPIT) has achieved promising\nprogress on single-channel multi-talker speech separation task. Long short-term\nmemory (LSTM) and bidirectional LSTM (BLSTM) are widely used as the separation\nnetworks of uPIT, i.e. uPIT-LSTM and uPIT-BLSTM. uPIT-LSTM has lower latency\nbut worse performance, while uPIT-BLSTM has better performance but higher\nlatency. In this paper, we propose using latency-controlled BLSTM (LC-BLSTM)\nduring inference to fulfill low-latency and good-performance speech separation.\nTo find a better training strategy for BLSTM-based separation network,\nchunk-level PIT (cPIT) and uPIT are compared. The experimental results show\nthat uPIT outperforms cPIT when LC-BLSTM is used during inference. It is also\nfound that the inter-chunk speaker tracing (ST) can further improve the\nseparation performance of uPIT-LC-BLSTM. Evaluated on the WSJ0 two-talker\nmixed-speech separation task, the absolute gap of signal-to-distortion ratio\n(SDR) between uPIT-BLSTM and uPIT-LC-BLSTM is reduced to within 0.7 dB.", "published": "2019-12-25 07:40:02", "link": "http://arxiv.org/abs/1912.11613v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Look, Listen, and Act: Towards Audio-Visual Embodied Navigation", "abstract": "A crucial ability of mobile intelligent agents is to integrate the evidence\nfrom multiple sensory inputs in an environment and to make a sequence of\nactions to reach their goals. In this paper, we attempt to approach the problem\nof Audio-Visual Embodied Navigation, the task of planning the shortest path\nfrom a random starting location in a scene to the sound source in an indoor\nenvironment, given only raw egocentric visual and audio sensory data. To\naccomplish this task, the agent is required to learn from various modalities,\ni.e. relating the audio signal to the visual environment. Here we describe an\napproach to audio-visual embodied navigation that takes advantage of both\nvisual and audio pieces of evidence. Our solution is based on three key ideas:\na visual perception mapper module that constructs its spatial memory of the\nenvironment, a sound perception module that infers the relative location of the\nsound source from the agent, and a dynamic path planner that plans a sequence\nof actions based on the audio-visual observations and the spatial memory of the\nenvironment to navigate toward the goal. Experimental results on a newly\ncollected Visual-Audio-Room dataset using the simulated multi-modal environment\ndemonstrate the effectiveness of our approach over several competitive\nbaselines.", "published": "2019-12-25 15:07:26", "link": "http://arxiv.org/abs/1912.11684v2", "categories": ["cs.CV", "cs.LG", "cs.RO", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
