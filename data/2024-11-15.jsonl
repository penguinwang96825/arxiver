{"title": "Deep Hedging Bermudan Swaptions", "abstract": "Abstract This paper proposes a novel approach to Bermudan swaption hedging by\napplying the deep hedging framework to address limitations of traditional\narbitrage-free methods. Conventional methods assume ideal conditions, such as\nzero transaction costs, perfect liquidity, and continuous-time hedging, which\noften differ from real market environments. This discrepancy can lead to\nresidual profit and loss (P&L), resulting in two primary issues. First,\nresidual P&L may prevent achieving the initial model price, especially with\nimproper parameter settings, potentially causing a negative P&L trend and\nsignificant financial impacts. Second, controlling the distribution of residual\nP&L to mitigate downside risk is challenging, as hedged positions may become\ncurve gamma-short, making them vulnerable to large interest rate movements. The\ndeep hedging approach enables flexible selection of convex risk measures and\nhedge strategies, allowing for improved residual P&L management. This study\nalso addresses challenges in applying the deep hedging approach to Bermudan\nswaptions, such as efficient arbitrage-free market scenario generation and\nmanaging early exercise conditions. Additionally, we introduce a unique \"Option\nSpread Hedge\" strategy, which allows for robust hedging and provides intuitive\ninterpretability. Numerical analysis results demonstrate the effectiveness of\nour approach.", "published": "2024-11-15 09:46:48", "link": "http://arxiv.org/abs/2411.10079v1", "categories": ["q-fin.CP", "q-fin.PR"], "primary_category": "q-fin.CP"}
{"title": "Guided Learning: Lubricating End-to-End Modeling for Multi-stage Decision-making", "abstract": "Multi-stage decision-making is crucial in various real-world artificial\nintelligence applications, including recommendation systems, autonomous\ndriving, and quantitative investment systems. In quantitative investment, for\nexample, the process typically involves several sequential stages such as\nfactor mining, alpha prediction, portfolio optimization, and sometimes order\nexecution. While state-of-the-art end-to-end modeling aims to unify these\nstages into a single global framework, it faces significant challenges: (1)\ntraining such a unified neural network consisting of multiple stages between\ninitial inputs and final outputs often leads to suboptimal solutions, or even\ncollapse, and (2) many decision-making scenarios are not easily reducible to\nstandard prediction problems. To overcome these challenges, we propose Guided\nLearning, a novel methodological framework designed to enhance end-to-end\nlearning in multi-stage decision-making. We introduce the concept of a\n``guide'', a function that induces the training of intermediate neural network\nlayers towards some phased goals, directing gradients away from suboptimal\ncollapse. For decision scenarios lacking explicit supervisory labels, we\nincorporate a utility function that quantifies the ``reward'' of the throughout\ndecision. Additionally, we explore the connections between Guided Learning and\nclassic machine learning paradigms such as supervised, unsupervised,\nsemi-supervised, multi-task, and reinforcement learning. Experiments on\nquantitative investment strategy building demonstrate that guided learning\nsignificantly outperforms both traditional stage-wise approaches and existing\nend-to-end methods.", "published": "2024-11-15 06:54:25", "link": "http://arxiv.org/abs/2411.10496v1", "categories": ["cs.LG", "cs.AI", "q-fin.CP"], "primary_category": "cs.LG"}
{"title": "Refined and Segmented Price Sentiment Indices from Survey Comments", "abstract": "We aim to enhance a price sentiment index and to more precisely understand\nprice trends from the perspective of not only consumers but also businesses. We\nextract comments related to prices from the Economy Watchers Survey conducted\nby the Cabinet Office of Japan and classify price trends using a large language\nmodel (LLM). We classify whether the survey sample reflects the perspective of\nconsumers or businesses, and whether the comments pertain to goods or services\nby utilizing information on the fields of comments and the industries of\nrespondents included in the Economy Watchers Survey. From these classified\nprice-related comments, we construct price sentiment indices not only for a\ngeneral purpose but also for more specific objectives by combining perspectives\non consumers and prices, as well as goods and services. It becomes possible to\nachieve a more accurate classification of price directions by employing a LLM\nfor classification. Furthermore, integrating the outputs of multiple LLMs\nsuggests the potential for the better performance of the classification. The\nuse of more accurately classified comments allows for the construction of an\nindex with a higher correlation to existing indices than previous studies. We\ndemonstrate that the correlation of the price index for consumers, which has a\nlarger sample size, is further enhanced by selecting comments for aggregation\nbased on the industry of the survey respondents.", "published": "2024-11-15 04:22:21", "link": "http://arxiv.org/abs/2411.09937v2", "categories": ["cs.CL", "q-fin.CP"], "primary_category": "cs.CL"}
{"title": "Portfolio Optimization with Feedback Strategies Based on Artificial Neural Networks", "abstract": "With the recent advancements in machine learning (ML), artificial neural\nnetworks (ANN) are starting to play an increasingly important role in\nquantitative finance. Dynamic portfolio optimization is among many problems\nthat have significantly benefited from a wider adoption of deep learning (DL).\nWhile most existing research has primarily focused on how DL can alleviate the\ncurse of dimensionality when solving the Hamilton-Jacobi-Bellman (HJB)\nequation, some very recent developments propose to forego derivation and\nsolution of HJB in favor of empirical utility maximization over dynamic\nallocation strategies expressed through ANN. In addition to being simple and\ntransparent, this approach is universally applicable, as it is essentially\nagnostic about market dynamics. To showcase the method, we apply it to optimal\nportfolio allocation between a cash account and the S&P 500 index modeled using\ngeometric Brownian motion or the Heston model. In both cases, the results are\ndemonstrated to be on par with those under the theoretical optimal weights\nassuming isoelastic utility and real-time rebalancing. A set of R codes for a\nbroad class of stochastic volatility models are provided as a supplement.", "published": "2024-11-15 02:46:38", "link": "http://arxiv.org/abs/2411.09899v1", "categories": ["q-fin.PM", "q-fin.CP", "91G10, 91G60, 91G80, 62M45, 49M25"], "primary_category": "q-fin.PM"}
{"title": "The role of debt valuation factors in systemic risk assessment", "abstract": "The fragility of financial systems was starkly demonstrated in early 2023\nthrough a cascade of major bank failures in the United States, including the\nsecond, third, and fourth largest collapses in the US history. The highly\ninterdependent financial networks and the associated high systemic risk have\nbeen deemed the cause of the crashes. The goal of this paper is to enhance\nexisting systemic risk analysis frameworks by incorporating essential debt\nvaluation factors. Our results demonstrate that these additional elements\nsubstantially influence the outcomes of risk assessment. Notably, by modeling\nthe dynamic relationship between interest rates and banks' credibility, our\nframework can detect potential cascading failures that standard approaches\nmight miss. The proposed risk assessment methodology can help regulatory bodies\nprevent future failures, while also allowing companies to more accurately\npredict turmoil periods and strengthen their survivability during such events.", "published": "2024-11-15 17:48:30", "link": "http://arxiv.org/abs/2411.10386v2", "categories": ["q-fin.RM", "q-fin.MF"], "primary_category": "q-fin.RM"}
{"title": "Optimal portfolio under ratio-type periodic evaluation in stochastic factor models under convex trading constraints", "abstract": "This paper studies a type of periodic utility maximization problems for\nportfolio management in incomplete stochastic factor models with convex trading\nconstraints. The portfolio performance is periodically evaluated on the\nrelative ratio of two adjacent wealth levels over an infinite horizon,\nfeaturing the dynamic adjustments in portfolio decision according to past\nachievements. Under power utility, we transform the original infinite horizon\noptimal control problem into an auxiliary terminal wealth optimization problem\nunder a modified utility function. To cope with the convex trading constraints,\nwe further introduce an auxiliary unconstrained optimization problem in a\nmodified market model and develop the martingale duality approach to establish\nthe existence of the dual minimizer such that the optimal unconstrained wealth\nprocess can be obtained using the dual representation. With the help of the\nduality results in the auxiliary problems, the relationship between the\nconstrained and unconstrained models as well as some fixed point arguments, we\nfinally derive and verify the optimal constrained portfolio process in a\nperiodic manner for the original problem over an infinite horizon.", "published": "2024-11-15 13:19:31", "link": "http://arxiv.org/abs/2411.13579v1", "categories": ["q-fin.MF", "math.OC", "q-fin.PM"], "primary_category": "q-fin.MF"}
{"title": "Research on Domain-Specific Chinese Spelling Correction Method Based on\n  Plugin Extension Modules", "abstract": "This paper proposes a Chinese spelling correction method based on plugin\nextension modules, aimed at addressing the limitations of existing models in\nhandling domain-specific texts. Traditional Chinese spelling correction models\nare typically trained on general-domain datasets, resulting in poor performance\nwhen encountering specialized terminology in domain-specific texts. To address\nthis issue, we design an extension module that learns the features of\ndomain-specific terminology, thereby enhancing the model's correction\ncapabilities within specific domains. This extension module can provide domain\nknowledge to the model without compromising its general spelling correction\nperformance, thus improving its accuracy in specialized fields. Experimental\nresults demonstrate that after integrating extension modules for medical,\nlegal, and official document domains, the model's correction performance is\nsignificantly improved compared to the baseline model without any extension\nmodules.", "published": "2024-11-15 02:08:58", "link": "http://arxiv.org/abs/2411.09884v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SlimLM: An Efficient Small Language Model for On-Device Document\n  Assistance", "abstract": "While small language models (SLMs) show promises for mobile deployment, their\nreal-world performance and applications on smartphones remains underexplored.\nWe present SlimLM, a series of SLMs optimized for document assistance tasks on\nmobile devices. Through extensive experiments on a Samsung Galaxy S24, we\nidentify the optimal trade-offs between model size (ranging from 125M to 7B\nparameters), context length, and inference time for efficient on-device\nprocessing. SlimLM is pre-trained on SlimPajama-627B and fine-tuned on\nDocAssist, our constructed dataset for summarization, question answering and\nsuggestion tasks. Our smallest model demonstrates efficient performance on S24,\nwhile larger variants offer enhanced capabilities within mobile constraints. We\nevaluate SlimLM against existing SLMs, showing comparable or superior\nperformance and offering a benchmark for future research in on-device language\nmodels. We also provide an Android application, offering practical insights\ninto SLM deployment. Our findings provide valuable insights and illuminate the\ncapabilities of running advanced language models on high-end smartphones,\npotentially reducing server costs and enhancing privacy through on-device\nprocessing.", "published": "2024-11-15 04:44:34", "link": "http://arxiv.org/abs/2411.09944v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LoRA-LiteE: A Computationally Efficient Framework for Chatbot\n  Preference-Tuning", "abstract": "Effective preference tuning is pivotal in aligning chatbot responses with\nhuman expectations, enhancing user satisfaction and engagement. Traditional\napproaches, notably Reinforcement Learning from Human Feedback (RLHF) as\nemployed in advanced models like GPT-4, have demonstrated considerable success\nin this domain. However, RLHF methods are often computationally intensive and\nresource-demanding, limiting their scalability and accessibility for broader\napplications. To address these challenges, this study introduces LoRA-Lite\nEnsemble (LoRA-LiteE), an innovative framework that combines Supervised\nFine-tuning (SFT) with Low-Rank Adaptation (LoRA) and Ensemble Learning\ntechniques to effectively aggregate predictions of lightweight models, which\naim to achieve a balance between the performance and computational cost.\nUtilizing the Chatbot Arena benchmark dataset, we conduct a comprehensive\ncomparative analysis among our LoRA-LiteE model, corresponding base models at\ndifferent scales, and GPT-4 trained with RLHF. Our empirical results\ndemonstrate that the proposed LoRA-LiteE model achieves comparable performance\nto un-finetuned GPT-4 and outperforms the single larger-scale models under\nlimited resource constraints. These findings highlight that our LoRA-LiteE\nprovides a feasible and efficient methodology for human preference prediction\nin chatbot systems, enhancing scalability and accessibility, and thereby\nbroadening the applicability of preference-tuned chatbots in\nresource-constrained environments.", "published": "2024-11-15 04:57:13", "link": "http://arxiv.org/abs/2411.09947v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HistoLens: An LLM-Powered Framework for Multi-Layered Analysis of\n  Historical Texts -- A Case Application of Yantie Lun", "abstract": "This paper proposes HistoLens, a multi-layered analysis framework for\nhistorical texts based on Large Language Models (LLMs). Using the important\nWestern Han dynasty text \"Yantie Lun\" as a case study, we demonstrate the\nframework's potential applications in historical research and education.\nHistoLens integrates NLP technology (especially LLMs), including named entity\nrecognition, knowledge graph construction, and geographic information\nvisualization. The paper showcases how HistoLens explores Western Han culture\nin \"Yantie Lun\" through multi-dimensional, visual, and quantitative methods,\nfocusing particularly on the influence of Confucian and Legalist thoughts on\npolitical, economic, military, and ethnic. We also demonstrate how HistoLens\nconstructs a machine teaching scenario using LLMs for explainable analysis,\nbased on a dataset of Confucian and Legalist ideas extracted with LLM\nassistance. This approach offers novel and diverse perspectives for studying\nhistorical texts like \"Yantie Lun\" and provides new auxiliary tools for history\neducation. The framework aims to equip historians and learners with\nLLM-assisted tools to facilitate in-depth, multi-layered analysis of historical\ntexts and foster innovation in historical education.", "published": "2024-11-15 06:21:13", "link": "http://arxiv.org/abs/2411.09978v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Once More, With Feeling: Measuring Emotion of Acting Performances in\n  Contemporary American Film", "abstract": "Narrative film is a composition of writing, cinematography, editing, and\nperformance. While much computational work has focused on the writing or visual\nstyle in film, we conduct in this paper a computational exploration of acting\nperformance. Applying speech emotion recognition models and a variationist\nsociolinguistic analytical framework to a corpus of popular, contemporary\nAmerican film, we find narrative structure, diachronic shifts, and genre- and\ndialogue-based constraints located in spoken performances.", "published": "2024-11-15 07:53:02", "link": "http://arxiv.org/abs/2411.10018v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Information Extraction from Clinical Notes: Are We Ready to Switch to\n  Large Language Models?", "abstract": "Backgrounds: Information extraction (IE) is critical in clinical natural\nlanguage processing (NLP). While large language models (LLMs) excel on\ngenerative tasks, their performance on extractive tasks remains debated.\nMethods: We investigated Named Entity Recognition (NER) and Relation Extraction\n(RE) using 1,588 clinical notes from four sources (UT Physicians, MTSamples,\nMIMIC-III, and i2b2). We developed an annotated corpus covering 4 clinical\nentities and 16 modifiers, and compared instruction-tuned LLaMA-2 and LLaMA-3\nagainst BERT in terms of performance, generalizability, computational\nresources, and throughput to BERT. Results: LLaMA models outperformed BERT\nacross datasets. With sufficient training data, LLaMA showed modest\nimprovements (1% on NER, 1.5-3.7% on RE); improvements were larger with limited\ntraining data. On unseen i2b2 data, LLaMA-3-70B outperformed BERT by 7% (F1) on\nNER and 4% on RE. However, LLaMA models required more computing resources and\nran up to 28 times slower. We implemented \"Kiwi,\" a clinical IE package\nfeaturing both models, available at https://kiwi.clinicalnlp.org/. Conclusion:\nThis study is among the first to develop and evaluate a comprehensive clinical\nIE system using open-source LLMs. Results indicate that LLaMA models outperform\nBERT for clinical NER and RE but with higher computational costs and lower\nthroughputs. These findings highlight that choosing between LLMs and\ntraditional deep learning methods for clinical IE applications should remain\ntask-specific, taking into account both performance metrics and practical\nconsiderations such as available computing resources and the intended use case\nscenarios.", "published": "2024-11-15 07:54:19", "link": "http://arxiv.org/abs/2411.10020v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Xmodel-1.5: An 1B-scale Multilingual LLM", "abstract": "We introduce Xmodel-1.5, a 1-billion-parameter multilingual large language\nmodel pretrained on 2 trillion tokens, designed for balanced performance and\nscalability. Unlike most large models that use the BPE tokenizer, Xmodel-1.5\nemploys a custom unigram tokenizer with 65,280 tokens, optimizing both\nefficiency and accuracy. The model delivers competitive results across multiple\nlanguages, including Thai, Arabic, French, Chinese, and English, outperforming\nAlibaba's PolyLM-1.7B on respective evaluation datasets. Xmodel-1.5 excels in\nbenchmarks like mMMLU and PIQA, and achieves state-of-the-art results in Thai.\nTo support low-resource language research, we release Xdata_Thai, a\nThai-specific evaluation dataset featuring unique linguistic challenges such as\ngendered particles and idioms. While the model demonstrates strong performance,\nthere is still room for improvement in handling culturally specific nuances. We\nhope this work contributes to advancements in multilingual AI research. Models\nand code are publicly available on GitHub at\nhttps://github.com/XiaoduoAILab/XmodelLM-1.5", "published": "2024-11-15 10:01:52", "link": "http://arxiv.org/abs/2411.10083v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Effective Framework to Help Large Language Models Handle\n  Numeric-involved Long-context Tasks", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nhandling long texts and have almost perfect performance in traditional\nretrieval tasks. However, their performance significantly degrades when it\ncomes to numerical calculations in the long-context. Numeric-involved\nlong-context tasks typically cannot be addressed by current LLMs in normal\nsettings due to their inherent limitations in simultaneously handling complex\nand massive information. Some CoT like prompting methods can improve accuracy\nbut demands massive output tokens, which is costly and slow. To address this\nissue, we propose a workflow, which decompose a numeric-involved long-context\ntask into 4 low-level subtasks: judging, extracting and processing with code\nand conclusion. The former 2 subtasks is relatively simple, which allows us to\nuse smaller models for efficiently processing long context. When numerical\ncalculations are required, we use code generated by LLMs to avoid the\ndisadvantage of LLM not being good at calculations. The results in 2\nnumeric-involved long-context benchmarks demonstrate our workflow can not only\nimprove accuracy, but also significantly reduce the cost of API calls.", "published": "2024-11-15 12:39:02", "link": "http://arxiv.org/abs/2411.10145v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compound-QA: A Benchmark for Evaluating LLMs on Compound Questions", "abstract": "Large language models (LLMs) demonstrate remarkable performance across\nvarious tasks, prompting researchers to develop diverse evaluation benchmarks.\nHowever, existing benchmarks typically measure the ability of LLMs to respond\nto individual questions, neglecting the complex interactions in real-world\napplications. In this paper, we introduce Compound Question Synthesis (CQ-Syn)\nto create the Compound-QA benchmark, focusing on compound questions with\nmultiple sub-questions. This benchmark is derived from existing QA datasets,\nannotated with proprietary LLMs and verified by humans for accuracy. It\nencompasses five categories: Factual-Statement, Cause-and-Effect,\nHypothetical-Analysis, Comparison-and-Selection, and Evaluation-and-Suggestion.\nIt evaluates the LLM capability in terms of three dimensions including\nunderstanding, reasoning, and knowledge. Our assessment of eight open-source\nLLMs using Compound-QA reveals distinct patterns in their responses to compound\nquestions, which are significantly poorer than those to non-compound questions.\nAdditionally, we investigate various methods to enhance LLMs performance on\ncompound questions. The results indicate that these approaches significantly\nimprove the models' comprehension and reasoning abilities on compound\nquestions.", "published": "2024-11-15 13:12:29", "link": "http://arxiv.org/abs/2411.10163v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling Topological Structures in Text: A Comprehensive Survey of\n  Topological Data Analysis Applications in NLP", "abstract": "The surge of data available on the internet has led to the adoption of\nvarious computational methods to analyze and extract valuable insights from\nthis wealth of information. Among these, the field of Machine Learning (ML) has\nthrived by leveraging data to extract meaningful insights. However, ML\ntechniques face notable challenges when dealing with real-world data, often due\nto issues of imbalance, noise, insufficient labeling, and high dimensionality.\nTo address these limitations, some researchers advocate for the adoption of\nTopological Data Analysis (TDA), a statistical approach that discerningly\ncaptures the intrinsic shape of data despite noise. Despite its potential, TDA\nhas not gained as much traction within the Natural Language Processing (NLP)\ndomain compared to structurally distinct areas like computer vision.\nNevertheless, a dedicated community of researchers has been exploring the\napplication of TDA in NLP, yielding 87 papers we comprehensively survey in this\npaper. Our findings categorize these efforts into theoretical and\nnon-theoretical approaches. Theoretical approaches aim to explain linguistic\nphenomena from a topological viewpoint, while non-theoretical approaches merge\nTDA with ML features, utilizing diverse numerical representation techniques. We\nconclude by exploring the challenges and unresolved questions that persist in\nthis niche field. Resources and a list of papers on this topic can be found at:\nhttps://github.com/AdaUchendu/AwesomeTDA4NLP.", "published": "2024-11-15 15:55:05", "link": "http://arxiv.org/abs/2411.10298v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotion Detection in Reddit: Comparative Study of Machine Learning and\n  Deep Learning Techniques", "abstract": "Emotion detection is pivotal in human communication, as it significantly\ninfluences behavior, relationships, and decision-making processes. This study\nconcentrates on text-based emotion detection by leveraging the GoEmotions\ndataset, which annotates Reddit comments with 27 distinct emotions. These\nemotions are subsequently mapped to Ekman's six basic categories: joy, anger,\nfear, sadness, disgust, and surprise. We employed a range of models for this\ntask, including six machine learning models, three ensemble models, and a Long\nShort-Term Memory (LSTM) model to determine the optimal model for emotion\ndetection. Results indicate that the Stacking classifier outperforms other\nmodels in accuracy and performance. We also benchmark our models against\nEmoBERTa, a pre-trained emotion detection model, with our Stacking classifier\nproving more effective. Finally, the Stacking classifier is deployed via a\nStreamlit web application, underscoring its potential for real-world\napplications in text-based emotion analysis.", "published": "2024-11-15 16:28:25", "link": "http://arxiv.org/abs/2411.10328v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"On the goals of linguistic theory\": Revisiting Chomskyan theories in\n  the era of AI", "abstract": "Theoretical linguistics seeks to explain what human language is, and why.\nLinguists and cognitive scientists have proposed different theoretical models\nof what language is, as well as cognitive factors that shape it, and allow\nhumans to 'produce', 'understand', and 'acquire' natural languages. However,\nhumans may no longer be the only ones learning to 'generate', 'parse', and\n'learn' natural language: artificial intelligence (AI) models such as large\nlanguage models are proving to have impressive linguistic capabilities. Many\nare thus questioning what role, if any, such models should play in helping\ntheoretical linguistics reach its ultimate research goals? In this paper, we\npropose to answer this question, by reiterating the tenets of generative\nlinguistics, a leading school of thought in the field, and by considering how\nAI models as theories of language relate to each of these important concepts.\nSpecifically, we consider three foundational principles, finding roots in the\nearly works of Noam Chomsky: (1) levels of theoretical adequacy; (2) procedures\nfor linguistic theory development; (3) language learnability and Universal\nGrammar. In our discussions of each principle, we give special attention to two\ntypes of AI models: neural language models and neural grammar induction models.\nWe will argue that such models, in particular neural grammar induction models,\ndo have a role to play, but that this role is largely modulated by the stance\none takes regarding each of these three guiding principles.", "published": "2024-11-15 19:09:22", "link": "http://arxiv.org/abs/2411.10533v1", "categories": ["cs.CL", "F.1.1; I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "MLAN: Language-Based Instruction Tuning Improves Zero-Shot\n  Generalization of Multimodal Large Language Models", "abstract": "We present a novel instruction tuning recipe to improve the zero-shot task\ngeneralization of multimodal large language models. In contrast to existing\ninstruction tuning mechanisms that heavily rely on visual instructions, our\napproach focuses on language-based instruction tuning, offering a distinct and\nmore training efficient path for multimodal instruction tuning. We evaluate the\nperformance of the proposed approach on 9 unseen datasets across both language\nand vision modalities. Our results show that our language-only instruction\ntuning is able to significantly improve the performance of two pretrained\nmultimodal models based on Llama 2 and Vicuna on those unseen datasets.\nInterestingly, the language instruction following ability also helps unlock the\nmodels to follow vision instructions without explicit training. Compared to the\nstate of the art multimodal instruction tuning approaches that are mainly based\non visual instructions, our language-based method not only achieves superior\nperformance but also significantly enhances training efficiency. For instance,\nthe language-only instruction tuning produces competitive average performance\nacross the evaluated datasets (with even better performance on language\ndatasets) with significant training efficiency improvements (on average 4x),\nthanks to the striking reduction in the need for vision data. With a small\nnumber of visual instructions, this emerging language instruction following\nability transfers well to the unseen vision datasets, outperforming the state\nof the art with greater training efficiency.", "published": "2024-11-15 20:09:59", "link": "http://arxiv.org/abs/2411.10557v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KULCQ: An Unsupervised Keyword-based Utterance Level Clustering Quality\n  Metric", "abstract": "Intent discovery is crucial for both building new conversational agents and\nimproving existing ones. While several approaches have been proposed for intent\ndiscovery, most rely on clustering to group similar utterances together.\nTraditional evaluation of these utterance clusters requires intent labels for\neach utterance, limiting scalability. Although some clustering quality metrics\nexist that do not require labeled data, they focus solely on cluster geometry\nwhile ignoring the linguistic nuances present in conversational transcripts. In\nthis paper, we introduce Keyword-based Utterance Level Clustering Quality\n(KULCQ), an unsupervised metric that leverages keyword analysis to evaluate\nclustering quality. We demonstrate KULCQ's effectiveness by comparing it with\nexisting unsupervised clustering metrics and validate its performance through\ncomprehensive ablation studies. Our results show that KULCQ better captures\nsemantic relationships in conversational data while maintaining consistency\nwith geometric clustering principles.", "published": "2024-11-15 00:21:02", "link": "http://arxiv.org/abs/2411.09853v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models as User-Agents for Evaluating\n  Task-Oriented-Dialogue Systems", "abstract": "Traditionally, offline datasets have been used to evaluate task-oriented\ndialogue (TOD) models. These datasets lack context awareness, making them\nsuboptimal benchmarks for conversational systems. In contrast, user-agents,\nwhich are context-aware, can simulate the variability and unpredictability of\nhuman conversations, making them better alternatives as evaluators. Prior\nresearch has utilized large language models (LLMs) to develop user-agents. Our\nwork builds upon this by using LLMs to create user-agents for the evaluation of\nTOD systems. This involves prompting an LLM, using in-context examples as\nguidance, and tracking the user-goal state. Our evaluation of diversity and\ntask completion metrics for the user-agents shows improved performance with the\nuse of better prompts. Additionally, we propose methodologies for the automatic\nevaluation of TOD models within this dynamic framework.", "published": "2024-11-15 06:05:45", "link": "http://arxiv.org/abs/2411.09972v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Orca: Enhancing Role-Playing Abilities of Large Language Models by\n  Integrating Personality Traits", "abstract": "Large language models has catalyzed the development of personalized dialogue\nsystems, numerous role-playing conversational agents have emerged. While\nprevious research predominantly focused on enhancing the model's capability to\nfollow instructions by designing character profiles, neglecting the\npsychological factors that drive human conversations. In this paper, we propose\nOrca, a framework for data processing and training LLMs of custom characters by\nintegrating personality traits. Orca comprises four stages: (1) Personality\ntraits inferring, leverage LLMs to infer user's BigFive personality trait\nreports and scores. (2) Data Augment, simulate user's profile, background\nstory, and psychological activities. (3) Dataset construction,\npersonality-conditioned instruction prompting (PCIP) to stimulate LLMs. (4)\nModeling and Training, personality-conditioned instruction tuning (PTIT and\nPSIT), using the generated data to enhance existing open-source LLMs. We\nintroduce OrcaBench, the first benchmark for evaluating the quality of content\ngenerated by LLMs on social platforms across multiple scales. Our experiments\ndemonstrate that our proposed model achieves superior performance on this\nbenchmark, demonstrating its excellence and effectiveness in perceiving\npersonality traits that significantly improve role-playing abilities. Our Code\nis available at https://github.com/Aipura/Orca.", "published": "2024-11-15 07:35:47", "link": "http://arxiv.org/abs/2411.10006v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CMATH: Cross-Modality Augmented Transformer with Hierarchical\n  Variational Distillation for Multimodal Emotion Recognition in Conversation", "abstract": "Multimodal emotion recognition in conversation (MER) aims to accurately\nidentify emotions in conversational utterances by integrating multimodal\ninformation. Previous methods usually treat multimodal information as equal\nquality and employ symmetric architectures to conduct multimodal fusion.\nHowever, in reality, the quality of different modalities usually varies\nconsiderably, and utilizing a symmetric architecture is difficult to accurately\nrecognize conversational emotions when dealing with uneven modal information.\nFurthermore, fusing multi-modality information in a single granularity may fail\nto adequately integrate modal information, exacerbating the inaccuracy in\nemotion recognition. In this paper, we propose a novel Cross-Modality Augmented\nTransformer with Hierarchical Variational Distillation, called CMATH, which\nconsists of two major components, i.e., Multimodal Interaction Fusion and\nHierarchical Variational Distillation. The former is comprised of two\nsubmodules, including Modality Reconstruction and Cross-Modality Augmented\nTransformer (CMA-Transformer), where Modality Reconstruction focuses on\nobtaining high-quality compressed representation of each modality, and\nCMA-Transformer adopts an asymmetric fusion strategy which treats one modality\nas the central modality and takes others as auxiliary modalities. The latter\nfirst designs a variational fusion network to fuse the fine-grained\nrepresentations learned by CMA- Transformer into a coarse-grained\nrepresentations. Then, it introduces a hierarchical distillation framework to\nmaintain the consistency between modality representations with different\ngranularities. Experiments on the IEMOCAP and MELD datasets demonstrate that\nour proposed model outperforms previous state-of-the-art baselines.\nImplementation codes can be available at https://github.com/ cjw-MER/CMATH.", "published": "2024-11-15 09:23:02", "link": "http://arxiv.org/abs/2411.10060v1", "categories": ["cs.MM", "cs.CL"], "primary_category": "cs.MM"}
{"title": "Layer Importance and Hallucination Analysis in Large Language Models via\n  Enhanced Activation Variance-Sparsity", "abstract": "Evaluating the importance of different layers in large language models (LLMs)\nis crucial for optimizing model performance and interpretability. This paper\nfirst explores layer importance using the Activation Variance-Sparsity Score\n(AVSS), which combines normalized activation variance and sparsity to quantify\neach layer's contribution to overall model performance. By ranking layers based\non AVSS and pruning the least impactful 25\\%, our experiments on tasks such as\nquestion answering, language modeling, and sentiment classification show that\nover 90\\% of the original performance is retained, highlighting potential\nredundancies in LLM architectures. Building on AVSS, we propose an enhanced\nversion tailored to assess hallucination propensity across layers (EAVSS). This\nimproved approach introduces Hallucination-Specific Activation Variance (HSAV)\nand Hallucination-Specific Sparsity (HSS) metrics, allowing precise\nidentification of hallucination-prone layers. By incorporating contrastive\nlearning on these layers, we effectively mitigate hallucination generation,\ncontributing to more robust and efficient LLMs(The maximum performance\nimprovement is 12\\%). Our results on the NQ, SciQ, TriviaQA, TruthfulQA, and\nWikiQA datasets demonstrate the efficacy of this method, offering a\ncomprehensive framework for both layer importance evaluation and hallucination\nmitigation in LLMs.", "published": "2024-11-15 09:33:47", "link": "http://arxiv.org/abs/2411.10069v1", "categories": ["cs.CL", "cs.PF"], "primary_category": "cs.CL"}
{"title": "Understanding The Effect Of Temperature On Alignment With Human Opinions", "abstract": "With the increasing capabilities of LLMs, recent studies focus on\nunderstanding whose opinions are represented by them and how to effectively\nextract aligned opinion distributions. We conducted an empirical analysis of\nthree straightforward methods for obtaining distributions and evaluated the\nresults across a variety of metrics. Our findings suggest that sampling and\nlog-probability approaches with simple parameter adjustments can return better\naligned outputs in subjective tasks compared to direct prompting. Yet, assuming\nmodels reflect human opinions may be limiting, highlighting the need for\nfurther research on how human subjectivity affects model uncertainty.", "published": "2024-11-15 09:50:27", "link": "http://arxiv.org/abs/2411.10080v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Memorization in Attention-only Transformers", "abstract": "Recent research has explored the memorization capacity of multi-head\nattention, but these findings are constrained by unrealistic limitations on the\ncontext size. We present a novel proof for language-based Transformers that\nextends the current hypothesis to any context size. Our approach improves upon\nthe state-of-the-art by achieving more effective exact memorization with an\nattention layer, while also introducing the concept of approximate memorization\nof distributions. Through experimental validation, we demonstrate that our\nproposed bounds more accurately reflect the true memorization capacity of\nlanguage models, and provide a precise comparison with prior work.", "published": "2024-11-15 11:29:31", "link": "http://arxiv.org/abs/2411.10115v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Legal Evalutions and Challenges of Large Language Models", "abstract": "In this paper, we review legal testing methods based on Large Language Models\n(LLMs), using the OPENAI o1 model as a case study to evaluate the performance\nof large models in applying legal provisions. We compare current\nstate-of-the-art LLMs, including open-source, closed-source, and legal-specific\nmodels trained specifically for the legal domain. Systematic tests are\nconducted on English and Chinese legal cases, and the results are analyzed in\ndepth. Through systematic testing of legal cases from common law systems and\nChina, this paper explores the strengths and weaknesses of LLMs in\nunderstanding and applying legal texts, reasoning through legal issues, and\npredicting judgments. The experimental results highlight both the potential and\nlimitations of LLMs in legal applications, particularly in terms of challenges\nrelated to the interpretation of legal language and the accuracy of legal\nreasoning. Finally, the paper provides a comprehensive analysis of the\nadvantages and disadvantages of various types of models, offering valuable\ninsights and references for the future application of AI in the legal field.", "published": "2024-11-15 12:23:12", "link": "http://arxiv.org/abs/2411.10137v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating the role of `Constitutions' for learning from AI feedback", "abstract": "The growing capabilities of large language models (LLMs) have led to their\nuse as substitutes for human feedback for training and assessing other LLMs.\nThese methods often rely on `constitutions', written guidelines which a critic\nmodel uses to provide feedback and improve generations. We investigate how the\nchoice of constitution affects feedback quality by using four different\nconstitutions to improve patient-centered communication in medical interviews.\nIn pairwise comparisons conducted by 215 human raters, we found that detailed\nconstitutions led to better results regarding emotive qualities. However, none\nof the constitutions outperformed the baseline in learning more\npractically-oriented skills related to information gathering and provision. Our\nfindings indicate that while detailed constitutions should be prioritised,\nthere are possible limitations to the effectiveness of AI feedback as a reward\nsignal in certain areas.", "published": "2024-11-15 13:16:11", "link": "http://arxiv.org/abs/2411.10168v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Increasing the Accessibility of Causal Domain Knowledge via Causal\n  Information Extraction Methods: A Case Study in the Semiconductor\n  Manufacturing Industry", "abstract": "The extraction of causal information from textual data is crucial in the\nindustry for identifying and mitigating potential failures, enhancing process\nefficiency, prompting quality improvements, and addressing various operational\nchallenges. This paper presents a study on the development of automated methods\nfor causal information extraction from actual industrial documents in the\nsemiconductor manufacturing industry. The study proposes two types of causal\ninformation extraction methods, single-stage sequence tagging (SST) and\nmulti-stage sequence tagging (MST), and evaluates their performance using\nexisting documents from a semiconductor manufacturing company, including\npresentation slides and FMEA (Failure Mode and Effects Analysis) documents. The\nstudy also investigates the effect of representation learning on downstream\ntasks. The presented case study showcases that the proposed MST methods for\nextracting causal information from industrial documents are suitable for\npractical applications, especially for semi structured documents such as FMEAs,\nwith a 93\\% F1 score. Additionally, MST achieves a 73\\% F1 score on texts\nextracted from presentation slides. Finally, the study highlights the\nimportance of choosing a language model that is more aligned with the domain\nand in-domain fine-tuning.", "published": "2024-11-15 13:18:18", "link": "http://arxiv.org/abs/2411.10172v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Measuring Non-Adversarial Reproduction of Training Data in Large\n  Language Models", "abstract": "Large language models memorize parts of their training data. Memorizing short\nsnippets and facts is required to answer questions about the world and to be\nfluent in any language. But models have also been shown to reproduce long\nverbatim sequences of memorized text when prompted by a motivated adversary. In\nthis work, we investigate an intermediate regime of memorization that we call\nnon-adversarial reproduction, where we quantify the overlap between model\nresponses and pretraining data when responding to natural and benign prompts.\nFor a variety of innocuous prompt categories (e.g., writing a letter or a\ntutorial), we show that up to 15% of the text output by popular conversational\nlanguage models overlaps with snippets from the Internet. In worst cases, we\nfind generations where 100% of the content can be found exactly online. For the\nsame tasks, we find that human-written text has far less overlap with Internet\ndata. We further study whether prompting strategies can close this reproduction\ngap between models and humans. While appropriate prompting can reduce\nnon-adversarial reproduction on average, we find that mitigating worst-case\nreproduction of training data requires stronger defenses -- even for benign\ninteractions.", "published": "2024-11-15 14:55:01", "link": "http://arxiv.org/abs/2411.10242v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling up the Evaluation of Collaborative Problem Solving: Promises and\n  Challenges of Coding Chat Data with ChatGPT", "abstract": "Collaborative problem solving (CPS) is widely recognized as a critical 21st\ncentury skill. Efficiently coding communication data is a big challenge in\nscaling up research on assessing CPS. This paper reports the findings on using\nChatGPT to directly code CPS chat data by benchmarking performance across\nmultiple datasets and coding frameworks. We found that ChatGPT-based coding\noutperformed human coding in tasks where the discussions were characterized by\ncolloquial languages but fell short in tasks where the discussions dealt with\nspecialized scientific terminology and contexts. The findings offer practical\nguidelines for researchers to develop strategies for efficient and scalable\nanalysis of communication data from CPS tasks.", "published": "2024-11-15 14:57:39", "link": "http://arxiv.org/abs/2411.10246v2", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "A Survey of Event Causality Identification: Principles, Taxonomy,\n  Challenges, and Assessment", "abstract": "Event Causality Identification (ECI) has become a crucial task in Natural\nLanguage Processing (NLP), aimed at automatically extracting causalities from\ntextual data. In this survey, we systematically address the foundational\nprinciples, technical frameworks, and challenges of ECI, offering a\ncomprehensive taxonomy to categorize and clarify current research\nmethodologies, as well as a quantitative assessment of existing models. We\nfirst establish a conceptual framework for ECI, outlining key definitions,\nproblem formulations, and evaluation standards. Our taxonomy classifies ECI\nmethods according to the two primary tasks of sentence-level (SECI) and\ndocument-level (DECI) event causality identification. For SECI, we examine\nfeature pattern-based matching, deep semantic encoding, causal knowledge\npre-training and prompt-based fine-tuning, and external knowledge enhancement\nmethods. For DECI, we highlight approaches focused on event graph reasoning and\nprompt-based techniques to address the complexity of cross-sentence causal\ninference. Additionally, we analyze the strengths, limitations, and open\nchallenges of each approach. We further conduct an extensive quantitative\nevaluation of various ECI methods on two benchmark datasets. Finally, we\nexplore future research directions, highlighting promising pathways to overcome\ncurrent limitations and broaden ECI applications.", "published": "2024-11-15 17:19:42", "link": "http://arxiv.org/abs/2411.10371v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding\n  Conversations", "abstract": "We introduce Llama Guard 3 Vision, a multimodal LLM-based safeguard for\nhuman-AI conversations that involves image understanding: it can be used to\nsafeguard content for both multimodal LLM inputs (prompt classification) and\noutputs (response classification). Unlike the previous text-only Llama Guard\nversions (Inan et al., 2023; Llama Team, 2024b,a), it is specifically designed\nto support image reasoning use cases and is optimized to detect harmful\nmultimodal (text and image) prompts and text responses to these prompts. Llama\nGuard 3 Vision is fine-tuned on Llama 3.2-Vision and demonstrates strong\nperformance on the internal benchmarks using the MLCommons taxonomy. We also\ntest its robustness against adversarial attacks. We believe that Llama Guard 3\nVision serves as a good starting point to build more capable and robust content\nmoderation tools for human-AI conversation with multimodal capabilities.", "published": "2024-11-15 18:34:07", "link": "http://arxiv.org/abs/2411.10414v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Towards Automatic Evaluation of Task-Oriented Dialogue Flows", "abstract": "Task-oriented dialogue systems rely on predefined conversation schemes\n(dialogue flows) often represented as directed acyclic graphs. These flows can\nbe manually designed or automatically generated from previously recorded\nconversations. Due to variations in domain expertise or reliance on different\nsets of prior conversations, these dialogue flows can manifest in significantly\ndifferent graph structures. Despite their importance, there is no standard\nmethod for evaluating the quality of dialogue flows. We introduce FuDGE (Fuzzy\nDialogue-Graph Edit Distance), a novel metric that evaluates dialogue flows by\nassessing their structural complexity and representational coverage of the\nconversation data. FuDGE measures how well individual conversations align with\na flow and, consequently, how well a set of conversations is represented by the\nflow overall. Through extensive experiments on manually configured flows and\nflows generated by automated techniques, we demonstrate the effectiveness of\nFuDGE and its evaluation framework. By standardizing and optimizing dialogue\nflows, FuDGE enables conversational designers and automated techniques to\nachieve higher levels of efficiency and automation.", "published": "2024-11-15 18:35:00", "link": "http://arxiv.org/abs/2411.10416v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing the Reasoning Ability of Multimodal Large Language Models via\n  Mixed Preference Optimization", "abstract": "Existing open-source multimodal large language models (MLLMs) generally\nfollow a training process involving pre-training and supervised fine-tuning.\nHowever, these models suffer from distribution shifts, which limit their\nmultimodal reasoning, particularly in the Chain-of-Thought (CoT) performance.\nTo address this, we introduce a preference optimization (PO) process to enhance\nthe multimodal reasoning capabilities of MLLMs. Specifically, (1) on the data\nside, we design an automated preference data construction pipeline to create\nMMPR, a high-quality, large-scale multimodal reasoning preference dataset; and\n(2) on the model side, we explore integrating PO with MLLMs, developing a\nsimple yet effective method, termed Mixed Preference Optimization (MPO), which\nboosts multimodal CoT performance. Our approach enhances the multimodal\nreasoning abilities of both InternVL2-8B and InternVL2-76B. Notably, our model,\nInternVL2-8B-MPO, achieves an accuracy of 67.0 on MathVista, outperforming\nInternVL2-8B by 8.7 points and achieving performance comparable to the\n10$\\times$ larger InternVL2-76B. We hope this study could inspire further\nadvancements in MLLMs. Code, data, and model are released.", "published": "2024-11-15 18:59:27", "link": "http://arxiv.org/abs/2411.10442v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Does Prompt Formatting Have Any Impact on LLM Performance?", "abstract": "In the realm of Large Language Models (LLMs), prompt optimization is crucial\nfor model performance. Although previous research has explored aspects like\nrephrasing prompt contexts, using various prompting techniques (like in-context\nlearning and chain-of-thought), and ordering few-shot examples, our\nunderstanding of LLM sensitivity to prompt templates remains limited.\nTherefore, this paper examines the impact of different prompt templates on LLM\nperformance. We formatted the same contexts into various human-readable\ntemplates, including plain text, Markdown, JSON, and YAML, and evaluated their\nimpact across tasks like natural language reasoning, code generation, and\ntranslation using OpenAI's GPT models. Experiments show that GPT-3.5-turbo's\nperformance varies by up to 40\\% in a code translation task depending on the\nprompt template, while larger models like GPT-4 are more robust to these\nvariations. Our analysis highlights the need to reconsider the use of fixed\nprompt templates, as different formats can significantly affect model\nperformance.", "published": "2024-11-15 19:26:38", "link": "http://arxiv.org/abs/2411.10541v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SoftLMs: Efficient Adaptive Low-Rank Approximation of Language Models\n  using Soft-Thresholding Mechanism", "abstract": "Extensive efforts have been made to boost the performance in the domain of\nlanguage models by introducing various attention-based transformers. However,\nthe inclusion of linear layers with large dimensions contributes to significant\ncomputational and memory overheads. The escalating computational demands of\nthese models necessitate the development of various compression techniques to\nensure their deployment on devices, particularly in resource-constrained\nenvironments. In this paper, we propose a novel compression methodology that\ndynamically determines the rank of each layer using a soft thresholding\nmechanism, which clips the singular values with a small magnitude in a\ndifferentiable form. This approach automates the decision-making process to\nidentify the optimal degree of compression for each layer. We have successfully\napplied the proposed technique to attention-based architectures, including BERT\nfor discriminative tasks and GPT2 and TinyLlama for generative tasks.\nAdditionally, we have validated our method on Mamba, a recently proposed\nstate-space model. Our experiments demonstrate that the proposed technique\nachieves a speed-up of 1.33X to 1.72X in the encoder/ decoder with a 50%\nreduction in total parameters.", "published": "2024-11-15 19:29:51", "link": "http://arxiv.org/abs/2411.10543v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Efficient Alignment of Large Language Models via Data Sampling", "abstract": "LLM alignment ensures that large language models behave safely and\neffectively by aligning their outputs with human values, goals, and intentions.\nAligning LLMs employ huge amounts of data, computation, and time. Moreover,\ncurating data with human feedback is expensive and takes time. Recent research\ndepicts the benefit of data engineering in the fine-tuning and pre-training\nparadigms to bring down such costs. However, alignment differs from the\nafore-mentioned paradigms and it is unclear if data efficient alignment is\nfeasible. In this work, we first aim to understand how the performance of LLM\nalignment scales with data. We find out that LLM alignment performance follows\nan exponential plateau pattern which tapers off post a rapid initial increase.\nBased on this, we identify data subsampling as a viable method to reduce\nresources required for alignment. Further, we propose an information\ntheory-based methodology for efficient alignment by identifying a small high\nquality subset thereby reducing the computation and time required by alignment.\nWe evaluate the proposed methodology over multiple datasets and compare the\nresults. We find that the model aligned using our proposed methodology\noutperforms other sampling methods and performs comparable to the model aligned\nwith the full dataset while using less than 10% data, leading to greater than\n90% savings in costs, resources, and faster LLM alignment.", "published": "2024-11-15 19:36:15", "link": "http://arxiv.org/abs/2411.10545v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "On the Shortcut Learning in Multilingual Neural Machine Translation", "abstract": "In this study, we revisit the commonly-cited off-target issue in multilingual\nneural machine translation (MNMT). By carefully designing experiments on\ndifferent MNMT scenarios and models, we attribute the off-target issue to the\noverfitting of the shortcuts of (non-centric, centric) language mappings.\nSpecifically, the learned shortcuts biases MNMT to mistakenly translate\nnon-centric languages into the centric language instead of the expected\nnon-centric language for zero-shot translation. Analyses on learning dynamics\nshow that the shortcut learning generally occurs in the later stage of model\ntraining, and multilingual pretraining accelerates and aggravates the shortcut\nlearning. Based on these observations, we propose a simple and effective\ntraining strategy to eliminate the shortcuts in MNMT models by leveraging the\nforgetting nature of model training. The only difference from the standard\ntraining is that we remove the training instances that may induce the shortcut\nlearning in the later stage of model training. Without introducing any\nadditional data and computational costs, our approach can consistently and\nsignificantly improve the zero-shot translation performance by alleviating the\nshortcut learning for different MNMT models and benchmarks.", "published": "2024-11-15 21:09:36", "link": "http://arxiv.org/abs/2411.10581v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A dataset of questions on decision-theoretic reasoning in Newcomb-like\n  problems", "abstract": "We introduce a dataset of natural-language questions in the decision theory\nof so-called Newcomb-like problems. Newcomb-like problems include, for\ninstance, decision problems in which an agent interacts with a similar other\nagent, and thus has to reason about the fact that the other agent will likely\nreason in similar ways. Evaluating LLM reasoning about Newcomb-like problems is\nimportant because interactions between foundation-model-based agents will often\nbe Newcomb-like. Some ways of reasoning about Newcomb-like problems may allow\nfor greater cooperation between models.\n  Our dataset contains both capabilities questions (i.e., questions with a\nunique, uncontroversially correct answer) and attitude questions (i.e.,\nquestions about which decision theorists would disagree). We use our dataset\nfor an investigation of decision-theoretical capabilities and expressed\nattitudes and their interplay in existing models (different models by OpenAI,\nAnthropic, Meta, GDM, Reka, etc.), as well as models under simple prompt-based\ninterventions. We find, among other things, that attitudes vary significantly\nbetween existing models; that high capabilities are associated with attitudes\nmore favorable toward so-called evidential decision theory; and that attitudes\nare consistent across different types of questions.", "published": "2024-11-15 21:19:04", "link": "http://arxiv.org/abs/2411.10588v3", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Leveraging large language models for efficient representation learning\n  for entity resolution", "abstract": "In this paper, the authors propose TriBERTa, a supervised entity resolution\nsystem that utilizes a pre-trained large language model and a triplet loss\nfunction to learn representations for entity matching. The system consists of\ntwo steps: first, name entity records are fed into a Sentence Bidirectional\nEncoder Representations from Transformers (SBERT) model to generate vector\nrepresentations, which are then fine-tuned using contrastive learning based on\na triplet loss function. Fine-tuned representations are used as input for\nentity matching tasks, and the results show that the proposed approach\noutperforms state-of-the-art representations, including SBERT without\nfine-tuning and conventional Term Frequency-Inverse Document Frequency\n(TF-IDF), by a margin of 3 - 19%. Additionally, the representations generated\nby TriBERTa demonstrated increased robustness, maintaining consistently higher\nperformance across a range of datasets. The authors also discussed the\nimportance of entity resolution in today's data-driven landscape and the\nchallenges that arise when identifying and reconciling duplicate data across\ndifferent sources. They also described the ER process, which involves several\ncrucial steps, including blocking, entity matching, and clustering.", "published": "2024-11-15 23:24:07", "link": "http://arxiv.org/abs/2411.10629v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "JRadiEvo: A Japanese Radiology Report Generation Model Enhanced by\n  Evolutionary Optimization of Model Merging", "abstract": "With the rapid advancement of large language models (LLMs), foundational\nmodels (FMs) have seen significant advancements. Healthcare is one of the most\ncrucial application areas for these FMs, given the significant time and effort\nrequired for physicians to analyze large volumes of patient data. Recent\nefforts have focused on adapting multimodal FMs to the medical domain through\ntechniques like instruction-tuning, leading to the development of medical\nfoundation models (MFMs). However, these approaches typically require large\namounts of training data to effectively adapt models to the medical field.\nMoreover, most existing models are trained on English datasets, limiting their\npracticality in non-English-speaking regions where healthcare professionals and\npatients are not always fluent in English. The need for translation introduces\nadditional costs and inefficiencies. To address these challenges, we propose a\n\\textbf{J}apanese \\textbf{Radi}ology report generation model enhanced by\n\\textbf{Evo}lutionary optimization of model merging (JRadiEvo). This is the\nfirst attempt to extend a non-medical vision-language foundation model to the\nmedical domain through evolutionary optimization of model merging. We\nsuccessfully created a model that generates accurate Japanese reports from\nX-ray images using only 50 translated samples from publicly available data.\nThis model, developed with highly efficient use of limited data, outperformed\nleading models from recent research trained on much larger datasets.\nAdditionally, with only 8 billion parameters, this relatively compact\nfoundation model can be deployed locally within hospitals, making it a\npractical solution for environments where APIs and other external services\ncannot be used due to strict privacy and security requirements.", "published": "2024-11-15 04:16:50", "link": "http://arxiv.org/abs/2411.09933v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.CV"}
{"title": "Towards unearthing neglected climate innovations from scientific\n  literature using Large Language Models", "abstract": "Climate change poses an urgent global threat, needing the rapid\nidentification and deployment of innovative solutions. We hypothesise that many\nof these solutions already exist within scientific literature but remain\nunderutilised. To address this gap, this study employs a curated dataset\nsourced from OpenAlex, a comprehensive repository of scientific papers.\nUtilising Large Language Models (LLMs), such as GPT4-o from OpenAI, we evaluate\ntitle-abstract pairs from scientific papers on seven dimensions, covering\nclimate change mitigation potential, stage of technological development, and\nreadiness for deployment. The outputs of the language models are then compared\nwith human evaluations to assess their effectiveness in identifying promising\nyet overlooked climate innovations. Our findings suggest that these LLM-based\nmodels can effectively augment human expertise, uncovering climate solutions\nthat are potentially impactful but with far greater speed, throughput and\nconsistency. Here, we focused on UK-based solutions, but the workflow is\nregion-agnostic. This work contributes to the discovery of neglected\ninnovations in scientific literature and demonstrates the potential of AI in\nenhancing climate action strategies.", "published": "2024-11-15 09:17:40", "link": "http://arxiv.org/abs/2411.10055v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Prompting and Fine-tuning Large Language Models for Automated Code\n  Review Comment Generation", "abstract": "Generating accurate code review comments remains a significant challenge due\nto the inherently diverse and non-unique nature of the task output. Large\nlanguage models pretrained on both programming and natural language data tend\nto perform well in code-oriented tasks. However, large-scale pretraining is not\nalways feasible due to its environmental impact and project-specific\ngeneralizability issues. In this work, first we fine-tune open-source Large\nlanguage models (LLM) in parameter-efficient, quantized low-rank (QLoRA)\nfashion on consumer-grade hardware to improve review comment generation. Recent\nstudies demonstrate the efficacy of augmenting semantic metadata information\ninto prompts to boost performance in other code-related tasks. To explore this\nin code review activities, we also prompt proprietary, closed-source LLMs\naugmenting the input code patch with function call graphs and code summaries.\nBoth of our strategies improve the review comment generation performance, with\nfunction call graph augmented few-shot prompting on the GPT-3.5 model\nsurpassing the pretrained baseline by around 90% BLEU-4 score on the\nCodeReviewer dataset. Moreover, few-shot prompted Gemini-1.0 Pro, QLoRA\nfine-tuned Code Llama and Llama 3.1 models achieve competitive results (ranging\nfrom 25% to 83% performance improvement) on this task. An additional human\nevaluation study further validates our experimental findings, reflecting\nreal-world developers' perceptions of LLM-generated code review comments based\non relevant qualitative metrics.", "published": "2024-11-15 12:01:38", "link": "http://arxiv.org/abs/2411.10129v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Entropy and type-token ratio in gigaword corpora", "abstract": "There are different ways of measuring diversity in complex systems. In\nparticular, in language, lexical diversity is characterized in terms of the\ntype-token ratio and the word entropy. We here investigate both diversity\nmetrics in six massive linguistic datasets in English, Spanish, and Turkish,\nconsisting of books, news articles, and tweets. These gigaword corpora\ncorrespond to languages with distinct morphological features and differ in\nregisters and genres, thus constituting a varied testbed for a quantitative\napproach to lexical diversity. We unveil an empirical functional relation\nbetween entropy and type-token ratio of texts of a given corpus and language,\nwhich is a consequence of the statistical laws observed in natural language.\nFurther, in the limit of large text lengths we find an analytical expression\nfor this relation relying on both Zipf and Heaps laws that agrees with our\nempirical findings.", "published": "2024-11-15 14:40:59", "link": "http://arxiv.org/abs/2411.10227v2", "categories": ["cs.CL", "cs.IR", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "P$^2$ Law: Scaling Law for Post-Training After Model Pruning", "abstract": "Pruning has become a widely adopted technique for reducing the hardware\nrequirements of large language models (LLMs). To recover model performance\nafter pruning, post-training is commonly employed to mitigate the resulting\nperformance degradation. While post-training benefits from larger datasets,\nonce the dataset size is already substantial, increasing the training data\nprovides only limited performance gains. To balance post-training cost and\nmodel performance, it is necessary to explore the optimal amount of\npost-training data.Through extensive experiments on the Llama-3 and Qwen-2.5\nseries models, pruned using various common pruning methods, we uncover the\nscaling \\textbf{Law} for \\textbf{P}ost-training after model \\textbf{P}runing,\nreferred to as the P$^2$ Law.This law identifies four key factors for\npredicting the pruned model's post-training loss: the model size before\npruning, the number of post-training tokens, the pruning rate, and the model's\nloss before pruning. Moreover, P$^2$ Law can generalize to larger dataset\nsizes, larger model sizes, and higher pruning rates, offering valuable insights\nfor the post-training of pruned LLMs.", "published": "2024-11-15 15:28:42", "link": "http://arxiv.org/abs/2411.10272v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer\n  Use", "abstract": "The recently released model, Claude 3.5 Computer Use, stands out as the first\nfrontier AI model to offer computer use in public beta as a graphical user\ninterface (GUI) agent. As an early beta, its capability in the real-world\ncomplex environment remains unknown. In this case study to explore Claude 3.5\nComputer Use, we curate and organize a collection of carefully designed tasks\nspanning a variety of domains and software. Observations from these cases\ndemonstrate Claude 3.5 Computer Use's unprecedented ability in end-to-end\nlanguage to desktop actions. Along with this study, we provide an\nout-of-the-box agent framework for deploying API-based GUI automation models\nwith easy implementation. Our case studies aim to showcase a groundwork of\ncapabilities and limitations of Claude 3.5 Computer Use with detailed analyses\nand bring to the fore questions about planning, action, and critic, which must\nbe considered for future improvement. We hope this preliminary exploration will\ninspire future research into the GUI agent community. All the test cases in the\npaper can be tried through the project:\nhttps://github.com/showlab/computer_use_ootb.", "published": "2024-11-15 16:23:52", "link": "http://arxiv.org/abs/2411.10323v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Safe Text-to-Image Generation: Simply Sanitize the Prompt Embedding", "abstract": "In recent years, text-to-image (T2I) generation models have made significant\nprogress in generating high-quality images that align with text descriptions.\nHowever, these models also face the risk of unsafe generation, potentially\nproducing harmful content that violates usage policies, such as explicit\nmaterial. Existing safe generation methods typically focus on suppressing\ninappropriate content by erasing undesired concepts from visual\nrepresentations, while neglecting to sanitize the textual representation.\nAlthough these methods help mitigate the risk of misuse to certain extent,\ntheir robustness remains insufficient when dealing with adversarial attacks.\n  Given that semantic consistency between input text and output image is a\nfundamental requirement for T2I models, we identify that textual\nrepresentations (i.e., prompt embeddings) are likely the primary source of\nunsafe generation. To this end, we propose a vision-agnostic safe generation\nframework, Embedding Sanitizer (ES), which focuses on erasing inappropriate\nconcepts from prompt embeddings and uses the sanitized embeddings to guide the\nmodel for safe generation. ES is applied to the output of the text encoder as a\nplug-and-play module, enabling seamless integration with different T2I models\nas well as other safeguards. In addition, ES's unique scoring mechanism assigns\na score to each token in the prompt to indicate its potential harmfulness, and\ndynamically adjusts the sanitization intensity to balance defensive performance\nand generation quality. Through extensive evaluation on five prompt benchmarks,\nour approach achieves state-of-the-art robustness by sanitizing the source\n(prompt embedding) of unsafe generation compared to nine baseline methods. It\nsignificantly outperforms existing safeguards in terms of interpretability and\ncontrollability while maintaining generation quality.", "published": "2024-11-15 16:29:02", "link": "http://arxiv.org/abs/2411.10329v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Features that Make a Difference: Leveraging Gradients for Improved\n  Dictionary Learning", "abstract": "Sparse Autoencoders (SAEs) are a promising approach for extracting neural\nnetwork representations by learning a sparse and overcomplete decomposition of\nthe network's internal activations. However, SAEs are traditionally trained\nconsidering only activation values and not the effect those activations have on\ndownstream computations. This limits the information available to learn\nfeatures, and biases the autoencoder towards neglecting features which are\nrepresented with small activation values but strongly influence model outputs.\nTo address this, we introduce Gradient SAEs (g-SAEs), which modify the\n$k$-sparse autoencoder architecture by augmenting the TopK activation function\nto rely on the gradients of the input activation when selecting the $k$\nelements. For a given sparsity level, g-SAEs produce reconstructions that are\nmore faithful to original network performance when propagated through the\nnetwork. Additionally, we find evidence that g-SAEs learn latents that are on\naverage more effective at steering models in arbitrary contexts. By considering\nthe downstream effects of activations, our approach leverages the dual nature\nof neural network features as both $\\textit{representations}$, retrospectively,\nand $\\textit{actions}$, prospectively. While previous methods have approached\nthe problem of feature discovery primarily focused on the former aspect, g-SAEs\nrepresent a step towards accounting for the latter as well.", "published": "2024-11-15 18:03:52", "link": "http://arxiv.org/abs/2411.10397v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Mitigating Hallucination in Multimodal Large Language Model via\n  Hallucination-targeted Direct Preference Optimization", "abstract": "Multimodal Large Language Models (MLLMs) are known to hallucinate, which\nlimits their practical applications. Recent works have attempted to apply\nDirect Preference Optimization (DPO) to enhance the performance of MLLMs, but\nhave shown inconsistent improvements in mitigating hallucinations. To address\nthis issue more effectively, we introduce Hallucination-targeted Direct\nPreference Optimization (HDPO) to reduce hallucinations in MLLMs. Unlike\nprevious approaches, our method tackles hallucinations from their diverse forms\nand causes. Specifically, we develop three types of preference pair data\ntargeting the following causes of MLLM hallucinations: (1) insufficient visual\ncapabilities, (2) long context generation, and (3) multimodal conflicts.\nExperimental results demonstrate that our method achieves superior performance\nacross multiple hallucination evaluation datasets, surpassing most\nstate-of-the-art (SOTA) methods and highlighting the potential of our approach.\nAblation studies and in-depth analyses further confirm the effectiveness of our\nmethod and suggest the potential for further improvements through scaling up.", "published": "2024-11-15 18:56:01", "link": "http://arxiv.org/abs/2411.10436v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Everything is a Video: Unifying Modalities through Next-Frame Prediction", "abstract": "Multimodal learning, which involves integrating information from various\nmodalities such as text, images, audio, and video, is pivotal for numerous\ncomplex tasks like visual question answering, cross-modal retrieval, and\ncaption generation. Traditional approaches rely on modality-specific encoders\nand late fusion techniques, which can hinder scalability and flexibility when\nadapting to new tasks or modalities. To address these limitations, we introduce\na novel framework that extends the concept of task reformulation beyond natural\nlanguage processing (NLP) to multimodal learning. We propose to reformulate\ndiverse multimodal tasks into a unified next-frame prediction problem, allowing\na single model to handle different modalities without modality-specific\ncomponents. This method treats all inputs and outputs as sequential frames in a\nvideo, enabling seamless integration of modalities and effective knowledge\ntransfer across tasks. Our approach is evaluated on a range of tasks, including\ntext-to-text, image-to-text, video-to-video, video-to-text, and audio-to-text,\ndemonstrating the model's ability to generalize across modalities with minimal\nadaptation. We show that task reformulation can significantly simplify\nmultimodal model design across various tasks, laying the groundwork for more\ngeneralized multimodal foundation models.", "published": "2024-11-15 12:59:37", "link": "http://arxiv.org/abs/2411.10503v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Hysteresis Activation Function for Efficient Inference", "abstract": "The widely used ReLU is favored for its hardware efficiency, {as the\nimplementation at inference is a one bit sign case,} yet suffers from issues\nsuch as the ``dying ReLU'' problem, where during training, neurons fail to\nactivate and constantly remain at zero, as highlighted by Lu et al. Traditional\napproaches to mitigate this issue often introduce more complex and less\nhardware-friendly activation functions. In this work, we propose a Hysteresis\nRectified Linear Unit (HeLU), an efficient activation function designed to\naddress the ``dying ReLU'' problem with minimal complexity. Unlike traditional\nactivation functions with fixed thresholds for training and inference, HeLU\nemploys a variable threshold that refines the backpropagation. This refined\nmechanism allows simpler activation functions to achieve competitive\nperformance comparable to their more complex counterparts without introducing\nunnecessary complexity or requiring inductive biases. Empirical evaluations\ndemonstrate that HeLU enhances model generalization across diverse datasets,\noffering a promising solution for efficient and effective inference suitable\nfor a wide range of neural network architectures.", "published": "2024-11-15 20:46:58", "link": "http://arxiv.org/abs/2411.10573v2", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "An exploration of the effect of quantisation on energy consumption and\n  inference time of StarCoder2", "abstract": "This study examines quantisation and pruning strategies to reduce energy\nconsumption in code Large Language Models (LLMs) inference. Using StarCoder2,\nwe observe increased energy demands with quantization due to lower throughput\nand some accuracy losses. Conversely, pruning reduces energy usage but impairs\nperformance. The results highlight challenges and trade-offs in LLM model\ncompression. We suggest future work on hardware-optimized quantization to\nenhance efficiency with minimal loss in accuracy.", "published": "2024-11-15 21:28:19", "link": "http://arxiv.org/abs/2411.12758v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Can Artificial Intelligence Generate Quality Research Topics Reflecting\n  Patient Concerns?", "abstract": "Patient-centered research is increasingly important in narrowing the gap\nbetween research and patient care, yet incorporating patient perspectives into\nhealth research has been inconsistent. We propose an automated framework\nleveraging innovative natural language processing (NLP) and artificial\nintelligence (AI) with patient portal messages to generate research ideas that\nprioritize important patient issues. We further quantified the quality of\nAI-generated research topics. To define patient clinical concerns, we analyzed\n614,464 patient messages from 25,549 individuals with breast or skin cancer\nobtained from a large academic hospital (2013 to 2024), constructing a 2-staged\nunsupervised NLP topic model. Then, we generated research topics to resolve the\ndefined issues using a widely used AI (ChatGPT-4o, OpenAI Inc, April 2024\nversion) with prompt-engineering strategies. We guided AI to perform\nmulti-level tasks: 1) knowledge interpretation and summarization (e.g.,\ninterpreting and summarizing the NLP-defined topics), 2) knowledge generation\n(e.g., generating research ideas corresponding to patients issues), 3)\nself-reflection and correction (e.g., ensuring and revising the research ideas\nafter searching for scientific articles), and 4) self-reassurance (e.g.,\nconfirming and finalizing the research ideas). Six highly experienced breast\noncologists and dermatologists assessed the significance and novelty of\nAI-generated research topics using a 5-point Likert scale (1-exceptional,\n5-poor). One-third of the AI-suggested research topics were highly significant\nand novel when both scores were lower than the average. Two-thirds of the\nAI-suggested topics were novel in both cancers. Our findings demonstrate that\nAI-generated research topics reflecting patient perspectives via a large volume\nof patient messages can meaningfully guide future directions in\npatient-centered health research.", "published": "2024-11-15 20:24:38", "link": "http://arxiv.org/abs/2411.14456v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "WavChat: A Survey of Spoken Dialogue Models", "abstract": "Recent advancements in spoken dialogue models, exemplified by systems like\nGPT-4o, have captured significant attention in the speech domain. Compared to\ntraditional three-tier cascaded spoken dialogue models that comprise speech\nrecognition (ASR), large language models (LLMs), and text-to-speech (TTS),\nmodern spoken dialogue models exhibit greater intelligence. These advanced\nspoken dialogue models not only comprehend audio, music, and other\nspeech-related features, but also capture stylistic and timbral characteristics\nin speech. Moreover, they generate high-quality, multi-turn speech responses\nwith low latency, enabling real-time interaction through simultaneous listening\nand speaking capability. Despite the progress in spoken dialogue systems, there\nis a lack of comprehensive surveys that systematically organize and analyze\nthese systems and the underlying technologies. To address this, we have first\ncompiled existing spoken dialogue systems in the chronological order and\ncategorized them into the cascaded and end-to-end paradigms. We then provide an\nin-depth overview of the core technologies in spoken dialogue models, covering\naspects such as speech representation, training paradigm, streaming, duplex,\nand interaction capabilities. Each section discusses the limitations of these\ntechnologies and outlines considerations for future research. Additionally, we\npresent a thorough review of relevant datasets, evaluation metrics, and\nbenchmarks from the perspectives of training and evaluating spoken dialogue\nsystems. We hope this survey will contribute to advancing both academic\nresearch and industrial applications in the field of spoken dialogue systems.\nThe related material is available at https://github.com/jishengpeng/WavChat.", "published": "2024-11-15 04:16:45", "link": "http://arxiv.org/abs/2411.13577v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Perceptual implications of simplifying geometrical acoustics models for\n  Ambisonics-based binaural reverberation", "abstract": "Different methods can be employed to render virtual reverberation, often\nrequiring substantial information about the room's geometry and the acoustic\ncharacteristics of the surfaces. However, fully comprehensive approaches that\naccount for all aspects of a given environment may be computationally costly\nand redundant from a perceptual standpoint. For these methods, achieving a\ntrade-off between perceptual authenticity and model's complexity becomes a\nrelevant challenge.\n  This study investigates this compromise through the use of geometrical\nacoustics to render Ambisonics-based binaural reverberation. Its precision is\ndetermined, among other factors, by its fidelity to the room's geometry and to\nthe acoustic properties of its materials.\n  The purpose of this study is to investigate the impact of simplifying the\nroom geometry and the frequency resolution of absorption coefficients on the\nperception of reverberation within a virtual sound scene. Several decimated\nmodels based on a single room were perceptually evaluated using the a\nmulti-stimulus comparison method. Additionally, these differences were\nnumerically assessed through the calculation of acoustic parameters of the\nreverberation.\n  According to numerical and perceptual evaluations, lowering the frequency\nresolution of absorption coefficients can have a significant impact on the\nperception of reverberation, while a less notable impact was observed when\ndecimating the geometry of the model.", "published": "2024-11-15 17:30:05", "link": "http://arxiv.org/abs/2411.10375v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "XLSR-Mamba: A Dual-Column Bidirectional State Space Model for Spoofing\n  Attack Detection", "abstract": "Transformers and their variants have achieved great success in speech\nprocessing. However, their multi-head self-attention mechanism is\ncomputationally expensive. Therefore, one novel selective state space model,\nMamba, has been proposed as an alternative. Building on its success in\nautomatic speech recognition, we apply Mamba for spoofing attack detection.\nMamba is well-suited for this task as it can capture the artifacts in spoofed\nspeech signals by handling long-length sequences. However, Mamba's performance\nmay suffer when it is trained with limited labeled data. To mitigate this, we\npropose combining a new structure of Mamba based on a dual-column architecture\nwith self-supervised learning, using the pre-trained wav2vec 2.0 model. The\nexperiments show that our proposed approach achieves competitive results and\nfaster inference on the ASVspoof 2021 LA and DF datasets, and on the more\nchallenging In-the-Wild dataset, it emerges as the strongest candidate for\nspoofing attack detection. The code has been publicly released in\nhttps://github.com/swagshaw/XLSR-Mamba.", "published": "2024-11-15 08:13:51", "link": "http://arxiv.org/abs/2411.10027v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Zero-shot Voice Conversion with Diffusion Transformers", "abstract": "Zero-shot voice conversion aims to transform a source speech utterance to\nmatch the timbre of a reference speech from an unseen speaker. Traditional\napproaches struggle with timbre leakage, insufficient timbre representation,\nand mismatches between training and inference tasks. We propose Seed-VC, a\nnovel framework that addresses these issues by introducing an external timbre\nshifter during training to perturb the source speech timbre, mitigating leakage\nand aligning training with inference. Additionally, we employ a diffusion\ntransformer that leverages the entire reference speech context, capturing\nfine-grained timbre features through in-context learning. Experiments\ndemonstrate that Seed-VC outperforms strong baselines like OpenVoice and\nCosyVoice, achieving higher speaker similarity and lower word error rates in\nzero-shot voice conversion tasks. We further extend our approach to zero-shot\nsinging voice conversion by incorporating fundamental frequency (F0)\nconditioning, resulting in comparative performance to current state-of-the-art\nmethods. Our findings highlight the effectiveness of Seed-VC in overcoming core\nchallenges, paving the way for more accurate and versatile voice conversion\nsystems.", "published": "2024-11-15 04:43:44", "link": "http://arxiv.org/abs/2411.09943v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "EveGuard: Defeating Vibration-based Side-Channel Eavesdropping with\n  Audio Adversarial Perturbations", "abstract": "Vibrometry-based side channels pose a significant privacy risk, exploiting\nsensors like mmWave radars, light sensors, and accelerometers to detect\nvibrations from sound sources or proximate objects, enabling speech\neavesdropping. Despite various proposed defenses, these involve costly hardware\nsolutions with inherent physical limitations. This paper presents EveGuard, a\nsoftware-driven defense framework that creates adversarial audio, protecting\nvoice privacy from side channels without compromising human perception. We\nleverage the distinct sensing capabilities of side channels and traditional\nmicrophones, where side channels capture vibrations and microphones record\nchanges in air pressure, resulting in different frequency responses. EveGuard\nfirst proposes a perturbation generator model (PGM) that effectively suppresses\nsensor-based eavesdropping while maintaining high audio quality. Second, to\nenable end-to-end training of PGM, we introduce a new domain translation task\ncalled Eve-GAN for inferring an eavesdropped signal from a given audio. We\nfurther apply few-shot learning to mitigate the data collection overhead for\nEve-GAN training. Our extensive experiments show that EveGuard achieves a\nprotection rate of more than 97 percent from audio classifiers and\nsignificantly hinders eavesdropped audio reconstruction. We further validate\nthe performance of EveGuard across three adaptive attack mechanisms. We have\nconducted a user study to verify the perceptual quality of our perturbed audio.", "published": "2024-11-15 08:32:03", "link": "http://arxiv.org/abs/2411.10034v2", "categories": ["cs.CR", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
