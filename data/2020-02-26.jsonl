{"title": "Detecting Potential Topics In News Using BERT, CRF and Wikipedia", "abstract": "For a news content distribution platform like Dailyhunt, Named Entity\nRecognition is a pivotal task for building better user recommendation and\nnotification algorithms. Apart from identifying names, locations, organisations\nfrom the news for 13+ Indian languages and use them in algorithms, we also need\nto identify n-grams which do not necessarily fit in the definition of\nNamed-Entity, yet they are important. For example, \"me too movement\", \"beef\nban\", \"alwar mob lynching\". In this exercise, given an English language text,\nwe are trying to detect case-less n-grams which convey important information\nand can be used as topics and/or hashtags for a news. Model is built using\nWikipedia titles data, private English news corpus and BERT-Multilingual\npre-trained model, Bi-GRU and CRF architecture. It shows promising results when\ncompared with industry best Flair, Spacy and Stanford-caseless-NER in terms of\nF1 and especially Recall.", "published": "2020-02-26 10:48:53", "link": "http://arxiv.org/abs/2002.11402v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Marathi To English Neural Machine Translation With Near Perfect Corpus\n  And Transformers", "abstract": "There have been very few attempts to benchmark performances of\nstate-of-the-art algorithms for Neural Machine Translation task on Indian\nLanguages. Google, Bing, Facebook and Yandex are some of the very few companies\nwhich have built translation systems for few of the Indian Languages. Among\nthem, translation results from Google are supposed to be better, based on\ngeneral inspection. Bing-Translator do not even support Marathi language which\nhas around 95 million speakers and ranks 15th in the world in terms of combined\nprimary and secondary speakers. In this exercise, we trained and compared\nvariety of Neural Machine Marathi to English Translators trained with\nBERT-tokenizer by huggingface and various Transformer based architectures using\nFacebook's Fairseq platform with limited but almost correct parallel corpus to\nachieve better BLEU scores than Google on Tatoeba and Wikimedia open datasets.", "published": "2020-02-26 17:18:49", "link": "http://arxiv.org/abs/2002.11643v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sparse Sinkhorn Attention", "abstract": "We propose Sparse Sinkhorn Attention, a new efficient and sparse method for\nlearning to attend. Our method is based on differentiable sorting of internal\nrepresentations. Concretely, we introduce a meta sorting network that learns to\ngenerate latent permutations over sequences. Given sorted sequences, we are\nthen able to compute quasi-global attention with only local windows, improving\nthe memory efficiency of the attention module. To this end, we propose new\nalgorithmic innovations such as Causal Sinkhorn Balancing and SortCut, a\ndynamic sequence truncation method for tailoring Sinkhorn Attention for\nencoding and/or decoding purposes. Via extensive experiments on algorithmic\nseq2seq sorting, language modeling, pixel-wise image generation, document\nclassification and natural language inference, we demonstrate that our memory\nefficient Sinkhorn Attention method is competitive with vanilla attention and\nconsistently outperforms recently proposed efficient Transformer models such as\nSparse Transformers.", "published": "2020-02-26 04:18:01", "link": "http://arxiv.org/abs/2002.11296v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Object Relational Graph with Teacher-Recommended Learning for Video\n  Captioning", "abstract": "Taking full advantage of the information from both vision and language is\ncritical for the video captioning task. Existing models lack adequate visual\nrepresentation due to the neglect of interaction between object, and sufficient\ntraining for content-related words due to long-tailed problems. In this paper,\nwe propose a complete video captioning system including both a novel model and\nan effective training strategy. Specifically, we propose an object relational\ngraph (ORG) based encoder, which captures more detailed interaction features to\nenrich visual representation. Meanwhile, we design a teacher-recommended\nlearning (TRL) method to make full use of the successful external language\nmodel (ELM) to integrate the abundant linguistic knowledge into the caption\nmodel. The ELM generates more semantically similar word proposals which extend\nthe ground-truth words used for training to deal with the long-tailed problem.\nExperimental evaluations on three benchmarks: MSVD, MSR-VTT and VATEX show the\nproposed ORG-TRL system achieves state-of-the-art performance. Extensive\nablation studies and visualizations illustrate the effectiveness of our system.", "published": "2020-02-26 15:34:52", "link": "http://arxiv.org/abs/2002.11566v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Train Large, Then Compress: Rethinking Model Size for Efficient Training\n  and Inference of Transformers", "abstract": "Since hardware resources are limited, the objective of training deep learning\nmodels is typically to maximize accuracy subject to the time and memory\nconstraints of training and inference. We study the impact of model size in\nthis setting, focusing on Transformer models for NLP tasks that are limited by\ncompute: self-supervised pretraining and high-resource machine translation. We\nfirst show that even though smaller Transformer models execute faster per\niteration, wider and deeper models converge in significantly fewer steps.\nMoreover, this acceleration in convergence typically outpaces the additional\ncomputational overhead of using larger models. Therefore, the most\ncompute-efficient training strategy is to counterintuitively train extremely\nlarge models but stop after a small number of iterations.\n  This leads to an apparent trade-off between the training efficiency of large\nTransformer models and the inference efficiency of small Transformer models.\nHowever, we show that large models are more robust to compression techniques\nsuch as quantization and pruning than small models. Consequently, one can get\nthe best of both worlds: heavily compressed, large models achieve higher\naccuracy than lightly compressed, small models.", "published": "2020-02-26 21:17:13", "link": "http://arxiv.org/abs/2002.11794v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-task Learning with Multi-head Attention for Multi-choice Reading\n  Comprehension", "abstract": "Multiple-choice Machine Reading Comprehension (MRC) is an important and\nchallenging Natural Language Understanding (NLU) task, in which a machine must\nchoose the answer to a question from a set of choices, with the question placed\nin context of text passages or dialog. In the last a couple of years the NLU\nfield has been revolutionized with the advent of models based on the\nTransformer architecture, which are pretrained on massive amounts of\nunsupervised data and then fine-tuned for various supervised learning NLU\ntasks. Transformer models have come to dominate a wide variety of leader-boards\nin the NLU field; in the area of MRC, the current state-of-the-art model on the\nDREAM dataset (see[Sunet al., 2019]) fine tunes Albert, a large pretrained\nTransformer-based model, and addition-ally combines it with an extra layer of\nmulti-head attention between context and question-answer[Zhuet al., 2020].The\npurpose of this note is to document a new state-of-the-art result in the DREAM\ntask, which is accomplished by, additionally, performing multi-task learning on\ntwo MRC multi-choice reading comprehension tasks (RACE and DREAM).", "published": "2020-02-26 16:32:25", "link": "http://arxiv.org/abs/2003.04992v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Density Ratio Approach to Language Model Fusion in End-To-End\n  Automatic Speech Recognition", "abstract": "This article describes a density ratio approach to integrating external\nLanguage Models (LMs) into end-to-end models for Automatic Speech Recognition\n(ASR). Applied to a Recurrent Neural Network Transducer (RNN-T) ASR model\ntrained on a given domain, a matched in-domain RNN-LM, and a target domain\nRNN-LM, the proposed method uses Bayes' Rule to define RNN-T posteriors for the\ntarget domain, in a manner directly analogous to the classic hybrid model for\nASR based on Deep Neural Networks (DNNs) or LSTMs in the Hidden Markov Model\n(HMM) framework (Bourlard & Morgan, 1994). The proposed approach is evaluated\nin cross-domain and limited-data scenarios, for which a significant amount of\ntarget domain text data is used for LM training, but only limited (or no)\n{audio, transcript} training data pairs are used to train the RNN-T.\nSpecifically, an RNN-T model trained on paired audio & transcript data from\nYouTube is evaluated for its ability to generalize to Voice Search data. The\nDensity Ratio method was found to consistently outperform the dominant approach\nto LM and end-to-end ASR integration, Shallow Fusion.", "published": "2020-02-26 02:53:42", "link": "http://arxiv.org/abs/2002.11268v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Zero-shot Learning for Automatic Phonemic Transcription", "abstract": "Automatic phonemic transcription tools are useful for low-resource language\ndocumentation. However, due to the lack of training sets, only a tiny fraction\nof languages have phonemic transcription tools. Fortunately, multilingual\nacoustic modeling provides a solution given limited audio training data. A more\nchallenging problem is to build phonemic transcribers for languages with zero\ntraining data. The difficulty of this task is that phoneme inventories often\ndiffer between the training languages and the target language, making it\ninfeasible to recognize unseen phonemes. In this work, we address this problem\nby adopting the idea of zero-shot learning. Our model is able to recognize\nunseen phonemes in the target language without any training data. In our model,\nwe decompose phonemes into corresponding articulatory attributes such as vowel\nand consonant. Instead of predicting phonemes directly, we first predict\ndistributions over articulatory attributes, and then compute phoneme\ndistributions with a customized acoustic model. We evaluate our model by\ntraining it using 13 languages and testing it using 7 unseen languages. We find\nthat it achieves 7.7% better phoneme error rate on average over a standard\nmultilingual model.", "published": "2020-02-26 20:38:42", "link": "http://arxiv.org/abs/2002.11781v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Universal Phone Recognition with a Multilingual Allophone System", "abstract": "Multilingual models can improve language processing, particularly for low\nresource situations, by sharing parameters across languages. Multilingual\nacoustic models, however, generally ignore the difference between phonemes\n(sounds that can support lexical contrasts in a particular language) and their\ncorresponding phones (the sounds that are actually spoken, which are language\nindependent). This can lead to performance degradation when combining a variety\nof training languages, as identically annotated phonemes can actually\ncorrespond to several different underlying phonetic realizations. In this work,\nwe propose a joint model of both language-independent phone and\nlanguage-dependent phoneme distributions. In multilingual ASR experiments over\n11 languages, we find that this model improves testing performance by 2%\nphoneme error rate absolute in low-resource conditions. Additionally, because\nwe are explicitly modeling language-independent phones, we can build a\n(nearly-)universal phone recognizer that, when combined with the PHOIBLE large,\nmanually curated database of phone inventories, can be customized into 2,000\nlanguage dependent recognizers. Experiments on two low-resourced indigenous\nlanguages, Inuktitut and Tusom, show that our recognizer achieves phone\naccuracy improvements of more than 17%, moving a step closer to speech\nrecognition for all languages in the world.", "published": "2020-02-26 21:28:57", "link": "http://arxiv.org/abs/2002.11800v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Personalized Taste and Cuisine Preference Modeling via Images", "abstract": "With the exponential growth in the usage of social media to share live\nupdates about life, taking pictures has become an unavoidable phenomenon.\nIndividuals unknowingly create a unique knowledge base with these images. The\nfood images, in particular, are of interest as they contain a plethora of\ninformation. From the image metadata and using computer vision tools, we can\nextract distinct insights for each user to build a personal profile. Using the\nunderlying connection between cuisines and their inherent tastes, we attempt to\ndevelop such a profile for an individual based solely on the images of his\nfood. Our study provides insights about an individual's inclination towards\nparticular cuisines. Interpreting these insights can lead to the development of\na more precise recommendation system. Such a system would avoid the generic\napproach in favor of a personalized recommendation system.", "published": "2020-02-26 01:07:56", "link": "http://arxiv.org/abs/2003.08769v1", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Lightweight Online Separation of the Sound Source of Interest through\n  BLSTM-Based Binary Masking", "abstract": "Online audio source separation has been an important part of auditory scene\nanalysis and robot audition. The main type of technique to carry this out,\nbecause of its online capabilities, has been spatial filtering (or\nbeamforming), where it is assumed that the location (mainly, the direction of\narrival; DOA) of the source of interest (SOI) is known. However, these\ntechniques suffer from considerable interference leakage in the final result.\nIn this paper, we propose a two step technique: 1) a phase-based beamformer\nthat provides, in addition to the estimation of the SOI, an estimation of the\ncumulative environmental interference; and 2) a BLSTM-based TF binary masking\nstage that calculates a binary mask that aims to separate the SOI from the\ncumulative environmental interference. In our tests, this technique provides a\nsignal-to-interference ratio (SIR) above 20 dB with simulated data. Because of\nthe nature of the beamformer outputs, the label permutation problem is handled\nfrom the beginning. This makes the proposed solution a lightweight alternative\nthat requires considerably less computational resources (almost an order of\nmagnitude) compared to current deep-learning based techniques, while providing\na comparable SIR performance.", "published": "2020-02-26 00:53:04", "link": "http://arxiv.org/abs/2002.11241v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Multitask Learning and Multistage Fusion for Dimensional Audiovisual\n  Emotion Recognition", "abstract": "Due to its ability to accurately predict emotional state using multimodal\nfeatures, audiovisual emotion recognition has recently gained more interest\nfrom researchers. This paper proposes two methods to predict emotional\nattributes from audio and visual data using a multitask learning and a fusion\nstrategy. First, multitask learning is employed by adjusting three parameters\nfor each attribute to improve the recognition rate. Second, a multistage fusion\nis proposed to combine results from various modalities' final prediction. Our\napproach used multitask learning, employed at unimodal and early fusion\nmethods, shows improvement over single-task learning with an average CCC score\nof 0.431 compared to 0.297. A multistage method, employed at the late fusion\napproach, significantly improved the agreement score between true and predicted\nvalues on the development set of data (from [0.537, 0.565, 0.083] to [0.68,\n0.656, 0.443]) for arousal, valence, and liking.", "published": "2020-02-26 06:06:00", "link": "http://arxiv.org/abs/2002.11312v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "BUT System for the Second DIHARD Speech Diarization Challenge", "abstract": "This paper describes the winning systems developed by the BUT team for the\nfour tracks of the Second DIHARD Speech Diarization Challenge. For tracks 1 and\n2 the systems were mainly based on performing agglomerative hierarchical\nclustering (AHC) of x-vectors, followed by another x-vector clustering based on\nBayes hidden Markov model and variational Bayes inference. We provide a\ncomparison of the improvement given by each step and share the implementation\nof the core of the system. For tracks 3 and 4 with recordings from the Fifth\nCHiME Challenge, we explored different approaches for doing multi-channel\ndiarization and our best performance was obtained when applying AHC on the\nfusion of per channel probabilistic linear discriminant analysis scores.", "published": "2020-02-26 08:41:15", "link": "http://arxiv.org/abs/2002.11356v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Multi-Modal Continuous Valence And Arousal Prediction in the Wild Using\n  Deep 3D Features and Sequence Modeling", "abstract": "Continuous affect prediction in the wild is a very interesting problem and is\nchallenging as continuous prediction involves heavy computation. This paper\npresents the methodologies and techniques used in our contribution to predict\ncontinuous emotion dimensions i.e., valence and arousal in ABAW competition on\nAff-Wild2 database. Aff-Wild2 database consists of videos in the wild labelled\nfor valence and arousal at frame level. Our proposed methodology uses fusion of\nboth audio and video features (multi-modal) extracted using state-of-the-art\nmethods. These audio-video features are used to train a sequence-to-sequence\nmodel that is based on Gated Recurrent Units (GRU). We show promising results\non validation data with simple architecture. The overall valence and arousal of\nthe proposed approach is 0.22 and 0.34, which is better than the competition\nbaseline of 0.14 and 0.24 respectively.", "published": "2020-02-26 06:58:51", "link": "http://arxiv.org/abs/2002.12766v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Expression Recognition in the Wild Using Sequence Modeling", "abstract": "As we exceed upon the procedures for modelling the different aspects of\nbehaviour, expression recognition has become a key field of research in Human\nComputer Interactions. Expression recognition in the wild is a very interesting\nproblem and is challenging as it involves detailed feature extraction and heavy\ncomputation. This paper presents the methodologies and techniques used in our\ncontribution to recognize different expressions i.e., neutral, anger, disgust,\nfear, happiness, sadness, surprise in ABAW competition on Aff-Wild2 database.\nAff-Wild2 database consists of videos in the wild labelled for seven different\nexpressions at frame level. We used a bi-modal approach by fusing audio and\nvisual features and train a sequence-to-sequence model that is based on Gated\nRecurrent Units (GRU) and Long Short Term Memory (LSTM) network. We show\nexperimental results on validation data. The overall accuracy of the proposed\napproach is 41.5 \\%, which is better than the competition baseline of 37\\%.", "published": "2020-02-26 07:03:06", "link": "http://arxiv.org/abs/2003.00170v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Open-set Recognition and Few-Shot Learning Dataset for Audio Event\n  Classification in Domestic Environments", "abstract": "The problem of training with a small set of positive samples is known as\nfew-shot learning (FSL). It is widely known that traditional deep learning (DL)\nalgorithms usually show very good performance when trained with large datasets.\nHowever, in many applications, it is not possible to obtain such a high number\nof samples. In the image domain, typical FSL applications include those related\nto face recognition. In the audio domain, music fraud or speaker recognition\ncan be clearly benefited from FSL methods. This paper deals with the\napplication of FSL to the detection of specific and intentional acoustic events\ngiven by different types of sound alarms, such as door bells or fire alarms,\nusing a limited number of samples. These sounds typically occur in domestic\nenvironments where many events corresponding to a wide variety of sound classes\ntake place. Therefore, the detection of such alarms in a practical scenario can\nbe considered an open-set recognition (OSR) problem. To address the lack of a\ndedicated public dataset for audio FSL, researchers usually make modifications\non other available datasets. This paper is aimed at poviding the audio\nrecognition community with a carefully annotated dataset\n(https://zenodo.org/record/3689288) for FSL in an OSR context comprised of 1360\nclips from 34 classes divided into pattern sounds} and unwanted sounds. To\nfacilitate and promote research on this area, results with state-of-the-art\nbaseline systems based on transfer learning are also presented.", "published": "2020-02-26 15:26:45", "link": "http://arxiv.org/abs/2002.11561v8", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
