{"title": "Loss-Versus-Fair: Efficiency of Dutch Auctions on Blockchains", "abstract": "Milionis et al.(2023) studied the rate at which automated market makers leak\nvalue to arbitrageurs when block times are discrete and follow a Poisson\nprocess, and where the risky asset price follows a geometric Brownian motion.\nWe extend their model to analyze another popular mechanism in decentralized\nfinance for onchain trading: Dutch auctions. We compute the expected losses\nthat a seller incurs to arbitrageurs and expected time-to-fill for Dutch\nauctions as a function of starting price, volatility, decay rate, and average\ninterblock time. We also extend the analysis to gradual Dutch auctions, a\nvariation on Dutch auctions for selling tokens over time at a continuous rate.\nWe use these models to explore the tradeoff between speed of execution and\nquality of execution, which could help inform practitioners in setting\nparameters for starting price and decay rate on Dutch auctions, or help\nplatform designers determine performance parameters like block times.", "published": "2024-05-31 18:04:54", "link": "http://arxiv.org/abs/2406.00113v2", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "DAFNet: Dynamic Auxiliary Fusion for Sequential Model Editing in Large\n  Language Models", "abstract": "Recently, while large language models (LLMs) have demonstrated impressive\nresults, they still suffer from hallucination, i.e., the generation of false\ninformation. Model editing is the task of fixing factual mistakes in LLMs; yet,\nmost previous works treat it as a one-time task, paying little attention to\never-emerging mistakes generated by LLMs. We address the task of sequential\nmodel editing (SME) that aims to rectify mistakes continuously. A Dynamic\nAuxiliary Fusion Network (DAFNet) is designed to enhance the semantic\ninteraction among the factual knowledge within the entire sequence, preventing\ncatastrophic forgetting during the editing process of multiple knowledge\ntriples. Specifically, (1) for semantic fusion within a relation triple, we\naggregate the intra-editing attention flow into auto-regressive self-attention\nwith token-level granularity in LLMs. We further leverage multi-layer diagonal\ninter-editing attention flow to update the weighted representations of the\nentire sequence-level granularity. (2) Considering that auxiliary parameters\nare required to store the knowledge for sequential editing, we construct a new\ndataset named \\textbf{DAFSet}, fulfilling recent, popular, long-tail and robust\nproperties to enhance the generality of sequential editing. Experiments show\nDAFNet significantly outperforms strong baselines in single-turn and sequential\nediting. The usage of DAFSet also consistently improves the performance of\nother auxiliary network-based methods in various scenarios", "published": "2024-05-31 02:56:49", "link": "http://arxiv.org/abs/2405.20588v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying while Learning for Document Event Causality Identification", "abstract": "Event Causality Identification (ECI) aims to detect whether there exists a\ncausal relation between two events in a document. Existing studies adopt a kind\nof identifying after learning paradigm, where events' representations are first\nlearned and then used for the identification. Furthermore, they mainly focus on\nthe causality existence, but ignoring causal direction. In this paper, we take\ncare of the causal direction and propose a new identifying while learning mode\nfor the ECI task. We argue that a few causal relations can be easily identified\nwith high confidence, and the directionality and structure of these identified\ncausalities can be utilized to update events' representations for boosting next\nround of causality identification. To this end, this paper designs an\n*iterative learning and identifying framework*: In each iteration, we construct\nan event causality graph, on which events' causal structure representations are\nupdated for boosting causal identification. Experiments on two public datasets\nshow that our approach outperforms the state-of-the-art algorithms in both\nevaluations for causality existence identification and direction\nidentification.", "published": "2024-05-31 03:48:00", "link": "http://arxiv.org/abs/2405.20608v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FineRadScore: A Radiology Report Line-by-Line Evaluation Technique\n  Generating Corrections with Severity Scores", "abstract": "The current gold standard for evaluating generated chest x-ray (CXR) reports\nis through radiologist annotations. However, this process can be extremely\ntime-consuming and costly, especially when evaluating large numbers of reports.\nIn this work, we present FineRadScore, a Large Language Model (LLM)-based\nautomated evaluation metric for generated CXR reports. Given a candidate report\nand a ground-truth report, FineRadScore gives the minimum number of\nline-by-line corrections required to go from the candidate to the ground-truth\nreport. Additionally, FineRadScore provides an error severity rating with each\ncorrection and generates comments explaining why the correction was needed. We\ndemonstrate that FineRadScore's corrections and error severity scores align\nwith radiologist opinions. We also show that, when used to judge the quality of\nthe report as a whole, FineRadScore aligns with radiologists as well as current\nstate-of-the-art automated CXR evaluation metrics. Finally, we analyze\nFineRadScore's shortcomings to provide suggestions for future improvements.", "published": "2024-05-31 04:05:09", "link": "http://arxiv.org/abs/2405.20613v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DORY: Deliberative Prompt Recovery for LLM", "abstract": "Prompt recovery in large language models (LLMs) is crucial for understanding\nhow LLMs work and addressing concerns regarding privacy, copyright, etc. The\ntrend towards inference-only APIs complicates this task by restricting access\nto essential outputs for recovery. To tackle this challenge, we extract\nprompt-related information from limited outputs and identify a strong(negative)\ncorrelation between output probability-based uncertainty and the success of\nprompt recovery. This finding led to the development of Deliberative PrOmpt\nRecoverY (DORY), our novel approach that leverages uncertainty to recover\nprompts accurately. DORY involves reconstructing drafts from outputs, refining\nthese with hints, and filtering out noise based on uncertainty. Our evaluation\nacross diverse LLMs and prompt benchmarks shows that DORY outperforms existing\nbaselines, improving performance by approximately 10.82% and establishing a new\nstate-of-the-art record in prompt recovery tasks. Significantly, DORY operates\nusing a single LLM without any external resources or model, offering a\ncost-effective, user-friendly prompt recovery solution.", "published": "2024-05-31 07:51:16", "link": "http://arxiv.org/abs/2405.20657v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "It is Simple Sometimes: A Study On Improving Aspect-Based Sentiment\n  Analysis Performance", "abstract": "Aspect-Based Sentiment Analysis (ABSA) involves extracting opinions from\ntextual data about specific entities and their corresponding aspects through\nvarious complementary subtasks. Several prior research has focused on\ndeveloping ad hoc designs of varying complexities for these subtasks. In this\npaper, we present a generative framework extensible to any ABSA subtask. We\nbuild upon the instruction tuned model proposed by Scaria et al. (2023), who\npresent an instruction-based model with task descriptions followed by\nin-context examples on ABSA subtasks. We propose PFInstruct, an extension to\nthis instruction learning paradigm by appending an NLP-related task prefix to\nthe task description. This simple approach leads to improved performance across\nall tested SemEval subtasks, surpassing previous state-of-the-art (SOTA) on the\nATE subtask (Rest14) by +3.28 F1-score, and on the AOOE subtask by an average\nof +5.43 F1-score across SemEval datasets. Furthermore, we explore the impact\nof the prefix-enhanced prompt quality on the ABSA subtasks and find that even a\nnoisy prefix enhances model performance compared to the baseline. Our method\nalso achieves competitive results on a biomedical domain dataset (ERSA).", "published": "2024-05-31 08:57:09", "link": "http://arxiv.org/abs/2405.20703v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving code-mixed hate detection by native sample mixing: A case\n  study for Hindi-English code-mixed scenario", "abstract": "Hate detection has long been a challenging task for the NLP community. The\ntask becomes complex in a code-mixed environment because the models must\nunderstand the context and the hate expressed through language alteration.\nCompared to the monolingual setup, we see much less work on code-mixed hate as\nlarge-scale annotated hate corpora are unavailable for the study. To overcome\nthis bottleneck, we propose using native language hate samples (native language\nsamples/ native samples hereafter). We hypothesise that in the era of\nmultilingual language models (MLMs), hate in code-mixed settings can be\ndetected by majorly relying on the native language samples. Even though the NLP\nliterature reports the effectiveness of MLMs on hate detection in many\ncross-lingual settings, their extensive evaluation in a code-mixed scenario is\nyet to be done. This paper attempts to fill this gap through rigorous empirical\nexperiments. We considered the Hindi-English code-mixed setup as a case study\nas we have the linguistic expertise for the same. Some of the interesting\nobservations we got are: (i) adding native hate samples in the code-mixed\ntraining set, even in small quantity, improved the performance of MLMs for\ncode-mixed hate detection, (ii) MLMs trained with native samples alone observed\nto be detecting code-mixed hate to a large extent, (iii) the visualisation of\nattention scores revealed that, when native samples were included in training,\nMLMs could better focus on the hate emitting words in the code-mixed context,\nand (iv) finally, when hate is subjective or sarcastic, naively mixing native\nsamples doesn't help much to detect code-mixed hate. We will release the data\nand code repository to reproduce the reported results.", "published": "2024-05-31 11:43:31", "link": "http://arxiv.org/abs/2405.20755v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Text Style Transfer: Datasets & Models for Indian Languages", "abstract": "Text style transfer (TST) involves altering the linguistic style of a text\nwhile preserving its core content. This paper focuses on sentiment transfer, a\npopular TST subtask, across a spectrum of Indian languages: Hindi, Magahi,\nMalayalam, Marathi, Punjabi, Odia, Telugu, and Urdu, expanding upon previous\nwork on English-Bangla sentiment transfer (Mukherjee et al., 2023). We\nintroduce dedicated datasets of 1,000 positive and 1,000 negative\nstyle-parallel sentences for each of these eight languages. We then evaluate\nthe performance of various benchmark models categorized into parallel,\nnon-parallel, cross-lingual, and shared learning approaches, including the\nLlama2 and GPT-3.5 large language models (LLMs). Our experiments highlight the\nsignificance of parallel data in TST and demonstrate the effectiveness of the\nMasked Style Filling (MSF) approach (Mukherjee et al., 2023) in non-parallel\ntechniques. Moreover, cross-lingual and joint multilingual learning methods\nshow promise, offering insights into selecting optimal models tailored to the\nspecific language and task requirements. To the best of our knowledge, this\nwork represents the first comprehensive exploration of the TST task as\nsentiment transfer across a diverse set of languages.", "published": "2024-05-31 14:05:27", "link": "http://arxiv.org/abs/2405.20805v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "That's Optional: A Contemporary Exploration of \"that\" Omission in\n  English Subordinate Clauses", "abstract": "The Uniform Information Density (UID) hypothesis posits that speakers\noptimize the communicative properties of their utterances by avoiding spikes in\ninformation, thereby maintaining a relatively uniform information profile over\ntime. This paper investigates the impact of UID principles on syntactic\nreduction, specifically focusing on the optional omission of the connector\n\"that\" in English subordinate clauses. Building upon previous research, we\nextend our investigation to a larger corpus of written English, utilize\ncontemporary large language models (LLMs) and extend the information-uniformity\nprinciples by the notion of entropy, to estimate the UID manifestations in the\nusecase of syntactic reduction choices.", "published": "2024-05-31 14:23:30", "link": "http://arxiv.org/abs/2405.20833v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Reward Models with Synthetic Critiques", "abstract": "Reward models (RMs) play a critical role in aligning language models through\nthe process of reinforcement learning from human feedback. RMs are trained to\npredict a score reflecting human preference, which requires significant time\nand cost for human annotation. Additionally, RMs tend to quickly overfit on\nsuperficial features in the training set, hindering their generalization\nperformance on unseen distributions. We propose a novel approach using\nsynthetic natural language critiques generated by large language models to\nprovide additional feedback, evaluating aspects such as instruction following,\ncorrectness, and style. This offers richer signals and more robust features for\nRMs to assess and score on. We demonstrate that high-quality critiques improve\nthe performance and data efficiency of RMs initialized from different\npretrained models, reducing the reliance on costly human annotations.\nFurthermore, incorporating critiques improves both the interpretability and\nrobustness of RM training.", "published": "2024-05-31 14:33:07", "link": "http://arxiv.org/abs/2405.20850v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Spoken Language Understanding via Multi-level Multi-grained\n  Contrastive Learning", "abstract": "Spoken language understanding (SLU) is a core task in task-oriented dialogue\nsystems, which aims at understanding the user's current goal through\nconstructing semantic frames. SLU usually consists of two subtasks, including\nintent detection and slot filling. Although there are some SLU frameworks joint\nmodeling the two subtasks and achieving high performance, most of them still\noverlook the inherent relationships between intents and slots and fail to\nachieve mutual guidance between the two subtasks. To solve the problem, we\npropose a multi-level multi-grained SLU framework MMCL to apply contrastive\nlearning at three levels, including utterance level, slot level, and word level\nto enable intent and slot to mutually guide each other. For the utterance\nlevel, our framework implements coarse granularity contrastive learning and\nfine granularity contrastive learning simultaneously. Besides, we also apply\nthe self-distillation method to improve the robustness of the model.\nExperimental results and further analysis demonstrate that our proposed model\nachieves new state-of-the-art results on two public multi-intent SLU datasets,\nobtaining a 2.6 overall accuracy improvement on the MixATIS dataset compared to\nprevious best models.", "published": "2024-05-31 14:34:23", "link": "http://arxiv.org/abs/2405.20852v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A comparison of correspondence analysis with PMI-based word embedding\n  methods", "abstract": "Popular word embedding methods such as GloVe and Word2Vec are related to the\nfactorization of the pointwise mutual information (PMI) matrix. In this paper,\nwe link correspondence analysis (CA) to the factorization of the PMI matrix. CA\nis a dimensionality reduction method that uses singular value decomposition\n(SVD), and we show that CA is mathematically close to the weighted\nfactorization of the PMI matrix. In addition, we present variants of CA that\nturn out to be successful in the factorization of the word-context matrix, i.e.\nCA applied to a matrix where the entries undergo a square-root transformation\n(ROOT-CA) and a root-root transformation (ROOTROOT-CA). While this study\nfocuses on traditional static word embedding methods, to extend the\ncontribution of this paper, we also include a comparison of transformer-based\nencoder BERT, i.e. contextual word embedding, with these traditional methods.\nAn empirical comparison among CA- and PMI-based methods as well as BERT shows\nthat overall results of ROOT-CA and ROOTROOT-CA are slightly better than those\nof the PMI-based methods and are competitive with BERT.", "published": "2024-05-31 15:04:15", "link": "http://arxiv.org/abs/2405.20895v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Superlatives in Context: Modeling the Implicit Semantics of Superlatives", "abstract": "Superlatives are used to single out elements with a maximal/minimal property.\nSemantically, superlatives perform a set comparison: something (or some things)\nhas the min/max property out of a set. As such, superlatives provide an ideal\nphenomenon for studying implicit phenomena and discourse restrictions. While\nthis comparison set is often not explicitly defined, its (implicit)\nrestrictions can be inferred from the discourse context the expression appears\nin. In this work we provide an extensive computational study on the semantics\nof superlatives. We propose a unified account of superlative semantics which\nallows us to derive a broad-coverage annotation schema. Using this unified\nschema we annotated a multi-domain dataset of superlatives and their semantic\ninterpretations. We specifically focus on interpreting implicit or ambiguous\nsuperlative expressions, by analyzing how the discourse context restricts the\nset of interpretations. In a set of experiments we then analyze how well models\nperform at variations of predicting superlative semantics, with and without\ncontext. We show that the fine-grained semantics of superlatives in context can\nbe challenging for contemporary models, including GPT-4.", "published": "2024-05-31 16:14:06", "link": "http://arxiv.org/abs/2405.20967v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the referential capacity of language models: An internalist rejoinder\n  to Mandelkern & Linzen", "abstract": "In a recent paper, Mandelkern & Linzen (2024) - henceforth M&L - address the\nquestion of whether language models' (LMs) words refer. Their argument draws\nfrom the externalist tradition in philosophical semantics, which views\nreference as the capacity of words to \"achieve 'word-to-world' connections\". In\nthe externalist framework, causally uninterrupted chains of usage, tracing\nevery occurrence of a name back to its bearer, guarantee that, for example,\n'Peano' refers to the individual Peano (Kripke 1980). This account is\nexternalist both because words pick out referents 'out there' in the world, and\nbecause what determines reference are coordinated linguistic actions by members\nof a community, and not individual mental states. The \"central question to\nask\", for M&L, is whether LMs too belong to human linguistic communities, such\nthat words by LMs may also trace back causally to their bearers. Their answer\nis a cautious \"yes\": inputs to LMs are linguistic \"forms with particular\nhistories of referential use\"; \"those histories ground the referents of those\nforms\"; any occurrence of 'Peano' in LM outputs is as causally connected to the\nindividual Peano as any other occurrence of the same proper name in human\nspeech or text; therefore, occurrences of 'Peano' in LM outputs refer to Peano.\nIn this commentary, we first qualify M&L's claim as applying to a narrow class\nof natural language expressions. Thus qualified, their claim is valid, and we\nemphasise an additional motivation for that in Section 2. Next, we discuss the\nactual scope of their claim, and we suggest that the way they formulate it may\nlead to unwarranted generalisations about reference in LMs. Our critique is\nlikewise applicable to other externalist accounts of LMs (e.g., Lederman &\nMahowald 2024; Mollo & Milliere 2023). Lastly, we conclude with a comment on\nthe status of LMs as members of human linguistic communities.", "published": "2024-05-31 19:43:20", "link": "http://arxiv.org/abs/2406.00159v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Re3: A Holistic Framework and Dataset for Modeling Collaborative\n  Document Revision", "abstract": "Collaborative review and revision of textual documents is the core of\nknowledge work and a promising target for empirical analysis and NLP\nassistance. Yet, a holistic framework that would allow modeling complex\nrelationships between document revisions, reviews and author responses is\nlacking. To address this gap, we introduce Re3, a framework for joint analysis\nof collaborative document revision. We instantiate this framework in the\nscholarly domain, and present Re3-Sci, a large corpus of aligned scientific\npaper revisions manually labeled according to their action and intent, and\nsupplemented with the respective peer reviews and human-written edit summaries.\nWe use the new data to provide first empirical insights into collaborative\ndocument revision in the academic domain, and to assess the capabilities of\nstate-of-the-art LLMs at automating edit analysis and facilitating text-based\ncollaboration. We make our annotation environment and protocols, the resulting\ndata and experimental code publicly available.", "published": "2024-05-31 21:19:09", "link": "http://arxiv.org/abs/2406.00197v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entangled Relations: Leveraging NLI and Meta-analysis to Enhance\n  Biomedical Relation Extraction", "abstract": "Recent research efforts have explored the potential of leveraging natural\nlanguage inference (NLI) techniques to enhance relation extraction (RE). In\nthis vein, we introduce MetaEntailRE, a novel adaptation method that harnesses\nNLI principles to enhance RE performance. Our approach follows past works by\nverbalizing relation classes into class-indicative hypotheses, aligning a\ntraditionally multi-class classification task to one of textual entailment. We\nintroduce three key enhancements: (1) Meta-class analysis which, instead of\nlabeling non-entailed premise-hypothesis pairs with the less informative\n\"neutral\" entailment label, provides additional context by analyzing\noverarching meta-relationships between classes; (2) Feasible hypothesis\nfiltering, which removes unlikely hypotheses from consideration based on domain\nknowledge derived from data; and (3) Group-based prediction selection, which\nfurther improves performance by selecting highly confident predictions.\nMetaEntailRE is conceptually simple and empirically powerful, yielding\nsignificant improvements over conventional relation extraction techniques and\nother NLI formulations. We observe surprisingly large F1 gains of 17.6 points\non BioRED and 13.4 points on ReTACRED compared to conventional methods,\nunderscoring the versatility of MetaEntailRE across both biomedical and general\ndomains.", "published": "2024-05-31 23:05:04", "link": "http://arxiv.org/abs/2406.00226v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Open the Data! Chuvash Datasets", "abstract": "In this paper, we introduce four comprehensive datasets for the Chuvash\nlanguage, aiming to support and enhance linguistic research and technological\ndevelopment for this underrepresented language. These datasets include a\nmonolingual dataset, a parallel dataset with Russian, a parallel dataset with\nEnglish, and an audio dataset. Each dataset is meticulously curated to serve\nvarious applications such as machine translation, linguistic analysis, and\nspeech recognition, providing valuable resources for scholars and developers\nworking with the Chuvash language. Together, these datasets represent a\nsignificant step towards preserving and promoting the Chuvash language in the\ndigital age.", "published": "2024-05-31 07:51:19", "link": "http://arxiv.org/abs/2407.11982v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open Ko-LLM Leaderboard: Evaluating Large Language Models in Korean with\n  Ko-H5 Benchmark", "abstract": "This paper introduces the Open Ko-LLM Leaderboard and the Ko-H5 Benchmark as\nvital tools for evaluating Large Language Models (LLMs) in Korean.\nIncorporating private test sets while mirroring the English Open LLM\nLeaderboard, we establish a robust evaluation framework that has been well\nintegrated in the Korean LLM community. We perform data leakage analysis that\nshows the benefit of private test sets along with a correlation study within\nthe Ko-H5 benchmark and temporal analyses of the Ko-H5 score. Moreover, we\npresent empirical support for the need to expand beyond set benchmarks. We hope\nthe Open Ko-LLM Leaderboard sets precedent for expanding LLM evaluation to\nfoster more linguistic diversity.", "published": "2024-05-31 02:05:45", "link": "http://arxiv.org/abs/2405.20574v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GAMedX: Generative AI-based Medical Entity Data Extractor Using Large\n  Language Models", "abstract": "In the rapidly evolving field of healthcare and beyond, the integration of\ngenerative AI in Electronic Health Records (EHRs) represents a pivotal\nadvancement, addressing a critical gap in current information extraction\ntechniques. This paper introduces GAMedX, a Named Entity Recognition (NER)\napproach utilizing Large Language Models (LLMs) to efficiently extract entities\nfrom medical narratives and unstructured text generated throughout various\nphases of the patient hospital visit. By addressing the significant challenge\nof processing unstructured medical text, GAMedX leverages the capabilities of\ngenerative AI and LLMs for improved data extraction. Employing a unified\napproach, the methodology integrates open-source LLMs for NER, utilizing\nchained prompts and Pydantic schemas for structured output to navigate the\ncomplexities of specialized medical jargon. The findings reveal significant\nROUGE F1 score on one of the evaluation datasets with an accuracy of 98\\%. This\ninnovation enhances entity extraction, offering a scalable, cost-effective\nsolution for automated forms filling from unstructured data. As a result,\nGAMedX streamlines the processing of unstructured narratives, and sets a new\nstandard in NER applications, contributing significantly to theoretical and\npractical advancements beyond the medical technology sphere.", "published": "2024-05-31 02:53:22", "link": "http://arxiv.org/abs/2405.20585v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Masked Language Modeling Becomes Conditional Density Estimation for\n  Tabular Data Synthesis", "abstract": "In this paper, our goal is to generate synthetic data for heterogeneous\n(mixed-type) tabular datasets with high machine learning utility (MLu). Since\nthe MLu performance depends on accurately approximating the conditional\ndistributions, we focus on devising a synthetic data generation method based on\nconditional distribution estimation. We introduce MaCoDE by redefining the\nconsecutive multi-class classification task of Masked Language Modeling (MLM)\nas histogram-based non-parametric conditional density estimation. Our approach\nenables the estimation of conditional densities across arbitrary combinations\nof target and conditional variables. We bridge the theoretical gap between\ndistributional learning and MLM by demonstrating that minimizing the orderless\nmulti-class classification loss leads to minimizing the total variation\ndistance between conditional distributions. To validate our proposed model, we\nevaluate its performance in synthetic data generation across 10 real-world\ndatasets, demonstrating its ability to adjust data privacy levels easily\nwithout re-training. Additionally, since masked input tokens in MLM are\nanalogous to missing data, we further assess its effectiveness in handling\ntraining datasets with missing values, including multiple imputations of the\nmissing entries.", "published": "2024-05-31 03:26:42", "link": "http://arxiv.org/abs/2405.20602v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "UniBias: Unveiling and Mitigating LLM Bias through Internal Attention\n  and FFN Manipulation", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in\nvarious tasks using the in-context learning (ICL) paradigm. However, their\neffectiveness is often compromised by inherent bias, leading to prompt\nbrittleness, i.e., sensitivity to design settings such as example selection,\norder, and prompt formatting. Previous studies have addressed LLM bias through\nexternal adjustment of model outputs, but the internal mechanisms that lead to\nsuch bias remain unexplored. Our work delves into these mechanisms,\nparticularly investigating how feedforward neural networks (FFNs) and attention\nheads result in the bias of LLMs. By Interpreting the contribution of\nindividual FFN vectors and attention heads, we identify the biased LLM\ncomponents that skew LLMs' prediction toward specific labels. To mitigate these\nbiases, we introduce UniBias, an inference-only method that effectively\nidentifies and eliminates biased FFN vectors and attention heads. Extensive\nexperiments across 12 NLP datasets demonstrate that UniBias significantly\nenhances ICL performance and alleviates prompt brittleness of LLMs.", "published": "2024-05-31 03:59:15", "link": "http://arxiv.org/abs/2405.20612v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models for Entity Matching", "abstract": "Entity matching (EM) is a critical task in data integration, aiming to\nidentify records across different datasets that refer to the same real-world\nentities. Traditional methods often rely on manually engineered features and\nrule-based systems, which struggle with diverse and unstructured data. The\nemergence of Large Language Models (LLMs) such as GPT-4 offers transformative\npotential for EM, leveraging their advanced semantic understanding and\ncontextual capabilities. This vision paper explores the application of LLMs to\nEM, discussing their advantages, challenges, and future research directions.\nAdditionally, we review related work on applying weak supervision and\nunsupervised approaches to EM, highlighting how LLMs can enhance these methods.", "published": "2024-05-31 05:22:07", "link": "http://arxiv.org/abs/2405.20624v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM-ESR: Large Language Models Enhancement for Long-tailed Sequential\n  Recommendation", "abstract": "Sequential recommender systems (SRS) aim to predict users' subsequent choices\nbased on their historical interactions and have found applications in diverse\nfields such as e-commerce and social media. However, in real-world systems,\nmost users interact with only a handful of items, while the majority of items\nare seldom consumed. These two issues, known as the long-tail user and\nlong-tail item challenges, often pose difficulties for existing SRS. These\nchallenges can adversely affect user experience and seller benefits, making\nthem crucial to address. Though a few works have addressed the challenges, they\nstill struggle with the seesaw or noisy issues due to the intrinsic scarcity of\ninteractions. The advancements in large language models (LLMs) present a\npromising solution to these problems from a semantic perspective. As one of the\npioneers in this field, we propose the Large Language Models Enhancement\nframework for Sequential Recommendation (LLM-ESR). This framework utilizes\nsemantic embeddings derived from LLMs to enhance SRS without adding extra\ninference load from LLMs. To address the long-tail item challenge, we design a\ndual-view modeling framework that combines semantics from LLMs and\ncollaborative signals from conventional SRS. For the long-tail user challenge,\nwe propose a retrieval augmented self-distillation method to enhance user\npreference representation using more informative interactions from similar\nusers. To verify the effectiveness and versatility of our proposed enhancement\nframework, we conduct extensive experiments on three real-world datasets using\nthree popular SRS models. The results show that our method surpasses existing\nbaselines consistently, and benefits long-tail users and items especially. The\nimplementation code is available at\nhttps://github.com/Applied-Machine-Learning-Lab/LLM-ESR.", "published": "2024-05-31 07:24:42", "link": "http://arxiv.org/abs/2405.20646v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Reward-based Input Construction for Cross-document Relation Extraction", "abstract": "Relation extraction (RE) is a fundamental task in natural language\nprocessing, aiming to identify relations between target entities in text. While\nmany RE methods are designed for a single sentence or document, cross-document\nRE has emerged to address relations across multiple long documents. Given the\nnature of long documents in cross-document RE, extracting document embeddings\nis challenging due to the length constraints of pre-trained language models.\nTherefore, we propose REward-based Input Construction (REIC), the first\nlearning-based sentence selector for cross-document RE. REIC extracts sentences\nbased on relational evidence, enabling the RE module to effectively infer\nrelations. Since supervision of evidence sentences is generally unavailable, we\ntrain REIC using reinforcement learning with RE prediction scores as rewards.\nExperimental results demonstrate the superiority of our method over heuristic\nmethods for different RE structures and backbones in cross-document RE. Our\ncode is publicly available at https://github.com/aailabkaist/REIC.", "published": "2024-05-31 07:30:34", "link": "http://arxiv.org/abs/2405.20649v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Passage-specific Prompt Tuning for Passage Reranking in Question\n  Answering with Large Language Models", "abstract": "Effective passage retrieval and reranking methods have been widely utilized\nto identify suitable candidates in open-domain question answering tasks, recent\nstudies have resorted to LLMs for reranking the retrieved passages by the\nlog-likelihood of the question conditioned on each passage. Although these\nmethods have demonstrated promising results, the performance is notably\nsensitive to the human-written prompt (or hard prompt), and fine-tuning LLMs\ncan be computationally intensive and time-consuming. Furthermore, this approach\nlimits the leverage of question-passage relevance pairs and passage-specific\nknowledge to enhance the ranking capabilities of LLMs. In this paper, we\npropose passage-specific prompt tuning for reranking in open-domain question\nanswering (PSPT): a parameter-efficient method that fine-tunes learnable\npassage-specific soft prompts, incorporating passage-specific knowledge from a\nlimited set of question-passage relevance pairs. The method involves ranking\nretrieved passages based on the log-likelihood of the model generating the\nquestion conditioned on each passage and the learned soft prompt. We conducted\nextensive experiments utilizing the Llama-2-chat-7B model across three publicly\navailable open-domain question answering datasets and the results demonstrate\nthe effectiveness of the proposed approach.", "published": "2024-05-31 07:43:42", "link": "http://arxiv.org/abs/2405.20654v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Unraveling and Mitigating Retriever Inconsistencies in\n  Retrieval-Augmented Large Language Models", "abstract": "Although Retrieval-Augmented Large Language Models (RALMs) demonstrate their\nsuperiority in terms of factuality, they do not consistently outperform the\noriginal retrieval-free Language Models (LMs). Our experiments reveal that this\nexample-level performance inconsistency exists not only between\nretrieval-augmented and retrieval-free LM but also among different retrievers.\nTo understand this phenomenon, we investigate the degeneration behavior of\nRALMs and theoretically decompose it into four categories. Further analysis\nbased on our decomposition reveals that the innate difference in knowledge\nsources and the unpredictable degeneration of the reader model contribute most\nto the inconsistency. Drawing from our analysis, we introduce Ensemble of\nRetrievers (EoR), a trainable framework that can adaptively retrieve from\ndifferent knowledge sources and effectively decrease unpredictable reader\nerrors. Our experiments on Open Domain Question Answering show that EoR\nsubstantially improves performance over the RALM with a single retriever by\nconsiderably reducing inconsistent behaviors.", "published": "2024-05-31 08:22:49", "link": "http://arxiv.org/abs/2405.20680v5", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Joint Embeddings for Graph Instruction Tuning", "abstract": "Large Language Models (LLMs) have achieved impressive performance in text\nunderstanding and have become an essential tool for building smart assistants.\nOriginally focusing on text, they have been enhanced with multimodal\ncapabilities in recent works that successfully built visual instruction\nfollowing assistants. As far as the graph modality goes, however, no such\nassistants have yet been developed. Graph structures are complex in that they\nrepresent relation between different features and are permutation invariant.\nMoreover, representing them in purely textual form does not always lead to good\nLLM performance even for finetuned models. As a result, there is a need to\ndevelop a new method to integrate graphs in LLMs for general graph\nunderstanding. This work explores the integration of the graph modality in LLM\nfor general graph instruction following tasks. It aims at producing a deep\nlearning model that enhances an underlying LLM with graph embeddings and trains\nit to understand them and to produce, given an instruction, an answer grounded\nin the graph representation. The approach performs significantly better than a\ngraph to text approach and remains consistent even for larger graphs.", "published": "2024-05-31 08:26:47", "link": "http://arxiv.org/abs/2405.20684v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization\n  for Prompt Enhancement", "abstract": "Large language models (LLMs) demonstrate exceptional instruct-following\nability to complete various downstream tasks. Although this impressive ability\nmakes LLMs flexible task solvers, their performance in solving tasks also\nheavily relies on instructions. In this paper, we reveal that LLMs are\nover-sensitive to lexical variations in task instructions, even when the\nvariations are imperceptible to humans. By providing models with neighborhood\ninstructions, which are closely situated in the latent representation space and\ndiffer by only one semantically similar word, the performance on downstream\ntasks can be vastly different. Following this property, we propose a black-box\nCombinatorial Optimization framework for Prompt Lexical Enhancement (COPLE).\nCOPLE performs iterative lexical optimization according to the feedback from a\nbatch of proxy tasks, using a search strategy related to word influence.\nExperiments show that even widely-used human-crafted prompts for current\nbenchmarks suffer from the lexical sensitivity of models, and COPLE recovers\nthe declined model ability in both instruct-following and solving downstream\ntasks.", "published": "2024-05-31 08:53:59", "link": "http://arxiv.org/abs/2405.20701v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FinGen: A Dataset for Argument Generation in Finance", "abstract": "Thinking about the future is one of the important activities that people do\nin daily life. Futurists also pay a lot of effort into figuring out possible\nscenarios for the future. We argue that the exploration of this direction is\nstill in an early stage in the NLP research. To this end, we propose three\nargument generation tasks in the financial application scenario. Our\nexperimental results show these tasks are still big challenges for\nrepresentative generation models. Based on our empirical results, we further\npoint out several unresolved issues and challenges in this research direction.", "published": "2024-05-31 09:00:43", "link": "http://arxiv.org/abs/2405.20708v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-Augmented Preference Optimization: Off-Policy Paradigms for\n  Language Model Alignment", "abstract": "Traditional language model alignment methods, such as Direct Preference\nOptimization (DPO), are limited by their dependence on static, pre-collected\npaired preference data, which hampers their adaptability and practical\napplicability. To overcome this limitation, we introduce Self-Augmented\nPreference Optimization (SAPO), an effective and scalable training paradigm\nthat does not require existing paired data. Building on the self-play concept,\nwhich autonomously generates negative responses, we further incorporate an\noff-policy learning pipeline to enhance data exploration and exploitation.\nSpecifically, we employ an Exponential Moving Average (EMA) model in\nconjunction with a replay buffer to enable dynamic updates of response\nsegments, effectively integrating real-time feedback with insights from\nhistorical data. Our comprehensive evaluations of the LLaMA3-8B and Mistral-7B\nmodels across benchmarks, including the Open LLM Leaderboard, IFEval,\nAlpacaEval 2.0, and MT-Bench, demonstrate that SAPO matches or surpasses\nestablished offline contrastive baselines, such as DPO and Odds Ratio\nPreference Optimization, and outperforms offline self-play methods like SPIN.\nOur code is available at https://github.com/yinyueqin/SAPO", "published": "2024-05-31 14:21:04", "link": "http://arxiv.org/abs/2405.20830v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Don't Buy it! Reassessing the Ad Understanding Abilities of Contrastive\n  Multimodal Models", "abstract": "Image-based advertisements are complex multimodal stimuli that often contain\nunusual visual elements and figurative language. Previous research on automatic\nad understanding has reported impressive zero-shot accuracy of contrastive\nvision-and-language models (VLMs) on an ad-explanation retrieval task. Here, we\nexamine the original task setup and show that contrastive VLMs can solve it by\nexploiting grounding heuristics. To control for this confound, we introduce\nTRADE, a new evaluation test set with adversarial grounded explanations. While\nthese explanations look implausible to humans, we show that they \"fool\" four\ndifferent contrastive VLMs. Our findings highlight the need for an improved\noperationalisation of automatic ad understanding that truly evaluates VLMs'\nmultimodal reasoning abilities. We make our code and TRADE available at\nhttps://github.com/dmg-illc/trade .", "published": "2024-05-31 14:31:46", "link": "http://arxiv.org/abs/2405.20846v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "clembench-2024: A Challenging, Dynamic, Complementary, Multilingual\n  Benchmark and Underlying Flexible Framework for LLMs as Multi-Action Agents", "abstract": "It has been established in recent work that Large Language Models (LLMs) can\nbe prompted to \"self-play\" conversational games that probe certain capabilities\n(general instruction following, strategic goal orientation, language\nunderstanding abilities), where the resulting interactive game play can be\nautomatically scored. In this paper, we take one of the proposed frameworks for\nsetting up such game-play environments, and further test its usefulness as an\nevaluation instrument, along a number of dimensions: We show that it can easily\nkeep up with new developments while avoiding data contamination, we show that\nthe tests implemented within it are not yet saturated (human performance is\nsubstantially higher than that of even the best models), and we show that it\nlends itself to investigating additional questions, such as the impact of the\nprompting language on performance. We believe that the approach forms a good\nbasis for making decisions on model choice for building applied interactive\nsystems, and perhaps ultimately setting up a closed-loop development\nenvironment of system and simulated evaluator.", "published": "2024-05-31 14:43:31", "link": "http://arxiv.org/abs/2405.20859v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models: A New Approach for Privacy Policy Analysis at\n  Scale", "abstract": "The number and dynamic nature of web and mobile applications presents\nsignificant challenges for assessing their compliance with data protection\nlaws. In this context, symbolic and statistical Natural Language Processing\n(NLP) techniques have been employed for the automated analysis of these\nsystems' privacy policies. However, these techniques typically require\nlabor-intensive and potentially error-prone manually annotated datasets for\ntraining and validation. This research proposes the application of Large\nLanguage Models (LLMs) as an alternative for effectively and efficiently\nextracting privacy practices from privacy policies at scale. Particularly, we\nleverage well-known LLMs such as ChatGPT and Llama 2, and offer guidance on the\noptimal design of prompts, parameters, and models, incorporating advanced\nstrategies such as few-shot learning. We further illustrate its capability to\ndetect detailed and varied privacy practices accurately. Using several renowned\ndatasets in the domain as a benchmark, our evaluation validates its exceptional\nperformance, achieving an F1 score exceeding 93%. Besides, it does so with\nreduced costs, faster processing times, and fewer technical knowledge\nrequirements. Consequently, we advocate for LLM-based solutions as a sound\nalternative to traditional NLP techniques for the automated analysis of privacy\npolicies at scale.", "published": "2024-05-31 15:12:33", "link": "http://arxiv.org/abs/2405.20900v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "OR-Bench: An Over-Refusal Benchmark for Large Language Models", "abstract": "Large Language Models (LLMs) require careful safety alignment to prevent\nmalicious outputs. While significant research focuses on mitigating harmful\ncontent generation, the enhanced safety often come with the side effect of\nover-refusal, where LLMs may reject innocuous prompts and become less helpful.\nAlthough the issue of over-refusal has been empirically observed, a systematic\nmeasurement is challenging due to the difficulty of crafting prompts that\nappear harmful but are benign. This study proposes a novel method for\nautomatically generating large-scale sets of \"seemingly toxic prompts\" (benign\nprompts likely rejected by LLMs). Leveraging this technique, we introduce\nOR-Bench, the first large-scale over-refusal benchmark. OR-Bench comprises\n80,000 seemingly toxic prompts across 10 common rejection categories, a subset\nof around 1,000 hard prompts that are challenging even for state-of-the-art\nLLMs, and an additional 600 toxic prompts to prevent indiscriminate responses.\nWe then conduct a comprehensive study to measure the over-refusal of 25 popular\nLLMs across 8 model families. Our datasets are available at\nhttps://huggingface.co/datasets/bench-llm/or-bench and the demo can be found at\nhttps://huggingface.co/spaces/bench-llm/or-bench. We hope this benchmark can\nhelp the community develop better safety aligned models.", "published": "2024-05-31 15:44:33", "link": "http://arxiv.org/abs/2405.20947v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Robot Walks into a Bar: Can Language Models Serve as Creativity\n  Support Tools for Comedy? An Evaluation of LLMs' Humour Alignment with\n  Comedians", "abstract": "We interviewed twenty professional comedians who perform live shows in front\nof audiences and who use artificial intelligence in their artistic process as\npart of 3-hour workshops on ``AI x Comedy'' conducted at the Edinburgh Festival\nFringe in August 2023 and online. The workshop consisted of a comedy writing\nsession with large language models (LLMs), a human-computer interaction\nquestionnaire to assess the Creativity Support Index of AI as a writing tool,\nand a focus group interrogating the comedians' motivations for and processes of\nusing AI, as well as their ethical concerns about bias, censorship and\ncopyright. Participants noted that existing moderation strategies used in\nsafety filtering and instruction-tuned LLMs reinforced hegemonic viewpoints by\nerasing minority groups and their perspectives, and qualified this as a form of\ncensorship. At the same time, most participants felt the LLMs did not succeed\nas a creativity support tool, by producing bland and biased comedy tropes, akin\nto ``cruise ship comedy material from the 1950s, but a bit less racist''. Our\nwork extends scholarship about the subtle difference between, one the one hand,\nharmful speech, and on the other hand, ``offensive'' language as a practice of\nresistance, satire and ``punching up''. We also interrogate the global value\nalignment behind such language models, and discuss the importance of\ncommunity-based value alignment and data ownership to build AI tools that\nbetter suit artists' needs.", "published": "2024-05-31 15:55:51", "link": "http://arxiv.org/abs/2405.20956v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "LCQ: Low-Rank Codebook based Quantization for Large Language Models", "abstract": "Large language models~(LLMs) have recently demonstrated promising performance\nin many tasks. However, the high storage and computational cost of LLMs has\nbecome a challenge for deploying LLMs. Weight quantization has been widely used\nfor model compression, which can reduce both storage and computational cost.\nMost existing weight quantization methods for LLMs use a rank-one codebook for\nquantization, which results in substantial accuracy loss when the compression\nratio is high. In this paper, we propose a novel weight quantization method,\ncalled low-rank codebook based quantization~(LCQ), for LLMs. LCQ adopts a\nlow-rank codebook, the rank of which can be larger than one, for quantization.\nExperiments show that LCQ can achieve better accuracy than existing methods\nwith a negligibly extra storage cost.", "published": "2024-05-31 16:21:05", "link": "http://arxiv.org/abs/2405.20973v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CWRCzech: 100M Query-Document Czech Click Dataset and Its Application to\n  Web Relevance Ranking", "abstract": "We present CWRCzech, Click Web Ranking dataset for Czech, a 100M\nquery-document Czech click dataset for relevance ranking with user behavior\ndata collected from search engine logs of Seznam$.$cz. To the best of our\nknowledge, CWRCzech is the largest click dataset with raw text published so\nfar. It provides document positions in the search results as well as\ninformation about user behavior: 27.6M clicked documents and 10.8M dwell times.\nIn addition, we also publish a manually annotated Czech test for the relevance\ntask, containing nearly 50k query-document pairs, each annotated by at least 2\nannotators. Finally, we analyze how the user behavior data improve relevance\nranking and show that models trained on data automatically harnessed at\nsufficient scale can surpass the performance of models trained on human\nannotated data. CWRCzech is published under an academic non-commercial license\nand is available to the research community at\nhttps://github.com/seznam/CWRCzech.", "published": "2024-05-31 16:38:54", "link": "http://arxiv.org/abs/2405.20994v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "You Only Scan Once: Efficient Multi-dimension Sequential Modeling with\n  LightNet", "abstract": "Linear attention mechanisms have gained prominence in causal language models\ndue to their linear computational complexity and enhanced speed. However, the\ninherent decay mechanism in linear attention presents challenges when applied\nto multi-dimensional sequence modeling tasks, such as image processing and\nmulti-modal learning. In these scenarios, the utilization of sequential\nscanning to establish a global receptive field necessitates multiple scans for\nmulti-dimensional data, thereby leading to inefficiencies. This paper\nidentifies the inefficiency caused by a multiplicative linear recurrence and\nproposes an efficient alternative additive linear recurrence to avoid the\nissue, as it can handle multi-dimensional data within a single scan. We further\ndevelop an efficient multi-dimensional sequential modeling framework called\nLightNet based on the new recurrence. Moreover, we present two new\nmulti-dimensional linear relative positional encoding methods, MD-TPE and\nMD-LRPE to enhance the model's ability to discern positional information in\nmulti-dimensional scenarios. Our empirical evaluations across various tasks,\nincluding image classification, image generation, bidirectional language\nmodeling, and autoregressive language modeling, demonstrate the efficacy of\nLightNet, showcasing its potential as a versatile and efficient solution for\nmulti-dimensional sequential modeling.", "published": "2024-05-31 17:09:16", "link": "http://arxiv.org/abs/2405.21022v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "LACIE: Listener-Aware Finetuning for Confidence Calibration in Large\n  Language Models", "abstract": "When answering questions, LLMs can convey not only an answer, but a level of\nconfidence about the answer being correct. This includes explicit confidence\nmarkers (e.g. giving a numeric score) as well as implicit markers, like an\nauthoritative tone or elaborating with additional knowledge. For LLMs to be\ntrustworthy knowledge sources, the confidence they convey should match their\nactual expertise; however, most current models tend towards overconfidence. To\ncalibrate both implicit and explicit confidence markers, we introduce a\npragmatic, listener-aware finetuning method (LACIE) that models the listener,\nconsidering not only whether an answer is right, but whether it will be\naccepted by a listener. We cast calibration as preference optimization,\ncreating data via a two-agent game, where a speaker model's outputs are judged\nby a simulated listener. We then finetune three LLMs (Mistral-7B, Llama3-8B,\nLlama3-70B) with LACIE, and show that the resulting models are better\ncalibrated w.r.t. a simulated listener. Crucially, these trends transfer to\nhuman listeners, helping them correctly predict model correctness: we conduct a\nhuman evaluation where annotators accept or reject an LLM's answers, finding\nthat training with LACIE results in 47% fewer incorrect answers being accepted\nwhile maintaining the same level of acceptance for correct answers.\nFurthermore, LACIE generalizes to another dataset, resulting in a large\nincrease in truthfulness on TruthfulQA when trained on TriviaQA. Our analysis\nindicates that LACIE leads to a better confidence separation between correct\nand incorrect examples. Qualitatively, we find that a LACIE-trained model\nhedges more and implicitly signals certainty when it is correct by using an\nauthoritative tone or including details. Finally, LACIE finetuning leads to an\nemergent increase in model abstention (e.g. saying \"I don't know\") for answers\nthat are likely wrong.", "published": "2024-05-31 17:16:38", "link": "http://arxiv.org/abs/2405.21028v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Direct Alignment of Language Models via Quality-Aware Self-Refinement", "abstract": "Reinforcement Learning from Human Feedback (RLHF) has been commonly used to\nalign the behaviors of Large Language Models (LLMs) with human preferences.\nRecently, a popular alternative is Direct Policy Optimization (DPO), which\nreplaces an LLM-based reward model with the policy itself, thus obviating the\nneed for extra memory and training time to learn the reward model. However, DPO\ndoes not consider the relative qualities of the positive and negative\nresponses, and can lead to sub-optimal training outcomes. To alleviate this\nproblem, we investigate the use of intrinsic knowledge within the on-the-fly\nfine-tuning LLM to obtain relative qualities and help to refine the loss\nfunction. Specifically, we leverage the knowledge of the LLM to design a\nrefinement function to estimate the quality of both the positive and negative\nresponses. We show that the constructed refinement function can help\nself-refine the loss function under mild assumptions. The refinement function\nis integrated into DPO and its variant Identity Policy Optimization (IPO).\nExperiments across various evaluators indicate that they can improve the\nperformance of the fine-tuned models over DPO and IPO.", "published": "2024-05-31 17:31:18", "link": "http://arxiv.org/abs/2405.21040v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Code Pretraining Improves Entity Tracking Abilities of Language Models", "abstract": "Recent work has provided indirect evidence that pretraining language models\non code improves the ability of models to track state changes of discourse\nentities expressed in natural language. In this work, we systematically test\nthis claim by comparing pairs of language models on their entity tracking\nperformance. Critically, the pairs consist of base models and models trained on\ntop of these base models with additional code data. We extend this analysis to\nadditionally examine the effect of math training, another highly structured\ndata type, and alignment tuning, an important step for enhancing the usability\nof models. We find clear evidence that models additionally trained on large\namounts of code outperform the base models. On the other hand, we find no\nconsistent benefit of additional math training or alignment tuning across\nvarious model families.", "published": "2024-05-31 17:56:33", "link": "http://arxiv.org/abs/2405.21068v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Video-MME: The First-Ever Comprehensive Evaluation Benchmark of\n  Multi-modal LLMs in Video Analysis", "abstract": "In the quest for artificial general intelligence, Multi-modal Large Language\nModels (MLLMs) have emerged as a focal point in recent advancements. However,\nthe predominant focus remains on developing their capabilities in static image\nunderstanding. The potential of MLLMs in processing sequential visual data is\nstill insufficiently explored, highlighting the absence of a comprehensive,\nhigh-quality assessment of their performance. In this paper, we introduce\nVideo-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of\nMLLMs in Video analysis. Our work distinguishes from existing benchmarks\nthrough four key features: 1) Diversity in video types, spanning 6 primary\nvisual domains with 30 subfields to ensure broad scenario generalizability; 2)\nDuration in temporal dimension, encompassing both short-, medium-, and\nlong-term videos, ranging from 11 seconds to 1 hour, for robust contextual\ndynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides\nvideo frames, including subtitles and audios, to unveil the all-round\ncapabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual\nlabeling by expert annotators to facilitate precise and reliable model\nassessment. 900 videos with a total of 254 hours are manually selected and\nannotated by repeatedly viewing all the video content, resulting in 2,700\nquestion-answer pairs. With Video-MME, we extensively evaluate various\nstate-of-the-art MLLMs, including GPT-4 series and Gemini 1.5 Pro, as well as\nopen-source image models like InternVL-Chat-V1.5 and video models like\nLLaVA-NeXT-Video. Our experiments reveal that Gemini 1.5 Pro is the\nbest-performing commercial model, significantly outperforming the open-source\nmodels. Our dataset along with these findings underscores the need for further\nimprovements in handling longer sequences and multi-modal data. Project Page:\nhttps://video-mme.github.io", "published": "2024-05-31 17:59:47", "link": "http://arxiv.org/abs/2405.21075v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Long-Span Question-Answering: Automatic Question Generation and\n  QA-System Ranking via Side-by-Side Evaluation", "abstract": "We explore the use of long-context capabilities in large language models to\ncreate synthetic reading comprehension data from entire books. Previous efforts\nto construct such datasets relied on crowd-sourcing, but the emergence of\ntransformers with a context size of 1 million or more tokens now enables\nentirely automatic approaches. Our objective is to test the capabilities of\nLLMs to analyze, understand, and reason over problems that require a detailed\ncomprehension of long spans of text, such as questions involving character\narcs, broader themes, or the consequences of early actions later in the story.\nWe propose a holistic pipeline for automatic data generation including question\ngeneration, answering, and model scoring using an ``Evaluator''. We find that a\nrelative approach, comparing answers between models in a pairwise fashion and\nranking with a Bradley-Terry model, provides a more consistent and\ndifferentiating scoring mechanism than an absolute scorer that rates answers\nindividually. We also show that LLMs from different model families produce\nmoderate agreement in their ratings. We ground our approach using the manually\ncurated NarrativeQA dataset, where our evaluator shows excellent agreement with\nhuman judgement and even finds errors in the dataset. Using our automatic\nevaluation approach, we show that using an entire book as context produces\nsuperior reading comprehension performance compared to baseline no-context\n(parametric knowledge only) and retrieval-based approaches.", "published": "2024-05-31 20:15:10", "link": "http://arxiv.org/abs/2406.00179v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Image captioning in different languages", "abstract": "This short position paper provides a manually curated list of non-English\nimage captioning datasets (as of May 2024). Through this list, we can observe\nthe dearth of datasets in different languages: only 23 different languages are\nrepresented. With the addition of the Crossmodal-3600 dataset (Thapliyal et\nal., 2022, 36 languages) this number increases somewhat, but still this number\nis small compared to the +/-500 institutional languages that are out there.\nThis paper closes with some open questions for the field of Vision & Language.", "published": "2024-05-31 09:37:54", "link": "http://arxiv.org/abs/2407.09495v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "The Point of View of a Sentiment: Towards Clinician Bias Detection in\n  Psychiatric Notes", "abstract": "Negative patient descriptions and stigmatizing language can contribute to\ngenerating healthcare disparities in two ways: (1) read by patients, they can\nharm their trust and engagement with the medical center; (2) read by\nphysicians, they may negatively influence their perspective of a future\npatient. In psychiatry, the patient-clinician therapeutic alliance is a major\ndeterminant of clinical outcomes. Therefore, language usage in psychiatric\nclinical notes may not only create healthcare disparities, but also perpetuate\nthem. Recent advances in NLP systems have facilitated the efforts to detect\ndiscriminatory language in healthcare. However, such attempts have only focused\non the perspectives of the medical center and its physicians. Considering both\nphysicians and non-physicians' point of view is a more translatable approach to\nidentifying potentially harmful language in clinical notes. By leveraging\npre-trained and large language models (PLMs and LLMs), this work aims to\ncharacterize potentially harmful language usage in psychiatric notes by\nidentifying the sentiment expressed in sentences describing patients based on\nthe reader's point of view. Extracting 39 sentences from the Mount Sinai Health\nSystem containing psychiatric lexicon, we fine-tuned three PLMs (RoBERTa,\nGatorTron, and GatorTron + Task Adaptation) and implemented zero-shot and\nfew-shot ICL approaches for three LLMs (GPT-3.5, Llama-3.1, and Mistral) to\nclassify the sentiment of the sentences according to the physician or\nnon-physician point of view. Results showed that GPT-3.5 aligned best to\nphysician point of view and Mistral aligned best to non-physician point of\nview. These results underline the importance of recognizing the reader's point\nof view, not only for improving the note writing process, but also for the\nquantification, identification, and reduction of bias in computational systems\nfor downstream analyses.", "published": "2024-05-31 02:28:41", "link": "http://arxiv.org/abs/2405.20582v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bi-Directional Transformers vs. word2vec: Discovering Vulnerabilities in\n  Lifted Compiled Code", "abstract": "Detecting vulnerabilities within compiled binaries is challenging due to lost\nhigh-level code structures and other factors such as architectural\ndependencies, compilers, and optimization options. To address these obstacles,\nthis research explores vulnerability detection using natural language\nprocessing (NLP) embedding techniques with word2vec, BERT, and RoBERTa to learn\nsemantics from intermediate representation (LLVM IR) code. Long short-term\nmemory (LSTM) neural networks were trained on embeddings from encoders created\nusing approximately 48k LLVM functions from the Juliet dataset. This study is\npioneering in its comparison of word2vec models with multiple bidirectional\ntransformers (BERT, RoBERTa) embeddings built using LLVM code to train neural\nnetworks to detect vulnerabilities in compiled binaries. Word2vec Skip-Gram\nmodels achieved 92% validation accuracy in detecting vulnerabilities,\noutperforming word2vec Continuous Bag of Words (CBOW), BERT, and RoBERTa. This\nsuggests that complex contextual embeddings may not provide advantages over\nsimpler word2vec models for this task when a limited number (e.g. 48K) of data\nsamples are used to train the bidirectional transformer-based models. The\ncomparative results provide novel insights into selecting optimal embeddings\nfor learning compiler-independent semantic code representations to advance\nmachine learning detection of vulnerabilities in compiled binaries.", "published": "2024-05-31 03:57:19", "link": "http://arxiv.org/abs/2405.20611v3", "categories": ["cs.CR", "cs.CL", "cs.LG", "cs.SE", "D.4.6; I.2.6; I.5.1"], "primary_category": "cs.CR"}
{"title": "ToxVidLM: A Multimodal Framework for Toxicity Detection in Code-Mixed\n  Videos", "abstract": "In an era of rapidly evolving internet technology, the surge in multimodal\ncontent, including videos, has expanded the horizons of online communication.\nHowever, the detection of toxic content in this diverse landscape, particularly\nin low-resource code-mixed languages, remains a critical challenge. While\nsubstantial research has addressed toxic content detection in textual data, the\nrealm of video content, especially in non-English languages, has been\nrelatively underexplored. This paper addresses this research gap by introducing\na benchmark dataset, the first of its kind, consisting of 931 videos with 4021\ncode-mixed Hindi-English utterances collected from YouTube. Each utterance\nwithin this dataset has been meticulously annotated for toxicity, severity, and\nsentiment labels. We have developed an advanced Multimodal Multitask framework\nbuilt for Toxicity detection in Video Content by leveraging Language Models\n(LMs), crafted for the primary objective along with the additional tasks of\nconducting sentiment and severity analysis. ToxVidLM incorporates three key\nmodules - the Encoder module, Cross-Modal Synchronization module, and Multitask\nmodule - crafting a generic multimodal LM customized for intricate video\nclassification tasks. Our experiments reveal that incorporating multiple\nmodalities from the videos substantially enhances the performance of toxic\ncontent detection by achieving an Accuracy and Weighted F1 score of 94.29% and\n94.35%, respectively.", "published": "2024-05-31 05:40:56", "link": "http://arxiv.org/abs/2405.20628v2", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Shotluck Holmes: A Family of Efficient Small-Scale Large Language Vision\n  Models For Video Captioning and Summarization", "abstract": "Video is an increasingly prominent and information-dense medium, yet it poses\nsubstantial challenges for language models. A typical video consists of a\nsequence of shorter segments, or shots, that collectively form a coherent\nnarrative. Each shot is analogous to a word in a sentence where multiple data\nstreams of information (such as visual and auditory data) must be processed\nsimultaneously. Comprehension of the entire video requires not only\nunderstanding the visual-audio information of each shot but also requires that\nthe model links the ideas between each shot to generate a larger,\nall-encompassing story. Despite significant progress in the field, current\nworks often overlook videos' more granular shot-by-shot semantic information.\nIn this project, we propose a family of efficient large language vision models\n(LLVMs) to boost video summarization and captioning called Shotluck Holmes. By\nleveraging better pretraining and data collection strategies, we extend the\nabilities of existing small LLVMs from being able to understand a picture to\nbeing able to understand a sequence of frames. Specifically, we show that\nShotluck Holmes achieves better performance than state-of-the-art results on\nthe Shot2Story video captioning and summary task with significantly smaller and\nmore computationally efficient models.", "published": "2024-05-31 07:30:24", "link": "http://arxiv.org/abs/2405.20648v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Position Coupling: Improving Length Generalization of Arithmetic\n  Transformers Using Task Structure", "abstract": "Even for simple arithmetic tasks like integer addition, it is challenging for\nTransformers to generalize to longer sequences than those encountered during\ntraining. To tackle this problem, we propose position coupling, a simple yet\neffective method that directly embeds the structure of the tasks into the\npositional encoding of a (decoder-only) Transformer. Taking a departure from\nthe vanilla absolute position mechanism assigning unique position IDs to each\nof the tokens, we assign the same position IDs to two or more \"relevant\"\ntokens; for integer addition tasks, we regard digits of the same significance\nas in the same position. On the empirical side, we show that with the proposed\nposition coupling, our models trained on 1 to 30-digit additions can generalize\nup to 200-digit additions (6.67x of the trained length). On the theoretical\nside, we prove that a 1-layer Transformer with coupled positions can solve the\naddition task involving exponentially many digits, whereas any 1-layer\nTransformer without positional information cannot entirely solve it. We also\ndemonstrate that position coupling can be applied to other algorithmic tasks\nsuch as Nx2 multiplication and a two-dimensional task.", "published": "2024-05-31 08:13:35", "link": "http://arxiv.org/abs/2405.20671v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "abstract": "Current Multimodal Large Language Models (MLLMs) typically integrate a\npre-trained LLM with another pre-trained vision transformer through a\nconnector, such as an MLP, endowing the LLM with visual capabilities. However,\nthe misalignment between two embedding strategies in MLLMs -- the structural\ntextual embeddings based on an embedding look-up table and the continuous\nembeddings generated directly by the vision encoder -- makes challenges for a\nmore seamless fusion of visual and textual information. We propose Ovis, a\nnovel MLLM architecture designed to structurally align visual and textual\nembeddings. Ovis integrates an additional learnable visual embedding table into\nthe visual encoder's process. To capture rich visual semantics, each image\npatch indexes the visual embedding table multiple times, resulting in a final\nvisual embedding that is a probabilistic combination of the indexed embeddings.\nThis structural approach mirrors the method used for generating textual\nembeddings. Empirical evaluations on various multimodal benchmarks show that\nOvis outperforms open-source MLLMs of similar parameter scales and even\nsurpasses the proprietary model Qwen-VL-Plus overall. These results highlight\nthe potential of Ovis' structured visual representation for advancing MLLM\narchitectural design and promoting more effective multimodal learning. Code,\ndatasets, and models are available at https://github.com/AIDC-AI/Ovis.", "published": "2024-05-31 13:59:18", "link": "http://arxiv.org/abs/2405.20797v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "An iterated learning model of language change that mixes supervised and\n  unsupervised learning", "abstract": "The iterated learning model is an agent model which simulates the\ntransmission of of language from generation to generation. It is used to study\nhow the language adapts to pressures imposed by transmission. In each\niteration, a language tutor exposes a na\\\"ive pupil to a limited training set\nof utterances, each pairing a random meaning with the signal that conveys it.\nThen the pupil becomes a tutor for a new na\\\"ive pupil in the next iteration.\nThe transmission bottleneck ensures that tutors must generalize beyond the\ntraining set that they experienced. Repeated cycles of learning and\ngeneralization can result in a language that is expressive, compositional and\nstable. Previously, the agents in the iterated learning model mapped signals to\nmeanings using an artificial neural network but relied on an unrealistic and\ncomputationally expensive process of obversion to map meanings to signals.\nHere, both maps are neural networks, trained separately through supervised\nlearning and together through unsupervised learning in the form of an\nautoencoder. This avoids the computational burden entailed in obversion and\nintroduces a mixture of supervised and unsupervised learning as observed during\nlanguage learning in children. The new model demonstrates a linear relationship\nbetween the dimensionality of meaning-signal space and effective bottleneck\nsize and suggests that internal reflection on potential utterances is important\nin language learning and evolution.", "published": "2024-05-31 14:14:01", "link": "http://arxiv.org/abs/2405.20818v3", "categories": ["cs.CL", "nlin.AO", "q-bio.PE"], "primary_category": "cs.CL"}
{"title": "Outliers and Calibration Sets have Diminishing Effect on Quantization of\n  Modern LLMs", "abstract": "Post-Training Quantization (PTQ) enhances the efficiency of Large Language\nModels (LLMs) by enabling faster operation and compatibility with more\naccessible hardware through reduced memory usage, at the cost of small\nperformance drops. We explore the role of calibration sets in PTQ, specifically\ntheir effect on hidden activations in various notable open-source LLMs.\nCalibration sets are crucial for evaluating activation magnitudes and\nidentifying outliers, which can distort the quantization range and negatively\nimpact performance. Our analysis reveals a marked contrast in quantization\neffectiveness across models. The older OPT model, upon which much of the\nquantization literature is based, shows significant performance deterioration\nand high susceptibility to outliers with varying calibration sets. In contrast,\nnewer models like Llama-2 7B, Llama-3 8B, Command-R 35B, and Mistral 7B\ndemonstrate strong robustness, with Mistral 7B showing near-immunity to\noutliers and stable activations. These findings suggest a shift in PTQ\nstrategies might be needed. As advancements in pre-training methods reduce the\nrelevance of outliers, there is an emerging need to reassess the fundamentals\nof current quantization literature. The emphasis should pivot towards\noptimizing inference speed, rather than primarily focusing on outlier\npreservation, to align with the evolving characteristics of state-of-the-art\nLLMs.", "published": "2024-05-31 14:24:33", "link": "http://arxiv.org/abs/2405.20835v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Preemptive Answer \"Attacks\" on Chain-of-Thought Reasoning", "abstract": "Large language models (LLMs) showcase impressive reasoning capabilities when\ncoupled with Chain-of-Thought (CoT) prompting. However, the robustness of this\napproach warrants further investigation. In this paper, we introduce a novel\nscenario termed preemptive answers, where the LLM obtains an answer before\nengaging in reasoning. This situation can arise inadvertently or induced by\nmalicious users by prompt injection attacks. Experiments reveal that preemptive\nanswers significantly impair the model's reasoning capability across various\nCoT methods and a broad spectrum of datasets. To bolster the robustness of\nreasoning, we propose two measures aimed at mitigating this issue to some\nextent.", "published": "2024-05-31 15:15:04", "link": "http://arxiv.org/abs/2405.20902v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Enhancing Vision Models for Text-Heavy Content Understanding and\n  Interaction", "abstract": "Interacting and understanding with text heavy visual content with multiple\nimages is a major challenge for traditional vision models. This paper is on\nenhancing vision models' capability to comprehend or understand and learn from\nimages containing a huge amount of textual information from the likes of\ntextbooks and research papers which contain multiple images like graphs, etc\nand tables in them with different types of axes and scales. The approach\ninvolves dataset preprocessing, fine tuning which is by using instructional\noriented data and evaluation. We also built a visual chat application\nintegrating CLIP for image encoding and a model from the Massive Text Embedding\nBenchmark which is developed to consider both textual and visual inputs. An\naccuracy of 96.71% was obtained. The aim of the project is to increase and also\nenhance the advance vision models' capabilities in understanding complex visual\ntextual data interconnected data, contributing to multimodal AI.", "published": "2024-05-31 15:17:47", "link": "http://arxiv.org/abs/2405.20906v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Learning to Estimate System Specifications in Linear Temporal Logic\n  using Transformers and Mamba", "abstract": "Temporal logic is a framework for representing and reasoning about\npropositions that evolve over time. It is commonly used for specifying\nrequirements in various domains, including hardware and software systems, as\nwell as robotics. Specification mining or formula generation involves\nextracting temporal logic formulae from system traces and has numerous\napplications, such as detecting bugs and improving interpretability. Although\nthere has been a surge of deep learning-based methods for temporal logic\nsatisfiability checking in recent years, the specification mining literature\nhas been lagging behind in adopting deep learning methods despite their many\nadvantages, such as scalability. In this paper, we introduce autoregressive\nmodels that can generate linear temporal logic formulae from traces, towards\naddressing the specification mining problem. We propose multiple architectures\nfor this task: transformer encoder-decoder, decoder-only transformer, and\nMamba, which is an emerging alternative to transformer models. Additionally, we\ndevise a metric for quantifying the distinctiveness of the generated formulae\nand a straightforward algorithm to enforce the syntax constraints. Our\nexperiments show that the proposed architectures yield promising results,\ngenerating correct and distinct formulae at a fraction of the compute cost\nneeded for the combinatorial baseline.", "published": "2024-05-31 15:21:53", "link": "http://arxiv.org/abs/2405.20917v1", "categories": ["cs.CL", "cs.LG", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Zero-Shot Next Location Predictors", "abstract": "Predicting the locations an individual will visit in the future is crucial\nfor solving many societal issues like disease diffusion and reduction of\npollution. However, next-location predictors require a significant amount of\nindividual-level information that may be scarce or unavailable in some\nscenarios (e.g., cold-start). Large Language Models (LLMs) have shown good\ngeneralization and reasoning capabilities and are rich in geographical\nknowledge, allowing us to believe that these models can act as zero-shot\nnext-location predictors. We tested more than 15 LLMs on three real-world\nmobility datasets and we found that LLMs can obtain accuracies up to 36.2%, a\nsignificant relative improvement of almost 640% when compared to other models\nspecifically designed for human mobility. We also test for data contamination\nand explored the possibility of using LLMs as text-based explainers for\nnext-location prediction, showing that, regardless of the model size, LLMs can\nexplain their decision.", "published": "2024-05-31 16:07:33", "link": "http://arxiv.org/abs/2405.20962v3", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "SaySelf: Teaching LLMs to Express Confidence with Self-Reflective\n  Rationales", "abstract": "Large language models (LLMs) often generate inaccurate or fabricated\ninformation and generally fail to indicate their confidence, which limits their\nbroader applications. Previous work elicits confidence from LLMs by direct or\nself-consistency prompting, or constructing specific datasets for supervised\nfinetuning. The prompting-based approaches have inferior performance, and the\ntraining-based approaches are limited to binary or inaccurate group-level\nconfidence estimates. In this work, we present the advanced SaySelf, a training\nframework that teaches LLMs to express more accurate fine-grained confidence\nestimates. In addition, beyond the confidence scores, SaySelf initiates the\nprocess of directing LLMs to produce self-reflective rationales that clearly\nidentify gaps in their parametric knowledge and explain their uncertainty. This\nis achieved by using an LLM to automatically summarize the uncertainties in\nspecific knowledge via natural language. The summarization is based on the\nanalysis of the inconsistency in multiple sampled reasoning chains, and the\nresulting data is utilized for supervised fine-tuning. Moreover, we utilize\nreinforcement learning with a meticulously crafted reward function to calibrate\nthe confidence estimates, motivating LLMs to deliver accurate, high-confidence\npredictions and to penalize overconfidence in erroneous outputs. Experimental\nresults in both in-distribution and out-of-distribution datasets demonstrate\nthe effectiveness of SaySelf in reducing the confidence calibration error and\nmaintaining the task performance. We show that the generated self-reflective\nrationales are reasonable and can further contribute to the calibration. The\ncode is made public at https://github.com/xu1868/SaySelf.", "published": "2024-05-31 16:21:16", "link": "http://arxiv.org/abs/2405.20974v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards a Fluid computer", "abstract": "In 1991, Moore [20] raised a question about whether hydrodynamics is capable\nof performing computations. Similarly, in 2016, Tao [25] asked whether a\nmechanical system, including a fluid flow, can simulate a universal Turing\nmachine. In this expository article, we review the construction in [8] of a\n\"Fluid computer\" in dimension 3 that combines techniques in symbolic dynamics\nwith the connection between steady Euler flows and contact geometry unveiled by\nEtnyre and Ghrist. In addition, we argue that the metric that renders the\nvector field Beltrami cannot be critical in the Chern-Hamilton sense [9]. We\nalso sketch the completely different construction for the Euclidean metric in\n$\\mathbb R^3$ as given in [7]. These results reveal the existence of\nundecidable fluid particle paths. We conclude the article with a list of open\nproblems.", "published": "2024-05-31 16:41:36", "link": "http://arxiv.org/abs/2405.20999v1", "categories": ["math.DS", "cs.CL", "math.AP", "math.SG"], "primary_category": "math.DS"}
{"title": "Improved Techniques for Optimization-Based Jailbreaking on Large\n  Language Models", "abstract": "Large language models (LLMs) are being rapidly developed, and a key component\nof their widespread deployment is their safety-related alignment. Many\nred-teaming efforts aim to jailbreak LLMs, where among these efforts, the\nGreedy Coordinate Gradient (GCG) attack's success has led to a growing interest\nin the study of optimization-based jailbreaking techniques. Although GCG is a\nsignificant milestone, its attacking efficiency remains unsatisfactory. In this\npaper, we present several improved (empirical) techniques for\noptimization-based jailbreaks like GCG. We first observe that the single target\ntemplate of \"Sure\" largely limits the attacking performance of GCG; given this,\nwe propose to apply diverse target templates containing harmful self-suggestion\nand/or guidance to mislead LLMs. Besides, from the optimization aspects, we\npropose an automatic multi-coordinate updating strategy in GCG (i.e.,\nadaptively deciding how many tokens to replace in each step) to accelerate\nconvergence, as well as tricks like easy-to-hard initialisation. Then, we\ncombine these improved technologies to develop an efficient jailbreak method,\ndubbed I-GCG. In our experiments, we evaluate on a series of benchmarks (such\nas NeurIPS 2023 Red Teaming Track). The results demonstrate that our improved\ntechniques can help GCG outperform state-of-the-art jailbreaking attacks and\nachieve nearly 100% attack success rate. The code is released at\nhttps://github.com/jiaxiaojunQAQ/I-GCG.", "published": "2024-05-31 17:07:15", "link": "http://arxiv.org/abs/2405.21018v2", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Exploratory Preference Optimization: Harnessing Implicit\n  Q*-Approximation for Sample-Efficient RLHF", "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as a central\ntool for language model alignment. We consider online exploration in RLHF,\nwhich exploits interactive access to human or AI feedback by deliberately\nencouraging the model to produce diverse, maximally informative responses. By\nallowing RLHF to confidently stray from the pre-trained model, online\nexploration offers the possibility of novel, potentially super-human\ncapabilities, but its full potential as a paradigm for language model training\nhas yet to be realized, owing to computational and statistical bottlenecks in\ndirectly adapting existing reinforcement learning techniques. We propose a new\nalgorithm for online exploration in RLHF, Exploratory Preference Optimization\n(XPO), which is simple and practical -- a one-line change to (online) Direct\nPreference Optimization (DPO; Rafailov et al., 2023) -- yet enjoys the\nstrongest known provable guarantees and promising empirical performance. XPO\naugments the DPO objective with a novel and principled exploration bonus,\nempowering the algorithm to explore outside the support of the initial model\nand human feedback data. In theory, we show that XPO is provably\nsample-efficient and converges to a near-optimal language model policy under\nnatural exploration conditions, irrespective of whether the initial model has\ngood coverage. Our analysis, which builds on the observation that DPO\nimplicitly performs a form of $Q^{\\star}$-approximation (or, Bellman error\nminimization), combines previously disparate techniques from language modeling\nand theoretical reinforcement learning in a serendipitous fashion through the\nperspective of KL-regularized Markov decision processes. Empirically, we find\nthat XPO is more sample-efficient than non-exploratory DPO variants in a\npreliminary evaluation.", "published": "2024-05-31 17:39:06", "link": "http://arxiv.org/abs/2405.21046v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Grammar-Aligned Decoding", "abstract": "Large Language Models (LLMs) struggle with reliably generating highly\nstructured outputs, such as program code, mathematical formulas, or well-formed\nmarkup. Constrained decoding approaches mitigate this problem by greedily\nrestricting what tokens an LLM can output at each step to guarantee that the\noutput matches a given constraint. Specifically, in grammar-constrained\ndecoding (GCD), the LLM's output must follow a given grammar. In this paper, we\ndemonstrate that GCD techniques (and in general constrained decoding\ntechniques) can distort the LLM's distribution, leading to outputs that are\ngrammatical but appear with likelihoods that are not proportional to the ones\ngiven by the LLM, and so ultimately are low-quality. We call the problem of\naligning sampling with a grammar constraint, grammar-aligned decoding (GAD),\nand propose adaptive sampling with approximate expected futures (ASAp), a\ndecoding algorithm that guarantees the output to be grammatical while provably\nproducing outputs that match the conditional probability of the LLM's\ndistribution conditioned on the given grammar constraint. Our algorithm uses\nprior sample outputs to soundly overapproximate the future grammaticality of\ndifferent output prefixes. Our evaluation on code generation and structured NLP\ntasks shows how ASAp often produces outputs with higher likelihood (according\nto the LLM's distribution) than existing GCD techniques, while still enforcing\nthe desired grammatical constraints.", "published": "2024-05-31 17:39:15", "link": "http://arxiv.org/abs/2405.21047v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "What Makes CLIP More Robust to Long-Tailed Pre-Training Data? A\n  Controlled Study for Transferable Insights", "abstract": "Severe data imbalance naturally exists among web-scale vision-language\ndatasets. Despite this, we find CLIP pre-trained thereupon exhibits notable\nrobustness to the data imbalance compared to supervised learning, and\ndemonstrates significant effectiveness in learning generalizable\nrepresentations. With an aim to investigate the reasons behind this finding, we\nconduct controlled experiments to study various underlying factors, and reveal\nthat CLIP's pretext task forms a dynamic classification problem wherein only a\nsubset of classes is present in training. This isolates the bias from dominant\nclasses and implicitly balances the learning signal. Furthermore, the\nrobustness and discriminability of CLIP improve with more descriptive language\nsupervision, larger data scale, and broader open-world concepts, which are\ninaccessible to supervised learning. Our study not only uncovers the mechanisms\nbehind CLIP's generalizability beyond data imbalance but also provides\ntransferable insights for the research community. The findings are validated in\nboth supervised and self-supervised learning, enabling models trained on\nimbalanced data to achieve CLIP-level performance on diverse recognition tasks.\nCode and data are available at: https://github.com/CVMI-Lab/clip-beyond-tail.", "published": "2024-05-31 17:57:24", "link": "http://arxiv.org/abs/2405.21070v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "From Unstructured Data to In-Context Learning: Exploring What Tasks Can\n  Be Learned and When", "abstract": "Large language models (LLMs) like transformers demonstrate impressive\nin-context learning (ICL) capabilities, allowing them to make predictions for\nnew tasks based on prompt exemplars without parameter updates. While existing\nICL theories often assume structured training data resembling ICL tasks (e.g.,\nx-y pairs for linear regression), LLMs are typically trained unsupervised on\nunstructured text, such as web content, which lacks clear parallels to tasks\nlike word analogy. To address this gap, we examine what enables ICL in models\ntrained on unstructured data, focusing on critical sequence model requirements\nand training data structure. We find that many ICL capabilities can emerge\nsimply from co-occurrence of semantically related word pairs in unstructured\ndata; word analogy completion, for example, can provably arise purely through\nco-occurrence modeling, using classical language models like continuous bag of\nwords (CBOW), without needing positional information or attention mechanisms.\nHowever, positional information becomes crucial for logic reasoning tasks\nrequiring generalization to unseen tokens. Finally, we identify two cases where\nICL fails: one in logic reasoning tasks that require generalizing to new,\nunseen patterns, and another in analogy completion where relevant word pairs\nappear only in fixed training positions. These findings suggest that LLMs' ICL\nabilities depend heavily on the structural elements within their training data.", "published": "2024-05-31 18:46:06", "link": "http://arxiv.org/abs/2406.00131v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Learning to Clarify: Multi-turn Conversations with Action-Based\n  Contrastive Self-Training", "abstract": "Large language models (LLMs) aligned through reinforcement learning from\nhuman feedback (RLHF) have quickly become one of the dominant paradigms for\nbuilding intelligent conversational assistant agents. However, despite their\nstrong performance across many benchmarks, LLM-based agents still lack\nconversational skills such as disambiguation: when generalized assistants are\nfaced with ambiguity, they often overhedge or implicitly guess users'\nground-truth intents rather than asking clarification questions, and under\ntask-specific settings, high-quality conversation samples are often limited,\naffecting models' ability to learn optimal dialogue action policies. We propose\nAction-Based Contrastive Self-Training (henceforth ACT), a quasi-online\npreference optimization algorithm based on Direct Preference Optimization (DPO)\nwhich allows for sample-efficient dialogue policy learning in multi-turn\nconversation. We demonstrate ACT's efficacy under sample-efficient conditions\nin three difficult conversational tasks: tabular-grounded question-answering,\nmachine reading comprehension, and AmbigSQL, a novel task for disambiguating\ninformation-seeking requests for text-to-SQL generation. Additionally, we\npropose evaluating LLMs' ability to function as conversational agents by\nexamining whether they can implicitly recognize and reason about ambiguity in\nconversation. ACT demonstrates substantial conversation modeling improvements\nover standard approaches to supervised fine-tuning and DPO.", "published": "2024-05-31 22:44:48", "link": "http://arxiv.org/abs/2406.00222v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM-RankFusion: Mitigating Intrinsic Inconsistency in LLM-based Ranking", "abstract": "Ranking passages by prompting a large language model (LLM) can achieve\npromising performance in modern information retrieval (IR) systems. A common\napproach to sort the ranking list is by prompting LLMs for a pairwise or\nsetwise comparison which often relies on sorting algorithms. However,\nsorting-based methods require consistent comparisons to correctly sort the\npassages, which we show that LLMs often violate. We identify two kinds of\nintrinsic inconsistency in LLM-based pairwise comparisons: order inconsistency\nwhich leads to conflicting results when switching the passage order, and\ntransitive inconsistency which leads to non-transitive triads among all\npreference pairs. Our study of these inconsistencies is relevant for\nunderstanding and improving the stability of any ranking scheme based on\nrelative preferences. In this paper, we propose LLM-RankFusion, an LLM-based\nranking framework that mitigates these inconsistencies and produces a robust\nranking list. LLM-RankFusion mitigates order inconsistency using in-context\nlearning (ICL) to demonstrate order-agnostic comparisons and calibration to\nestimate the underlying preference probability between two passages. We then\naddress transitive inconsistency by aggregating the ranking results from\nmultiple rankers. In our experiments, we empirically show that LLM-RankFusion\ncan significantly reduce inconsistent comparison results, improving the ranking\nquality by making the final ranking list more robust. Our code is available at\n\\href{https://github.com/XHMY/LLM-RankFusion}{https://github.com/XHMY/LLM-RankFusion}", "published": "2024-05-31 23:29:42", "link": "http://arxiv.org/abs/2406.00231v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "LOLAMEME: Logic, Language, Memory, Mechanistic Framework", "abstract": "The performance of Large Language Models has achieved superhuman breadth with\nunprecedented depth. At the same time, the language models are mostly black box\nmodels and the underlying mechanisms for performance have been evaluated using\nsynthetic or mechanistic schemes. We extend current mechanistic schemes to\nincorporate Logic, memory, and nuances of Language such as latent structure.\nThe proposed framework is called LOLAMEME and we provide two instantiations of\nLOLAMEME: LoLa and MeMe languages. We then consider two generative language\nmodel architectures: transformer-based GPT-2 and convolution-based Hyena. We\npropose the hybrid architecture T HEX and use LOLAMEME framework is used to\ncompare three architectures. T HEX outperforms GPT-2 and Hyena on select tasks.", "published": "2024-05-31 21:18:25", "link": "http://arxiv.org/abs/2406.02592v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "SocialNLP Fake-EmoReact 2021 Challenge Overview: Predicting Fake Tweets\n  from Their Replies and GIFs", "abstract": "This paper provides an overview of the Fake-EmoReact 2021 Challenge, held at\nthe 9th SocialNLP Workshop, in conjunction with NAACL 2021. The challenge\nrequires predicting the authenticity of tweets using reply context and\naugmented GIF categories from EmotionGIF dataset. We offer the Fake-EmoReact\ndataset with more than 453k as the experimental materials, where every tweet is\nlabeled with authenticity. Twenty-four teams registered to participate in this\nchallenge, and 5 submitted their results successfully in the evaluation phase.\nThe best team achieves 93.9 on Fake-EmoReact 2021 dataset using F1 score. In\naddition, we show the definition of share task, data collection, and the teams'\nperformance that joined this challenge and their approaches.", "published": "2024-05-31 21:14:11", "link": "http://arxiv.org/abs/2406.04368v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Exfiltration of personal information from ChatGPT via prompt injection", "abstract": "We report that ChatGPT 4 and 4o are susceptible to a prompt injection attack\nthat allows an attacker to exfiltrate users' personal data. It is applicable\nwithout the use of any 3rd party tools and all users are currently affected.\nThis vulnerability is exacerbated by the recent introduction of ChatGPT's\nmemory feature, which allows an attacker to command ChatGPT to monitor the user\nfor the desired personal data.", "published": "2024-05-31 21:21:19", "link": "http://arxiv.org/abs/2406.00199v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY", "cs.ET"], "primary_category": "cs.CR"}
{"title": "Very Low Complexity Speech Synthesis Using Framewise Autoregressive GAN\n  (FARGAN) with Pitch Prediction", "abstract": "Neural vocoders are now being used in a wide range of speech processing\napplications. In many of those applications, the vocoder can be the most\ncomplex component, so finding lower complexity algorithms can lead to\nsignificant practical benefits. In this work, we propose FARGAN, an\nautoregressive vocoder that takes advantage of long-term pitch prediction to\nsynthesize high-quality speech in small subframes, without the need for\nteacher-forcing. Experimental results show that the proposed 600~MFLOPS FARGAN\nvocoder can achieve both higher quality and lower complexity than existing\nlow-complexity vocoders. The quality even matches that of existing\nhigher-complexity vocoders.", "published": "2024-05-31 17:56:37", "link": "http://arxiv.org/abs/2405.21069v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Survey of Deep Learning Audio Generation Methods", "abstract": "This article presents a review of typical techniques used in three distinct\naspects of deep learning model development for audio generation. In the first\npart of the article, we provide an explanation of audio representations,\nbeginning with the fundamental audio waveform. We then progress to the\nfrequency domain, with an emphasis on the attributes of human hearing, and\nfinally introduce a relatively recent development. The main part of the article\nfocuses on explaining basic and extended deep learning architecture variants,\nalong with their practical applications in the field of audio generation. The\nfollowing architectures are addressed: 1) Autoencoders 2) Generative\nadversarial networks 3) Normalizing flows 4) Transformer networks 5) Diffusion\nmodels. Lastly, we will examine four distinct evaluation metrics that are\ncommonly employed in audio generation. This article aims to offer novice\nreaders and beginners in the field a comprehensive understanding of the current\nstate of the art in audio generation methods as well as relevant studies that\ncan be explored for future research.", "published": "2024-05-31 19:20:27", "link": "http://arxiv.org/abs/2406.00146v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
