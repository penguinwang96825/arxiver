{"title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization", "abstract": "Graphs that capture relations between textual units have great benefits for\ndetecting salient information from multiple documents and generating overall\ncoherent summaries. In this paper, we develop a neural abstractive\nmulti-document summarization (MDS) model which can leverage well-known graph\nrepresentations of documents such as similarity graph and discourse graph, to\nmore effectively process multiple input documents and produce abstractive\nsummaries. Our model utilizes graphs to encode documents in order to capture\ncross-document relations, which is crucial to summarizing long documents. Our\nmodel can also take advantage of graphs to guide the summary generation\nprocess, which is beneficial for generating coherent and concise summaries.\nFurthermore, pre-trained language models can be easily combined with our model,\nwhich further improve the summarization performance significantly. Empirical\nresults on the WikiSum and MultiNews dataset show that the proposed\narchitecture brings substantial improvements over several strong baselines.", "published": "2020-05-20 13:39:47", "link": "http://arxiv.org/abs/2005.10043v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Large-Scale Multi-Document Summarization Dataset from the Wikipedia\n  Current Events Portal", "abstract": "Multi-document summarization (MDS) aims to compress the content in large\ndocument collections into short summaries and has important applications in\nstory clustering for newsfeeds, presentation of search results, and timeline\ngeneration. However, there is a lack of datasets that realistically address\nsuch use cases at a scale large enough for training supervised models for this\ntask. This work presents a new dataset for MDS that is large both in the total\nnumber of document clusters and in the size of individual clusters. We build\nthis dataset by leveraging the Wikipedia Current Events Portal (WCEP), which\nprovides concise and neutral human-written summaries of news events, with links\nto external source articles. We also automatically extend these source articles\nby looking for related articles in the Common Crawl archive. We provide a\nquantitative analysis of the dataset and empirical results for several\nstate-of-the-art MDS techniques.", "published": "2020-05-20 14:33:33", "link": "http://arxiv.org/abs/2005.10070v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Examining the State-of-the-Art in News Timeline Summarization", "abstract": "Previous work on automatic news timeline summarization (TLS) leaves an\nunclear picture about how this task can generally be approached and how well it\nis currently solved. This is mostly due to the focus on individual subtasks,\nsuch as date selection and date summarization, and to the previous lack of\nappropriate evaluation metrics for the full TLS task. In this paper, we compare\ndifferent TLS strategies using appropriate evaluation frameworks, and propose a\nsimple and effective combination of methods that improves over the\nstate-of-the-art on all tested benchmarks. For a more robust evaluation, we\nalso present a new TLS dataset, which is larger and spans longer time periods\nthan previous datasets.", "published": "2020-05-20 15:06:39", "link": "http://arxiv.org/abs/2005.10107v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Applying the Transformer to Character-level Transduction", "abstract": "The transformer has been shown to outperform recurrent neural network-based\nsequence-to-sequence models in various word-level NLP tasks. Yet for\ncharacter-level transduction tasks, e.g. morphological inflection generation\nand historical text normalization, there are few works that outperform\nrecurrent models using the transformer. In an empirical study, we uncover that,\nin contrast to recurrent sequence-to-sequence models, the batch size plays a\ncrucial role in the performance of the transformer on character-level tasks,\nand we show that with a large enough batch size, the transformer does indeed\noutperform recurrent models. We also introduce a simple technique to handle\nfeature-guided character-level transduction that further improves performance.\nWith these insights, we achieve state-of-the-art performance on morphological\ninflection and historical text normalization. We also show that the transformer\noutperforms a strong baseline on two other character-level transduction tasks:\ngrapheme-to-phoneme conversion and transliteration.", "published": "2020-05-20 17:25:43", "link": "http://arxiv.org/abs/2005.10213v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence level estimation of psycholinguistic norms using joint\n  multidimensional annotations", "abstract": "Psycholinguistic normatives represent various affective and mental constructs\nusing numeric scores and are used in a variety of applications in natural\nlanguage processing. They are commonly used at the sentence level, the scores\nof which are estimated by extrapolating word level scores using simple\naggregation strategies, which may not always be optimal. In this work, we\npresent a novel approach to estimate the psycholinguistic norms at sentence\nlevel. We apply a multidimensional annotation fusion model on annotations at\nthe word level to estimate a parameter which captures relationships between\ndifferent norms. We then use this parameter at sentence level to estimate the\nnorms. We evaluate our approach by predicting sentence level scores for various\nnormative dimensions and compare with standard word aggregation schemes.", "published": "2020-05-20 17:47:56", "link": "http://arxiv.org/abs/2005.10232v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is MAP Decoding All You Need? The Inadequacy of the Mode in Neural\n  Machine Translation", "abstract": "Recent studies have revealed a number of pathologies of neural machine\ntranslation (NMT) systems. Hypotheses explaining these mostly suggest there is\nsomething fundamentally wrong with NMT as a model or its training algorithm,\nmaximum likelihood estimation (MLE). Most of this evidence was gathered using\nmaximum a posteriori (MAP) decoding, a decision rule aimed at identifying the\nhighest-scoring translation, i.e. the mode. We argue that the evidence\ncorroborates the inadequacy of MAP decoding more than casts doubt on the model\nand its training algorithm. In this work, we show that translation\ndistributions do reproduce various statistics of the data well, but that beam\nsearch strays from such statistics. We show that some of the known pathologies\nand biases of NMT are due to MAP decoding and not to NMT's statistical\nassumptions nor MLE. In particular, we show that the most likely translations\nunder the model accumulate so little probability mass that the mode can be\nconsidered essentially arbitrary. We therefore advocate for the use of decision\nrules that take into account the translation distribution holistically. We show\nthat an approximation to minimum Bayes risk decoding gives competitive results\nconfirming that NMT models do capture important aspects of translation well in\nexpectation.", "published": "2020-05-20 18:05:51", "link": "http://arxiv.org/abs/2005.10283v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ScriptWriter: Narrative-Guided Script Generation", "abstract": "It is appealing to have a system that generates a story or scripts\nautomatically from a story-line, even though this is still out of our reach. In\ndialogue systems, it would also be useful to drive dialogues by a dialogue\nplan. In this paper, we address a key problem involved in these applications --\nguiding a dialogue by a narrative. The proposed model ScriptWriter selects the\nbest response among the candidates that fit the context as well as the given\nnarrative. It keeps track of what in the narrative has been said and what is to\nbe said. A narrative plays a different role than the context (i.e., previous\nutterances), which is generally used in current dialogue systems. Due to the\nunavailability of data for this new application, we construct a new large-scale\ndata collection GraphMovie from a movie website where end-users can upload\ntheir narratives freely when watching a movie. Experimental results on the\ndataset show that our proposed approach based on narratives significantly\noutperforms the baselines that simply use the narrative as a kind of context.", "published": "2020-05-20 19:48:50", "link": "http://arxiv.org/abs/2005.10331v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pretraining with Contrastive Sentence Objectives Improves Discourse\n  Performance of Language Models", "abstract": "Recent models for unsupervised representation learning of text have employed\na number of techniques to improve contextual word representations but have put\nlittle focus on discourse-level representations. We propose CONPONO, an\ninter-sentence objective for pretraining language models that models discourse\ncoherence and the distance between sentences. Given an anchor sentence, our\nmodel is trained to predict the text k sentences away using a sampled-softmax\nobjective where the candidates consist of neighboring sentences and sentences\nrandomly sampled from the corpus. On the discourse representation benchmark\nDiscoEval, our model improves over the previous state-of-the-art by up to 13%\nand on average 4% absolute across 7 tasks. Our model is the same size as\nBERT-Base, but outperforms the much larger BERT- Large model and other more\nrecent approaches that incorporate discourse. We also show that CONPONO yields\ngains of 2%-6% absolute even for tasks that do not explicitly evaluate\ndiscourse: textual entailment (RTE), common sense reasoning (COPA) and reading\ncomprehension (ReCoRD).", "published": "2020-05-20 23:21:43", "link": "http://arxiv.org/abs/2005.10389v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Computational Analysis of Polarization on Indian and Pakistani Social\n  Media", "abstract": "Between February 14, 2019 and March 4, 2019, a terrorist attack in Pulwama,\nKashmir followed by retaliatory airstrikes led to rising tensions between India\nand Pakistan, two nuclear-armed countries. In this work, we examine polarizing\nmessaging on Twitter during these events, particularly focusing on the\npositions of Indian and Pakistani politicians. We use a label propagation\ntechnique focused on hashtag co-occurrences to find polarizing tweets and\nusers. Our analysis reveals that politicians in the ruling political party in\nIndia (BJP) used polarized hashtags and called for escalation of conflict more\nso than politicians from other parties. Our work offers the first analysis of\nhow escalating tensions between India and Pakistan manifest on Twitter and\nprovides a framework for studying polarizing messages.", "published": "2020-05-20 00:44:38", "link": "http://arxiv.org/abs/2005.09803v2", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Positive emotions help rank negative reviews in e-commerce", "abstract": "Negative reviews, the poor ratings in postpurchase evaluation, play an\nindispensable role in e-commerce, especially in shaping future sales and firm\nequities. However, extant studies seldom examine their potential value for\nsellers and producers in enhancing capabilities of providing better services\nand products. For those who exploited the helpfulness of reviews in the view of\ne-commerce keepers, the ranking approaches were developed for customers\ninstead. To fill this gap, in terms of combining description texts and emotion\npolarities, the aim of the ranking method in this study is to provide the most\nhelpful negative reviews under a certain product attribute for online sellers\nand producers. By applying a more reasonable evaluating procedure, experts with\nrelated backgrounds are hired to vote for the ranking approaches. Our ranking\nmethod turns out to be more reliable for ranking negative reviews for sellers\nand producers, demonstrating a better performance than the baselines like BM25\nwith a result of 8% higher. In this paper, we also enrich the previous\nunderstandings of emotions in valuing reviews. Specifically, it is surprisingly\nfound that positive emotions are more helpful rather than negative emotions in\nranking negative reviews. The unexpected strengthening from positive emotions\nin ranking suggests that less polarized reviews on negative experience in fact\noffer more rational feedbacks and thus more helpfulness to the sellers and\nproducers. The presented ranking method could provide e-commerce practitioners\nwith an efficient and effective way to leverage negative reviews from online\nconsumers.", "published": "2020-05-20 03:34:20", "link": "http://arxiv.org/abs/2005.09837v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "GM-CTSC at SemEval-2020 Task 1: Gaussian Mixtures Cross Temporal\n  Similarity Clustering", "abstract": "This paper describes the system proposed for the SemEval-2020 Task 1:\nUnsupervised Lexical Semantic Change Detection. We focused our approach on the\ndetection problem. Given the semantics of words captured by temporal word\nembeddings in different time periods, we investigate the use of unsupervised\nmethods to detect when the target word has gained or loosed senses. To this\nend, we defined a new algorithm based on Gaussian Mixture Models to cluster the\ntarget similarities computed over the two periods. We compared the proposed\napproach with a number of similarity-based thresholds. We found that, although\nthe performance of the detection methods varies across the word embedding\nalgorithms, the combination of Gaussian Mixture with Temporal Referencing\nresulted in our best system.", "published": "2020-05-20 10:14:01", "link": "http://arxiv.org/abs/2005.09946v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Word Embeddings with Knowledge Extracted from Lexical\n  Resources", "abstract": "In this work, we present an effective method for semantic specialization of\nword vector representations. To this end, we use traditional word embeddings\nand apply specialization methods to better capture semantic relations between\nwords. In our approach, we leverage external knowledge from rich lexical\nresources such as BabelNet. We also show that our proposed post-specialization\nmethod based on an adversarial neural network with the Wasserstein distance\nallows to gain improvements over state-of-the-art methods on two tasks: word\nsimilarity and dialog state tracking.", "published": "2020-05-20 13:45:49", "link": "http://arxiv.org/abs/2005.10048v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BERTweet: A pre-trained language model for English Tweets", "abstract": "We present BERTweet, the first public large-scale pre-trained language model\nfor English Tweets. Our BERTweet, having the same architecture as BERT-base\n(Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu\net al., 2019). Experiments show that BERTweet outperforms strong baselines\nRoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better\nperformance results than the previous state-of-the-art models on three Tweet\nNLP tasks: Part-of-speech tagging, Named-entity recognition and text\nclassification. We release BERTweet under the MIT License to facilitate future\nresearch and applications on Tweet data. Our BERTweet is available at\nhttps://github.com/VinAIResearch/BERTweet", "published": "2020-05-20 17:05:57", "link": "http://arxiv.org/abs/2005.10200v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BlaBla: Linguistic Feature Extraction for Clinical Analysis in Multiple\n  Languages", "abstract": "We introduce BlaBla, an open-source Python library for extracting linguistic\nfeatures with proven clinical relevance to neurological and psychiatric\ndiseases across many languages. BlaBla is a unifying framework for accelerating\nand simplifying clinical linguistic research. The library is built on\nstate-of-the-art NLP frameworks and supports multithreaded/GPU-enabled feature\nextraction via both native Python calls and a command line interface. We\ndescribe BlaBla's architecture and clinical validation of its features across\n12 diseases. We further demonstrate the application of BlaBla to a task\nvisualizing and classifying language disorders in three languages on real\nclinical data from the AphasiaBank dataset. We make the codebase freely\navailable to researchers with the hope of providing a consistent,\nwell-validated foundation for the next generation of clinical linguistic\nresearch.", "published": "2020-05-20 17:31:35", "link": "http://arxiv.org/abs/2005.10219v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PyChain: A Fully Parallelized PyTorch Implementation of LF-MMI for\n  End-to-End ASR", "abstract": "We present PyChain, a fully parallelized PyTorch implementation of end-to-end\nlattice-free maximum mutual information (LF-MMI) training for the so-called\n\\emph{chain models} in the Kaldi automatic speech recognition (ASR) toolkit.\nUnlike other PyTorch and Kaldi based ASR toolkits, PyChain is designed to be as\nflexible and light-weight as possible so that it can be easily plugged into new\nASR projects, or other existing PyTorch-based ASR tools, as exemplified\nrespectively by a new project PyChain-example, and Espresso, an existing\nend-to-end ASR toolkit. PyChain's efficiency and flexibility is demonstrated\nthrough such novel features as full GPU training on numerator/denominator\ngraphs, and support for unequal length sequences. Experiments on the WSJ\ndataset show that with simple neural networks and commonly used machine\nlearning techniques, PyChain can achieve competitive results that are\ncomparable to Kaldi and better than other end-to-end ASR systems.", "published": "2020-05-20 02:10:21", "link": "http://arxiv.org/abs/2005.09824v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Further Study of Unsupervised Pre-training for Transformer Based\n  Speech Recognition", "abstract": "Building a good speech recognition system usually requires large amounts of\ntranscribed data, which is expensive to collect. To tackle this problem, many\nunsupervised pre-training methods have been proposed. Among these methods,\nMasked Predictive Coding achieved significant improvements on various speech\nrecognition datasets with BERT-like Masked Reconstruction loss and Transformer\nbackbone. However, many aspects of MPC have not been fully investigated. In\nthis paper, we conduct a further study on MPC and focus on three important\naspects: the effect of pre-training data speaking style, its extension on\nstreaming model, and how to better transfer learned knowledge from pre-training\nstage to downstream tasks. Experiments reveled that pre-training data with\nmatching speaking style is more useful on downstream recognition tasks. A\nunified training objective with APC and MPC provided 8.46% relative error\nreduction on streaming model trained on HKUST. Also, the combination of target\ndata adaption and layer-wise discriminative training helped the knowledge\ntransfer of MPC, which achieved 3.99% relative error reduction on AISHELL over\na strong baseline.", "published": "2020-05-20 06:22:29", "link": "http://arxiv.org/abs/2005.09862v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-End Speaker Diarization for an Unknown Number of Speakers with\n  Encoder-Decoder Based Attractors", "abstract": "End-to-end speaker diarization for an unknown number of speakers is addressed\nin this paper. Recently proposed end-to-end speaker diarization outperformed\nconventional clustering-based speaker diarization, but it has one drawback: it\nis less flexible in terms of the number of speakers. This paper proposes a\nmethod for encoder-decoder based attractor calculation (EDA), which first\ngenerates a flexible number of attractors from a speech embedding sequence.\nThen, the generated multiple attractors are multiplied by the speech embedding\nsequence to produce the same number of speaker activities. The speech embedding\nsequence is extracted using the conventional self-attentive end-to-end neural\nspeaker diarization (SA-EEND) network. In a two-speaker condition, our method\nachieved a 2.69 % diarization error rate (DER) on simulated mixtures and a 8.07\n% DER on the two-speaker subset of CALLHOME, while vanilla SA-EEND attained\n4.56 % and 9.54 %, respectively. In unknown numbers of speakers conditions, our\nmethod attained a 15.29 % DER on CALLHOME, while the x-vector-based clustering\nmethod achieved a 19.43 % DER.", "published": "2020-05-20 09:08:41", "link": "http://arxiv.org/abs/2005.09921v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Relative Positional Encoding for Speech Recognition and Direct\n  Translation", "abstract": "Transformer models are powerful sequence-to-sequence architectures that are\ncapable of directly mapping speech inputs to transcriptions or translations.\nHowever, the mechanism for modeling positions in this model was tailored for\ntext modeling, and thus is less ideal for acoustic inputs. In this work, we\nadapt the relative position encoding scheme to the Speech Transformer, where\nthe key addition is relative distance between input states in the\nself-attention network. As a result, the network can better adapt to the\nvariable distributions present in speech data. Our experiments show that our\nresulting model achieves the best recognition result on the Switchboard\nbenchmark in the non-augmentation condition, and the best published result in\nthe MuST-C speech translation benchmark. We also show that this model is able\nto better utilize synthetic data than the Transformer, and adapts better to\nvariable sentence segmentation quality for speech translation.", "published": "2020-05-20 09:53:06", "link": "http://arxiv.org/abs/2005.09940v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Artificial Intelligence versus Maya Angelou: Experimental evidence that\n  people cannot differentiate AI-generated from human-written poetry", "abstract": "The release of openly available, robust natural language generation\nalgorithms (NLG) has spurred much public attention and debate. One reason lies\nin the algorithms' purported ability to generate human-like text across various\ndomains. Empirical evidence using incentivized tasks to assess whether people\n(a) can distinguish and (b) prefer algorithm-generated versus human-written\ntext is lacking. We conducted two experiments assessing behavioral reactions to\nthe state-of-the-art Natural Language Generation algorithm GPT-2 (Ntotal =\n830). Using the identical starting lines of human poems, GPT-2 produced samples\nof poems. From these samples, either a random poem was chosen\n(Human-out-of-the-loop) or the best one was selected (Human-in-the-loop) and in\nturn matched with a human-written poem. In a new incentivized version of the\nTuring Test, participants failed to reliably detect the\nalgorithmically-generated poems in the Human-in-the-loop treatment, yet\nsucceeded in the Human-out-of-the-loop treatment. Further, people reveal a\nslight aversion to algorithm-generated poetry, independent on whether\nparticipants were informed about the algorithmic origin of the poem\n(Transparency) or not (Opacity). We discuss what these results convey about the\nperformance of NLG algorithms to produce human-like text and propose\nmethodologies to study such learning algorithms in human-agent experimental\nsettings.", "published": "2020-05-20 11:52:28", "link": "http://arxiv.org/abs/2005.09980v2", "categories": ["cs.AI", "cs.CL", "econ.GN", "q-fin.EC"], "primary_category": "cs.AI"}
{"title": "On embedding Lambek calculus into commutative categorial grammars", "abstract": "We consider tensor grammars, which are an example of \\commutative\" grammars,\nbased on the classical (rather than intuitionistic) linear logic. They can be\nseen as a surface representation of abstract categorial grammars ACG in the\nsense that derivations of ACG translate to derivations of tensor grammars and\nthis translation is isomorphic on the level of string languages. The basic\ningredient are tensor terms, which can be seen as encoding and generalizing\nproof-nets. Using tensor terms makes the syntax extremely simple and a direct\ngeometric meaning becomes transparent. Then we address the problem of encoding\nnoncommutative operations in our setting. This turns out possible after\nenriching the system with new unary operators. The resulting system allows\nrepresenting both ACG and Lambek grammars as conservative fragments, while the\nformalism remains, as it seems to us, rather simple and intuitive.", "published": "2020-05-20 14:08:56", "link": "http://arxiv.org/abs/2005.10058v4", "categories": ["math.LO", "cs.CL", "cs.LO"], "primary_category": "math.LO"}
{"title": "Investigation of Large-Margin Softmax in Neural Language Modeling", "abstract": "To encourage intra-class compactness and inter-class separability among\ntrainable feature vectors, large-margin softmax methods are developed and\nwidely applied in the face recognition community. The introduction of the\nlarge-margin concept into the softmax is reported to have good properties such\nas enhanced discriminative power, less overfitting and well-defined geometric\nintuitions. Nowadays, language modeling is commonly approached with neural\nnetworks using softmax and cross entropy. In this work, we are curious to see\nif introducing large-margins to neural language models would improve the\nperplexity and consequently word error rate in automatic speech recognition.\nSpecifically, we first implement and test various types of conventional margins\nfollowing the previous works in face recognition. To address the distribution\nof natural language data, we then compare different strategies for word vector\nnorm-scaling. After that, we apply the best norm-scaling setup in combination\nwith various margins and conduct neural language models rescoring experiments\nin automatic speech recognition. We find that although perplexity is slightly\ndeteriorated, neural language models with large-margin softmax can yield word\nerror rate similar to that of the standard softmax baseline. Finally, expected\nmargins are analyzed through visualization of word vectors, showing that the\nsyntactic and semantic relationships are also preserved.", "published": "2020-05-20 14:53:19", "link": "http://arxiv.org/abs/2005.10089v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Comparison of Label-Synchronous and Frame-Synchronous End-to-End\n  Models for Speech Recognition", "abstract": "End-to-end models are gaining wider attention in the field of automatic\nspeech recognition (ASR). One of their advantages is the simplicity of building\nthat directly recognizes the speech frame sequence into the text label sequence\nby neural networks. According to the driving end in the recognition process,\nend-to-end ASR models could be categorized into two types: label-synchronous\nand frame-synchronous, each of which has unique model behaviour and\ncharacteristic. In this work, we make a detailed comparison on a representative\nlabel-synchronous model (transformer) and a soft frame-synchronous model\n(continuous integrate-and-fire (CIF) based model). The results on three public\ndataset and a large-scale dataset with 12000 hours of training data show that\nthe two types of models have respective advantages that are consistent with\ntheir synchronous mode.", "published": "2020-05-20 15:10:35", "link": "http://arxiv.org/abs/2005.10113v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Investigation of learning abilities on linguistic features in\n  sequence-to-sequence text-to-speech synthesis", "abstract": "Neural sequence-to-sequence text-to-speech synthesis (TTS) can produce\nhigh-quality speech directly from text or simple linguistic features such as\nphonemes. Unlike traditional pipeline TTS, the neural sequence-to-sequence TTS\ndoes not require manually annotated and complicated linguistic features such as\npart-of-speech tags and syntactic structures for system training. However, it\nmust be carefully designed and well optimized so that it can implicitly extract\nuseful linguistic features from the input features. In this paper we\ninvestigate under what conditions the neural sequence-to-sequence TTS can work\nwell in Japanese and English along with comparisons with deep neural network\n(DNN) based pipeline TTS systems. Unlike past comparative studies, the pipeline\nsystems also use autoregressive probabilistic modeling and a neural vocoder. We\ninvestigated systems from three aspects: a) model architecture, b) model\nparameter size, and c) language. For the model architecture aspect, we adopt\nmodified Tacotron systems that we previously proposed and their variants using\nan encoder from Tacotron or Tacotron2. For the model parameter size aspect, we\ninvestigate two model parameter sizes. For the language aspect, we conduct\nlistening tests in both Japanese and English to see if our findings can be\ngeneralized across languages. Our experiments suggest that a) a neural\nsequence-to-sequence TTS system should have a sufficient number of model\nparameters to produce high quality speech, b) it should also use a powerful\nencoder when it takes characters as inputs, and c) the encoder still has a room\nfor improvement and needs to have an improved architecture to learn\nsupra-segmental features more appropriately.", "published": "2020-05-20 23:26:14", "link": "http://arxiv.org/abs/2005.10390v2", "categories": ["eess.AS", "cs.CL", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Early Stage LM Integration Using Local and Global Log-Linear Combination", "abstract": "Sequence-to-sequence models with an implicit alignment mechanism (e.g.\nattention) are closing the performance gap towards traditional hybrid hidden\nMarkov models (HMM) for the task of automatic speech recognition. One important\nfactor to improve word error rate in both cases is the use of an external\nlanguage model (LM) trained on large text-only corpora. Language model\nintegration is straightforward with the clear separation of acoustic model and\nlanguage model in classical HMM-based modeling. In contrast, multiple\nintegration schemes have been proposed for attention models. In this work, we\npresent a novel method for language model integration into implicit-alignment\nbased sequence-to-sequence models. Log-linear model combination of acoustic and\nlanguage model is performed with a per-token renormalization. This allows us to\ncompute the full normalization term efficiently both in training and in\ntesting. This is compared to a global renormalization scheme which is\nequivalent to applying shallow fusion in training. The proposed methods show\ngood improvements over standard model combination (shallow fusion) on our\nstate-of-the-art Librispeech system. Furthermore, the improvements are\npersistent even if the LM is exchanged for a more powerful one after training.", "published": "2020-05-20 13:49:55", "link": "http://arxiv.org/abs/2005.10049v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Graph-based, Self-Supervised Program Repair from Diagnostic Feedback", "abstract": "We consider the problem of learning to repair programs from diagnostic\nfeedback (e.g., compiler error messages). Program repair is challenging for two\nreasons: First, it requires reasoning and tracking symbols across source code\nand diagnostic feedback. Second, labeled datasets available for program repair\nare relatively small. In this work, we propose novel solutions to these two\nchallenges. First, we introduce a program-feedback graph, which connects\nsymbols relevant to program repair in source code and diagnostic feedback, and\nthen apply a graph neural network on top to model the reasoning process.\nSecond, we present a self-supervised learning paradigm for program repair that\nleverages unlabeled programs available online to create a large amount of extra\nprogram repair examples, which we use to pre-train our models. We evaluate our\nproposed approach on two applications: correcting introductory programming\nassignments (DeepFix dataset) and correcting the outputs of program synthesis\n(SPoC dataset). Our final system, DrRepair, significantly outperforms prior\nwork, achieving 68.2% full repair rate on DeepFix (+22.9% over the prior best),\nand 48.4% synthesis success rate on SPoC (+3.7% over the prior best).", "published": "2020-05-20 07:24:28", "link": "http://arxiv.org/abs/2005.10636v2", "categories": ["cs.SE", "cs.CL", "cs.LG", "cs.PL", "stat.ML"], "primary_category": "cs.SE"}
{"title": "Jointly optimal denoising, dereverberation, and source separation", "abstract": "This paper proposes methods that can optimize a Convolutional BeamFormer\n(CBF) for jointly performing denoising, dereverberation, and source separation\n(DN+DR+SS) in a computationally efficient way. Conventionally, cascade\nconfiguration composed of a Weighted Prediction Error minimization (WPE)\ndereverberation filter followed by a Minimum Variance Distortionless Response\nbeamformer has been usedas the state-of-the-art frontend of far-field speech\nrecognition, however, overall optimality of this approach is not guaranteed. In\nthe blind signal processing area, an approach for jointly optimizing\ndereverberation and source separation (DR+SS) has been proposed, however, this\napproach requires huge computing cost, and has not been extended for\napplication to DN+DR+SS. To overcome the above limitations, this paper develops\nnew approaches for jointly optimizing DN+DR+SS in a computationally much more\nefficient way. To this end, we first present an objective function to optimize\na CBF for performing DN+DR+SS based on the maximum likelihood estimation, on an\nassumption that the steering vectors of the target signals are given or can be\nestimated, e.g., using a neural network. This paper refers to a CBF optimized\nby this objective function as a weighted Minimum-Power Distortionless Response\n(wMPDR) CBF. Then, we derive two algorithms for optimizing a wMPDR CBF based on\ntwo different ways of factorizing a CBF into WPE filters and beamformers.\nExperiments using noisy reverberant sound mixtures show that the proposed\noptimization approaches greatly improve the performance of the speech\nenhancement in comparison with the conventional cascade configuration in terms\nof the signal distortion measures and ASR performance. It is also shown that\nthe proposed approaches can greatly reduce the computing cost with improved\nestimation accuracy in comparison with the conventional joint optimization\napproach.", "published": "2020-05-20 04:38:26", "link": "http://arxiv.org/abs/2005.09843v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Statistical and Neural Network Based Speech Activity Detection in\n  Non-Stationary Acoustic Environments", "abstract": "Speech activity detection (SAD), which often rests on the fact that the noise\nis \"more\" stationary than speech, is particularly challenging in non-stationary\nenvironments, because the time variance of the acoustic scene makes it\ndifficult to discriminate speech from noise. We propose two approaches to SAD,\nwhere one is based on statistical signal processing, while the other utilizes\nneural networks. The former employes sophisticated signal processing to track\nthe noise and speech energies and is meant to support the case for a resource\nefficient, unsupervised signal processing approach. The latter introduces a\nrecurrent network layer that operates on short segments of the input speech to\ndo temporal smoothing in the presence of non-stationary noise. The systems are\ntested on the Fearless Steps challenge, which consists of the transmission data\nfrom the Apollo-11 space mission. The statistical SAD achieves comparable\ndetection performance to earlier proposed neural network based SADs, while the\nneural network based approach leads to a decision cost function of 1.07% on the\nevaluation set of the 2020 Fearless Steps Challenge, which sets a new state of\nthe art.", "published": "2020-05-20 08:45:00", "link": "http://arxiv.org/abs/2005.09913v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SADDEL: Joint Speech Separation and Denoising Model based on Multitask\n  Learning", "abstract": "Speech data collected in real-world scenarios often encounters two issues.\nFirst, multiple sources may exist simultaneously, and the number of sources may\nvary with time. Second, the existence of background noise in recording is\ninevitable. To handle the first issue, we refer to speech separation\napproaches, that separate speech from an unknown number of speakers. To address\nthe second issue, we refer to speech denoising approaches, which remove noise\ncomponents and retrieve pure speech signals. Numerous deep learning based\nmethods for speech separation and denoising have been proposed that show\npromising results. However, few works attempt to address the issues\nsimultaneously, despite speech separation and denoising tasks having similar\nnature. In this study, we propose a joint speech separation and denoising\nframework based on the multitask learning criterion to tackle the two issues\nsimultaneously. The experimental results show that the proposed framework not\nonly performs well on both speech separation and denoising tasks, but also\noutperforms related methods in most conditions.", "published": "2020-05-20 11:13:35", "link": "http://arxiv.org/abs/2005.09966v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Evaluating Features and Metrics for High-Quality Simulation of Early\n  Vocal Learning of Vowels", "abstract": "The way infants use auditory cues to learn to speak despite the acoustic\nmismatch of their vocal apparatus is a hot topic of scientific debate. The\nsimulation of early vocal learning using articulatory speech synthesis offers a\nway towards gaining a deeper understanding of this process. One of the crucial\nparameters in these simulations is the choice of features and a metric to\nevaluate the acoustic error between the synthesised sound and the reference\ntarget. We contribute with evaluating the performance of a set of 40\nfeature-metric combinations for the task of optimising the production of static\nvowels with a high-quality articulatory synthesiser. Towards this end we assess\nthe usability of formant error and the projection of the feature-metric error\nsurface in the normalised F1-F2 formant space. We show that this approach can\nbe used to evaluate the impact of features and metrics and also to offer\ninsight to perceptual results.", "published": "2020-05-20 12:08:37", "link": "http://arxiv.org/abs/2005.09986v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-End Multi-Look Keyword Spotting", "abstract": "The performance of keyword spotting (KWS), measured in false alarms and false\nrejects, degrades significantly under the far field and noisy conditions. In\nthis paper, we propose a multi-look neural network modeling for speech\nenhancement which simultaneously steers to listen to multiple sampled look\ndirections. The multi-look enhancement is then jointly trained with KWS to form\nan end-to-end KWS model which integrates the enhanced signals from multiple\nlook directions and leverages an attention mechanism to dynamically tune the\nmodel's attention to the reliable sources. We demonstrate, on our large noisy\nand far-field evaluation sets, that the proposed approach significantly\nimproves the KWS performance against the baseline KWS system and a recent\nbeamformer based multi-beam KWS system.", "published": "2020-05-20 22:59:10", "link": "http://arxiv.org/abs/2005.10386v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Spoofing Attack Detection using the Non-linear Fusion of Sub-band\n  Classifiers", "abstract": "The threat of spoofing can pose a risk to the reliability of automatic\nspeaker verification. Results from the bi-annual ASVspoof evaluations show that\neffective countermeasures demand front-ends designed specifically for the\ndetection of spoofing artefacts. Given the diversity in spoofing attacks,\nensemble methods are particularly effective. The work in this paper shows that\na bank of very simple classifiers, each with a front-end tuned to the detection\nof different spoofing attacks and combined at the score level through\nnon-linear fusion, can deliver superior performance than more sophisticated\nensemble solutions that rely upon complex neural network architectures. Our\ncomparatively simple approach outperforms all but 2 of the 48 systems submitted\nto the logical access condition of the most recent ASVspoof 2019 challenge.", "published": "2020-05-20 23:37:28", "link": "http://arxiv.org/abs/2005.10393v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Active Speakers in Context", "abstract": "Current methods for active speak er detection focus on modeling short-term\naudiovisual information from a single speaker. Although this strategy can be\nenough for addressing single-speaker scenarios, it prevents accurate detection\nwhen the task is to identify who of many candidate speakers are talking. This\npaper introduces the Active Speaker Context, a novel representation that models\nrelationships between multiple speakers over long time horizons. Our Active\nSpeaker Context is designed to learn pairwise and temporal relations from an\nstructured ensemble of audio-visual observations. Our experiments show that a\nstructured feature ensemble already benefits the active speaker detection\nperformance. Moreover, we find that the proposed Active Speaker Context\nimproves the state-of-the-art on the AVA-ActiveSpeaker dataset achieving a mAP\nof 87.1%. We present ablation studies that verify that this result is a direct\nconsequence of our long-term multi-speaker analysis.", "published": "2020-05-20 01:14:23", "link": "http://arxiv.org/abs/2005.09812v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Exploring Recurrent, Memory and Attention Based Architectures for\n  Scoring Interactional Aspects of Human-Machine Text Dialog", "abstract": "An important step towards enabling English language learners to improve their\nconversational speaking proficiency involves automated scoring of multiple\naspects of interactional competence and subsequent targeted feedback. This\npaper builds on previous work in this direction to investigate multiple neural\narchitectures -- recurrent, attention and memory based -- along with\nfeature-engineered models for the automated scoring of interactional and topic\ndevelopment aspects of text dialog data. We conducted experiments on a\nconversational database of text dialogs from human learners interacting with a\ncloud-based dialog system, which were triple-scored along multiple dimensions\nof conversational proficiency. We find that fusion of multiple architectures\nperforms competently on our automated scoring task relative to expert\ninter-rater agreements, with (i) hand-engineered features passed to a support\nvector learner and (ii) transformer-based architectures contributing most\nprominently to the fusion.", "published": "2020-05-20 03:23:00", "link": "http://arxiv.org/abs/2005.09834v1", "categories": ["cs.HC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Consistent ICA: Determined BSS meets spectrogram consistency", "abstract": "Multichannel audio blind source separation (BSS) in the determined situation\n(the number of microphones is equal to that of the sources), or determined BSS,\nis performed by multichannel linear filtering in the time-frequency domain to\nhandle the convolutive mixing process. Ordinarily, the filter treats each\nfrequency independently, which causes the well-known permutation problem, i.e.,\nthe problem of how to align the frequency-wise filters so that each separated\ncomponent is correctly assigned to the corresponding sources. In this paper, it\nis shown that the general property of the time-frequency-domain representation\ncalled spectrogram consistency can be an assistant for solving the permutation\nproblem.", "published": "2020-05-20 06:46:14", "link": "http://arxiv.org/abs/2005.09873v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Towards Cover Song Detection with Siamese Convolutional Neural Networks", "abstract": "A cover song, by definition, is a new performance or recording of a\npreviously recorded, commercially released song. It may be by the original\nartist themselves or a different artist altogether and can vary from the\noriginal in unpredictable ways including key, arrangement, instrumentation,\ntimbre and more. In this work we propose a novel approach to learning audio\nrepresentations for the task of cover song detection. We train a neural\narchitecture on tens of thousands of cover-song audio clips and test it on a\nheld out set. We obtain a mean precision@1 of 65% over mini-batches, ten times\nbetter than random guessing. Our results indicate that Siamese network\nconfigurations show promise for approaching the cover song identification\nproblem.", "published": "2020-05-20 18:14:41", "link": "http://arxiv.org/abs/2005.10294v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "TinyLSTMs: Efficient Neural Speech Enhancement for Hearing Aids", "abstract": "Modern speech enhancement algorithms achieve remarkable noise suppression by\nmeans of large recurrent neural networks (RNNs). However, large RNNs limit\npractical deployment in hearing aid hardware (HW) form-factors, which are\nbattery powered and run on resource-constrained microcontroller units (MCUs)\nwith limited memory capacity and compute capability. In this work, we use model\ncompression techniques to bridge this gap. We define the constraints imposed on\nthe RNN by the HW and describe a method to satisfy them. Although model\ncompression techniques are an active area of research, we are the first to\ndemonstrate their efficacy for RNN speech enhancement, using pruning and\ninteger quantization of weights/activations. We also demonstrate state update\nskipping, which reduces the computational load. Finally, we conduct a\nperceptual evaluation of the compressed models to verify audio quality on human\nraters. Results show a reduction in model size and operations of 11.9$\\times$\nand 2.9$\\times$, respectively, over the baseline for compressed models, without\na statistical difference in listening preference and only exhibiting a loss of\n0.55dB SDR. Our model achieves a computational latency of 2.39ms, well within\nthe 10ms target and 351$\\times$ better than previous work.", "published": "2020-05-20 20:37:47", "link": "http://arxiv.org/abs/2005.11138v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
