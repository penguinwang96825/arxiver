{"title": "Advancing Generative AI for Portuguese with Open Decoder Gerv\u00e1sio PT*", "abstract": "To advance the neural decoding of Portuguese, in this paper we present a\nfully open Transformer-based, instruction-tuned decoder model that sets a new\nstate of the art in this respect. To develop this decoder, which we named\nGerv\\'asio PT*, a strong LLaMA~2 7B model was used as a starting point, and its\nfurther improvement through additional training was done over language\nresources that include new instruction data sets of Portuguese prepared for\nthis purpose, which are also contributed in this paper. All versions of\nGerv\\'asio are open source and distributed for free under an open license,\nincluding for either research or commercial usage, and can be run on\nconsumer-grade hardware, thus seeking to contribute to the advancement of\nresearch and innovation in language technology for Portuguese.", "published": "2024-02-29 00:19:13", "link": "http://arxiv.org/abs/2402.18766v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Utilizing Local Hierarchy with Adversarial Training for Hierarchical\n  Text Classification", "abstract": "Hierarchical text classification (HTC) is a challenging subtask of\nmulti-label classification due to its complex taxonomic structure. Nearly all\nrecent HTC works focus on how the labels are structured but ignore the\nsub-structure of ground-truth labels according to each input text which\ncontains fruitful label co-occurrence information. In this work, we introduce\nthis local hierarchy with an adversarial framework. We propose a HiAdv\nframework that can fit in nearly all HTC models and optimize them with the\nlocal hierarchy as auxiliary information. We test on two typical HTC models and\nfind that HiAdv is effective in all scenarios and is adept at dealing with\ncomplex taxonomic hierarchies. Further experiments demonstrate that the\npromotion of our framework indeed comes from the local hierarchy and the local\nhierarchy is beneficial for rare classes which have insufficient training data.", "published": "2024-02-29 03:20:45", "link": "http://arxiv.org/abs/2402.18825v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When does word order matter and when doesn't it?", "abstract": "Language models (LMs) may appear insensitive to word order changes in natural\nlanguage understanding (NLU) tasks. In this paper, we propose that linguistic\nredundancy can explain this phenomenon, whereby word order and other linguistic\ncues such as case markers provide overlapping and thus redundant information.\nOur hypothesis is that models exhibit insensitivity to word order when the\norder provides redundant information, and the degree of insensitivity varies\nacross tasks. We quantify how informative word order is using mutual\ninformation (MI) between unscrambled and scrambled sentences. Our results show\nthe effect that the less informative word order is, the more consistent the\nmodel's predictions are between unscrambled and scrambled sentences. We also\nfind that the effect varies across tasks: for some tasks, like SST-2, LMs'\nprediction is almost always consistent with the original one even if the\nPointwise-MI (PMI) changes, while for others, like RTE, the consistency is near\nrandom when the PMI gets lower, i.e., word order is really important.", "published": "2024-02-29 04:11:10", "link": "http://arxiv.org/abs/2402.18838v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reducing Hallucinations in Entity Abstract Summarization with\n  Facts-Template Decomposition", "abstract": "Entity abstract summarization aims to generate a coherent description of a\ngiven entity based on a set of relevant Internet documents. Pretrained language\nmodels (PLMs) have achieved significant success in this task, but they may\nsuffer from hallucinations, i.e. generating non-factual information about the\nentity. To address this issue, we decompose the summary into two components:\nFacts that represent the factual information about the given entity, which PLMs\nare prone to fabricate; and Template that comprises generic content with\ndesignated slots for facts, which PLMs can generate competently. Based on the\nfacts-template decomposition, we propose SlotSum, an explainable framework for\nentity abstract summarization. SlotSum first creates the template and then\npredicts the fact for each template slot based on the input documents.\nBenefiting from our facts-template decomposition, SlotSum can easily locate\nerrors and further rectify hallucinated predictions with external knowledge. We\nconstruct a new dataset WikiFactSum to evaluate the performance of SlotSum.\nExperimental results demonstrate that SlotSum could generate summaries that are\nsignificantly more factual with credible external knowledge.", "published": "2024-02-29 05:43:43", "link": "http://arxiv.org/abs/2402.18873v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Principal Component Analysis as a Sanity Check for Bayesian\n  Phylolinguistic Reconstruction", "abstract": "Bayesian approaches to reconstructing the evolutionary history of languages\nrely on the tree model, which assumes that these languages descended from a\ncommon ancestor and underwent modifications over time. However, this assumption\ncan be violated to different extents due to contact and other factors.\nUnderstanding the degree to which this assumption is violated is crucial for\nvalidating the accuracy of phylolinguistic inference. In this paper, we propose\na simple sanity check: projecting a reconstructed tree onto a space generated\nby principal component analysis. By using both synthetic and real data, we\ndemonstrate that our method effectively visualizes anomalies, particularly in\nthe form of jogging.", "published": "2024-02-29 05:47:34", "link": "http://arxiv.org/abs/2402.18877v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PopALM: Popularity-Aligned Language Models for Social Media Trendy\n  Response Prediction", "abstract": "Social media platforms are daily exhibiting millions of events. To\npreliminarily predict the mainstream public reaction to these events, we study\ntrendy response prediction to automatically generate top-liked user replies to\nsocial media events. While previous works focus on generating responses without\nfactoring in popularity, we propose Popularity-Aligned Language Models (PopALM)\nto distinguish responses liked by a larger audience through reinforcement\nlearning. Recognizing the noisy labels from user \"likes\", we tailor-make\ncurriculum learning in proximal policy optimization (PPO) to help models\ncapture the essential samples for easy-to-hard training. In experiments, we\nbuild a large-scale Weibo dataset for trendy response prediction, and its\nresults show that PopALM can help boost the performance of advanced language\nmodels.", "published": "2024-02-29 08:28:04", "link": "http://arxiv.org/abs/2402.18950v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pointing out the Shortcomings of Relation Extraction Models with\n  Semantically Motivated Adversarials", "abstract": "In recent years, large language models have achieved state-of-the-art\nperformance across various NLP tasks. However, investigations have shown that\nthese models tend to rely on shortcut features, leading to inaccurate\npredictions and causing the models to be unreliable at generalization to\nout-of-distribution (OOD) samples. For instance, in the context of relation\nextraction (RE), we would expect a model to identify the same relation\nindependently of the entities involved in it. For example, consider the\nsentence \"Leonardo da Vinci painted the Mona Lisa\" expressing the\ncreated(Leonardo_da_Vinci, Mona_Lisa) relation. If we substiute \"Leonardo da\nVinci\" with \"Barack Obama\", then the sentence still expresses the created\nrelation. A robust model is supposed to detect the same relation in both cases.\nIn this work, we describe several semantically-motivated strategies to generate\nadversarial examples by replacing entity mentions and investigate how\nstate-of-the-art RE models perform under pressure. Our analyses show that the\nperformance of these models significantly deteriorates on the modified datasets\n(avg. of -48.5% in F1), which indicates that these models rely to a great\nextent on shortcuts, such as surface forms (or patterns therein) of entities,\nwithout making full use of the information present in the sentences.", "published": "2024-02-29 12:01:46", "link": "http://arxiv.org/abs/2402.19076v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TEncDM: Understanding the Properties of the Diffusion Model in the Space\n  of Language Model Encodings", "abstract": "This paper presents the Text Encoding Diffusion Model (TEncDM), a novel\napproach to diffusion modeling that operates in the space of pre-trained\nlanguage model encodings. In contrast to traditionally used embeddings,\nencodings integrate contextual information. In our approach, we also employ a\ntransformer-based decoder, specifically designed to incorporate context in the\ntoken prediction process. We conduct a comprehensive examination of the\ninfluence of the encoder, decoder, noise scheduler, and self-conditioning on\nzero-shot generation. Furthermore, we compare TEncDM with previous approaches\non three conditional text generation tasks: QQP, XSum, and Wiki-Auto. The\nresults show that TEncDM exhibits superior performance compared to existing\nnon-autoregressive diffusion models. Our code is available at\nhttps://github.com/M0RJIQUE/tencdm.", "published": "2024-02-29 12:25:45", "link": "http://arxiv.org/abs/2402.19097v4", "categories": ["cs.CL", "I.2; I.7"], "primary_category": "cs.CL"}
{"title": "Evaluating Webcam-based Gaze Data as an Alternative for Human Rationale\n  Annotations", "abstract": "Rationales in the form of manually annotated input spans usually serve as\nground truth when evaluating explainability methods in NLP. They are, however,\ntime-consuming and often biased by the annotation process. In this paper, we\ndebate whether human gaze, in the form of webcam-based eye-tracking recordings,\nposes a valid alternative when evaluating importance scores. We evaluate the\nadditional information provided by gaze data, such as total reading times, gaze\nentropy, and decoding accuracy with respect to human rationale annotations. We\ncompare WebQAmGaze, a multilingual dataset for information-seeking QA, with\nattention and explainability-based importance scores for 4 different\nmultilingual Transformer-based language models (mBERT, distil-mBERT, XLMR, and\nXLMR-L) and 3 languages (English, Spanish, and German). Our pipeline can easily\nbe applied to other tasks and languages. Our findings suggest that gaze data\noffers valuable linguistic insights that could be leveraged to infer task\ndifficulty and further show a comparable ranking of explainability methods to\nthat of human rationales.", "published": "2024-02-29 13:09:26", "link": "http://arxiv.org/abs/2402.19133v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Teaching Large Language Models an Unseen Language on the Fly", "abstract": "Existing large language models struggle to support numerous low-resource\nlanguages, particularly the extremely low-resource ones, for which there is\nminimal training data available for effective parameter updating. We thus\ninvestigate whether LLMs can learn a new language on the fly solely through\nprompting. To study this question, we collect a research suite for Zhuang, a\nlanguage supported by no LLMs currently. We introduce DiPMT++, a framework for\nadapting LLMs to unseen languages by in-context learning. Using a dictionary\nand 5K parallel sentences only, DiPMT++ significantly enhances the performance\nof GPT-4 from 0 to 16 BLEU for Chinese-to-Zhuang translation and achieves 32\nBLEU for Zhuang-to-Chinese translation. We also validate the effectiveness of\nour framework on Kalamang, another unseen language. Furthermore, we demonstrate\nthe practical utility of DiPMT++ in aiding humans in translating completely\nunseen languages, which could contribute to the preservation of linguistic\ndiversity.", "published": "2024-02-29 13:50:47", "link": "http://arxiv.org/abs/2402.19167v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PeLLE: Encoder-based language models for Brazilian Portuguese based on\n  open data", "abstract": "In this paper we present PeLLE, a family of large language models based on\nthe RoBERTa architecture, for Brazilian Portuguese, trained on curated, open\ndata from the Carolina corpus. Aiming at reproducible results, we describe\ndetails of the pretraining of the models. We also evaluate PeLLE models against\na set of existing multilingual and PT-BR refined pretrained Transformer-based\nLLM encoders, contrasting performance of large versus smaller-but-curated\npretrained models in several downstream tasks. We conclude that several tasks\nperform better with larger models, but some tasks benefit from\nsmaller-but-curated data in its pretraining.", "published": "2024-02-29 14:34:03", "link": "http://arxiv.org/abs/2402.19204v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Memory-Augmented Generative Adversarial Transformers", "abstract": "Conversational AI systems that rely on Large Language Models, like\nTransformers, have difficulty interweaving external data (like facts) with the\nlanguage they generate. Vanilla Transformer architectures are not designed for\nanswering factual questions with high accuracy. This paper investigates a\npossible route for addressing this problem. We propose to extend the standard\nTransformer architecture with an additional memory bank holding extra\ninformation (such as facts drawn from a knowledge base), and an extra attention\nlayer for addressing this memory. We add this augmented memory to a Generative\nAdversarial Network-inspired Transformer architecture. This setup allows for\nimplementing arbitrary felicity conditions on the generated language of the\nTransformer. We first demonstrate how this machinery can be deployed for\nhandling factual questions in goal-oriented dialogues. Secondly, we demonstrate\nthat our approach can be useful for applications like {\\it style adaptation} as\nwell: the adaptation of utterances according to certain stylistic (external)\nconstraints, like social properties of human interlocutors in dialogues.", "published": "2024-02-29 14:47:24", "link": "http://arxiv.org/abs/2402.19218v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question\n  Answering Benchmark", "abstract": "How to better evaluate the capabilities of Large Language Models (LLMs) is\nthe focal point and hot topic in current LLMs research. Previous work has noted\nthat due to the extremely high cost of iterative updates of LLMs, they are\noften unable to answer the latest dynamic questions well. To promote the\nimprovement of Chinese LLMs' ability to answer dynamic questions, in this\npaper, we introduce CDQA, a Chinese Dynamic QA benchmark containing\nquestion-answer pairs related to the latest news on the Chinese Internet. We\nobtain high-quality data through a pipeline that combines humans and models,\nand carefully classify the samples according to the frequency of answer changes\nto facilitate a more fine-grained observation of LLMs' capabilities. We have\nalso evaluated and analyzed mainstream and advanced Chinese LLMs on CDQA.\nExtensive experiments and valuable insights suggest that our proposed CDQA is\nchallenging and worthy of more further study. We believe that the benchmark we\nprovide will become one of the key data resources for improving LLMs' Chinese\nquestion-answering ability in the future.", "published": "2024-02-29 15:22:13", "link": "http://arxiv.org/abs/2402.19248v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of\n  LLMs as Mathematical Problem Solvers", "abstract": "Large language models (LLMs) have achieved impressive performance across\nvarious mathematical reasoning benchmarks. However, there are increasing\ndebates regarding whether these models truly understand and apply mathematical\nknowledge or merely rely on shortcuts for mathematical reasoning. One essential\nand frequently occurring evidence is that when the math questions are slightly\nchanged, LLMs can behave incorrectly. This motivates us to evaluate the\nrobustness of LLMs' math reasoning capability by testing a wide range of\nquestion variations. We introduce the adversarial grade school math (GSM-Plus)\ndataset, an extension of GSM8K augmented with various mathematical\nperturbations. Our experiments on 25 LLMs and 4 prompting techniques show that\nwhile LLMs exhibit different levels of math reasoning abilities, their\nperformances are far from robust. In particular, even for problems that have\nbeen solved in GSM8K, LLMs can make mistakes when new statements are added or\nthe question targets are altered. We also explore whether more robust\nperformance can be achieved by composing existing prompting methods, in which\nwe try an iterative method that generates and verifies each intermediate\nthought based on its reasoning goal and calculation result.", "published": "2024-02-29 15:26:14", "link": "http://arxiv.org/abs/2402.19255v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PlanGPT: Enhancing Urban Planning with Tailored Language Model and\n  Efficient Retrieval", "abstract": "In the field of urban planning, general-purpose large language models often\nstruggle to meet the specific needs of planners. Tasks like generating urban\nplanning texts, retrieving related information, and evaluating planning\ndocuments pose unique challenges. To enhance the efficiency of urban\nprofessionals and overcome these obstacles, we introduce PlanGPT, the first\nspecialized Large Language Model tailored for urban and spatial planning.\nDeveloped through collaborative efforts with institutions like the Chinese\nAcademy of Urban Planning, PlanGPT leverages a customized local database\nretrieval framework, domain-specific fine-tuning of base models, and advanced\ntooling capabilities. Empirical tests demonstrate that PlanGPT has achieved\nadvanced performance, delivering responses of superior quality precisely\ntailored to the intricacies of urban planning.", "published": "2024-02-29 15:41:20", "link": "http://arxiv.org/abs/2402.19273v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset", "abstract": "This paper presents WanJuan-CC, a safe and high-quality open-sourced English\nwebtext dataset derived from Common Crawl data. The study addresses the\nchallenges of constructing large-scale pre-training datasets for language\nmodels, which require vast amounts of high-quality data. A comprehensive\nprocess was designed to handle Common Crawl data, including extraction,\nheuristic rule filtering, fuzzy deduplication, content safety filtering, and\ndata quality filtering. From approximately 68 billion original English\ndocuments, we obtained 2.22T Tokens of safe data and selected 1.0T Tokens of\nhigh-quality data as part of WanJuan-CC. We have open-sourced 100B Tokens from\nthis dataset. The paper also provides statistical information related to data\nquality, enabling users to select appropriate data according to their needs. To\nevaluate the quality and utility of the dataset, we trained 1B-parameter and\n3B-parameter models using WanJuan-CC and another dataset, RefinedWeb. Results\nshow that WanJuan-CC performs better on validation datasets and downstream\ntasks.", "published": "2024-02-29 15:49:15", "link": "http://arxiv.org/abs/2402.19282v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Here's a Free Lunch: Sanitizing Backdoored Models with Model Merge", "abstract": "The democratization of pre-trained language models through open-source\ninitiatives has rapidly advanced innovation and expanded access to cutting-edge\ntechnologies. However, this openness also brings significant security risks,\nincluding backdoor attacks, where hidden malicious behaviors are triggered by\nspecific inputs, compromising natural language processing (NLP) system\nintegrity and reliability. This paper suggests that merging a backdoored model\nwith other homogeneous models can significantly remediate backdoor\nvulnerabilities even if such models are not entirely secure. In our\nexperiments, we verify our hypothesis on various models (BERT-Base,\nRoBERTa-Large, Llama2-7B, and Mistral-7B) and datasets (SST-2, OLID, AG News,\nand QNLI). Compared to multiple advanced defensive approaches, our method\noffers an effective and efficient inference-stage defense against backdoor\nattacks on classification and instruction-tuned tasks without additional\nresources or specific knowledge. Our approach consistently outperforms recent\nadvanced baselines, leading to an average of about 75% reduction in the attack\nsuccess rate. Since model merging has been an established approach for\nimproving model performance, the extra advantage it provides regarding defense\ncan be seen as a cost-free bonus.", "published": "2024-02-29 16:37:08", "link": "http://arxiv.org/abs/2402.19334v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompting Explicit and Implicit Knowledge for Multi-hop Question\n  Answering Based on Human Reading Process", "abstract": "Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to\nsimulate human reasoning and inference processes, achieving proficient\nperformance in multi-hop QA. However, a gap persists between PLMs' reasoning\nabilities and those of humans when tackling complex problems. Psychological\nstudies suggest a vital connection between explicit information in passages and\nhuman prior knowledge during reading. Nevertheless, current research has given\ninsufficient attention to linking input passages and PLMs' pre-training-based\nknowledge from the perspective of human cognition studies. In this study, we\nintroduce a Prompting Explicit and Implicit knowledge (PEI) framework, which\nuses prompts to connect explicit and implicit knowledge, aligning with human\nreading process for multi-hop QA. We consider the input passages as explicit\nknowledge, employing them to elicit implicit knowledge through unified prompt\nreasoning. Furthermore, our model incorporates type-specific reasoning via\nprompts, a form of implicit knowledge. Experimental results show that PEI\nperforms comparably to the state-of-the-art on HotpotQA. Ablation studies\nconfirm the efficacy of our model in bridging and integrating explicit and\nimplicit knowledge.", "published": "2024-02-29 16:56:36", "link": "http://arxiv.org/abs/2402.19350v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Query-OPT: Optimizing Inference of Large Language Models via Multi-Query\n  Instructions in Meeting Summarization", "abstract": "This work focuses on the task of query-based meeting summarization in which\nthe summary of a context (meeting transcript) is generated in response to a\nspecific query. When using Large Language Models (LLMs) for this task, usually\na new call to the LLM inference endpoint/API is triggered for each new query,\neven if the context stays the same. However, repeated calls to the LLM\ninference endpoints would significantly increase the costs of using them in\nproduction, making LLMs impractical for many real-world use cases. To address\nthis problem, in this paper, we investigate whether combining the queries for\nthe same input context in a single prompt to minimize repeated calls can be\nsuccessfully used in meeting summarization. In this regard, we conduct\nextensive experiments by comparing the performance of various popular LLMs:\nGPT-4, Gemini, Claude-3, LLaMA-2, Mistral, Phi-3, and Qwen-2 in single-query\nand multi-query settings. We observe that 100% reliability in generating the\nresponse in the expected format is usually limited to certain closed-source\nLLMs, with most open-source LLMs lagging behind (except a few 7B parameters\nLLMs like Mistral and Phi-3). We conclude that multi-query prompting could be\nuseful to significantly optimize the inference costs in meeting summarization.", "published": "2024-02-29 19:00:47", "link": "http://arxiv.org/abs/2403.00067v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PROC2PDDL: Open-Domain Planning Representations from Texts", "abstract": "Planning in a text-based environment continues to be a major challenge for AI\nsystems. Recent approaches have used language models to predict a planning\ndomain definition (e.g., PDDL) but have only been evaluated in closed-domain\nsimulated environments. To address this, we present Proc2PDDL , the first\ndataset containing open-domain procedural texts paired with expert-annotated\nPDDL representations. Using this dataset, we evaluate state-of-the-art models\non defining the preconditions and effects of actions. We show that Proc2PDDL is\nhighly challenging, with GPT-3.5's success rate close to 0% and GPT-4's around\n35%. Our analysis shows both syntactic and semantic errors, indicating LMs'\ndeficiency in both generating domain-specific prgorams and reasoning about\nevents. We hope this analysis and dataset helps future progress towards\nintegrating the best of LMs and formal planning.", "published": "2024-02-29 19:40:25", "link": "http://arxiv.org/abs/2403.00092v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FAC$^2$E: Better Understanding Large Language Model Capabilities by\n  Dissociating Language and Cognition", "abstract": "Large language models (LLMs) are primarily evaluated by overall performance\non various text understanding and generation tasks. However, such a paradigm\nfails to comprehensively differentiate the fine-grained language and cognitive\nskills, rendering the lack of sufficient interpretation to LLMs' capabilities.\nIn this paper, we present FAC$^2$E, a framework for Fine-grAined and\nCognition-grounded LLMs' Capability Evaluation. Specifically, we formulate\nLLMs' evaluation in a multi-dimensional and explainable manner by dissociating\nthe language-related capabilities and the cognition-related ones. Besides,\nthrough extracting the intermediate reasoning from LLMs, we further break down\nthe process of applying a specific capability into three sub-steps: recalling\nrelevant knowledge, utilizing knowledge, and solving problems. Finally,\nFAC$^2$E evaluates each sub-step of each fine-grained capability, providing a\ntwo-faceted diagnosis for LLMs. Utilizing FAC$^2$E, we identify a common\nshortfall in knowledge utilization among models and propose a straightforward,\nknowledge-enhanced method to mitigate this issue. Our results not only showcase\npromising performance enhancements but also highlight a direction for future\nLLM advancements.", "published": "2024-02-29 21:05:37", "link": "http://arxiv.org/abs/2403.00126v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language\n  Models", "abstract": "Model editing has emerged as a cost-effective strategy to update knowledge\nstored in language models. However, model editing can have unintended\nconsequences after edits are applied: information unrelated to the edits can\nalso be changed, and other general behaviors of the model can be wrongly\naltered. In this work, we investigate how model editing methods unexpectedly\namplify model biases post-edit. We introduce a novel benchmark dataset,\nSeesaw-CF, for measuring bias-related harms of model editing and conduct the\nfirst in-depth investigation of how different weight-editing methods impact\nmodel bias. Specifically, we focus on biases with respect to demographic\nattributes such as race, geographic origin, and gender, as well as qualitative\nflaws in long-form texts generated by edited language models. We find that\nedited models exhibit, to various degrees, more biased behavior as they become\nless confident in attributes for Asian, African, and South American subjects.\nFurthermore, edited models amplify sexism and xenophobia in text generations\nwhile remaining seemingly coherent and logical. Finally, editing facts about\nplace of birth, country of citizenship, or gender have particularly negative\neffects on the model's knowledge about unrelated features like field of work.", "published": "2024-02-29 23:11:55", "link": "http://arxiv.org/abs/2403.00180v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ARTiST: Automated Text Simplification for Task Guidance in Augmented\n  Reality", "abstract": "Text presented in augmented reality provides in-situ, real-time information\nfor users. However, this content can be challenging to apprehend quickly when\nengaging in cognitively demanding AR tasks, especially when it is presented on\na head-mounted display. We propose ARTiST, an automatic text simplification\nsystem that uses a few-shot prompt and GPT-3 models to specifically optimize\nthe text length and semantic content for augmented reality. Developed out of a\nformative study that included seven users and three experts, our system\ncombines a customized error calibration model with a few-shot prompt to\nintegrate the syntactic, lexical, elaborative, and content simplification\ntechniques, and generate simplified AR text for head-worn displays. Results\nfrom a 16-user empirical study showed that ARTiST lightens the cognitive load\nand improves performance significantly over both unmodified text and text\nmodified via traditional methods. Our work constitutes a step towards\nautomating the optimization of batch text data for readability and performance\nin augmented reality.", "published": "2024-02-29 01:58:49", "link": "http://arxiv.org/abs/2402.18797v1", "categories": ["cs.HC", "cs.CL", "H.1.2; I.2.7"], "primary_category": "cs.HC"}
{"title": "On the Decision-Making Abilities in Role-Playing using Large Language\n  Models", "abstract": "Large language models (LLMs) are now increasingly utilized for role-playing\ntasks, especially in impersonating domain-specific experts, primarily through\nrole-playing prompts. When interacting in real-world scenarios, the\ndecision-making abilities of a role significantly shape its behavioral\npatterns. In this paper, we concentrate on evaluating the decision-making\nabilities of LLMs post role-playing thereby validating the efficacy of\nrole-playing. Our goal is to provide metrics and guidance for enhancing the\ndecision-making abilities of LLMs in role-playing tasks. Specifically, we first\nuse LLMs to generate virtual role descriptions corresponding to the 16\npersonality types of Myers-Briggs Type Indicator (abbreviated as MBTI)\nrepresenting a segmentation of the population. Then we design specific\nquantitative operations to evaluate the decision-making abilities of LLMs post\nrole-playing from four aspects: adaptability, exploration$\\&$exploitation\ntrade-off ability, reasoning ability, and safety. Finally, we analyze the\nassociation between the performance of decision-making and the corresponding\nMBTI types through GPT-4. Extensive experiments demonstrate stable differences\nin the four aspects of decision-making abilities across distinct roles,\nsignifying a robust correlation between decision-making abilities and the roles\nemulated by LLMs. These results underscore that LLMs can effectively\nimpersonate varied roles while embodying their genuine sociological\ncharacteristics.", "published": "2024-02-29 02:22:23", "link": "http://arxiv.org/abs/2402.18807v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How do Large Language Models Handle Multilingualism?", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities across\ndiverse languages. This study explores how LLMs handle multilingualism. Based\non observed language ratio shifts among layers and the relationships between\nnetwork structures and certain capabilities, we hypothesize the LLM's\nmultilingual workflow ($\\texttt{MWork}$): LLMs initially understand the query,\nconverting multilingual inputs into English for task-solving. In the\nintermediate layers, they employ English for thinking and incorporate\nmultilingual knowledge with self-attention and feed-forward structures,\nrespectively. In the final layers, LLMs generate responses aligned with the\noriginal language of the query. To verify $\\texttt{MWork}$, we introduce\nParallel Language-specific Neuron Detection ($\\texttt{PLND}$) to identify\nactivated neurons for inputs in different languages without any labeled data.\nUsing $\\texttt{PLND}$, we validate $\\texttt{MWork}$ through extensive\nexperiments involving the deactivation of language-specific neurons across\nvarious layers and structures. Moreover, $\\texttt{MWork}$ allows fine-tuning of\nlanguage-specific neurons with a small dataset, enhancing multilingual\nabilities in a specific language without compromising others. This approach\nresults in an average improvement of $3.6\\%$ for high-resource languages and\n$2.3\\%$ for low-resource languages across all tasks with just $400$ documents.", "published": "2024-02-29 02:55:26", "link": "http://arxiv.org/abs/2402.18815v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AKEW: Assessing Knowledge Editing in the Wild", "abstract": "Knowledge editing injects knowledge updates into language models to keep them\ncorrect and up-to-date. However, its current evaluations deviate significantly\nfrom practice: their knowledge updates solely consist of structured facts\nderived from meticulously crafted datasets, instead of practical sources --\nunstructured texts like news articles, and they often overlook practical\nreal-world knowledge updates. To address these issues, in this paper we propose\nAKEW (Assessing Knowledge Editing in the Wild), a new practical benchmark for\nknowledge editing. AKEW fully covers three editing settings of knowledge\nupdates: structured facts, unstructured texts as facts, and extracted triplets.\nIt further introduces new datasets featuring both counterfactual and real-world\nknowledge updates. Through extensive experiments, we demonstrate the\nconsiderable gap between state-of-the-art knowledge-editing methods and\npractical scenarios. Our analyses further highlight key insights to motivate\nfuture research for practical knowledge editing.", "published": "2024-02-29 07:08:34", "link": "http://arxiv.org/abs/2402.18909v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AdaMergeX: Cross-Lingual Transfer with Large Language Models via\n  Adaptive Adapter Merging", "abstract": "As an effective alternative to the direct fine-tuning on target tasks in\nspecific languages, cross-lingual transfer addresses the challenges of limited\ntraining data by decoupling ''task ability'' and ''language ability'' by\nfine-tuning on the target task in the source language and another selected task\nin the target language, respectively. However, they fail to fully separate the\ntask ability from the source language or the language ability from the chosen\ntask. In this paper, we acknowledge the mutual reliance between task ability\nand language ability and direct our attention toward the gap between the target\nlanguage and the source language on tasks. As the gap removes the impact of\ntasks, we assume that it remains consistent across tasks. Based on this\nassumption, we propose a new cross-lingual transfer method called\n$\\texttt{AdaMergeX}$ that utilizes adaptive adapter merging. By introducing a\nreference task, we can determine that the divergence of adapters fine-tuned on\nthe reference task in both languages follows the same distribution as the\ndivergence of adapters fine-tuned on the target task in both languages. Hence,\nwe can obtain target adapters by combining the other three adapters.\nFurthermore, we propose a structure-adaptive adapter merging method. Our\nempirical results demonstrate that our approach yields new and effective\ncross-lingual transfer, outperforming existing methods across all settings.", "published": "2024-02-29 07:11:24", "link": "http://arxiv.org/abs/2402.18913v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SemEval 2024 -- Task 10: Emotion Discovery and Reasoning its Flip in\n  Conversation (EDiReF)", "abstract": "We present SemEval-2024 Task 10, a shared task centred on identifying\nemotions and finding the rationale behind their flips within monolingual\nEnglish and Hindi-English code-mixed dialogues. This task comprises three\ndistinct subtasks - emotion recognition in conversation for code-mixed\ndialogues, emotion flip reasoning for code-mixed dialogues, and emotion flip\nreasoning for English dialogues. Participating systems were tasked to\nautomatically execute one or more of these subtasks. The datasets for these\ntasks comprise manually annotated conversations focusing on emotions and\ntriggers for emotion shifts (The task data is available at\nhttps://github.com/LCS2-IIITD/EDiReF-SemEval2024.git). A total of 84\nparticipants engaged in this task, with the most adept systems attaining\nF1-scores of 0.70, 0.79, and 0.76 for the respective subtasks. This paper\nsummarises the results and findings from 24 teams alongside their system\ndescriptions.", "published": "2024-02-29 08:20:06", "link": "http://arxiv.org/abs/2402.18944v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring the Efficacy of Large Language Models in Summarizing Mental\n  Health Counseling Sessions: A Benchmark Study", "abstract": "Comprehensive summaries of sessions enable an effective continuity in mental\nhealth counseling, facilitating informed therapy planning. Yet, manual\nsummarization presents a significant challenge, diverting experts' attention\nfrom the core counseling process. This study evaluates the effectiveness of\nstate-of-the-art Large Language Models (LLMs) in selectively summarizing\nvarious components of therapy sessions through aspect-based summarization,\naiming to benchmark their performance. We introduce MentalCLOUDS, a\ncounseling-component guided summarization dataset consisting of 191 counseling\nsessions with summaries focused on three distinct counseling components (aka\ncounseling aspects). Additionally, we assess the capabilities of 11\nstate-of-the-art LLMs in addressing the task of component-guided summarization\nin counseling. The generated summaries are evaluated quantitatively using\nstandard summarization metrics and verified qualitatively by mental health\nprofessionals. Our findings demonstrate the superior performance of\ntask-specific LLMs such as MentalLlama, Mistral, and MentalBART in terms of\nstandard quantitative metrics such as Rouge-1, Rouge-2, Rouge-L, and BERTScore\nacross all aspects of counseling components. Further, expert evaluation reveals\nthat Mistral supersedes both MentalLlama and MentalBART based on six parameters\n-- affective attitude, burden, ethicality, coherence, opportunity costs, and\nperceived effectiveness. However, these models share the same weakness by\ndemonstrating a potential for improvement in the opportunity costs and\nperceived effectiveness metrics.", "published": "2024-02-29 11:29:47", "link": "http://arxiv.org/abs/2402.19052v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Survey in Characterization of Semantic Change", "abstract": "Live languages continuously evolve to integrate the cultural change of human\nsocieties. This evolution manifests through neologisms (new words) or\n\\textbf{semantic changes} of words (new meaning to existing words).\nUnderstanding the meaning of words is vital for interpreting texts coming from\ndifferent cultures (regionalism or slang), domains (e.g., technical terms), or\nperiods. In computer science, these words are relevant to computational\nlinguistics algorithms such as translation, information retrieval, question\nanswering, etc. Semantic changes can potentially impact the quality of the\noutcomes of these algorithms. Therefore, it is important to understand and\ncharacterize these changes formally. The study of this impact is a recent\nproblem that has attracted the attention of the computational linguistics\ncommunity. Several approaches propose methods to detect semantic changes with\ngood precision, but more effort is needed to characterize how the meaning of\nwords changes and to reason about how to reduce the impact of semantic change.\nThis survey provides an understandable overview of existing approaches to the\n\\textit{characterization of semantic changes} and also formally defines three\nclasses of characterizations: if the meaning of a word becomes more general or\nnarrow (change in dimension) if the word is used in a more pejorative or\npositive/ameliorated sense (change in orientation), and if there is a trend to\nuse the word in a, for instance, metaphoric or metonymic context (change in\nrelation). We summarized the main aspects of the selected publications in a\ntable and discussed the needs and trends in the research activities on semantic\nchange characterization.", "published": "2024-02-29 12:13:50", "link": "http://arxiv.org/abs/2402.19088v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Whispers that Shake Foundations: Analyzing and Mitigating False Premise\n  Hallucinations in Large Language Models", "abstract": "Large Language Models (LLMs) have shown impressive capabilities but still\nsuffer from the issue of hallucinations. A significant type of this issue is\nthe false premise hallucination, which we define as the phenomenon when LLMs\ngenerate hallucinated text when confronted with false premise questions. In\nthis paper, we perform a comprehensive analysis of the false premise\nhallucination and elucidate its internal working mechanism: a small subset of\nattention heads (which we designate as false premise heads) disturb the\nknowledge extraction process, leading to the occurrence of false premise\nhallucination. Based on our analysis, we propose \\textbf{FAITH} (\\textbf{F}alse\npremise \\textbf{A}ttention head constra\\textbf{I}ining for mi\\textbf{T}igating\n\\textbf{H}allucinations), a novel and effective method to mitigate false\npremise hallucinations. It constrains the false premise attention heads during\nthe model inference process. Impressively, extensive experiments demonstrate\nthat constraining only approximately $1\\%$ of the attention heads in the model\nyields a notable increase of nearly $20\\%$ of model performance.", "published": "2024-02-29 12:35:45", "link": "http://arxiv.org/abs/2402.19103v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How to Understand \"Support\"? An Implicit-enhanced Causal Inference\n  Approach for Weakly-supervised Phrase Grounding", "abstract": "Weakly-supervised Phrase Grounding (WPG) is an emerging task of inferring the\nfine-grained phrase-region matching, while merely leveraging the coarse-grained\nsentence-image pairs for training. However, existing studies on WPG largely\nignore the implicit phrase-region matching relations, which are crucial for\nevaluating the capability of models in understanding the deep multimodal\nsemantics. To this end, this paper proposes an Implicit-Enhanced Causal\nInference (IECI) approach to address the challenges of modeling the implicit\nrelations and highlighting them beyond the explicit. Specifically, this\napproach leverages both the intervention and counterfactual techniques to\ntackle the above two challenges respectively. Furthermore, a high-quality\nimplicit-enhanced dataset is annotated to evaluate IECI and detailed\nevaluations show the great advantages of IECI over the state-of-the-art\nbaselines. Particularly, we observe an interesting finding that IECI\noutperforms the advanced multimodal LLMs by a large margin on this\nimplicit-enhanced dataset, which may facilitate more research to evaluate the\nmultimodal LLMs in this direction.", "published": "2024-02-29 12:49:48", "link": "http://arxiv.org/abs/2402.19116v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VIXEN: Visual Text Comparison Network for Image Difference Captioning", "abstract": "We present VIXEN - a technique that succinctly summarizes in text the visual\ndifferences between a pair of images in order to highlight any content\nmanipulation present. Our proposed network linearly maps image features in a\npairwise manner, constructing a soft prompt for a pretrained large language\nmodel. We address the challenge of low volume of training data and lack of\nmanipulation variety in existing image difference captioning (IDC) datasets by\ntraining on synthetically manipulated images from the recent InstructPix2Pix\ndataset generated via prompt-to-prompt editing framework. We augment this\ndataset with change summaries produced via GPT-3. We show that VIXEN produces\nstate-of-the-art, comprehensible difference captions for diverse image contents\nand edit types, offering a potential mitigation against misinformation\ndisseminated via manipulated image content. Code and data are available at\nhttp://github.com/alexblck/vixen", "published": "2024-02-29 12:56:18", "link": "http://arxiv.org/abs/2402.19119v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Improving Legal Judgement Prediction in Romanian with Long Text Encoders", "abstract": "In recent years,the entire field of Natural Language Processing (NLP) has\nenjoyed amazing novel results achieving almost human-like performance on a\nvariety of tasks. Legal NLP domain has also been part of this process, as it\nhas seen an impressive growth. However, general-purpose models are not readily\napplicable for legal domain. Due to the nature of the domain (e.g. specialized\nvocabulary, long documents) specific models and methods are often needed for\nLegal NLP. In this work we investigate both specialized and general models for\npredicting the final ruling of a legal case, task known as Legal Judgment\nPrediction (LJP). We particularly focus on methods to extend to sequence length\nof Transformer-based models to better understand the long documents present in\nlegal corpora. Extensive experiments on 4 LJP datasets in Romanian, originating\nfrom 2 sources with significantly different sizes and document lengths, show\nthat specialized models and handling long texts are critical for a good\nperformance.", "published": "2024-02-29 13:52:33", "link": "http://arxiv.org/abs/2402.19170v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PRSA: PRompt Stealing Attacks against Large Language Models", "abstract": "In recent years, \"prompt as a service\" has greatly enhanced the utility of\nlarge language models (LLMs) by enabling them to perform various downstream\ntasks efficiently without fine-tuning. This has also increased the commercial\nvalue of prompts. However, the potential risk of leakage in these\ncommercialized prompts remains largely underexplored. In this paper, we\nintroduce a novel attack framework, PRSA, designed for prompt stealing attacks\nagainst LLMs. The main idea of PRSA is to infer the intent behind a prompt by\nanalyzing its input-output content, enabling the generation of a surrogate\nprompt that replicates the original's functionality. Specifically, PRSA mainly\nconsists of two key phases: prompt mutation and prompt pruning. In the mutation\nphase, we propose a prompt attention algorithm based on output difference. The\nalgorithm facilitates the generation of effective surrogate prompts by learning\nkey factors that influence the accurate inference of prompt intent. During the\npruning phase, we employ a two-step related word identification strategy to\ndetect and mask words that are highly related to the input, thus improving the\ngeneralizability of the surrogate prompts. We verify the actual threat of PRSA\nthrough evaluation in both real-world settings, non-interactive and interactive\nprompt services. The results strongly confirm the PRSA's effectiveness and\ngeneralizability. We have reported these findings to prompt service providers\nand actively collaborate with them to implement defensive measures.", "published": "2024-02-29 14:30:28", "link": "http://arxiv.org/abs/2402.19200v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Robust Guidance for Unsupervised Data Selection: Capturing Perplexing\n  Named Entities for Domain-Specific Machine Translation", "abstract": "Low-resourced data presents a significant challenge for neural machine\ntranslation. In most cases, the low-resourced environment is caused by high\ncosts due to the need for domain experts or the lack of language experts.\nTherefore, identifying the most training-efficient data within an unsupervised\nsetting emerges as a practical strategy. Recent research suggests that such\neffective data can be identified by selecting 'appropriately complex data'\nbased on its volume, providing strong intuition for unsupervised data\nselection. However, we have discovered that establishing criteria for\nunsupervised data selection remains a challenge, as the 'appropriate level of\ndifficulty' may vary depending on the data domain. We introduce a novel\nunsupervised data selection method named 'Capturing Perplexing Named Entities,'\nwhich leverages the maximum inference entropy in translated named entities as a\nmetric for selection. When tested with the 'Korean-English Parallel Corpus of\nSpecialized Domains,' our method served as robust guidance for identifying\ntraining-efficient data across different domains, in contrast to existing\nmethods.", "published": "2024-02-29 15:38:28", "link": "http://arxiv.org/abs/2402.19267v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EAMA : Entity-Aware Multimodal Alignment Based Approach for News Image\n  Captioning", "abstract": "News image captioning requires model to generate an informative caption rich\nin entities, with the news image and the associated news article. Current MLLMs\nstill bear limitations in handling entity information in news image captioning\ntasks. Besides, generating high-quality news image captions requires a\ntrade-off between sufficiency and conciseness of textual input information. To\nexplore the potential of MLLMs and address problems we discovered, we propose\nEAMA: an Entity-Aware Multimodal Alignment based approach for News Image\nCaptioning. Our approach first aligns the MLLM with two extra alignment tasks:\nEntity-Aware Sentence Selection task and Entity Selection task, together with\nNews Image Captioning task. The aligned MLLM will utilize the additional\nentity-related information extracted by itself to supplement the textual input\nwhile generating news image captions. Our approach achieves better results than\nall previous models on two mainstream news image captioning datasets.", "published": "2024-02-29 18:03:00", "link": "http://arxiv.org/abs/2402.19404v5", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "On the Scaling Laws of Geographical Representation in Language Models", "abstract": "Language models have long been shown to embed geographical information in\ntheir hidden representations. This line of work has recently been revisited by\nextending this result to Large Language Models (LLMs). In this paper, we\npropose to fill the gap between well-established and recent literature by\nobserving how geographical knowledge evolves when scaling language models. We\nshow that geographical knowledge is observable even for tiny models, and that\nit scales consistently as we increase the model size. Notably, we observe that\nlarger language models cannot mitigate the geographical bias that is inherent\nto the training data.", "published": "2024-02-29 18:04:11", "link": "http://arxiv.org/abs/2402.19406v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Griffin: Mixing Gated Linear Recurrences with Local Attention for\n  Efficient Language Models", "abstract": "Recurrent neural networks (RNNs) have fast inference and scale efficiently on\nlong sequences, but they are difficult to train and hard to scale. We propose\nHawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that\nmixes gated linear recurrences with local attention. Hawk exceeds the reported\nperformance of Mamba on downstream tasks, while Griffin matches the performance\nof Llama-2 despite being trained on over 6 times fewer tokens. We also show\nthat Griffin can extrapolate on sequences significantly longer than those seen\nduring training. Our models match the hardware efficiency of Transformers\nduring training, and during inference they have lower latency and significantly\nhigher throughput. We scale Griffin up to 14B parameters, and explain how to\nshard our models for efficient distributed training.", "published": "2024-02-29 18:24:46", "link": "http://arxiv.org/abs/2402.19427v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Functional Benchmarks for Robust Evaluation of Reasoning Performance,\n  and the Reasoning Gap", "abstract": "We propose a framework for robust evaluation of reasoning capabilities of\nlanguage models, using functional variants of benchmarks. Models that solve a\nreasoning test should exhibit no difference in performance over the static\nversion of a problem compared to a snapshot of the functional variant. We have\nrewritten the relevant fragment of the MATH benchmark into its functional\nvariant MATH(), with functionalization of other benchmarks to follow. When\nevaluating current state-of-the-art models over snapshots of MATH(), we find a\nreasoning gap -- the percentage difference between the static and functional\naccuracies. We find reasoning gaps from 58.35% to 80.31% among the\nstate-of-the-art closed and open weights models that perform well on static\nbenchmarks, with the caveat that the gaps are likely to be smaller with more\nsophisticated prompting strategies. Here we show that models which anecdotally\nhave good reasoning performance over real-world tasks, have quantifiable lower\ngaps, motivating the open problem of building \"gap 0\" models. Code for\nevaluation and new evaluation datasets, three MATH() snapshots, are publicly\navailable at https://github.com/consequentai/fneval/.", "published": "2024-02-29 18:48:18", "link": "http://arxiv.org/abs/2402.19450v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "$\\texttt{COSMIC}$: Mutual Information for Task-Agnostic Summarization\n  Evaluation", "abstract": "Assessing the quality of summarizers poses significant challenges. In\nresponse, we propose a novel task-oriented evaluation approach that assesses\nsummarizers based on their capacity to produce summaries that are useful for\ndownstream tasks, while preserving task outcomes. We theoretically establish a\ndirect relationship between the resulting error probability of these tasks and\nthe mutual information between source texts and generated summaries. We\nintroduce $\\texttt{COSMIC}$ as a practical implementation of this metric,\ndemonstrating its strong correlation with human judgment-based metrics and its\neffectiveness in predicting downstream task performance. Comparative analyses\nagainst established metrics like $\\texttt{BERTScore}$ and $\\texttt{ROUGE}$\nhighlight the competitive performance of $\\texttt{COSMIC}$.", "published": "2024-02-29 18:51:23", "link": "http://arxiv.org/abs/2402.19457v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period\n  of Large Language Models", "abstract": "Ensuring the trustworthiness of large language models (LLMs) is crucial. Most\nstudies concentrate on fully pre-trained LLMs to better understand and improve\nLLMs' trustworthiness. In this paper, to reveal the untapped potential of\npre-training, we pioneer the exploration of LLMs' trustworthiness during this\nperiod, focusing on five key dimensions: reliability, privacy, toxicity,\nfairness, and robustness. To begin with, we apply linear probing to LLMs. The\nhigh probing accuracy suggests that \\textit{LLMs in early pre-training can\nalready distinguish concepts in each trustworthiness dimension}. Therefore, to\nfurther uncover the hidden possibilities of pre-training, we extract steering\nvectors from a LLM's pre-training checkpoints to enhance the LLM's\ntrustworthiness. Finally, inspired by~\\citet{choi2023understanding} that mutual\ninformation estimation is bounded by linear probing accuracy, we also probe\nLLMs with mutual information to investigate the dynamics of trustworthiness\nduring pre-training. We are the first to observe a similar two-phase\nphenomenon: fitting and compression~\\citep{shwartz2017opening}. This research\nprovides an initial exploration of trustworthiness modeling during LLM\npre-training, seeking to unveil new insights and spur further developments in\nthe field. We will make our code publicly accessible at\n\\url{https://github.com/ChnQ/TracingLLM}.", "published": "2024-02-29 18:55:06", "link": "http://arxiv.org/abs/2402.19465v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Loose LIPS Sink Ships: Asking Questions in Battleship with\n  Language-Informed Program Sampling", "abstract": "Questions combine our mastery of language with our remarkable facility for\nreasoning about uncertainty. How do people navigate vast hypothesis spaces to\npose informative questions given limited cognitive resources? We study these\ntradeoffs in a classic grounded question-asking task based on the board game\nBattleship. Our language-informed program sampling (LIPS) model uses large\nlanguage models (LLMs) to generate natural language questions, translate them\ninto symbolic programs, and evaluate their expected information gain. We find\nthat with a surprisingly modest resource budget, this simple Monte Carlo\noptimization strategy yields informative questions that mirror human\nperformance across varied Battleship board scenarios. In contrast, LLM-only\nbaselines struggle to ground questions in the board state; notably, GPT-4V\nprovides no improvement over non-visual baselines. Our results illustrate how\nBayesian models of question-asking can leverage the statistics of language to\ncapture human priors, while highlighting some shortcomings of pure LLMs as\ngrounded reasoners.", "published": "2024-02-29 18:58:15", "link": "http://arxiv.org/abs/2402.19471v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Resonance RoPE: Improving Context Length Generalization of Large\n  Language Models", "abstract": "This paper addresses the challenge of train-short-test-long (TSTL) scenarios\nin Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE),\nwhere models pre-trained on shorter sequences face difficulty with\nout-of-distribution (OOD) token positions in longer sequences. We introduce\nResonance RoPE, a novel approach designed to narrow the generalization gap in\nTSTL scenarios by refining the interpolation of RoPE features for OOD\npositions, significantly improving the model performance without additional\nonline computational costs. Furthermore, we present PosGen, a new synthetic\nbenchmark specifically designed for fine-grained behavior analysis in TSTL\nscenarios, aiming to isolate the constantly increasing difficulty of token\ngeneration on long contexts from the challenges of recognizing new token\npositions. Our experiments on synthetic tasks show that after applying\nResonance RoPE, Transformers recognize OOD position better and more robustly.\nOur extensive LLM experiments also show superior performance after applying\nResonance RoPE to the current state-of-the-art RoPE scaling method, YaRN, on\nboth upstream language modeling tasks and a variety of downstream long-text\napplications.", "published": "2024-02-29 19:02:03", "link": "http://arxiv.org/abs/2403.00071v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EROS: Entity-Driven Controlled Policy Document Summarization", "abstract": "Privacy policy documents have a crucial role in educating individuals about\nthe collection, usage, and protection of users' personal data by organizations.\nHowever, they are notorious for their lengthy, complex, and convoluted language\nespecially involving privacy-related entities. Hence, they pose a significant\nchallenge to users who attempt to comprehend organization's data usage policy.\nIn this paper, we propose to enhance the interpretability and readability of\npolicy documents by using controlled abstractive summarization -- we enforce\nthe generated summaries to include critical privacy-related entities (e.g.,\ndata and medium) and organization's rationale (e.g.,target and reason) in\ncollecting those entities. To achieve this, we develop PD-Sum, a\npolicy-document summarization dataset with marked privacy-related entity\nlabels. Our proposed model, EROS, identifies critical entities through a\nspan-based entity extraction model and employs them to control the information\ncontent of the summaries using proximal policy optimization (PPO). Comparison\nshows encouraging improvement over various baselines. Furthermore, we furnish\nqualitative and human evaluations to establish the efficacy of EROS.", "published": "2024-02-29 21:44:50", "link": "http://arxiv.org/abs/2403.00141v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text\n  Classification with Minimal Supervision", "abstract": "Hierarchical text classification aims to categorize each document into a set\nof classes in a label taxonomy, which is a fundamental web text mining task\nwith broad applications such as web content analysis and semantic indexing.\nMost earlier works focus on fully or semi-supervised methods that require a\nlarge amount of human annotated data which is costly and time-consuming to\nacquire. To alleviate human efforts, in this paper, we work on hierarchical\ntext classification with a minimal amount of supervision: using the sole class\nname of each node as the only supervision. Recently, large language models\n(LLM) have shown competitive performance on various tasks through zero-shot\nprompting, but this method performs poorly in the hierarchical setting because\nit is ineffective to include the large and structured label space in a prompt.\nOn the other hand, previous weakly-supervised hierarchical text classification\nmethods only utilize the raw taxonomy skeleton and ignore the rich information\nhidden in the text corpus that can serve as additional class-indicative\nfeatures. To tackle the above challenges, we propose TELEClass, Taxonomy\nEnrichment and LLM-Enhanced weakly-supervised hierarchical text Classification,\nwhich combines the general knowledge of LLMs and task-specific features mined\nfrom an unlabeled corpus. TELEClass automatically enriches the raw taxonomy\nwith class-indicative features for better label space understanding and\nutilizes novel LLM-based data annotation and generation methods specifically\ntailored for the hierarchical setting. Experiments show that TELEClass can\nsignificantly outperform previous baselines while achieving comparable\nperformance to zero-shot prompting of LLMs with drastically less inference\ncost.", "published": "2024-02-29 22:26:07", "link": "http://arxiv.org/abs/2403.00165v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ToolNet: Connecting Large Language Models with Massive Tools via Tool\n  Graph", "abstract": "While achieving remarkable progress in a broad range of tasks, large language\nmodels (LLMs) remain significantly limited in properly using massive external\ntools. Existing in-context learning approaches simply format tools into a list\nof plain text descriptions and input them to LLMs, from which, LLMs generate a\nsequence of tool calls to solve problems step by step. Such a paradigm ignores\nthe intrinsic dependency between tools and offloads all reasoning loads to\nLLMs, making them restricted to a limited number of specifically designed\ntools. It thus remains challenging for LLMs to operate on a library of massive\ntools, casting a great limitation when confronted with real-world scenarios.\nThis paper proposes ToolNet, a plug-and-play framework that scales up the\nnumber of tools to thousands with a moderate increase in token consumption.\nToolNet organizes tools into a directed graph. Each node represents a tool, and\nweighted edges denote tool transition. Starting from an initial tool node, an\nLLM navigates in the graph by iteratively choosing the next one from its\nsuccessors until the task is resolved. Extensive experiments show that ToolNet\ncan achieve impressive results in challenging multi-hop tool learning datasets\nand is resilient to tool failures.", "published": "2024-02-29 02:04:00", "link": "http://arxiv.org/abs/2403.00839v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "EyeGPT: Ophthalmic Assistant with Large Language Models", "abstract": "Artificial intelligence (AI) has gained significant attention in healthcare\nconsultation due to its potential to improve clinical workflow and enhance\nmedical communication. However, owing to the complex nature of medical\ninformation, large language models (LLM) trained with general world knowledge\nmight not possess the capability to tackle medical-related tasks at an expert\nlevel. Here, we introduce EyeGPT, a specialized LLM designed specifically for\nophthalmology, using three optimization strategies including role-playing,\nfinetuning, and retrieval-augmented generation. In particular, we proposed a\ncomprehensive evaluation framework that encompasses a diverse dataset, covering\nvarious subspecialties of ophthalmology, different users, and diverse inquiry\nintents. Moreover, we considered multiple evaluation metrics, including\naccuracy, understandability, trustworthiness, empathy, and the proportion of\nhallucinations. By assessing the performance of different EyeGPT variants, we\nidentify the most effective one, which exhibits comparable levels of\nunderstandability, trustworthiness, and empathy to human ophthalmologists (all\nPs>0.05). Overall, ur study provides valuable insights for future research,\nfacilitating comprehensive comparisons and evaluations of different strategies\nfor developing specialized LLMs in ophthalmology. The potential benefits\ninclude enhancing the patient experience in eye care and optimizing\nophthalmologists' services.", "published": "2024-02-29 09:35:41", "link": "http://arxiv.org/abs/2403.00840v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NewsBench: A Systematic Evaluation Framework for Assessing Editorial\n  Capabilities of Large Language Models in Chinese Journalism", "abstract": "We present NewsBench, a novel evaluation framework to systematically assess\nthe capabilities of Large Language Models (LLMs) for editorial capabilities in\nChinese journalism. Our constructed benchmark dataset is focused on four facets\nof writing proficiency and six facets of safety adherence, and it comprises\nmanually and carefully designed 1,267 test samples in the types of multiple\nchoice questions and short answer questions for five editorial tasks in 24 news\ndomains. To measure performances, we propose different GPT-4 based automatic\nevaluation protocols to assess LLM generations for short answer questions in\nterms of writing proficiency and safety adherence, and both are validated by\nthe high correlations with human evaluations. Based on the systematic\nevaluation framework, we conduct a comprehensive analysis of ten popular LLMs\nwhich can handle Chinese. The experimental results highlight GPT-4 and ERNIE\nBot as top performers, yet reveal a relative deficiency in journalistic safety\nadherence in creative writing tasks. Our findings also underscore the need for\nenhanced ethical guidance in machine-generated journalistic content, marking a\nstep forward in aligning LLMs with journalistic standards and safety\nconsiderations.", "published": "2024-02-29 21:05:14", "link": "http://arxiv.org/abs/2403.00862v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "X-AMR Annotation Tool", "abstract": "This paper presents a novel Cross-document Abstract Meaning Representation\n(X-AMR) annotation tool designed for annotating key corpus-level event\nsemantics. Leveraging machine assistance through the Prodigy Annotation Tool,\nwe enhance the user experience, ensuring ease and efficiency in the annotation\nprocess. Through empirical analyses, we demonstrate the effectiveness of our\ntool in augmenting an existing event corpus, highlighting its advantages when\nintegrated with GPT-4. Code and annotations:\nhttps://github.com/ahmeshaf/gpt_coref", "published": "2024-02-29 05:16:19", "link": "http://arxiv.org/abs/2403.15407v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FlexLLM: A System for Co-Serving Large Language Model Inference and\n  Parameter-Efficient Finetuning", "abstract": "Parameter-efficient finetuning (PEFT) is a widely used technique to adapt\nlarge language models for different tasks. Service providers typically create\nseparate systems for users to perform PEFT model finetuning and inference\ntasks. This is because existing systems cannot handle workloads that include a\nmix of inference and PEFT finetuning requests. As a result, shared GPU\nresources are underutilized, leading to inefficiencies. To address this\nproblem, we present FlexLLM, the first system that can serve inference and\nparameter-efficient finetuning requests in the same iteration. Our system\nleverages the complementary nature of these two tasks and utilizes shared GPU\nresources to run them jointly, using a method called co-serving. To achieve\nthis, FlexLLM introduces a novel token-level finetuning mechanism, which breaks\ndown the finetuning computation of a sequence into smaller token-level\ncomputations and uses dependent parallelization and graph pruning, two static\ncompilation optimizations, to minimize the memory overhead and latency for\nco-serving. Compared to existing systems, FlexLLM's co-serving approach reduces\nthe activation GPU memory overhead by up to 8x, and the end-to-end GPU memory\nrequirement of finetuning by up to 36% while maintaining a low inference\nlatency and improving finetuning throughput. For example, under a heavy\ninference workload, FlexLLM can still preserve more than 80% of the peak\nfinetuning throughput, whereas existing systems cannot make any progress with\nfinetuning. The source code of FlexLLM is publicly available at\nhttps://github.com/flexflow/FlexFlow.", "published": "2024-02-29 01:33:08", "link": "http://arxiv.org/abs/2402.18789v1", "categories": ["cs.DC", "cs.CL", "cs.LG"], "primary_category": "cs.DC"}
{"title": "MPAT: Building Robust Deep Neural Networks against Textual Adversarial\n  Attacks", "abstract": "Deep neural networks have been proven to be vulnerable to adversarial\nexamples and various methods have been proposed to defend against adversarial\nattacks for natural language processing tasks. However, previous defense\nmethods have limitations in maintaining effective defense while ensuring the\nperformance of the original task. In this paper, we propose a malicious\nperturbation based adversarial training method (MPAT) for building robust deep\nneural networks against textual adversarial attacks. Specifically, we construct\na multi-level malicious example generation strategy to generate adversarial\nexamples with malicious perturbations, which are used instead of original\ninputs for model training. Additionally, we employ a novel training objective\nfunction to ensure achieving the defense goal without compromising the\nperformance on the original task. We conduct comprehensive experiments to\nevaluate our defense method by attacking five victim models on three benchmark\ndatasets. The result demonstrates that our method is more effective against\nmalicious adversarial attacks compared with previous defense methods while\nmaintaining or further improving the performance on the original task.", "published": "2024-02-29 01:49:18", "link": "http://arxiv.org/abs/2402.18792v1", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Enhancing Steganographic Text Extraction: Evaluating the Impact of NLP\n  Models on Accuracy and Semantic Coherence", "abstract": "This study discusses a new method combining image steganography technology\nwith Natural Language Processing (NLP) large models, aimed at improving the\naccuracy and robustness of extracting steganographic text. Traditional Least\nSignificant Bit (LSB) steganography techniques face challenges in accuracy and\nrobustness of information extraction when dealing with complex character\nencoding, such as Chinese characters. To address this issue, this study\nproposes an innovative LSB-NLP hybrid framework. This framework integrates the\nadvanced capabilities of NLP large models, such as error detection, correction,\nand semantic consistency analysis, as well as information reconstruction\ntechniques, thereby significantly enhancing the robustness of steganographic\ntext extraction. Experimental results show that the LSB-NLP hybrid framework\nexcels in improving the extraction accuracy of steganographic text, especially\nin handling Chinese characters. The findings of this study not only confirm the\neffectiveness of combining image steganography technology and NLP large models\nbut also propose new ideas for research and application in the field of\ninformation hiding. The successful implementation of this interdisciplinary\napproach demonstrates the great potential of integrating image steganography\ntechnology with natural language processing technology in solving complex\ninformation processing problems.", "published": "2024-02-29 04:53:06", "link": "http://arxiv.org/abs/2402.18849v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient\n  Tuning", "abstract": "Existing research has shown that large language models (LLMs) exhibit\nremarkable performance in language understanding and generation. However, when\nLLMs are continuously fine-tuned on complex and diverse domain-specific\ndownstream tasks, the inference performance on historical tasks decreases\ndramatically, which is known as a catastrophic forgetting problem. A trade-off\nneeds to be kept between learning plasticity and memory stability. Plenty of\nexisting works have explored strategies like memory replay, regularization and\nparameter isolation, but little is known about the geometric connection of\nvarious adjacent minima in the continual LLMs fine-tuning scenarios. In this\nwork, we investigate the geometric connections of different minima through the\nlens of mode connectivity, which means different minima can be connected by a\nlow-loss valley. Through extensive experiments, we uncover the mode\nconnectivity phenomenon in the LLMs continual learning scenario and find that\nit can strike a balance between plasticity and stability. Building upon these\nfindings, we propose a simple yet effective method called Interpolation-based\nLoRA (I-LoRA), which constructs a dual-memory experience replay framework based\non LoRA parameter interpolations. Extensive experiments and analysis on eight\ndomain-specific CL benchmarks demonstrate that I-LoRA consistently show\nsignificant improvement over the previous state-of-the-art approaches with up\nto $11\\%$ performance gains, providing a strong baseline and insights for\nfuture research on the large language model continual learning problem. Our\ncode is available at \\url{https://github.com/which47/LLMCL}.", "published": "2024-02-29 05:27:45", "link": "http://arxiv.org/abs/2402.18865v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Inappropriate Pause Detection In Dysarthric Speech Using Large-Scale\n  Speech Recognition", "abstract": "Dysarthria, a common issue among stroke patients, severely impacts speech\nintelligibility. Inappropriate pauses are crucial indicators in severity\nassessment and speech-language therapy. We propose to extend a large-scale\nspeech recognition model for inappropriate pause detection in dysarthric\nspeech. To this end, we propose task design, labeling strategy, and a speech\nrecognition model with an inappropriate pause prediction layer. First, we treat\npause detection as speech recognition, using an automatic speech recognition\n(ASR) model to convert speech into text with pause tags. According to the newly\ndesigned task, we label pause locations at the text level and their\nappropriateness. We collaborate with speech-language pathologists to establish\nlabeling criteria, ensuring high-quality annotated data. Finally, we extend the\nASR model with an inappropriate pause prediction layer for end-to-end\ninappropriate pause detection. Moreover, we propose a task-tailored metric for\nevaluating inappropriate pause detection independent of ASR performance. Our\nexperiments show that the proposed method better detects inappropriate pauses\nin dysarthric speech than baselines. (Inappropriate Pause Error Rate: 14.47%)", "published": "2024-02-29 07:29:42", "link": "http://arxiv.org/abs/2402.18923v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SynGhost: Invisible and Universal Task-agnostic Backdoor Attack via\n  Syntactic Transfer", "abstract": "Although pre-training achieves remarkable performance, it suffers from\ntask-agnostic backdoor attacks due to vulnerabilities in data and training\nmechanisms. These attacks can transfer backdoors to various downstream tasks.\nIn this paper, we introduce $\\mathtt{maxEntropy}$, an entropy-based poisoning\nfilter that mitigates such risks. To overcome the limitations of manual target\nsetting and explicit triggers, we propose $\\mathtt{SynGhost}$, an invisible and\nuniversal task-agnostic backdoor attack via syntactic transfer, further\nexposing vulnerabilities in pre-trained language models (PLMs). Specifically,\n$\\mathtt{SynGhost}$ injects multiple syntactic backdoors into the pre-training\nspace through corpus poisoning, while preserving the PLM's pre-training\ncapabilities. Second, $\\mathtt{SynGhost}$ adaptively selects optimal targets\nbased on contrastive learning, creating a uniform distribution in the\npre-training space. To identify syntactic differences, we also introduce an\nawareness module to minimize interference between backdoors. Experiments show\nthat $\\mathtt{SynGhost}$ poses significant threats and can transfer to various\ndownstream tasks. Furthermore, $\\mathtt{SynGhost}$ resists defenses based on\nperplexity, fine-pruning, and $\\mathtt{maxEntropy}$. The code is available at\nhttps://github.com/Zhou-CyberSecurity-AI/SynGhost.", "published": "2024-02-29 08:20:49", "link": "http://arxiv.org/abs/2402.18945v4", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Controllable Preference Optimization: Toward Controllable\n  Multi-Objective Alignment", "abstract": "Alignment in artificial intelligence pursues the consistency between model\nresponses and human preferences as well as values. In practice, the\nmultifaceted nature of human preferences inadvertently introduces what is known\nas the \"alignment tax\" -a compromise where enhancements in alignment within one\nobjective (e.g.,harmlessness) can diminish performance in others\n(e.g.,helpfulness). However, existing alignment techniques are mostly\nunidirectional, leading to suboptimal trade-offs and poor flexibility over\nvarious objectives. To navigate this challenge, we argue the prominence of\ngrounding LLMs with evident preferences. We introduce controllable preference\noptimization (CPO), which explicitly specifies preference scores for different\nobjectives, thereby guiding the model to generate responses that meet the\nrequirements. Our experimental analysis reveals that the aligned models can\nprovide responses that match various preferences among the \"3H\" (helpfulness,\nhonesty, harmlessness) desiderata. Furthermore, by introducing diverse data and\nalignment goals, we surpass baseline methods in aligning with single\nobjectives, hence mitigating the impact of the alignment tax and achieving\nimprovements in multi-objective alignment.", "published": "2024-02-29 12:12:30", "link": "http://arxiv.org/abs/2402.19085v3", "categories": ["cs.CL", "cs.AI", "cs.SY", "eess.SY"], "primary_category": "cs.CL"}
{"title": "Compact Speech Translation Models via Discrete Speech Units Pretraining", "abstract": "We propose a pretraining method to use Self-Supervised Speech (SSS) model to\ncreating more compact Speech-to-text Translation. In contrast to using the SSS\nmodel for initialization, our method is more suitable to memory constrained\nscenario such as on-device deployment. Our method is based on Discrete Speech\nUnits (DSU) extracted from the SSS model. In the first step, our method\npretrains two smaller encoder-decoder models on 1) Filterbank-to-DSU\n(Fbk-to-DSU) and 2) DSU-to-Translation (DSU-to-Trl) data respectively. The DSU\nthus become the distillation inputs of the smaller models. Subsequently, the\nencoder from the Fbk-to-DSU model and the decoder from the DSU-to-Trl model are\ntaken to initialise the compact model. Finally, the compact model is finetuned\non the paired Fbk-Trl data. In addition to being compact, our method requires\nno transcripts, making it applicable to low-resource settings. It also avoids\nspeech discretization in inference and is more robust to the DSU tokenization.\nEvaluation on CoVoST-2 (X-En) shows that our method has consistent improvement\nover the baseline in three metrics while being compact i.e., only half the SSS\nmodel size.", "published": "2024-02-29 16:36:51", "link": "http://arxiv.org/abs/2402.19333v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "OpenMedLM: Prompt engineering can out-perform fine-tuning in medical\n  question-answering with open-source large language models", "abstract": "LLMs have become increasingly capable at accomplishing a range of\nspecialized-tasks and can be utilized to expand equitable access to medical\nknowledge. Most medical LLMs have involved extensive fine-tuning, leveraging\nspecialized medical data and significant, thus costly, amounts of computational\npower. Many of the top performing LLMs are proprietary and their access is\nlimited to very few research groups. However, open-source (OS) models represent\na key area of growth for medical LLMs due to significant improvements in\nperformance and an inherent ability to provide the transparency and compliance\nrequired in healthcare. We present OpenMedLM, a prompting platform which\ndelivers state-of-the-art (SOTA) performance for OS LLMs on medical benchmarks.\nWe evaluated a range of OS foundation LLMs (7B-70B) on four medical benchmarks\n(MedQA, MedMCQA, PubMedQA, MMLU medical-subset). We employed a series of\nprompting strategies, including zero-shot, few-shot, chain-of-thought (random\nselection and kNN selection), and ensemble/self-consistency voting. We found\nthat OpenMedLM delivers OS SOTA results on three common medical LLM benchmarks,\nsurpassing the previous best performing OS models that leveraged\ncomputationally costly extensive fine-tuning. The model delivers a 72.6%\naccuracy on the MedQA benchmark, outperforming the previous SOTA by 2.4%, and\nachieves 81.7% accuracy on the MMLU medical-subset, establishing itself as the\nfirst OS LLM to surpass 80% accuracy on this benchmark. Our results highlight\nmedical-specific emergent properties in OS LLMs which have not yet been\ndocumented to date elsewhere, and showcase the benefits of further leveraging\nprompt engineering to improve the performance of accessible LLMs for medical\napplications.", "published": "2024-02-29 17:19:39", "link": "http://arxiv.org/abs/2402.19371v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Rival\n  Human Crowd Accuracy", "abstract": "Human forecasting accuracy in practice relies on the 'wisdom of the crowd'\neffect, in which predictions about future events are significantly improved by\naggregating across a crowd of individual forecasters. Past work on the\nforecasting ability of large language models (LLMs) suggests that frontier\nLLMs, as individual forecasters, underperform compared to the gold standard of\na human crowd forecasting tournament aggregate. In Study 1, we expand this\nresearch by using an LLM ensemble approach consisting of a crowd of twelve\nLLMs. We compare the aggregated LLM predictions on 31 binary questions to that\nof a crowd of 925 human forecasters from a three-month forecasting tournament.\nOur preregistered main analysis shows that the LLM crowd outperforms a simple\nno-information benchmark and is not statistically different from the human\ncrowd. In exploratory analyses, we find that these two approaches are\nequivalent with respect to medium-effect-size equivalence bounds. We also\nobserve an acquiescence effect, with mean model predictions being significantly\nabove 50%, despite an almost even split of positive and negative resolutions.\nMoreover, in Study 2, we test whether LLM predictions (of GPT-4 and Claude 2)\ncan be improved by drawing on human cognitive output. We find that both models'\nforecasting accuracy benefits from exposure to the median human prediction as\ninformation, improving accuracy by between 17% and 28%: though this leads to\nless accurate predictions than simply averaging human and machine forecasts.\nOur results suggest that LLMs can achieve forecasting accuracy rivaling that of\nhuman crowd forecasting tournaments: via the simple, practically applicable\nmethod of forecast aggregation. This replicates the 'wisdom of the crowd'\neffect for LLMs, and opens up their use for a variety of applications\nthroughout society.", "published": "2024-02-29 17:27:59", "link": "http://arxiv.org/abs/2402.19379v6", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "PaECTER: Patent-level Representation Learning using Citation-informed\n  Transformers", "abstract": "PaECTER is a publicly available, open-source document-level encoder specific\nfor patents. We fine-tune BERT for Patents with examiner-added citation\ninformation to generate numerical representations for patent documents. PaECTER\nperforms better in similarity tasks than current state-of-the-art models used\nin the patent domain. More specifically, our model outperforms the next-best\npatent specific pre-trained language model (BERT for Patents) on our patent\ncitation prediction test dataset on two different rank evaluation metrics.\nPaECTER predicts at least one most similar patent at a rank of 1.32 on average\nwhen compared against 25 irrelevant patents. Numerical representations\ngenerated by PaECTER from patent text can be used for downstream tasks such as\nclassification, tracing knowledge flows, or semantic similarity search.\nSemantic similarity search is especially relevant in the context of prior art\nsearch for both inventors and patent examiners. PaECTER is available on Hugging\nFace.", "published": "2024-02-29 18:09:03", "link": "http://arxiv.org/abs/2402.19411v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Compositional API Recommendation for Library-Oriented Code Generation", "abstract": "Large language models (LLMs) have achieved exceptional performance in code\ngeneration. However, the performance remains unsatisfactory in generating\nlibrary-oriented code, especially for the libraries not present in the training\ndata of LLMs. Previous work utilizes API recommendation technology to help LLMs\nuse libraries: it retrieves APIs related to the user requirements, then\nleverages them as context to prompt LLMs. However, developmental requirements\ncan be coarse-grained, requiring a combination of multiple fine-grained APIs.\nThis granularity inconsistency makes API recommendation a challenging task. To\naddress this, we propose CAPIR (Compositional API Recommendation), which adopts\na \"divide-and-conquer\" strategy to recommend APIs for coarse-grained\nrequirements. Specifically, CAPIR employs an LLM-based Decomposer to break down\na coarse-grained task description into several detailed subtasks. Then, CAPIR\napplies an embedding-based Retriever to identify relevant APIs corresponding to\neach subtask. Moreover, CAPIR leverages an LLM-based Reranker to filter out\nredundant APIs and provides the final recommendation. To facilitate the\nevaluation of API recommendation methods on coarse-grained requirements, we\npresent two challenging benchmarks, RAPID (Recommend APIs based on\nDocumentation) and LOCG (Library-Oriented Code Generation). Experimental\nresults on these benchmarks, demonstrate the effectiveness of CAPIR in\ncomparison to existing baselines. Specifically, on RAPID's Torchdata-AR\ndataset, compared to the state-of-the-art API recommendation approach, CAPIR\nimproves recall@5 from 18.7% to 43.2% and precision@5 from 15.5% to 37.1%. On\nLOCG's Torchdata-Code dataset, compared to code generation without API\nrecommendation, CAPIR improves pass@100 from 16.0% to 28.0%.", "published": "2024-02-29 18:27:27", "link": "http://arxiv.org/abs/2402.19431v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL", "abstract": "A broad use case of large language models (LLMs) is in goal-directed\ndecision-making tasks (or \"agent\" tasks), where an LLM needs to not just\ngenerate completions for a given prompt, but rather make intelligent decisions\nover a multi-turn interaction to accomplish a task (e.g., when interacting with\nthe web, using tools, or providing customer support). Reinforcement learning\n(RL) provides a general paradigm to address such agent tasks, but current RL\nmethods for LLMs largely focus on optimizing single-turn rewards. By\nconstruction, most single-turn RL methods cannot endow LLMs with the ability to\nintelligently seek information over multiple turns, perform credit assignment,\nor reason about their past actions -- all of which are critical in agent tasks.\nThis raises the question: how can we design effective and efficient multi-turn\nRL algorithms for LLMs? In this paper, we develop a framework for building\nmulti-turn RL algorithms for fine-tuning LLMs, that preserves the flexibility\nof existing single-turn RL methods for LLMs (e.g., proximal policy\noptimization), while accommodating multiple turns, long horizons, and delayed\nrewards effectively. To do this, our framework adopts a hierarchical RL\napproach and runs two RL algorithms in parallel: a high-level off-policy\nvalue-based RL algorithm to aggregate reward over utterances, and a low-level\nRL algorithm that utilizes this high-level value function to train a token\npolicy within each utterance or turn. Our hierarchical framework, Actor-Critic\nFramework with a Hierarchical Structure (ArCHer), can also give rise to other\nRL methods. Empirically, we find that ArCHer significantly improves efficiency\nand performance on agent tasks, attaining a sample efficiency of about 100x\nover existing methods, while also improving with larger model capacity (upto\nthe 7 billion scale that we tested on).", "published": "2024-02-29 18:45:56", "link": "http://arxiv.org/abs/2402.19446v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent\n  on Language Models", "abstract": "Adam has been shown to outperform gradient descent on large language models\nby a larger margin than on other tasks, but it is unclear why. We show that a\nkey factor in this performance gap is the heavy-tailed class imbalance found in\nlanguage tasks. When trained with gradient descent, the loss of infrequent\nwords decreases more slowly than the loss of frequent ones. This leads to a\nslow decrease on the average loss as most samples come from infrequent words.\nOn the other hand, Adam and sign-based methods are less sensitive to this\nproblem. To establish that this behavior is caused by class imbalance, we show\nempirically that it can be reproduced across architectures and data types, on\nlanguage transformers, vision CNNs, and linear models. On a linear model with\ncross-entropy loss, we show that class imbalance leads to imbalanced,\ncorrelated gradients and Hessians that have been hypothesized to benefit Adam.\nWe also prove that, in continuous time, gradient descent converges slowly on\nlow-frequency classes while sign descent does not.", "published": "2024-02-29 18:47:52", "link": "http://arxiv.org/abs/2402.19449v2", "categories": ["cs.LG", "cs.CL", "math.OC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Accelerating materials discovery for polymer solar cells: Data-driven\n  insights enabled by natural language processing", "abstract": "We present a simulation of various active learning strategies for the\ndiscovery of polymer solar cell donor/acceptor pairs using data extracted from\nthe literature spanning $\\sim$20 years by a natural language processing\npipeline. While data-driven methods have been well established to discover\nnovel materials faster than Edisonian trial-and-error approaches, their\nbenefits have not been quantified for material discovery problems that can take\ndecades. Our approach demonstrates a potential reduction in discovery time by\napproximately 75 %, equivalent to a 15 year acceleration in material\ninnovation. Our pipeline enables us to extract data from greater than 3300\npapers which is $\\sim$5 times larger and therefore more diverse than similar\ndata sets reported by others. We also trained machine learning models to\npredict the power conversion efficiency and used our model to identify\npromising donor-acceptor combinations that are as yet unreported. We thus\ndemonstrate a pipeline that goes from published literature to extracted\nmaterial property data which in turn is used to obtain data-driven insights.\nOur insights include active learning strategies that can be used to train\nstrong predictive models of material properties or be robust to the initial\nmaterial system used. This work provides a valuable framework for data-driven\nresearch in materials science.", "published": "2024-02-29 18:54:46", "link": "http://arxiv.org/abs/2402.19462v2", "categories": ["cond-mat.mtrl-sci", "cs.CL", "physics.app-ph"], "primary_category": "cond-mat.mtrl-sci"}
{"title": "Curiosity-driven Red-teaming for Large Language Models", "abstract": "Large language models (LLMs) hold great potential for many natural language\napplications but risk generating incorrect or toxic content. To probe when an\nLLM generates unwanted content, the current paradigm is to recruit a\n\\textit{red team} of human testers to design input prompts (i.e., test cases)\nthat elicit undesirable responses from LLMs. However, relying solely on human\ntesters is expensive and time-consuming. Recent works automate red teaming by\ntraining a separate red team LLM with reinforcement learning (RL) to generate\ntest cases that maximize the chance of eliciting undesirable responses from the\ntarget LLM. However, current RL methods are only able to generate a small\nnumber of effective test cases resulting in a low coverage of the span of\nprompts that elicit undesirable responses from the target LLM. To overcome this\nlimitation, we draw a connection between the problem of increasing the coverage\nof generated test cases and the well-studied approach of curiosity-driven\nexploration that optimizes for novelty. Our method of curiosity-driven red\nteaming (CRT) achieves greater coverage of test cases while mantaining or\nincreasing their effectiveness compared to existing methods. Our method, CRT\nsuccessfully provokes toxic responses from LLaMA2 model that has been heavily\nfine-tuned using human preferences to avoid toxic outputs. Code is available at\n\\url{https://github.com/Improbable-AI/curiosity_redteam}", "published": "2024-02-29 18:55:03", "link": "http://arxiv.org/abs/2402.19464v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning", "abstract": "It is challenging for models to understand complex, multimodal content such\nas television clips, and this is in part because video-language models often\nrely on single-modality reasoning and lack interpretability. To combat these\nissues we propose TV-TREES, the first multimodal entailment tree generator.\nTV-TREES serves as an approach to video understanding that promotes\ninterpretable joint-modality reasoning by searching for trees of entailment\nrelationships between simple text-video evidence and higher-level conclusions\nthat prove question-answer pairs. We also introduce the task of multimodal\nentailment tree generation to evaluate reasoning quality. Our method's\nperformance on the challenging TVQA benchmark demonstrates interpretable,\nstate-of-the-art zero-shot performance on full clips, illustrating that\nmultimodal entailment tree generation can be a best-of-both-worlds alternative\nto black-box systems.", "published": "2024-02-29 18:57:01", "link": "http://arxiv.org/abs/2402.19467v4", "categories": ["cs.CL", "cs.AI", "cs.CV", "I.2.7; I.2.10"], "primary_category": "cs.CL"}
{"title": "Evolving to the Future: Unseen Event Adaptive Fake News Detection on\n  Social Media", "abstract": "With the rapid development of social media, the wide dissemination of fake\nnews on social media is increasingly threatening both individuals and society.\nOne of the unique challenges for fake news detection on social media is how to\ndetect fake news on future events. Recently, numerous fake news detection\nmodels that utilize textual information and the propagation structure of posts\nhave been proposed. Unfortunately, most of the existing approaches can hardly\nhandle this challenge since they rely heavily on event-specific features for\nprediction and cannot generalize to unseen events. To address this, we\nintroduce \\textbf{F}uture \\textbf{AD}aptive \\textbf{E}vent-based Fake news\nDetection (FADE) framework. Specifically, we train a target predictor through\nan adaptive augmentation strategy and graph contrastive learning to obtain\nhigher-quality features and make more accurate overall predictions.\nSimultaneously, we independently train an event-only predictor to obtain biased\npredictions. We further mitigate event bias by subtracting the event-only\npredictor's output from the target predictor's output to obtain the final\nprediction. Encouraging results from experiments designed to emulate real-world\nsocial media conditions validate the effectiveness of our method in comparison\nto existing state-of-the-art approaches.", "published": "2024-02-29 06:40:53", "link": "http://arxiv.org/abs/2403.00037v2", "categories": ["cs.SI", "cs.AI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "SEED: Customize Large Language Models with Sample-Efficient Adaptation\n  for Code Generation", "abstract": "Although Large Language Models (LLMs) have made significant progress in code\ngeneration, they still struggle with code generation tasks in specific\nscenarios. These scenarios usually necessitate the adaptation of LLMs to\nfulfill specific needs, but the limited training samples available in practice\nlead to poor code generation performance. Therefore, how to effectively adapt\nLLMs to new scenarios with few training samples is a major challenge for\ncurrent code generation. In this paper, we propose a novel adaptation approach\nnamed SEED, which stands for Sample-Efficient adaptation with Error-Driven\nlearning for code generation. SEED leverages the errors made by LLMs as\nlearning opportunities, using error revision to overcome its own shortcomings,\nthus achieving efficient learning. Specifically, SEED involves identifying\nerror code generated by LLMs, employing Self-revise for code revision,\noptimizing the model with revised code, and iteratively adapting the process\nfor continuous improvement. Experimental results show that, compared to other\nmainstream fine-tuning approaches, SEED achieves superior performance with few\ntraining samples, showing an average relative improvement of 54.7% in Pass@1 on\nmultiple code generation benchmarks. We also validate the effectiveness of\nSelf-revise, which generates revised code that optimizes the model more\nefficiently compared to the code samples from datasets. Moreover, SEED\nconsistently demonstrates strong performance across various LLMs, underscoring\nits generalizability.", "published": "2024-02-29 16:09:02", "link": "http://arxiv.org/abs/2403.00046v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario", "abstract": "Fine-tuning LLMs is crucial to enhancing their task-specific performance and\nensuring model behaviors are aligned with human preferences. Among various\nfine-tuning methods, LoRA is popular for its efficiency and ease to use,\nallowing end-users to easily post and adopt lightweight LoRA modules on\nopen-source platforms to tailor their model for different customization.\nHowever, such a handy share-and-play setting opens up new attack surfaces, that\nthe attacker can render LoRA as an attacker, such as backdoor injection, and\nwidely distribute the adversarial LoRA to the community easily. This can result\nin detrimental outcomes. Despite the huge potential risks of sharing LoRA\nmodules, this aspect however has not been fully explored. To fill the gap, in\nthis study we thoroughly investigate the attack opportunities enabled in the\ngrowing share-and-play scenario. Specifically, we study how to inject backdoor\ninto the LoRA module and dive deeper into LoRA's infection mechanisms. We found\nthat training-free mechanism is possible in LoRA backdoor injection. We also\ndiscover the impact of backdoor attacks with the presence of multiple LoRA\nadaptions concurrently as well as LoRA based backdoor transferability. Our aim\nis to raise awareness of the potential risks under the emerging share-and-play\nscenario, so as to proactively prevent potential consequences caused by\nLoRA-as-an-Attack. Warning: the paper contains potential offensive content\ngenerated by models.", "published": "2024-02-29 20:25:16", "link": "http://arxiv.org/abs/2403.00108v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Prompting ChatGPT for Translation: A Comparative Analysis of Translation\n  Brief and Persona Prompts", "abstract": "Prompt engineering has shown potential for improving translation quality in\nLLMs. However, the possibility of using translation concepts in prompt design\nremains largely underexplored. Against this backdrop, the current paper\ndiscusses the effectiveness of incorporating the conceptual tool of translation\nbrief and the personas of translator and author into prompt design for\ntranslation tasks in ChatGPT. Findings suggest that, although certain elements\nare constructive in facilitating human-to-human communication for translation\ntasks, their effectiveness is limited for improving translation quality in\nChatGPT. This accentuates the need for explorative research on how translation\ntheorists and practitioners can develop the current set of conceptual tools\nrooted in the human-to-human communication paradigm for translation purposes in\nthis emerging workflow involving human-machine interaction, and how translation\nconcepts developed in translation studies can inform the training of GPT models\nfor translation tasks.", "published": "2024-02-29 21:05:38", "link": "http://arxiv.org/abs/2403.00127v2", "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Tree-Averaging Algorithms for Ensemble-Based Unsupervised Discontinuous\n  Constituency Parsing", "abstract": "We address unsupervised discontinuous constituency parsing, where we observe\na high variance in the performance of the only previous model in the\nliterature. We propose to build an ensemble of different runs of the existing\ndiscontinuous parser by averaging the predicted trees, to stabilize and boost\nperformance. To begin with, we provide comprehensive computational complexity\nanalysis (in terms of P and NP-complete) for tree averaging under different\nsetups of binarity and continuity. We then develop an efficient exact algorithm\nto tackle the task, which runs in a reasonable time for all samples in our\nexperiments. Results on three datasets show our method outperforms all\nbaselines in all metrics; we also provide in-depth analyses of our approach.", "published": "2024-02-29 21:49:31", "link": "http://arxiv.org/abs/2403.00143v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EBBS: An Ensemble with Bi-Level Beam Search for Zero-Shot Machine\n  Translation", "abstract": "The ability of zero-shot translation emerges when we train a multilingual\nmodel with certain translation directions; the model can then directly\ntranslate in unseen directions. Alternatively, zero-shot translation can be\naccomplished by pivoting through a third language (e.g., English). In our work,\nwe observe that both direct and pivot translations are noisy and achieve less\nsatisfactory performance. We propose EBBS, an ensemble method with a novel\nbi-level beam search algorithm, where each ensemble component explores its own\nprediction step by step at the lower level but they are synchronized by a \"soft\nvoting\" mechanism at the upper level. Results on two popular multilingual\ntranslation datasets show that EBBS consistently outperforms direct and pivot\ntranslations as well as existing ensemble techniques. Further, we can distill\nthe ensemble's knowledge back to the multilingual model to improve inference\nefficiency; profoundly, our EBBS-based distillation does not sacrifice, or even\nimproves, the translation quality.", "published": "2024-02-29 21:49:31", "link": "http://arxiv.org/abs/2403.00144v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; I.2.m; I.5.1; I.7.m"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Learnable Planners for Long-Term\n  Recommendation", "abstract": "Planning for both immediate and long-term benefits becomes increasingly\nimportant in recommendation. Existing methods apply Reinforcement Learning (RL)\nto learn planning capacity by maximizing cumulative reward for long-term\nrecommendation. However, the scarcity of recommendation data presents\nchallenges such as instability and susceptibility to overfitting when training\nRL models from scratch, resulting in sub-optimal performance. In this light, we\npropose to leverage the remarkable planning capabilities over sparse data of\nLarge Language Models (LLMs) for long-term recommendation. The key to achieving\nthe target lies in formulating a guidance plan following principles of\nenhancing long-term engagement and grounding the plan to effective and\nexecutable actions in a personalized manner. To this end, we propose a Bi-level\nLearnable LLM Planner framework, which consists of a set of LLM instances and\nbreaks down the learning process into macro-learning and micro-learning to\nlearn macro-level guidance and micro-level personalized recommendation\npolicies, respectively. Extensive experiments validate that the framework\nfacilitates the planning ability of LLMs for long-term recommendation. Our code\nand data can be found at https://github.com/jizhi-zhang/BiLLP.", "published": "2024-02-29 13:49:56", "link": "http://arxiv.org/abs/2403.00843v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Direct Alignment of Draft Model for Speculative Decoding with\n  Chat-Fine-Tuned LLMs", "abstract": "Text generation with Large Language Models (LLMs) is known to be memory bound\ndue to the combination of their auto-regressive nature, huge parameter counts,\nand limited memory bandwidths, often resulting in low token rates. Speculative\ndecoding has been proposed as a solution for LLM inference acceleration.\nHowever, since draft models are often unavailable in the modern open-source LLM\nfamilies, e.g., for Llama 2 7B, training a high-quality draft model is required\nto enable inference acceleration via speculative decoding. In this paper, we\npropose a simple draft model training framework for direct alignment to\nchat-capable target models. With the proposed framework, we train Llama 2 Chat\nDrafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\\% of\nthe original size. Our training framework only consists of pretraining,\ndistillation dataset generation, and finetuning with knowledge distillation,\nwith no additional alignment procedure. For the finetuning step, we use\ninstruction-response pairs generated by target model for distillation in\nplausible data distribution, and propose a new Total Variation Distance++\n(TVD++) loss that incorporates variance reduction techniques inspired from the\npolicy gradient method in reinforcement learning. Our empirical results show\nthat Llama 2 Chat Drafter 115M with speculative decoding achieves up to 2.3\nblock efficiency and 2.4$\\times$ speed-up relative to autoregressive decoding\non various tasks with no further task-specific fine-tuning.", "published": "2024-02-29 19:55:06", "link": "http://arxiv.org/abs/2403.00858v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LLM-Ensemble: Optimal Large Language Model Ensemble Method for\n  E-commerce Product Attribute Value Extraction", "abstract": "Product attribute value extraction is a pivotal component in Natural Language\nProcessing (NLP) and the contemporary e-commerce industry. The provision of\nprecise product attribute values is fundamental in ensuring high-quality\nrecommendations and enhancing customer satisfaction. The recently emerging\nLarge Language Models (LLMs) have demonstrated state-of-the-art performance in\nnumerous attribute extraction tasks, without the need for domain-specific\ntraining data. Nevertheless, varying strengths and weaknesses are exhibited by\ndifferent LLMs due to the diversity in data, architectures, and\nhyperparameters. This variation makes them complementary to each other, with no\nsingle LLM dominating all others. Considering the diverse strengths and\nweaknesses of LLMs, it becomes necessary to develop an ensemble method that\nleverages their complementary potentials. In this paper, we propose a novel\nalgorithm called LLM-ensemble to ensemble different LLMs' outputs for attribute\nvalue extraction. We iteratively learn the weights for different LLMs to\naggregate the labels with weights to predict the final attribute value. Not\nonly can our proposed method be proven theoretically optimal, but it also\nensures efficient computation, fast convergence, and safe deployment. We have\nalso conducted extensive experiments with various state-of-the-art LLMs,\nincluding Llama2-13B, Llama2-70B, PaLM-2, GPT-3.5, and GPT-4, on Walmart's\ninternal data. Our offline metrics demonstrate that the LLM-ensemble method\noutperforms all the state-of-the-art single LLMs on Walmart's internal dataset.\nThis method has been launched in several production models, leading to improved\nGross Merchandise Volume (GMV), Click-Through Rate (CTR), Conversion Rate\n(CVR), and Add-to-Cart Rate (ATC).", "published": "2024-02-29 23:03:19", "link": "http://arxiv.org/abs/2403.00863v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Towards Modeling Learner Performance with Large Language Models", "abstract": "Recent work exploring the capabilities of pre-trained large language models\n(LLMs) has demonstrated their ability to act as general pattern machines by\ncompleting complex token sequences representing a wide array of tasks,\nincluding time-series prediction and robot control. This paper investigates\nwhether the pattern recognition and sequence modeling capabilities of LLMs can\nbe extended to the domain of knowledge tracing, a critical component in the\ndevelopment of intelligent tutoring systems (ITSs) that tailor educational\nexperiences by predicting learner performance over time. In an empirical\nevaluation across multiple real-world datasets, we compare two approaches to\nusing LLMs for this task, zero-shot prompting and model fine-tuning, with\nexisting, non-LLM approaches to knowledge tracing. While LLM-based approaches\ndo not achieve state-of-the-art performance, fine-tuned LLMs surpass the\nperformance of naive baseline models and perform on par with standard Bayesian\nKnowledge Tracing approaches across multiple metrics. These findings suggest\nthat the pattern recognition capabilities of LLMs can be used to model complex\nlearning trajectories, opening a novel avenue for applying LLMs to educational\ncontexts. The paper concludes with a discussion of the implications of these\nfindings for future research, suggesting that further refinements and a deeper\nunderstanding of LLMs' predictive mechanisms could lead to enhanced performance\nin knowledge tracing tasks.", "published": "2024-02-29 14:06:34", "link": "http://arxiv.org/abs/2403.14661v1", "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Speaker-Independent Dysarthria Severity Classification using\n  Self-Supervised Transformers and Multi-Task Learning", "abstract": "Dysarthria, a condition resulting from impaired control of the speech muscles\ndue to neurological disorders, significantly impacts the communication and\nquality of life of patients. The condition's complexity, human scoring and\nvaried presentations make its assessment and management challenging. This study\npresents a transformer-based framework for automatically assessing dysarthria\nseverity from raw speech data. It can offer an objective, repeatable,\naccessible, standardised and cost-effective and compared to traditional methods\nrequiring human expert assessors. We develop a transformer framework, called\nSpeaker-Agnostic Latent Regularisation (SALR), incorporating a multi-task\nlearning objective and contrastive learning for speaker-independent multi-class\ndysarthria severity classification. The multi-task framework is designed to\nreduce reliance on speaker-specific characteristics and address the intrinsic\nintra-class variability of dysarthric speech. We evaluated on the Universal\nAccess Speech dataset using leave-one-speaker-out cross-validation, our model\ndemonstrated superior performance over traditional machine learning approaches,\nwith an accuracy of $70.48\\%$ and an F1 score of $59.23\\%$. Our SALR model also\nexceeded the previous benchmark for AI-based classification, which used support\nvector machines, by $16.58\\%$. We open the black box of our model by\nvisualising the latent space where we can observe how the model substantially\nreduces speaker-specific cues and amplifies task-specific ones, thereby showing\nits robustness. In conclusion, SALR establishes a new benchmark in\nspeaker-independent multi-class dysarthria severity classification using\ngenerative AI. The potential implications of our findings for broader clinical\napplications in automated dysarthria severity assessments.", "published": "2024-02-29 18:30:52", "link": "http://arxiv.org/abs/2403.00854v1", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.LG", "cs.SD", "eess.AS", "I.2.7; I.2.1; J.3"], "primary_category": "q-bio.NC"}
{"title": "Extending Multilingual Speech Synthesis to 100+ Languages without\n  Transcribed Data", "abstract": "Collecting high-quality studio recordings of audio is challenging, which\nlimits the language coverage of text-to-speech (TTS) systems. This paper\nproposes a framework for scaling a multilingual TTS model to 100+ languages\nusing found data without supervision. The proposed framework combines\nspeech-text encoder pretraining with unsupervised training using untranscribed\nspeech and unspoken text data sources, thereby leveraging massively\nmultilingual joint speech and text representation learning. Without any\ntranscribed speech in a new language, this TTS model can generate intelligible\nspeech in >30 unseen languages (CER difference of <10% to ground truth). With\njust 15 minutes of transcribed, found data, we can reduce the intelligibility\ndifference to 1% or less from the ground-truth, and achieve naturalness scores\nthat match the ground-truth in several languages.", "published": "2024-02-29 07:49:10", "link": "http://arxiv.org/abs/2402.18932v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Ambisonics Networks -- The Effect Of Radial Functions Regularization", "abstract": "Ambisonics, a popular format of spatial audio, is the spherical harmonic (SH)\nrepresentation of the plane wave density function of a sound field. Many\nalgorithms operate in the SH domain and utilize the Ambisonics as their input\nsignal. The process of encoding Ambisonics from a spherical microphone array\ninvolves dividing by the radial functions, which may amplify noise at low\nfrequencies. This can be overcome by regularization, with the downside of\nintroducing errors to the Ambisonics encoding. This paper aims to investigate\nthe impact of different ways of regularization on Deep Neural Network (DNN)\ntraining and performance. Ideally, these networks should be robust to the way\nof regularization. Simulated data of a single speaker in a room and\nexperimental data from the LOCATA challenge were used to evaluate this\nrobustness on an example algorithm of speaker localization based on the\ndirect-path dominance (DPD) test. Results show that performance may be\nsensitive to the way of regularization, and an informed approach is proposed\nand investigated, highlighting the importance of regularization information.", "published": "2024-02-29 09:12:37", "link": "http://arxiv.org/abs/2402.18968v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Do End-to-End Neural Diarization Attractors Need to Encode Speaker\n  Characteristic Information?", "abstract": "In this paper, we apply the variational information bottleneck approach to\nend-to-end neural diarization with encoder-decoder attractors (EEND-EDA). This\nallows us to investigate what information is essential for the model. EEND-EDA\nutilizes attractors, vector representations of speakers in a conversation. Our\nanalysis shows that, attractors do not necessarily have to contain speaker\ncharacteristic information. On the other hand, giving the attractors more\nfreedom to allow them to encode some extra (possibly speaker-specific)\ninformation leads to small but consistent diarization performance improvements.\nDespite architectural differences in EEND systems, the notion of attractors and\nframe embeddings is common to most of them and not specific to EEND-EDA. We\nbelieve that the main conclusions of this work can apply to other variants of\nEEND. Thus, we hope this paper will be a valuable contribution to guide the\ncommunity to make more informed decisions when designing new systems.", "published": "2024-02-29 16:28:58", "link": "http://arxiv.org/abs/2402.19325v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A SOUND APPROACH: Using Large Language Models to generate audio\n  descriptions for egocentric text-audio retrieval", "abstract": "Video databases from the internet are a valuable source of text-audio\nretrieval datasets. However, given that sound and vision streams represent\ndifferent \"views\" of the data, treating visual descriptions as audio\ndescriptions is far from optimal. Even if audio class labels are present, they\ncommonly are not very detailed, making them unsuited for text-audio retrieval.\nTo exploit relevant audio information from video-text datasets, we introduce a\nmethodology for generating audio-centric descriptions using Large Language\nModels (LLMs). In this work, we consider the egocentric video setting and\npropose three new text-audio retrieval benchmarks based on the EpicMIR and\nEgoMCQ tasks, and on the EpicSounds dataset. Our approach for obtaining\naudio-centric descriptions gives significantly higher zero-shot performance\nthan using the original visual-centric descriptions. Furthermore, we show that\nusing the same prompts, we can successfully employ LLMs to improve the\nretrieval on EpicSounds, compared to using the original audio class labels of\nthe dataset. Finally, we confirm that LLMs can be used to determine the\ndifficulty of identifying the action associated with a sound.", "published": "2024-02-29 12:38:43", "link": "http://arxiv.org/abs/2402.19106v1", "categories": ["eess.AS", "cs.IR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Point Processes and spatial statistics in time-frequency analysis", "abstract": "A finite-energy signal is represented by a square-integrable, complex-valued\nfunction $t\\mapsto s(t)$ of a real variable $t$, interpreted as time.\nSimilarly, a noisy signal is represented by a random process. Time-frequency\nanalysis, a subfield of signal processing, amounts to describing the temporal\nevolution of the frequency content of a signal. Loosely speaking, if $s$ is the\naudio recording of a musical piece, time-frequency analysis somehow consists in\nwriting the musical score of the piece. Mathematically, the operation is\nperformed through a transform $\\mathcal{V}$, mapping $s \\in L^2(\\mathbb{R})$\nonto a complex-valued function $\\mathcal{V}s \\in L^2(\\mathbb{R}^2)$ of time $t$\nand angular frequency $\\omega$. The squared modulus $(t, \\omega) \\mapsto\n\\vert\\mathcal{V}s(t,\\omega)\\vert^2$ of the time-frequency representation is\nknown as the spectrogram of $s$; in the musical score analogy, a peaked\nspectrogram at $(t_0,\\omega_0)$ corresponds to a musical note at angular\nfrequency $\\omega_0$ localized at time $t_0$. More generally, the intuition is\nthat upper level sets of the spectrogram contain relevant information about in\nthe original signal. Hence, many signal processing algorithms revolve around\nidentifying maxima of the spectrogram. In contrast, zeros of the spectrogram\nindicate perfect silence, that is, a time at which a particular frequency is\nabsent. Assimilating $\\mathbb{R}^2$ to $\\mathbb{C}$ through $z = \\omega +\n\\mathrm{i}t$, this chapter focuses on time-frequency transforms $\\mathcal{V}$\nthat map signals to analytic functions. The zeros of the spectrogram of a noisy\nsignal are then the zeros of a random analytic function, hence forming a Point\nProcess in $\\mathbb{C}$. This chapter is devoted to the study of these Point\nProcesses, to their links with zeros of Gaussian Analytic Functions, and to\ndesigning signal detection and denoising algorithms using spatial statistics.", "published": "2024-02-29 13:53:21", "link": "http://arxiv.org/abs/2402.19172v1", "categories": ["eess.SP", "cs.SD", "eess.AS", "math.PR"], "primary_category": "eess.SP"}
{"title": "Unraveling Adversarial Examples against Speaker Identification --\n  Techniques for Attack Detection and Victim Model Classification", "abstract": "Adversarial examples have proven to threaten speaker identification systems,\nand several countermeasures against them have been proposed. In this paper, we\npropose a method to detect the presence of adversarial examples, i.e., a binary\nclassifier distinguishing between benign and adversarial examples. We build\nupon and extend previous work on attack type classification by exploring new\narchitectures. Additionally, we introduce a method for identifying the victim\nmodel on which the adversarial attack is carried out. To achieve this, we\ngenerate a new dataset containing multiple attacks performed against various\nvictim models. We achieve an AUC of 0.982 for attack detection, with no more\nthan a 0.03 drop in performance for unknown attacks. Our attack classification\naccuracy (excluding benign) reaches 86.48% across eight attack types using our\nLightResNet34 architecture, while our victim model classification accuracy\nreaches 72.28% across four victim models.", "published": "2024-02-29 17:06:52", "link": "http://arxiv.org/abs/2402.19355v1", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Probing the Information Encoded in Neural-based Acoustic Models of\n  Automatic Speech Recognition Systems", "abstract": "Deep learning architectures have made significant progress in terms of\nperformance in many research areas. The automatic speech recognition (ASR)\nfield has thus benefited from these scientific and technological advances,\nparticularly for acoustic modeling, now integrating deep neural network\narchitectures. However, these performance gains have translated into increased\ncomplexity regarding the information learned and conveyed through these\nblack-box architectures. Following many researches in neural networks\ninterpretability, we propose in this article a protocol that aims to determine\nwhich and where information is located in an ASR acoustic model (AM). To do so,\nwe propose to evaluate AM performance on a determined set of tasks using\nintermediate representations (here, at different layer levels). Regarding the\nperformance variation and targeted tasks, we can emit hypothesis about which\ninformation is enhanced or perturbed at different architecture steps.\nExperiments are performed on both speaker verification, acoustic environment\nclassification, gender classification, tempo-distortion detection systems and\nspeech sentiment/emotion identification. Analysis showed that neural-based AMs\nhold heterogeneous information that seems surprisingly uncorrelated with\nphoneme recognition, such as emotion, sentiment or speaker identity. The\nlow-level hidden layers globally appears useful for the structuring of\ninformation while the upper ones would tend to delete useless information for\nphoneme recognition.", "published": "2024-02-29 18:43:53", "link": "http://arxiv.org/abs/2402.19443v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
