{"title": "Efficacy of Machine-Generated Instructions", "abstract": "Large \"instruction-tuned\" language models (i.e., finetuned to respond to\ninstructions) have demonstrated a remarkable ability to generalize zero-shot to\nnew tasks. Nevertheless, they depend heavily on human-written instruction data\nthat is often limited in quantity, diversity, and creativity, therefore\nhindering the generality of the tuned model. We conducted a quantitative study\nto figure out the efficacy of machine-generated annotations, where we compare\nthe results of a fine-tuned BERT model with human v/s machine-generated\nannotations. Applying our methods to the vanilla GPT-3 model, we saw that\nmachine generated annotations were 78.54% correct and the fine-tuned model\nachieved a 96.01% model performance compared to the performance with\nhuman-labelled annotations. This result shows that machine-generated\nannotations are a resource and cost effective way to fine-tune down-stream\nmodels.", "published": "2023-12-22 04:01:30", "link": "http://arxiv.org/abs/2312.14423v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Data Retrieval for Cross Lingual Summarization", "abstract": "Cross-lingual summarization involves the summarization of text written in one\nlanguage to a different one. There is a body of research addressing\ncross-lingual summarization from English to other European languages. In this\nwork, we aim to perform cross-lingual summarization from English to Hindi. We\npropose pairing up the coverage of newsworthy events in textual and video\nformat can prove to be helpful for data acquisition for cross lingual\nsummarization. We analyze the data and propose methods to match articles to\nvideo descriptions that serve as document and summary pairs. We also outline\nfiltering methods over reasonable thresholds to ensure the correctness of the\nsummaries. Further, we make available 28,583 mono and cross-lingual\narticle-summary pairs https://github.com/tingc9/Cross-Sum-News-Aligned. We also\nbuild and analyze multiple baselines on the collected data and report error\nanalysis.", "published": "2023-12-22 09:13:24", "link": "http://arxiv.org/abs/2312.14542v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aurora:Activating Chinese chat capability for Mixtral-8x7B sparse\n  Mixture-of-Experts through Instruction-Tuning", "abstract": "Existing research has demonstrated that refining large language models (LLMs)\nthrough the utilization of machine-generated instruction-following data\nempowers these models to exhibit impressive zero-shot capabilities for novel\ntasks, without requiring human-authored instructions. In this paper, we\nsystematically investigate, preprocess, and integrate three Chinese\ninstruction-following datasets with the aim of enhancing the Chinese\nconversational capabilities of Mixtral-8x7B sparse Mixture-of-Experts model.\nThrough instruction fine-tuning on this carefully processed dataset, we\nsuccessfully construct the Mixtral-8x7B sparse Mixture-of-Experts model named\n\"Aurora.\" To assess the performance of Aurora, we utilize three widely\nrecognized benchmark tests: C-Eval, MMLU, and CMMLU. Empirical studies validate\nthe effectiveness of instruction fine-tuning applied to Mixtral-8x7B sparse\nMixture-of-Experts model. This work is pioneering in the execution of\ninstruction fine-tuning on a sparse expert-mixed model, marking a significant\nbreakthrough in enhancing the capabilities of this model architecture. Our\ncode, data and model are publicly available at\n  https://github.com/WangRongsheng/Aurora", "published": "2023-12-22 09:30:41", "link": "http://arxiv.org/abs/2312.14557v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reasons to Reject? Aligning Language Models with Judgments", "abstract": "As humans, we consistently interact with our peers and receive feedback in\nthe form of natural language. This language feedback allows us to maintain\nappropriate behavior, and rectify potential errors. The question arises\nnaturally: can we use language feedback to align large language models (LLMs)?\nIn contrast to previous research that aligns LLMs with scalar rewards, we\npresent the first systematic exploration of alignment through the lens of\nlanguage feedback (i.e., judgment). We start with an in-depth investigation of\npotential methods that can be adapted for aligning LLMs with judgments,\nrevealing that these methods cannot fully capitalize on judgments. To\nfacilitate more effective utilization of judgments, we propose a novel\nframework, Contrastive Unlikelihood Training (CUT), that allows for\nfine-grained inappropriate content detection and correction based on judgments.\nOur results show that, with merely 1317 off-the-shelf judgment data, CUT\n(LLaMA2-13b) can beat the 175B DaVinci003 and surpass the best baseline by\n50.84 points on AlpacaEval. CUT (LLaMA2-chat-13b) can also align LLMs in an\niterative fashion using up-to-date model-specific judgments, improving\nperformance from 81.09 to 91.68 points on AlpacaEval. Further analysis suggests\nthat judgments hold greater potential than rewards in LLM alignment.", "published": "2023-12-22 10:29:43", "link": "http://arxiv.org/abs/2312.14591v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Balancing the Style-Content Trade-Off in Sentiment Transfer Using\n  Polarity-Aware Denoising", "abstract": "Text sentiment transfer aims to flip the sentiment polarity of a sentence\n(positive to negative or vice versa) while preserving its sentiment-independent\ncontent. Although current models show good results at changing the sentiment,\ncontent preservation in transferred sentences is insufficient. In this paper,\nwe present a sentiment transfer model based on polarity-aware denoising, which\naccurately controls the sentiment attributes in generated text, preserving the\ncontent to a great extent and helping to balance the style-content trade-off.\nOur proposed model is structured around two key stages in the sentiment\ntransfer process: better representation learning using a shared encoder and\nsentiment-controlled generation using separate sentiment-specific decoders.\nEmpirical results show that our methods outperforms state-of-the-art baselines\nin terms of content preservation while staying competitive in terms of style\ntransfer accuracy and fluency.", "published": "2023-12-22 14:06:54", "link": "http://arxiv.org/abs/2312.14708v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Parsing for Complex Data Retrieval: Targeting Query Plans vs.\n  SQL for No-Code Access to Relational Databases", "abstract": "Large Language Models (LLMs) have spurred progress in text-to-SQL, the task\nof generating SQL queries from natural language questions based on a given\ndatabase schema. Despite the declarative nature of SQL, it continues to be a\ncomplex programming language. In this paper, we investigate the potential of an\nalternative query language with simpler syntax and modular specification of\ncomplex queries. The purpose is to create a query language that can be learned\nmore easily by modern neural semantic parsing architectures while also enabling\nnon-programmers to better assess the validity of the query plans produced by an\ninteractive query plan assistant.\n  The proposed alternative query language is called Query Plan Language (QPL).\nIt is designed to be modular and can be translated into a restricted form of\nSQL Common Table Expressions (CTEs). The aim of QPL is to make complex data\nretrieval accessible to non-programmers by allowing users to express their\nquestions in natural language while also providing an easier-to-verify target\nlanguage. The paper demonstrates how neural LLMs can benefit from QPL's\nmodularity to generate complex query plans in a compositional manner. This\ninvolves a question decomposition strategy and a planning stage.\n  We conduct experiments on a version of the Spider text-to-SQL dataset that\nhas been converted to QPL. The hierarchical structure of QPL programs enables\nus to measure query complexity naturally. Based on this assessment, we identify\nthe low accuracy of existing text-to-SQL systems on complex compositional\nqueries. We present ways to address the challenge of complex queries in an\niterative, user-controlled manner, using fine-tuned LLMs and a variety of\nprompting strategies in a compositional manner.", "published": "2023-12-22 16:16:15", "link": "http://arxiv.org/abs/2312.14798v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Use of Metaphor Translation in Psychiatry", "abstract": "Providing mental healthcare to individuals with limited English proficiency\n(LEP) remains a pressing problem within psychiatry. Because the majority of\nindividuals trained in providing psychiatric care are English speakers, the\nquality of mental healthcare given to LEP patients is significantly lower than\nthat provided for English speakers. The provision of mental healthcare is\ncontingent on communication and understanding between the patient and\nhealthcare provider, much more so than in the realm of physical healthcare, and\nEnglish speakers are often unable to comprehend figurative language such as\nmetaphors used by LEPs. Hence, Figurative Language Translation is invaluable to\nproviding equitable psychiatric care. Now, metaphor has been shown to be\nparamount in both identifying individuals struggling with mental problems and\nhelping those individuals understand and communicate their experiences.\nTherefore, this paper aims to survey the potential of Machine Translation for\nproviding equitable psychiatric healthcare and highlights the need for further\nresearch on the transferability of existing machine and metaphor translation\nresearch in the domain of psychiatry.", "published": "2023-12-22 17:19:33", "link": "http://arxiv.org/abs/2312.14845v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Numerical Reasoning for Financial Reports", "abstract": "Financial reports offer critical insights into a company's operations, yet\ntheir extensive length typically spanning 30 40 pages poses challenges for\nswift decision making in dynamic markets. To address this, we leveraged\nfinetuned Large Language Models (LLMs) to distill key indicators and\noperational metrics from these reports basis questions from the user. We\ndevised a method to locate critical data, and leverage the FinQA dataset to\nfine-tune both Llama-2 7B and T5 models for customized question answering. We\nachieved results comparable to baseline on the final numerical answer, a\ncompetitive accuracy in numerical reasoning and calculation.", "published": "2023-12-22 17:46:36", "link": "http://arxiv.org/abs/2312.14870v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards a Unified Multimodal Reasoning Framework", "abstract": "Recent advancements in deep learning have led to the development of powerful\nlanguage models (LMs) that excel in various tasks. Despite these achievements,\nthere is still room for improvement, particularly in enhancing reasoning\nabilities and incorporating multimodal data. This report investigates the\npotential impact of combining Chain-of-Thought (CoT) reasoning and Visual\nQuestion Answering (VQA) techniques to improve LM's accuracy in solving\nmultiple-choice questions. By employing TextVQA and ScienceQA datasets, we\nassessed the effectiveness of three text embedding methods and three visual\nembedding approaches. Our experiments aimed to fill the gap in current research\nby investigating the combined impact of CoT and VQA, contributing to the\nunderstanding of how these techniques can improve the reasoning capabilities of\nstate-of-the-art models like GPT-4. Results from our experiments demonstrated\nthe potential of these approaches in enhancing LM's reasoning and\nquestion-answering capabilities, providing insights for further research and\ndevelopment in the field, and paving the way for more accurate and reliable AI\nsystems that can handle complex reasoning tasks across multiple modalities.", "published": "2023-12-22 19:07:00", "link": "http://arxiv.org/abs/2312.15021v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Don't Believe Everything You Read: Enhancing Summarization\n  Interpretability through Automatic Identification of Hallucinations in Large\n  Language Models", "abstract": "Large Language Models (LLMs) are adept at text manipulation -- tasks such as\nmachine translation and text summarization. However, these models can also be\nprone to hallucination, which can be detrimental to the faithfulness of any\nanswers that the model provides. Recent works in combating hallucinations in\nLLMs deal with identifying hallucinated sentences and categorizing the\ndifferent ways in which models hallucinate. This paper takes a deep dive into\nLLM behavior with respect to hallucinations, defines a token-level approach to\nidentifying different kinds of hallucinations, and further utilizes this\ntoken-level tagging to improve the interpretability and faithfulness of LLMs in\ndialogue summarization tasks. Through this, the paper presents a new, enhanced\ndataset and a new training paradigm.", "published": "2023-12-22 00:31:46", "link": "http://arxiv.org/abs/2312.14346v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Model is a Branch Predictor for Simultaneous Machine\n  Translation", "abstract": "The primary objective of simultaneous machine translation (SiMT) is to\nminimize latency while preserving the quality of the final translation. Drawing\ninspiration from CPU branch prediction techniques, we propose incorporating\nbranch prediction techniques in SiMT tasks to reduce translation latency.\nSpecifically, we utilize a language model as a branch predictor to predict\npotential branch directions, namely, future source words. Subsequently, we\nutilize the predicted source words to decode the output in advance. When the\nactual source word deviates from the predicted source word, we use the real\nsource word to decode the output again, replacing the predicted output. To\nfurther reduce computational costs, we share the parameters of the encoder and\nthe branch predictor, and utilize a pre-trained language model for\ninitialization. Our proposed method can be seamlessly integrated with any SiMT\nmodel. Extensive experimental results demonstrate that our approach can improve\ntranslation quality and latency at the same time. Our code is available at\nhttps://github.com/YinAoXiong/simt_branch_predictor .", "published": "2023-12-22 07:32:47", "link": "http://arxiv.org/abs/2312.14488v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Theory of Hallucinations based on Equivariance", "abstract": "This study aims to acquire knowledge for creating very large language models\nthat are immune to hallucinations. Hallucinations in contemporary large\nlanguage models are often attributed to a misunderstanding of real-world social\nrelationships. Therefore, I hypothesize that very large language models capable\nof thoroughly grasping all these relationships will be free from\nhallucinations. Additionally, I propose that certain types of equivariant\nlanguage models are adept at learning and understanding these relationships.\nBuilding on this, I have developed a specialized cross-entropy error function\nto create a hallucination scale for language models, which measures their\nextent of equivariance acquisition. Utilizing this scale, I tested language\nmodels for their ability to acquire character-level equivariance. In\nparticular, I introduce and employ a novel technique based on T5 (Text To Text\nTransfer Transformer) that efficiently understands permuted input texts without\nthe need for explicit dictionaries to convert token IDs (integers) to texts\n(strings). This T5 model demonstrated a moderate ability to acquire\ncharacter-level equivariance. Additionally, I discovered scale laws that can\naid in developing hallucination-free language models at the character level.\nThis methodology can be extended to assess equivariance acquisition at the word\nlevel, paving the way for very large language models that can comprehensively\nunderstand relationships and, consequently, avoid hallucinations.", "published": "2023-12-22 08:08:45", "link": "http://arxiv.org/abs/2312.14504v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SIG: Speaker Identification in Literature via Prompt-Based Generation", "abstract": "Identifying speakers of quotations in narratives is an important task in\nliterary analysis, with challenging scenarios including the out-of-domain\ninference for unseen speakers, and non-explicit cases where there are no\nspeaker mentions in surrounding context. In this work, we propose a simple and\neffective approach SIG, a generation-based method that verbalizes the task and\nquotation input based on designed prompt templates, which also enables easy\nintegration of other auxiliary tasks that further bolster the speaker\nidentification performance. The prediction can either come from direct\ngeneration by the model, or be determined by the highest generation probability\nof each speaker candidate. Based on our approach design, SIG supports\nout-of-domain evaluation, and achieves open-world classification paradigm that\nis able to accept any forms of candidate input. We perform both cross-domain\nevaluation and in-domain evaluation on PDNC, the largest dataset of this task,\nwhere empirical results suggest that SIG outperforms previous baselines of\ncomplicated designs, as well as the zero-shot ChatGPT, especially excelling at\nthose hard non-explicit scenarios by up to 17% improvement. Additional\nexperiments on another dataset WP further corroborate the efficacy of SIG.", "published": "2023-12-22 10:29:18", "link": "http://arxiv.org/abs/2312.14590v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BLSTM-Based Confidence Estimation for End-to-End Speech Recognition", "abstract": "Confidence estimation, in which we estimate the reliability of each\nrecognized token (e.g., word, sub-word, and character) in automatic speech\nrecognition (ASR) hypotheses and detect incorrectly recognized tokens, is an\nimportant function for developing ASR applications. In this study, we perform\nconfidence estimation for end-to-end (E2E) ASR hypotheses. Recent E2E ASR\nsystems show high performance (e.g., around 5% token error rates) for various\nASR tasks. In such situations, confidence estimation becomes difficult since we\nneed to detect infrequent incorrect tokens from mostly correct token sequences.\nTo tackle this imbalanced dataset problem, we employ a bidirectional long\nshort-term memory (BLSTM)-based model as a strong binary-class\n(correct/incorrect) sequence labeler that is trained with a class balancing\nobjective. We experimentally confirmed that, by utilizing several types of ASR\ndecoding scores as its auxiliary features, the model steadily shows high\nconfidence estimation performance under highly imbalanced settings. We also\nconfirmed that the BLSTM-based model outperforms Transformer-based confidence\nestimation models, which greatly underestimate incorrect tokens.", "published": "2023-12-22 11:12:45", "link": "http://arxiv.org/abs/2312.14609v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "YAYI 2: Multilingual Open-Source Large Language Models", "abstract": "As the latest advancements in natural language processing, large language\nmodels (LLMs) have achieved human-level language understanding and generation\nabilities in many real-world tasks, and even have been regarded as a potential\npath to the artificial general intelligence. To better facilitate research on\nLLMs, many open-source LLMs, such as Llama 2 and Falcon, have recently been\nproposed and gained comparable performances to proprietary models. However,\nthese models are primarily designed for English scenarios and exhibit poor\nperformances in Chinese contexts. In this technical report, we propose YAYI 2,\nincluding both base and chat models, with 30 billion parameters. YAYI 2 is\npre-trained from scratch on a multilingual corpus which contains 2.65 trillion\ntokens filtered by our pre-training data processing pipeline. The base model is\naligned with human values through supervised fine-tuning with millions of\ninstructions and reinforcement learning from human feedback. Extensive\nexperiments on multiple benchmarks, such as MMLU and CMMLU, consistently\ndemonstrate that the proposed YAYI 2 outperforms other similar sized\nopen-source models.", "published": "2023-12-22 17:34:47", "link": "http://arxiv.org/abs/2312.14862v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Robust Knowledge Extraction from Large Language Models using Social\n  Choice Theory", "abstract": "Large-language models (LLMs) can support a wide range of applications like\nconversational agents, creative writing or general query answering. However,\nthey are ill-suited for query answering in high-stake domains like medicine\nbecause they are typically not robust - even the same query can result in\ndifferent answers when prompted multiple times. In order to improve the\nrobustness of LLM queries, we propose using ranking queries repeatedly and to\naggregate the queries using methods from social choice theory. We study ranking\nqueries in diagnostic settings like medical and fault diagnosis and discuss how\nthe Partial Borda Choice function from the literature can be applied to merge\nmultiple query results. We discuss some additional interesting properties in\nour setting and evaluate the robustness of our approach empirically.", "published": "2023-12-22 17:57:29", "link": "http://arxiv.org/abs/2312.14877v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sparsity-Guided Holistic Explanation for LLMs with Interpretable\n  Inference-Time Intervention", "abstract": "Large Language Models (LLMs) have achieved unprecedented breakthroughs in\nvarious natural language processing domains. However, the enigmatic\n``black-box'' nature of LLMs remains a significant challenge for\ninterpretability, hampering transparent and accountable applications. While\npast approaches, such as attention visualization, pivotal subnetwork\nextraction, and concept-based analyses, offer some insight, they often focus on\neither local or global explanations within a single dimension, occasionally\nfalling short in providing comprehensive clarity. In response, we propose a\nnovel methodology anchored in sparsity-guided techniques, aiming to provide a\nholistic interpretation of LLMs. Our framework, termed SparseCBM, innovatively\nintegrates sparsity to elucidate three intertwined layers of interpretation:\ninput, subnetwork, and concept levels. In addition, the newly introduced\ndimension of interpretable inference-time intervention facilitates dynamic\nadjustments to the model during deployment. Through rigorous empirical\nevaluations on real-world datasets, we demonstrate that SparseCBM delivers a\nprofound understanding of LLM behaviors, setting it apart in both interpreting\nand ameliorating model inaccuracies. Codes are provided in supplements.", "published": "2023-12-22 19:55:58", "link": "http://arxiv.org/abs/2312.15033v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Personalized Large Language Model Assistant with Evolving Conditional\n  Memory", "abstract": "With the rapid development of large language models, AI assistants like\nChatGPT have become increasingly integrated into people's works and lives but\nare limited in personalized services. In this paper, we present a plug-and-play\nframework that could facilitate personalized large language model assistants\nwith evolving conditional memory. The personalized assistant focuses on\nintelligently preserving the knowledge and experience from the history dialogue\nwith the user, which can be applied to future tailored responses that better\nalign with the user's preferences. Generally, the assistant generates a set of\nrecords from the dialogue dialogue, stores them in a memory bank, and retrieves\nrelated memory to improve the quality of the response. For the crucial memory\ndesign, we explore different ways of constructing the memory and propose a new\nmemorizing mechanism named conditional memory. We also investigate the\nretrieval and usage of memory in the generation process. We build the first\nbenchmark to evaluate personalized assistants' ability from three aspects. The\nexperimental results illustrate the effectiveness of our method.", "published": "2023-12-22 02:39:15", "link": "http://arxiv.org/abs/2312.17257v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Empowering Working Memory for Large Language Model Agents", "abstract": "Large language models (LLMs) have achieved impressive linguistic\ncapabilities. However, a key limitation persists in their lack of human-like\nmemory faculties. LLMs exhibit constrained memory retention across sequential\ninteractions, hindering complex reasoning. This paper explores the potential of\napplying cognitive psychology's working memory frameworks, to enhance LLM\narchitecture. The limitations of traditional LLM memory designs are analyzed,\nincluding their isolation of distinct dialog episodes and lack of persistent\nmemory links. To address this, an innovative model is proposed incorporating a\ncentralized Working Memory Hub and Episodic Buffer access to retain memories\nacross episodes. This architecture aims to provide greater continuity for\nnuanced contextual reasoning during intricate tasks and collaborative\nscenarios. While promising, further research is required into optimizing\nepisodic memory encoding, storage, prioritization, retrieval, and security.\nOverall, this paper provides a strategic blueprint for developing LLM agents\nwith more sophisticated, human-like memory capabilities, highlighting memory\nmechanisms as a vital frontier in artificial general intelligence.", "published": "2023-12-22 05:59:00", "link": "http://arxiv.org/abs/2312.17259v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Logic-Scaffolding: Personalized Aspect-Instructed Recommendation\n  Explanation Generation using LLMs", "abstract": "The unique capabilities of Large Language Models (LLMs), such as the natural\nlanguage text generation ability, position them as strong candidates for\nproviding explanation for recommendations. However, despite the size of the\nLLM, most existing models struggle to produce zero-shot explanations reliably.\nTo address this issue, we propose a framework called Logic-Scaffolding, that\ncombines the ideas of aspect-based explanation and chain-of-thought prompting\nto generate explanations through intermediate reasoning steps. In this paper,\nwe share our experience in building the framework and present an interactive\ndemonstration for exploring our results.", "published": "2023-12-22 00:30:10", "link": "http://arxiv.org/abs/2312.14345v2", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "MetaAID 2.5: A Secure Framework for Developing Metaverse Applications\n  via Large Language Models", "abstract": "Large language models (LLMs) are increasingly being used in Metaverse\nenvironments to generate dynamic and realistic content and to control the\nbehavior of non-player characters (NPCs). However, the cybersecurity concerns\nassociated with LLMs have become increasingly prominent. Previous research has\nprimarily focused on patching system vulnerabilities to enhance cybersecurity,\nbut these approaches are not well-suited to the Metaverse, where the virtual\nspace is more complex, LLMs are vulnerable, and ethical user interaction is\ncritical. Moreover, the scope of cybersecurity in the Metaverse is expected to\nexpand significantly. This paper proposes a method for enhancing cybersecurity\nthrough the simulation of user interaction with LLMs. Our goal is to educate\nusers and strengthen their defense capabilities through exposure to a\ncomprehensive simulation system. This system includes extensive Metaverse\ncybersecurity Q&A and attack simulation scenarios. By engaging with these,\nusers will improve their ability to recognize and withstand risks.\nAdditionally, to address the ethical implications of user input, we propose\nusing LLMs as evaluators to assess user content across five dimensions. We\nfurther adapt the models through vocabulary expansion training to better\nunderstand personalized inputs and emoticons. We conduct experiments on\nmultiple LLMs and find that our approach is effective.", "published": "2023-12-22 07:15:55", "link": "http://arxiv.org/abs/2312.14480v1", "categories": ["cs.CR", "cs.CL", "cs.CY"], "primary_category": "cs.CR"}
{"title": "Collaborative Synthesis of Patient Records through Multi-Visit Health\n  State Inference", "abstract": "Electronic health records (EHRs) have become the foundation of machine\nlearning applications in healthcare, while the utility of real patient records\nis often limited by privacy and security concerns. Synthetic EHR generation\nprovides an additional perspective to compensate for this limitation. Most\nexisting methods synthesize new records based on real EHR data, without\nconsideration of different types of events in EHR data, which cannot control\nthe event combinations in line with medical common sense. In this paper, we\npropose MSIC, a Multi-visit health Status Inference model for Collaborative EHR\nsynthesis to address these limitations. First, we formulate the synthetic EHR\ngeneration process as a probabilistic graphical model and tightly connect\ndifferent types of events by modeling the latent health states. Then, we derive\na health state inference method tailored for the multi-visit scenario to\neffectively utilize previous records to synthesize current and future records.\nFurthermore, we propose to generate medical reports to add textual descriptions\nfor each medical event, providing broader applications for synthesized EHR\ndata. For generating different paragraphs in each visit, we incorporate a\nmulti-generator deliberation framework to collaborate the message passing of\nmultiple generators and employ a two-phase decoding strategy to generate\nhigh-quality reports. Our extensive experiments on the widely used benchmarks,\nMIMIC-III and MIMIC-IV, demonstrate that MSIC advances state-of-the-art results\non the quality of synthetic data while maintaining low privacy risks.", "published": "2023-12-22 12:28:29", "link": "http://arxiv.org/abs/2312.14646v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Computational Semantics and Evaluation Benchmark for Interrogative\n  Sentences via Combinatory Categorial Grammar", "abstract": "We present a compositional semantics for various types of polar questions and\nwh-questions within the framework of Combinatory Categorial Grammar (CCG). To\nassess the explanatory power of our proposed analysis, we introduce a\nquestion-answering dataset QSEM specifically designed to evaluate the semantics\nof interrogative sentences. We implement our analysis using existing CCG\nparsers and conduct evaluations using the dataset. Through the evaluation, we\nhave obtained annotated data with CCG trees and semantic representations for\nabout half of the samples included in QSEM. Furthermore, we discuss the\ndiscrepancy between the theoretical capacity of CCG and the capabilities of\nexisting CCG parsers.", "published": "2023-12-22 14:46:02", "link": "http://arxiv.org/abs/2312.14737v1", "categories": ["cs.CL", "cs.AI", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Large Language Model (LLM) Bias Index -- LLMBI", "abstract": "The Large Language Model Bias Index (LLMBI) is a pioneering approach designed\nto quantify and address biases inherent in large language models (LLMs), such\nas GPT-4. We recognise the increasing prevalence and impact of LLMs across\ndiverse sectors. This research introduces a novel metric, LLMBI, to\nsystematically measure and mitigate biases potentially skewing model responses.\nWe formulated LLMBI using a composite scoring system incorporating multiple\ndimensions of bias, including but not limited to age, gender, and racial\nbiases. To operationalise this metric, we engaged in a multi-step process\ninvolving collecting and annotating LLM responses, applying sophisticated\nNatural Language Processing (NLP) techniques for bias detection, and computing\nthe LLMBI score through a specially crafted mathematical formula. The formula\nintegrates weighted averages of various bias dimensions, a penalty for dataset\ndiversity deficiencies, and a correction for sentiment biases. Our empirical\nanalysis, conducted using responses from OpenAI's API, employs advanced\nsentiment analysis as a representative method for bias detection. The research\nreveals LLMs, whilst demonstrating impressive capabilities in text generation,\nexhibit varying degrees of bias across different dimensions. LLMBI provides a\nquantifiable measure to compare biases across models and over time, offering a\nvital tool for systems engineers, researchers and regulators in enhancing the\nfairness and reliability of LLMs. It highlights the potential of LLMs in\nmimicking unbiased human-like responses. Additionally, it underscores the\nnecessity of continuously monitoring and recalibrating such models to align\nwith evolving societal norms and ethical standards.", "published": "2023-12-22 15:38:13", "link": "http://arxiv.org/abs/2312.14769v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "I.2.7", "I.2.7"], "primary_category": "cs.CL"}
{"title": "VIEScore: Towards Explainable Metrics for Conditional Image Synthesis\n  Evaluation", "abstract": "In the rapidly advancing field of conditional image generation research,\nchallenges such as limited explainability lie in effectively evaluating the\nperformance and capabilities of various models. This paper introduces VIEScore,\na Visual Instruction-guided Explainable metric for evaluating any conditional\nimage generation tasks. VIEScore leverages general knowledge from Multimodal\nLarge Language Models (MLLMs) as the backbone and does not require training or\nfine-tuning. We evaluate VIEScore on seven prominent tasks in conditional image\ntasks and found: (1) VIEScore (GPT4-o) achieves a high Spearman correlation of\n0.4 with human evaluations, while the human-to-human correlation is 0.45. (2)\nVIEScore (with open-source MLLM) is significantly weaker than GPT-4o and GPT-4v\nin evaluating synthetic images. (3) VIEScore achieves a correlation on par with\nhuman ratings in the generation tasks but struggles in editing tasks. With\nthese results, we believe VIEScore shows its great potential to replace human\njudges in evaluating image synthesis tasks.", "published": "2023-12-22 17:45:19", "link": "http://arxiv.org/abs/2312.14867v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language\n  Models via Complexity Classes", "abstract": "Complex reasoning ability is one of the most important features of current\nLLMs, which has also been leveraged to play an integral role in complex\ndecision-making tasks. Therefore, the investigation into the reasoning\ncapabilities of Large Language Models (LLMs) is critical: numerous benchmarks\nhave been established to assess the reasoning abilities of LLMs. However,\ncurrent benchmarks are inadequate in offering a rigorous evaluation of the full\nextent of reasoning abilities that LLMs are capable of achieving. They are also\nprone to the risk of overfitting, as these benchmarks, being publicly\naccessible and static, allow models to potentially tailor their responses to\nspecific benchmark metrics, thereby inflating their performance. Addressing\nthese limitations, our research introduces a new benchmark, named NPHardEval.\nThis benchmark is designed to evaluate the reasoning abilities of LLMs across a\nbroad spectrum of 900 algorithmic questions, extending up to the NP-Hard\ncomplexity class. These questions are meticulously chosen to represent a wide\nrange of complexity class below the NP-hard complexity class, offering a\nrigorous measure of the reasoning ability of LLMs. Through this study, we shed\nlight on the current state of reasoning in LLMs, providing an objective and\nrigorous perspective through the comparison of LLMs' performance across complex\nclasses. Moreover, this benchmark is designed with a dynamic update mechanism,\nwhere the datapoints are refreshed on a monthly basis. Such regular updates\nplay a crucial role in mitigating the risk of LLMs overfitting to the\nbenchmark, promoting a more accurate and reliable assessment of their reasoning\ncapabilities. The benchmark dataset and code of NPHardEval are available at\nhttps://github.com/casmlab/NPHardEval.", "published": "2023-12-22 18:07:44", "link": "http://arxiv.org/abs/2312.14890v4", "categories": ["cs.AI", "cs.CC", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Assessing the Impact of Prompting Methods on ChatGPT's Mathematical\n  Capabilities", "abstract": "This study critically evaluates the efficacy of prompting methods in\nenhancing the mathematical reasoning capability of large language models\n(LLMs). The investigation uses three prescriptive prompting methods - simple,\npersona, and conversational prompting - known for their effectiveness in\nenhancing the linguistic tasks of LLMs. We conduct this analysis on OpenAI's\nLLM chatbot, ChatGPT-3.5, on extensive problem sets from the MATH, GSM8K, and\nMMLU datasets, encompassing a broad spectrum of mathematical challenges. A\ngrading script adapted to each dataset is used to determine the effectiveness\nof these prompting interventions in enhancing the model's mathematical analysis\npower. Contrary to expectations, our empirical analysis reveals that none of\nthe investigated methods consistently improves over ChatGPT-3.5's baseline\nperformance, with some causing significant degradation. Our findings suggest\nthat prompting strategies do not necessarily generalize to new domains, in this\nstudy failing to enhance mathematical performance.", "published": "2023-12-22 17:39:40", "link": "http://arxiv.org/abs/2312.15006v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Refining GPT-3 Embeddings with a Siamese Structure for Technical Post\n  Duplicate Detection", "abstract": "One goal of technical online communities is to help developers find the right\nanswer in one place. A single question can be asked in different ways with\ndifferent wordings, leading to the existence of duplicate posts on technical\nforums. The question of how to discover and link duplicate posts has garnered\nthe attention of both developer communities and researchers. For example, Stack\nOverflow adopts a voting-based mechanism to mark and close duplicate posts.\nHowever, addressing these constantly emerging duplicate posts in a timely\nmanner continues to pose challenges. Therefore, various approaches have been\nproposed to detect duplicate posts on technical forum posts automatically. The\nexisting methods suffer from limitations either due to their reliance on\nhandcrafted similarity metrics which can not sufficiently capture the semantics\nof posts, or their lack of supervision to improve the performance.\nAdditionally, the efficiency of these methods is hindered by their dependence\non pair-wise feature generation, which can be impractical for large amount of\ndata. In this work, we attempt to employ and refine the GPT-3 embeddings for\nthe duplicate detection task. We assume that the GPT-3 embeddings can\naccurately represent the semantics of the posts. In addition, by training a\nSiamese-based network based on the GPT-3 embeddings, we obtain a latent\nembedding that accurately captures the duplicate relation in technical forum\nposts. Our experiment on a benchmark dataset confirms the effectiveness of our\napproach and demonstrates superior performance compared to baseline methods.\nWhen applied to the dataset we constructed with a recent Stack Overflow dump,\nour approach attains a Top-1, Top-5, and Top-30 accuracy of 23.1%, 43.9%, and\n68.9%, respectively. With a manual study, we confirm our approach's potential\nof finding unlabelled duplicates on technical forums.", "published": "2023-12-22 21:14:37", "link": "http://arxiv.org/abs/2312.15068v2", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Unsupervised Auditory and Semantic Entrainment Models with Deep Neural\n  Networks", "abstract": "Speakers tend to engage in adaptive behavior, known as entrainment, when they\nbecome similar to their interlocutor in various aspects of speaking. We present\nan unsupervised deep learning framework that derives meaningful representation\nfrom textual features for developing semantic entrainment. We investigate the\nmodel's performance by extracting features using different variations of the\nBERT model (DistilBERT and XLM-RoBERTa) and Google's universal sentence encoder\n(USE) embeddings on two human-human (HH) corpora (The Fisher Corpus English\nPart 1, Columbia games corpus) and one human-machine (HM) corpus (Voice\nAssistant Conversation Corpus (VACC)). In addition to semantic features we also\ntrained DNN-based models utilizing two auditory embeddings (TRIpLet Loss\nnetwork (TRILL) vectors, Low-level descriptors (LLD) features) and two units of\nanalysis (Inter pausal unit and Turn). The results show that semantic\nentrainment can be assessed with our model, that models can distinguish between\nHH and HM interactions and that the two units of analysis for extracting\nacoustic features provide comparable findings.", "published": "2023-12-22 22:33:54", "link": "http://arxiv.org/abs/2312.15098v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Moderating New Waves of Online Hate with Chain-of-Thought Reasoning in\n  Large Language Models", "abstract": "Online hate is an escalating problem that negatively impacts the lives of\nInternet users, and is also subject to rapid changes due to evolving events,\nresulting in new waves of online hate that pose a critical threat. Detecting\nand mitigating these new waves present two key challenges: it demands\nreasoning-based complex decision-making to determine the presence of hateful\ncontent, and the limited availability of training samples hinders updating the\ndetection model. To address this critical issue, we present a novel framework\ncalled HATEGUARD for effectively moderating new waves of online hate. HATEGUARD\nemploys a reasoning-based approach that leverages the recently introduced\nchain-of-thought (CoT) prompting technique, harnessing the capabilities of\nlarge language models (LLMs). HATEGUARD further achieves prompt-based zero-shot\ndetection by automatically generating and updating detection prompts with new\nderogatory terms and targets in new wave samples to effectively address new\nwaves of online hate. To demonstrate the effectiveness of our approach, we\ncompile a new dataset consisting of tweets related to three recently witnessed\nnew waves: the 2022 Russian invasion of Ukraine, the 2021 insurrection of the\nUS Capitol, and the COVID-19 pandemic. Our studies reveal crucial longitudinal\npatterns in these new waves concerning the evolution of events and the pressing\nneed for techniques to rapidly update existing moderation tools to counteract\nthem. Comparative evaluations against state-of-the-art tools illustrate the\nsuperiority of our framework, showcasing a substantial 22.22% to 83.33%\nimprovement in detecting the three new waves of online hate. Our work\nhighlights the severe threat posed by the emergence of new waves of online hate\nand represents a paradigm shift in addressing this threat practically.", "published": "2023-12-22 22:34:49", "link": "http://arxiv.org/abs/2312.15099v2", "categories": ["cs.CL", "cs.CY", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Voila-A: Aligning Vision-Language Models with User's Gaze Attention", "abstract": "In recent years, the integration of vision and language understanding has led\nto significant advancements in artificial intelligence, particularly through\nVision-Language Models (VLMs). However, existing VLMs face challenges in\nhandling real-world applications with complex scenes and multiple objects, as\nwell as aligning their focus with the diverse attention patterns of human\nusers. In this paper, we introduce gaze information, feasibly collected by AR\nor VR devices, as a proxy for human attention to guide VLMs and propose a novel\napproach, Voila-A, for gaze alignment to enhance the interpretability and\neffectiveness of these models in real-world applications. First, we collect\nhundreds of minutes of gaze data to demonstrate that we can mimic human gaze\nmodalities using localized narratives. We then design an automatic data\nannotation pipeline utilizing GPT-4 to generate the VOILA-COCO dataset.\nAdditionally, we innovate the Voila Perceiver modules to integrate gaze\ninformation into VLMs while preserving their pretrained knowledge. We evaluate\nVoila-A using a hold-out validation set and a newly collected VOILA-GAZE\nTestset, which features real-life scenarios captured with a gaze-tracking\ndevice. Our experimental results demonstrate that Voila-A significantly\noutperforms several baseline models. By aligning model attention with human\ngaze patterns, Voila-A paves the way for more intuitive, user-centric VLMs and\nfosters engaging human-AI interaction across a wide range of applications.", "published": "2023-12-22 17:34:01", "link": "http://arxiv.org/abs/2401.09454v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "ZMM-TTS: Zero-shot Multilingual and Multispeaker Speech Synthesis\n  Conditioned on Self-supervised Discrete Speech Representations", "abstract": "Neural text-to-speech (TTS) has achieved human-like synthetic speech for\nsingle-speaker, single-language synthesis. Multilingual TTS systems are limited\nto resource-rich languages due to the lack of large paired text and\nstudio-quality audio data. TTS systems are typically built using a single\nspeaker's voices, but there is growing interest in developing systems that can\nsynthesize voices for new speakers using only a few seconds of their speech.\nThis paper presents ZMM-TTS, a multilingual and multispeaker framework\nutilizing quantized latent speech representations from a large-scale,\npre-trained, self-supervised model. Our paper combines text-based and\nspeech-based self-supervised learning models for multilingual speech synthesis.\nOur proposed model has zero-shot generalization ability not only for unseen\nspeakers but also for unseen languages. We have conducted comprehensive\nsubjective and objective evaluations through a series of experiments. Our model\nhas proven effective in terms of speech naturalness and similarity for both\nseen and unseen speakers in six high-resource languages. We also tested the\nefficiency of our method on two hypothetically low-resource languages. The\nresults are promising, indicating that our proposed approach can synthesize\naudio that is intelligible and has a high degree of similarity to the target\nspeaker's voice, even without any training data for the new, unseen language.", "published": "2023-12-22 02:54:15", "link": "http://arxiv.org/abs/2312.14398v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Noise Morphing for Audio Time Stretching", "abstract": "This letter introduces an innovative method to enhance the quality of audio\ntime stretching by precisely decomposing a sound into sines, transients, and\nnoise and by improving the processing of the latter component. While there are\nestablished methods for time-stretching sines and transients with high quality,\nthe manipulation of noise or residual components has lacked robust solutions in\nprior research. The proposed method combines sound decomposition with previous\ntechniques for audio spectral resynthesis. The time-stretched noise component\nis achieved by morphing its time-interpolated spectral magnitude with a\nwhite-noise excitation signal. This method stands out for its simplicity,\nefficiency, and audio quality. The results of a subjective experiment affirm\nthe superiority of this approach over current state-of-the-art methods across\nall evaluated stretch factors. The proposed technique notably excels in extreme\nstretching scenarios, signifying a substantial elevation in performance. The\nproposed method holds promise for a wide range of applications in slow-motion\nmedia content, such as music or sports video production.", "published": "2023-12-22 10:23:09", "link": "http://arxiv.org/abs/2312.14586v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Room Acoustic Rendering Networks with Control of Scattering and Early\n  Reflections", "abstract": "Room acoustic synthesis can be used in Virtual Reality (VR), Augmented\nReality (AR) and gaming applications to enhance listeners' sense of immersion,\nrealism and externalisation. A common approach is to use Geometrical Acoustics\n(GA) models to compute impulse responses at interactive speed, and fast\nconvolution methods to apply said responses in real time. Alternatively,\ndelay-network-based models are capable of modeling certain aspects of room\nacoustics, but with a significantly lower computational cost. In order to\nbridge the gap between these classes of models, recent work introduced delay\nnetwork designs that approximate Acoustic Radiance Transfer (ART), a GA model\nthat simulates the transfer of acoustic energy between discrete surface patches\nin an environment. This paper presents two key extensions of such designs. The\nfirst extension involves a new physically-based and stability-preserving design\nof the feedback matrices, enabling more accurate control of scattering and,\nmore in general, of late reverberation properties. The second extension allows\nan arbitrary number of early reflections to be modeled with high accuracy,\nmeaning the network can be scaled at will between computational cost and early\nreverb precision. The proposed extensions are compared to the baseline\nART-approximating delay network as well as two reference GA models. The\nevaluation is based on objective measures of perceptually-relevant features,\nincluding frequency-dependent reverberation times, echo density build-up, and\nearly decay time. Results show how the proposed extensions result in a\nsignificant improvement over the baseline model, especially for the case of\nnon-convex geometries or the case of unevenly distributed wall absorption, both\nscenarios of broad practical interest.", "published": "2023-12-22 12:47:23", "link": "http://arxiv.org/abs/2312.14658v2", "categories": ["cs.SD", "eess.AS", "76Q05 (Primary) 93C43, 94A12 (Secondary)"], "primary_category": "cs.SD"}
{"title": "The UmboMic: A PVDF Cantilever Microphone", "abstract": "Objective: We present the \"UmboMic,\" a prototype piezoelectric cantilever\nmicrophone designed for future use with totally-implantable cochlear implants.\nMethods: The UmboMic sensor is made from polyvinylidene difluoride (PVDF)\nbecause of its low Young's modulus and biocompatibility. The sensor is designed\nto fit in the middle ear and measure the motion of the underside of the eardrum\nat the umbo. To maximize its performance, we developed a low noise charge\namplifier in tandem with the UmboMic sensor. This paper presents the\nperformance of the UmboMic sensor and amplifier in fresh cadaveric human\ntemporal bones. Results: When tested in human temporal bones, the UmboMic\napparatus achieves an equivalent input noise of 32.3 dB SPL over the frequency\nrange 100 Hz to 7 kHz, good linearity, and a flat frequency response to within\n10 dB from about 100 Hz to 6 kHz. Conclusion: These results demonstrate the\nfeasibility of a PVDF-based microphone when paired with a low-noise amplifier.\nThe reported UmboMic apparatus is comparable in performance to a conventional\nhearing aid microphone. Significance: The proof-of-concept UmboMic apparatus is\na promising step towards creating a totally-implantable cochlear implant. A\ncompletely internal system would enhance the quality of life of cochlear\nimplant users.", "published": "2023-12-22 00:03:59", "link": "http://arxiv.org/abs/2312.14339v1", "categories": ["eess.AS", "cs.SD", "physics.med-ph"], "primary_category": "eess.AS"}
{"title": "Multimodal Attention Merging for Improved Speech Recognition and Audio\n  Event Classification", "abstract": "Training large foundation models using self-supervised objectives on\nunlabeled data, followed by fine-tuning on downstream tasks, has emerged as a\nstandard procedure. Unfortunately, the efficacy of this approach is often\nconstrained by both limited fine-tuning compute and scarcity in labeled\ndownstream data. We introduce Multimodal Attention Merging (MAM), an attempt\nthat facilitates direct knowledge transfer from attention matrices of models\nrooted in high resource modalities, text and images, to those in\nresource-constrained domains, speech and audio, employing a zero-shot paradigm.\nMAM reduces the relative Word Error Rate (WER) of an Automatic Speech\nRecognition (ASR) model by up to 6.70%, and relative classification error of an\nAudio Event Classification (AEC) model by 10.63%. In cases where some\ndata/compute is available, we present Learnable-MAM, a data-driven approach to\nmerging attention matrices, resulting in a further 2.90% relative reduction in\nWER for ASR and 18.42% relative reduction in AEC compared to fine-tuning.", "published": "2023-12-22 02:08:40", "link": "http://arxiv.org/abs/2312.14378v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Unsupervised Harmonic Parameter Estimation Using Differentiable DSP and\n  Spectral Optimal Transport", "abstract": "In neural audio signal processing, pitch conditioning has been used to\nenhance the performance of synthesizers. However, jointly training pitch\nestimators and synthesizers is a challenge when using standard audio-to-audio\nreconstruction loss, leading to reliance on external pitch trackers. To address\nthis issue, we propose using a spectral loss function inspired by optimal\ntransportation theory that minimizes the displacement of spectral energy. We\nvalidate this approach through an unsupervised autoencoding task that fits a\nharmonic template to harmonic signals. We jointly estimate the fundamental\nfrequency and amplitudes of harmonics using a lightweight encoder and\nreconstruct the signals using a differentiable harmonic synthesizer. The\nproposed approach offers a promising direction for improving unsupervised\nparameter estimation in neural audio applications.", "published": "2023-12-22 08:10:30", "link": "http://arxiv.org/abs/2312.14507v3", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Creating New Voices using Normalizing Flows", "abstract": "Creating realistic and natural-sounding synthetic speech remains a big\nchallenge for voice identities unseen during training. As there is growing\ninterest in synthesizing voices of new speakers, here we investigate the\nability of normalizing flows in text-to-speech (TTS) and voice conversion (VC)\nmodes to extrapolate from speakers observed during training to create unseen\nspeaker identities. Firstly, we create an approach for TTS and VC, and then we\ncomprehensively evaluate our methods and baselines in terms of intelligibility,\nnaturalness, speaker similarity, and ability to create new voices. We use both\nobjective and subjective metrics to benchmark our techniques on 2 evaluation\ntasks: zero-shot and new voice speech synthesis. The goal of the former task is\nto measure the precision of the conversion to an unseen voice. The goal of the\nlatter is to measure the ability to create new voices. Extensive evaluations\ndemonstrate that the proposed approach systematically allows to obtain\nstate-of-the-art performance in zero-shot speech synthesis and creates various\nnew voices, unobserved in the training set. We consider this work to be the\nfirst attempt to synthesize new voices based on mel-spectrograms and\nnormalizing flows, along with a comprehensive analysis and comparison of the\nTTS and VC modes.", "published": "2023-12-22 10:00:24", "link": "http://arxiv.org/abs/2312.14569v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Effects of Signal-to-Noise Ratio on Generative Adversarial Networks\n  Applied to Marine Bioacoustic Data", "abstract": "In recent years generative adversarial networks (GANs) have been used to\nsupplement datasets within the field of marine bioacoustics. This is driven by\nfactors such as the cost to collect data, data sparsity and aid preprocessing.\nOne notable challenge with marine bioacoustic data is the low signal-to-noise\nratio (SNR) posing difficulty when applying deep learning techniques such as\nGANs. This work investigates the effect SNR has on the audio-based GAN\nperformance and examines three different evaluation methodologies for GAN\nperformance, yielding interesting results on the effects of SNR on GANs,\nspecifically WaveGAN.", "published": "2023-12-22 16:27:12", "link": "http://arxiv.org/abs/2312.14806v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Implantable Piezofilm Middle Ear Microphone: Performance in Human\n  Cadaveric Temporal Bones", "abstract": "Purpose: One of the major reasons that totally implantable cochlear\nmicrophones are not readily available is the lack of good implantable\nmicrophones. An implantable microphone has the potential to provide a range of\nbenefits over external microphones for cochlear implant users including the\nfiltering ability of the outer ear, cosmetics, and usability in all situations.\nThis paper presents results from experiments in human cadaveric ears of a\npiezofilm microphone concept under development as a possible component of a\nfuture implantable microphone system for use with cochlear implants. This\nmicrophone is referred to here as a drum microphone (DrumMic) that senses the\nrobust and predictable motion of the umbo, the tip of the malleus. Methods: The\nperformance was measured of five DrumMics inserted in four different human\ncadaveric temporal bones. Sensitivity, linearity, bandwidth, and equivalent\ninput noise were measured during these experiments using a sound stimulus and\nmeasurement setup. Results: The sensitivity of the DrumMics was found to be\ntightly clustered across different microphones and ears despite differences in\numbo and middle ear anatomy. The DrumMics were shown to behave linearly across\na large dynamic range (46 dB SPL to 100 dB SPL) across a wide bandwidth (100 Hz\nto 8 kHz). The equivalent input noise (0.1-10 kHz) of the DrumMic and amplifier\nreferenced to the ear canal was measured to be 54 dB SPL and estimated to be 46\ndB SPL after accounting for the pressure gain of the outer ear. Conclusion: The\nresults demonstrate that the DrumMic behaves robustly across ears and\nfabrication. The equivalent input noise performance was shown to approach that\nof commercial hearing aid microphones. To advance this demonstration of the\nDrumMic concept to a future prototype implantable in humans, work on\nencapsulation, biocompatibility, connectorization will be required.", "published": "2023-12-22 17:18:24", "link": "http://arxiv.org/abs/2312.14844v1", "categories": ["eess.AS", "cs.SD", "physics.med-ph"], "primary_category": "eess.AS"}
