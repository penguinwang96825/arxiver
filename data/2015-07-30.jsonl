{"title": "One model, two languages: training bilingual parsers with harmonized\n  treebanks", "abstract": "We introduce an approach to train lexicalized parsers using bilingual corpora\nobtained by merging harmonized treebanks of different languages, producing\nparsers that can analyze sentences in either of the learned languages, or even\nsentences that mix both. We test the approach on the Universal Dependency\nTreebanks, training with MaltParser and MaltOptimizer. The results show that\nthese bilingual parsers are more than competitive, as most combinations not\nonly preserve accuracy, but some even achieve significant improvements over the\ncorresponding monolingual parsers. Preliminary experiments also show the\napproach to be promising on texts with code-switching and when more languages\nare added.", "published": "2015-07-30 10:53:11", "link": "http://arxiv.org/abs/1507.08449v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Sentence Simplification Using Deep Semantics", "abstract": "We present a novel approach to sentence simplification which departs from\nprevious work in two main ways. First, it requires neither hand written rules\nnor a training corpus of aligned standard and simplified sentences. Second,\nsentence splitting operates on deep semantic structure. We show (i) that the\nunsupervised framework we propose is competitive with four state-of-the-art\nsupervised systems and (ii) that our semantic based approach allows for a\nprincipled and effective handling of sentence splitting.", "published": "2015-07-30 11:05:01", "link": "http://arxiv.org/abs/1507.08452v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilayer Network of Language: a Unified Framework for Structural\n  Analysis of Linguistic Subsystems", "abstract": "Recently, the focus of complex networks research has shifted from the\nanalysis of isolated properties of a system toward a more realistic modeling of\nmultiple phenomena - multilayer networks. Motivated by the prosperity of\nmultilayer approach in social, transport or trade systems, we propose the\nintroduction of multilayer networks for language. The multilayer network of\nlanguage is a unified framework for modeling linguistic subsystems and their\nstructural properties enabling the exploration of their mutual interactions.\nVarious aspects of natural language systems can be represented as complex\nnetworks, whose vertices depict linguistic units, while links model their\nrelations. The multilayer network of language is defined by three aspects: the\nnetwork construction principle, the linguistic subsystem and the language of\ninterest. More precisely, we construct a word-level (syntax, co-occurrence and\nits shuffled counterpart) and a subword level (syllables and graphemes) network\nlayers, from five variations of original text (in the modeled language). The\nobtained results suggest that there are substantial differences between the\nnetworks structures of different language subsystems, which are hidden during\nthe exploration of an isolated layer. The word-level layers share structural\nproperties regardless of the language (e.g. Croatian or English), while the\nsyllabic subword level expresses more language dependent structural properties.\nThe preserved weighted overlap quantifies the similarity of word-level layers\nin weighted and directed networks. Moreover, the analysis of motifs reveals a\nclose topological structure of the syntactic and syllabic layers for both\nlanguages. The findings corroborate that the multilayer network framework is a\npowerful, consistent and systematic approach to model several linguistic\nsubsystems simultaneously and hence to provide a more unified view on language.", "published": "2015-07-30 15:11:00", "link": "http://arxiv.org/abs/1507.08539v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Information-theoretical analysis of the statistical dependencies among\n  three variables: Applications to written language", "abstract": "We develop the information-theoretical concepts required to study the\nstatistical dependencies among three variables. Some of such dependencies are\npure triple interactions, in the sense that they cannot be explained in terms\nof a combination of pairwise correlations. We derive bounds for triple\ndependencies, and characterize the shape of the joint probability distribution\nof three binary variables with high triple interaction. The analysis also\nallows us to quantify the amount of redundancy in the mutual information\nbetween pairs of variables, and to assess whether the information between two\nvariables is or is not mediated by a third variable. These concepts are applied\nto the analysis of written texts. We find that the probability that a given\nword is found in a particular location within the text is not only modulated by\nthe presence or absence of other nearby words, but also, on the presence or\nabsence of nearby pairs of words. We identify the words enclosing the key\nsemantic concepts of the text, the triplets of words with high pairwise and\ntriple interactions, and the words that mediate the pairwise interactions\nbetween other words.", "published": "2015-07-30 03:10:18", "link": "http://arxiv.org/abs/1508.03530v1", "categories": ["cs.CL", "physics.data-an", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Tag-Weighted Topic Model For Large-scale Semi-Structured Documents", "abstract": "To date, there have been massive Semi-Structured Documents (SSDs) during the\nevolution of the Internet. These SSDs contain both unstructured features (e.g.,\nplain text) and metadata (e.g., tags). Most previous works focused on modeling\nthe unstructured text, and recently, some other methods have been proposed to\nmodel the unstructured text with specific tags. To build a general model for\nSSDs remains an important problem in terms of both model fitness and\nefficiency. We propose a novel method to model the SSDs by a so-called\nTag-Weighted Topic Model (TWTM). TWTM is a framework that leverages both the\ntags and words information, not only to learn the document-topic and topic-word\ndistributions, but also to infer the tag-topic distributions for text mining\ntasks. We present an efficient variational inference method with an EM\nalgorithm for estimating the model parameters. Meanwhile, we propose three\nlarge-scale solutions for our model under the MapReduce distributed computing\nplatform for modeling large-scale SSDs. The experimental results show the\neffectiveness, efficiency and the robustness by comparing our model with the\nstate-of-the-art methods in document modeling, tags prediction and text\nclassification. We also show the performance of the three distributed solutions\nin terms of time and accuracy on document modeling.", "published": "2015-07-30 06:44:37", "link": "http://arxiv.org/abs/1507.08396v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
